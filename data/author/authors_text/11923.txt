The Automated Acquisit ion of Topic Signatures for Text 
Summarizat ion 
Chin -Yew L in  and  Eduard  Hovy  
In fo rmat ion  S(:i(umes I l l s t i tu te  
Un ivers i ty  of Southern  Ca l i fo rn ia  
Mar ina  del Rey, CA  90292, USA 
{ cyl,hovy }C~isi.edu 
Abst rac t  
In order to produce, a good summary, one has 
to identify the most relevant portions of a given 
text. We describe in this t)at)er a method for au- 
tomatically training tel)it, signatures--sets of related 
words, with associated weights, organized around 
head topics and illustrate with signatm'es we cre- 
;tt.ed with 6,194 TREC collection texts over 4 se- 
lected tot)ics. We descril)e the l)ossible integration 
of' tolli(: signatures with ontoh)gies and its evaluaton 
on an automate(l text summarization system. 
1 I n t roduct ion  
This t)aper describes the automated (:reation of what 
we call topic signatures, constructs that can I)lay a 
central role. in automated text summarization and 
information retrieval. ToI)ic signatures can lie used 
to identify the t)resence of a (:omph~x conce.pt a 
concept hat consists of several related coinl)onents 
in fixed relationships. \]~.c.sta'uvant-'uisit, for examph~, 
invoh,es at h,ast the concel)ts lltCgFIt, t'.(tt, pay, and 
possibly waiter, all(l Dragon Boat PcstivaI (in Tat- 
wan) involves the Ct)llC(!l)t,S cal(tlzt'lt,s (a talisman to 
ward off evil), rnoza (something with the t)ower of 
preventing pestilen(:e and strengthening health), pic- 
tures of Ch, un9 Kuei (a nemesis of evil spirits), eggs 
standing on end, etc. Only when the concepts co- 
occur is one licensed to infer the comph:x concept; 
cat or moza alone, for example, are not sufficient. At 
this time, we do not c.onsider the imerrelationships 
among tile concepts. 
Since many texts may describe all the compo- 
nents of a comI)lex concept without ever exI)lic- 
itly mentioning the mlderlying complex concel/t--a 
tol)ic--itself, systems that have to identify topic(s), 
for summarization or information retrieval, require 
a method of infcu'ring comt)h'x concelltS fl'om their 
component words in the text. 
2 Re la ted  Work  
In late 1970's, \])e.long (DeJong, 1982) developed a
system called I"tIUMP (Fast Reading Understand- 
ing and Memory Program) to skim newspaper sto- 
ries and extract the main details. FRUMP uses 
a data structure called sketchy script to organize 
its world knowh'dge. Each sketchy script is what 
FRUMI ) knows al)out what can occur in l)articu- 
lar situations such as denmnstrations, earthquakes, 
labor strike.s, an(t so on. FRUMP selects a t)artic- 
ular sketchy script based on clues to styled events 
in news articles. In other words, FRUMP selects an 
eml)t3 ~ t(uni)late 1whose slots will be tilled on the fly 
as t"F\[UMP reads a news artMe. A summary is gen- 
erated })ased on what has been (:al)tured or filled in 
the teml)Iate. 
The recent success of infornmtion extractk)n re- 
search has encore'aged the FI{UM1 ) api)roach. The 
SUMMONS (SUMMarizing Online News artMes) 
system (McKeown and Radev, 1999) takes tem- 
l)late outputs of information extra(:tion systems de- 
velofmd for MUC conference and generating smn- 
maries of multit)le news artMes. FRUMP and SUM- 
MONS both rely on t/rior knowledge of their do- 
mains, th)wever, to acquire such t)rior knowledge 
is lal)or-intensive and time-consuming. I~)r exam-- 
l)le, the Unive.rsity of Massa(:husetts CIRCUS sys- 
l.enl use(l ill the MUC-3 (SAIC, 1998) terrorism do- 
main required about 1500 i)erson-llours to define ex- 
traction lmtterns 2 (Rilotf, 1996). In order to make 
them practical, we need to reduce the knowhxlge n- 
gineering bottleneck and iml)rove the portability of 
FI{UMI ) or SUMMONS-like systems. 
Since the worhi contains thousands, or perhal)s 
millions, of COml)lex (:on(:et)ts , it is important; to be 
able to learn sketchy scripts or extraction patterns 
automatically from corpora -no existing knowledge 
base contains nearly enough information. (Rilotf aim 
Lorenzen, 1999) 1)resent a system AutoSlog-TS that 
generates extraction i)atterns and learns lexical con- 
straints automatically fl'om t)rec\]assified text to al- 
leviate the knowledge ngineering I)ottleneck men- 
tioned above. Although Riloff al)plied AutoSlog-TS 
l\Ve viewed sketchy s(:lil)tS and teml)lates as equivalent 
(ollstrllctS ill the sense that they sl)ecil ~, high level entities 
and relationships for specific tot)its. 
2Aii extra(:l;iOll pattt!rlk is essentially ;t case fraine contains 
its trigger word, enabling conditions, variable slots, and slot 
constraints. C IRCUS uses a database of extraction patterns 
to t~alSe texts (l{ilolI', 1996). 
495 
to text categorization and information extraction, 
the concept of relevancy signatures introduced by 
her is very similar to the topic si.qnatures we pro- 
posed in this paper. Relevancy signatures and topic 
signatures arc both trained on preclassitied ocu- 
ments of specific topics and used to identify the 
presence of the learned topics in previously unseen 
documents. The main differences to our approach 
are: relevancy signatures require a parser. They are 
sentence-based and applied to text categorization. 
On the contrary, topic signatures only rely on cor- 
pus statistics, arc docmnent-based a and used in text 
smnmarization. 
In the next section, we describe the automated 
text smmnarization system SUMMARIST that we 
used in the experiments to provide the context of 
discussion. We then define topic signatures and de- 
tail the procedures for automatically constructing 
topic signatures. In Section 5, we give an overview 
of the corpus used in the evaluation. In Section 6 we 
present he experimental results and the possibility 
of enriching topic signatures using an existing ontol- 
ogy. Finally, we end this paper with a conclusion. 
3 SUMMARIST  
SUMMARIST (How and Lin, 1999) is a system 
designed to generate summaries of multilingual in- 
put texts. At this time, SUMMARIST can process 
English, Arabic, Bahasa Indonesia, Japanese, Ko- 
rean, and Spanish texts. It combines robust natural 
language processing methods (morl)hologieal trans- 
formation and part-of-speech tagging), symbolic 
world knowledge, and information retrieval tech- 
niques (term distribution and frequency) to achieve 
high robustness and better concept-level generaliza-- 
tion. 
The core of SUMMARIST is based on the follow- 
ing 'equation!: 
summarization = topic identification + 
topic interpretation + generation. 
These three stages are: 
Topic Ident i f ieat lon:  Identify the most imtmrtant 
(central) topics of the texts. SUMMARIST 
uses positional importance, topic signature, and 
term frequency. Importance based on discourse 
structure will be added later. This is tile most 
developed stage in SUMMARIST. 
Topic I n te rpretat ion :  ~i~-) fllse concepts such as 
waiter, menu, and food into one generalized 
concept restaurant, we need more than the sin> 
pie word aggregation used in traditional infor- 
mation retrieval. We have investigated concept 
aWe would like to use only the relevant parts of documents 
to generate topic signatures in the future, qkext segmentation 
algorithms uch as TextTiling (Ilearst, 1997) can be used to 
find subtopic segments in text. 
ABCNEWS.cona  : De lay  in  Hand ing  F l ight  990  \ [ ' robe  to  FB I  
N'I 'SI3 C l la i tn lan  Jarl leS t la l l  says  Egypt ian  clff icials Iv811l to  I,~view res t l l t s  
of  t i le  invest igat ion  intcl lhe  cras l l  o f  l lggyptA i r  F l ight  990 before  t i le  case  
i~ lu r l led  over  Ic, t i le Fi31, 
Nt lv.  IG - U S. i lxvestigl~lo\[~ lLppear to  be leat l i l lg i i Iore thg l l  eveF low~trd 
t i le poss ib i l i ty  that  one  o f  the  cc~-pilot~ o f  EgyptA i r  F l ight  990 may have  
de \ [ ihera le ly  c rashed t i le p lane  las t  I lafl l lth, k i l l i ng  all 217 peop le  on  board .  
f la i l ' ever .  US .  o f f i c ia ls  say  t i le  Nat iona l  T ran~por ' ta t ion  Sa fety  Board  wi l l  
de lay  t rans fer r ing  tile invegt iga l ion  o f  the  Oct  31 c rash  to  tilt: FI31 - the  
agency  that  wot l id  lead i~ c r imina l  p robe  - for at  least  tt few days .  to  M Iow 
Egypt ian  exper ts  to rev iew ev idence  ill t i le case.  
gtts l ) ic iot l~ of  fou l  p lay  were  ra i sed  a f te r  invest igators  l i s ten ing  to  rt tape  
ftol l l  l i l t  cockp i t  vo ice recorder  i so la ted  a re l ig ious  prayer  or s ta te l l l e l l t  
made by t i le co -p i lo t  jus t  be fore  t i le  p lane 's  autop i lo t  was turned  o f f  
s l id  the  p lane  began i ts  in i t ia l  p lunge  in to  t i le A t lant i c  Ocean of f  Mas -  
s r tcht tset t$ '  Na l l tucket  \ [s ia l ld .  
Over  tile' pas t  week .  a f te r  muc i l  e f fo r t ,  t i le  NTSJB and  t i le  Navy  succeeded 
ill I ocat i l lg  the  p lane 's  two  "b lack  boxes , "  th~ cockp i t  vo ice recorder  and  
lhe  f l ight  data  recorder .  
The  tape  ind icates  t l l a t  shor t ly  a f te r  the  p lane  leve led ~ff  a t  i ts c ru i s ing  
a l t i tude  o f  as ,000  feet ,  t i le  cl~ief p i lo t  o f  t i le a i rc ra f t  left  the  p lane 's  
cockp i t ,  l eav ing  one  o f  t i le  twc~ co-p i lo ts  nIol le t i lere as the  a i rc ra f t  began 
its descent .  
Figure 1: A Nov. 16 1999 ABC News page sumnmry 
generated by SUMMARIST. 
counting and topic signatures to tackle tile fll- 
sion problem. 
Summary Generat ion :  SUMMARIST can pro- 
duce keyword and extract type summaries. 
Figure 1 shows an ABC News page summary 
about EgyptAir Flight 990 by SUMMARIST. SUM- 
MARIST employs several different heuristics in tile 
topic identification stage to score terms and sen- 
tences. The score of a sentence is simply the sum 
of all the scores of content-bearing terms in the sen- 
tence. These heuristics arc implemented in separate 
modules using inputs from preprocessing modules 
such as tokenizer, part-of-speech tagger, morpholog- 
ical analyzer, term frequency and tfidf weights cal- 
culator, sentence length calculator, and sentence lo- 
cation identifier. \Ve only activate the position mod- 
ule, tile tfidfmodule, and the. topic signature module 
for comparison. We discuss the effectiveness of these 
modules in Section 6. 
4 Top ic  S ignatures  
Before addressing the problem of world knowledge 
acquisition head-on, we decided to investigate what 
type of knowledge would be useflfl for summariza- 
tion. After all, one can spend a lifetime acquir- 
ing knowledge in just a small domain. But what 
is tile minimum amount of knowledge we need to 
enable effective topic identification ms illustrated by 
the restaurant-visit example? Our idea is simple. 
We would collect a set of terms 4 that were typi- 
cally highly correlated with a target concept from a 
preclassified corpus such as TREC collections, and 
then, during smnmarization, group the occurrence of 
the related terms by the target concept. For exam- 
pie, we would replace joint instances of table, inert'u, 
waiter, order, eat, pay, tip, and so on, by the single 
phrase restaurant-visit, in producing an indicative 
4Terms can be stemmed words, bigrams, or trigrams. 
496 
sulnlllary. \Ve thus defined a tot)it signat.ure as a 
family of related terms, as follows: 
~I'S = { topic, sifl~zutu.rc. } 
= {topic,< ( t , ,w l ) , . . . , ( t , , ,w , , )  >} (1) 
where topic is the target concet)t and .,d.q)zat~Lrc is 
a vector of related ternls. Each t, is an term ldghly 
correlated to topic with association weight w/. The 
number of related terms 7z can tie set empirically 
according to a cutot\[ associated weight. We. describe 
how to acquire related terms and their associated 
weights in the next section. 
4.1  S ignature  Term Ext rac t ion  and  Weight  
Es t imat ion  
()n the assumption that semantically related terms 
tend to co-occur, on(' can construct topic signa- 
tures fl'om preclassified text using the X 2 test, mu-. 
tual information, or other standard statistic tests 
and infornlation-theoreti(: measures. Instead of X '2, 
we use likclih.ood ratio (Dunniug, 1993) A, sin(:e A 
i,; more apI)rot)riate for si/arse data than X 2 test 
and the quantity -21o9A is asymi)t(/tically X~ dis- 
tril)ute(15. Therefore, we Call (leterndnc the (:onti- 
( lence level for a specific -21o9A value l/y looking ut) 
X :~ (tistril)ution table and use tlm value to sel(,,ct an 
at)i)rot)riate cutoff associated weight. 
We have documents l)\['e.classitied into a :;('~t, "R. of 
relevant exts and a set ~. of nonrelewmt exl;s for a 
given topic. Assuming the following two hyl)othe,'~es: 
t typothes is  1 ( I f l ) :  t'(~Pvlti) = P = P('PvltT/), i.e. 
the r(.,lewmcy of a d()(:|lment is in(teI)en(hmt, of 
t i .  
I \ ] \ [ypothes is  2 ( t t2) :  I'('Pv\[ti) == lh ~ 1)'2 - 
t)('Pvlt, i), i.e. th(~ t)r(.':;(;n(:(~ of t i indi(:~Lt(.~.'; strong 
r(~levan(:y ~ssunling \]h >> 1)2 ?
and the following 2-10=2 contingency tabl(;: 
where Ol~ is the fiequency of term ti occurring in 
the. l 'e lev;tnt  set ,  012 is the \ [ r ( !qu(nlcy of Lerm t i t)c- 
curring in the  \] lol lreleval lt ,  set ,  O21 is the  f le(l l lel l( :y 
of tt;rnl \ [ i?  ti occurring in the rtdevant set, O._,~ is 
the fl'equ(mcy of term l.i ? ti o(:curring in the non- 
l ' e leva i i t  seL.  
-kssmning a l)inomial distril)ution: 
C;) b(~; ,,., :/.) = :,:~(1 - .~:)(" ") (2 )  
5This assumes |ha l  the ratio is between the inaximuni like> 
\[ihood est, im&t.(! over a .qll})part of l;}l(! i)alatlllC't(~r sl)a(:(~ ;tll(\] l.h(! 
lllaxillUllll likelihood (}sI.i|II}tlA~ ov(!r the (Hltill! i)al'aillt~tt!r si);t(:e. 
Set! (Manning ;tnd Sch/itze, I999) t)ag, es 172 l.o 175 for d(!t.ails. 
then the likelihood for HI is: 
L(H~) = b(Ot~; 0~ + Ou,,p)b(O:,~; 0:,, + Om,,p) 
and for //2 is: 
L(H2)  = D(OI 1; O11 Jr" (')12, Pl )b(O21; (')21 Jr- (,)22,1)2) 
The -2log,\ value is then computed ms follows: 
1.(f/1 ) 
m --21o 9 - -  
L( i t  2 ) 
b(O 11 ; O I  1 + O12,  P) I J (021 : O21 + 022 , P) 
- -21o 9 
1'((-)1 l ; ( )11  + O1'-),  P I )h (O21 ; O21 q- ( )22  , P2 ) 
: - -2 ( (O l l  +021) lo r_ Jp+( ( )12+022) lo9(1 - - l~) - -  (,~1) 
(? ) l l l o ' JP l+Ol21og( l  " t '1 )+0211ogp2-~0221o0(1- f~2) ) )  
-- '.2.,~' x (~ ' i (7~) -  ;~(~19- ) )  (4 )  
= 2,'v x Z(P~;  T )  (5 )  
whel 'e  N = O l t  -F O12 -1- O21 -I- 022 is the  to ta l  l lum-. 
her of t, ernl occurrence, in the corpus, 7/('/~) is the 
entropy of terms over relevant and nonrelevant sets 
of documents, 7/ ( ' fe l t  ) is the entropy of a given term 
OVel" relev;inL ~/nd nonl ' ( .qeval l t  sets  of doel l inel lLS, ~tll(1 
Z('R.; T) i:; the inutual information between docu- 
ment relevancy and a given t('.rm. Equation 5 indi- 
cates that mutual inforntation 6 is an e(tuiwdent mea- 
sur(.' t() lik(.qiho(id ratio when we assume a binomial 
distribution and a 2-by-2 ('ontingency table. 
To crest(; topic .~dgnature for a given tot)ic , we: 
1. (:lassify doctunents as relevant or nonrclcwmt 
according t() tile given topic 
2. comt)ut.e the -21oflA wdue using Equation 3 for 
each Lcrm in the document colle(:Lion 
"{. rank t, erms according 1o their -2lo9~ value 
4. select a c(mfid(mce l , vel fi'om the A;: (listril)utiotl 
table; (letermin(~ the cutotf associated weight, 
mid the numl)(n" of t(nms to he included iIl the 
signatures 
5 The Corpus  
The training data derives Kern the Question and 
Answering summary evahmtion data provided l)y 
T IPSTEI / . -SUMMAC (Mani et al, 1998) that is a 
sttbset of the TREC collectioliS. The TREC data is a 
collection of texts, classified into various topics, used 
for formal ewduaLions of information retrieval sys- 
tems in a seri(~s of annual (:omparisons. This data set: 
contains essential text fragnients (phrases, (:Iausos, 
iuld sentences) which must 1)e included in SUllltIlarios 
to ~tnswer some TI{EC tel)its. These fl'agments are 
each judged 1)y a hmnan judge. As described in Se(:- 
tion 3, SUMMAI~IST employs several independent 
nlo(hlles to assign a score to each SelltA:llCe~ and Chell 
COlll})illeS the st.'or(.'.% L() decide which sentences to ex- 
tract from the input text;. ()n0. can gauge the efticacy 
(>l'he lllll\[lla} inrormalion is defined according to chapter 2 
of ((;over and Thomas, i991) and is not tile i)airwis(~ mutual 
inforlnalion us(!d in ((;hur(:h and llanks, 1990). 
497 
TREC Top ic  Da~cr lp t ion  
(nunQ Number :  151 
( t i t le}  Top ic :  Co, p ing  w i th  overc rowded pr i sons  
(dese} Deser i l l t io l l :  
The  doeu l laent  will p rov ide  in f ,~rn lat ion ol~ jai l  and  pr ison overc rowd iuK  
and  how i r lmates  are forced to cope  wi th  th,~se cond i t ions ;  or it wil l  
revea l  p lan~ to  re l ieve  ti le overc rowded ?o l ld i t lon .  
(nar t )  Nar ra t ive :  
A re levant  docunaent  will descr ibe  scene~ of overcro~vdi l lg that  have  
beco lne  all too  crlllllllOll ill ja i l s  and  pr i sons  a ro t tnd  the  count ry ,  T i le  
document  will i dent i fy  how inmates  are forced to  cope w i th  those  over -  
crowded cond i t ion~,  and/or  what  ti le Cc l r reet iona l  Syste l l l  is do ing ,  or 
ph lnn ing  to do,  to a l lev ia te  ti le c rowded col ld i t io l l .  
(/top) 
Test  Quest ions  
QI  Me'hat are  name and/or  locat ion  of ti le cor rec t ion  fae i l i l ies  
where  the  repor ted  ~vercrowd ing  ex is ts?  
Q2 x~Vhat negat ive  exper iences  have  there  been  at t i le overc rowded 
fac i l i t ies  (whether  or not tile)" are thought  to have  been  caused  
by  the  overc rowd lng)?  
Q3 What  measures  have  been  taken/p la ia i led / recommended (e tc . )  
to aecon l lnod~te  more  i l l l l la Ies zlt pena l  fac i l i t ies ,  e .g . ,  doub l i l l g  
tip, Ile~y COllStructlon? 
Q,I ~,Vhat measures  have  been  taken/planned/rec~mnlel,ded (etc .} 
to reduce  ti le l lt l l l lber of Dew il l l i \]ate$, e .g . ,  morator iums 
on admisMon,  a l te rnat ive  pena l t ies ,  p rograme to reduce  
c r ime/ rec ld iv i sm? 
Q5 What  measures  have  been  taken/p lanned/ recommended (e tc . )  
to reduce  ti le number  of ex i s t ing  inmates  at an overcrc~wded 
fac i l i ty ,  e .g . .  g rant ing  ear ly  re lease ,  t rnns fer ing  to  uncrowded 
fac i l i t ies?  
Sample  Answer  Keys  
(DOCNO)  AP891027-0063 ( /DOCNO)  
(F ILE ID)  AP -NR-  10-27-89 0615EDT( /F ILE ID)  
( IST_L INE) r  a PM-Cha ined Inmates  10-27 0335( / IST .L INE)  
(2ND-L INE)PM-Cha ined  lnmates ,0344 ( /2ND_L INE)  
( I IEAD)  lnmates  Cha ined  to 1.Vails in 13Mtimore Po l i ce  
S ta t ions ( / l lEAD)  
(DATEL INE)BALT IMOIT IE  (AP)  ( /DATEL INE} 
('tEXT) 
(Q ,q )pr i soner~ are  kept  cha ined  to the wall~ of local po l ice  lcJekup~ for 
as long as th ree  days  at a t ln~e I)ecattse of overc rowd ing  ill regu la r  je l l  
cel ls ,  pol ice sa id . ( /Q3} 
Overcrowd ing  at  the  (Q1) lqMt l rnore  County  Detent ion  Center ( /Q1)  
h~ forced pn l lee  tn  . .  
(/TEXT) 
Table 1: TREC topic description for topic 151, test 
questions expected to be answered by relewmt doc- 
uments, and a smnple document with answer key, s. 
of each module by comparing, for ditferent amounts 
of extraction, how many :good' sentences the module 
selects by itself. We rate a sentence as good simply 
if it also occurs in the ideal human-made xtract, 
and measure it using combined recall and precision 
(F-score). We used four topics r of total 6,194 doc- 
uments from the TREC collection. 138 of them are 
relevant documents with T IPSTER-SUMMAC pro- 
vided answer keys for the question and answering 
evaluation. Model extracts are created automati- 
cally from sentences contailfing answer keys. Table 
1 shows TREC topic description for topic 151, test 
questions expected to be answered by relevant doc- 
uments , and a sample relevant document with an- 
swer keys markup. 
7These four topics are: 
topic 151: Overcrowded Prisons, 1211 texts, 85 relevant; 
topic 257: Cigarette Consumption, 1727 texts, 126 relevant; 
topic 258: Computer Security, 1701 texts, 49 relevant; 
topic 271: Solar Power, 1555 texts, 59 relevant. 
SA relevant: document only needs to answer at least one of 
the five questions. 
6 Experimental Results 
In order to assess the utility of topic signatures in 
text sununarization, we follow the procedure de- 
scribed at the end of Section 4.1 to create topic 
signature for each selected TREC topic. Docu- 
ments are separated into relevant and nom'elevant 
sets according to their TREC relevancy judgments 
for each topic. We then run each document hrough 
a part-of-speech tagger and convert each word into 
its root form based on the \\h)rdNct lexical database. 
We also collect individual root word (unigram) fi'e- 
quency, two consecutive non-stopword 9 (bigram) fi'e- 
quency, and three consecutive non-stopwords (tri- 
gram) fi'equeney to facilitate the computation of the 
-21ogA value for each term. We expect high rank- 
ing bigram and trigram signature terms to be very 
informative. We set the cutoff associated weight at 
10.83 with confidence level ~t = 0.001 by looking up 
a X 2 statistical table. 
Table 2 shows the top 10 unigrmn, bigram, and tri- 
gram topic signature terms for each topic m. Several 
conclusions can be drawn directly. Terms with high 
-21ogA are indeed good indicators for their corre- 
sponding topics. The -2logA values decrease as the 
number of words in a term increases. This is rea- 
sonable, since longer terms usually occur less often 
than their constituents. However, bigram terms are 
more informative than nnigrant erms as we can ob- 
serve: jail//prison overervwding of topic 151, tobacco 
industry of topic 257, computer security of topic 258, 
and solar e'n, ergy/imwer of topic 271. These mLto- 
matically generated signature terms closely resemble 
or equal the given short TREC topic descriptions. 
Although trigram terms shown in the table, such 
as federal court order, philip morris 7~r, jet propul.. 
sion laboratory, and mobile telephone s:qstem are also 
meaningflfl, they do not demonstrate he closer term 
relationship among other terms in their respective 
topics that is seen in tlm bigram cases. We expect 
that more training data can improve tile situation. 
We notice that the -2logA values for topic 258 
are higher than those of the other three topics. As 
indicated by (Mani et al, 1998) the majority of rel- 
evant documents for topic 258 have the query topic 
as their main theme; while the others mostly have 
the query topics as their subsidiary themes. This 
implies that it is too liberal to assume all the terms 
in relevant documents of the other three topics are 
relevant. We plan to apply text segmentation algo- 
rithms such as TextTiling (Hearst, t997) to segment 
documents into subtopic units. We will then per- 
form the topic signature creation procedure only on 
tile relevant units to prevent inchlsion of noise terms. 
9\,Ve use the stopword list supplied with the SMAIIT re- 
trieval system. 
l?q'he -2logA values are not comparable across ngram cat- 
egories, since each ngraln category has its own sample space. 
498 
Top ic  
I :ll h~l al l l  -21,~gX \ ] l i~ la l l I  -21,,9X 
j a i l  t)3L I)1,1 e()tH~t 2, ja i l  Dit) 27:1 
c+,l l l l l} .IIJN ~21 eae ly  le+\]+.~lSt ? N,'~ :{t;\] 
, )v , . ) , '~ , ,wd ln~;  :?12:1. I~ ~ta l , .  D l~. , ,n  7.1 R72 
i l l ln?lt ," 2 : t l  7d5  s ta l , "  1,) i~, ,n, . i  67  ,3(~t~ 
~h+. l i f \ [  IF, I . i l o  ,1:~ 3 fill," l ; l  t(;2") 
s ta le  151 9t t~ ia i l  r l%l ' lctr~%vI\] l r ld I;1 ~\[ i  
I}l l~t l l i l ' l  I I  ~" I ";~' C(,tlt I + , l , i " t  t{ll.O!)l} 
i+tl-s,,rl 1,17, 3t),i h . .a l  j a i l  56  t i t+  
C l )y  133177 p l l . , )D  ( )vcy lc l , , i v th l l~  55 37:  +, 
, , v , . r , . r , ,wd ,+d 12N I)t)S i-(*lllt :l\[ fac i l i t  3 52 9o9  
10 S ignat t t ro  Torms o f  Tup ic  151  Ovorcrowdod Pr i sons  
"I'I I~I al I l  -21,,~11\ 
f - , I t . l~t l  c , ,u l~ <, l t t , . l  -I.', :),;11 
C, , l l l p \ ]y  c+,ll~('lll ,\]+'c\[+++" 3,5 12L 
+l,.kali+ i'i)iI i,\[~' +h, ' l l \ [ \ [  \[15 121 
~,11  i,) t l ; ,nk  :;.5 L21 
j , )o t l ; l l l k  IH ) l i5  :~.',.1'21 
pl l~C, l l , ' r  c+)l l : l l~ la i l  :~5 121 
91: i f , .  \ ] ) l l~t ) l l  i21) l l l t l~ ~N t).l\[\] 
t put  pl is+, l l  .2t~ :t-II 
c+~uuly  jaiL ~l ; l l , .  2 t~31 l 
h,,hl l,~e~l ja i l  2d  :l I I 
Top ic  10  S ig l ln t t l re  Tern ls  o f  Top ic  257  - ( l igar~t t~ Co| l s l l l ) l | ) t lo | l  
l :n i<r tun  21ogX I+i ~.rarll -- 2 / , , f / . \  'i r i4~am - 21, ,~A 
c lgrt l , . I t?+ .171; \[}:iN ~tlb:xt'c+) LIt( \ ] l l~l l~ ~il 7)iN I ,h i l ip  I I l , ) l l i+ t j l  2.~ ~DSI 
l ( )ht lcc~) : l l ; l  017  hn  t - lg / l l , - l l t -  t ;7 t2}I I r ) l \ ] i l I l a l l s  beDs~, l l  h<.(\[~f. 211 ~)t~\[l 
s I I IOk i l l~  28.t  19~ ph i l ip  t l l ( , l \ [ i~  5t  ()7;~ \[1111~ L'ikll('e\[ d '+\ [ l l l l  22 21. t  
~n l~,ke  15913.1  clarxl<'t1, :  %, 'at  t80 . t5  q t t  iri l ln cl l~ '.2'1 I IS  
I ,~ lh l l l a l l?  156. )375 to lh l l l l l l l~  i t l Y , ' l I l a t l t ) l lgk l  -t.t .13.1 qt l  q t t  f i~ ln  21 - I lS  
, ,~ha  I . tS 372  tobac? , )  elll()k.~ 112){}I  bll  b\[i bl l  20  22t i  
s ,~i la 12)i .121 ~il pat r i ck  t0 .  t55  c+)l lst l l l l l} l lo l l  bn  c lgar , . l te  2022d 
Illtll 113 ~+1~) c l~at+' l l~"  c~l l lpa l lV  :ID \[$1)D ~\[t+gtt a l l l . r \ [ iCtt l l  ~llI,lk?'(}llt 20226 
al l l , )k( ' l  10.1 I i0  ('el l l  l l l a lk+' l  36223 \[ l l l l~ Ca l l t 'e \ [  ht:gl\[ I  2(~ 22{i 
b\[~t 79 .90:1  ?~IN illt+ll+il++t :1t;.22:1 i l l a \ [ay~ia l l  +illk~\[tl>,ll+e t'4)l l lpi l l IV 2( I .22t \ ]  
Top ic  I0 S ignatur .  "I'~r)ns of Top ic  258 -- Co)nputor  Secur i ty  
I ~llial /lilt "2Ionia I t  i+/,r al l l  21,QIX "I'1 i~ratn  --21o9, X 
(:+lll l l)l ltOr 115!~ :151 C4, l l lp I l l l ' t  ae l ' t l l l l y  213331 )e l  I l t t /p l l \ [~i() l l  \ ] l th t )h l t ( l l y  \ [~  ~5.t  
v i rus  927.G7-1 ~\[ ; idt lgt l , "  s l t l \ [ t l ' l l l  17~ 5NN I l lh . ' l l  I lilt) 9R 85, t  
hacker  867 .377 FOl l lp t l t , ' t  +yS le l l l  1 -16.32~ C,+ltl++\]l I l l l iVet~il~,'  ~ lad l l \ [ t le  7}) IJNI 
in,) l  rl+ +i+;+~ 2i13'.! ) l , .~+-arch c,+ulte\[  l ; i2  .l I :i l awte l l c l "  b , ' rk , '  *'j~ lal l , ) l  al+,l  ,,. 79.0N \[ 
c , , rn , ' l l  3P+5 6+4 c , : ,ml ,u t , ' r  wrus  12~k033 I~+,++, je t  p tO l , t l L+ io t+ 79 .0~1 
un lv+' l? i ty  31)5 .95~ corne l i  U lX iVe le i ty  1(1~4 7-t l  U l ; iV ,q+i )y  ~; radu lx t , .  +tx ldt .  lll 79U~1 
+ysl+' l l l  290 .3"17  Iltl(:l,P;ll %t++npl)ll 107 .283 lawt l l l e ,+ l i v+: r tn ( . te  I igt l i () l lal  i\]\[) l\[I;~ 
I / tb , . ra lL : ) ly  2N7 521 in i l i ta ry  t ' ( , l l lp , l l . : r  106 .522 l iv~qll l ,) l?" i lu) i ,maL  lubora lo ry  {59195 
\ [ab  225 .51) ;  v i tu~ plo~t<' l l l l  1U6 522 c,) l l lp l l I (~r S ,~CUl i ly  eXpet l  66 .19G 
mecLa ly  128 .515 %vesl ~et l l l a l l  82  2 \ [0  ~ecu\ [ i t? , '  cenl{~\[  13ethesda  -19423 
Top ic  10  S ignature  Ter lns  o f  Top ic  271  So lar  Power  
I ' l l i g ta ln  - -21oqX t i ig t  ~ltn --2logX "I'r i ~;r hi l l  - -21o!~A 
so la r  -1S- l .315 e,~la~ e l te t l4y  2{Di 521 d iv i~ i ,m Inu l l ip l ,~  acress  31 3-17 
) t lazd i t  :10Pt 0IY) s<,lal l , t lw , ' t  9,1 210  n l , )b i l , :  l , , l , -ph , ,n , .  #c iv ic , ,  313 .17  
le,) 271; .932  ( 'h r i~t ia l l  a id  8 f i .211  b l i l l sh  It .cl l l l i l l l )R}' g \ [ , , l l p  23510 
it JtLi It l l l  2.5N.71):"+ l++,a S3'Sl,*III 711 5:{5 el l l I} l  he iNht  llXile 23+5111 
pax+lh , ,n  2133 81 I ill++tlllt. Ie i t ' j ) l l l ) l le  (115;l+i I'illllllCilll I lack i l l+;  I l Jd l l l l l l  22i+51(1 
i)(~tltld 12 / ,121  i t i , l i un l  p l , , j , . c l  112.697 ~l,~l lal  In r )h i l ,  + sa l , ' l l i te  23  511J 
t , lw~r  12G.35:1 lei l i  <+, , ,d -  61.~111 ha l ld l le ld  IIled~il," t , ' l eph , , l l , :  '23510 
\ [ , , , , k , ,u t  125 .ll3t; scie. l lc, ,  pa lk  ~'>.1 NS{) i l l ( ,h i le  ~ate l l l l . ,  . v>tetn  23  510 
i i l \ [ l l l i lSRl  1O9728 ~()llkl t ' i l l l t ' l ' l l t l i l I l , l  51t ~5{} I l l l l t l l lvl i l l  i g id i l ln l  I> l , l j ec t  23 ,510  
hc ,ydsh , t l  7N :173 l)p s l l la l  ?+1 ; /17  act iv t -  s+,la\[ *ys tern  15673 
Tattle 2: Top 10 signat.m'e t.erm.~; of mfigram, bigram, and trigram for fore" TREe  t.opics. 
6.1 Comparing Summary Extraction 
Effectiveness Using Topic Signatures+ 
TFII)t",  and Bas(,line Algor i thms 
In orde)" I() (~vahla(.(~ the (d\[+:ct.iv(,im.~s nf l(>l)i(: .~dgna- 
l;lll'(~S llS(~(\] ill SlllllIIN/l'y (~Xtl'it(:t;iOll, W{,  ~ CtIllll)~ll'(~ +flit! 
Sllltllll~tl'y StHII~011('CS ex(~ract,(~d 1)y the tol) ic si~Ilil\[lll'0, 
module+, basulin(.' module, and tfidf lnothll(~s with lm- 
nta'n annot,  at(~(l lllo(lo,\] Sllllllll}ll'ios. \VC III(+~}/SIlI'(+ + l;h(; 
l)c'rfl)rmanc(~' using a c()ml)ined umasure of l'n'call (I~) 
and pr(~cisi(m (P), F. F-score is defined by: 
I " - -  (1 +H'2)I'l? where 
/3-'P + I~ 
t ) 
2 \7 . )  ,. 
f'~rln 
fVln 
~\,, 
# of  .sc,tcncc.~ c:rtratcd th,t  olso 
atqwar in. tim model ,s.mn)?lr!l 
# of  sc+lt(!ncc,s i11 tim nlo,h:l .~um.tav!l 
# of  ,s('./Itclwcs c:rlv?lclcd t)1,t ll*c .Sll.Slcln 
rclatic'c iml,ortancc of  l~ aml 1:' 
(6) 
(7) 
\Ve as.~um(~ (,(lual importance of re(:all iIIld preci- 
sion aim set H to 1 in our (+,Xl)('+rimtml;s. The Imselitm 
(I)ositi(m) module scores ('at:h S(!llt(':llC{} hy its I)osi- 
ti(>n in the text. The first sent(race gets the high- 
esc s(:ortL the last S(HIt(H1Co the lowest. The l)as(~liIl(~ 
method is eXlmCted to lm ('.f\[ectiv(~ for news gem'e. 
The tfidf module assigns a score t.o a tt++rllI ti at:cord- 
ing to the product; of its flequc, ncy within a dot:- 
lllll(Hlt .j ( t f i j )  and its illV(~I'S(} doctmmnt  t?equoncy  
(idfi lo.q ,~). N is the total mmfl)or of document.s 
in the (:()rlms and dfj is the, numl)er of (Io(:HnloAll;.q 
(:OlH:nining te rm ti. 
The topic sigjlla(.lll(++ module sciliis each ,q(~llt;(H1C(~: 
assigning to ('ach word that occurs in a topic signa- 
(ure thu weigh(, of that, keyword in t.hc' tol)ic signa- 
tltl'tL Eit{'h s(++llt(,+ItC(~ Ill(ill l'(:c(:ive.q a top ic  s ignature  
score equal to tlm total of all signature word scores it 
(:Olllailis, normalizc'd 1) 3' the. highest sentence score. 
This s(:ol( 3 indical.es l;h(~ l'(!l(wall(:(~ of l.h(; S(!llt.t~n(:(! to 
t, lw sigmmlre topic. 
SU.~\[.MAt/IST In'oduced (!xttat:ts of tlm samu 
l(~xI.q sui)aralely for each ,,lodul0, for a s(~l'i(,s of ex- 
tracts ranging from ()cX; to 100% of the. original l;(}xI. 
Althottgh many rel<want docttments are avaita})l+, 
for each t01>ic, Ollly SOlll0 o\[ \[h0111 htlv(~ allSWOl kc!y 
499 
markut)s. The mnnber of documents with answer 
keys are listed in the row labeled: "# of Relevant 
Does Used in Training". To ensure we utilize all 
the available data and conduct a sound evaluation, 
we perform a three-fold (:ross validation. We re- 
serve one-third of documents as test set, use the rest 
as training set, and ret)eat three times with non- 
overlapl)ing test set. Furthernmre, we use only uni- 
gram topic signatures fin" evaluation. 
The result is shown in Figure 2 and TaMe 3. We 
find that the topic signature method outperforms 
the other two methods and the tfidfmethod performs 
poorly. Among 40 possibh,, test points fl)r four topics 
with 10% SUmlnary length increment (0% means se- 
lect at least one sentence) as shown in Table 3, the 
topic signature method beats the baseline method 
34 times. This result is really encouraging and in- 
dicates that the topic signature method is a worthy 
addition to a variety of text summarization methods. 
6.2 Enriching Topic Signatures Using 
Existing Ontologies 
We have shown in the previous sections that topic 
signatures can be used to al)I)roximate topic iden- 
tification at the lexieal level. Although the au- 
tomatically acquired signature terms for a specific 
topic seem to 1)e bound by unknown relationships as 
shown in Table 2, it is hard to image how we can 
enrich the inherent fiat structure of tol)ie signatures 
as defined in Equation 1 to a construct as complex 
as a MUC template or script. 
As discussed in (Agirre et al, 2000), we propose 
using an existing ontology such as SENSUS (Knight 
and Luk, 1994) to identify signature term relations. 
The external hierarchical framework can be used to 
generalize topic signatures and suggest richer rep- 
resentations for topic signatures. Automated entity 
recognizers can be used to (:lassify unknown enti- 
ties into their appropriate SENSUS concept nodes. 
We are also investigating other approaches to attto- 
matieally learn signature term relations. The idea 
mentioned in this paper is just a starting point. 
'7 Conc lus ion  
In this paI)er we l)resented a t)rocedure to automati- 
(:ally acquire topic signatures and valuated the eflk~c- 
tiveness of applying tol)i(: signatures to extract ot)i(: 
relevant senten(:es against two other methods. The 
tot)ie signature method outt)erforms the baseline and 
the tfidfmethods for all test topics. Topic signatures 
can not only recognize related terms (topic identifi- 
(:ation), but grout) related terms togetlmr under one 
target concept (topic interpretation). 'IbI)i(: identi- 
fication and interpretation are two essential steps in 
a typical automated text summarization system as 
we l)resent in Section 3. 
'\]))pic: signatures (:an also been vie.wed as an in- 
verse process of query expansion. Query expansion 
intends to alleviate the word mismatch ln'oblenl in 
infornmtion retrieval, since documents are normally 
written in different vocabulary, ttow to atttomati- 
(ally identify highly e(nrelated terms and use them 
to improve information retrieval performance has 
been a main research issue since late 19611's. Re- 
cent advances in the query expansion (Xu and Croft, 
1996) can also shed some light on the creation of 
topic signatures. Although we focus the ltse of topic 
signatures to aid text summarization i this paper, 
we plan to explore the possibility of applying topic 
signatures to perform query expansion in the future. 
The results reported are encouraging enough to 
allow us to contimm with topic signatures as the ve- 
hMe for a first approximation to worht knowledge. 
We are now busy creating a large nmnber of signa- 
ture.s to overcome the world knowledge acquisition 
problem and use them in topic interpretation. 
8 Acknowledgements  
YVe thank the anonymous reviewers for very use- 
tiff suggestions. This work is supported in part by 
DARPA contract N66001-97-9538. 
References  
Eneko .~girre, Olatz Ansa, Edum'd Hovy, and David 
Martinez. 2000. Enriching very large ontologies 
using the www. In Proceedings of the Work,,;hop 
on Ontology Construction of the European Con- 
fl:rencc of AI (ECAI). 
Kenneth Church and Patrick Hanks. 1990. Word as- 
sociation IIOrlllS, mutual information and lexicog- 
raphy. In Proceedings of the 28th Annual Meeting 
of the Association for Computational Lingui.vtic.~" 
(,4CL-90), pages 76~-83. 
Thomas Cover and Joy A. Thomas. 1991. Elcment.~ 
of Information Theory..John Wiley & Sons. 
Gerald DeJong. 1982. An overview of the FRUMP 
system. In ~2mdy G. Lehnert and Martin H. 
Ringle, editors, Strategies for natural language 
processing, pages 149-76. Lawrence Erlbaum A.s- 
so(lares. 
Ted Dunning. 1993. A~i:eurate methods for the 
statistics of surprise and coincidence. Computa- 
tional Li'nguistics, 19:61--7'4. 
Marti Itearst. 1997. TextTiling: Segmenting text 
into nmlti-l)aragraph subtopic passages. Compu- 
tational Linguistics, 23:33-64. 
Eduard Hovy and Chin-Yew Lin. 1999. Automated 
text summarization i SUMMAIRIST. In Inder- 
jeer Mani and Mark T. Maybury, editors, Ad- 
vances in Automatic 71xxt Summarization, chap- 
ter 8, pages 81 94. MIT Press. 
Kevin Knight and Steve K. Luk. 1994. Building a 
large knowledge base for machine translation, ht 
Proceedings of the Eleventh National Coy@re'nee 
on Arti\]icial Intelligence (AAAI-9/~). 
500 
-~  ..~:,., .;~ ,5. " ' - -=-"  _..  _ . .&ass  
0 50000 n ~ .~ . . . . . . . . . . . . . .  n ~ . . . . . . . . . . . . . . . . . . . .  ~ . . . . . . . . . .  
- - ' - . , ,q ,~  . . . .  + +. ,x.  o + ~ ? 
. . . . .  1,* '+  .~  *+-  . . . .  - -  
. . ; -5; , :~:;  . . . .  h ~.:~.7~.~ ~ ~ ^' - -~- . .  
. . . . . . . .  , ? - -g2  . . . . . . . . . .  o 400OO f , ' " +- ~-" + ~ -, "~2x-+, 
\[ ? . . . .  : . . . . . . .  : 
\[ - ...... ; ""7 ........ 2,=_ 
~ 0 =0000 j '  +J" J j  1" " 
.,::i'ff "4. 4 "A- . I  - i+ ~. : 
4" .,~t . -a  + -a--  -#. - . -~-- - .a  ~ ' . 
0 ~o00o 'd-;9~7~ -7 '+ 5~:7~:=-+': ; :  ~ . . . .  =-~++:7:: ~ -:'~ +--~ ....... " ~5_~Ztt::~:ll;: ; i  
I " , ; .  A ' / , -?~-  <F" ~. " "~" ~ 257 
44" ? 
o ;oo~ ._~-.c_-__~ / 
0 00000 
I 
000 005 010 015 020 025 030 ,335 040 045 050 055 060 065 070 o75 050 085 090 095 ~00 
.~ umrn~i-~ Lenqth 
Figure 2:  F-measur(: w;. summary length for all fimr topics. ~bi)ic signature cl(mrly outperforin tfidf and 
baselin(', ex(:ei)t for tit(: case of topic 258 where t)(~rforman(:(; for tim thr(;e methods are roughly equal. 
I__ I - - - - - -~~g-  lO% I ___~o~a -ao~~a--- -  4o~ I ~0%- - \ [  ~o~ \[ ~,o~ ~o% I 9o~ I lOO% I 
\[ ~.+,_~.,~dl .... i . . :ms o.a-~9 I o..~.o o.aa4 o . ,~:c -  I ...ao=, \[ '__2 :~r'  I~  o.ara oar , ;  I . .a t , ,  I e.a.~w-I 
+4.58 +7.48 +15.6a +14.17 +8.66 +3. I 51_ top lc . s i~  I -2 ,7d  -2 .19- -  
\[ 257-h , , * , , l i  .... r--- (1.1-}98 {~.15.__.5 I c,,, ,,.is., ".'~L I o.,~~--F--,~.~,, I - -o  t,l o.1~, I ?.!s~ 
\[ '_,.~r_,a,,r \[ -55.11- -38.56 I -'".5U ~">' ~".0'; ' ' " '+ '  I S '~: ' '  I ~ ~ " " "  I +r  0 '~t  . . . . .  , 
257_,~i,ic.~ig +45.5~ +64.06 +31.88 ~ +20.40 \[ +20.60 \[ 4_-~01 +12.4&- I14 .24  - O.(h.~\] 
\[_ 25u_h~,~.,li . . . .  L_  o l  tk_ o 270 I "'4'-'2 ~' *'~:~ I ,, ~,r_, L_  "47t_ J  .4 r , ,  1 - - ~ - ' 1  o.~,'__,+~ o s'_,Z._J 
\[ 271_l,aseli . . . .  I <,at tT_.._,~.:,,~; T--,Ta77--- ..a:~ _L ,, :s:,r, .L .... ~~~~:~- i~-  T -  o.ae~ \] , ) . :~:~~\[--~ 
. . . . . . .  +a~. lO  j _ _  + 4 ~ _ ~ ~ s . r o . ~  +~.a,~ I +~.~o_ l l  0.,~, \] 
Table 3: F..measul'e t)erformanc(~ differen(:e compared to 1)aselin(~ nt(:thod in t)ercentage. Cohmms indicate 
at diffe.rent summary lengths related to fldl length docum(mts. Values in the 1)aselin(,. rows are F-measure 
s(:ores. Vahms in the tfidf and tot)i(: signatur(~ rows arc i)('.rformmlc(~ increase or (h,.crease divide(l by their 
(:orr(.,sI)ontling baseline scores and shown in I)er(:(mtag(!. 
Inderje(?t Mani, David House, Gary KMn, Lyn(~tt(~ 
ttirschman, Leo ()brst, Thdr6se Firmin, Micha(d 
Chrzanowski, and Beth Sundheim. 1998. 
The T IPSTER SUMMAC t~xl smmnm'iza- 
tion evaluation final r(:t)ort. %~(:hnical I/,ol)orl; 
MTR98W0000138, The MITRE Corporation. 
Christopher Manning and Hinrich Schiitzc. 1999. 
t'}mdatious of Statistical Natural Language Pro~ 
cessi'ng. MIT Press .  
Kathh~(m M(:K(!own and l)rag(mfir R. I ladev. 1999. 
( 'TO. I I (  ra t ,  i l l g  S l l l l l l l l ; l l ' i ( : s  o f  I l t l l l t ,  i \ [ ) l \ [~  l l ( !~vs  articles. 
In hMtu.iet~t Mani and Mark T. Maybury, edi 
t,ors, Admm.ces i'n Automatic Text Sv,.mmarization, 
chapter 24, pagc+s 381 :/89. MiT Press. 
Ellen Riloff and Jeffrey Lorenzen. 1999. Ext:raction- 
t)a:;e,d text cateI,dorization: Generating donmin- 
qmcitic role relationships atttonmtically. In 
Tomek Strzalkowski, editor, Natural Language In- 
formation, Retrieval. Kluwer Academic Publishc, r.q. 
Ellen tlilolf. 1996. An ompirical study of automated 
dictionary construction for information extraction 
in three domains. Artificial Intelligence ,Journal, 
85, August. 
SAIC. 1998. Introduction to information extraction. 
http://www.mu(:.sai(:.(:om. 
Jinxi Xu and W. Bruc(! Croft. 1996. Query ex- 
pal>ion using local and gh)bal document analysis. 
In l'rocee.dings of the 17th Annual Inter'national 
A(JM SIGIR Co't@rence. on Research and Devel- 
opment in Information l{et'rieval, pages 4 -11. 
50 I  
.The Effectiveness of Dictionary and Web-Based Answer Reranking 
Chin-Yew Lin  
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
cyl@isi.edu 
 
Abstract 
We describe an in-depth study of using 
a dictionary (WordNet) and web 
search engines (Altavista, MSN, and 
Google) to boost the performance of 
an automated question answering 
system, Webclopedia, in answering 
definition questions. The results 
indicate applying dictionary and 
web-based answer reranking together 
increase the performance of 
Webclopedia on a set of 102 TREC-10 
definition questions by 25% in mean 
reciprocal rank score and 14% in 
finding answers in the top 5. 
1 Introduction 
In an attempt to further progress in information 
retrieval research, the Text REtrieval Conference 
(TREC) sponsored by the National Institute of 
Standards and Technology (NIST) started a 
series of large-scale evaluations of domain 
independent automated question answering 
systems in TREC-8 (Voorhees 2000) and 
continued in TREC-9 and TREC-10. NTCIR 
(NII-NACSIS Test Collection for IR Systems, 
TREC?s counterpart in Japan) initiated its 
question answering evaluation effort, Question 
Answering Challenge (QAC) in 2001 (Fukumoto 
et al 2001). Research systems participating in 
TRECs and the coming QAC focused on the 
problem of answering closed-class questions that 
have short fact-based answers (?factoids?) from a 
large collection of text.  
These systems bear a similar structure: 
(1) Question analysis ? identify question 
keywords to be submitted to search engines 
(local or web), recognize question types, and 
suggest expected answer types. Although most 
systems rely on a taxonomy of expected answer 
types, the number of nodes in the taxonomy 
varies widely from single digits to a few 
thousands. For example, Abney et al (2000) used 
5; Ittycheriah et al (2001), 31; Hovy et al 
(2001), 140; Harabagiu et al (2001), 8,797. 
These taxonomies were mostly based on named 
entities and WordNet (Fellbaum 1998). Special 
types such definition questions (ex: ?What is an 
atom??) were added as necessary. 
(2) Passage or Sentence retrieval ? this aims to 
provide a text pool of manageable size for 
extracting candidate answers. Most top 
performing systems in TRECs use their own 
retrieval methods for passages (Brill et al 2001; 
Clarke et al 2001; Harabagiu et al 2001) or 
sentences (Hovy et al 2001).  
(3) Candidate answer extraction ? extract 
candidate answers according to answer types. If 
the expected answer types are typical named 
entities, information extraction engines (Bikel et 
al. 1999, Srihari and Li 2000) are used to extract 
candidate answers. Otherwise special answer 
patterns are used to pinpoint answers. For 
example, Soubbotin and Soubbotin (2001) create 
a set of 6 answer patterns for definition questions. 
(4) Answer ranking ? assign scores to candidate 
answers according to their frequency in top 
ranked passages (Abney et al 2000; Clarke et al 
2001), similarity to candidate answers extracted 
from external sources such as the web (Brill et al 
2001; Buchholz 2001) or WordNet (Harabagiu et 
al. 2001; Hovy et al 2001), density, distance, or 
order of question keywords around the 
candidates, similarity between the dependency 
structures of questions and candidate answers 
(Harabagiu et al 2001; Hovy et al 2001; 
Ittycheriah et al 2001), and match of expected 
answer types. 
In this paper, we describe an in-depth study of 
answer reranking for definition questions. 
Definition questions account for over 100 (20%) 
test questions in TREC-10. They are not named 
entities that have been the cornerstones of many 
.high performance QA systems (Srihari and Li 
2000; Harabagiu et al 2001).  
By reranking we mean the following. Assume a 
QA system such as Webclopedia (Section 3) 
provides an initial set of ranked candidate 
answers from the TREC corpus. The ranking is 
based on the IR engine?s passage or sentence 
match scores. One can then measure the 
effectiveness of utilizing resources such as 
WordNet or the web to rerank the initial results, 
hoping to achieve better mean reciprocal rank 
(MRR) and percent of correctness in the top 5 
(PTC5). 
Answer reranking is often overlooked. The 
answer candidates (<= 400 instances per 
question) generated by Webclopedia from TREC 
corpus included answers for 83% of 102 
definition questions used in this study (the 
TREC-10 definition questions). However, 
Webclopedia ranked only 64% of them in the top 
5, giving an MRR score of 45%. If a perfect 
answer reranking function had been used, the 
best achievable MRR would have been 83% (an 
84% increase over the original 45%). 
Section 2 gives a brief overview of TREC-10. 
Section 3 outlines the Webclopedia system. 
Section 4 defines definition questions and 
describes our dictionary and web-based 
reranking methods. Section 5 presents 
experiments and results. We conclude with 
lessons learned and future work.  
2 TREC-10 Q&A Track 
The main task of the TREC-10 (Voorhees and 
Harman 2002) QA track required participants to 
return a ranked list of five answers of no more 
than 50 bytes long per question that were 
supported by the TREC-10 QA text collection. 
The TREC-10 QA document collection consists 
of newspaper and newswire articles on TREC 
disks 1 to 5. It contains about 3 GB of texts. Test 
questions were drawn from filtered MSNSearch 
and AskJeeves logs. NIST assessors then sifted 
500 questions from the filtered logs as test set. 
The questions were closed-class fact-based 
(?factoid?) questions such as ?How far is it from 
Denver to Aspen?? and ?What is an atom??. 
Mean reciprocal rank (MRR) was used as the 
indicator of system performance. Each question 
receives a score as the reciprocal of the rank of 
the first correct answer in the 5 submitted 
responses. No score is given if none of the 5 
responses contain a correct answer. MRR is then 
computed for a system by taking the mean of the 
reciprocal ranks of all questions. 
Besides MRR score, we are also interested in 
learning how well a system places a correct 
answer within the five responses regardless of its 
rank. We called this percent of correctness in the 
top 5 (PCT5). PCT5 is a precision related metric 
and indicates the upper bound that a system can 
achieve if it always places the correct answer as 
its first response. 
3 Webclopedia: An Automated 
Question Answering System 
Webclopedia?s architecture follows the principle 
outlined in Section 1. We briefly describe each 
stage in the following. Please refer to (Hovy et al  
2002) for more detail. 
 
(1) Question Analysis: We used an in-house 
parser, CONTEX (Hermjakob 2001), to parse and 
analyze questions and relied on BBN?s 
IdentiFinder (Bikel et al, 1999) to provide basic 
named entity extraction capability. 
 
(2) Document Retrieval/Sentence Ranking: 
The IR engine MG (Witten et al 1994) was used 
to return at least 500 documents using Boolean 
queries generated from the query formation 
stage. However, fewer than 500 documents may 
be returned when very specific queries are given. 
To decrease the amount of text to be processed, 
the documents were broken into sentences. Each 
sentence was scored using a formula that rewards 
word and phrase overlap with the question and 
expanded query words. The ranked sentences 
were then filtered by expected answer types (ex: 
dates, metrics, and countries) and fed to the 
answer extraction module. 
 
(3) Candidate Answer Extraction: We again 
used CONTEX to parse each of the top N 
sentences, marked candidate answers by named 
entities and special answer patterns such as 
definition patterns, and then started the ranking 
process. 
 
(4) Answer Ranking: For each candidate answer 
several steps of matching were performed. The 
matching process considered question keyword 
overlaps, expected answer types, answer 
patterns, semantic type, and the correspondence 
.of question and answer parse trees. Scores were 
given according to the goodness of the matching. 
The candidate answers? scores were compared 
and ranked. 
 
(5) Answer Reranking, Duplication Removal, 
and Answer output: For some special question 
type such as definition questions (e.g., ?What is 
cryogenics??), we used WordNet glosses or web 
search results to rerank the answers.  Duplicate 
answers were removed and only one instance was 
kept to increase coverage. The best 5 answers 
were output. Answer reranking is the main topic 
of this paper. Section 4 presents these methods in 
detail.   
4 Dictionary and Web-Based 
Answer Reranking 
4.1 Definition Questions 
Compared to other question types, definition 
questions are special. They are typically very 
short and in the form of ?What is|are (a|an) X??, 
where X is a 1 to 3 words term1, for example: 
?What is autism??, ?What is spider veins?? and 
?What is bangers and mash??. As we learned 
from past TREC experience, it was more difficult 
to find relevant documents for short queries. As 
stated earlier, over 20% of questions in TREC-10 
were of definition type, which was a reflection of 
real user queries mined from the web search 
engine logs (Voorhees 2001). Several top 
performing systems in the evaluation treated this 
type of question as a special category and most of 
them used definition answer patterns. The best 
performing system, InsightSoft-M, (Soubbotin 
and Soubbotin 2001) used a set of six definition 
patterns including P1:{<Q; is/are; [a/an/the]; A>, 
<A; is/are; [a/an/the]; Q>} and P2:{<Q; comma; 
[a/an/the]; A; [comma/period]>, <A; comma; 
[a/an/the]; Q; [comma/period]>}, where Q is the 
term to be defined and A is the candidate answer. 
The InsightSoft-M system returned 88 correct 
responses based on these patterns. The runner up 
system (Harabagiu et al 2001) used 12 answer 
patterns with extension of WordNet hypernyms. 
They did not report their success rate for 
TREC-10 but according to Pa?ca (2001)2, this set 
                                                     
1
 Among the 102 TREC-10 definition questions, 81 
asked the definition of one word; 19, two words; 2, 
three words.  
2
 Among them 31 were extracted through pattern 
of patterns with WordNet extension extracted 59 
out of 67 definition questions in TREC-8 and 
TREC-9. 
The success stories of these systems indicated 
that carefully crafted answer patterns were 
effective in candidate answer extraction. 
However, just applying answer patterns blindly 
might lead to disastrous results, as shown by 
Hermjakob (2002), since correct and incorrect 
answers were equally likely to match these 
patterns. For example, for the question ?What is 
autism??, the following answers are found in the 
TREC-10 corpus using the patterns described by 
the InsightSoft-M system: 
? autismQ, a nourishingA, equivocal ? 
? autismQ, the disorder isA, in fact, ? 
? autismQ, the discovery could open new 
approaches for treating tAhe ? 
? autismQ is a mental disorder that is a 
?severely incapacitatinAg ? 
? autismQ, the inability to communicate 
with othersA. 
Obviously, patterns alone cannot distinguish 
which one is the best answer. Some other 
mechanisms are necessary. We propose two 
different methods to solve this problem. One is a 
dictionary-based method using WordNet glosses 
and the other is to go directly to the web and 
compile web glosses on the fly to help select the 
best answers. The effect of combining both 
methods was also studied. We describe these two 
methods in the following sections. 
4.2 Dictionary-Based Reranking 
Using a dictionary to look up the definition of a 
term is the most straightforward solution for 
answering definition questions. For example, the 
definition of autism in the WordNet is: ?an 
abnormal absorption with the self; marked by 
communication disorders and short attention 
span and inability to treat others as people?. 
However, we need to find a candidate answer 
string from the TREC-10 corpus that is 
equivalent to this definition. By inspection, we 
find that candidate answers ?, ?, and ? shown 
in the previous section are more compatible to 
the definition and ? seems to be the best one. 
To automate the decision process, we construct a 
definition database based on the WordNet noun 
                                                                               
matching and 27 were from WordNet hypernym 
expansion. 
.glosses. Closed class words are thrown away and 
each word wi in the glosses is assigned a gloss 
weight wnis  as follows3: 
)1/log( += iwni nNs  
where ni is the number of times word wi 
occurring in the WordNet noun glosses and N is 
total number of occurrences of all noun gloss 
words in the WordNet. The goodness of the 
matching Mwn for each candidate answer is 
simply the sum of the weight of the matched 
word stems between its WordNet definition and 
itself. For example, candidate answer ?  and 
autism?s WordNet definition have these matches: 
{inability5 ? inabilitywn, communicate5 ? 
communicationwn, others5 ? otherswn}. The 
reranking score Swn for each candidate answer is 
its original score multiplied by Mwn. The final 
ranking is then sorted according to Swn, duplicate 
answers are removed, and the top 5 answers are 
output. Table 1 shows the top 5 answers returned 
before and after applying dictionary-based 
reranking. It demonstrates that dictionary-based 
reranking not only pushes the best answer to the 
first place but also boosts other lower ranked 
good answers i.e. ?a mental disorder? to the 
second place. 
Harabagiu et al (2001) also used WordNet to 
assist in answering definition questions. 
However, they took the hypernyms of the term to 
be defined as the default answers while we used 
its glosses. The hypernym of ?autism? is 
?syndrome?. In this case it would not boost the 
desired answer to the top but it would instead 
?validate? ?Down?s syndrome? as a good answer. 
Further research is needed to investigate the 
tradeoff between using hypernyms and glosses. 
WordNet glosses were incorporated in IBM?s 
statistical question answering system as 
definition features (Ittycheriah et al 2001). 
                                                     
3
 This is essentially inverse document (WordNet gloss 
entry) frequency (IDF) used in the information 
retrieval research. 
However, they did not report the effectiveness of 
the features in definition answer extraction. 
Out of vocabulary words is the major problem of 
dictionary-based reranking. For example, no 
WordNet entry is found for ?e-coli? but 
searching the term ?e-coli? at www.altavista.com 
and www.google.com yield the following: 
? E. coli is a food borne illness. Learn 
about prevention, symptoms and risks, 
detection, ... Risks Detection Recent 
Outbreaks Resources The term E. coli is 
an abbreviation for the bacteria 
Escherichia. (1st hit, www.altavista.com) 
? The E. coli Index (part of the WWW 
Virtual Library) ? Description: Guide to 
information relating to the model 
organism Escherichia coli. From the 
WWW Virtual Library. (1st hit, 
www.google.com) 
This brings us to the web-based reranking 
method that we introduce in the next section. 
4.3 Web-Based Reranking 
The World Wide Web contains massive amounts 
of information covering almost any thinkable 
topic. The TREC-10 questions are typical 
instances of queries for which users tend to 
believe answers can be found from the web. 
However, the candidate answers extracted from 
the web have to find support in the TREC-10 
corpus in order to be judged as correct otherwise 
they will be marked as unsupported.  
The search results of ?e-coli? from two online 
search engines indicate that ?e-coli? is an 
abbreviation for the bacteria Escherichia. 
However, to automatically identify ?e-coli? as 
?Escherichia? from these two pages is the same 
QA problem that we set off to resolve. The only 
advantage of using the web instead of just the 
TREC-10 corpus is the assumption that the web 
contains many more redundant candidate 
answers due to its huge size. Compared to 
Table 1. Top 5 answers returned before (Webclopedia) and after (Webclopedia + WordNet) 
dictionary-based answer reranking for question ?What is autism??. A ?-? indicates wrong answers and 
a ?+? indicates correct answers. 
Autism Webclopedia Webclopedia + WordNet
1 - Down's syndrome + the inability to communicate with others
2 - mental retardation + a mental disorder
3 + the inability to communicate with others - NIL
4 - NIL - Down's syndrome
5 - a group of similar-looking diseases - mental retardation
.Google?s 2,073,418,204 web pages4, TREC-10 
corpus contains only about 979,000 articles.  
For a given question, we first query the web, 
apply answer extraction algorithms over a set of 
top ranked web pages (usually in the lower 
hundreds), and then rank candidate answers 
according to their frequency in the set. This 
assumes the more a candidate answer occurs in 
the set the more likely it is the correct answer. 
Clarke et al (2001) and Brill et al (2001) both 
applied this principle and achieved good results. 
Instead of using Webclopedia to extract 
candidate answers from the web and then project 
back to the TREC-10 corpus, we treat the web as 
a huge dynamic dictionary. We compile web 
glosses on the fly for each definition question and 
apply the same reranking procedure used in the 
dictionary-based method. We detail the 
procedure in the following. 
(1) Query a search engine (e.g., Altavista) with 
the term (e.g., ?e-coli?) to be defined. 
(2) Download the first R pages (e.g., R = 70). 
(3) Extract context word ciw  within a window of 
W (e.g., W = 10) words centered at the term to be 
defined from each page. Closed class words are 
ignored. These context words are used as 
candidate web glosses. 
(4) The gloss weight webis  for each word ciw  is 
computed as follows5: 
)1/log( +?= iiwebi nNts  
where ti is the frequency of ciw  in the set of 
context words extracted in (3), N is the total 
number of training questions, and ni is the 
number of training questions in which ciw  
occurs. (5) The goodness of the matching Mweb 
                                                     
4
 This was the number that Google 
(www.google.com) advertised at its front page as of 
January 31, 2002. 
5
 This is essentially TFIDF (product of term frequency 
and inverse document frequency) used in the 
information retrieval research. 
for each candidate answer is simply the sum of 
the weights of the matched word stems between 
its web gloss definition and itself. Only words 
with gloss weight Tswebi ?  are used to compute 
Mweb. The value of T serves as a cut-off threshold 
to filter out low confidence words. 
(6) The reranking score Sweb for each candidate 
answer is its original score multiplied by Mweb. 
The final ranking is then sorted according to Sweb, 
duplicate answers are removed, and the top 5 
answers are output. Table 2 shows the top 5 
answers returned before and after applying 
web-based reranking for the question ?What is 
Wimbledon??. Google was used as the search 
engine with T=5, W=10, and R=70. 
5 Experiments and Results 
We used a set of 102 definition questions from 
TREC-10 QA track as our test set. The 
performance of Webclopedia without dictionary 
or web-based answer reranking was used as the 
baseline. Webclopedia with dictionary-based 
answer reranking.  
To study the effect of using different search 
engines, context window sizes, number of top 
ranked web pages, and web gloss weight cut-off 
threshold on the performance of web-based 
answer reranking, we had the following setup: 
? Three search engines (E): Altavista (EA), 
Google (EG), and MSNSearch (EM).  
? A run that combined all three search 
engines? results (EX).  
? Two different context window sizes (W): 
5 (W5) and 10 (W10). 
? Eleven sets of top ranked web pages 
(Rx): top 5, 10, 20, 30, 40, 50, 60, 70, 80, 
90, and 100. 
? Two different gloss weight cut-off 
thresholds (T): 5 (T5) and 10 (T10). 
To investigate the performance of combining 
dictionary and web-based answer reranking, we 
ran the above setup again but each question?s 
reranking score Sweb+wn was the multiplication of 
Table 2. Top 5 answers returned before (Webclopedia) and after (Webclopedia + Google) web-based 
answer reranking for question ?What is Wimbledon??. A ?-? indicates wrong answers and a ?+? 
indicates correct answers. 
Wimbledon Webclopedia Webclopedia + Google (T=5, W=10, R=70)
1 - the French Open and the U.S. Open. + the most famous front yard in tennis and scene
2 - SW20, which includes a Japanese-style water garden - the French Open and the U.S. Open.
3 + the most famous front yard in tennis and scene - NIL
4 - NIL - Sampras' biggest letdown of the year
5 - Sampras' biggest letdown of the year - Lawn Tennis & Croquet Club, home of the Wimbledon
.its original score, web-based matching score 
Mweb, and dictionary-based matching score Mwn. 
A total of 354 runs were performed. Manual 
evaluation of these 354 runs was not impossible 
but would be time consuming. We instead used 
the answer patterns provided by NIST to score all 
runs automatically.  
Due to space constraint, Table 3 shows the 
(MRR, PCT5) score pair for 90 runs out of 352 
runs. The other two runs were the baseline run 
with a score pair of (0.450, 0.637) and the 
dictionary-based run, (0.535, 0.667). The best 
run was the combined dictionary and web-based 
run using Google as the search engine with 
10-word context window, 70 top ranked pages, 
and a gloss weight cut-off threshold of 5. 
Analyzing all runs according to Table 3, we made 
the following observations. 
(1) Dictionary-based reranking improved 
baseline performance by 19% in MRR and 5% in 
PCT5 (MRR: 0.535, PCT5: 0.667). 
(2) The best web-based reranking (MRR: 0.539, 
PCT5: 0.676) was achieved with W=10, R=70, 
and T=5. It was comparable to the 
dictionary-based reranking. 
(3) Web-based reranking generally improved 
results. Only 6 runs6 (not shown in the table) did 
worse in their MRR scores than just using 
Webclopedia alone and these runs concentrated 
on low ranked page counts of 5 and 10. 
                                                     
6
 These were EAT5W5R5 (0.437, 0.598), EAT10W5R5 
(0.434, 0.608), EAT10W10R5 (0.437, 0.598), 
EMT5W5R5 (0.436, 0.608), EMT10W5R5 (0.438, 0.608), 
and EMT10W10R5 (0.443, 0.618). 
(4) Different search engines reached their best 
performance at different parameter settings. 
Overall Google did better. 
(5) Combining multiple search engine results 
(runs designed with X and X+) did not always 
improve performance. In some cases, it even 
degraded system performance (ExT5W10R70: 
0.519, 0.637). 
(6) Lower web gloss weight cut-off threshold 
was better at 5. 
(7) Longer context window was better at 10 (not 
shown in the table). 
(8) Taking top ranked pages of 50 to 90 pages 
provided better results.  
(9) Combining dictionary and web-based 
reranking always did better than using the 
web-based method alone.  
(10) Using WordNet and Google together was 
always better than just using WordNet alne in 
both MRR and PCT5 (the underlined cells).  
5.1 Question Difficulty 
To investigate the effectiveness of using 
dictionary and web-based answer reranking on 
question of different difficulty, we define 
question difficulty as: )/(1 Nnd ?= , where n 
is the number of systems participating in 
TREC-10 that returned answers in top 5 and N is 
the number of total runs (that is, 67 for 
TREC-10). When d = 1 no systems provided an 
answer in top 5; while d = 0 if all runs provided at 
least one answer in top 5. Table 4 shows the 
improvement of MRR and PCT5 scores at four 
different question difficulty levels with four 
different system setups. The results indicate that 
using either dictionary or web-based answer 
reranking improved system performance at all 
levels. The best results were achieved when 
evidence from both resources was used. 
However, it also demonstrates the difficulty of 
improving performance on very hard questions 
(d>=0.75). This implies we might need to 
consider alternative methods to improve the 
system performance further. 
Table 3. Results of 90 runs shown in (MRR, PCT5) 
score pair where A: Altavista, G: Google, M: 
MSNSearch, X: all three search engines, W: context 
window size, R: number of top ranked web paged used, 
T: web gloss weight cut-off threshold. Runs marked 
with ?+? indicate both dictionary and web-based 
answer reranking are used.  
Table 4. System performance at different question 
difficulty levels. (F: Webclopedia only, F+: 
Webclopedia with WordNet, FG: Webclopedia 
with Google, and F+G: Webclopedia with 
WordNet and Google) 
d>=0.00 (102) d>=0.25 (95) d>=0.50 (71) d>=0.75 (40)
F (0.450,0.637) (0.394,0.611) (0.264,0.549) (0.084,0.375)
F+ (0.535,0.667) (0.474,0.642) (0.323,0.592) (0.100,0.375)
FG (0.539,0.676) (0.475,0.653) (0.319,0.592) (0.125,0.375)
F+G (0.561,0.725) (0.498,0.705) (0.333,0.648) (0.128,0.400)
W10R 10 W10R 30 W10R 50 W10R 70 W10R 90
A T 5 (0.485,0.637) (0.499,0.637) (0.516,0.637) (0.525,0.667) (0.519,0.647)
A T 10 (0.463,0.618) (0.497,0.637) (0.506,0.627) (0.511,0.657) (0.502,0.647)
A +T 5 (0.503,0.667) (0.518,0.667) (0.538,0.676) (0.555,0.696) (0.548,0.696)
A +T 10 (0.502,0.667) (0.515,0.667) (0.528,0.667) (0.528,0.676) (0.525,0.676)
GT 5 (0.513,0.637) (0.515,0.647) (0.536,0.647) (0.539,0.676) (0.515,0.657)
GT 10 (0.497,0.637) (0.503,0.647) (0.527,0.647) (0.523,0.657) (0.518,0.637)
G+T 5 (0.551,0.686) (0.537,0.667) (0.557,0.676) (0.561,0.725) (0.547,0.706)
G+T 10 (0.536,0.676) (0.530,0.676) (0.547,0.676) (0.544,0.706) (0.545,0.686)
M T 5 (0.521,0.647) (0.513,0.627) (0.517,0.647) (0.514,0.637) (0.499,0.637)
M T 10 (0.505,0.627) (0.499,0.608) (0.502,0.637) (0.488,0.627) (0.493,0.608)
M +T 5 (0.543,0.676) (0.552,0.667) (0.544,0.676) (0.542,0.696) (0.533,0.676)
M +T 10 (0.527,0.647) (0.537,0.647) (0.525,0.667) (0.519,0.696) (0.520,0.667)
XT 5 (0.526,0.647) (0.539,0.676) (0.533,0.627) (0.519,0.637)* (0.515,0.627)
XT 10 (0.509,0.618) (0.524,0.657) (0.532,0.627) (0.524,0.647) (0.517,0.637)
X+T 5 (0.553,0.696) (0.551,0.696) (0.556,0.686) (0.550,0.696) (0.546,0.686)
X+T 10 (0.531,0.657) (0.543,0.686) (0.555,0.686) (0.550,0.696) (0.546,0.686)
.6 Conclusions 
We described dictionary-based answer reranking 
using WordNet, web-based answer reranking 
using three different online search engines, and 
their evaluations at various parameter settings on 
a set of 102 TREC-10 definition questions. We 
showed that using either approach alone 
improved MRR score by 19% and PCT5 score by 
5%  over the baseline. However, the best 
performance was achieved when both methods 
were used together. In that setting a 25% increase 
in MRR score and 14% improvement in PCT5 
score were obtained.  
The difference on the best MRR and PCT5 scores 
(0.56 vs. 0.73) suggests neither dictionary-based  
nor web-based will solve the reranking problem 
completely. 
To improve the performance further, we need 
better ways to compile web glosses and combine 
them with WordNet glosses. We also need a 
better combination function ? a statistical model 
for combining patterns, dictionary, and web 
scores. We have started investigating the 
possibility of applying answer reranking to other 
question types and exploring specialized web 
resources. 
References 
Abney, S., M. Collins, and A. Singhal. 2000. 
Answer Extraction. In Proceedings of the Applied 
Natural Language Processing Conference 
(ANLP-NAACL-00), Seattle, WA, 296?301.  
Bikel, D., R. Schwartz, and R. Weischedel.  1999.  
An Algorithm that Learns What?s in a Name.  In 
Machine Learning?Special Issue on NL Learning, 
34, 1?3. 
Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng. 
2001. Data-Intensive Question Answering.  In 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 183?189.  
Buchholz, S. 2001. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. In Proceedings of the 
TREC-10 Conference. NIST, Gaithersburg, MD, 
496?503.  
Clarke, C.L.A., G.V. Cormack, T.R. Lynam, C.M. 
Li, and G.L. McLearn. 2001. Web Reinforced 
Question Answering. In Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 620?626.  
Fellbaum, Ch. (ed). 1998. WordNet: An Electronic 
Lexical Database. Cambridge: MIT Press. 
Fukumoto, J, T. Kato, and F. Masui. 2001. NTCIR 
Workshop 3 QA Task ? Question Answering 
Challenge (QAC). http://research.nii.ac.jp/ntcir/ 
workshop/qac/cfp-en.html. 
Harabagiu, S., D. Moldovan, M. Pa?ca, R. 
Mihalcea, M. Surdeanu, R. Buneascu, R. G?rju, V. 
Rus and P. Morarescu. 2001. FALCON: Boosting 
Knowledge for Answer Engines. In Proceedings of 
the 9th Text Retrieval Conference (TREC-9), NIST, 
479?488. 
Hermjakob, U. 2001. Parsing and Question 
Classification for Question Answering. In 
Proceedings of the Workshop on Open-Domain 
Question Answering post-conference workshop of 
ACL-2001, Toulouse, France. 
Hermjakob, U. 2002. Open Questions Regarding 
Precision of the Insight Q&A System. Personal 
communication.. 
Hovy, E.H., U. Hermjakob, and C.-Y. Lin. 2001. 
The Use of External Knowledge in Factoid QA.   In 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 166?174. 
E.H. Hovy, U. Hermjakob, C-Y. Lin, and D. 
Ravichandran. 2002. Using Knowledge to Facilitate 
Pinpointing of Factoid Answers. In Proceedings of the 
19th International Conference on Computational 
Linguistics (COLING 2002), Taipei, Taiwan, August 
24 - September 1, 2002.   
Ittycheriah, A., M. Franz, and S. Roukos. 2001. 
IBM?s Statistical Question Answering System. In 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 317?323.  
Pa?ca, M. 2001. High Performance, Open-Domain 
Question Answering from Large Text Collections. 
Ph.D. dissertation, Southern Methodist University, 
Dallas, TX. 
Soubbotin, M.M. and S.M. Soubbotin. 2001. 
Patterns of Potential Answer Expressions as Clues to 
the Right Answer. In Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 175?182.   
Srihari, R. and W. Li. 2000. A Question 
Answering System Supported by Information 
Extraction. In Proceedings of the 1st Meeting of the 
North American Chapter of the Association for 
Computational Linguistics (ANLP-NAACL-00), 
Seattle, WA, 166?172. 
Voorhees, E.M. 1999. The TREC-8 Question and 
Answering Track Report. In Proceedings of the 
Eighth Text REtrieval Conference (TREC-8), pages 
77?82, 2000. NIST Special Publication 500-246. 
Voorhees, E. 2001. Overview of the Question 
Answering Track. In Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 71?81. 
Voorhees, E.M. and D.K. Harman. 2001. The 
Proceedings of the Eighth Text REtrieval Conference 
(TREC-10), NIST. 
Witten, I.H., A. Moffat, and T.C. Bell. 1994. 
Managing Gigabytes: Compressing and Indexing 
Documents and Images. New York: Van Nostrand 
Reinhold. 
 Using Knowledge to Facilitate Factoid Answer Pinpointing 
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran  
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
USA 
{hovy,ulf,cyl,ravichan}@isi.edu 
 
Abstract 
In order to answer factoid questions, the 
Webclopedia QA system employs a 
range of knowledge resources.  These 
include a QA Typology with answer 
patterns, WordNet, information about 
typical numerical answer ranges, and 
semantic relations identified by a robust 
parser, to filter out likely-looking but 
wrong candidate answers.  This paper 
describes the knowledge resources and 
their impact on system performance. 
1.   Introduction 
The TREC evaluations of QA systems 
(Voorhees, 1999) require answers to be drawn 
from a given source corpus.  Early QA systems 
used a simple filtering technique, question word 
density within a fixed n-word window, to 
pinpoint answers.  Robust though this may be, 
the window method is not accurate enough.  In 
response, factoid question answering systems 
have evolved into two types:  
? Use-Knowledge: extract query words from 
the input question, perform IR against the 
source corpus, possibly segment resulting 
documents, identify a set of segments 
containing likely answers, apply a set of 
heuristics that each consults a different 
source of knowledge to score each 
candidate, rank them, and select the best 
(Harabagiu et al, 2001; Hovy et al, 2001; 
Srihari and Li, 2000; Abney et al, 2000).  
? Use-the-Web: extract query words from the 
question, perform IR against the web, 
extract likely answer-bearing sentences, 
canonicalize the results, and select the most 
frequent answer(s).  Then, for justification, 
locate examples of the answers in the source 
corpus (Brill et al, 2001; Buchholz, 2001).  
Of course, these techniques can be combined: 
the popularity ratings from Use-the-Web can 
also be applied as a filtering criterion (Clarke et 
al., 2001), or the knowledge resource heuristics 
can filter the web results.  However, simply 
going to the web without using further 
knowledge (Brill et al, 2001) may return the 
web?s majority opinions on astrology, the killers 
of JFK, the cancerous effects of microwave 
ovens, etc.?fun but not altogether trustworthy.   
In this paper we describe the range of 
filtering techniques our system Webclopedia 
applies, from simplest to most sophisticated, and 
indicate their impact on the system.   
2.   Webclopedia Architecture  
As shown in Figure 1, Webclopedia adopts the 
Use-Knowledge architecture. Its modules are 
described in more detail in (Hovy et al, 2001; 
Hovy et al, 1999):  
? Question parsing: Using BBN?s 
IdentiFinder (Bikel et al, 1999), the 
CONTEX parser (Hermjakob, 1997) 
produces a syntactic-semantic analysis of 
the question and determines the QA type.   
? Query formation: Single- and multi-word 
units (content words) are extracted from the 
analysis, and WordNet synsets (Fellbaum, 
1998) are used for query expansion.  A 
series of Boolean queries of decreasing 
specificity is formed.  
? IR: The publicly available IR engine MG 
(Witten et al, 1994) returns the top-ranked 
N documents.  
 ? Selecting and ranking sentences: For each 
document, the most promising K sentences 
are located and scored using a formula that  
 rewards word and phrase overlap with the 
question and its expanded query words.  Results 
are ranked.   
? Parsing candidates: CONTEX parses the 
top-ranked 300 sentences.   
? Pinpointing: As described in Section 3, a 
number of knowledge resources are used to 
perform filtering/pinpointing operations.   
? Ranking of answers: The candidate 
answers? scores are compared and the 
winner(s) are output. 
3. Knowledge Used for Pinpointing 
3.1   Type 1: Question Word Matching 
Unlike (Prager et al, 1999), we do not first 
annotate the source corpus, but perform IR 
directly on the source text, using MG (Witten et 
al., 1994).  To determine goodness, we assign an 
initial base score to each retrieved sentence.  We 
then compare the sentence to the question and 
adapt this score as follows:  
? exact matches of proper names double the 
base score. 
? matching an upper-cased term adds a 60% 
bonus of the base score for multi-words 
terms and 30% for single words (matching 
?United States? is better than just ?United?).  
? matching a WordNet synonym of a term 
discounts by 10% (lower case) and 50% 
(upper case).  (When ?Cage? matches 
?cage?, the former may be the last name of a 
person and the latter an object; the case 
mismatch signals less reliability.)  
? lower-case term matches after Porter 
stemming are discounted 30%; upper-case 
matches 70% (Porter stemming is more 
aggressive than WordNet stemming).  
? Porter stemmer matches of both question 
and sentence words with lower case are 
discounted 60%; with upper case, 80%.  
? if CONTEX indicates a term as being 
qsubsumed (see Section 3.9) the term is 
discouned 90% (in ?Which country 
manufactures weapons of mass 
destruction??, ?country? will be marked as 
qsubsumed).   
The top-scoring 300 sentences are passed on for 
further filtering.   
3.2  Type 2: Qtargets, the QA Typology, 
and the Semantic Ontology 
We classify desired answers by their semantic 
type, which have been taxonomized in the 
Webclopedia QA Typology (Hovy et al, 2002), 
Candidate answer parsing
? Steps: parse sentences
? Engines: CONTEX
Matching
? Steps: match general constraint patterns against parse trees
             match desired semantic type against parse tree elements
             assign score to words in sliding window
? Engine: Matcher
Ranking and answer extraction
? Steps: rank candidate answers
             extract and format them
? Engine: Answer ranker/formatter
QA typology
? QA types, categorized in taxonomy
Constraint patterns
? Identify likely answers in relation to
   other parts of the sentence
Create query
Retrieve documents
Select & rank sentences
Parse top sentences
Parse question
Input question
Perform additional inference
Rank and prepare answers
Output answers
Question parsing
? Steps: parse question
             find desired semantic type
? Engines:  IdentiFinder  (BBN)
                 CONTEX
Match sentences against answers
Query creation
?  Steps: extract, combine important words
 expand query words using WordNet
 create queries, order by specificity
?  Engines: Query creator
IR
?  Steps: retrieve top 1000 documents
?  Engines: MG (RMIT Melbourne)
Sentence selection and ranking
?  Steps: score each sentence in each document
 rank sentences and pass top 300 along
?  Engines:Ranker
Figure 1. Webclopedia architecture. 
 http://www.isi.edu/natural-language/projects/we 
bclopedia/Taxonomy/taxonomy_toplevel.html). 
The currently approx. 180 classes,  which we 
call qtargets, were developed after an analysis of 
over 17,000 questions (downloaded in 1999 
from answers.com) and later enhancements to 
Webclopedia.  They are of several types:  
? common semantic classes such as PROPER-
PERSON, EMAIL-ADDRESS, LOCATION, 
PROPER-ORGANIZATION;  
? classes particular to QA such as YES:NO, 
ABBREVIATION-EXPANSION, and WHY-
FAMOUS;  
? syntactic classes such as NP and NOUN, 
when no semnatic type can be determined 
(e.g., ?What does Peugeot manufacture??);  
? roles and slots, such as REASON and TITLE-
P respectively, to indicate a desired relation 
with an anchoring concept.    
Given a question, the CONTEX parser uses a 
set of 276 hand-built rules to identify its most 
likely qtarget(s), and records them in a backoff 
scheme (allowing more general qtarget nodes to 
apply when more specific ones fail to find a 
match).  The generalizations are captured in a 
typical concept ontology, a 10,000-node extract 
of WordNet.   
The recursive part of pattern matching is 
driven mostly by interrogative phrases.  For 
example, the rule that determines the 
applicability of the qtarget WHY-FAMOUS 
requires the question word ?who?, followed by 
the copula, followed by a proper name.  When 
there is no match at the current level, the system 
examines any interrogative constituent, or words 
in special relations to it.  For example, the 
qtarget TEMPERATURE-QUANTITY (as in 
?What is the melting point of X?? requires as 
syntactic object something that in the ontology is 
subordinate to TEMP-QUANTIFIABLE-ABS-
TRACT with, as well, the word ?how? paired 
with ?warm?, ?cold?, ?hot?, etc., or the phrase  
?how many degrees? and a TEMPERATURE-
UNIT (as defined in the ontology).   
3.3 Type 3: Surface Pattern Matching 
Often qtarget answers are expressed using rather 
stereotypical words or phrases.  For example, the 
year of birth of a person is typically expressed 
using one of these phrases:  
<name> was born in <birthyear> 
<name> (<birthyear>?<deathyear>) 
We have developed a method to learn such 
patterns automatically from text on the web 
(Ravichandran and Hovy, 2002).  We have 
added into the QA Typology the patterns for 
appropriate qtargets (qtargets with closed-list 
answers, such as PLANETS, require no patterns).  
Where some QA systems use such patterns 
exclusively (Soubbotin and Soubbotin, 2001) or 
partially (Wang et al, 2001; Lee et al, 2001), 
we employ them as an additional source of 
evidence for the answer.  Preliminary results on 
for a range of qtargets, using the TREC-10 
questions and the TREC corpus, are:  
Question type 
(qtarget) 
Number of 
questions 
MRR on 
TREC docs 
BIRTHYEAR 8 0.47875 
INVENTORS 6 0.16667 
DISCOVERERS 4 0.1250 
DEFINITIONS 102 0.3445 
WHY-FAMOUS 3 0.6666 
LOCATIONS 16 0.75 
3.4  Type 4: Expected Numerical Ranges  
Quantity-targeting questions are often 
underspecified and rely on culturally shared  
cooperativeness rules and/or world knowledge: 
Q: How many people live in Chile?  
S1: ?From our correspondent comes good 
news about the nine people living in  Chile?? 
A1: nine  
While certainly nine people do live in Chile, 
we know what the questioner intends.  We have 
hand-implemented a rule that provides default 
range assumptions for POPULATION questions 
and biases quantity questions accordingly.  
3.5 Type 5: Abbreviation Expansion  
Abbreviations often follow a pattern: 
Q: What does NAFTA stand for? 
S1: ?This range of topics includes the North 
American Free Trade Agreement, NAFTA, 
and the world trade agreement GATT.?  
S2: ?The interview now changed to the subject 
of trade and pending economic issues, such as 
the issue of opening the rice market, NAFTA, 
and the issue of Russia repaying economic 
cooperation funds.?  
After Webclopedia identifies the qtarget as 
ABBREVIATION-EXPANSION, it extracts 
 possible answer candidates, including ?North 
American Free Trade Agreement? from S1 and 
?the rice market? from S2.  Rules for acronym 
matching easily prefer the former.  
3.6 Type 6: Semantic Type Matching  
Phone numbers, zip codes, email addresses, 
URLs, and different types of quantities obey 
lexicographic patterns that can be exploited for 
matching, as in  
Q: What is the zip code for Fremont, CA?  
S1: ??from Everex Systems Inc., 48431 
Milmont Drive, Fremont, CA 94538.?  
and  
Q: How hot is the core of the earth?  
S1. ?The temperature of Earth?s inner core 
may be as high as 9,000 degrees Fahrenheit 
(5,000 degrees Celsius).?  
Webclopedia identifies the qtargets respectively 
as ZIP-CODE and TEMPERATURE-QUANTITY.  
Approx. 30 heuristics (cascaded) apply to the 
input before parsing to mark up numbers and 
other orthographically recognizable units of all 
kinds, including (likely) zip codes, quotations, 
year ranges, phone numbers, dates, times, 
scores, cardinal and ordinal numbers, etc.  
Similar work is reported in (Kwok et al, 2001).  
3.7 Type 7: Definitions from WordNet  
We have found a 10% increase in accuracy in 
answering definition questions by using external 
glosses obtained from WordNet.  For  
Q: What is the Milky Way?  
Webclopedia identified two leading answer 
candidates:   
A1: outer regions  
A2: the galaxy that contains the Earth  
Comparing these with the WordNet gloss:  
WordNet: ?Milky Way?the galaxy containing 
the solar system?  
allows Webclopedia to straightforwardly match 
the candidate with the greater word overlap.   
Curiously, the system also needs to use 
WordNet to answer questions involving 
common knowledge, as in:  
Q: What is the capital of the United States?  
because authors of the TREC collection do not 
find it necessary to explain what Washington is:  
Ex: ?Later in the day, the president returned to 
Washington, the capital of the United States.?  
While WordNet?s definition  
Wordnet: ?Washington?the capital of the 
United States?  
directly provides the answer to the matcher, it 
also allows the IR module to focus its search on 
passages containing ?Washington?, ?capital?, 
and ?United States?, and the matcher to pick a 
good motivating passage in the source corpus.   
Clearly, this capability can be extended to 
include (definitional and other) information 
provided by other sources, including 
encyclopedias and the web (Lin 2002). 
3.8 Type 8: Semantic Relation Matching  
So far, we have considered individual words and 
groups of words.  But often this is insufficient to 
accurately score an answer.  As also noted in 
(Buchholz, 2001), pinpointing can be improved 
significantly by matching semantic relations 
among constituents:  
Q: Who killed Lee Harvey Oswald?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?Belli?s clients have included Jack Ruby, 
who killed John F. Kennedy assassin Lee 
Harvey Oswald, and Jim and Tammy Bakker.?  
S2: ?On Nov. 22, 1963, the building gained 
national notoriety when Lee Harvey Oswald 
allegedly shot and killed President John F. 
Kennedy from a sixth floor window as the 
presidential motorcade passed.?  
The CONTEX parser (Hermjakob, 1997; 
2001) provides the semantic relations.  The 
parser uses machine learning techniques to build 
a robust grammar that produces semantically 
annotated syntax parses of English (and Korean 
and Chinese) sentences at approx. 90% accuracy 
(Hermjakob, 1999).   
The matcher compares the parse trees of S1 
and S2 to that of the question.  Both S1 and S2 
receive credit for matching question words ?Lee 
Harvey Oswald? and ?kill? (underlined), as well 
as for finding an answer (bold) of the proper 
qtarget type (PROPER-PERSON).  However, is 
the answer ?Jack Ruby? or ?President John F. 
Kennedy??  The only way to determine this is to 
consider the semantic relationship between these 
 candidates and the verb ?kill? (parse trees 
simplified, and only portions shown here):   
 
[1] Who killed Lee Harvey Oswald?  [S-SNT] 
    (SUBJ) [2] Who  [S-INTERR-NP] 
        (PRED) [3] Who  [S-INTERR-PRON] 
    (PRED) [4] killed  [S-TR-VERB] 
    (OBJ) [5] Lee Harvey Oswald  [S-NP] 
        (PRED) [6] Lee?Oswald  [S-PROPER-NAME] 
            (MOD) [7] Lee  [S-PROPER-NAME] 
            (MOD) [8] Harvey  [S-PROPER-NAME] 
            (PRED) [9] Oswald  [S-PROPER-NAME] 
    (DUMMY) [10] ?  [D-QUESTION-MARK] 
 
[1] Jack Ruby, who killed John F. Kennedy assassin  
  Lee Harvey Oswald  [S-NP] 
   (PRED) [2] <Jack Ruby>1  [S-NP] 
   (DUMMY) [6] ,  [D-COMMA] 
   (MOD) [7] who killed John F. Kennedy assassin  
                 Lee Harvey Oswald  [S-REL-CLAUSE] 
     (SUBJ) [8] who<1>  [S-INTERR-NP] 
     (PRED) [10] killed  [S-TR-VERB] 
     (OBJ) [11] JFK assassin?Oswald  [S-NP] 
         (PRED) [12] JFK?Oswald [S-PROP-NAME] 
             (MOD) [13] JFK  [S-PROPER-NAME] 
             (MOD) [19] assassin  [S-NOUN] 
             (PRED) [20] ?Oswald [S-PROPER-NAME] 
Although the PREDs of both S1 and S2 
match that of the question ?killed?, only S1 
matches ?Lee Harvey Oswald? as the head of 
the logical OBJect.  Thus for S1, the matcher 
awards additional credit to node [2] (Jack Ruby) 
for being the logical SUBJect of the killing 
(using anaphora resolution). In S2, the parse tree 
correctly records that node [13] (?John F. 
Kennedy?) is not the object of the killing.  Thus 
despite its being closer to ?killed?, the candidate 
in S2 receives no extra credit from semantic 
relation matching.   
It is important to note that the matcher 
awards extra credit for each matching semantic 
relationship between two constituents, not only 
when everything matches.  This granularity 
improves robustness in the case of partial 
matches.   
Semantic relation matching applies not only 
to logical subjects and objects, but also to all 
other roles such as location, time, reason, etc. 
(for additional examples see http://www.isi.edu/ 
natural-language/projects/webclopedia/sem-rel-
examples.html).  It also applies at not only the 
sentential level, but at all levels, such as post-
modifying prepositional and pre-modifying 
determiner phrases  
Additionally, Webclopedia uses 10 lists of 
word variations with a total of 4029 entries for 
semantically related concepts such as ?to 
invent?, ?invention? and ?inventor?, and rules 
for handling them.  For example, via coercing 
?invention? to ?invent?, the system can give 
?Johan Vaaler? extra credit for being a likely 
logical subject of ?invention?:  
Q: Who invented the paper clip?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?The paper clip, weighing a desk-crushing 
1,320 pounds, is a faithful copy of Norwegian 
Johan Vaaler?s 1899 invention, said Per 
Langaker of the Norwegian School of 
Management.?  
while ?David? actually loses points for being 
outside of the clausal scope of the inventing:  
S2: ??Like the guy who invented the safety pin, 
or the guy who invented the paper clip,? David 
added.?  
3.9 Type 9: Word Window Scoring  
Webclopedia also includes a typical window-
based scoring module that moves a window over 
the text and assigns a score to each window 
position depending on a variety of criteria (Hovy 
et al, 1999).  Unlike (Clarke et al, 2001; Lee et 
al., 2001; Chen et al, 2001), we have not 
developed a very sophisticated scoring function, 
preferring to focus on the modules that employ 
information deeper than the word level.  
This method is applied only when no other 
method provides a sufficiently high-scoring 
answer.  The window scoring function is  
S  = (500/(500+w))*(1/r) * ?[(?I1.5*q*e*b*u)1.5] 
Factors: 
w: window width (modulated by gaps of 
various lengths: ?white house? ? ?white car and 
house?), 
r: rank of qtarget in list returned by 
CONTEX, 
I: window word information content (inverse 
log frequency score of each word), summed,  
q: # different question words matched, plus 
specific rewards (bonus q=3.0),  
e: penalty if word matches one of question 
word?s WordNet synset items (e=0.8),  
 b: bonus for matching main verb, proper 
names, certain target words (b=2.0),  
u: (value 0 or 1) indicates whether a word has 
been qsubsumed (?subsumed? by the qtarget) 
and should not contribute (again) to the score.  
For example, ?In what year did Columbus 
discover America?? the qsubsumed words are 
?what? and ?year?. 
4. Performance Evaluation  
In TREC-10?s QA track, Webclopedia received 
an overall Mean Reciprocal Rank (MRR) score 
of 0.435, which put it among the top 4 
performers of the 68 entrants (the average MRR 
score for the main QA task was about 0.234).  
The pinpointing heuristics are fairly accurate: 
when Webclopedia finds answers, it usually 
ranks them in the first place (1st place: 35.5%; 
2nd: 8.94%; 3rd: 5.69%; 4th: 3.05%; 5th: 5.28%; 
not found: 41.87%).  
We determined the impact of each 
knowledge source on system performance, using 
the TREC-10 test corpus using the standard 
MRR scoring.  We applied the system to the 
questions of each knowledge type separately, 
with and without its specific knowledge 
source/algorithm.  Results are shown in Table 1, 
columns A (without) and B (with).  To indicate 
overall effect, we also show (in columns C and 
D) the percentage of questions in TREC-10 and 
-9 respecively of each knowledge type.   
5. Conclusions 
It is tempting to search for a single technique 
that will solve the whole problem (for example, 
Ittycheriah et al (2001) focus on the subset of 
factoid questions answerable by NPs, and train a 
statistical model to perform NP-oriented answer 
pinpointing).  Our experience, however, is that 
even factoid QA is varied enough to require 
various special-purpose techniques and 
knowledge.  The theoretical limits of the various 
techniques are not known, though Light et al?s 
(2001) interesting work begins to study this.   
Column A: % questions of the knowledge type  
     answered correctly without using knowlege 
Column B: % questions, now using knowledge 
Column C: % questions of type in TREC-10  
Column D: % questions of type in TREC-9  
 A B C D 
Abbreviation exp. 20.0 70.0  1.0 2.3 
Number ranges 50.0 50.0  1.2 1.8 
WordNet (def Qs) 48.3 67.5 20.9 5.1 
Semantic types     
- locator types N/A N/A  0.0 0.4 
- quantity types 22.5 48.7 10.8 5.5 
- date/year types 45.0 57.3  9.2 10.2 
Patterns      
- definitions ? 34.4 20.9 5.1 
- why-famous  ? 66.7 0.6 ? 
- locations ? 75.0 3.2 ? 
- birthyear ? 47.9 1.6 ? 
Semantic relations 39.4 46.5 72.2 85.7 
Table 1. Performance of knowledge sources. 
Semantic relation scores measured only on 
questions in which they could logically apply.   
We conclude that factoid QA performance 
can be significantly improved by the use of 
knowledge attuned to specific question types 
and specific information characteristics.  Most of 
the techniques for exploiting this knowledge 
require learning to ensure robustness.  To 
improve performance beyond this, we believe a 
combination of going to the web and turning to 
deeper world knowledge and automated 
inference (Harabagiu et al, 2001) to be the 
answer.  It remains an open question how much 
work these techniques would require, and what 
their payoff limits are.   
References  
Abney, S., M. Collins, and A. Singhal. 2000. Answer 
Extraction. Proceedings of the Applied Natural 
Language Processing Conference (ANLP-
NAACL-00), Seattle, WA, 296?301.  
Bikel, D., R. Schwartz, and R. Weischedel.  1999.  
An Algorithm that Learns What?s in a Name.  
Machine Learning?Special Issue on NL 
Learning, 34, 1?3. 
Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng. 
2001. Data-Intensive Question Answering.  
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 183?189.  
Buchholz, S. 2001. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. Proceedings of the 
TREC-10 Conference. NIST, 496?503.  
Chen, J., A.R. Diekema, M.D. Taffet, N. McCracken, 
N. Ercan Ozgencil, O. Yilmazel, and E.D. Liddy. 
 2001. CNLP at TREC-10 QA Track. Proceedings 
of the TREC-10 Conference. NIST, 480?490. 
Clarke, C.L.A., G.V. Cormack, T.R. Lynam, C.M. Li, 
and G.L. McLearn. 2001. Web Reinforced 
Question Answering. Proceedings of the TREC-
10 Conference. NIST, 620?626.  
Clarke, C.L.A., G.V. Cormack, and T.R. Lynam. 
2001. Exploiting Redundancy in Question 
Answering. Proceedings of the SIGIR 
Conference. New Orleans, LA, 358?365.  
Fellbaum, Ch. (ed). 1998. WordNet: An Electronic 
Lexical Database. Cambridge: MIT Press. 
Harabagiu, S., D. Moldovan, M. Pasca, R. Mihalcea, 
M. Surdeanu, R. Buneascu, R. G?rju, V. Rus and 
P. Morarescu. 2001. FALCON: Boosting 
Knowledge for Answer Engines. Proceedings of 
the 9th Text Retrieval Conference (TREC-9), 
NIST, 479?488.  
Hermjakob, U. 1997. Learning Parse and 
Translation Decisions from Examples with Rich 
Context.  Ph.D. dissertation, University of Texas 
Austin. file://ftp.cs.utexas.edu/pub/mooney/paper 
s/hermjakob-dissertation 97.ps.gz.  
Hermjakob, U. 2001. Parsing and Question 
Classification for Question Answering. 
Proceedings of the Workshop on Question 
Answering at ACL-2001.  Toulouse, France.  
Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and 
C.-Y. Lin. 1999. Question Answering in 
Webclopedia.  Proceedings of the TREC-9 
Conference.  NIST. Gaithersburg, MD, 655?673. 
Hovy, E.H., U. Hermjakob, and D. Ravichandran. 
2002. A Question/Answer Typology with Surface 
Text Patterns.  Poster in Proceedings of the 
DARPA Human Language Technology 
Conference (HLT).  San Diego, CA, 234?238.   
Hovy, E.H., U. Hermjakob, and C.-Y. Lin. 2001. The 
Use of External Knowledge in Factoid QA.   
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 166?174.  
Ittycheriah, A., M. Franz, and S. Roukos. 2001. 
IBM?s Statistical Question Answering System. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 317?323.  
Kwok, K.L., L. Grunfeld, N. Dinstl, and M. Chan. 
2001. TREC2001 Question-Answer, Web and 
Cross Language experiments using PIRCS. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 447?451.  
Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C. Lee, 
B-K. Kwak, J, Cha, D. Kim, J-H. An, H. Kim, 
and K. Kim. 2001. SiteQ: Engineering High 
Performance QA System Using Lexico=Semantic 
Pattern Matching and Shallow NLP. Proceedings 
of the TREC-10 Conference. NIST, Gaithersburg, 
MD, 437?446.  
Light, M., G.S. Mann, E. Riloff, and E. Breck. 2001. 
Analyses for Elucidating Current Question 
Answering Technology. Natural Language 
Engineering, 7:4, 325?342.  
Lin, C.-Y. 2002. The Effectiveness of Dictionary and 
Web-Based Answer Reranking.  Proceedings of 
the 19th International Conference on 
Computational Linguistics (COLING 2002), 
Taipei, Taiwan.  
Oh, JH., KS. Lee, DS. Chang, CW. Seo, and KS. 
Choi. 2001. TREC-10 Experiments at KAIST: 
Batch Filtering and Question Answering. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 354?361. 
Prager, J., E. Brown, D.R. Radev, and K. Czuba. 
1999. One Search Engine or Two for Question 
Answering. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 235?240. 
Ravichandran, D. and E.H. Hovy. 2002. Learning 
Surface Text Patterns for a Question Answering 
System. Proceedings of the ACL conference. 
Philadelphia, PA.  
Soubbotin, M.M. and S.M. Soubbotin. 2001. Patterns 
of Potential Answer Expressions as Clues to the 
Right Answer. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 175?182.   
Srihari, R. and W. Li. 2000. A Question Answering 
System Supported by Information Extraction. 
Proceedings of the 1st Meeting of the North 
American Chapter of the Association for 
Computational Linguistics (ANLP-NAACL-00), 
Seattle, WA, 166?172. 
Voorhees, E. 1999. Overview of the Question 
Answering Track. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 71?81.  
Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D. Bu, 
and S. Bai. 2001. TREC-10 Experiments at CAS-
ICT: Filtering, Web, and QA. Proceedings of the 
TREC-10 Conference. NIST, 229?241.  
Witten, I.H., A. Moffat, and T.C. Bell. 1994. 
Managing Gigabytes: Compressing and Indexing 
Documents and Images. New York: Van 
Nostrand Reinhold. 
 
ORANGE: a Method for Evaluating Automatic Evaluation Metrics                    
for Machine Translation 
Chin-Yew Lin and Franz Josef Och 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292, USA 
{cyl,och}@isi.edu 
 
Abstract 
Comparisons of automatic evaluation metrics 
for machine translation are usually conducted 
on corpus level using correlation statistics 
such as Pearson?s product moment correlation 
coefficient or Spearman?s rank order 
correlation coefficient between human scores 
and automatic scores. However, such 
comparisons rely on human judgments of 
translation qualities such as adequacy and 
fluency. Unfortunately, these judgments are 
often inconsistent and very expensive to 
acquire. In this paper, we introduce a new 
evaluation method, ORANGE, for evaluating 
automatic machine translation evaluation 
metrics automatically without extra human 
involvement other than using a set of reference 
translations. We also show the results of 
comparing several existing automatic metrics 
and three new automatic metrics using 
ORANGE. 
1 Introduction 
To automatically evaluate machine translations, 
the machine translation community recently 
adopted an n-gram co-occurrence scoring 
procedure BLEU (Papineni et al 2001). A similar 
metric, NIST, used by NIST (NIST 2002) in a 
couple of machine translation evaluations in the 
past two years is based on BLEU. The main idea of 
BLEU is to measure the translation closeness 
between a candidate translation and a set of 
reference translations with a numerical metric. 
Although the idea of using objective functions to 
automatically evaluate machine translation quality 
is not new (Su et al 1992), the success of BLEU 
prompts a lot of interests in developing better 
automatic evaluation metrics. For example, Akiba 
et al (2001) proposed a metric called RED based 
on edit distances over a set of multiple references.  
Nie?en et al (2000) calculated the length-
normalized edit distance, called word error rate 
(WER), between a candidate and multiple 
reference translations. Leusch et al (2003) 
proposed a related measure called position-
independent word error rate (PER) that did not 
consider word position, i.e. using bag-of-words 
instead. Turian et al (2003) introduced General 
Text Matcher (GTM) based on accuracy measures 
such as recall, precision, and F-measure.  
With so many different automatic metrics 
available, it is necessary to have a common and 
objective way to evaluate these metrics. 
Comparison of automatic evaluation metrics are 
usually conducted on corpus level using correlation 
analysis between human scores and automatic 
scores such as BLEU, NIST, WER, and PER. 
However, the performance of automatic metrics in 
terms of human vs. system correlation analysis is 
not stable across different evaluation settings. For 
example, Table 1 shows the Pearson?s linear 
correlation coefficient analysis of 8 machine 
translation systems from 2003 NIST Chinese-
English machine translation evaluation. The 
Pearson? correlation coefficients are computed 
according to different automatic evaluation 
methods vs. human assigned adequacy and 
fluency. BLEU1, 4, and 12 are BLEU with 
maximum n-gram lengths of 1, 4, and 12 
respectively. GTM10, 20, and 30 are GTM with 
exponents of 1.0, 2.0, and 3.0 respectively. 95% 
confidence intervals are estimated using bootstrap 
resampling (Davison and Hinkley 1997). From the 
BLEU group, we found that shorter BLEU has better 
adequacy correlation while longer BLEU has better 
fluency correlation. GTM with smaller exponent 
has better adequacy correlation and GTM with 
larger exponent has better fluency correlation. 
NIST is very good in adequacy correlation but not 
as good as GTM30 in fluency correlation. Based 
on these observations, we are not able to conclude 
which metric is the best because it depends on the 
manual evaluation criteria. This results also 
indicate that high correlation between human and 
automatic scores in both adequacy and fluency 
cannot always been achieved at the same time. 
The best performing metrics in fluency 
according to Table 1 are BLEU12 and GTM30 
(dark/green cells). However, many metrics are 
statistically equivalent (gray cells) to them when 
we factor in the 95% confidence intervals. For 
example, even PER is as good as BLEU12 in 
adequacy. One reason for this might be due to data 
sparseness since only 8 systems are available. 
The other potential problem for correlation 
analysis of human vs. automatic framework is that 
high corpus-level correlation might not translate to 
high sentence-level correlation. However, high 
sentence-level correlation is often an important 
property that machine translation researchers look 
for. For example, candidate translations shorter 
than 12 words would have zero BLEU12 score but 
BLEU12 has the best correlation with human 
judgment in fluency as shown in Table 1. 
In order to evaluate the ever increasing number 
of automatic evaluation metrics for machine 
translation objectively, efficiently, and reliably, we 
introduce a new evaluation method: ORANGE. We 
describe ORANGE in details in Section 2 and 
briefly introduce three new automatic metrics that 
will be used in comparisons in Section 3. The 
results of comparing several existing automatic 
metrics and the three new automatic metrics using 
ORANGE will be presented in Section 4. We 
conclude this paper and discuss future directions in 
Section 5. 
2 ORANGE 
Intuitively a good evaluation metric should give 
higher score to a good translation than a bad one. 
Therefore, a good translation should be ranked 
higher than a bad translation based their scores. 
One basic assumption of all automatic evaluation 
metrics for machine translation is that reference 
translations are good translations and the more a 
machine translation is similar to its reference 
translations the better. We adopt this assumption 
and add one more assumption that automatic 
translations are usually worst than their reference 
translations. Therefore, reference translations 
should be ranked higher than machine translations 
on average if a good automatic evaluation metric is 
used. Based on these assumptions, we propose a 
new automatic evaluation method for evaluation of 
automatic machine translation metrics as follows: 
 
Given a source sentence, its machine 
translations, and its reference translations, we 
compute the average rank of the reference 
translations within the combined machine and 
reference translation list. For example, a 
statistical machine translation system such as 
ISI?s AlTemp SMT system (Och 2003) can 
generate a list of n-best alternative translations 
given a source sentence. We compute the 
automatic scores for the n-best translations 
and their reference translations. We then rank 
these translations, calculate the average rank 
of the references in the n-best list, and 
compute the ratio of the average reference 
rank to the length of the n-best list. We call 
this ratio ?ORANGE? (Oracle1 Ranking for 
Gisting Evaluation) and the smaller the ratio 
is, the better the automatic metric is. 
 
There are several advantages of the proposed 
ORANGE evaluation method: 
? No extra human involvement ? ORANGE 
uses the existing human references but not 
human evaluations. 
? Applicable on sentence-level ? Diagnostic 
error analysis on sentence-level is naturally 
provided. This is a feature that many 
machine translation researchers look for. 
? Many existing data points ? Every sentence 
is a data point instead of every system 
(corpus-level). For example, there are 919 
sentences vs. 8 systems in the 2003 NIST 
Chinese-English machine translation 
evaluation. 
? Only one objective function to optimize ? 
Minimize a single ORANGE score instead of 
maximize Pearson?s correlation coefficients 
between automatic scores and human 
judgments in adequacy, fluency, or other 
quality metrics.  
? A natural fit to the existing statistical 
machine translation framework ? A metric 
that ranks a good translation high in an n-
best list could be easily integrated in a 
minimal error rate statistical machine 
translation training framework (Och 2003). 
The overall system performance in terms of 
                                                     
1 Oracles refer to the reference translations used in 
the evaluation procedure. 
Method Pearson 95%L 95%U Pearson 95%L 95%U
BLEU1 0.86 0.83 0.89 0.81 0.75 0.86
BLEU4 0.77 0.72 0.81 0.86 0.81 0.90
BLEU12 0.66 0.60 0.72 0.87 0.76 0.93
NIST 0.89 0.86 0.92 0.81 0.75 0.87
WER 0.47 0.41 0.53 0.69 0.62 0.75
PER 0.67 0.62 0.72 0.79 0.74 0.85
GTM10 0.82 0.79 0.85 0.73 0.66 0.79
GTM20 0.77 0.73 0.81 0.86 0.81 0.90
GTM30 0.74 0.70 0.78 0.87 0.81 0.91
Adequacy Fluency
Table 1. Pearson's correlation analysis of 8 
machine translation systems in 2003 NIST 
Chinese-English machine translation 
evaluation. 
generating more human like translations 
should also be improved.  
Before we demonstrate how to use ORANGE to 
evaluate automatic metrics, we briefly introduce 
three new metrics in the next section. 
3 Three New Metrics 
ROUGE-L and ROUGE-S are described in details 
in Lin and Och (2004). Since these two metrics are 
relatively new, we provide short summaries of 
them in Section 3.1 and Section 3.3 respectively. 
ROUGE-W, an extension of ROUGE-L, is new and 
is explained in details in Section 3.2. 
3.1 ROUGE-L: Longest Common Sub-
sequence 
Given two sequences X and Y, the longest 
common subsequence (LCS) of X and Y is a 
common subsequence with maximum length 
(Cormen et al 1989). To apply LCS in machine 
translation evaluation, we view a translation as a 
sequence of words. The intuition is that the longer 
the LCS of two translations is, the more similar the 
two translations are. We propose using LCS-based 
F-measure to estimate the similarity between two 
translations X of length m and Y of length n, 
assuming X is a reference translation and Y is a 
candidate translation, as follows: 
 
Rlcs m
YXLCS ),(
=       (1) 
Plcs n
YXLCS ),(
=       (2) 
Flcs
lcslcs
lcslcs
PR
PR
2
2 )1(
?
?
+
+
=   (3) 
 
Where LCS(X,Y) is the length of a longest 
common subsequence of X and Y, and ? = Plcs/Rlcs 
when ?Flcs/?Rlcs_=_?Flcs/?Plcs. We call the LCS-
based F-measure, i.e. Equation 3, ROUGE-L. 
Notice that ROUGE-L is 1 when X = Y since 
LCS(X,Y) = m or n; while ROUGE-L is zero when 
LCS(X,Y) = 0, i.e. there is nothing in common 
between X and Y.  
One advantage of using LCS is that it does not 
require consecutive matches but in-sequence 
matches that reflect sentence level word order as n-
grams. The other advantage is that it automatically 
includes longest in-sequence common n-grams, 
therefore no predefined n-gram length is necessary.  
By only awarding credit to in-sequence unigram 
matches, ROUGE-L also captures sentence level 
structure in a natural way. Consider the following 
example: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
 
Using S1 as the reference translation, S2 has a 
ROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE-
L score of 2/4 = 0.5, with ? = 1. Therefore S2 is 
better than S3 according to ROUGE-L. This 
example illustrated that ROUGE-L can work 
reliably at sentence level. However, LCS suffers 
one disadvantage: it only counts the main in-
sequence words; therefore, other alternative LCSes 
and shorter sequences are not reflected in the final 
score. In the next section, we introduce ROUGE-W. 
3.2 ROUGE-W: Weighted Longest Common 
Subsequence 
LCS has many nice properties as we have 
described in the previous sections. Unfortunately, 
the basic LCS also has a problem that it does not 
differentiate LCSes of different spatial relations 
within their embedding sequences. For example, 
given a reference sequence X and two candidate 
sequences Y1 and Y2 as follows: 
 
X:  [A B C D E F G] 
Y1: [A B C D H I K] 
Y2:  [A H B K C I D] 
 
Y1 and Y2 have the same ROUGE-L score. 
However, in this case, Y1 should be the better 
choice than Y2 because Y1 has consecutive matches. 
To improve the basic LCS method, we can simply 
remember the length of consecutive matches 
encountered so far to a regular two dimensional 
dynamic program table computing LCS. We call 
this weighted LCS (WLCS) and use k to indicate 
the length of the current consecutive matches 
ending at words xi and yj. Given two sentences X 
and Y, the recurrent relations can be written as 
follows: 
 
(1) If xi = yj Then 
// the length of consecutive matches at 
// position i-1 and j-1 
k = w(i-1,j-1) 
c(i,j) = c(i-1,j-1) + f(k+1) ? f(k) 
// remember the length of consecutive 
// matches at position i, j 
w(i,j) = k+1 
(2) Otherwise 
If c(i-1,j) > c(i,j-1) Then 
c(i,j) = c(i-1,j) 
w(i,j) = 0           // no match at i, j 
Else c(i,j) = c(i,j-1) 
 w(i,j) = 0           // no match at i, j 
(3) WLCS(X,Y) = c(m,n) 
 
Where c is the dynamic programming table, 0 <= 
i <= m, 0 <= j <= n, w is the table storing the 
length of consecutive matches ended at c table 
position i and j, and f is a function of consecutive 
matches at the table position, c(i,j). Notice that by 
providing different weighting function f, we can 
parameterize the WLCS algorithm to assign 
different credit to consecutive in-sequence 
matches.  
The weighting function f must have the property 
that f(x+y) > f(x) + f(y) for any positive integers x 
and y. In other words, consecutive matches are 
awarded more scores than non-consecutive 
matches. For example, f(k)-=-?k ? ? when k >= 0, 
and ?, ? > 0. This function charges a gap penalty 
of ?? for each non-consecutive n-gram sequences. 
Another possible function family is the polynomial 
family of the form k? where -? > 1. However, in 
order to normalize the final ROUGE-W score, we 
also prefer to have a function that has a close form 
inverse function. For example, f(k)-=-k2 has a close 
form inverse function f -1(k)-=-k1/2. F-measure 
based on WLCS can be computed as follows, 
given two sequences X of length m and Y of length 
n: 
Rwlcs ???
?
???
?
=
?
)(
),(1
mf
YXWLCSf       (4) 
Pwlcs ???
?
???
?
=
?
)(
),(1
nf
YXWLCSf       (5) 
Fwlcs  
wlcswlcs
wlcswlcs
PR
PR
2
2 )1(
?
?
+
+
=           (6) 
   
 f -1 is the inverse function of f. We call the 
WLCS-based F-measure, i.e. Equation 6, ROUGE-
W. Using Equation 6 and f(k)-=-k2 as the 
weighting function, the ROUGE-W scores for 
sequences Y1 and Y2 are 0.571 and 0.286 
respectively. Therefore, Y1 would be ranked 
higher than Y2 using WLCS. We use the 
polynomial function of the form k? in the 
experiments described in Section 4 with the 
weighting factor ? varying from 1.1 to 2.0 with 0.1 
increment. ROUGE-W is the same as ROUGE-L 
when ? is set to 1. 
In the next section, we introduce the skip-bigram 
co-occurrence statistics. 
3.3 ROUGE-S: Skip-Bigram Co-Occurrence 
Statistics 
Skip-bigram is any pair of words in their sentence 
order, allowing for arbitrary gaps. Skip-bigram co-
occurrence statistics measure the overlap of skip-
bigrams between a candidate translation and a set 
of reference translations. Using the example given 
in Section 3.1: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
S4. the gunman police killed 
 
each sentence has C(4,2)2 = 6 skip-bigrams. For 
example, S1 has the following skip-bigrams: 
 
(?police killed?, ?police the?, ?police gunman?, 
?killed the?, ?killed gunman?, ?the gunman?) 
 
Given translations X of length m and Y of length 
n, assuming X is a reference translation and Y is a 
candidate translation, we compute skip-bigram-
based F-measure as follows: 
 
Rskip2 )2,(
),(2
mC
YXSKIP
=           (7) 
Pskip2 )2,(
),(2
nC
YXSKIP
=           (8) 
Fskip2 
2
2
2
22
2 )1(
skipskip
skipskip
PR
PR
?
?
+
+
=   (9) 
 
Where SKIP2(X,Y) is the number of skip-bigram 
matches between X and Y, ? = Pskip2/Rskip2 when 
?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and  C is the 
combination function. We call the skip-bigram-
based F-measure, i.e. Equation 9, ROUGE-S. Using 
Equation 9 with ? = 1 and S1 as the reference, S2?s 
ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333. 
Therefore, S2 is better than S3 and S4, and S4 is 
better than S3.  
One advantage of skip-bigram vs. BLEU is that it 
does not require consecutive matches but is still 
sensitive to word order. Comparing skip-bigram 
with LCS, skip-bigram counts all in-order 
matching word pairs while LCS only counts one 
longest common subsequence. We can limit the 
maximum skip distance, between two in-order 
words to control the admission of a skip-bigram. 
We use skip distances of 1 to 9 with increment of 1 
(ROUGE-S1 to 9) and without any skip distance 
constraint (ROUGE-S*).  
In the next section, we present the evaluations of 
BLEU, NIST, PER, WER, ROUGE-L, ROUGE-W, 
and ROUGE-S using the ORANGE evaluation 
method described in Section 2. 
                                                     
2 Combinations: C(4,2) = 4!/(2!*2!) = 6. 
4 Experiments 
Comparing automatic evaluation metrics using 
the ORANGE evaluation method is straightforward. 
To simulate real world scenario, we use n-best lists 
from ISI?s state-of-the-art statistical machine 
translation system, AlTemp (Och 2003), and the 
2002 NIST Chinese-English evaluation corpus as 
the test corpus. There are 878 source sentences in 
Chinese and 4 sets of reference translations 
provided by LDC3. For exploration study, we 
generate 1024-best list using AlTemp for 872 
source sentences. AlTemp generates less than 1024 
alternative translations for 6 out of the 878 source 
                                                     
3 Linguistic Data Consortium prepared these manual 
translations as part of the DARPA?s TIDES project. 
sentences. These 6 source sentences are excluded 
from the 1024-best set. In order to compute BLEU 
at sentence level, we apply the following 
smoothing technique: 
Add one count to the n-gram hit and total n-
gram count for n > 1. Therefore, for candidate 
translations with less than n words, they can 
still get a positive smoothed BLEU score from 
shorter n-gram matches; however if nothing 
matches then they will get zero scores. 
We call the smoothed BLEU: BLEUS. For each 
candidate translation in the 1024-best list and each 
reference, we compute the following scores: 
1. BLEUS1 to 9 
2. NIST, PER, and WER 
3. ROUGE-L 
4. ROUGE-W with weight ranging from 1.1 
to 2.0 with increment of 0.1 
5. ROUGE-S with maximum skip distance 
ranging from 0 to 9 (ROUGE-S0 to S9) 
and without any skip distance limit 
(ROUGE-S*) 
We compute the average score of the references 
and then rank the candidate translations and the 
references according to these automatic scores. 
The ORANGE score for each metric is calculated as 
the average rank of the average reference (oracle) 
score over the whole corpus (872 sentences) 
divided by the length of the n-best list plus 1. 
Assuming the length of the n-best list is N and the 
size of the corpus is S (in number of sentences), we 
compute Orange as follows: 
 
       ORANGE = 
)1(
)(
1
+
???
?
???
??
=
NS
OracleRank
S
i
i
    (10) 
 
Rank(Oraclei) is the average rank of source 
sentence i?s reference translations in n-best list i. 
Table 2 shows the results for BLEUS1 to 9. To 
assess the reliability of the results, 95% confidence 
intervals (95%-CI-L for lower bound and CI-U for 
upper bound) of average rank of the oracles are 
Method ORANGE Avg Rank 95%-CI-L  95%-CI-U
BLEUS1 35.39% 363 337 387
BLEUS2 25.51% 261 239 283
BLEUS3 23.74% 243 221 267
BLEUS4 23.13% 237 215 258
BLEUS5 23.13% 237 215 260
BLEUS6 22.91% 235 211 257
BLEUS7 22.98% 236 213 258
BLEUS8 23.20% 238 214 261
BLEUS9 23.56% 241 218 265
Table 2. ORANGE scores for BLEUS1 to 9. 
Method Pearson 95%L 95%U Pearson 95%L 95%U
BLEUS1 0.87 0.84 0.90 0.83 0.77 0.88
BLEUS2 0.84 0.81 0.87 0.85 0.80 0.90
BLEUS3 0.80 0.76 0.84 0.87 0.82 0.91
BLEUS4 0.76 0.72 0.80 0.88 0.83 0.92
BLEUS5 0.73 0.69 0.78 0.88 0.83 0.91
BLEUS6 0.70 0.65 0.75 0.87 0.82 0.91
BLEUS7 0.65 0.60 0.70 0.85 0.80 0.89
BLEUS8 0.58 0.52 0.64 0.82 0.76 0.86
BLEUS9 0.50 0.44 0.57 0.76 0.70 0.82
Adequacy Fluency
Table 3. Pearson's correlation analysis 
BLEUS1 to 9 vs. adequacy and fluency of 8 
machine translation systems in 2003 NIST 
Chinese-English machine translation 
evaluation. 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
ROUGE-L 20.56% 211 190 234
ROUGE-W-1.1 20.45% 210 189 232
ROUGE-W-1.2 20.47% 210 186 230
ROUGE-W-1.3 20.69% 212 188 234
ROUGE-W-1.4 20.91% 214 191 238
ROUGE-W-1.5 21.17% 217 196 241
ROUGE-W-1.6 21.47% 220 199 242
ROUGE-W-1.7 21.72% 223 200 245
ROUGE-W-1.8 21.88% 224 204 246
ROUGE-W-1.9 22.04% 226 203 249
ROUGE-W-2.0 22.25% 228 206 250
Table 4. ORANGE scores for ROUGE-L and 
ROUGE-W-1.1 to 2.0. 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
ROUGE-S0 25.15% 258 234 280
ROUGE-S1 22.44% 230 209 253
ROUGE-S2 20.38% 209 186 231
ROUGE-S3 19.81% 203 183 226
ROUGE-S4 19.66% 202 177 224
ROUGE-S5 19.95% 204 184 226
ROUGE-S6 20.32% 208 187 230
ROUGE-S7 20.77% 213 191 236
ROUGE-S8 21.42% 220 198 242
ROUGE-S9 21.92% 225 204 247
ROUGE-S* 27.43% 281 259 304
Table 5. ORANGE scores for ROUGE-S1 to 9 
and ROUGE-S*. 
estimated using bootstrap resampling (Davison and 
Hinkley). According to Table 2, BLEUS6 
(dark/green cell) is the best performer among all 
BLEUSes, but it is statistically equivalent to 
BLEUS3, 4, 5, 7, 8, and 9 with 95% of confidence. 
Table 3 shows Pearson?s correlation coefficient 
for BLEUS1 to 9 over 8 participants in 2003 NIST 
Chinese-English machine translation evaluation. 
According to Table 3, we find that shorter BLEUS 
has better correlation with adequacy. However, 
correlation with fluency increases when longer n-
gram is considered but decreases after BLEUS5. 
There is no consensus winner that achieves best 
correlation with adequacy and fluency at the same 
time. So which version of BLEUS should we use? 
A reasonable answer is that if we would like to 
optimize for adequacy then choose BLEUS1; 
however, if we would like to optimize for fluency 
then choose BLEUS4 or BLEUS5. According to 
Table 2, we know that BLEUS6 on average places 
reference translations at rank 235 in a 1024-best 
list machine translations that is significantly better 
than BLEUS1 and BLEUS2. Therefore, we have 
better chance of finding more human-like 
translations on the top of an n-best list by choosing 
BLEUS6 instead of BLEUS2. To design automatic 
metrics better than BLEUS6, we can carry out error 
analysis over the machine translations that are 
ranked higher than their references. Based on the 
results of error analysis, promising modifications 
can be identified. This indicates that the ORANGE 
evaluation method provides a natural automatic 
evaluation metric development cycle. 
Table 4 shows the ORANGE scores for ROUGE-L 
and ROUGE-W-1.1 to 2.0. ROUGE-W 1.1 does have 
better ORANGE score but it is equivalent to other 
ROUGE-W variants and ROUGE-L. Table 5 lists 
performance of different ROUGE-S variants. 
ROUGE-S4 is the best performer but is only 
significantly better than ROUGE-S0 (bigram), 
ROUGE-S1, ROUGE-S9 and ROUGE-S*. The 
relatively worse performance of ROUGE-S* might 
to due to spurious matches such as ?the the? or 
?the of?. 
Table 6 summarizes the performance of 7 
different metrics. ROUGE-S4 (dark/green cell) is 
the best with an ORANGE score of 19.66% that is 
statistically equivalent to ROUGE-L and ROUGE-
W-1.1 (gray cells) and is significantly better than 
BLEUS6, NIST, PER, and WER. Among them 
PER is the worst. 
To examine the length effect of n-best lists on  
the relative performance of automatic metrics, we 
use the AlTemp SMT system to generate a 16384-
best list and compute ORANGE scores for BLEUS4, 
PER, WER, ROUGE-L, ROUGE-W-1.2, and 
ROUGE-S4. Only 474 source sentences that have 
more than 16384 alternative translations are used 
in this experiment. Table 7 shows the results. It 
confirms that when we extend the length of the n-
best list to 16 times the size of the 1024-best, the 
relative performance of each automatic evaluation 
metric group stays the same. ROUGE-S4 is still the 
best performer. Figure 1 shows the trend of 
ORANGE scores for these metrics over N-best list 
of N from 1 to 16384 with length increment of 64. 
It is clear that relative performance of these metrics 
stay the same over the entire range. 
5 Conclusion 
In this paper we introduce a new automatic 
evaluation method, ORANGE, to evaluate automatic 
evaluation metrics for machine translations. We 
showed that the new method can be easily 
implemented and integrated with existing 
statistical machine translation frameworks. 
ORANGE assumes a good automatic evaluation 
metric should assign high scores to good 
translations and assign low scores to bad 
translations. Using reference translations as 
examples of good translations, we measure the 
quality of an automatic evaluation metric based on 
the average rank of the references within a list of 
alternative machine translations. Comparing with 
traditional approaches that require human 
judgments on adequacy or fluency, ORANGE 
requires no extra human involvement other than 
the availability of reference translations.  It also 
streamlines the process of design and error analysis 
for developing new automatic metrics. Using 
ORANGE, we have only one parameter, i.e. 
ORANGE itself, to optimize vs. two in correlation 
analysis using human assigned adequacy and 
fluency. By examining the rank position of the 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
BLEUS6 22.91% 235 211 257
NIST 29.70% 304 280 328
PER 36.84% 378 350 403
WER 23.90% 245 222 268
ROUGE-L 20.56% 211 190 234
ROUGE-W-1.1 20.45% 210 189 232
ROUGE-S4 19.66% 202 177 224
Table 6. Summary of ORANGE scores for 7 
automatic evaluation metrics. 
Method ORANGE Avg Rank 95%-CI-L 95%-CI-U
BLEUS4 18.27% 2993 2607 3474
PER 28.95% 4744 4245 5292
WER 19.36% 3172 2748 3639
ROUGE-L 16.22% 2657 2259 3072
ROUGE-W-1.2 15.87% 2600 2216 2989
ROUGE-S4 14.92% 2444 2028 2860
Table 7. Summary of ORANGE scores for 6 
automatic evaluation metrics (16384-best list). 
references, we can easily identify the confusion set 
of the references and propose new features to 
improve automatic metrics. 
One caveat of the ORANGE method is that what 
if machine translations are as good as reference 
translations? To rule out this scenario, we can 
sample instances where machine translations are 
ranked higher than human translations. We then 
check the portion of the cases where machine 
translations are as good as the human translations.  
If the portion is small then the ORANGE method 
can be confidently applied. We conjecture that this 
is the case for the currently available machine 
translation systems. However, we plan to conduct 
the sampling procedure to verify this is indeed the 
case. 
References  
Akiba, Y., K. Imamura, and E. Sumita. 2001. Using 
Multiple Edit Distances to Automatically Rank 
Machine Translation Output. In Proceedings of the 
MT Summit VIII, Santiago de Compostela, Spain. 
Cormen, T. R., C. E. Leiserson, and R. L. Rivest. 1989. 
Introduction to Algorithms. The MIT Press. 
Davison, A. C. and D. V. Hinkley. 1997. Bootstrap 
Methods and Their Application. Cambridge 
University Press. 
Leusch, G., N. Ueffing, and H. Ney. 2003. A Novel 
String-to-String Distance Measure with Applications 
to Machine Translation Evaluation. In Proceedings of 
MT Summit IX, New Orleans, U.S.A. 
Lin, C-Y. and F.J. Och. 2004. Automatic Evaluation of 
Machine Translation Quality Using Longest 
Common Subsequence and Skip-Bigram Statistics. 
Submitted. 
Nie?en S., F.J. Och, G, Leusch, H. Ney. 2000. An 
Evaluation Tool for Machine Translation: Fast 
Evaluation for MT Research. In Proceedings of the 
2nd International Conference on Language Resources 
and Evaluation, Athens, Greece. 
NIST. 2002. Automatic Evaluation of Machine 
Translation Quality using N-gram Co-Occurrence 
Statistics.   AAAAAAAAAAA 
http://www.nist.gov/speech/tests/mt/doc 
Franz Josef Och. 2003. Minimum Error Rate Training 
for Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sapporo, 
Japan. 
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2001. 
Bleu: a Method for Automatic Evaluation of Machine 
Translation. IBM Research Report RC22176 
(W0109-022). 
Su, K.-Y., M.-W. Wu, and J.-S. Chang. 1992. A New 
Quantitative Quality Measure for Machine 
Translation System. In Proceedings of COLING-92, 
Nantes, France. 
Turian, J. P., L. Shen, and I. D. Melamed. 2003. 
Evaluation of Machine Translation and its 
Evaluation. In Proceedings of MT Summit IX, New 
Orleans, U.S.A. 
 
2000 4000 6000 8000 10000 12000 14000 16000
0.1
0.15
0.2
0.25
0.3
0.35
0.4
ORANGE at Different NBEST Cutoff Length avg
NBEST Cutoff Length = 1 to 16384
O
RA
NG
E
BLEUS4
ROUGE?L
PER
ROUGE?S4
WER
ROUGE?W?1?2
Figure 1. ORANGE scores for 6 metrics vs. length of n-best list from 1 to 
16384 with increment of 64.
Toward Semantics-Based Answer Pinpointing
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292-6695
USA
tel: +1-310-448-8731
{hovy,gerber,ulf,cyl,ravichan}@isi.edu
ABSTRACT
We describe the treatment of questions (Question-Answer
Typology, question parsing, and results) in the Weblcopedia
question answering system.  
1. INTRODUCTION
Several research projects have recently investigated the
problem of automatically answering simple questions that have
brief phrasal answers (?factoids?), by identifying and extracting
the answer from a large collection of text.  
The systems built in these projects exhibit a fairly standard
structure: they create a query from the user?s question, perform
IR with the query to locate (segments of) documents likely to
contain an answer, and then pinpoint the most likely answer
passage within the candidate documents.  The most common
difference lies in the pinpointing. Many projects employ a
window-based word scoring method that rewards desirable
words in the window.  They move the window across the
candidate answers texts/segments and return the window at the
position giving the highest total score.  A word is desirable if
it is a content word and it is either contained in the question, or
is a variant of a word contained in the question, or if it matches
the words of the expected answer.  Many variations of this
method are possible?of the scores, of the treatment of multi-
word phrases and gaps between desirable words, of the range of
variations allowed, and of the computation of the expected
answer words.  
Although it works to some degree (giving results of up to 30%
in independent evaluations), the window-based method has
several quite serious limitations:
? it cannot pinpoint answer boundaries precisely (e.g., an
exact name or noun phrase),
? it relies solely on information at the word level, and
hence cannot recognize information of the desired type
(such as Person or Location),
? it cannot locate and compose parts of answers that are
distributed over areas wider than the window.
Window-based pinpointing is therefore not satisfactory in the
long run, even for factoid QA.  In this paper we describe work
in our Webclopedia project on semantics-based answer
pinpointing. Initially, though, recognizing the simplicity and
power of the window-based technique for getting started, we
implemented a version of it as a fallback method.  We then
implemented two more sophisticated methods: syntactic-
semantic question analysis and QA pattern matching.  This
involves classification of QA types to facilitate recognition of
desired answer types, a robust syntactic-semantic parser to
analyze the question and candidate answers, and a matcher that
combines word- and parse-tree-level information to identify
answer passages more precisely.  We expect that the two
methods will really show their power when more complex non-
factoid answers are sought.  In this paper we describe how well
the three methods did relative to each other.  Section 2 outlines
the Webclopedia system.  Sections 3, 4, and 5 describe the
semantics-based components: a QA Typology, question and
answer parsing, and matching.  Finally, we outline current
work on automatically learning QA patterns using the Noisy
Channel Model.  
2. WEBCLOPEDIA
Webclopedia?s architecture (Figure 1) follows the pattern
outlined above:
Question parsing: Using BBN?s IdentiFinder [1], our
parser CONTEX (Section 4) produces a syntactic-semantic
analysis of the question and determines the QA type (Section
3).  
Query formation : Single- and multi-word units (content
words) are extracted from the analysis, and WordNet synsets are
used for query expansion.  A Boolean query is formed. See [9].
IR: The IR engine MG [12] returns the top-ranked 1000
documents.
Segmentat ion : To decrease the amount of text to be
processed, the documents are broken into semantically
coherent segments.  Two text segmenter?TexTiling [5] and
C99 [2]?were tried; the first is used; see [9].
Ranking segments : For each segment, each sentence i s
scored using a formula that rewards word and phrase overlap
with the question and its expanded query words.  Segments are
ranked.  See [9]
Parsing segments : CONTEX parses each sentence of the
top-ranked 100 segments (Section 4).  
Pinpointing: For each sentence, three steps of matching are
performed (Section 5); two compare the analyses of the
question and the sentence; the third uses the window method to
compute a goodness score.  
Ranking of answers : The candidate answers? scores are
compared and the winner(s) are output.
3. THE QA TYPOLOGY
In order to perform pinpointing deeper than the word level, the
system has to produce a representation of what the user i s
asking.  Some previous work in automated question answering
has categorized questions by question word or by a mixture of
question word and the semantic class of the answer [11, 10].  To
ensure full coverage of all forms of simple question and answer,
and to be able to factor in deviations and special requirements,
we are developing a QA Typology.  
We motivate the Typology (a taxonomy of QA types) as
follows.  
There are many ways to ask the same thing: What is the age o f
the Queen of Holland?  How old is the Netherlands? queen?  How
long has the ruler of Holland been alive?  Likewise, there are
many ways of delivering the same answer: about 60; 63 years
old; since January 1938.  Such variations form a sort of
semantic equivalence class of both questions and answers.
Since the user may employ any version of his or her question,
and the source documents may contain any version(s) of the
answer, an efficient system should group together equivalent
question types and answer types.  Any specific question can
then be indexed into its type, from which all equivalent forms
of the answer can be ascertained.  These QA equivalence types
can help with both query expansion and answer pinpointing.
However, the equivalence is fuzzy; even slight variations
introduce exceptions: who invented the gas laser? can be
answered by both Ali Javan and a scientist at MIT, while what
is the name of the person who invented the gas laser? requires
the former only.  This inexactness suggests that the QA types
be organized in an inheritance hierarchy, allowing the answer
requirements satisfying more general questions to be
overridden by more specific ones ?lower down?.  
These considerations help structure the Webclopedia QA
Typology.  Instead of focusing on question word or semantic
type of the answer, our classes attempt to represent the user?s
intention, including for example the classes Why-Famous (for
Who was Christopher Columbus? but not Who discovered
IR
? Steps: create query from question (WordNet-expand)
             retrieve top 1000 documents
? Engines: MG (Sydney)?(Lin)
                  AT&T (TREC)?(Lin)
Segmentation
? Steps:segment each document into topical segments
? Engines: fixed-length (not used)
                 TexTiling (Hearst 94)?(Lin)
                 C99 (Choi 00)?(Lin)
                 MAXNET (Lin 00, not used)
Ranking
? Steps: score each sentence in each segment,
                              using WordNet expansion
             rank segments
? Engines: FastFinder (Junk)
Matching
? Steps: match general constraint patterns against parse trees
            match desired semantic type against parse tree elements
            match desired words against words in sentences
? Engines: matcher (Junk)
Ranking and answer extraction
? Steps: rank candidate answers
            extract and format them
? Engines: part of matcher (Junk)
Question parsing
? Steps: parse question
            find desired semantic type
? Engines: IdentiFinder (BBN)
                CONTEX (Hermjakob)
QA typology
? Categorize QA types in taxonomy (Gerber)
Constraint patterns
? Identify likely answers in relation to other
   parts of the sentence (Gerber)
Retrieve documents
Segment documents
Rank segments
Parse top segments
Parse question
Input question
Match segments against question
Rank and prepare answers
Create query
Output answers
Segment Parsing
? Steps: parse segment sentences
? Engines: CONTEX (Hermjakob)
Figure 1. Webclopedia architecture.
America?, which is the QA type Proper-Person) and
Abbreviation-Expansion (for What does HLT stand for?).  In
addition, the QA Typology becomes increasingly specific as
one moves from the root downward.
To create the QA Typology, we analyzed 17,384 questions and
their answers (downloaded from answers.com); see (Gerber, in
prep.).  The Typology (Figure 2) contains 72 nodes, whose leaf
nodes capture QA variations that can in many cases be further
differentiated.
Each Typology node has been annotated with examples and
typical patterns of expression of both Question and Answer,
using a simple template notation that expressed configurations
of words and parse tree annotations (Figure 3).  Question
pattern information (specifically, the semantic type of the
answer required, which we call a Qtarget) is produced by the
CONTEX parser (Section 4) when analyzing the question,
enabling it to output its guess(s) for the QA type.  Answer
pattern information is used by the Matcher (Section 5) to
pinpoint likely answer(s) in the parse trees of candidate answer
sentences.
Question examples and question templates
Who was Johnny Mathis' high school track coach?
Who was Lincoln's Secretary of State?
who be <entity>'s <role>
Who was President of Turkmenistan in 1994?
Who is the composer of Eugene Onegin?
Who is the chairman of GE?
who be <role> of <entity>
Answer templates and actual answers
<person>, <role> of  <entity>
Lou Vasquez, track coach of?and Johnny Mathis
<person> <role-title*> of <entity>
Signed Saparmurad Turkmenbachy [Niyazov],
president of Turkmenistan
<entity>?s <role> <person>
...Turkmenistan?s President Saparmurad Niyazov
<person>'s <entity>
...in Tchaikovsky's Eugene Onegin...
<role-title> <person> ... <entity> <role>
Mr. Jack Welch, GE chairman...
<subject>|<psv object> of related role-verb
       ...Chairman John Welch said ...GE's
Figure 3. Some QA Typology node annotations for
Proper-Person.
At the time of the TREC-9 Q&A evaluation, we had produced
approx. 500 patterns by simply cross-combining approx. 20
Question patterns with approx. 25 Answer patterns.  To our
disappointment (Section 6), these patterns were both too
specific and too few to identify answers frequently?when they
applied, they were quite accurate, but they applied too seldom.
We therefore started work on automatically learning QA
patterns in parse trees (Section 7).  On the other hand, the
semantic class of the answer (the Qtarget) is used to good effect
(Sections 4 and 6).
4. PARSING
CONTEX is a deterministic machine-learning based grammar
learner/parser that was originally built for MT [6].  For
English, parses of unseen sentences measured 87.6% labeled
precision and 88.4% labeled recall, trained on 2048 sentences
from the Penn Treebank. Over the past few years it has been
extended to Japanese and Korean [7].
4.1 Parsing Questions
Accuracy is particularly important for question parsing,
because for only one question there may be several answers in a
large document collection.  In particular, it is important to
identify as specific a Qtarget as possible.  But grammar rules
ERACITY YES:NO
TRUE:FALSE
NTIT Y A GENT NAME LAST-NAME
FIRST-NAME
ORGANIZATION
GROUP-OF-PEOPLE
A NIMAL
PERSON OCCUPATION-PERSON
GEOGRAPHICA L-PERSON
PROPER-NAMED-ENTITY PROPER-PERSON
PROPER-ORGANIZATION
PROPER-PLACE CITY
COUNTRY
STATE-DISTRICT
QUANTITY NU MERICAL-QUANTI TY
MONETARY-QUANTITY
TEMPORAL-QUANTITY
MASS-QUANTI TY
SPATIAL-QUANTITY DISTANCE-QUANTITY
AREA-QUANTITY
VOLUME-QUANTI TY
TEMP-LOC DATE
DATE-RANGE
LOCATOR ADDRESS
EMAIL-ADDRESS
PHONE-NUMBER
URL
TANGIBLE-OBJECT HU MAN-FOOD
SUBS TANCE LIQUID
BODY-PART
INSTRUMENT
GARMENT
TITLED-WORK
ABSTRACT SHAPE
ADJECTIVE COLOR
DISEASE
TEXT
NARRATIVE GENERAL-INFO DEFINITION USE
EXPRESSION-ORIGIN
HISTORY WHY-FAMOUS BIO
ANTECEDENT
INFLUENCE CONSEQUENT
CAUSE-EFFECT METHOD-MEANS
CIRCUMSTANCE-MEANS REASON
EVALUATION PRO-CON
CONTRAST
RATING
COUNSEL-ADVICE
Figure 2. Portion of Webclopedia QA Typology.
for declarative sentences do not apply well to questions, which
although typically shorter than declaratives, exhibit markedly
different word order, preposition stranding (?What university
was Woodrow Wilson President of??), etc.  
Unfortunately for CONTEX, questions to train on were not
initially easily available; the Wall Street Journal sentences
contain a few questions, often from quotes, but not enough and
not representative enough to result in an acceptable level of
question parse accuracy.  By collecting and treebanking,
however, we increased the number of questions in the training
data from 250 (for our TREC-9 evaluation version of
Webclopedia) to 400 on Oct 16 to 975 on Dec 9.  The effect i s
shown in Table 1.  In the first test run (?[trained] without
[additional questions]?), CONTEX was trained mostly on
declarative sentences (2000 Wall Street Journal sentences,
namely the enriched Penn Treebank, plus a few other non-
question sentences such as imperatives and short phrases).  In
later runs (?[trained] with [add. questions]?), the system was
trained on the same examples plus a subset of the 1153
questions we have treebanked at ISI (38 questions from the pre-
TREC-8 test set, all 200 from TREC-8 and 693 TREC-9, and
222 others).
The TREC-8 and TREC-9 questions were divided into 5 subsets,
used in a five-fold cross validation test in which the system was
trained on all but the test questions, and then evaluated on the
test questions.   
Reasons for the improvement include (1) significantly more
training data; (2) a few additional features, some more treebank
cleaning, a bit more background knowledge etc.; and (3) the
251 test questions on Oct. 16 were probably a little bit harder
on average, because a few of the TREC-9 questions initially
treebanked (and included in the October figures) were selected
for early treebanking because they represented particular
challenges, hurting subsequent Qtarget processing.
4.2 Parsing Potential Answers
The semantic type ontology in CONTEX was extended to
include 115 Qtarget types, plus some combined types; more
details in [8].  Beside the Qtargets that refer to concepts in
CONTEX?s concept ontology (see first example below),
Qtargets can also refer to part of speech labels (first example),
to constituent roles or slots of parse trees (second and third
examples), and to more abstract nodes in the QA Typology
(later examples). For questions with the Qtargets Q-WHY-
FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and
others, the parser also provides Qargs?information helpful for
matching (final examples).
Semantic ontology types (I-EN-CITY)
and part of speech labels (S-PROPER-NAME):
What is the capital of Uganda?
QTARGET: (((I-EN-CITY S-PROPER-NAME))
((EQ I-EN-PROPER-PLACE)))
Parse tree roles:
Why can't ostriches fly?
      QTARGET: (((ROLE REASON)))
Name a film in which Jude Law acted.
      QTARGET: (((SLOT TITLE-P TRUE)))
QA Typology nodes:
What are the Black Hills known for?
     Q-WHY-FAMOUS
What is Occam's Razor?
     Q-DEFINITION
What is another name for nearsightedness?
     Q-SYNONYM
Should you exercise when you're sick?
     Q-YES-NO-QUESTION
Qargs for additional information:
Who was Betsy Ross?
     QTARGET: (((Q-WHY-FAMOUS-PERSON)))  
     QARGS: (("Betsy Ross"))
How is "Pacific Bell" abbreviated?
     QTARGET: (((Q-ABBREVIATION)))
     QARGS: (("Pacific Bell"))
What are geckos?
     QTARGET: (((Q-DEFINITION)))
     QARGS: (("geckos" "gecko") ("animal"))
These Qtargets are determined during parsing using 276 hand-
written rules.  Still, for approx. 10% of the TREC-8&9
questions there is no easily determinable Qtarget (?What does
the Peugeot company manufacture??; ?What is caliente in
English??).  Strategies for dealing with this are under
investigation.  More details appear in (Hermjakob, 2001).  The
current accuracy of the parser on questions and resulting
Qtargets sentences is shown in Table 2.
5. ANSWER MATCHING
The Matcher performs three independent matches, in order:
? match QA patterns in the parse tree,
? match Qtargets and Qwords in the parse tree,
? match over the answer text using a word window.
Details appear in [9].
Table 1. Improvement in parsing of questions.
Labeled Labeled Tagging Crossing
Precision Recall Precision Recall Accuracy Brackets
Without, Oct 16 90.74% 90.72% 84.62% 83.48% 94.95% 0.6
With, Oct 16 94.19% 94.86% 91.63% 91.91% 98.00% 0.48
With, Dec 9 97.33% 97.13% 95.40% 95.13% 98.64% 0.19
Table 1.  Improvement in parsing of questions.
6. RESULTS
We entered the TREC-9 short form QA track, and received an
overall Mean Reciprocal Rank score of 0.318, which put
Webclopedia in essentially tied second place with two others.
(The best system far outperformed those in second place.)  
In order to determine the relative performance of the modules,
we counted how many correct answers their output contained,
working on our training corpus.  Table 3 shows the evolution
of the system over a sample one-month period, reflecting the
amount of work put into different modules.  The modules QA
pattern, Qtarget, Qword, and Window were all run in parallel
from the same Ranker output.  
The same pattern, albeit with lower scores, occurred in the
TREC test (Table 4).  The QA patterns made only a small
contribution, the Qtarget made by far the largest contribution,
and, interestingly, the word-level window match lay
somewhere in between.
Table 4. TREC-9 test: correct answers
attributable to each module.
IR hits QA pattern Qtarget Window Total
78.1 5.5 26.2 10.4 30.3
We are pleased with the performance of the Qtarget match.  This
shows that CONTEX is able to identify to some degree the
semantic type of the desired answer, and able to pinpoint these
types also in candidate answers.  The fact that it outperforms
the window match indicates the desirability of looking deeper
than the surface level.  As discussed in Section 4, we are
strengthening the parser?s ability to identify Qtargets.  
We are disappointed in the performance of the 500 QA patterns.
Analysis suggests that we had too few patterns, and the ones we
had were too specific.  When patterns matched, they were rather
accurate, both in finding correct answers and more precisely
pinpointing the boundaries of answers.  However, they were
too sensitive to variations in phrasing.  Furthermore, it was
difficult to construct robust and accurate question and answer
phraseology patterns manually, for several reasons.  First,
manual construction relies on the inventiveness of the pattern
builder to foresee variations of phrasing, for both question and
answer.  It is however nearly impossible to think of all
possible variations when building patterns.  
Second, it is not always clear at what level of representation to
formulate the pattern: when should one specify using words?
Parts of speech? Other parse tree nodes? Semantic classes?  The
patterns in Figure 3 include only a few of these alternatives.
Specifying the wrong elements can result in non-optimal
coverage.  Third, the work is simply tedious.  We therefore
decided to try to learn QA patterns automatically.  
7. TOWARD LEARNING QA PATTERNS
AUTOMATICALLY
To learn corresponding question and answer expressions, we
pair up the parse trees of a question and (each one of) its
answer(s).  We then apply a set of matching criteria to identify
potential corresponding portions of the trees.  We then use the
EM algorithm to learn the strengths of correspondence
combinations at various levels of representation.  This work i s
still in progress.  
In order to learn this information we observe the truism that
there are many more answers than questions. This holds for the
two QA corpora we have access to?TREC and an FAQ website
(since discontinued).  We therefore use the familiar version of
the Noisy Channel Model and Bayes? Rule.   For each basic QA
type (Location, Why-Famous, etc.):
Table 2. Question parse tree and Qtarget accuracies.
# Penn # Question Crossing Qtarget Qtarget
Treebank sentences Labeled Labele d Tagging brackets accuracy accuracy
sentences added Precision Recall Accuracy (/ sent) (strict) (lenient)
2000 0 83.47% 82.49% 94.65% 0.34 63.00% 65.50%
3000 0 84.74% 84.16% 94.51% 0.35 65.30% 67.40%
2000 38 91.20% 89.37% 97.63% 0.26 85.90% 87.20%
3000 38 91.52% 90.09% 97.29% 0.26 86.40% 87.80%
2000 975 95.71% 95.45% 98.83% 0.17 96.10% 97.30%
Date Number
Qs
IR
hits
Ranker
hits
QA
pattern
Qtgt
match
Qword
fallback
Window
fallback
Total
2-Jul 52 1.00 0.61 0.12 0.49 0.15 0.19 0.62
8-Jul 38 0.89 0.40 0.28 0.40 0.12 n/a 0.53
13-Jul 52 1.00 0.61 0.04 0.48 0.15 0.22 0.53
3-Aug 55 n/a n/a 0.04 0.32 0.15 0.19 0.41
Table 3. Relative performance of Webclopedia modules on training corpus.
P(A|Q)  =  argmax P(Q|A) . P(A)
P(A)  =   ?all trees (# nodes that may express a true A) 
/  (number of nodes in tree)
P(Q|A)  =  ?all QA tree pairs (number of covarying nodes 
in Q and A trees)
/ (number of nodes in A tree)
As usual, many variations are possible, including how to
determine likelihood of expressing a true answer; whether to
consider all nodes or just certain major syntactic ones (N, NP,
VP, etc.); which information within each node to consider
(syntactic? semantic? lexical?); how to define ?covarying
information??node identity? individual slot value equality?;
what to do about the actual answer node in the A trees; if (and
how) to represent the relationships among A nodes that have
been found to be important; etc.  Figure 4 provides an answer
parse tree that indicates likely Location nodes, determined by
appropriate syntactic class, semantic type, and syntactic role
in the sentence.  
Our initial model focuses on bags of corresponding QA parse
tree nodes, and will help to indicate for a given question what
type of node(s) will contain the answer.  We plan to extend this
model to capture structured configurations of nodes that, when
matched to a question, will help indicate where in the parse tree
of a potential answer sentence the answer actually lies.  Such
bags or structures of nodes correspond, at the surface level, to
important phrases or words.  However, by using CONTEX
output we abstract away from the surface level, and learn to
include whatever syntactic and/or semantic information is best
suited for predicting likely answers.
8. REFERENCES
[1] Bikel, D., R. Schwartz, and R. Weischedel.  1999.  An
Algorithm that Learns What s in a Name.  Machine
Learning Special Issue on NL Learning, 34, 1?3.
[2] Choi, F.Y.Y. 2000. Advances in independent linear text
segmentation. Proceedings of the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-00), 26?33.
[3] Fellbaum, Ch. (ed). 1998. WordNet: An Electronic Lexical
Database. Cambridge: MIT Press.
[4] Gerber, L. 2001.  A QA Typology for Webclopedia. In prep.
[5] Hearst, M.A. 1994. Multi-Paragraph Segmentation of
Expository Text.  Proceedings of the Annual Conference
of the Association for Computational Linguistics (ACL-
94).
[6] Hermjakob, U. 1997. Learning Parse and Translation
Decisions from Examples with Rich Context.  Ph.D.
dissertation, University of Texas at Austin.
file://ftp.cs.utexas.edu/pub/ mooney/papers/hermjakob-
dissertation-97.ps.gz.
[7] Hermjakob, U.  2000. Rapid Parser Development: A
Machine Learning Approach for Korean. Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-2000).
http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz.
[8] Hermjakob, U. 2001. Parsing and Question Classification
for Question Answering. In prep.
[9] Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.
Lin. 2000. Question Answering in Webclopedia.
Proceedings of the TREC-9 Conference.  NIST.
Gaithersburg, MD.
[10] Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,, R.
Girju, R. Goodrum, and V. Rus. 2000. The Structure and
Performance of an Open-Domain Question Answering
System. Proceedings of the Conference of the Association
for Computational Linguistics (ACL-2000), 563?570.
[11] Srihari, R. and W. Li. 2000. A Question Answering System
Supported by Information Extraction. In Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-00), 166?172.
[12] Witten, I.H., A. Moffat, and T.C. Bell. 1994. Managing
Gigabytes: Compressing and Indexing Documents and
Images. New York: Van Nostrand Reinhold.
SU
RF
  L
ux
or
 is
 fa
m
ed
 fo
r i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
.  
CA
T 
S-
SN
T 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(S
UB
J) 
SC
O
RE
 4
 
SU
RF
  i
s  
CA
T 
S-
A
U
X
 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
JP
 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(C
OM
PL
) 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 0
 
SU
RF
  f
or
 it
s V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  .
  
CA
T 
D
-P
ER
IO
D
 
LE
X
  .
  
R
O
LE
S 
(D
UM
M
Y)
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
J 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(P
RE
D)
 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 1
 
SU
RF
  f
or
  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-F
O
R 
CL
A
SS
ES
 (I
-E
P-
FO
R)
 
LE
X
  f
or
  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  a
nd
  
CA
T 
S-
CO
O
RD
-C
O
N
J 
CL
A
SS
 I-
EC
-A
N
D
 
CL
A
SS
ES
 (I
-E
C-
AN
D)
 
LE
X
  a
nd
  
R
O
LE
S 
(C
ON
J) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(C
OO
RD
) 
SC
O
RE
 2
 
SU
RF
  i
ts 
 
CA
T 
S-
PO
SS
-P
RO
N
 
CL
A
SS
 I-
EN
-P
O
SS
-P
RO
N
O
U
N
 
CL
A
SS
ES
 (I
-E
N-
PO
SS
-P
RO
NO
UN
) 
LE
X
  P
O
SS
-P
RO
N
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
RO
PE
R-
O
RG
A
N
IZ
A
TI
O
N
 
CL
A
SS
ES
 (I
-E
N-
PR
OP
ER
-O
RG
AN
IZ
AT
IO
N 
I-E
N-
OR
GA
NI
ZA
TI
ON
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  V
al
le
y 
of
 th
e 
K
in
gs
  
R
O
LE
S 
(M
OD
) 
N
A
M
ED
-E
N
TI
TY
-U
N
IT
-P
 T
RU
E 
SC
O
RE
 4
 
SU
RF
  P
ha
ra
on
ic
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
H
A
RA
O
N
IC
 
CL
A
SS
ES
 (I
-E
N-
PH
AR
AO
NI
C)
 
LE
X
  P
ha
ra
on
ic
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  o
f t
he
 K
in
gs
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  o
f  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-O
F 
CL
A
SS
ES
 (I
-E
P-
OF
) 
LE
X
  o
f  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
in
gs
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  c
om
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-K
A
RN
A
K
 
CL
A
SS
ES
 (I
-E
N-
KA
RN
AK
) 
LE
X
  k
ar
na
k 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  t
em
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
Figure 4. Candidate answer tree showing likely Location answers.
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 4?5,
Vancouver, October 2005.
Classummary:
Introducing Discussion Summarization to Online Classrooms
Liang Zhou, Erin Shaw, Chin-Yew Lin, and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, shaw, hovy}@isi.edu
Abstract
This paper describes a novel summariza-
tion system, Classummary, for interactive
online classroom discussions. This system
is originally designed for Open Source
Software (OSS) development forums.
However, this new application provides
valuable feedback on designing summari-
zation systems and applying them to eve-
ryday use, in addition to the traditional
natural language processing evaluation
methods. In our demonstration at HLT,
new users will be able to direct this sum-
marizer themselves.
1 Introduction
The availability of many chat forums reflects the
formation of globally dispersed virtual communi-
ties, one of which is the very active and growing
movement of Open Source Software (OSS) devel-
opment. Working together in a virtual community
in non-collocated environments, OSS developers
communicate and collaborate using a wide range
of web-based tools including Internet Relay Chat
(IRC), electronic mailing lists, and more.
Another similarly active virtual community is
the distributed education community. Whether
courses are held entirely online or mostly on-
campus, online asynchronous discussion boards
play an increasingly important role, enabling class-
room-like communication and collaboration
amongst students, tutors and instructors. The Uni-
versity of Southern California, like many other
universities, employs a commercial online course
management system (CMS).  In an effort to bridge
research and practice in education, researchers at
ISI replaced the native CMS discussion board with
an open source board that is currently used by se-
lected classes. The board provides a platform for
evaluating new teaching and learning technologies.
Within the discussion board teachers and students
post messages about course-related topics. The
discussions are organized chronologically within
topics and higher-level forums. These ?live? dis-
cussions are now enabling a new opportunity, the
opportunity to apply and evaluate advanced natural
language processing (NLP) technology.
Recently we designed a summarization system
for technical chats and emails on the Linux kernel
(Zhou and Hovy, 2005). It clusters discussions ac-
cording to subtopic structures on the sub-message
level, identifies immediate responding pairs using
machine-learning methods, and generates subtopic-
based mini-summaries for each chat log. Incorpo-
ration of this system into the ISI Discussion Board
framework, called Classummary, benefits both
distance learning and NLP communities. Summa-
ries are created periodically and sent to students
and teachers via their preferred medium (emails,
text messages on mobiles, web, etc). This relieves
users of the burden of reading through a large vol-
ume of messages before participating in a particu-
lar discussion. It also enables users to keep track of
all ongoing discussions without much effort. At the
same time, the discussion summarization system
can be measured beyond the typical NLP evalua-
4
tion methodologies, i.e. measures on content cov-
erage. Teachers and students? willingness and con-
tinuing interest in using the software will be a
concrete acknowledgement and vindication of such
research-based NLP tools. We anticipate a highly
informative survey to be returned by users at the
end of the service.
2  Summarization Framework
In this section, we will give a brief description of
the discussion summarization framework that is
applied to online classroom discussions.
One important component in the original system
(Zhou and Hovy, 2005) is the sub-message clus-
tering. The original chat logs are in-depth technical
discussions that often involve multiple sub-topics,
clustering is used to model this behavior. In Clas-
summary, the discussions are presented in an orga-
nized fashion where users only respond to and
comment on specific topics. Thus, it eliminates the
need for clustering.
 All messages in a discussion are related to the
central topic, but to varying degrees. Some are an-
swers to previously asked questions, some make
suggestions and give advice where they are re-
quested, etc. We can safely assume that for this
type of conversational interactions, the goal of the
participants is to seek help or advice and advance
their current knowledge on various course-related
subjects. This kind of interaction can be modeled
as one problem-initiating message and one or more
corresponding problem-solving messages, formally
defined as Adjacent Pairs (AP). A support vector
machine, pre-trained on lexical and structural fea-
tures for OSS discussions, is used to identify the
most relevant responding messages to the initial
post within a topic.
Having obtained all relevant responses, we
adopt the typical summarization paradigm to ex-
tract informative sentences to produce concise
summaries. This component is modeled after the
BE-based multi-document summarizer (Hovy et
al., 2005). It consists of three steps. First, impor-
tant basic elements (BEs) are identified according
to their likelihood ratio (LR). BEs are automati-
cally created minimal semantic units of the form
head-modifier-relation (for example, ?Libyans |
two | nn?, ?indicted | Libyans | obj?, and ?indicted
| bombing | for?). Next, each sentence is given a
score which is the sum of its BE scores, computed
in the first step, normalized by its length. Lastly,
taking into consideration the interactions among
summary sentences, a MMR (Maximum Marginal
Relevancy) model (Goldstein et al, 1999) is used
to extract sentences from the list of top-ranked
sentences computed from the second step.
3 Accessibility
Classummary is accessible to students and teachers
while classes are in session. At HLT, we will dem-
onstrate an equivalent web-based version. Discus-
sions are displayed on a per-topic basis; and
messages belonging to a specific discussion are
arranged in ascending order according to their
timestamps. While viewing a new message on a
topic, the user can choose to receive a summary of
the discussion so far or an overall summary on the
topic. Upon receiving the summary (for students,
at the end of an academic term), a list of questions
is presented to the user to gather comments on
whether Classummary is useful. We will show the
survey results from the classes (which will have
concluded by then) at the conference.
References
Hovy, E., C.Y. Lin, and L. Zhou. 2005. A BE-based
multi-document summarizer with sentence compres-
sion. To appear in Proceedings of Multilingual Sum-
marization Evaluation (ACL 2005), Ann Arbor, MI.
Goldstein, J., M. Kantrowitz, V. Mittal, and J. Car-
bonell. Summarizing Text Documents: Sentence Se-
lection and Evaluation Metrics. Proceedings of the
22nd International ACM Conference on Research and
Development in Information Retrieval (SIGIR-99),
Berkeley, CA, 121-128.
Zhou, L. and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet re-
lay chats. To appear in Proceedings of Association of
Computational Linguistics (ACL 2005), Ann Arbor,
MI.
5
Automated Text Summarization 
 Chin-Yew LIN 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way, Marina del Rey, CA 90292-6695 
cyl@isi.edu 
 
 
 
Abstract 
After lying dormant for over two decades, 
automated text summarization has experienced a 
tremendous resurgence of interest in the past 
few years. Research is being conducted in China, 
Europe, Japan, and North America, and industry 
has brought to market more than 30 
summarization systems; most recently, a series 
of large-scale text summarization evaluations, 
Document Understanding Conference (DUC) 
and Text Summarization Challenge (TSC) have 
been held yearly in the United States and Japan. 
In this tutorial, we will review the state of the art 
in automatic summarization, and will discuss 
and critically evaluate current approaches to the 
problem. We will first outline the major types of 
summary: indicative vs. informative; abstract vs. 
extract; generic vs. query-oriented; background 
vs. just-the-news; single-document vs. multi-
document; and so on. We will describe the 
typical decomposition of summarization into 
three stages, and explain in detail the major 
approaches to each stage. For topic 
identification, we will outline techniques based 
on stereotypical text structure, cue words, high-
frequency indicator phrases, intratext 
connectivity, and discourse structure centrality. 
For topic fusion, we will outline some ideas that 
have been proposed, including concept 
generalization and semantic association. For 
summary generation, we will describe the 
problems of sentence planning to achieve 
information compaction. 
How good is a summary? Evaluation is a 
difficult issue. We will describe various 
suggested measures and discuss the adequacy of 
current evaluation methods including manual 
evaluation procedures used in DUC, the factoid 
and pyramid method reference summary 
creation procedures and fully automatic 
evaluation method such as ROUGE. The 
recently developed automatic evaluation method 
based on basic element (BE) will also be 
covered. 
Throughout, we will highlight the strengths and 
weaknesses of statistical and symbolic/linguistic 
techniques in implementing efficient 
summarization systems. We will discuss ways in 
which summarization systems can interact with 
and/or complement natural language generation, 
discourse parsing, information extraction, and 
information retrieval systems. 
Finally, we will present a set of open problems 
that we perceive as being crucial for immediate 
progress in automatic summarization. 
Biography 
Chin-Yew Lin is a senior research scientist at 
the Information Sciences Institute of the 
University of Southern California. He was the 
chief architect of SUMMARIST and NeATS. 
He also developed the automatic summarization 
evaluation package ROUGE that have been used 
in the DUC evaluations. He has co-chaired 
several text summarization and question 
answering workshops in ACL, NAACL, 
COLING. 
 
 
274
 Automatic Evaluation of Summaries Using N-gram  
Co-Occurrence Statistics 
Chin-Yew Lin and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{cyl,hovy}@isi.edu 
 
Abstract 
Following the recent adoption by the machine 
translation community of automatic evalua-
tion using the BLEU/NIST scoring process, 
we conduct an in-depth study of a similar idea 
for evaluating summaries. The results show 
that automatic evaluation using unigram co-
occurrences between summary pairs correlates 
surprising well with human evaluations, based 
on various statistical metrics; while direct ap-
plication of the BLEU evaluation procedure 
does not always give good results. 
1 Introduction 
Automated text summarization has drawn a lot of inter-
est in the natural language processing and information 
retrieval communities in the recent years. A series of 
workshops on automatic text summarization (WAS 
2000, 2001, 2002), special topic sessions in ACL, 
COLING, and SIGIR, and government sponsored 
evaluation efforts in the United States (DUC 2002) and 
Japan (Fukusima and Okumura 2001) have advanced 
the technology and produced a couple of experimental 
online systems (Radev et al 2001, McKeown et al 
2002). Despite these efforts, however, there are no 
common, convenient, and repeatable evaluation meth-
ods that can be easily applied to support system devel-
opment and just-in-time comparison among different 
summarization methods. 
The Document Understanding Conference (DUC 2002) 
run by the National Institute of Standards and Technol-
ogy (NIST) sets out to address this problem by provid-
ing annual large scale common evaluations in text 
summarization. However, these evaluations involve 
human judges and hence are subject to variability (Rath 
et al 1961). For example, Lin and Hovy (2002) pointed 
out that 18% of the data contained multiple judgments 
in the DUC 2001 single document evaluation1.  
To further progress in automatic summarization, in this 
paper we conduct an in-depth study of automatic 
evaluation methods based on n-gram co-occurrence in 
the context of DUC. Due to the setup in DUC, the 
evaluations we discussed here are intrinsic evaluations 
(Sp?rck Jones and Galliers 1996). Section 2 gives an 
overview of the evaluation procedure used in DUC. 
Section 3 discusses the IBM BLEU (Papineni et al 
2001) and NIST (2002) n-gram co-occurrence scoring 
procedures and the application of a similar idea in 
evaluating summaries. Section 4 compares n-gram co-
occurrence scoring procedures in terms of their correla-
tion to human results and on the recall and precision of 
statistical significance prediction. Section 5 concludes 
this paper and discusses future directions. 
2 Document Understanding Conference 
The 2002 Document Understanding Conference2 in-
cluded the follow two main tasks: 
? Fully automatic single-document summarization: 
given a document, participants were required to 
create a generic 100-word summary.  The training 
set comprised 30 sets of approximately 10 docu-
ments each, together with their 100-word human 
written summaries.  The test set comprised 30 un-
seen documents. 
? Fully automatic multi-document summarization: 
given a set of documents about a single subject, 
participants were required to create 4 generic sum-
maries of the entire set, containing 50, 100, 200, 
and 400 words respectively.  The document sets 
were of four types: a single natural disaster event; a 
                                                           
1 Multiple judgments occur when more than one performance 
score is given to the same system (or human) and human sum-
mary pairs by the same human judge. 
2 DUC 2001 and DUC 2002 have similar tasks, but summaries 
of 10, 50, 100, and 200 words are requested in the multi-
document task in DUC 2002. 
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 71-78
                                                         Proceedings of HLT-NAACL 2003
 single event; multiple instances of a type of event; 
and information about an individual.  The training 
set comprised 30 sets of approximately 10 docu-
ments, each provided with their 50, 100, 200, and 
400-word human written summaries.  The test set 
comprised 30 unseen sets. 
A total of 11 systems participated in the single-
document summarization task and 12 systems partici-
pated in the multi-document task.   
2.1 Evaluation Materials 
For each document or document set, one human sum-
mary was created as the ?ideal? model summary at each 
specified length.  Two other human summaries were 
also created at each length.  In addition, baseline sum-
maries were created automatically for each length as 
reference points.  For the multi-document summariza-
tion task, one baseline, lead baseline, took the first 50, 
100, 200, and 400 words in the last document in the 
collection.  A second baseline, coverage baseline, took 
the first sentence in the first document, the first sentence 
in the second document and so on until it had a sum-
mary of 50, 100, 200, or 400 words. Only one baseline 
(baseline1) was created for the single document summa-
rization task. 
2.2 Summary Evaluation Environment 
To evaluate system performance NIST assessors who 
created the ?ideal? written summaries did pairwise com-
parisons of their summaries to the system-generated 
summaries, other assessors? summaries, and baseline 
summaries.  They used the Summary Evaluation Envi-
ronment (SEE) 2.0 developed by (Lin 2001) to support 
the process.  Using SEE, the assessors compared the 
system?s text (the peer text) to the ideal (the model 
text).  As shown in Figure 1, each text was decomposed 
into a list of units and displayed in separate windows. 
SEE 2.0 provides interfaces for assessors to judge both 
the content and the quality of summaries.  To measure 
content, assessors step through each model unit, mark 
all system units sharing content with the current model 
unit (green/dark gray highlight in the model summary 
window), and specify that the marked system units ex-
press all, most, some, or hardly any of the content of the 
Figure 1. SEE in an evaluation session. 
 current model unit.  To measure quality, assessors rate 
grammaticality3, cohesion4, and coherence5 at five dif-
ferent levels: all, most, some, hardly any, or none6.  For 
example, as shown in Figure 1, an assessor marked sys-
tem units 1.1 and 10.4 (red/dark underlines in the left 
pane) as sharing some content with the current model 
unit 2.2 (highlighted green/dark gray in the right). 
2.3 Evaluation Metrics 
Recall at different compression ratios has been used in 
summarization research to measure how well an auto-
matic system retains important content of original 
documents (Mani et al 1998). However, the simple sen-
tence recall measure cannot differentiate system per-
formance appropriately, as is pointed out by Donaway 
et al (2000). Therefore, instead of pure sentence recall 
score, we use coverage score C. We define it as fol-
lows7: 
)1(
summary model in the MUs ofnumber  Total
  marked) MUs of(Number EC ?=
E, the ratio of completeness, ranges from 1 to 0: 1 for 
all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 
for none.  If we ignore E (set it to 1), we obtain simple 
sentence recall score.  We use average coverage scores 
derived from human judgments as the references to 
evaluate various automatic scoring methods in the fol-
lowing sections.  
3 BLEU and N-gram Co-Occurrence 
To automatically evaluate machine translations the ma-
chine translation community recently adopted an n-gram 
co-occurrence scoring procedure BLEU (Papineni et al 
2001). The NIST (NIST 2002) scoring metric is based 
on BLEU. The main idea of BLEU is to measure the 
translation closeness between a candidate translation 
and a set of reference translations with a numerical met-
ric. To achieve this goal, they used a weighted average 
of variable length n-gram matches between system 
translations and a set of human reference translations 
and showed that a weighted average metric, i.e. BLEU, 
correlating highly with human assessments.  
Similarly, following the BLEU idea, we assume that the 
closer an automatic summary to a professional human 
                                                           
3 Does the summary observe English grammatical rules inde-
pendent of its content? 
4 Do sentences in the summary fit in with their surrounding 
sentences?  
5 Is the content of the summary expressed and organized in an 
effective way? 
6 These category labels are changed to numerical values of 
100%, 80%, 60%, 40%, 20%, and 0% in DUC 2002. 
7 DUC 2002 uses a length adjusted version of coverage metric 
C?, where C? = ?*C + (1-?)*B. B is the brevity and ? is a pa-
rameter reflecting relative importance (DUC 2002). 
summary, the better it is. The question is: ?Can we ap-
ply BLEU directly without any modifications to evalu-
ate summaries as well??. We first ran IBM?s BLEU 
evaluation script unmodified over the DUC 2001 model 
and peer summary set. The resulting Spearman rank 
order correlation coefficient (?) between BLEU and the 
human assessment for the single document task is 0.66 
using one reference summary and 0.82 using three ref-
erence summaries; while Spearman ? for the multi-
document task is 0.67 using one reference and 0.70 us-
ing three. These numbers indicate that they positively 
correlate at ? = 0.018. Therefore, BLEU seems a prom-
ising automatic scoring metric for summary evaluation. 
According to Papineni et al (2001), BLEU is essentially 
a precision metric. It measures how well a machine 
translation overlaps with multiple human translations 
using n-gram co-occurrence statistics. N-gram precision 
in BLEU is computed as follows: 
? ?
? ?
? ??
? ??
?
?
=
}{
}{
)(
)(
CandidatesC Cgramn
CandidatesC Cgramn
clip
n gramnCount
gramnCount
p   (2) 
Where Countclip(n-gram) is the maximum number of n-
grams co-occurring in a candidate translation and a ref-
erence translation, and Count(n-gram) is the number of 
n-grams in the candidate translation. To prevent very 
short translations that try to maximize their precision 
scores, BLEU adds a brevity penalty, BP, to the for-
mula: 
)3(
1
|)|/||1( ??
?
??
?
?
>
=
? rcife
rcif
BP cr  
Where |c| is the length of the candidate translation and 
|r| is the length of the reference translation. The BLEU 
formula is then written as follows: 
)4(logexp
1
??
???
?
?= ?
=
N
n
nn pwBPBLEU  
N is set at 4 and wn, the weighting factor, is set at 1/N. 
For summaries by analogy, we can express equation (1) 
in terms of n-gram matches following equation (2): 
)5(
)(
)(
}{
}{ ? ?
? ?
? ??
? ??
?
?
=
UnitsModelC Cgramn
UnitsModelC Cgramn
match
n gramnCount
gramnCount
C  
Where Countmatch(n-gram) is the maximum number of 
n-grams co-occurring in a peer summary and a model 
unit and Count(n-gram) is the number of n-grams in the 
model unit. Notice that the average n-gram coverage 
score, Cn, as shown in equation 5 is a recall metric 
                                                           
8 The number of instances is 14 (11 systems, 2 humans, and 1 
baseline) for the single document task and is 16 (12 systems, 2 
humans, and 2 baselines) for the multi-document task. 
 instead of a precision one as pn. Since the denominator 
of equation 5 is the total sum of the number of n-grams 
occurring at the model summary side instead of the peer 
side and only one model summary is used for each 
evaluation; while there could be multiple references 
used in BLEU and Countclip(n-gram) could come from 
matching different reference translations. Furthermore, 
instead of a brevity penalty that punishes overly short 
translations, a brevity bonus, BB, should be awarded to 
shorter summaries that contain equivalent content. In 
fact, a length adjusted average coverage score was used 
as an alternative performance metric in DUC 2002. 
However, we set the brevity bonus (or penalty) to 1 for 
all our experiments in this paper. In summary, the n-
gram co-occurrence statistics we use in the following 
sections are based on the following formula: 
)6(logexp),( ???
?
???
?
?= ?
=
j
in
nn CwBBjiNgram  
Where j ? i, i and j range from 1 to 4, and wn is 1/(j-
i+1). Ngram(1, 4) is a weighted variable length n-gram 
match score similar to the IBM BLEU score; while 
Ngram(k, k), i.e. i = j = k, is simply the average k-gram 
coverage score Ck.  
With these formulas, we describe how to evaluate them 
in the next section. 
4 Evaluations of N-gram Co-Occurrence 
Metrics 
In order to evaluate the effectiveness of automatic 
evaluation metrics, we propose two criteria: 
1. Automatic evaluations should correlate highly, 
positively, and consistently with human assess-
ments. 
2. The statistical significance of automatic evaluations 
should be a good predictor of the statistical signifi-
cance of human assessments with high reliability. 
The first criterion ensures whenever a human recognizes 
a good summary/translation/system, an automatic 
evaluation will do the same with high probability. This 
enables us to use an automatic evaluation procedure in 
place of human assessments to compare system per-
formance, as in the NIST MT evaluations (NIST 2002). 
The second criterion is critical in interpreting the sig-
nificance of automatic evaluation results. For example, 
if an automatic evaluation shows there is a significant 
difference between run A and run B at ? = 0.05 using 
the z-test (t-test or bootstrap resampling), how does this 
translate to ?real? significance, i.e. the statistical signifi-
cance in a human assessment of run A and run B? Ide-
ally, we would like there to be a positive correlation 
between them. If this can be asserted with strong reli-
ability (high recall and precision), then we can use the 
automatic evaluation to assist system development and 
to be reasonably sure that we have made progress. 
4.1 Correlation with Human Assessments 
As stated in Section 3, direct application of BLEU on 
the DUC 2001 data showed promising results. However, 
BLEU is a precision-based metric while the human 
evaluation protocol in DUC is essentially recall-based. 
We therefore prefer the metric given by equation 6 and 
use it in all our experiments. Using DUC 2001 data, we 
compute average Ngram(1,4) scores for each  peer sys-
tem at different summary sizes and rank systems ac-
cording to their scores. We then compare the 
Ngram(1,4) ranking with the human ranking. Figure 2 
shows the result of DUC 2001 multi-document data. 
Stopwords are ignored during the computation of 
Ngram(1,4) scores and words are stemmed using a Por-
ter stemmer (Porter 1980). The x-axis is the human 
ranking and the y-axis gives the corresponding 
Ngram(1,4) rankings for summaries of difference sizes. 
The straight line marked by AvgC is the ranking given 
by human assessment. For example, a system at (5,8) 
Table 1. Spearman rank order correlation coeffi-
cients of different DUC 2001 data between 
Ngram(1, 4)n rankings and human rankings includ-
ing (S) and excluding (SX) stopwords. SD-100 is 
for single document summaries of 100 words and 
MD-50, 100, 200, and 400 are for multi-document 
summaries of 50, 100, 200, and 400 words. MD-All 
averages results from summaries of all sizes.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Human Ranking
N
g
ra
m
(1
, 4
)n
 R
an
ki
n
g
AvgC
Ngram(1, 4)50
Ngram(1, 4)100
Ngram(1, 4)200
Ngram(1, 4)400
Ngram(1, 4)all
Figure 2. Scatter plot of Ngram(1,4)n score rank-
ings versus human ranking for the multi-
document task data from DUC 2001. The same 
system is at each vertical line with ranking given 
by different Ngram(1,4)n scores. The straight line 
(AvgC) is the human ranking and n marks sum-
maries of different sizes. Ngram(1,4)all combines 
results from all sizes. 
SD-100 MD-All MD-50 MD-100 MD-200 MD-400
SX 0.604 0.875 0.546 0.575 0.775 0.861
S 0.615 0.832 0.646 0.529 0.814 0.843
 means that human ranks its performance at the 5th rank 
while Ngram(1,4)400 ranks it at the 8th. If an automatic 
ranking fully matches the human ranking, its plot will 
coincide with the heavy diagonal. A line with less de-
viation from the heavy diagonal line indicates better 
correlation with the human assessment. 
To quantify the correlation, we compute the Spearman 
rank order correlation coefficient (?) for each N-
gram(1,4)n run at different summary sizes (n). We also 
test the effect of inclusion or exclusion of stopwords. 
The results are summarized in Table 1. 
Although these results are statistically significant (? = 
0.025) and are comparable to IBM BLEU?s correlation 
figures shown in Section 3, they are not consistent 
across summary sizes and tasks. For example, the corre-
lations of the single document task are at the 60% level; 
while they range from 50% to 80% for the multi-
document task. The inclusion or exclusion of stopwords 
also shows mixed results. In order to meet the require-
ment of the first criterion stated in Section 3, we need 
better results. 
The Ngram(1,4)n score is a weighted average of variable 
length n-gram matches. By taking a log sum of the n-
gram matches, the Ngram(1,4)n favors match of longer 
n-grams. For example, if ?United States of America? 
occurs in a reference summary, while one peer sum-
mary, A, uses ?United States? and another summary, B, 
uses the full phrase ?United States of America?, sum-
mary B gets more contribution to its overall score sim-
ply due to the longer version of the name. However, 
intuitively one should prefer a short version of the name 
in summarization. Therefore, we need to change the 
weighting scheme to not penalize or even reward shorter 
equivalents. We conduct experiments to understand the 
effect of individual n-gram co-occurrence scores in ap-
proximating human assessments. Tables 2 and 3 show 
the results of these runs without and with stopwords 
respectively. 
For each set of DUC 2001 data, single document 100-
word summarization task, multi-document 50, 100, 200, 
and 400 -word summarization tasks, we compute 4 dif-
ferent correlation statistics: Spearman rank order corre-
lation coefficient (Spearman ?),  linear regression t-test 
(LRt, 11 degree of freedom for single document task and 
13 degree of freedom for multi-document task), Pearson 
product moment coefficient of correlation (Pearson ?), 
and coefficient of determination (CD) for each 
Ngram(i,j) evaluation metric. Among them Spearman ? 
is a nonparametric test, a higher number indicates 
higher correlation; while the other three tests are para-
metric tests. Higher LRt, Pearson ?, and CD also sug-
gests higher linear correlation.  
Analyzing all runs according to Tables 2 and 3, we 
make the following observations: 
(1) Simple unigram, Ngram(1,1), and bi-gram, 
Ngram(2,2), co-occurrence statistics consistently 
outperform (0.99 ? Spearman ? ? 0.75) the 
weighted average of n-gram of variable length 
Ngram(1, 4) (0.88 ? Spearman ? ? 0.55) in single 
and multiple document tasks when stopwords are 
ignored. Importantly, unigram performs especially 
well with Spearman ? ranging from 0.88 to 0.99 
that is better than the best case in which weighted 
average of variable length n-gram matches is used 
and is consistent across different data sets. 
(2) The performance of weighted average n-gram 
scores is in the range between bi-gram and tri-gram 
co-occurrence scores. This might suggest some 
summaries are over-penalized by the weighted av-
erage metric due to the lack of longer n-gram 
matches. For example, given a model string 
?United States, Japan, and Taiwan?, a candidate 
Table 3. Various Ngram(i, j) rank/score correlations 
for 4 different statistics (with stopwords). 
Table 2. Various Ngram(i,j) rank/score correlations 
for 4 different statistics (without stopwords): Spear-
man rank order coefficient correlation (Spearman ?), 
linear regression t-test (LRt), Pearson product mo-
ment coefficient of correlation (Pearson ?), and co-
efficient of determination (CD).  
Ngram (1,4) Ngram (1,1) Ngram (2,2) Ngram (3,3) Ngram (4,4)
Single Doc Spearman ? 0.604 0.989 0.868 0.527 0.505
100 LRt 1.025 7.130 2.444 0.704 0.053
Pearson ? 0.295 0.907 0.593 0.208 0.016
CD 0.087 0.822 0.352 0.043 0.000
Multi-Doc Spearman ? 0.875 0.993 0.950 0.782 0.736
All LRt 3.910 13.230 5.830 3.356 2.480
Pearson ? 0.735 0.965 0.851 0.681 0.567
CD 0.540 0.931 0.723 0.464 0.321
Multi-Doc Spearman ? 0.546 0.879 0.746 0.496 0.343
50 LRt 2.142 5.681 3.350 2.846 2.664
Pearson ? 0.511 0.844 0.681 0.620 0.594
CD 0.261 0.713 0.463 0.384 0.353
Multi-Doc Spearman ? 0.575 0.896 0.761 0.543 0.468
100 LRt 2.369 7.873 3.641 1.828 1.385
Pearson ? 0.549 0.909 0.711 0.452 0.359
CD 0.301 0.827 0.505 0.204 0.129
Multi-Doc Spearman ? 0.775 0.979 0.904 0.782 0.754
200 LRt 3.243 15.648 4.929 2.772 2.126
Pearson ? 0.669 0.974 0.807 0.609 0.508
CD 0.447 0.950 0.651 0.371 0.258
Multi-Doc Spearman ? 0.861 0.982 0.961 0.854 0.661
400 LRt 4.390 10.569 6.409 3.907 2.755
Pearson ? 0.773 0.946 0.872 0.735 0.607
CD 0.597 0.896 0.760 0.540 0.369
Ngram (1,4) Ngram (1,1) Ngram (2,2) Ngram (3,3) Ngram (4,4)
Single Doc Spearman ? 0.615 0.951 0.863 0.615 0.533
100 LRt 1.076 4.873 2.228 0.942 0.246
Pearson ? 0.309 0.827 0.558 0.273 0.074
CD 0.095 0.683 0.311 0.075 0.005
Multi-Doc Spearman ? 0.832 0.918 0.936 0.832 0.732
All LRt 3.752 6.489 5.451 3.745 2.640
Pearson ? 0.721 0.874 0.834 0.720 0.591
CD 0.520 0.764 0.696 0.519 0.349
Multi-Doc Spearman ? 0.646 0.586 0.650 0.589 0.600
50 LRt 2.611 2.527 2.805 2.314 1.691
Pearson ? 0.587 0.574 0.614 0.540 0.425
CD 0.344 0.329 0.377 0.292 0.180
Multi-Doc Spearman ? 0.529 0.636 0.625 0.571 0.468
100 LRt 2.015 3.338 2.890 2.039 1.310
Pearson ? 0.488 0.679 0.625 0.492 0.342
CD 0.238 0.462 0.391 0.242 0.117
Multi-Doc Spearman ? 0.814 0.964 0.879 0.814 0.746
200 LRt 3.204 10.134 4.926 3.328 2.173
Pearson ? 0.664 0.942 0.807 0.678 0.516
CD 0.441 0.888 0.651 0.460 0.266
Multi-Doc Spearman ? 0.843 0.914 0.946 0.857 0.721
400 LRt 4.344 5.358 6.344 4.328 3.066
Pearson ? 0.769 0.830 0.869 0.768 0.648
CD 0.592 0.688 0.756 0.590 0.420
 string ?United States, Taiwan, and Japan? has a 
unigram score of 1, bi-gram score of 0.5, and tri-
gram and 4-gram scores of 0 when the stopword 
?and? is ignored. The weighted average n-gram 
score for the candidate string is 0.  
(3) Excluding stopwords in computing n-gram co-
occurrence statistics generally achieves better cor-
relation than including stopwords. 
4.2 Statistical Significance of N-gram Co-
Occurrence Scores versus Human As-
sessments 
We have shown that simple unigram, Ngram(1,1), or bi-
gram, Ngram(2,2), co-occurrence statistics based on 
equation 6 outperform the weighted average of n-gram 
matches, Ngram(1,4), in the previous section. To exam-
ine how well the statistical significance in the automatic 
Ngram(i,j) metrics translates to real significance when 
human assessments are involved, we set up the follow-
ing test procedures: 
(1) Compute pairwise statistical significance test such 
as z-test or t-test for a system pair (X,Y) at certain ? 
level, for example ? = 0.05, using automatic met-
rics and human assigned scores. 
(2) Count the number of cases a z-test indicates there is 
a significant difference between X and Y based on 
the automatic metric. Call this number NAs. 
(3) Count the number of cases a z-test indicates there is 
a significant difference between X and Y based on 
the human assessment. Call this number NHs. 
(4) Count the cases when an automatic metric predicts 
a significant difference and the human assessment 
also does. Call this Nhit. For example, if a z-test in-
dicates system X is significantly different from Y 
with ? = 0.05 based on the automatic metric scores 
and the corresponding z-test also suggests the same 
based on the human agreement, then we have a hit. 
(5) Compute the recall and precision using the follow-
ing formulas: 
recall = 
 Hs
hit
N
N
 
precision = 
 As
hit
N
N
 
A good automatic metric should have high recall and 
precision. This implies that if a statistical test indicates a 
significant difference between two runs using the auto-
matic metric then very probably there is also a signifi-
cant difference in the manual evaluation. This would be 
very useful during the system development cycle to 
gauge if an improvement is really significant or not. 
Figure 3 shows the recall and precision curves for the 
DUC 2001 single document task at different ? levels 
and Figure 4 is for the multi-document task with differ-
ent summary sizes. Both of them exclude stopwords. 
We use z-test in all the significance tests with ? level at 
0.10, 0.05, 0.25, 0.01, and 0.005. 
From Figures 3 and 4, we can see Ngram(1,1) and 
Ngram(2,2) reside on the upper right corner of the recall 
and precision graphs. Ngram(1,1) has the best overall 
behavior. These graphs confirm Ngram(1,1) (simple 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Significance Predication Recall and Precision Curve
Ngram(1,4)
Ngram(1,1)
Ngram(2,2)
Ngram(3,3)
Ngram(4,4)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Significance Predication Recall and Precision Curve
Ngram(1,4)
Ngram(1,1)
Ngram(2,2)
Ngram(3,3)
Ngram(4,4)
Figure 3. Recall and precision curves of N-
gram co-occurrence statistics versus human 
assessment for DUC 2001 single document 
task. The 5 points on each curve represent val-
ues for the 5 ? levels. 
Figure 4. Recall and precision curves of N-gram 
co-occurrence statistics versus human assessment 
for DUC 2001 multi-document task. Dark (black) 
solid lines are for average of all summary sizes, 
light (red) solid lines are for 50-word summaries, 
dashed (green) lines are for 100-word summaries, 
dash-dot lines (blue) are for 200-word summaries, 
and dotted (magenta) lines are for 400-word 
summaries.
 unigram) is a good automatic scoring metric with good 
statistical significance prediction power. 
5 Conclusions 
In this paper, we gave a brief introduction of the manual 
summary evaluation protocol used in the Document 
Understanding Conference. We then discussed the IBM 
BLEU MT evaluation metric, its application to sum-
mary evaluation, and the difference between precision-
based BLEU translation evaluation and recall-based 
DUC summary evaluation. The discrepancy led us to 
examine the effectiveness of individual n-gram co-
occurrence statistics as a substitute for expensive and 
error-prone manual evaluation of summaries. To evalu-
ate the performance of automatic scoring metrics, we 
proposed two test criteria. One was to make sure system 
rankings produced by automatic scoring metrics were 
similar to human rankings. This was quantified by 
Spearman?s rank order correlation coefficient and three 
other parametric correlation coefficients. Another was 
to compare the statistical significance test results be-
tween automatic scoring metrics and human assess-
ments. We used recall and precision of the agreement 
between the test statistics results to identify good auto-
matic scoring metrics. 
According to our experiments, we found that unigram 
co-occurrence statistics is a good automatic scoring 
metric. It consistently correlated highly with human 
assessments and had high recall and precision in signifi-
cance test with manual evaluation results. In contrast, 
the weighted average of variable length n-gram matches 
derived from IBM BLEU did not always give good cor-
relation and high recall and precision. We surmise that a 
reason for the difference between summarization and 
machine translation might be that extraction-based 
summaries do not really suffer from grammar problems, 
while translations do. Longer n-grams tend to score for 
grammaticality rather than content. 
It is encouraging to know that the simple unigram co-
occurrence metric works in the DUC 2001 setup. The 
reason for this might be that most of the systems par-
ticipating in DUC generate summaries by sentence ex-
traction. We plan to run similar experiments on DUC 
2002 data to see if unigram does as well. If it does, we 
will make available our code available via a website to 
the summarization community. 
Although this study shows that unigram co-occurrence 
statistics exhibit some good properties in summary 
evaluation, it still does not correlate to human assess-
ment 100% of the time. There is more to be desired in 
the recall and precision of significance test agreement 
with manual evaluation. We are starting to explore vari-
ous metrics suggested in Donaway et al (2000). For 
example, weight n-gram matches differently according 
to their information content measured by tf, tfidf, or 
SVD. In fact, NIST MT automatic scoring metric (NIST 
2002) already integrates such modifications. 
One future direction includes using an automatic ques-
tion answer test as demonstrated in the pilot study in 
SUMMAC (Mani et al 1998). In that study, an auto-
matic scoring script developed by Chris Buckley 
showed high correlation with human evaluations, al-
though the experiment was only tested on a small set of 
3 topics. 
According to Over (2003), NIST spent about 3,000 man 
hours each in DUC 2001 and 2002 for topic and docu-
ment selection, summary creation, and manual evalua-
tion. Therefore, it would be wise to use these valuable 
resources, i.e. manual summaries and evaluation results, 
not only in the formal evaluation every year but also in 
developing systems and designing automatic evaluation 
metrics. We would like to propose an annual automatic 
evaluation track in DUC that encourages participants to 
invent new automated evaluation metrics. Each year the 
human evaluation results can be used to evaluate the 
effectiveness of the various automatic evaluation met-
rics. The best automatic metric will be posted at the 
DUC website and used as an alternative in-house and 
repeatable evaluation mechanism during the next year. 
In this way the evaluation technologies can advance at 
the same pace as the summarization technologies im-
prove. 
References 
Donaway, R.L., Drummey, K.W., and Mather, L.A. 
2000. A Comparison of Rankings Produced by 
Summarization Evaluation Measures. In Proceeding 
of the Workshop on Automatic Summarization, post-
conference workshop of ANLP-NAACL-2000, pp. 
69-78, Seattle, WA, 2000. 
DUC. 2002. The Document Understanding Conference.  
http://duc.nist.gov.  
Fukusima, T. and Okumura, M. 2001. Text Summariza-
tion Challenge: Text Summarization Evaluation at 
NTCIR Workshop2. In Proceedings of the Second 
NTCIR Workshop on Research in Chinese & Japa-
nese Text Retrieval and Text Summarization, NII, 
Tokyo, Japan, 2001. 
Lin, C.-Y. 2001. Summary Evaluation Environment.  
http://www.isi.edu/~cyl/SEE. 
Lin, C.-Y. and E. Hovy. 2002. Manual and Automatic 
Evaluations of Summaries. In Proceedings of the 
Workshop on Automatic Summarization, post-
conference workshop of ACL-2002, pp. 45-51, Phila-
delphia, PA, 2002. 
McKeown, K., R. Barzilay, D. Evans, V. Hatzivassi-
loglou, J. L. Klavans, A. Nenkova, C. Sable, B. 
Schiffman, S. Sigelman. Tracking and Summarizing 
 News on a Daily Basis with Columbia?s Newsblaster. 
In Proceedings of Human Language Technology 
Conference 2002 (HLT 2002). San Diego, CA, 2002. 
Mani, I., D. House, G. Klein, L. Hirschman, L. Obrst, T. 
Firmin, M. Chrzanowski, and B. Sundheim. 1998. 
The TIPSTER SUMMAC Text Summarization 
Evaluation: Final Report.  MITRE Corp. Tech. Re-
port. 
NIST. 2002. Automatic Evaluation of Machine Transla-
tion Quality using N-gram Co-Occurrence Statistics. 
Over, P. 2003. Personal Communication.  
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. 
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report RC22176 
(W0109-022). 
Porter, M. F. 1980. An Algorithm for Suffix Stripping. 
Program, 14, pp. 130-137. 
Radev, D. R., S. Blair-Goldensohn, Z. Zhang, and R. 
S. Raghavan. Newsinessence: A System for Domain-
Independent, Real-Time News Clustering and Multi-
Document Summarization. In Proceedings of human 
Language Technology Conference (HLT 2001), San 
Diego, CA, 2001. 
Sp?rck Jones, K. and J. R. Galliers. 1996. Evaluating 
Natural Language Processing Systems: An Analysis 
and Review. New York: Springer. 
Rath, G.J., Resnick, A., and Savage, T.R. 1961. The 
Formation of Abstracts by the Selection of Sen-
tences. American Documentation, 12(2), pp. 139-
143. Reprinted in Mani, I., and Maybury, M., eds, 
Advances in Automatic Text Summarization, MIT 
Press, pp. 287-292. 
WAS. 2000. Workshop on Automatic Summarization, 
post-conference workshop of ANLP-NAACL-2000, 
Seattle, WA, 2000. 
WAS. 2001. Workshop on Automatic Summarization, 
pre-conference workshop of NAACL-2001, Pitts-
burgh, PA, 2001. 
WAS. 2002. Workshop on Automatic Summarization, 
post-conference workshop of ACL-2002, Philadel-
phia, PA, 2002. 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 447?454,
New York, June 2006. c?2006 Association for Computational Linguistics
ParaEval: Using Paraphrases to Evaluate Summaries Automatically
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, cyl, dragos, hovy} @isi.edu
Abstract
ParaEval is an automated evaluation
method for comparing reference and peer
summaries. It facilitates a tiered-
comparison strategy where recall-oriented
global optimal and local greedy searches
for paraphrase matching are enabled in
the top tiers. We utilize a domain-
independent paraphrase table extracted
from a large bilingual parallel corpus us-
ing methods from Machine Translation
(MT). We show that the quality of ParaE-
val?s evaluations, measured by correlating
with human judgments, closely resembles
that of ROUGE?s.
1 Introduction
Content coverage is commonly measured in sum-
mary comparison to assess how much information
from the reference summary is included in a peer
summary. Both manual and automatic methodolo-
gies have been used. Naturally, there is a great
amount of confidence in manual evaluation since
humans can infer, paraphrase, and use world
knowledge to relate text units with similar mean-
ings, but which are worded differently. Human
efforts are preferred if the evaluation task is easily
conducted and managed, and does not need to be
performed repeatedly. However, when resources
are limited, automated evaluation methods become
more desirable.
For years, the summarization community has
been actively seeking an automatic evaluation
methodology that can be readily applied to various
summarization tasks. ROUGE (Lin and Hovy,
2003) has gained popularity due to its simplicity
and high correlation with human judgments. Even
though validated by high correlations with human
judgments gathered from previous Document Un-
derstanding Conference (DUC) experiments, cur-
rent automatic procedures (Lin and Hovy, 2003;
Hovy et al, 2005) only employ lexical n-gram
matching. The lack of support for word or phrase
matching that stretches beyond strict lexical
matches has limited the expressiveness and utility
of these methods. We need a mechanism that sup-
plements literal matching?i.e. paraphrase and
synonym?and approximates semantic closeness.
In this paper we present ParaEval, an automatic
summarization evaluation method, which facili-
tates paraphrase matching in an overall three-level
comparison strategy. At the top level, favoring
higher coverage in reference, we perform an opti-
mal search via dynamic programming to find
multi-word to multi-word paraphrase matches be-
tween phrases in the reference summary (usually
human-written) and those in the peer summary
(system-generated). The non-matching fragments
from the previous level are then searched by a
greedy algorithm to find single-word para-
phrase/synonym matches. At the third and the low-
est level, we perform literal lexical unigram
matching on the remaining texts. This tiered design
for summary comparison guarantees at least a
ROUGE-1 level of summary content matching if
no paraphrases are found.
The first two levels employ a paraphrase table.
Since manually created multi-word paraphrases-
?phrases determined by humans to be paraphrases
of one another?are not available in sufficient
quantities, we automatically build a paraphrase
447
table using methods from the Machine Translation
(MT) field. The assumption made in creating this
table is that if two English phrases are translated
into the same foreign phrase with high probability
(shown in the alignment results from a statistically
trained alignment algorithm), then the two English
phrases are paraphrases of each other.
This paper is organized in the following way:
Section 2 introduces previous work in summariza-
tion evaluation; Section 3 describes the motivation
behind this work; paraphrase acquisition is dis-
cussed in Section 4; Section 5 explains in detail
our summary comparison mechanism; Section 6
validates ParaEval with human summary judg-
ments; and we conclude and discuss future work in
Section 7.
2 Previous Work
There has been considerable work in both manual
and automatic summarization evaluations. Three
most noticeable efforts in manual evaluation are
SEE (Lin and Hovy, 2001), Factoid (Van Halteren
and Teufel, 2003), and the Pyramid method
(Nenkova and Passonneau, 2004).
SEE provides a user-friendly environment in
which human assessors evaluate the quality of
system-produced peer summary by comparing it to
a reference summary. Summaries are represented
by a list of summary units (sentences, clauses,
etc.). Assessors can assign full or partial content
coverage score to peer summary units in compari-
son to the corresponding reference summary units.
Grammaticality can also be graded unit-wise.
The goal of the Factoid work is to compare the
information content of different summaries of the
same text and determine the minimum number of
summaries, which was shown through experimen-
tation to be 20-30, needed to achieve stable con-
sensus among 50 human-written summaries.
The Pyramid method uses identified consen-
sus?a pyramid of phrases created by annota-
tors?from multiple reference summaries as the
gold-standard reference summary. Summary com-
parisons are performed on Summarization Content
Units (SCUs) that are approximately of clause
length.
To facilitate fast summarization system design-
evaluation cycles, ROUGE was created (Lin and
Hovy, 2003). It is an automatic evaluation package
that measures a number of n-gram co-occurrence
statistics between peer and reference summary
pairs. ROUGE was inspired by BLEU (Papineni et
al., 2001) which was adopted by the machine
translation (MT) community for automatic MT
evaluation. A problem with ROUGE is that the
summary units used in automatic comparison are
of fixed length. A more desirable design is to have
summary units of variable size. This idea was im-
plemented in the Basic Elements (BE) framework
(Hovy et al, 2005) which has not been completed
due to its lack of support for paraphrase matching.
Both ROUGE and BE have been shown to corre-
late well with past DUC human summary judg-
ments, despite incorporating only lexical matching
on summary units (Lin and Hovy, 2003; Hovy et
al., 2005).
3 Motivation
3.1 Paraphrase Matching
An important difference that separates current
manual evaluation methods from their automatic
counterparts is that semantic matching of content
units is performed by human summary assessors.
An essential part of the semantic matching in-
volves paraphrase matching?determining whether
phrases worded differently carry the same semantic
information. This paraphrase matching process is
observed in the Pyramid annotation procedure
shown in (Nenkova and Passonneau, 2004) over
three summary sets (10 summaries each). In the
example shown in Figure 1 (reproduced from
Pyramid results), each of the 10 phrases (numbered
1 to 10) extracted from summary sentences carries
the same semantic content as the overall summary
content unit labeled SCU1 does. Each extracted
phrase is identified as a summary content unit
(SCU). In our work in building an automatic
evaluation procedure that enables paraphrase
SCU1: the crime in question was the Lockerbie {Scotland} bombing
1 [for the Lockerbie bombing]
2 [for blowing up] [over Lockerbie, Scotland]
3 [of bombing] [over Lockerbie, Scotland]
4 [was blown up over Lockerbie, Scotland, ]
5 [the bombing of Pan Am Flight 103]
6 [bombing over Lockerbie, Scotland, ]
7 [for Lockerbie bombing]
8 [bombing of Pan Am flight 103 over Lockerbie. ]
9 [linked to the Lockerbie bombing]
10 [in the Lockerbie bombing case. ]
Figure 1. Paraphrases created by Pyramid annotation.
448
matching, we aim to automatically identify these
10 phrases as paraphrases of one another.
3.2 Synonymy Relations
Synonym matching and paraphrase matching are
often mentioned in the same context in discussions
of extending current automated summarization
evaluation methods to incorporate the matching of
semantic units. While evaluating automatically
extracted paraphrases via WordNet (Miller et al,
1990), Barzilay and McKeown (2001) quantita-
tively validated that synonymy is not the only
source of paraphrasing. We envisage that this
claim is also valid for summary comparisons.
From an in-depth analysis on the manually cre-
ated SCUs of the DUC2003 summary set D30042
(Nenkova and Passonneau, 2004), we find that
54.48% of 1746 cases where a non-stop word from
one SCU did not match with its supposedly hu-
man-aligned pairing SCUs are in need of some
level of paraphrase matching support. For example,
in the first two extracted SCUs (labeled as 1 and 2)
in Figure 1??for the Lockerbie bombing? and ?for
blowing up ? over Lockerbie, Scotland??no
non-stop word other than the word ?Lockerbie?
occurs in both phrases. But these two phrases were
judged to carry the same semantic meaning be-
cause human annotators think the word ?bombing?
and the phrase ?blowing up? refer to the same ac-
tion, namely the one associated with ?explosion.?
However, ?bombing? and ?blowing up? cannot be
matched through synonymy relations by using
WordNet, since one is a noun and the other is a
verb phrase (if tagged within context). Even when
the search is extended to finding synonyms and
hypernyms for their categorical variants and/or
using other parts of speech (verb for ?bombing?
and noun phrase for ?blowing up?), a match still
cannot be found.
To include paraphrase matching in summary
evaluation, a collection of less-strict paraphrases
must be created and a matching strategy needs to
be investigated.
4 Paraphrase Acquisition
Paraphrases are alternative verbalizations for con-
veying the same information and are required by
many Natural Language Processing (NLP) appli-
cations. In particular, summary creation and
evaluation methods need to recognize paraphrases
and their semantic equivalence. Unfortunately, we
have yet to incorporate into the evaluation frame-
work previous findings in paraphrase identification
and extraction (Barzilay and McKeown, 2001;
Pang et al, 2003; Bannard and Callison-Burch,
2005).
4.1 Related Work on Paraphrasing
Three major approaches in paraphrase collection
are manual collection (domain-specific), collection
utilizing existing lexical resources (i.e. WordNet),
and derivation from corpora. Hermjakob et al
(2002) view paraphrase recognition as
reformulation by pattern recognition. Pang et al
(2003) use word lattices as paraphrase representa-
tions from semantically equivalent translations
sets. Using parallel corpora, Barzilay and McKe-
own (2001) identify paraphrases from multiple
translations of classical novels, where as Bannard
and Callison-Burch (2005) develop a probabilistic
representation for paraphrases extracted from large
Machine Translation (MT) data sets.
4.2 Extracting Paraphrases
Our method to automatically construct a large do-
main-independent paraphrase collection is based
on the assumption that two different English
phrases of the same meaning may have the same
translation in a foreign language.
Phrase-based Statistical Machine Translation
(SMT) systems analyze large quantities of bilin-
gual parallel texts in order to learn translational
alignments between pairs of words and phrases in
two languages (Och and Ney, 2004). The sentence-
based translation model makes word/phrase align-
ment decisions probabilistically by computing the
optimal model parameters with application of the
statistical estimation theory. This alignment proc-
ess results in a corpus of word/phrase-aligned par-
allel sentences from which we can extract phrase
pairs that are translations of each other. We ran the
alignment algorithm from (Och and Ney, 2003) on
a Chinese-English parallel corpus of 218 million
English words. Phrase pairs are extracted by fol-
lowing the method described in (Och and Ney,
2004) where all contiguous phrase pairs having
consistent alignments are extraction candidates.
The resulting phrase table is of high quality; both
the alignment models and phrase extraction meth-
449
ods have been shown to produce very good results
for SMT. Using these pairs we build paraphrase
sets by joining together all English phrases with
the same Chinese translation. Figure 2 shows an
example word/phrase alignment for two parallel
sentence pairs from our corpus where the phrases
?blowing up? and ?bombing? have the same Chi-
nese translation. On the right side of the figure we
show the paraphrase set which contains these two
phrases, which is typical in our collection of ex-
tracted paraphrases.
5 Summary Comparison in ParaEval
This section describes the process of comparing a
peer summary against a reference summary and the
summary grading mechanism.
5.1 Description
We adopt a three-tier matching strategy for sum-
mary comparison. The score received by a peer
summary is the ratio of the number of reference
words matched to the total number of words in the
reference summary. The total number of matched
reference words is the sum of matched words in
reference throughout all three tiers. At the top
level, favoring high recall coverage, we perform an
optimal search to find multi-word paraphrase
matches between phrases in the reference summary
and those in the peer. Then a greedy search is per-
formed to find single-word paraphrase/synonym
matches among the remaining text. Operations
conducted in these two top levels are marked as
linked rounded rectangles in Figure 3. At the bot-
tom level, we find lexical identity matches, as
marked in rectangles in the example. If no para-
phrases are found, this last level provides a guar-
antee of lexical comparison that is equivalent to
what other automated systems give. In our system,
the bottom level currently performs unigram
matching. Thus, we are ensured with at least a
ROUGE-1 type of summary comparison. Alterna-
tively, equivalence of other ROUGE configura-
tions can replace the ROUGE-1 implementation.
There is no theoretical reason why the first two
levels should not merge. But due to high computa-
tional cost in modeling an optimal search, the sepa-
ration is needed. We explain this in detail below.
5.2 Multi-Word Paraphrase Matching
In this section we describe the algorithm that per-
forms the multi-word paraphrase matching be-
tween phrases from reference and peer summaries.
Using the example in Figure 3, this algorithm cre-
ates the phrases shown in the rounded rectangles
and establishes the appropriate links indicating
corresponding paraphrase matches.
Problem Description
Measuring content coverage of a peer summary
using a single reference summary requires com-
puting the recall score of how much information
from the reference summary is included in the
peer. A summary unit, either from reference or
peer, cannot be matched for more than once. For
Figure 2. An example of paraphrase extraction.
Figure 3. Comparison of summaries.
450
example, the phrase ?imposed sanctions on Libya?
(r1) in Figure 3?s reference summary was matched
with the peer summary?s ?voted sanctions against
Libya? (p1). If later in the peer summary there is
another phrase p2 that is also a paraphrase of r1, the
match of r1 cannot be counted twice. Conversely,
double counting is not permissible for
phrase/words in the peer summary, either.
We conceptualize the comparison of peer
against reference as a task that is to complete over
several time intervals. If the reference summary
contains n sentences, there will be n time intervals,
where at time ti, phrases from a particular sentence
i of the reference summary are being considered
with all possible phrases from the peer summary
for paraphrase matches. A decision needs to be
made at each time interval:
? Do we employ a local greedy match algo-
rithm that is recall generous (preferring more
matched words from reference) towards only the
reference sentence currently being analyzed,
? Or do we need to explore globally, in-
specting all reference sentences and find the best
overall matching combinations?
Consider the scenario in Figure 4:
1) at t0: L(p1 = r2) > L(p2 = r1) and r2 contains r1.
A local search algorithm leads to match(p1, r2). L() indi-
cates the number of words in reference matched by the
peer phrase through paraphrase matching and match()
indicates a paraphrase match has occurred (more in the
figure).
2) at t1: L(p1 = r3) > L(p1 = r2). A global algo-
rithm reverses the decision match(p1, r2) made at t0 and
concludes match(p1, r3) and match(p2, r1) . A local
search algorithm would have returned no match.
Clearly, the global search algorithm achieves
higher overall recall (in words). The matching of
paraphrases between a reference and its peer be-
comes a global optimization problem, maximizing
the content coverage of the peer compared in refer-
ence.
Solution Model
We use dynamic programming to derive the solu-
tion of finding the best paraphrase-matching com-
binations. The optimization problem is as follows:
Sentences from a reference summary and a peer
summary can be broken into phrases of various
lengths. A paraphrase lookup table is used to find
whether a reference phrase and a peer phrase are
paraphrases of each other. What is the optimal
paraphrase matching combination of phrases from
reference and peer that gives the highest recall
score (in number of matched reference words) for
this given peer? The solution should be recall ori-
ented (favoring a peer phrase that matches more
reference words than those match less).
Following (Trick, 1997), the solution can be
characterized as:
1) This problem can be divided into n stages
corresponding to the n sentences of the reference
summary. At each stage, a decision is required to
determine the best combination of matched para-
phrases between the reference sentence and the
entire peer summary that results in no double
counting of phrases on the peer side. There is no
double counting of reference phrases across stages
since we are processing one reference sentence at a
time and are finding the best paraphrase matches
using the entire peer summary. As long as there is
no double counting in peers, we are guaranteed to
have none in reference, either.
2) At each stage, we define a number of pos-
sible states as follows.  If, out of all possible
phrases of any length extracted from the reference
sentence, m phrases were found to have matching
paraphrases in the peer summary, then a state is
any subset of the m phrases.
3) Since no double counting in matched
phrases/words is allowed in either the reference
summary or the peer summary, the decision of
which phrases (leftover text segments in reference
Pj and ri represent phrases chosen for   paraphrase
matching from peer and reference respectively.
Pj = ri indicates that the phrase Pj from peer is
found to be a paraphrase to the phrase ri from
reference.
L(Pj = ri) indicates the number of words matched
by Pj in ri when they are found to be paraphrases of
each other.
L(Pj = ri) and L(Pj = ri+1) may not be equal if the
number of words in ri, indicated by L(ri), does not
equal to the number of words in ri+1, indicated by
L(ri+1).
Figure 4. Local vs. global paraphrase matching.
451
and in peer) are allowed to match for the next stage
is made in the current stage.
4) Principle of optimality: at a given state, it
is not necessary to know what matches occurred at
previous stages, only on the accumulated recall
score (matched reference words) from previous
stages and what text segments (phrases) in peer
have not been taken/matched in previous stages.
5) There exists a recursive relationship that
identifies the optimal decision for stage s (out of n
total stages), given that stage s+1 has already been
solved.
6) The final stage, n (last sentence in refer-
ence), is solved by choosing the state that has the
highest accumulated recall score and yet resulted
no double counting in any phrase/word in peer the
summary.
Figure 5 demonstrates the optimal solution (12
reference words matched) for the example shown
in Figure 4. We can express the calculations in the
following formulas:
where fy(xb) denotes the optimal recall coverage
(number of words in the reference summary
matched by the phrases from the peer summary) at
state xb in stage y. r(xb) is the recall coverage given
state xb. And c(xb) records the phrases matched in
peer with no double counting, given state xb.
5.3 Synonym Matching
All paraphrases whose pairings do not involve
multi-word to multi-word matching are called
synonyms in our experiment. Since these phrases
have either a n-to-1 or 1-to-n matching ratio (such
as the phrases ?blowing up? and ?bombing?), a
greedy algorithm favoring higher recall coverage
reduces the state creation and stage comparison
costs associated with the optimal procedure
(O(m6): O(m3) for state creation, and for 2 stages at
any time)). The paraphrase table described in Sec-
tion 4 is used.
 Synonym matching is performed only on parts
of the reference and peer summaries that were not
matched from the multi-word paraphrase-matching
phase.
5.4 Lexical Matching
This matching phase performs straightforward
lexical matching, as exemplified by the text frag-
ments marked in rectangles in Figure 3. Unigrams
are used as the units for counting matches in ac-
cordance with the previous two matching phases.
 During all three matching phases, we employed
a ROUGE-1 style of counting. Other alternatives,
such as ROUGE-2, ROUGE-SU4, etc., can easily
be adapted to each phase.
6  Evaluation of ParaEval
To evaluate and validate the effectiveness of an
automatic evaluation metric, it is necessary to
show that automatic evaluations correlate with
human assessments highly, positively, and consis-
tently (Lin and Hovy, 2003). In other words, an
automatic evaluation procedure should be able to
distinguish good and bad summarization systems
by assigning scores with close resemblance to hu-
mans? assessments.
6.1 Document Understanding Conference
The Document Understanding Conference has
provided large-scale evaluations on both human-
created and system-generated summaries annually.
Research teams are invited to participate in solving
summarization problems with their systems. Sys-
tem-generated summaries are then assessed by
humans and/or automatic evaluation procedures.
The collection of human judgments on systems and
their summaries has provided a test-bed for devel-
oping and validating automated summary grading
methods (Lin and Hovy, 2003; Hovy et al, 2005).
The correlations reported by ROUGE and BE
show that the evaluation correlations between these
two systems and DUC human evaluations are
much higher on single-document summarization
tasks. One possible explanation is that when sum-
Figure 5. Solution for the example in Figure 4.
452
marizing from only one source (text), both human-
and system-generated summaries are mostly ex-
tractive. The reason for humans to take phrases (or
maybe even sentences) verbatim is that there is less
motivation to abstract when the input is not highly
redundant, in contrast to input for multi-document
summarization tasks, which we speculate allows
more abstracting. ROUGE and BE both facilitate
lexical n-gram matching, hence, achieving amaz-
ing correlations. Since our baseline matching strat-
egy is lexically based when paraphrase matching is
not activated, validation on single-doc summariza-
tion results is not repeated in our experiment.
6.2 Validation and Discussion
We use summary judgments from DUC2003?s
multi-document summarization (MDS) task to
evaluate ParaEval. During DUC2003, participating
systems created short summaries (~100 words) for
30 document sets. For each set, one assessor-
written summary was used as the reference to
compare peer summaries created by 18 automatic
systems (including baselines) and 3 other human-
written summaries. A system ranking was pro-
duced by taking the averaged performance on all
summaries created by systems. This evaluation
process is replicated in our validation setup for
ParaEval. In all, 630 summary pairs were com-
pared. Pearson?s correlation coefficient is com-
puted for the validation tests, using DUC2003
assessors? results as the gold standard.
Table 1 illustrates the correlation figures from
the DUC2003 test set. ParaEval-para_only shows
the correlation result when using only paraphrase
and synonym matching, without the baseline uni-
gram matching. ParaEval-2 uses multi-word para-
phrase matching and unigram matching, omitting
the greedy synonym-matching phrase. ParaEval-3
incorporates matching at all three granularity lev-
els.
We see that the current implementation of
ParaEval closely resembles the way ROUGE-1
differentiates system-generated summaries. We
believe this is due to the identical calculations of
recall scores. The score that a peer summary re-
ceives from ParaEval depends on the number of
words matched in the reference summary from its
paraphrase, synonym, and unigram matches. The
counting of individual words in reference indicates
a ROUGE-1 design in grading. However, a de-
tailed examination on individual reference-peer
comparisons shows that paraphrase and synonym
comparisons and matches, in addition to lexical n-
gram matching, do measure a higher level of con-
tent coverage. This is demonstrated in Figure 6a
and b. Strict unigram matching reflects the content
retained by a peer summary mostly in the 0.2-0.4
ranges in recall, shown as dark-colored dots in the
graphs. Allowing paraphrase and synonym match-
ing increases the detection of peer coverage to the
range of 0.3-0.5, shown as light-colored dots.
We conducted a manual evaluation to further
examine the paraphrases being matched. Using 10
summaries from the Pyramid data, we asked three
human subjects to judge the validity of 128 (ran-
domly selected) paraphrase pairs extracted and
identified by ParaEval. Each pair of paraphrases
was coupled with its respective sentences as con-
texts. All paraphrases judged were multi-word.
ParaEval received an average precision of 68.0%.
The complete agreement between judges is 0.582
according to the Kappa coefficient (Cohen, 1960).
In Figure 7, we show two examples that the human
judges consider to be good paraphrases produced
and matched by ParaEval. Judges voiced difficul-
DUC-2003 Pearson
ROUGE-1 0.622
ParaEval-para_only 0.41
ParaEval-2 0.651
ParaEval-3 0.657
Table 1. Correlation with DUC 2003 MDS results.
Human Summaries: ParaEval vs. ROUGE-1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
DUC2003 Summary Writers
Re
ca
ll 
(%
 w
or
d 
m
at
ch
)
ROUGE-1 Scores
ParaEval-3 Scores
System Summaries: ParaEval vs. ROUGE-1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
DUC2003 Systems
Re
ca
ll 
(%
 w
or
d 
m
at
ch
)
ROUGE-1 Scores
ParaEval-3 Scores
a).
Human-
written
summaries.
b).
System-
generated
summaries.
Figure 6. A detailed look at the scores assigned by
lexical and paraphrase/synonym comparisons.
453
Figure 7. Paraphrases matched by ParaEval.
ties in determining ?semantic equivalence.? There
were cases where paraphrases would be generally
interchangeable but could not be matched because
of non-semantic equivalence in their contexts. And
there were paraphrases that were determined as
matches, but if taken out of context, would not be
direct replacements of each other. These two situa-
tions are where the judges mostly disagreed.
7 Conclusion and Future Work
In this paper, we have described an automatic
summarization evaluation method, ParaEval, that
facilitates paraphrase matching using a large do-
main-independent paraphrase table extracted from
a bilingual parallel corpus. The three-layer match-
ing strategy guarantees a ROUGE-like baseline
comparison if paraphrase matching fails.
The paraphrase extraction module from the cur-
rent implementation of ParaEval does not dis-
criminate among the phrases that are found to be
paraphrases of one another. We wish to incorporate
the probabilistic paraphrase extraction model from
(Bannard and Callison-Burch, 2005) to better ap-
proximate the relations between paraphrases. This
adaptation will also lead to a stochastic model for
the low-level lexical matching and scoring.
We chose English-Chinese MT parallel data be-
cause they are news-oriented which coincides with
the task genre from DUC. However, it is unknown
how large a parallel corpus is sufficient in provid-
ing a paraphrase collection good enough to help
the evaluation process. The quality of the para-
phrase table is also affected by changes in the do-
main and language pair of the MT parallel data.
We plan to use ParaEval to investigate the impact
of these changes on paraphrase quality under the
assumption that better paraphrase collections lead
to better summary evaluation results.
The immediate impact and continuation of the
described work would be to incorporate paraphrase
matching and extraction into the summary creation
process. And with ParaEval, it is possible for us to
evaluate systems that do incorporate some level of
abstraction, especially paraphrasing.
References
Bannard, C. and C. Callison-Burch. 2005. Paraphrasing with bilingual
parallel corpora. Proceedings of ACL-2005.
Barzilay, R. and K. McKeown. 2001. Extracting paraphrases from a
parallel corpus. Proceedings of ACL/EACL-2001.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, R. L. Mercer.
1993. The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(2): 263?311, 1993.
Cohen, J. 1960. A coefficient of agreement for nominal scales. Edu-
cation and Psychological Measurement, 43(6):37?46.
Diab, M. and P. Resnik. 2002. An unsupervised method for word
sense tagging using parallel corpora. Proceedings of ACL-2002.
DUC. 2001?2005. Document Understanding Conferences.
Hermjakob, U., A. Echihabi, and D. Marcu. 2002. Natural language
based reformulation resource and web exploitation for question
answering. Proceedings of TREC-2002.
Hovy, E, C.Y. Lin, and L. Zhou. 2005. Evaluating DUC 2005 using
basic elements. Proceedings of DUC-2005.
Hovy, E., C.Y. Lin, L. Zhou, and J. Fukumoto. 2005a. Basic Ele-
ments. http://www.isi.edu/~cyl/BE.
Lin, C.Y.  2001. http://www.isi.edu/~cyl/SEE.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of summaries
using n-gram co-occurrence statistics. Proceedings of the HLT-
2003.
Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller.
1990. Introduction to WordNet: An on-line lexical database. Inter-
national Journal of Lexicography, 3(4): 235?245.
Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in
summarization: the pyramid method. Proceedings of the HLT-
NAACL 2004.
Och, F. J. and H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1): 19-
?51, 2003.
Och, F. J. and H. Ney. 2004. The alignment template approach to
statistical machine translation. Computational Linguistics, 30(4),
2004.
Pang, B. , K. Knight and D. Marcu. 2003. Syntax-based alignment of
multiple translations: extracting paraphrases and generating new
sentences. Proceedings of HLT/NAACL-2003.
Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. IBM research report
Bleu: a method for automatic evaluation of machine translation
IBM Research Division Technical Report, RC22176, 2001.
Trick, M. A. 1997. A tutorial on dynamic program-
ming.http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html.
Van Halteren, H. and S. Teufel. 2003. Examining the consensus be-
tween human summaries: initial experiments with factoid analysis.
Proceedings of HLT-2003.
454
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 463?470,
New York, June 2006. c?2006 Association for Computational Linguistics
	

	


	
		
	

From Single to Multi-document Summarization:  
A Prototype System and its Evaluation 
Chin-Yew Lin and Eduard Hovy  
University of Southern California / Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{cyl,hovy}@isi.edu 
 
Abstract 
NeATS is a multi-document 
summarization system that attempts 
to extract relevant or interesting 
portions from a set of documents 
about some topic and present them 
in coherent order. NeATS is among 
the best performers in the large scale 
summarization evaluat ion DUC 
2001. 
1 Introduction 
In recent years, text summarization has been 
enjoying a period of revival.  Two workshops 
on Automatic Summarization were held in 
2000 and 2001.  However, the area is still 
being fleshed out: most past efforts have 
focused only on single-document 
summarization (Mani 2000), and no standard 
test sets and large scale evaluations have been 
reported or made available to the English-
speaking research community except the 
TIPSTER SUMMAC Text Summarization 
evaluation (Mani et al 1998). 
To address these issues, the Document 
Understanding Conference (DUC) sponsored 
by the National Institute of Standards and 
Technology (NIST) started in 2001 in the 
United States.  The Text Summarization 
Challenge (TSC) task under the NTCIR (NII-
NACSIS Test Collection for IR Systems) 
project started in 2000 in Japan.  DUC and 
TSC both aim to compile standard training and 
test collections that can be shared among 
researchers and to provide common and large 
scale evaluations in single and multiple 
document summarization for their participants. 
In this paper we describe a multi-document 
summarization system NeATS.  It attempts to 
extract relevant or interesting portions from a 
set of documents about some topic and present 
them in coherent order.  We outline the 
NeATS system and describe how it performs 
content selection, filtering, and presentation in 
Section 2.  Section 3 gives a brief overview of 
the evaluation procedure used in DUC -2001 
(DUC 2001).  Section 4 discusses evaluation 
metrics, and Section 5 the results.  We 
conclude with future directions. 
2 NeATS 
NeATS is an extraction-based multi-document 
summarization system.  It leverages techniques 
proved effective in single document 
summarization such as: term frequency (Luhn 
1969), sentence position (Lin and Hovy 1997), 
stigma words (Edmundson 1969), and a 
simplified version of MMR (Goldstein et al 
1999) to select and filter content.  To improve 
topic coverage and readability, it uses term 
clustering, a ?buddy system? of paired 
sentences, and explicit time annotation. 
Most of the techniques adopted by NeATS are 
not new.  However, applying them in the 
proper places to summarize multiple 
documents and evaluating the results on large 
scale common tasks are new. 
Given an input of a collection of sets of 
newspaper articles, NeATS generates 
summaries in three stages: content selection, 
filtering, and presentation. We describe each 
stage in the following sections. 
2.1 Content Selection 
The goal of content selection is to identify 
important concepts mentioned in a document 
collection.  For example, AA flight 11, AA 
flight 77, UA flight 173, UA flight 93, New 
York, World Trade Center, Twin Towers, 
Osama bin Laden, and al-Qaida are key 
concepts for a document collection about the 
September 11 terrorist attacks in the US. 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 457-464.
                         Proceedings of the 40th Annual Meeting of the Association for
In a key step for locating important sentences, 
NeATS computes the likelihood ratio l  
(Dunning, 1993) to identify key concepts in 
unigrams, bigrams, and trigrams1, using the 
on- topic document collection as the relevant 
set and the off-topic document collection as the 
irrelevant set.  Figure 1 shows the top 5 
concepts with their relevancy scores (-2l) for 
the topic ?Slovenia Secession from 
Yugoslavia? in the DUC-2001 test collection.  
This is similar to the idea of topic signature 
introduced in (Lin and Hovy 2000). 
With the individual key concepts available, we 
proceed to cluster these concepts in order to 
identify major subtopics within the main topic. 
Clusters are formed through strict lexical 
connection.  For example, Milan and Kucan 
are grouped as ?Milan Kucan? since ?Milan 
Kucan? is a key bigram concept; while 
Croatia, Yugoslavia, Slovenia, republic, and 
are joined due to the connections as follows: 
? Slovenia Croatia 
? Croatia Slovenia 
? Yugoslavia Slovenia 
? republic Slovenia 
                                              
1 Closed class words (of, in, and, are, and so on) 
were ignored in constructing unigrams, bigrams and 
trigrams. 
? Croatia republic 
Each sentence in the document set is then 
ranked, using the key concept structures. An 
example is shown in Figure 2.  The ranking 
algorithm rewards most specific concepts first; 
for example, a sentence containing ?Milan 
Kucan? has a higher score than a sentence 
contains only either Milan or Kucan.  A 
sentence containing both Milan and Kucan  but 
not in consecutive order gets a lower score too.  
This ranking algorithm performs relatively 
well, but it also results in many ties.  
Therefore, it is necessary to apply some 
filtering mechanism to maintain a reasonably 
sized sentence pool for final presentation. 
2.2 Content Filtering 
NeATS uses three different filters: sentence 
position, stigma words, and maximum 
marginal relevancy. 
2.2.1  Sentence Position  
Sentence position has been used as a good 
important content filter since the late 60s 
(Edmundson 1969).  It was also used as a 
baseline in a preliminary multi-document 
summarization study by Marcu and Gerber 
(2001) with relatively good results.  We apply 
a simple sentence filter that only retains the 
lead 10 sentences. 
2.2.2  Stigma Words  
Some sentences start with 
? conjunctions (e.g., but, although, however), 
? the verb say and its derivatives, 
? quotation marks, 
? pronouns such as he, she, and they, 
and usually cause discontinuity in summaries. 
Since we do not use discourse level selection 
criteria ? la (Marcu 1999), we simply reduce 
the scores of these sentences to avoid including 
them in short summaries. 
2.2.3   Maximum Marginal Relevancy  
Figure 2. Top 5 unigram, bigram, and trigram concepts for topic "Slovenia Secession from Yugoslavia".  
Rank Unigram (-2l) Bigram (-2l) Trigram (-2l)
1 Slovenia 319.48 federal army 21.27 Slovenia central bank 5.80
2 Yugoslavia 159.55 Slovenia Croatia 19.33 minister foreign affairs 5.80
3 Slovene 87.27 Milan Kucan 17.40 unallocated federal debt 5.80
4 Croatia 79.48 European Community 13.53 Drnovsek prime minister 3.86
5 Slovenian 67.82 foreign exchange 13.53 European Community countries 3.86
Figure 1. Sample key concept structure. 
n1
(:S URF " WEBCL -SUMM MARIZ ER-KU CAN"
 :C AT S- NP
 :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
 :L EX  0 .6363 63636 36363 6
 :S UBS
 ( ((KUC AN-0)
   (:S URF " Milan  Ku can"
    :C AT S- NP
    :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
    :L EX 0. 63636 36363 63636
    :S UBS
    ((( KUCAN -1)
      (:S URF " Ku can"
       :C AT S- NP
       :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
       :L EX 0. 63636 36363 63636 ))
     (( KUCAN -2)
      (:S URF " Milan "
       :C AT S- NP
       :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
       :L EX 0. 63636 36363 63636 ))))) ))
The content selection and filtering methods 
described in the previous section only  concern 
individual sentences.  They do not consider the 
redundancy issue when two top ranked 
sentences refer to similar things.  To address 
the problem, we use a simplified version of 
CMU?s MMR (Goldstein et al 1999) 
algorithm.  A sentence is added to the 
summary if and only if its content has less than 
X percent overlap with the summary.  The 
overlap ratio is computed using simple 
stemmed word overlap and the threshold X is 
set empirically. 
2.3 Content Presentation 
NeATS so far only considers features 
pertaining to individual sentences.  As we 
mentioned in Section 2.2.2, we can demote 
some sentences containing stigma words to 
improve the cohesion and coherence of 
summaries.  However, we still face two 
problems: definite noun phrases and events 
spread along an extended timeline.  We 
describe these problems and our solutions in 
the following sections. 
2.3.1  A Buddy System of Paired Sentences  
The problem of definite noun phrases can be 
illustrated in Figure 3.  These sentences are 
from documents of the DUC -2001 topic US 
Drought of 1988.  According to pure sentence 
scores, sentence 3 of document AP891210-
0079 has a higher score (34.60) than sentence 
1 (32.20) and should be included in the shorter 
summary (size=?50?).  However, if we select 
sentence 3 without also including sentence 1, 
the definite noun phrase ?The record $3.9 
billion drought relief program of 1988? seems 
to come without any context.  To remedy this 
problem, we introduce a buddy system to 
improve cohesion and coherence.  Each 
sentence is paired with a suitable introductory 
sentence unless it is already an introductory 
sentence.  In DUC -2001 we simply used the 
first sentence of its document.  This assumes 
lead sentences provide introduction and 
context information about what is coming next.  
2.3.2  Time Annotation and Sequence 
One main problem in multi-document 
summarization is that documents in a 
collection might span an extended time period. 
For example, the DUC-2001 topic ?Slovenia 
Secession from Yugoslavia? contains 11 
documents dated from 1988 to 1994, from 5 
different sources 2.  Although a source 
document for single-document summarization 
might contain information collected across an 
extended time frame and from multiple 
sources, the author at least would synchronize 
them and present them in a coherent order.  In 
multi-document summarization, a date 
expression such as Monday occurring in two 
different documents might mean the same date 
or different dates.  For example, sentences in 
the 100 word summary shown in Figure 4 
come from 3 main time periods, 1990, 1991, 
and 1994.  If no absolute time references are 
given, the summary might mislead the reader 
to think that all the events mentioned in the 
four summary sentences occurred in a single 
week.  Therefore, time disambiguation and 
normalization are very important in multi-
document summarization.  As the first attempt, 
we use publication dates as reference points 
and compute actual dates for the following 
date expressions: 
? weekdays (Sunday, Monday, etc); 
? (past | next | coming) + weekdays; 
? today, yesterday, last night.  
We then order the summary sentences in their 
chronological order. Figure 4 shows an 
                                              
2 Sources include Associated Press, Foreign 
Broadcast Information Service, Financial Times, 
San Jose Mercury News, and Wall Street Journal. 
<multi size="50" docset="d50i">  
AP891210-0079 1 (32.20) (12/10/89) America's 1988 drought captured attention everywhere, but especially in 
Washington where politicians pushed through the largest disaster relief measure in U.S. history.  
AP891213-0004 1 (34.60) (12/13/89) The drought of 1988 hit ? 
</multi> 
<multi size="100" docset="d50i"> 
AP891210-0079 1 (32.20) (12/10/89) America's 1988 drought captured attention everywhere, but especially in 
Washington where politicians pushed through the largest disaster relief measure in U.S. history.  
AP891210-0079 3 (41.18) (12/10/89) The record $3.9 billion drought relief program of 1988, hailed as 
salvation for small farmers devastated by a brutal dry spell, became much more _ an unexpected, election-
year windfall for thousands of farmers who collected millions of dollars for nature's normal quirks.  
AP891213-0004 1 (34.60) (12/13/89) The drought of 1988 hit ?  
</multi> 
Figure 3. 50 and 100 word summaries for topic "US Drought of 1988". 
example 100 words summary with time 
annotations. Each sentence is marked with its 
publication date and a reference date 
(MM/DD/YY) is inserted after every date 
expression. 
3 DUC 2001 
Before we present our results, we describe the 
corpus and evaluation procedures of the 
Document Understanding Conference 2001 
(DUC 2001). 
DUC is a new evaluation series supported by 
NIST under TIDES, to further progress in 
summarization and enable researchers to 
participate in large-scale experiments.  There 
were three tasks in 2001: 
(1) Fully automatic summarization of a single 
document. 
(2) Fully automatic summarization of multiple 
documents: given a set of document on a 
single subject, participants were required to 
create 4 generic summaries of the entire set 
with approximately 50, 100, 200, and 400 
words. 30 document sets of approximately 10 
documents each were provided with their 50, 
100, 200, and 400 human written summaries 
for training (training set) and another 30 
unseen sets were used for testing (test set). 
(3) Exploratory summarization: participants 
were encouraged to investigate alternative 
approaches in summarization and report their 
results. 
NeATS participated only in the fully automatic 
multi-document summarization task.  A total 
of 12 systems participated in that task.   
The training data were distributed in early 
March of 2001 and the test data were 
distributed in mid-June of 2001.  Results were 
submitted to NIST for evaluation by July 1st. 
3.1 Evaluation Procedures 
NIST assessors who created the ?ideal? written 
summaries did pairwise comparisons of their 
summaries to the system-generated summaries, 
other assessors? summaries, and baseline 
summaries.  In addition, two baseline 
summaries were created automatically as 
reference points.  The first baseline, lead 
baseline, took the first 50, 100, 200, and 400 
words in the last document in the collection.  
The second baseline, coverage baseline, took 
the first sentence in the first document, the first 
sentence in the second document and so on 
until it had a summary of 50, 100, 200, or 400 
words. 
3.2 Summary Evaluation 
Environment 
NIST used the Summary Evaluation 
Environment (SEE) 2.0 developed by one of 
the authors (Lin 2001) to support its human 
evaluation process.  Using SEE, the assessors 
evaluated the quality of the system?s text (the 
peer text) as compared to an ideal (the model 
text).  The two texts were broken into lists of 
units and displayed in separate windows.  In 
DUC-2001 the sentence was used as the 
smallest unit of evaluation.  
SEE 2.0 provides interfaces for assessors to 
judge the quality of summaries in 
grammatically3, cohesion4, and coherence5 at 
five different levels: all, most, some, hardly 
any, or none.  It also allow s assessors to step 
through each model unit, mark all system units 
sharing content with the current model unit, 
and specify that the marked system units 
                                              
3 Does a summary follow the rule of English 
grammatical rules independent of its content? 
4 Do sentences in a summary fit in with their 
surrounding sentences?  
5 Is the content of a summary expressed and 
organized in an effectiv e way? 
Figure 4. 100 word summary with explicit time annotation. 
<multi size="100" docset="d45h"> 
AP900625-0160  1 (26.60) (06/25/90) The republic of Slovenia plans to begin work on a constitution 
that will give it full sovereignty within a new Yugoslav confederation, the state Tanjug news agency 
reported Monday (06/25/90).  
WSJ910628-0109 3 (9.48)  (06/28/91) On Wednesday (06/26/91), the Slovene soldiers manning this border 
post raised a new flag to mark Slovenia's independence from Yugoslavia.  
WSJ910628-0109 5 (53.77) (06/28/91) Less than two days after Slovenia and Croatia, two of Yugoslavia's 
six republics, unilaterally seceded from the nation, the federal government in Belgrade mobilized 
troops to regain control.  
FBIS3-30788    2 (49.14) (02/09/94) In the view of Yugoslav diplomats, the normalization of relations 
between Slovenia and the Federal Republic of Yugoslavia will certainly be a strenuous and long-term 
project.  
</multi> 
express all, most, some or hardly any of the 
content of the current model unit. 
4 Evaluation Metrics  
One goal of DUC-2001 was to debug the 
evaluation procedures and identify stable 
metrics that could serve as common reference 
points. NIST did not define any official 
performance metric in DUC-2001.  It released 
the raw evaluation results to DUC -2001 
participants and encouraged them to propose 
metrics that would help progress the field. 
4.1.1  Recall, Coverage, Retention and 
Weighted Retention  
Recall at different compression ratios has been 
used in summarization research (Mani 2001) to 
measure how well an automatic system retains 
important content of original documents.  
Assume we have a system summary Ss and a 
model summary Sm. The number of sentences 
occurring in both Ss and Sm is Na, the number 
of sentences in Ss is Ns, and the number of 
sentences in Sm is Nm. Recall is defined as 
Na/Nm.  The Compression Ratio is defined as 
the length of a summary (by words or 
sentences) divided by the length of its original 
document. DUC-2001 set the compression 
lengths to 50, 100, 200, and 400 words for the 
multi-document summarization task.  
However, applying recall in DUC-2001 
without modification is not appropriate 
because: 
1. Multiple system units contribute to 
multiple model units. 
2. Ss and Sm do not exactly overlap.  
3. Overlap judgment is not binary.  
For example, in an evaluation session an 
assessor judged system units S1.1 and S10.4 as 
sharing some content with model unit M2.2.  
Unit S1.1 says ?Thousands of people are 
feared dead? and unit M2.2 says ?3,000 and 
perhaps ? 5,000 people have been killed?.  
Are ?thousands? equivalent to ?3,000 to 
5,000? or not?  Unit S10.4 indicates it was an 
?earthquake of magnitude 6.9? and unit M2.2 
says it was ?an earthquake measuring 6.9 on 
the Richter scale?.  Both of them report a ?6.9? 
earthquake.   But the second part of system 
unit S10.4, ?in an area so isolated??, seems 
to share some content with model unit M4.4 
?the quake was centered in a remote 
mountainous area?. Are these two equivalent? 
This example highlights the difficulty of 
judging the content coverage of system 
summaries against model summaries and the 
inadequacy of using recall as defined.  
As we mentioned earlier, NIST assessors not 
only marked the sharing relations among 
system units (SU) and model units (MU), they 
also indicated the degree of match, i.e., all, 
most , some, hardly any,  or none.  This enables 
us to compute weighted recall.  
Different versions of weighted recall were 
proposed by DUC-2001 participants. 
McKeown et al (2001) treated the 
completeness of coverage as threshold: 4 for 
all, 3 for most  and above, 2 for some and 
above, and 1 for hardly any and above.  They 
then proceeded to compare system 
performances at different threshold levels.  
They defined recall at threshold t, Recallt, as 
follows:  
summary model in the MUs ofnumber  Total
 aboveor at  marked MUs ofNumber t  
We used the completeness of coverage as 
coverage score, C, instead of threshold: 1 for 
all, 3/4 for most, 1/2 for some, and 1/4 for 
hardly any, 0 for none.  To avoid confusion 
with the recall used in information retrieval, 
we call our metric weighted retention, 
Retentionw, and define it as follows: 
summary model in the MUs ofnumber  Total
  marked) MUs of(Number C?  
if we ignore C and set it always to 1, we obtain 
an unweighted retention, Retention1.  We used 
Retention1 in our evaluation to illustrate that 
relative system performance changes when 
different evaluation metrics are chosen.  
Therefore, it is important to have common and 
agreed upon metrics to facilitate large scale 
evaluation efforts.  
4.1.2  Precision and Pseudo Precision 
Precision is also a common measure.  
Borrowed from information retrieval research, 
precision is used to measure how effectively a 
system generates good summary sentences.  It 
is defined as Na/ Ns. Precision in a fixed length 
summary output is equal to recall since N s =  
Nm.  However, due to the three reasons stated 
at the beginning of the previous section, no 
straightforward computation of the traditional 
precision is available in DUC-2001. 
If we count the number of model units that are 
marked as good summary units and are 
selected by systems, and use the number of 
model units in various summary lengths as the 
sample space, we obtain a precision metric 
equal to Retention1.  Alternatively, we can 
count how many unique system units share 
content with model units and use the total 
number of system units as the sample space.  
We define this as pseudo precision, Precisionp, 
as follows: 
summary system in the SUs ofnumber  Total
marked SUs ofNumber  
Most of the participants in DUC-2001 reported 
their pseudo precision figures. 
5 Results and Discussion 
We present the performance of NeATS in 
DUC-2001 in content and quality measures.  
5.1 Content 
With respect to content, we computed 
Retention1, Retention w, and Precisionp using 
the formulas defined in the previous section.  
The scores are shown in Table 1 (overall 
average and per size).  Analyzing all systems? 
results according to these, we made the 
following observations. 
 (1) NeATS (system N) is consistently ranked 
among the top 3 in average and per size 
Retention1 and Retention w. 
(2) NeATS?s performance for averaged pseudo 
precision equals human?s at about 58% (Pp all). 
(3) The performance in weighted retention is 
really low.  Even humans6 score only 29% (Rw 
all). This indicates low inter-human agreement 
(which we take to reflect the undefinedness of 
the ?generic summary? task).  However, the 
unweighted retention of humans is 53%.  This 
suggests assessors did write something similar 
in their summaries but not exactly the same; 
once again illustrating the difficulty of 
summarization evaluation.  
(4) Despite the low inter -human agreement, 
humans score better than any system.  They 
outscore the nearest system by about 11% in 
averaged unweighted retention (R1 all : 53% vs. 
42%) and weighted retention (Rw all : 29% vs. 
18%).  There is obviously still considerable 
room for systems to improve.  
(5) System performances are separated into 
two major groups by baseline 2 (B2: coverage 
baseline) in averaged weighted retention.  This 
confirms that lead sentences are good 
summary sentence candidates and that one 
does need to cover all documents in a topic to 
achieve reasonable performance in multi-
document summarization. NeATS?s strategies 
of filtering sentences by position and adding 
lead sentences to set context are proved 
effective. 
(6) Different metrics result in different 
performance rankings.  This is demonstrated 
by the top 3 systems T, N, and Y.  If we use 
the averaged unweighted retention (R1 all), Y is 
                                              
6 NIST assessors wrote two separate summaries per 
topic.  One was used to judge all system summaries 
and the two baselines. The other was used to 
determine the (potential) upper bound. 
Table 1.  Pseudo precision, unweighted retention, and weighted retention for all summary lengths: overall 
average, 400, 200, 100, and 50 words. 
SYS Pp All R1 All Rw Al l Pp  4 0 0 R1  4 0 0 Rw  4 0 0 Pp  2 0 0 R1 200 Rw  2 0 0 Pp 100 R1 100 Rw 100 Pp  50 R1 50 Rw 50
HM 58.71% 53.00% 28.81% 59.33% 52.95% 33.23% 59.91% 57.23% 33.82% 58.73% 54.67% 27.54% 56.87% 47.16% 21.62%
T 48.96% 35.53% (3) 18.48% (1) 56.51% (3) 38.50% (3) 25.12% (1) 53.85% (3) 35.62% 21.37% (1) 43.53% 32.82% (3) 14.28% (3) 41.95% 35.17% (2) 13.89% (2)
N* 58.72% (1) 37.52% (2) 17.92% (2) 61.01% (1) 41.21% (1) 23.90% (2) 63.34% (1) 38.21% (3) 21.30% (2) 58.79% (1) 36.34% (2) 16.44% (2) 51.72% (1) 34.31% (3) 10.98% (3)
Y 41.51% 41.58% (1) 17.78% (3) 49.78% 38.72% (2) 20.04% 43.63% 39.90% (1) 16.86% 34.75% 43.27% (1) 18.39% (1) 37.88% 44.43% (1) 15.55% (1)
P 49.56% 33.94% 15.78% 57.21% (2) 37.76% 22.18% (3) 51.45% 37.49% 19.40% 46.47% 31.64% 13.92% 43.10% 28.85% 9.09%
L 51.47% (3) 33.67% 15.49% 52.62% 36.34% 21.80% 53.51% 36.87% 18.34% 48.62% (3) 29.00% 12.54% 51.15% (2) 32.47% 9.90%
B2 47.27% 30.98% 14.56% 60.99% 33.51% 18.35% 49.89% 33.27% 17.72% 47.18% 29.48% 14.96% 31.03% 27.64% 8.02%
S 52.53% (2) 30.52% 12.89% 55.55% 36.83% 20.35% 58.12% (2) 38.70% (2) 19.93% (3) 49.70% (2) 26.81% 10.72% 46.43% (3) 19.23% 4.04%
M 43.39% 27.27% 11.32% 54.78% 33.81% 19.86% 45.59% 27.80% 13.27% 41.89% 23.40% 9.13% 31.30% 24.07% 5.05%
R 41.86% 27.63% 11.19% 48.63% 24.80% 12.15% 43.96% 31.28% 15.17% 38.35% 27.61% 11.46% 36.49% 26.84% 6.17%
O 43.76% 25.87% 11.19% 50.73% 27.53% 15.76% 42.94% 26.80% 13.07% 40.55% 25.13% 9.36% 40.80% 24.02% 7.03%
Z 37.98% 23.21% 8.99% 47.51% 31.17% 17.38% 46.76% 25.65% 12.83% 28.91% 17.29% 5.45% 28.74% 18.74% 3.23%
B1 32.92% 18.86% 7.45% 33.48% 17.58% 9.98% 43.13% 18.60% 8.65% 30.23% 17.42% 6.05% 24.83% 21.84% 4.20%
W 30.08% 20.38% 6.78% 38.14% 25.89% 12.10% 26.86% 21.01% 7.93% 28.31% 19.15% 5.36% 27.01% 15.46% 3.21%
U 23.88% 21.38% 6.57% 31.49% 29.76% 13.17% 24.20% 22.64% 8.49% 19.13% 17.54% 3.77% 20.69% 15.57% 3.04%
the best, followed by N, and then T; if we 
choose averaged weighted retention (Rw all), T 
is the best, followed by N, and then Y.  The 
reversal of T and Y due to different metrics 
demonstrates the importance of common 
agreed upon metrics.  We believe that metrics 
have to take coverage score (C, Section 4.1.1) 
into consideration to be reasonable since most 
of the content sharing among system units and 
model units is partial.  The recall at threshold t, 
Recallt (Section 4.1.1), proposed by 
(McKeown et al 2001), is a good example.  In 
their evaluation, NeATS ranked second at t=1, 
3, 4 and first at t=2.   
(7) According to Table 1, NeATS performed 
better on longer summaries (400 and 200 
words) based on weighted retention than it did 
on shorter ones.  This is the result of the 
sentence extraction-based nature of NeATS.  
We expect that systems that use syntax-based 
algorithms to compress their output will 
thereby gain more space to include additional 
important material. For example, System Y 
was the best in shorter summaries.  Its 100- 
and 50-word summaries contain only 
important headlines.  The results confirm this 
is a very effective strategy in composing short 
summaries.  However, the quality of the 
summaries suffered because of the 
unconventional syntactic structure of news 
headlines (Table 2).  
5.2 Quality 
Table 2 shows the macro-averaged scores for 
the humans, two baselines, and 12 systems.  
We assign a score of 4 to all, 3 to most, 2 to 
some, 1 to hardly any, and 0 to none.  The 
value assignment is for convenience of 
computing averages, since it is more 
appropriate to treat these measures as stepped 
values instead of continuous ones.  With this in 
mind, we have the following observations. 
(1) Most systems scored well in 
grammaticality.  This is not a surprise since 
most of the participants extracted sentences as 
summaries.  
But no system or human scored perfect in 
grammaticality. This might be due to the 
artifact of cutting sentences at the 50, 100, 200, 
and 400 words boundaries.  Only system Y 
scored lower than 3, which reflects its headline 
inclusion strategy.  
(2) When it came to the measure for cohesion 
the results are confusing.  If even the human-
made summaries score only 2.74 out of 4, it is 
unclear what this category means, or how the 
assessors arrived at these scores.  However, the 
humans and baseline 1 (lead baseline) did 
score in the upper range of 2 to 3 and all others 
had scores lower than 2.5.  Some of the 
systems (including B2) fell into the range of 1 
to 2 meaning some or hardly any cohesion.  
The lead baseline (B1), taking the first 50, 100, 
200, 400 words from the last document of a 
topic, did well.  On the contrary, the coverage 
baseline (B2) did poorly.  This indicates the 
difficulty of fitting sentences from different 
documents together.  Even selecting 
continuous sentences from the same document 
(B1) seems not to work well.  We need to 
define this metric more clearly and improve 
the capabilities of systems in this respect. 
(3) Coherence scores roughly track cohesion 
scores.  Most systems did better in coherence 
than in cohesion.   The human is the only one 
scoring above 3.  Again the room for 
improvement is abundant. 
(4) NeATS did not fare badly in quality 
measures.  It was in the same categories as 
other top performers: grammaticality is 
between most and all, cohesion, some and 
most , and coherence, some and most.  This 
indicates the strategies employed by NeATS 
(stigma word filtering, adding lead sentence, 
and time annotation) worked to some extent 
but left room for improvement. 
6 Conclusions  
Table 2. Averaged grammaticality, cohesion, and 
coherence over all summary sizes. 
SYS Grammar Cohesion Coherence
Human 3.74 2.74 3.19
B1 3.18 2.63 2.8
B2 3.26 1.71 1.65
L 3.72 1.83 1.9
M 3.54 2.18 2.4
N* 3.65 2 2.22
O 3.78 2.15 2.33
P 3.67 1.93 2.17
R 3.6 2.16 2.45
S 3.67 1.93 2.04
T 3.51 2.34 2.61
U 3.28 1.31 1.11
W 3.13 1.48 1.28
Y 2.45 1.73 1.77
Z 3.28 1.8 1.94
We described a multi-document 
summarization system, NeATS, and its 
evaluation in DUC-2001. We were encouraged 
by the content and readability of the results.  
As a prototype system, NeATS deliberately 
used simple methods guided by a few 
principles: 
? Extracting important concepts based on 
reliable statistics. 
? Filtering sentences by their positions and 
stigma words. 
? Reducing redundancy using MMR. 
? Presenting summary sentences in their 
chronological order with time annotations. 
These simple principles worked effectively.  
However, the simplicity of the system also 
lends itself to further improvements.  We 
would like to apply some compression 
techniques or use linguistic units smaller than 
sentences to improve our retention score.  The 
fact that NeATS performed as well as the 
human in pseudo precision but did less well in 
retention indicates its summaries might include 
good but duplicated information.  Working 
with sub-sentence units should help.  
To improve NeATS?s capability in content 
selection, we have started to parse sentences 
containing key unigram, bigram, and trigram 
concepts to identify their relations within their 
concept clusters. 
To enhance cohesion and coherence, we are 
looking into incorporating discourse 
processing techniques (Marcu 1999) or Radev 
and McKeown?s (1998) summary operators. 
We are analyzing the DUC evaluation scores 
in the hope of suggesting improved and more 
stable metrics. 
References 
DUC. 2001. The Document Understanding 
Workshop 2001. http://www-nlpir.nist.gov/ 
projects/duc/2001.html. 
Dunning, T. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence.  
Computational Linguistics 19, 61?74. 
Edmundson, H.P. 1969. New Methods in 
Automatic Abstracting.  Journal of the 
Association for Computing Machinery.  
16(2). 
Goldstein, J., M. Kantrowitz, V. Mittal, and J. 
Carbonell. 1999. Summarizing Text 
Documents: Sentence Selection and 
Evaluation Metrics. Proceedings of the 22nd 
International ACM Conference on 
Research and Development in Information 
Retrieval (SIGIR-99), Berkeley, CA, 121?
128. 
Lin, C.-Y. and E.H. Hovy. 2000. The 
Automated Acquisition of Topic 
Signatures for Text Summarization. 
Proceedings of the COLING 
Conference.  Saarbr?cken , Germany. 
Lin, C.-Y. 2001.  Summary Evaluation 
Environment. http://www.isi.edu/~cyl/SEE. 
Luhn, H. P. 1969. The Automatic Creation of 
Literature Abstracts. IBM Journal of 
Research and Development 2(2), 1969. 
Mani, I., D. House, G. Klein, L. Hirschman, L. 
Obrst, T. Firmin, M. Chrzanow ski, and B. 
Sundheim. 1998. The TIPSTER SUMMAC 
Text Summarization Evaluation: Final 
Report. MITRE Corp. Tech. Report. 
Mani, I. 2001. Automatic Summarization. John 
Benjamins Pub Co. 
Marcu, D. 1999. Discourse trees are good 
indicators of importance in text. In I. Mani 
and M. Maybury (eds), Advances in 
Automatic Text Summarization, 123?136. 
MIT Press. 
Marcu, D. and L. Gerber. 2001. An Inquiry 
into the Nature of Multidocument 
Abstracts, Extracts, and their Evaluation. 
Proceedings of the NAACL -2001 Workshop 
on Automatic Summarization.  Pittsburgh, 
PA. 
McKeown, K., R. Barzilay, D. Evans, V. 
Hatzivassiloglou, M-Y Kan, B, Schiffman, 
and S. Teufel 2001. Columbia Multi-
Document Summarization: Approach and 
Evaluation.  DUC-01 Workshop on Text 
Summarization.  New Orleans, LA.  
Radev, D.R. and K.R. McKeown. 1998. 
Generating Natural Language Summaries 
from Multiple On-line Sources. 
Computational Linguistics, 24(3):469?500.  
iNeATS: Interactive Multi-Document Summarization
Anton Leuski, Chin-Yew Lin, Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina Del Rey, CA 90292-6695
{leuski,cyl,hovy}@isi.edu
Abstract
We describe iNeATS ? an interactive
multi-document summarization system
that integrates a state-of-the-art summa-
rization engine with an advanced user in-
terface. Three main goals of the sys-
tem are: (1) provide a user with control
over the summarization process, (2) sup-
port exploration of the document set with
the summary as the staring point, and (3)
combine text summaries with alternative
presentations such as a map-based visual-
ization of documents.
1 Introduction
The goal of a good document summary is to provide
a user with a presentation of the substance of a body
of material in a coherent and concise form. Ideally, a
summary would contain only the ?right? amount of
the interesting information and it would omit all the
redundant and ?uninteresting? material. The quality
of the summary depends strongly on users? present
need ? a summary that focuses on one of several top-
ics contained in the material may prove to be either
very useful or completely useless depending on what
users? interests are.
An automatic multi-document summarization
system generally works by extracting relevant sen-
tences from the documents and arranging them in a
coherent order (McKeown et al, 2001; Over, 2001).
The system has to make decisions on the summary?s
size, redundancy, and focus. Any of these deci-
sions may have a significant impact on the quality
of the output. We believe a system that directly in-
volves the user in the summary generation process
and adapts to her input will produce better sum-
maries. Additionally, it has been shown that users
are more satisfied with systems that visualize their
decisions and give the user a sense of control over
the process (Koenemann and Belkin, 1996).
We see three ways in which interactivity and
visualization can be incorporated into the multi-
document summarization process:
1. give the user direct control over the summariza-
tion parameters such as size, redundancy, and
focus of the summaries.
2. support rapid browsing of the document set us-
ing the summary as the starting point and com-
bining the multi-document summary with sum-
maries for individual documents.
3. incorporate alternative formats for organizing
and displaying the summary, e.g., a set of news
stories can be summarized by placing the sto-
ries on a world map based on the locations of
the events described in the stories.
In this paper we describe iNeATS (Interactive
NExt generation Text Summarization) which ad-
dresses these three directions. The iNeATS system
is built on top of the NeATS multi-document sum-
marization system. In the following section we give
a brief overview of the NeATS system and in Sec-
tion 3 describe the interactive version.
2 NeATS
NeATS (Lin and Hovy, 2002) is an extraction-
based multi-document summarization system. It is
among the top two performers in DUC 2001 and
2002 (Over, 2001). It consists of three main com-
ponents:
Content Selection The goal of content selection is
to identify important concepts mentioned in
a document collection. NeATS computes the
likelihood ratio (Dunning, 1993) to identify key
concepts in unigrams, bigrams, and trigrams
and clusters these concepts in order to identify
major subtopics within the main topic. Each
sentence in the document set is then ranked, us-
ing the key concept structures. These n-gram
key concepts are called topic signatures.
Content Filtering NeATS uses three different fil-
ters: sentence position, stigma words, and re-
dundancy filter. Sentence position has been
used as a good important content filter since
the late 60s (Edmundson, 1969). NeATS ap-
plies a simple sentence filter that only retains
the N lead sentences. Some sentences start
with conjunctions, quotation marks, pronouns,
and the verb ?say? and its derivatives. These
stigma words usually cause discontinuities in
summaries. The system reduces the scores of
these sentences to demote their ranks and avoid
including them in summaries of small sizes. To
address the redundancy problem, NeATS uses a
simplified version of CMU?s MMR (Goldstein
et al, 1999) algorithm. A sentence is added to
the summary if and only if its content has less
than X percent overlap with the summary.
Content Presentation To ensure coherence of the
summary, NeATS pairs each sentence with an
introduction sentence. It then outputs the final
sentences in their chronological order.
3 Interactive Summarization
Figure 1 shows a screenshot of the iNeATS system.
We divide the screen into three parts corresponding
to the three directions outlined in Section 1. The
control panel displays the summarization parame-
ters on the left side of the screen. The document
panel shows the document text on the right side. The
summary panel presents the summaries in the mid-
dle of the screen.
3.1 Controlling Summarization Process
The top of the control panel provides the user with
control over the summarization process. The first set
of widgets contains controls for the summary size,
sentence position, and redundancy filters. The sec-
ond row of parameters displays the set of topic sig-
natures identified by the iNeATS engine. The se-
lected subset of the topic signatures defines the con-
tent focus for the summary. If the user enters a new
value for one of the parameters or selects a different
subset of the topic signatures, iNeATS immediately
regenerates and redisplays the summary text in the
top portion of the summary panel.
3.2 Browsing Document Set
iNeATS facilitates browsing of the document set by
providing (1) an overview of the documents, (2)
linking the sentences in the summary to the original
documents, and (3) using sentence zooming to high-
light the most relevant sentences in the documents.
The bottom part of the control panel is occupied
by the document thumbnails. The documents are ar-
ranged in chronological order and each document is
assigned a unique color to paint the text background
for the document. The same color is used to draw
the document thumbnail in the control panel, to fill
up the text background in the document panel, and to
paint the background of those sentences in the sum-
mary that were collected from the document. For
example, the screenshot shows that a user selected
the second document which was assigned the or-
ange color. The document panel displays the doc-
ument text on orange background. iNeATS selected
the first two summary sentences from this document,
so both sentences are shown in the summary panel
with orange background.
The sentences in the summary are linked to the
original documents in two ways. First, the docu-
ment can be identified by the color of the sentence.
Second, each sentence is a hyperlink to the docu-
ment ? if the user moves the mouse over a sentence,
the sentence is underlined in the summary and high-
lighted in the document text. For example, the first
sentence of the summary is the document sentence
Figure 1: Screenshot of the iNeATS system.
highlighted in the document panel. If the user clicks
on the sentence, iNeATS brings the source document
into the document panel and scrolls the window to
make the sentence visible.
The relevant parts of the documents are illumi-
nated using the technique that we call sentence
zooming. We make the text color intensity of each
sentence proportional to the relevance score com-
puted by the iNeATS engine and a zooming parame-
ter which can be controlled by the user with a slider
widget at the top of the document panel. The higher
the sentence score, the darker the text is. Conversely,
sentences that blend into the background have a very
low sentence score. The zooming parameter con-
trols the proportion of the top ranked sentences vis-
ible on the screen at each moment. This zooming
affects both the full-text and the thumbnail docu-
ment presentations. Combining the sentence zoom-
ing with the document set overview, the user can
quickly see which document contains most of the
relevant material and where approximately in the
document this material is placed.
The document panel in Figure 1 shows sentences
that achieve 50% on the sentence score scale. We see
that the first half of the document contains two black
sentences: the first sentence that starts with ?US In-
surers...?, the other starts with ?President George...?.
Both sentences have a very high score and they were
selected for the summary. Note, that the very first
sentence in the document is the headline and it is not
used for summarization. Note also that the sentence
that starts with ?However,...? scored much lower
than the selected two ? its color is approximately
half diluted into the background.
There are quite a few sentences in the second part
of the document that scored relatively high. How-
ever, these sentences are below the sentence position
cutoff so they do not appear in the summary. We il-
lustrate this by rendering such sentences in slanted
style.
3.3 Alternative Summaries
The bottom part of the summary panel is occupied
by the map-based visualization. We use BBN?s
IdentiFinder (Bikel et al, 1997) to detect the names
of geographic locations in the document set. We
then select the most frequently used location names
and place them on world map. Each location is iden-
tified by a black dot followed by a frequency chart
and the location name. The frequency chart is a bar
chart where each bar corresponds to a document.
The bar is painted using the document color and the
length of the bar is proportional to the number of
times the location name is used in the document.
The document set we used in our example de-
scribes the progress of the hurricane Andrew and its
effect on Florida, Louisiana, and Texas. Note that
the source documents and therefore the bars in the
chart are arranged in the chronological order. The
name ?Miami? appears first in the second document,
?New Orleans? in the third document, and ?Texas? is
prominent in the last two documents. We can make
some conclusions on the hurricane?s path through
the region ? it traveled from south-east and made its
landing somewhere in Louisiana and Texas.
4 Discussion
The iNeATS system is implemented in Java. It uses
the NeATS engine implemented in Perl and C. It
runs on any platform that supports these environ-
ments. We are currently working on making the sys-
tem available on our web site.
We plan to extend the system by adding temporal
visualization that places the documents on a timeline
based on the date and time values extracted from the
text.
We plan to conduct a user-based evaluation of the
system to compare users? satisfaction with both the
automatically generated summaries and summaries
produced by iNeATS.
References
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
ANLP-97, pages 194?201.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
H. P. Edmundson. 1969. New methods in automatic ex-
traction. Journal of the ACM, 16(2):264?285.
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and
Jaime G. Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics. In
Research and Development in Information Retrieval,
pages 121?128.
Jurgen Koenemann and Nicholas J. Belkin. 1996. A case
for interaction: A study of interactive information re-
trieval behavior and effectivness. In Proceedings of
ACM SIGCHI Conference on Human Factors in Com-
puting Systems, pages 205?212, Vancouver, British
Columbia, Canada.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype sys-
tem and it evaluation. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics (ACL-02), Philadelphia, PA, USA.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Barry Schiffman, and Si-
mone Teufel. 2001. Columbia multi-document sum-
marization: Approach and evaluation. In Proceed-
ings of the Workshop on Text Summarization, ACM SI-
GIR Conference 2001. DARPA/NIST, Document Un-
derstanding Conference.
Paul Over. 2001. Introduction to duc-2001: an intrin-
sic evaluation of generic news text summarization sys-
tems. In Proceedings of the Workshop on Text Summa-
rization, ACM SIGIR Conference 2001. DARPA/NIST,
Document Understanding Conference.
Automatic Evaluation of Machine Translation Quality Using Longest Com-
mon Subsequence and Skip-Bigram Statistics  
Chin-Yew Lin and Franz Josef Och 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292, USA 
{cyl,och}@isi.edu 
 
Abstract 
In this paper we describe two new objective 
automatic evaluation methods for machine 
translation. The first method is based on long-
est common subsequence between a candidate 
translation and a set of reference translations. 
Longest common subsequence takes into ac-
count sentence level structure similarity natu-
rally and identifies longest co-occurring in-
sequence n-grams automatically.  The second 
method relaxes strict n-gram matching to skip-
bigram matching. Skip-bigram is any pair of 
words in their sentence order. Skip-bigram co-
occurrence statistics measure the overlap of 
skip-bigrams between a candidate translation 
and a set of reference translations. The empiri-
cal results show that both methods correlate 
with human judgments very well in both ade-
quacy and fluency. 
1 Introduction 
Using objective functions to automatically evalu-
ate machine translation quality is not new. Su et al 
(1992) proposed a method based on measuring edit 
distance (Levenshtein 1966) between candidate 
and reference translations. Akiba et al (2001) ex-
tended the idea to accommodate multiple refer-
ences.  Nie?en et al (2000) calculated the length-
normalized edit distance, called word error rate 
(WER), between a candidate and multiple refer-
ence translations. Leusch et al (2003) proposed a 
related measure called position-independent word 
error rate (PER) that did not consider word posi-
tion, i.e. using bag-of-words instead. Instead of 
error measures, we can also use accuracy measures 
that compute similarity between candidate and ref-
erence translations in proportion to the number of 
common words between them as suggested by 
Melamed (1995). An n-gram co-occurrence meas-
ure, BLEU, proposed by Papineni et al (2001) that 
calculates co-occurrence statistics based on n-gram 
overlaps have shown great potential. A variant of 
BLEU developed by NIST (2002) has been used in 
two recent large-scale machine translation evalua-
tions. 
Recently, Turian et al (2003) indicated that 
standard accuracy measures such as recall, preci-
sion, and the F-measure can also be used in evalua-
tion of machine translation. However, results based 
on their method, General Text Matcher (GTM), 
showed that unigram F-measure correlated best 
with human judgments while assigning more 
weight to higher n-gram (n > 1) matches achieved 
similar performance as Bleu. Since unigram 
matches do not distinguish words in consecutive 
positions from words in the wrong order, measures 
based on position-independent unigram matches 
are not sensitive to word order and sentence level 
structure. Therefore, systems optimized for these 
unigram-based measures might generate adequate 
but not fluent target language. 
Since BLEU has been used to report the perform-
ance of many machine translation systems and it 
has been shown to correlate well with human 
judgments, we will explain BLEU in more detail 
and point out its limitations in the next section. We 
then introduce a new evaluation method called 
ROUGE-L that measures sentence-to-sentence 
similarity based on the longest common subse-
quence statistics between a candidate translation 
and a set of reference translations in Section 3. 
Section 4 describes another automatic evaluation 
method called ROUGE-S that computes skip-
bigram co-occurrence statistics. Section 5 presents 
the evaluation results of ROUGE-L, and ROUGE-
S and compare them with BLEU, GTM, NIST, 
PER, and WER in correlation with human judg-
ments in terms of adequacy and fluency. We con-
clude this paper and discuss extensions of the 
current work in Section 6. 
2 BLEU and N-gram Co-Occurrence 
To automatically evaluate machine translations 
the machine translation community recently 
adopted an n-gram co-occurrence scoring proce-
dure BLEU (Papineni et al 2001). In two recent 
large-scale machine translation evaluations spon-
sored by NIST, a closely related automatic evalua-
tion method, simply called NIST score, was used. 
The NIST (NIST 2002) scoring method is based on 
BLEU. 
The main idea of BLEU is to measure the simi-
larity between a candidate translation and a set of 
reference translations with a numerical metric. 
They used a weighted average of variable length n-
gram matches between system translations and a 
set of human reference translations and showed 
that the weighted average metric correlating highly 
with human assessments.  
BLEU measures how well a machine translation 
overlaps with multiple human translations using n-
gram co-occurrence statistics. N-gram precision in 
BLEU is computed as follows: 
 
? ?
? ?
? ??
? ??
?
?
=
}{
}{
)(
)(
CandidatesC Cgramn
CandidatesC Cgramn
clip
n gramnCount
gramnCount
p  (1) 
 
Where Countclip(n-gram) is the maximum num-
ber of n-grams co-occurring in a candidate transla-
tion and a reference translation, and Count(n-
gram) is the number of n-grams in the candidate 
translation. To prevent very short translations that 
try to maximize their precision scores, BLEU adds a 
brevity penalty, BP, to the formula: 
 
)2(
1
|)|/||1( ??
?
??
?
?
>
=
? rcife
rcif
BP cr  
 
Where |c| is the length of the candidate transla-
tion and |r| is the length of the reference transla-
tion. The BLEU formula is then written as follows: 
 
)3(logexp
1
??
???
?
?= ?
=
N
n
nn pwBPBLEU  
 
The weighting factor, wn, is set at 1/N. 
Although BLEU has been shown to correlate well 
with human assessments, it has a few things that 
can be improved. First the subjective application of 
the brevity penalty can be replaced with a recall 
related parameter that is sensitive to reference 
length. Although brevity penalty will penalize can-
didate translations with low recall by a factor of e(1-
|r|/|c|), it would be nice if we can use the traditional 
recall measure that has been a well known measure 
in NLP as suggested by Melamed (2003). Of 
course we have to make sure the resulting compos-
ite function of precision and recall is still correlates 
highly with human judgments. 
Second, although BLEU uses high order n-gram 
(n>1) matches to favor candidate sentences with 
consecutive word matches and to estimate their 
fluency, it does not consider sentence level struc-
ture. For example, given the following sentences: 
 
S1. police killed the gunman 
S2. police kill the gunman1 
S3. the gunman kill police 
 
We only consider BLEU with unigram and bi-
gram, i.e. N=2, for the purpose of explanation and 
call this BLEU-2. Using S1 as the reference and S2 
and S3 as the candidate translations, S2 and S3 
would have the same BLEU-2 score, since they 
both have one bigram and three unigram matches2. 
However, S2 and S3 have very different meanings. 
Third, BLEU is a geometric mean of unigram to 
N-gram precisions. Any candidate translation 
without a N-gram match has a per-sentence BLEU 
score of zero. Although BLEU is usually calculated 
over the whole test corpus, it is still desirable to 
have a measure that works reliably at sentence 
level for diagnostic and introspection purpose. 
To address these issues, we propose three new 
automatic evaluation measures based on longest 
common subsequence statistics and skip bigram 
co-occurrence statistics in the following sections. 
3 Longest Common Subsequence 
3.1 ROUGE-L 
A sequence Z = [z1, z2, ..., zn] is a subsequence of 
another sequence X = [x1, x2, ..., xm], if there exists 
a strict increasing sequence [i1, i2, ..., ik] of indices 
of X such that for all j = 1, 2, ..., k, we have xij = zj  
(Cormen et al 1989). Given two sequences X and 
Y, the longest common subsequence (LCS) of X 
and Y is a common subsequence with maximum 
length. We can find the LCS of two sequences of 
length m and n using standard dynamic program-
ming technique in O(mn) time. 
LCS has been used to identify cognate candi-
dates during construction of N-best translation 
lexicons from parallel text. Melamed (1995) used 
the ratio (LCSR) between the length of the LCS of 
two words and the length of the longer word of the 
two words to measure the cognateness between 
them. He used as an approximate string matching 
algorithm. Saggion et al (2002) used normalized 
pairwise LCS (NP-LCS) to compare similarity be-
tween two texts in automatic summarization 
evaluation. NP-LCS can be shown as a special case 
of Equation (6) with ? = 1. However, they did not 
provide the correlation analysis of NP-LCS with 
                                                     
1 This is a real machine translation output. 
2 The ?kill? in S2 or S3 does not match with ?killed? in 
S1 in strict word-to-word comparison.  
human judgments and its effectiveness as an auto-
matic evaluation measure. 
To apply LCS in machine translation evaluation, 
we view a translation as a sequence of words. The 
intuition is that the longer the LCS of two transla-
tions is, the more similar the two translations are. 
We propose using LCS-based F-measure to esti-
mate the similarity between two translations X of 
length m and Y of length n, assuming X is a refer-
ence translation and Y is a candidate translation, as 
follows: 
Rlcs 
m
YXLCS ),(
=       (4) 
Plcs 
n
YXLCS ),(
=       (5) 
Flcs
lcslcs
lcslcs
PR
PR
2
2 )1(
?
?
+
+
=   (6) 
 
Where LCS(X,Y) is the length of a longest common 
subsequence of X and Y, and ? = Plcs/Rlcs when 
?Flcs/?Rlcs_=_?Flcs/?Plcs. We call the LCS-based F-
measure, i.e. Equation 6, ROUGE-L. Notice that 
ROUGE-L is 1 when X = Y since LCS(X,Y) = m or 
n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. 
there is nothing in common between X and Y. F-
measure or its equivalents has been shown to have 
met several theoretical criteria in measuring accu-
racy involving more than one factor (Van Rijsber-
gen 1979). The composite factors are LCS-based 
recall and precision in this case. Melamed et al 
(2003) used unigram F-measure to estimate ma-
chine translation quality and showed that unigram 
F-measure was as good as BLEU.  
One advantage of using LCS is that it does not 
require consecutive matches but in-sequence 
matches that reflect sentence level word order as n-
grams. The other advantage is that it automatically 
includes longest in-sequence common n-grams, 
therefore no predefined n-gram length is necessary.  
ROUGE-L as defined in Equation 6 has the prop-
erty that its value is less than or equal to the mini-
mum of unigram F-measure of X and Y. Unigram 
recall reflects the proportion of words in X (refer-
ence translation) that are also present in Y (candi-
date translation); while unigram precision is the 
proportion of words in Y that are also in X. Uni-
gram recall and precision count all co-occurring 
words regardless their orders; while ROUGE-L 
counts only in-sequence co-occurrences.  
By only awarding credit to in-sequence unigram 
matches, ROUGE-L also captures sentence level 
structure in a natural way. Consider again the ex-
ample given in Section 2 that is copied here for 
convenience: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
 
As we have shown earlier, BLEU-2 cannot differ-
entiate S2 from S3. However, S2 has a ROUGE-L 
score of 3/4 = 0.75 and S3 has a ROUGE-L score 
of 2/4 = 0.5, with ? = 1. Therefore S2 is better than 
S3 according to ROUGE-L. This example also il-
lustrated that ROUGE-L can work reliably at sen-
tence level. 
However, LCS only counts the main in-sequence 
words; therefore, other longest common subse-
quences and shorter sequences are not reflected in 
the final score. For example, consider the follow-
ing candidate sentence: 
 
S4. the gunman police killed 
 
Using S1 as its reference, LCS counts either ?the 
gunman? or ?police killed?, but not both; therefore, 
S4 has the same ROUGE-L score as S3. BLEU-2 
would prefer S4 over S3. In Section 4, we will in-
troduce skip-bigram co-occurrence statistics that 
do not have this problem while still keeping the 
advantage of in-sequence (not necessary consecu-
tive) matching that reflects sentence level word 
order. 
3.2 Multiple References 
So far, we only demonstrated how to compute 
ROUGE-L using a single reference. When multiple 
references are used, we take the maximum LCS 
matches between a candidate translation, c, of n 
words and a set of u reference translations of  mj 
words. The LCS-based F-measure can be 
computed as follows: 
Rlcs-multi ???
?
???
?
=
=
j
ju
j m
crLCS ),(
max 1       (7) 
Plcs-multi ???
?
???
?
=
= n
crLCS ju
j
),(
max 1       (8) 
Flcs-multi  
multilcsmultilcs
multilcsmultilcs
PR
PR
??
??
+
+
= 2
2 )1(
?
?
 (9) 
 
where ? = Plcs-multi/Rlcs-multi when ?Flcs-multi/?Rlcs-
multi_=_?Flcs-multi/?Plcs-multi. 
 
This procedure is also applied to computation of 
ROUGE-S when multiple references are used. In 
the next section, we introduce the skip-bigram co-
occurrence statistics. In the next section, we de-
scribe how to extend ROUGE-L to assign more 
credits to longest common subsequences with con-
secutive words. 
3.3 ROUGE-W: Weighted Longest Common 
Subsequence 
LCS has many nice properties as we have de-
scribed in the previous sections. Unfortunately, the 
basic LCS also has a problem that it does not dif-
ferentiate LCSes of different spatial relations 
within their embedding sequences. For example, 
given a reference sequence X and two candidate 
sequences Y1 and Y2 as follows: 
 
X:  [A B C D E F G] 
Y1: [A B C D H I K] 
Y2:  [A H B K C I D] 
 
Y1 and Y2 have the same ROUGE-L score. How-
ever, in this case, Y1 should be the better choice 
than Y2 because Y1 has consecutive matches. To 
improve the basic LCS method, we can simply re-
member the length of consecutive matches encoun-
tered so far to a regular two dimensional dynamic 
program table computing LCS. We call this 
weighted LCS (WLCS) and use k to indicate the 
length of the current consecutive matches ending at 
words xi and yj. Given two sentences X and Y, the 
WLCS score of X and Y can be computed using the 
following dynamic programming procedure: 
 
(1) For (i = 0; i <=m; i++) 
        c(i,j) = 0  // initialize c-table 
        w(i,j) = 0 // initialize w-table 
(2) For (i = 1; i <= m; i++) 
        For (j = 1; j <= n; j++) 
          If xi = yj Then 
     // the length of consecutive matches at 
     // position i-1 and j-1 
     k = w(i-1,j-1) 
     c(i,j) = c(i-1,j-1) + f(k+1) ? f(k) 
     // remember the length of consecutive 
     // matches at position i, j 
     w(i,j) = k+1 
          Otherwise 
     If c(i-1,j) > c(i,j-1) Then 
    c(i,j) = c(i-1,j) 
    w(i,j) = 0           // no match at i, j 
     Else c(i,j) = c(i,j-1) 
     w(i,j) = 0           // no match at i, j 
(3) WLCS(X,Y) = c(m,n) 
 
Where c is the dynamic programming table, c(i,j) 
stores the WLCS score ending at word xi of X and 
yj of Y, w is the table storing the length of consecu-
tive matches ended at c table position i and j, and f 
is a function of consecutive matches at the table 
position, c(i,j). Notice that by providing different 
weighting function f, we can parameterize the 
WLCS algorithm to assign different credit to con-
secutive in-sequence matches.  
The weighting function f must have the property 
that f(x+y) > f(x) + f(y) for any positive integers x 
and y. In other words, consecutive matches are 
awarded more scores than non-consecutive 
matches. For example, f(k)-=-?k ? ? when k >= 0, 
and ?, ? > 0. This function charges a gap penalty 
of ?? for each non-consecutive n-gram sequences. 
Another possible function family is the polynomial 
family of the form k? where -? > 1. However, in 
order to normalize the final ROUGE-W score, we 
also prefer to have a function that has a close form 
inverse function. For example, f(k)-=-k2 has a close 
form inverse function f -1(k)-=-k1/2. F-measure 
based on WLCS can be computed as follows, 
given two sequences X of length m and Y of length 
n: 
Rwlcs ???
?
???
?
=
?
)(
),(1
mf
YXWLCSf       (10) 
Pwlcs ???
?
???
?
=
?
)(
),(1
nf
YXWLCSf       (11) 
Fwlcs  
wlcswlcs
wlcswlcs
PR
PR
2
2 )1(
?
?
+
+
=           (12) 
 
Where f -1 is the inverse function of f. We call the 
WLCS-based F-measure, i.e. Equation 12, 
ROUGE-W. Using Equation 12 and f(k)-=-k2 as the 
weighting function, the ROUGE-W scores for se-
quences Y1 and Y2 are 0.571 and 0.286 respec-
tively. Therefore, Y1 would be ranked higher than 
Y2 using WLCS. We use the polynomial function 
of the form k? in the ROUGE evaluation package. In 
the next section, we introduce the skip-bigram co-
occurrence statistics. 
4 ROUGE-S: Skip-Bigram Co-Occurrence 
Statistics 
Skip-bigram is any pair of words in their sen-
tence order, allowing for arbitrary gaps. Skip-
bigram co-occurrence statistics measure the over-
lap of skip-bigrams between a candidate translation 
and a set of reference translations. Using the ex-
ample given in Section 3.1: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
S4. the gunman police killed 
 
Each sentence has C(4,2)3 = 6 skip-bigrams. For 
example, S1 has the following skip-bigrams: 
 
                                                     
3 Combination: C(4,2) = 4!/(2!*2!) = 6. 
(?police killed?, ?police the?, ?police gunman?, 
?killed the?, ?killed gunman?, ?the gunman?)  
 
S2 has three skip-bigram matches with S1 (?po-
lice the?, ?police gunman?, ?the gunman?), S3 has 
one skip-bigram match with S1 (?the gunman?), 
and S4 has two skip-bigram matches with S1 (?po-
lice killed?, ?the gunman?).  Given translations X 
of length m and Y of length n, assuming X is a ref-
erence translation and Y is a candidate translation, 
we compute skip-bigram-based F-measure as fol-
lows: 
 
Rskip2 )2,(
),(2
mC
YXSKIP
=           (13) 
Pskip2 )2,(
),(2
nC
YXSKIP
=           (14) 
Fskip2 
2
2
2
22
2 )1(
skipskip
skipskip
PR
PR
?
?
+
+
=   (15) 
 
Where SKIP2(X,Y) is the number of skip-bigram 
matches between X and Y, ? = Pskip2/Rskip2 when 
?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and  C is the combi-
nation function. We call the skip-bigram-based F-
measure, i.e. Equation 15, ROUGE-S. 
Using Equation 15 with ? = 1 and S1 as the ref-
erence, S2?s ROUGE-S score is 0.5, S3 is 0.167, 
and S4 is 0.333. Therefore, S2 is better than S3 and 
S4, and S4 is better than S3. This result is more 
intuitive than using BLEU-2 and ROUGE-L. One 
advantage of skip-bigram vs. BLEU is that it does 
not require consecutive matches but is still sensi-
tive to word order. Comparing skip-bigram with 
LCS, skip-bigram counts all in-order matching 
word pairs while LCS only counts one longest 
common subsequence. 
We can limit the maximum skip distance, dskip, 
between two in-order words that is allowed to form 
a skip-bigram. Applying such constraint, we limit 
skip-bigram formation to a fix window size. There-
fore, computation time can be reduced and hope-
fully performance can be as good as the version 
without such constraint. For example, if we set dskip 
to 0 then ROUGE-S is equivalent to bigram over-
lap. If we set dskip to 4 then only word pairs of at 
most 4 words apart can form skip-bigrams. 
Adjusting Equations 13, 14, and 15 to use maxi-
mum skip distance limit is straightforward: we 
only count the skip-bigram matches, SKIP2(X,Y), 
within the maximum skip distance and replace de-
nominators of Equations 13, C(m,2), and 14, 
C(n,2), with the actual numbers of within distance 
skip-bigrams from the reference and the candidate 
respectively.  
In the next section, we present the evaluations of 
ROUGE-L, ROUGE-S, and compare their per-
formance with other automatic evaluation meas-
ures. 
5 Evaluations 
One of the goals of developing automatic evalua-
tion measures is to replace labor-intensive human 
evaluations. Therefore the first criterion to assess 
the usefulness of an automatic evaluation measure 
is to show that it correlates highly with human 
judgments in different evaluation settings. How-
ever, high quality large-scale human judgments are 
hard to come by. Fortunately, we have access to 
eight MT systems? outputs, their human assess-
ment data, and the reference translations from 2003 
NIST Chinese MT evaluation (NIST 2002a). There 
were 919 sentence segments in the corpus. We first 
computed averages of the adequacy and fluency 
scores of each system assigned by human evalua-
tors. For the input of automatic evaluation meth-
ods, we created three evaluation sets from the MT 
outputs: 
1. Case set: The original system outputs with 
case information. 
2. NoCase set: All words were converted 
into lower case, i.e. no case information 
was used. This set was used to examine 
whether human assessments were affected 
by case information since not all MT sys-
tems generate properly cased output. 
3. Stem set: All words were converted into 
lower case and stemmed using the Porter 
stemmer (Porter 1980). Since ROUGE 
computed similarity on surface word 
level, stemmed version allowed ROUGE 
to perform more lenient matches. 
To accommodate multiple references, we use a 
Jackknifing procedure. Given N references, we 
compute the best score over N sets of N-1 refer-
ences. The final score is the average of the N best 
scores using N different sets of N-1 references.  
The Jackknifing procedure is adopted since we 
often need to compare system and human perform-
ance and the reference translations are usually the 
only human translations available. Using this pro-
cedure, we are able to estimate average human per-
formance by averaging N best scores of one 
reference vs. the rest N-1 references.  
We then computed average BLEU1-12 4 , GTM 
with exponents of 1.0, 2.0, and 3.0, NIST, WER, 
and PER scores over these three sets. Finally we 
applied ROUGE-L, ROUGE-W with weighting 
function k1.2, and ROUGE-S without skip distance 
                                                     
4 BLEUN computes BLEU over n-grams up to length N. 
Only BLEU1, BLEU4, and BLEU12 are shown in Table 1.  
limit and with skip distant limits of 0, 4, and 9. 
Correlation analysis based on two different correla-
tion statistics, Pearson?s ? and Spearman?s ?, with 
respect to adequacy and fluency are shown in Ta-
ble 1.  
The Pearson?s correlation coefficient5 measures the 
strength and direction of a linear relationship be-
tween any two variables, i.e. automatic metric 
score and human assigned mean coverage score in 
our case. It ranges from +1 to -1. A correlation of 1 
means that there is a perfect positive linear rela-
tionship between the two variables, a correlation of 
-1 means that there is a perfect negative linear rela-
tionship between them, and  a correlation of 0 
means that there is no linear relationship between 
them. Since we would like to use automatic 
evaluation metric not only in comparing systems 
                                                     
5 For a quick overview of the Pearson?s coefficient, see: 
http://davidmlane.com/hyperstat/A34739.html. 
but also in in-house system development, a good 
linear correlation with human judgment would en-
able us to use automatic scores to predict corre-
sponding human judgment scores. Therefore, 
Pearson?s correlation coefficient is a good measure 
to look at. 
Spearman?s correlation coefficient 6  is also a 
measure of correlation between two variables. It is 
a non-parametric measure and is a special case of 
the Pearson?s correlation coefficient when the val-
ues of data are converted into ranks before comput-
ing the coefficient. Spearman?s correlation 
coefficient does not assume the correlation be-
tween the variables is linear. Therefore it is a use-
ful correlation indicator even when good linear 
correlation, for example, according to Pearson?s 
correlation coefficient between two variables could 
                                                     
6 For a quick overview of the Spearman?s coefficient, see: 
http://davidmlane.com/hyperstat/A62436.html. 
Adequacy
Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U
BLEU1 0.86 0.83 0.89 0.80 0.71 0.90 0.87 0.84 0.90 0.76 0.67 0.89 0.91 0.89 0.93 0.85 0.76 0.95
BLEU4 0.77 0.72 0.81 0.77 0.71 0.89 0.79 0.75 0.82 0.67 0.55 0.83 0.82 0.78 0.85 0.76 0.67 0.89
BLEU12 0.66 0.60 0.72 0.53 0.44 0.65 0.72 0.57 0.81 0.65 0.25 0.88 0.72 0.58 0.81 0.66 0.28 0.88
NIST 0.89 0.86 0.92 0.78 0.71 0.89 0.87 0.85 0.90 0.80 0.74 0.92 0.90 0.88 0.93 0.88 0.83 0.97
WER 0.47 0.41 0.53 0.56 0.45 0.74 0.43 0.37 0.49 0.66 0.60 0.82 0.48 0.42 0.54 0.66 0.60 0.81
PER 0.67 0.62 0.72 0.56 0.48 0.75 0.63 0.58 0.68 0.67 0.60 0.83 0.72 0.68 0.76 0.69 0.62 0.86
ROUGE-L 0.87 0.84 0.90 0.84 0.79 0.93 0.89 0.86 0.92 0.84 0.71 0.94 0.92 0.90 0.94 0.87 0.76 0.95
ROUGE-W 0.84 0.81 0.87 0.83 0.74 0.90 0.85 0.82 0.88 0.77 0.67 0.90 0.89 0.86 0.91 0.86 0.76 0.95
ROUGE-S* 0.85 0.81 0.88 0.83 0.76 0.90 0.90 0.88 0.93 0.82 0.70 0.92 0.95 0.93 0.97 0.85 0.76 0.94
ROUGE-S0 0.82 0.78 0.85 0.82 0.71 0.90 0.84 0.81 0.87 0.76 0.67 0.90 0.87 0.84 0.90 0.82 0.68 0.90
ROUGE-S4 0.82 0.78 0.85 0.84 0.79 0.93 0.87 0.85 0.90 0.83 0.71 0.90 0.92 0.90 0.94 0.84 0.74 0.93
ROUGE-S9 0.84 0.80 0.87 0.84 0.79 0.92 0.89 0.86 0.92 0.84 0.76 0.93 0.94 0.92 0.96 0.84 0.76 0.94
GTM10 0.82 0.79 0.85 0.79 0.74 0.83 0.91 0.89 0.94 0.84 0.79 0.93 0.94 0.92 0.96 0.84 0.79 0.92
GTM20 0.77 0.73 0.81 0.76 0.69 0.88 0.79 0.76 0.83 0.70 0.55 0.83 0.83 0.79 0.86 0.80 0.67 0.90
GTM30 0.74 0.70 0.78 0.73 0.60 0.86 0.74 0.70 0.78 0.63 0.52 0.79 0.77 0.73 0.81 0.64 0.52 0.80
Fluency
Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U
BLEU1 0.81 0.75 0.86 0.76 0.62 0.90 0.73 0.67 0.79 0.70 0.62 0.81 0.70 0.63 0.77 0.79 0.67 0.90
BLEU4 0.86 0.81 0.90 0.74 0.62 0.86 0.83 0.78 0.88 0.68 0.60 0.81 0.83 0.78 0.88 0.70 0.62 0.81
BLEU12 0.87 0.76 0.93 0.66 0.33 0.79 0.93 0.81 0.97 0.78 0.44 0.94 0.93 0.84 0.97 0.80 0.49 0.94
NIST 0.81 0.75 0.87 0.74 0.62 0.86 0.70 0.64 0.77 0.68 0.60 0.79 0.68 0.61 0.75 0.77 0.67 0.88
WER 0.69 0.62 0.75 0.68 0.57 0.85 0.59 0.51 0.66 0.70 0.57 0.82 0.60 0.52 0.68 0.69 0.57 0.81
PER 0.79 0.74 0.85 0.67 0.57 0.82 0.68 0.60 0.73 0.69 0.60 0.81 0.70 0.63 0.76 0.65 0.57 0.79
ROUGE-L 0.83 0.77 0.88 0.80 0.67 0.90 0.76 0.69 0.82 0.79 0.64 0.90 0.73 0.66 0.80 0.78 0.67 0.90
ROUGE-W 0.85 0.80 0.90 0.79 0.63 0.90 0.78 0.73 0.84 0.72 0.62 0.83 0.77 0.71 0.83 0.78 0.67 0.90
ROUGE-S* 0.84 0.78 0.89 0.79 0.62 0.90 0.80 0.74 0.86 0.77 0.64 0.90 0.78 0.71 0.84 0.79 0.69 0.90
ROUGE-S0 0.87 0.81 0.91 0.78 0.62 0.90 0.83 0.78 0.88 0.71 0.62 0.82 0.82 0.77 0.88 0.76 0.62 0.90
ROUGE-S4 0.84 0.79 0.89 0.80 0.67 0.90 0.82 0.77 0.87 0.78 0.64 0.90 0.81 0.75 0.86 0.79 0.67 0.90
ROUGE-S9 0.84 0.79 0.89 0.80 0.67 0.90 0.81 0.76 0.87 0.79 0.69 0.90 0.79 0.73 0.85 0.79 0.69 0.90
GTM10 0.73 0.66 0.79 0.76 0.60 0.87 0.71 0.64 0.78 0.80 0.67 0.90 0.66 0.58 0.74 0.80 0.64 0.90
GTM20 0.86 0.81 0.90 0.80 0.67 0.90 0.83 0.77 0.88 0.69 0.62 0.81 0.83 0.77 0.87 0.74 0.62 0.89
GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83
With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem)
With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem)
Table 1. Pearson?s ? and Spearman?s ? correlations of automatic evaluation measures vs. adequacy
and fluency: BLEU1, 4, and 12 are BLEU with maximum of 1, 4, and 12 grams, NIST is the NIST 
score, ROUGE-L is LCS-based F-measure (? = 1), ROUGE-W is weighted LCS-based  F-measure (? 
= 1). ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGE-
SN is skip-bigram-based F-measure (? = 1) with maximum skip distance of N, PER is position inde-
pendent word error rate, and WER is word error rate. GTM 10, 20, and 30 are general text matcher 
with exponents of 1.0, 2.0, and 3.0. (Note, only BLEU1, 4, and 12 are shown here to preserve space.) 
 
not be found. It also suits the NIST MT evaluation 
scenario where multiple systems are ranked ac-
cording to some performance metrics. 
To estimate the significance of these correlation 
statistics, we applied bootstrap resampling, gener-
ating random samples of the 919 different sentence 
segments. The lower and upper values of 95% con-
fidence interval are also shown in the table. Dark 
(green) cells are the best correlation numbers in 
their categories and light gray cells are statistically 
equivalent to the best numbers in their categories. 
Analyzing all runs according to the adequacy and 
fluency table, we make the following observations: 
Applying the stemmer achieves higher correla-
tion with adequacy but keeping case information 
achieves higher correlation with fluency except for 
BLEU7-12 (only BLEU12 is shown). For example, 
the Pearson?s ? (P) correlation of ROUGE-S* with 
adequacy increases from 0.85 (Case) to 0.95 
(Stem) while its Pearson?s ? correlation with flu-
ency drops from 0.84 (Case) to 0.78 (Stem). We 
will focus our discussions on the Stem set in ade-
quacy and Case set in fluency. 
The Pearson's ? correlation values in the Stem 
set of the Adequacy Table, indicates that ROUGE-
L and ROUGE-S with a skip distance longer than 0 
correlate highly and linearly with adequacy and 
outperform BLEU and NIST. ROUGE-S* achieves 
that best correlation with a Pearson?s ? of 0.95. 
Measures favoring consecutive matches, i.e. 
BLEU4 and 12, ROUGE-W, GTM20 and 30, 
ROUGE-S0 (bigram), and WER have lower Pear-
son?s ?. Among them WER (0.48) that tends to 
penalize small word movement is the worst per-
former. One interesting observation is that longer 
BLEU has lower correlation with adequacy. 
Spearman?s ? values generally agree with Pear-
son's ? but have more equivalents. 
The Pearson's ? correlation values in the Stem 
set of the Fluency Table, indicates that BLEU12 has 
the highest correlation (0.93) with fluency. How-
ever, it is statistically indistinguishable with 95% 
confidence from all other metrics shown in the 
Case set of the Fluency Table except for WER and 
GTM10.  
GTM10 has good correlation with human judg-
ments in adequacy but not fluency; while GTM20 
and GTM30, i.e. GTM with exponent larger than 
1.0, has good correlation with human judgment in 
fluency but not adequacy. 
ROUGE-L and ROUGE-S*, 4, and 9 are good 
automatic evaluation metric candidates since they 
perform as well as BLEU in fluency correlation 
analysis and outperform BLEU4 and 12 signifi-
cantly in adequacy. Among them, ROUGE-L is the 
best metric in both adequacy and fluency correla-
tion with human judgment according to Spear-
man?s correlation coefficient and is statistically 
indistinguishable from the best metrics in both 
adequacy and fluency correlation with human 
judgment according to Pearson?s correlation coef-
ficient. 
6 Conclusion 
In this paper we presented two new objective 
automatic evaluation methods for machine transla-
tion, ROUGE-L based on longest common subse-
quence (LCS) statistics between a candidate 
translation and a set of reference translations. 
Longest common subsequence takes into account 
sentence level structure similarity naturally and 
identifies longest co-occurring in-sequence n-
grams automatically while this is a free parameter 
in BLEU.   
To give proper credit to shorter common se-
quences that are ignored by LCS but still retain the 
flexibility of non-consecutive matches, we pro-
posed counting skip bigram co-occurrence. The 
skip-bigram-based ROUGE-S* (without skip dis-
tance restriction) had the best Pearson's ? correla-
tion of 0.95 in adequacy when all words were 
lower case and stemmed. ROUGE-L, ROUGE-W, 
ROUGE-S*, ROUGE-S4, and ROUGE-S9 were 
equal performers to BLEU in measuring fluency. 
However, they have the advantage that we can ap-
ply them on sentence level while longer BLEU such 
as BLEU12 would not differentiate any sentences 
with length shorter than 12 words (i.e. no 12-gram 
matches). We plan to explore their correlation with 
human judgments on sentence-level in the future. 
We also confirmed empirically that adequacy and 
fluency focused on different aspects of machine 
translations. Adequacy placed more emphasis on 
terms co-occurred in candidate and reference trans-
lations as shown in the higher correlations in Stem 
set than Case set in Table 1; while the reverse was 
true in the terms of fluency. 
The evaluation results of ROUGE-L, ROUGE-
W, and ROUGE-S in machine translation evalua-
tion are very encouraging. However, these meas-
ures in their current forms are still only applying 
string-to-string matching. We have shown that bet-
ter correlation with adequacy can be reached by 
applying stemmer. In the next step, we plan to ex-
tend them to accommodate synonyms and para-
phrases. For example, we can use an existing 
thesaurus such as WordNet (Miller 1990) or creat-
ing a customized one by applying automated syno-
nym set discovery methods (Pantel and Lin 2002) 
to identify potential synonyms. Paraphrases can 
also be automatically acquired using statistical 
methods as shown by Barzilay and Lee (2003). 
Once we have acquired synonym and paraphrase 
data, we then need to design a soft matching func-
tion that assigns partial credits to these approxi-
mate matches. In this scenario, statistically 
generated data has the advantage of being able to 
provide scores reflecting the strength of similarity 
between synonyms and paraphrased.  
ROUGE-L, ROUGE-W, and ROUGE-S have 
also been applied in automatic evaluation of sum-
marization and achieved very promising results 
(Lin 2004). In Lin and Och (2004), we proposed a 
framework that automatically evaluated automatic 
MT evaluation metrics using only manual transla-
tions without further human involvement. Accord-
ing to the results reported in that paper, ROUGE-L, 
ROUGE-W, and ROUGE-S also outperformed 
BLEU and NIST. 
References 
Akiba, Y., K. Imamura, and E. Sumita. 2001. Us-
ing Multiple Edit Distances to Automatically 
Rank Machine Translation Output. In Proceed-
ings of the MT Summit VIII, Santiago de Com-
postela, Spain. 
Barzilay, R. and L. Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Mul-
tiple-Sequence Alignmen. In Proceeding of 
NAACL-HLT 2003, Edmonton, Canada. 
Leusch, G., N. Ueffing, and H. Ney. 2003. A 
Novel String-to-String Distance Measure with 
Applications to Machine Translation Evaluation. 
In Proceedings of MT Summit IX, New Orleans, 
U.S.A. 
Levenshtein, V. I. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. 
Soviet Physics Doklady. 
Lin, C.Y. 2004. ROUGE: A Package for Automatic 
Evaluation of Summaries. In Proceedings of the 
Workshop on Text Summarization Branches 
Out, post-conference workshop of ACL 2004, 
Barcelona, Spain. 
Lin, C.-Y. and F. J. Och. 2004. ORANGE: a Method 
for Evaluating Automatic Evaluation Metrics for 
Machine Translation. In Proceedings of 20th In-
ternational Conference on Computational Lin-
guistic (COLING 2004), Geneva, Switzerland. 
Miller, G. 1990. WordNet: An Online Lexical Da-
tabase. International Journal of Lexicography, 
3(4). 
Melamed, I.D. 1995. Automatic Evaluation and 
Uniform Filter Cascades for Inducing N-best 
Translation Lexicons. In Proceedings of the 3rd 
Workshop on Very Large Corpora (WVLC3). 
Boston, U.S.A. 
Melamed, I.D., R. Green and J. P. Turian. 2003. 
Precision and Recall of Machine Translation. In 
Proceedings of NAACL/HLT 2003, Edmonton, 
Canada. 
Nie?en S., F.J. Och, G, Leusch, H. Ney. 2000. An 
Evaluation Tool for Machine Translation: Fast 
Evaluation for MT Research. In Proceedings of 
the 2nd International Conference on Language 
Resources and Evaluation, Athens, Greece. 
NIST. 2002. Automatic Evaluation of Machine 
Translation Quality using N-gram Co-
Occurrence Statistics.   AAAAAAAAAAA 
http://www.nist.gov/speech/tests/mt/doc/ngram-
study.pdf 
Pantel, P. and Lin, D. 2002. Discovering Word 
Senses from Text. In Proceedings of SIGKDD-
02. Edmonton, Canada. 
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 
2001. BLEU: a Method for Automatic Evaluation 
of Machine Translation. IBM Research Report 
RC22176 (W0109-022). 
Porter, M.F. 1980. An Algorithm for Suffix Strip-
ping. Program, 14, pp. 130-137. 
Saggion H., D. Radev, S. Teufel, and W. Lam. 
2002. Meta-Evaluation of Summaries in a 
Cross-Lingual Environment Using Content-
Based Metrics. In Proceedings of COLING-
2002, Taipei, Taiwan. 
Su, K.-Y., M.-W. Wu, and J.-S. Chang. 1992. A 
New Quantitative Quality Measure for Machine 
Translation System. In Proceedings of 
COLING-92, Nantes, France. 
Thompson, H. S. 1991. Automatic Evaluation of 
Translation Quality: Outline of Methodology 
and Report on Pilot Experiment. In Proceedings 
of the Evaluator?s Forum, ISSCO, Geneva, 
Switzerland. 
Turian, J. P., L. Shen, and I. D. Melamed. 2003. 
Evaluation of Machine Translation and its 
Evaluation. In Proceedings of MT Summit IX, 
New Orleans, U.S.A. 
Van Rijsbergen, C.J. 1979. Information Retrieval. 
Butterworths. London. 
 Manual and Automatic Evaluation of Summaries 
Chin-Yew Lin and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292  
+1-310-448-8711/8731 
{cyl,hovy}@isi.edu 
 
Abstract 
In this paper we discuss manual and 
automatic evaluations of summaries using 
data from the Document Understanding 
Conference 2001 (DUC-2001).  We first 
show the instability of the manual 
evaluation. Specifically, the low inter-
human agreement indicates that more 
reference summaries are needed. To 
investigate the feasibility of automated 
summary evaluation based on the recent 
BLEU method from machine translation, we 
use accumulative n-gram overlap scores 
between system and human summaries.  The 
initial results provide encouraging 
correlations with human judgments, based 
on the Spearman rank-order correlation 
coefficient.  However, relative ranking of 
systems needs to take into account the 
instability.   
1 Introduction 
Previous efforts in large-scale evaluation of text 
summarization include TIPSTER SUMMAC 
(Mani et al 1998) and the Document 
Understanding Conference (DUC) sponsored by 
the National Institute of Standards and 
Technology (NIST).  DUC aims to compile 
standard training and test collections that can be 
shared among researchers and to provide 
common and large scale evaluations in single 
and multiple document summarization for their 
participants. 
In this paper we discuss manual and automatic 
evaluations of summaries using data from the 
Document Understanding Conference 2001 
(DUC-2001).  Section 2 gives a brief overview 
of the evaluation procedure used in DUC-2001 
and the Summary Evaluation Environment 
(SEE) interface used to support the DUC-2001 
human evaluation protocol.  Section 3 discusses 
evaluation metrics.  Section 4 shows the 
instability of manual evaluations.  Section 5 
outlines a method of automatic summary 
evaluation using accumulative n-gram matching 
score (NAMS) and proposes a view that casts 
summary evaluation as a decision making 
process.  It shows that the NAMS method is 
bounded and in most cases not usable, given 
only a single reference summary to compare 
with.  Section 6 discusses why this is so, 
illustrating various forms of mismatching 
between human and system summaries.  We 
conclude with lessons learned and future 
directions. 
2 Document Understanding 
Conference (DUC) 
DUC2001 included three tasks: 
? Fully automatic single-document 
summarization: given a document, 
participants were required to create a 
generic 100-word summary.  The training 
set comprised 30 sets of approximately 10 
documents each, together with their 100-
word human written summaries.  The test 
set comprised 30 unseen documents. 
? Fully automatic multi-document 
summarization: given a set of documents 
about a single subject, participants were 
required to create 4 generic summaries of 
the entire set, containing 50, 100, 200, and 
400 words respectively.  The document sets 
were of four types: a single natural disaster 
event; a single event; multiple instances of a 
type of event; and information about an 
individual.  The training set comprised 30 
sets of approximately 10 documents, each 
provided with their 50, 100, 200, and 400-
word human written summaries.  The test 
set comprised 30 unseen sets. 
       Philadelphia, July 2002, pp. 45-51.  Association for Computational Linguistics.
          Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
 ? Exploratory summarization: participants 
were encouraged to investigate alternative 
approaches to evaluating summarization and 
report their results. 
A total of 11 systems participated in the single-
document summarization task and 12 systems 
participated in the multi-document task.   
The training data were distributed in early 
March of 2001 and the test data were distributed 
in mid-June of 2001.  Results were submitted to 
NIST for evaluation by July 1st 2001. 
2.1 Evaluation Materials 
For each document or document set, one human 
summary was created as the ?ideal? model 
summary at each specified length.  Two other 
human summaries were also created at each 
length.  In addition, baseline summaries were 
created automatically for each length as 
reference points.  For the multi-document 
summarization task, one baseline, lead baseline, 
took the first 50, 100, 200, and 400 words in the 
last document in the collection.  A second 
baseline, coverage baseline, took the first 
sentence in the first document, the first sentence 
in the second document and so on until it had a 
summary of 50, 100, 200, or 400 words. Only 
one baseline (baseline1) was created for the 
single document summarization task. 
2.2 Summary Evaluation Environment 
NIST assessors who created the ?ideal? written 
summaries did pairwise comparisons of their 
summaries to the system-generated summaries, 
other assessors? summaries, and baseline 
summaries.  They used the Summary Evaluation 
Environment (SEE) 2.0 developed by one of the 
authors (Lin 2001) to support the process.  
Using SEE, the assessors compared the system?s 
text (the peer text) to the ideal (the model text).  
As shown in Figure 1, each text was 
decomposed into a list of units and displayed in 
separate windows.  In DUC-2001 the sentence 
was used as the smallest unit of evaluation. 
Figure 1. SEE in an evaluation session. 
 SEE 2.0 provides interfaces for assessors to 
judge both the content and the quality of 
summaries.  To measure content, assessors step 
through each model unit, mark all system units 
sharing content with the current model unit 
(shown in green highlight in the model summary 
window), and specify that the marked system 
units express all, most, some or hardly any of 
the content of the current model unit.  To 
measure quality, assessors rate grammaticality1, 
cohesion2, and coherence3 at five different 
levels: all, most, some, hardly any, or none.   
For example, as shown in Figure 1, an assessor 
marked system units 1.1 and 10.4 (shown in red 
underlines) as sharing some content with the 
current model unit 2.2 (highlighted green). 
3 Evaluation Metrics 
One goal of DUC-2001 was to debug the 
evaluation procedures and identify stable 
metrics that could serve as common reference 
points. NIST did not define any official 
performance metric in DUC-2001.  It released 
the raw evaluation results to DUC-2001 
participants and encouraged them to propose 
metrics that would help progress the field. 
3.1 Recall, Coverage, Retention and 
Weighted Retention 
Recall at different compression ratios has been 
used in summarization research to measure how 
well an automatic system retains important 
content of original documents (Mani and 
Maybury 1999).  Assume we have a system 
summary Ss and a model summary Sm.  The 
number of sentences occurring in Ss is Ns, the 
number of sentences in Sm is Nm, and the number 
in both Ss and Sm is Na.  Recall is defined as 
Na/Nm.  The Compression Ratio is defined as the 
length of a summary (by words or sentences) 
divided by the length of its original document.   
Applying this direct all-or-nothing recall in 
DUC-2001 without modification is not 
appropriate because: 
                                                     
1 Does the summary observe English grammatical 
rules independent of its content? 
2 Do sentences in the summary fit in with their 
surrounding sentences?  
3 Is the content of the summary expressed and 
organized in an effective way? 
1. Multiple system units contribute to multiple 
model units.  
2. Exact overlap between Ss and Sm rarely 
occurs.  
3. Overlap judgment is not binary.  
For example in Figure 1, an assessor judged 
system units 1.1 and 10.4 sharing some content 
with model unit 2.2.  Unit 1.1 says ?Thousands 
of people are feared dead? and unit 2.2 says 
?3,000 and perhaps ? 5,000 people have been 
killed?.  Are ?thousands? equivalent to ?3,000 to 
5,000? or not?  Unit 10.4 indicates it was an 
?earthquake of magnitude 6.9? and unit 2.2 says 
it was ?an earthquake measuring 6.9 on the 
Richter scale?.  Both of them report a ?6.9? 
earthquake.   But the second part of system unit 
10.4, ?in an area so isolated??, seems to share 
some content with model unit 4.4 ?the quake 
was centered in a remote mountainous area?.  
Are these two equivalent?  This example 
highlights the difficulty of judging the content 
coverage of system summaries against model 
summaries and the inadequacy of using simple 
recall as defined. 
For this reason, NIST assessors not only marked 
the segments shared between system units (SU) 
and model units (MU), they also indicated the 
degree of match, i.e., all, most, some, hardly 
any, or none.  This enables us to compute 
weighted recall.  
Different versions of weighted recall were 
proposed by DUC-2001 participants. (McKeown 
et al 2001) treated the completeness of coverage 
as a threshold: 4 for all, 3 for most and above, 2 
for some and above, and 1 for hardly any and 
above.  They then proceeded to compare system 
performances at different threshold levels.  They 
defined recall at threshold t, Recallt, as follows:  
summary model in the MUs ofnumber  Total
 aboveor at  marked MUs ofNumber t  
Instead of thresholds, we use here as coverage 
score the ratio of completeness of coverage C: 1 
for all, 3/4 for most, 1/2 for some, 1/4 for hardly 
any, and 0 for none.  To avoid confusion with 
the recall used in information retrieval, we call 
our metric weighted retention, Retentionw, and 
define it as follows: 
summary model in the MUs ofnumber  Total
  marked) MUs of(Number C?  
 If we ignore C (set it to 1), we obtain an 
unweighted retention, Retention1.  We used 
Retention1 in our evaluation to illustrate that 
relative system performance (i.e., system 
ranking) changes when different evaluation 
metrics are chosen.  Therefore, it is important to 
have common and agreed upon metrics to 
facilitate large scale evaluation efforts.  
4 Instability of Manual Judgments 
In the human evaluation protocol described in 
Section 2, nothing prevents an assessor from 
assigning different coverage scores to the same 
system units produced by different systems 
against the same model unit.  (Since most 
systems produce extracts, the same sentence 
may appear in many summaries, especially for 
single-document summaries.)  Analyzing the 
DUC-2001 results, we found the following: 
? Single document task  
o A total of 5,921 judgments  
o Among them, 1,076 (18%) contain 
multiple judgments for the same units  
o 143 (2.4%) of them have three different 
coverage scores  
? Multi-document task  
o A total of 6,963 judgments  
o Among them 528 (7.6%) contain multiple 
judgments  
o 27 (0.4%) of them have three different 
coverage scores  
Intuitively this is disturbing; the same phrase 
compared to the same model unit should always 
have the same score regardless of which system 
produced it.  The large percentage of multiple 
judgments found in the single document 
evaluation are test-retest errors that need to be 
addressed in computing performance metrics.   
Figure 2 and Figure 3 show the retention scores 
for systems participating in the single- and 
multi-document tasks respectively.  The error 
bars are bounded at the top by choosing the 
maximum coverage score (MAX) assigned by 
an assessor in the case of multiple judgment 
scores and at the bottom by taking the minimum 
assignment (MIN).  We also compute system 
39
.75
40
.95
33
.98
31
.32 31
.72
31
.15
36
.29
27
.93
34
.44
29
.90 30
.21
28
.02
30
.71
26
.03
39.57
40.88
30.12
28.09
28.26 27.71
33.01
24.46
34.44
26.02
27.40
25.54
28.66
23.90
39.67
40.90
32.24
29.89 30.21 29.61
34.78
26.38
34.44
28.25
28.99
27.02
29.76
25.12
20.00
25.00
30.00
35.00
40.00
45.00
Hu
ma
n1
Hu
ma
n2
Ba
se
line
1 O P Q R S T V W X Y Z
Systems
Re
te
nt
io
n
MAJORITY
ORIGINAL
MAX
MIN
AVG
Figure 2. DUC 2001 single document retention score distribution. 
 retentions using the majority (MAJORITY) and 
average (AVG) of assigned coverage scores.  
The original (ORIGINAL) does not consider the 
instability in the data.   
Analyzing all systems? results, we made the 
following observations.   
(1) Inter-human agreement is low in the single-
document task (~40%) and even lower in 
multi-documents task (~29%). This 
indicates that using a single model as 
reference summary is not adequate. 
(2) Despite the low inter-human agreement, 
human summaries are still much better than 
the best performing systems. 
(3) The relative performance (rankings) of 
systems changes when the instability of 
human judgment is considered.  However, 
the rerankings remain local; systems remain 
within performance groups. For example, 
we have the following groups in the multi-
document summarization task (Figure 3, 
considering 0.5% error): 
a. {Human1, Human2} 
b. {N, T, Y} 
c. {Baseline2, L, P} 
d. {S} 
e. {M, O, R} 
f. {Z} 
g. {Baseline1, U, W} 
The existence of stable performance regions is 
encouraging.  Still, given the large error bars, 
one can produce 162 different rankings of these 
16 systems.  Groups are less obvious in the 
single document summarization task due to 
close performance among systems.  
Table 1 shows relative performance between 
systems x and y in the single document 
Table 1. Pairwise relative system performance 
(single document summarization task). 
28
.55 29
.03
7.4
9
15
.42 15
.71
11
.56
17
.92
11
.39
16
.49
11
.56
13
.60
18
.63
6.6
1 6.9
0
17
.94
9.1
6
28.55
29.02
7.30
14.02
15.00
10.96
17.92
10.93
15.06
10.87
12.40
18.29
6.47
6.74
17.60
8.87
28.55
29.03
7.38
14.76
15.38
11.26
17.92
11.19
15.80
11.22
13.02
18.47
6.54 6.81
17.80
9.02
5.00
10.00
15.00
20.00
25.00
30.00
Hu
ma
n1
Hu
ma
n2
Ba
se
line
1
Ba
se
line
2 L M N O P R S T U W Y Z
Systems
Re
te
nt
io
n
MAJORITY
ORIGINAL
MAX
MIN
AVG
Figure 3. DUC 2001 multi-document retention score distribution. 
H1 H2 B1 O P Q R S T V W X Y Z
H1 = - + + + + + + + + + + + +
H2 + = + + + + + + + + + + + +
B1 - - = ~ ~ ~ ~ + - + ~ + ~ +
O - - ~ = ~ ~ - + - ~ ~ + ~ +
P - - ~ ~ = ~ - + - ~ ~ + ~ +
Q - - ~ ~ ~ = - ~ - ~ ~ ~ ~ +
R - - ~ + + + = + ~ + + + + +
S - - - - - ~ - = - ~ ~ ~ - ~
T - - + + + + ~ + = + + + + +
V - - - ~ ~ ~ - ~ - = ~ ~ ~ ~
W - - ~ ~ ~ ~ - ~ - ~ = ~ ~ +
X - - - - - ~ - ~ - ~ ~ = - ~
Y - - ~ ~ ~ ~ - + - ~ ~ + = +
Z - - - - - - - ~ - ~ - ~ - =
 summarization task. A ?+? indicates the 
minimum retention score of x (row) is higher 
than the maximum retention score of y 
(column), a ?-? indicates the maximum retention 
score of x is lower than the minimum retention 
score of y, and a ?~? means x and y are 
indistinguishable.  Table 2 shows relative 
system performance in the multi-document 
summarization task.  
Despite the instability of the manual evaluation, 
we discuss automatic summary evaluation in an 
attempt to approximate the human evaluation 
results in the next section. 
5 Automatic Summary Evaluation 
Inspired by recent progress in automatic 
evaluation of machine translation (BLEU; 
Papineni et al 2001), we would like to apply the 
same idea in the evaluation of summaries.  
Following BLEU, we used the automatically 
computed accumulative n-gram matching scores 
(NAMS) between a model unit (MU) and a 
system summary (S)4 as performance indicator, 
considering multi-document summaries.  Only 
content words were used in forming n-grams. 
NAMS is defined as follows:  
a1?NAM1 + a2?NAM2 + a3?NAM3 + a4?NAM4 
NAMn is n-gram hit ratio defined as: 
MUin  grams-n of # total
S and MUbetween  grams-n matched of #  
We tested three different configurations of ai: 
                                                     
4 The whole system summary was used to compute 
NAMS against a model unit. 
C1: a1 = 1 and a2 = a3 = a4 = 0; 
C2: a1 = 1/3, a2 = 2/3, and a3 = a4 = 0; 
C3: a1 = 1/6, a2 = 2/6, a3 = 3/6, and a4 = 0; 
C1 is simply unigram matching.  C2 and C3 
give more credit to longer n-gram matches.  To 
examine the effect of stemmers in helping the n-
gram matching, we also tested all configurations 
with two different stemmers (Lovin?s and 
Porter?s).  Figure 4 shows the results with and 
without using stemmers and their Spearman 
rank-order correlation coefficients (rho) 
compared against the original retention ranking 
from Figure 4.  X-nG is configuration n without 
using any stemmer, L-nG with the Lovin 
stemmer, and P-nG with the Porter stemmer. 
The results in Figure 4 indicate that unigram 
matching provides a good approximation, but 
the best correlation is achieved using C2 with 
the Porter stemmer.  Using stemmers did 
improve correlation.  Notice that rank inversion 
remains within the performance groups 
identified in Section 4.  For example, the 
retention ranking of Baseline1, U, and W is 14, 
16, and 15 respectively.  The P-2G ranking of 
these three systems is 15, 14, and 16.  The only 
system crossing performance groups is Y.  Y 
should be grouped with N and T but the 
automatic evaluations place it lower, in the 
group with Baseline2, L, and P.  The primary 
reason for Y?s behavior may be that its 
summaries consist mainly of headlines, whose 
abbreviated style differs from the language 
models derived from normal newspaper text.   
For comparison, we also ran IBM?s BLEU 
evaluation script5 over the same model and 
system summary set. The Spearman rank-order 
correlation coefficient (?) for the single 
document task is 0.66 using one reference 
summary and 0.82 using three reference 
summaries; while Spearman ? for the multi-
document task is 0.67 using one reference and 
0.70 using three.  
6 Conclusions 
We described manual and automatic evaluation 
of single and multi-document summarization in 
DUC-2001.  We showed the instability of 
                                                     
5 We thank Kishore Papineni for sending us BLEU 
1.0. 
Table 2. Pairwise relative system performance 
(multi-document summarization task). 
H1 H2 B1 B2 L M N O P R S T U W Y Z
H1 = - + + + + + + + + + + + + + +
H2 + = + + + + + + + + + + + + + +
B1 - - = - - - - - - - - - + + - -
B2 - - + = ~ + - + ~ + + - + + - +
L - - + ~ = + - + ~ + + - + + - +
M - - + - - = - ~ - ~ - - + + - +
N - - + + + + = + + + + - + + ~ +
O - - + - - ~ - = - ~ - - + + - +
P - - - ~ ~ + - + = + + - + + - +
R - - + - - ~ - ~ - = - - + + - +
S - - + - - + - + - + = - + + - +
T - - + + + + + + + + + = + + + +
U - - - - - - - - - - - - = - - -
W - - - - - - - - - - - - + = - -
Y - - + + + + ~ + + + + - + + = +
Z - - + - - - - - - - - - + + - =
 human evaluations and the need to consider this 
factor when comparing system performances.  
As we factored in the instability, systems tended 
to form separate performance groups.  One 
should treat with caution any interpretation of 
performance figures that ignores this instability.   
Automatic evaluation of summaries using 
accumulative n-gram matching scores seems 
promising.  System rankings using NAMS and 
retention ranking had a Spearman rank-order 
correlation coefficient above 97%.  Using 
stemmers improved the correlation. However, 
satisfactory correlation is still elusive. The main 
problem we ascribe to automated summary 
evaluation is the large expressive range of 
English since human summarizers tend to create 
fresh text.  No n-gram matching evaluation 
procedure can overcome the paraphrase or 
synonym problem unless (many) model 
summaries are available.   
We conclude the following:  
(1) We need more than one model summary 
although we cannot estimate how many 
model summaries are required to achieve 
reliable automated summary evaluation. 
(2) We need more than one evaluation for each 
summary against each model summary.  
(3) We need to ensure a single rating for each 
system unit.  
References 
DUC. 2001. The Document Understanding 
Conference 2001. http://www-nlpir.nist.gov/ 
projects/duc/2001.html.  
Lin, C.-Y. 2001. Summary Evaluation 
Environment.  http://www.isi.edu/~cyl/SEE. 
Mani, I., D. House, G. Klein, L. Hirschman, L. 
Obrst, T. Firmin, M. Chrzanowski, and B. 
Sundheim. 1998. The TIPSTER SUMMAC 
Text Summarization Evaluation: Final 
Report.  MITRE Corp. Tech. Report. 
Papineni K., S. Roukos, T. Ward, W.-J. Zhu. 
2001. BLEU: a Method for Automatic 
Evaluation of Machine Translation. IBM 
Research Report RC22176(W0109-022). 
Original 
SYSCODE Retention X-1G X-2G X-3G L-1G L-2G L-3G P-1G P-2G P-3G
ranking (unigram) (unigram) (unigram)
Human1 1 2 1 1 1 1 1 1 1 1
Human2 2 1 2 2 2 2 2 2 2 2
Baseline1 14 15 15 15 16 15 14 16 15 14
Baseline2 8 8 7 6 8 8 6 8 8 6
L 7 7 6 7 7 7 7 7 7 7
M 10 10 10 10 10 11 11 9 10 11
N 4 4 4 4 4 4 4 4 4 4
O 11 12 12 12 12 12 12 11 12 12
P 6 5 5 5 5 5 5 5 5 5
R 11 11 11 11 11 10 10 12 11 10
S 9 9 9 9 9 9 9 9 9 9
T 3 3 3 3 3 3 3 3 3 3
U 16 14 14 14 14 14 15 14 14 15
W 15 16 16 16 15 16 16 15 16 16
Y 5 6 8 8 6 6 8 6 6 8
Z 13 13 13 13 13 13 13 13 13 13
Spearman ? 1.00000 0.98382 0.97206 0.96912 0.98382 0.98382 0.97206 0.98235 0.98676 0.97206
No stemmer Lovin stemmer Porter stemmer
Figure 4.  Manual and automatic ranking comparisons. 
?The?Potential?and?Limitations?of?Automatic?????????????????????????????????????????
Sentence?Extraction?for?Summarization?
Chin-Yew?Lin?and?Eduard?Hovy?
University?of?Southern?California/Information?Sciences?Institute?
4676?Admiralty?Way?
Marina?del?Rey,?CA?90292,?USA?
{cyl,hovy}@isi.edu 
?
?
Abstract?
In?this?paper?we?present?an?empirical?study?of?
the?potential?and?limitation?of?sentence?extrac-
tion? in? text? summarization.?Our? results? show?
that? the? single? document? generic? summariza-
tion?task?as?defined?in?DUC?2001?needs?to?be?
carefully?refocused?as?reflected?in?the? low?in-
ter-human? agreement? at? 100-word 1 ?(0.40?
score)? and? high? upper? bound? at? full? text 2?
(0.88)? summaries.? For? 100-word? summaries,?
the?performance?upper?bound,?0.65,?achieved?
oracle?extracts3.?Such?oracle?extracts?show?the?
promise? of? sentence? extraction? algorithms;?
however,? we? first? need? to? raise? inter-human?
agreement?to?be?able?to?achieve?this?perform-
ance? level.?We? show? that? compression? is? a?
promising?direction? and? that? the? compression?
ratio?of?summaries?affects?average?human?and?
system?performance.?
1? Introduction?
Most? automatic? text? summarization? systems? existing?
today?are?extraction?systems? that?extract?parts?of?origi-
nal? documents? and? output? the? results? as? summaries.?
Among? them,? sentence? extraction? is? by? far? the? most?
???????????????????????????????????????????????????????????
1?We?compute?unigram?co-occurrence?score?of?a?pair?of?man-
ual? summaries,? one? as? candidate? summary? and? the? other? as?
reference.?
2?We?compute?unigram?co-occurrence?scores?of?a?full?text?and?
its?manual?summaries?of?100?words.?These?scores?are?the?best?
achievable? using? the? unigram? co-occurrence? scoring? metric?
since? all?possible?words? are? contained? in? the? full? text.?Three?
manual?summaries?are?used.?
3 ?Oracle? extracts? are? the? best? scoring? extracts? generated? by?
exhaustive? search? of? all? possible? sentence? combinations? of?
100?5?words.??
popular? (Edmundson? 1969,? Luhn? 1969,? Kupiec? et? al.?
1995,?Goldstein? et? al.?1999,?Hovy? and?Lin?1999).?The?
majority?of?systems?participating? in? the?past?Document?
Understanding? Conference? (DUC? 2002),? a? large? scale?
summarization? evaluation? effort? sponsored? by? the? US?
government,? are? extraction? based.? Although? systems?
based?on? information?extraction? (Radev?and?McKeown?
1998,?White?et?al.?2001,?McKeown?et?al.?2002)?and?dis-
course?analysis?(Marcu?1999b,?Strzalkowski?et?al.?1999)?
also?exist,?we?focus?our?study?on?the?potential?and?limi-
tations?of?sentence?extraction?systems?with?the?hope?that?
our?results?will?further?progress?in?most?of?the?automatic?
text?summarization?systems?and?evaluation?setup.?
The?evaluation?results?of?the?single?document?summari-
zation?task?in?DUC?2001?and?2002?(DUC?2002,?Paul?&?
Liggett?2002)?indicate?that?most?systems?are?as?good?as?
the?baseline?lead-based?system?and?that?humans?are?sig-
nificantly?better,?though?not?by?much.?This?leads?to?the?
belief?that?lead-based?summaries?are?as?good?as?we?can?
get? for? single? document? summarization? in? the? news?
genre,? implying? that? the? research? community? should?
invest?future?efforts? in?other?areas.? In?fact,?a?very?short?
summary? of? about? 10? words? (headline-like)? task? has?
replaced? the? single? document? 100-word? summary? task?
in?DUC?2003.?The?goal?of?this?study?is?to?renew?interest?
in? sentence? extraction-based? summarization? and? its?
evaluation?by? estimating? the?performance?upper?bound?
using?oracle?extracts,?and?to?highlight?the?importance?of?
taking? into? account? the? compression? ratio? when? we?
evaluate?extracts?or?summaries.??
Section? 2? gives? an? overview? of?DUC? relevant? to? this?
study.?Section?3? introduces? a? recall-based?unigram? co-
occurrence? automatic?evaluation?metric.?Section?4?pre-
sents?the?experimental?design.?Section?5?shows?the?em-
pirical? results.? Section? 6? concludes? this? paper? and?
discusses?future?directions.?
?2? Document?Understanding?Conference?
Fully? automatic? single-document? summarization? was?
one? of? two?main? tasks? in? the? 2001?Document?Under-
standing?Conference.?Participants?were? required? to?cre-
ate? a? generic? 100-word? summary.? ?There?were? 30? test?
sets? in?DUC?2001?and?each? test?set?contained?about?10?
documents.?For?each?document,?one?summary?was?cre-
ated? manually? as? the? ?ideal?? model? summary? at? ap-
proximately? 100?words.? ?We?will? refer? to? this?manual?
summary? as? H1.? Two? other? manual? summaries? were?
also?created?at?about?that?length.??We?will?refer?to?these?
two?additional?human?summaries?as?H2?and?H3.?In?addi-
tion,?baseline?summaries?were?created?automatically?by?
taking? the? first? n? sentences? up? to? 100?words.?We?will?
refer?this?baseline?extract?as?B1.?
3? Unigram?Co-Occurrence?Metric?
In?a?recent?study?(Lin?and?Hovy?2003),?we?showed?that?
the?recall-based?unigram?co-occurrence?automatic?scor-
ing?metric?correlated?highly?with?human?evaluation?and?
has? high? recall? and? precision? in? predicting? statistical?
significance?of? results?comparing?with? its?human?coun-
terpart.? The? idea? is? to? measure? the? content? similarity?
between?a?system?extract?and?a?manual?summary?using?
simple? n-gram? overlap.? A? similar? idea? called? IBM?
BLEU? score? has? proved? successful? in? automatic? ma-
chine?translation?evaluation?(Papineni?et?al.?2001,?NIST?
2002).?For?summarization,?we?can?express?the?degree?of?
content?overlap? in? terms?of?n-gram?matches?as? the? fol-
lowing?equation:?
)1(
)(
)(
}{
}{
? ?
? ?
? ??
? ??
?
?
=
UnitsModelC Cgramn
UnitsModelC Cgramn
match
n gramnCount
gramnCount
C ?
Model?units? are? segments?of?manual? summaries.?They?
are? typically? either? sentences? or? elementary? discourse?
units?as?defined?by?Marcu? (1999b).?Countmatch(n-gram)?
is? the?maximum? number? of? n-grams? co-occurring? in? a?
system?extract? and? a?model?unit.?Count(n-gram)? is? the?
number? of? n-grams? in? the?model? unit.?Notice? that? the?
average?n-gram? coverage? score,?Cn,? as? shown? in?equa-
tion?1,?is?a?recall-based?metric,?since?the?denominator?of?
equation? 1? is? the? sum? total? of? the? number? of? n-grams?
occurring? in? the?model? summary? instead?of? the? system?
summary?and?only?one?model?summary?is?used?for?each?
evaluation.? In? summary,? the? unigram? co-occurrence?
statistics?we?use? in? the?following?sections?are?based?on?
the?following?formula:?
)2(logexp),( ???
?
???
?
= ?
=
j
in
nn CwjiNgram ?
Where? j??? i,? i? and? j? range? from?1? to?4,?and?wn? is?1/(j-
i+1).?Ngram(1,?4)?is?a?weighted?variable? length?n-gram?
match? score? similar? to? the? IBM? BLEU? score;? while?
Ngram(k,?k),?i.e.?i?=?j?=?k,?is?simply?the?average?k-gram?
co-occurrence?score?Ck.? In? this?study,?we?set? i?=? j?=?1,?
i.e.?unigram?co-occurrence?score.???
With?a?test?collection?available?and?an?automatic?scoring?
metric? defined,?we? describe? the? experimental? setup? in?
the?next?section.?
4? Experimental?Designs?
As? stated? in? the? introduction,?we? aim? to? find? the? per-
formance?upper?bound?of?a? sentence? extraction? system?
and? the?effect?of?compression?ratio?on? its?performance.?
We? present? our? experimental? designs? to? address? these?
questions?in?the?following?sections.?
4.1? Performance? Upper? Bound? Estimation?
Using?Oracle?Extract?
In?order?to?estimate?the?potential?of?sentence?extraction?
systems,?it?is?important?to?know?the?upper?bound?that?an?
ideal? sentence? extraction? method? might? achieve? and?
how? far? the? state-of-the-art? systems? are? away? from? the?
bound.? If? the? upper? bound? is? close? to? state-of-the-art?
systems?? performance? then?we? need? to? look? for? other?
summarization?methods? to? improve?performance.? If? the?
upper? bound? is?much? higher? than? any? current? systems?
can?achieve,?then?it?is?reasonable?to?invest?more?effort?in?
sentence? extraction? methods.? The? question? is? how? to?
estimate? the?performance?upper?bound.?Our? solution? is?
to?cast?this?estimation?problem?as?an?optimization?prob-
lem.? We? exhaustively? generate? all? possible? sentence?
combinations? that?satisfy?given? length?constraints? for?a?
summary,? for? example,? all? the? sentence? combinations?
totaling? 100?5? words.?We? then? compute? the? unigram?
co-occurrence? score? for? each? sentence? combination,?
against? the? ideal.? The? best? combinations? are? the? ones?
with? the?highest?unigram? co-occurrence? score.?We?call?
this? sentence? combination? the? oracle? extract.? Figure? 1?
shows?an?oracle?extract? for?document?AP900424-0035.?
One?of?its?human?summaries?is?shown?in?Figure?2.?The?
oracle? extract? covers? almost? all? aspects? of? the? human?
summary?except?sentences?5?and?6?and?part?of?sentence?
4.?However,?if?we?allow?the?automatic?extract?to?contain?
more?words,?for?example,?150?words?shown?in?Figure?3,?
the? longer?oracle? extract? then? covers? everything? in? the?
human?summary.?This?indicates?that?lower?compression?
can? boost? system? performance.? The? ultimate? effect? of?
compression?can?be?computed?using?the?full?text?as?the?
oracle? extract,? since? the? full? text? should?contain?every-
thing? included? in? the? human? summary.? That? situation?
provides? the? best? achievable? unigram? co-occurrence?
score.?A?near?optimal?score?also?confirms?the?validity?of?
using? the?unigram?co-occurrence? scoring?method?as?an?
automatic?evaluation?method.?
?4.2? Compression?Ratio? and? Its?Effect? on? System?
Performance?
One? important? factor? that? affects? the? average?perform-
ance? of? sentence? extraction? system? is? the? number? of?
sentences? contained? in? the? original? documents.? This?
factor?is?often?overlooked?and?has?never?been?addressed?
systematically.? For? example,? if? a? document? contains?
only?one?sentence?then?this?document?will?not?be?useful?
in?differentiating? summarization? system?performance???
there? is? only? one? choice.?However,? for? a? document? of?
100?sentences?and?assuming?each?sentence? is?20?words?
long,? there? are? C(100,5)? =? 75,287,520? different? 100-
word?extracts.?This?huge?search?space?lowers?the?chance?
of? agreement? between? humans? on? what? constitutes? a?
good? summary.? It? also?makes? system? and? human? per-
formance? approach? average? since? it? is?more? likely? to?
include? some?good? sentences?but?not? all?of? them.?Em-
pirical?results?shown? in?Section?5?confirm? this?and? that?
leads?us?to?the?question?of?how?to?construct?a?corpus?to?
evaluate? summarization? systems.?We?discuss? this? issue?
in?the?conclusion?section.??
4.3? Inter-Human?Agreement?and?Its?Effect?on?
System?Performance?
In? this? section? we? study? how? inter-human? agreement?
affects? system? performance.? Lin? and?Hovy? (2002)? re-
ported? that,? compared? to? a?manually? created? ideal,?hu-
mans? scored?about?0.40? in?average?coverage? score? and?
the? best? system? scored? about? 0.35.?According? to? these?
numbers,?we?might?assume?that?humans?cannot?agree?to?
each?other?on?what? is? important?and? the?best?system? is?
almost?as?good?as?humans.?If?this?is?true?then?estimating?
an?upper?bound?using?oracle?extracts?is?meaningless.?No?
matter?how?high?the?estimated?upper?bounds?may?be,?we?
probably?would?never?be?able? to?achieve? that?perform-
ance? due? to? lack? of? agreement? between? humans:? the?
oracle? approximating? one? human?would? fail?miserably?
with?another.??Therefore?we?set?up?experiments?to?inves-
tigate?the?following:?
1.? What? is? the?distribution?of? inter-human?agree-
ment??
Figure?3.?A?150-word?oracle?extract? for?docu-
ment?AP900424-0035.?
Figure? 2.? A? manual? summary? for? document?
AP900424-0035.?
Figure?1.?A?100-word?oracle?extract? for?docu-
ment?AP900424-0035.?
<DOC>?
<DOCNO>AP900424-0035</DOCNO>?
<DATE>04/24/90</DATE>?
<HEADLINE>?
<S?HSNTNO="1">Elizabeth?Taylor?in?Intensive?Care?Unit</S>?
<S?HSNTNO="2">By?JEFF?WILSON</S>?
<S?HSNTNO="3">Associated?Press?Writer</S>?
<S?HSNTNO="4">SANTA?MONICA,?Calif.?(AP)</S>?
</HEADLINE>?
<TEXT>?
<S?SNTNO="1">A?seriously?ill?Elizabeth?Taylor?battled?pneumonia?at?her?
hospital,?her?breathing?assisted?by?a?ventilator,?doctors?say.</S>?
<S?SNTNO="2">Hospital?officials?described?her?condition?late?Monday?
as?stabilizing?after?a?lung?biopsy?to?determine?the?cause?of?the?pneumo-
nia.</S>?
<S?SNTNO="3">Analysis?of?the?tissue?sample?was?expected?to?take?until?
Thursday,?said?her?spokeswoman,?Chen?Sam.</S>?
<S?SNTNO="9">Another?spokewoman?for?the?actress,?Lisa?Del?Favaro,?
said?Miss?Taylor's?family?was?at?her?bedside.</S>?
<S?SNTNO="13">``It?is?serious,?but?they?are?really?pleased?with?her?
progress.</S>?
<S?SNTNO="22">During?a?nearly?fatal?bout?with?pneumonia?in?1961,?
Miss?Taylor?underwent?a?tracheotomy,?an?incision?into?her?windpipe?to?
help?her?breathe.</S>?
</TEXT>?
</DOC>
<DOC>?
<TEXT>?
<S?SNTNO="1">Elizabeth?Taylor?battled?pneumonia?at?her?hospital,?
assisted?by?a?ventilator,?doctors?say.</S>?
<S?SNTNO="2">Hospital?officials?described?her?condition?late?Monday?
as?stabilizing?after?a?lung?biopsy?to?determine?the?cause?of?the?pneumo-
nia.</S>?
<S?SNTNO="3">Analysis?of?the?tissue?sample?was?expected?to?be?com-
plete?by?Thursday.</S>?
<S?SNTNO="4">Ms.?Sam,?spokeswoman?said?"it?is?serious,?but?they?are?
really?pleased?with?her?progress.</S>?
<S?SNTNO="5">She's?not?well.</S>?
<S?SNTNO="6">She's?not?on?her?deathbed?or?anything.</S>?
<S?SNTNO="7">Another?spokeswoman,?Lisa?Del?Favaro,?said?Miss?
Taylor's?family?was?at?her?bedside.</S>?
<S?SNTNO="8">During?a?nearly?fatal?bout?with?pneumonia?in?1961,?Miss?
Taylor?underwent?a?tracheotomy?to?help?her?breathe.</S>?
</TEXT>?
</DOC>?
<DOC>?
<DOCNO>AP900424-0035</DOCNO>?
<DATE>04/24/90</DATE>?
<HEADLINE>?
<S?HSNTNO="1">Elizabeth?Taylor?in?Intensive?Care?Unit</S>?
<S?HSNTNO="2">By?JEFF?WILSON</S>?
<S?HSNTNO="3">Associated?Press?Writer</S>?
<S?HSNTNO="4">SANTA?MONICA,?Calif.?(AP)</S>?
</HEADLINE>?
<TEXT>?
<S?SNTNO="1">A?seriously?ill?Elizabeth?Taylor?battled?pneumonia?at?her?
hospital,?her?breathing?assisted?by?a?ventilator,?doctors?say.</S>?
<S?SNTNO="2">Hospital?officials?described?her?condition?late?Monday?
as?stabilizing?after?a?lung?biopsy?to?determine?the?cause?of?the?pneumo-
nia.</S>?
<S?SNTNO="3">Analysis?of?the?tissue?sample?was?expected?to?take?until?
Thursday,?said?her?spokeswoman,?Chen?Sam.</S>?
<S?SNTNO="4">The?58-year-old?actress,?who?won?best-actress?Oscars?
for?``Butterfield?8''?and?``Who's?Afraid?of?Virginia?Woolf,''?has?been?
hospitalized?more?than?two?weeks.</S>?
<S?SNTNO="8">Her?condition?is?presently?stabilizing?and?her?physicians?
are?pleased?with?her?progress.''</S>?
<S?SNTNO="9">Another?spokewoman?for?the?actress,?Lisa?Del?Favaro,?
said?Miss?Taylor's?family?was?at?her?bedside.</S>?
<S?SNTNO="13">``It?is?serious,?but?they?are?really?pleased?with?her?
progress.</S>?
<S?SNTNO="14">She's?not?well.</S>?
<S?SNTNO="15">She's?not?on?her?deathbed?or?anything,''?Ms.?Sam?said?
late?Monday.</S>?
<S?SNTNO="22">During?a?nearly?fatal?bout?with?pneumonia?in?1961,?
Miss?Taylor?underwent?a?tracheotomy,?an?incision?into?her?windpipe?to?
help?her?breathe.</S>?
</TEXT>?
</DOC>
?2.? How?does?a?state-of-the-art?system?differ?from?
average?human?performance?at?different? inter-
human?agreement?levels???
We? present? our? results? in? the? next? section? using? 303?
newspaper?articles?from?the?DUC?2001?single?document?
summarization?task.?Besides?the?original?documents,?we?
also? have? three? human? summaries,? one? lead? summary?
(B1),? and? one? automatic? summary? from? one? top? per-
forming?system?(T)?for?each?document.?
5? Results?
In? order? to? determine? the? empirical? upper? and? lower?
bounds? of? inter-human? agreement,?we? first? ran? cross-
human?evaluation?using?unigram?co-occurrence?scoring?
through? six? human? summary? pairs,? i.e.? (H1,H2),?
(H1,H3),?(H2,H1),?(H2,H3),?(H3,H1),?and?(H3,H2).?For?
a? summary? pair? (X,Y),?we? used?X? as? the?model? sum-
mary?and?Y?as?the?system?summary.?Figure?4?shows?the?
distributions?of?four?different?scenarios.?The?MaxH?dis-
tribution? picks? the? best? inter-human? agreement? scores?
for?each?document,?the?MinH?distribution?the?minimum?
one,? the?MedH?distribution? the?median,? and? the?AvgH?
distribution? the? average.?The? average?of? the?best? inter-
human? agreement? and? the? average? of? average? inter-
human?agreement?differ?by?about?10?percent?in?unigram?
co-occurrence?score?and?18?percent?between?MaxH?and?
MinH.? These? big? differences? might? come? from? two?
sources.?The? first? one? is? the? limitation? of? the? unigram?
0
10
20
30
40
50
60
70
80
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Unigram?Co-occurrence?Scores
#?
o
f?I
n
st
an
ce
s
AvgH MaxH MedH MinH
Average?MAX?=?0.50
Average?AVG?=?0.40
Average?MED?=?0.39
Average?MIN?=?0.32
Figure? 4.? DUC? 2001? single? document? inter-
human? unigram? co-occurrence? score? distribu-
tions? for? maximum,? minimum,? average,? and?
median.?
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D4
1.A
P8
81
21
1-0
02
7
D5
3.F
BI
S3
-2
29
42
D3
1.L
A0
21
68
9-
02
27
D3
4.A
P8
80
91
4-0
07
9
D5
3.A
P8
80
81
6-0
23
4
D2
8.L
A1
10
59
0-
00
38
D1
9.A
P8
80
33
0-0
11
9
D1
4.A
P9
01
01
0-0
03
6
D2
2.L
A0
70
18
9-
00
80
D2
7.L
A1
00
78
9-
00
07
D1
2.F
T9
34
-1
10
14
D2
2.A
P8
81
21
6-0
01
7
D3
7.F
BI
S3
-1
19
19
D1
9.L
A1
02
18
9-
01
51
D0
5.F
T9
41
-1
54
7
D5
7.L
A1
10
58
9-
00
82
D3
4.A
P8
80
91
3-0
20
4
D4
5.A
P9
00
62
5-0
16
0
D5
0.A
P8
81
22
2-0
11
9
D1
4.A
P9
01
01
2-0
03
2
D4
1.S
JM
N9
1-
06
07
10
22
D4
3.F
T9
23
-5
85
9
D0
8.A
P8
90
31
6-0
01
8
D1
9.A
P8
80
62
3-0
13
5
D4
3.F
T9
33
-8
94
1
D4
4.F
T9
34
-9
11
6
D1
2.W
SJ
87
02
27
-0
14
9
D0
4.F
T9
23
-5
08
9
D1
5.A
P8
90
30
2-0
06
3
D0
4.F
T9
23
-6
03
8
D3
7.A
P8
90
70
4-0
04
3
D1
2.W
SJ
87
01
23
-0
10
1
D1
5.A
P8
90
51
1-0
12
6
D1
5.A
P9
00
52
1-0
06
3
D0
6.F
T9
22
-1
02
00
D3
4.A
P9
00
60
1-0
04
0
Document?IDs
U
n
ig
ra
m
?C
o-
oc
cu
rr
en
ce
?S
co
re
s
MaxH B1 T E100 E150 FT Avg?MaxH Avg?B1 Avg?T Avg?E100 Avg?E150 Avg?FT
Figure?5.?DUC?2001?single?document? inter-human,?baseline,?system,?100-word,?150-word,?and?full? text?
oracle?extracts?unigram?co-occurrence?score?distributions?(#?of?sentences<=30).?Document?IDs?are?sorted?
by?decreasing?MaxH.?
?co-occurrence?scoring?applied?to?manual?summaries?that?
it?cannot? recognize?synonyms?or?paraphrases.?The?sec-
ond?one?is?the?true? lack?of?agreement?between?humans.?
We?would? like? to?conduct?an? in-depth?study? to?address?
this? question,? and?would? just? assume? the? unigram? co-
occurrence?scoring?is?reliable.?
In? other? experiments,? we? used? the? best? inter-human?
agreement?results?as?the?reference?point?for?human?per-
formance?upper?bound.?This?also? implied? that?we?used?
the? human? summary? achieving? the? best? inter-human?
agreement?score?as?our?reference?summary.?
Figure? 5? shows? the? unigram? co-occurrence? scores? of?
human,?baseline,? system?T,? and? three?oracle? extraction?
systems?at?different?extraction?lengths.?We?generated?all?
possible? sentence? combinations? that? satisfied? 100?5?
words?constraints.?Due? to?computation-intensive?nature?
of?this?task,?we?only?used?documents?with?fewer?than?30?
sentences.? We? then? computed? the? unigram? co-
occurrence?score?for?each?combination,?selected?the?best?
one?as?the?oracle?extraction,?and?plotted?the?score?in?the?
figure.?The?curve?for?100?5?words?oracle?extractions?is?
the?upper?bound? that? a? sentence? extraction? system? can?
achieve? within? the? given? word? limit.? If? an? automatic?
system?is?allowed?to?extract?more?words,?we?can?expect?
that? longer? extracts? would? boost? system? performance.?
The?question? is?how?much?better? and?what? is? the?ulti-
mate? limit??To? address? these? questions,?we? also? com-
puted? unigram? co-occurrence? scores? for? oracle?
extractions?of?150?5?words?and?full?text4.?The?perform-
ance?of? full? text? is? the?ultimate?performance?an?extrac-
tion?system?can? reach?using? the?unigram?co-occurrence?
scoring? method.?We? also? computed? the? scores? of? the?
lead?baseline?system?(B1)?and?an?automatic?system?(T).??
The? average? unigram? co-occurrence? score? for? full? text?
(FT)?was?0.833,?150?5?words?(E150)?was?0.796,?100?5?
words? (E100)? was? 0.650,? the? best? inter-human? agree-
ment?(MaxH)?was?0.546,?system?T?was?0.465,?and?base-
line?was?0.456.?It?is?interesting?to?note?that?the?state-of-
the-art?system?performed?at? the?same? level?as? the?base-
line?system?but?was?still?about?10%?away?from?human.?
The?10%?difference?between?E100?and?MaxH?(0.650?vs.?
0.546)? implies?we?might? need? to? constraint?humans? to?
focus? their?summaries? in?certain?aspects? to?boost? inter-
human? agreement? to? the? level?of?E100;?while? the?15%?
and?24%?improvements?from?E100?to?E150?and?FT?in-
dicate?compression?would?help?push?overall?system?per-
formance? to?a?much?higher? level,? if?a?system? is?able? to?
compress? longer? summaries? into? a? shorter?without? los-
ing?important?content.?
To? investigate? relative? performance? of? humans,? sys-
tems,? and? oracle? extracts? at? different? inter-human?
agreement? levels,?we? created? three? separate? document?
sets? based? on? their? maximum? inter-human? agreement?
(MaxH)?scores.?Set?Set?A?had?MaxH?score?greater?than?
or?equal?to?0.70,?set?B?was?between?0.70?and?0.60,?and?
???????????????????????????????????????????????????????????
4?We? used? full? text? as? extract? and? computed? its? unigram? co-
occurrence?score?against?a?reference?summary.?
Figure? 7.? DUC? 2001? single? document? inter-
human,?baseline,?system,?and?full? text?unigram?
co-occurrence?score?distributions?(Set?B).?
Figure? 6.? DUC? 2001? single? document? inter-
human,?baseline,?system,?and?full? text?unigram?
co-occurrence?score?distributions?(Set?A).?
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D4
1.A
P8
81
21
1-0
02
7
D1
1.A
P8
90
40
3-0
12
3
D1
4.A
P8
80
90
2-0
06
2
D4
1.A
P8
90
80
1-0
02
5
D0
6.S
JM
N9
1-
06
19
10
81
D4
1.L
A0
51
59
0-
00
65
D0
6.W
SJ
91
07
10
-0
14
8
D5
3.F
BI
S3
-2
29
42
D2
8.L
A1
10
49
0-
01
84
D3
1.L
A0
30
88
9-
01
63
D1
3.S
JM
N9
1-
06
25
54
34
D2
4.L
A0
51
19
0-
01
85
D3
1.L
A0
21
68
9-
02
27
D0
5.F
T9
31
-3
88
3
D0
6.S
JM
N9
1-
06
28
30
83
D3
1.A
P8
91
00
6-0
02
9
D5
0.A
P8
80
71
4-0
14
2
D3
4.A
P8
80
91
4-0
07
9
D1
4.A
P8
80
62
9-0
15
9
D1
3.A
P9
00
30
6-0
10
5
D3
1.L
A0
30
78
9-
00
47
D1
4.L
A1
03
08
9-
00
70
Document?IDs
U
ni
g
ra
m
?C
o
-o
cc
u
rr
en
ce
?S
co
re
s
MaxH B1 T AvgMaxH Avg?B1 Avg?T
Avg?FT FT Avg?E100 Avg?E150
Avg?E150?=?0.863
Avg?FT?=?0.924
Avg?E100?=?0.705
Avg?MaxH?=?0.741
Avg?T?=?0.525
Avg?B1?=?0.516
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D5
3.A
P8
80
81
6-0
23
4
D0
5.F
T9
21
-9
31
0
D4
1.L
A0
81
49
0-
00
30
D2
4.A
P9
00
42
4-0
03
5
D3
7.F
BI
S4
-2
76
02
D0
6.L
A0
71
59
0-
00
68
D2
8.L
A1
10
59
0-
00
38
D3
1.L
A0
61
58
9-
01
43
D3
2.A
P9
00
32
3-0
03
6
D3
7.A
P9
01
01
3-0
04
6
D5
6.A
P8
81
12
6-0
00
7
D0
6.A
P8
90
32
2-0
01
0
D1
9.A
P8
80
33
0-0
11
9
D1
4.A
P8
80
91
3-0
12
9
D0
4.F
T9
23
-5
79
7
D4
4.F
T9
32
-5
85
5
D1
4.A
P9
01
01
0-0
03
6
D3
7.A
P8
80
51
0-0
17
8
D4
1.A
P8
90
80
5-0
12
6
D2
2.A
P8
80
70
5-0
10
9
D5
0.A
P9
00
91
0-0
02
0
D3
1.S
JM
N9
1-
06
08
42
28
D3
4.A
P9
00
52
9-0
00
5
D2
2.L
A0
70
18
9-
00
80
D0
4.F
T9
23
-5
83
5
D1
4.A
P8
81
22
2-0
12
6
D2
4.A
P9
00
51
2-0
03
8
D2
7.L
A1
00
78
9-
00
07
D3
1.A
P8
81
00
9-0
07
2
D4
5.A
P8
80
52
0-0
26
4
D0
8.A
P8
80
31
8-0
05
1
D1
5.F
BI
S4
-6
77
21
D1
2.F
T9
34
-1
10
14
D3
7.A
P9
00
42
8-0
10
8
D4
5.
FT
92
1-
30
5
D5
4.L
A0
92
79
0-
00
10
D5
6.S
JM
N9
1-
06
13
63
05
Dcoument?IDs
U
ni
g
ra
m
?C
o
-o
cc
u
rr
en
ce
?S
co
re
s
MaxH B1 T FT AvgMaxH Avg?B1
Avg?T Avg?FT Avg?E100 Avg?E150
Avg?E150?=?0.840
Avg?E100?=?0.698
Avg?FT?=?0.917
Avg?MaxH?=?0.645
Avg?B1?=?0.509
Avg?T?=?0.490
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D2
2.A
P8
81
21
6-0
01
7
D4
4.F
T9
33
-1
08
81
D4
3.A
P8
90
13
1-0
28
0
D0
6.L
A0
11
88
9-
00
67
D2
7.A
P8
90
72
2-0
08
1
D4
1.S
JM
N9
1-
06
14
21
26
D5
7.A
P9
01
20
3-0
16
6
D3
1.S
JM
N9
1-
06
01
22
24
D2
8.S
JM
N9
1-
06
31
21
20
D3
0.A
P9
00
41
6-0
18
8
D2
7.W
SJ
91
11
21
-0
13
6
D3
4.A
P8
80
91
3-0
20
4
D2
7.W
SJ
91
12
12
-0
08
0
D0
6.W
SJ
91
04
05
-0
15
4
D1
5.L
A1
01
69
0-
00
40
D5
0.A
P8
81
22
2-0
11
9
D0
8.A
P8
90
30
7-0
15
0
D4
4.F
T9
33
-5
70
9
D3
7.A
P9
00
42
8-0
00
5
D5
0.A
P8
91
21
3-0
00
4
D5
4.W
SJ
91
10
31
-0
01
2
D1
9.L
A0
71
58
9-
00
76
D1
4.A
P9
00
82
9-0
12
0
D4
3.F
T9
11
-3
46
3
D3
1.A
P8
80
92
7-0
11
7
D2
8.L
A1
21
18
9-
00
17
D0
8.A
P8
90
31
6-0
01
8
D4
4.F
T9
34
-8
62
8
D0
8.A
P9
00
72
1-0
11
0
D1
9.A
P8
80
62
3-0
13
5
D2
2.A
P8
80
70
5-0
01
8
D3
2.A
P8
90
32
6-0
08
1
D4
3.F
T9
33
-8
94
1
D5
3.A
P8
80
61
3-0
16
1
Document?IDs
U
ni
g
ra
m
?C
o
-o
cc
u
rr
en
ce
?S
co
re
s
MaxH B1 T AvgMaxH Avg?B1 Avg?T
Avg?FT FT Avg?E100 Avg?E150
Avg?E150?=?0.790
Avg?E100?=?0.645
Avg?FT?=?0.897
Avg?MaxH?=?0.536
Avg?T?=?0.435
Avg?B1?=?0.423
Figure? 8.? DUC? 2001? single? document? inter-
human,?baseline,?system,?and?full? text?unigram?
co-occurrence?score?distributions?(Set?C).?
?set?C?between?0.60?and?0.50.?A?had?22?documents,?set?B?
37,?and?set?C?100.?Total?was?about?52%?(=159/303)?of?
the? test? collection.?The? 100?5? and? 150?5?words? aver-
ages?were? computed? over? documents?which? contain? at?
most?30?sentences.?The?results?are?shown? in?Figures?6,?
7,?and?8.?In?the?highest?inter-human?agreement?set?(A),?
we? found? that? average?MaxH,? 0.741,?was? higher? than?
average? 100?5? words? oracle? extract,? 0.705;? while? the?
average? automatic? system? performance? was? around?
0.525.? This? is? good? news? since? the? high? inter-human?
agreement?and?the?big?difference?(0.18)?between?100?5?
words? oracle? and? automatic? system? performance? pre-
sents? a? research? opportunity? for? improving? sentence?
extraction? algorithms.?The? scores? of?MaxH? (0.645? for?
set?B?and?0.536?for?set?C)?in?the?other?two?sets?are?both?
lower?than?100?5?words?oracles?(0.698?for?set?B,?5.3%?
lower,? and? 0.645? for? set? C,? 9.9%? lower).? This? result?
suggests? that?optimizing?sentence?extraction?algorithms?
at? the? Set? C? level?might? not? be?worthwhile? since? the?
algorithms? are? likely? to? overfit? the? training? data.? The?
reason? is? that? the? average? run? time? performance? of? a?
sentence?extraction?algorithm?depends?on?the?maximum?
inter-human? agreement.? For? example,? given? a? training?
reference? summary?TSUM1? and? its? full?document?TDOC1,?
we?optimize?our?sentence?extraction?algorithm?to?gener-
ate?an?oracle?extract?based?on?TSUM1?from?TDOC1.?In?the?
run?time,?we?test?on?a?reference?summary?RSUM1?and?its?
full?document?RDOC1.? In? the?unlikely?case? that?RDOC1? is?
the?same?as?TDOC1?and?RSUM1? is? the?same?as?TSUM1,? i.e.?
TSUM1?and?RSUM1?have?unigram?co-occurrence?score?of?1?
(perfect? inter-human? agreement? for? two? summaries? of?
one?document),?the?optimized?algorithm?will?generate?a?
perfect?extract?for?RDOC1?and?achieve? the?best?perform-
ance? since? it? is?optimized?on?TSUM1.?However,?usually?
TSUM1?and?RSUM1?are?different.?Then?the?performance?of?
the?algorithm?will?not?exceed?the?maximum?unigram?co-
occurrence?score?between?TSUM1?and?RSUM1.?Therefore?it?
is? important? to? ensure? high? inter-human? agreement? to?
allow? researchers? room? to?optimize?sentence?extraction?
algorithms?using?oracle?extracts.???
Finally,?we? present? the? effect? of? compression? ratio? on?
inter-human? agreement? (MaxH)? and? performance? of?
baseline?(B1),?automatic?system?T?(T),?and?full?text?ora-
cle?(FT)? in?Figure?9.?Compression?ratio? is?computed? in?
terms?of?words?instead?of?sentences.?For?example,?a?100?
words? summary?of? a? 500?words?document?has? a? com-
pression? ratio?of?0.80? (=1?100/500).?The? figure? shows?
that?three?human?summaries?(H1,?H2,?and?H3)?had?dif-
ferent? compression? ratios? (CMPR?H1,?CMPR?H2,? and?
CMPR?H3)? for? different? documents? but? did? not? differ?
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D2
4.L
A0
51
19
0-
01
85
D1
9.A
P8
80
21
7-0
17
5
D2
4.A
P9
00
51
1-0
15
9
D5
6.A
P8
81
12
6-0
00
7
D5
3.F
BI
S3
-2
29
42
D4
4.F
T9
34
-1
33
50
D5
7.A
P8
91
01
7-0
20
4
D0
4.F
T9
23
-6
03
8
D3
7.A
P8
80
51
0-0
17
8
D2
2.L
A0
70
18
9-
00
80
D4
4.F
T9
22
-3
17
1
D3
1.A
P8
81
00
9-0
07
2
D3
1.S
JM
N9
1-
06
01
22
24
D0
6.A
P9
01
02
9-0
03
5
D0
8.A
P9
01
23
1-0
01
2
D1
5.L
A1
01
69
0-
00
40
D4
4.F
T9
33
-2
76
0
D0
8.S
JM
N9
1-
06
19
30
81
D0
8.A
P8
80
31
8-0
05
1
D1
5.A
P9
00
52
1-0
06
3
D0
6.A
P8
90
32
2-0
01
0
D3
1.S
JM
N9
1-
06
08
42
28
D3
2.A
P8
90
50
2-0
20
5
D4
5.S
JM
N9
1-
06
18
20
91
D3
2.A
P9
00
31
3-0
19
1
D3
9.A
P8
81
01
7-0
23
5
D3
2.L
A0
40
78
9-
00
51
D5
3.A
P8
81
22
7-0
18
5
D5
0.A
P8
91
21
0-0
07
9
D0
6.L
A0
80
79
0-
01
11
D5
4.L
A1
02
19
0-
00
45
D3
7.S
JM
N9
1-
06
14
30
70
D1
3.W
SJ
91
07
02
-0
07
8
D2
2.W
SJ
88
09
23
-0
16
3
Document?IDs
U
ni
g
ra
m
?C
o-
oc
cu
rr
en
ce
?S
co
re
s
B1 MaxH T CMPR?H1 CMPR?H2 CMPR?H3
FT Linear?(B1) Linear?(MaxH) Linear?(T) Linear?(FT)
Figure?9.?DUC?2001?single?doc?inter-human,?baseline,?and?system?unigram?co-occurrence?score?versus?
compression?ratio.?Document?IDs?are?sorted?by?increasing?compression?ratio?CMPR?H1.?
?much.?The?unigram?co-occurrence?scores?for?B1,?T,?and?
MaxH?were?noisy?but?had? a?general? trend? (Linear?B1,?
Linear? T,? and? Linear? MaxH)? of? drifting? into? lower?
performance? when? compression? ratio? increased? (i.e.?
when? summaries? became? shorter);? while? the? per-
formance? of? FT? did? not? exhibit? a? similar? trend.? This?
confirms? our? earlier? hypothesis? that? humans? are? less?
likely? to? agree? at? high? compression? ratio? and? system?
performance?will?also?suffer?at?high?compression? ratio.?
The?constancy?of?FT?across?different?compression?ratios?
is? reasonable? since? FT? scores? should? only? depend? on?
how? well? the? unigram? co-occurrence? scoring? method?
captures?content?overlap?between?a?full?text?and?its?ref-
erence? summaries? and? how? likely? humans? use?
vocabulary?outside?the?original?document.?
6? Conclusions?
In? this? paper? we? presented? an? empirical? study? of? the?
potential? and? limitations? of? sentence? extraction? as? a?
method? of? automatic? text? summarization.?We? showed?
the?following:?
(1)? How?to?use?oracle?extracts?to?estimate?the?per-
formance? upper? bound? of? sentence? extraction?
methods?at?different?extract?lengths.?We?under-
stand?that?summaries?optimized?using?unigram?
co-occurrence? score? do? not? guarantee? good?
quality? in? terms? of? coherence,? cohesion,? and?
overall?organization.?However,?we?would?argue?
that?a?good?summary?does?require?good?content?
and?we?will?leave?how?to?make?the?content?co-
hesive,? coherent,? and? organized? to? future? re-
search.??
(2)? Inter-human?agreement?varied?a?lot?and?the?dif-
ference?between?maximum?agreement? (MaxH)?
and? minimum? agreement? (MinH)? was? about?
18%?on? the?DUC?2001?data.?To?minimize? the?
gap,?we?need?to?define?the?summarization?task?
better.? This? has? been? addressed? by? providing?
guided? summarization? tasks? in? DUC? 2003?
(DUC? 2002).?We? guesstimate? the? gap? should?
be?smaller?in?DUC?2003?data.?
(3)? State-of-the-art?systems?performed?at?the?same?
level?as?the?baseline?system?but?were?still?about?
10%? away? from? the? average? human? perform-
ance.??
(4)? The? potential? performance? gains? (15%? from?
E100? to? E150? and? 24%? to? FT)? estimated? by?
oracle?extracts?of?different?sizes? indicated? that?
sentence? compression? or? sub-sentence? extrac-
tion?are?promising?future?directions.?
(5)? The?relative?performance?of?humans?and?oracle?
extracts? at? three? inter-human? agreement? inter-
vals?showed?that?it?was?only?meaningful?to?op-
timize? sentence? extraction? algorithms? if? inter-
human? agreement?was? high.?Although? overall?
high? inter-human?agreement?was? low?but?sub-
sets? of? high? inter-human? agreement? did? exist.?
For? example,? about? human? achieved? at? least?
60%? agreement? in? 59? out? of? 303? (~19%)?
documents?of?30?sentences?or?less.?
(6)? We? also? studied? how? compression? ratio? af-
fected? inter-human?agreement?and?system?per-
formance,? and? the? results? supported? our?
hypothesis? that? humans? tend? to? agree? less? at?
high? compression? ratio,? and? similar? between?
humans?and?systems.?How?to?take?into?account?
this?factor?in?future?summarization?evaluations?
is?an?interesting?topic?to?pursue?further.?
Using?exhaustive?search?to?identify?oracle?extraction?has?
been? studied?by?other? researchers?but? in?different? con-
texts.?Marcu?(1999a)?suggested?using?exhaustive?search?
to?create?training?extracts?from?abstracts.?Donaway?et?al.?
(2000)?used?exhaustive?search?to?generate?all?three?sen-
tences?extracts? to?evaluate?different?evaluation?metrics.?
The?main?difference?between?their?work?and?ours?is?that?
we? searched? for? extracts? of? a? fixed? number? of? words?
while?they?looked?for?extracts?of?a?fixed?number?of?sen-
tences.??
In?the?future,?we?would? like?to?apply?a?similar?method-
ology? to?different? text?units,? for?example,?sub-sentence?
units?such?as?elementary?discourse?unit?(Marcu?1999b).?
We?want? to? study?how? to? constrain? the? summarization?
task? to? achieve? higher? inter-human? agreement,? train?
sentence? extraction? algorithms? using? oracle? extracts? at?
different? compression? sizes,? and? explore? compression?
techniques?to?go?beyond?simple?sentence?extraction.?
References?
Donaway,? R.L.,? Drummey,? K.W.,? and? Mather,? L.A.?
2000.? A? Comparison? of? Rankings? Produced? by?
Summarization?Evaluation?Measures.? In? Proceeding?
of?the?Workshop?on?Automatic?Summarization,?post-
conference?workshop?of?ANLP-NAACL-2000,?Seat-
tle,?WA,?USA,?69?78.?
DUC.?2002.?The?Document?Understanding?Conference.??
http://duc.nist.gov.??
Edmundson,? H.P.? 1969.? New? Methods? in? Automatic?
Abstracting.??Journal?of?the?Association?for?Comput-
ing?Machinery.??16(2).?
Goldstein,? J.,? M.? Kantrowitz,? V.? Mittal,? and? J.? Car-
bonell.? 1999.? Summarizing? Text? Documents:? Sen-
tence? Selection? and? Evaluation? Metrics.? In?
Proceedings?of? the?22nd? International?ACM?Confer-
ence? on?Research? and?Development? in? Information?
Retrieval?(SIGIR-99),?Berkeley,?CA,?USA,?121?128.?
Hovy,?E.?and?C.-Y.?Lin.?1999.?Automatic?Text?Summa-
rization? in? SUMMARIST.? In? I.? Mani? and? M.?
?Maybury? (eds),? Advances? in? Automatic? Text? Sum-
marization,?81?94.?MIT?Press.?
Kupiec,?J.,?J.?Pederson,?and?F.?Chen.?1995.?A?Trainable?
Document? Summarizer.? In? Proceedings? of? the? 18th?
International?ACM?Conference?on?Research?and?De-
velopment? in? Information?Retrieval? (SIGIR-95),?Se-
attle,?WA,?USA,?68?73.?
Lin,?C.-Y.? and?E.?Hovy.?2002.?Manual? and?Automatic?
Evaluations? of? Summaries.? In? Proceedings? of? the?
Workshop? on? Automatic? Summarization,? post-
conference? workshop? of? ACL-2002,? pp.? 45-51,?
Philadelphia,?PA,?2002.?
Lin,?C.-Y.?and?E.H.?Hovy.?2003.?Automatic?Evaluation?
of? Summaries?Using?N-gram? Co-occurrence? Statis-
tics.? In? Proceedings? of? the? 2003?Human? Language?
Technology? Conference? (HLT-NAACL? 2003),? Ed-
monton,?Canada,?May?27???June?1,?2003.?
Luhn,?H.?P.?1969.?The?Automatic?Creation?of?Literature?
Abstracts.? IBM? Journal? of? Research? and? Develop-
ment.?2(2),?1969.?
Marcu,?D.?1999a.?The?automatic?construction?of? large-
scale? corpora? for? summarization? research.? Proceed-
ings? of? the? 22nd? International?ACM?Conference? on?
Research?and?Development? in? Information?Retrieval?
(SIGIR-99),?Berkeley,?CA,?USA,?137?144.?
Marcu,?D.?1999b.?Discourse?trees?are?good?indicators?of?
importance?in?text.?In?I.?Mani?and?M.?Maybury?(eds),?
Advances? in? Automatic? Text? Summarization,? 123?
136.?MIT?Press.?
McKeown,? K.,? R.? Barzilay,? D.? Evans,? V.? Hatzivassi-
loglou,? J.? L.? Klavans,? A.? Nenkova,? C.? Sable,? B.?
Schiffman,?S.?Sigelman.?2002.?Tracking?and?Summa-
rizing?News?on?a?Daily?Basis?with?Columbia?s?News-
blaster.? In? Proceedings? of? Human? Language?
Technology? Conference? 2002? (HLT? 2002).? San?
Diego,?CA,?USA.?
NIST.?2002.?Automatic?Evaluation?of?Machine?Transla-
tion?Quality?using?N-gram?Co-Occurrence?Statistics.?
Over,? P.? and?W.? Liggett.? 2002.? Introduction? to?DUC-
2002:?an? Intrinsic?Evaluation?of?Generic?News?Text?
Summarization? Systems.? In? Proceedings? of? Work-
shop? on? Automatic? Summarization? (DUC? 2002),?
Philadelphia,?PA,?USA.?
http://www-nlpir.nist.gov/projects/duc/pubs/?
2002slides/overview.02.pdf?
Papineni,? K.,? S.? Roukos,? T.?Ward,?W.-J.? Zhu.? 2001.?
Bleu:?a?Method?for?Automatic?Evaluation?of?Machine?
Translation.? IBM? Research? Report? RC22176?
(W0109-022).?
Radev,? D.R.? and? K.R.? McKeown.? 1998.? Generating?
Natural?Language?Summaries?from?Multiple?On-line?
Sources.?Computational?Linguistics,?24(3):469?500.?
Strzalkowski,?T,?G.?Stein,?J.?Wang,?and?B,?Wise.?A?Ro-
bust?Practical?Text?Summarizer.?1999.?In?I.?Mani?and?
M.? Maybury? (eds),? Advances? in? Automatic? Text?
Summarization,?137?154.?MIT?Press.?
White,?M.,?T.?Korelsky,?C.?Cardie,?V.?Ng,?D.? Pierce,?
and?K.?Wagstaff.?2001.?Multidocument?Summariza-
tion? via? Information? Extraction.? In? Proceedings? of?
Human? Language? Technology? Conference? 2001?
(HLT?2001),?San?Diego,?CA,?USA.?
?
?
 Improving Summarization Performance by Sentence Compression ?                 
A Pilot Study 
 
Chin-Yew Lin 
University of Southern California/Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292, USA 
cyl@isi.edu 
Abstract 
In this paper we study the effectiveness of 
applying sentence compression on an ex-
traction based multi-document summari-
zation system. Our results show that pure 
syntactic-based compression does not im-
prove system performance. Topic signa-
ture-based reranking of compressed 
sentences does not help much either.  
However reranking using an oracle 
showed a significant improvement re-
mains possible.  
Keywords: Text Summarization, Sentence 
Extraction, Sentence Compression, 
Evaluation. 
1 Introduction 
The majority of systems participating in the past 
Document Understanding Conference (DUC, 2002) 
(a large scale summarization evaluation effort 
sponsored by the United States government), and 
the Text Summarization Challenge (Fukusima and 
Okumura, 2001) (sponsored by Japanese govern-
ment) are extraction based. Extraction-based auto-
matic text summarization systems extract parts of 
original documents and output the results as sum-
maries (Chen et al, 2003; Edmundson, 1969; 
Goldstein et al, 1999; Hovy and Lin, 1999; Kupiec 
et al, 1995; Luhn, 1969). Other systems based on 
information extraction (McKeown et al, 2002; 
Radev and McKeown, 1998; White et al, 2001) 
and discourse analysis (Marcu, 1999; Strzalkowski 
et al, 1999) also exist but they are not yet usable 
for general-domain summarization. Our study fo-
cuses on the effectiveness of applying sentence 
compression techniques to improve the perform-
ance of extraction-based automatic text summariza-
tion systems. 
Sentence compression aims to retain the most sali-
ent information of a sentence, rewritten in a short 
form (Knight and Marcu, 2000). It can be used to 
deliver compressed content to portable devices 
(Buyukkokten et al, 2001; Corston-Oliver, 2001) 
or as a reading aid for aphasic readers (Carroll et 
al., 1998) or the blind (Grefenstette, 1998). Earlier 
research in sentence compression focused on com-
pressing single sentences, and were evaluated on a 
sentence by sentence basis. For example, Jing 
(2000) trained her system on a set of 500 sentences 
from the Benton Foundation 
(http://www.benton.org) and their reduced forms 
written by humans. The results were evaluated at 
the parse tree level against the reduced trees; while 
Knight and Marcu (2000) trained their system on a 
set of 1,067 sentences from Ziff-Davis magazine 
articles and evaluated their results on grammatical-
ity and importance rated by humans. Both reported 
success in their evaluation criteria. However, nei-
ther of them reported their techniques? effective-
ness in improving the overall performance of 
automatic text summarization systems. The goal of 
this pilot study is set to answer this question and 
provide a guideline for future research. 
Section 2 gives an overview of Knight and Marcu?s 
sentence compression algorithm that we used to 
compressed summary sentences. Section 3 de-
scribes the multi-document summarization system, 
NeATS, which was used as our testbed. Section 4 
introduces a recall-based unigram co-occurrence 
automatic evaluation metric. Section 5 presents the 
experimental design. Section 6 shows the empirical 
results. Section 7 concludes this paper and dis-
cusses future directions. 
 2 A Noisy-Channel Model for Sentence 
Compression 
Knight and Marcu (K&M) (2000) introduced two 
sentence compression algorithms, one based on the 
noisy-channel model and the other decision-based. 
We use the noisy-channel model in our experi-
ments since it is able to generate a list of ranked 
candidates, while the decision-based is not. 
? Source model P(s) ? The compressed sen-
tence language model. This would assign low 
probability to short sentences with undesir-
able features, for example, ungrammatical or 
too short. 
? Channel model P(t | s) ? Given a compressed 
sentence s, the channel model assigns the 
probability of an original sentence, t, which 
could have been generated by s.  
? Decoder ? Given the original sentence t, find 
the best short sentence s generated from t, i.e. 
maximizing P(s | t). This is equivalent to 
maximizing P(t | s)?P(s). 
We used K&M?s sentence compression algorithm 
as it was and did not retrain on new corpus. We 
also adopted the compression length-adjusted log 
probability to avoid the tendency of selecting very 
short compressions. Figure 1 shows a list of com-
pressions for the sentence ?In Louisiana, the hurri-
cane landed with wind speeds of about 120 miles 
per hour and caused severe damage in small 
coastal centres such as Morgan City, Franklin and 
New Iberia.? ranked according to their length-
adjusted log-probability. 
3 NeATS ? a Multi-Document Summarization 
System 
NeATS (Lin and Hovy, 2002) is an extraction-
based multi-document summarization system. It is 
among the top two performers in DUC 2001 and 
2002 (Over and Liggett, 2002). It consists of three 
main components: 
? Content Selection ? The goal of content selec-
tion is to identify important concepts men-
tioned in a document collection. NeATS 
computes the likelihood ratio ? (Dunning, 
1993) to identify key concepts in unigrams, bi-
grams, and trigrams, and clusters these con-
cepts in order to identify major subtopics 
within the main topic. Each sentence in the 
document set is then ranked, using the key 
concept structures. These n-gram key concepts 
are called topic signatures (Lin and Hovy 
2000). We used key n-grams to rerank com-
pressions in our experiments. 
? Content Filtering ? NeATS uses three different 
filters: sentence position, stigma words, and 
maximum marginal relevancy. Sentence posi-
tion has been used as a good content filter since 
the late 60s (Edmundson, 1969). We apply a 
simple sentence filter that only retains the 10 
lead sentences. Some sentences start with 
stigma words such as conjunctions, quotation 
marks, pronouns, and the verb ?say? and its de-
rivatives usually cause discontinuity in summa-
ries. We simply reduce the scores of these 
sentences to demote their ranks and avoid in-
cluding them in summaries of small sizes. To 
Number of Words Adjusted Log-Prob Raw Log-Prob Sentence
14 -9.212 -128.967 In Louisiana, the hurricane landed with wind speeds of about 120 miles per hour.
14 -9.216 -129.022 The hurricane landed and caused severe damage in small centres such as Morgan City.
12 -9.252 -111.020 In Louisiana, the hurricane landed with wind speeds and caused severe damage.
14 -9.315 -130.406 In Louisiana the hurricane landed with wind speeds of about 120 miles per hour.
12 -9.372 -112.459 In Louisiana the hurricane landed with wind speeds and caused severe damage.
12 -9.680 -116.158 The hurricane landed with wind speeds of about 120 miles per hour.
10 -9.821 -98.210 The hurricane landed with wind speeds and caused severe damage.
13 -9.986 -129.824 The hurricane landed and caused damage in small centres such as Morgan City.
13 -10.023 -130.299 In Louisiana hurricane landed with wind speeds of about 120 miles per hour.
13 -10.048 -130.620 The hurricane landed and caused severe damage in centres such as Morgan City.
9 -10.053 -90.477 In Louisiana, the hurricane landed and caused severe damage.
13 -10.091 -131.183 In Louisiana, hurricane landed with wind speeds of about 120 miles per hour.
13 -10.104 -131.356 In Louisiana, the hurricane landed and caused severe damage in small coastal centres.
9 -10.213 -91.915 In Louisiana the hurricane landed and caused severe damage.
11 -10.214 -112.351 In Louisiana hurricane landed with wind speeds and caused severe damage.
Figure 1. Top 15 compressions ranked by their adjusted log-probability for sentence ?In Louisi-
ana, the hurricane landed with wind speeds of about 120 miles per hour and caused severe damage 
in small coastal centres such as Morgan City, Franklin and New Iberia.?
 address the redundancy problem, we use a sim-
plified version of CMU?s MMR (Goldstein et 
al., 1999) algorithm.  A sentence is added to 
the summary if and only if its content has less 
than X percent overlap with the summary. 
? Content Presentation ? To ensure coherence of 
the summary, NeATS pairs each sentence with 
an introduction sentence. It then outputs the fi-
nal sentences in their chronological order. 
We ran NeATS to generate summaries of different 
sizes that were used as our test bed. The topic sig-
natures created in the process were used to rerank 
compressions. We describe the automatic evalua-
tion metric used in our experiments in the next sec-
tion.   
4 Unigram Co-Occurrence Metric 
In a recent study (Lin and Hovy, 2003a), we 
showed that the recall-based unigram co-
occurrence automatic scoring metric correlates 
highly with human evaluation and has high recall 
and precision in predicting the statistical signifi-
cance of results comparing with its human counter-
part. The idea is to measure the content similarity 
between a system extract and a manual summary 
using simple n-gram overlap. A similar idea called 
IBM BLEU score has proved successful in auto-
matic machine translation evaluation (NIST, 2002; 
Papineni et al, 2001). For summarization, we can 
express the degree of content overlap in terms of n-
gram matches as the following equation: 
)1(
)(
)(
}{
}{
? ?
? ?
? ??
? ??
?
?
=
UnitsModelC Cgramn
UnitsModelC Cgramn
match
n gramnCount
gramnCount
C  
Model units are segments of manual summaries. 
They are typically either sentences or elementary 
discourse units as defined by Marcu (1999). Count-
match(n-gram) is the maximum number of n-grams 
co-occurring in a system extract and a model unit. 
Count(n-gram) is the number of n-grams in the 
model unit. Notice that the average n-gram cover-
age score, Cn, as shown in equation 1, is a recall-
based metric, since the denominator of equation 1 
[0.40-0.50)
18,284
[0.50-0.60)
115,240
[0.70-0.80)
212,116
0
10000
20000
30000
40000
50000
60000
70000
80000
90000
100000
110000
120000
130000
140000
150000
160000
170000
180000
190000
200000
210000
220000
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00
Unigram Co-occurrence Scores
# 
of
 In
st
an
ce
s
100 ? 5 Words
150 ? 5 Words
200 ? 5 Words
200 Words
1G Avg: 0.63
CMP Ratio: 0.52
150 Words
1G Avg: 0.55
CMP Ratio: 0.64
100 Words
1G Avg: 0.42
CMP Ratio: 0.76
Figure 2. AP900424-0035 100, 150, and 200 words oracle extract instance distributions.
 is the sum total of the number of n-grams occurring 
in the model summary instead of the system sum-
mary and only one model summary is used for each 
evaluation. In summary, the unigram co-occurrence 
statistics we use in the following sections are based 
on the following formula: 
)2(logexp),( ???
?
???
?= ?
=
j
in
nn CwjiNgram  
Where j ? i, i and j range from 1 to 4, and wn is 
1/(j-i+1). Ngram(1, 4) is a weighted variable length 
n-gram match score similar to the IBM BLEU 
score; while Ngram(k, k), i.e. i = j = k, is simply the 
average k-gram co-occurrence score Ck. In this 
study, we set i = j = 1, i.e. unigram co-occurrence 
score.   
With an automatic scoring metric defined, we de-
scribe the experimental setup in the next section. 
5 Experimental Designs 
As stated in the introduction, we aim to investigate 
the effectiveness of sentence compression on over-
all system performance. If we can have a lossless 
compression function that compresses a given sen-
tence to a minimal length and still retains the most 
important content of the sentence then we would be 
able to pack more information content into a fixed 
size summary. Figure 2 illustrates this effect 
<multi size="225" docset="d19d" org-size="227" comp-size="227">  
Lawmakers clashed on 06/23/1988 over the question of counting illegal aliens in the 1990 Census, debating whether following 
the letter of the Constitution results in a system that is unfair to citizens. The forum was a Census subcommittee hearing on bills 
which would require the Census Bureau to figure out whether people are in the country legally and, if not, to delete them from the 
counts used in reapportioning seats in the House of Representatives. Simply put, the question was who should be counted as a 
person and who, if anybody, should not. The point at issue in Senate debate on a new immigration bill was whether illegal aliens 
should be counted in the process that will reallocate House seats among states after the 1990 census. The national head count will 
be taken April 1, 1990. In a blow to California and other states with large immigrant populations, the Senate voted on 09/29/1989 
to bar the Census Bureau from counting illegal aliens in the 1990 population count. At stake are the number of seats in Congress 
for California, Florida, New York, Illinois, Pennsylvania and other states that will be reapportioned on the basis of next year's 
census. Federal aid to states also is frequently based on population counts, so millions of dollars in grants and other funds made 
available on a per capita basis would be affected.  
</multi> 
Figure 3. 227-word summary for topic D19 (?Aliens?). 
<multi size="225" docset="d19d" org-size="227" comp-size="98"> 
Lawmakers clashed over question of counting illegal aliens Census debating whether results. Forum was a Census hearing, to 
delete them from the counts. Simply put question was who should be counted and who, if anybody, should not. Point at issue in 
debate on an immigration bill was whether illegal aliens should be counted. National count will be taken April 1, 1990. Senate 
voted to bar Census Bureau from counting illegal aliens. At stake are number of seats for California New York. Aid to states is 
frequently based on population counts, so millions would be affected.  
</multi> 
Figure 4. Compressed summary for topic D19 ("Aliens"), 98 words. 
<DOC> 
<TEXT> 
<S SNTNO="1">Elizabeth Taylor battled pneumonia at her hospital, assisted by a ventilator, doctors say.</S> 
<S SNTNO="2">Hospital officials described her condition late Monday as stabilizing after a lung biopsy to determine the cause 
of the pneumonia.</S> 
<S SNTNO="3">Analysis of the tissue sample was expected to be complete by Thursday.</S> 
<S SNTNO="4">Ms. Sam, spokeswoman said "it is serious, but they are really pleased with her progress.</S> 
<S SNTNO="5">She's not well.</S> 
<S SNTNO="6">She's not on her deathbed or anything.</S> 
<S SNTNO="7">Another spokeswoman, Lisa Del Favaro, said Miss Taylor's family was at her bedside.</S> 
<S SNTNO="8">During a nearly fatal bout with pneumonia in 1961, Miss Taylor underwent a tracheotomy to help her 
breathe.</S> 
</TEXT> 
Figure 5. A manual summary for document AP900424-0035. 
 graphically. For document AP900424-0035, which 
consists of 23 sentences or 417 words, we generate 
the full permutation set of sentence extracts, i.e., all 
possible 100?5, 150?5, and 200?5 words extracts. 
The 100?5 words extract at average compression 
ratio of 0.76 has most of its unigram co-occurrence 
score instances (18,284/61,762 ? 30%) falling 
within the interval between 0.40 and 0.50, i.e., the 
expected performance of an extraction-based sys-
tem would be between 0.40 and 0.50. The 150?5 
words extract at lower compression ratio of 0.64 
has most of its instances between 0.50 and 0.60 
(115,240/377,933 ? 30%) and the 200?5 words 
extract at compression ratio of 0.52 has most of its 
instances between 0.70 and 0.80 (212,116/731,819 
? 29%). If we can compress 150 or 200-word 
summaries into 100 words and retain their impor-
tant content, we would be to achieve an average 
30% to 50% increase in performance.  
The question is: can an off-the-shelf sentence 
compression algorithm such as K&M?s noisy-
channel model achieve this? If the answer is yes, 
then how much performance gain can be achieved? 
If not, are there other ways to use sentence 
compression to improve system performance? To 
improve system performance? To answer these 
questions, we conduct the following experiments 
over 30 DUC 2001 topic sets: 
(1) Run NeATS through the 30 DUC 2001 
topic sets and generate summaries of 
size: 100, 120, 125, 130, 140, 150, 160, 
175, 200, 225, 250, 275, 300, 325, 350, 
375, and 400. 
(2) Run K&M?s sentence compression algo-
rithm over all summary sentences (run 
KM). For each summary sentence, we 
have a set of candidate compressions. 
See Figure 1 for example. 
(3) Rerank each candidate compression set 
using different scoring methods: 
a. Rerank each candidate compression 
set using topic signatures (run SIG). 
b. Rerank each candidate compression 
set using combination of KM and 
SIG scores using linear interpola-
tion of topic signature score (SIG) 
and K&M?s log-probability score 
(KM). We use the following for-
mula in this experiment: 
Avg Var Std AvgCR VarCR StdCR
KM 0.227 0.005 0.068 0.412 0.016 0.125
ORACLE 0.287 0.006 0.078 0.471 0.009 0.092
ORG 0.253 0.006 0.075 0.000 0.000 0.000
SIG 0.244 0.006 0.078 0.537 0.007 0.085
SIGKMa 0.242 0.006 0.077 0.370 0.015 0.123
SIGKMb 0.248 0.006 0.079 0.372 0.014 0.119
Table 1. Result table for six runs. Avg: mean unigram co-occurrence scores of 
30 topics, Var: variance, Std: standard deviation, AvgCR: mean compression 
ratio, VarCR: variance of compression ratio, and StdCR: standard deviation of 
compression ratio. 
KM ORACLE ORG SIG SIGKMa SIGKMb
KM - -17.123 -7.681 -4.975 -4.474 -6.199
ORACLE - 9.237 11.39 11.98 10.181
ORG - 2.411 2.949 1.208
SIG - 0.508 -1.168
SIGKMa - -1.682
SIGKMb -
Sentence Compression Z-Test (30 instances) Pairwise Observed Z-
Score 95% (Size: 100)
Table 2. Pairwise Z-test for six runs shown in Table 1 (? = 5%). Light gray 
(green) indicates runs on the column that are significantly better than runs on 
the row; dark gray indicates significantly worse.  
 SIGKM=??SIG + (1-?)?KM 
? is set to 2/3 (run SIGKMa). 
c. Rerank each candidate compression 
set using SIG score first and then 
KM is used to break ties (run 
SIGKMb). 
d. Rerank each candidate compression 
set using unigram co-occurrence 
score against manual references.  
This gives the upper bound for the 
K&M?s algorithm applied to the 
output generated by NeATS (run 
ORACLE). 
(4) Select the best compression combination. 
For a given length constraint, for exam-
ple 100 words, we produce the final re-
sult by selecting a compressed summary 
across different summary sizes for each 
topic that fits the length limit (<= 100?5 
words), and output them as the final 
summary. For example, we found that a 
227-word summary for topic D19 could 
be compressed to 98 words using the 
topic signature reranking method. The 
compressed summary would then be se-
lected as the final summary for topic 
D19. Figure 3 shows the original 227- 
word summary and Figure 4 shows its 
compressed version.  
There were 30 test topics in DUC 2001 and 
each topic contained about 10 documents. For 
each topic, four summaries of approximately 50, 
100, 200, and 400 words were created manually 
as the ?ideal? model summaries. We used the set 
of 100-word manual summaries as our refer-
ences in our experiments. An example manual 
summary is shown in Figure 5. We report re-
sults of these experiments in the next section.  
 
6 Results 
Tables 1 and 2 summarize the results. Analyzing all 
runs according to these two tables, we made the 
following observations. 
(1) Selecting compressed sentences using 
length-adjusted scores (K&M) without any 
modification performed significantly worse 
(at ? = 5%, table cells marked in dark gray 
in Table 2) than all other runs. This indi-
cates we cannot rely on pure syntactic-
based compression to improve overall sys-
tem performance although the compression 
algorithm performed well in the individual 
sentence level.  
(2) The original run (ORG) achieved an aver-
age unigram co-occurrence score of 0.253 
and was significantly better than all other 
runs except the ORACLE and SIGKMb 
runs. This result was a little bit discourag-
ing; it means that no/most reranking is not 
useful, and indicates that we need to invest 
more time in finding a better way to rank 
the compressed sentences. Pure syntactic 
(noisy-channel model), shallow semantic 
(by topic signatures), or simple combina-
tions of them did not improve system per-
formance and in some cases even degraded 
it. 
(3) Comparing the ORACLE (0.287) run with 
the average human performance of 0.270 
(not shown in the Tables), we should re-
main optimistic about finding a better rank-
ing algorithm to select the best 
compression. However, the low human 
performance posts a challenge for machine 
learning algorithms to learn this function. 
We provided more in-depth discussion of 
this issue in other papers (Lin and Hovy, 
2002; Lin and Hovy 2003b). 
(4) That the ORACLE run did not achieve 
higher score also implied the following: 
a. The sentence compression algo-
rithm that we used might drop 
some important content. Therefore 
the compressed summaries did not 
achieve 20% increase in perform-
ance as Figure 1 might suggest 
when systems were allowed to 
output 100% longer  summary than 
the given constraint (i.e. if a 100-
word summary is requested, a sys-
tem can provide a 200-word sum-
mary in response.)  
b. The way we generated our com-
pressed summaries was not effec-
tive. We might need to optimize 
and select compressions according 
to a global optimization function. 
For example, if some important 
 content is mentioned in sentences 
already included in a summary, we 
would want to take this into ac-
count and to add compressions 
with new information to the final 
summary. 
7 Conclusions 
In this paper we presented an empirical study of the 
effectiveness of applying sentence compression to 
improve summarization performance. We used a 
good sentence compression algorithm, compared 
the performance of five different ranking algo-
rithms, and found that pure a-sentence-at-a-time 
syntactic or shallow semantic-based reranking was 
not enough to boost system performance. However, 
the significant difference between the ORACLE 
run and the original run (ORG) indicated there is 
potential in sentence compression but we need to 
find a better compression selection function that 
should take into account global cross-sentence op-
timization. This indicated local optimization at the 
sentence level such as Knight and Marcu?s (2000) 
noisy-channel model is not enough when our goal 
is to find the best compressed summaries not the 
best compressed sentences.  In the future, we 
would like to apply a similar methodology to dif-
ferent text units, for example, sub-sentence units 
such as elementary discourse unit (Marcu, 1999) 
and a larger corpus, for example, DUC 2002 and 
DUC 2003. We want to explore compression tech-
niques to go beyond simple sentence extraction. 
References 
O. Buyukkokten, H. Garcia-Molina, A. Paepcke. 
2001. Seeing the Whole in Parts: Text Summa-
rization for Web Browsing on Handheld De-
vices. The 10th International WWW 
Conference (WWW10). Hong Kong, China. 
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and 
J. Tait. 1998. Practical Simplification of Eng-
lish Newspaper Text to Assist Aphasic Read-
ers. In Proceedings of AAAI-98 Workshop on 
Integrating Artificial Intelligence and Assistive 
Technology, Madison, WI, USA. 
H.H. Chen, J.J. Kuo, and T.C. Su 2003. Clustering 
and Visualization in a Multi-Lingual Multi-
Document Summarization System. In Proceed-
ings of 25th European Conference on Informa-
tion Retrieval Research, Lecture Note in 
Computer Science, April 14-16, Pisa, Italy. 
S. Corston-Oliver. 2001. Text Compaction for Dis-
play on Very Small Screens. In Proceedings of 
the Workshop on Automatic Summarization 
(WAS 2001), Pittsburgh, PA, USA. 
DUC. 2002. The Document Understanding Confer-
ence.  http://duc.nist.gov. 
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence.  Computa-
tional Linguistics 19, 61?74. 
H.P. Edmundson. 1969. New Methods in Auto-
matic Abstracting.  Journal of the Association 
for Computing Machinery.  16(2). 
T. Fukusima and M. Okumura. 2001. Text Summa-
rization Challenge Text Summarization 
Evaluation in Japan. In Proceedings of the 
Workshop on Automatic Summarization (WAS 
2001), Pittsburgh, PA, USA. 
J. Goldstein, M. Kantrowitz, V. Mittal, and J. Car-
bonell. 1999. Summarizing Text Documents: 
Sentence Selection and Evaluation Metrics. In 
Proceedings of the 22nd International ACM 
Conference on Research and Development in 
Information Retrieval (SIGIR-99), Berkeley, 
CA, USA, 121?128. 
G. Grefenstette. 1998. Producing Intelligent Tele-
graphic Text Reduction to Provide an Audio 
Scanning Service for the Blind. In Working 
Notes of the AAAI Spring Symposium on In-
telligent Text Summarization, Stanford Univer-
sity, CA, USA, 111?118. 
E. Hovy and C.-Y. Lin. 1999. Automatic Text 
Summarization in SUMMARIST. In I. Mani 
and M. Maybury (eds), Advances in Automatic 
Text Summarization, 81?94. MIT Press. 
H. Jing. 2000. Sentence simplification in automatic 
text summarization. In the Proceedings of the 
6th Applied Natural Language Processing Con-
ference (ANLP'00). Seattle, Washington, USA. 
K. Knight and D. Marcu. 2000. Statistics-Based 
Summarization ? Step One: Sentence Com-
 pression. In Proceedings of AAAI-2000, Aus-
tin, TX, USA. 
J. Kupiec, J. Pederson, and F. Chen. 1995. A 
Trainable Document Summarizer. In Proceed-
ings of the 18th International ACM Conference 
on Research and Development in Information 
Retrieval (SIGIR-95), Seattle, WA, USA, 68?
73. 
C.-Y. Lin and E. Hovy. 2000. The Automated Ac-
quisition of Topic Signatures for Text Summa-
rization. In Proceedings of the 18th 
International Conference on Computational 
Linguistics (COLING 2000), Saarbr?cken, 
Germany. 
C.-Y. Lin and E. Hovy. 2002. From Single to 
Multi-document Summarization: A Prototype 
System and its Evaluation. In Proceedings of 
the 40th Anniversary Meeting of the Associa-
tion for Computational Linguistics (ACL-
2002), Philadelphia, PA, U.S.A. 
C.-Y. Lin and E. Hovy. 2002. Manual and Auto-
matic Evaluations of Summaries. In 
Proceedings of the Workshop on Automatic 
Summarization, post-conference workshop of 
ACL-2002, pp. 45-51, Philadelphia, PA, USA. 
C.-Y. Lin and E. Hovy. 2003a. Automatic Evalua-
tion of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of the 
2003 Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Can-
ada. 
C.-Y. Lin and E. Hovy 2003b. The Potential and 
Limitations of Sentence Extraction for Summa-
rization. In Proceedings of the Workshop on 
Automatic Summarization post-conference 
workshop of HLT-NAACL-2003, Edmonton, 
Canada. 
H.P. Luhn. 1969. The Automatic Creation of Lit-
erature Abstracts. IBM Journal of Research and 
Development. 2(2). 
D. Marcu. 1999. Discourse trees are good indica-
tors of importance in text. In I. Mani and M. 
Maybury (eds), Advances in Automatic Text 
Summarization, 123?136. MIT Press. 
K. McKeown, Barzilay, D. Evans, V. Hatzivassi-
loglou, J. L. Klavans, A. Nenkova, C. Sable, B. 
Schiffman, S. Sigelman. 2002. Tracking and 
Summarizing News on a Daily Basis with Co-
lumbia?s Newsblaster. In Proceedings of Hu-
man Language Technology Conference 2002 
(HLT 2002). San Diego, CA, USA. 
NIST. 2002. Automatic Evaluation of Machine 
Translation Quality using N-gram Co-
Occurrence Statistics. 
P. Over and W. Liggett. 2002. Introduction to 
DUC-2002: an Intrinsic Evaluation of Generic 
News Text Summarization Systems. In Pro-
ceedings of Workshop on Automatic Summari-
zation (DUC 2002), Philadelphia, PA, USA. 
http://www-nlpir.nist.gov/projects/duc/pubs/ 
2002slides/overview.02.pdf 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2001. 
Bleu: a Method for Automatic Evaluation of 
Machine Translation. IBM Research Report 
RC22176 (W0109-022). 
D.R. Radev and K.R. McKeown. 1998. Generating 
Natural Language Summaries from Multiple 
On-line Sources. Computational Linguistics, 
24(3):469?500. 
T. Strzalkowski, G. Stein, J. Wang, and B, Wise. A 
Robust Practical Text Summarizer. 1999. In I. 
Mani and M. Maybury (eds), Advances in 
Automatic Text Summarization, 137?154. MIT 
Press. 
M. White, T. Korelsky, C. Cardie, V. Ng, D. 
Pierce, and K. Wagstaff. 2001. Multidocument 
Summarization via Information Extraction. In 
Proceedings of Human Language Technology 
Conference 2001 (HLT 2001), San Diego, CA, 
USA. 
ROUGE: A Package for Automatic Evaluation of Summaries 
Chin-Yew Lin 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
cyl@isi.edu 
 
Abstract 
ROUGE stands for Recall-Oriented Understudy for 
Gisting Evaluation. It includes measures to auto-
matically determine the quality of a summary by 
comparing it to other (ideal) summaries created by 
humans. The measures count the number of over-
lapping units such as n-gram, word sequences, and 
word pairs between the computer-generated sum-
mary to be evaluated and the ideal summaries cre-
ated by humans. This paper introduces four different 
ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, 
and ROUGE-S included in the ROUGE summariza-
tion evaluation package and their evaluations. Three 
of them have been used in the Document Under-
standing Conference (DUC) 2004, a large-scale 
summarization evaluation sponsored by NIST. 
1 Introduction 
Traditionally evaluation of summarization involves 
human judgments of different quality metrics, for 
example, coherence, conciseness, grammaticality, 
readability, and content (Mani, 2001). However, 
even simple manual evaluation of summaries on a 
large scale over a few linguistic quality questions 
and content coverage as in the Document Under-
standing Conference (DUC) (Over and Yen, 2003) 
would require over 3,000 hours of human efforts. 
This is very expensive and difficult to conduct in a 
frequent basis. Therefore, how to evaluate summa-
ries automatically has drawn a lot of attention in the 
summarization research community in recent years. 
For example, Saggion et al (2002) proposed three 
content-based evaluation methods that measure 
similarity between summaries. These methods are: 
cosine similarity, unit overlap (i.e. unigram or bi-
gram), and longest common subsequence. However, 
they did not show how the results of these automatic 
evaluation methods correlate to human judgments. 
Following the successful application of automatic 
evaluation methods, such as BLEU (Papineni et al, 
2001), in machine translation evaluation, Lin and 
Hovy (2003) showed that methods similar to BLEU, 
i.e. n-gram co-occurrence statistics, could be applied 
to evaluate summaries. In this paper, we introduce a 
package, ROUGE, for automatic evaluation of sum-
maries and its evaluations. ROUGE stands for Re-
call-Oriented Understudy for Gisting Evaluation. It 
includes several automatic evaluation methods that 
measure the similarity between summaries. We de-
scribe ROUGE-N in Section 2, ROUGE-L in Section 
3, ROUGE-W in Section 4, and ROUGE-S in Section 
5. Section 6 shows how these measures correlate 
with human judgments using DUC 2001, 2002, and 
2003 data. Section 7 concludes this paper and dis-
cusses future directions. 
2 ROUGE-N: N-gram Co-Occurrence Statistics  
Formally, ROUGE-N is an n-gram recall between a 
candidate summary and a set of reference summa-
ries. ROUGE-N is computed as follows: 
 
ROUGE-N 
? ?
? ?
? ?
? ?=
}{
}{
)(
)(
SummariesReferenceS Sgram
SummariesReferemceS Sgram
match
n
n
n
n
gramCount
gramCount
 (1) 
 
Where n stands for the length of the n-gram, 
gramn, and Countmatch(gramn) is the maximum num-
ber of n-grams co-occurring in a candidate summary 
and a set of reference summaries.  
It is clear that ROUGE-N is a recall-related meas-
ure because the denominator of the equation is the 
total sum of the number of n-grams occurring at the 
reference summary side. A closely related measure, 
BLEU, used in automatic evaluation of machine 
translation, is a precision-based measure. BLEU 
measures how well a candidate translation matches 
a set of reference translations by counting the per-
centage of n-grams in the candidate translation over-
lapping with the references. Please see Papineni et 
al. (2001) for details about BLEU. 
Note that the number of n-grams in the denomina-
tor of the ROUGE-N formula increases as we add 
more references. This is intuitive and reasonable 
because there might exist multiple good summaries. 
Every time we add a reference into the pool, we ex-
pand the space of alternative summaries. By con-
trolling what types of references we add to the 
reference pool, we can design evaluations that focus 
on different aspects of summarization. Also note 
that the numerator sums over all reference summa-
ries. This effectively gives more weight to matching 
n-grams occurring in multiple references. Therefore 
a candidate summary that contains words shared by 
more references is favored by the ROUGE-N meas-
ure. This is again very intuitive and reasonable be-
cause we normally prefer a candidate summary that 
is more similar to consensus among reference sum-
maries. 
2.1 Multiple References 
So far, we only demonstrated how to compute 
ROUGE-N using a single reference. When multiple 
references are used, we compute pairwise summary-
level ROUGE-N between a candidate summary s and 
every reference, ri, in the reference set. We then 
take the maximum of pairwise summary-level 
ROUGE-N scores as the final multiple reference 
ROUGE-N score. This can be written as follows: 
 
ROUGE-Nmulti  = argmaxi ROUGE-N(ri,s)  
 
This procedure is also applied to computation of 
ROUGE-L (Section 3), ROUGE-W (Section 4) , and 
ROUGE-S (Section 5). In the implementation, we use 
a Jackknifing procedure. Given M references, we 
compute the best score over M sets of M-1 refer-
ences. The final ROUGE-N score is the average of 
the M ROUGE-N scores using different M-1 refer-
ences.  The Jackknifing procedure is adopted since 
we often need to compare system and human per-
formance and the reference summaries are usually 
the only human summaries available. Using this 
procedure, we are able to estimate average human 
performance by averaging M ROUGE-N scores of 
one reference vs. the rest M-1 references. Although 
the Jackknif ing procedure is not necessary when we 
just want to compute ROUGE scores using multiple 
references, it is applied in all ROUGE score compu-
tations in the ROUGE evaluation package. 
In the next section, we describe a ROUGE measure 
based on longest common subsequences between 
two summaries. 
3 ROUGE-L: Longest Common Subsequence  
A sequence Z = [z1, z2, ..., zn] is a subsequence of 
another sequence X = [x1, x2, ..., xm], if there exists a 
strict increasing sequence [i1, i2, ..., ik] of indices of 
X such that for all j = 1, 2, ..., k, we have xij = zj  
(Cormen et al, 1989). Given two sequences X and 
Y, the longest common subsequence (LCS) of X and 
Y is a common subsequence with maximum length. 
LCS has been used in identifying cognate candi-
dates during construction of N-best translation lexi-
con from parallel text. Melamed (1995) used the 
ratio (LCSR) between the length of the LCS of two 
words and the length of the longer word of the two 
words to measure the cognateness between them. 
He used LCS as an approximate string matching 
algorithm. Saggion et al (2002) used normalized 
pairwise LCS to compare simila rity between two 
texts in automatic summarization evaluation.  
3.1 Sentence-Level LCS 
To apply LCS in summarization evaluation, we 
view a summary sentence as a sequence of words. 
The intuition is that the longer the LCS of two 
summary sentences is, the more similar the two 
summaries are. We propose using LCS-based F-
measure to estimate the similarity between two 
summaries X of length m and Y of length n, assum-
ing X is a reference summary sentence and Y is a 
candidate summary sentence, as follows: 
 
Rlcs 
m
YXLCS ),(=       (2) 
Plcs 
n
YXLCS ),(=       (3) 
Flcs  
lcslcs
lcslcs
PR
PR
2
2)1(
b
b
+
+= (4) 
 
Where LCS(X,Y) is the length of a longest com-
mon subsequence of X and Y, and ? = Plcs/Rlcs when 
?Flcs/?Rlcs_=_?Flcs/?Plcs.  In DUC, ? is set to a very 
big number (?  8 ). Therefore, only Rlcs is consid-
ered. We call the LCS-based F-measure, i.e. Equa-
tion 4, ROUGE-L. Notice that ROUGE-L is 1 when X 
= Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. 
there is nothing in common between X and Y. F-
measure or its equivalents has been shown to have 
met several theoretical criteria in measuring accu-
racy involving more than one factor (Van Rijsber-
gen, 1979). The composite factors are LCS-based 
recall and precision in this case. Melamed et al 
(2003) used unigram F-measure to estimate machine 
translation quality and showed that unigram F-
measure was as good as BLEU.  
One advantage of using LCS is that it does not re-
quire consecutive matches but in-sequence matches 
that reflect sentence level word order as n-grams. 
The other advantage is that it automatically includes 
longest in-sequence common n-grams, therefore no 
predefined n-gram length is necessary.  
ROUGE-L as defined in Equation 4 has the prop-
erty that its value is less than or equal to the min i-
mum of unigram F-measure of X and Y. Unigram 
recall reflects the proportion of words in X (refer-
ence summary sentence) that are also present in Y 
(candidate summary sentence); while unigram pre-
cision is the proportion of words in Y that are also in 
X. Unigram recall and precision count all co-
occurring words regardless their orders; while 
ROUGE-L counts only in-sequence co-occurrences.  
By only awarding credit to in-sequence unigram 
matches, ROUGE-L also captures sentence level 
structure in a natural way. Consider the following 
example: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
 
We only consider ROUGE-2, i.e. N=2, for the pur-
pose of explanation. Using S1 as the reference and 
S2 and S3 as the candidate summary sentences, S2 
and S3 would have the same ROUGE-2 score, since 
they both have one bigram, i.e. ?the gunman?. How-
ever, S2 and S3 have very different meanings. In the 
case of ROUGE-L, S2 has a score of 3/4 = 0.75 and 
S3 has a score of 2/4 = 0.5, with ? = 1. Therefore S2 
is better than S3 according to ROUGE-L. This exam-
ple also illustrated that ROUGE-L can work reliably 
at sentence level. 
However, LCS suffers one disadvantage that it 
only counts the main in-sequence words; therefore, 
other alternative LCSes and shorter sequences are 
not reflected in the final score. For example, given 
the following candidate sentence: 
S4. the gunman police killed 
Using S1 as its reference, LCS counts either ?the 
gunman? or ?police killed?, but not both; therefore, 
S4 has the same ROUGE-L score as S3. ROUGE-2 
would prefer S4 than S3. 
3.2 Summary-Level LCS 
Previous section described how to compute sen-
tence-level LCS-based F-measure score. When ap-
plying to summary-level, we take the union LCS 
matches between a reference summary sentence, ri, 
and every candidate summary sentence, cj. Given a 
reference summary of u sentences containing a total 
of m words and a candidate summary of v sentences 
containing a total of n words, the summary-level 
LCS-based F-measure can be computed as follows: 
Rlcs m
CrLCS
u
i
i?
=
?
= 1
),(
      (5) 
Plcs n
CrLCS
u
i
i?
=
?
= 1
),(
      (6) 
Flcs  
lcslcs
lcslcs
PR
PR
2
2)1(
b
b
+
+=    (7) 
 
Again ? is set to a very big number (?  8 ) in 
DUC, i.e. only Rlcs is considered. ),( CrLCS i? is the 
LCS score of the union longest common subse-
quence between reference sentence ri and candidate 
summary C. For example, if ri = w1 w2 w3 w4 w5, and 
C contains two sentences: c1 = w1 w2 w6 w7 w8 and c2 
= w1 w3 w8 w9 w5, then the longest common subse-
quence of ri and c1 is ?w1 w2? and the longest com-
mon subsequence of ri and c2 is ?w1 w3 w5?. The 
union longest common subsequence of ri, c1, and c2 
is ?w1 w2 w3 w5? and ),( CrLCS i? = 4/5. 
3.3 ROUGE-L vs. Normalized Pairwise LCS 
The normalized pairwise LCS proposed by Radev et 
al. (page 51, 2002) between two summaries S1 and 
S2, LCS(S1 ,S2)MEAD , is written as follows: 
 
? ?
? ?
? ?
? ? ??
+
+
1 2
1 2 12
)()(
),(max),(max
Ss Ss ji
Ss Ss jiSsjiSs
i j
i j ij
slengthslength
ssLCSssLCS  (8) 
 
Assuming S1 has m words and S2 has n words, 
Equation 8 can be rewritten as Equation 9 due to 
symmetry: 
 
nm
ssLCS
Ss jiSsi j
+
? ? ?1 2 ),(max*2                        (9) 
 
We then define MEAD LCS recall (Rlcs-MEAD) and 
MEAD LCS precision (Plcs-MEAD) as follows: 
 
 Rlcs-MEAD m
ssLCS
Ss jiSsi j? ? ?= 1 2 ),(max       (10) 
Plcs-MEAD n
ssLCS
Ss jiSsi j? ? ?= 1 2 ),(max        (11) 
 
We can rewrite Equation (9) in terms of Rlcs-MEAD 
and Plcs-MEAD with a constant parameter ? = 1 as fol-
lows: 
LCS(S1 ,S2)MEAD  
MEADlcsMEADlcs
MEADlcsMEADlcs
PR
PR
--
--
+
+= 2
2 )1(
b
b  (12) 
Equation 12 shows that normalized pairwise LCS 
as defined in Radev et al (2002) and implemented 
in MEAD is also a F-measure with ? = 1. Sentence-
level normalized pairwise LCS is the same as 
ROUGE-L with ? = 1. Besides setting ? = 1, sum-
mary-level normalized pairwise LCS is different 
from ROUGE-L in how a sentence gets its LCS score 
from its references. Normalized pairwise LCS takes 
the best LCS score while ROUGE-L takes the union 
LCS score. 
4 ROUGE-W: Weighted Longest Common Sub-
sequence 
LCS has many nice properties as we have described 
in the previous sections. Unfortunately, the basic 
LCS also has a problem that it does not differentiate 
LCSes of different spatial relations within their em-
bedding sequences. For example, given a reference 
sequence X and two candidate sequences Y1 and Y2 
as follows: 
 
X:  [A B C D E F G] 
Y1: [A B C D H I K] 
Y2:  [A H B K C I D] 
 
Y1 and Y2 have the same ROUGE-L score. How-
ever, in this case, Y1 should be the better choice than 
Y2 because Y1 has consecutive matches. To improve 
the basic LCS method, we can simply remember the 
length of consecutive matches encountered so far to 
a regular two dimensional dynamic program table 
computing LCS. We call this weighted LCS 
(WLCS) and use k to indicate the length of the cur-
rent consecutive matches ending at words xi and yj. 
Given two sentences X and Y, the WLCS score of X 
and Y can be computed using the following dynamic 
programming procedure: 
 
(1) For (i = 0; i <=m; i++) 
        c(i,j) = 0  // initialize c-table  
        w(i,j) = 0 // initialize w-table  
(2) For (i = 1; i <= m; i++) 
        For (j = 1; j <= n; j++) 
          If xi = yj Then 
     // the length of consecutive matches at 
     // position i-1 and j -1 
     k  = w(i-1,j-1) 
     c(i,j) = c(i-1,j-1) + f(k+1 ) ? f(k) 
     // remember the length of consecutive 
     // matches at position i, j  
     w(i,j) = k+1 
          Otherwise 
     If c(i-1,j) > c(i,j-1) Then 
    c(i,j) = c(i-1,j) 
    w(i,j) = 0           // no match at i, j 
     Else c(i,j) = c(i,j-1) 
     w(i,j) = 0           // no match at i, j 
(3) WLCS(X,Y) = c(m,n) 
 
Where c is the dynamic programming table, c(i,j) 
stores the WLCS score ending at word xi of X and yj 
of Y, w is the table storing the length of consecutive 
matches ended at c table position i and j, and f is a 
function of consecutive matches at the table posi-
tion, c(i,j). Notice that by providing different 
weighting function f, we can parameterize the 
WLCS algorithm to assign different credit to con-
secutive in-sequence matches.  
The weighting function f must have the property 
that f(x+y) > f(x) + f(y) for any positive integers x 
and y. In other words, consecutive matches are 
awarded more scores than non-consecutive matches. 
For example, f(k)-=-ak ? b when k >= 0, and a, b > 
0. This function charges a gap penalty of ?b for 
each non-consecutive n-gram sequences. Another 
possible function family is the polynomial family of 
the form ka where -a > 1. However, in order to 
normalize the final ROUGE-W score, we also prefer 
to have a function that has a close form inverse 
function. For example, f(k)-=-k2 has a close form 
inverse function f -1(k)-=-k1/2.  F-measure based on 
WLCS can be computed as follows, given two se-
quences X of length m and Y of length n: 
Rwlcs ???
?
???
?= -
)(
),(1
mf
YXWLCS
f       (13) 
Pwlcs ???
?
???
?= -
)(
),(1
nf
YXWLCS
f       (14) 
Fwlcs  
wlcswlcs
wlcswlcs
PR
PR
2
2)1(
b
b
+
+=           (15) 
 
Where f -1 is the inverse function of f. In DUC, ? is 
set to a very big number (?  8 ). Therefore, only 
Rwlcs is considered. We call the WLCS-based F-
measure, i.e. Equation 15, ROUGE-W. Using Equa-
tion 15 and f(k)-=-k2 as the weighting function, the 
ROUGE-W scores for sequences Y1 and Y2 are 0.571 
and 0.286 respectively. Therefore, Y1 would be 
ranked higher than Y2 using WLCS. We use the 
polynomial function of the form ka in the ROUGE 
evaluation package. In the next section, we intro-
duce the skip-bigram co-occurrence statistics. 
5 ROUGE-S: Skip-Bigram Co-Occurrence Sta-
tistics  
Skip-bigram is any pair of words in their sentence 
order, allowing for arbitrary gaps. Skip-bigram co-
occurrence statistics measure the overlap of skip-
bigrams between a candidate translation and a set of 
reference translations. Using the example given in 
Section 3.1: 
 
S1. police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
S4. the gunman police killed 
each sentence has C(4,2)1 = 6 skip-bigrams. For ex-
ample, S1 has the following skip-bigrams: 
(?police killed?, ?police the?, ?police gunman?, 
?killed the?, ?killed gunman?, ?the gunman?)  
S2 has three skip-bigram matches with S1 (?po-
lice the?, ?police gunman?, ?the gunman?), S3 has 
one skip-bigram match with S1 (?the gunman?), and 
S4 has two skip-bigram matches with S1 (?police 
killed?, ?the gunman?).  Given translations X of 
length m and Y of length n, assuming X is a refer-
ence translation and Y is a candidate translation, we 
compute skip-bigram-based F-measure as follows: 
Rskip2 
)2,(
),(2
mC
YXSKIP=           (16) 
Pskip2 
)2,(
),(2
nC
YXSKIP=           (17) 
Fskip2 
2
2
2
22
2)1(
skipskip
skipskip
PR
PR
b
b
+
+=   (18) 
 
Where SKIP2(X,Y) is the number of skip-bigram 
matches between X and Y, ? controlling the relative 
importance of Pskip2 and Rskip2, and  C is the combi-
nation function. We call the skip-bigram-based F-
measure, i.e. Equation 18, ROUGE-S. 
Using Equation 18 with ? = 1 and S1 as the refer-
ence, S2?s ROUGE-S score is 0.5, S3 is 0.167, and 
S4 is 0.333. Therefore, S2 is better than S3 and S4, 
and S4 is better than S3. This result is more intuitive 
than using BLEU-2 and ROUGE-L. One advantage of 
skip-bigram vs. BLEU is that it does not require con-
secutive matches but is still sensitive to word order. 
Comparing skip-bigram with LCS, skip-bigram 
counts all in-order matching word pairs while LCS 
only counts one longest common subsequence. 
Applying skip-bigram without any constraint on 
the distance between the words, spurious matches 
such as ?the the? or ?of in? might be counted as 
valid matches. To reduce these spurious matches, 
we can limit the maximum skip distance, dskip, be-
tween two in-order words that is allowed to form a 
skip-bigram. For example, if we set dskip to 0 then 
ROUGE-S is equivalent to bigram overlap F-
measure. If we set dskip to 4 then only word pairs of 
at most 4 words apart can form skip-bigrams. 
Adjusting Equations 16, 17, and 18 to use maxi-
mum skip distance limit is straightforward: we only 
count the skip-bigram matches, SKIP2 (X,Y), within 
the maximum skip distance and replace denomina-
tors of Equations 16, C(m,2), and 17, C(n,2), with 
the actual numbers of within distance skip-bigrams 
from the reference and the candidate respectively.  
 
                                                                 
1 C(4,2) = 4!/(2!*2!) = 6. 
5.1 ROUGE-SU: Extension of ROUGE-S 
One potential problem for ROUGE-S is that it does 
not give any credit to a candidate sentence if the 
sentence does not have any word pair co-occurring 
with its references. For example, the following sen-
tence has a ROUGE-S score of zero: 
 
S5. gunman the killed police 
 
S5 is the exact reverse of S1 and there is no skip 
bigram match between them. However, we would 
like to differentiate sentences similar to S5 from 
sentences that do not have single word co-
occurrence with S1. To achieve this, we extend 
ROUGE-S with the addition of unigram as counting 
unit. The extended version is called ROUGE-SU. We 
can also obtain ROUGE-SU from ROUGE-S by add-
ing a begin-of-sentence marker at the beginning of 
candidate and reference sentences. 
6 Evaluations of ROUGE 
To assess the effectiveness of ROUGE measures, we 
compute the correlation between ROUGE assigned 
summary scores and human assigned summary 
scores. The intuition is that a good evaluation meas-
ure should assign a good score to a good summary 
and a bad score to a bad summary. The ground truth 
is based on human assigned scores. Acquiring hu-
man judgments are usually very expensive; fortu-
nately, we have DUC 2001, 2002, and 2003 
evaluation data that include human judgments for 
the following: 
? Single document summaries of about 100 
words: 12 systems 2 for DUC 2001 and 14 sys-
tems for 2002. 149 single document summaries 
were judged per system in DUC 2001 and 295 
were judged in DUC 2002. 
? Single document very short summaries of about 
10 words (headline-like, keywords, or phrases): 
14 systems for DUC 2003. 624 very short sum-
maries were judged per system in DUC 2003. 
? Multi-document summaries of about 10 words: 
6 systems for DUC 2002; 50 words: 14 systems 
for DUC 2001 and 10 systems for DUC 2002; 
100 words: 14 systems for DUC 2001, 10 sys-
tems for DUC 2002, and 18 systems for DUC 
2003; 200 words: 14 systems for DUC 2001 and 
10 systems for DUC 2002; 400 words: 14 sys-
tems for DUC 2001. 29 summaries were judged 
per system per summary size in DUC 2001, 59 
were judged in DUC 2002, and 30 were judged 
in DUC 2003. 
                                                                 
2 All systems include 1 or 2 baselines. Please see DUC 
website for details. 
Besides these human judgments, we also have 3 sets 
of manual summaries for DUC 2001, 2 sets for 
DUC 2002, and 4 sets for DUC 2003. Human 
judges assigned content coverage scores to a candi-
date summary by examining the percentage of con-
tent overlap between a manual summary unit, i.e. 
elementary discourse unit or sentence, and the can-
didate summary using Summary Evaluation Envi-
ronment3 (SEE) developed by  the University of 
Southern California?s Information Sciences Institute 
(ISI). The overall candidate summary score is the 
average of the content coverage scores of all the 
units in the manual summary. Note that human 
judges used only one manual summary in all the 
evaluations although multiple alternative summaries 
were available. 
With the DUC data, we computed Pearson?s 
product moment correlation coefficients, Spear-
man?s rank order correlation coefficients, and 
Kendall?s correlation coefficients between systems? 
average ROUGE scores and their human assigned 
average coverage scores using single reference and 
multiple references. To investigate the effect of 
stemming and inclusion or exclusion of stopwords, 
we also ran experiments over orig inal automatic  and 
                                                                 
3 SEE is available online at http://www.isi.edu/~cyl. 
manual summaries (CASE set), stemmed4 version of 
the summaries (STEM set), and stopped version of 
the summaries (STOP set). For example, we com-
puted ROUGE scores for the 12 systems participated 
in the DUC 2001 single document summarization 
evaluation using the CASE set with single reference 
and then calculated the three correlation scores for 
these 12 systems? ROUGE scores vs. human assigned 
average coverage scores. After that we repeated the 
process using multiple references and then using 
STEM and STOP sets. Therefore, 2 (multi or single) 
x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spear-
man, or Kendall) = 18 data points were collected for 
each ROUGE measure and each DUC task. To assess 
the significance of the results, we applied bootstrap 
resampling technique (Davison and Hinkley, 1997) 
to estimate 95% confidence intervals for every cor-
relation computation. 
17 ROUGE measures were tested for each run us-
ing ROUGE evaluation package v1.2.1: ROUGE-N  
with N = 1 to 9, ROUGE-L, ROUGE-W with 
weighting factor a  = 1.2, ROUGE-S and ROUGE-SU 
with maximum skip distance dskip = 1, 4, and 9. Due 
to limitation of space, we only report correlation 
analysis results based on Pearson?s correlation coef-
ficient. Correlation analyses based on Spearman?s 
and Kendall?s correlation coefficients are tracking 
Pearson?s very closely and will be posted later at the 
ROUGE website5 for reference. The critical value6 
for Pearson?s correlation is 0.632 at 95% confidence 
with 8 degrees of freedom. 
Table 1 shows the Pearson?s correlation coeffi-
cients of the 17 ROUGE measures vs. human judg-
ments on DUC 2001 and 2002 100 words single 
document summarization data. The best values in 
each column are marked with dark (green) color and 
statistically equivalent values to the best values are 
marked with gray. We found that correlations were 
not affected by stemming or removal of stopwords 
in this data set, ROUGE-2 performed better among 
the ROUGE-N variants, ROUGE-L, ROUGE-W, and 
ROUGE-S were all performing well, and using mul-
tiple references improved performance though not 
much. All ROUGE measures achieved very good 
correlation with human judgments in the DUC 2002 
data. This might due to the double sample size in 
DUC 2002 (295 vs. 149 in DUC 2001) for each sys-
tem. 
Table 2 shows the correlation analysis results on 
the DUC 2003 single document very short summary 
data. We found that ROUGE-1, ROUGE-L, ROUGE-
                                                                 
4 Porter?s stemmer was used. 
5 ROUGE website: http://www.isi.edu/~cyl/ROUGE. 
6 The critical values for Pearson?s correlation at 95% 
confidence with 10, 12, 14, and 16 degrees of freedom 
are 0.576, 0.532, 0.497, and 0.468 respectively. 
Method CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP
R-1 0.76 0.76 0.84 0.80 0.78 0.84 0.98 0.98 0.99 0.98 0.98 0.99
R-2 0.84 0.84 0.83 0.87 0.87 0.86 0.99 0.99 0.99 0.99 0.99 0.99
R-3 0.82 0.83 0.80 0.86 0.86 0.85 0.99 0.99 0.99 0.99 0.99 0.99
R-4 0.81 0.81 0.77 0.84 0.84 0.83 0.99 0.99 0.98 0.99 0.99 0.99
R-5 0.79 0.79 0.75 0.83 0.83 0.81 0.99 0.99 0.98 0.99 0.99 0.98
R-6 0.76 0.77 0.71 0.81 0.81 0.79 0.98 0.99 0.97 0.99 0.99 0.98
R-7 0.73 0.74 0.65 0.79 0.80 0.76 0.98 0.98 0.97 0.99 0.99 0.97
R-8 0.69 0.71 0.61 0.78 0.78 0.72 0.98 0.98 0.96 0.99 0.99 0.97
R-9 0.65 0.67 0.59 0.76 0.76 0.69 0.97 0.97 0.95 0.98 0.98 0.96
R-L 0.83 0.83 0.83 0.86 0.86 0.86 0.99 0.99 0.99 0.99 0.99 0.99
R-S* 0.74 0.74 0.80 0.78 0.77 0.82 0.98 0.98 0.98 0.98 0.97 0.98
R-S4 0.84 0.85 0.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99
R-S9 0.84 0.85 0.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99
R-SU* 0.74 0.74 0.81 0.78 0.77 0.83 0.98 0.98 0.98 0.98 0.98 0.98
R-SU4 0.84 0.84 0.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99
R-SU9 0.84 0.84 0.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99
R-W-1.2 0.85 0.85 0.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99
DUC 2001 100 WORDS SINGLE DOC DUC 2002 100 WORDS SINGLE DOC
1 REF 3 REFS 1 REF 2 REFS
Table 1: Pearson?s correlations of 17 ROUGE
measure scores vs. human judgments for the DUC 
2001 and 2002 100 words single document sum-
marization tasks  
1 REF 4REFS 1 REF 4 REFS 1 REF 4 REFS
Method
R-1 0.96 0.95 0.95 0.95 0.90 0.90
R-2 0.75 0.76 0.75 0.75 0.76 0.77
R-3 0.71 0.70 0.70 0.68 0.73 0.70
R-4 0.64 0.65 0.62 0.63 0.69 0.66
R-5 0.62 0.64 0.60 0.63 0.63 0.60
R-6 0.57 0.62 0.55 0.61 0.46 0.54
R-7 0.56 0.56 0.58 0.60 0.46 0.44
R-8 0.55 0.53 0.54 0.55 0.00 0.24
R-9 0.51 0.47 0.51 0.49 0.00 0.14
R-L 0.97 0.96 0.97 0.96 0.97 0.96
R-S* 0.89 0.87 0.88 0.85 0.95 0.92
R-S4 0.88 0.89 0.88 0.88 0.95 0.96
R-S9 0.92 0.92 0.92 0.91 0.97 0.95
R-SU* 0.93 0.90 0.91 0.89 0.96 0.94
R-SU4 0.97 0.96 0.96 0.95 0.98 0.97
R-SU9 0.97 0.95 0.96 0.94 0.97 0.95
R-W-1.2 0.96 0.96 0.96 0.96 0.96 0.96
DUC 2003 10 WORDS SINGLE DOC
CASE STEM STOP
Table 2: Pearson?s correlations of 17 ROUGE
measure scores vs. human judgments for the DUC 
2003 very short summary task 
SU4 and 9, and ROUGE-W were very good measures 
in this category, ROUGE-N with N > 1 performed 
significantly worse than all other measures, and ex-
clusion of stopwords improved performance in gen-
eral except for ROUGE-1. Due to the large number 
of samples (624) in this data set, using multiple ref-
erences did not improve correlations. 
In Table 3 A1, A2, and A3, we show correlation 
analysis results on DUC 2001, 2002, and 2003 100 
words multi-document summarization data. The 
results indicated that using multiple references im-
proved correlation and exclusion of stopwords usu-
ally improved performance. ROUGE-1, 2, and 3 
performed fine but were not consistent. ROUGE-1, 
ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGE-
SU9 with stopword removal had correlation above 
0.70. ROUGE-L and ROUGE-W did not work well in 
this set of data. 
Table 3 C, D1, D2, E1, E2, and F show the corre-
lation analyses using multiple references on the rest 
of DUC data. These results again suggested that 
exclusion of stopwords achieved better performance 
especially in multi-document summaries of 50 
words. Better correlations (> 0.70) were observed 
on long summary tasks, i.e. 200 and 400 words 
summaries. The relative performance of ROUGE 
measures followed the pattern of the 100 words 
multi-document summarization task. 
Comparing the results in Table 3 with Tables 1 
and 2, we found that correlation values in the multi-
document tasks rarely reached high 90% except in 
long summary tasks. One possible explanation of 
this outcome is that we did not have large amount of 
samples for the multi-document tasks. In the single 
document summarization tasks we had over 100 
samples; while we only had about 30 samples in the 
multi-document tasks. The only tasks that had over 
30 samples was from DUC 2002 and the correla-
tions of ROUGE measures with human judgments on 
the 100 words summary task were much better and 
more stable than similar tasks in DUC 2001 and 
2003. Statistically stable human judgments of sys-
tem performance might not be obtained due to lack 
of samples and this in turn caused instability of cor-
relation analyses. 
7 Conclusions  
In this paper, we introduced ROUGE, an automatic 
evaluation package for summarization, and con-
ducted comprehensive evaluations of the automatic 
measures included in the ROUGE package using 
three years of DUC data. To check the significance 
of the results, we estimated confidence intervals of 
correlations using bootstrap resampling. We found 
that (1) ROUGE-2, ROUGE-L, ROUGE-W, and 
ROUGE-S worked well in single document summa-
rization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, 
ROUGE-SU4, and ROUGE-SU9 performed great in 
evaluating very short summaries (or headline-like 
summaries), (3) correlation of high 90% was hard to 
achieve for multi-document summarization tasks but 
ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, 
ROUGE-SU4, and ROUGE-SU9 worked reasonably 
well when stopwords were excluded from matching, 
(4) exclusion of  stopwords usually improved corre-
lation, and (5) correlations to human judgments 
were increased by using multiple references. 
In summary, we showed that the ROUGE package 
could be used effectively in automatic evaluation of 
summaries. In a separate study (Lin and Och, 2004), 
Method CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP
R-1 0.48 0.56 0.86 0.53 0.57 0.87 0.66 0.66 0.77 0.71 0.71 0.78 0.58 0.57 0.71 0.58 0.57 0.71
R-2 0.55 0.57 0.64 0.59 0.61 0.71 0.83 0.83 0.80 0.88 0.87 0.85 0.69 0.67 0.71 0.79 0.79 0.81
R-3 0.46 0.45 0.47 0.53 0.53 0.55 0.85 0.84 0.76 0.89 0.88 0.83 0.54 0.51 0.48 0.76 0.75 0.74
R-4 0.39 0.39 0.43 0.48 0.49 0.47 0.80 0.80 0.63 0.83 0.82 0.75 0.37 0.36 0.36 0.62 0.61 0.52
R-5 0.38 0.39 0.33 0.47 0.48 0.43 0.73 0.73 0.45 0.73 0.73 0.62 0.25 0.25 0.27 0.45 0.44 0.38
R-6 0.39 0.39 0.20 0.45 0.46 0.39 0.71 0.72 0.38 0.66 0.64 0.46 0.21 0.21 0.26 0.34 0.31 0.29
R-7 0.31 0.31 0.17 0.44 0.44 0.36 0.63 0.65 0.33 0.56 0.53 0.44 0.20 0.20 0.23 0.29 0.27 0.25
R-8 0.18 0.19 0.09 0.40 0.40 0.31 0.55 0.55 0.52 0.50 0.46 0.52 0.18 0.18 0.21 0.23 0.22 0.23
R-9 0.11 0.12 0.06 0.38 0.38 0.28 0.54 0.54 0.52 0.45 0.42 0.52 0.16 0.16 0.19 0.21 0.21 0.21
R-L 0.49 0.49 0.49 0.56 0.56 0.56 0.62 0.62 0.62 0.65 0.65 0.65 0.50 0.50 0.50 0.53 0.53 0.53
R-S* 0.45 0.52 0.84 0.51 0.54 0.86 0.69 0.69 0.77 0.73 0.73 0.79 0.60 0.60 0.67 0.61 0.60 0.70
R-S4 0.46 0.50 0.71 0.54 0.57 0.78 0.79 0.80 0.79 0.84 0.85 0.82 0.63 0.64 0.70 0.73 0.73 0.78
R-S9 0.42 0.49 0.77 0.53 0.56 0.81 0.79 0.80 0.78 0.83 0.84 0.81 0.65 0.65 0.70 0.70 0.70 0.76
R-SU* 0.45 0.52 0.84 0.51 0.54 0.87 0.69 0.69 0.77 0.73 0.73 0.79 0.60 0.59 0.67 0.60 0.60 0.70
R-SU4 0.47 0.53 0.80 0.55 0.58 0.83 0.76 0.76 0.79 0.80 0.81 0.81 0.64 0.64 0.74 0.68 0.68 0.76
R-SU9 0.44 0.50 0.80 0.53 0.57 0.84 0.77 0.78 0.78 0.81 0.82 0.81 0.65 0.65 0.72 0.68 0.68 0.75
R-W-1.2 0.52 0.52 0.52 0.60 0.60 0.60 0.67 0.67 0.67 0.69 0.69 0.69 0.53 0.53 0.53 0.58 0.58 0.58
Method CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP
R-1 0.71 0.68 0.49 0.49 0.49 0.73 0.44 0.48 0.80 0.81 0.81 0.90 0.84 0.84 0.91 0.74 0.73 0.90
R-2 0.82 0.85 0.80 0.43 0.45 0.59 0.47 0.49 0.62 0.84 0.85 0.86 0.93 0.93 0.94 0.88 0.88 0.87
R-3 0.59 0.74 0.75 0.32 0.33 0.39 0.36 0.36 0.45 0.80 0.80 0.81 0.90 0.91 0.91 0.84 0.84 0.82
R-4 0.25 0.36 0.16 0.28 0.26 0.36 0.28 0.28 0.39 0.77 0.78 0.78 0.87 0.88 0.88 0.80 0.80 0.75
R-5 -0.25 -0.25 -0.24 0.30 0.29 0.31 0.28 0.30 0.49 0.77 0.76 0.72 0.82 0.83 0.84 0.77 0.77 0.70
R-6 0.00 0.00 0.00 0.22 0.23 0.41 0.18 0.21 -0.17 0.75 0.75 0.67 0.78 0.79 0.77 0.74 0.74 0.63
R-7 0.00 0.00 0.00 0.26 0.23 0.50 0.11 0.16 0.00 0.72 0.72 0.62 0.72 0.73 0.74 0.70 0.70 0.58
R-8 0.00 0.00 0.00 0.32 0.32 0.34 -0.11 -0.11 0.00 0.68 0.68 0.54 0.71 0.71 0.70 0.66 0.66 0.52
R-9 0.00 0.00 0.00 0.30 0.30 0.34 -0.14 -0.14 0.00 0.64 0.64 0.48 0.70 0.69 0.59 0.63 0.62 0.46
R-L 0.78 0.78 0.78 0.56 0.56 0.56 0.50 0.50 0.50 0.81 0.81 0.81 0.88 0.88 0.88 0.82 0.82 0.82
R-S* 0.83 0.82 0.69 0.46 0.45 0.74 0.46 0.49 0.80 0.80 0.80 0.90 0.84 0.85 0.93 0.75 0.74 0.89
R-S4 0.85 0.86 0.76 0.40 0.41 0.69 0.42 0.44 0.73 0.82 0.82 0.87 0.91 0.91 0.93 0.85 0.85 0.85
R-S9 0.82 0.81 0.69 0.42 0.41 0.72 0.40 0.43 0.78 0.81 0.82 0.86 0.90 0.90 0.92 0.83 0.83 0.84
R-SU* 0.75 0.74 0.56 0.46 0.46 0.74 0.46 0.49 0.80 0.80 0.80 0.90 0.84 0.85 0.93 0.75 0.74 0.89
R-SU4 0.76 0.75 0.58 0.45 0.45 0.72 0.44 0.46 0.78 0.82 0.83 0.89 0.90 0.90 0.93 0.84 0.84 0.88
R-SU9 0.74 0.73 0.56 0.44 0.44 0.73 0.41 0.45 0.79 0.82 0.82 0.88 0.89 0.89 0.92 0.83 0.82 0.87
R-W-1.2 0.78 0.78 0.78 0.56 0.56 0.56 0.51 0.51 0.51 0.84 0.84 0.84 0.90 0.90 0.90 0.86 0.86 0.86
(A1) DUC 2001 100 WORDS MULTI (A2) DUC 2002 100 WORDS MULTI (A3) DUC 2003 100 WORDS MULTI
1 RFF 3 REFS 1 REF 2 REFS 1 REF 4 REFS
(E2) DUC02 200 (F) DUC01 400(C) DUC02 10 (D1) DUC01 50 (D2) DUC02 50 (E1) DUC01 200
Table 3: Pearson?s correlations of 17 ROUGE measure scores vs. human judgments for 
the DUC 2001, 2002, and 2003 mult i-document summarization tasks  
ROUGE-L, W, and S were also shown to be very 
effective in automatic  evaluation of machine 
translation. The stability and reliability of ROUGE at 
different sample sizes was reported by the author in 
(Lin, 2004). However, how to achieve high correla-
tion with human judgments in multi-document 
summarization tasks as ROUGE already did in single 
document summarization tasks is still an open re-
search topic. 
8  Acknowledgements 
The author would like to thank the anonymous re-
viewers for their constructive comments, Paul Over 
at NIST, U.S.A, and ROUGE users around the world  
for testing and providing useful feedback on earlier 
versions of the ROUGE evaluation package, and the 
DARPA TIDES project for supporting this research. 
References 
 
Cormen, T. R., C. E. Leiserson, and R. L. Rivest. 
1989. Introduction to Algorithms. The MIT Press. 
Davison, A. C. and D. V. Hinkley. 1997. Bootstrap 
Methods and Their Application. Cambridge Uni-
versity Press. 
Lin, C.-Y. and E. H. Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence 
statistics. In Proceedings of 2003 Language 
Technology Conference (HLT-NAACL 2003), 
Edmonton, Canada. 
Lin, C.-Y. 2004. Looking for a few good metrics: 
ROUGE and its evaluation. In Proceedings of 
NTCIR Workshop 2004, Tokyo, Japan. 
Lin, C.-Y. and F. J. Och. 2004. Automatic evalua-
tion of machine translation quality using longest 
common subsequence and skip-bigram statistics. 
In Proceedings of 42nd Annual Meeting of ACL 
(ACL 2004), Barcelona, Spain. 
Mani, I. 2001. Automatic Summarization. John Ben-
jamins Publishing Co. 
Melamed, I. D. 1995. Automatic evaluation and uni-
form filter cascades for inducing n-best transla-
tion lexicons. In Proceedings of the 3rd Workshop 
on Very Large Corpora (WVLC3). Boston, 
U.S.A. 
Melamed, I. D., R. Green and J. P. Turian (2003). 
Precision and recall of machine translation. In 
Proceedings of 2003 Language Technology Con-
ference (HLT-NAACL 2003), Edmonton, Can-
ada. 
Over, P. and J. Yen. 2003. An introduction to DUC 
2003 ? Intrinsic evaluation of generic news text 
summarization systems. AAAAAAAAAA                               
http://www-nlpir.nist.gov/projects/duc/pubs/ 
2003slides/duc2003intro.pdf 
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 
2001. BLEU : A method for automatic evaluation 
of machine translation. IBM Research Report 
RC22176 (W0109-022). 
Saggion H., D. Radev, S. Teufel, and W. Lam. 
2002. Meta-evaluation of summaries in a cross-
lingual environment using content-based metrics. 
In Proceedings of COLING-2002, Taipei, Tai-
wan. 
Radev, D.  S. Teufel, H. Saggion, W. Lam, J. Blit-
zer, A. Gelebi, H. Qi, E. Drabek, and D. Liu. 
2002. Evaluation of Text Summarization in a 
Cross-Lingual Information Retrieval Framework. 
Technical report, Center for Language and 
Speech Processing, Johns Hopkins University, 
Balt imore, MD, USA. 
Van Rijsbergen, C. J. 1979. Information Retrieval. 
Butterworths. London. 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 77?84,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Re-evaluating Machine Translation Results with Paraphrase Support 
Liang Zhou, Chin-Yew Lin, and Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{liangz, cyl, hovy} @isi.edu 
      
 
  
 
Abstract 
In this paper, we present ParaEval, an 
automatic evaluation framework that uses 
paraphrases to improve the quality of 
machine translation evaluations. Previous 
work has focused on fixed n-gram 
evaluation metrics coupled with lexical 
identity matching. ParaEval addresses 
three important issues: support for para-
phrase/synonym matching, recall meas-
urement, and correlation with human 
judgments. We show that ParaEval corre-
lates significantly better than BLEU with 
human assessment in measurements for 
both fluency and adequacy. 
1 Introduction 
The introduction of automated evaluation proce-
dures, such as BLEU (Papineni et al, 2001) for 
machine translation (MT) and ROUGE (Lin and 
Hovy, 2003) for summarization, have prompted 
much progress and development in both of these 
areas of research in Natural Language Processing 
(NLP). Both evaluation tasks employ a compari-
son strategy for comparing textual units from 
machine-generated and gold-standard texts. Ide-
ally, this comparison process would be per-
formed manually, because of humans? abilities to 
infer, paraphrase, and use world knowledge to 
relate differently worded pieces of equivalent 
information. However, manual evaluations are 
time consuming and expensive, thus making 
them a bottleneck in system development cycles.  
BLEU measures how close machine-generated 
translations are to professional human transla-
tions, and ROUGE does the same with respect to 
summaries. Both methods incorporate the com-
parison of a system-produced text to one or more 
corresponding reference texts. The closeness be-
tween texts is measured by the computation of a 
numeric score based on n-gram co-occurrence 
statistics. Although both methods have gained 
mainstream acceptance and have shown good 
correlations with human judgments, their defi-
ciencies have become more evident and serious 
as research in MT and summarization progresses 
(Callison-Burch et al, 2006).   
Text comparisons in MT and summarization 
evaluations are performed at different text granu-
larity levels. Since most of the phrase-based, 
syntax-based, and rule-based MT systems trans-
late one sentence at a time, the text comparison 
in the evaluation process is also performed at the 
single-sentence level. In summarization evalua-
tions, there is no sentence-to-sentence corre-
spondence between summary pairs?essentially 
a multi-sentence-to-multi-sentence comparison, 
making it more difficult and requiring a com-
pletely different implementation for matching 
strategies. In this paper, we focus on the intrica-
cies involved in evaluating MT results and ad-
dress two prominent problems associated with 
the BLEU-esque metrics, namely their lack of 
support for paraphrase matching and the absence 
of recall scoring. Our solution, ParaEval, utilizes 
a large collection of paraphrases acquired 
through an unsupervised process?identifying 
phrase sets that have the same translation in an-
other language?using state-of-the-art statistical 
MT word alignment and phrase extraction meth-
ods. This collection facilitates paraphrase match-
ing, additionally coupled with lexical identity 
matching which is designed for comparing 
text/sentence fragments that are not consumed by 
paraphrase matching. We adopt a unigram count-
ing strategy for contents matched between sen-
tences from peer and reference translations. This 
unweighted scoring scheme, for both precision 
and recall computations, allows us to directly 
examine both the power and limitations of 
77
ParaEval.  We show that ParaEval is a more sta-
ble and reliable comparison mechanism than 
BLEU, in both fluency and adequacy rankings.  
This paper is organized in the following way: 
Section 2 shows an overview on BLEU and lexi-
cal identity n-gram statistics; we describe ParaE-
val?s implementation in detail in Section 3; the 
evaluation of ParaEval is shown in Section 4; 
recall computation is discussed in Section 5; in 
Section 6, we discuss the differences between 
BLEU and ParaEval when the numbers of refer-
ence translations change; and we conclude and 
discuss future work in Section 7.  
2  N-gram Co-occurrence Statistics 
Being an $8 billion industry (Browner, 2006), 
MT calls for rapid development and the ability to 
differentiate good systems from less adequate 
ones. The evaluation process consists of compar-
ing system-generated peer translations  to human 
written reference translations  and assigning a 
numeric score to each system. While human as-
sessments are still the most reliable evaluation 
measurements, it is not practical to solicit manual 
evaluations repeatedly while making incremental 
system design changes that would only result in 
marginal performance gains. To overcome the 
monetary and time constraints associated with 
manual evaluations, automated procedures have 
been successful in delivering benchmarks for 
performance hill-climbing with little or no cost.  
While a variety of automatic evaluation meth-
ods have been introduced, the underlining com-
parison strategy is similar?matching based on 
lexical identity. The most prominent implemen-
tation of this type of matching is demonstrated in 
BLEU (Papineni et al 2002). The remaining part 
of this section is devoted to an overview of 
BLEU, or the BLEU-esque philosophy.  
2 .1  The BL E U-esque Matching Philosophy 
The primary task that a BLEU-esque procedure 
performs is to compare n-grams from the peer 
translation with the n-grams from one or more 
reference translations and count the number of 
matches. The more matches a peer translation 
gets, the better it is.  
BLEU is a precision-based metric, which is 
the ratio of the number of n-grams from the peer 
translation that occurred in reference translations 
to the total number of n-grams in the peer trans-
lation. The notion of Modified n-gram Precisi on  
was introduced to detect and avoid rewarding 
false positives generated by translation systems. 
To gain high precision, systems could potentially 
over-generate ?good? n-grams, which occur mul-
tiple times in multiple references. The solution to 
this problem was to adopt the policy that an n-
gram, from both reference and peer translations, 
is considered exhausted after participating in a 
match. As a result, the maximum number of 
matches an n-gram from a peer translation can 
receive, when comparing to a set of reference 
translations, is the maximum number of times 
this n-gram occurred in any single reference 
translation. Papineni et al (2002) called this cap-
ping technique clipp ing.  Figure 1, taken from the 
original BLEU paper, demonstrates the computa-
tion of the modified unigram precision for a peer 
translation sentence.  
To compute the modified n-gram precision, 
P n, for a whole test set, including all translation 
segments (usually in sentences), the formula is: 
 
2 .2  Lack of Paraphrasing Support 
Humans are very good at finding creative ways 
to convey the same information. There is no one 
definitive reference translation in one language 
for a text written in another. Having acknowl-
edged this phenomenon, however natural it is, 
human evaluations on system-generated transla-
tions are much more preferred and trusted. How-
ever, what humans can do with ease puts ma-
chines at a loss. BLEU-esque procedures recog-
nize equivalence only when two n-grams exhibit 
the same surface-level representations, i.e. the 
same lexical identities. The BLEU implementa-
tion addresses its deficiency in measuring seman-
tic closeness by incorporating the comparison 
with multiple reference translations. The ration-
ale is that multiple references give a higher 
chance that the n-grams, assuming correct trans-
lations, appearing in the peer translation would 
be rewarded by one of the reference?s n-grams. 
The more reference translations used, the better 
Figure 1. Modified n-gram precision from
BLEU.
 
78
the matching and overall evaluation quality. Ide-
ally (and to an extreme), we would need to col-
lect a large set of human-written translations to 
capture all possible combinations of verbalizing 
variations before the translation comparison pro-
cedure reaches its optimal matching ability.  
One can argue that an infinite number of ref-
erences are not needed in practice because any 
matching procedure would stabilize at a certain 
number of references. This is true if precision 
measure is the only metric computed. However, 
using precision scores alone unfairly rewards 
systems that ?under-generate??producing un-
reasonably short translations. Recall measure-
ments would provide more balanced evaluations. 
When using multiple reference translations, if an 
n-gram match is made for the peer, this n-gram 
could appear in any of the references. The com-
putation of recall becomes difficult, if not impos-
sible. This problem can be reversed if there is 
crosschecking for phrases occurring across refer-
ences?paraphrase recognition. BLEU uses the 
calculation of a brevity penalty to compensate 
the lack of recall computation problem. The 
brevity penalty is computed as follows: 
 
Then, the BLEU score for a peer translation is 
computed as: 
 
BLEU?s adoption of the brevity penalty to off-
set the effect of not having a recall computation 
has drawn criticism on its crudeness in measur-
ing translation quality. Callison-Burch et al 
(2006) point out three prominent factors: 
? ``Synonyms and paraphrases are only 
handled if they are in the set of multiple 
reference translations [available].  
? The scores for words are equally 
weighted so missing out on content-
bearing material brings no additional pen-
alty.  
? The brevity penalty is a stop-gap meas-
ure to compensate for the fairly serious 
problem of not being able to calculate re-
call.? 
With the introduction of ParaEval, we will ad-
dress two of these three issues, namely the para-
phrasing problem and providing a recall meas-
ure.  
3  ParaEval for MT Evaluation 
3.1  Overview 
Reference translations are created from the same 
source text (written in the foreign language) to 
the target language. Ideally, they are supposed to 
be semantically equivalent, i.e. overlap com-
pletely. However, as shown in Figure 2, when 
matching based on lexical identity is used (indi-
cated by links), only half (6 from the left and 5 
from the right) of the 12 words from these two 
sentences are matched. Also, ?to? was a mis-
match. In applying paraphrase matching for MT 
evaluation from ParaEval, we aim to match all 
shaded words from both sentences. 
3 .2  Paraphrase Acquisition 
The process of acquiring a large enough collec-
tion of paraphrases is not an easy task. Manual 
corpus analyses produce domain-specific collec-
tions that are used for text generation and are 
application-specific. But operating in multiple 
domains and for multiple tasks translates into 
multiple manual collection efforts, which could 
be very time-consuming and costly. In order to 
facilitate smooth paraphrase utilization across a 
variety of NLP applications, we need an unsu-
pervised paraphrase collection mechanism that 
can be easily conducted, and produces para-
phrases that are of adequate quality and can be 
readily used with minimal amount of adaptation 
effort.  
Our method (Anonymous, 2006), also illus-
trated in (Bannard and Callison-Burch, 2005), to 
automatically construct a large domain-
independent paraphrase collection is based on the 
assumption that two different phrases of the 
same meaning may have the same translation in a 
Figure 2. Two reference translations. Grey
areas are matched by using BLEU.
79
foreign language. Phrase-based Statistical Ma-
chine Translation (SMT) systems analyze large 
quantities of bilingual parallel texts in order to 
learn translational alignments between pairs of 
words and phrases in two languages (Och and 
Ney, 2004). The sentence-based translation 
model makes word/phrase alignment decisions 
probabilistically by computing the optimal model 
parameters with application of the statistical es-
timation theory. This alignment process results in 
a corpus of word/phrase-aligned parallel sen-
tences from which we can extract phrase pairs 
that are translations of each other. We ran the 
alignment algorithm from (Och and Ney, 2003) 
on a Chinese-English parallel corpus of 218 mil-
lion English words, available from the Linguistic 
Data Consortium (LDC). Phrase pairs are ex-
tracted by following the method described in 
(Och and Ney, 2004) where all contiguous 
phrase pairs having consistent alignments are 
extraction candidates. Using these pairs we build 
paraphrase sets by joining together all English 
phrases that have the same Chinese translation. 
Figure 3 shows an example word/phrase align-
ment for two parallel sentence pairs from our 
corpus where the phrases ?blowing up? and 
?bombing? have the same Chinese translation. 
On the right side of the figure we show the para-
phrase set which contains these two phrases, 
which is typical in our collection of extracted 
paraphrases.  
Although our paraphrase extraction method is 
similar to that of (Bannard and Callison-Burch, 
2005), the paraphrases we extracted are for com-
pletely different applications, and have a broader 
definition for what constitutes a paraphrase. In 
(Bannard and Callison-Burch, 2005), a language 
model is used to make sure that the paraphrases 
extracted are direct substitutes, from the same 
syntactic categories, etc. So, using the example 
in Figure 3, the paraphrase table would contain 
only ?bombing? and ?bombing attack?. Para-
phrases that are direct substitutes of one another 
are useful when translating unknown phrases. 
For instance, if a MT system does not have the 
Chinese translation for the word ?bombing?, but 
has seen it in another set of parallel data (not in-
volving Chinese) and has determined it to be a 
direct substitute of the phrase ?bombing attack?, 
then the Chinese translation of ?bombing attack? 
would be used in place of the translation for 
?bombing?. This substitution technique has 
shown some improvement in translation quality 
(Callison-Burch et al, 2006).  
3 .3  The ParaEval Evaluation Procedure 
We adopt a two-tier matching strategy for MT 
evaluation in ParaEval. At the top tier, a para-
phrase match is performed on system-translated 
sentences and corresponding reference sentences. 
Then, unigram matching is performed on the 
words not matched by paraphrases. Precision is 
measured as the ratio of the total number of 
words matched to the total number of words in 
the peer translation.  
Running our system on the example in Figure 
2, the paraphrase-matching phase consumes the 
words marked in grey and aligns ?have been? 
and ?to be?, ?completed? and ?fully?, ?to date? 
and ?up till now?, and ?sequence? and ?se-
quenced?. The subsequent unigram-matching 
aligns words based on lexical identity.  
We maintain the computation of modified uni-
gram precisi on , defined by the BLEU-esque Phi-
losophy, in principle. In addition to clipping in-
dividual candidate words  with their correspond-
ing maximum reference counts (only for words 
not matched by paraphrases), we clip candidate 
paraphrases  by their maximum reference para-
phrase counts. So two completely different 
phrases in a reference sentence can be counted as 
two occurrences of one phrase. For example in 
Figure 4, candidate phrases ?blown up? and 
?bombing? matched with three phrases from the 
references, namely ?bombing? and two instances 
of ?explosion?. Treating these two candidate 
phrases as one (paraphrase match), we can see its 
clip is 2 (from Ref 1, where ?bomb ing? and ?ex-
plosion? are counted as two occurrences of a sin-
gle phrase). The only word that was matched by 
its lexical identity is ?was?. The modified uni-
gram precision calculated by our method is 4/5, 
where as BLEU gives 2/5.  
Figure 3. An example of the paraphrase extraction
process.
80
4  Evaluating ParaEval 
To be effective in MT evaluations, an automated 
procedure should be capable of distinguishing 
good translation systems from bad ones, human 
translations from systems?, and human transla-
tions of differing quality. For a particular evalua-
tion exercise, an evaluation system produces a 
ranking for system and human translations, and 
compares this ranking with one created by hu-
man judges (Turian et al, 2003). The closer a 
system?s ranking is to the human?s, the better the 
evaluation system is. 
4 .1  Validating ParaEval 
To test ParaEval?s ability, NIST 2003 Chinese 
MT evaluation results were used (NIST 2003). 
This collection consists of 100 source documents 
in Chinese, translations from eight individual 
translation systems, reference translations from 
four humans, and human assessments (on flu-
ency and adequacy). The Spearman rank-order 
coefficient is computed as an indicator of how 
close a system ranking is to gold-standard human 
ranking. It should be noted that the 2003 MT 
data is separate from the corpus that we extracted 
paraphrases from.  
For comparison purposes, BLEU 1  was also 
run. Table 1 shows the correlation figures for the 
two automatic systems with the NIST rankings 
on fluency and adequacy. The lower and higher 
95% confidence intervals are labeled as ?L-CI? 
and ?H-CI?. To estimate the significance of the 
rank-order correlation figures, we applied boot-
strap resampling to calculate the confidence in-
tervals.  In each of 1000 runs, systems were 
ranked based on their translations of 100 ran-
domly selected documents.  Each ranking was 
compared with the NIST ranking, producing a 
correlation score for each run. A t-test was then 
                                                
1 Results shown are from BLEU v.11 (NIST).  
performed on the 1000 correlation scores. In both 
fluency and adequacy measurements, ParaEval 
correlates significantly better than BLEU. The 
ParaEval scores used were precision scores. In 
addition to distinguishing the quality of MT sys-
tems, a reliable evaluation procedure must be 
able to distinguish system translations from hu-
mans? (Lin and Och, 2004). Figure 5 shows the 
overall system and human ranking. In the upper 
left corner, human translators are grouped to-
gether, significantly separated from the auto-
matic MT systems clustered into the lower right 
corner.  
4 .2  Implications to Word-alignment 
We experimented with restricting the para-
phrases being matched to various lengths. When 
allowing only paraphrases of three or more 
words to match, the correlation figures become 
stabilized and ParaEval achieves even higher 
correlation with fluency measurement to 0.7619 
on the Spearman ranking coefficient.   
This phenomenon indicates to us that the bi-
gram and unigram paraphrases extracted using 
SMT word-alignment and phrase extraction pro-
grams are not reliable enough to be applied to 
evaluation tasks. We speculate that word pairs 
extracted from (Liang et al, 2006), where a bidi-
rectional discriminative training method was 
used to achieve consensus for word-alignment 
Figure 4. ParaEval?s matching process.
 
BLEU ParaEval
Fluency 0.6978 0.7575
95% L-CI 0.6967 0.7553
95% H-CI 0.6989 0.7596
Adequacy 0.6108 0.6918
95% L-CI 0.6083 0.6895
95% H-CI 0.6133 0.694
Table 1. Ranking correlations with human
assessments.
 
Figure 5. Overall system and human ranking.
 
81
(mostly lower n-grams), would help to elevate 
the level of correlation by ParaEval.  
4 .3  Implications to Evaluating Paraphrase 
Quality 
Utilizing paraphrases in MT evaluations is also a 
realistic way to measure the quality of para-
phrases acquired through unsupervised channels. 
If a comparison strategy, coupled with para-
phrase matching, distinguishes good and bad MT 
and summarization systems in close accordance 
with what human judges do, then this strategy 
and the paraphrases used are of sufficient quality. 
Since our underlining comparison strategy is that 
of BLEU-1 for MT evaluation, and BLEU has 
been proven to be a good metric for their respec-
tive evaluation tasks, the performance of the 
overall comparison is directly and mainly af-
fected by the paraphrase collection.  
5  ParaEval?s Support for Recall Com-
putation 
Due to the use of multiple references and allow-
ing an n-gram from the peer translation to be 
matched with its corresponding n-gram from any 
of the reference translations, BLEU cannot be 
used to compute recall scores, which are conven-
tionally paired with precision to detect length-
related problems from systems under evaluation.  
5 .1  Using Single References for Recall 
The primary goal in using multiple references is 
to overcome the limitation in matching on lexical 
identity. More translation choices give more 
variations in verbalization, which could lead to 
more matches between peer and reference trans-
lations. Since MT results are generated and 
evaluated at a sentence-to-sentence level (or a 
segment level, where each segment may contain 
a small number of sentences) and no text con-
densation is employed, the number of different 
and correct ways to state the same sentence is 
small. This is in comparison to writing generic 
multi-document summaries, each of which con-
tains multiple sentences and requires significant 
amount of ?rewriting?. When using a large col-
lection of paraphrases while evaluating, we are 
provided with the alternative verbalizations 
needed. This property allows us to use single 
references to evaluate MT results and compute 
recall measurements.  
5 .2  Recall and Adequacy Correlations 
When validating the computed recall scores for 
MT systems, we correlate with human assess-
ments on adequacy only. The reason is that ac-
cording to the definition of recall, the content 
coverage in references, and not the fluency re-
flected from the peers, is being measured. Table 
2 shows ParaEval?s recall correlation with NIST 
2003 Chinese MT evaluation results on systems 
ranking. We see that ParaEval?s correlation with 
adequacy has improved significantly when using 
recall scores to rank than using precision scores.  
5 .3  Not All Single References are Created 
Equal 
Human-written translations differ not only in 
word choice, but also in other idiosyncrasies that 
cannot be captured with paraphrase recognition. 
So it would be presumptuous to declare that us-
ing paraphrases from ParaEval is enough to al-
low using just one reference translation to evalu-
ate. Using multiple references allow more para-
phrase sets to be explored in matching.  
In Table 3, we show ParaEval?s correlation 
figures when using single reference translations. 
E01?E04 indicate the sets of human translations 
used correspondingly.  
Notice that the correlation figures vary a great 
deal depending on the set of single references 
used. How do we differentiate human transla-
tions and know which set of references to use? It 
is difficult to quantify the quality that a human 
written translation reflects. We can only define 
?good? human translations as translations that 
are written not very differently from what other 
humans would write, and ?bad? translations as 
the ones that are written in an unconventional 
fashion. Table 4 shows the differences between 
the four sets of reference translations when com-
BLEU ParaEval
Adequacy 0.6108 0.7373
95% L-CI 0.6083 0.7368
95% H-CI 0.6133 0.7377
Table 2. ParaEval?s recall ranking correlation.
 
Table 3. ParaEval?s correlation (precision)
while using only single references.
E01 E02 E03 E04
Fluency 0.683 0.6501 0.7284 0.6192
95% L-CI 0.6795 0.6482 0.7267 0.6172
95% H-CI 0.6864 0.6519 0.73 0.6208
Adequacy 0.6308 0.5741 0.6688 0.5858
95% L-CI 0.6266 0.5705 0.665 0.5821
95% H-CI 0.635 0.5777 0.6727 0.5895
 
82
paring one set of references to the other three. 
The scores here are the raw ParaEval precision 
scores. E01 and E03 are better, which explains 
the higher correlations ParaEval has using these 
two sets of references individually, shown in Ta-
ble 3.  
6  Observation of Change in Number of 
References 
When matching on lexical identity, it is the gen-
eral consensus that using more reference transla-
tions would increase the reliability of the MT 
evaluation (Turian et al, 2003). It is expected 
that we see an improvement in ranking correla-
tions when moving from using one reference 
translation to more. However, when running 
BLEU for the NIST 2003 Chinese MT evalua-
tion, this trend is inverted, and using single refer-
ence translation gave higher correlation than us-
ing all four references, as illustrated in Table 5.  
Turian et al (2003) reports the same peculiar 
behavior from BLEU on Arabic MT evaluations 
in Figure 5b of their paper. When using three 
reference translations, as the number of segments 
(sentences usually) increases, BLEU correlates 
worse than using single references.  
Since the matching and underlining counting 
mechanisms of ParaEval are built upon the 
fundamentals of BLEU, we were keen to find out 
the differences, other than paraphrase matching, 
between the two methods when the number of 
reference translation changes. By following the 
description from the original BLEU paper, three 
incremental steps were set up for duplicating its 
implementation, namely modified unigram preci-
sion (MUP), geometric mean of MUP (GM), and 
multiplying brevity penalty with GM to get the 
final score (BP-BLEU). At each step, correla-
tions were computed for both using single- and 
multi- references, shown in Table 6a, b, and c. 
 Given that many small changes have been 
made to the original BLEU design, our replica-
tion would not produce the same scores from the 
current version of BLEU. Nevertheless, the in-
verted behavior was observed in fluency correla-
tions at the BP-BLEU step, not at MUP and GM. 
This indicates to us that the multiplication of the 
brevity penalty to balance precision scores is 
problematic. According to (Turian et al, 2003), 
correlation scores computed from using fewer 
references are inflated because the comparisons 
exclude the longer n-gram matches that make 
automatic evaluation procedures diverge from 
the human judgments. Using a large collection of 
paraphrases in comparisons allows those longer 
n-gram matches to happen even if single refer-
ences are used. This collection also allows 
ParaEval to directly compute recall scores, 
avoiding an approximation of recall that is 
problematic.  
ParaEval 95% L-CI 95% H- C I
E01 0.8086 0.8 0 .8172
E02 0.7383 0.7268 0.7497
E03 0.7839 0.7754 0.7923
E04 0.7742 0.7617 0.7866
Table 4. Differences among reference
translations (raw ParaEval precision
scores).  
6(a). System-ranking correlation when using modified
unigram precision (MUP) scores.
6(b). System-ranking correlation when using geometric mean
(GM) of MUPs.
6(c). System-ranking correlation when multiplying the
brevity penalty with GM.
Table 6. Incremental implementation of
BLEU and the correlation behavior at the
three steps: MUP, GM, and BP-BLEU.
MUP E01 E02 E03 E04 4 refs
Fluency 0.6597 0.6216 0.6923 0.4912 0.692
95% L-CI 0.6568 0.6189 0.6917 0.4863 0.6915
95% H-CI 0.6626 0.6243 0.6929 0.496 0.6925
Adequacy 0.5818 0.5459 0.6141 0.4602 0.6165
95% L-CI 0.5788 0.5432 0.6132 0.4566 0.6156
95% H-CI 0.5847 0.5486 0.6151 0.4638 0.6174
GM E01 E02 E03 E04 4 refs
Fluency 0.6633 0.6228 0.6925 0.4911 0.6922
95% L-CI 0.6604 0.6201 0.692 0.4862 0.6918
95% H-CI 0.6662 0.6255 0.6931 0.4961 0.6929
Adequacy 0.5817 0.548 0.615 0.4641 0.6159
95% L-CI 0.5813 0.5453 0.614 0.4606 0.615
95% H-CI 0.5871 0.5508 0.616 0.4676 0.6169
BP-BLEU E01 E02 E03 E04 4 refs
Fluency 0.6637 0.6227 0.6921 0.4947 0.5743
95% L-CI 0.6608 0.62 0.6916 0.4899 0.5699
95% H-CI 0.6666 0.6254 0.6927 0.4996 0.5786
Adequacy 0.5812 0.5486 0.5486 0.5486 0.6671
95% L-CI 0.5782 0.5481 0.5458 0.5458 0.6645
95% H-CI 0.5842 0.5514 0.5514 0.5514 0.6697
 
Table 5. BLEU?s correlating behavior with
multi- and single-reference.
BLEU E01 E02 E03 E04 4 refs
Fluency 0.7114 0.701 0.7084 0.7192 0.6978
95% L-CI 0.7099 0.6993 0.7065 0.7177 0.6967
95% H-CI 0.7129 0.7026 0.7102 0.7208 0.6989
Adequacy 0.644 0.6238 0.6535 0.675 0.6108
95% L-CI 0.6404 0.6202 0.6496 0.6714 0.6083
95% H-CI 0.6476 0.6274 0.6574 0.6786 0.6133
83
7  Conclusion and Future Work 
In this paper, we have described ParaEval, an 
automatic evaluation framework for measuring 
machine translation results. A large collection of 
paraphrases, extracted through an unsupervised 
fashion using SMT methods, is used to improve 
the quality of the evaluations. We addressed 
three important issues, the paraphrasing support, 
the computation of recall measurement, and pro-
viding high correlations with human judgments.  
Having seen that using paraphrases helps a 
great deal in evaluation tasks, naturally the next 
task is to explore the possibility in paraphrase 
induction. The question becomes how to use con-
textual information to calculate semantic close-
ness between two phrases. Can we expand the 
identification of paraphrases to longer ones, ide-
ally sentences?  
The problem in which content bearing words 
carry the same weights as the non-content bear-
ing ones is not addressed. From examining the 
paraphrase extraction process, it is unclear how 
to relate translation probabilities and confidences 
with semantic closeness. We plan to explore the 
parallels between the two to enable a weighted 
implementation of ParaEval.  
Reference 
Anonymous. 2006. Complete citation omitted due to 
the blind review process.  
Bannard, C. and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. Proceedings of 
ACL-20 0 5 .  
Browner, J. 2006. The translator?s blues. 
http://www.slate.com/id/2133922/.  
Callison-Burch, P. Koehn, and M. Osborne. 2006. 
Improved statistical machine translation using 
paraphrases. In Proceedings of H L T / NA A CL-20 0 6 .  
Callison-Burch, C., M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of bleu in machine transla-
tion research. In Proceedings of EA CL-20 0 6 .  
Inkpen, D. Z. and G. Hirst. 2003. Near-synonym 
choice in natural language generation. Proceedings 
of R A NL P-20 0 3 .  
Leusch, G., N. Ueffing, and H. Ney. 2003. A novel 
string-to-string distance measure with applications 
to machine translation evaluation. In Proceedings 
of MT  Summit I X .  
Liang, P., B. Taskar, and D. Klein. Consensus of sim-
ple unsupervised models for word alignment. In 
Proceedings in H LT / N AA CL-2 0 0 6 .  
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. 
Proceedings of the H L T-20 0 3 . 
Lin, C.Y. and F. J. Och. 2004. Automatic evaluation 
of machine translation quality using longest com-
mon subsequence and skip-bigram statistics. Pro-
ceedings of A CL- 20 0 4 .  
Och, F. J. and H. Ney. 2003. A systematic comparison 
of various statistical alignment models. Computa-
tional Linguist ics , 29(1): 19?51, 2003.  
Och, F. J. and H. Ney. 2004. The alignment template 
approach to statistical machine translation. Compu-
tational L inguis tics , 30(4), 2004.  
Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. 
2002. IBM research report Bleu: a method for 
automatic evaluation of machine translation I B M  
Research D iv is ion Technical Report , RC22176, 
2001.  
Turian, J. P., L. Shen, and I. D. Melamed. 2003. 
Evaluation of machine translation and its evalua-
tion. Proceedings of MT  Summit I X .  
 
84
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 497?504
Manchester, August 2008
 
Understanding and Summarizing Answers in Community-Based 
Question Answering Services 
Yuanjie Liu1, Shasha Li2, Yunbo Cao1,3, Chin-Yew Lin3, Dingyi Han1, Yong Yu1 
1Shanghai Jiao Tong University, 
Shanghai, China, 200240 
{lyjgeorge,handy,yyu} 
@apex.sjtu.edu.cn 
2National University of 
Defense Technology, 
Changsha, China, 410074
Shashali 
@nudt.edu.cn 
3Microsoft Research Asia,
Beijing, China, 100080 
{yunbo.cao,cyl} 
@microsoft.com 
 
Abstract 
Community-based question answering 
(cQA) services have accumulated millions 
of questions and their answers over time. 
In the process of accumulation, cQA ser-
vices assume that questions always have 
unique best answers. However, with an in-
depth analysis of questions and answers 
on cQA services, we find that the assump-
tion cannot be true. According to the anal-
ysis, at least 78% of the cQA best answers 
are reusable when similar questions are 
asked again, but no more than 48% of 
them are indeed the unique best answers. 
We conduct the analysis by proposing 
taxonomies for cQA questions and an-
swers. To better reuse the cQA content, 
we also propose applying automatic sum-
marization techniques to summarize an-
swers. Our results show that question-type 
oriented summarization techniques can 
improve cQA answer quality significantly. 
1 Introduction 
Community-based question and answering (cQA) 
service is becoming a popular type of search re-
lated activity. Major search engines around the 
world have rolled out their own versions of cQA 
service. Yahoo! Answers, Baidu Zhidao, and 
Naver Ji-Sik-In1 are some examples.  
In general, a cQA service has the following 
workflow. First, a question is posted by the asker 
in a cQA service and then people in the commu-
nity can answer the question. After enough num-
ber of answers are collected, a best answer can 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution- Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
be chosen by the asker or voted by the communi-
ty. The resulting question and answer archives 
are large knowledge repositories and can be used 
to complement online search. For example, Nav-
er?s Ji-Sik-In (Knowledge iN) has accumulated 
about 70 million entries2.  
In an ideal scenario, a search engine can serve 
similar questions or use best answers as search 
result snippets when similar queries are submit-
ted. To support such applications, we have to 
assume the best answers from cQA services are 
good and relevant answers for their pairing ques-
tions. However, the assumption might not be true 
as exemplified by the following examples. 
Question Title 
Which actress has the most seductive 
voice?..could range from a giggly goldie 
hawn..to a sultry anne bancroft? 
Question  
Description 
or any other type of voice that you find allur-
ing. .. 
Best Answer 
(Polls & Surveys) Fenella Fielding, wow!!!! 
Best Answer 
(Movies) i think joanna lumlley has a really sexy voice 
Table 1. Same Question / Different Best Answers
 
Question Title Does anyone know of any birthdays coming up soon? 
Question  
Description 
Celerities, people you know, you? Anyway I 
need the name and the date. If you want to 
know it is for my 
site,  http://www.jessicaparke2.piczo.com... 
and that is not site advertising.  
Answer 
Novembers Are: 
Paul Dickov nov 1st 
Nelly (not furtado) nov 2nd ? 
Best Answer Check imdb.com, they have this celebrity birthdays listed. 
Table 2. Question with Alternative Answers 
Table 1 presents a question asking communi-
ty opinions about ?who is the actress has the 
most seductive voice?. The asker posted the same 
question twice at different Yahoo! Answers cate-
gories: one in Polls & Surveys and one in Movies. 
                                                 
1 Yahoo! Answers: answers.yahoo.com; Baidu Zhidao: zhi-
dao.baidu.com; Naver Ji-Sik-In: kin.naver.com 
2www.iht.com/articles/2007/07/04/technology/naver.php 
497
 Two different best answers were chosen by the 
same asker due to non-overlapping of answers. 
Table 2 shows another example, it asks about 
?the coming birthdays of stars?. The best answer 
chosen by the asker is very good because it pro-
vides useful URL information where the asker 
can find her answers. However, other answers 
listed a variety of birthdays of stars that also 
answered the question. These two examples indi-
cate that the conventional cQA policy of allow-
ing askers or voters to choose best answers might 
be working fine with the purpose of cQA but it 
might not be a good one if we want to reuse these 
best answers without any post-processing. 
To find out what might be the alternatives to 
the best answers, we first carried out an in-depth 
analysis of cQA data by developing taxonomies 
for questions and answers. Then we propose 
summarizing answers in a consideration of ques-
tion type, as the alternative to the best answers. 
For example, for the ?actress voice? question, a 
summary of different people?s opinions ranked 
by popularity might be a better way for express-
ing the question?s answers. Similar to the ?ac-
tress voice? question, the ?celebrity birthday? 
question does not have a fix set of answers but is 
different from the ?actress voice? question that its 
answers are facts not opinions. For fact-based 
open ended questions, combining different an-
swers will be useful for reuse of those answers.  
The rest of this paper is arranged as follows. 
We review related work in Section 2. We devel-
op a framework for answer type taxonomy in 
Section 3 and a cQA question taxonomy in Sec-
tion 4. Section 5 presents methods to summarize 
cQA answers. Finally, we conclude this paper 
and discuss future work in Section 6. 
2 Related Work 
Previous research on cQA (community-based 
Question and Answering) domain focused on 
three major areas: (1) how to find similar ques-
tions given a new question (Jeon et al 2005a; 
Jeon et al, 2005b), (2) how to find experts given 
a community network(Liu et al, 2005; Jurczyk & 
Agichtein, 2007), and (3) how to measure answer 
quality and its effect on question retrieval. The 
third area of focus is the most relevant to our re-
search. Jeon et al (2006)?s work on assessing 
cQA answer quality is one typical example. They 
found that about 1/3 of the answers among the 
1,700 Q&A pairs from Naver.com cQA data 
have quality problems and approximately 1/10 of 
them have bad answers 3 . They used 13 non-
textual features and trained a maximum entropy 
model to predict answer quality. They showed 
that retrieval relevance was significantly im-
proved when answer quality measure was inte-
grated in a log likelihood retrieval model.  
As mentioned in Section 1, cQA services 
provide an alternative way for users to find in-
formation online. Questions posted on cQA sites 
should reflect users? needs as queries submitted 
to search engines do. Broder (2002) proposed 
that search queries can be classified into three 
categories, i.e. navigational, informational, and 
transactional. Ross and Levinson (2004) sug-
gested a more elaborated taxonomy with five 
more subcategories for informational queries and 
four more subcategories for resource (transac-
tional) queries. In open-domain question answer-
ing research that automatic systems are required 
to extract exact answers from a text database 
given a set of factoid questions (Voorhees and M. 
Ellen, 2003), all top performing systems had in-
corporated question taxonomies (Hovy et al, 
2001; Moldovan et al, 2000; Lytinen et al, 2002; 
Jijkoun et al, 2005). Based on the past expe-
riences from the annual NIST TREC Question 
and Answering Track 4  (TREC QA Track), an 
international forum dedicating to evaluate and 
compare different open-domain question answer-
ing systems, we conjecture that a cQA question 
taxonomy would help us determine what type of 
best answer is expected given a question type.  
Automatic summarization of cQA answers is 
one of the main focuses of this paper. We pro-
pose that summarization techniques (Hovy and 
Lin, 1999; Lin and Hovy, 2002) can be used to 
create cQA answer summaries for different ques-
tion types. Creating an answer summary given a 
question and its answers can be seen as a multi-
document summarization task. We simply re-
place documents with answers and apply these 
techniques to generate the answer summary. The 
task has been one of the main tasks the Docu-
ment Understanding Conference5 since 2004. 
3 A Framework  for Answer Type 
To study how to exploit the best answers of cQA, 
we need to first analyze cQA answers. We would 
like to know whether the existing best answer of 
a specific question is good for reuse. If not, we 
                                                 
3 Answers in Jeon el al.?s work were rated in three levels: 
good, medium, and bad. 
4 http://trec.nist.gov/data/qamain.html 
5 http://duc.nist.gov 
498
want to 
are. We 
by cQA 
ferentiat
tomatica
We m
swers fo
for answ
al categ
examini
the 4 mo
ries (100
tainmen
(S&C), H
we dev
based on
termines
can be r
ilar to th
One o
ercise an
The tax
cussions
notators 
category
annotato
on a sin
made t
taxonom
discuss 
answer t
Figur
Figur
onomy. 
Reusabl
means th
similar 
while a 
reused. 
Factual 
that can 
jective B
as the be
F
Unique
understand w
will refer to
askers or vo
e it with be
lly generate
ade use of
r developin
er type. The
ories in Yah
ng 400 rando
st popular 
 questions f
t & Music 
ealth, and 
eloped a cQ
 the princip
 a BA?s ans
eused or not
e BA?s ques
f the author
d developed
onomy was 
 among the 
to do the a
 label that 
rs. If none o
gle categor
he final d
y is describ
the question
axonomy in 
e 1. cQA Ser
e 1 shows th
It first divid
e and Not 
at it can be 
question to 
Not Reusab
The Reusabl
and Subject
be used as t
A is one of 
st answer. 
Reusable
actual
Not?
Unique
Direct Indire
Su
hy and wha
 the ?best a
ters as BA h
st answers 
d in our expe
 questions fr
g and testing
re are over 
oo! Answe
mly selecte
top Yahoo! 
rom each ca
(E&M), So
Computers &
A answer 
le of BA reu
wer type bas
 when a que
tion is asked
s carried ou
 the initial a
then modif
authors. We
nnotation. W
was agreed 
f the three a
y label, one
ecision. Th
ed in this 
 type and t
next section
vices BA Ty
e resulting 
es BA into
Reusable. A
reused as the
its question
le BA mea
e BA is fur
ive. A Fact
he best answ
the opinions
Best?
Answer
ct
bjective Re
t the alterna
nswers? sele
enceforth to
annotated or
riments. 
om Yahoo!
 our framew
1,000 hierar
rs. By manu
d questions 
Answers cat
tegory) ? E
ciety & Cu
 Internet (C
type taxon
sability tha
ed on ?if th
stion that is 
 again?. 
t this manua
nswer taxon
ied through
 asked three
e assigned
by at least
nnotators ag
 of the aut
e answer 
section and
he relation 
. 
pe Taxonom
answer type
 two catego
 Reusable
 best answe
 is asked a
ns it canno
ther divided
ual BA is a
er; while a 
 that can be 
Not??
Reusable
levant Irre
 
tives 
cted 
 dif-
 au-
 An-
ork 
chic-
ally 
from 
ego-
nter-
lture 
&I), 
omy 
t de-
e BA 
sim-
l ex-
omy. 
 dis-
 an-
 the 
 two 
reed 
hors 
type 
 we 
with 
 
y. 
 tax-
ries: 
BA 
r if a 
gain; 
t be 
 into 
 fact 
Sub-
used 
T
Uniq
a un
answ
Uniq
The 
type
its q
swer
ple, 
Indi
whil
birth
A
for o
ques
Each
T
vant
used
leva
aske
Nick
?I'm
ly So
answ
er?s 
ques
best
its q
tion 
give
answ
ema
To b
Answ
Uniqu
Direc
Indire
Factu
Subje
Reusa
Relev
Irrelev
Not R
T
type
mor
ries 
two 
 
A
mos
(50%
the o
or v
answ
levant
he Factual
ue and Not
ique best an
er add m
ue BA has
Not Unique
s: Direct an
uestion dire
s its questio
the question
rect BA wh
e there is al
day lists. 
 Subjective
pinions or r
tion asked ?
 answerer w
he Not Reus
 and Irrelev
 as a best an
nt to its qu
d ?Why was
 Lachey so 
 not sure wh
uth Jersey, 
er is releva
location wh
tion; an Irre
answer to i
uestion. Th
period has 
n that meets
er?.? of th
il without sh
ox? is in thi
er Type 
e 
t 
ct 
al Total 
ctive 
ble Total 
ant
ant
eusable Total
Table 3. D
able 3 show
s on four ca
e than 48%. 
tend to hav
categories.
mong the fo
tly not uniq
) of subjec
ne BA per c
oters is not g
er. Howeve
BA type 
 Unique. A 
swer to its q
ore informa
 other alter
BA type is d
d Indirect. A
ctly; while 
n through i
 mentioned
ich gives a
so a Direct a
BA answers
ecommenda
Which is th
ould have h
able BA has
ant. A Relev
swer to its 
estion, for 
 "I Can't Ha
shortlived??
ere you live
that song wa
nt but witho
ich does n
levant BA c
ts question a
e BA ?It ap
expired. If 
 your needs
e question ?
owing the em
s case. 
C&I
47%
28%
9%
84%
4%
88%
3%
9%
12%
istribution o
s the dist
tegories. Un
The C&I an
e more fac
ur categorie
ue and hav
tive answers
QA questio
ood enough
r, we might
has two s
Unique BA 
uestion and 
tion; while
native best 
ivided into 
 Direct BA
an Indirect
nference. Fo
 in section 1
 website re
nswer just g
 questions t
tions. For ex
e best sci-fi 
is own idea.
 two subtyp
ant BA cou
question bu
example, a 
te You Anym
 A Relevant
, but in NJ, 
s played ou
ut knowing
ot really an
ould not be u
nd it is irre
pears that t
an answer h
, please pic
how to for
ail address
E&M Heal
28% 48
7% 30
3% 5
38% 83%
40% 7
78% 90%
1% 1
21% 9
22% 10%
f Answer Ty
ribution of 
ique answer
d the Health
tual BAs th
s, S&C ans
e a high pe
. This indic
n chosen by
 for reuse as
 be able to a
ubtypes: 
has only 
no other 
 a Not 
answers. 
two sub-
 answers 
 BA an-
r exam-
 has the 
ference, 
ives the 
hat look 
ample, a 
movie?? 
 
es: Rele-
ld not be 
t it is re-
question 
ore" by 
BA said 
especial-
t??, this 
 the ask-
swer the 
sed as a 
levant to 
he ques-
as been 
k a ?best 
ward an 
es in the 
th S&C
% 13%
% 18%
% 2%
 33%
% 50%
 83%
% 0%
% 17%
 17%
pe 
Answer 
s are no 
 catego-
an other 
wers are 
rcentage 
ates that 
 its asker 
 the best 
pply au-
499
tomatic 
summar
(but not 
ible solu
E
T
Table
over wh
on a sin
the ques
stable (o
4 A C
As we w
my, we 
themselv
well. As
question
best answ
Rose an
engine q
their tax
engine q
we follo
onomy a
modate t
Fi
Figur
my. We
and prop
Informa
ilar as in
ry consi
an answ
with peo
Navig
seeking 
would li
know the
Trans
tend to g
compute
Navigat
summariza
ized answers
unique) answ
tions in Sect
Category
Computer & In
ntertainment &
Health 
Society & Cul
able 4. Disag
 4 shows t
ich none of
gle category
tion taxonom
ver at least 7
QA Quest
ere develop
often could
es and had 
 we discuss
 would help
er types.  
d Levinson?
ueries has 
onomy was 
ueries. Inste
wed the ba
nd made so
he particula
gure 2.  Que
e 2 shows th
 retain Brod
ose a new S
tional and Tr
 Broder?s ta
sts of questi
er but just w
ple in cQA 
ational ca
URLs of sp
ke to visit, 
 fan sites of
actional ca
et resources
r program th
ional Inform
Constant
Opinion
tion techni
 for at least
ers. We pro
ion 5. 
 
ternet 
 Music 
ture 
reement on 
he percenta
 the three a
 label. The r
y develope
9% question
ion Taxon
ing our answ
 not solely 
to consider t
ed in Sectio
 us determ
s (2004) tax
similar goal
developed t
ad of starti
sic hierarchy
me modific
r of cQA ser
stion Type T
e resulting 
er?s taxono
ocial catego
ansactional
xonomy whi
ons that do 
ere used to 
services. 
tegory con
ecific websit
for example
 Hannah Mo
tegory con
. A typical o
at lets you c
cQA
Question
ational
Dynamic
Context?
Dependent
Trans
ques to c
 half of reus
vide some p
Percenta
18%
17%
21%
20%
Answer Typ
ge of ques
nnotators ag
esults show
d above is p
s). 
omy 
er type tax
rely on ans
heir question
n 2, the typ
ine the expe
onomy of se
 to ours th
o classify se
ng from scr
 of R&L?s 
ations to acc
vices. 
axonomy
question tax
my at top le
ry. Navigati
 are defined 
le Social cat
not intend to
elicit intera
tains ques
es that the a
, ?Does any
ntana?? 
tains ques
ne is ?Is the
reate a plan
Open
actional
 
reate 
able 
oss-
ge 
e 
tions 
reed 
 that 
retty 
ono-
wers 
s as 
e of 
cted 
arch 
ough 
arch 
atch, 
tax-
om-
 
ono-
vels 
onal, 
sim-
ego-
 get 
ction 
tions 
sker 
body 
tions 
re a 
et?? 
F
to t
Con
answ
dich
port
betw
taxo
R&L
ques
latio
wou
F
tego
Opin
Que
peop
think
jects
ple.
tion
diffe
?Wh
diffe
Ope
som
have
selv
tion 
com
follo
clud
cont
T
serv
to g
joke
tially
or o
lazy
toge
beco
goog
will 
they
a ne
who
T
ques
cate
only
ques
occu
sinc
sear
Social
or Informati
wo subcateg
stant questio
ers while d
otomy of in
 our intentio
een the que
nomy. Cons
?s closed q
tion is ?Whi
n?? but ?W
ld be a dyna
or Dynamic
ries: Opinio
ion questio
stions in th
le in cQA 
 of some p
. ?Is Micros
Context-dep
s having dif
rent contex
at is the pop
rent answer
n category
e facts or m
 a variety o
es may have
?Can you li
ing week??
ws R&L?s 
es what is n
ext-depende
he new Soc
ices. Questio
et an answer
s and expre
, askers trea
nline forums
people com
ther with th
me a hacker
le search?
continue to 
 can give up
gative sentim
 asked how t
able 5 show
tion types 
gories. We 
 occupy 1
tions are ev
r in the sam
e people ve
ch engines 
on category,
ories: Con
ns have a f
ynamic que
formational
n to establi
stion taxon
tant questio
uery type. A
ch country h
hat is the po
mic question
category, w
n, Context-D
ns are those 
is category 
communiti
eople, some
oft Vista wo
endent ques
ferent answ
t. For exa
ulation of C
s according 
contains q
ethods. Th
f answers o
 unconstrain
st some birt
is an exam
open query 
ot covered 
nt categories
ial category
ns in this ca
. These que
ssing askers
t cQA servi
. The questi
e on here si
e question 
? It really is
hopefully so
ask, will cli
 faster? ? a
ent toward
o become a 
s the distr
on 4 differe
observe tha
1% ~ 20%
en fewer su
ple question
ry likely w
to discover 
 we first div
stant and D
ixed or stab
stions do n
 category is
sh intuitive 
omy and the
n type is s
n example 
as the large
pulation of 
. 
e define thre
ependent an
asking for o
seek opinio
es about w
 events, or s
rth it?? is a
tions are tho
ers accordin
mple, the 
hina?? sho
to the differ
uestions ask
e questions
r their answ
ed depth. T
hdays of sta
ple. This es
category. It
by the opin
. 
 is specific
tegory do n
stions includ
? own ideas
ce as chattin
on ?Why do 
mply just to
description 
n't that har
me of the pe
ck the link b
ctually is ex
s a number o
hacker. 
ibution of 
nt Yahoo! 
t constant q
 while nav
ch that they
s. This is re
ould be abl
answers of
ide it in-
ynamic. 
le set of 
ot. This 
 to sup-
mapping 
 answer 
imilar to 
constant 
st popu-
China?? 
e subca-
d Open. 
pinions. 
ns from 
hat they 
ome ob-
n exam-
se ques-
g to the 
question 
uld have 
ent date. 
ing for 
 usually 
er them-
he ques-
rs in the 
sentially 
 also in-
ion and 
 to cQA 
ot intend 
e telling 
. Essen-
g rooms 
so many 
 ask...?? 
?how to 
d to do a 
ople that 
elow so 
pressing 
f people 
different 
Answers 
uestions 
igational 
 do not 
asonable 
e to use 
 naviga-
500
 tional and constant questions. They do not have 
to ask these types of question on community-
based question answering services. On the con-
trary, open and opinion questions are frequently 
asked, it ranges from 56%~83%.  
Question Type C&I E&M Health S&C
Navigational Total 0% 0% 0% 0%
Constant 15% 20% 15% 11%
Opinion 8% 37% 16% 60%
Context     Dependent 0% 1% 1% 0%
Open 59% 19% 67% 18%
Dynamic Total 67% 57% 84% 78%
Informational Total 82% 77% 99% 89%
Transactional Total 14% 8% 0% 1%
Social Total 4% 15% 1% 10%
Table 5 Distribution of Question Type 
Intersection Number UNI DIR IND SUB REL IRR
Navigational 0 0 0 0 0 0
Constant 48 9 3 0 1 0
Open 51 62 13 15 5 17
Context-dep 0 0 1 0 0 1
Opinion 15 13 1 84 0 8
Transactional 10 7 4 1 0 1
Social 0 0 0 1 0 29
Table 6. Question Answer Correlation 
Table 6 (UNI: unique, DIR: direct, IND: indi-
rect, SUB: subjective, REL: relevant, IRR: irre-
levant) gives the correlation statistics of question 
type vs. answer type. There exists a strong corre-
lation between question type and answer type. 
Every question type tends to be associated with 
only one or two answer types (bold numbers in 
Table 6).  
5 Question-Type Oriented Answer 
Summarization 
Since the BAs for at least half of questions do 
not cover all useful information of other answers, 
it is better to adopt post-processing techniques 
such as answer summarization for better reuse of 
the BAs. As observed in the previous sections, 
answer types can be basically predicted by ques-
tion type. Thus, in this section, we propose to use 
multi-document summarization (MDS) tech-
niques for summarizing answers according to 
question type. Here we assume that question type 
can be determined automatically. In the follow-
ing sub-sections, we will focus on the summari-
zation of answers to open or opinion questions as 
they occupy more than half of the cQA questions. 
5.1 Open Questions 
Algorithm: For open questions, we follow typi-
cal MDS procedure: topic identification, inter-
pretation & fusion, and then summary generation 
(Hovy and Lin, 1999; Lin and Hovy, 2002). Ta-
ble 7 describes the algorithm.  
1. Employ the clustering algorithm on answers 
2. Extract the noun phrases in each cluster, using a shallow parser.6 
3. For each cluster and each label (or noun phrase), calculate the 
score by using the Relevance Scoring Function:  
?p?w|??PMI?w, l|C? ?  D??|C?
?
 
Where ? is the cluster, w is the word, l is the label or noun phrase, C 
is the background context which is composed of 5,000 questions 
in the same category, p(?) is conditional probability, PMI(?) is 
pointwise mutual information, and D(?) is KL-divergence 
4. Extract the key answer which contains the noun phrase that has 
the highest score in each cluster 
5. Rank these key answers by cluster size and present the results. 
Table 7. Summarization Algorithm(Open-Type) 
In the first step, we use a bottom-up approach 
for clustering answers to do topic identification. 
Initially, each answer forms a cluster. Then we 
combine the most similar two clusters as a new 
cluster if their similarity is higher than a thre-
shold. This process is repeated until no new clus-
ters can be formed. For computing similarities, 
we regard the highest cosine similarity of two 
sentences from two different clusters as the simi-
larity of the two clusters. Then we extract salient 
noun phrases, i.e. cluster labels, from each clus-
ter using the first-order relevance scoring func-
tion proposed by Mei et al (2007), (step 2,3 in 
Table 7).  In the fusion phase (step 4), these 
phrases are then used to rank answers within 
their cluster. Finally in the generation phase (step 
5), we present the summarized answer by ex-
tracting the most important answer in every clus-
ter and sort them according to the cluster size 
where they come from.  
Case Example: Table 8 presents an example 
of summarization results of open-type questions. 
The question asks how to change Windows XP 
desktop to Mac style. There are many softwares 
providing such functionalities. The BA only lists 
one choice ? the StarDock products, while other 
answers suggest Flyakite and LiteStep. The au-
tomatic summarized answer (ASA) contains a 
variety of for turning Windows XP desktop into 
Mac style with their names highlighted as cluster 
labels. Compared with manually-summarized 
answer (MSA), ASA contains most information 
of MSA while retains similar length with BA and 
MSA. 
5.2 Opinion Questions 
Algorithm: For opinion questions, a comprehen-
sive investigation of this topic would be beyond 
the scope of this paper since this is still a field 
                                                 
6 http://opennlp.sourceforge.net 
501
 under active development (Wiebe et al, 2003; 
Kim and Hovy, 2004). We build a simple yet 
novel opinion-focused answer summarizer which 
provides a global view of all answers. We divide 
opinion questions into two subcategories. One is 
sentiment-oriented question that asks the senti-
ment about something, for example, ?what do 
you think of ??. The other is list-oriented ques-
tion that intends to get a list of answers and see 
what item is the most popular.  
For sentiment-oriented questions, askers care 
about how many people support or against some-
thing. We use an opinion word dictionary7, a cue 
phrase list, a simple voting strategy, and some 
heuristic rules to classify the sentences into Sup-
port, Neutral, or Against category and use the 
overall attitude with key sentences to build sum-
marization. For list-oriented questions, a simple 
counting algorithm that tallies different answers 
of questions together with their supporting votes 
would be good answer summaries. Details of the 
algorithm are shown in Table 9, 10. 
Case Example: Table 11 presents the summa-
rization result of an sentiment-oriented question, 
it asks ?whether it is strange for a 16-year child 
to talk to a teddy bear??, the BA is a negative 
response. However, if we consider all answers, 
                                                 
7 Inquirer dictionary  http://www.wjh.harvard.edu/~inquirer. 
we find that half of the answers agree but another 
half of them disagree. The distribution of differ-
ent sentiments is similar as MSA. Table 12 
shows the summarization result of a list-oriented 
question, the question asks ?what is the best sci-fi 
movie?? The BA just gives one choice ?Indepen-
dence day? while the summarized answer gives a 
list of best sci-fi movies with the number of sup-
porting vote. Though it is not complete compared 
with MSA, it contains most of the options which 
has highest votes among all answers. 
1. Employ the same cluster procedure of Open-Type question. 
2. If an answer begins with negative cue phrase (e.g. ?No, it isn?t? 
etc.), it is annotated as Against. If a response begins with positive 
cue phrase (e.g. ?Yes, it is? etc.), it is annotated as Support. 
3. For a clause, if number of positive sentiment word is larger than 
negative sentiment word, the sentiment of the clause is Positive. 
Otherwise, the sentiment of the clause is Negative. 
4. If there are negative indicators such as ?don?t/never/?? in front 
of the clause, the sentiment should be reversed. 
5. If number of negative clauses is larger than number of positive 
clauses, the sentiment of the answer is Negative. Otherwise, the 
sentiment of the answer is Positive. 
6. Denote the sentiment value of question as s(q), the sentiment 
value of an answer as s(a), and then the final sentiment of the an-
swer is logical AND of s(q) and s(a) 
7. Present key sentiments with attitude label 
Table 9. Summarization Algorithm (Senti-
ment-Opinion) 
1. Segment the answers into sentences 
2. Cluster sentences  by using similar process in open-type 
3. For each cluster, choose the key sentence based on mutual infor-
mation between itself and other sentences within the cluster 
4. Rank the key sentences by the cluster size and present them 
ogether with votes 
Table 10. Summarization Algorithm (List-
Opinion) 
Question (http://answers.yahoo.com/question/?qid=1006050125145) 
I am 16 and i stil talk to my erm..teddy bear..am i wierd??? 
Best Answer Chosen 
not at all i'm 14 and i too do that 
Auto-summarized Answer 
Support 
A: It's might be a little uncommon for a 16 year old to talk to a 
teddy bear but there would be a serious problem if you told me that 
your teddy bear answered back as you talked to him!!:)  
A: I slept with my teddy bear until I graduated.  Can't say that I 
ever had a conversation with him, but if I had I'm sure he would've 
been a very good listener. 
Against 
A: i talk to a  seed im growing .. its not weird .... :)  
A: No, you're not weird.....you're Pratheek! :D  
A: no, i like to hold on to my old memories too. i do it sometimes 
too.  
A: It will get weird when he starts to answer back!  
A: not really. it depends how you talk i mean not if you talk to it 
like its a little kid like my brother does.  
Overall Attitude: Support 5 / Neutral 1 / Against 5  
Manually-summarized Answer 
support (vote 4) 
neutral (vote 2) 
against (vote 5) reasons: i like to hold on to my old memories too. 
(vote 1) I slept with my teddy bear until I graduated. (vote 1) i'm 14 
and i too do that (vote 1) 
Table 11. Summary of Sentiment-Opinion 
Question 
 
 
Question 
(http://answers.yahoo.com/question/?qid=1005120801427) 
What is the best way to make XP look like Mac osX? 
Best Answer Chosen 
I found the best way to do this is to use WindowsBlinds. A pro-
gram that, if you use the total StarDock, package will allow you to 
add the ObjectBar in addition to changed the toolbars to be OS X 
stylized. If you want added functionality you can download pro-
grams off the internet that will mimic the Expose feature which will 
show you a tiled set of all open windows. Programs that will do this 
include: WinPlosion, Windows Exposer, and Top Desk 
Auto-summarized Answer 
LiteStep:An additional option is LiteStep - a "Shell Replacement" 
for Windows that has a variety of themes you can install. Undoub-
tedly there are various Mac OSX themes avaialable for LiteStep. I 
have included a source to a max osx theme for Litestep at custom-
ize.org.  
Flyakite:Flyakite is a transformation pack and the most compre-
hensive in terms of converting an XP system's look to that of an OS 
X system, google it up and you should find it, v3 seems to be in 
development and should be out soon. 
Window Blinds:http://www.stardock.com/products/windowb... 
Manually-summarized Answer 
One way is to use WindowsBlinds. The package will allow you to 
add the ObjectBar for changing to the OSX theme. You can also 
make added functionality of Expose feature by downloading the 
programs like WinPlosion, Windows Exposer and Top Desk. The  
URL of it is http://www.stardock.com/products/windowblinds/. 
Another option is to use Flyakite which is a transformation pack. 
The third Option is the LiteStep, it is a "Shell Replacement" for 
windows that has a variety of Mac OSX tehmes you can install. 
The url is http://litestep.net and I have included a source of Mac OS 
theme for Litestep at http://www.customize.org/details/33409. 
Table 8. Summary of Open-Question 
502
  
Question (http://answers.yahoo.com/question/?qid= 
20060718083151AACYQJn) 
What is the best sci-fi movie u ever saw? 
Best Answer Chosen 
Independance Day 
Auto-summarized Answer 
star wars (5)  
Blade Runner (3) 
fi movie has to be Night of the Lepus (2)  
But the best "B" sci (2)  
I liked Stargate it didn't scare me and I thought they did a great job 
recreating Egypt (3)  
Independance Day (3) 
Manually-summarized Answer 
Star Wars (vote 6); The Matrix (vote 3); Independence Day (vote 
2); Blade Runner (vote 2); Starship Troopers (vote 2); Alien (vote 
2); Alien v.s Predator (vote 1); MST3K (vote 1);  
Table 12. Summary of List-Opinion Question
5.3 Experiments 
Information Content: To evaluate the effec-
tiveness of automatic summarization, we use the 
information content criterion for comparing ASA 
with BA. It focuses on whether ASA or BA con-
tains more useful information to the question. 
Information point is used in the evaluation. 
Usually, one kind of solution for open questions 
or one kind of reason for opinion questions can 
contribute one information point. By summing 
all information points in both ASA and BA, we 
then can compare which one contains more in-
formation. Intuitively, longer texts would contain 
more information. Thus, when comparing the 
information content, we limit the length of ASA 
with several levels to do the evaluation. Take 
question in Table 8 as an example, the BA just 
gives one software, which contributes one infor-
mation point while the ASA lists three kinds of 
software which contributes three information 
points. Thus, ASA is considered better than BA.  
For each question, we generate 100%, 150%, 
and 200% BA word-length ASAs. Three annota-
tors are asked to determine whether an ASA is 
better than, equal to, or worse than its corres-
ponding BA in terms of information content. 
Voting strategy is used to determine the final 
label. If three labels are all different, it is labeled 
as Unknown. We extract 163 open questions and 
121 opinion questions from all four categories by 
using final question category labels mentioned in 
Section 4. To make meaningful comparison, 
questions having unique answers or having only 
one answer are excluded. After the removal, 
there are 104 open questions and 99 opinion 
questions left for comparison. The results are 
shown in Table 13.  
We are encouraged by the evaluation results 
that our automatic summarization methods gen-
erate better coverage of contents in most of the 
cases at every answer summary length. We ob-
serve a big difference between 100% and 150% 
answer summaries. It should not be a surprise 
since a 150% answer summary contains 50% 
more content than its corresponding BA. While 
at the 100% length, we still have about 30% 
ASAs better than BA. Questions which have bet-
ter ASA than BA usually have a long BA but 
with little information. Table 14 provides the 
example. By using summarization, answers that 
are compact and direct to the question can be 
included. The results indicate that summary 
compression technique might be helpful to pack 
more information in short answers. 
Open ASA Better BA Better Equal Unknown 
100% 30% 12% 45% 13% 
150% 55% 7% 28% 10% 
200% 63% 4% 24% 9% 
Opinion ASA Better BA Better Equal Unknown
100% 37% 20% 32% 11% 
150% 44% 16% 30% 10% 
200% 54% 16% 23% 7% 
Table 13. Evaluation by Information Content 
Q Why wont japanese characters burn onto the DVD? 
BA man, the answers here are too stupid for hteir own.You are 
creating a DVD on Western Platform. I take it, you are 
using an OS that is in English?In order to "view" japanese 
as part of your filenames, you need your operating system 
to accept Japanese coding (characters).If you are using 
Windows, then you will need ot isntall the Japanese cha-
racter Set for your operating system 
If you are using MacOS . i have no idea. 
100% 
ASA
 
The dvd writer 
Probably because your burner, the DVD writer, doesn't 
support double bytes code, such as Japanese, Korean, and 
Chinese. Check the supporting language of your software. 
Or change all the file name in single byte code, like alpha-
bets. man, the answers here are too stupid for hteir own. 
You are creating a DVD on Western Platform. I take it, 
you are using an OS that is in English? 
Table 14. Examples of 100% ASA 
Readability: Besides the information content, 
we would also like to study the readability of 
automatic summarized answers. 10 questions 
(each from open and opinion category) are ex-
tracted and we make both manual summarized 
answer (MSA) and automatic summarized an-
swer (ASA) for comparison with BA. We used 
the information content (INFO) and readability 
(READ) criteria for evaluation. The readability is 
judged basically by the time for understanding. 
We make two kinds of comparison: ASA vs. BA 
and MSA vs. BA. The first one is used to judge 
whether the current summarization method is 
better than current cQA scenario. The second one 
is used as an expectation for how much the 
summarization methods can be better than BA. 
503
 For ASA vs. BA, the results in Table 15 show 
that all the annotators agree ASAs providing 
more information content but not being with sa-
tisfying readability. For MSA vs. BA, better re-
sults in readability can be achieved as Table 16. 
This suggests that the proposed approach can 
succeed as more sophisticated summarization 
techniques are developed. 
Open Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 40% 10% 90% 10% 80% 0%
Equal 60% 60% 10% 80% 20% 60%
Worse 0% 30% 0% 10% 0% 40%
Opinion Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 90% 10% 90% 10% 70% 40%
Equal 10% 60% 10% 60% 10% 20%
Worse 0% 30% 0% 30% 20% 40%
Table 15. ASA vs. BA Evaluation 
Open Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 100% 30% 100% 90% 100% 90%
Equal 0% 50% 0% 0% 0% 0%
Worse 0% 20% 0% 10% 0% 10%
Opinion Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 90% 20% 60% 70% 100% 100%
Equal 10% 80% 40% 30% 0% 0%
Worse 0% 0% 0% 0% 0% 0%
Table 16. MSA vs. BA Evaluation 
6 Conclusion and Future Work 
In this paper, we have carried out a comprehen-
sive analysis of the question types in community-
based question answering (cQA) services and 
have developed taxonomies for questions and 
answers. We find that questions do not always 
have unique best answers. Open and opinion 
questions usually have multiple good answers. 
They occupied about 56%~83% and most of 
their best answers can be improved. By using 
question type as a guide, we propose applying 
automatic summarization techniques to summa-
rization answers or improving cQA best answers 
through answer editing. Our results show that 
customized question-type focused summarization 
techniques can improve cQA answer quality sig-
nificantly.  
Looking into the future, we are to develop au-
tomatic question type identification methods to 
fully automate answer summarization. Further-
more, we would also like to utilize more sophis-
ticated summarization techniques to improve 
content compaction and readability. 
Acknowledgements 
We thank the anonymous reviewers for their val-
uable suggestions and comments to this paper. 
References 
Broder  A. A taxonomy of web search. 2002. SIGIR 
Forum Vol.36, No. 2, 3-10. 
Hovy Edward, Laurie Gerber,  Ulf Hermjakob, Chin-
Yew Lin,   Deepak Ravichandran. 2001. Toward 
Semantics-Based Answer Pinpointing. In Proc. of 
HLT?01. 
Hovy E., C. Lin. 1999. Automated Text Summariza-
tion and the SUMMARIST System. In Advances 
in Automated Text Summarization 
Jeon J., W. B. Croft, and J. Lee. 2005a. Finding se-
mantically similar questions based on their an-
swers. In Proc. of SIGIR?05. 
Jeon J., W. B. Croft, and J. Lee. 2005b. Finding simi-
lar questions in large question and answer arc-
hives. In Proc. of CIKM?05. 
Jurczyk P., E. Agichtein. 2007. Hits on question an-
swer portals: exploration of link analysis for au-
thor ranking. In Proc. of SIGIR '07. 
Jeon J. , W.B. Croft, J. Lee, S. Park. 2006. A Frame-
work to predict the quality of answers with non-
textual features. In Proc. of SIGIR ?06. 
Jijkoun V., M. R. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Kleinberg J. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, vol. 46,  
Kim S., E.  Hovy. 2004. Determining the Sentiment of 
Opinions. In Proc. of COLING?04. 
Liu X., W.B. Croft, M. Koll. 2005. Finding experts in 
community-based question-answering services. 
In Proc. of CIKM '05. 
Lin C.Y., E. Hovy. 2002.  From single to multi-
document summarization: a prototype system and 
its evaluation.  In Proc. of ACL'02. 
Lytinen S., N. Tomuro. 2002. The Use of Question 
Types to Match Questions in FAQFinder. In Proc. 
of AAAI?02. 
Moldovan D., S. Harabagiu, et al 2000. The Structure 
and an Open-Domain Question Answering Sys-
tem. In Proc. of ACL?00. 
Mei Q., X. Shen, C. Zhai. 2007. Automatic labeling of 
multinomial topic models. In Proc. of   KDD'07. 
Rose  D. E., D. Levinson. 2004. Understanding user 
goals in web search. In Proc. of WWW '04. 
Voorhees, M. Ellen. 2003. Overview of the TREC 
2003 Question Answering Track. In Proc. of 
TREC?03. 
Wiebe J., E. Breck, et al 2003. Recognizing and Or-
ganizing Opinions Expressed in the World Press 
504
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 334?342, Prague, June 2007. c?2007 Association for Computational Linguistics
Low-Quality Product Review Detection in Opinion Summarization 
Jingjing Liu 
Nankai University 
Tianjin, China 
v-jingil@microsoft.com 
Yunbo Cao 
Microsoft Research Asia 
Beijing, China 
yucao@microsoft.com 
 
Chin-Yew Lin 
Microsoft Research Asia 
Beijing, China 
cyl@microsoft.com 
Yalou Huang 
Nankai University 
Tianjin, China 
huangyl@nankai.edu.cn 
Ming Zhou 
Microsoft Research Asia 
Beijing, China 
mingzhou@microsoft.com 
Abstract 
Product reviews posted at online shopping 
sites vary greatly in quality. This paper ad-
dresses the problem of detecting low-
quality product reviews. Three types of bi-
ases in the existing evaluation standard of 
product reviews are discovered. To assess 
the quality of product reviews, a set of spe-
cifications for judging the quality of re-
views is first defined. A classification-
based approach is proposed to detect the 
low-quality reviews. We apply the pro-
posed approach to enhance opinion sum-
marization in a two-stage framework. Ex-
perimental results show that the proposed 
approach effectively (1) discriminates low-
quality reviews from high-quality ones and 
(2) enhances the task of opinion summari-
zation by detecting and filtering low-
quality reviews. 
1 Introduction 
In the past few years, there has been an increasing 
interest in mining opinions from product reviews 
(Pang, et al 2002; Liu, et al 2004; Popescu and 
Etzioni, 2005). However, due to the lack of 
editorial and quality control, reviews on products 
vary greatly in quality. Thus, it is crucial to have a 
mechanism capable of assessing the quality of 
reviews and detecting low-quality/noisy reviews.  
Some shopping sites already provide a function 
of assessing the quality of reviews. For example, 
Amazon1 allows users to vote for the helpfulness 
of each review and then ranks the reviews based on 
the accumulated votes. However, according to our 
survey in Section 3, users? votes at Amazon have 
three kinds of biases as follows: (1) imbalance vote 
bias, (2) winner circle bias, and (3) early bird bias. 
Existing studies (Kim et al 2006; Zhang and Va-
radarajan, 2006) used these users? votes for train-
ing ranking models to assess the quality of reviews, 
which therefore are subject to these biases.  
In this paper, we demonstrate the aforemen-
tioned biases and define a standard specification to 
measure the quality of product reviews. We then 
manually annotate a set of ground-truth with real 
world product review data conforming to the speci-
fication.  
To automatically detect low-quality product re-
views, we propose a classification-based approach 
learned from the annotated ground-truth. The pro-
posed approach explores three aspects of product 
reviews, namely informativeness, readability, and 
subjectiveness.  
We apply the proposed approach to opinion 
summarization, a typical opinion mining task. The 
proposed approach enhances the existing work in a 
two-stage framework, where the low-quality re-
view detection is applied right before the summari-
zation stage.  
Experimental results show that the proposed ap-
proach can discriminate low-quality reviews from 
high-quality ones effectively. In addition, the task 
of opinion summarization can be enhanced by de-
tecting and filtering low-quality reviews. 
                                                 
1 http://www.amazon.com 
334
The rest of the paper is organized as follows: 
Section 2 introduces the related work. In Section 3, 
we define the quality of product reviews. In Sec-
tion 4, we present our approach to detecting low-
quality reviews. In Section 5, we empirically verify 
the effectiveness of the proposed approach and its 
use for opinion summarization. Section 6 summa-
rizes our work in this paper and points out the fu-
ture work. 
2 Related Work 
2.1 Evaluating Helpfulness of Reviews 
The problem of evaluating helpfulness of reviews 
(Kim et al 2006), also known as learning utility of 
reviews (Zhang and Varadarajan, 2006), is quite 
similar to our problem of assessing the quality of 
reviews.  
In practice, researchers in this area considered 
the problem as a ranking problem and solved it 
with regression models. In the process of model 
training and testing, they used the ground-truth 
derived from users? votes of helpfulness provided 
by Amazon. As we will show later in Section 3, 
these models all suffered from three types of vot-
ing bias.  
In our work, we avoid using users? votes by de-
veloping a specification on the quality of reviews 
and building a ground-truth according to the speci-
fication.  
2.2 Mining Opinions from Reviews 
One area of research on opinion mining from 
product reviews is to judge whether a review 
expresses a positive or a negative opinion. For 
example, Turney (2006) presented a simple 
unsupervised learning algorithm in judging 
reviews as ?thumbs up? (recommended) or 
?thumbs down? (not recommended). Pang et al
(2002) considered the same problem and presented 
a set of supervised machine learning approaches to 
it. For other work see also Dave et al (2003), Pang 
and Lee (2004, 2005). 
Another area of research on opinion mining is to 
extract and summarize users? opinions from prod-
uct reviews (Hu and Liu, 2004; Liu et al, 2005; 
Popescu and Etzioni, 2005). Typically, a sentence 
or a text segment in the reviews is treated as the 
basic unit. The polarity of users? sentiments on a 
product feature in each unit is extracted. Then the 
aggregation of the polarities of individual senti-
ments is presented to users so that they can have an 
at-a-glance view on how other experienced users 
rated on a certain product. The major weakness in 
the existing studies is that all the reviews, includ-
ing low-quality ones, are taken into consideration 
and treated equally for generating the summary. In 
this paper, we enhance the application by detecting 
and filtering low-quality reviews. In order to 
achieve that, we first define what the quality of 
reviews is. 
3 Quality of Product Reviews 
In this section, we will first show three biases of 
users? votes observed on Amazon, and then present 
our specification on the quality of product reviews. 
3.1 Amazon Ground-truth 
In our study, we use the product reviews on digital 
cameras crawled from Amazon as our data set. The 
data set consists of 23,141 reviews on 946 digital 
cameras. At the Amazon site, users could vote for 
a review with a ?helpful? or ?unhelpful? label. 
Thus, for each review there are two numbers 
indicating the statistics of these two labels, namely 
the number of ?helpful? votes and that of 
?unhelpful? ones. Kim et al(2006) used the 
percentage of ?helpful? votes as the measure of 
evaluating the ?quality of reviews? in their 
experiments. We call the ground-truth based on 
this measure as ?Amazon ground-truth?. 
Certainly, the ground-truth has the advantage of 
convenience. However, we identify three types of 
biases that make the Amazon ground-truth not al-
ways suitable for determining the quality of re-
views. We describe these biases in details in the 
rest of this section. 
3.1.1 Imbalance Vote Bias 
 
Figure 1. Reviews? percentage scores 
At the Amazon site, users tend to value others? 
opinions positively rather than negatively. From 
Figure 1, we can see that a half of the 23,141 
0
2000
4000
6000
8000
10000
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
# 
R
e
vi
e
w
s
Percentage of 'helpful' votes
335
reviews (corresponding to the two bars on the right 
of the figure) have more than 90% ?helpful? votes, 
including 9,100 reviews with 100% ?helpful? 
votes. From an in-depth investigation on these 
highly-voted reviews, we observed that some did 
not really have as good quality as the votes hint. 
For example, in Figure 2, the review about Canon 
PowerShot S500 receives 40 ?helpful? votes out of 
40 votes although it only gives very brief 
description on the product features in its second 
paragraph. We call this type of bias ?imbalance 
vote? bias. 
 
This is my second Canon digital elph camera. Both were great 
cameras. Recently upgraded to the S500. About 6 months later I get 
the dreaded E18 error. I searched the Internet and found numerous 
people having problems. When I determined the problem to be the 
lens not fully extending I decided to give it a tug. It clicked and the 
camera came on, ready to take pictures. Turning it off and on pro-
duced the E18 again. While turning it on I gave it a nice little bump 
on the side (where the USB connector is) and the lens popped out 
on its own. No problems since. 
 It?s a nice compact and light camera and takes great photos and 
videos. Only complaint (other than E18) is the limit of 30-second 
videos on 640x480 mode. I've got a 512MB compact flash card, I 
should be able to take as much footage as I have memory in one 
take. 
Figure 2. An example review 
3.1.2 Winner Circle Bias 
 
Figure 3. Votes of the top-50 ranked reviews 
There also exists a bootstrapping effect of ?hot? 
reviews at the Amazon site. Figure 3 shows the 
?helpful? votes for the top 50 ranked reviews. The 
numbers are averaged over 127 digital cameras 
which have no less than 50 reviews. As shown in 
this figure, the top two reviews hold more than 250 
and 140 votes respectively on average; while the 
numbers of votes held by lower-ranked reviews 
decrease exponentially. This is so-called the 
?winner circle? bias: the more votes a review 
gains, the more default authority it would appear to 
the readers, which in turn will influence the 
objectivity of the readers? votes. Also, the higher 
ranked reviews would attract more eyeballs and 
therefore gain more people?s votes. This mutual 
influence among labelers should be avoided when 
the votes are used as the evaluation standard. 
3.1.3 Early Bird Bias 
 
   
Figure 4. Dependency on publication date 
Publication date can influence the accumulation of 
users? votes. In Figure 4, the n?th publication date 
represents the n?th month after the product is 
released. The number in the figure is averaged over 
all the digital cameras in the data set. We can 
observe a clear trend that the earlier a review is 
posted, the more votes it will get. This is simply 
because reviews posted earlier are exposed to users 
for a longer time. Therefore, some high quality 
reviews may get fewer users? vote because of later 
publication. We call this ?early bird? bias. 
3.2 Specification of Quality 
Besides these aforementioned biases, using the raw 
rating from readers directly also fails to provide a 
clear guideline for what a good review consists of. 
In this section, we provide such a guideline, which 
we name as the specification (SPEC). 
In the SPEC, we define four categories of re-
view quality which represent different values of 
the reviews to users? purchase decision: ?best re-
view?, ?good review?, ?fair review?, and ?bad re-
view?. A generic description of the SPEC is as fol-
lows: 
A best review must be a rather complete and de-
tailed comment on a product. It presents several 
aspects of a product and provides convincing opi-
nions with enough evidence. Usually a best review 
could be taken as the main reference that users on-
ly need to read before making their purchase deci-
sion on a certain product. The first review in Fig-
ure 5 is a best review. It presents several product 
features and provides convincing opinions with 
sufficient evidence. It is also in a good format for 
readers to easily understand. Note that we omit 
some words in the example to save the space. 
0
50
100
150
200
250
300
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
#V
o
te
s 
h
e
ld
 b
y 
re
vi
e
w
s
Ranking positions of reviews
0
10
20
30
40
50
60
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
# 
V
o
te
s 
h
e
ld
 b
y 
re
vi
e
w
s
Publication Date
336
A good review is a relatively complete comment 
on a product, but not with as much supporting evi-
dence as necessary. It could be used as a strong 
and influential reference, but not as the only rec-
ommendation. The second review in Figure 5 is 
such an example. 
A fair review contains a very brief description 
on a product. It does not supply detailed evaluation 
on the product, but only comments on some as-
pects of the product. For example, the third review 
in Figure 5 mainly talks about ?the delay between 
pictures?, but less about other aspects of the cam-
era. 
A bad review is usually an incorrect description 
of a product with misleading information. It talks 
little about a specific product but much about some 
general topics (e.g. photography). For example, the 
last review in Figure 5 talks about the topic of ?ge-
neric battery?, but does not specify any digital 
camera. A bad review is an ?unhelpful? review that 
can be ignored.  
 
Best Review: 
I purchased this camera about six months ago after my Kodak 
Easyshare camera completely died on me. I did a little research 
and read only good things about this Canon camera so I decided to 
go with it because it was very reasonably priced (about $200). Not 
only did the camera live up to my expectations, it surpassed them 
by leaps and bounds! Here are the things I have loved about this 
camera: 
 
BATTERY - this camera has the best battery of any digital cam-
era I have ever owned or used. ? 
 
EASY TO USE - I was able to ? 
 
PICTURE QUALITY - all of the pictures I've taken and printed 
out have been great. ? 
 
FEATURES - I love the ability to quickly and easily ? 
 
LCD SCREEN - I was hoping ? 
 
SD MEMORY CARD - I was also looking for a camera that used 
SD memory cards. Mostly because? 
 
I cannot stress how highly I recommend this camera. I will never 
buy another digital camera besides Canon again. And the A610 (as 
well as the A620 - the 7.0MP version) is the best digital camera I've 
ever used. 
Good Review: 
The Sony DSC "P10" Digital Camera is the top pick for CSC. 
Running against cameras like Olympus stylus, Canon Powereshot, 
Sony V1, Nikon, Fuji, and More. The new release of 5.0 mega pix-
els has shot prices for digital cameras up to $1000+. This camera I 
purchased through a Private Dealer cost me $400.86. The Retail 
Price is Running $499.00 to $599.00. Purchase this camera from a 
wholesale dealer for the best price $377.00. Great Photo Even in 
dim light w/o a flash. The p10 is very compact. Can easily fit into 
any pocket. The camera can record 90 minutes of mpeg like a home 
movie. There are a lot of great digital cameras on the market that 
shoot good pictures and video. What makes the p10 the top pick is 
it comes with a rechargeable lithium battery. Many use AA batte-
ries, the digital camera consumes theses AA batteries in about two 
hours time while the unit is on. That can add continuous expense to 
the camera. It's also the best resolution on the market. 6.0 megapix 
is out, though only a few. And the smallest that we found. Also the 
best price for a major brand. 
Fair Review: 
There is nothing wrong with the 2100 except for the very notice-
able delay between pics. The camera's digital processor takes 
about 5 seconds after a photo is snapped to ready itself for the next 
one. Otherwise, the optics, the 3X optical zoom and the 2 megapixel 
resolution are fine for anything from Internet apps to 8" x 10" print 
enlarging. It is competent, not spectacular, but it gets the job done 
at an agreeable price point. 
Bad Review: 
I want to point out that you should never buy a generic battery, 
like the person from San Diego who reviewed the S410 on May 15, 
2004, was recommending. Yes you'd save money, but there have 
been many reports of generic batteries exploding when charged for 
too long. And don't think if your generic battery explodes you can 
sue somebody and win millions. These batteries are made in sweat-
shops in China, India and Korea, and I doubt you can find anybody 
to sue. So play it safe, both for your own sake and the camera's 
sake. If you want a spare, get a real Canon one. 
Figure 5. Example reviews 
3.3 Annotation of Quality 
According to the SPEC defined above, we built a 
ground-truth from the Amazon data set. We 
randomly selected 100 digital cameras and 50 
reviews for each camera. Totally we have 4,909 
reviews since some digital cameras have fewer 
than 50 unique reviews. Then we hired two 
annotators to label the reviews with the SPEC as 
their guideline. As the result, we have two 
independent copies of annotations on 4,909 
reviews, with the labels of ?best?, ?good?, ?fair?, 
and ?bad?. Table 1 shows the confusion matrix 
between the two copies of annotation. The value of 
the kappa statistic (Cohen, 1960) calculated from 
the matrix is 0.8142. This shows that the two 
annotators achieved highly consistent results by 
following the SPEC, although they worked 
independently.  
 Annota-
tion 1 
Annotation 2 
best good fair bad total 
best 294 44 2 0 340 
good 66 639 113 0 818 
fair 0 200 1,472 113 1,785 
bad 1 2 78 1,885 1,966 
total 361 885 1,665 1,998 4,909 
Table 1. Confusion matrix bet. the annotations 
 
In order to examine the difference between our 
annotations and Amazon ground-truth, we evaluate 
the Amazon ground-truth against the annotations, 
337
with the measure of ?error rate of preference pairs? 
(Herbrich et al 1999).  
????????? =
|????????? ?????????? ????? |
|??? ?????????? ?????|
 (1) 
where the ?preference pair? is defined as a pair of 
reviews with a order. For example, a best review 
and a good review correspond to a preference pair 
with the order of ?best review preferring to good 
review?. The ?all preference pairs? are collected 
from one of the annotations (the annotation 1 or 
the annotation 2) by ignoring the pairs from the 
same category. The ?incorrect preference pairs? 
are the preference pairs collected from the Ama-
zon ground-truth but not with the same order as 
that in the all preference pairs. The order of the 
preference pair collected from the Amazon 
ground-truth is evaluated on the basis of the per-
centage score as described in Section 3.1.  
The error rate of preference pairs based on the 
annotation 1 and that based on the annotation 2 are 
0.448 and 0.446, respectively, averaged over 100 
digital cameras. The high error rate of preference 
pairs demonstrates that the Amazon ground-truth 
diverges from the annotations (our ground-truth) 
significantly. 
To discover which kind of ground-truth is more 
reasonable, we ask an additional annotator (the 
third annotator) to compare these two kinds of 
ground-truth. More specifically, we randomly se-
lected 100 preference pairs whose orders the two 
kinds of ground-truth don?t agree on (called incor-
rect preference pairs in the evaluation above). As 
for our ground-truth, we choose the Annotation 1 
in the new test. Then, the third annotator is asked 
to assign a preference order for each selected pair. 
Note that the third annotator is blind to both our 
specification and the existing preference order.  
Last, we evaluate the two kinds of ground-truth 
with the new annotation. Among 100 pairs, our 
ground-truth agrees to the new annotation on 85 
pairs while the Amazon ground-truth agrees to the 
new annotation on 15 pairs. To confirm the result, 
yet another annotator (the fourth annotator) is 
called to repeat the same annotation independently 
as the third one. And we obtain the same statistical 
result (85 vs. 15) although the fourth annotator 
does not agree with the third annotator on some 
pairs. 
In practice, we treat the reviews in the first three 
categories (?best?, ?good? and ?fair?) as high-
quality reviews and those in the ?bad? category as 
low-quality reviews, since our goal is to identify 
low quality reviews that should not be considered 
when creating product review summaries. 
4 Classification of Product Reviews  
We employ a statistical machine learning approach 
to address the problem of detecting low-quality 
products reviews.  
Given a training data set? =  ?? ,?? 1
? , we 
construct a model that can minimize the error in 
prediction of y given x (generalization error). Here 
?? ? ?  and ?? = {???? ??????? , ??? ???????} 
represents a product review and a label, 
respectively. When applied to a new instance x, the 
model predicts the corresponding y and outputs the 
score of the prediction. 
4.1 The Learning Model 
In our study, we focus on differentiating low-
quality product reviews from high-quality ones. 
Thus, we treat the task as a binary classification 
problem.  
We employ SVM (Support Vector Machines) 
(Vapnik, 1995) as the model of classification. 
Given an instance x (product review), SVM assigns 
a score to it based on 
? ? = ??? + ? (2) 
where w denotes a vector of weights and b denotes 
an intercept. The higher the value of f(x) is, the 
higher the quality of the instance x is. In 
classification, the sign of f(x) is used. If it is 
positive, then x is classified into the positive 
category (high-quality reviews), otherwise into the 
negative category (low-quality reviews). 
The construction of SVM needs labeled training 
data (in our case, the categories are ?high-quality 
reviews? and ?low-quality reviews?). Briefly, the 
learning algorithm creates the ?hyper plane? in (2), 
such that the hyper plane separates the positive and 
negative instances in the training data with the 
largest ?margin?.  
4.2 Product Feature Resolution 
Product features (e.g., ?image quality? for digital 
camera) in a review are good indicators of review 
quality. However, different product features may 
refer to the same meaning (e.g., ?battery life? and 
?power?), which will bring redundancy in the 
study. In this paper, we formulize the problem as 
the ?resolution of product features?. Thus, the 
338
problem is reduced to how to determine the equi-
valence of a product feature in different forms.  
In (Hu and Liu, 2004), the matching of different 
product features is mentioned briefly and ad-
dressed by fuzzy matching. However, there exist 
many cases where the method fails to match the 
multiple mentions, e.g., ?battery life? and ?power?, 
because it only considers string similarity. In this 
paper we propose to resolve the problem by leve-
raging two kinds of evidence: one is ?surface string? 
evidence, the other is ?contextual evidence?.  
We use edit distance (Ukkonen, 1985) to com-
pare the similarity between the surface strings of 
two mentions, and use contextual similarity to re-
flect the semantic similarity between two mentions. 
When using contextual similarity, we split all 
the reviews into sentences. For each mention of a 
product feature, we take it as a query and search 
for all the relevant sentences. Then we construct a 
vector for the mention, by taking each unique term 
in the relevant sentences as a dimension of the vec-
tor. The cosine similarity between two vectors of 
mentions is then present to measure the contextual 
similarity between two mentions.  
4.3 Feature Development for Learning 
To detect low-quality reviews, our proposed 
approach explores three aspects of product reviews, 
namely informativeness, subjectiveness, and 
readability. We denote the features employed for 
learning as ?learning features?, discriminative from 
the ?product features? we discussed above. 
4.3.1 Features on Informativeness 
As for informativeness, the resolution of product 
features is employed when we generate the 
learning features as listed below. Pairs mapping to 
the same product feature will be treated as the 
same product feature, when we calculate the 
frequency and the number of product features. We 
apply the approach proposed in (Hu and Liu, 2004) 
to extract product features.  
We also use a list of product names and a list of 
brand names to generate the learning features. Both 
lists can be collected from the Amazon site be-
cause they are relatively stable within a time inter-
val. 
The learning features on the informativeness of 
a review are as follows. 
? Sentence level (SL) 
? The number of sentences in the review 
? The average length of sentences  
? The number of sentences with product features 
? Word level (WL) 
? The number of words in the review 
? The number of products (e.g., DMC-FZ50, 
EX-Z1000) in the review 
? The number of products in the title of a review  
? The number of brand names (e.g., Canon, Sony) 
in the review  
? The number of brand names in the title of a 
review 
? Product feature level (PFL) 
? The number of product features in the review 
? The total frequency of product features in the 
review 
? The average frequency of product features in 
the review 
? The number of product features in the title of a 
review 
?  The total frequency of product features in the 
title of a review 
4.3.2 Features on Readability 
We make use of several features at paragraph level 
which indicate the underlying structure of the 
reviews.  These features include, 
? The number of paragraphs in the review 
? The average length of paragraphs in the review 
? The number of paragraph separators in the re-
view 
Here, we refer to the keywords, such as ?Pros? 
vs. ?Cons? as ?paragraph separators?. The key-
words usually appear at the beginning of para-
graphs for categorizing two contrasting aspects of 
a product. We extract the nouns and noun phrases 
at the beginning of each paragraph from the 4,909 
reviews and use the most frequent 30 pairs of key-
words as paragraph separators. Table 2 provides 
some examples of the extracted separators. 
Separators Separators 
Positive Negative Positive Negative 
Pros Cons The Good The Bad 
Strength Weakness Thumb up Bummer 
PLUSES MINUSES Positive Negative 
Advantages Drawbacks Likes Dislikes 
The  upsides Downsides 
GOOD 
THINGS 
BAD 
THINGS 
Table 2. Examples of paragraph separators 
339
4.3.3 Features on Subjectiveness 
We also take the subjectiveness of reviews into 
consideration. Unlike previous work (Kim et al 
2006; Zhang and Varadarajan, 2006) using shallow 
syntactic information directly, we use a sentiment 
analysis tool (Hu and Liu, 2004) which aggregates 
a set of shallow syntactic information. The tool is a 
classifier capable of determining the sentiment 
polarity of each sentence. We create three learning 
features regarding the subjectiveness of reviews. 
? The percentage of positive sentences in the 
review 
? The percentage of negative sentences in the 
review 
? The percentage of subjective sentences (re-
gardless of positive or negative) in the review 
5 Experiments 
In this section, we describe our experiments with 
the proposed classification-based approach to low-
quality review detection, and its effectiveness on 
the task of opinion summarization. 
5.1 Detecting Low-quality Reviews 
In our proposed approach, the problem of assessing 
quality of reviews is formalized as a binary classi-
fication problem. We conduct experiments by tak-
ing reviews in the categories of ?best?, ?good?, and 
?fair? as high-quality reviews and those in the 
?bad? category as low-quality reviews.  
As for classification model, we utilize the 
SVMLight toolkit (Joachims, 2004). We randomly 
divide the 100 queries of digital cameras into two 
sets, namely a training set of 50 queries and a test 
set of 50 queries. For the two copies of annota-
tions, we use the same division. We use the train-
ing set from ?annotation 1? to train the model and 
apply the model to the test sets from both ?annota-
tion 1? and ?annotation 2?, respectively. Table 3 
reports the accuracies of our approach to review 
classification. The accuracy is defined as the per-
centage of correctly classified reviews. 
We take the approach that utilizes only the cate-
gory of features on sentence level (SL) as the base-
line, and incrementally add other categories of fea-
tures on informativeness, readability and subjec-
tiveness. We can see that both the features on word 
level (WL) and those on product feature level (PFL) 
can improve the performance of classification 
much. The features on readability can still increase 
the accuracy although the contribution is much 
less. The features on subjectiveness, however, 
make no contribution.   
 
Feature Category Annotation1 Annotation2 
Informative-
ness  
SL 73.59% 72.81% 
WL 80.41% 79.15% 
PFL 83.30% 82.37% 
Readability 83.93% 82.91% 
Subjectiveness 83.84% 82.96% 
Table 3. Low-quality reviews detection 
We also conduct a more detailed analysis on 
each individual feature. Two categories of features 
on ?title? and ?brand name? have poor perfor-
mance, which is due to the lack of information in 
the title and the low coverage of brand names in a 
review, respectively. 
5.2 Summarizing Sentiments of Reviews 
One potential application of low-quality review 
detection is the opinion summarization of reviews.  
The process of opinion summarization of re-
views with regards to a query of a product consists 
of the following steps (Liu et al 2005): 
1. From each of the reviews, identify every text 
segment with opinion in the review, and de-
termine the polarities of the opinion segments. 
2. For each product feature, generate a positive 
opinion set and a negative opinion set of opi-
nion segments, denoted as POS(?) 
and NOS(?). 
3. For each product feature, aggregate the num-
bers of segments in POS(?)  andNOS(?) , as 
opinion summarization on the product feature. 
In this process, all the reviews contribute the 
same. However, different reviews do hold different 
authorities. A positive/negative opinion from a 
high-quality review should not have the same 
weight as that from a low-quality review.  
We use a two-stage approach to enhance the re-
liability of summarization. That is, we add a 
process of low-quality review detection before the 
summarization process, so that the summarization 
result is obtained based on the high-quality reviews 
only. We are to demonstrate how much difference 
the proposed two-stage approach can bring into the 
opinion summarization. 
We use the best classification model trained as 
described in Section 5.1 to filter low-quality re-
views, and do summarization on the high-quality 
340
reviews associated to the 50 test queries. We de-
note the proposed approach and the old approach 
as ?two-stage? and ?one-stage?, respectively. Due 
to the limited space, we only give a visual compar-
ison of the two approaches on ?image quality? in 
Figure 6. The upper figure shows the summariza-
tion of positive opinions and the lower figure 
shows that of negative opinions. From the figures 
we can see that the two-stage approach preserves 
fewer text segments as the result of filtering out 
many low-quality product reviews. 
 
 
 
Figure 6. Summarization on ?image quality? 
To show the comparison on more features in a 
compressed space, we give the statistic ratio of 
change between two approaches instead. As for the 
evaluation measure, we define ?RatioOfChange? 
(ROC) on a feature f as, 
 
ROC ? =
Rateone?stage  ? ? Ratetwo?stage (?)
Rateone?stage (?)
 (3) 
 
where Rate *(f) is defined as, 
  
Rate?(?) =
|POS(?)|
|POS(?)| + |NOS(?)|
 (4) 
 
Table 4 shows some statistic results on ROC on 
five product features, namely ?image quality?(IQ), 
?battery?, ?LCD screen? (LCD), ?flash? and ?mov-
ie mode? (MM). The values in the cells are the 
percentage of queries whose ROC is larger/smaller 
than the respective thresholds. We can see that a 
large portion of queries have big changes on the 
values of ROC. This means that the result achieved 
by the two-stage approach is substantially different 
from that achieved by the one-stage approach. 
 
%Query 
RatioOfChange (+) 
>0.30 >0.25 >0.20 >0.15 >0.10 >0.05 
IQ 2% 4% 4% 10% 14% 22% 
Battery 10% 14% 18% 30% 38% 50% 
LCD  12% 18% 20% 22% 24% 28% 
Flash  6% 10% 16% 20% 26% 42% 
MM 6% 8% 8% 12% 18% 26% 
%Query 
RatioOfChange (-) 
<-0.30 <-0.25 <-0.20 <-0.15 <-0.10 <-0.05 
IQ 4% 6% 10% 14% 18% 44% 
Battery 2% 4% 4% 10% 14% 22% 
LCD  4% 4% 8% 12% 22% 28% 
Flash  4% 6% 8% 16% 18% 28% 
MM 8% 10% 16% 18% 34% 42% 
Table 4. RatioOfChange on five features 
There is no standard way to evaluate the quality 
of opinion summarization as it is rather a subjec-
tive problem. In order to demonstrate the impact of 
the two-stage approach, we turn to external author-
itative sources other than Amazon.com as the ob-
jective evaluation reference. We observe that 
CNET2 provides a professional ?editor?s review? 
for many products, which gives a rating in the 
range of 1~10 on product features. 9 digital cam-
eras out of the 50 test queries are found to have the 
editor?s rating on ?image quality? at CNET. We 
use this rating to compare with the results of our 
opinion summarization. We rescale the Rate scores 
obtained by both the one-stage approach and the 
two-stage approach into the range of 1-10 in order 
to perform the comparison.  
Figure 7 provides the visual comparison. We 
can see that the result achieved by the two-stage 
approach has a much better (closer) resemblance to 
CNET rating than one-stage approach does. This 
indicates that our two-stage approach can achieve a 
more consistent summarization result to the profes-
sional evaluations by the editors. Although the 
CNET rating is not the absolute standard for prod-
uct evaluation, it provides a professional yet objec-
tive evaluation of the products. Therefore, the ex-
perimental results demonstrate that our proposed 
approach could achieve more reliable opinion 
summarization which is closer to the generic eval-
uation from authoritative sources. 
 
                                                 
2 http://www.cnet.com 
0
30
60
90
120
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
N
u
m
b
e
r 
o
f 
su
p
p
o
rt
in
g 
se
n
te
n
ce
s 
(P
o
si
ti
ve
)
QueryID
One-stage Two-stage
0
20
40
60
80
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
N
u
m
b
e
r 
o
f 
su
p
p
o
rt
in
g 
se
n
te
n
ce
s 
(N
e
ga
ti
ve
)
QueryID
One-stage Two-stage
341
 Figure 7. Comparison with CNET rating 
6 Conclusion 
In this paper, we studied the problem of detecting 
low-quality product reviews. Our contribution can 
be summarized in two-fold: (1) we discovered 
three types of biases in the ground-truth used ex-
tensively in the existing work, and proposed a spe-
cification on the quality of product reviews. The 
three biases that we discovered are imbalance vote 
bias, winner circle bias, and early bird bias. (2) 
Rooting on the new ground-truth (conforming to 
the proposed specification), we proposed a classi-
fication-based approach to low-quality product 
review detection, which yields better performance 
of opinion summarization. 
We hope to explore our future work in several 
areas, such as further consolidating the new 
ground-truth from different points of view and ve-
rifying the effectiveness of low-quality review de-
tection with other applications. 
References 
Jacob Cohen. 1960. A coefficient of agreement for no-
minal scales, Educational and Psychological Mea-
surement 20: 37?46.  
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. 
WWW?03. 
Harris Drucker, Chris J.C., Burges Linda Kaufman, 
Alex Smola and Vladimir Vapnik. 1997. Support 
vector regression machines. Advances in Neural In-
formation Processing Systems.  
Christiane Fellbaum. 1998. WordNet: an Electronic 
Lexical Database, MIT Press. 
Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 
1999. Support Vector Learning for Ordinal Regres-
sion. In Proc. of the 9th International Conference on 
Artificial Neural Networks. 
Minqing Hu and Bing Liu. 2004a. Mining and Summa-
rizing Customer Reviews. KDD?04.  
Minqing Hu and Bing Liu. 2004b. Mining Opinion Fea-
tures in Customer Reviews. AAAI?04. 
Kalervo Jarvelin & Jaana Kekalainen. 2000. IR: evalua-
tion methods for retrieving highly relevant docu-
ments. SIGIR?00.  
Nitin Jindal and Bing Liu. 2006. Identifying Compara-
tive Sentences in Text Documents. SIGIR?06. 
Nitin Jindal and Bing Liu. 2006. Mining comparative 
sentences and relations. AAAI?06. 
Thorsten Joachims. SVMlight -- Support Vector Ma-
chine. http://svmlight.joachims.org/, 2004. 
Soo-Min Kim, Patrick Pantel, Tim Chklovski, Marco 
Pennacchiotti. 2006. Automatically Assessing Re-
view Helpfulness. EMNLP?06. 
Dekang Lin. 1998, Automatic retrieval and clustering of 
similar words. COLING-ACL?98. 
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. 
Opinion observer: analyzing and comparing opinions 
on the web. WWW ?05.  
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summari-
zation based on minimum cuts. ACL?04. 
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting 
class relationships for sentiment categorization with 
respect to rating scales. ACL?05. 
Bo Pang and Lillian Lee, and S. Vaithyanathan. 2002. 
Thumbs up? sentiment classification using machine 
learning techniques. EMNLP?02.  
Ana-Maria Popescu and O Etzioni. 2005. Extracting 
product    features and opinions from reviews. HLT-
EMNLP?05.  
Peter D. Turney. 2001. Thumbs up or thumbs down?: 
semantic orientation applied to unsupervised classifi-
cation of reviews. ACL?02  
Esko Ukkonen. 1985. Algorithms for approximate string 
matching. Information and Control, pp. 100 ? 118. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer. 
Zhu Zhang and Balaji Varadarajan. 2006. Utility Scor-
ing of Product Reviews. CIKM?06 
 
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
R
at
in
g 
Sc
o
re
QueryID
One-stage
Two-stage
CNET Ground-truth
342
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 167?176,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Better Binarization for the CKY Parsing
Xinying Song? ? Shilin Ding? ? Chin-Yew Lin?
?MOE-MS Key Laboratory of NLP and Speech, Harbin Institute of Technology, Harbin, China
?Department of Statistics, University of Wisconsin-Madison, Madison, USA
?Microsoft Research Asia, Beijing, China
xysong@mtlab.hit.edu.cn dingsl@gmail.com cyl@microsoft.com
Abstract
We present a study on how grammar binariza-
tion empirically affects the efficiency of the
CKY parsing. We argue that binarizations af-
fect parsing efficiency primarily by affecting
the number of incomplete constituents gener-
ated, and the effectiveness of binarization also
depends on the nature of the input. We pro-
pose a novel binarization method utilizing rich
information learnt from training corpus. Ex-
perimental results not only show that differ-
ent binarizations have great impacts on pars-
ing efficiency, but also confirm that our learnt
binarization outperforms other existing meth-
ods. Furthermore we show that it is feasible to
combine existing parsing speed-up techniques
with our binarization to achieve even better
performance.
1 Introduction
Binarization, which transforms an n-ary grammar
into an equivalent binary grammar, is essential for
achieving an O(n3) time complexity in the context-
free grammar parsing. O(n3) tabular parsing al-
gorithms, such as the CKY algorithm (Kasami,
1965; Younger, 1967), the GHR parser (Graham
et al, 1980), the Earley algorithm (Earley, 1970) and
the chart parsing algorithm (Kay, 1980; Klein and
Manning, 2001) all convert their grammars into bi-
nary branching forms, either explicitly or implicitly
(Charniak et al, 1998).
In fact, the number of all possible binarizations
of a production with n + 1 symbols on its right
?This work was done when Xinying Song and Shilin Ding
were visiting students at Microsoft Research Asia.
hand side is known to be the nth Catalan Number
Cn = 1n+1
(2n
n
). All binarizations lead to the same
parsing accuracy, but maybe different parsing effi-
ciency, i.e. parsing speed. We are interested in in-
vestigating whether and how binarizations will af-
fect the efficiency of the CKY parsing.
Do different binarizations lead to different pars-
ing efficiency? Figure 1 gives an example to help
answer this question. Figure 1(a) illustrates the cor-
rect parse of the phrase ?get the bag and go?. We
assume that NP ? NP CC NP is in the original
grammar. The symbols enclosed in square brackets
in the figure are intermediate symbols.
VP
VP VPVB NP CC VBget DT NN and gothe bag
(a) final parse
VP
[NP CC]VP VPVB NP CC VBget DT NN and gobag
[VP CC] NP?
NP?
the
(b) with left
VP
VP VPVB NP CC VBget DT NN and gobag
[CC VP]
the
(c) with right
Figure 1: Parsing with left and right binarization.
If a left binarized grammar is used, see Fig-
ure 1(b), an extra constituent [NP CC] spanning
?the bag and? will be produced. Because rule
[NP CC] ? NP CC is in the left binarized gram-
mar and there is an NP over ?the bag? and a CC
over the right adjacent ?and?. Having this con-
stituent is unnecessary, because it lacks an NP to
the right to complete the production. However, if a
right binarization is used, as shown in Figure 1(c),
such unnecessary constituent can be avoided.
One observation from this example is that differ-
ent binarizations affect constituent generation, thus
affect parsing efficiency. Another observation is that
167
for rules like X ? Y CC Y , it is more suitable to
binarize them in a right branching way. This can
be seen as a linguistic nature: for ?and?, usually
the right neighbouring word can indicate the correct
parse. A good binarization should reflect such ligu-
istic nature.
In this paper, we aim to study the effect of bina-
rization on the efficiency of the CKY parsing. To our
knowledge, this is the first work on this problem.
We propose the problem to find the optimal bina-
rization in terms of parsing efficiency (Section 3).
We argue that binarizations affect parsing efficiency
primarily by affecting the number of incomplete
constituents generated, and the effectiveness of bi-
narization also depends on the nature of the input
(Section 4). Therefore we propose a novel binariza-
tion method utilizing rich information learnt from
training corpus (Section 5). Experimental results
show that our binarization outperforms other exist-
ing methods (Section 7.2).
Since binarization is usually a preprocessing step
before parsing, we argue that better performance can
be achieved by combining other parsing speed-up
techniques with our binarization (Section 6). We
conduct experiments to confirm this (Section 7.3).
2 Binarization
In this paper we assume that the original gram-
mar, perhaps after preprocessing, contains no ?-
productions or useless symbols. However, we allow
the existence of unary productions, since we adopt
an extended version of the CKY algorithm which
can handle the unary productions. Moreover we do
not distinguish nonterminals and terminals explic-
itly. We treat them as symbols. What we focus on is
the procedure of binarization.
Definition 1. A binarization is a function pi, map-
ping an n-ary grammar G to an equivalent binary
grammar G?. We say that G? is a binarized grammar
of G, denoted as pi(G).
Two grammars are equivalent if they define the
same probability distribution over strings (Charniak
et al, 1998).
We use the most widely used left binarization
(Aho and Ullman, 1972) to show the procedure of
binarization, as illustrated in Table 1, where p and q
are the probabilities of the productions.
Original grammar Left binarized grammar
Y ? ABC : p [AB] ? AB : 1.0
Z ? ABD : q Y ? [AB]C : p
Z ? [AB]D : q
Table 1: Left binarization
In the binarized grammar, symbols of form [AB]
are new (also called intermediate) nonterminals.
Left binarization always selects the left most pair of
symbols and combines them to form an intermedi-
ate nonterminal. This procedure is repeated until all
productions are binary.
In this paper, we assume that all binarizations fol-
low the fashion above, except that the choice of pair
of symbols for combination can be arbitrary. Next
we show three other known binarizations.
Right binarization is almost the same with left
binarization, except that it always selects the right
most pair, instead of left, to combine.
Head binarization always binarizes from the head
outward (Klein and Manning, 2003b). Please refer
to Charniak et al (2006) for more details.
Compact binarization (Schmid, 2004) tries to
minimize the size of the binarized grammar. It leads
to a compact grammar. We therefore call it compact
binarization. It is done via a greedy approach: it al-
ways selects the pair that occurs most on the right
hand sides of rules to combine.
3 The optimal binarization
The optimal binarization should help CKY parsing
to achieve its best efficiency. We formalize the idea
as follows:
Definition 2. The optimal binarization is pi?, for a
given n-ary grammar G and a test corpus C:
pi? = argminpi T (pi(G), C) (1)
where T (pi(G), C) is the running time for CKY to
parse corpus C, using the binarized grammar pi(G).
It is hard to find the optimal binarization directly
from Definition 2. We next give an empirical anal-
ysis of the running time of the CKY algorithm and
simplify the problem by introducing assumptions.
3.1 Analysis of CKY parsing efficiency
It is known that the complexity of the CKY algo-
rithm is O(n3L). The constant L depends on the bi-
168
narized grammar in use. Therefore binarization will
affect L. Our goal is to find a good binarization that
makes parsing more efficient.
It is also known that in the inner most loop of
CKY as shown in Algorithm 1, the for-statement in
Line 1 can be implemented in several different meth-
ods. The choice will affect the efficiency of CKY.
We present here four possible methods:
M1 Enumerate all rules X ? Y Z, and check if Y is in
left span and Z in right span.
M2 For each Y in left span, enumerate all rules X ?
Y Z, and check if Z is in right span.
M3 For each Z in right span, enumerate all rules X ?
Y Z, and check if Y is in left span.
M4 Enumerate each Y in left span and Z in right span1,
check if there are any rules X ? Y Z.
Algorithm 1 The inner most loop of CKY
1: for X ? Y Z, Y in left span and Z in right span
2: Add X to parent span
3.2 Model assumption
We have shown that both binarization and the for-
statement implementation in the inner most loop of
CKY will affect the parsing speed.
About the for-statement implementations, no pre-
vious study has addressed which one is superior.
The actual choice may affect our study on binariza-
tion. If using M1, since it enumerates all rules in
the grammar, the optimal binarization will be the
one with minimal number of rules, i.e. minimal bi-
narized grammar size. However, M1 is usually not
preferred in practice (Goodman, 1997). For other
methods, it is hard to tell which binarization is op-
timal theoretically. In this paper, for simplicity rea-
sons we do not consider the effect of for-statement
implementations on the optimal binarization.
On the other hand, it is well known that reduc-
ing the number of constituents produced in parsing
can greatly improve CKY parsing efficiency. That
is how most thresholding systems (Goodman, 1997;
Tsuruoka and Tsujii, 2004; Charniak et al, 2006)
speed up CKY parsing. Apparently, the number of
1Note that we should skip Y (Z) if it never appears as the
first (second) symbol on the right hand side of any rule.
constituents produced in parsing is not affected by
for-statement implementations.
Therefore we assume that the running time of
CKY is primarily determined by the number of con-
stituents generated in parsing. We simplify the opti-
mal binarization to be:
pi? ? argminpi E(pi(G), C) (2)
where E(pi(G), C) is the number of constituents
generated when CKY parsing C with pi(G).
We next discuss how binarizations affect the num-
ber of constituents generated in parsing, and present
our algorithm for finding a good binarization.
4 How binarizations affect constituents
Throughout this section and the next, we will use an
example to help illustrate the idea. The grammar is:
X ? A B C D
Y ? A B C
C ? C D
Z ? A B C E
W ? F C D E
The input sentence is 0A1B2C3D4E5, where the
subscripts are used to indicate the positions of spans.
For example, [1, 3] stands for BC. The final parse2
is shown in Figure 2. Symbols surrounded by dashed
circles are fictitious, which do not actually exist in
the parse.
F
B:[1,2]A:[0,1] C:[2,3] D:[3,4] E:[4,5]
WY:[0,3] X:[0,4] C:[2,4]
Y:[0,4] Z:[0,5]
Figure 2: Parse of the sentence ABC DE
4.1 Complete and incomplete constituents
In the procedure of CKY parsing, there are two kinds
of constituents generated: complete and incomplete.
Complete constituents (henceforth CCs) are those
composed by the original grammar symbols and
2More precisely, it is more than a parse tree for it contains
all symbols recognized in parsing.
169
spans. For example in Figure 2, X : [0, 4], Y : [0, 3]
and Y :[0, 4] are all CCs.
Incomplete constituents (henceforth ICs) are
those labeled by intermediate symbols. Figure 2
does not show them directly, but we can still read the
possible ones. For example, if the binarized gram-
mar in use contains an intermediate symbol [ABC],
then there will be two related ICs [ABC]:[0, 3] and
[ABC]:[0, 4] (the latter is due to C:[2, 4]) produced
in parsing. ICs represent the intermediate steps to
recognize and complete CCs.
4.2 Impact on complete constituents
Binarizations do not affect whether a CC will be pro-
duced. If there is a CC in the parse, whatever bi-
narization we use, it will be produced. The differ-
ence merely lies on what intermediate ICs are used.
Therefore given a grammar and an input sentence,
no matter what binarization is used, the CKY pars-
ing will generate the same set of CCs.
For example in Figure 2 there is a CC X : [0, 4],
which is associated with rule X ? ABC D. No
matter what binarization we use, this CC will be rec-
ognized eventually. For example if using left bina-
rization, we will get [AB]:[0, 2], [ABC]:[0, 3] and
finally X :[0, 4]; if using right binarization, we will
get [C D]:[2, 4], [BC D]:[1, 4] and again X:[0, 4].
4.3 Impact on incomplete constituents
Binarizations do affect the generation of ICs, be-
cause they generate different intermediate symbols.
We discuss the impact on two aspects:
Shared IC. Some ICs can be used to generate
multiple CCs in parsing. We call them shared. If a
binarization can lead to more shared ICs, then over-
all there will be fewer ICs needed in parsing.
For example, in Figure 2, if we use left binariza-
tion, then [AB]:[0, 2] can be shared to generate both
X :[0, 4] and Y :[0, 3], in which we can save one IC
overall. However, if right binarization is used, there
will be no common ICs to share in the generation
steps of X :[0, 4] and Y :[0, 3], and overall there are
one more IC generated.
Failed IC. For a CC, if it can be recognized even-
tually by applying an original rule of length k, what-
ever binarization to use, we will have to generate the
same number of k ? 2 ICs before we can complete
the CC. However, if the CC cannot be fully recog-
nized but only partially recognized, then the number
of ICs needed will be quite different.
For example, in Figure 2, the rule W ? F C DE
can be only partially recognized over [2, 5], so it can-
not generate the corresponding CC. Right binariza-
tion needs two ICs ([DE]:[3, 5] and [C DE]:[2, 5])
to find that the CC cannot be recognized, while left
binarization needs none.
As mentioned earlier, ICs are auxiliary means to
generate CCs. If an IC cannot help generate any
CCs, it is totally useless and even harmful. We call
such an IC failed, otherwise it is successful. There-
fore, if a binarization can help generate fewer failed
ICs then parsing would be more efficient.
4.4 Binarization and the nature of the input
Now we show that the impact of binarization also
depends on the actual input. When the input
changes, the impact may also change.
For example, in the previous example about the
rule W ? F C DE in Figure 2, we believe that
left binarization is better based on the observation
that there are more snippets of [C DE] in the in-
put which lack for F to the left. If there are more
snippets of [F C D] in the input lacking for E to the
right, then right binarization would be better.
The discussion above confirms such a view: the
effect of binarization depends on the nature of the
input language, and a good binarization should re-
flect this nature. This accords with our intuition. So
we use training corpus to learn a good binarization.
And we verify the effectiveness of the learnt bina-
rization using a test corpus with the same nature.
In summary, binarizations affect the efficiency of
parsing primarily by affecting the number of ICs
generated, where more shared and fewer failed ICs
will help lead to higher efficiency. Meanwhile, the
effectiveness of binarization also depends on the na-
ture of its input language.
5 Towards a good binarization
Based on the analysis in the previous section, we
employ a greedy approach to find a good binariza-
tion. We use training corpus to compute metrics
for every possible intermediate symbol. We use this
information to greedily select the best pair to com-
bine.
170
5.1 Algorithm
Given the original grammar G and training corpus
C, for every sentence in C, we firstly obtain the final
parse (like Figure 2). For every possible intermedi-
ate symbol, i.e. every ngram of the original symbols,
denoted by w, we compute the following two met-
rics:
1. How many ICs labeled by w can be generated
in the final parse, denoted by num(w) (number
of related ICs).
2. How many CCs can be generated via ICs la-
beled by w, denoted by ctr(w) (contribution of
related ICs).
For example in Figure 2, for a possible inter-
mediate symbol [ABC], there are two related ICs
([ABC] : [0, 3] and [ABC] : [0, 4]) in the parse,
so we have num([ABC]) = 2. Meanwhile, four
CCs (Y : [0, 3], X : [0, 4], Y : [0, 4] and Z : [0, 5]) can
be generated from the two related ICs. Therefore
ctr([ABC]) = 4. We list the two metrics for every
ngram in Figure 2 in Table 2. We will discuss how
to compute these two metrics in Section 5.2.
w num ctr w num ctr
[AB] 1 4 [BC E] 1 1
[ABC] 2 4 [C D] 1 2
[ABC D] 1 1 [C DE] 1 0
[ABC E] 1 1 [C E] 1 1
[BC] 2 4 [DE] 1 0
[BC D] 1 1
Table 2: Metrics of every ngram
The two metrics indicate the goodness of a possi-
ble intermediate symbol w: num(w) indicates how
many ICs labeled by w are likely to be generated in
parsing; while ctr(w) represents how much w can
contribute to the generation of CCs. If ctr(w) is
larger, the corresponding ICs are more likely to be
shared. If ctr is zero, those ICs are surely failed.
Therefore the smaller num(w) is and the larger
ctr(w) is, the better w would be.
Combining num and ctr, we define a utility func-
tion for each ngram w in the original grammar:
utility(w) = f(num(w), ctr(w)) (3)
where f is a ranking function, satisfying that f(x, y)
is larger when x is smaller and y is larger. We will
discuss more details about it in Section 5.3.
Using utility as the ranking function, we sort all
pairs of symbols and choose the best to combine.
The formal algorithm is as follows:
S1 For every symbol pair of ?v1, v2? (where v1 and
v2 can be original symbols or intermediate symbols
generated in previous rounds), let w1 and w2 be the
ngrams of original symbols represented by v1 and
v2, respectively. Let w = w1w2 be the ngram rep-
resented by the symbol pair. Compute utility(w).
S2 Select the ngram w with the highest utility(w), let
it be w? (in case of a tie, select the one with a
smaller num). Let the corresponding symbol pair
be ?v?1 , v?2?.
S3 Add a new intermediate symbol v?, and replace all
the occurrences of ?v?1 , v?2? on the right hand sides
of rules with v?.
S4 Add a new rule v? ? v?1v?2 : 1.0.
S5 Repeat S1 ? S4, until there are no rules with more
than two symbols on the right hand side.
5.2 Metrics computing
In this section, we discuss how to compute num and
ctr in details.
Computing ctr is straightforward. First we get
final parses like in Figure 2 for training sentences.
From a final parse, we traverse along every parent
node and enumerate every subsequence of its child
nodes. For example in Figure 2, from the parent
node of X : [0, 4], we can enumerate the follow-
ing: [AB] : [0, 2], [ABC] : [0, 3], [ABC D] : [0, 4],
[BC]:[1, 3], [BC D]:[1, 4], [C D]:[2, 4]. We add 1 to
all the ctr of these ngrams, respectively.
To compute num, we resort to the same idea
of dynamic programming as in CKY. We perform
a normal left binarization except that we add all
ngrams in the original grammar G as intermediate
symbols into the binarized grammar G?. For exam-
ple, for the rule of S ? ABC : p, the constructed
grammar is as follows:
[AB] ? A B : 1.0
S ? [AB] C : p
[BC] ? B C : 1.0
Using the constructed G?, we employ a normal
CKY parsing on the training corpus and compute
171
how many constituents are produced for each ngram.
The result is num. Suppose the length of the train-
ing sentence is n, the original grammar G has N
symbols, and the maximum length of rules is k,
then the complexity of this method can be written
as O(Nkn3).
5.3 Ranking function
We discuss the details of the ranking function f used
to compute the utility of each ngram w. We come
up with two forms for f : linear and log-linear
1. linear: f(x, y) = ??1x+ ?2y
2. log-linear3: f(x, y) = ??1 log(x) + ?2 log(y)
where ?1 and ?2 are non-negative weights subject to
?1 + ?2 = 14.
We will use development set to determine which
form is better and to learn the best weight settings.
6 Combination with other techniques
Binarization usually plays a role of preprocessing in
the procedure of parsing. Grammars are binarized
before they are fed into the stage of parsing. There
are many known works on speeding up the CKY
parsing. So we can expect that if we replace the
part of binarization by a better one while keeping
the subsequent parsing unchanged, the parsing will
be more efficient. We will conduct experiment to
confirm this idea in the next section.
We would like to make more discussions be-
fore we advance to the experiments. The first is
about parsing accuracy in combining binarization
with other parsing speed-up techniques. Binariza-
tion itself does not affect parsing accuracy. When
combined with exact inference algorithms, like the
iterative CKY (Tsuruoka and Tsujii, 2004), the ac-
curacy will be the same. However, if combined with
other inexact pruning techniques like beam-pruning
(Goodman, 1997) or coarse-to-fine parsing (Char-
niak et al, 2006), binarization may interact with
those pruning methods in a complicated way to af-
fect parsing accuracy. This is due to different bina-
rizations generate different sets of intermediate sym-
3For log-linear form, if num(w) = 0 (and consequently
ctr(w) = 0), we set f(num(w), ctr(w)) = 0; if num(w) >
0 but ctr(w) = 0, we set f(num(w), ctr(w)) = ??.
4Since f is used for ranking, the magnitude is not important.
bols. With the same complete constituents, one bi-
narization might derive incomplete constitutes that
could be pruned while another binarization may not.
This would affect the accuracy. We do not address
this interaction on in this paper, but leave it to the
future work. In Section 7.3 we will use the iterative
CKY for testing.
In addition, we believe there exist some speed-up
techniques which are incompatible with our bina-
rization. One such example may be the top-down
left-corner filtering (Graham et al, 1980; Moore,
2000), which seems to be only applicable to the pro-
cess of left binarization. A detailed investigation on
this problem will be left to the future work.
The last issue is how our binarization performs
on a lexicalized parser, like Collins (1997). Our in-
tuition is that we cannot apply our binarization to
Collins (1997). The key fact in lexicalized parsers
is that we cannot explicitly write down all rules
and compute their probabilities precisely, due to the
great number of rules and the severe data sparsity
problem. Therefore in Collins (1997) grammar rules
are already factorized into a set of probabilities.
In order to capture the dependency relationship be-
tween lexcial heads Collins (1997) breaks down the
rules from head outwards, which prevents us from
factorizing them in other ways. Therefore our bina-
rization cannot apply to the lexicalized parser. How-
ever, there are state-of-the-art unlexicalized parsers
(Klein and Manning, 2003b; Petrov et al, 2006), to
which we believe our binarization can be applied.
7 Experiments
We conducted two experiments on Penn Treebank II
corpus (Marcus et al, 1994). The first is to com-
pare the effects of different binarizations on parsing
and the second is to test the feasibility to combine
our work with iterative CKY parsing (Tsuruoka and
Tsujii, 2004) to achieve even better efficiency.
7.1 Experimental setup
Following conventions, we learnt the grammar from
Wall Street Journal (WSJ) section 2 to 21 and mod-
ified it by discarding all functional tags and empty
nodes. The parser obtained this way is a pure un-
lexicalized context-free parser with the raw treebank
grammar. Its accuracy turns out to be 72.46% in
172
terms of F1 measure, quite the same as 72.62% as
stated in Klein and Manning (2003b). We adopt this
parser in our experiment not only because of sim-
plicity but also because we focus on parsing effi-
ciency.
For all sentences with no more than 40 words in
section 22, we use the first 10% as the development
set, and the last 90% as the test set. There are 158
and 1,420 sentences in development set and test set,
respectively. We use the whole 2,416 sentences in
section 23 as the training set.
We use the development set to determine the bet-
ter form of the ranking function f as well as to
tune its weights. Both metrics of num and ctr
are normalized before use. Since there is only one
free variable in ?1 and ?2, we can just enumerate
0 ? ?1 ? 1, and set ?2 = 1 ? ?1. The increasing
step is firstly set to 0.05 for the approximate loca-
tion of the optimal weight, then set to 0.001 to learn
more precisely around the optimal.
We find that the optimal is 5,773,088 (constituents
produced in parsing development set) with ?1 =
0.014 for linear form, while for log-linear form the
optimal is 5,905,292 with ?1 = 0.691. Therefore we
determine that the better form for the ranking func-
tion is linear with ?1 = 0.014 and ?2 = 0.986.
The size of each binarized grammar used in the
experiment is shown in Table 3. ?Original? refers
to the raw treebank grammar. ?Ours? refers to the
learnt binarized grammar by our approach. For the
rest please refer to Section 2.
# of Symbols # of Rules
Original 72 14,971
Right 10,654 25,553
Left 12,944 27,843
Head 11,798 26,697
Compact 3,644 18,543
Ours 8,407 23,306
Table 3: Grammar size of different binarizations
We also tested whether the size of the training set
would have significant effect. We use the first 10%,
20%, ? ? ? , up to 100% of section 23 as the training
set, respectively, and parse the development set. We
find that all sizes examined have a similar impact,
since the numbers of constituents produced are all
around 5,780,000. It means the training corpus does
not have to be very large.
The entire experiments are conducted on a server
with an Intel Xeon 2.33 GHz processor and 8 GB
memory.
7.2 Experiment 1: compare among
binarizations
In this part, we use CKY to parse the entire test set
and evaluate the efficiency of different binarizations.
The for-statement implementation of the inner
most loop of CKY will affect the parsing time
though it won?t affect the number of constituents
produced as discussed in Section 3.2. The best im-
plementations may be different for different bina-
rized grammars. We examine M1?M4, testing their
parsing time on the development set. Results show
that for right binarization the best method is M3,
while for the rest the best is M2. We use the best
method for each binarized grammar when compar-
ing the parsing time in Experiment 1.
Table 4 reports the total number of constituents
and total time required for parsing the entire test set.
It shows that different binarizations have great im-
pacts on the efficiency of CKY. With our binariza-
tion, the number of constituents produced is nearly
20% of that required by right binarization and nearly
25% of that by the widely-used left binarization. As
for the parsing time, CKY with our binarization is
about 2.5 times as fast as with right binarization and
about 1.75 times as fast as with left binarization.
This illustrates that our binarization can significantly
improve the efficiency of the CKY parsing.
Binarization Constituents Time (s)
Right 241,924,229 5,747
Left 193,238,759 3,474
Head 166,425,179 3,837
Compact 94,257,478 2,302
Ours 52,206,466 2,182
Table 4: Performance on test set
Figure 3 reports the detailed number of complete
constituents, successful incomplete constituents and
failed incomplete constituents produced in parsing.
The result proves that our binarization can signifi-
cantly reduce the number of failed incomplete con-
stituents, by a factor of 10 in contrast with left bi-
narization. Meanwhile, the number of successful in-
173
complete constituents is also reduced by a factor of
2 compared to left binarization.
0.0e+00
2.0e+07
4.0e+07
6.0e+07
8.0e+07
1.0e+08
1.2e+08
1.4e+08
1.6e+08
1.8e+08
Right Left Head Compact Ours
# 
of 
Co
ns
titu
en
ts
Binarizations
complete
successful incomplete
failed incomplete
Figure 3: Comparison on various constituents
Another interesting observation is that parsing
with a smaller grammar does not always yield a
higher efficiency. Our binarized grammar is more
than twice the size of compact binarization, but ours
is more efficient. It proves that parsing efficiency is
related to both the size of grammar in use as well as
the number of constituents produced.
In Section 1, we used an example of ?get the
bag and go? to illustrate that for rules like X ?
Y CC Y , right binarization is more suitable. We
also investigated the corresponding linguistic nature
that the word to the right of ?and? is more likely to
indicate the true relationship represented by ?and?.
We argued that a better binarization can reflect such
linguistic nature of the input language. To our sur-
prise, our learnt binarization indeed captures this lin-
guistic insight, by binarizing NP ? NP CC NP
from right to left.
Finally, we would like to acknowledge the limi-
tation of our assumption made in Section 3.2. Ta-
ble 4 shows that the parsing time of CKY is not
always monotonic increasing with the number of
constituents produced. Head binarization produces
fewer constituents than left binarization but con-
sumes more parsing time.
7.3 Experiment 2: combine with iterative CKY
In this part, we test the performance of combining
our binarization with the iterative CKY (Tsuruoka
and Tsujii, 2004) (henceforth T&T) algorithm.
Iterative CKY is a procedure of multiple passes
of normal CKY: in each pass, it uses a threshold to
prune bad constituents; if it cannot find a successful
parse in one pass, it will relax the threshold and start
another; this procedure is repeated until a successful
parse is returned. T&T used left binarization. We
re-implement their experiments and combine itera-
tive CKY with our binarization. Note that iterative
CKY is an exact inference algorithm that guarantees
to return the optimal parse. As discussed in Sec-
tion 6, the parsing accuracy is not changed in this
experiment.
T&T used a held-out set to learn the best step of
threshold decrease. They reported that the best step
was 11 (in log-probability). We found that the best
step was indeed 11 for left binarization; for our bina-
rizaiton, the best step was 17. T&T used M4 as the
for-statement implementation of CKY. In this part,
we follow the same method.
The result is shown in Table 5. We can see that
iterative CKY can achieve better performance by us-
ing a better binarization. We also see that the reduc-
tion by binarization with pruning is less significant
than without pruning. It seems that the pruning itself
in iterative CKY can counteract the reduction effect
of binarization to some extent. Still the best per-
formance is archieved by combining iterative CKY
with a better binarization.
CKY + Binarization Constituents Time (s)
Tsuruoka and Tsujii (2004)
CKY + Left 45,406,084 1,164
Iterative CKY + Left 17,520,427 613
Reimplement
CKY + Left 52,128,941 932
CKY + Ours 14,892,203 571
Iterative CKY + Left 23,267,594 377
Iterative CKY + Ours 10,966,272 314
Table 5: Combining with iterative CKY parsing
8 Related work
Almost all work on parsing starts from a binarized
grammar. Usually binarization plays a role of pre-
processing. Left binarization is widely used (Aho
and Ullman, 1972; Charniak et al, 1998; Tsuruoka
and Tsujii, 2004) while right binarization is rarely
used in the literature. Compact binarization was in-
troduced in Schmid (2004), based on the intuition
that a more compact grammar will help acheive a
highly efficient CKY parser, though from our exper-
iment it is not always true.
174
We define the fashion of binarizations in Sec-
tion 2, where we encode an intermediate symbol us-
ing the ngrams of original symbols (content) it de-
rives. This encoding is known as the Inside-Trie (I-
Trie) in Klein and Manning (2003a), in which they
also mentioned another encoding called Outside-
Trie (O-Trie). O-Trie encodes an intermediate sym-
bol using the its parent and the symbols surrounding
it in the original rule (context). Klein and Manning
(2003a) claimed that O-Trie is superior for calculat-
ing estimates for A* parsing. We plan to investigate
binarization defined by O-Trie in the future.
Both I-Trie and O-Trie are equivalent encodings,
resulting in equivalent grammars, because they both
encode using the complete content or context infor-
mation of an intermediate symbol. If we use part of
the information to encode, for example just parent in
O-Trie case, the encoding will be non-equivalent.
Proper non-equivalent encodings are used to gen-
eralize the grammar and prevent the binarized gram-
mar becoming too specific (Charniak et al, 2006). It
is equipped with head binarization to help improve
parsing accuracy, following the traditional linguistic
insight that phrases are organized around the head
(Collins, 1997; Klein and Manning, 2003b). In con-
trast, we focus our attention on parsing efficiency
not accuracy in this paper.
Binarization also attracts attention in the syntax-
based models for machine translation, where trans-
lation can be modeled as a parsing problem and bi-
narization is essential for efficient parsing (Zhang
et al, 2006; Huang, 2007).
Wang et al (2007) employs binarization to de-
compose syntax trees to acquire more re-usable
translation rules in order to improve translation ac-
curacy. Their binarization is restricted to be a mix-
ture of left and right binarization. This constraint
may decrease the power of binarization when ap-
plied to speeding up parsing in our problem.
9 Conclusions and future work
We have studied the impact of grammar binarization
on parsing efficiency and presented a novel bina-
rization which utilizes rich information learnt from
training corpus. Experiments not only showed that
our learnt binarization outperforms other existing
ones in terms of parsing efficiency, but also demon-
strated the feasibility to combine our binarization
with known parsing speed-up techniques to achieve
even better performance.
An advantage of our approach to finding a good
binarization would be that the training corpus does
not need to be parsed sentences. Only POS tagged
sentences will suffice for training. This will save the
effort to adapt the model to a new domain.
Our approach is based on the assumption that the
efficiency of CKY parsing is primarily determined
by the number of constituents produced. This is a
fairly sound one, but not always true, as shown in
Section 7.2. One future work will be relaxing the
assumption and finding a better appraoch.
Another future work will be to apply our work to
chart parsing. It is known that binarization is also
essential for an O(n3) complexity of chart parsing,
where dotted rules are used to binarize the grammar
implicitly from left. As shown in Charniak et al
(1998), we can binarize explicitly and use intermedi-
ate symbols to replace dotted rules in chart parsing.
Therefore chart parsing can use multiple binariza-
tions. We expect that a better binarization will also
help improve the efficiency of chart parsing.
Acknowledgements
We thank the anonymous reviwers for their pertinent
comments, Yoshimasa Tsuruoka for the detailed ex-
planations on his referred paper, Yunbo Cao, Shu-
jian Huang, Zhenxing Wang , John Blitzer and Liang
Huang for their valuable suggestions in preparing
the paper.
References
Aho, A. V. and Ullman, J. D. (1972). The theory
of parsing, translation, and compiling. Prentice-
Hall, Inc., Upper Saddle River, NJ, USA.
Charniak, E., Goldwater, S., and Johnson, M.
(1998). Edge-based best-first chart parsing. In
Proceedings of the Six Workshop on Very Large
Corpora, pages 127?133.
Charniak, E., Johnson, M., Elsner, M., Austerweil,
J., Ellis, D., Haxton, I., Hill, C., Shrivaths, R.,
Moore, J., Pozar, M., and Vu, T. (2006). Multi-
level coarse-to-fine pcfg parsing. In HLT-NAACL.
Collins, M. (1997). Three generative, lexicalised
models for statistical parsing. In ACL.
175
Earley, J. (1970). An efficient context-free parsing
algorithm. Commun. ACM, 13(2):94?102.
Goodman, J. (1997). Global thresholding and
multiple-pass parsing. In EMNLP.
Graham, S. L., Harrison, M. A., and Ruzzo, W. L.
(1980). An improved context-free recognizer.
ACM Trans. Program. Lang. Syst., 2(3):415?462.
Huang, L. (2007). Binarization, synchronous bina-
rization, and target-side binarization. In Proceed-
ings of SSST, NAACL-HLT 2007 / AMTA Work-
shop on Syntax and Structure in Statistical Trans-
lation, pages 33?40, Rochester, New York. Asso-
ciation for Computational Linguistics.
Kasami, T. (1965). An efficient recognition and
syntax analysis algorithm for context-free lan-
guages. Technical Report AFCRL-65-758, Air
Force Cambridge Research Laboratory, Bedford,
Massachusetts.
Kay, M. (1980). Algorithm schemata and data struc-
tures in syntactic processing. Technical Report
CSL80-12, Xerox PARC, Palo Alto, CA.
Klein, D. and Manning, C. D. (2001). Parsing and
hypergraphs. In IWPT.
Klein, D. and Manning, C. D. (2003a). A* parsing:
Fast exact viterbi parse selection. In HLT-NAACL.
Klein, D. and Manning, C. D. (2003b). Accurate
unlexicalized parsing. In ACL.
Marcus, M. P., Kim, G., Marcinkiewicz, M. A.,
MacIntyre, R., Bies, A., Ferguson, M., Katz, K.,
and Schasberger, B. (1994). The penn treebank:
Annotating predicate argument structure. In HLT-
NAACL.
Moore, R. C. (2000). Improved left-corner chart
parsing for large context-free grammars. In IWPT.
Petrov, S., Barrett, L., Thibaux, R., and Klein, D.
(2006). Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Schmid, H. (2004). Efficient parsing of highly am-
biguous context-free grammars with bit vectors.
In COLING.
Tsuruoka, Y. and Tsujii, J. (2004). Iterative cky pars-
ing for probabilistic context-free grammars. In
IJCNLP.
Wang, W., Knight, K., and Marcu, D. (2007). Bina-
rizing syntax trees to improve syntax-based ma-
chine translation accuracy. In EMNLP-CoNLL.
Younger, D. H. (1967). Recognition and parsing of
context-free languages in time n3. Information
and Control, 10(2):189?208.
Zhang, H., Huang, L., Gildea, D., and Knight, K.
(2006). Synchronous binarization for machine
translation. In HLT-NAACL.
176
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 514?523,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Structural Support Vector Method for Extracting Contexts and
Answers of Questions from Online Forums
Wen-Yun Yang
??
Yunbo Cao
??
Chin-Yew Lin
?
?
Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, China
?
Microsoft Research Asia, Beijing, China
wenyun.yang@gmail.com {yunbo.cao; cyl}@microsoft.com
Abstract
This paper addresses the issue of extract-
ing contexts and answers of questions
from post discussion of online forums.
We propose a novel and unified model by
customizing the structural Support Vector
Machine method. Our customization has
several attractive properties: (1) it gives a
comprehensive graphical representation of
thread discussion. (2) It designs special
inference algorithms instead of general-
purpose ones. (3) It can be readily ex-
tended to different task preferences by
varying loss functions. Experimental re-
sults on a real data set show that our meth-
ods are both promising and flexible.
1 Introduction
Recently, extracting questions, contexts and an-
swers from post discussions of online forums in-
curs increasing academic attention (Cong et al,
2008; Ding et al, 2008). The extracted knowl-
edge can be used either to enrich the knowledge
base of community question answering (QA) ser-
vices such as Yahoo! Answers or to augment the
knowledge base of chatbot (Huang et al, 2007).
Figure 1 gives an example of a forum thread
with questions, contexts and answers annotated.
This thread contains three posts and ten sentences,
among which three questions are discussed. The
three questions are proposed in three sentences,
S3, S5 and S6. The context sentences S1 and
S2 provide contextual information for question
sentence S3. Similarly, the context sentence S4
provides contextual information for question sen-
tence S5 and S6. There are three question-context-
answer triples in this example, (S3) ? (S1,S2) ?
(S8,S9), (S5)? (S4)? (S10) and (S6)? (S4)?
?
This work was done while the first author visited Mi-
crosoft Research Asia.
Post1: <context id=1> S1: Hi I am looking for
a pet friendly hotel in Hong Kong because all of
my family is going there for vacation. S2: my fam-
ily has 2 sons and a dog. </context> <question
id=1> S3: Is there any recommended hotel near
Sheung Wan or Tsing Sha Tsui? </question>
<context id=2, 3> S4: We also plan to go shopping
in Causeway Bay. </context> <question id=2>
S5: What?s the traffic situation around those com-
mercial areas? </question> <question id=3> S6:
Is it necessary to take a taxi? </question> S7: Any
information would be appreciated.
Post2: <answer id=1> S8: The Comfort Lodge
near Kowloon Park allows pet as I know, and usu-
ally fits well within normal budgets. S9: It is also
conveniently located, nearby the Kowloon railway
station and subway. </answer>
Post3: <answer id=2, 3> S10: It?s very crowd in
those areas, so I recommend MTR in Causeway Bay
because it is cheap to take you around. </answer>
Figure 1: An example thread with three posts and
ten sentences
(S10). As shown in the example, a forum question
usually requires contextual information to com-
plement its expression. For example, the ques-
tion sentence S3 would be of incomplete meaning
without the contexts S1 and S2, since the impor-
tant keyword pet friendly would be lost.
The problem of extracting questions, contexts,
and answers can be solved in two steps: (1) iden-
tify questions and then (2) extract contexts and an-
swers for them. Since identifying questions from
forum discussions is already well solved in (Cong
et al, 2008), in this paper, we are focused on step
(2) while assuming questions already identified.
Previously, Ding et al (2008) employ general-
purpose graphical models without any customiza-
tions to the specific extraction problem (step 2).
In this paper, we improve the existing models in
514
three aspects: graphical representation, inference
algorithm and loss function.
Graphical representation. We propose a more
comprehensive and unified graphical representa-
tion to model the thread for relational learning.
Our graphical representation has two advantages
over previous work (Ding et al, 2008): unifying
sentence relations and incorporating question in-
teractions.
Three types of relation should be considered for
context and answer extraction: (a) relations be-
tween successive sentences (e.g., context sentence
S2 occurs immediately before question sentence
S3); (b) relations between context sentences and
answer sentences (e.g., context S4 presents the
phrase Causeway Bay linking to answer which is
absent from question S6); and (c) relations be-
tween multiple labels for one sentence (e.g., one
question sentence is unlikely to be the answer to
another question although one sentence can serve
as contexts for more than one questions). Our pro-
posed graphical representation improves the mod-
eling of the three types of sentence relation (Sec-
tion 2.2).
Certain interactions exist among questions. For
example, question sentences S5 and S6 interact by
sharing context sentence S4. Our proposed graphi-
cal representation can naturally model the interac-
tions. Previous work (Ding et al, 2008) performs
the extraction of contexts and answers in multiple
passes of the thread (with each pass corresponding
to one question), which cannot address the interac-
tions well. In comparison, our model performs the
extraction in one pass of the thread.
Inference algorithm. Inference is usually a
time-consuming process for structured prediction.
We design special inference algorithms, instead of
general-purpose inference algorithms used in pre-
vious works (Cong et al, 2008; Ding et al, 2008),
by taking advantage of special properties of our
task. Specifically, we utilize two special properties
of thread structure to reduce the inference (time)
cost. First, context sentences and question sen-
tences usually occur in the same post while answer
sentences can only occur in the following posts.
With this properties, we can greatly reduce context
(or answer) candidate sets of a question, which
results in a significant decrease in inference cost
(Section 3). Second, context candidate set is usu-
ally much smaller than the number of sentences
in a thread. This property enables our proposal to
have an exact and efficient inference (Section 4.1).
Moreover, an approximate inference algorithm is
also given (Section 4.2).
Loss function. In practice, different applica-
tion settings usually imply different requirements
for system performance. For example, we expect
a higher recall for the purpose of archiving ques-
tions but a higher precision for the purpose of re-
trieving questions. A flexible framework should
be able to cope with various requirements. We
employ structural Support Vector Machine (SVM)
model that could naturally incorporate different
loss functions (Section 5).
We use a real data set to evaluate our approach
to extracting contexts and answers of questions.
The experimental results show both the effective-
ness and the flexibility of our approach.
In the next section, we formalize the problem
of context and answer extraction and introduce the
structural model. In Sections 3, 4 and 5 we give
the details of customizing structural model for our
task. In Section 6, we evaluate our methods. In
Section 7, we discuss the related work. Finally,
we conclude this paper in Section 8.
2 Problem Statement
We first introduce our notations in Section 2.1 and
then in Section 2.2 introduce how we model the
problem of extracting contexts and answers for
questions with a novel form of graphical represen-
tation. In Section 2.3 we introduce the structured
model based on the new representation.
2.1 Notations
Assuming that a given thread contains p posts
{p
1
, . . . , p
p
}, which are authored by a set of
users {u
1
, . . . , u
p
}. The p posts can be further
segmented into n sentences x = {x
1
, . . . , x
n
}.
Among the n sentences, m question sentences q =
{x
q
1
, . . . , x
q
m
} have been identified. Our task is
to identify the context sentences and the answer
sentences for those m question sentences. More
formally, we use four types of label {C,A,Q, P}
to stand for context, answer, question and plain la-
bels. Then, our task is to predict an m ? n label
matrix y = (y
ij
)
1?i?m,1?j?n
, except m elements
{y
1,q
1
, . . . , y
m,q
m
} which correspond to (known)
question labels. The element y
ij
in label matrix y
represents the role that the jth sentence plays for
the ith question. We denote the ith row and jth
column of the label matrix y by y
i.
and y
.j
.
515
y2 y3 y5y4 y6y1 y7
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
x1 x2 x3 x4 x5 x6 x7
(a) Skip-chain model
y2 y3 y5y4 y6y1 y7
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
x1 x2 x3 x4 x5 x6 x7
(b) Complete skip-chain model
y12 y13 y14y11 y1n
y22 y23 y24y21 y2n
ym 2 ym 3 ym 4ym 1 ym n
(c) 2D model
y12 y13 y14y11 y1n
y22 y23 y24y21 y2n
ym 2 ym 3 ym 4ym 1 ym n
(d) Label group model
Figure 2: Structured models
2.2 Graphical Representation
Recently, Ding et al (2008) use skip-chain and
2D Conditional Random Fields (CRFs) (Lafferty
et al, 2001) to perform the relational learning for
context and answer extraction. The skip-chain
CRFs (Sutton and McCallum, 2004; Galley, 2006)
model the long distance dependency between con-
text and answer sentences and the 2D CRFs (Zhu
et al, 2005) model the dependency between con-
tiguous questions. The graphical representation
of those two models are shown in Figures 2(a)
and 2(c), respectively. Those two CRFs are both
extensions of the linear chain CRFs for the sake
of powerful relational learning. However, di-
rectly using the skip-chain and 2D CRFs with-
out any customization has obvious disadvantages:
(a) the skip-chain model does not model the de-
pendency between answer sentence and multiple
context sentences; and (b) the 2D model does not
model the dependency between non-contiguous
questions.
To better model the problem of extracting con-
texts and answers of questions, we propose two
more comprehensive models, complete skip-chain
model and label group model to improve the ca-
pability of the two previous models. These two
models are shown in Figures 2(b) and 2(d).
In Figures 2(a) and 2(b), each label node is an-
notated with its allowed labels and the labels C, A,
Q and P stand for context, answer, question and
plain sentence labels, respectively. Note that the
complete skip-chain model completely links each
two context and answer candidates and the label
group model combines the labels of one sentence
into one label group.
2.3 Structured Model
Following the standard machine learning setup,
we denote the input and output spaces by X and
Y , then formulate our task as learning a hypoth-
esis function h : X ? Y to predict a y when
given x. In this setup, x represents a thread of n
sentences and m identified questions. y represents
the m? n label matrix to be predicted.
Given a set of training examples, S =
{(x
(i)
,y
(i)
) ? X ? Y : i = 1, . . . , N}, we
restrict ourselves to the supervised learning sce-
nario. We focus on hypothesis functions that
take the form h(x;w) = argmax
y?Y
F(x,y;w)
with discriminant function F : X ? Y ? R
where F(x,y;w) = w
T
?(x,y). As will be
introduced in Section 4, we employ structural
SVMs (Joachims et al, 2009) to find the optimal
parameters w. The structural SVMs have sev-
eral competitive properties as CRFs. First, it fol-
lows from the maximum margin strategy, which
has been shown with competitive or even better
516
performance (Tsochantaridis et al, 2005; Nguyen
and Guo, 2007). Second, it allows flexible choices
of loss functions to users. Moreover, in general,
it has theoretically proved convergence in polyno-
mial time (Joachims et al, 2009).
To use structural SVMs in relational learning,
one needs to customize three steps according to
specific tasks. The three steps are (a) definition of
joint feature mapping for encoding relations, (b)
algorithm of finding the most violated constraint
(inference) for efficient trainings and (c) definition
of loss function for flexible uses.
In the following Sections 3, 4 and 5, we describe
the customizations of the three steps for our con-
text and answer extraction task, respectively.
3 Encoding Relations
We use a joint feature mapping to model the rela-
tions between sentences in a thread. For context
and answer extraction, the joint feature mapping
can be defined as follows,
?(x,y) =
?
?
?
n
(x,y)
?
h
(x,y)
?
v
(x,y)
?
?
,
where the sub-mappings ?
n
(x,y), ?
h
(x,y), and
?
v
(x,y) encode three types of feature mappings,
node features, edge features and label group fea-
tures. The node features provide the basic infor-
mation for the output labels. The edge features
consist of the sequential edge features and skip-
chain edge features for successive label dependen-
cies. The label group features encode the relations
within each label group.
Before giving the detail definitions of the sub-
mappings, we first introduce the context and an-
swer candidate sets, which will be used for the
definitions and inferences. Each row of the label
matrix y corresponds to one question. Assuming
that the ith row y
i.
corresponds to the question
with sentence index q
i
, we thus have two candi-
date sets of contexts and answers for this question
denoted by C and A, respectively. We denote the
post indices and the author indices for the n sen-
tences as p = (p
1
, . . . , p
n
) and u = (u
1
, . . . , u
n
).
Then, we can formally define the two candidate
sets for the y
i.
as
C =
{
c
j
?
?
?
?
?
p
c
j
= p
q
i
? ?? ?
In Question Post
, c
j
6= q
i
? ?? ?
Not Question Sentence
}
,
A =
{
a
j
?
?
?
?
?
p
a
j
> p
q
i
? ?? ?
After Question Post
, u
a
j
6= u
q
i
? ?? ?
Not by the Same User
}
.
In the following, we describe formally about the
definitions of the three feature sub-mappings.
The node feature mapping ?
n
(x,y) encodes
the relations between sentence and label pairs, we
define it as follows,
?
n
(x,y) =
m
?
i=1
n
?
j=1
?
n
(x
j
, y
ij
),
where ?
n
(x
j
, y
ij
) is a feature mapping for a given
sentence and a label. It can be formally defined as
follows,
?
n
(x
j
, y
ij
) = ?(y
ij
)? ?
q
i
(x
j
), (1)
where ? denotes a tensor product, ?
q
i
(x
j
) and
?(y
ij
) denote two vectors. ?
q
i
(x
j
) contains ba-
sic information for output label. ?(y
ij
) is a 0/1
vector defined as
?(y
ij
) = [?
C
(y
ij
), ?
A
(y
ij
), ?
P
(y
ij
)]
T
,
where ?
C
(y
ij
) equal to one if y
ij
= C, otherwise
zero. The ?
A
(y
ij
) and ?
P
(y
ij
) are similarly de-
fined. Thus, for example, writing out ?
n
(x
j
, y
ij
)
for y
ij
= C one gets,
?
n
(x
j
, y
ij
) =
?
?
?
q
i
(x
j
)
0
0
?
?
? context
? answer
? plain
.
Note that the node feature mapping does not in-
corporate the relations between sentences.
The edge feature mapping ?
h
(x,y) is used
to incorporate two types of relation, the relation
between successive sentences and the relation be-
tween context and answer sentences. It can be de-
fined as follows,
?
h
(x,y) =
[
?
hn
(x,y)
?
hc
(x,y)
]
,
where ?
hn
(x,y) and ?
hc
(x,y) denote the two
types of feature mappings corresponding to se-
quential edges and skip-chain edges, respectively.
Their formal definitions are given as follows,
?
hn
(x,y) =
m
?
i=1
n?1
?
j=1
?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
),
517
Descriptions Dimensions
?
q
i
(x
j
) (32 dimensions) in ?
n
(x,y)
The cosine, WordNet and KL-divergence similarities with the question x
q
i
3
The cosine, WordNet and KL-divergence similarities with the questions other than x
q
i
3
The cosine, WordNet and KL-divergence similarities with previous and next sentences 6
Is this sentence x
j
exactly x
q
i
or one of the questions in {x
q
1
, . . . , x
q
m
}? 2
Is this sentence x
j
in the three beginning sentences? 3
The relative position of this sentence x
j
to questions 4
Is this sentence x
j
share the same author with the question sentence x
q
i
? 1
Is this sentence x
j
in the same post with question sentences? 2
Is this sentence x
j
in the same paragraph with question sentences? 2
The presence of greeting (e.g., ?hi?) and acknowledgement words in this sentence x
j
2
The length of this sentence x
j
1
The number of nouns, verbs and pronouns in this sentence x
j
, respectively 3
?
h
(x,y) (704 dimensions)
For ?
hn
(x,y), the above 32 dimension features w.r.t. 4? 4 = 16 transition patterns 512
For ?
hc
(x,y), 12 types of pairwise or merged similarities w.r.t. 16 transition patterns 192
?
v
(x,y) (32 dimensions)
The transition patterns for any two non-contiguous labels in a label group 16
The transition patterns for any two contiguous labels in a label group 16
Table 1: Feature descriptions and demisions
?
hc
(x,y) =
m
?
i=1
?
j?C
?
k?A
? ?? ?
Complete Edges
?
hc
(x
j
, x
k
, y
ij
, y
ik
),
?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
)
= ?(y
ij
, y
i,j+1
)? ?
hn
(x
j
, x
j+1
, y
ij
, y
i,j+1
),
?
hc
(x
j
, x
k
, y
ij
, y
ik
)
= ?(y
ij
, y
ik
)? ?
hc
(x
j
, x
k
, y
ij
, y
ik
)
where ?(y
ij
, y
ik
) is a 16-dimensional vector. It in-
dicates all 4?4 pairwise transition patterns of four
types of labels, the context, answer, question and
plain. Note that apart from previous work (Ding
et al, 2008) we use complete skip-chain (context-
answer) edges in ?
hc
(x,y).
The label group feature mapping ?
v
(x,y) is
defined as follows,
?
v
(x,y) =
n
?
j=1
?
v
(x
j
,y
.j
),
where ?
v
(x
j
,y
.j
) encodes each label group pat-
tern into a vector.
The detail descriptions and vector dimensions
of the used features are listed in Table 1.
4 Structural SVMs and Inference
Given a training set S = {(x
(i)
,y
(i)
) ? X ?
Y : i = 1, . . . , N}, we use the structural
SVMs (Taskar et al, 2003; Tsochantaridis et
al., 2005; Joachims et al, 2009) formulation, as
shown in Optimization Problem 1 (OP1), to learn
a weight vector w.
OP 1 (1-Slack Structural SVM)
min
w,??0
1
2
||w||
2
+
C
N
?
s.t. ?(y?
(1)
, . . . , y?
(N)
) ? Y
n
,
1
N
w
T
N
?
i=1
[?(x
(i)
,y
(i)
)??(x
(i)
, y?
(i)
)]
?
1
N
N
?
i=1
?(y
(i)
, y?
(i)
)? ?,
where ? is a slack variable, ?(x,y) is the joint
feature mapping and ?(y, y?) is the loss func-
tion that measures the loss caused by the dif-
ference between y and y?. Though OP1 is al-
ready a quadratic optimization problem, directly
using off-the-shelf quadratic optimization solver
will fail, due to the large number of constraints.
Instead, a cutting plane algorithm is used to ef-
ficiently solve this problem. For the details of the
518
{C , P } {C , P } {C , P } {Q } {P } {A, P } {A, P }
(a) Original graph
{P P P , P P C , P C P , P C C , C P P , C P C , C C P , C C C }
{Q } {P } {A, P } {A, P }
(b) Transformed graph
{P P P }
{Q } {P } {A, P } {A, P }
{C C C }
{Q } {P } {A, P } {A, P }
....
(c) Decomposed graph
Figure 3: The equivalent transform of graphs
Algorithm 1 Exact Inference Algorithm
1: Input: (C
i
,A
i
) for each q
i
, w, x, y
2: for i ? {1, . . . ,m} do
3: for C
s
? C
i
do
4: [R(C
s
), y?
i.
(C
s
)] ? Viterbi(w,x; C
s
)
5: end for
6: C
?
s
= argmax
C
s
?C
i
R(C
s
)
7: y?
?
i.
= y?
i.
(C
?
s
)
8: end for
9: return y?
?
structural SVMs, please refer to (Tsochantaridis et
al., 2005; Joachims et al, 2009).
The most essential and time-consuming step in
structural SVMs is finding the most violated con-
straint, which is equivalent to solve
argmax
y?Y
w
T
?(x
(i)
,y) + ?(y
(i)
,y). (2)
Without the ability to efficiently find the most vio-
lated constraint, the cutting plane algorithm is not
tractable.
In the next sub-sections, we introduce the al-
gorithms for finding the most violated constraint,
also called loss-augmented inference. The algo-
rithms are essential for the success of customizing
structural SVMs to our problem.
4.1 Exact Inference
The exact inference algorithm is designed for a
simplified model with two sub-mappings ?
n
and
?
h
, except ?
v
.
One naive approach to finding the most violated
constraint for the simplified model is to enumer-
ate all the 2
|C|+|A|
cases for each row of the label
matrix. However, it would be intractable for large
candidate sets.
An important property is that the context can-
didate set is usually much smaller than the whole
number of sentences in a thread. This property en-
ables us to design efficient and exact inference al-
gorithm by transforming from the original graph
representation in Figure 2 to the graphs in Fig-
ure 3. This graph transform merges all the nodes
in the context candidate set C to one node with 2
|C|
possible labels.
We design an exact inference algorithm in Algo-
rithm 1 based on the graph in Figure 3(c). The al-
gorithm can be summarized in three steps: (1) enu-
merate all the 2
|C|
possible labels
1
for the merged
node (line 3). (2) For each given label of the
merged node, perform the Viterbi algorithm (Ra-
biner, 1989) on the decomposed graph (line 4) and
store the Viterbi algorithm outputs in R and y?
i.
.
(3) From the 2
|C|
Viterbi algorithm outputs, select
the one with highest score as the output (lines 6
and 7).
The use of the Viterbi algorithm is assured by
the fact that there exists certain equivalence be-
tween the decomposed graph (Figure 3(c)) and a
linear chain. By fixing the the label of the merged
node, we could remove the dashed edges in the
decomposed graph and regard the rest graph as a
linear chain, which results in the Viterbi decoding.
4.2 Approximate Inference
The exact inference cannot handle the complete
model with three sub-mappings, ?
n
, ?
h
, and
?
v
, since the label group defeats the graph trans-
form in Figure 3. Thus, we design two ap-
proximate algorithms by employing undergener-
ating and overgenerating approaches (Finley and
Joachims, 2008).
First, we develop an undergenerating local
greedy search algorithm shown in Algorithm 2. In
the algorithm, there are two loops, inner and outer
loops. The outer loop terminates when no labels
change (steps 3-11). The inner loop enumerates
the whole label matrix and greedily determines
each label (step 7) by maximizing the Equation
(2). Since the whole algorithm terminates only if
1
Since the merged node is from context candidate set C,
enumerating its label is equivalent to enumerating subsets C
s
of the candidate set C
519
Algorithm 2 Greedy Inference Algorithm
1: Input: w, x, y
2: initialize solution: y? ? y
0
3: repeat
4: y
?
? y?
5: for i ? {1, . . . ,m} do
6: for j ? {1, . . . , n} do
7:
y?
?
ij
? argmax
y?
ij
w
T
?(x, y?)
+4(y, y?)
8: y?
ij
? y?
?
ij
9: end for
10: end for
11: until y? = y
?
12: y?
?
? y?
13: return y?
?
the label matrix does not change during the last
outer loop. This indicates that at least a local opti-
mal solution is obtained.
Second, an overgenerating method can be
designed by using linear programming relax-
ation (Finley and Joachims, 2008). To save the
space, we skip the details of this algorithm here.
5 Loss Functions
Structural SVMs allow users to customize the loss
function 4 : Y ? Y ? R according to different
system requirements. In this section, we introduce
the loss functions used in our work.
Basic loss function. The simplest way to quan-
tify the prediction quality is counting the number
of wrongly predicted labels. Formally,
4
b
(y, y?) =
m
?
i=1
n
?
j=1
I[y
ij
6= y?
ij
], (3)
where I[.] is an indicative function that equals to
one if the condition holds and zero otherwise.
Recall-vs-precision loss function. In practice,
we may place different emphasis on recall and pre-
cision according to application settings. We could
include this preference into the model by defining
the following loss function,
4
p
(y,
?
y) =
m
?
i=1
n
?
j=1
I[y
ij
6= P, y?
ij
= P ] ? c
r
+I[y
ij
= P, y?
ij
6= P ] ? c
p
. (4)
This function penalizes the wrong prediction de-
creasing recall and that decreasing precision with
Items in the data set #items
Thread 515
Post 2, 035
Sentence 8, 500
question annotation 1, 407
context annotation 1, 962
answer annotation 4, 652
plain annotation 18, 198
Table 2: The data statistics
two weights c
r
and c
p
respectively. Specifically,
we denote the loss function with c
p
/c
r
= 2 and
that with c
r
/c
p
= 2 by 4
p
p
and 4
r
p
, respectively.
Various types of loss function can be defined in
a similar fashion. To save the space, we skip the
definitions of other loss functions and only use the
above two types of loss functions to show the flex-
ibility of our approach.
6 Experiments
6.1 Experimental Setup
Corpus. We made use of the same data set as
introduced in (Cong et al, 2008; Ding et al,
2008). Specifically, the data set includes about
591 threads from the forum TripAdvisor
2
. Each
sentence in the threads is tagged with the labels
?question?, ?context?, ?answer?, or ?plain? by two
annotators. We removed 76 threads that have no
question sentences or more than 40 sentences and
6 questions. The remaining 515 forum threads
form our data set.
Table 2 gives the statistics on the data set. On
average, each thread contains 3.95 posts and 2.73
questions, and each question has 1.39 context sen-
tences and 3.31 answer sentences. Note that the
number of annotations is much larger than the
number of sentences because one sentence can be
annotated with multiple labels.
Experimental Details. In all the experiments,
we made use of linear models for the sake of com-
putational efficiency. As a preprocessing step, we
normalized the value of each feature value into
the interval [0, 1] and then followed the heuristic
used in SVM-light (Joachims, 1998) to set C to
1/||x||
2
, where ||x|| is the average length of input
samples (in our case, sentences). The tolerance pa-
rameter ? was set to 0.1 (the value also used in (Cai
2
TripAdvisor (http://www.tripadvisor.com/
ForumHome) is one of the most popular travel forums
520
and Hofmann, 2004)) in all the runs of the experi-
ments.
Evaluation. We calculated the standard preci-
sion (P), recall (R) and F
1
-score (F
1
) for both tasks
(context extraction and answer extraction). All the
experimental results were obtained through 5-fold
cross validation.
6.2 Baseline Methods
We employed binary SVMs (B-SVM), multiclass
SVMs (M-SVM), and C4.5 (Quinlan, 1993) as our
baseline methods:
B-SVM. We trained two binary SVMs for con-
text extraction (context vs. non-context) and an-
swer extraction (answer vs. non-answer), respec-
tively. We used the feature mapping ?
q
i
(x
j
) de-
fined in Equation (1) while training the binary
SVM models.
M-SVM. We extended the binary SVMs by
training multiclass SVMs for three category labels
(context, answer, plain).
C4.5. This decision tree algorithm solved the
same classification problem as binary SVMs and
made use of the same set of features.
6.3 Modeling Sentence Relations and
Question Interactions
We demonstrate in Table 3 that our approach can
make use of the three types of relation among sen-
tences well to boost the performance.
In Table 3, S-SVM represents the structural
SVMs only using the node features ?
n
(x,y). The
suffixes H, C, and V denote the models using
horizontal sequential edges, complete skip-chain
edges and vertical label groups, respectively. The
suffixes C* and V* denote the models using in-
complete skip-chain edges and vertical sequential
edges proposed in (Ding et al, 2008), as shown
in Figures 2(a) and 2(c). All the structural SVMs
were trained using basic loss function ?
b
in Equa-
tion (3). From Table 3, we can observe the follow-
ing advantages of our approaches.
Overall improvement. Our structural approach
steadily improves the extraction as more types of
relation (corresponding to more types of edge) are
included. The best results obtained by using the
three types of relation together improve the base-
line methods binary SVMs by about 6% and 20%
in terms of F
1
values for context extraction and
answer extraction, respectively.
The usefulness of relations. The relations
encoded by horizontal sequential edges and la-
Method 4
b
P (%) R (%) F
1
(%)
Context Extraction
C4.5 ? 74.2 68.7 71.2
B-SVM ? 78.3 72.2 74.9
M-SVM ? 68.0 77.6 72.1
S-SVM 8.86 75.6 71.7 73.4
S-SVM-H 8.60 77.5 75.5 76.3
S-SVM-HC* 8.65 77.9 74.1 75.8
S-SVM-HC 8.62 77.5 75.2 76.2
S-SVM-HCV* 8.08 79.5 79.6 79.5
S-SVM-HCV 7.98 79.7 80.2 79.9
Answer Extraction
C4.5 ? 61.3 45.2 51.8
B-SVM ? 69.7 42.0 51.8
M-SVM ? 63.2 51.5 55.8
S-SVM 8.86 67.0 48.0 55.6
S-SVM-H 8.60 66.9 49.7 56.7
S-SVM-HC* 8.65 66.5 49.4 56.4
S-SVM-HC 8.62 65.7 51.5 57.4
S-SVM-HCV* 8.08 65.5 58.7 61.7
S-SVM-HCV 7.98 65.1 61.2 63.0
Table 3: The effectiveness of our approach
bel groups are useful for both context extraction
and answer extraction. The relation encoded by
complete skip-chain edges is useful for answer
extraction. The complete skip-chain edges not
only avoid preprocessing but also boost the per-
formance when compared with the preprocessed
skip-chain edges. The label groups improve the
vertical sequential edges.
Interactions among questions. The interac-
tions encoded by label groups are especially use-
ful. We conducted significance tests (sign test) on
the experimental results. The test result shows that
S-SVM-HCV outperforms all the other methods
without vertical edges statistically significantly (p-
value < 0.01). Our proposed graphical represen-
tation in Figure 2(d) eases us to model the complex
interactions. In comparison, the 2D model in Fig-
ure 2(c) used in previous work (Ding et al, 2008)
can only model the interaction between adjacent
questions.
6.4 Loss Function Results
We report in Table 4 the comparison between
structural SVMs using different loss functions.
Note that ?
p
p
prefers precision and ?
r
p
prefers re-
call. From Table 4, we can observe that the ex-
perimental results also exhibit this kind of system
521
Method P (%) R (%) F
1
(%)
Context Extraction
S-SVM-HCV-4
b
79.7 80.2 79.9
S-SVM-HCV-4
p
p
82.0 70.3 75.6
S-SVM-HCV-4
r
p
75.7 84.2 79.7
Answer Extraction
S-SVM-HCV-4
b
65.1 61.2 63.0
S-SVM-HCV-4
p
p
71.8 52.2 60.2
S-SVM-HCV-4
r
p
61.8 66.1 63.7
Table 4: The use of different loss functions
preference. Moreover, we further demonstrate the
capability of the loss function ?
p
in Figure 4. The
curves are achieved by varying the ratio between
two parameters c
p
/c
r
in Equation (4). The curves
confirm our intuition: when log(c
p
/c
r
) becomes
larger, the precisions increase but the recalls de-
crease and vice versa.
7 Related work
Previous work on extracting questions, answers
and contexts is most related with our work. Cong
et al (2008) proposed a supervised approach for
question detection and an unsupervised approach
for answer detection without considering contexts.
Ding et al (2008) used CRFs to detect contexts
and answers of questions from forum threads.
Some researches on summarizing discussion
threads and emails are related to our work, too.
Zhou and Hovy (2005) segmented internet re-
lay chat, clustered segments into sub-topics, and
identified responding segments of the first seg-
ment in each sub-topic by assuming the first seg-
ment to be focus. In (Nenkova and Bagga, 2003;
Wan and McKeown, 2004; Rambow et al, 2004),
email summaries were organized by extracting
overview sentences as discussion issues. The
work (Shrestha and McKeown, 2004) used RIP-
PER as a classifier to detect interrogative questions
and their answers then used the resulting question
and answer pairs as summaries. We also note the
existing work on extracting knowledge from dis-
cussion threads. Huang et al (2007) used SVMs
to extract input-reply pairs from forums for chat-
bot knowledge. Feng et al (2006) implemented
a discussion-bot which used cosine similarity to
match students? query with reply posts from an an-
notated corpus of archived threaded discussions.
Moreover, extensive researches have been done
within the area of question answering (Burger et
?1.5 ?1 ?0.5 0 0.5 1 1.50.6
0.7
0.8
0.9
1
Log loss ratio
Pre
cisi
on
 
 ContextAnswer
?1.5 ?1 ?0.5 0 0.5 1 1.50.4
0.6
0.8
1
Log loss ratio
Rec
all
 
 ContextAnswer
Figure 4: Balancing between precision and recall
al., 2006; Jeon et al, 2005; Harabagiu and Hickl,
2006; Cui et al, 2005; Dang et al, 2006). They
mainly focused on using sophisticated linguistic
analysis to construct answer from a large docu-
ment collection.
8 Conclusion and Future Work
We have proposed a new form of graphical rep-
resentation for modeling the problem of extract-
ing contexts and answers of questions from online
forums and then customized structural SVM ap-
proach to solve it.
The proposed graphical representation is able
to naturally express three types of relation among
sentences: relation between successive sentences,
relation between context sentences and answer
sentences, and relation between multiple labels for
one sentence. The representation also enables us
to address interactions among questions. We also
developed the inference algorithms for the struc-
tural SVM model by exploiting the special struc-
ture of thread discussions.
Experimental results on a real data set show that
our approach significantly improves the baseline
methods by effectively utilizing various types of
relation among sentences.
Our future work includes: (a) to summa-
rize threads and represent the forum threads in
question-context-answer triple, which will change
the organization of online forums; and (b) to en-
hance QA services (e.g., Yahoo! Answers) by the
contents extracted from online forums.
Acknowledgement
The authors would like to thank the anonymous re-
viewers for their comments to improve this paper.
522
References
John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
George Miller, Dan Moldovan, Bill Ogden, John
Prager, Ellen Riloff, Amit Singhal, Rohini Shrihari,
Tomek Strzalkowski, Ellen Voorhees, and Ralph
Weishedel. 2006. Issues, tasks and program struc-
tures to roadmap research in question and answering
(qna). ARAD: Advanced Research and Development
Activity (US).
Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal document categorization with support vector ma-
chines. In Proceedings of CIKM, pages 78?87.
Gao Cong, Long Wang, Chin-Yew Lin, and Young-In
Song. 2008. Finding question-answer pairs from
online forums. In Proceedings of SIGIR, pages 467?
474.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of SIGIR, pages 400?407.
Hoa Dang, Jimmy Lin, and Diane Kelly. 2006.
Overview of the trec 2006 question answering track.
In Proceedings of TREC, pages 99?116.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random field to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL, pages 710?718.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Proceedings of IUI, pages 171?177.
Thomas Finley and Thorsten Joachims. 2008. Training
structural SVMs when exact inference is intractable.
In Proceedings of ICML, pages 304?311.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
364?372.
Sanda M. Harabagiu and Andrew Hickl. 2006. Meth-
ods for using textual entailment in open-domain
question answering. In Proceedings of ACL, pages
905?912.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In Proceedings of IJCAI, pages 423?428.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of CIKM, pages 84?
90.
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural SVMs.
Machine Learning.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rele-
vant features. In Proceedings of ECML, pages 137?
142.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Ani Nenkova and Amit Bagga. 2003. Facilitating
email thread access by extractive summary genera-
tion. In Proceedings of RANLP, pages 287?296.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of sequence labeling algorithms and extensions. In
Proceedings of ICML, pages 681?688.
John Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publisher Incorpora-
tion.
Lawrence Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of IEEE, pages 257?286.
Owen Rambow, Lokesh Shrestha, John Chen, and
Chirsty Lauridsen. 2004. Summarizing email
threads. In Proceedings of HLT-NAACL, pages 105?
108.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of COLING, pages 889?895.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. Technical Report 04-49.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Advances
in Neural Information Processing Systems 16. MIT
Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484.
Stephen Wan and Kathy McKeown. 2004. Generating
overview summaries of ongoing email thread discus-
sions. In Proceedings of COLING, pages 549?555.
Liang Zhou and Eduard Hovy. 2005. Digesting vir-
tual ?geek? culture: The summarization of technical
internet relay chats. In Proceedings of ACL, pages
298?305.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and
Wei-Ying Ma. 2005. 2d conditional random fields
for web information extraction. In Proceedings of
ICML, pages 1044?1051.
523
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250?1259,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Semi-supervised Speech Act Recognition in Emails and Forums
Minwoo Jeong
??
Chin-Yew Lin
?
Gary Geunbae Lee
?
?
Pohang University of Science & Technology, Pohang, Korea
?
Microsoft Research Asia, Beijing, China
?
{stardust,gblee}@postech.ac.kr
?
cyl@microsoft.com
Abstract
In this paper, we present a semi-supervised
method for automatic speech act recogni-
tion in email and forums. The major chal-
lenge of this task is due to lack of labeled
data in these two genres. Our method
leverages labeled data in the Switchboard-
DAMSL and the Meeting Recorder Dia-
log Act database and applies simple do-
main adaptation techniques over a large
amount of unlabeled email and forum data
to address this problem. Our method uses
automatically extracted features such as
phrases and dependency trees, called sub-
tree features, for semi-supervised learn-
ing. Empirical results demonstrate that
our model is effective in email and forum
speech act recognition.
1 Introduction
Email and online forums are important social me-
dia. For example, thousands of emails and posts
are created daily in online communities, e.g.,
Usenet newsgroups or the TripAdvisor travel fo-
rum
1
, in which users interact with each other us-
ing emails/posts in complicated ways in discus-
sion threads. To uncover the rich interactions in
these email exchanges and forum discussions, we
propose to apply speech act recognition to email
and forum threads.
Despite extensive studies of speech act recogni-
tion in many areas, developing speech act recogni-
tion for online forms of conversation is very chal-
lenging. A major challenge is that emails and
forums usually have no labeled data for training
statistical speech act recognizers. Fortunately, la-
beled speech act data are available in other do-
mains (i.e., telephone and meeting conversations
?
This work was conducted during the author?s internship
at Microsoft Research Asia.
1
http://tripadvisor.com/
in this paper) and large unlabeled data sets can be
collected from the Web. Thus, we focus on the
problem of how to accurately recognize speech
acts in emails and forums by making maximum
use of data from existing resources.
Recently, there are increasing interests in
speech act recognition of online text-based con-
versations. Analysis of speech acts for online
chat and instant messages and have been studied
in computer-mediated communication (CMC) and
distance learning (Twitchell et al, 2004; Nastri et
al., 2006; Ros?e et al, 2008). In natural language
processing, Cohen et al (2004) and Feng et al
(2006) used speech acts to capture the intentional
focus of emails and discussion boards. However,
they assume that enough labeled data are available
for developing speech act recognition models.
A main contribution of this paper is that we ad-
dress the problem of learning speech act recog-
nition in a semi-supervised way. To our knowl-
edge, this is the first use of semi-supervised speech
act recognition in emails and online forums. To
do this, we make use of labeled data from spo-
ken conversations (Jurafsky et al, 1997; Dhillon
et al, 2004). A second contribution is that our
model learns subtree features that constitute dis-
criminative patterns: for example, variable length
n-grams and partial dependency structures. There-
fore, our model can capture both local features
such as n-grams and non-local dependencies. In
this paper, we extend subtree pattern mining to the
semi-supervised learning problem.
This paper is structured as follows. Section 2
reviews prior work on speech act recognition and
Section 3 presents the problem statement and our
data sets. Section 4 describes a supervised method
of learning subtree features that shows the effec-
tiveness of subtree features on labeled data sets.
Section 5 proposes semi-supervised learning tech-
niques for speech act recognition and Section 6
demonstrates our method applied to email and on-
1250
line forum thread data. Section 7 concludes this
paper with future work.
2 Related Work
Speech act theory is fundamental to many stud-
ies in discourse analysis and pragmatics (Austin,
1962; Searle, 1969). A speech act is an illo-
cutionary act of conversation and reflects shal-
low discourse structures of language. Recent re-
search on spoken dialog processing has investi-
gated computational speech act models of human-
human and human-computer conversations (Stol-
cke et al, 2000) and applications of these mod-
els to CMC and distance learning (Twitchell et al,
2004; Nastri et al, 2006; Ros?e et al, 2008).
Our work in this paper is closely related to prior
work on email and forum speech act recognition.
Cohen et al (2004) proposed the notion of ?email
speech act? for classifying the intent of an email
sender. They defined verb and noun categories
for email speech acts and used supervised learn-
ing to recognize them. Feng et al (2006) pre-
sented a method of detecting conversation focus
based on the speech acts of messages in discus-
sion boards. Extending Feng et al (2006)?s work,
Ravi and Kim (2007) applied speech act classifi-
cation to detect unanswered questions. However,
none of these studies have focused on the semi-
supervised speech act recognition problem and ex-
amined their methods across different genres.
The speech processing community frequently
employs two large-scale corpora for speech act
annotation: Switchboard-DAMSL (SWBD) and
Meeting Recorder Dialog Act (MRDA). SWBD is
an annotation scheme and collection of labeled di-
alog act
2
data for telephone conversations (Juraf-
sky et al, 1997). The main purpose of SWBD is
to acquire stochastic discourse grammars for train-
ing better language models for automatic speech
recognition. More recently, an MRDA corpus has
been adapted from SWBD but its tag set for la-
beling meetings has been modified to better reflect
the types of interaction in multi-party face-to-face
meetings (Dhillon et al, 2004). These two corpora
have been extensively studied, e.g., (Stolcke et al,
2000; Ang et al, 2005; Galley et al, 2004). We
also use these for our experiments.
2
A dialog act is the meaning of an utterance at the level
of illocutionary force (Austin, 1962), and broadly covers the
speech act and adjacency pair (Stolcke et al, 2000). In this
paper, we use only the term ?speech act? for clarity.
This paper focuses on the problem of semi-
supervised speech act recognition. The goal of
semi-supervised learning techniques is to use aux-
iliary data to improve a model?s capability to rec-
ognize speech acts. The approach in Tur et al
(2005) presented semi-supervised learning to em-
ploy auxiliary unlabeled data in call classification,
and is closely related to our work. However, our
approach uses the most discriminative subtree fea-
tures, which is particularly attractive for reducing
the model?s size. Our problem setting is closely re-
lated to the domain adaptation problem (Ando and
Zhang, 2005), i.e., we seek to obtain a model that
analyzes target domains (emails and forums) by
adapting a method that analyzes source domains
(SWBD and MRDA). Recently, this type of do-
main adaptation has become an important topic in
natural language processing.
3 Problem Definition
3.1 Problem Statement
We define speech act recognition to be the task
that, given a sentence, maps it to one of the speech
act types. Figure 1 shows two examples of our
email and forum speech act recognition. E1?6 are
all sentences in an email message. F1?3, F4?5,
and F6 are three posts in a forum thread. A sen-
tence interacts alone or with others, for example,
F6 agrees with the previous post (F4?5). To gain
insight into our work, it is useful to consider that
E2, 3 and F1, 4, 6 are summaries of two dis-
courses. In particular, F1 denotes a question and
F4 and F6 are corresponding answers. More re-
cently, using speech acts has become an appealing
approach in summarizing the discussions (Galley
et al, 2004; McKeown et al, 2007).
Next, we define speech act category based on
MRDA. Dhillon et al (2004) included definitions
of speech acts for colloquial style interactions
(e.g., backchannel, disruption, and floorgrabber),
but these are not applicable in emails and forums.
After removing these categories, we define 12 tags
(Table 1). Dhillon et al (2004) provides detailed
descriptions of each tag. We note that our tag set
definition is different from (Cohen et al, 2004;
Feng et al, 2006; Ravi and Kim, 2007) for two
reasons. First, prior work primarily interested in
the domain-specific speech acts, but our work use
domain-independent speech act tags. Second, we
focus on speech act recognition on the sentence-
level.
1251
E1: I am planning my schedule at CHI 2003 (http://www.chi2003.org/) S
E2: - will there be anything happening at the conference related to this W3C User interest group? QY
E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S
E4: - a chance to meet others and bring someone like me up to speed on what is happening. S
E5: There will be many competing activities, so the sooner we can set this up the more likely I can attend. S
E6: Keith S
F1: If given a choice, should I choose Huangpu area, or should I choose Pudong area? QR
F2: Both location are separated by a Huangpu river, not sure which area is more convenient for sight seeing? QW
F3: Thanks in advance for reply! P
F4: Stay on the Puxi side of the Huangpu river and visit the Pudong side by the incredible tourist tunnel. AC
F5: If you stay on the Pudong side add half an hour to visit the majority of the tourist attractions. S
F6: I definitely agree with previous post. AA
Figure 1: Examples of speech act recognition in emails and online forums. Tags are defined in Table 1.
Table 1: Tags used to describe components of
speech acts
Tag Description
A Accept response
AA Acknowledge and appreciate
AC Action motivator
P Polite mechanism
QH Rhetorical question
QO Open-ended question
QR Or/or-clause question
QW Wh-question
QY Yes-no question
R Reject response
S Statement
U Uncertain response
The goal of semi-supervised speech act recogni-
tion is to learn a classifier using both labeled and
unlabeled data. We formally define our problem
as follows. Let x = {x
j
} be a forest, i.e., a set of
trees that represents a natural language structure,
for example, a sequence of words and a depen-
dency parse tree. We will describe this in more
detail in Section 4. Let y be a speech act. Then,
we define D
L
= {x
i
, y
i
}
n
i=1
as the set of labeled
training data, and D
U
= {x
i
}
l
i=n+1
as the set of
unlabeled training data where l = n+m and m is
the number of unlabeled data instances. Our goal
is to find a learning method to minimize the clas-
sification errors in D
L
and D
U
.
3.2 Data Preparation
In this paper, we separate labeled (D
L
) and un-
labeled data (D
U
). First we use SWBD
3
and
MRDA
4
as our labeled data. We automatically
3
LDC Catalog No. LDC97S62
4
http://www.icsi.berkeley.edu/?ees/dadb/
map original annotations in SWBD and MRDA to
one of the 12 speech acts.
5
Inter-annotator agree-
ment ? in both data sets is ? 0.8 (Jurafsky et al,
1997; Dhillon et al, 2004). For evaluation pur-
poses, we divide labeled data into three sets: train-
ing, development, and evaluation sets (Table 2).
Of the 1,155 available conversations in the SWBD
corpus, we use 855 for training, 100 for devel-
opment, and 200 for evaluation. Among the 75
available meetings in the MRDA corpus, we ex-
clude two meetings of different natures (btr001
and btr002). Of the remaining meetings, we use
59 for training, 6 for development, and 8 for eval-
uation. Then we merge multi-segments utterances
that belong to the same speaker and then divide all
data sets into sentences.
As stated earlier, our unlabeled data consists
of email (EMAIL) and online forum (FORUM)
data. For the EMAIL set, we selected 22,391
emails from Enron data
6
(discussion threads,
all documents, and calendar folders). For the FO-
RUM set, we crawled 11,602 threads and 55,743
posts from the TripAdvisor travel forum site (Bei-
jing, Shanghai, and Hongkong forums). As our
evaluation sets, we used 40 email threads of the
BC3 corpus
7
for EMAIL and 100 threads selected
from the same travel forum site for FORUM. Ev-
ery sentences was automatically segmented by the
MSRA sentence boundary detector (Table 2). An-
notation was performed by two human annotators,
and inter-annotator agreements were ? = 0.79 for
EMAIL and ? = 0.73 for FORUM.
Overall performance of automatic evaluation
measures usually depends on the distribution of
tags. In both labeled and unlabeled sets, the most
5
Our mapping tables are available at
http://home.postech.ac.kr/?stardust/acl09/.
6
http://www.cs.cmu.edu/?enron/
7
http://www.cs.ubc.ca/nest/lci/bc3.html
1252
Table 2: Number of sentences in labeled and unlabeled data
Set SWBD MRDA
Training 96,553 50,865
Development 12,299 8,366
Evaluation 24,264 10,492
Set EMAIL FORUM
Unlabeled 122,125 297,017
Evaluation 2,267 3,711
Figure 2: Distribution of speech acts in the evaluation sets. Tags are defined in Table 1.
frequent tag is the statement (S) tag (Figure 2).
Distributions of tags are similar in training and de-
velopment sets of SWBD and MRDA.
4 Speech Act Recognition
Previous work in speech act recognition used a
large set of lexical features, e.g., bag-of-words,
bigrams and trigrams (Stolcke et al, 2000; Co-
hen et al, 2004; Ang et al, 2005; Ravi and Kim,
2007). However, these methods create a large
number of lexical features that might not be nec-
essary for speech act identification. For example,
a Wh-question ?What site should we use to book a
Beijing-Chonqing flight?? can be predicted by two
discriminative features, ?(<s>, WRB) ? QW?
and ?(?, </s>) ? QW? where <s> and </s>
are sentence start and end symbols, and WRB is
a part-of-speech tag that denotes a Wh-adverb.
In addition, useful features could be of various
lengths, i.e. not fixed length n-grams, and non-
adjacent. One key idea of this paper is a novel use
of subtree features to model these for speech act
recognition.
4.1 Exploiting Subtree Features
To exploit subtree features in our model, we use
a subtree pattern mining method proposed by
Kudo and Matsumoto (2004). We briefly intro-
duce this algorithm here. In Section 3.1, we de-
fined x = {x
j
} as the forest that is a set of trees.
More precisely, x
j
is a labeled ordered tree where
each node has its own label and is ordered left-
to-right. Several types of labeled ordered trees
Figure 3: Representations of tree: (a) bag-of-
words, (b) n-gram, (c) word pair, and (d) depen-
dency tree. A node denotes a word and a directed
edge indicates a parent-and-child relationship.
are possible (Figure 3). Note that S-expression
can be used instead for computation, for example
(a(b(c(d)))) for the n-gram (Figure 3(b)).
Moreover, we employ a combination of multiple
trees as the input of the subtree pattern mining al-
gorithm.
We extract subtree features from the forest set
{x
i
}. A subtree t is a tree if t ? x. For exam-
ple, (a), (a(b)), and (b(c(d))) are subtrees
of Figure 3(b). We define the subtree feature as a
weak learner:
f(y, t,x) ,
{
+y t ? x,
?y otherwise,
(1)
where we assume a binary case y ? Y =
{+1,?1} for simplicity. Even though the ap-
proach in Kudo and Matsumoto (2004) and ours
are simiar, there are two clear distinctions. First,
our method employs multiple tree structures, and
uses different constraints to generate subtree can-
didates. In this paper, we only restrict generating
1253
the dependency subtrees which should have 3 or
more nodes. Second, our method is of interest
for semi-supervised learning problems. To learn
subtree features, Kudo and Matsumoto (2004) as-
sumed supervised data {(x
i
, y
i
)}. Here, we de-
scribe the supervised learning method and will de-
scribe our semi-supervised method in Section 5.
4.2 Supervised Boosting Learning
Given training examples, we construct a ensem-
ble learner F (x) =
?
k
?
k
f(y
k
, t
k
,x), where ?
k
is a coefficient for linear combination. A final
classifier h(x) can be derived from the ensemble
learner, i.e., h(x) , sgn (F (x)). As an optimiza-
tion framework (Mason et al, 2000), the objective
of boosting learning is to find F such that the cost
of functional
C(F ) =
?
i?D
?
i
C[y
i
F (x
i
)] (2)
is minimized for some non-negative and monoton-
ically decreasing cost function C : R ? R and
the weight ?
i
? R
+
. In this paper, we use the
AdaBoost algorithm (Schapire and Singer, 1999);
thus the cost function is defined as C(z) = e
?z
.
Constructing an ensemble learner requires that
the user choose a base learner, f(y, t,x), to
maximize the inner product ???C(F ), f? (Ma-
son et al, 2000). Finding f(y, t,x) to maxi-
mize ???C(F ), f? is equivalent to searching for
f(y, t,x) to minimize 2
?
i:f(y,t,x
i
)6=y
i
w
i
? 1,
where w
i
for i ? D
L
, is the empirical data dis-
tribution w
(k)
i
at step k. It is defined as:
w
(k)
i
= ?
i
? e
?y
i
F (x
i
)
. (3)
From Eq. 3, a proper base learner (i.e., subtree)
can be found by maximizing weighted gain, where
gain(t, y) =
?
i?D
L
y
i
w
i
f(y, t,x
i
). (4)
Thus, subtree mining is formulated as the prob-
lem of finding (
?
t, y?) = argmax
(t,y)?X?Y
gain(t, y). We
need to search with respect to a non-monotonic
score function (Eq. 4), thus we use the monotonic
bound, gain(t, y) ? ?(t), where
?(t) =max
?
?
2
?
w
i
{i|y
i
=+1,t?x
i
}
?
n
?
i=1
y
i
f(y, t,x
i
),
2
?
w
i
{i|y
i
=?1,t?x
i
}
+
n
?
i=1
y
i
f(y, t,x
i
)
?
?
. (5)
Table 3: Result of supervised learning experiment;
columns are micro-averaged F
1
score with macro-
averaged F
1
score in parentheses. MAXENT:
maximum entropy model; BOW: bag-of-words
model; NGRAM: n-gram model; +POSTAG,
+DEPTREE, +SPEAKER indicate that the com-
ponents were added individually onto NGRAM.
?
?
? indicates results significantly better than the
NGRAM model (p < 0.001).
Model SWBD MRDA
MAXENT 92.76 (63.54) 82.48 (57.19)
BOW 91.32 (54.47) 82.17 (55.42)
NGRAM 92.60 (58.43) 83.30 (57.53)
+POSTAG 92.69 (60.07) 83.60 (58.46)
+DEPTREE 92.67 (61.75)
?
83.57 (57.45)
+SPEAKER
?
92.86 (63.13) 83.40 (58.20)
ALL
?
92.87 (63.77) 83.49 (59.04)
The subtree set is efficiently enumerated using a
branch-and-bound procedure based on ?(t) (Kudo
and Matsumoto, 2004).
After finding an optimal base leaner, f(y?,
?
t,x),
we need to set the coefficient ?
k
to form a new en-
semble, F (x
i
) ? F (x
i
) + ?
k
f(
?
t, y?,x
i
). In Ad-
aBoost, we choose
?
k
=
1
2
log
(
1 + gain(
?
t, y?)
1? gain(
?
t, y?)
)
. (6)
After K iterations, the boosting algorithm returns
the ensemble learner F (x) which consists of a set
of appropriate base learners f(y, t,x).
4.3 Evaluation on Labeled Data
We verified the effectiveness of using subtree fea-
tures on the SWBD and MRDA data sets. For
boosting learning, one typically assumes ?
i
= 1.
In addition, the number of iterations, which relates
to the number of patterns, was determined by a
development set. We also used a one-vs.-all strat-
egy for the multi-class problem. Precision and re-
call were computed and combined into micro- and
macro-averaged F
1
scores. The significance of our
results was evaluated using the McNemar paired
test (Gillick and Cox, 1989), which is based on in-
dividual labeling decisions to compare the correct-
ness of two models. All experiments were imple-
mented in C++ and executed in Windows XP on a
PC with a Dual 2.1 GHz Intel Core2 processor and
2.0 Gbyte of main memory.
1254
Figure 4: Comparison of different trees (SWBD)
We show that use of subtree features is ef-
fective to solve the supervised speech act recog-
nition problem. We also compared our model
with the state-of-the-art maximum entropy classi-
fier (MAXENT). We used bag-of-words, bigram
and trigram features for MAXENT, which mod-
eled 702k (SWBD) and 460k (MRDA) parameters
(i.e., patterns), and produced micro-averaged F
1
scores of 92.76 (macro-averaged F
1
= 63.54) for
SWBD and 82.48 (macro-averaged F
1
= 57.19)
for MRDA. In contrast, our method generated ap-
proximately 4k to 5k patterns on average with sim-
ilar or greater F
1
scores (Table 3); hence, com-
pared to MAXENT, our model requires fewer cal-
culations and is just as accurate.
The n-gram model (NGRAM) performed signif-
icantly better than the bag-of-words model (Mc-
Nemar test; p < 0.001) (Table 3). Unlike MAX-
ENT, NGRAM automatically selects a relevant set
of variable length n-gram features (i.e., phrase
features). To this set, we separately added two
syntax type features, part-of-speech tag n-gram
(POSTAG) and dependency parse tree (DEPTREE)
automatically parsed by Minipar
8
, and one dis-
course type feature, speaker n-gram (SPEAKER).
Although some micro-averaged F
1
are not statisti-
cally significant between the original NGRAM and
the models that include POSTAG, DEPTREE or
SPEAKER, macro-averaged F
1
values indicate that
minor classes can take advantage of other struc-
tures. For example, in the result of SWBD (Fig-
ure 4), DEPTREE and SPEAKER models help to
predict uncertain responses (U), whereas NGRAM
and POSTAG cannot do this.
5 Semi-supervised Learning
Our goal is to eventually make maximum use
of existing resources in SWBD and MRDA for
8
http://www.cs.ualberta.ca/?lindek/minipar.htm
email/forum speech act recognition. We call the
model trained on the mixed data of these two cor-
pora BASELINE. We use ALL features in con-
structing the BASELINE for the semi-supervised
experiments. While this model gave promising re-
sults using SWBD and MRDA, language used in
emails and forums differs from that used in spo-
ken conversation. For example, ?thanx? is an ex-
pression commonly used as a polite mechanism
in online communications. To adapt our model to
understand this type of difference between spoken
and online text-based conversations, we should in-
duce new patterns from unlabeled email and fo-
rum data. We describe here two methods of semi-
supervised learning.
5.1 Method 1: Bootstrapping
First, we bootstrap the BASELINE model using au-
tomatically predicted unlabeled examples. How-
ever, using all of the unlabeled data results in noisy
models; therefore filtering or selecting data is very
important in practice. To this end, we only select
similar examples by criterion, d(x
i
,x
j
) < r or k
nearest neighbors where x
i
? D
L
and x
j
? D
U
.
In practice, r or k are fixed. In our method, exam-
ples are represented by trees; hence we use a ?tree
edit distance? for calculating d(x
i
,x
j
) (Shasha
and Zhang, 1990). Selected examples are evalu-
ated using BASELINE, and using subtree pattern
mining runs on the augmented data (i.e. unla-
beled). We call this method BOOTSTRAP.
5.2 Method 2: Semi-supervised Boosting
Our second method is based on a principle of
semi-supervised boosting learning (Bennett et al,
2002). Because we have no supervised guidance
for D
U
, our objective functional to find F is de-
fined as:
C(F ) =
?
i?D
L
?
i
C[y
i
F (x
i
)] +
?
i?D
U
?
i
C[|F (x
i
)|]
(7)
This cost functional is non-differentiable. To
solve it, we introduce pseudo-labels y? where y? =
sgn(F (x)) and |F (x)| = y?F (x). Using the same
derivation in Section 4.2, we obtain the following
1255
gain function and update rules:
gain(t, y) =
?
i?D
L
y
i
w
i
f(y, t,x
i
)
+
?
i?D
U
y?
i
w
i
f(y, t,x
i
), (8)
w
i
=
{
?
i
? e
?y
i
F (x
i
)
i ? D
L
,
?
i
? e
?y?
i
F (x
i
)
i ? D
U
.
(9)
Intuitively, an unlabeled example that has a
high-confidence |F (x)| at the current step, will
probably receive more weight at the next step.
That is, similar instances become more impor-
tant when learning and mining subtrees. This
semi-supervised boosting learning iteratively gen-
erates pseudo-labels for unlabeled data and finds
the value of F that minimizes training errors (Ben-
nett et al, 2002). Also, the algorithm infers new
features from unlabeled data, and these features
are iteratively re-evaluated by the current ensem-
ble learner. We call this method SEMIBOOST.
6 Experiment
6.1 Setting
We describe specific settings used in our exper-
iment. Because we have no development set,
we set the maximum number of iterations K at
10,000. At most K patterns can be extracted, but
this seldom happens because duplicated patterns
are merged. Typical settings for semi-supervised
boosting are ?
i
= 1 and ?
i
= 0.5, that is, we
penalize the weights for unlabeled data.
For efficiency, BASELINE model used 10% of
the SWBD and MRDA data, selected at random.
We observed that this data set does not degrade the
results of semi-supervised speech act recognition.
For BOOTSTRAP and SEMIBOOST, we selected
k = 100 nearest neighbors of unlabeled exam-
ples for each labeled example using tree edit dis-
tance, and then used 24,625 (SWBD) and 54,961
(MRDA) sentences for the semi-supervised set-
ting.
All trees were combined as described in Section
4.3 (ALL model). In EMAIL and FORUM data we
added different types of discourse features: mes-
sage type (e.g., initial or reply posts), authorship
(e.g., an identification of 2nd or 3rd posts written
by the same author), and relative position of a sen-
tence. In Figure 1, for example, F1?3 is an initial
post, and F4?5 and F6 are reply posts. Moreover,
F1, F4, and F6 are the first sentence in each post.
Table 4: Results of speech act recognition on on-
line conversations; columns are micro-averaged
F
1
score with macro-averaged scores in parenthe-
ses. ?
?
? indicates that the result is significantly bet-
ter than BASELINE (p < 0.001).
Model EMAIL FORUM
BASELINE 78.87 (37.44) 78.93 (35.57)
BOOTSTRAP
?
83.11 (44.90) 79.09 (44.38)
SEMIBOOST
?
82.80 (44.64)
?
81.76 (44.21)
SUPERVISED 90.95 (75.71) 83.67 (40.68)
These features do not occur in SWBD or MRDA
because these are utterance-by-utterance conver-
sations.
6.2 Result and Discussion
First, we show that our method of semi-supervised
learning can improve modeling of the speech
act of emails and forums. As our baseline,
BASELINE achieved a micro-averaged F
1
score
of ? 79 for both data sets. This implies that
SWBD and MRDA data are useful for our prob-
lem. Using unlabeled data, semi-supervised meth-
ods BOOTSTRAP and SEMIBOOST perform bet-
ter than BASELINE (Table 4; Figure 5). To verify
our claim, we evaluated the supervised speech act
recognition on EMAIL and FORUM evaluation
sets with 5-fold cross validation (SUPERVISED in
Table 4). In particular, our semi-supervised speech
act recognition is competitive with the supervised
model in FORUM data.
The difference in performance between super-
vised results in EMAIL and FORUM seems to
indicate that the latter is a more difficult data
set. However, our SEMIBOOST method were able
to come close to the supervised FORUM results
(81.76 vs. 83.67). This is also close to the range of
supervised MRDA data set (F
1
= 83.49 for ALL,
Table 3). Moreover, we analyzed a main reason of
why transfer results were competitive in the FO-
RUM but not in the EMAIL. This might be due
to the mismatch in the unlabeled data, that is, we
used different email collections, the BC3 corpus
(email communication of W3C on w3.org sites),
for evaluation while used Enron data for adaption.
We also conjecture that the discrepancy between
EMAIL and FORUM is probably due to the more
heterogeneous nature of the FORUM data where
anyone can post and reply while EMAIL (Enron or
1256
(a) EMAIL (b) FORUM
Figure 5: Result of the semi-supervised learning method
BC3) might have a more fix set of participants.
The improvement of less frequent tags is promi-
nent, for example 25% for action motivator (AC),
40% for polite mechanism (P), and 15% for rhetor-
ical question (QR) error rate reductions were
achieved in FORUM data (Figure 5(b)). There-
fore, the semi-supervised learning method is more
effective with small amounts of labeled data (i.e.,
less frequent annotations). We believe that despite
their relative rarity, these speech acts are more im-
portant than the statement (S) in some applica-
tions, e.g., summarization.
Next, we give a qualitative analysis for better
interpretation of our problem and results. Due to
limited space, we focus on FORUM data, which
can potentially be applied to many applications.
Of the top ranked patterns extracted by SEMI-
BOOST (Figure 6(a)), subtree patterns of n-gram,
part-of-speech, dependency parse trees are most
discriminative. The patterns from unlabeled data
have relatively lower ranks, but this is not surpris-
ing. This indicates that BASELINE model provides
the base knowledge for semi-supervised speech
act recognition. Also, unlabeled data for EMAIL
and FORUM help to induce new patterns or ad-
just the model?s parameters. As a result, the semi-
supervised method is better than the BASELINE
when an identical number of patterns is modeled
(Figure 6(b)). For this result, we conclude that our
method successfully transfers knowledge from a
source domain (i.e., SWBD and MRDA) to a tar-
get domain (i.e., EMAIL and FORUM); hence it
can be a solution to the domain adaption problem.
Finally, we determine the main reasons for error
(in SEMIBOOST), to gain insights that may allow
development of better models in future work (Fig-
ure 6(c)). We sorted speech act tags by their se-
mantics and partitioned the confusion matrix into
question type (Q*) and statement, which are two
high-level speech acts. Most errors occur in the
similar categories, that is, language usage in ques-
tion discourse is definitely distinct from that in
statement discourse. From this analysis, we be-
lieve that more advanced techniques (e.g. two-
stage classification and learning with hierarchy-
augmented loss) can improve our model.
7 Conclusion
Despite the increasing interest in online text-based
conversations, no study to date has investigated
semi-supervised speech act recognition in email
and forum threads. This paper has addressed the
problem of learning to recognize speech acts us-
ing labeled and unlabeled data. We have also con-
tributed to the development of a novel applica-
tion of boosting subtree mining. Empirical results
have demonstrated that semi-supervised learning
of speech act recognition with subtree features im-
proves the performance in email and forum data
sets. An attractive future direction is to exploit
prior knowledge for semi-supervised speech act
recognition. Druck et al (2008) described gen-
eralized expectation criteria in which a discrimi-
native model can employ the labeled features and
unlabeled instances. Using prior knowledge, we
expect that our model will effectively learn useful
patterns from unlabeled data.
As work progresses on analyzing online text-
based conversations such as emails, forums, and
online chats, the importance of developing models
for discourse without annotating much new data
will become more important. In the future, we
plan to explore other related problems such as ad-
jacency pairs (Levinson, 1983) and discourse pars-
ing (Soricut and Marcu, 2003) for large-scale on-
line forum data.
1257
(a) Example patterns
0 2000 4000 6000
20
25
30
35
40
45
Number of base leaners
Erro
r Ra
te(%)
BASELINEBOOTSTRAPSEMIBOOST
(b) Learning behavior (c) Confusion matrix
Figure 6: Analysis on FORUM data
Acknowledgement
We would like to thank to anonymous reviewers
for their valuable comments, and Yunbo Cao, Wei
Lai, Xinying Song, Jingtian Jing, and Wei Wu for
their help in preparing our data.
References
R. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning Re-
search, 6:1817?1853.
J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dia-
log act segmentation and classification in multiparty
meetings. In Proceedings of ICASSP, pages 1061?
106.
J. Austin. 1962. How to Do Things With Words. Har-
vard Univ. Press, Cambridge, MA.
K.P. Bennett, A. Demiriz, and R. Maclin. 2002. Ex-
ploiting unlabeled data in ensemble methods. In
Proceedings of ACM SIGKDD, pages 289?296.
W.W. Cohen, V.R. Carvalho, and T. Mitchell. 2004.
Learning to classify email into ?speech acts?. In
Proceedings of EMNLP, pages 309?316.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.
2004. Meeting recorder project: Dialog act label-
ing guide. Technical report, International Computer
Science Institute.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of ACM SIGIR, pages
595?602.
D. Feng, E. Shaw, J. Kim, and E. H. Hovy. 2006.
Learning to detect conversation focus of threaded
discussions. In Proceedings of HLT-NAACL, pages
208?215.
M. Galley, K. McKeown, J. Hirschberg, and
E. Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of bayesian
networks to model pragmatic dependencies. In Pro-
ceedings of ACL.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP, pages 532?535.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL labeling project coder?s
manual, draft 13. Technical report, Univ. of Col-
orado Institute of Cognitive Science.
T. Kudo and Y. Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In
Proceedings of EMNLP, pages 301?308.
S. Levinson. 1983. Pragmatics. Cambridge Univ.
Press, Cambridge.
L. Mason, P. Bartlett, J. Baxter, and M. Frean. 2000.
Functional gradient techniques for combining hy-
potheses. In A.J. Smola, P.L. Bartlett, B. Sch?olkopf,
and D. Schuurmans, editors, Advances in Large
Margin Classifiers, pages 221?246. MIT Press,
Cambridge, MA.
K. McKeown, L. Shrestha, and O. Rambow. 2007. Us-
ing question-answer pairs in extractive summariza-
tion of email conversations. In Proceedings of CI-
CLing, volume 4394 of Lecture Notes in Computer
Science, pages 542?550.
J. Nastri, J. Pe na, and J. T. Hancock. 2006. The
construction of away messages: A speech act anal-
ysis. Journal of Computer-Mediated Communica-
tion, 11(4):article 7.
S. Ravi and J. Kim. 2007. Profiling student interac-
tions in threaded discussions with speech act classi-
fiers. In Proceedings of the AI in Education Confer-
ence.
1258
C. Ros?e, Y. Wang, Y. Cui, J. Arguello, K. Stegmann,
A. Weinberger, and F. Fischer. 2008. Analyzing
collaborative learning processes automatically: Ex-
ploiting the advances of computational linguistics in
computer-supported collaborative learning. Interna-
tional Journal of Computer-Supported Collabora-
tive Learning, 3(3):237?271.
R.E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):297?336.
J. Searle. 1969. Speech Acts. Cambridge Univ. Press,
Cambridge.
D. Shasha and K. Zhang. 1990. Fast algorithms for the
unit cost editing distance between trees. Journal of
Algorithms, 11(4):581?621.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL-HLT, pages 149?
156.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Computational Linguistics,
26(3):339?373.
G. Tur, D. Hakkani-T?ur, and R. E. Schapire. 2005.
Combining active and semi-supervised learning for
spoken language understanding. Speech Communi-
cation, 45(2):171?186.
D. P. Twitchell, J. F. Nunamaker, and J. K. Burgoon.
2004. Using speech act profiling for deception de-
tection. In Second Symposium on Intelligence and
Security Informatics, volume 3073 of Lecture Notes
in Computer Science, pages 403?410.
1259
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81?88,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Detecting Erroneous Sentences using Automatically Mined Sequential
Patterns
Guihua Sun ? Xiaohua Liu Gao Cong Ming Zhou
Chongqing University Microsoft Research Asia
sunguihua5018@163.com {xiaoliu, gaocong, mingzhou}@microsoft.com
Zhongyang Xiong John Lee ? Chin-Yew Lin
Chongqing University MIT Microsoft Research Asia
zyxiong@cqu.edu.cn jsylee@mit.edu cyl@microsoft.com
Abstract
This paper studies the problem of identify-
ing erroneous/correct sentences. The prob-
lem has important applications, e.g., pro-
viding feedback for writers of English as
a Second Language, controlling the quality
of parallel bilingual sentences mined from
the Web, and evaluating machine translation
results. In this paper, we propose a new
approach to detecting erroneous sentences
by integrating pattern discovery with super-
vised learning models. Experimental results
show that our techniques are promising.
1 Introduction
Detecting erroneous/correct sentences has the fol-
lowing applications. First, it can provide feedback
for writers of English as a Second Language (ESL)
as to whether a sentence contains errors. Second, it
can be applied to control the quality of parallel bilin-
gual sentences mined from the Web, which are criti-
cal sources for a wide range of applications, such as
statistical machine translation (Brown et al, 1993)
and cross-lingual information retrieval (Nie et al,
1999). Third, it can be used to evaluate machine
translation results. As demonstrated in (Corston-
Oliver et al, 2001; Gamon et al, 2005), the better
human reference translations can be distinguished
from machine translations by a classification model,
the worse the machine translation system is.
?Work done while the author was a visiting student at MSRA
?Work done while the author was a visiting student at MSRA
The previous work on identifying erroneous sen-
tences mainly aims to find errors from the writing of
ESL learners. The common mistakes (Yukio et al,
2001; Gui and Yang, 2003) made by ESL learners
include spelling, lexical collocation, sentence struc-
ture, tense, agreement, verb formation, wrong Part-
Of-Speech (POS), article usage, etc. The previous
work focuses on grammar errors, including tense,
agreement, verb formation, article usage, etc. How-
ever, little work has been done to detect sentence
structure and lexical collocation errors.
Some methods of detecting erroneous sentences
are based on manual rules. These methods (Hei-
dorn, 2000; Michaud et al, 2000; Bender et al,
2004) have been shown to be effective in detect-
ing certain kinds of grammatical errors in the writ-
ing of English learners. However, it could be ex-
pensive to write rules manually. Linguistic experts
are needed to write rules of high quality; Also, it
is difficult to produce and maintain a large num-
ber of non-conflicting rules to cover a wide range of
grammatical errors. Moreover, ESL writers of differ-
ent first-language backgrounds and skill levels may
make different errors, and thus different sets of rules
may be required. Worse still, it is hard to write rules
for some grammatical errors, for example, detecting
errors concerning the articles and singular plural us-
age (Nagata et al, 2006).
Instead of asking experts to write hand-crafted
rules, statistical approaches (Chodorow and Lea-
cock, 2000; Izumi et al, 2003; Brockett et al, 2006;
Nagata et al, 2006) build statistical models to iden-
tify sentences containing errors. However, existing
81
statistical approaches focus on some pre-defined er-
rors and the reported results are not attractive. More-
over, these approaches, e.g., (Izumi et al, 2003;
Brockett et al, 2006) usually need errors to be spec-
ified and tagged in the training sentences, which re-
quires expert help to be recruited and is time con-
suming and labor intensive.
Considering the limitations of the previous work,
in this paper we propose a novel approach that is
based on pattern discovery and supervised learn-
ing to successfully identify erroneous/correct sen-
tences. The basic idea of our approach is to build
a machine learning model to automatically classify
each sentence into one of the two classes, ?erro-
neous? and ?correct.? To build the learning model,
we automatically extract labeled sequential patterns
(LSPs) from both erroneous sentences and correct
sentences, and use them as input features for classi-
fication models. Our main contributions are:
? We mine labeled sequential patterns(LSPs)
from the preprocessed training data to build
leaning models. Note that LSPs are also very
different from N-gram language models that
only consider continuous sequences.
? We also enrich the LSP features with other auto-
matically computed linguistic features, includ-
ing lexical collocation, language model, syn-
tactic score, and function word density. In con-
trast with previous work focusing on (a spe-
cific type of) grammatical errors, our model can
handle a wide range of errors, including gram-
mar, sentence structure, and lexical choice.
? We empirically evaluate our methods on two
datasets consisting of sentences written by
Japanese and Chinese, respectively. Experi-
mental results show that labeled sequential pat-
terns are highly useful for the classification
results, and greatly outperform other features.
Our method outperforms Microsoft Word03
and ALEK (Chodorow and Leacock, 2000)
from Educational Testing Service (ETS) in
some cases. We also apply our learning model
to machine translation (MT) data as a comple-
mentary measure to evaluate MT results.
The rest of this paper is organized as follows.
The next section discusses related work. Section 3
presents the proposed technique. We evaluate our
proposed technique in Section 4. Section 5 con-
cludes this paper and discusses future work.
2 Related Work
Research on detecting erroneous sentences can be
classified into two categories. The first category
makes use of hand-crafted rules, e.g., template
rules (Heidorn, 2000) and mal-rules in context-free
grammars (Michaud et al, 2000; Bender et al,
2004). As discussed in Section 1, manual rule based
methods have some shortcomings.
The second category uses statistical techniques
to detect erroneous sentences. An unsupervised
method (Chodorow and Leacock, 2000) is em-
ployed to detect grammatical errors by inferring
negative evidence from TOEFL administrated by
ETS. The method (Izumi et al, 2003) aims to de-
tect omission-type and replacement-type errors and
transformation-based leaning is employed in (Shi
and Zhou, 2005) to learn rules to detect errors for
speech recognition outputs. They also require spec-
ifying error tags that can tell the specific errors
and their corrections in the training corpus. The
phrasal Statistical Machine Translation (SMT) tech-
nique is employed to identify and correct writing er-
rors (Brockett et al, 2006). This method must col-
lect a large number of parallel corpora (pairs of er-
roneous sentences and their corrections) and perfor-
mance depends on SMT techniques that are not yet
mature. The work in (Nagata et al, 2006) focuses
on a type of error, namely mass vs. count nouns.
In contrast to existing statistical methods, our tech-
nique needs neither errors tagged nor parallel cor-
pora, and is not limited to a specific type of gram-
matical error.
There are also studies on automatic essay scoring
at document-level. For example, E-rater (Burstein
et al, 1998), developed by the ETS, and Intelligent
Essay Assessor (Foltz et al, 1999). The evaluation
criteria for documents are different from those for
sentences. A document is evaluated mainly by its or-
ganization, topic, diversity of vocabulary, and gram-
mar while a sentence is done by grammar, sentence
structure, and lexical choice.
Another related work is Machine Translation (MT)
evaluation. Classification models are employed
in (Corston-Oliver et al, 2001; Gamon et al, 2005)
82
to evaluate the well-formedness of machine transla-
tion outputs. The writers of ESL and MT normally
make different mistakes: in general, ESL writers can
write overall grammatically correct sentences with
some local mistakes while MT outputs normally pro-
duce locally well-formed phrases with overall gram-
matically wrong sentences. Hence, the manual fea-
tures designed for MT evaluation are not applicable
to detect erroneous sentences from ESL learners.
LSPs differ from the traditional sequential pat-
terns, e.g., (Agrawal and Srikant, 1995; Pei et al,
2001) in that LSPs are attached with class labels and
we prefer those with discriminating ability to build
classification model. In our other work (Sun et al,
2007), labeled sequential patterns, together with la-
beled tree patterns, are used to build pattern-based
classifier to detect erroneous sentences. The clas-
sification method in (Sun et al, 2007) is different
from those used in this paper. Moreover, instead of
labeled sequential patterns, in (Sun et al, 2007) the
most significant k labeled sequential patterns with
constraints for each training sentence are mined to
build classifiers. Another related work is (Jindal and
Liu, 2006), where sequential patterns with labels are
used to identify comparative sentences.
3 Proposed Technique
This section first gives our problem statement and
then presents our proposed technique to build learn-
ing models.
3.1 Problem Statement
In this paper we study the problem of identifying
erroneous/correct sentences. A set of training data
containing correct and erroneous sentences is given.
Unlike some previous work, our technique requires
neither that the erroneous sentences are tagged with
detailed errors, nor that the training data consist of
parallel pairs of sentences (an error sentence and its
correction). The erroneous sentence contains a wide
range of errors on grammar, sentence structure, and
lexical choice. We do not consider spelling errors in
this paper.
We address the problem by building classifica-
tion models. The main challenge is to automatically
extract representative features for both correct and
erroneous sentences to build effective classification
models. We illustrate the challenge with an exam-
ple. Consider an erroneous sentence, ?If Maggie will
go to supermarket, she will buy a bag for you.? It is
difficult for previous methods using statistical tech-
niques to capture such an error. For example, N-
gram language model is considered to be effective
in writing evaluation (Burstein et al, 1998; Corston-
Oliver et al, 2001). However, it becomes very ex-
pensive if N > 3 and N-grams only consider contin-
uous sequence of words, which is unable to detect
the above error ?if...will...will?.
We propose labeled sequential patterns to effec-
tively characterize the features of correct and er-
roneous sentences (Section 3.2), and design some
complementary features ( Section 3.3).
3.2 Mining Labeled Sequential Patterns ( LSP )
Labeled Sequential Patterns (LSP). A labeled se-
quential pattern, p, is in the form of LHS? c, where
LHS is a sequence and c is a class label. Let I be a
set of items and L be a set of class labels. Let D be a
sequence database in which each tuple is composed
of a list of items in I and a class label in L. We say
that a sequence s1 =< a1, ..., am > is contained in
a sequence s2 =< b1, ..., bn > if there exist integers
i1, ...im such that 1 ? i1 < i2 < ... < im ? n and
aj = bij for all j ? 1, ...,m. Similarly, we say that
a LSP p1 is contained by p2 if the sequence p1.LHS
is contained by p2.LHS and p1.c = p2.c. Note that
it is not required that s1 appears continuously in s2.
We will further refine the definition of ?contain? by
imposing some constraints (to be explained soon).
A LSP p is attached with two measures, support and
confidence. The support of p, denoted by sup(p),
is the percentage of tuples in database D that con-
tain the LSP p. The probability of the LSP p being
true is referred to as ?the confidence of p ?, denoted
by conf(p), and is computed as sup(p)sup(p.LHS) . The
support is to measure the generality of the pattern p
and minimum confidence is a statement of predictive
ability of p.
Example 1: Consider a sequence database contain-
ing three tuples t1 = (< a, d, e, f >,E), t2 = (<
a, f, e, f >,E) and t3 = (< d, a, f >,C). One
example LSP p1 = < a, e, f >? E, which is con-
tained in tuples t1 and t2. Its support is 66.7% and
its confidence is 100%. As another example, LSP p2
83
= < a, f >? E with support 66.7% and confidence
66.7%. p1 is a better indication of class E than p2.
2
Generating Sequence Database. We generate the
database by applying Part-Of-Speech (POS) tagger
to tag each training sentence while keeping func-
tion words1 and time words2. After the process-
ing, each sentence together with its label becomes
a database tuple. The function words and POS tags
play important roles in both grammars and sentence
structures. In addition, the time words are key
clues in detecting errors of tense usage. The com-
bination of them allows us to capture representative
features for correct/erroneous sentences by mining
LSPs. Some example LSPs include ?<a, NNS> ?
Error?(singular determiner preceding plural noun),
and ?<yesterday, is>?Error?. Note that the con-
fidences of these LSPs are not necessary 100%.
First, we use MXPOST-Maximum Entropy Part of
Speech Tagger Toolkit3 for POS tags. The MXPOST
tagger can provide fine-grained tag information. For
example, noun can be tagged with ?NN?(singular
noun) and ?NNS?(plural noun); verb can be tagged
with ?VB?, ?VBG?, ?VBN?, ?VBP?, ?VBD? and
?VBZ?. Second, the function words and time words
that we use form a key word list. If a word in a
training sentence is not contained in the key word
list, then the word will be replaced by its POS. The
processed sentence consists of POS and the words of
key word list. For example, after the processing, the
sentence ?In the past, John was kind to his sister? is
converted into ?In the past, NNP was JJ to his NN?,
where the words ?in?, ?the?, ?was?, ?to? and ?his?
are function words, the word ?past? is time word,
and ?NNP?, ?JJ?, and ?NN? are POS tags.
Mining LSPs. The length of the discovered LSPs
is flexible and they can be composed of contiguous
or distant words/tags. Existing frequent sequential
pattern mining algorithms (e.g. (Pei et al, 2001))
use minimum support threshold to mine frequent se-
quential patterns whose support is larger than the
threshold. These algorithms are not sufficient for our
problem of mining LSPs. In order to ensure that all
our discovered LSPs are discriminating and are capa-
1http://www.marlodge.supanet.com/museum/funcword.html
2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html
3http://www.cogsci.ed.ac.uk/?jamesc/taggers/MXPOST.html
ble of predicting correct or erroneous sentences, we
impose another constraint minimum confidence. Re-
call that the higher the confidence of a pattern is, the
better it can distinguish between correct sentences
and erroneous sentences. In our experiments, we
empirically set minimum support at 0.1% and mini-
mum confidence at 75%.
Mining LSPs is nontrivial since its search space
is exponential, althought there have been a host of
algorithms for mining frequent sequential patterns.
We adapt the frequent sequence mining algorithm
in (Pei et al, 2001) for mining LSPs with constraints.
Converting LSPs to Features. Each discovered LSP
forms a binary feature as the input for classification
model. If a sentence includes a LSP, the correspond-
ing feature is set at 1.
The LSPs can characterize the correct/erroneous
sentence structure and grammar. We give some ex-
amples of the discovered LSPs. (1) LSPs for erro-
neous sentences. For example, ?<this, NNS>?(e.g.
contained in ?this books is stolen.?), ?<past,
is>?(e.g. contained in ?in the past, John is kind to
his sister.?), ?<one, of, NN>?(e.g. contained in ?it is
one of important working language?, ?<although,
but>?(e.g. contained in ?although he likes it, but
he can?t buy it.?), and ?<only, if, I, am>?(e.g. con-
tained in ?only if my teacher has given permission,
I am allowed to enter this room?). (2) LSPs for cor-
rect sentences. For instance, ?<would, VB>?(e.g.
contained in ?he would buy it.?), and ?<VBD,
yeserday>?(e.g. contained in ?I bought this book
yesterday.?).
3.3 Other Linguistic Features
We use some linguistic features that can be com-
puted automatically as complementary features.
Lexical Collocation (LC) Lexical collocation er-
ror (Yukio et al, 2001; Gui and Yang, 2003) is com-
mon in the writing of ESL learners, such as ?strong
tea? but not ?powerful tea.? Our LSP features can-
not capture all LCs since we replace some words
with POS tags in mining LSPs. We collect five types
of collocations: verb-object, adjective-noun, verb-
adverb, subject-verb, and preposition-object from a
general English corpus4. Correct LCs are collected
4The general English corpus consists of about 4.4 million
native sentences.
84
by extracting collocations of high frequency from
the general English corpus. Erroneous LC candi-
dates are generated by replacing the word in correct
collocations with its confusion words, obtained from
WordNet, including synonyms and words with sim-
ilar spelling or pronunciation. Experts are consulted
to see if a candidate is a true erroneous collocation.
We compute three statistical features for each sen-
tence below. (1) The first feature is computed by
m?
i=1
p(coi)/n, where m is the number of CLs, n is
the number of collocations in each sentence, and
probability p(coi) of each CL coi is calculated us-
ing the method (Lu? and Zhou, 2004). (2) The sec-
ond feature is computed by the ratio of the number
of unknown collocations (neither correct LCs nor er-
roneous LCs) to the number of collocations in each
sentence. (3) The last feature is computed by the ra-
tio of the number of erroneous LCs to the number of
collocations in each sentence.
Perplexity from Language Model (PLM) Perplex-
ity measures are extracted from a trigram language
model trained on a general English corpus using
the SRILM-SRI Language Modeling Toolkit (Stolcke,
2002). We calculate two values for each sentence:
lexicalized trigram perplexity and part of speech
(POS) trigram perplexity. The erroneous sentences
would have higher perplexity.
Syntactic Score (SC) Some erroneous sentences of-
ten contain words and concepts that are locally cor-
rect but cannot form coherent sentences (Liu and
Gildea, 2005). To measure the coherence of sen-
tences, we use a statistical parser Toolkit (Collins,
1997) to assign each sentence a parser?s score that
is the related log probability of parsing. We assume
that erroneous sentences with undesirable sentence
structures are more likely to receive lower scores.
Function Word Density (FWD) We consider the
density of function words (Corston-Oliver et al,
2001), i.e. the ratio of function words to content
words. This is inspired by the work (Corston-Oliver
et al, 2001) showing that function word density can
be effective in distinguishing between human refer-
ences and machine outputs. In this paper, we calcu-
late the densities of seven kinds of function words 5
5including determiners/quantifiers, all pronouns, different
pronoun types: Wh, 1st, 2nd, and 3rd person pronouns, prepo-
Dataset Type Source Number
JC
(+) the Japan Times newspaperand Model English Essay 16,857
(-)
HEL (Hiroshima English
Learners? Corpus) and JLE
(Japanese Learners of En-
glish Corpus)
17,301
CC (+) the 21st Century newspaper 3,200
(-)
CLEC (Chinese Learner Er-
ror Corpus) 3,199
Table 1: Corpora ((+): correct; (-): erroneous)
respectively as 7 features.
4 Experimental Evaluation
We evaluated the performance of our techniques
with support vector machine (SVM) and Naive
Bayesian (NB) classification models. We also com-
pared the effectiveness of various features. In ad-
dition, we compared our technique with two other
methods of checking errors, Microsoft Word03 and
ALEK method (Chodorow and Leacock, 2000). Fi-
nally, we also applied our technique to evaluate the
Machine Translation outputs.
4.1 Experimental Setup
Classification Models. We used two classification
models, SVM6 and NB classification model.
Data. We collected two datasets from different do-
mains, Japanese Corpus (JC) and Chinese Corpus
(CC). Table 1 gives the details of our corpora. In
the learner?s corpora, all of the sentences are erro-
neous. Note that our data does not consist of parallel
pairs of sentences (one error sentence and its correc-
tion). The erroneous sentences includes grammar,
sentence structure and lexical choice errors, but not
spelling errors.
For each sentence, we generated five kinds of fea-
tures as presented in Section 3. For a non-binary
feature X , its value x is normalized by z-score,
norm(x) = x?mean(X)?var(X) , where mean(x) is the em-
pirical mean of X and var(X) is the variance of X .
Thus each sentence is represented by a vector.
Metrics We calculated the precision, recall,
and F-score for correct and erroneous sentences,
respectively, and also report the overall accuracy.
sitions and adverbs, auxiliary verbs, and conjunctions.
6http://svmlight.joachims.org/
85
All the experimental results are obtained thorough
10-fold cross-validation.
4.2 Experimental Results
The Effectiveness of Various Features. The exper-
iment is to evaluate the contribution of each feature
to the classification. The results of SVM are given in
Table 2. We can see that the performance of labeled
sequential patterns (LSP) feature consistently out-
performs those of all the other individual features. It
also performs better even if we use all the other fea-
tures together. This is because other features only
provide some relatively abstract and simple linguis-
tic information, whereas the discovered LSP s char-
acterize significant linguistic features as discussed
before. We also found that the results of NB are a
little worse than those of SVM. However, all the fea-
tures perform consistently on the two classification
models and we can observe the same trend. Due to
space limitation, we do not give results of NB.
In addition, the discovered LSPs themselves are
intuitive and meaningful since they are intuitive fea-
tures that can distinguish correct sentences from er-
roneous sentences. We discovered 6309 LSPs in
JC data and 3742 LSPs in CC data. Some exam-
ple LSPs discovered from erroneous sentences are
<a, NNS> (support:0.39%, confidence:85.71%),
<to, VBD> (support:0.11%, confidence:84.21%),
and <the, more, the, JJ> (support:0.19%, confi-
dence:0.93%) 7; Similarly, we also give some exam-
ple LSPs mined from correct sentences: <NN, VBZ>
(support:2.29%, confidence:75.23%), and <have,
VBN, since> (support:0.11%, confidence:85.71%)
8. However, other features are abstract and it is hard
to derive some intuitive knowledge from the opaque
statistical values of these features.
As shown in Table 2, our technique achieves
the highest accuracy, e.g. 81.75% on the Japanese
dataset, when we use all the features. However, we
also notice that the improvement is not very signif-
icant compared with using LSP feature individually
(e.g. 79.63% on the Japanese dataset). The similar
results are observed when we combined the features
PLM, SC, FWD, and LC. This could be explained
7a + plural noun; to + past tense format; the more + the +
base form of adjective
8singular or mass noun + the 3rd person singular present
format; have + past participle format + since
by two reasons: (1) A sentence may contain sev-
eral kinds of errors. A sentence detected to be er-
roneous by one feature may also be detected by an-
other feature; and (2) Various features give conflict-
ing results. The two aspects suggest the directions
of our future efforts to improve the performance of
our models.
Comparing with Other Methods. It is difficult
to find benchmark methods to compare with our
technique because, as discussed in Section 2, exist-
ing methods often require error tagged corpora or
parallel corpora, or focus on a specific type of er-
rors. In this paper, we compare our technique with
the grammar checker of Microsoft Word03 and the
ALEK (Chodorow and Leacock, 2000) method used
by ETS. ALEK is used to detect inappropriate usage
of specific vocabulary words. Note that we do not
consider spelling errors. Due to space limitation, we
only report the precision, recall, F-score
for erroneous sentences, and the overall accuracy.
As can be seen from Table 3, our method out-
performs the other two methods in terms of over-
all accuracy, F-score, and recall, while the three
methods achieve comparable precision. We realize
that the grammar checker of Word is a general tool
and the performance of ALEK (Chodorow and Lea-
cock, 2000) can be improved if larger training data is
used. We found that Word and ALEK usually cannot
find sentence structure and lexical collocation errors,
e.g., ?The more you listen to English, the easy it be-
comes.? contains the discovered LSP <the, more, the,
JJ>? Error.
Cross-domain Results. To study the performance
of our method on cross-domain data from writers
of the same first-language background, we collected
two datasets from Japanese writers, one is composed
of 694 parallel sentences (+:347, -:347), and the
other 1,671 non-parallel sentences (+:795, -:876).
The two datasets are used as test data while we use
JC dataset for training. Note that the test sentences
come from different domains from the JC data. The
results are given in the first two rows of Table 4. This
experiment shows that our leaning model trained for
one domain can be effectively applied to indepen-
dent data in the other domains from the writes of the
same first-language background, no matter whether
the test data is parallel or not. We also noticed that
86
Dataset Feature A (-)F (-)R (-)P (+)F (+)R (+)P
JC
LSP 79.63 80.65 85.56 76.29 78.49 73.79 83.85
LC 69.55 71.72 77.87 66.47 67.02 61.36 73.82
PLM 61.60 55.46 50.81 64.91 62 70.28 58.43
SC 53.66 57.29 68.40 56.12 34.18 39.04 32.22
FWD 68.01 72.82 86.37 62.95 61.14 49.94 78.82
LC + PLM + SC + FWD 71.64 73.52 79.38 68.46 69.48 64.03 75.94
LSP + LC + PLM + SC + FWD 81.75 81.60 81.46 81.74 81.90 82.04 81.76
CC
LSP 78.19 76.40 70.64 83.20 79.71 85.72 74.50
LC 63.82 62.36 60.12 64.77 65.17 67.49 63.01
PLM 55.46 64.41 80.72 53.61 40.41 30.22 61.30
SC 50.52 62.58 87.31 50.64 13.75 14.33 13.22
FWD 61.36 60.80 60.70 60.90 61.90 61.99 61.80
LC + PLM + SC + FWD 67.69 67.62 67.51 67.77 67.74 67.87 67.64
LSP + LC + PLM + SC + FWD 79.81 78.33 72.76 84.84 81.10 86.92 76.02
Table 2: The Experimental Results (A: overall accuracy; (-): erroneous sentences; (+): correct sentences; F:
F-score; R: recall; P: precision)
Dataset Model A (-)F (-)R (-)P
JC
Ours 81.39 81.25 81.24 81.28
Word 58.87 33.67 21.03 84.73
ALEK 54.69 20.33 11.67 78.95
CC
Ours 79.14 77.81 73.17 83.09
Word 58.47 32.02 19.81 84.22
ALEK 55.21 22.83 13.42 76.36
Table 3: The Comparison Results
LSPs play dominating role in achieving the results.
Due to space limitation, no details are reported.
To further see the performance of our method
on data written by writers with different first-
language backgrounds, we conducted two experi-
ments. (1) We merge the JC dataset and CC dataset.
The 10-fold cross-validation results on the merged
dataset are given in the third row of Table 4. The
results demonstrate that our models work well when
the training data and test data contain sentences from
different first-language backgrounds. (2) We use the
JC dataset (resp. CC dataset) for training while the
CC dataset (resp. JC dataset) is used as test data. As
shown in the fourth (resp. fifth) row of Table 4, the
results are worse than their corresponding results of
Word given in Table 3. The reason is that the mis-
takes made by Japanese and Chinese are different,
thus the learning model trained on one data does not
work well on the other data. Note that our method is
not designed to work in this scenario.
Application to Machine Translation Evaluation.
Our learning models could be used to evaluate the
MT results as an complementary measure. This is
based on the assumption that if the MT results can
be accurately distinguished from human references
Dataset A (-)F (-)R (-)P
JC(Train)+nonparallel(Test) 72.49 68.55 57.51 84.84
JC(Train)+parallel(Test) 71.33 69.53 65.42 74.18
JC + CC 79.98 79.72 79.24 80.23
JC(Train)+ CC(Test) 55.62 41.71 31.32 62.40
CC(Train)+ JC(Test) 57.57 23.64 16.94 39.11
Table 4: The Cross-domain Results of our Method
by our technique, the MT results are not natural and
may contain errors as well.
The experiment was conducted using 10-fold
cross validation on two LDC data, low-ranked and
high-ranked data9. The results using SVM as classi-
fication model are given in Table 5. As expected, the
classification accuracy on low-ranked data is higher
than that on high-ranked data since low-ranked MT
results are more different from human references
than high-ranked MT results. We also found that
LSPs are the most effective features. In addition, our
discovered LSPs could indicate the common errors
made by the MT systems and provide some sugges-
tions for improving machine translation results.
As a summary, the mined LSPs are indeed effec-
tive for the classification models and our proposed
technique is effective.
5 Conclusions and Future Work
This paper proposed a new approach to identifying
erroneous/correct sentences. Empirical evaluating
using diverse data demonstrated the effectiveness of
9One LDC data contains 14,604 low ranked (score 1-3) ma-
chine translations and the corresponding human references; the
other LDC data contains 808 high ranked (score 3-5) machine
translations and the corresponding human references
87
Data Feature A (-)F (-)R (-)P (+)F (+)R (+)P
Low-ranked data (1-3 score) LSP 84.20 83.95 82.19 85.82 84.44 86.25 82.73
LSP+LC+PLM+SC+FWD 86.60 86.84 88.96 84.83 86.35 84.27 88.56
High-ranked data (3-5 score) LSP 71.74 73.01 79.56 67.59 70.23 64.47 77.40
LSP+LC+PLM+SC+FWD 72.87 73.68 68.95 69.20 71.92 67.22 77.60
Table 5: The Results on Machine Translation Data
our techniques. Moreover, we proposed to mine
LSPs as the input of classification models from a set
of data containing correct and erroneous sentences.
The LSPs were shown to be much more effective than
the other linguistic features although the other fea-
tures were also beneficial.
We will investigate the following problems in the
future: (1) to make use of the discovered LSPs to pro-
vide detailed feedback for ESL learners, e.g. the er-
rors in a sentence and suggested corrections; (2) to
integrate the features effectively to achieve better re-
sults; (3) to further investigate the application of our
techniques for MT evaluation.
References
Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining se-
quential patterns. In ICDE.
Emily M. Bender, Dan Flickinger, Stephan Oepen, Annemarie
Walsh, and Timothy Baldwin. 2004. Arboretum: Using a
precision grammar for grammmar checking in call. In Proc.
InSTIL/ICALL Symposium on Computer Assisted Learning.
Chris Brockett, William Dolan, and Michael Gamon. 2006.
Correcting esl errors using phrasal smt techniques. In ACL.
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Martin
Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998.
Automated scoring using a hybrid feature identification tech-
nique. In Proc. ACL.
Martin Chodorow and Claudia Leacock. 2000. An unsuper-
vised method for detecting grammatical errors. In NAACL.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. ACL.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proc. ACL.
P.W. Foltz, D. Laham, and T.K. Landauer. 1999. Automated
essay scoring: Application to educational technology. In Ed-
Media ?99.
Michael Gamon, Anthony Aue, and Martine Smets. 2005.
Sentence-level mt evaluation without reference translations:
Beyond language modeling. In Proc. EAMT.
Shicun Gui and Huizhong Yang. 2003. Zhongguo Xuexizhe
Yingyu Yuliaohu. (Chinese Learner English Corpus). Shang-
hai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).
George E. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert Dale,
Hermann Moisi and Harold Somers (ed.). Marcel Dekker.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Sup-
nithi, and Hitoshi Isahara. 2003. Automatic error detection
in the japanese learners? english spoken data. In Proc. ACL.
Nitin Jindal and Bing Liu. 2006. Identifying comparative sen-
tences in text documents. In SIGIR.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In Proc. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization.
Yajuan Lu? and Ming Zhou. 2004. Collocation translation ac-
quisition using monolingual corpora. In Proc. ACL.
Lisa N. Michaud, Kathleen F. McCoy, and Christopher A. Pen-
nington. 2000. An intelligent tutoring system for deaf learn-
ers of written english. In Proc. 4th International ACM Con-
ference on Assistive Technologies.
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and Naoki Isu.
2006. A feedback-augmented method for detecting errors in
the writing of learners of english. In Proc. ACL.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Du-
rand. 1999. Cross-language information retrieval based on
parallel texts and automatic mining of parallel texts from the
web. In SIGIR, pages 74?81.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, and Helen Pinto.
2001. Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. ICDE.
Yongmei Shi and Lina Zhou. 2005. Error detection using lin-
guistic features. In HLT/EMNLP.
Andreas Stolcke. 2002. Srilm-an extensible language modeling
toolkit. In Proc. ICSLP.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and Ming
Zhou. 2007. Mining sequential patterns and tree patterns to
detect erroneous sentences. In AAAI.
Tono Yukio, T. Kaneko, H. Isahara, T. Saiga, and E. Izumi.
2001. The standard speaking test corpus: A 1 million-word
spoken corpus of japanese learners of english and its impli-
cations for l2 lexicography. In ASIALEX: Asian Bilingualism
and the Dictionary.
88
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024?1031,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Topic Analysis for Psychiatric Document Retrieval 
Liang-Chih Yu*?, Chung-Hsien Wu*, Chin-Yew Lin?, Eduard Hovy? and Chia-Ling Lin*
*Department of CSIE, National Cheng Kung University, Tainan, Taiwan, R.O.C. 
?Microsoft Research Asia, Beijing, China 
?Information Sciences Institute, University of Southern California, Marina del Rey, CA, USA 
liangchi@isi.edu,{chwu,totalwhite}@csie.ncku.edu.tw,cyl@microsoft.com,hovy@isi.edu 
 
 
Abstract 
Psychiatric document retrieval attempts to 
help people to efficiently and effectively 
locate the consultation documents relevant 
to their depressive problems. Individuals 
can understand how to alleviate their symp-
toms according to recommendations in the 
relevant documents. This work proposes 
the use of high-level topic information ex-
tracted from consultation documents to im-
prove the precision of retrieval results. The 
topic information adopted herein includes 
negative life events, depressive symptoms 
and semantic relations between symptoms, 
which are beneficial for better understand-
ing of users' queries. Experimental results 
show that the proposed approach achieves 
higher precision than the word-based re-
trieval models, namely the vector space 
model (VSM) and Okapi model, adopting 
word-level information alone. 
1 Introduction 
Individuals may suffer from negative or stressful 
life events, such as death of a family member, ar-
gument with a spouse and loss of a job. Such 
events play an important role in triggering depres-
sive symptoms, such as depressed moods, suicide 
attempts and anxiety. Individuals under these cir-
cumstances can consult health professionals using 
message boards and other services. Health profes-
sionals respond with suggestions as soon as possi-
ble. However, the response time is generally sev-
eral days, depending on both the processing time 
required by health professionals and the number of 
problems to be processed. Such a long response 
time is unacceptable, especially for patients suffer-
ing from psychiatric emergencies such as suicide 
attempts. A potential solution considers the prob-
lems that have been processed and the correspond-
ing suggestions, called consultation documents, as 
the psychiatry web resources. These resources gen-
erally contain thousands of consultation documents 
(problem-response pairs), making them a useful 
information resource for mental health care and 
prevention. By referring to the relevant documents, 
individuals can become aware that they are not 
alone because many people have suffered from the 
same or similar problems. Additionally, they can 
understand how to alleviate their symptoms ac-
cording to recommendations. However, browsing 
and searching all consultation documents to iden-
tify the relevant documents is time consuming and 
tends to become overwhelming. Individuals need 
to be able to retrieve the relevant consultation 
documents efficiently and effectively. Therefore, 
this work presents a novel mechanism to automati-
cally retrieve the relevant consultation documents 
with respect to users' problems. 
Traditional information retrieval systems repre-
sent queries and documents using a bag-of-words 
approach. Retrieval models, such as the vector 
space model (VSM) (Baeza-Yates and Ribeiro-
Neto, 1999) and Okapi model (Robertson et al, 
1995; Robertson et al, 1996; Okabe et al, 2005), 
are then adopted to estimate the relevance between 
queries and documents. The VSM represents each 
query and document as a vector of words, and 
adopts the cosine measure to estimate their rele-
vance. The Okapi model, which has been used on 
the Text REtrieval Conference (TREC) collections, 
developed a family of word-weighting functions 
1024
for relevance estimation. These functions consider 
word frequencies and document lengths for word 
weighting. Both the VSM and Okapi models esti-
mate the relevance by matching the words in a 
query with the words in a document. Additionally, 
query words can further be expanded by the con-
cept hierarchy within general-purpose ontologies 
such as WordNet (Fellbaum, 1998), or automati-
cally constructed ontologies (Yeh et al, 2004). 
However, such word-based approaches only 
consider the word-level information in queries and 
documents, ignoring the high-level topic informa-
tion that can help improve understanding of users' 
queries. Consider the example consultation docu-
ment in Figure 1. A consultation document com-
prises two parts: the query part and recommenda-
tion part. The query part is a natural language text, 
containing rich topic information related to users' 
depressive problems. The topic information in-
cludes negative life events, depressive symptoms, 
and semantic relations between symptoms. As in-
dicated in Figure 1, the subject suffered from a 
love-related event, and several depressive symp-
toms, such as <Depressed>, <Suicide>, <Insom-
nia> and <Anxiety>. Moreover, there is a cause-
effect relation holding between <Depressed> and 
<Suicide>, and a temporal relation holding be-
tween <Depressed> and <Insomnia>. Different 
topics may lead to different suggestions decided by 
experts. Therefore, an ideal retrieval system for 
consultation documents should consider such topic 
information so as to improve the retrieval precision. 
Natural language processing (NLP) techniques 
can be used to extract more precise information 
from natural language texts (Wu et al, 2005a; Wu 
et al, 2005b; Wu et al, 2006; Yu et al, 2007). 
This work adopts the methodology presented in 
(Wu et al 2005a) to extract depressive symptoms 
and their relations, and adopts the pattern-based 
method presented in (Yu et al, 2007) to extract 
negative life events from both queries and consul-
tation documents. This work also proposes a re-
trieval model to calculate the similarity between a 
query and a document by combining the similari-
ties of the extracted topic information. 
The rest of this work is organized as follows. 
Section 2 briefly describes the extraction of topic 
information. Section 3 presents the retrieval model. 
Section 4 summarizes the experimental results. 
Conclusions are finally drawn in Section 5. 
2 Framework of Consultation Document 
Retrieval 
Figure 2 shows the framework of consultation 
document retrieval. The retrieval process begins 
with receiving a user?s query about his depressive 
problems in natural language. The example query 
is shown in Figure 1. The topic information is then 
extracted from the query, as shown in the center of 
Figure 2. The extracted topic information is repre-
Consultation DocumentQuery:
It's normal to feel this way when going through these kinds of struggles, but over 
time your emotions should level out. Suicide doesn't solve anything; think about 
how it would affect your family........ There are a few things you can try to help 
you get to sleep at night, like drinking warm milk, listening to relaxing music....... 
Recommendation:
After that, it took me a long time to fall asleep at night.  
<Depressed>
<Suicide>
<Insomnia>
<Anxiety>
cause-effect temporal
I broke up with my boyfriend.
I often felt like crying and felt pain every day. 
So, I tried to kill myself several times. 
In recent months, I often lose my temper for no reason.
 
Figure 1.  Example of a consultation document. The bold arrowed lines denote cause-effect relations; ar-
rowed lines denote temporal relations; dashed lines denote temporal boundaries, and angle brackets de-
note depressive symptoms 
1025
sented by the sets of negative life events, depres-
sive symptoms, and semantic relations. Each ele-
ment in the event set and symptom set denotes an 
individual event and symptom, respectively, while 
each element in the relation set denotes a symptom 
chain to retain the order of symptoms. Similarly, 
the query parts of consultation documents are rep-
resented in the same manner. The relevance esti-
mation then calculates the similarity between the 
input query and the query part of each consultation 
document by combining the similarities of the sets 
of events, symptoms, and relations within them. 
Finally, a list of consultation documents ranked in 
the descending order of similarities is returned to 
the user. 
In the following, the extraction of topic informa-
tion is described briefly. The detailed process is 
described in (Wu et al 2005a) for symptom and 
relation identification, and in (Yu et al, 2007) for 
event identification. 
1) Symptom identification: A total of 17 symp-
toms are defined based on the Hamilton De-
pression Rating Scale (HDRS) (Hamilton, 
1960). The identification of symptoms is sen-
tence-based. For each sentence, its structure is 
first analyzed by a probabilistic context free 
grammar (PCFG), built from the Sinica Tree-
bank corpus developed by Academia Sinica, 
Taiwan (http://treebank.sinica.edu.tw), to gen-
erate a set of dependencies between word to-
kens. Each dependency has the format (modi-
fier, head, relmodifier,head). For instance, the de-
pendency (matters, worry about, goal) means 
that "matters" is the goal to the head of the sen-
tence "worry about". Each sentence can then 
be associated with a symptom based on the 
probabilities that dependencies occur in all 
symptoms, which are obtained from a set of 
training sentences. 
2) Relation Identification: The semantic rela-
tions of interest include cause-effect and tem-
poral relations. After the symptoms are ob-
tained, the relations holding between symp-
toms (sentences) are identified by a set of dis-
course markers. For instance, the discourse 
markers "because" and "therefore" may signal 
cause-effect relations, and "before" and "after" 
may signal temporal relations. 
3) Negative life event identification: A total of 5 
types of events, namely <Family>, <Love>, 
<School>, <Work> and <Social> are defined 
based on Pagano et als (2004) research. The 
identification of events is a pattern-based ap-
proach. A pattern denotes a semantically plau-
sible combination of words, such as <parents, 
divorce> and <exam, fail>. First, a set of pat-
terns is acquired from psychiatry web corpora 
by using an evolutionary inference algorithm. 
The event of each sentence is then identified 
by using an SVM classifier with the acquired 
patterns as features. 
3 Retrieval Model 
The similarity between a query and a document, 
( , )Sim q d , is calculated by combining the similari-
ties of the sets of events, symptoms and relations 
within them, as shown in (1). 
Consultation
Documents
Ranking
Relevance
Estimation
Query
(Figure 1)
Topic Information
Symptom 
Identification
Negative Life Event
Identification
Relation
Identification
D S A
D S Cause-Effect
D I A
Temporal
I
S I A
<Love>
Topic Analysis
 
Figure 2.  Framework of consultation document retrieval. The rectangle denotes a negative life event re-
lated to love relation. Each circle denotes a symptom. D: Depressed, S: Suicide, I: Insomnia, A: Anxiety. 
1026
( , )
( , ) ( , ) (1 ) ( , ),Evn Sym Rel
Sim q d
Sim q d Sim q d Sim q d? ? ? ?
=
+ + ? ?   (1) 
where ( , )EvnSim q d , ( , )SymSim q d  and ( , )RelSim q d , 
denote the similarities of the sets of events, symp-
toms and relations, respectively, between a query 
and a document, and ?  and ? denote the combi-
nation factors. 
3.1 Similarity of events and symptoms 
The similarities of the sets of events and symptoms 
are calculated in the same method. The similarity 
of the event set (or symptom set) is calculated by 
comparing the events (or symptoms) in a query 
with those in a document. Additionally, only the 
events (or symptoms) with the same type are 
considered. The events (or symptoms) with 
different types are considered as irrelevant, i.e., no 
similarity. For instance, the event <Love> is 
considered as irrelevant to <Work>. The similarity 
of the event set is calculated by 
( , )
1
( , ) cos( , ) .,
( )
Evn
q d q d
q d e q d
Sim q d
Type e e e e const
N Evn Evn ? ?
= +? ?  (2) 
where qEvn  and dEvn  denote the event set in a 
query and a document, respectively; qe  and de  
denote the events; ( )q dN Evn Evn?  denotes the 
cardinality of the union of qEvn  and dEvn  as a 
normalization factor, and ( , )q dType e e  denotes an 
identity function to check whether two events have 
the same type, defined as 
1     ( ) ( )
( , ) .
0    otherwise
q d
q d
Type e Type e
Type e e
=??= ???
  (3) 
The cos( , )q de e  denotes the cosine angle between 
two vectors of words representing qe  and de , as 
shown below. 
( ) ( )
1
2 2
1 1
cos( , ) ,q d
q d
T i i
e ei
q d
T Ti i
e ei i
w w
e e
w w
=
= =
= ?
? ?
  (4) 
where w denotes a word in a vector, and T denotes 
the dimensionality of vectors. Accordingly, when 
two events have the same type, their similarity is 
given as cos( , )q de e  plus a constant, const.. Addi-
tionally, cos( , )q de e  and const. can be considered 
as the word-level and topic-level similarities, re-
spectively. The optimal setting of const. is deter-
mined empirically. 
3.2 Similarity of relations 
When calculating the similarity of relations, only 
the relations with the same type are considered. 
That is, the cause-effect (or temporal) relations in a 
query are only compared with the cause-effect (or 
temporal) relations in a document. Therefore, the 
similarity of relation sets can be calculated as 
,
1
( , ) ( , ) ( , ),
q d
Rel q d q d
r r
Sim q d Type r r Sim r r
Z
= ?  (5) 
( ) ( ) ( ) ( ),C q C d T q T dZ N r N r N r N r= +   (6) 
where qr and dr denote the relations in a query and 
a document, respectively; Z denotes the normaliza-
tion factor for the number of relations; ( , )q dType e e  
denotes an identity function similar to (3), and 
( )CN i   and ( )TN i  denote the numbers of cause-
effect and temporal relations. 
Both cause-effect and temporal relations are rep-
resented by symptom chains. Hence, the similarity 
of relations is measured by the similarity of symp-
tom chains. The main characteristic of a symptom 
chain is that it retains the cause-effect or temporal 
order of the symptoms within it. Therefore, the 
order of the symptoms must be considered when 
calculating the similarity of two symptom chains. 
Accordingly, a sequence kernel function (Lodhi et 
al., 2002; Cancedda et al, 2003) is adopted to cal-
culate the similarity of two symptom chains. A 
sequence kernel compares two sequences of sym-
bols (e.g., characters, words) based on the subse-
quences within them, but not individual symbols. 
Thereby, the order of the symptoms can be incor-
porated into the comparison process. 
The sequence kernel calculates the similarity of 
two symptom chains by comparing their sub-
symptom chains at different lengths. An increasing 
number of common sub-symptom chains indicates 
a greater similarity between two symptom chains. 
For instance, both the two symptom chains 
1 2 3 4s s s s  and 3 2 1s s s  contain the same symptoms 1s , 
2s  and 3s , but in different orders. To calculate the 
similarity between these two symptom chains, the 
sequence kernel first calculates their similarities at 
length 2 and 3, and then averages the similarities at 
the two lengths. To calculate the similarity at 
1027
length 2, the sequence kernel compares their sub-
symptom chains of length 2, i.e., 
1 2 1 3 1 4 2 3 2 4 3 4{ , , , , , }s s s s s s s s s s s s  and 3 2 3 1 2 1{ , , }s s s s s s . 
Similarly, their similarity at length 3 is calculated 
by comparing their sub-symptom chains of length 
3, i.e., 1 2 3 1 2 4 1 3 4 2 3 4{ ,  ,  ,  }s s s s s s s s s s s s  and 3 2 1{ }s s s . 
Obviously, no similarity exists between 1 2 3 4s s s s  
and 3 2 1s s s , since no sub-symptom chains are 
matched at both lengths. In this example, the sub-
symptom chains of length 1, i.e., individual symp-
toms, do not have to be compared because they 
contain no information about the order of symp-
toms. Additionally, the sub-symptom chains of 
length 4 do not have to be compared, because the 
two symptom chains share no sub-symptom chains 
at this length. Hence, for any two symptom chains, 
the length of the sub-symptom chains to be com-
pared ranges from two to the minimum length of 
the two symptom chains. The similarity of two 
symptom chains can be formally denoted as 
1 2
1 2
1 2
2
( , ) ( , )
                ( , )
1
                ( , ),
1
N N
q d q d
N N
q d
N
N N
n q d
n
Sim r r Sim sc sc
K sc sc
K sc sc
N =
?
=
= ? ?
  (7) 
where 1Nqsc  and 2
N
dsc  denote the symptom chains 
corresponding to qr  and dr , respectively; 1N  and 
2N  denote the length of 1
N
qsc  and 2
N
dsc , respec-
tively; (  ,   )K i i  denotes the sequence kernel for 
calculating the similarity between two symptom 
chains; (  ,   )nK i i  denotes the sequence kernel for 
calculating the similarity between two symptom 
chains at length n, and N is the minimum length of 
the two symptom chains, i.e., 1 2min( , )N N N= . 
The sequence kernel 1 2( , )N Nn i jK sc sc  is defined as 
21
1 2
1 2
1 2
1 1 2 2
( )( )
( , )   
( ) ( )
( ) ( )
         ,
( ) ( ) ( ) ( )
n
n n
NN
n jN N n i
n i j N N
n i n j
N N
u i u j
u SC
N N N N
u i u j u i u j
u SC u SC
scsc
K sc sc
sc sc
sc sc
sc sc sc sc
? ?
? ? ? ?
?
? ?
??=
? ?
=
?
? ?
i
(8) 
where 1 2( , )N Nn i jK sc sc  is the normalized inner 
product of vectors 1( )Nn isc?  and 2( )Nn jsc? ; ( )n? i  
denotes a mapping that transforms a given symp-
tom chain into a vector of the sub-symptom chains 
of length n; ( )u? i  denotes an element of the vector, 
representing the weight of a sub-symptom chain u , 
and nSC  denotes the set of all possible sub-
symptom chains of length n. The weight of a sub-
symptom chain, i.e., ( )u? i , is defined as 
1
1
1
1       is a contiguous sub-symptom chain of 
    is a non-contiguous sub-symptom chain
( )  
        with  skipped symptoms
0       does not appear in ,
N
i
N
u i
N
i
u sc
u
sc
u sc
??? ?
???= ????
(9) 
where [0,1]??  denotes a decay factor that is 
adopted to penalize the non-contiguous sub-
symptom chains occurred in a symptom chain 
based on the skipped symptoms. For instance, 
1 2 2 31 2 3 1 2 3
( ) ( ) 1s s s ss s s s s s? ?= =  since 1 2s s  and 2 3s s  
are considered as contiguous in 1 2 3s s s , and 
1 3
1
1 2 3( )s s s s s? ?=  since 1 3s s  is a non-contiguous 
sub-symptom chain with one skipped symptom. 
The decay factor is adopted because a contiguous 
sub-symptom chain is preferable to a non-
contiguous chain when comparing two symptom 
chains. The setting of the decay factor is domain 
dependent. If 1? = , then no penalty is applied for 
skipping symptoms, and the cause-effect and tem-
poral relations are transitive. The optimal setting of 
Figure 3.  Illustrative example of relevance com-
putation using the sequence kernel function. 
1028
?  is determined empirically. Figure 3 presents an 
example to summarize the computation of the 
similarity between two symptom chains. 
4 Experimental Results 
4.1 Experiment setup 
1) Corpus: The consultation documents were 
collected from the mental health website of the 
John Tung Foundation (http://www.jtf.org.tw) 
and the PsychPark (http://www.psychpark.org), 
a virtual psychiatric clinic, maintained by a 
group of volunteer professionals of Taiwan 
Association of Mental Health Informatics (Bai 
et al 2001). Both of the web sites provide 
various kinds of free psychiatric services and 
update the consultation documents periodically. 
For privacy consideration, all personal infor-
mation has been removed. A total of 3,650 
consultation documents were collected for 
evaluating the retrieval model, of which 20 
documents were randomly selected as the test 
query set, 100 documents were randomly se-
lected as the tuning set to obtain the optimal 
parameter settings of involved retrieval models, 
and the remaining 3,530 documents were the 
reference set to be retrieved. Table 1 shows the 
average number of events, symptoms and rela-
tions in the test query set. 
2) Baselines: The proposed method, denoted as 
Topic, was compared to two word-based re-
trieval models: the VSM and Okapi BM25 
models. The VSM was implemented in terms 
of the standard TF-IDF weight. The Okapi 
BM25 model is defined as 
(1) 31
2
3
( 1)( 1)
| | ,
t Q
k qtfk tf avdl dl
w k Q
K tf k qtf avdl dl?
++ ?++ + +?  (10) 
where t denotes a word in a query Q; qtf and tf 
denote the word frequencies occurring in a 
query and a document, respectively, and  (1)w  
denotes the Robertson-Sparck Jones weight of 
t (without relevance feedback), defined as 
(1) 0.5log ,
0.5
N n
w
n
? += +             (11) 
where N denotes the total number of docu-
ments, and n denotes the number of documents 
containing t. In (10), K is defined as 
1((1 ) / ),K k b b dl avdl= ? + ?             (12) 
where dl and avdl denote the length and aver-
age length of a document, respectively. The 
default values of 1k , 2k , 3k  and b are describe 
in (Robertson et al, 1996), where 1k  ranges 
from 1.0 to 2.0; 2k  is set to 0; 3k  is set to 8, 
and b ranges from 0.6 to 0.75. Additionally, 
BM25 can be considered as BM15 and BM11 
when b is set to 1 and 0, respectively. 
3) Evaluation metric: To evaluate the retrieval 
models, a multi-level relevance criterion was 
adopted. The relevance criterion was divided 
into four levels, as described below. 
z Level 0: No topics are matched between a 
query and a document. 
z Level 1: At least one topic is partially 
matched between a query and a document. 
z Level 2: All of the three topics are partially 
matched between a query and a document. 
z Level 3: All of the three topics are partially 
matched, and at least one topic is exactly 
matched between a query and a document. 
To deal with the multi-level relevance, the dis-
counted cumulative gain (DCG) (Jarvelin and 
Kekalainen, 2000) was adopted as the evalua-
tion metric, defined as 
[1],                                   1     [ ]
[ 1] [ ]/ log , otherwisec
G if i
DCG i
DCG i G i i
=??= ? ? +??
(13) 
where i denotes the i-th document in the re-
trieved list; G[i] denotes the gain value, i.e., 
relevance levels, of the i-th document, and c 
denotes the parameter to penalize a retrieved 
document in a lower rank. That is, the DCG 
simultaneously considers the relevance levels, 
and the ranks in the retrieved list to measure 
the retrieval precision. For instance, let 
<3,2,3,0,0> denotes the retrieved list of five 
documents with their relevance levels. If no 
penalization is used, then the DCG values for 
Topic Avg. Number
Negative Life Event 1.45 
Depressive Symptom 4.40 
Semantic Relation 3.35 
Table 1. Characteristics of the test query set. 
1029
the retrieved list are <3,5,8,8,8>, and thus 
DCG[5]=8. Conversely, if c=2, then the docu-
ments retrieved at ranks lower than two are pe-
nalized. Hence, the DCG values for the re-
trieved list are <3,5,6.89,6.89,6.89>, and 
DCG[5]=6.89. 
The relevance judgment was performed by 
three experienced physicians. First, the pooling 
method (Voorhees, 2000) was adopted to gen-
erate the candidate relevant documents for 
each test query by taking the top 50 ranked 
documents retrieved by each of the involved 
retrieval models, namely the VSM, BM25 and 
Topic. Two physicians then judged each can-
didate document based on the multilevel rele-
vance criterion. Finally, the documents with 
disagreements between the two physicians 
were judged by the third physician. Table 2 
shows the average number of relevant docu-
ments for the test query set. 
4) Optimal parameter setting: The parameter 
settings of BM25 and Topic were evaluated us-
ing the tuning set. The optimal setting of 
BM25 were k1 =1 and b=0.6. The other two pa-
rameters were set to the default values, i.e., 
2 0k =  and 3 8k = . For the Topic model, the 
parameters required to be evaluated include the 
combination factors, ?  and ? , described in 
(1); the constant const. described in (2), and 
the decay factor, ? , described in (9). The op-
timal settings were 0.3? = ; 0.5? = ; 
const.=0.6 and 0.8? = . 
4.2 Retrieval results 
The results are divided into two groups: the preci-
sion and efficiency. The retrieval precision was 
measured by DCG values. Additionally, a paired, 
two-tailed t-test was used to determine whether the 
performance difference was statistically significant. 
The retrieval efficiency was measure by the query 
processing time, i.e., the time for processing all the 
queries in the test query set. 
Table 3 shows the comparative results of re-
trieval precision. The two variants of BM25, 
namely BM11 and BM15, are also considered in 
comparison. For the word-based retrieval models, 
both BM25 and BM11 outperformed the VSM, and 
BM15 performed worst. The Topic model 
achieved higher DCG values than both the BM-
series models and VSM. The reasons are three-fold. 
First, a negative life event and a symptom can each 
be expressed by different words with the same or 
similar meaning. Therefore, the word-based mod-
els often failed to retrieve the relevant documents 
when different words were used in the input query. 
Second, a word may relate to different events and 
symptoms. For instance, the term "worry about" is 
Relevance Level Avg. Number
Level 1 18.50 
Level 2 9.15 
Level 3 2.20 
Table 2. Average number of relevant documents 
for the test query set. 
 DCG(5) DCG(10) DCG(20) DCG(50) DCG(100) 
Topic 4.7516* 6.9298 7.6040* 8.3606* 9.3974* 
BM25 4.4624 6.7023 7.1156 7.8129 8.6597 
BM11 3.8877 4.9328 5.9589 6.9703 7.7057 
VSM 2.3454 3.3195 4.4609 5.8179 6.6945 
BM15 2.1362 2.6120 3.4487 4.5452 5.7020 
Table 3. DCG values of different retrieval models.  * Topic vs BM25 significantly different (p<0.05) 
Retrieval Model Avg. Time (seconds)
Topic 17.13 
VSM 0.68 
BM25 0.48 
Table 4. Average query processing time of differ-
ent retrieval models. 
1030
a good indicator for both the symptoms <Anxiety> 
and <Hypochondriasis>. This may result in ambi-
guity for the word-based models. Third, the word-
based models cannot capture semantic relations 
between symptoms. The Topic model incorporates 
not only the word-level information, but also more 
useful topic information about depressive problems, 
thus improving the retrieval results. 
The query processing time was measured using 
a personal computer with Windows XP operating 
system, a 2.4GHz Pentium IV processor and 
512MB RAM. Table 4 shows the results. The topic 
model required more processing time than both 
VSM and BM25, since identification of topics in-
volves more detailed analysis, such as semantic 
parsing of sentences and symptom chain construc-
tion. This finding indicates that although the topic 
information can improve the retrieval precision, 
incorporating such high-precision features reduces 
the retrieval efficiency. 
5 Conclusion 
This work has presented the use of topic informa-
tion for retrieving psychiatric consultation docu-
ments. The topic information can provide more 
precise information about users' depressive prob-
lems, thus improving the retrieval precision. The 
proposed framework can also be applied to differ-
ent domains as long as the domain-specific topic 
information is identified. Future work will focus on 
more detailed experiments, including the contribu-
tion of each topic to retrieval precision, the effect 
of using different methods to combine topic infor-
mation, and the evaluation on real users. 
References 
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. Addison-Wesley, Reading, 
MA. 
Cancedda, N., E. Gaussier, C. Goutte, and J. M. Renders. 
2003. Word-Sequence Kernels. Journal of Machine 
Learning Research, 3(6):1059-1082. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, MA. 
Hamilton, M. 1960. A Rating Scale for Depression. 
Journal of Neurology, Neurosurgery and Psychiatry, 
23:56-62 
Jarvelin, K. and J. Kekalainen. 2000. IR Evaluation 
Methods for Retrieving Highly Relevant Documents. 
In Proc. of the 23rd Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, pages 41-48. 
Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini, 
and C. Watkins. 2002. Text Classification Using 
String Kernels. Journal of Machine Learning Re-
search, 2(3):419-444. 
Okabe, M., K. Umemura and S. Yamada. 2005. Query 
Expansion with the Minimum User Feedback by 
Transductive Learning. In Proc. of HLT/EMNLP, 
Vancouver, Canada, pages 963-970. 
Pagano, M.E., A.E. Skodol, R.L. Stout, M.T. Shea, S. 
Yen, C.M. Grilo, C.A. Sanislow, D.S. Bender, T.H. 
McGlashan, M.C. Zanarini, and J.G. Gunderson. 
2004. Stressful Life Events as Predictors of Function-
ing: Findings from the Collaborative Longitudinal 
Personality Disorders Study. Acta Psychiatrica Scan-
dinavica, 110: 421-429. 
Robertson, S. E., S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M.Gatford. 1995. Okapi at TREC-3. In 
Proc. of the Third Text REtrieval Conference (TREC-
3), NIST. 
Robertson, S. E., S. Walker, M. M. Beaulieu, and 
M.Gatford. 1996. Okapi at TREC-4. In Proc. of the 
fourth Text REtrieval Conference (TREC-4), NIST. 
Voorhees, E. M. and D. K. Harman. 2000. Overview of 
the Sixth Text REtrieval Conference (TREC-6). In-
formation Processing and Management, 36(1):3-35. 
Wu, C. H., L. C. Yu, and F. L. Jang. 2005a. Using Se-
mantic Dependencies to Mine Depressive Symptoms 
from Consultation Records. IEEE Intelligent System, 
20(6):50-58. 
Wu, C. H., J. F. Yeh, and M. J. Chen. 2005b. Domain-
Specific FAQ Retrieval Using Independent Aspects. 
ACM Trans. Asian Language Information Processing, 
4(1):1-17. 
Wu, C. H., J. F. Yeh, and Y. S. Lai. 2006. Semantic 
Segment Extraction and Matching for Internet FAQ 
Retrieval. IEEE Trans. Knowledge and Data Engi-
neering, 18(7):930-940. 
Yeh, J. F., C. H. Wu, M. J. Chen, and L. C. Yu. 2004. 
Automated Alignment and Extraction of Bilingual 
Domain Ontology for Cross-Language Domain-
Specific Applications. In Proc. of the 20th COLING, 
Geneva, Switzerland, pages 1140-1146. 
Yu, L. C., C. H. Wu, Yeh, J. F., and F. L. Jang. 2007. 
HAL-based Evolutionary Inference for Pattern Induc-
tion from Psychiatry Web Resources. Accepted by 
IEEE Trans. Evolutionary Computation. 
1031
Proceedings of ACL-08: HLT, pages 156?164,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Searching Questions by Identifying Question Topic and Question Focus 
 
Huizhong Duan1, Yunbo Cao1,2, Chin-Yew Lin2 and Yong Yu1 
1Shanghai Jiao Tong University,  
Shanghai, China, 200240 
{summer, yyu}@apex.sjtu.edu.cn 
2Microsoft Research Asia,  
Beijing, China, 100080 
{yunbo.cao, cyl}@microsoft.com 
 
 
Abstract 
This paper is concerned with the problem of 
question search. In question search, given a 
question as query, we are to return questions 
semantically equivalent or close to the queried 
question. In this paper, we propose to conduct 
question search by identifying question topic 
and question focus. More specifically, we first 
summarize questions in a data structure con-
sisting of question topic and question focus. 
Then we model question topic and question 
focus in a language modeling framework for 
search. We also propose to use the MDL-
based tree cut model for identifying question 
topic and question focus automatically. Expe-
rimental results indicate that our approach of 
identifying question topic and question focus 
for search significantly outperforms the base-
line methods such as Vector Space Model 
(VSM) and Language Model for Information 
Retrieval (LMIR).  
1 Introduction 
Over the past few years, online services have been 
building up very large archives of questions and 
their answers, for example, traditional FAQ servic-
es and emerging community-based Q&A services 
(e.g., Yahoo! Answers1 , Live QnA2, and Baidu 
Zhidao3).   
To make use of the large archives of questions 
and their answers, it is critical to have functionality 
facilitating users to search previous answers. Typi-
cally, such functionality is achieved by first re-
trieving questions expected to have the same 
answers as a queried question and then returning 
the related answers to users. For example, given 
question Q1 in Table 1, question Q2 can be re-
                                                          
1 http://answers.yahoo.com 
2 http://qna.live.com 
3 http://zhidao.baidu.com 
turned and its answer will then be used to answer 
Q1 because the answer of Q2 is expected to par-
tially satisfy the queried question Q1. This is what 
we called question search. In question search, re-
turned questions are semantically equivalent or 
close to the queried question.  
 
Query: 
Q1: Any cool clubs in Berlin or Hamburg? 
Expected: 
Q2: What are the best/most fun clubs in Berlin? 
Not Expected: 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
Table 1. An Example on Question Search 
Many methods have been investigated for tack-
ling the problem of question search. For example, 
Jeon et al have compared the uses of four different 
retrieval methods, i.e. vector space model, Okapi, 
language model, and translation-based model, 
within the setting of question search (Jeon et al, 
2005b).  However, all the existing methods treat 
questions just as plain texts (without considering 
question structure). For example, obviously, Q2 
can be considered semantically closer to Q1 than 
Q3-Q5 although all questions (Q2-Q5) are related 
to Q1. The existing methods are not able to tell the 
difference between question Q2 and questions Q3, 
Q4, and Q5 in terms of their relevance to question 
Q1. We will clarify this in the following. 
In this paper, we propose to conduct question 
search by identifying question topic and question 
focus.  
The question topic usually represents the major 
context/constraint of a question (e.g., Berlin, Ham-
burg) which characterizes users? interests. In con-
trast, question focus (e.g., cool club, cheap hotel) 
presents certain aspect (or descriptive features) of 
the question topic. For the aim of retrieving seman-
tically equivalent (or close) questions, we need to 
156
assure that returned questions are related to the 
queried question with respect to both question top-
ic and question focus. For example, in Table 1, Q2 
preserves certain useful information of Q1 in the 
aspects of both question topic (Berlin) and ques-
tion focus (fun club) although it loses some useful 
information in question topic (Hamburg). In con-
trast, questions Q3-Q5 are not related to Q1 in 
question focus (although being related in question 
topic, e.g. Hamburg, Berlin), which makes them 
unsuitable as the results of question search.  
We also propose to use the MDL-based (Mini-
mum Description Length) tree cut model for auto-
matically identifying question topic and question 
focus. Given a question as query, a structure called 
question tree is constructed over the question col-
lection including the queried question and all the 
related questions, and then the MDL principle is 
applied to find a cut of the question tree specifying 
the question topic and the question focus of each 
question. 
In a summary, we summarize questions in a data 
structure consisting of question topic and question 
focus. On the basis of this, we then propose to 
model question topic and question focus in a lan-
guage modeling framework for search. To the best 
of our knowledge, none of the existing studies ad-
dressed question search by modeling both question 
topic and question focus. 
We empirically conduct the question search with 
questions about ?travel? and ?computers & internet?. 
Both kinds of questions are from Yahoo! Answers. 
Experimental results show that our approach can 
significantly improve traditional methods (e.g. 
VSM, LMIR) in retrieving relevant questions.  
The rest of the paper is organized as follow. In 
Section 2, we present our approach to question 
search which is based on identifying question topic 
and question focus. In Section 3, we empirically 
verify the effectiveness of our approach to question 
search. In Section 4, we employ a translation-based 
retrieval framework for extending our approach to 
fix the issue called ?lexical chasm?. Section 5 sur-
veys the related work. Section 6 concludes the pa-
per by summarizing our work and discussing the 
future directions.  
2 Our Approach to Question Search 
Our approach to question search consists of two 
steps: (a) summarize questions in a data structure 
consisting of question topic and question focus; (b) 
model question topic and question focus in a lan-
guage modeling framework for search. 
In the step (a), we employ the MDL-based (Min-
imum Description Length) tree cut model for au-
tomatically identifying question topic and question 
focus. Thus, this section will begin with a brief 
review of the MDL-based tree cut model and then 
follow that by an explanation of steps (a) and (b). 
2.1 The MDL-based tree cut model 
Formally, a tree cut model ? (Li and Abe, 1998) 
can be represented by a pair consisting of a tree cut 
?, and a probability parameter vector ? of the same 
length, that is, 
? ? ??, ??  (1) 
where ? and ? are 
? ? ???, ??, . . ???,  
? ? ??????, ?????, ? , ??????  
(2) 
where ??, ??, ????are classes determined by a cut 
in the tree and ? ????? ? 1
?
??? . A ?cut? in a tree is 
any set of nodes in the tree that defines a partition 
of all the nodes, viewing each node as representing 
the set of child nodes as well as itself. For example, 
the cut indicated by the dash line in Figure 1 cor-
responds to three classes:???, ????,????, ????, and 
????, ???, ???, ????. 
Figure 1. An Example on the Tree Cut Model 
A straightforward way for determining a cut of a 
tree is to collapse the nodes of less frequency into 
their parent nodes. However, the method is too 
heuristic for it relies much on manually tuned fre-
quency threshold. In our practice, we turn to use a 
theoretically well-motivated method based on the 
MDL principle. MDL is a principle of data com-
pression and statistical estimation from informa-
tion theory (Rissanen, 1978). 
Given a sample ? and a tree cut ?, we employ 
MLE to estimate the parameters of the correspond-
ing tree cut model ?? ? ??, ??? , where ??  denotes 
the estimated parameters.  
According to the MDL principle, the description 
length (Li and Abe, 1998)  ????, ?? of the tree cut 
model ??  and the sample ?? is the sum of the model 
?? 
??? ??? ??? 
??? ??? ??? ??? 
157
description length ????, the parameter description 
length ????|?? , and the data description length 
???|?, ???, i.e. 
????, ?? ? ???? ? ??????? ? ???|?, ???  (3) 
The model description length ???? is a subjec-
tive quantity which depends on the coding scheme 
employed. Here, we simply assume that each tree 
cut model is equally likely a priori. 
The parameter description length ????|?? is cal-
culated as  
??????? ? ?
?
? log |?|  (4) 
where |?|  denotes the sample size and ?  denotes 
the number of free parameters in the tree cut model, 
i.e. ? equals the number of nodes in ? minus one. 
The data description length ???|?, ???  is calcu-
lated as 
?????, ??? ? ?? ???????????   (5) 
where 
 ????? ? ?
|?|
? ????
|?|
 (6) 
where ?  is the class that ?  belongs to and ???? 
denotes the total frequency of instances in class ? 
in the sample ?. 
With the description length defined as (3), we 
wish to select a tree cut model with the minimum 
description length and output it as the result. Note 
that the model description length ???? can be ig-
nored because it is the same for all tree cut models. 
The MDL-based tree cut model was originally 
introduced for handling the problem of generaliz-
ing case frames using a thesaurus (Li and Abe, 
1998). To the best of our knowledge, no existing 
work utilizes it for question search. This may be 
partially because of the unavailability of the re-
sources (e.g., thesaurus) which can be used for 
embodying the questions in a tree structure. In Sec-
tion 2.2, we will introduce a tree structure called 
question tree for representing questions. 
2.2 Identifying question topic and question 
focus 
In principle, it is possible to identify question topic 
and question focus of a question by only parsing 
the question itself (for example, utilizing a syntac-
tic parser). However, such a method requires accu-
rate parsing results which cannot be obtained from 
the noisy data from online services. 
Instead, we propose using the MDL-based tree 
cut model which identifies question topics and 
question foci for a set of questions together. More 
specifically, the method consists of two phases: 
1) Constructing a question tree: represent the 
queried question and all the related questions 
in a tree structure called question tree; 
2) Determining a tree cut: apply the MDL prin-
ciple to the question tree, which yields the cut 
specifying question topic and question focus.  
2.2.1 Constructing a question tree 
In the following, with a series of definitions, we 
will describe how a question tree is constructed 
from a collection of questions. 
Let?s begin with explaining the representation of 
a question. A straightforward method is to 
represent a question as a bag-of-words (possibly 
ignoring stop words). However, this method cannot 
discern ?the hotels in Paris? from ?the Paris hotel?. 
Thus, we turn to use the linguistic units carrying on 
more semantic information. Specifically, we make 
use of two kinds of units: BaseNP (Base Noun 
Phrase) and WH-ngram. A BaseNP is defined as a 
simple and non-recursive noun phrase (Cao and Li, 
2002). A WH-ngram is an ngram beginning with 
WH-words. The WH-words that we consider in-
clude ?when?, ?what?, ?where?, ?which?, and ?how?.  
We refer to these two kinds of units as ?topic 
terms?. With ?topic terms?, we represent a question 
as a topic chain and a set of questions as a question 
tree.  
Definition 1 (Topic Profile) The topic profile 
?? of a topic term ? in a categorized question col-
lection is a probability distribution of categories 
????|??????  where ? is a set of categories.  
???|?? ? ???????,??
? ???????,?????
  (7) 
where ???????, ??  is the frequency of the topic 
term ?  within category ? . Clearly, we 
have?? ???|????? ? 1.  
By ?categorized questions?, we refer to the ques-
tions that are organized in a tree of taxonomy. For 
example, at Yahoo! Answers, the question ?How 
do I install my wireless router? is categorized as 
?Computers & Internet ? Computer Networking?. 
Actually, we can find categorized questions at oth-
er online services such as FAQ sites, too. 
Definition 2 (Specificity) The specificity ?????of 
a topic term ?? is the inverse of the entropy of the 
topic profile???. More specifically, 
???? ? 1 ??? ???|?? log ???|????? ? ???
  (8) 
158
where ?  is a smoothing parameter used to cope 
with the topic terms whose entropy is 0. In our ex-
periments, the value of ? was set 0.001. 
We use the term specificity to denote how spe-
cific a topic term is in characterizing information 
needs of users who post questions. A topic term of 
high specificity (e.g., Hamburg, Berlin) usually 
specifies the question topic corresponding to the 
main context of a question because it tends to oc-
cur only in a few categories. A topic term of low 
specificity is usually used to represent the question 
focus (e.g., cool club, where to see) which is rela-
tively volatile and might occur in many categories. 
Definition 3 (Topic Chain) A topic chain ?? of 
a question ? is a sequence of ordered topic terms 
?? ? ?? ? ? ? ?? such that  
1) ?? is included in 1  ,? ? ? ? ?;  
2) ????? ? ?????,  1 ? ? ? ? ? ?.  
For example, the topic chain of ?any cool clubs 
in Berlin or Hamburg?? is ?Hamburg ? Berlin ?
cool?club? because the specificities for ?Hamburg?, 
?Berlin?, and ?cool club? are 0.99, 0.62, and 0.36. 
Definition 4 (Question Tree) A question tree of 
a question set ? ? ???????
?  is a prefix tree built 
over the topic chains ?? ? ???
?????
?  of the question 
set ?. Clearly, if a question set contains only one 
question, its question tree will be exactly same as 
the topic chain of the question. 
Note that the root node of a question tree is as-
sociated with empty string as the definition of pre-
fix tree requires (Fredkin, 1960). 
 
Figure 2. An Example of a Question Tree 
 
Given the topic chains with respect to the ques-
tions in Table 1 as follow, 
? Q1: Hamburg ? Berlin ? cool?club?
? Q2: Berlin ? fun?club?
? Q3: Hamburg ? Berlin ? nice?hotel?
? Q4: Hamburg ? Berlin ? how?long?does?it?take?
? Q5: Berlin ? cheap?hotel?
we can have the question tree presented in Figure 2.  
2.2.2 Determining the tree cut 
According to the definition of a topic chain, the 
topic terms in a topic chain of a question are or-
dered by their specificity values. Thus, a cut of a 
topic chain naturally separates the topic terms of 
low specificity (representing question focus) from 
the topic terms of high specificity (representing 
question topic). Given a topic chain of a question 
consisting of ?  topic terms, there exist (? ? 1? 
possible cuts. The question is: which cut is the best?  
We propose using the MDL-based tree cut mod-
el for the search of the best cut in a topic chain. 
Instead of dealing with each topic chain individual-
ly, the proposed method handles a set of questions 
together. Specifically, given a queried question, we 
construct a question tree consisting of both the 
queried question and the related questions, and 
then apply the MDL principle to select the best cut 
of the question tree. For example, in Figure 2, we 
hope to get the cut indicated by the dashed line. 
The topic terms on the left of the dashed line 
represent the question topic and those on the right 
of the dashed line represent the question focus. 
Note that the tree cut yields a cut for each individ-
ual topic chain (each path) within the question tree 
accordingly.  
A cut of a topic chain ??? of a question q sepa-
rates the topic chain in two parts: HEAD and TAIL. 
HEAD (denoted as ?????) is the subsequence of 
the original topic chain ???  before the cut. TAIL 
(denoted as ?????) is the subsequence of ??? after 
the cut. Thus,??? ? ????? ? ?????. For instance, 
given the tree cut specified in Figure 2, for the top-
ic chain of Q1 ?Hamburg ? Berlin ? cool?club?, 
the HEAD and TAIL are ?Hamburg ? Berlin? 
and ?cool?club? respectively. 
2.3 Modeling question topic and question fo-
cus for search 
We employ the framework of language modeling 
(for information retrieval) to develop our approach 
to question search. 
In the language modeling approach to informa-
tion retrieval, the relevance of a targeted question 
?? to a queried question ? is given by the probabili-
ty ???|???  of generating the queried question ? 
Q1: Any cool clubs in Berlin or Hamburg? 
Q2: What are the most/best fun clubs in Berlin? 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
ROOT 
Hamburg 
Berlin 
Berlin 
cheap hotel 
fun club 
cool club
nice hotel
how long does it take
159
from the language model formed by the targeted 
question ??.  The targeted question ?? is from a col-
lection ? of questions. 
Following the framework, we propose a mixture 
model for modeling question structure (namely, 
question topic and question focus) within the 
process of searching questions: 
???|??? ? ? ? ??????|??????
????????1 ? ??? ? ??????|??????
 (9) 
In the mixture model, it is assumed that the 
process of generating question topics and the 
process of generating question foci are independent 
from each other.  
In traditional language modeling, a single multi-
nomial model ???|??? over terms is estimated for 
each targeted question ?? . In our case, two multi-
nomial models ??????????  and ??????????  need to 
be estimated for each targeted question ??. 
If unigram document language models are used, 
the equation (9) can then be re-written as, 
???|??? ? ? ? ? ??????????
???????,??
?????? ?
?1 ? ??? ? ? ??????????
???????,??
??????   
(10)
where ???????, ?? is the frequency of ? within ?. 
To avoid zero probabilities and estimate more 
accurate language models, the HEAD and TAIL of 
questions are smoothed using background collec-
tion, 
?????????? ? ? ? ???????????  
?????????????????????????1 ? ?? ? ????|??  
 
(11)
?????????? ? ? ? ???????????  
??????????????????????????1 ? ?? ? ????|??  
 
(12)
where ????|?????? , ????|?????? , and ????|??  are the 
MLE  estimators with respect to the HEAD of ??, 
the TAIL of ??, and the collection ?.  
3 Experimental Results  
We have conducted experiments to verify the ef-
fectiveness of our approach to question search. 
Particularly, we have investigated the use of identi-
fying question topic and question focus for search. 
3.1 Dataset and evaluation measures 
We made use of the questions obtained from Ya-
hoo! Answers for the evaluation. More specifically, 
we utilized the resolved questions under two of the 
top-level categories at Yahoo! Answers, namely 
?travel? and ?computers & internet?. The questions 
include 314,616 items from the ?travel? category 
and 210,785 items from the ?computers & internet? 
category. Each resolved question consists of three 
fields: ?title?, ?description?, and ?answers?. For 
search we use only the ?title? field. It is assumed 
that the titles of the questions already provide 
enough semantic information for understanding 
users? information needs. 
We developed two test sets, one for the category 
?travel? denoted as ?TRL-TST?, and the other for 
?computers & internet? denoted as ?CI-TST?. In 
order to create the test sets, we randomly selected 
200 questions for each category.  
To obtain the ground-truth of question search, 
we employed the Vector Space Model (VSM) (Sal-
ton et al, 1975) to retrieve the top 20 results and 
obtained manual judgments. The top 20 results 
don?t include the queried question itself. Given a 
returned result by VSM, an assessor is asked to 
label it with ?relevant? or ?irrelevant?. If a returned 
result is considered semantically equivalent (or 
close) to the queried question, the assessor will 
label it as ?relevant?; otherwise, the assessor will 
label it as ?irrelevant?. Two assessors were in-
volved in the manual judgments. Each of them was 
asked to label 100 questions from ?TRL-TST? and 
100 from ?CI-TST?. In the process of manually 
judging questions, the assessors were presented 
only the titles of the questions (for both the queried 
questions and the returned questions). Table 2 pro-
vides the statistics on the final test set. 
 
 # Queries # Returned # Relevant
TRL-TST 200 4,000 256 
CI-TST 200 4,000 510 
Table 2. Statistics on the Test Data 
 
We utilized two baseline methods for demon-
strating the effectiveness of our approach, the 
VSM and the LMIR (language modeling method 
for information retrieval) (Ponte and Croft, 1998).  
We made use of three measures for evaluating 
the results of question search methods. They are 
MAP, R-precision, and MRR.  
3.2 Searching questions about ?travel? 
In the experiments, we made use of the questions 
about ?travel? to test the performance of our ap-
proach to question search. More specifically, we 
used the 200 queries in the test set ?TRL-TST? to 
search for ?relevant? questions from the 314,616 
160
questions categorized as ?travel?. Note that only the 
questions occurring in the test set can be evaluated. 
We made use of the taxonomy of questions pro-
vided at Yahoo! Answers for the calculation of 
specificity of topic terms. The taxonomy is orga-
nized in a tree structure. In the following experi-
ments, we only utilized as the categories of 
questions the leaf nodes of the taxonomy tree (re-
garding ?travel?), which includes 355 categories. 
We randomly divided the test queries into five 
even subsets and conducted 5-fold cross-validation 
experiments. In each trial, we tuned the parameters 
?, ?, and ? in the equation (10)-(12) with four of 
the five subsets and then applied it to one remain-
ing subset. The experimental results reported be-
low are those averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.198 0.138 0.228 
LMIR 0.203 0.154 0.248 
LMIR-CUT 0.236 0.192 0.279 
Table 3. Searching Questions about ?Travel? 
 
In Table 3, our approach denoted by LMIR-
CUT is implemented exactly as equation (10).  
Neither VSM nor LMIR uses the data structure 
composed of question topic and question focus.  
From Table 3, we see that our approach outper-
forms the baseline approaches VSM and LMIR in 
terms of all the measures. We conducted a signi-
ficance test (t-test) on the improvements of our 
approach over VSM and LMIR. The result indi-
cates that the improvements are statistically signif-
icant (p-value < 0.05) in terms of all the evaluation 
measures.  
 
 
Figure 3. Balancing between Question Topic and Ques-
tion Focus 
 
In equation (9), we use the parameter ? to bal-
ance the contribution of question topic and the con-
tribution of question focus. Figure 3 illustrates how 
influential the value of ? is on the performance of 
question search in terms of MRR. The result was 
obtained with the 200 queries directly, instead of 
5-fold cross-validation. From Figure 3, we see that 
our approach performs best when ? is around 0.7. 
That is, our approach tends to emphasize question 
topic more than question focus.  
We also examined the correctness of question 
topics and question foci of the 200 queried ques-
tions. The question topics and question foci were 
obtained with the MDL-based tree cut model au-
tomatically. In the result, 69 questions have incor-
rect question topics or question foci. Further 
analysis shows that the errors came from two cate-
gories: (a) 59 questions have only the HEAD parts 
(that is, none of the topic terms fall within the 
TAIL part), and (b) 10 have incorrect orders of 
topic terms because the specificities of topic terms 
were estimated inaccurately. For questions only 
having the HEAD parts, our approach (equation (9)) 
reduces to traditional language modeling approach.  
Thus, even when the errors of category (a) occur, 
our approach can still work not worse than the tra-
ditional language modeling approach. This also 
explains why our approach performs best when ? is 
around 0.7. The error category (a) pushes our mod-
el to emphasize more in question topic. 
 
Methods Results 
VSM 
1. How cold does it usually get in Charlotte, 
NC during winters? 
2. How long and cold are the winters in 
Rochester, NY? 
3. How cold is it in Alaska? 
LMIR 
1. How cold is it in Alaska? 
2. How cold does it get really in Toronto in 
the winter? 
3. How cold does the Mojave Desert get in 
the winter? 
LMIR-
CUT 
1. How cold is it in Alaska? 
2. How cold is Alaska in March and out-
door activities? 
3. How cold does it get in Nova Scotia in the 
winter? 
Table 4. Search Results for 
?How cold does it get in winters in Alaska?? 
 
Table 4 provides the TOP-3 search results which 
are given by VSM, LMIR, and LMIR-CUT (our 
approach) respectively. The questions in bold are 
labeled as ?relevant? in the evaluation set. The que-
ried question seeks for the ?weather? information 
about ?Alaska?. Both VSM and LMIR rank certain 
0.05
0.1
0.15
0.2
0.25
0.3
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
161
?irrelevant? questions higher than ?relevant? ques-
tions. The ?irrelevant? questions are not about 
?Alaska? although they are about ?weather?. The 
reason is that neither VSM nor PVSM is aware that 
the query consists of the two aspects ?weather? 
(how cold, winter) and ?Alaska?.  In contrast, our 
approach assures that both aspects are matched. 
Note that the HEAD part of the topic chain of the 
queried question given by our approach is ?Alaska? 
and the TAIL part is ?winter ? how?cold?. 
3.3 Searching questions about ?computers & 
internet? 
In the experiments, we made use of the questions 
about ?computers & internet? to test the perfor-
mance of our proposed approach to question search. 
More specifically, we used the 200 queries in the 
test set ?CI-TST?? to search for ?relevant? questions 
from the 210,785 questions categorized as ?com-
puters & internet?. For the calculation of specificity 
of topic terms, we utilized as the categories of 
questions the leaf nodes of the taxonomy tree re-
garding ?computers & Internet?, which include 23 
categories.  
We conducted 5-fold cross-validation for the pa-
rameter tuning. The experimental results reported 
in Table 5 are averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.236 0.175 0.289 
LMIR 0.248 0.191 0.304 
LMIR-CUT 0.279 0.230 0.341 
Table 5. Searching Questions about ?Computers & In-
ternet? 
 
Again, we see that our approach outperforms the 
baseline approaches VSM and LMIR in terms of 
all the measures. We conducted a significance test 
(t-test) on the improvements of our approach over 
VSM and LMIR. The result indicates that the im-
provements are statistically significant (p-value < 
0.05) in terms of all the evaluation measures.  
We also conducted the experiment similar to 
that in Figure 3. Figure 4 provides the result. The 
trend is consistent with that in Figure 3.  
We examined the correctness of (automatically 
identified) question topics and question foci of the 
200 queried questions, too. In the result, 65 ques-
tions have incorrect question topics or question 
foci. Among them, 47 fall in the error category (a) 
and 18 in the error category (b). The distribution of 
errors is also similar to that in Section 3.2, which 
also justifies the trend presented in Figure 4. 
 
 
Figure 4. Balancing between Question Topic and Ques-
tion Focus 
4 Using Translation Probability 
In the setting of question search, besides the topic 
what we address in the previous sections, another 
research topic is to fix lexical chasm between ques-
tions.  
Sometimes, two questions that have the same 
meaning use very different wording. For example, 
the questions ?where to stay in Hamburg?? and 
?the best hotel in Hamburg?? have almost the same 
meaning but are lexically different in question fo-
cus (where to stay vs. best hotel). This is the so-
called ?lexical chasm?. 
Jeon and Bruce (2007) proposed a mixture mod-
el for fixing the lexical chasm between questions. 
The model is a combination of the language mod-
eling approach (for information retrieval) and 
translation-based approach (for information re-
trieval). Our idea of modeling question structure 
for search can naturally extend to Jeon et al?s 
model. More specifically, by using translation 
probabilities, we can rewrite equation (11) and (12) 
as follow: 
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????  
??1 ? ?? ? ??? ? ????|??  
(13)
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????   
??1 ? ?? ? ??? ? ????|??  
 
(14)
where ????|???  denotes the probability that topic 
term ? is the translation of ??. In our experiments, 
to estimate the probability ????|???, we used the 
collections of question titles and question descrip-
tions as the parallel corpus and the IBM model 1 
(Brown et al, 1993) as the alignment model. 
0.15
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
162
Usually, users reiterate or paraphrase their ques-
tions (already described in question titles) in ques-
tion descriptions. 
We utilized the new model elaborated by equa-
tion (13) and (14) for searching questions about 
?travel? and ?computers & internet?. The new mod-
el is denoted as ?SMT-CUT?. Table 6 provides the 
evaluation results. The evaluation was conducted 
with exactly the same setting as in Section 3. From 
Table 6, we see that the performance of our ap-
proach can be further boosted by using translation 
probability.  
 
Data Methods MAP R-Precision MRR
TRL-
TST 
LMIR-CUT 0.236 0.192 0.279
SMT-CUT 0.266 0.225 0.308
CI-
TST 
LMIR-CUT 0.279 0.230 0.341
SMT-CUT 0.282 0.236 0.337
Table 6. Using Translation Probability 
5 Related Work 
The major focus of previous research efforts on 
question search is to tackle the lexical chasm prob-
lem between questions.  
The research of question search is first con-
ducted using FAQ data. FAQ Finder (Burke et al, 
1997) heuristically combines statistical similarities 
and semantic similarities between questions to rank 
FAQs. Conventional vector space models are used 
to calculate the statistical similarity and WordNet 
(Fellbaum, 1998) is used to estimate the semantic 
similarity. Sneiders (2002) proposed template 
based FAQ retrieval systems. Lai et al (2002) pro-
posed an approach to automatically mine FAQs 
from the Web. Jijkoun and Rijke (2005) used su-
pervised learning methods to extend heuristic ex-
traction of Q/A pairs from FAQ pages, and treated 
Q/A pair retrieval as a fielded search task.  
Harabagiu et al (2005) used a Question Answer 
Database (known as QUAB) to support interactive 
question answering. They compared seven differ-
ent similarity metrics for selecting related ques-
tions from QUAB and found that the concept-
based metric performed best. 
Recently, the research of question search has 
been further extended to the community-based 
Q&A data. For example, Jeon et al (Jeon et al, 
2005a; Jeon et al, 2005b) compared four different 
retrieval methods, i.e. vector space model, Okapi, 
language model (LM), and translation-based model, 
for automatically fixing the lexical chasm between 
questions of question search. They found that the 
translation-based model performed best. 
However, all the existing methods treat ques-
tions just as plain texts (without considering ques-
tion structure). In this paper, we proposed to 
conduct question search by identifying question 
topic and question focus. To the best of our know-
ledge, none of the existing studies addressed ques-
tion search by modeling both question topic and 
question focus. 
Question answering (e.g., Pasca and Harabagiu, 
2001; Echihabi and Marcu, 2003; Voorhees, 2004; 
Metzler and Croft, 2005) relates to question search. 
Question answering automatically extracts short 
answers for a relatively limited class of question 
types from document collections. In contrast to that, 
question search retrieves answers for an unlimited 
range of questions by focusing on finding semanti-
cally similar questions in an archive. 
6 Conclusions and Future Work 
In this paper, we have proposed an approach to 
question search which models question topic and 
question focus in a language modeling framework. 
The contribution of this paper can be summa-
rized in 4-fold: (1) A data structure consisting of 
question topic and question focus was proposed for 
summarizing questions; (2) The MDL-based tree 
cut model was employed to identify question topic 
and question focus automatically; (3) A new form 
of language modeling using question topic and 
question focus was developed for question search; 
(4) Extensive experiments have been conducted to 
evaluate the proposed approach using a large col-
lection of real questions obtained from Yahoo! An-
swers.  
Though we only utilize data from community-
based question answering service in our experi-
ments, we could also use categorized questions 
from forum sites and FAQ sites. Thus, as future 
work, we will try to investigate the use of the pro-
posed approach for other kinds of web services.  
Acknowledgement 
We would like to thank Xinying Song, Shasha Li, 
and Shilin Ding for their efforts on developing the 
evaluation data. We would also like to thank Ste-
phan H. Stiller for his proof-reading of the paper. 
 
163
References  
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proc. of ACL?03. 
C. Fellbaum. 1998. WordNet: An electronic lexical da-
tabase. MIT Press. 
D. Metzler and W. B. Croft. 2005. Analysis of statistical 
question classification for fact-based questions. In-
formation Retrieval, 8(3), pages 481-504. 
E. Fredkin. 1960. Trie memory. Communications of the 
ACM, D. 3(9):490-499. 
E. M. Voorhees. 2004. Overview of the TREC 2004 
question answering track. In Proc. of TREC?04. 
E. Sneiders. 2002. Automated question answering using 
question templates that cover the conceptual model 
of the database. In Proc. of the 6th International 
Conference on Applications of Natural Language to 
Information Systems, pages 235-239. 
G. Salton, A. Wong, and C. S. Yang 1975. A vector 
space model for automatic indexing. Communica-
tions of the ACM, vol. 18, nr. 11, pages 613-620.  
H.  Li and N. Abe. 1998. Generalizing case frames us-
ing a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2), pages 217-244. 
J. Jeon and W.B. Croft. 2007. Learning translation-
based language models using Q&A archives. Tech-
nical report, University of Massachusetts. 
J. Jeon, W. B. Croft, and J. Lee. 2005a. Finding seman-
tically similar questions based on their answers. In 
Proc. of SIGIR?05. 
J. Jeon, W. B. Croft, and J. Lee. 2005b. Finding similar 
questions in large question and answer archives. In 
Proc. of CIKM ?05, pages 84-90. 
J. Rissanen. 1978. Modeling by shortest data description. 
Automatica, vol. 14,  pages. 465-471 
J.M. Ponte, W.B. Croft. 1998. A language modeling 
approach to information retrieval. In Proc. of 
SIGIR?98. 
M. A. Pasca and S. M. Harabagiu. 2001. High perfor-
mance question/answering. In Proc. of SIGIR?01, 
pages 366-374. 
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. 
Mercer. 1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2):263-311. 
R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. 
Lytinen, N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question files: 
Experiences with the FAQ finder system. Technical 
report, University of Chicago. 
S. Harabagiu, A. Hickl, J. Lehmann and D. Moldovan. 
2005. Experiments with Interactive Question-
Answering. In Proc. of ACL?05. 
V. Jijkoun, M. D. Rijke. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Y. Cao and H. Li. 2002. Base noun phrase translation 
using web data and the EM algorithm. In Proc. of 
COLING?02. 
Y.-S. Lai, K.-A. Fung, and C.-H. Wu. 2002. Faq mining 
via list detection. In Proc. of the Workshop on Multi-
lingual Summarization and Question Answering, 
2002. 
 
164
Proceedings of ACL-08: HLT, pages 710?718,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Conditional Random Fields to Extract Contexts and Answers of
Questions from Online Forums
Shilin Ding ? ? Gao Cong? ? Chin-Yew Lin? Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China
?Department of Computer Science, Aalborg University, Denmark
?Microsoft Research Asia, Beijing, China
dingsl@gmail.com gaocong@cs.aau.dk
cyl@microsoft.com zxy-dcs@tsinghua.edu.cn
Abstract
Online forum discussions often contain vast
amounts of questions that are the focuses of
discussions. Extracting contexts and answers
together with the questions will yield not only
a coherent forum summary but also a valu-
able QA knowledge base. In this paper, we
propose a general framework based on Con-
ditional Random Fields (CRFs) to detect the
contexts and answers of questions from forum
threads. We improve the basic framework by
Skip-chain CRFs and 2D CRFs to better ac-
commodate the features of forums for better
performance. Experimental results show that
our techniques are very promising.
1 Introduction
Forums are web virtual spaces where people can ask
questions, answer questions and participate in dis-
cussions. The availability of vast amounts of thread
discussions in forums has promoted increasing in-
terests in knowledge acquisition and summarization
for forum threads. Forum thread usually consists
of an initiating post and a number of reply posts.
The initiating post usually contains several ques-
tions and the reply posts usually contain answers to
the questions and perhaps new questions. Forum
participants are not physically co-present, and thus
reply may not happen immediately after questions
are posted. The asynchronous nature and multi-
participants make multiple questions and answers
?This work was done when Shilin Ding was a visiting stu-
dent at the Microsoft Research Asia
?This work was done when Gao Cong worked as a re-
searcher at the Microsoft Research Asia.
<context id=1>S1: Hi I am looking for a pet friendly
hotel in Hong Kong because all of my family is go-
ing there for vacation. S2: my family has 2 sons
and a dog.</context> <question id=1>S3: Is there
any recommended hotel near Sheung Wan or Tsing
Sha Tsui?</question><context id=2,3>S4: We also
plan to go shopping in Causeway Bay.</context>
<question id=2>S5: What?s the traffic situa-
tion around those commercial areas?</question>
<question id=3>S6: Is it necessary to take a
taxi?</question>. S7: Any information would be ap-
preciated.
<answer qid=1>S8: The Comfort Lodge near
Kowloon Park allows pet as I know, and usually fits
well within normal budget. S9: It is also conve-
niently located, nearby the Kowloon railway station
and subway.</answer>
<answer qid=2,3> S10: It?s very crowd in those ar-
eas, so I recommend MTR in Causeway Bay because
it is cheap to take you around </answer>
Figure 1: An example thread with question-context-
answer annotated
interweaved together, which makes it more difficult
to summarize.
In this paper, we address the problem of detecting
the contexts and answers from forum threads for the
questions identified in the same threads. Figure 1
gives an example of a forum thread with questions,
contexts and answers annotated. It contains three
question sentences, S3, S5 and S6. Sentences S1
and S2 are contexts of question 1 (S3). Sentence S4
is the context of questions 2 and 3, but not 1. Sen-
tence S8 is the answer to question 3. (S4-S5-S10) is
one example of question-context-answer triple that
we want to detect in the thread. As shown in the ex-
ample, a forum question usually requires contextual
information to provide background or constraints.
710
Moreover, it sometimes needs contextual informa-
tion to provide explicit link to its answers. For
example, S8 is an answer of question 1, but they
cannot be linked with any common word. Instead,
S8 shares word pet with S1, which is a context of
question 1, and thus S8 could be linked with ques-
tion 1 through S1. We call contextual information
the context of a question in this paper.
A summary of forum threads in the form of
question-context-answer can not only highlight the
main content, but also provide a user-friendly orga-
nization of threads, which will make the access to
forum information easier.
Another motivation of detecting contexts and an-
swers of the questions in forum threads is that it
could be used to enrich the knowledge base of
community-based question and answering (CQA)
services such as Live QnA and Yahoo! Answers,
where context is comparable with the question de-
scription while question corresponds to the question
title. For example, there were about 700,000 ques-
tions in the Yahoo! Answers travel category as of
January 2008. We extracted about 3,000,000 travel
related questions from six online travel forums. One
would expect that a CQA service with large QA data
will attract more users to the service. To enrich the
knowledge base, not only the answers, but also the
contexts are critical; otherwise the answer to a ques-
tion such as How much is the taxi would be useless
without context in the database.
However, it is challenging to detecting contexts
and answers for questions in forum threads. We as-
sume the questions have been identified in a forum
thread using the approach in (Cong et al, 2008).
Although identifying questions in a forum thread is
also nontrivial, it is beyond the focus of this paper.
First, detecting contexts of a question is important
and non-trivial. We found that 74% of questions in
our corpus, which contain 1,064 questions from 579
forum threads about travel, need contexts. However,
relative position information is far from adequate to
solve the problem. For example, in our corpus 63%
of sentences preceding questions are contexts and
they only represent 34% of all correct contexts. To
effectively detect contexts, the dependency between
sentences is important. For example in Figure 1,
both S1 and S2 are contexts of question 1. S1 could
be labeled as context based on word similarity, but it
is not easy to link S2 with the question directly. S1
and S2 are linked by the common word family, and
thus S2 can be linked with question 1 through S1.
The challenge here is how to model and utilize the
dependency for context detection.
Second, it is difficult to link answers with ques-
tions. In forums, multiple questions and answers
can be discussed in parallel and are interweaved to-
gether while the reply relationship between posts is
usually unavailable. To detect answers, we need to
handle two kinds of dependencies. One is the depen-
dency relationship between contexts and answers,
which should be leveraged especially when ques-
tions alone do not provide sufficient information to
find answers; the other is the dependency between
answer candidates (similar to sentence dependency
described above). The challenge is how to model
and utilize these two kinds of dependencies.
In this paper we propose a novel approach for de-
tecting contexts and answers of the questions in fo-
rum threads. To our knowledge this is the first work
on this.We make the following contributions:
First, we employ Linear Conditional Random
Fields (CRFs) to identify contexts and answers,
which can capture the relationships between con-
tiguous sentences.
Second, we also found that context is very im-
portant for answer detection. To capture the depen-
dency between contexts and answers, we introduce
Skip-chain CRF model for answer detection. We
also extend the basic model to 2D CRFs to model
dependency between contiguous questions in a fo-
rum thread for context and answer identification.
Finally, we conducted experiments on forum data.
Experimental results show that 1) Linear CRFs out-
perform SVM and decision tree in both context
and answer detection; 2) Skip-chain CRFs outper-
form Linear CRFs for answer finding, which demon-
strates that context improves answer finding; 3)
2D CRF model improves the performance of Linear
CRFs and the combination of 2D CRFs and Skip-
chain CRFs achieves better performance for context
detection.
The rest of this paper is organized as follows:
The next section discusses related work. Section 3
presents the proposed techniques. We evaluate our
techniques in Section 4. Section 5 concludes this
paper and discusses future work.
711
2 Related Work
There is some research on summarizing discussion
threads and emails. Zhou and Hovy (2005) seg-
mented internet relay chat, clustered segments into
subtopics, and identified responding segments of
the first segment in each sub-topic by assuming
the first segment to be focus. In (Nenkova and
Bagga, 2003; Wan and McKeown, 2004; Rambow
et al, 2004), email summaries were organized by
extracting overview sentences as discussion issues.
Carenini et al(2007) leveraged both quotation re-
lation and clue words for email summarization. In
contrast, given a forum thread, we extract questions,
their contexts, and their answers as summaries.
Shrestha and McKeown (2004)?s work on email
summarization is closer to our work. They used
RIPPER as a classifier to detect interrogative ques-
tions and their answers and used the resulting ques-
tion and answer pairs as summaries. However, it did
not consider contexts of questions and dependency
between answer sentences.
We also note the existing work on extracting
knowledge from discussion threads. Huang et
al.(2007) used SVM to extract input-reply pairs from
forums for chatbot knowledge. Feng et al (2006a)
used cosine similarity to match students? query with
reply posts for discussion-bot. Feng et al (2006b)
identified the most important message in online
classroom discussion board. Our problem is quite
different from the above work.
Detecting context for question in forums is related
to the context detection problem raised in the QA
roadmap paper commissioned by ARDA (Burger et
al., 2006). To our knowledge, none of the previous
work addresses the problem of context detection.
The method of finding follow-up questions (Yang
et al, 2006) from TREC context track could be
adapted for context detection. However, the follow-
up relationship is limited between questions while
context is not. In our other work (Cong et al, 2008),
we proposed a supervised approach for question de-
tection and an unsupervised approach for answer de-
tection without considering context detection.
Extensive research has been done in question-
answering, e.g. (Berger et al, 2000; Jeon et al,
2005; Cui et al, 2005; Harabagiu and Hickl, 2006;
Dang et al, 2007). They mainly focus on con-
structing answer for certain types of question from a
large document collection, and usually apply sophis-
ticated linguistic analysis to both questions and the
documents in the collection. Soricut and Brill (2006)
used statistical translation model to find the appro-
priate answers from their QA pair collections from
FAQ pages for the posted question. In our scenario,
we not only need to find answers for various types
of questions in forum threads but also their contexts.
3 Context and Answer Detection
A question is a linguistic expression used by a ques-
tioner to request information in the form of an an-
swer. The sentence containing request focus is
called question. Context are the sentences contain-
ing constraints or background information to the
question, while answer are that provide solutions. In
this paper, we use sentences as the detection segment
though it is applicable to other kinds of segments.
Given a thread and a set of m detected questions
{Qi}mi=1, our task is to find the contexts and an-
swers for each question. We first discuss using Lin-
ear CRFs for context and answer detection, and then
extend the basic framework to Skip-chain CRFs and
2D CRFs to better model our problem. Finally, we
will briefly introduce CRF models and the features
that we used for CRF model.
3.1 Using Linear CRFs
For ease of presentation, we focus on detecting con-
texts using Linear CRFs. The model could be easily
extended to answer detection.
Context detection. As discussed in Introduction
that context detection cannot be trivially solved by
position information (See Section 4.2 for details),
and dependency between sentences is important for
context detection. Recall that in Figure 1, S2 could
be labeled as context of Q1 if we consider the de-
pendency between S2 and S1, and that between S1
and Q1, while it is difficult to establish connection
between S2 and Q1 without S1. Table 1 shows that
the correlation between the labels of contiguous sen-
tences is significant. In other words, when a sen-
tence Yt?s previous Yt?1 is not a context (Yt?1 6= C)
then it is very likely that Yt (i.e. Yt 6= C) is also not a
context. It is clear that the candidate contexts are not
independent and there are strong dependency rela-
712
Contiguous sentences yt = C yt 6= C
yt?1 = C 901 1,081
yt?1 6= C 1,081 47,190
Table 1: Contingency table(?2 = 9,386,p-value<0.001)
tionships between contiguous sentences in a thread.
Therefore, a desirable model should be able to cap-
ture the dependency.
The context detection can be modeled as a clas-
sification problem. Traditional classification tools,
e.g. SVM, can be employed, where each pair of
question and candidate context will be treated as an
instance. However, they cannot capture the depen-
dency relationship between sentences.
To this end, we proposed a general framework to
detect contexts and answers based on Conditional
Random Fields (Lafferty et al, 2001) (CRFs) which
are able to model the sequential dependencies be-
tween contiguous nodes. A CRF is an undirected
graphical model G of the conditional distribution
P (Y|X). Y are the random variables over the la-
bels of the nodes that are globally conditioned on X,
which are the random variables of the observations.
(See Section 3.4 for more about CRFs)
Linear CRF model has been successfully applied
in NLP and text mining tasks (McCallum and Li,
2003; Sha and Pereira, 2003). However, our prob-
lem cannot be modeled with Linear CRFs in the
same way as other NLP tasks, where one node has a
unique label. In our problem, each node (sentence)
might have multiple labels since one sentence could
be the context of multiple questions in a thread.
Thus, it is difficult to find a solution to tag context
sentences for all questions in a thread in single pass.
Here we assume that questions in a given thread
are independent and are found, and then we can
label a thread with m questions one-by-one in m-
passes. In each pass, one question Qi is selected
as focus and each other sentence in the thread will
be labeled as context C of Qi or not using Linear
CRF model. The graphical representations of Lin-
ear CRFs is shown in Figure2(a). The linear-chain
edges can capture the dependency between two con-
tiguous nodes. The observation sequence x = <x1,
x2,...,xt>, where t is the number of sentences in a
thread, represents predictors (to be described in Sec-
tion 3.5), and the tag sequence y=<y1,...,yt>, where
yi ? {C,P}, determines whether a sentence is plain
text P or context C of question Qi.
Answer detection. Answers usually appear in the
posts after the post containing the question. There
are also strong dependencies between contiguous
answer segments. Thus, position and similarity in-
formation alone are not adequate here. To cope
with the dependency between contiguous answer
segments, Linear CRFs model are employed as in
context detection.
3.2 Leveraging Context for Answer Detection
Using Skip-chain CRFs
We observed in our corpus 74% questions lack con-
straints or background information which are very
useful to link question and answers as discussed in
Introduction. Therefore, contexts should be lever-
aged to detect answers. The Linear CRF model can
capture the dependency between contiguous sen-
tences. However, it cannot capture the long distance
dependency between contexts and answers.
One straightforward method of leveraging context
is to detect contexts and answers in two phases, i.e.
to first identify contexts, and then label answers us-
ing both the context and question information (e.g.
the similarity between context and answer can be
used as features in CRFs). The two-phase proce-
dure, however, still cannot capture the non-local de-
pendency between contexts and answers in a thread.
To model the long distance dependency between
contexts and answers, we will use Skip-chain CRF
model to detect context and answer together. Skip-
chain CRF model is applied for entity extraction
and meeting summarization (Sutton and McCallum,
2006; Galley, 2006). The graphical representation
of a Skip-chain CRF given in Figure2(b) consists
of two types of edges: linear-chain (yt?1 to yt) and
skip-chain edges (yi to yj).
Ideally, the skip-chain edges will establish the
connection between candidate pairs with high prob-
ability of being context and answer of a question.
To introduce skip-chain edges between any pairs of
non-contiguous sentences will be computationally
expensive, and also introduce noise. To make the
cardinality and number of cliques in the graph man-
ageable and also eliminate noisy edges, we would
like to generate edges only for sentence pairs with
high possibility of being context and answer. This is
713
(a) Linear CRFs (b) Skip-chain CRFs (c) 2D CRFs
Figure 2: CRF Models
Skip-Chain yv = A yv 6= A
yu = C 4,105 5,314
yu 6= C 3,744 9,740
Table 2: Contingence table(?2=615.8,p-value < 0.001)
achieved as follows. Given a question Qi in post Pj
of a thread with n posts, its contexts usually occur
within post Pj or before Pj while answers appear in
the posts after Pj . We will establish an edge between
each candidate answer v and one condidate context
in {Pk}jk=1 such that they have the highest possibil-
ity of being a context-answer pair of question Qi:
u = argmax
u?{Pk}jk=1
sim(xu, Qi).sim(xv, {xu, Qi})
here, we use the product of sim(xu, Qi) and
sim(xv, {xu, Qi} to estimate the possibility of be-
ing a context-answer pair for (u, v) , where sim(?, ?)
is the semantic similarity calculated on WordNet as
described in Section 3.5. Table 2 shows that yu and
yv in the skip chain generated by our heuristics in-
fluence each other significantly.
Skip-chain CRFs improve the performance of
answer detection due to the introduced skip-chain
edges that represent the joint probability conditioned
on the question, which is exploited by skip-chain
feature function: f(yu, yv, Qi,x).
3.3 Using 2D CRF Model
Both Linear CRFs and Skip-chain CRFs label the
contexts and answers for each question in separate
passes by assuming that questions in a thread are in-
dependent. Actually the assumption does not hold
in many cases. Let us look at an example. As in Fig-
ure 1, sentence S10 is an answer for both question
Q2 and Q3. S10 could be recognized as the answer
of Q2 due to the shared word areas and Causeway
bay (in Q2?s context, S4), but there is no direct re-
lation between Q3 and S10. To label S10, we need
consider the dependency relation between Q2 and
Q3. In other words, the question-answer relation be-
tween Q3 and S10 can be captured by a joint mod-
eling of the dependency among S10, Q2 and Q3.
The labels of the same sentence for two contigu-
ous questions in a thread would be conditioned on
the dependency relationship between the questions.
Such a dependency cannot be captured by both Lin-
ear CRFs and Skip-chain CRFs.
To capture the dependency between the contigu-
ous questions, we employ 2D CRFs to help context
and answer detection. 2D CRF model is used in
(Zhu et al, 2005) to model the neighborhood de-
pendency in blocks within a web page. As shown
in Figure2(c), 2D CRF models the labeling task for
all questions in a thread. For each thread, there are
m rows in the grid, where the ith row corresponds
to one pass of Linear CRF model (or Skip-chain
model) which labels contexts and answers for ques-
tion Qi. The vertical edges in the figure represent
the joint probability conditioned on the contiguous
questions, which will be exploited by 2D feature
function: f(yi,j , yi+1,j , Qi, Qi+1,x). Thus, the in-
formation generated in single CRF chain could be
propagated over the whole grid. In this way, context
and answer detection for all questions in the thread
could be modeled together.
3.4 Conditional Random Fields (CRFs)
The Linear, Skip-Chain and 2D CRFs can be gen-
eralized as pairwise CRFs, which have two kinds of
cliques in graph G: 1) node yt and 2) edge (yu, yv).
The joint probability is defined as:
p(y|x)= 1Z(x) exp
{?
k,t
?kfk(yt,x)+
?
k,t
?kgk(yu, yv,x)
}
714
where Z(x) is the normalization factor, fk is the
feature on nodes, gk is on edges between u and v,
and ?k and ?k are parameters.
Linear CRFs are based on the first order Markov
assumption that the contiguous nodes are dependent.
The pairwise edges in Skip-chain CRFs represent
the long distance dependency between the skipped
nodes, while the ones in 2D CRFs represent the de-
pendency between the neighboring nodes.
Inference and Parameter Estimation. For Linear
CRFs, dynamic programming is used to compute the
maximum a posteriori (MAP) of y given x. How-
ever, for more complicated graphs with cycles, ex-
act inference needs the junction tree representation
of the original graph and the algorithm is exponen-
tial to the treewidth. For fast inference, loopy Belief
Propagation (Pearl, 1988) is implemented.
Given the training Data D = {x(i),y(i)}ni=1, the
parameter estimation is to determine the parame-
ters based on maximizing the log-likelihood L? =?n
i=1 log p(y(i)|x(i)). In Linear CRF model, dy-
namic programming and L-BFGS (limited memory
Broyden-Fletcher-Goldfarb-Shanno) can be used to
optimize objective function L?, while for compli-
cated CRFs, Loopy BP are used instead to calculate
the marginal probability.
3.5 Features used in CRF models
The main features used in Linear CRF models for
context detection are listed in Table 3.
The similarity feature is to capture the word sim-
ilarity and semantic similarity between candidate
contexts and answers. The word similarity is based
on cosine similarity of TF/IDF weighted vectors.
The semantic similarity between words is computed
based on Wu and Palmer?s measure (Wu and Palmer,
1994) using WordNet (Fellbaum, 1998).1 The simi-
larity between contiguous sentences will be used to
capture the dependency for CRFs. In addition, to
bridge the lexical gaps between question and con-
text, we learned top-3 context terms for each ques-
tion term from 300,000 question-description pairs
obtained from Yahoo! Answers using mutual infor-
mation (Berger et al, 2000) ( question description
in Yahoo! Answers is comparable to contexts in fo-
1The semantic similarity between sentences is calculated as
in (Yang et al, 2006).
Similarity features:
? Cosine similarity with the question
? Similarity with the question using WordNet
? Cosine similarity between contiguous sentences
? Similarity between contiguous sentences using WordNet
? Cosine similarity with the expanded question using the lexical
matching words
Structural features:
? The relative position to current question
? Is its author the same with that of the question?
? Is it in the same paragraph with its previous sentence?
Discourse and lexical features:
? The number of Pronouns in the question
? The presence of fillers, fluency devices (e.g. ?uh?, ?ok?)
? The presence of acknowledgment tokens
? The number of non-stopwords
? Whether the question has a noun or not?
? Whether the question has a verb or not?
Table 3: Features for Linear CRFs. Unless otherwise
mentioned, we refer to features of the sentence whose la-
bel to be predicted
rums), and then use them to expand question and
compute cosine similarity.
The structural features of forums provide strong
clues for contexts. For example, contexts of a ques-
tion usually occur in the post containing the question
or preceding posts.
We extracted the discourse features from a ques-
tion, such as the number of pronouns in the question.
A more useful feature would be to find the entity in
surrounding sentences referred by a pronoun. We
tried GATE (Cunningham et al, 2002) for anaphora
resolution of the pronouns in questions, but the per-
formance became worse with the feature, which is
probably due to the difficulty of anaphora resolution
in forum discourse. We also observed that questions
often need context if the question do not contain a
noun or a verb.
In addition, we use similarity features between
skip-chain sentences for Skip-chain CRFs and simi-
larity features between questions for 2D CRFs.
4 Experiments
4.1 Experimental setup
Corpus. We obtained about 1 million threads
from TripAdvisor forum; we randomly selected 591
threads and removed 22 threads which has more than
40 sentences and 6 questions; the remaining 579 fo-
rum threads form our corpus 2. Each thread in our
2TripAdvisor (http://www.tripadvisor.com/ForumHome) is
one of the most popular travel forums; the list of 579 urls is
715
Model Prec(%) Rec(%) F1(%)
Context Detection
SVM 75.27 68.80 71.32
C4.5 70.16 64.30 67.21
L-CRF 75.75 72.84 74.45
Answer Detection
SVM 73.31 47.35 57.52
C4.5 65.36 46.55 54.37
L-CRF 63.92 58.74 61.22
Table 4: Context and Answer Detection
corpus contains at least two posts and on average
each thread consists of 3.87 posts. Two annotators
were asked to tag questions, their contexts, and an-
swers in each thread. The kappa statistic for identi-
fying question is 0.96, for linking context and ques-
tion given a question is 0.75, and for linking answer
and question given a question is 0.69. We conducted
experiments on both the union and intersection of
the two annotated data. The experimental results on
both data are qualitatively comparable. We only re-
port results on union data due to space limitation.
The union data contains 1,064 questions, 1,458 con-
texts and 3,534 answers.
Metrics. We calculated precision, recall,
and F1-score for all tasks. All the experimental
results are obtained through the average of 5 trials
of 5-fold cross validation.
4.2 Experimental results
Linear CRFs for Context and Answer Detection.
This experiment is to evaluate Linear CRF model
(Section 3.1) for context and answer detection by
comparing with SVM and C4.5(Quinlan, 1993). For
SVM, we use SVMlight(Joachims, 1999). We tried
linear, polynomial and RBF kernels and report the
results on polynomial kernel using default param-
eters since it performs the best in the experiment.
SVM and C4.5 use the same set of features as Lin-
ear CRFs. As shown in Table 4, Linear CRF model
outperforms SVM and C4.5 for both context and an-
swer detection. The main reason for the improve-
ment is that CRF models can capture the sequen-
tial dependency between segments in forums as dis-
cussed in Section 3.1.
given in http://homepages.inf.ed.ac.uk/gcong/acl08/; Removing
the 22 long threads can greatly reduce the training and test time.
position Prec(%) Rec(%) F1(%)
Context Detection
Previous One 63.69 34.29 44.58
Previous All 43.48 76.41 55.42
Anwer Detection
Following One 66.48 19.98 30.72
Following All 31.99 100 48.48
Table 5: Using position information for detection
Context Prec(%) Rec(%) F1(%)
No context 63.92 58.74 61.22
Prev. sentence 61.41 62.50 61.84
Real context 63.54 66.40 64.94
L-CRF+context 65.51 63.13 64.06
Table 6: Contextual Information for Answer Detection.
Prev. sentence uses one previous sentence of the current
question as context. RealContext uses the context anno-
tated by experts. L-CRF+context uses the context found
by Linear CRFs
We next report a baseline of context detection
using previous sentences in the same post with its
question since contexts often occur in the question
post or preceding posts. Similarly, we report a base-
line of answer detecting using following segments of
a question as answers. The results given in Table 5
show that location information is far from adequate
to detect contexts and answers.
The usefulness of contexts. This experiment is to
evaluate the usefulness of contexts in answer de-
tection, by adding the similarity between the con-
text (obtained with different methods) and candi-
date answer as an extra feature for CRFs. Table 6
shows the impact of context on answer detection
using Linear CRFs. Linear CRFs with contextual
information perform better than those without con-
text. L-CRF+context is close to that using real con-
text, while it is better than CRFs using the previous
sentence as context. The results clearly shows that
contextual information greatly improves the perfor-
mance of answer detection.
Improved Models. This experiment is to evaluate
the effectiveness of Skip-Chain CRFs (Section 3.2)
and 2D CRFs (Section 3.3) for our tasks. The results
are given in Table 7 and Table 8.
In context detection, Skip-Chain CRFs have simi-
716
Model Prec(%) Rec(%) F1(%)
L-CRF+Context 75.75 72.84 74.45
Skip-chain 74.18 74.90 74.42
2D 75.92 76.54 76.41
2D+Skip-chain 76.27 78.25 77.34
Table 7: Skip-chain and 2D CRFs for context detection
lar results as Linear CRFs, i.e. the inter-dependency
captured by the skip chains generated using the
heuristics in Section 3.2 does not improve the con-
text detection. The performance of Linear CRFs is
improved in 2D CRFs (by 2%) and 2D+Skip-chain
CRFs (by 3%) since they capture the dependency be-
tween contiguous questions.
In answer detection, as expected, Skip-chain
CRFs outperform L-CRF+context since Skip-chain
CRFs can model the inter-dependency between con-
texts and answers while in L-CRF+context the con-
text can only be reflected by the features on the ob-
servations. We also observed that 2D CRFs improve
the performance of L-CRF+context due to the de-
pendency between contiguous questions. In contrast
with our expectation, the 2D+Skip-chain CRFs does
not improve Skip-chain CRFs in terms of answer de-
tection. The possible reason could be that the struc-
ture of the graph is very complicated and too many
parameters need to be learned on our training data.
Evaluating Features. We also evaluated the con-
tributions of each category of features in Table 3
to context detection. We found that similarity fea-
tures are the most important and structural feature
the next. We also observed the same trend for an-
swer detection. We omit the details here due to space
limitation.
As a summary, 1) our CRF model outperforms
SVM and C4.5 for both context and answer detec-
tions; 2) context is very useful in answer detection;
3) the Skip-chain CRF method is effective in lever-
aging context for answer detection; and 4) 2D CRF
model improves the performance of Linear CRFs for
both context and answer detection.
5 Discussions and Conclusions
We presented a new approach to detecting contexts
and answers for questions in forums with good per-
formance. We next discuss our experience not cov-
ered by the experiments, and future work.
Model Prec(%) Rec(%) F1(%)
L-CRF+context 65.51 63.13 64.06
Skip-chain 67.59 71.06 69.40
2D 65.77 68.17 67.34
2D+Skip-chain 66.90 70.56 68.89
Table 8: Skip-chain and 2D CRFs for answer detection
Since contexts of questions are largely unexplored
in previous work, we analyze the contexts in our
corpus and classify them into three categories: 1)
context contains the main content of question while
question contains no constraint, e.g. ?i will visit NY at
Oct, looking for a cheap hotel but convenient. Any good
suggestion? ?; 2) contexts explain or clarify part of
the question, such as a definite noun phrase, e.g. ?We
are going on the Taste of Paris. Does anyone know if it is
advisable to take a suitcase with us on the tour., where
the first sentence is to describe the tour; and 3) con-
texts provide constraint or background for question
that is syntactically complete, e.g. ?We are inter-
ested in visiting the Great Wall(and flying from London).
Can anyone recommend a tour operator.? In our corpus,
about 26% questions do not need context, 12% ques-
tions need Type 1 context, 32% need Type 2 context
and 30% Type 3. We found that our techniques often
do not perform well on Type 3 questions.
We observed that factoid questions, one of fo-
cuses in the TREC QA community, take less than
10% question in our corpus. It would be interesting
to revisit QA techniques to process forum data.
Other future work includes: 1) to summarize mul-
tiple threads using the triples extracted from indi-
vidual threads. This could be done by clustering
question-context-answer triples; 2) to use the tradi-
tional text summarization techniques to summarize
the multiple answer segments; 3) to integrate the
Question Answering techniques as features of our
framework to further improve answer finding; 4) to
reformulate questions using its context to generate
more user-friendly questions for CQA services; and
5) to evaluate our techniques on more online forums
in various domains.
Acknowledgments
We thank the anonymous reviewers for their detailed
comments, and Ming Zhou and Young-In Song for
their valuable suggestions in preparing the paper.
717
References
A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. Mit-
tal. 2000. Bridging the lexical chasm: statistical ap-
proaches to answer-finding. In Proceedings of SIGIR.
J. Burger, C. Cardie, V. Chaudhri, R. Gaizauskas,
S. Harabagiu, D. Israel, C. Jacquemin, C. Lin,
S. Maiorano, G. Miller, D. Moldovan, B. Ogden,
J. Prager, E. Riloff, A. Singhal, R. Shrihari, T. Strza-
lkowski16, E. Voorhees, and R. Weishedel. 2006. Is-
sues, tasks and program structures to roadmap research
in question and answering (qna). ARAD: Advanced
Research and Development Activity (US).
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proceedings
of WWW.
G. Cong, L. Wang, C.Y. Lin, Y.I. Song, and Y. Sun. 2008.
Finding question-answer pairs from online forums. In
Proceedings of SIGIR.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion answering passage retrieval using dependency re-
lations. In Proceedings of SIGIR.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: A framework and graphical
development environment for robust nlp tools and ap-
plications. In Proceedings of ACL.
H. Dang, J. Lin, and D. Kelly. 2007. Overview of the
trec 2007 question answering track. In Proceedings of
TREC.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database (Language, Speech, and Communica-
tion). The MIT Press, May.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006a. An intel-
ligent discussion-bot for answering student queries in
threaded discussions. In Proceedings of IUI.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006b. Learning
to detect conversation focus of threaded discussions.
In Proceedings of HLT-NAACL.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Pro-
ceedings of EMNLP.
S. Harabagiu and A. Hickl. 2006. Methods for using tex-
tual entailment in open-domain question answering.
In Proceedings of ACL.
J. Huang, M. Zhou, and D. Yang. 2007. Extracting chat-
bot knowledge from online discussion forums. In Pro-
ceedings of IJCAI.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proceedings of CIKM.
T. Joachims. 1999. Making large-scale support vector
machine learning practical. MIT Press, Cambridge,
MA, USA.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In Pro-
ceedings of CoNLL-2003.
A. Nenkova and A. Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
Proceedings of RANLP.
J. Pearl. 1988. Probabilistic reasoning in intelligent sys-
tems: networks of plausible inference. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
J. Quinlan. 1993. C4.5: programs for machine learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proceedings of
HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL.
L. Shrestha and K. McKeown. 2004. Detection of
question-answer pairs in email conversations. In Pro-
ceedings of COLING.
R. Soricut and E. Brill. 2006. Automatic question an-
swering using the web: Beyond the Factoid. Informa-
tion Retrieval, 9(2):191?206.
C. Sutton and A. McCallum. 2006. An introduction to
conditional random fields for relational learning. In
Lise Getoor and Ben Taskar, editors, Introduction to
Statistical Relational Learning. MIT Press. To appear.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of COLING.
Z. Wu and M. S. Palmer. 1994. Verb semantics and lexi-
cal selection. In Proceedings of ACL.
F. Yang, J. Feng, and G. Fabbrizio. 2006. A data
driven approach to relevancy recognition for contex-
tual question answering. In Proceedings of the Inter-
active Question Answering Workshop at HLT-NAACL
2006.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet relay
chats. In Proceedings of ACL.
J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2005. 2d
conditional random fields for web information extrac-
tion. In Proceedings of ICML.
718
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 281?284,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Efficient Inference of CRFs for Large-Scale Natural Language Data
Minwoo Jeong
??
Chin-Yew Lin
?
Gary Geunbae Lee
?
?
Pohang University of Science & Technology, Pohang, Korea
?
Microsoft Research Asia, Beijing, China
?
{stardust,gblee}@postech.ac.kr
?
cyl@microsoft.com
Abstract
This paper presents an efficient inference algo-
rithm of conditional random fields (CRFs) for
large-scale data. Our key idea is to decompose
the output label state into an active set and an
inactive set in which most unsupported tran-
sitions become a constant. Our method uni-
fies two previous methods for efficient infer-
ence of CRFs, and also derives a simple but
robust special case that performs faster than
exact inference when the active sets are suffi-
ciently small. We demonstrate that our method
achieves dramatic speedup on six standard nat-
ural language processing problems.
1 Introduction
Conditional random fields (CRFs) are widely used in
natural language processing, but extending them to
large-scale problems remains a significant challenge.
For simple graphical structures (e.g. linear-chain), an
exact inference can be obtained efficiently if the num-
ber of output labels is not large. However, for large
number of output labels, the inference is often pro-
hibitively expensive.
To alleviate this problem, researchers have begun to
study the methods of increasing inference speeds of
CRFs. Pal et al (2006) proposed a Sparse Forward-
Backward (SFB) algorithm, in which marginal distribu-
tion is compressed by approximating the true marginals
using Kullback-Leibler (KL) divergence. Cohn (2006)
proposed a Tied Potential (TP) algorithm which con-
strains the labeling considered in each feature function,
such that the functions can detect only a relatively small
set of labels. Both of these techniques efficiently com-
pute the marginals with a significantly reduced runtime,
resulting in faster training and decoding of CRFs.
This paper presents an efficient inference algorithm
of CRFs which unifies the SFB and TP approaches. We
first decompose output labels states into active and in-
active sets. Then, the active set is selected by feasible
heuristics and the parameters of the inactive set are held
a constant. The idea behind our method is that not all
of the states contribute to the marginals, that is, only a
?
Parts of this work were conducted during the author?s
internship at Microsoft Research Asia.
small group of the labeling states has sufficient statis-
tics. We show that the SFB and the TP are special cases
of our method because they derive from our unified al-
gorithm with a different setting of parameters. We also
present a simple but robust variant algorithm in which
CRFs efficiently learn and predict large-scale natural
language data.
2 Linear-chain CRFs
Many versions of CRFs have been developed for use
in natural language processing, computer vision, and
machine learning. For simplicity, we concentrate on
linear-chain CRFs (Lafferty et al, 2001; Sutton and
McCallum, 2006), but the generic idea described here
can be extended to CRFs of any structure.
Linear-chain CRFs are conditional probability dis-
tributions over label sequences which are conditioned
on input sequences (Lafferty et al, 2001). Formally,
x = {x
t
}
T
t=1
and y = {y
t
}
T
t=1
are sequences of in-
put and output variables. Respectively, where T is the
length of sequence, x
t
? X and y
t
? Y where X is the
finite set of the input observations and Y is that of the
output label state space. Then, a first-order linear-chain
CRF is defined as:
p
?
(y|x) =
1
Z(x)
T
?
t=1
?
t
(y
t
, y
t?1
,x), (1)
where ?
t
is the local potential that denotes the factor
at time t, and ? is the parameter vector. Z(x) is a
partition function which ensures the probabilities of all
state sequences sum to one. We assume that the poten-
tials factorize according to a set of observation features
{?
1
k
} and transition features {?
2
k
}, as follows:
?
t
(y
t
, y
t?1
,x) =?
1
t
(y
t
,x) ??
2
t
(y
t
, y
t?1
), (2)
?
1
t
(y
t
,x) =e
?
k
?
1
k
?
1
k
(y
t
,x)
, (3)
?
2
t
(y
t
, y
t?1
) =e
?
k
?
2
k
?
2
k
(y
t
,y
t?1
)
, (4)
where {?
1
k
} and {?
2
k
} are weight parameters which we
wish to learn from data.
Inference is significantly challenging both in learn-
ing and decoding CRFs. Time complexity is O(T |Y|
2
)
for exact inference (i.e., forward-backward and Viterbi
algorithm) of linear-chain CRFs (Lafferty et al, 2001).
The inference process is often prohibitively expensive
281
when |Y| is large, as is common in large-scale tasks.
This problem can be alleviated by introducing approx-
imate inference methods based on reduction of the
search spaces to be explored.
3 Efficient Inference Algorithm
3.1 Method
The key idea of our proposed efficient inference
method is that the output label state Y can be decom-
posed to an active set A and an inactive set A
c
. Intu-
itively, many of the possible transitions (y
t?1
? y
t
) do
not occur, or are unsupported, that is, only a small part
of the possible labeling set is informative. The infer-
ence algorithm need not precisely calculate marginals
or maximums (more generally, messages) for unsup-
ported transitions. Our efficient inference algorithm
approximates the unsupported transitions by assigning
them a constant value. When |A| < |Y|, both train-
ing and decoding times are remarkably reduced by this
approach.
We first define the notation for our algorithm. Let
A
i
be the active set and A
c
i
be the inactive set of output
label i where Y
i
= A
i
? A
c
i
. We define A
i
as:
A
i
= {j|?(y
t
= i, y
t?1
= j) > ?} (5)
where ? is a criterion function of transitions (y
t?1
?
y
t
) and ? is a hyperparameter. For clarity, we define the
local factors as:
?
1
t,i
, ?
1
t
(y
t
= i,x), (6)
?
2
j,i
, ?
2
t
(y
t?1
= j, y
t
= i). (7)
Note that we can ignore the subscript t at ?
2
t
(y
t?1
=
j, y
t
= i) by defining an HMM-like model, that is,
transition matrix ?
2
j,i
is independent of t.
As exact inference, we use the forward-backward
procedure to calculate marginals (Sutton and McCal-
lum, 2006). We formally describe here an efficient
calculation of ? and ? recursions for the forward-
backward procedure. The forward value ?
t
(i) is the
sum of the unnormalized scores for all partial paths that
start at t = 0 and converge at y
t
= i at time t. The
backward value ?
t
(i) similarly defines the sum of un-
normalized scores for all partial paths that start at time
t + 1 with state y
t+1
= j and continue until the end
of the sequences, t = T + 1. Then, we decompose the
equations of exact ? and ? recursions as follows:
?
t
(i) = ?
1
t,i
?
?
?
j?A
i
(
?
2
j,i
? ?
)
?
t?1
(j) + ?
?
?
, (8)
?
t?1
(j) =
?
i?A
j
?
1
t,i
(
?
2
j,i
? ?
)
?
t
(i) + ?
?
i?Y
?
1
t,i
?
t
(i),
(9)
where ? is a shared transition parameter value for set
A
c
i
, that is, ?
2
j,i
= ? if j ? A
c
i
. Note that
?
i
?
t
(i) = 1
(Sutton and McCallum, 2006). Because all unsup-
ported transitions in A
c
i
are calculated simultaneously,
the complexities of Eq. (8) and (9) are approximately
O(T |A
avg
||Y|) where |A
avg
| is the average number of
states in the active set, i.e.,
1
T
?
T
t=1
|A
i
|. The worst
case complexity of our ? and ? equations is O(T |Y|
2
).
Similarly, we decompose a ? recursion for the
Viterbi algorithm as follows:
?
t
(i) = ?
1
t,i
{
max
(
max
j?A
i
?
2
j,i
?
t?1
(j),max
j?Y
??
t?1
(j)
)}
,
(10)
where ?
t
(i) is the sum of unnormalized scores for the
best-scored partial path that starts at time t = 0 and
converges at y
t
= i at time t. Because ? is constant,
max
j?Y
?
t?1
(j) can be pre-calculated at time t ? 1.
By analogy with Eq. (8) and (9), the complexity is ap-
proximately O(T |A
avg
||Y|).
3.2 Setting ? and ?
To implement our inference algorithm, we need a
method of choosing appropriate values for the setting
function ? of the active set and for the constant value
? of the inactive set. These two problems are closely
related. The size of the active set affects both the com-
plexity of inference algorithm and the quality of the
model. Therefore, our goal for selecting ? and ? is
to make a plausible assumption that does not sacrifice
much accuracy but speeds up when applying large state
tasks. We describe four variant special case algorithms.
Method 1: We set ?(i, j) = Z(L) and ? = 0 where
L is a beam set, L = {l
1
, l
2
, . . . , l
m
} and the sub-
partition function Z(L) is approximated by Z(L) ?
?
t?1
(j). In this method, all sub-marginals in the inac-
tive set are totally excluded from calculation of the cur-
rent marginal. ? and ? in the inactive sets are set to 0
by default. Therefore, at each time step t the algorithm
prunes all states i in which ?
t
(i) < ?. It also generates
a subset L of output labels that will be exploited in next
time step t + 1.
1
This method has been derived the-
oretically from the process of selecting a compressed
marginal distribution within a fixed KL divergence of
the true marginal (Pal et al, 2006). This method most
closely resembles SFB algorithm; hence we refer an al-
ternative of SFB.
Method 2: We define ?(i, j) = |?
2
j,i
?1| and ? = 1.
In practice, unsupported transition features are not pa-
rameterized
2
; this means that ?
k
= 0 and ?
2
j,i
= 1
if j ? A
c
i
. Thus, this method estimates nearly-exact
1
In practice, dynamically selecting L increases the num-
ber of computations, and this is the main disadvantage of
Method 1. However, in inactive sets ?
t?1
(j) = 0 by de-
fault; hence, we need not calculate ?
t?1
(j). Therefore, it
counterbalances the extra computations in ? recursion.
2
This is a common practice in implementation of input
and output joint feature functions for large-scale problems.
This scheme uses only supported features that are used at
least once in the training examples. We call it the sparse
model. While a complete and dense feature model may per-
282
CRFs if the hyperparameter is ? = 0; hence this cri-
terion does not change the parameter. Although this
method is simple, it is sufficiently efficient for training
and decoding CRFs in real data.
Method 3: We define ?(i, j) = E
p?
??
2
k
(i, j)? where
E
p?
?z? is an empirical count of event z in training data.
We also assign a real value for the inactive set, i.e.,
? = c ? R, c 6= 0, 1. The value c is estimated in the
training phase; hence, c is a shared parameter for the
inactive set. This method is equivalent to TP (Cohn,
2006). By setting ? larger, we can achieve faster infer-
ence, a tradeoff exists between efficiency and accuracy.
Method 4: We define the shared parameter as a func-
tion of output label y in the inactive set, i.e., c(y). As in
Method 3, c(y) is estimated during the training phase.
When the problem expects different aspects of unsup-
ported transitions, this method would be better than us-
ing only one parameter c for all labels in inactive set.
4 Experiment
We evaluated our method on six large-scale natu-
ral language data sets (Table 1): Penn Treebank
3
for part-of-speech tagging (PTB), phrase chunk-
ing data
4
(CoNLL00), named entity recognition
data
5
(CoNLL03), grapheme-to-phoneme conversion
data
6
(NetTalk), spoken language understanding data
(Communicator) (Jeong and Lee, 2006), and fine-
grained named entity recognition data (Encyclopedia)
(Lee et al, 2007). The active set is sufficiently small in
Communicator and Encyclopedia despite their large
numbers of output labels. In all data sets, we selected
the current word, ?2 context words, bigrams, trigrams,
and prefix and suffix features as basic feature templates.
A template of part-of-speech tag features was added for
CoNLL00, CoNLL03, and Encyclopedia. In particu-
lar, all tasks except PTB and NetTalk require assigning
a label to a phrase rather than to a word; hence, we used
standard ?BIO? encoding. We used un-normalized log-
likelihood, accuracy and training/decoding times as our
evaluation measures. We did not use cross validation
and development set for tuning the parameter because
our goal is to evaluate the efficiency of inference algo-
rithms. Moreover, using the previous state-of-the-art
features we expect the achievement of better accuracy.
All our models were trained until parameter estima-
tion converged with a Gaussian prior variance of 4.
During training, a pseudo-likelihood parameter estima-
tion (Sutton and McCallum, 2006) was used as an ini-
tial weight (estimated in 30 iterations). We used com-
plete and dense input/output joint features for dense
model (Dense), and only supported features that are
used at least once in the training examples for sparse
form better, the sparse model performs well in practice with-
out significant loss of accuracy (Sha and Pereira, 2003).
3
Penn Treebank3: Catalog No. LDC99T42
4
http://www.cnts.ua.ac.be/conll2000/chunking/
5
http://www.cnts.ua.ac.be/conll2003/ner/
6
http://archive.ics.uci.edu/ml/
Table 1: Data sets: number of sentences in the train-
ing (#Train) and the test data sets (#Test), and number
of output labels (#Label). |A
?=1
avg
| denotes the average
number of active set when ? = 1, i.e., the supported
transitions that are used at least once in the training set.
Set #Train #Test #Label |A
?=1
avg
|
PTB 38,219 5462 45 30.01
CoNLL00 8,936 2,012 22 6.59
CoNLL03 14,987 3,684 8 4.13
NetTalk 18,008 2,000 51 22.18
Communicator 13,111 1,193 120 3.67
Encyclopedia 25,348 6,336 279 3.27
model (Sparse). All of our model variants were based
on Sparse model. For the hyper parameter ?, we empir-
ically selected 0.001 for Method 1 (this preserves 99%
of probability density), 0 for Method 2, and 4 for Meth-
ods 3 and 4. Note that ? for Methods 2, 3, and 4 indi-
cates an empirical count of features in training set. All
experiments were implemented in C++ and executed in
Windows 2003 with XEON 2.33 GHz Quad-Core pro-
cessor and 8.0 Gbyte of main memory.
We first show that our method is efficient for learning
CRFs (Figure 1). In all learning curves, Dense gener-
ally has a higher training log-likelihood than Sparse.
For PTB and Encyclopedia, results for Dense are not
available because training in a single machine failed
due to out-of-memory errors. For both Dense and
Sparse, we executed the exact inference method. Our
proposed method (Method 1?4) performs faster than
Sparse. In most results, Method 1 was the fastest, be-
cause it was terminated after fewer iterations. How-
ever, Method 1 sometimes failed to converge, for ex-
ample, in Encyclopedia. Similarly, Method 3 and 4
could not find the optimal solution in the NetTalk data
set. Method 2 showed stable results.
Second, we evaluated the accuracy and decoding
time of our methods (Table 2). Most results obtained
using our method were as accurate as those of Dense
and Sparse. However, some results of Method 1, 3,
and 4 were significantly inferior to those of Dense and
Sparse for one of two reasons: 1) parameter estimation
failed (NetTalk and Encyclopedia), or 2) approximate
inference caused search errors (CoNLL00 and Com-
municator). The improvements of decoding time on
Communicator and Encyclopedia were remarkable.
Finally, we compared our method with two open-
source implementations of CRFs: MALLET
7
and
CRF++
8
. MALLET can support the Sparse model, and
the CRF++ toolkit implements only the Dense model.
We compared them with Method 2 on the Commu-
nicator data set. In the accuracy measure, the re-
sults were 91.56 (MALLET), 91.87 (CRF++), and 91.92
(ours). Our method performs 5?50 times faster for
training (1,774 s for MALLET, 18,134 s for CRF++,
7
Ver. 2.0 RC3, http://mallet.cs.umass.edu/
8
Ver. 0.51, http://crfpp.sourceforge.net/
283
0 10000 20000 30000 40000?
1400
00
?
1000
00
Training time (sec)
Log?
likelih
ood
SparseMethod 1Method 2Method 3Method 4
(a) PTB
0 500 1500 2500?
1000
0
?
6000
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(b) CoNLL00
0 500 1000 1500?1
4000
?
1000
0?
6000
?
2000
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(c) CoNLL03
0 1000 3000 5000
?
4100
0
?
3900
0
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(d) NetTalk
0 1000 3000 5000?75
00
?
6500
?
5500
?
4500
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(e) Communicator
0 100000 200000 300000
?
3000
0?
2000
0?
1000
0
Training time (sec)
Log?
likelih
ood
SparseMethod 1Method 2Method 3Method 4
(f) Encyclopedia
Figure 1: Result of training linear-chain CRFs: Un-normalized training log-likelihood and training times are
compared. Dashed lines denote the termination of training step.
Table 2: Decoding result; columns are percent accuracy (Acc), and decoding time in milliseconds (Time) measured
per testing example. ?
?
? indicates that the result is significantly different from the Sparse model. N/A indicates
failure due to out-of-memory error.
Method
PTB CoNLL00 CoNLL03 NetTalk Communicator Encyclopedia
Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time
Dense N/A N/A 96.1 0.89 95.8 0.26 88.4 0.49 91.6 0.94 N/A N/A
Sparse 96.6 1.12 95.9 0.62 95.9 0.21 88.4 0.44 91.9 0.83 93.6 34.75
Method 1 96.8 0.74 95.9 0.55
?
94.0 0.24
?
88.3 0.34 91.7 0.73
?
69.2 15.77
Method 2 96.6 0.92
?
95.7 0.52 95.9 0.21
?
87.4 0.32 91.9 0.30 93.6 4.99
Method 3 96.5 0.84
?
94.2 0.51 95.9 0.24
?
78.2 0.29
?
86.7 0.30 93.7 6.14
Method 4 96.6 0.85
?
92.1 0.51 95.9 0.24
?
77.9 0.30 91.9 0.29 93.3 4.88
and 368 s for ours) and 7?12 times faster for decod-
ing (2.881 ms for MALLET, 5.028 ms for CRF++, and
0.418 ms for ours). This result demonstrates that learn-
ing and decoding CRFs for large-scale natural language
problems can be efficiently solved using our method.
5 Conclusion
We have demonstrated empirically that our efficient in-
ference method can function successfully, allowing for
a significant speedup of computation. Our method links
two previous algorithms, the SFB and the TP. We have
also showed that a simple and robust variant method
(Method 2) is effective in large-scale problems.
9
The
empirical results show a significant improvement in
the training and decoding speeds especially when the
problem has a large state space of output labels. Fu-
ture work will consider applications to other large-scale
problems, and more-general graph topologies.
9
Code used in this work is available at
http://argmax.sourceforge.net/.
References
T. Cohn. 2006. Efficient inference in large conditional ran-
dom fields. In Proc. ECML, pages 606?613.
M. Jeong and G. G. Lee. 2006. Exploiting non-local fea-
tures for spoken language understanding. In Proc. of COL-
ING/ACL, pages 412?419, Sydney, Australia, July.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282?289.
C. Lee, Y. Hwang, and M. Jang. 2007. Fine-grained named
entity recognition and relation extraction for question an-
swering. In Proc. SIGIR Poster, pages 799?800.
C. Pal, C. Sutton, and A. McCallum. 2006. Sparse forward-
backward using minimum divergence beams for fast train-
ing of conditional random fields. In Proc. ICASSP.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL/HLT, pages 134?141.
C. Sutton and A. McCallum. 2006. An introduction to condi-
tional random fields for relational learning. In Lise Getoor
and Ben Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press, Cambridge, MA.
284
Proceedings of the Second Workshop on Statistical Machine Translation, pages 240?247,
Prague, June 2007. c?2007 Association for Computational Linguistics
Sentence Level Machine Translation Evaluation as a Ranking Problem: one
step aside from BLEU
Yang Ye
University of Michigan
yye@umich.edu
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Chin-Yew Lin
Microsoft Research Asia
cyl@microsoft.com
Abstract
The paper proposes formulating MT evalu-
ation as a ranking problem, as is often done
in the practice of assessment by human. Un-
der the ranking scenario, the study also in-
vestigates the relative utility of several fea-
tures. The results show greater correlation
with human assessment at the sentence level,
even when using an n-gram match score as
a baseline feature. The feature contributing
the most to the rank order correlation be-
tween automatic ranking and human assess-
ment was the dependency structure relation
rather than BLEU score and reference lan-
guage model feature.
1 Introduction
In recent decades, alongside the growing research
on Machine Translation (MT), automatic MT evalu-
ation has become a critical problem for MT system
developers, who are interested in quick turnaround
development cycles. The state-of-the-art automatic
MT evaluation is an n-gram based metric repre-
sented by BLEU (Papineni et al, 2001) and its vari-
ants. Ever since its creation, the BLEU score has
been the gauge of Machine Translation system eval-
uation. Nevertheless, the research community has
been largely aware of the deficiency of the BLEU
metric. BLEU captures only a single dimension
of the vitality of natural languages: a candidate
translation gets acknowledged only if it uses ex-
actly the same lexicon as the reference translation.
Natural languages, however, are characterized by
their extremely rich mechanisms for reproduction
via a large number of syntactic, lexical and semantic
rewriting rules. Although BLEU has been shown
to correlate positively with human assessments at
the document level (Papineni et al, 2001), efforts to
improve state-of-the-art MT require that human as-
sessment be approximated at sentence level as well.
Researchers report the BLEU score at document
level in order to combat the sparseness of n-grams
in BLEU scoring. But, ultimately, document-level
MT evaluation has to be pinned down to the gran-
ularity of the sentence. Unfortunately, the corre-
lation between human assessment and BLEU score
at sentence level is extremely low (Liu et al, 2005,
2006). While acknowledging the appealing simplic-
ity of BLEU as a way to access one perspective of an
MT candidate translation?s quality, we observe the
following facts of n-gram based MT metrics. First,
they may not reflect the mechanism of how human
beings evaluate sentence translation quality. More
specifically, optimizing BLEU does not guarantee
the optimization of sentence quality approved by hu-
man assessors. Therefore, BLEU is likely to have
a low correlation with human assessment at sen-
tence level for most candidate translations. Second,
it is conceivable that human beings are more reli-
able ranking the quality of multiple candidate trans-
lations than assigning a numeric value to index the
quality of the candidate translation even with signif-
icant deliberation. Consequently, a more intuitive
approach for automatic MT evaluation is to repli-
cate the quality ranking ability of human assessors.
Thirdly, the BLEU score is elusive and hard to in-
terpret; for example, what can be concluded for a
240
candidate translation?s quality if the BLEU score is
0.0168, particularly when we are aware that even
a human translation can receive an embarrassingly
low BLEU score? In light of the discussion above,
we propose an alternative scenario for MT evalua-
tion, where, instead of assigning a numeric score to
a candidate translation under evaluation, we predict
its rank with regard to its peer candidate translations.
This formulation of the MT evaluation task fills the
gap between an automatic scoring function and hu-
man MT evaluation practice. The results from the
current study will not only interest MT system eval-
uation moderators but will also inform the research
community about which features are useful in im-
proving the correlation between human rankings and
automatic rankings.
2 Problem Formulation
2.1 Data and Human Annotation Reliability
We use two data sets for the experiments:
the test data set from the LDC MTC corpus
(LDC2003T171) and the data set from the MT eval-
uation workshop at ACL052. Both data sets are for
Chinese-English language pairs and each has four
reference translations and seven MT system transla-
tions as well as human assessments for fluency and
adequacy on a scale of 1 to 5, with 5 indicating the
best quality. For the LDC2003T17 data, human as-
sessments exist for only three MT systems; for the
ACL05 workshop data, there are human assessments
for all seven MT systems. Table 1 summarizes the
information from these two data sets.
The Kappa scores (Cohen, 1960) for the human
assessment scores are negative, both for fluency and
adequacy, indicating that human beings are not con-
sistent when assigning quality scores to the candi-
date translations. We have much sympathy with a
concern expressed in (Turian, 2003) that ?Automatic
MT evaluation cannot be faulted for poor correlation
with the human judges, when the judges do not cor-
relate well each other.?To determine whether human
assessor might be more consistent when ranking
pairs of sentences, we examined the ?ranking con-
sistency score?of the human assessment data for the
LDC2003T17 data. For this consistency score, we
1http://www.ldc.upenn.edu/Catalog/
2http://www.isi.edu/? cyl/MTSE2005/
are only concerned with whether multiple judges are
consistent in terms of which sentence of the two sen-
tences is better: we are not concerned with the quan-
titative difference between judges. Since some sen-
tences are judged by three judges while others are
judged by only two judges, we calculated the consis-
tency scores under both circumstances, referred to as
?Consistent 2?and ?Consistent 3?in the following ta-
ble. For ?Consistent 2?, for every pair of sentences,
where sentence 1 is scored higher (or lower or equal)
than sentence 2 by both judges, then the two judges
are deemed consistent. For ?Consistent 3?, the pro-
portion of sentences that achieved the above consis-
tency from triple judges is reported. Additionally,
we also considered a consistency rate that excludes
pairs for which only one judge says sentence 1 is bet-
ter and the other judge(s) say(s) sentence 2 is better.
We call these ?Consistent 2 with tie?and ?Consistent
3 with tie?. From the rank consistency scores in Ta-
ble 2, we observe that two annotators are more con-
sistent with the relative rankings for sentence pairs
than with the absolute quality scores. This finding
further supports the task of ranking MT candidate
sentences as more reliable than the one of classify-
ing the quality labels.
2.2 Ranking Over Classification and
Regression
As discussed in the previous section, it is difficult for
human assessors to perform MT candidate transla-
tion evaluation with fine granularity (e.g., using real-
valued numeric score). But humans? assessments
are relatively reliable for judgments of quality rank-
ing using a coarser ordinal scale, as we have seen
above. Several approaches for automatically assign-
ing quality scores to candidate sentences are avail-
able, including classification, regression or ranking,
of which ranking is deemed to be a more appropri-
ate approach. Nominalize the quality scores and for-
mulating the task as a classification problem would
result in a loss of the ordinal information encoded
in the different scores. Additionally, the low Kappa
scores in the human annotation reliability analysis
reported above also confirms our previous specula-
tion that a classification approach is less appropriate.
Regression would be more reasonable than classifi-
cation because it preserves the ordinal information
in the quality labels, but it also inappropriately im-
241
Data Index MT Systems References Documents Sentences
LDC2003T17 7 4 100 878
ACL05 Workshop 7 4 100 919
Table 1: Data Sets Information
Inter-Judge Score Consistent
2
Consistent
3
Consistent
2 with Tie
Consistent
3 with Tie
Ranking Consistency Score 45.3% 23.4% 92.6% 87.0%
Table 2: Ranking Consisteny Scores for LDC2003T17 Data
poses interval scaling onto the quality labels. In
contrast, ranking considers only the relative rank-
ing information from human labels and does not im-
pose any extra information onto the quality labels
assigned by human beings.
The specific research question addressed in this
paper is three-fold: First, in addition to investigating
the correlation between automatic numeric scoring
and human assessments, is ranking of peer candidate
translations an alternative way of examining correla-
tion that better suits the state of affairs of human an-
notation? Second, if the answer to the above ques-
tion is yes, can better correlation be achieved with
human assessment under the new task scenario? Fi-
nally, in addition to n-gram matching, which other
knowledge sources can combat and even improve
the rank order correlation? The process of rank-
ing is a crucial technique in Information Retrieval
(IR) where search engines rank web pages depend-
ing on their relevance to a query. In this work, sen-
tence level MT evaluation is considered as a ranking
problem. For all candidate translations of the same
source Chinese sentence, we predict their transla-
tion quality ranks. We evaluate the ranker by Spear-
man?s rank order correlation coefficient between hu-
man ranks and predicted ranks described by the fol-
lowing formula (Siegel,1956):
r = 1? ( 6
?
D2
N(N2 ? 1)
) (1)
where D is the difference between each pair of ranks
and N is the number of candidates for ranking.
3 Related Works
Papineni et al(2001) pioneered the automatic MT
evaluation study, which scores translation quality via
n-gram matching between the candidate and refer-
ence translations. Following the growing awareness
of the deficiency of n-gram based automatic MT
evaluation, many studies attempted to improve upon
n-gram based metrics (Zhou et al, 2006; Liu, et
al., 2005,2006) as well as propose ways to evaluate
MT evaluation metrics (Lin, et al 2004). Previous
studies, however, have focused on MT evaluation at
the document level in order to fight n-gram sparse-
ness problem. While document level correlation
provides us with a general impression of the qual-
ity of an MT system, researchers desire to get more
informative diagnostic evaluation at sentence level
to improve the MT system instead of just an over-
all score that does not provide details. Recent years
have seen several studies investigating MT evalu-
ation at the sentence level (Liu et al, 2005,2006;
Quirk, 2004). The state-of-the-art sentence level
correlations reported in previous work between hu-
man assessments and automatic scoring are around
0.20. Kulesza et al(2004) applied Support Vec-
tor Machine classification learning to sentence level
MT evaluation and reported improved correlation
with human judgment over BLEU. However, the
classification taxonomy in their work is binary, be-
ing either machine translation or human translation.
Additionally, as discussed above, the inconsistency
from the human annotators weakens the legitimacy
of the classification approach. Gamon et al(2005)
reported a study of English to French sentence-level
MT evaluation without reference translations. In or-
der to improve on the correlation between human as-
sessments and the perplexity score alone, they com-
bined a perplexity score with a classification score
obtained from an SVM binary classifier distinguish-
ing machine-translated sentences from human trans-
242
lations. The results showed that even the combi-
nation of the above two scores cannot outperform
BLEU.
To sum up, very little consideration has been
taken in previous research as to which learning ap-
proach is better motivated and justified by the state
of affairs of human annotation reliability. Presum-
ably, research that endeavors to emulate human per-
formance on tasks that demontrate good inter-judge
reliability is most useful.
a learning approach that is better supported by
human annotation reliability can alleviate the noise
from human assessments and therefore achieve more
reliable correlations.
4 Experiments and Evaluation
4.1 Ranking SVM Learning Algorithm
Ranking peer candidate sentence translations is a
task in which the translation instances are classi-
fied into a number of ranks. This is a canonical or-
dinal regression scenario, which differs from stan-
dard classification and metric regression. For imple-
mentation, we use the Ranking SVM of SVMlight
(Joachims, 2004), which was originally developed
to rank the web pages returned upon a certain query
in search engines. Given an instance of a candidate
translation, Ranking SVM assigns it a score based
on:
U(x) = W Tx (2)
where W represents a vector of weights (Xu et al,
2005). The higher the value of U(x), the better x is as
a candidate translation. In an ordinal regression, the
values of U(x) are mapped into intervals correspond-
ing to the ordinal categories. An instance falling
into one interval is classified into the corresponding
translation quality. In ranking experiments, we use
the Ranking SVM scores to rank the candidate sen-
tences under evaluation.
4.2 Features
We experiment with three different knowledge
sources in our ranking experiments:
1. N-gram matching between the candidate trans-
lation and the reference translation, for which
we use BLEU scores calculated by the NIST
script with smoothing3 to avoid undefined log
probabilities for zero n-gram probabilities.
2. Dependency relation matching between the
candidate translation and the reference transla-
tion.
3. The log of the perplexity score of the candidate
translation, where the perplexity score is ob-
tained from a local language model trained on
all sentences in the four reference translations
using CMU SLM toolkit. The n-gram order is
the default trigram.
4.2.1 N-gram matching feature
N-gram matching is certainly an important cri-
terion in some cases for evaluating the translation
quality of a candidate translation. We use the BLEU
score calculated by the BLEU score script from
NIST for this feature.
As has been observed by many researchers,
BLEU fails to capture any non n-gram based match-
ing between the reference and candidate transla-
tions. We carried out a pair-wise experiment on
four reference translations from the LDC2003T17
test data, where we took one reference sentence as
the reference and the other three references as can-
didate translations. Presumably, since the candidate
sentences are near-optimal translations, the BLEU
scores obtained in such a way should be close to
1. But our analysis shows a mean BLEU of only
0.1456398, with a standard deviation of 0.1522381,
which means that BLEU is not very predictive of
sentence level evaluation. The BLEU score is, how-
ever, still informative in judging the average MT
system?s translation.
4.2.2 Dependency Structure Matching
Dependency relation information has been widely
used in Machine Translation in recent years. Fox
(2002) reported that dependency trees correspond
better across translation pairs than constituent trees.
The information summarization community has also
seen successful implementation of ideas similar to
the depedency structure. Zhou et al(2005) and Hovy
et al(2005) reported using Basic Elements (BE) in
text summarization and its evaluation. In the current
3We added an extremely small number to both matched n-
grams and total number of n-grams.
243
paper, we match a candidate translation with a ref-
erence translation on the following five dependency
structure (DS) types:
? Agent - Verb
? Verb - Patient
? Modified Noun - Modifier
? Modified Verb - Modifier
? Preposition - Object
Besides the consideration of the presence of cer-
tain lexical items, DS captures information as to
how the lexical items are assembled into a good sen-
tence. By using their dependency relation match for
ranking the quality of peer translations, we assume
that the dependency structure in the source language
should be well preserved in the target language and
that multiple translations of the same source sen-
tence should significantly share dependency struc-
tures. Liu et al(2005) make use of dependency
structure in sentence level machine translation eval-
uation in the form of headword chains, which are
lexicalized dependency relations. We propose that
unlexicalized dependency relations can also be in-
formative. Previous research has shown that key de-
pendency relations tend to have a strong correspon-
dence between Chinese and English (Zhou et al,
2001). More than 80 % of subject-verb, adjective-
noun and adverb-verb dependency relations were
able to be mapped, although verb-object DS map-
ping is weaker at a rate of 64.8%. In our paper, we
considered three levels of matching for dependency
relation triplets, where a triplet consists of the DS
type and the two lexical items as the arguments.
We used an in-house dependency parser to extract
the dependency relations from the sentences. Figure
1 illustrates how dependency relation matching can
go beyond n-gram matching. We calculated 15 DS
scores for each sentence correponding to the counts
of match for the 5 DS types at the 3 different levels.
4.2.3 Reference language model (RLM) feature
Statistical Language Modeling (SLM) is a key
component in Statistical Machine Translation. The
most dominant technology in SLM is n-gram mod-
els, which are typically trained on a large corpus
for applications such as SMT and speech recogni-
tion. Depending on the size of the corpora used
to train the language model, a language model can
Figure 1: Dependency Relation Matching Scheme
Figure 2: An Example - A Sentence Gets Credits for
Dependency Relation Matching
244
be tuned to reflect n-gram probabilities for both a
narrowed scope as well as a general scope covering
the distribution of n-gram probabilities of the whole
language. In the BLEU calculation, the candidate
sentence is evaluated against an extremely local lan-
guage model of merely the reference sentence. We
speculate that a language model that stands in be-
tween such an immediate local language model and
the large general English language model could help
capture the variation of lexical and even structural
selections in the translations by using information
beyond the scope of the local sentence. Addition-
ally, this language model could represent the style
of a certain group of translators in a certain domain
on the genre of news articles. To pursue such a lan-
guage model, we explore a language model that is
trained on all sentences in the four references. We
obtain the perplexity score of each candidate sen-
tence based on the reference language model. The
perplexity score obtained this way reflects the de-
gree to which a candidate translation can be gen-
erated from the n-gram probability distribution of
the whole collection of sentences in the four refer-
ences. It adds new information to BLEU because it
not only compares the candidate sentence to its cor-
responding reference sentence but also reaches out
to other sentences in the current document and other
documents on the same topics. We choose perplex-
ity over the language model score because the per-
plexity score is normalized with regard to the length
of the sentence; that is, it does not favor sentences of
relatively shorter length.
In our ranking experiments, for training, both the
seven MT translations and the four reference trans-
lations of the same source sentence are evaluated
as ?candidate? translations, and then each of these
eleven sentences is evaluated against the four ref-
erence sentences in turn. The BLEU score of each
of these sentences is calculated with multiple refer-
ences. Each dependency score is the average score
of the four references. For the reference language
model feature, the perplexity score is used for each
sentence.
Conceptually, the reference language model and
dependency structure features are more relevant to
the fluency of the sentence than to the adequacy.
Because the candidate sentences? adequacy scores
are based on arbitrary reference sentences out of the
Feature Set Mean Corr Corr Var
BLEU 0.3590644 0.0076498
DS 0.4002753 0.0061299
PERP 0.4273000 0.0014043
BLEU+DS 0.4128991 0.0027576
BLEU+PERP 0.4288112 0.0013783
PERP+DS 0.4313611 0.0014594
All 0.4310457 0.0014494
Table 3: Training and Testing on Within-year Data
(Test on 7 MT and 4 Human)
four references in the human assessment data, we
decided to focus on fluency ranking for this paper.
The ranking scenario and features can easily be gen-
eralized to adequacy evaluation: the full and partial
match dependency structure features are relevant to
adeqaucy too. The high correlation between ade-
quacy and fluency scores from human assessments
(both pearson and spearman correlations are 0.67)
also indicates that the same features will achieve im-
provements for adequacy evaluation.
4.3 Sentence Ranking on Within-year Data
In the first experiment, we performed the ranking
experiment on the ACL05 workshop data and test on
the same data set. We did three-fold cross-validation
on two different test scenarios. On the first sce-
nario, we tested the ranking models on the seven MT
system output sentences and the four human refer-
ence sentences. It is widely agreed upon among re-
searchers that a good evalutation metric should rank
reference translation as higher than machine trans-
lation (Lin et al, 2004). We include the four hu-
man reference sentences into the ranking to test the
ranker?s ability to discriminate optimal translations
from poor ones. For the second scenario, we test
the ranking models on only the seven MT system
output sentences. Because the quality differences
across the seven system translations are more subtle,
we are particularly interested in the ranking quality
on those sentences. Tables 3 and 4 summarize the
results from both scenarios.
The experimental results in the above tables con-
veyed several important messages: in the ranking
setup, for both the MT and human mixed output and
MT only output scenarios, we have a significantly
245
Feature Set Mean Corr Corr Var
BLEU 0.2913541 0.0324386
DS 0.3058766 0.0226442
PERP 0.2921684 0.0210605
BLEU+DS 0.315106 0.0206144
BLEU+PERP 0.2954833 0.0211094
PERP+DS 0.3067157 0.0217037
All 0.305248 0.0218777
Table 4: Training and Testing on Within-year Data
(Test on MT only)
improved correlation between human scoring and
automatic ranking at sentence level compared to the
state-of-the-art sentence level correlation for fluency
score of approximately 0.202 found previously (Liu
et al, 2006). When the ranking task is performed on
a mixture of MT sentences and human translations,
dependency structure and reference language model
perplexity scores sequentially improve on BLEU in
increasing the correlation. When the ranking task
is performed only on MT system output sentences,
dependency structure still significantly outperforms
BLEU in increasing the correlation, and the refer-
ence language model, even trained on a small num-
ber of sentences, demonstrates utility equal to that
of BLEU. The dependency structure feature proves
to have robust utility in informing fluency quality
in both scenarios, even with noise from the depen-
dency parser, likely because a dependency triplet
with inaccurate arguments is still rewarded as a type
match or partial match. Additionally, the feature is
reward-based and not penalty-based. We only re-
ward matches and do not penalize mismatches, such
that the impact of the noise from the MT system and
the dependency parser is weakened.
4.4 Sentence Ranking on Across-year Data
It is trivial to retrain the ranking model and test on
a new year?s data. But we speculate that a model
trained from a different data set can have almost the
same ranking power as a model trained on the same
data set. Therefore, we conducted an experiment
where we trained the ranking model on the ACL
2005 workshop data and test on the LDC2003T17
data. We do not need to retrain the ranking SVM
model; we only need to retrain the reference lan-
Feature Set Mean Corr Corr Var
BLEU 0.3133257 0.1957059
DS 0.4896355 0.0727430
PERP 0.4582005 0.0542485
BLEU+DS 0.4907745 0.0678395
BLEU+PERP 0.4577449 0.0563994
PERP+DS 0.4709567 0.0549708
All 0.4707289 0.0565538
Table 5: Training and Testing on Across-year Data
(test on 3 MT plus 1 human)
guage model on the multiple references from the
new year?s data to obtain the perplexity scores.
Because LDC2003T17 has human assessments for
only three MT systems, we test on the three system
outputs plus a human translation chosen randomly
from the four reference translations. The results in
Table 5 show an encouraging rank order correlation
with human assessments. Similar to training and
testing on within-year data, both dependency struc-
ture and perplexity scores achieve higher correlation
than the BLEU score. Combining BLEU and depen-
dency structure achieves the best correlation.
4.5 Document Level Ranking Testing
Previously, most researchers working on MT evalu-
ation studied the correlation between automatic met-
ric and human assessment on the granularity of the
document to mitigate n-gram sparseness. Presum-
ably, good correlation at sentence level should lead
to good correlation at document level but not vice
versa. Table 6 reports the correlations using the
model trained on the 2005 workshop data and tested
on the 100 documents of the LDC 2003 data. Com-
paring these correlations with the correlations re-
ported in the previous section, we see that using the
same model, the document level rank order corre-
lation is substantially higher than the sentence level
correlation, with the dependency structure showing
the highest utility.
5 Conclusion and Future Work
The current study proposes to formulate MT evalu-
ation as a ranking problem. We believe that a reli-
able ranker can inform the improvement of BLEU
for a better automatic scoring function. Ranking in-
246
Feature Set Mean Corr Corr Var
BLEU 0.543 0.0853
DS 0.685 0.0723
PERP 0.575 0.0778
BLEU+DS 0.639 0.0773
BLEU+PERP 0.567 0.0785
PERP+DS 0.597 0.0861
All 0.599 0.0849
Table 6: Document Level Ranking Testing Results
formation could also be integrated into tuning pro-
cess to better inform the optimization of weights of
the different factors for SMT models. Our ranking
experiments show a better correlation with human
assessments at sentence level for fluency score com-
pared to the previous non-ranking scenario, even
with BLEU as the baseline feature. On top of BLEU,
both the dependency structure and reference lan-
guage model have shown encouraging utility for dif-
ferent testing scenarios. Looking toward the fu-
ture work, more features could be explored, e.g., a
parsing-based score of each candidate sentence and
better engineering for dependency triplet extraction.
Additionally, the entire research community on MT
evaluation would benefit from a systematic and de-
tailed analysis of real data that can provide a quanti-
tative breakdown of the proportions of different ?op-
erations? needed to rewrite one sentence to another.
Such an effort will guide MT evaluation researchers
to decide which features to focus on.
References
J. Cohen, A Coefficient of Agreement for Nominal
Scales, Educational and Psychological Measurement,
20, 37-46, 1960.
G. Doddington. Automatic Evaluation of Machine Trans-
lation Quality Using N-gram Co-occurrence Statistics.
HLT, pages 128?132, 2002.
H. J. Fox, Phrasal Cohesion and Statistical Machine
Translation. EMNLP, 2002.
M. Gamon, et al, Sentence-level MT Evaluation without
Reference Translations: Beyond Language Modeling,
Proceedings of EAMT, 2005.
T. Joachims, Making Large-scale Support Vector Ma-
chine Learning Practical, in B. Scholkopf, C. Burges,
A. Smola. Advances in Kernel Methods: Support Vec-
tor Machines, MIT Press, Cambridge, MA, December,
1998.
A. Kulesza and S. M. Shieber, A Learning Approach to
Improving Sentence-Level MT Evaluation, 10th Inter-
national Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, 2004.
C. Lin, et al, ORANGE: a Method for Evaluating Au-
tomatic Evaluation Metrics for Machine Translation.
COLING, 2004.
C. Lin, et al, Automatic Evaluation of Machine Trans-
lation Quality Using Longest Common Subsequence
and Skip-Bigram Statistics, ACL, 2004.
D. Liu, et al, Syntactic Features for Evaluation of Ma-
chine Translation, ACLWorkshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, 2005.
D. Liu, et al, Stochastic Iterative Alignment for Ma-
chine Translation Evaluation, COLING/ACL Poster
Session, Sydney, 2006.
C. B. Quirk, Training a Sentence-Level Machine Trans-
lation Confidence Measure, In Proceedings of LREC,
2004.
E. Hovy, et al, Evaluating DUC 2005 using Basic El-
ements. Document Understanding Conference (DUC-
2005), 2005.
K. Papineni, et al, BLEU: a Method for Automatic Eval-
uation of Machine Translation, IBM research division
technical report, RC22176 (W0109-022), 2001.
S. Siegel and N.J. Catellan, Non-parametric Statistics for
the Behavioral Sciences, McGraw-Hill, 2nd edition,
1988.
M. Snover, et al, A Study of Translation Error Rate with
Targeted Human Annotation, LAMP-TR-126, CS-TR-
4755, UMIACS-TR-2005-58, University of Maryland,
2005.
J. Turian, et al, Evaluation of Machine Translation and
its Evaluation, MT Summit IX, 2003.
J. Xu, et al, Ranking Definitions with Supervised Learn-
ing Method, WWW?05 industry track, 811-819, 2005.
L. Zhou, et al, A BE-based Multi-document Summarizer
with Query Interpretation. Document Understanding
Conference (DUC-2005), 2005.
L. Zhou, C. Lin, E-evaluating Machine Translation Re-
sults with Paraphrase Support, EMNLP, 2006.
M. Zhou, C. Huang, Approach to the Chinese depen-
dency formalism for the tagging of corpus. Journal of
Chinese Information Processing, 8(3): 35-52, 1994.
247
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1027?1037, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Ensemble Semantics for Large-scale Unsupervised Relation Extraction 
 
 
Bonan Min1* Shuming Shi2 Ralph Grishman1 Chin-Yew Lin2 
  
1New York University 2Microsoft Research Asia 
New York, NY, USA Beijing, China 
{min,grishman}@cs.nyu.edu {shumings,cyl}@microsoft.com 
  
Abstract 
Discovering significant types of relations 
from the web is challenging because of its 
open nature. Unsupervised algorithms are 
developed to extract relations from a cor-
pus without knowing the relations in ad-
vance, but most of them rely on tagging 
arguments of predefined types. Recently, 
a new algorithm was proposed to jointly 
extract relations and their argument se-
mantic classes, taking a set of relation in-
stances extracted by an open IE algorithm 
as input. However, it cannot handle poly-
semy of relation phrases and fails to 
group many similar (?synonymous?) rela-
tion instances because of the sparseness of 
features. In this paper, we present a novel 
unsupervised algorithm that provides a 
more general treatment of the polysemy 
and synonymy problems. The algorithm 
incorporates various knowledge sources 
which we will show to be very effective 
for unsupervised extraction. Moreover, it 
explicitly disambiguates polysemous rela-
tion phrases and groups synonymous 
ones. While maintaining approximately 
the same precision, the algorithm achieves 
significant improvement on recall com-
pared to the previous method. It is also 
very efficient. Experiments on a real-
world dataset show that it can handle 14.7 
million relation instances and extract a 
very large set of relations from the web.  
1 Introduction 
Relation extraction aims at discovering semantic 
relations between entities. It is an important task 
that has many applications in answering factoid 
questions, building knowledge bases and improv-
ing search engine relevance. The web has become 
a massive potential source of such relations. How-
ever, its open nature brings an open-ended set of 
relation types. To extract these relations, a system 
should not assume a fixed set of relation types, nor 
rely on a fixed set of relation argument types.  
The past decade has seen some promising solu-
tions, unsupervised relation extraction (URE) algo-
rithms that extract relations from a corpus without 
knowing the relations in advance. However, most 
algorithms (Hasegawa et al 2004, Shinyama and 
Sekine, 2006, Chen et. al, 2005) rely on tagging 
predefined types of entities as relation arguments, 
and thus are not well-suited for the open domain.  
Recently, Kok and Domingos (2008) proposed 
Semantic Network Extractor (SNE), which gener-
ates argument semantic classes and sets of synon-
ymous relation phrases at the same time, thus 
avoiding the requirement of tagging relation argu-
ments of predefined types. However, SNE has 2 
limitations: 1) Following previous URE algo-
rithms, it only uses features from the set of input 
relation instances for clustering.  Empirically we 
found that it fails to group many relevant relation 
instances. These features, such as the surface forms 
of arguments and lexical sequences in between, are 
very sparse in practice. In contrast, there exist sev-
eral well-known corpus-level semantic resources 
that can be automatically derived from a source 
corpus and are shown to be useful for generating 
the key elements of a relation: its 2 argument se-
mantic classes and a set of synonymous phrases. 
For example, semantic classes can be derived from 
a source corpus with contextual distributional simi-
larity and web table co-occurrences. The ?synony-
my? 1  problem for clustering relation instances 
                                                          
* Work done during an internship at Microsoft Research Asia 
1027
could potentially be better solved by adding these 
resources. 2) SNE assumes that each entity or rela-
tion phrase belongs to exactly one cluster, thus is 
not able to effectively handle polysemy of relation 
phrases2. An example of a polysemous phrase is be 
the currency of  as in 2 triples <Euro, be the cur-
rency of, Germany> and <authorship, be the cur-
rency of, science>. As the target corpus expands 
from mostly news to the open web, polysemy be-
comes more important as input covers a wider 
range of domains. In practice, around 22% (section 
3) of relation phrases are polysemous. Failure to 
handle these cases significantly limits its effective-
ness. 
    To move towards a more general treatment of 
the polysemy and synonymy problems, we present a 
novel algorithm WEBRE for open-domain large-
scale unsupervised relation extraction without pre-
defined relation or argument types. The contribu-
tions are: 
? WEBRE incorporates a wide range of cor-
pus-level semantic resources for improving rela-
tion extraction. The effectiveness of each 
knowledge source and their combination are stud-
ied and compared. To the best of our knowledge, it 
is the first to combine and compare them for unsu-
pervised relation extraction. 
? WEBRE explicitly disambiguates polyse-
mous relation phrases and groups synonymous 
phrases, thus fundamentally it avoids the limitation 
of previous methods. 
? Experiments on the Clueweb09 dataset 
(lemurproject.org/clueweb09.php) show that 
WEBRE is effective and efficient. We present a 
large-scale evaluation and show that WEBRE can 
extract a very large set of high-quality relations. 
Compared to the closest prior work, WEBRE sig-
nificantly improves recall while maintaining the 
same level of precision. WEBRE is efficient. To 
the best of our knowledge, it handles the largest 
triple set to date (7-fold larger than largest previous 
effort). Taking 14.7 million triples as input, a com-
plete run with one CPU core takes about a day.  
 
 
 
                                                                                           
1 We use the term synonymy broadly as defined in Section 3. 
2 A cluster of relation phrases can, however, act as a whole as 
the phrase cluster for 2 different relations in SNE. However, 
this only accounts for 4.8% of the polysemous cases. 
2 Related Work 
Unsupervised relation extraction (URE) algorithms 
(Hasegawa et al 2004; Chen et al 2005; Shinya-
ma and Sekine, 2006) collect pairs of co-occurring 
entities as relation instances, extract features for 
instances and then apply unsupervised clustering 
techniques to find the major relations of a corpus. 
These UREs rely on tagging a predefined set of 
argument types, such as Person, Organization, and 
Location, in advance. Yao et al2011 learns fine-
grained argument classes with generative models, 
but they share the similar requirement of tagging 
coarse-grained argument types. Most UREs use a 
quadratic clustering algorithm such as Hierarchical 
Agglomerate Clustering (Hasegawa et al 2004, 
Shinyama and Sekine, 2006), K-Means (Chen et 
al., 2005), or both (Rosenfeld and Feldman, 2007); 
thus they are not scalable to very large corpora.  
As the target domain shifts to the web, new 
methods are proposed without requiring predefined 
entity types. Resolver (Yates and Etzioni, 2007) 
resolves objects and relation synonyms. Kok and 
Domingos (2008) proposed Semantic Network Ex-
tractor (SNE) to extract concepts and relations. 
Based on second-order Markov logic, SNE used a 
bottom-up agglomerative clustering algorithm to 
jointly cluster relation phrases and argument enti-
ties. However, both Resolver and SNE require 
each entity and relation phrase to belong to exactly 
one cluster. This limits their ability to handle poly-
semous relation phrases. Moreover, SNE only uses 
features in the input set of relation instances for 
clustering, thus it fails to group many relevant in-
stances. Resolver has the same sparseness problem 
but it is not affected as much as SNE because of its 
different goal (synonym resolution).  
As the preprocessing instance-detection step for 
the problem studied in this paper, open IE extracts 
relation instances (in the form of triples) from the 
open domain (Etzioni et al 2004; Banko et al 
2007; Fader et al 2011; Wang et al2011). For 
efficiency, they only use shallow features. Reverb 
(Fader et al 2011) is a state-of-the-art open do-
main extractor that targets verb-centric relations, 
which have been shown in Banko and Etzioni 
(2008) to cover over 70% of open domain rela-
tions. Taking their output as input, algorithms have 
been proposed to resolve objects and relation syn-
onyms (Resolver),  extract semantic networks 
1028
(SNE), and map extracted relations into an existing 
ontology (Soderland and Mandhani, 2007).  
Recent work shows that it is possible to con-
struct semantic classes and sets of similar phrases 
automatically with data-driven approaches. For 
generating semantic classes, previous work applies 
distributional similarity (Pasca, 2007; Pantel et al 
2009), uses a few linguistic patterns (Pasca 2004; 
Sarmento et al 2007), makes use of structure in 
webpages (Wang and Cohen 2007, 2009), or com-
bines all of them (Shi et al 2010). Pennacchiotti 
and Pantel (2009) combines several sources and 
features. To find similar phrases, there are 2 close-
ly related tasks: paraphrase discovery and recog-
nizing textual entailment. Data-driven paraphrase 
discovery methods (Lin and Pantel, 2001; Pasca 
and Dienes, 2005; Wu and Zhou, 2003; Sekine, 
2005) extends the idea of distributional similarity 
to phrases. The Recognizing Textual Entailment 
algorithms (Berant et al2011) can also be used to 
find related phrases since they find pairs of phrases 
in which one entails the other.  
To efficiently cluster high-dimensional datasets, 
canopy clustering (McCallum et al 2000) uses a 
cheap, approximate distance measure to divide da-
ta into smaller subsets, and then cluster each subset 
using an exact distance measure. It has been ap-
plied to reference matching. The second phase of 
WEBRE applies the similar high-level idea of par-
tition-then-cluster for speeding up relation cluster-
ing. We design a graph-based partitioning 
subroutine that uses various types of evidence, 
such as shared hypernyms.  
3 Problem Analysis 
The basic input is a collection of relation instances 
(triples) of the form <ent1, ctx, ent2>. For each tri-
ple, ctx is a relational phrase expressing the rela-
tion between the first argument ent1 and the second 
argument ent2. An example triple is <Obama, win 
in, NY>. The triples can be generated by an open 
IE extractor such as TextRunner or Reverb. Our 
goal is to automatically build a list of relations 
? = {< ent1, ???, ent2 >} ? 3 < ?1,?,?2 >  where P 
is the set of relation phrases, and ?1  and  ?2  are 
two argument classes. Examples of triples and rela-
tions R (as Type B) are shown in Figure 1. 
                                                          
3 This approximately equal sign connects 2 possible represen-
tations of a relation: as a set of triple instances or a triple with 
2 entity classes and a relation phrase class. 
The first problem is the polysemy of relation 
phrases, which means that a relation phrase ctx can 
express different relations in different triples. For 
example, the meaning of be the currency of in the 
following two triples is quite different: <Euro, be 
the currency of, Germany> and <authorship, be 
the currency of, science>. It is more appropriate to 
assign these 2 triples to 2 relations ?a currency is 
the currency of a country? and ?a factor is im-
portant in an area? than to merge them into one. 
Formally, a relation phrase ctx is polysemous if 
there exist 2 different relations < ?1,?,?2 >  and 
< ?1
?
,??,?2
?
> where ??? ? ? ? ??. In the previ-
ous example, be the currency of  is polysemous 
because it appears in 2 different relations.  
Polysemy of relation phrases is not uncommon. 
We generate clusters from a large sample of triples 
with the assistance of a soft clustering algorithm, 
and found that around 22% of relation phrases can 
be put into at least 2 disjoint clusters that represent 
different relations. More importantly, manual in-
spection reveals that some common phrases are 
polysemous. For example, be part of can be put 
into a relation ?a city is located in a country? when 
connecting Cities to Countries, and another rela-
tion ?a company is a subsidiary of a parent com-
pany? when connecting Companies to Companies. 
Failure to handle polysemous relation phrases fun-
damentally limits the effectiveness of an algorithm. 
The WEBRE algorithm described later explicitly 
handles polysemy and synonymy of relation 
phrases in its first and second phase respectively. 
The second problem is the ?synonymy? of rela-
tion instances. We use the term synonymy broadly 
and we say 2 relation instances are synonymous if 
they express the same semantic relation between 
the same pair of semantic classes. For example, 
both <Euro, be the currency used in, Germany> 
and <Dinar, be legal tender in, Iraq> express the 
relation <Currencies, be currency of, Countries>. 
Solving this problem requires grouping synony-
mous relation phrases and identifying argument 
semantic classes for the relation.  
Various knowledge sources can be derived from 
the source corpus for this purpose. In this paper we 
pay special attention to incorporating various se-
mantic resources for relation extraction. We will 
show that these semantic sources can significantly 
improve the coverage of extracted relations and the  
 
1029
Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec-
tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys-
tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the 
green arrows show the resources used in phase 2. 
 
best performance is achieved when various re-
sources are combined together.  
4 Mining Relations from the Web 
We first describe relevant knowledge sources, and 
then introduce the WEBRE algorithm, followed by 
a briefly analysis on its computational complexity.  
4.1 Knowledge Sources 
Entity similarity graph We build two similarity 
graphs for entities: a distributional similarity (DS) 
graph and a pattern-similarity (PS) graph. The DS 
graph is based on the distributional hypothesis 
(Harris, 1985), saying that terms sharing similar 
contexts tend to be similar. We use a text window 
of size 4 as the context of a term, use Pointwise 
Mutual Information (PMI) to weight context fea-
tures, and use Jaccard similarity to measure the 
similarity of term vectors. The PS graph is gener-
ated by adopting both sentence lexical patterns and 
HTML tag patterns (Hearst, 1992; Kozareva et al 
2008; Zhang et al 2009; Shi et al 2010). Two 
terms (T) tend to be semantically similar if they co-
occur in multiple patterns. One example of sen-
tence lexical patterns is (such as | including) 
T{,T}* (and|,|.). HTML tag patterns include tables, 
dropdown boxes, etc. In these two graphs, nodes 
are entities and the edge weights indicate entity 
similarity. In all there are about 29.6 million nodes 
and 1.16 billion edges. 
Hypernymy graph Hypernymy relations are 
very useful for finding semantically similar term 
pairs. For example, we observed that a small city 
in UK and another small city in Germany share 
common hypernyms such as city, location, and 
place. Therefore the similarity between the two 
cities is large according to the hypernymy graph, 
while their similarity in the DS graph and the PS 
graph may be very small. Following existing work 
(Hearst, 1992, Pantel & Ravichandran 2004; Snow 
et al 2005; Talukdar et al 2008; Zhang et al 
2011), we adopt a list of lexical patterns to extract 
hypernyms. The patterns include NP {,} (such as) 
{NP,}* {and|or} NP, NP (is|are|was|were|being) 
(a|an|the) NP, etc. The hypernymy graph is a bi-
partite graph with two types of nodes: entity nodes 
and label (hypernym) nodes. There is an edge (T, 
L) with weight w if L is a hypernym of entity T 
with probability w. There are about 8.2 million 
nodes and 42.4 million edges in the hypernymy 
graph. In this paper, we use the terms hypernym 
and label interchangeably. 
Relation phrase similarity: To generate the pair-
wise similarity graph for relation phrases with re-
gard to the probability of expressing the same 
relation, we apply a variant of the DIRT algorithm 
(Lin and Pantel, 2001). Like DIRT, the paraphrase 
discovery relies on the distributional hypothesis, 
but there are a few differences: 1) we use stemmed 
lexical sequences (relation phrases) instead of de-
pendency paths as phrase candidates because of the 
very large scale of the corpus. 2) We used ordered 
1030
pairs of arguments as features of phrases while 
DIRT uses them as independent features. We em-
pirically tested both feature schemes and found 
that using ordered pairs results in likely para-
phrases but using independent features the result 
contains general inference rules4. 
4.2 WEBRE for Relation Extraction 
WEBRE consists of two phases. In the first 
phase, a set of semantic classes are discovered and 
used as argument classes for each relation phrase. 
This results in a large collection of relations whose 
arguments are pairs of semantic classes and which 
have exactly one relation phrase. We call these 
relations the Type A relations. An example Type A 
relation is <{New York, London?}, be locate in, 
{USA, England, ?}>. During this phase, polyse-
mous relation phrases are disambiguated and 
placed into multiple Type A relations. The second 
phase is an efficient algorithm which groups simi-
lar Type A relations together. This step enriches 
the argument semantic classes and groups synon-
ymous relation phrases to form relations with mul-
tiple expressions, which we called Type B 
relations. Both Type A and Type B relations are 
system outputs since both are valuable resources 
for downstream applications such as QA and Web 
Search. An overview of the algorithm is shown in 
Figure 1. Here we first briefly describe a clustering 
subroutine that is used in both phases, and then 
describe the algorithm in detail. 
To handle polysemy of objects (e.g., entities or 
relations) during the clustering procedure, a key 
building block is an effective Multi-Membership 
Clustering algorithm (MMClustering). For simplic-
ity and effectiveness, we use a variant of Hierar-
chical Agglomerative Clustering (HAC), in which 
we first cluster objects with HAC, and then reas-
sign each object to additional clusters when its 
similarities with these clusters exceed a certain 
threshold5. In the remainder of this paper, we use 
{C} = MMClustering({object}, SimFunc, ?) to rep-
resent running MMClustering over a set of objects, 
                                                          
4 For example, be part of  has ordered argument pairs <A, B> 
and <C, D>, and be not part of has ordered argument pairs 
<A, D> and <B, C>. If arguments are used as independent 
features, these two phrases shared the same set of features {A, 
B, C, D}. However, they are inferential (complement relation-
ship) rather than being similar phrases. 
5 This threshold should be slightly greater than the clustering 
threshold for HAC to avoid generating duplicated clusters. 
with threshold ? to generate a list of clusters {C} of 
the objects, given the pairwise object similarity 
function SimFunc. Our implementation uses HAC 
with average linkage since empirically it performs 
well. 
Discovering Type A Relations The first phase 
of the relation extraction algorithm generates Type 
A relations, which have exactly one relation phrase 
and two argument entity semantic classes. For each 
relation phrase, we apply a clustering algorithm on 
each of its two argument sets to generate argument 
semantic classes. The Phase 1 algorithm processes 
relation phrases one by one. For each relation 
phrase ctx, step 4 clusters the set {ent1} using 
MMClustering to find left-hand-side argument se-
mantic classes {C1}. Then for each cluster C in 
{C1}, it gathers the right-hand-side arguments 
which appeared in some triple whose left hand-
side-side argument is in C, and puts them into 
{ent2?}. Following this, it clusters {ent2?} to find 
right-hand-side argument semantic classes. This 
results in pairs of semantic classes which are ar-
guments of ctx. Each relation phrase can appear in 
multiple non-overlapping Type A relations. For 
example, <Cities, be part of, Countries> and 
<Companies, be part of, Companies> are different 
Type A relations which share the same relation 
phrase be part of. In the pseudo code, SimEntFunc 
is encoded in the entity similarity graphs.  
 
Algorithm Phase 1: Discovering Type A relations 
Input:  set of triples T={<ent1, ctx, ent2>} 
 entity similarity function SimEntFunc 
 Similarity threshold ? 
Output:  list of Type A relations {<C1, ctx, C2>} 
Steps:  
01. For each relation phrase ctx 
02.     {ent1, ctx, ent2} = set of triples sharing ctx 
03.     {ent1} = set of ent1 in {ent1, ctx, ent2} 
04.     {C1} = MMClustering({ent1}, SimEntFunc, ?) 
05.     For each C in { C1} 
06.         {ent2?} = set of ???2 ?. ?.?< ???1, ???, ???2 > ?
 ? ? ???1 ? ?1 
07.         {C2} = MMClustering({ent2?}, SimEntFunc, ?) 
08.         For each C2 in {C2} 
09.             Add <C1, ctx, C2> into {<C1, ctx, C2>} 
10. Return {<C1, ctx, C2>} 
 
    Discovering Type B Relations  The goal of 
phase 2 is to merge similar Type A relations, such 
as <Cities, be locate in, Countries> and <Cities, 
be city of, Countries>, to produce Type B relations, 
which have a set of synonymous relation phrases 
and more complete argument entity classes. The 
challenge for this phase is to cluster a very large 
1031
set of Type A relations, on which it is infeasible to 
run a clustering algorithm that does pairwise all 
pair comparison. Therefore, we designed an evi-
dence-based partition-then-cluster algorithm. 
The basic idea is to heuristically partition the 
large set of Type A relations into small subsets, 
and run clustering algorithms on each subset. It is 
based on the observation that most pairs of Type A 
relations are not similar because of the sparseness 
in the entity class and the relation semantic space. 
If there is little or no evidence showing that two 
Type A relations are similar, they can be put into 
different partitions. Once partitioned, the clustering 
algorithm only has to be run on each much smaller 
subset, thus computation complexity is reduced.  
The 2 types of evidence we used are shared 
members and shared hypernyms of relation argu-
ments. For example, 2 Type A relations 
r1=<Cities, be city of, Countries> and r2=<Cities, 
be locate in, Countries> share a pair of arguments 
<Tokyo, Japan>, and a pair of hypernyms <?city?, 
?country?>. These pieces of evidence give us hints 
that they are likely to be similar. As shown in the 
pseudo code, shared arguments and hypernyms are 
used as independent evidence to reduce sparseness. 
 
Algorithm Phase 2: Discovering Type B relations 
Input:  Set of Type A relations {r}={<C1, ctx, C2>} 
 Relation similarity function SimRelationFunc 
 Map from entities to their hypernyms: Mentity2label 
 Similarity threshold ? 
Edge weight threshold ? 
Variables G(V, E) = weighted graph in which V={r} 
Output:  Set of Type B relations {<C1, P, C2>} 
Steps:  
01. {<ent, {r?}>} = build  inverted index from argument 
ent to set of Type A relations {r?} on {<C1, ctx, C2>}  
02 {<l, {r?}>} = build  inverted index from hypernym l 
of arguments to set of Type A relations {r?} on {<C1, 
ctx, C2>} with map Mentity2label  
03. For each ent in {<ent, {r?}>} 
04.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
05.        weight_edge(<r1, r2>) += weight (ent) 
06. For each l in {<l, {r?}>} 
07.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
08.        weight_edge(<r1, r2>) += weight (l) 
09. For each edge <r1, r2> in G 
10.     If weight_edge(<r1, r2>) < ? 
11.         Remove edge <r1, r2> from G 
12. {CC}= DFS(G) 
13. For each connected component CC in {CC} 
14.     {<C1, ctx, C2>} = vertices in CC 
15. {<C1?, P?, C2?>} = MMClustering({<C1, ctx, C2>},  
  SimRelationFunc, ?) 
16.     Add {<C1?, P?, C2?>} into {<C1, P, C2>} 
17. Return {<C1, P, C2>} 
 
Steps 1 and 2 build an inverted index from evi-
dence to sets of Type A relations. On the graph G 
whose vertices are Type A relations, steps 3 to 8 
set the value of edge weights based on the strength 
of evidence that shows the end-points are related. 
The weight of evidence E is calculated as follows: 
 
??????(?) =
# ?????? ?????? ?? ????? ? ??????? ?? 
max(# ??????? ? ??????? ??)
 
 
The idea behind this weighting scheme is similar 
to that of TF-IDF in that the weight of evidence is 
higher if it appears more frequently and is less am-
biguous (appeared in fewer semantic classes during 
clustering of phase 1). The weighting scheme is 
applied to both shared arguments and labels. 
After collecting evidence, we prune (steps 9 to 
11) the edges with a weight less than a threshold ? 
to remove noise. Then a Depth-First Search (DFS) 
is called on G to find all Connected Components 
CC of the graph. These CCs are the partitions of 
likely-similar Type A relations. We run MMClus-
tering on each CC in {CC} and generate Type B  
relations (step 13 to step 16).  The similarity of 2 
relations (SimRelationFunc) is defined as follows: 
???(< ?1,?,?2 >, < ?1
?,??,?2
? >) 
 
= ?
0,     ?? ???(?,??) <  ?
min????(?1,?1
?), ???(?2,?2
?)? ,   ???? 
  
4.3 Computational Complexity 
WEBRE is very efficient since both phases de-
compose the large-clustering task into much small-
er clustering tasks over partitions. Given n objects 
for clustering, a hierarchical agglomerative cluster-
ing algorithm requires ?(?2)  pairwise compari-
sons. Assuming the clustering task is split into 
subtasks of size ?1, ?2, ?, ??, thus the computa-
tional complexity is reduced to ?(? ??
2?
1 ). Ideally 
each subtask has an equal size of ?/?, so the com-
putational complexity is reduced to O(?2/?) , a 
factor of ? speed up. In practice, the sizes of parti-
tions are not equal. Taking the partition sizes ob-
served in the experiment with 0.2 million Type A 
relations as input, the phase 2 algorithm achieves 
around a 100-fold reduction in pairwise compari-
sons compared to the agglomerative clustering al-
gorithm. The combination of phase 1 and phase 2 
achieves more than a 1000-fold reduction in pair-
wise comparison, compared to running an agglom-
erative clustering algorithm directly on 14.7 
million triples. This reduction of computational 
1032
complexity makes the unsupervised extraction of 
relations on a large dataset a reality. In the experi-
ments with 14.7 million triples as input, phase 1 
finished in 22 hours, and the phase 2 algorithm 
finished in 4 hours with one CPU core. 
Furthermore, both phases can be run in parallel 
in a distributed computing environment because 
data is partitioned. Therefore it is scalable and effi-
cient for clustering a very large number of relation 
instances from a large-scale corpus like the web.  
5 Experiment 
Data preparation We tested WEBRE on re-
sources extracted from the English subset of the 
Clueweb09 Dataset, which contains 503 million 
webpages. For building knowledge resources, all 
webpages are cleaned and then POS tagged and 
chunked with in-house tools. We implemented the 
algorithms described in section 4.1 to generate the 
knowledge sources, including a hypernym graph, 
two entity similarity graphs and a relation phrase 
similarity graph. 
We used Reverb Clueweb09 Extractions 1.1 
(downloaded from reverb.cs.washington.edu) as 
the triple store (relation instances). It is the com-
plete extraction of Reverb over Clueweb09 after 
filtering low confidence and low frequency triples. 
It contains 14.7 million distinct triples with 3.3 
million entities and 1.3 million relation phrases. 
We choose it because 1) it is extracted by a state-
of-the-art open IE extractor from the open-domain, 
and 2) to the best of our knowledge, it contains the 
largest number of distinct triples extracted from the 
open-domain and which is publicly available. 
 
Evaluation setup The evaluations are organized as 
follows: we evaluate Type A relation extraction 
and Type B relation extraction separately, and then 
we compare WEBRE to its closest prior work 
SNE.  Since both phases are essentially clustering 
algorithms, we compare the output clusters with 
human labeled gold standards and report perfor-
mance measures, following most previous work 
such as Kok and Domingos (2008) and Hasegawa 
et al(2004). Three gold standards are created for 
evaluating Type A relations, Type B relations and 
the comparison to SNE, respectively. In the exper-
iments, we set ?=0.6, ?=0.1 and ?=0.02 based on 
trial runs on a small development set of 10k rela-
tion instances. We filtered out the Type A relations 
and Type B relations which only contain 1 or 2 
triples since most of these relations are not differ-
ent from a single relation instance and are not very 
interesting. Overall, 0.2 million Type A relations 
and 84,000 Type B relations are extracted. 
 
Evaluating Type A relations To understand the 
effectiveness of knowledge sources, we run Phase 
1 multiple times taking entity similarity graphs 
(matrices) constructed with resources listed below: 
? TS: Distributional similarity based on the triple 
store. For each triple <ent1, ctx, ent2>, features 
of ent1 are {ctx} and {ctx ent2}; features of ent2 
are {ctx} and {ent1 ctx}. Features are weighted 
with PMI. Cosine is used as similarity measure.  
? LABEL: The similarity between two entities is 
computed according to the percentage of top 
hypernyms they share. 
? SIM: The similarity between two entities is the 
linear combination of their similarity scores in 
the distributional similarity graph and in the 
pattern similarity graph. 
? SIM+LABEL SIM and LABEL are combined. 
Observing that SIM generates high quality but 
overly fine-grained semantic classes, we modify 
the entity clustering procedure to cluster argu-
ment entities based on SIM first, and then fur-
ther clustering the results based on LABEL. 
The outputs of these runs are pooled and mixed 
for labeling. We randomly sampled 60 relation 
phrases. For each phrase, we select the 5 most fre-
quent Type A relations from each run (4?5=206 
Type A relations in all). For each relation phrase, 
we ask a human labeler to label the mixed pool of 
Type A relations that share the phrase: 1) The la-
belers7 are asked to first determine the major se-
mantic relation of each Type A relation, and then 
label the triples as good, fair or bad based on 
whether they express the major relation. 2) The 
labeler also reads all Type A relations and manual-
ly merges the ones that express the same relation. 
These 2 steps are repeated for each phrase. After 
labeling, we create a gold standard GS1, which 
contains roughly 10,000 triples for 60 relation 
phrases. On average, close to 200 triples are manu-
                                                          
6  Here 4 means the 4 methods (the bullet items above) of 
computing similarity. 
7 4 human labelers perform the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 79%. Moreover, each judgment is cross-checked 
by at least one more annotator, further improving quality. 
1033
ally labeled and clustered for each phrase. This 
creates a large data set for evaluation.  
We report micro-average of precision, recall and 
F1 on the 60 relation phrases for each method. Pre-
cision (P) and Recall (R) of a given relation phrase 
is defined as follows. Here ?? and ??
?  represents a 
Type A relation in the algorithm output and GS1, 
respectively. We use t for triples and s(t) to repre-
sent the score of the labeled triple t. s(t) is set to 
1.0, 0.5 or 0 for t labeled as good, fair and bad, 
respectively. 
 
? =
? ? ?(?) ????  ??
? |??|??
, ? =
? ? ?(?) ????  ??
? ? ?(??) ?????
???
?
 
 
The results are in table 1. Overall, LABEL per-
forms 53% better than TS in F-measure, and 
SIM+LABEL performs the best, 8% better than 
LABEL. Applying a simple sign test shows both 
differences are clearly significant (p<0.001). Sur-
prisingly, SIM, which uses the similarity matrix 
extracted from full text, has a F1 of 0.277, which is 
lower than TS. We also tried combining TS and 
LABEL but did not find encouraging performance 
compared to SIM+LABEL. 
 
Algorithm Precision Recall F1 
TS 0.842 (0.886) 0.266 0.388 
LABEL 0.855 (0.870) 0.481 0.596 
SIM 0.755 (0.964) 0.178 0.277 
SIM+LABEL 0.843 (0.872) 0.540 0.643 
 
Table 1. Phase 1 performance (averaged on multiple runs) of 
the 4 methods. The highest performance numbers are in bold. 
(The number in parenthesis is the micro-average when empty-
result relation phrases are not considered for the method). 
 
Among the 4 methods, SIM has the highest preci-
sion (0.964) when relation phrases for which it 
fails to generate any Type A relations are exclud-
ed, but its recall is low. Manual checking shows 
that SIM tends to generate overly fine-grained ar-
gument classes. If fine-grained argument classes or 
extremely high-precision Type A relations are pre-
ferred, SIM is a good choice. LABEL performs 
significantly better than TS, which shows that hy-
pernymy information is very useful for finding ar-
gument semantic classes. However, it has coverage 
problems in that the hypernym finding algorithm 
failed to find any hypernym from the corpus for 
some entities. Following up, we found that 
SIM+LABEL has similar precision and the highest 
recall. This shows that the combination of semantic 
spaces is very helpful. The significant recall im-
provement from TS to SIM+LABEL shows that 
the corpus-based knowledge resources significant-
ly reduce the data sparseness, compared to using 
features extracted from the triple store only. The 
result of the phase 1 algorithm with SIM+LABEL 
is used as input for phase 2. 
 
Evaluating Type B relations The goal is 2-fold: 
1) to evaluate the phase 2 algorithm. This involves 
comparing system output to a gold standard con-
structed by hand, and reporting performance; 2) to 
evaluate the quality of Type B relations. For this, 
we will also report triple-level precision. 
    We construct a gold standard GS28 for evaluat-
ing Type B relations as follows: We randomly 
sampled 178 Type B relations, which contain 1547 
Type A relations and more than 100,000 triples. 
Since the number of triples is very large, it is in-
feasible for labelers to manually cluster triples to 
construct a gold standard. To report precision, we 
asked the labelers to label each Type A relation 
contained in this Type B relation as good, fair or 
bad based on whether it expresses the same rela-
tion. For recall evaluation, we need to know how 
many Type A relations are missing from each Type 
B relation. We provide the full data set of Type A 
relations along with three additional resources: 1) a 
tool which, given a Type A relation, returns a 
ranked list of similar Type A relations based on the 
pairwise relation similarity metric in section 4, 2) 
DIRT paraphrase collection, 3) WordNet (Fell-
baum, 1998) synsets. The labelers are asked to find 
similar phrases by checking phrases which contain 
synonyms of the tokens in the query phrase. Given 
a Type B relation, ideally we expect the labelers to 
find all missing Type A relations using these re-
sources. We report precision (P) and recall (R) as 
follows. Here ??  and ??
?  represent Type B rela-
tions in the algorithm output and GS2, respective-
ly. ??  and ??
?  represent Type A relations. ?(??) 
denotes the score of ??. It is set to 1.0, 0.5 and 0 
for good, fair or bad respectively.  
 
? =
? ? |??|??(??) ???????
? ? |??|  ???????
, ? =
? ? |??|??(??)  ???????
? ? ???
? ???
? ???
???
?
 
 
We also ask the labeler to label at most 50 ran-
domly sampled triples from each Type B relation, 
and calculate triple-level precision as the ratio of 
the sum of scores of triples over the number of  
                                                          
8 3 human labelers performed the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 73%. Similar to labeling Type A relations, each 
judgment is cross-checked by at least one more annotator, 
further improving quality. 
1034
Argument 1 Relation phrase Argument 2 
marijuana, caffeine, nicotine? result in, be risk factor for, be major cause of? insomnia, emphysema, breast cancer,? 
C# 2.0, php5, java, c++, ? allow the use of, also use, introduce the concept of? destructors, interfaces, template,? 
clinton, obama, mccain, ? win, win in, take, be lead in,? ca, dc, fl, nh, pa, va, ga, il, nc,? 
Table 3. Sample Type B relations extracted. 
 
sampled triples. We use ???? to represent the preci-
sion calculated based on labeled triples. Moreover, 
as we are interested in how many phrases are 
found by our algorithm, we also include ???????, 
which is the recall of synonymous phrases. Results 
are shown in Table 2.  
 
Interval P R (???????) F1 ???? count 
[3, 5) 0.913 0.426 (0.026) 0.581 0.872 52149 
[5, 10) 0.834 0.514 (0.074) 0.636 0.863 21981 
[10, 20) 0.854 0.569 (0.066) 0.683 0.883 6277 
[20, 50) 0.899 0.675 (0.406) 0.771 0.894 2630 
[50, +?) 0.922 0.825 (0.594) 0.871 0.929 1089 
Overall 0.897 0.684 (0.324) 0.776 0.898 84126 
Table 2. Performance for Type B relation extraction. The first 
column shows the range of the maximum sizes of Type A 
relations in the Type B relation. The last column shows the 
number of Type B relations that are in this range. The number 
in parenthesis in the third column is the recall of phrases.  
 
The result shows that WEBRE can extract Type B 
relations at high precision (both P and ????). The 
overall recall is 0.684. Table 2 also shows a trend 
that if the maximum number of Type A relation in 
the target Type B relation is larger, the recall is 
better. This shows that the recall of Type B rela-
tions depends on the amount of data available for 
that relation. Some examples of Type B relations 
extracted are shown in Table 3. 
  
Comparison with SNE We compare WEBRE?s 
extracted Type B relations to the relations extract-
ed by its closest prior work SNE9. We found SNE 
is not able to handle the 14.7 million triples in a 
foreseeable amount of time, so we randomly sam-
pled 1 million (1M) triples 10 and test both algo-
rithms on this set. We also filtered out result 
clusters which have only 1 or 2 triples from both 
system outputs. For comparison purposes, we con-
structed a gold standard GS3 as follows: randomly 
select 30 clusters from both system outputs, and 
then find similar clusters from the other system 
output, followed by manually refining the clusters 
                                                          
9 Obtained from alchemy.cs.washington.edu/papers/kok08 
10 We found that SNE?s runtime on 1M triples varies from 
several hours to over a week, depending on the parameters. 
The best performance is achieved with runtime of approxi-
mately 3 days. We also tried SNE with 2M triples, on which 
many runs take several days and show no sign of convergence. 
For fairness, the comparison was done on 1M triples. 
by merging similar ones and splitting non-coherent 
clusters. GS3 contains 742 triples and 135 clusters. 
We report triple-level pairwise precision, recall 
and F1 for both algorithms against GS3, and report 
results in Table 4. We fine-tuned SNE (using grid 
search, internal cross-validation, and coarse-to-fine 
parameter tuning), and report its best performance. 
 
Algorithm Precision Recall F1 
WEBRE 0.848 0.734 0.787 
SNE 0.850 0.080 0.146 
 
Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.  
 
Table 4 shows that WEBRE outperforms SNE 
significantly in pairwise recall while having similar 
precision. There are two reasons. First, WEBRE 
makes use of several corpus-level semantic sources 
extracted from the corpus for clustering entities 
and phrases while SNE uses only features in the 
triple store. These semantic resources significantly 
reduced data sparseness. Examination of the output 
shows that SNE is unable to group many triples 
from the same generally-recognized fine-grained 
relations. For example, SNE placed relation in-
stances <Barbara, grow up in, Santa Fe> and 
<John, be raised mostly in, Santa Barbara> into 2 
different clusters because the arguments and 
phrases do not share features nor could be grouped 
by SNE?s mutual clustering. In contrast, WEBRE 
groups them together. Second, SNE assumes a re-
lation phrase to be in exactly one cluster. For ex-
ample, SNE placed be part of in the phrase cluster 
be city of and failed to place it in another cluster be 
subsidiary of. This limits SNE?s ability to placing 
relation instances with polysemous phrases into 
correct relation clusters. 
It should be emphasized that we use pairwise 
precision and recall in table 4 to be consistent with 
the original SNE paper. Pairwise metrics are much 
more sensitive than instance-level metrics, and pe-
nalize recall exponentially in the worst case11 if an 
algorithm incorrectly splits a coherent cluster; 
therefore the absolute pairwise recall difference 
                                                          
11 Pairwise precision and recall are calculated on all pairs that 
are in the same cluster, thus are very sensitive. For example, if 
an algorithm incorrectly split a cluster of size N to a smaller 
main cluster of size N/2 and some constant-size clusters, pair-
wise recall could drop to as much as ? of its original value. 
1035
should not be interpreted as the same as the in-
stance-level recall reported in previous experi-
ments. On 1 million triples, WEBRE generates 
12179 triple clusters with an average size12 of 13 
while SNE generate 53270 clusters with an aver-
age size 5.1. In consequence, pairwise recall drops 
significantly. Nonetheless, at above 80% pairwise 
precision, it demonstrates that WEBRE can group 
more related triples by adding rich semantics har-
vested from the web and employing a more general 
treatment of polysemous relation phrases.  On 1M 
triples, WEBRE finished in 40 minutes, while the 
run time of SNE varies from 3 hours to a few days. 
6 Conclusion 
We present a fully unsupervised algorithm 
WEBRE for large-scale open-domain relation ex-
traction. WEBRE explicitly handles polysemy rela-
tions and achieves a significant improvement on 
recall by incorporating rich corpus-based semantic 
resources. Experiments on a large data set show 
that it can extract a very large set of high-quality 
relations. 
 
Acknowledgements 
Supported in part by the Intelligence Advanced 
Research Projects Activity (IARPA) via Air Force 
Research Laboratory (AFRL) contract number 
FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for 
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and 
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either 
expressed or implied, of IARPA, AFRL, or the 
U.S. Government. 
 
References 
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007. Open 
Information Extraction from the Web. In Proceedings 
of IJCAI 2007. 
                                                          
12 The clusters which have only 1 or 2 triples are removed and 
not counted here for both algorithms. 
Michele Banko and Oren Etzioni. 2008. The Tradeoffs 
Between Open and Traditional Relation Extraction. 
In Proceedings of ACL 2008. 
Jonathan Berant, Ido Dagan and Jacob Goldberger. 
2011. Global Learning of Typed Entailment Rules. In 
Proceedings of ACL 2011. 
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective Information Extraction with Relational Mar-
kov Networks. In Proceedings of ACL 2004. 
Jinxiu Chen, Donghong Ji, Chew Lim Tan, Zhengyu 
Niu. 2005. Unsupervised Feature Selection for Rela-
tion Extraction. In Proceedings of IJCNLP 2005. 
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen 
Soderland, Daniel S. Weld, and Alexander Yates. 
2004. Web-scale information extraction in 
KnowItAll (preliminary results). In Proceedings of 
WWW 2004. 
Oren Etzioni, Michael Cafarella, Doug Downey, 
AnaMaria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An 
Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Anthony Fader, Stephen Soderland, and Oren Etzioni. 
2011. Identifying Relations for Open Information Ex-
traction. In Proceedings of EMNLP 2011. 
Christiane Fellbaum (Ed.). 1998. WordNet: An Elec-
tronic Lexical Database. Cambridge, MA: MIT Press. 
Zelig S. Harris. 1985. Distributional Structure. The Phi-
losophy of Linguistics. New York: Oxford Uni-
versity Press. 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman . 
2004.Discovering Relations among Named Entities 
from Large Corpora. In Proceedings of ACL 2004. 
Marti A. Hearst. 1992. Automatic  Acquisition of  Hy-
ponyms from Large Text Corpora. In Proceedings of 
COLING 1992. 
Stanley Kok and Pedro Domingos. 2008. Extracting 
Semantic Networks from Text via Relational Cluster-
ing. In Proceedings of ECML 2008. 
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. 
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. In Proceedings of ACL 
2008. 
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of 
KDD 2001. 
Andrew McCallum, Kamal Nigam and Lyle Ungar. 
2000. Efficient Clustering of High-Dimensional Data 
Sets with Application to Reference Matching. In Pro-
ceedings of KDD 2000. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009. 
1036
Patrick Pantel and Dekang Lin. 2002. Discovering word 
senses from text. In Proceedings of KDD2002. 
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. In Proceedings 
of HLT/NAACL-2004. 
Marius Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search, In Proceedings of CIKM 
2004. 
Marius Pasca. 2007. Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM 2007. 
Marius Pasca and Peter Dienes. 2005. Aligning needles 
in a haystack: Paraphrase acquisition across the Web. 
In Proceedings of IJCNLP 2005. 
Marco Pennacchiotti and Patrick Pantel. 2009. Entity 
Extraction via Ensemble Semantics. In Proceedings 
of EMNLP 2009. 
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In 
Proceedings of CIKM 2007. 
Luis Sarmento, Valentin Jijkoun, Maarten de Rijke and 
Eugenio Oliveira. 2007. ?More like these?: growing 
entity classes from seeds. In Proceedings of CIKM 
2007. 
Satoshi Sekine. 2005. Automatic paraphrase discovery 
based on context and keywords between NE pairs. In 
Proceedings of the International Workshop on Para-
phrasing, 2005. 
Shuming Shi, Huibin Zhang, Xiaojie Yuan, Ji-Rong 
Wen. 2010. Corpus-based Semantic Class Mining: 
Distributional vs. Pattern-Based Approaches. In Pro-
ceedings of COLING 2010. 
Yusuke Shinyama, Satoshi Sekine. 2006. Preemptive 
Information Extraction using Unrestricted Relation 
Discovery, In Proceedings of NAACL 2006. 
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. 
Learning Syntactic Patterns for Automatic Hypernym 
Discovery. In Proceedings of  In NIPS 17, 2005. 
Stephen Soderland and Bhushan Mandhani. 2007. Mov-
ing from Textual Relations to Ontologized Relations. 
In Proceedings of the 2007 AAAI Spring Symposium 
on Machine Reading. 
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, 
Deepak Ravichandran, Rahul Bhagat and Fernando 
Pereira. 2008. Weakly-Supervised Acquisition of La-
beled Class Instances using Graph Random Walks. In 
Proceedings of EMNLP 2008. 
David Vickrey, Oscar Kipersztok and Daphne Koller. 
2010. An Active Learning Approach to Finding Re-
lated Terms. In Proceedings of ACL 2010. 
Vishnu Vyas and Patrick Pantel. 2009. SemiAutomatic 
Entity Set Refinement. In Proceedings of 
NAACL/HLT 2009. 
Vishnu Vyas, Patrick Pantel and Eric Crestan. 2009, 
Helping Editors Choose Better Seed Sets for Entity 
Set Expansion, In Proceedings of CIKM 2009. 
Richard C. Wang and William W. Cohen. 2007. Lan-
guage- Independent Set Expansion of Named Entities 
Using the Web. In Proceedings of ICDM 2007. 
Richard C. Wang and William W. Cohen. 
2009. Automatic Set Instance Extraction using the 
Web. In Proceedings of ACL-IJCNLP 2009. 
Wei Wang, Romaric Besan?on and Olivier Ferret. 2011. 
Filtering and Clustering Relations for Unsupervised 
Information Extraction in Open Domain. In Proceed-
ings of CIKM 2011. 
Fei Wu and Daniel S. Weld. 2010. Open information 
extraction using Wikipedia. In Proceedings of ACL 
2010. 
Hua Wu and Ming Zhou. 2003. Synonymous colloca-
tion extraction using translation information. In Pro-
ceedings of the ACL Workshop on Multiword 
Expressions: Integrating Processing 2003. 
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew 
McCallum. 2011. Structured Relation Discovery Us-
ing Generative Models. In Proceedings of EMNLP 
2011.  
Alexander Yates and Oren Etzioni. 2007. Unsupervised 
Resolution of Objects and Relations on the Web.  In 
Proceedings of HLT-NAACL 2007.  
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, Chin-
Yew Lin. 2011. Nonlinear Evidence Fusion and 
Propagation for Hyponymy Relation Mining. In Pro-
ceedings of ACL 2011. 
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong 
Wen. 2009. Employing Topic Models for Pattern-
based Semantic Class Discovery. In Proceedings of 
ACL 2009. 
1037
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85?90,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Question Difficulty Estimation in Community Question Answering Services?
Jing Liu? Quan Wang? Chin-Yew Lin? Hsiao-Wuen Hon?
?Harbin Institute of Technology, Harbin 150001, P.R.China
?Peking University, Beijing 100871, P.R.China
?Microsoft Research Asia, Beijing 100080, P.R.China
jliu@ir.hit.edu.cn quanwang1012@gmail.com {cyl,hon}@microsoft.com
Abstract
In this paper, we address the problem of
estimating question difficulty in community
question answering services. We propose a
competition-based model for estimating ques-
tion difficulty by leveraging pairwise compar-
isons between questions and users. Our ex-
perimental results show that our model sig-
nificantly outperforms a PageRank-based ap-
proach. Most importantly, our analysis shows
that the text of question descriptions reflects
the question difficulty. This implies the pos-
sibility of predicting question difficulty from
the text of question descriptions.
1 Introduction
In recent years, community question answering (C-
QA) services such as Stackoverflow1 and Yahoo!
Answers2 have seen rapid growth. A great deal
of research effort has been conducted on CQA, in-
cluding: (1) question search (Xue et al, 2008; Du-
an et al, 2008; Suryanto et al, 2009; Zhou et al,
2011; Cao et al, 2010; Zhang et al, 2012; Ji et
al., 2012); (2) answer quality estimation (Jeon et al,
2006; Agichtein et al, 2008; Bian et al, 2009; Liu
et al, 2008); (3) user expertise estimation (Jurczyk
and Agichtein, 2007; Zhang et al, 2007; Bouguessa
et al, 2008; Pal and Konstan, 2010; Liu et al, 2011);
and (4) question routing (Zhou et al, 2009; Li and
King, 2010; Li et al, 2011).
?This work was done when Jing Liu and Quan Wang were
visiting students at Microsoft Research Asia. Quan Wang is
currently affiliated with Institute of Information Engineering,
Chinese Academy of Sciences.
1http://stackoverflow.com
2http://answers.yahoo.com
However, less attention has been paid to question
difficulty estimation in CQA. Question difficulty es-
timation can benefit many applications: (1) Experts
are usually under time constraints. We do not want
to bore experts by routing every question (including
both easy and hard ones) to them. Assigning ques-
tions to experts by matching question difficulty with
expertise level, not just question topic, will make
better use of the experts? time and expertise (Ack-
erman and McDonald, 1996). (2) Nam et al (2009)
found that winning the point awards offered by the
reputation system is a driving factor in user partici-
pation in CQA. Question difficulty estimation would
be helpful in designing a better incentive mechanis-
m by assigning higher point awards to more diffi-
cult questions. (3) Question difficulty estimation can
help analyze user behavior in CQA, since users may
make strategic choices when encountering questions
of different difficulty levels.
To the best of our knowledge, not much research
has been conducted on the problem of estimating
question difficulty in CQA. The most relevant work
is a PageRank-based approach proposed by Yang et
al. (2008) to estimate task difficulty in crowdsourc-
ing contest services. Their key idea is to construct
a graph of tasks: creating an edge from a task t1 to
a task t2 when a user u wins task t1 but loses task
t2, implying that task t2 is likely to be more diffi-
cult than task t1. Then the standard PageRank al-
gorithm is employed on the task graph to estimate
PageRank score (i.e., difficulty score) of each task.
This approach implicitly assumes that task difficulty
is the only factor affecting the outcomes of competi-
tions (i.e. the best answer). However, the outcomes
of competitions depend on both the difficulty levels
of tasks and the expertise levels of competitors (i.e.
85
other answerers).
Inspired by Liu et al (2011), we propose a
competition-based approach which jointly models
question difficulty and user expertise level. Our ap-
proach is based on two intuitive assumptions: (1)
given a question answering thread, the difficulty s-
core of the question is higher than the expertise score
of the asker, but lower than that of the best answerer;
(2) the expertise score of the best answerer is higher
than that of the asker as well as all other answer-
ers. Given the two assumptions, we can determine
the question difficulty score and user expertise score
through pairwise comparisons between (1) a ques-
tion and an asker, (2) a question and a best answerer,
(3) a best answerer and an asker, and (4) a best an-
swerer and all other non-best answerers.
The main contributions of this paper are:
?We propose a competition-based approach to es-
timate question difficulty (Sec. 2). Our model signif-
icantly outperforms the PageRank-based approach
(Yang et al, 2008) for estimating question difficulty
on the data of Stack Overflow (Sec. 3.2).
?Additionally, we calibrate question difficulty s-
cores across two CQA services to verify the effec-
tiveness of our model (Sec. 3.3).
?Most importantly, we demonstrate that different
words or tags in the question descriptions indicate
question difficulty levels. This implies the possibil-
ity of predicting question difficulty purely from the
text of question descriptions (Sec. 3.4).
2 Competition based Question Difficulty
Estimation
CQA is a virtual community where people can ask
questions and seek opinions from others. Formally,
when an asker ua posts a question q, there will be
several answerers to answer her question. One an-
swer among the received ones will be selected as the
best answer by the asker ua or voted by the com-
munity. The user who provides the best answer is
called the best answerer ub, and we denote the set of
all non-best answerers as S = {uo1 , ? ? ? , uoM}. As-
suming that question difficulty scores and user ex-
pertise scores are expressed on the same scale, we
make the following two assumptions:
?The difficulty score of question q is higher than
the expertise score of asker ua, but lower than that
of the best answerer ub. This is intuitive since the
best answer ub correctly responds to question q that
asker ua does not know.
?The expertise score of the best answerer ub is
higher than that of asker ua and all answerers in S.
This is straightforward since the best answerer ub
solves question q better than asker ua and all non-
best answerers in S.
Let?s view question q as a pseudo user uq. Tak-
ing a competitive viewpoint, each pairwise compar-
ison can be viewed as a two-player competition with
one winner and one loser, including (1) one compe-
tition between pseudo user uq and asker ua, (2) one
competition between pseudo user uq and the best
answerer ub, (3) one competition between the best
answerer ub and asker ua, and (4) |S| competitions
between the best answerer ub and all non-best an-
swers in S. Additionally, pseudo user uq wins the
first competition and the best answerer ub wins all
remaining (|S| + 2) competitions.
Hence, the problem of estimating the question d-
ifficulty score (and the user expertise score) is cast
as a problem of learning the relative skills of play-
ers from the win-loss results of the generated two-
player competitions. Formally, let Q denote the set
of all questions in one category (or topic), andRq de-
note the set of all two-player competitions generated
from question q ? Q, i.e., Rq = {(ua ? uq), (uq ?
ub), (ua ? ub), (uo1 ? ub), ? ? ? , (uo|S| ? ub)},
where j ? i means that user i beats user j in the
competition. Define
R =
?
q?Q
Rq (1)
as the set of all two-player competitions. Our prob-
lem is then to learn the relative skills of players from
R. The learned skills of the pseudo question users
are question difficulty scores, and the learned skills
of all other users are their expertise scores.
TrueSkill In this paper, we follow (Liu et al,
2011) and apply TrueSkill to learn the relative skill-
s of players from the set of generated competitions
R (Equ. 1). TrueSkill (Herbrich et al, 2007) is a
Bayesian skill rating model that is developed for es-
timating the relative skill levels of players in games.
In this paper, we present a two-player version of
TrueSkill with no-draw.
TrueSkill assumes that the practical performance
of each player in a game follows a normal distribu-
86
tion N(?, ?2), where ? means the skill level of the
player and ? means the uncertainty of the estimated
skill level. Basically, TrueSkill learns the skill lev-
els of players by leveraging Bayes? theorem. Giv-
en the current estimated skill levels of two players
(priori probability) and the outcome of a new game
between them (likelihood), TrueSkill model updates
its estimation of player skill levels (posterior prob-
ability). TrueSkill updates the skill level ? and the
uncertainty ? intuitively: (a) if the outcome of a new
competition is expected, i.e. the player with higher
skill level wins the game, it will cause small updates
in skill level ? and uncertainty ?; (b) if the outcome
of a new competition is unexpected, i.e. the player
with lower skill level wins the game, it will cause
large updates in skill level ? and uncertainty ?. Ac-
cording to these intuitions, the equations to update
the skill level ? and uncertainty ? are as follows:
?winner = ?winner +
?2winner
c ? v
(
t
c ,
?
c
)
, (2)
?loser = ?loser ?
?2loser
c
? v
(
t
c
, ?
c
)
, (3)
?2winner = ?2winner ?
[
1 ? ?
2
winner
c2
? w
(
t
c
, ?
c
)]
,
(4)
?2loser = ?2loser ?
[
1 ?
?2loser
c2
? w
(
t
c
, ?
c
)]
, (5)
where t = ?winner ? ?loser and c2 = 2?2 +
?2winner+?2loser. Here, ? is a parameter representing
the probability of a draw in one game, and v(t, ?)
and w(t, ?) are weighting factors for skill level ?
and standard deviation ? respectively. Please refer
to (Herbrich et al, 2007) for more details. In this
paper, we set the initial values of the skill level ?
and the standard deviation ? of each player the same
as the default values used in (Herbrich et al, 2007).
3 Experiments
3.1 Data Set
In this paper, we use Stack Overflow (SO) for our
experiments. We obtained a publicly available da-
ta set3 of SO between July 31, 2008 and August 1,
2012. SO contains questions with various topics,
such as programming, mathematics, and English. In
this paper, we use SO C++ programming (SO/CPP)
3http://blog.stackoverflow.com/category/
cc-wiki-dump/
and mathematics4 (SO/Math) questions for our main
experiments. Additionally, we use the data of Math
Overflow5 (MO) for calibrating question difficulty
scores across communities (Sec. 3.3). The statistics
of these data sets are shown in Table 1.
SO/CPP SO/Math MO
# of questions 122, 012 51, 174 27, 333
# of answers 357, 632 94, 488 65, 966
# of users 67, 819 16, 961 12, 064
Table 1: The statistics of the data sets.
To evaluate the effectiveness of our proposed
model for estimating question difficulty scores, we
randomly sampled 300 question pairs from both
SO/CPP and SO/Math, and we asked experts to
compare the difficulty of every pair. We had two
graduate students majoring in computer science an-
notate the SO/CPP question pairs, and two gradu-
ate students majoring in mathematics annotate the
SO/Math question pairs. When annotating each
question pair, only the titles, descriptions, and tags
of the questions were shown, and other information
(e.g. users, answers, etc.) was excluded. Given each
pair of questions (q1 and q2), the annotators were
asked to give one of four labels: (1) q1 ? q2, which
means that the difficulty of q1 was higher than q2;
(2) q1 ? q2, which means that the difficulty of q1
was lower than q2; (3) q1 = q2, which means that
the difficulty of q1 was equal to q2; (4) Unknown,
which means that the annotator could not make a
decision. The agreements between annotators on
both SO/CPP (kappa value = 0.741) and SO/Math
(kappa value = 0.873) were substantial. When eval-
uating models, we only kept the pairs that annotators
had given the same labels. There were 260 SO/CPP
question pairs and 280 SO/Math question pairs re-
maining.
3.2 Accuracy of Question Difficulty Estimation
We employ a standard evaluation metric for infor-
mation retrieval: accuracy (Acc), defined as follows:
Acc = the number of correct pairwise comparisons
the total number of pairwise comparisons
.
We use the PageRank-based approach proposed
by Yang et al (2008) as a baseline. As described in
4http://math.stackexchange.com
5http://mathoverflow.net
87



	


 	     	     	 	 	 
  	






Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1521?1532,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Hierarchical Entity-based Approach to Structuralize User Generated
Content in Social Media: A Case of Yahoo! Answers
Baichuan Li1,2?, Jing Liu3?, Chin-Yew Lin4, Irwin King1,2, and Michael R. Lyu1,2
1Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China
2Department of Computer Science and Engineering
The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
3Harbin Institute of Technology, Harbin 150001, P.R. China
4Microsoft Research Asia, Beijing 100080, P.R. China
bcli@cse.cuhk.edu.hk jliu@ir.hit.edu.cn cyl@microsoft.com
{king,lyu}@cse.cuhk.edu.hk
Abstract
Social media like forums and microblogs have
accumulated a huge amount of user generated
content (UGC) containing human knowledge.
Currently, most of UGC is listed as a whole
or in pre-defined categories. This ?list-based?
approach is simple, but hinders users from
browsing and learning knowledge of certain
topics effectively. To address this problem, we
propose a hierarchical entity-based approach
for structuralizing UGC in social media. By
using a large-scale entity repository, we design
a three-step framework to organize UGC in
a novel hierarchical structure called ?cluster
entity tree (CET)?. With Yahoo! Answers as
a test case, we conduct experiments and the
results show the effectiveness of our frame-
work in constructing CET. We further evaluate
the performance of CET on UGC organiza-
tion in both user and system aspects. From
a user aspect, our user study demonstrates
that, with CET-based structure, users perform
significantly better in knowledge learning than
using traditional list-based approach. From
a system aspect, CET substantially boosts
the performance of two information retrieval
models (i.e., vector space model and query
likelihood language model).
1 Introduction
With the development of Web 2.0, social
media websites?such as online forums, blogs,
microblogs, social networks, and community
?This work was done when the first two authors were on
internship at MSRA.
Table 1: Sample questions about Edinburgh
1. Where can i buy a hamburger in Edinburgh?
2. Where can I get a shawarma in Edinburgh?
3. How long does it take to drive between Glasgow
and Edinburgh?
4. Whats the difference between Glasgow and Edinburgh?
5. Good hotels in London and Edinburgh?
6. Looking for nice , clean cheap hotel in Edinburgh?
7. Does anyone know of a reasonably cheap hotel in
Edinburgh that is near to Niddry Street South ?
8. Who can recommend a affordable hotel in
Edinburgh City Center?
question answering (CQA) portals?have become
the mainstream of web, where users create, share,
and exchange information with each other. As a
result, more and more UGC is accumulated, with
social media websites retaining a huge amount of
human knowledge and user experience. At present,
most of UGC is organized in a list structure with
extra information (e.g., category hierarchies in
online forums), or without any other information.
This ?list-of-content? (list-based approach) is
simple and straightforward, but ineffective for
browsing and knowledge learning. Consider
the following case: a user wants to spend his
vacation in Edinburgh. He visits a CQA website
to explore which aspects are mostly asked. In this
scenario, he may browse some relevant categories
like ?Travel:United Kingdom:Edinburgh? to get
useful information. He may also issue a query like
?travel in Edinburgh? to search relevant questions.
However, both the browsing and the searching give
the user a list of relevant contents (e.g., questions
shown in Table 1), not the direct knowledge. Thus,
the user has to read these contents, understand them,
classify them into various topics, and gain valuable
1521
HGLQEXUJK
^Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 799?809,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Template Mining for Semantic Category Understanding
Lei Shi
1,2?
, Shuming Shi
3
, Chin-Yew Lin
3
, Yi-Dong Shen
1
, Yong Rui
3
1
State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of Sciences
2
University of Chinese Academy of Sciences
3
Microsoft Research
{shilei,ydshen}@ios.ac.cn
{shumings,cyl,yongrui}@microsoft.com
Abstract
We propose an unsupervised approach to
constructing templates from a large collec-
tion of semantic category names, and use
the templates as the semantic representa-
tion of categories. The main challenge is
that many terms have multiple meanings,
resulting in a lot of wrong templates. Sta-
tistical data and semantic knowledge are
extracted from a web corpus to improve
template generation. A nonlinear scoring
function is proposed and demonstrated to
be effective. Experiments show that our
approach achieves significantly better re-
sults than baseline methods. As an imme-
diate application, we apply the extracted
templates to the cleaning of a category col-
lection and see promising results (preci-
sion improved from 81% to 89%).
1 Introduction
A semantic category is a collection of items shar-
ing common semantic properties. For example,
all cities in Germany form a semantic category
named ?city in Germany? or ?German city?. In
Wikipedia, the category names of an entity are
manually edited and displayed at the end of the
page for the entity. There have been quite a lot of
approaches (Hearst, 1992; Pantel and Ravichan-
dran, 2004; Van Durme and Pasca, 2008; Zhang et
al., 2011) in the literature to automatically extract-
ing category names and instances (also called is-a
or hypernymy relations) from the web.
Most existing work simply treats a category
name as a text string containing one or multiple
words, without caring about its internal structure.
In this paper, we explore the semantic structure
of category names (or simply called ?categories?).
?
This work was performed when the first author was vis-
iting Microsoft Research Asia.
For example, both ?CEO of General Motors? and
?CEO of Yahoo? have structure ?CEO of [com-
pany]?. We call such a structure a category tem-
plate. Taking a large collection of open-domain
categories as input, we construct a list of category
templates and build a mapping from categories to
templates. Figure 1 shows some example semantic
categories and their corresponding templates.
Templates can be treated as additional features
of semantic categories. The new features can be
exploited to improve some upper-layer applica-
tions like web search and question answering. In
addition, by linking categories to templates, it is
possible (for a computer program) to infer the se-
mantic meaning of the categories. For example in
Figure 1, from the two templates linking to cat-
egory ?symptom of insulin deficiency?, it is rea-
sonable to interpret the category as: ?a symptom
of a medical condition called insulin deficiency
which is about the deficiency of one type of hor-
mone called insulin.? In this way, our knowledge
about a category can go beyond a simple string
and its member entities. An immediate application
of templates is removing invalid category names
from a noisy category collection. Promising re-
sults are observed for this application in our ex-
periments.
An intuitive approach to this task (i.e., extract-
ing templates from a collection of category names)
national holiday of South Africa(instances: Heritage Day, Christmas?)
national holiday of Brazil(instances: Carnival, Christmas?) national holiday of [country]
symptom of cortisol deficiency(instances: low blood sugar?)
symptom of insulin deficiency(instances: nocturia, weight loss?) symptom of [hormone] deficiencysymptom of [medical condition]
school in Denverschool in Houston school in [place]school in [city]
Semantic Categories Category templates
football playerbasketball player [sport] player
Figure 1: Examples of semantic categories and
their corresponding templates.
799
contains two stages: category labeling, and tem-
plate scoring.
Category labeling: Divide a category name
into multiple segments and replace some key seg-
ments with its hypernyms. As an example, as-
sume ?CEO of Delphinus? is divided to three seg-
ments ?CEO + of + Delphinus?; and the last seg-
ment (Delphinus) has hypernyms ?constellation?,
?company?, etc. By replacing this segment with
its hypernyms, we get candidate templates ?CEO
of [constellation]? (a wrong template), ?CEO of
[company]?, and the like.
Template scoring: Compute the score of each
candidate template by aggregating the information
obtained in the first phase.
A major challenge here is that many segments
(like ?Delphinus? in the above example) have mul-
tiple meanings. As a result, wrong hypernyms
may be adopted to generate incorrect candidate
templates (like ?CEO of [constellation]?). In this
paper, we focus on improving the template scor-
ing stage, with the goal of assigning lower scores
to bad templates and larger scores to high-quality
ones.
There have been some research efforts (Third,
2012; Fernandez-Breis et al., 2010; Quesada-
Mart?nez et al., 2012) on exploring the structure of
category names by building patterns. However, we
automatically assign semantic types to the pattern
variables (or called arguments) while they do not.
For example, our template has the form of ?city
in [country]? while their patterns are like ?city in
[X]?. More details are given in the related work
section.
A similar task is query understanding, including
query tagging and query template mining. Query
tagging (Li et al., 2009; Reisinger and Pasca,
2011) corresponds to the category labeling stage
described above. It is different from template gen-
eration because the results are for one query only,
without merging the information of all queries to
generate the final templates. Category template
construction are slightly different from query tem-
plate construction. First, some useful features
such as query click-through is not available in cat-
egory template construction. Second, categories
should be valid natural language phrases, while
queries need not. For example, ?city Germany? is
a query but not a valid category name. We discuss
in more details in the related work section.
Our major contributions are as follows.
1) To the best of our knowledge, this is the first
work of template generation specifically for cate-
gories in unsupervised manner.
2) We extract semantic knowledge and statisti-
cal information from a web corpus for improving
template generation. Significant performance im-
provement is obtained in our experiments.
3) We study the characteristics of the scoring
function from the viewpoint of probabilistic evi-
dence combination and demonstrate that nonlinear
functions are more effective in this task.
4) We employ the output templates to clean
our category collection mined from the web, and
get apparent quality improvement (precision im-
proved from 81% to 89%).
After discussing related work in Section 2, we
define the problem and describe one baseline ap-
proach in Section 3. Then we introduce our ap-
proach in Section 4. Experimental results are re-
ported and analyzed in Section 5. We conclude the
paper in Section 6.
2 Related work
Several kinds of work are related to ours.
Hypernymy relation extraction: Hypernymy
relation extraction is an important task in text min-
ing. There have been a lot of efforts (Hearst, 1992;
Pantel and Ravichandran, 2004; Van Durme and
Pasca, 2008; Zhang et al., 2011) in the literature to
extract hypernymy (or is-a) relations from the web.
Our target here is not hypernymy extraction, but
discovering the semantic structure of hypernyms
(or category names).
Category name exploration: Category name
patterns are explored and built in some ex-
isting research work. Third (2012) pro-
posed to find axiom patterns among category
names on an existing ontology. For ex-
ample, infer axiom pattern ?SubClassOf(AB,
B)? from ?SubClassOf(junior school school)?
and ?SubClassOf(domestic mammal mammal)?.
Fernandez-Breis et al. (2010) and Quesada-
Mart?nez et al. (2012) proposed to find lexical pat-
terns in category names to define axioms (in med-
ical domain). One example pattern mentioned in
their papers is ?[X] binding?. They need man-
ual intervention to determine what X means. The
main difference between the above work and ours
is that we automatically assign semantic types to
the pattern variables (or called arguments) while
they do not.
800
Template mining for IE: Some research work
in information extraction (IE) involves patterns.
Yangarber (2003) and Stevenson and Greenwood
(2005) proposed to learn patterns which were in
the form of [subject, verb, object]. The category
names and learned templates in our work are not
in this form. Another difference between our work
and their work is that, their methods need a super-
vised name classifer to generate the candidate pat-
terns while our approach is unsupervised. Cham-
bers and Jurafsky (2011) leverage templates to de-
scribe an event while the templates in our work are
for understanding category names (a kind of short
text).
Query tagging/labeling: Some research work
in recent years focuses on segmenting web search
queries and assigning semantic tags to key seg-
ments. Li et al. (2009) and Li (2010) employed
CRF (Conditional Random Field) or semi-CRF
models for query tagging. A crowdsourcing-
assisted method was proposed by Han et al. (2013)
for query structure interpretation. These super-
vised or semi-supervised approaches require much
manual annotation effort. Unsupervised meth-
ods were proposed by Sarkas et al. (2010) and
Reisinger and Pasca (2011). As been discussed
in the introduction section, query tagging is only
one of the two stages of template generation. The
tagging results are for one query only, without ag-
gregating the global information of all queries to
generate the final templates.
Query template construction: Some existing
work leveraged query templates or patterns for
query understanding. A semi-supervised random
walk based method was proposed by Agarwal et
al. (2010) to generate a ranked templates list which
are relevant to a domain of interest. A predefined
domain schema and seed information is needed for
this method. Pandey and Punera (2012) proposed
an unsupervised method based on graphical mod-
els to mine query templates. The above methods
are either domain-specific (i.e., generating tem-
plates for a specific domain), or have some degree
of supervision (supervised or semi-supervised).
Cheung and Li (2012) proposed an unsupervised
method to generate query templates by the aid of
knowledge bases. An approach was proposed in
(Szpektor et al., 2011) to improve query recom-
mendation via query templates. Query session in-
formation (which is not available in our task) is
needed in this approach for templates generation.
Li et al. (2013) proposed an clustering algorithm
to group existing query templates by search intents
of users.
Compared to the open-domain unsupervised
methods for query template construction, our ap-
proach improves on two aspects. First, we propose
to incorporate multiple types of semantic knowl-
edge (e.g., term peer similarity and term clusters)
to improve template generation. Second, we pro-
pose a nonlinear template scoring function which
is demonstrated to be more effective.
3 Problem Definition and Analysis
3.1 Problem definition
The goal of this paper is to construct a list of cat-
egory templates from a collection of open-domain
category names.
Input: The input is a collection of category
names, which can either be manually compiled
(like Wikipedia categories) or be automatically ex-
tracted. The categories used in our experiments
were automatically mined from the web, by fol-
lowing existing work (Hearst, 1992, Pantel and
Ravichandran 2004; Snow et al., 2005; Talukdar
et al., 2008; Zhang et al., 2011). Specifically,
we applied Hearst patterns (e.g., ?NP [,] (such
as | including) {NP, }
?
{and|or} NP?) and is-
a patterns (?NP (is|are|was|were|being) (a|an|the)
NP?) to a large corpus containing 3 billion En-
glish web pages. As a result, we obtained a
term?hypernym bi-partite graph containing 40
million terms, 74 million hypernyms (i.e., cate-
gory names), and 321 million edges (e.g., one
example edge is ?Berlin???city in Germany?,
where ?Berlin? is a term and ?city in Germany? is
the corresponding hypernym). Then all the multi-
word hypernyms are used as the input category
collection.
Output: The output is a list of templates, each
having a score indicating how likely it is valid. A
template is a multi-word string with one headword
and at least one argument. For example, in tem-
plate ?national holiday of [country]?, ?holiday? is
the headword, and ?[country]? is the argument.
We only consider one-argument templates in this
paper, and the case of multiple arguments is left as
future work. A template is valid if it is syntacti-
cally and semantically correct. ?CEO of [constel-
lation]? (wrongly generated from ?CEO of Del-
phinus?, ?CEO of Aquila?, etc.) is not valid be-
cause it is semantically unreasonable.
801
3.2 Baseline approach
An intuitive approach to this task contains two
stages: category labeling and template scoring.
Figure 2 shows its workflow with simple exam-
ples.
3.2.1 Phase-1: Category labeling
At this stage, each category name is automatically
segmented and labeled; and some candidate tem-
plate tuples (CTTs) are derived based on the la-
beling results. This can be done in the following
steps.
Category segmentation: Divide each cate-
gory name into multiple segments (e.g., ?holi-
day of South Africa? to ?holiday + of + South
Africa?). Each segment is one word or a phrase
appearing in an entity dictionary. The dictionary
used in this paper is comprised of all Freebase
(www.freebase.com) entities.
Segment to hypernym: Find hypernyms for
every segment (except for the headword and some
trivial segments like prepositions and articles), by
referring to a term?hypernym mapping graph.
Following most existing query labeling work, we
derive the term?hypernym graph from a dump of
Freebase. Below are some examples of Freebase
types (hypernyms),
German city (id: /location/de city)
Italian province (id: /location/it province)
Poem character (id: /book/poem character)
Book (id: /book/book)
To avoid generating too fine-grained templates
like ?mayor of [Germany city]? and ?mayor of
[Italian city]? (semantically ?mayor of [city]?
is more desirable), we discard type modifiers
and map terms to the headwords of Freebase
types. For example, ?Berlin? is mapped to
?city?. In this way, we build our basic version of
term?hypernym mapping which contains 16.13
million terms and 696 hypernyms. Since ?South
Africa? is both a country and a book name in Free-
base, hypernyms ?country?, ?book?, and others
are assigned to the segment ?South Africa? in this
step.
CTT generation: Construct CTTs by choosing
one segment (called the target segment) each time
and replacing the segment with its hypernyms. An
CTT is formed by the candidate template (with
one argument), the target segment (as an argument
value), and the tuple score (indicating tuple qual-
ity). Below are example CTTs obtained after the
last segment of ?holiday + of + South Africa? is
processed,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [book], South Africa, w
2
)
3.2.2 Phase-2: Template scoring
The main objective of this stage is to merge all
the CTTs obtained from the previous stage and to
compute a final score for each template. In this
stage, the CTTs are first grouped by the first ele-
ment (i.e., the template string). For example, tu-
ples for ?holiday of [country]? may include,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [country], Brazil, w
2
)
U
3
: (holiday of [country], Germany, w
3
)
...
Then a scoring function is employed to calcu-
late the template score from the tuple scores. For-
mally, given n tuples
~
U=(U
1
, U
2
..., U
n
) for a tem-
plate, the goal is to find a score fusion function
F (
~
U) which yields large values for high-quality
templates and small (or zero) values for invalid
ones.
Borrowing the idea of TF-IDF from information
retrieval, a reasonable scoring function is,
F (
~
U) =
n
?
i=1
w
i
? IDF (h) (1)
where h is the argument type (i.e., the hypernym
of the argument value) of each tuple. TF means
the ?term frequency? and IDF means the ?inverse
document frequency?. An IDF function assigns
lower scores to common hypernyms (like person
and music track which contain a lot of entities).
Let DF (h) be the number of entities having hy-
pernym h, we test two IDF functions in our exper-
iments,
IDF
1
(h) = log
1 +N
1 +DF (h)
IDF
2
(h) = 1/sqrt(DF (h))
(2)
where N is total number of entities in the entity
dictionary.
The next problem is estimating tuple score w
i
.
Please note that there is no weight or score infor-
mation in the term?hypernym mapping of Free-
base. So we have to set w
i
to be constant in the
baseline,
w
i
= 1 (3)
802
Wikipedia
holiday of Brazil
holiday of South Africa
?
Brazil ? country
Brazil ? book
South Africa ? country
South Africa ? book
?
holiday of [country], Brazil, w 1
holiday of [book], Brazil, w 2
holiday of [country], South Africa, w 3
holiday of [book], South Africa, w 4
?
holiday of [country], S 1
holiday of [book], S 2
?
Phase-1: Category labeling
Phase-2: Template scoring
Phase-1 Phase-2
Input: Category names
Term-hypernym mapping
Output: Category templates
head argument
argument
argument value
tuple score
Candidate template tuples (CTTs)
Figure 2: Problem definition and baseline approach.
4 Approach: Enhancing Template
Scoring
In our approach, we follow the same framework
as in the above baseline approach, and focus on
improving the template scoring phase (i.e., phase-
2).
We try three techniques: First, a better tuple
scorew
i
is calculated in Section 4.1 by performing
statistics on a large corpus. The corpus is a collec-
tion of 3 billion web pages crawled in early 2013
by ourselves. During this paper, we use ?our web
corpus? or ?our corpus? to refer to this corpus.
Second, a nonlinear function is adopted in Sec-
tion 4.2 to replace the baseline tuple fusion func-
tion (Formula 1). Third, we extract term peer sim-
ilarity and term clusters from our corpus and use
them as additional semantic knowledge to refine
template scores.
4.1 Enhancing tuple scoring
Let?s examine the following two template tuples,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [book], South Africa, w
2
)
Intuitively, ?South Africa? is more likely to be
a country than a book when it appears in text. So
for a reasonable tuple scoring formula, we should
have w
1
> w
2
.
The main idea is to automatically calculate
the popularity of a hypernym given a term, by
referring to a large corpus. Then by adding
the popularity information to (the edges of) the
term?hypernym graph of Freebase, we obtain a
weighted term?hypernym graph. The weighted
graph is then employed to enhance the estimation
of w
i
.
For popularity calculation, we apply Hearst
patterns (Hearst, 1992) and is-a patterns (?NP
(is|are|was|were|being) (a|an|the) NP?) to every
sentence of our web corpus. For a (term, hyper-
nym) pair, its popularity F is calculated as the
number of sentences in which the term and the hy-
pernym co-occur and also follow at least one of
the patterns.
For a template tuple U
i
with argument type h
and argument value v, we test two ways of esti-
mating the tuple score w
i
,
w
i
= log (1 + F (v, h)) (4)
w
i
=
F (v, h))
?+
?
h
j
?H
F (v, h
j
)
(5)
where F (v, h) is the popularity of the (v, h) pair
in our corpus, H is the set of all hypernyms for v
in the weighted term?hypernym graph. Parame-
ter ? (=1.0 in our experiments) is introduced for
smoothing purpose. Note that the second formula
is the conditional probability of hypernym h given
term v.
Since it is intuitive to estimate tuple scores with
their frequencies in a corpus, we treat the approach
with the improved w
i
as another baseline (our
strong baseline).
4.2 Enhancing tuple combination function
Now we study the possibility of improving the tu-
ple combination function (Formula 1), by examin-
ing the tuple fusion problem from the viewpoint
of probabilistic evidence combination. We first
demonstrate that the linear function in Formula 1
corresponds to the conditional independence as-
sumption of the tuples. Then we propose to adopt
a series of nonlinear functions for combining tuple
scores.
We define the following events:
T : Template T is a valid template;
T : T is an invalid template;
E
i
: The observation of tuple U
i
.
803
Let?s compute the posterior odds of event T ,
given two tuples U
1
and U
2
. Assuming E
1
and
E
2
are conditionally independent given T or T ,
according to the Bayes rule, we have,
P (T |E
1
, E
2
)
P (T |E
1
, E
2
)
=
P (E
1
, E
2
|T ) ? P (T )
P (E
1
, E
2
|T ) ? P (T )
=
P (E
1
|T )
P (E
1
|T )
?
P (E
2
|T )
P (E
2
|T )
?
P (T )
P (T )
=
P (T |E
1
) ? P (T )
P (T |E
1
) ? P (T )
?
P (T |E
2
) ? P (T )
P (T |E
2
) ? P (T )
?
P (T )
P (T )
(6)
Define the log-odds-gain of T given E as,
G(T |E) = log
P (T |E)
P (T |E)
? log
P (T )
P (T )
(7)
Here G means the gain of the log-odds of T af-
ter E occurs. By combining formulas 6 and 7, we
get
G(T |E
1
, E
2
) = G(T |E
1
) +G(T |E
2
) (8)
It is easy to prove that the above conclusion
holds true when n > 2, i.e.,
G(T |E
1
, ..., E
n
) =
n
?
i=1
G(T |E
i
) (9)
If we treat G(T |E
i
) as the score of template T
when only U
i
is observed, andG(T |E
1
, ..., E
n
) as
the template score after the n tuples are observed,
then the above equation means that the combined
template score should be the sum of w
i
? IDF (h),
which is exactly Formula 1. Please keep in mind
that Equation 9 is based on the assumption that the
tuples are conditional independent. This assump-
tion, however, may not hold in reality. The case
of conditional dependence was studied in (Zhang
et al., 2011), where a group of nonlinear combina-
tion functions were proposed and achieved good
performance in their task of hypernymy extrac-
tion. We choose p-Norm as our nonlinear fusion
functions, as below,
F (
~
U) =
p
?
?
?
?
n
?
i=1
w
p
i
? IDF (h) (p > 1) (10)
where p (=2 in experiments) is a parameter.
Experiments show that the above nonlinear
function performs better than the linear function
of Formula 1. Let?s use an example to show the
intuition. Consider a good template ?city of [coun-
try]? corresponding to CTTs
~
U
A
and a wrong tem-
plate ?city of [book]? having tuples
~
U
B
. Sup-
pose |
~
U
A
| = 200 (including most countries in
the world) and |
~
U
B
| = 1000 (considering that
many place names have already been used as book
names). We observe that each tuple score corre-
sponding to ?city of [country]? is larger than the
tuple score corresponding to ?city of [book]?. For
simplicity, we assume each tuple in
~
U
A
has score
1.0 and each tuple in
~
U
B
has score 0.2. With the
linear and nonlinear (p=2) fusion functions, we
can get,
Linear:
F (
~
U
A
) = 200 ? 1.0 = 200
F (
~
U
B
) = 1000 ? 0.2 = 200
(11)
Nonlinear:
F (
~
U
A
) = 14.1
F (
~
U
B
) = 6.32
(12)
In the above settings the nonlinear function
yields a much higher score for the good template
(than for the invalid template), while the linear one
does not.
4.3 Refinement with term similarity and
term clusters
The above techniques neglect the similarity among
terms, which has a high potential to improve the
template scoring process. Intuitively, for a toy set
{?city in Brazil?, ?city in South Africa?,?city in
China?, ?city in Japan?}, since ?Brazil?, ?South
Africa?, ?China? and ?Japan? are very similar to
each other and they all have a large probability to
be a ?country?, so we have more confidence that
?city in [country]? is a good template. In this sec-
tion, we propose to leverage the term similarity
information to improve the template scoring pro-
cess.
We start with building a large group of small
and overlapped clusters from our web corpus.
4.3.1 Building term clusters
Term clusters are built in three steps.
Mining term peer similarity: Two terms are
peers if they share a common hypernym and they
are semantically correlated. For example, ?dog?
and ?cat? should have a high peer similarity score.
Following existing work (Hearst, 1992; Kozareva
804
et al., 2008; Shi et al., 2010; Agirre et al., 2009;
Pantel et al., 2009), we built a peer similarity graph
containing about 40.5 million nodes and 1.33 bil-
lion edges.
Clustering: For each term, choose its top-30
neighbors from the peer similarity graph and run a
hierarchical clustering algorithm, resulting in one
or multiple clusters. Then we merge highly du-
plicated clusters. The algorithm is similar to the
first part of CBC (Pantel and Lin, 2002), with the
difference that a very high merging threshold is
adopted here in order to generate small and over-
lapped clusters. Please note that one term may be
included in many clusters.
Assigning top hypernyms: Up to two hyper-
nyms are assigned for each term cluster by major-
ity voting of its member terms, with the aid of the
weighted term?hypernym graph of Section 4.1.
To be an eligible hypernym for the cluster, it has
to be the hypernym of at least 70% of terms in the
cluster. The score of each hypernym is the aver-
age of the term?hypernym weights over all the
member terms.
4.3.2 Template score refinement
With term clusters at hand, now we describe the
score refinement procedure for a template T hav-
ing argument type h and supporting tuples
~
U=(U
1
,
U
2
..., U
n
). Denote V = {V
1
, V
2
, ..., V
n
} to be the
set of argument values for the tuples (where V
i
is
the argument value of U
i
).
By computing the intersection of V and every
term cluster, we can get a distribution of the argu-
ment values in the clusters. We find that for a good
template like ?holiday in [country]?, we can often
find at least one cluster (one of the country clus-
ters in this example) which has hypernym h and
also contains many elements in V . However, for
invalid templates like ?holiday of [book]?, every
cluster having hypernym h (=?book? here) only
contains a few elements in V . Inspired by such
an observation, our score refinement algorithm for
template T is as follows,
Step-1. Calculating supporting scores: For
each term cluster C having hypernym h, compute
its supporting score to T as follows:
S(C, T ) = k(C, V ) ? w(C, h) (13)
where k(C, V ) is the number of elements shared
by C and V , and w(C, h) is hypernym score of h
to C (computed in the last step of building clus-
ters).
Step-2. Calculating the final template score:
Let term cluster C
?
has the maximal supporting
score to T , the final template score is computed
as,
S(T ) = F (
~
U) ? S(C
?
, T ) (14)
where F (
~
U) is the template score before refine-
ment.
5 Experiments
5.1 Experimental setup
5.1.1 Methods for comparison
We make a comparison among 10 methods.
SC: The method is proposed in (Cheung and Li,
2012) to construct templates from queries. The
method firstly represents a query as a matrix based
on Freebase data. Then a hierarchical clustering
algorithm is employed to group queries having the
same structure and meaning. Then an intent sum-
marization algorithm is employed to create tem-
plates for each query group.
Base: The linear function in Formula 1 is
adopted to combine the tuple scores. We use
IDF
2
here because it achieves higher precision
than IDF
1
in this setting.
LW: The linear function in Formula 1 is
adopted to combine the tuple scores generated by
Formula 4. IDF
1
is used rather than IDF
2
for
better performance.
LP: The linear function in Formula 1 is adopted
to combine the tuple scores generated by Formula
5. IDF
2
is used rather than IDF
1
for better per-
formance.
NLW: The nonlinear fusion function in For-
mula 10 is used. Other settings are the same as
LW.
NLP: The nonlinear fusion function in Formula
10 is used. Other settings are the same as LP.
LW+C, LP+C, NLW+C, NLP+C: All the set-
tings of LW, LP, NLW, NLP respectively, with the
refinement technology in Section 4.3 applied.
5.1.2 Data sets, annotation and evaluation
metrics
The input category names for experiments are au-
tomatically extracted from a web corpus (Section
3.1). Two test-sets are built for evaluation from the
output templates of various methods.
Subsets: In order to conveniently compare the
performance of different methods, we create 20
sub-collections (called subsets) from the whole in-
put category collection. Each subset contains all
805
the categories having the same headword (e.g.,
?symptom of insulin deficiency? and ?depression
symptom? are in the same subset because they
share the same headword ?symptom?). To choose
the 20 headwords, we first sample 100 at ran-
dom from the set of all headwords; then manu-
ally choose 20 for diversity. The headwords in-
clude symptom, school, food, gem, hero, weapon,
model, etc. We run the 10 methods on these sub-
sets and sort the output templates by their scores.
Top-30 templates from each method on each sub-
set are selected and mixed together for annotation.
Fullset: We run method NLP+C (which has
the best performance according to our subsets
experiments) on the input categories and sort
the output templates by their scores. Then we
split the templates into 9 sections according
to their ranking position. The sections are:
[1?100], (100?1K], (1K?10K], (10K?100K],
(100K,120K], (120K?140K], (140K?160K],
(160K?180K], (180K?200K]. Then 40 templates
are randomly chosen from each section and mixed
together for annotation.
The selected templates (from subsets and the
fullset) are annotated by six annotators, with each
template assigned to two annotators. A template is
assigned a label of ?good?, ?fair?, or ?bad? by an
annotator. The percentage agreement between the
annotators is 80.2%, with kappa 0.624.
For the subset experiments, we adopt
Precision@k (k=10,20,30) to evaluate the
top templates generated by each method. The
scores for ?good?, ?fair?, and ?bad? are 1, 0.5,
and 0. The score of each template is the average
annotation score over two annotators (e.g., if a
template is annotated ?good? by one annotator and
?fair? by another, its score is (1.0+0.5)/2=0.75).
The evaluation score of a method is the average
over the 20 subsets. For the fullset experiments,
we report the precision for each section.
5.2 Experimental results
5.2.1 Results for subsets
The results of each method on the 20 subsets
are presented in Table 1. A few observations
can be made. First, by comparing the per-
formance of baseline-1 (Base) and the methods
adopting term?hypernym weight (LW and LP),
we can see big performance improvement. The
bad performance of baseline-1 is mainly due to
the lack of weight (or frequency) information on
Method P@10 P@20 P@30
Base (baseline-1) 0.359 0.361 0.358
SC (Cheung and Li, 2012) 0.382 0.366 0.371
Weighted LW 0.633 0.582 0.559
(baseline-2) LP 0.771 0.734 0.707
Nonlinear NLW 0.711 0.671 0.638
NLP 0.818 0.791 0.765
LW+C 0.813 0.786 0.754
Term cluster NLW+C 0.854 0.833 0.808
LP+C 0.818 0.788 0.778
NLP+C 0.868 0.839 0.788
Table 1: Performance comparison among the
methods on subset.
term?hypernym edges. The results demonstrate
that edge scores are critical for generating high
quality templates. Manually built semantic re-
sources typically lack such kinds of scores. There-
fore, it is very important to enhance them by de-
riving statistical data from a large corpus. Since
it is relatively easy to have the idea of adopt-
ing a weighted term?hypernym graph, we treat
LW and LP as another (stronger) baseline named
baseline-2.
As the second observation, the results show that
the nonlinear methods (NLP and NLW) achieve
performance improvement over their linear ver-
sions (LW and LP).
Third, let?s examine the methods with template
scores refined by term similarity and term clus-
ters (LW+C, NLW+C, LP+C, NLP+C). It is shown
that the refine-by-cluster technology brings addi-
tional performance gains on all the four settings
(linear and nonlinear, two different ways of calcu-
lating tuple scores). So we can conclude that the
peer similarity and term clusters are quite effective
in improving template generation.
Fourth, the best performance is achieved
when the three techniques (i.e., term?hypernym
weight, nonlinear fusion function, and refine-by-
cluster) are combined together. For instance, by
comparing the P@20 scores of baseline-2 and
NLP+C, we see a performance improvement of
14.3% (from 0.734 to 0.839). Therefore every
technique studied in this paper has its own merit
in template generation.
Finally, by comparing the method SC (Cheung
and Li, 2012) with other methods, we can see that
SC is slightly better than baseline-1, but has much
lower performance than others. The major reason
may be that this method did not employ a weighted
term?hypernym graph or term peer similarity in-
formation in template construction.
806
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@10 NLP > ?? > ?? >
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?? >
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@20 NLP > ?? > ?? > ??
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?? > ??
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@30 NLP > ?? > ?? > ??
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?
Table 2: Paired t-test results on subsets.
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@10 NLW > ?? > ?? > ?
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ?
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@20 NLW > ?? > ?? > ??
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ??
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@30 NLW > ?? > ?? > ??
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ??
Table 3: Paired t-test results on subsets.
Are the performance differences between meth-
ods significant enough for us to say that one is bet-
ter than the other? To answer this question, we run
paired two-tailed t-test on every pair of methods.
We report the t-test values among methods in ta-
bles 2, 3 and 4.
The meaning of the symbols in the tables are,
?: The method on the row and the one on the
column have similar performance.
>: The method on the row outperforms the
method on the column, but the performance dif-
ference is not statistically significant (0.05 ? P <
0.1 in two-tailed t-test).
> ?: The performance difference is statistically
significant (P < 0.05 in two-tailed t-test).
> ??: The performance difference is statisti-
cally highly significant (P < 0.01 in two-tailed
t-test).
P@10 P@20 P@30
LP V.S. LW > ?? > ?? > ??
NLP V.S. NLW > ?? > ?? > ??
LP+C V.S. LW+C ? ? ?
NLP+C V.S. NLW+C ? ? ?
Table 4: Paired t-test results on subsets.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9
Pr
ec
isi
on
Section ID
Figure 3: Precision by section in the fullset.
5.2.2 Fullset results
As described in the Section 5.1.2, for the fullset
experiments, we conduct a section-wise evalua-
tion, selecting 40 templates from each of the 9 sec-
tions of the NLP+C results. The results are shown
in Figure 3. It can be observed that the precision
for each section decreases when the section ID in-
creases. The results indicate the effectiveness of
our approach, since it can rank good templates in
top sections and bad templates in bottom sections.
According to the section-wise precision data, we
are able to determine the template score threshold
for choosing different numbers of top templates in
different applications.
5.2.3 Templates for category collection
cleaning
Since our input category collection is automati-
cally constructed from the web, some wrong or
invalid category names is inevitably contained. In
this subsection, we apply our category templates
to clean the category collection. The basic idea is
that if a category can match a template, it is more
likely to be correct. We compute a new score for
every category name H as follows,
S
new
(H) = log(1 + S(H)) ? S(T
?
) (15)
where S(H) is the existing category score, deter-
mined by its frequency in the corpus. Here S(T
?
)
is the score of template T
?
, the best template (i.e.,
the template with the highest score) for the cate-
gory.
Then we re-rank the categories according to
their new scores to get a re-ranked category list.
We randomly sampled 150 category names from
the top 2 million categories of each list (the old list
and the new list) and asked annotators to judge the
807
quality of the categories. The annotation results
show that, after re-ranking, the precision increases
from 0.81 to 0.89 (i.e., the percent of invalid cate-
gory names decreases from 19% to 11%).
6 Conclusion
In this paper, we studied the problem of build-
ing templates for a large collection of category
names. We tested three techniques (tuple scor-
ing by weighted term?hypernym mapping, non-
linear score fusion, refinement by term clusters)
and found that all of them are very effective and
their combination achieves the best performance.
By employing the output templates to clean our
category collection mined from the web, we get
apparent quality improvement. Future work in-
cludes supporting multi-argument templates, dis-
ambiguating headwords of category names and ap-
plying our approach to general short text template
mining.
Acknowledgments
We would like to thank the annotators for their ef-
forts in annotating the templates. Thanks to the
anonymous reviewers for their helpful comments
and suggestions. This work is supported in part by
China National 973 program 2014CB340301 and
NSFC grant 61379043.
References
Ganesh Agarwal, Govind Kabra, and Kevin Chen-
Chuan Chang. 2010. Towards rich query interpreta-
tion: walking back and forth for mining query tem-
plates. In Proceedings of the 19th international con-
ference on World wide web, pages 1?10. ACM.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 976?986. Association for Computational Lin-
guistics.
Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence
clustering and labeling for unsupervised query intent
discovery. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
pages 383?392. ACM.
Jesualdo Tomas Fernandez-Breis, Luigi Iannone, Ig-
nazio Palmisano, Alan L Rector, and Robert
Stevens. 2010. Enriching the gene ontology via the
dissection of labels using the ontology pre-processor
language. In Knowledge Engineering and Manage-
ment by the Masses, pages 59?73. Springer.
Jun Han, Ju Fan, and Lizhu Zhou. 2013.
Crowdsourcing-assisted query structure interpreta-
tion. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2092?2098. AAAI Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics -
Volume 2, COLING ?92, pages 539?545, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048?1056.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 572?579. ACM.
Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai.
2013. Unsupervised identification of synonymous
query intent templates for attribute intents. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information & knowledge man-
agement, pages 2029?2038. ACM.
Xiao Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1337?1345. Association
for Computational Linguistics.
Sandeep Pandey and Kunal Punera. 2012. Unsuper-
vised extraction of template structure in web search
queries. In Proceedings of the 21st international
conference on World Wide Web, pages 409?418.
ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619.
ACM.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In HLT-
NAACL, volume 4, pages 321?328.
808
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 938?947. Association for Com-
putational Linguistics.
Manuel Quesada-Mart?nez, Jesualdo Tom?as
Fern?andez-Breis, and Robert Stevens. 2012.
Enrichment of owl ontologies: a method for defin-
ing axioms from labels. In Proceedings of the First
International Workshop on Capturing and Refining
Knowledge in the Medical Domain (K-MED 2012),
Galway, Ireland, pages 1?10.
Joseph Reisinger and Marius Pasca. 2011. Fine-
grained class label markup of search queries. In
ACL, pages 1200?1209.
Nikos Sarkas, Stelios Paparizos, and Panayiotis
Tsaparas. 2010. Structured annotations of web
queries. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
pages 771?782. ACM.
Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: distributional vs. pattern-based approaches.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 993?1001. As-
sociation for Computational Linguistics.
Mark Stevenson and Mark A Greenwood. 2005. A
semantic approach to ie pattern induction. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 379?386. As-
sociation for Computational Linguistics.
Idan Szpektor, Aristides Gionis, and Yoelle Maarek.
2011. Improving recommendation for long-tail
queries via templates. In Proceedings of the 20th
international conference on World wide web, pages
47?56. ACM.
Allan Third. 2012. Hidden semantics: what can we
learn from the names in an ontology? In Proceed-
ings of the Seventh International Natural Language
Generation Conference, pages 67?75. Association
for Computational Linguistics.
Benjamin Van Durme and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In AAAI, volume 8, pages
1243?1248.
Roman Yangarber. 2003. Counter-training in discov-
ery of semantic patterns. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 343?350. Association
for Computational Linguistics.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and
Chin-Yew Lin. 2011. Nonlinear evidence fusion
and propagation for hyponymy relation mining. In
ACL, volume 11, pages 1159?1168.
809
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1986?1996,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Self-disclosure topic model for classifying and analyzing Twitter
conversations
JinYeong Bak
?
Department of Computer Science
KAIST
Daejeon, South Korea
jy.bak@kaist.ac.kr
Chin-Yew Lin
Microsoft Research
Beijing 100080, P.R. China
cyl@microsoft.com
Alice Oh
Department of Computer Science
KAIST
Daejeon, South Korea
alice.oh@kaist.edu
Abstract
Self-disclosure, the act of revealing one-
self to others, is an important social be-
havior that strengthens interpersonal rela-
tionships and increases social support. Al-
though there are many social science stud-
ies of self-disclosure, they are based on
manual coding of small datasets and ques-
tionnaires. We conduct a computational
analysis of self-disclosure with a large
dataset of naturally-occurring conversa-
tions, a semi-supervised machine learning
algorithm, and a computational analysis
of the effects of self-disclosure on subse-
quent conversations. We use a longitu-
dinal dataset of 17 million tweets, all of
which occurred in conversations that con-
sist of five or more tweets directly reply-
ing to the previous tweet, and from dyads
with twenty of more conversations each.
We develop self-disclosure topic model
(SDTM), a variant of latent Dirichlet al-
location (LDA) for automatically classi-
fying the level of self-disclosure for each
tweet. We take the results of SDTM and
analyze the effects of self-disclosure on
subsequent conversations. Our model sig-
nificantly outperforms several comparable
methods on classifying the level of self-
disclosure, and the analysis of the longitu-
dinal data using SDTM uncovers signifi-
cant and positive correlation between self-
disclosure and conversation frequency and
length.
1 Introduction
Self-disclosure is an important and pervasive so-
cial behavior. People disclose personal informa-
tion about themselves to improve and maintain
?
This work was done when JinYeong Bak was a visiting
student at Microsoft Research, Beijing, China.
relationships (Jourard, 1971; Joinson and Paine,
2007). A common instance of self-disclosure is
the start of a conversation with an exchange of
names and additional self-introductions. Another
example of self-disclosure, shown in Figure 1c,
where the information disclosed about a family
member?s serious illness, is much more personal
than the exchange of names. In this paper, we seek
to understand this important social behavior using
a large-scale Twitter conversation data, automati-
cally classifying the level of self-disclosure using
machine learning and correlating the patterns with
conversational behaviors which can serve as prox-
ies for measuring intimacy between two conversa-
tional partners.
Twitter conversation data, explained in more
detail in section 4.1, enable an extremely large
scale study of naturally-occurring self-disclosure
behavior, compared to traditional social science
studies. One challenge of such large scale study,
though, remains in the lack of labeled ground-
truth data of self-disclosure level. That is,
naturally-occurring Twitter conversations do not
come tagged with the level of self-disclosure in
each conversation. To overcome that challenge,
we propose a semi-supervised machine learning
approach using probabilistic topic modeling. Our
self-disclosure topic model (SDTM) assumes that
self-disclosure behavior can be modeled using a
combination of simple linguistic features (e.g.,
pronouns) with automatically discovered seman-
tic themes (i.e., topics). For instance, an utterance
?I am finally through with this disastrous relation-
ship? uses a first-person pronoun and contains a
topic about personal relationships.
In comparison with various other models,
SDTM shows the highest accuracy, and the result-
ing conversation frequency and length patterns on
self-disclosure are shown different over time. Our
contributions to the research community include
the following:
1986
? We present key features and prior knowl-
edge for identifying self-disclosure level, and
show relevance of it with experiment results
(Sec. 2).
? We present a topic model that explicitly in-
cludes the level of self-disclosure in a conver-
sation using linguistic features and the latent
semantic topics (Sec. 3).
? We collect a large dataset of Twitter conver-
sations over three years and annotate a small
subset with self-disclosure level (Sec. 4).
? We compare the classification accuracy of
SDTM with other models and show that it
performs the best (Sec. 5).
? We correlate the self-disclosure patterns and
conversation behaviors to show that there is
significant relationship over time (Sec. 6).
2 Self-Disclosure
In this section, we look at social science literature
for definition of the levels of self-disclosure. Us-
ing that definition, we devise an approach to au-
tomatically identify the levels of self-disclosure
in a large corpus of OSN conversations. We dis-
cuss three approaches, first, using first-person pro-
noun features, and second, extracting seed words
and phrases from the Twitter conversation cor-
pus, and third, extracting seed words and phrases
from an external corpus of anonymously posted
secrets, and we demonstrate the efficacy of those
approaches with an annotated corpus.
2.1 Self-disclosure (SD) level
To analyze self-disclosure, researchers categorize
self-disclosure language into three levels: G (gen-
eral) for no disclosure, M for medium disclosure,
and H for high disclosure (Vondracek and Von-
dracek, 1971; Barak and Gluck-Ofri, 2007). Ut-
terances that contain general (non-sensitive) in-
formation about the self or someone close (e.g.,
a family member) are categorized as M. Exam-
ples are personal events, past history, or future
plans. Utterances about age, occupation and hob-
bies are also included. Utterances that contain
sensitive information about the self or someone
close are categorized as H. Sensitive information
includes personal characteristics, problematic be-
haviors, physical appearance and wishful ideas.
Generally, these are thoughts and information that
(a) A G level Twitter conversation
(b) A M level Twitter conversation
(c) A H level Twitter conversation
Figure 1: An example of a Twitter conversation
(from annotated dataset) with G, M and H level of
self-disclosure.
one would keep as secrets to himself. All other
utterances, those that do not contain information
about the self or someone close are categorized
as G. Examples include gossip about celebrities
or factual discourse about current events. Figure
1 shows Twitter conversation examples with G,
M and H levels from annotated dataset (see Sec-
tion 4.2 for a detailed description of the annotated
dataset).
2.2 G Level of Self-Disclosure
An obvious clue of self-disclosure is the use of
first-person pronouns. For example, phrases such
as ?I live? or ?My name is? indicate that the ut-
terance contains personal information. In pre-
vious research, the simple method of counting
first-person pronouns was used to measure the de-
gree of self-disclosure (Joinson, 2001; Barak and
Gluck-Ofri, 2007). Consequently, the absence of a
first-person pronoun signals that the utterance be-
longs in the G level of self-disclosure. We ver-
ify this pattern with a dataset of Tweets annotated
with G, M, and H levels. We divide the annotated
Tweets into two classes, G and M/H. Then we com-
pute mutual information of each unigram, bigram,
or trigram feature to see which features are most
discriminative. As Table 1 shows, 18 out of 30
1987
Category Words/Expressions
Unigram my, I, I?m, I?ll, but, was, I?ve, love, dad, have
Bigram I love, I was, I have, my dad, go to, my mom,
with my, have to, to go, my mum
Trigram I have a, is going to, to go to, want to go, and I
was, going to miss, I love him, I think I, I was
like, I wish I
Table 1: High ranked words and expressions by
mutual information between G and M/H level in
annotated conversations.
most highly ranked discriminative features contain
a first-person pronoun.
2.3 M Level of Self-Disclosure
Utterances with M level include two types: 1)
information related with past events and future
plans, and 2) general information about self
(Barak and Gluck-Ofri, 2007). For the former, we
add as seed trigrams ?I have been? and ?I will?.
For the latter, we use seven types of information
generally accepted to be personally identifiable in-
formation (McCallister, 2010), as listed in the left
column of Table 2. To find the appropriate tri-
grams for those, we take Twitter conversation data
(described in Section 4.1) and look for trigrams
that begin with ?I? and ?my? and occur more than
200 times. We then check each one to see whether
it is related with any of the seven types listed in
the table. As a result, we find 57 seed trigrams for
M level. Table 2 shows several examples.
Type Trigram
Name My name is, My last name
Birthday My birthday is, My birthday party
Location I live in, I lived in, I live on
Contact My email address, My phone number
Occupation My job is, My new job
Education My high school, My college is
Family My dad is, My mom is, My family is
Table 2: Example seed trigrams for identifying M
level of SD. There are 51 of these used in SDTM.
2.4 H Level of Self-Disclosure
Utterances with H level express secretive wishes
or sensitive information that exposes self or some-
one close (Barak and Gluck-Ofri, 2007). These
are generally kept as secrets. With this intuition,
we crawled 26,523 posts from Six Billion Secrets
1
site where users post secrets anonymously
2
. We
1
http://www.sixbillionsecrets.com
2
This site is regularly monitored for spam.
Category Words - SECRET Words - Annotated
physical
appear-
ance
acne, hair, overweight,
stomach, chest, hand,
scar, thighs, chubby
ankle, face, toe,
skin
mental/
physical
condition
addicted, bulimia, doc-
tor, illness, alcoholic,
disease, drugs, pills
ache, epilepsy,
pain, chiropractor,
codeine
Table 3: Example words for identifying H level of
SD from secret posts (2nd column) and annotated
data (3rd column). Categories are hand-labeled.
call this external dataset SECRET. Unlike G and M
levels, evidence of H level of self-disclosure tends
to be topical, such as physical appearance, mental
and physical illnesses, and family problems, so we
take an approach of fitting a topic model driven by
seed words. A similar approach has been success-
ful in sentiment classification (Jo and Oh, 2011;
Kim et al., 2013).
A critical component of this approach is the set
of seed words with which to drive the discovery
of topics that are most indicative of H level self-
disclosure. To extract the seed words that express
secretive personal information, we compute mu-
tual information (Manning et al., 2008) with SE-
CRET and 24,610 randomly selected tweets. We
select 1,000 words with high mutual information
and filter out stop words. Table 3 shows some of
these words. To extract seed trigrams of secretive
wishes, we again look for trigrams that start with
?I? or ?my?, occur more than 200 times, and select
trigrams of wishful thinking, such as ?I want to?,
and ?I wish I?. In total, there are 88 seed words
and 8 seed trigrams for H.
Since SECRET is quite different from Twitter,
we must show that posts in SECRET are seman-
tically similar to the H level Tweets. Rather than
directly comparing SECRET posts and Tweets, we
use the same method of extracting discriminative
word features from the annotated H level Tweets
(see Section 4.2). Table 3 shows the seed words
extracted from SECRET as well as the annotated
Tweets. Because the annotated dataset consists of
only 200 conversations, the coverage of the topics
seems narrower than the much larger SECRETS,
but both datasets show similarities in the topics.
This, combined with the results of the model with
the two sets of seed words (see Section 5 for the
results), shows that SECRETS is an effective and
simple-to-obtain substitute for an annotated cor-
pus of H level of self-disclosure.
1988
??
??
??
CTN
??
?
?
?? 3?????? 3
Figure 2: Graphical model of SDTM
Notation Description
G; M ; H {general; medium; high} SD level
C; T ; N Number of conversations; tweets;
words
K
G
;K
M
;K
H
Number of topics for {G; M; H}
c; ct Conversation; tweet in conversation c
y
ct
SD level of tweet ct, G or M/H
r
ct
SD level of tweet ct, M or H
z
ct
Topic of tweet ct
w
ctn
n
th
word in tweet ct
? Learned Maximum entropy parameters
x
ct
First-person pronouns features
?
ct
Distribution over SD level of tweet ct
pi
c
SD level proportion of conversation c
?G
c
;?M
c
;?H
c
Topic proportion of {G; M; H} in con-
versation c
?G;?M ;?H Word distribution of {G; M; H}
?; ? Dirichlet prior for ?; pi
?G,?M ;?H Dirichlet prior for ?G;?M ;?H
n
cl
Number of tweets assigned SD level l
in conversation c
n
l
ck
Number of tweets assigned SD level l
and topic k in conversation c
n
l
kv
Number of instances of word v assigned
SD level l and topic k
m
ctkv
Number of instances of word v assigned
topic k in tweet ct
Table 4: Summary of notations used in SDTM
3 Self-Disclosure Topic Model
This section describes our model, the self-
disclosure topic model (SDTM), for classifying
self-disclosure level and discovering topics for
each self-disclosure level.
3.1 Model
In section 2, we discussed different approaches
to identifying each level of self-disclosure, based
on social science literature, annotated and unan-
notated Tweets, and an external corpus of se-
cret posts. In this section, we describe our
self-disclosure topic model, based on the widely
used latent Dirichlet allocation (Blei et al., 2003),
which incorporates those approaches.
Figure 2 illustrates the graphical model of
1. For each level l ? {G, M, H}:
For each topic k ? {1, . . . ,Kl}:
Draw ?lk ? Dir(?l)
2. For each conversation c ? {1, . . . , C}:
(a) Draw ?Gc ? Dir(?)
(b) Draw ?Mc ? Dir(?)
(c) Draw ?Hc ? Dir(?)
(d) Draw pic ? Dir(?)
(e) For each message t ? {1, . . . , T}:
i. Observe first-person pronouns features xct
ii. Draw ?ct ?MaxEnt(xct,?)
iii. Draw yct ? Bernoulli(?ct)
iv. If yct = 0 which is G level:
A. Draw zct ?Mult(?Gc )
B. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?Gzct)
Else which can be M or H level:
A. Draw rct ?Mult(pic)
B. Draw zct ?Mult(?rctc )
C. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?rctzct)
Figure 3: Generative process of SDTM.
SDTM and how those approaches are embodied
in it. The first approach based on the first-person
pronouns is implemented by the observed vari-
able x
ct
and the parameters ? from a maximum
entropy classifier for G vs. M/H level. The ap-
proach of seed words and phrases for levels M and
H is implemented by the three separate word-topic
probability vectors for the three levels of SD: ?
l
which has a Bayesian informative prior ?
l
where
l ? {G,M,H}, the three levels of self-disclosure.
Table 4 lists the notations used in the model and
the generative process, and Figure 3 describes the
generative process.
3.2 Classifying G vs M/H levels
Classifying the SD level for each tweet is done in
two parts, and the first part classifies G vs. M/H
levels with first-person pronouns (I, my, me). In
the graphical model, y is the latent variable that
represents this classification, and ? is the distri-
bution over y. x is the observation of the first-
person pronoun in the tweets, and? are the param-
eters learned from the maximum entropy classifier.
With the annotated Twitter conversation dataset
(described in Section 4.2), we experimented with
several classifiers (Decision tree, Naive Bayes)
and chose the maximum entropy classifier because
it performed the best, similar to other joint topic
models (Zhao et al., 2010; Mukherjee et al., 2013).
1989
3.3 Classifying M vs H levels
The second part of the classification, the M and the
H level, is driven by informative priors with seed
words and seed trigrams. In the graphical model,
r is the latent variable that represents this classi-
fication, and pi is the distribution over r. ? is a
non-informative prior for pi, and ?
l
is an informa-
tive prior for each SD level by seed words. For
example, we assign a high value for the seed word
?acne? for ?
H
, and a low value for ?My name is?.
This approach is the same as joint models of topic
and sentiment (Jo and Oh, 2011; Kim et al., 2013).
3.4 Inference
For posterior inference of SDTM, we use col-
lapsed Gibbs sampling which integrates out la-
tent random variables ?,pi,?, and ?. Then we
only need to compute y, r and z for each tweet.
We compute full conditional distribution p(y
ct
=
j
?
, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x) for
tweet ct as follows:
p(y
ct
= 0, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
0
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
g(c, t, l
?
, k
?
),
p(y
ct
= 1, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
1
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
(?
l
?
+ n
(?ct)
cl
?
) g(c, t, l
?
, k
?
),
where z
?ct
, r
?ct
,y
?ct
are z, r,y without tweet
ct, m
ctk
?
(?)
is the marginalized sum over word v of
m
ctk
?
v
and the function g(c, t, l
?
, k
?
) as follows:
g(c, t, l
?
, k
?
) =
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
(?)
)
(
?
k
?
+ n
l
?
(?ct)
ck
?
?
K
k=1
?
k
+ n
l
?
ck
)
V
?
v=1
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
v
)
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
.
4 Data Collection and Annotation
To test our self-disclosure topic model, we use a
large dataset of conversations consisting of Tweets
over three years such that we can analyze the re-
lationship between self-disclosure behavior and
conversation frequency and length over time. We
chose to crawl Twitter because it offers a prac-
tical and large source of conversations (Ritter et
al., 2010). Others have also analyzed Twitter con-
versations for natural language and social media
Users Dyads Conv?s Tweets
101,686 61,451 1,956,993 17,178,638
Table 5: Dataset of Twitter conversations. We
chose conversations consisting of five or more
tweets each. We chose dyads with twenty or more
conversations.
research (boyd et al., 2010; Danescu-Niculescu-
Mizil et al., 2011), but we collect conversations
from the same set of dyads over several months for
a unique longitudinal dataset. We also make sure
that each conversation is at least five tweets, and
that each dyad has at least twenty conversations.
4.1 Collecting Twitter conversations
We define a Twitter conversation as a chain of
tweets where two users are consecutively reply-
ing to each other?s tweets using the Twitter reply
button. We initialize the set of users by randomly
sampling thirteen users who reply to other users
in English from the Twitter public streams
3
. Then
we crawl each user?s public tweets, and look at
users who are mentioned in those tweets. It is
a breadth-first search in the network defined by
users as nodes and edges as conversations. We
run this search for dyads until the depth of four,
and filter out users who tweet in a non-English
language. We use an open source tool for de-
tecting English tweets
4
. To protect users? privacy,
we replace Twitter userid, usernames and url in
tweets with random strings. This dataset consists
of 101,686 users, 61,451 dyads, 1,956,993 conver-
sations and 17,178,638 tweets which were posted
between August 2007 to July 2013. Table 5 sum-
marizes the dataset.
4.2 Annotating self-disclosure level
To measure the accuracy of our model, we ran-
domly sample 301 conversations, each with ten or
fewer tweets, and ask three judges, fluent in En-
glish and graduate students/researchers, to anno-
tate each tweet with the level of self-disclosure.
Judges first read and discussed the definitions and
examples of self-disclosure level shown in (Barak
and Gluck-Ofri, 2007), then they worked sepa-
rately on a Web-based platform.
As a result of annotation, there are 122 G level
converstaions, 147 M level and 32 H level con-
3
https://dev.twitter.com/docs/api/
streaming
4
https://github.com/shuyo/ldig
1990
Figure 4: Screenshot of annotation web-based
platform. Annotators read a Twitter conversation
and annotate self-disclosure level to each tweet.
versations, and inter-rater agreement using Fleiss
kappa (Fleiss, 1971) is 0.68, which is substantial
agreement result (Landis and Koch, 1977).
5 Classification of Self-Disclosure Level
This section describes experiments and results of
SDTM as well as several other methods for classi-
fication of self-disclosure level.
We first start with the annotated dataset in sec-
tion 4.2 in which each tweet is annotated with SD
level. We then aggregate all of the tweets of a
conversation, and we compute the proportions of
tweets in each SD level. When the proportion of
tweets at M or H level is equal to or greater than
0.2, we take the level of the larger proportion and
assign that level to the conversation. When the
proportions of tweets at M or H level are both less
than 0.2, we assign G to the SD level. The reason
for setting 0.2 as the threshold is that a conversa-
tion containing tweets with H or M level of self-
disclosure usually starts with a greeting or a gen-
eral comment, and contains one or more questions
or comments before or after the self-disclosure
tweet.
We compare SDTM with the following methods
for classifying conversations for SD level:
? LDA (Blei et al., 2003): A Bayesian topic
model. Each conversation is treated as a doc-
ument. Used in previous work (Bak et al.,
2012).
? MedLDA (Zhu et al., 2012): A super-
vised topic model for document classifica-
tion. Each conversation is treated as a doc-
ument and response variable can be mapped
to a SD level.
? LIWC (Tausczik and Pennebaker, 2010):
Word counts of particular categories
5
. Used
in previous work (Houghton and Joinson,
2012).
? Bag of Words + Bigrams + Trigrams
(BOW+): A bag of words, bigram and tri-
gram features. We exclude features that ap-
pear only once or twice.
? Seed words and trigrams (SEED): Occur-
rences of seed words/trigrams from SECRET
which are described in section 3.3.
? SDTM with seed words from annotated
Tweets (SDTM?): To compare with SDTM
below using seed words from SECRET, this
uses seed words from the annotated data de-
scribed in section 2.4.
? ASUM (Jo and Oh, 2011): A joint model
of sentiments and topics. We map each SD
level to one sentiment and use the same seed
words/trigrams from SECRET as in SDTM
below. Used in previous work (Bak et al.,
2012).
? First-person pronouns (FirstP): Occurrence
of first-person pronouns which are described
in section 3.2. To identify first-person pro-
nouns, we tagged parts of speech in each
tweet with the Twitter POS tagger (Owoputi
et al., 2013).
? First-person pronouns + Seed words/trigrams
(FP+SE1): First-person pronouns and seed
words/trigrams from SECRET.
? Two stage classifier with First-person pro-
nouns + Seed words/trigrams (FP+SE2): A
5
personal pronouns, 3rd person singular words, family
words, human words, sexual words, etc
1991
Method Acc G F
1
M F
1
H F
1
Avg F
1
LDA 49.2 0.00 0.65 0.05 0.23
MedLDA 43.3 0.41 0.52 0.09 0.34
LIWC 49.2 0.34 0.61 0.18 0.38
BOW+ 54.1 0.50 0.59 0.15 0.41
SEED 54.4 0.52 0.60 0.14 0.42
ASUM 56.6 0.32 0.70 0.38 0.47
SDTM? 60.4 0.57 0.70 0.14 0.47
FirstP 63.2 0.63 0.69 0.10 0.47
FP+SE1 61.0 0.61 0.67 0.16 0.48
FP+SE2 60.4 0.64 0.69 0.17 0.50
SDTM 64.5 0.61 0.71 0.43 0.58
Table 6: SD level classification accuracies and F-
measures using annotated data. Acc is accuracy,
and G F
1
is F-measure for classifying the G level.
Avg F
1
is the macroaveraged value of G F
1
, M F
1
and H F
1
. SDTM outperforms all other methods
compared. The difference between SDTM and
FirstP is statistically significant (p-value < 0.05
for accuracy, < 0.0001 for Avg F
1
).
two stage classifier with first-person pro-
nouns and seed words/trigrams from SE-
CRET. In the first stage, the classifier identi-
fies G with first-person pronouns. Then in the
second stage, the classifier uses seed words
and trigrams to identify M and H levels.
? SDTM: Our model with first-person pro-
nouns and seed words/trigrams from SE-
CRET.
SEED, LIWC, LDA and FirstP cannot be used
directly for classification, so we use Maximum en-
tropy model with outputs of each of those models
as features
6
. BOW+ uses SVM with a radial ba-
sis kernel which performs better than all other set-
tings tried including maximum entropy. We split
the data randomly into 80/20 for train/test. We run
MedLDA, ASUM and SDTM 20 times each and
compute the average accuracies and F-measure for
each level. We run LDA and MedLDA with var-
ious number of topics from 80 to 140, and 120
topics shows best outputs. So we set 120 topics
for LDA, MedLDA and ASUM, 60; 40; 40 topics
for SDTM K
G
,K
M
and K
H
respectively which
is best perform from 40; 40; 40 to 60; 60; 60 top-
ics. We assume that a conversation has few topics
6
It performs better than other classifiers (C4.5, Naive-
Bayes, SVM with linear kernel, polynomial kernel and radial
basis)
and self-disclosure levels, so we set ? = ? = 0.1
(Tang et al., 2014). To incorporate the seed words
and trigrams into ASUM and SDTM, we initial-
ize ?
G
,?
M
and ?
H
differently. We assign a high
value of 2.0 for each seed word and trigram for
that level, and a low value of 10
?6
for each word
that is a seed word for another level, and a default
value of 0.01 for all other words. This approach
is the same as previous papers (Jo and Oh, 2011;
Kim et al., 2013).
As Table 6 shows, SDTM performs better than
the other methods for accuracy as well as F-
measure. LDA and MedLDA generally show
the lowest performance, which is not surprising
given these models are quite general and not tuned
specifically for this type of semi-supervised clas-
sification task. BOW which is simple word fea-
tures also does not perform well, showing espe-
cially low F-measure for the H level. LIWC and
SEED perform better than LDA, but these have
quite low F-measure for G and H levels. ASUM
shows better performance for classifying H level
than others, confirming the effectiveness of a topic
modeling approach to this difficult task, but not as
well as SDTM. FirstP shows good F-measure for
the G level, but the H level F-measure is quite low,
even lower than SEED. Combining first-person
pronouns and seed words and trigrams (FP+SE1)
shows better than each feature alone, and the two
stage classifier (FP+SE2) which is a similar ap-
proach taken in SDTM shows better results. Fi-
nally, SDTM classifies G and M level at a similar
accuracy with FirstP, FP+SE1 and FP+SE2, but
it significantly improves accuracy for the H level
compared to all other methods.
6 Relations of Self-Disclosure and
Conversation Behaviors
In this section, we investigate whether there is
a relationship between self-disclosure and con-
versation behaviors over time. Self-disclosure is
one way to maintain and improve relationships
(Jourard, 1971; Joinson and Paine, 2007). So
two people?s intimacy changes over time has rela-
tionship with self-disclosure in their conversation.
However, it is hard to identify intimacy between
users in large scale online social network. So we
choose conversation behaviors such as conversa-
tion frequency and length which can be treated as
proxies for measuring intimacy between two peo-
ple (Emmers-Sommer, 2004; Bak et al., 2012).
1992
With SDTM, we can automatically classify the
SD level of a large number of conversations, so
we investigate whether there is a similar relation-
ship between self-disclosure in conversations and
subsequent conversation behaviors with the same
partner on Twitter.
For comparing conversation behaviors over
time, we divided the conversations into two sets
for each dyad. For the initial period, we include
conversations from the dyad?s first conversation to
20 days later. And for the subsequent period,
we include conversations during the subsequent 10
days. We compute proportions of conversation for
each SD level for each dyad in the initial and
subsequent periods.
More specifically, we ask the following three
questions:
1. If a dyad shows high conversation frequency
at a particular time period, would they dis-
play higher SD in their subsequent conver-
sations?
2. If a dyad displays high SD level in their con-
versations at a particular time period, would
their subsequent conversations be longer?
3. If a dyad displays high overall SD level,
would their conversations increase in length
over time more than dyads with lower overall
SD level?
6.1 Experiment Setup
We first run SDTM with all of our Twitter con-
versation data with 150; 120; 120 topics for
SDTM K
G
,K
M
and K
H
respectively. The
hyper-parameters are the same as in section 5. To
handle a large dataset, we employ a distributed al-
gorithm (Newman et al., 2009), and run with 28
threads.
Table 7 shows some of the topics that were
prominent in each SD level by KL-divergence. As
expected, G level includes general topics such as
food, celebrity, soccer and IT devices, M level in-
cludes personal communication and birthday, and
finally, H level includes sickness and profanity.
We define a new measurement, SD level score
for a dyad in the period, which is a weighted sum
of each conversation with SD levels mapped to 1,
2, and 3, for the levels G, M, and H, respectively.
0 5 10 15 20 25 30 35Initial conversation frequency
2.00
2.02
2.04
2.06
2.08
2.10
2.12
2.14
Sub
seq
uen
t SD
 lev
el
Figure 5: Relationship between initial conversa-
tion frequency and subsequent SD level. The
solid line is the linear regression line, and the co-
efficient is 0.0020 with p < 0.0001, which shows
a significant positive relationship.
6.2 Does high frequency of conversation lead
to more self-disclosure?
We investigate whether the initial conversation
frequency is correlated with the SD level in the
subsequent period. We run linear regression with
the initial conversation frequency as the indepen-
dent variable, and SD level in the subsequent pe-
riod as the dependent variable.
The regression coefficient is 0.0020 with low p-
value (p < 0.0001). Figure 5 shows the scatter
plot. We can see that the slope of the regression
line is positive.
6.3 Does high self-disclosure lead to longer
conversations?
Now we investigate the effect of the self-
disclosure level to conversation length. We run
linear regression with the intial SD level score as
the independent variable, and the rate of change
in conversation length between initial period
and subsequent period as the dependent variable.
Conversation length is measured by the number of
tweets in a conversation.
The result of regression is that the independent
variable?s coefficient is 0.048 with a low p-value
(p < 0.0001). Figure 6 shows the scatter plot with
the regression line, and we can see that the slope
of regression line is positive.
1993
G level M level H level
101 184 176 36 104 82 113 33 19
chocolate obama league send twitter going ass better lips
butter he?s win email follow party bitch sick kisses
good romney game i?ll tumblr weekend fuck feel love
cake vote season sent tweet day yo throat smiles
peanut right team dm following night shit cold softly
milk president cup address account dinner fucking hope hand
sugar people city know fb birthday lmao pain eyes
cream good arsenal check followers tomorrow shut good neck
make going chelsea link facebook come dick cough arms
love time liverpool need followed i?ll kick bad head
yum party won message omg family face i?ve smirks
hot election football let right fun hoe need slowly
cookies gop united sure saw friends lmfao sore hair
banana paul final thanks page tonight nigga flu face
bread way away my email timeline plans bi today chest
Table 7: High ranked topics in each level by comparing KL-divergence with other level?s topics
1.0 1.5 2.0 2.5 3.0Initial SD level
0.10
0.05
0.00
0.05
0.10
0.15
# T
wee
ts in
 con
ver
sat
ion
 cha
nge
s pr
opo
rtio
n o
ver
 tim
e
Figure 6: Relationship between initial SD level
and conversation length changes over time. The
solid line is the linear regression line, and the co-
efficient is 0.048 with p < 0.0001, which shows a
significant positive relationship.
6.4 Is there a difference in conversation
length patterns over time depending on
overall SD level?
Now we investigate the conversation length
changes over time with three groups, low,
medium, and high, by overall SD level. Then
we investigate changes in conversation length over
time.
Figure 7 shows the results of this investigation.
First, conversations are generally lengthier when
SD level is high. This phenomenon is also ob-
0 5 10 15 20 25 30 35 40Conversation order
8.0
8.5
9.0
9.5
10.0
10.5
# T
wee
ts i
n c
onv
ers
atio
n
high mid low
Figure 7: Changes in conversation length over
time. We divide dyads into three groups by SD
level score as low, medium, and high. Conversa-
tion length noticeably increases over time in the
medium and high groups, but only slight in the low
group.
served in figure 6, but here we can see it as a
long-term persistent pattern. Second, conversation
length increases consistently and significantly for
the high and medium groups, but for the low SD
group, there is not a significant increase of conver-
sation length over time.
7 Related Work
Prior work on quantitatively analyzing self-
disclosure has relied on user surveys (Ledbetter et
1994
al., 2011; Trepte and Reinecke, 2013) or human
annotation (Barak and Gluck-Ofri, 2007; Court-
ney Walton and Rice, 2013). These methods con-
sume much time and effort, so they are not suit-
able for large-scale studies. In prior work clos-
est to ours, Bak et al. (2012) showed that a topic
model can be used to identify self-disclosure, but
that work applies a two-step process in which a
basic topic model is first applied to find the top-
ics, and then the topics are post-processed for bi-
nary classification of self-disclosure. We improve
upon this work by applying a single unified model
of topics and self-disclosure for high accuracy in
classifying the three levels of self-disclosure.
Subjectivity which is aspect of expressing opin-
ions (Pang and Lee, 2008; Wiebe et al., 2004) is
related with self-disclosure, but they are different
dimensions of linguistic behavior. Because there
indeed are many high self-disclosure tweets that
are subjective, but there are also counter examples
in annotated dataset. The tweet ?England manager
is Roy Hodgson.? is low self-disclosure and low
subjectivity, ?I have barely any hair left.? is high
self-disclosure but low subjectivity, and ?Senator
stop lying!? is low self-disclosure but high subjec-
tivity.
8 Conclusion and Future Work
In this paper, we have presented the self-disclosure
topic model (SDTM) for discovering topics and
classifying SD levels from Twitter conversation
data. We devised a set of effective seed words
and trigrams, mined from a dataset of secrets. We
also annotated Twitter conversations to make a
ground-truth dataset for SD level. With anno-
tated data, we showed that SDTM outperforms
previous methods in classification accuracy and F-
measure. We publish the source code of SDTM
and the dataset include annotated Twitter conver-
sations and SECRET publicly
7
.
We also analyzed the relationship between SD
level and conversation behaviors over time. We
found that there is a positive correlation be-
tween initial SD level and subsequent conversa-
tion length. Also, dyads show higher level of
SD if they initially display high conversation fre-
quency. Finally, dyads with overall medium and
high SD level will have longer conversations over
time. These results support previous results in so-
7
http://uilab.kaist.ac.kr/research/
EMNLP2014
cial psychology research with more robust results
from a large-scale dataset, and show the effective-
ness of computationally analyzing at SD behavior.
There are several future directions for this re-
search. First, we can improve our modeling for
higher accuracy and better interpretability. For
instance, SDTM only considers first-person pro-
nouns and topics. Naturally, there are other lin-
guistic patterns that can be identified by humans
but not captured by pronouns and topics. Sec-
ond, the number of topics for each level is varied,
and so we can explore nonparametric topic mod-
els (Teh et al., 2006) which infer the number of
topics from the data. Third, we can look at the
relationship between self-disclosure behavior and
general online social network usage beyond con-
versations. We will explore these directions in our
future work.
Acknowledgments
We would like to thank Jing Liu and Wayne Xin
Zhao for inspiring discussions, and the anony-
mous reviewers for helpful comments. Alice Oh
is supported by ICT R&D program of MSIP/IITP
[10041313, UX-oriented Mobile SW Platform].
References
JinYeong Bak, Suin Kim, and Alice Oh. 2012. Self-
disclosure and relationship strength in twitter con-
versations. In Proceedings of ACL.
Azy Barak and Orit Gluck-Ofri. 2007. Degree and
reciprocity of self-disclosure in online forums. Cy-
berPsychology & Behavior, 10(3):407?417.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
danah boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of HICSS.
S Courtney Walton and Ronald E Rice. 2013. Medi-
ated disclosure on twitter: The roles of gender and
identity in boundary impermeability, valence, dis-
closure, and stage. Computers in Human Behavior,
29(4):1465?1474.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words!: Lin-
guistic style accommodation in social media. In
Proceedings of WWW.
Tara M Emmers-Sommer. 2004. The effect of com-
munication quality and quantity indicators on inti-
macy and relational satisfaction. Journal of Social
and Personal Relationships, 21(3):399?411.
1995
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
David J Houghton and Adam N Joinson. 2012.
Linguistic markers of secrets and sensitive self-
disclosure in twitter. In Proceedings of HICSS.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM.
Adam N Joinson and Carina B Paine. 2007. Self-
disclosure, privacy and the internet. The Oxford
handbook of Internet psychology, pages 237?252.
Adam N Joinson. 2001. Self-disclosure in
computer-mediated communication: The role of
self-awareness and visual anonymity. European
Journal of Social Psychology, 31(2):177?192.
Sidney M Jourard. 1971. Self-disclosure: An experi-
mental analysis of the transparent self.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A hierarchical aspect-sentiment
model for online reviews. In Proceedings of AAAI.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159?174.
Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeG-
root, Kevin R Meyer, Yuping Mao, and Brian Swaf-
ford. 2011. Attitudes toward online social con-
nection and self-disclosure as predictors of facebook
communication and relational closeness. Communi-
cation Research, 38(1):27?53.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Erika McCallister. 2010. Guide to protecting the confi-
dentiality of personally identifiable information. DI-
ANE Publishing.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Sharon Meraz. 2013. Public dialogue: Analysis of
tolerance in online discussions. In Proceedings of
ACL.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms
for topic models. Journal of Machine Learning Re-
search, 10:1801?1828.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of HLT-NAACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, Qiaozhu
Mei, and Ming Zhang. 2014. Understanding the
limiting factors of topic modeling via posterior con-
traction analysis. In Proceedings of The 31st In-
ternational Conference on Machine Learning, pages
190?198.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the american statistical associ-
ation, 101(476).
Sabine Trepte and Leonard Reinecke. 2013. The re-
ciprocal effects of social network site use and the
disposition for self-disclosure: A longitudinal study.
Computers in Human Behavior, 29(3):1102 ? 1112.
Sarah I Vondracek and Fred W Vondracek. 1971. The
manipulation and measurement of self-disclosure in
preadolescents. Merrill-Palmer Quarterly of Behav-
ior and Development, 17(1):51?58.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational linguistics,
30(3):277?308.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
EMNLP.
Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda:
maximum margin supervised topic models. Journal
of Machine Learning Research, 13:2237?2278.
1996
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 650?658,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Comparable Entity Mining from Comparative Questions 
 
 
Shasha Li1?Chin-Yew Lin2?Young-In Song2?Zhoujun Li3 
1National University of Defense Technology, Changsha, China 
2Microsoft Research Asia, Beijing, China 
3Beihang University, Beijing, China 
shashali@nudt.edu.cn1, {cyl,yosong}@microsoft.com2, 
lizj@buaa.edu.cn3 
  
 
Abstract 
Comparing one thing with another is a typical 
part of human decision making process. How-
ever, it is not always easy to know what to 
compare and what are the alternatives. To ad-
dress this difficulty, we present a novel way to 
automatically mine comparable entities from 
comparative questions that users posted on-
line. To ensure high precision and high recall, 
we develop a weakly-supervised bootstrapping 
method for comparative question identification 
and comparable entity extraction by leveraging 
a large online question archive. The experi-
mental results show our method achieves F1-
measure of 82.5% in comparative question 
identification and 83.3% in comparable entity 
extraction. Both significantly outperform an 
existing state-of-the-art method.  
1 Introduction 
Comparing alternative options is one essential 
step in decision-making that we carry out every 
day. For example, if someone is interested in cer-
tain products such as digital cameras, he or she 
would want to know what the alternatives are 
and compare different cameras before making a 
purchase. This type of comparison activity is 
very common in our daily life but requires high 
knowledge skill. Magazines such as Consumer 
Reports and PC Magazine and online media such 
as CNet.com strive in providing editorial com-
parison content and surveys to satisfy this need.  
In the World Wide Web era, a comparison ac-
tivity typically involves: search for relevant web 
pages containing information about the targeted 
products, find competing products, read reviews, 
and identify pros and cons. In this paper, we fo-
cus on finding a set of comparable entities given 
a user?s input entity. For example, given an enti-
ty, Nokia N95 (a cellphone), we want to find 
comparable entities such as Nokia N82, iPhone 
and so on.  
In general, it is difficult to decide if two enti-
ties are comparable or not since people do com-
pare apples and oranges for various reasons.  For 
example, ?Ford? and ?BMW? might be compa-
rable as ?car manufacturers? or as ?market seg-
ments that their products are targeting?, but we 
rarely see people comparing ?Ford Focus? (car 
model) and ?BMW 328i?.   Things also get more 
complicated when an entity has several functio-
nalities. For example, one might compare 
?iPhone? and ?PSP? as ?portable game player? 
while compare ?iPhone? and ?Nokia N95? as 
?mobile phone?. Fortunately, plenty of compara-
tive questions are posted online, which provide 
evidences for what people want to compare, e.g. 
?Which to buy, iPod or iPhone??. We call ?iPod? 
and ?iPhone? in this example as comparators.  In 
this paper, we define comparative questions and 
comparators as: 
 
? Comparative question: A question that in-
tends to compare two or more entities and it 
has to mention these entities explicitly in the 
question. 
? Comparator: An entity which is a target of 
comparison in a comparative question.  
 
According to these definitions, Q1 and Q2 be-
low are not comparative questions while Q3 is. 
?iPod Touch? and ?Zune HD? are comparators. 
 
Q1: ?Which one is better?? 
Q2: ?Is Lumix GH-1 the best camera?? 
Q3: ?What?s the difference between iPod 
Touch and Zune HD?? 
 
The goal of this work is mining comparators 
from comparative questions. The results would 
be very useful in helping users? exploration of 
650
alternative choices by suggesting comparable 
entities based on other users? prior requests.  
To mine comparators from comparative ques-
tions, we first have to detect whether a question 
is comparative or not. According to our defini-
tion, a comparative question has to be a question 
with intent to compare at least two entities. 
Please note that a question containing at least 
two entities is not a comparative question if it 
does not have comparison intent. However, we 
observe that a question is very likely to be a 
comparative question if it contains at least two 
entities. We leverage this insight and develop a 
weakly supervised bootstrapping method to iden-
tify comparative questions and extract compara-
tors simultaneously. 
To our best knowledge, this is the first attempt 
to specially address the problem on finding good 
comparators to support users? comparison activi-
ty. We are also the first to propose using com-
parative questions posted online that reflect what 
users truly care about as the medium from which 
we mine comparable entities. Our weakly super-
vised method achieves 82.5% F1-measure in 
comparative question identification, 83.3% in 
comparator extraction, and 76.8% in end-to-end 
comparative question identification and compa-
rator extraction which outperform the most rele-
vant state-of-the-art method by Jindal & Liu 
(2006b) significantly.   
The rest of this paper is organized as follows. 
The next section discusses previous works. Sec-
tion 3 presents our weakly-supervised method for 
comparator mining. Section 4 reports the evalua-
tions of our techniques, and we conclude the pa-
per and discuss future work in Section 5. 
 
2 Related Work 
2.1 Overview 
In terms of discovering related items for an enti-
ty, our work is similar to the research on recom-
mender systems, which recommend items to a 
user. Recommender systems mainly rely on simi-
larities between items and/or their statistical cor-
relations in user log data (Linden et al, 2003). 
For example, Amazon recommends products to 
its customers based on their own purchase histo-
ries, similar customers? purchase histories, and 
similarity between products. However, recom-
mending an item is not equivalent to finding a 
comparable item. In the case of Amazon, the 
purpose of recommendation is to entice their cus-
tomers to add more items to their shopping carts 
by suggesting similar or related items. While in 
the case of comparison, we would like to help 
users explore alternatives, i.e. helping them make 
a decision among comparable items. 
For example, it is reasonable to recommend 
?iPod speaker? or ?iPod batteries? if a user is 
interested in ?iPod?, but we would not compare 
them with ?iPod?. However, items that are com-
parable with ?iPod? such as ?iPhone? or ?PSP? 
which were found in comparative questions post-
ed by users are difficult to be predicted simply 
based on item similarity between them. Although 
they are all music players, ?iPhone? is mainly a 
mobile phone, and ?PSP? is mainly a portable 
game device. They are similar but also different 
therefore beg comparison with each other. It is 
clear that comparator mining and item recom-
mendation are related but not the same.  
Our work on comparator mining is related to 
the research on entity and relation extraction in 
information extraction (Cardie, 1997; Califf and 
Mooney, 1999; Soderland, 1999; Radev et al, 
2002; Carreras et al, 2003). Specifically, the 
most relevant work is by Jindal and Liu (2006a 
and 2006b) on mining comparative sentences and 
relations. Their methods applied class sequential 
rules (CSR) (Chapter 2, Liu 2006) and label se-
quential rules (LSR) (Chapter 2, Liu 2006) 
learned from annotated corpora to identify com-
parative sentences and extract comparative rela-
tions respectively in the news and review do-
mains. The same techniques can be applied to 
comparative question identification and compa-
rator mining from questions. However, their me-
thods typically can achieve high precision but 
suffer from low recall (Jindal and Liu, 2006b) 
(J&L). However, ensuring high recall is crucial 
in our intended application scenario where users 
can issue arbitrary queries. To address this prob-
lem, we develop a weakly-supervised bootstrap-
ping pattern learning method by effectively leve-
raging unlabeled questions.  
Bootstrapping methods have been shown to be 
very effective in previous information extraction 
research (Riloff, 1996; Riloff and Jones, 1999; 
Ravichandran and Hovy, 2002; Mooney and Bu-
nescu, 2005; Kozareva et al, 2008). Our work is 
similar to them in terms of methodology using 
bootstrapping technique to extract entities with a 
specific relation. However, our task is different 
from theirs in that it requires not only extracting 
entities (comparator extraction) but also ensuring 
that the entities are extracted from comparative 
questions (comparative question identification), 
which is generally not required in IE task. 
651
2.2 Jindal & Liu 2006 
In this subsection, we provide a brief summary 
of the comparative mining method proposed by 
Jindal and Liu (2006a and 2006b), which is used 
as baseline for comparison and represents the 
state-of-the-art in this area.  We first introduce 
the definition of CSR and LSR rule used in their 
approach, and then describe their comparative 
mining method. Readers should refer to J&L?s 
original papers for more details. 
CSR and LSR 
CSR is a classification rule. It maps a sequence 
pattern S(?1?2???) to a class C.  In our problem, 
C is either comparative or non-comparative. 
Given a collection of sequences with class in-
formation, every CSR is associated to two para-
meters: support and confidence. Support is the 
proportion of sequences in the collection contain-
ing S as a subsequence. Confidence is the propor-
tion of sequences labeled as C in the sequences 
containing the S. These parameters are important 
to evaluate whether a CSR is reliable or not. 
LSR is a labeling rule. It maps an input se-
quence pattern ?(?1?2??? ???)  to a labeled 
sequence ??(?1?2? ?? ???) by replacing one to-
ken (??) in the input sequence with a designated 
label (?? ). This token is referred as the anchor. 
The anchor in the input sequence could be ex-
tracted if its corresponding label in the labeled 
sequence is what we want (in our case, a compa-
rator). LSRs are also mined from an annotated 
corpus, therefore each LSR also have two para-
meters: support and confidence. They are simi-
larly defined as in CSR. 
Supervised Comparative Mining Method 
J&L treated comparative sentence identification 
as a classification problem and comparative rela-
tion extraction as an information extraction prob-
lem. They first manually created a set of 83 key-
words such as beat, exceed, and outperform that 
are likely indicators of comparative sentences. 
These keywords were then used as pivots to 
create part-of-speech (POS) sequence data. A 
manually annotated corpus with class informa-
tion, i.e. comparative or non-comparative, was 
used to create sequences and CSRs were mined. 
A Na?ve Bayes classifier was trained using the 
CSRs as features. The classifier was then used to 
identify comparative sentences. 
Given a set of comparative sentences, J&L 
manually annotated two comparators with labels 
$ES1 and $ES2 and the feature compared with 
label $FT for each sentence. J&L?s method was 
only applied to noun and pronoun. To differen-
tiate noun and pronoun that are not comparators 
or features, they added the fourth label $NEF, i.e. 
non-entity-feature. These labels were used as 
pivots together with special tokens li & rj
1 (token 
position), #start (beginning of a sentence), and 
#end (end of a sentence) to generate sequence 
data, sequences with single label only and mini-
mum support greater than 1% are retained, and 
then LSRs were created. When applying the 
learned LSRs for extraction, LSRs with higher 
confidence were applied first. 
J&L?s method have been proved effective in 
their experimental setups. However, it has the 
following weaknesses:  
 
? The performance of J&L?s method relies 
heavily on a set of comparative sentence in-
dicative keywords. These keywords were 
manually created and they offered no guide-
lines to select keywords for inclusion. It is 
also difficult to ensure the completeness of 
the keyword list.  
? Users can express comparative sentences or 
questions in many different ways. To have 
high recall, a large annotated training corpus 
is necessary. This is an expensive process.  
? Example CSRs and LSRs given in Jindal & 
Liu (2006b) are mostly a combination of 
POS tags and keywords. It is a surprise that 
their rules achieved high precision but low 
recall. They attributed most errors to POS 
tagging errors. However, we suspect that 
their rules might be too specific and overfit 
their small training set (about 2,600 sen-
tences). We would like to increase recall, 
avoid overfitting, and allow rules to include 
discriminative lexical tokens to retain preci-
sion. 
 
In the next section, we introduce our method to 
address these shortcomings. 
3 Weakly Supervised Method for Com-
parator Mining 
Our weakly supervised method is a pattern-based 
approach similar to J&L?s method, but it is dif-
ferent in many aspects: Instead of using separate 
CSRs and LSRs, our method aims to learn se-
                                                 
1 li marks a token is at the i
th 
position to the left of the pivot 
and rj marks a token is at j
th position to the right of the 
pivot where i and j are between 1 and 4 in J&L (2006b). 
652
quential patterns which can be used to identify 
comparative question and extract comparators 
simultaneously.  
In our approach, a sequential pattern is defined 
as a sequence S(s1s2? si ? sn) where si can be a 
word, a POS tag, or a symbol denoting either a 
comparator ($C), or the beginning (#start) or the 
end of a question (#end). A sequential pattern is 
called an indicative extraction pattern (IEP) if it 
can be used to identify comparative questions 
and extract comparators in them with high relia-
bility. We will formally define the reliability 
score of a pattern in the next section.  
Once a question matches an IEP, it is classified 
as a comparative question and the token se-
quences corresponding to the comparator slots in 
the IEP are extracted as comparators.  When a 
question can match multiple IEPs, the longest 
IEP is used 2 . Therefore, instead of manually 
creating a list of indicative keywords, we create a 
set of IEPs. We will show how to acquire IEPs 
automatically using a bootstrapping procedure 
with minimum supervision by taking advantage 
of a large unlabeled question collection in the 
following subsections. The evaluations shown in 
section 4 confirm that our weakly supervised 
method can achieve high recall while retain high 
precision. 
This pattern definition is inspired by the work 
of Ravichandran and Hovy (2002). Table 1 
shows some examples of such sequential pat-
terns. We also allow POS constraint on compara-
tors as shown in the pattern ?<, $C/NN or $C/NN 
? #end>?. It means that a valid comparator must 
have a NN POS tag. 
3.1 Mining Indicative Extraction Patterns 
Our weakly supervised IEP mining approach is 
based on two key assumptions:  
 
                                                 
2 It is because the longest IEP is likely to be the most specif-
ic and relevant pattern for the given question. 
 
Figure 1: Overview of the bootstrapping alogorithm  
 
? If a sequential pattern can be used to extract 
many reliable comparator pairs, it is very likely 
to be an IEP.  
? If a comparator pair can be extracted by an 
IEP, the pair is reliable. 
 
Based on these two assumptions, we design 
our bootstrapping algorithm as shown in Figure 1. 
The bootstrapping process starts with a single 
IEP. From it, we extract a set of initial seed com-
parator pairs. For each comparator pair, all ques-
tions containing the pair are retrieved from a 
question collection and regarded as comparative 
questions. From the comparative questions and 
comparator pairs, all possible sequential patterns 
are generated and evaluated by measuring their 
reliability score defined later in the Pattern Eval-
uation section. Patterns evaluated as reliable ones 
are IEPs and are added into an IEP repository.  
Then, new comparator pairs are extracted from 
the question collection using the latest IEPs. The 
new comparators are added to a reliable compa-
rator repository and used as new seeds for pattern 
learning in the next iteration. All questions from 
which reliable comparators are extracted are re-
moved from the collection to allow finding new 
patterns efficiently in later iterations. The 
process iterates until no more new patterns can 
be found from the question collection.  
There are two key steps in our method: (1) 
pattern generation and (2) pattern evaluation. In 
the following subsections, we will explain them 
in details.   
Pattern Generation 
To generate sequential patterns, we adapt the 
surface text pattern mining method introduced in 
(Ravichandran and Hovy, 2002). For any given 
comparative question and its comparator pairs, 
comparators in the question are replaced with 
symbol $Cs. Two symbols, #start and #end, are 
attached to the beginning and the end of a sen-
Sequential Patterns 
<#start which city is better, $C or $C ? #end> 
<, $C or $C ? #end> 
<#start $C/NN or $C/NN ? #end> 
<which NN is better, $C or $C ?> 
<which city is JJR, $C or $C ?>  
<which NN is JJR, $C or $C ?> 
... 
Table 1: Candidate indicative extraction pattern (IEP) 
examples of the question ?which city is better, NYC or 
Paris?? 
 
653
tence in the question. Then, the following three 
kinds of sequential patterns are generated from 
sequences of questions: 
 
 
? Lexical patterns: Lexical patterns indicate 
sequential patterns consisting of only words 
and symbols ($C, #start, and #end). They are 
generated by suffix tree algorithm (Gusfield, 
1997) with two constraints: A pattern should 
contain more than one $C, and its frequency 
in collection should be more than an empiri-
cally determined number ?. 
? Generalized patterns: A lexical pattern can 
be too specific. Thus, we generalize lexical 
patterns by replacing one or more words with 
their POS tags. 2? ? 1 generalized patterns 
can be produced from a lexical pattern con-
taining N words excluding $Cs.  
? Specialized patterns: In some cases, a pat-
tern can be too general. For example, al-
though a question ?ipod or zune?? is com-
parative, the pattern ?<$C or $C>? is too 
general, and there can be many non-
comparative questions matching the pattern, 
for instance, ?true or false??. For this reason, 
we perform pattern specialization by adding 
POS tags to all comparator slots. For exam-
ple, from the lexical pattern ?<$C or $C>? 
and the question ?ipod or zune??, ?<$C/NN 
or $C/NN?>? will be produced as a specia-
lized pattern.  
 
Note that generalized patterns are generated from 
lexical patterns and the specialized patterns are 
generated from the combined set of generalized 
patterns and lexical patterns. The final set of 
candidate patterns is a mixture of lexical patterns, 
generalized patterns and specialized patterns. 
Pattern Evaluation  
According to our first assumption, a reliability 
score ??(??) for a candidate pattern ??  at itera-
tion k can be defined as follows: 
 
?? ?? =
 ?? (????? ? )??? ????
??1
?? (????)
        (1) 
 
, where ??  can extract known reliable comparator 
pairs ??? . ??
??1 indicates the reliable compara-
tor pair repository accumulated until the 
(? ? 1)?? iteration. ??(?) means the number of 
questions satisfying a condition x. The condition 
?? ? ???  denotes that ???  can be extracted from 
a question by applying pattern ??  while the con-
dition ?? ??  denotes any question containing 
pattern ?? .  
However, Equation (1) can suffer from in-
complete knowledge about reliable comparator 
pairs. For example, very few reliable pairs are 
generally discovered in early stage of bootstrap-
ping. In this case, the value of Equation (1) 
might be underestimated which could affect the 
effectiveness of equation (1) on distinguishing 
IEPs from non-reliable patterns. We mitigate this 
problem by a lookahead procedure. Let us denote 
the set of candidate patterns at the iteration k by 
? ? . We define the support ? for comparator pair  
?? ?  which can be extracted by ? 
?   and does not 
exist in the current reliable set:  
 
? ?? ? = ??( ? 
?
? ?? ?)     (2) 
 
where ? ? ? ?? ?  means that one of the patterns in 
? ?  can extract ???  in certain questions. Intuitive-
ly, if  ?? ?  can be extracted by many candidate 
patterns in ? ? , it is likely to be extracted as a 
reliable one in the next iteration. Based on this 
intuition, a pair ???  whose support S is more than 
a threshold ? is regarded as a likely-reliable pair. 
Using likely-reliable pairs, lookahead reliability 
score ?  ??  is defined: 
 
? ? ?? =
 ?? (????? i )??? ???? ???
?
?? (????)
      (3) 
 
, where ?? ???
?  indicates a set of likely-reliable 
pairs based on ? ? .  
By interpolating Equation (1) and (3), the final 
reliability score ?(??)?????
?  for a pattern is de-
fined as follows: 
 
?(??)?????
? = ? ? ?? ?? + (1? ?) ? ? 
?(??)     (4) 
 
Using Equation (4), we evaluate all candidate 
patterns and select patterns whose score is more 
than threshold ? as IEPs. All necessary parame-
ter values are empirically determined. We will 
explain how to determine our parameters in sec-
tion 4. 
4 Experiments 
4.1 Experiment Setup 
Source Data 
All experiments were conducted on about 60M 
questions mined from Yahoo! Answers? question 
title field. The reason that we used only a title 
654
field is that they clearly express a main intention 
of an asker with a form of simple questions in 
general.  
Evaluation Data 
Two separate data sets were created for evalua-
tion. First, we collected 5,200 questions by sam-
pling 200 questions from each Yahoo! Answers 
category3. Two annotators were asked to label 
each question manually as comparative, non-
comparative, or unknown. Among them, 139 
(2.67%) questions were classified as comparative,  
4,934 (94.88%) as non-comparative, and 127 
(2.44%) as unknown questions which are diffi-
cult to assess. We call this set SET-A. 
Because there are only 139 comparative ques-
tions in SET-A, we created another set which 
contains more comparative questions. We ma-
nually constructed a keyword set consisting of 53 
words such as ?or? and ?prefer?, which are good 
indicators of comparative questions. In SET-A, 
97.4% of comparative questions contains one or 
more keywords from the keyword set. We then 
randomly selected another 100 questions from 
each Yahoo! Answers category with one extra 
condition that all questions have to contain at 
least one keyword. These questions were labeled 
in the same way as SET-A except that their com-
parators were also annotated. This second set of 
questions is referred as SET-B. It contains 853 
comparative questions and 1,747 non-
comparative questions. For comparative question 
identification experiments, we used all labeled 
questions in SET-A and SET-B. For comparator 
extraction experiments, we used only SET-B. All 
the remaining unlabeled questions (called as 
SET-R) were used for training our weakly super-
vised method. 
As a baseline method, we carefully imple-
mented J&L?s method. Specifically, CSRs for 
comparative question identification were learned 
from the labeled questions, and then a statistical 
classifier was built by using CSR rules as fea-
tures. We examined both SVM and Na?ve Bayes 
(NB) models as reported in their experiments.  
For the comparator extraction, LSRs were 
learned from SET-B and applied for comparator 
extraction.  
To start the bootstrapping procedure, we ap-
plied the IEP ?<#start nn/$c vs/cc nn/$c ?/. 
#end>? to all the questions in SET-R and ga-
thered 12,194 comparator pairs as the initial 
seeds.  For our weakly supervised method, there 
                                                 
3 There are 26 top level categories in Yahoo! Answers. 
are four parameters, i.e. ?, ?, ?, and ?, need to be 
determined empirically. We first mined all poss-
ible candidate patterns from the suffix tree using 
the initial seeds. From these candidate patterns, 
we applied them to SET-R and got a new set of 
59,410 candidate comparator pairs. Among these 
new candidate comparator pairs, we randomly 
selected 100 comparator pairs and manually clas-
sified them into reliable or non-reliable compara-
tors. Then we found ? that maximized precision 
without hurting recall by investigating frequen-
cies of pairs in the labeled set. By this method, ? 
was set to 3 in our experiments. Similarly, the 
threshold parameters ? and ? for pattern evalua-
tion were set to 10 and 0.8 respectively. For the 
interpolation parameter ?  in Equation (3), we 
simply set the value to 0.5 by assuming that two 
reliability scores are equally important.  
As evaluation measures for comparative ques-
tion identification and comparator extraction, we 
used precision, recall, and F1-measure. All re-
sults were obtained from 5-fold cross validation. 
Note that J&L?s method needs a training data but 
ours use the unlabeled data (SET-R) with weakly 
supervised method to find parameter setting. 
This 5-fold evaluation data is not in the unla-
beled data. Both methods were tested on the 
same test split in the 5-fold cross validation. All 
evaluation scores are averaged across all 5 folds. 
For question processing, we used our own sta-
tistical POS tagger developed in-house4.  
4.2 Experiment Results 
Comparative Question Identification and 
Comparator Extraction 
Table 2 shows our experimental results. In the 
table, ?Identification only? indicates the perfor-
mances in comparative question identification, 
?Extraction only? denotes the performances of 
comparator extraction when only comparative 
questions are used as input, and ?All? indicates 
the end-to-end performances when question 
identification results were used in comparator 
extraction. Note that the results of J&L?s method 
on our collections are very comparable to what is 
reported in their paper.  
In terms of precision, the J&L?s method is 
competitive to our method in comparative ques-
                                                 
4  We used NLC-PosTagger which is developed by NLC 
group of Microsoft Research Asia. It uses the modified 
Penn Treebank POS set for its output; for example, NNS 
(plural nouns), NN (nouns), NP (noun phrases), NPS (plural 
noun phrases), VBZ (verb, present tense, 3rd person singu-
lar), JJ (adjective), RB(adverb), and so on. 
655
tion identification. However, the recall is signifi-
cantly lower than ours. In terms of recall, our 
method outperforms J&L?s method by 35% and 
22% in comparative question identification and 
comparator extraction respectively. In our analy-
sis, the low recall of J&L?s method is mainly 
caused by low coverage of learned CSR patterns 
over the test set.  
In the end-to-end experiments, our weakly su-
pervised method performs significantly better 
than J&L?s method. Our method is about 55% 
better in F1-measure. This result also highlights 
another advantage of our method that identifies 
comparative questions and extracts comparators 
simultaneously using one single pattern. J&L?s 
method uses two kinds of pattern rules, i.e. CSRs 
and LSRs. Its performance drops significantly 
due to error propagations. F1-measure of J&L?s 
method in ?All? is about 30% and 32% worse 
than the scores of ?Identification only? and ?Ex-
traction? only respectively, our method only 
shows small amount of performance decrease 
(approximately 7-8%).  
We also analyzed the effect of pattern genera-
lization and specialization. Table 3 shows the 
results. Despite of the simplicity of our methods, 
they significantly contribute to performance im-
provements. This result shows the importance of 
learning patterns flexibly to capture various 
comparative question expressions. Among the 
6,127 learned IEPs in our database, 5,930 pat-
terns are generalized ones, 171 are specialized 
ones, and only 26 patterns are non-generalized 
and specialized ones.  
To investigate the robustness of our bootstrap-
ping algorithm for different seed configurations, 
we compare the performances between two dif-
ferent seed IEPs. The results are shown in Table 
4. As shown in the table, the performance of our 
bootstrapping algorithm is stable regardless of 
significantly different number of seed pairs gen-
erated by the two IEPs. This result implies that 
our bootstrapping algorithm is not sensitive to 
the choice of IEP.  
Table 5 also shows the robustness of our boot-
strapping algorithm. In Table 5, ?All? indicates 
the performances that all comparator pairs from a 
single seed IEP is used for the bootstrapping, and 
?Partial? indicate the performances using only 
1,000 randomly sampled pairs from ?All?. As 
shown in the table, there is no significant per-
formance difference.  
In addition, we conducted error analysis for 
the cases where our method fails to extract cor-
rect comparator pairs: 
 
? 23.75% of errors on comparator extraction 
are due to wrong pattern selection by our 
simple maximum IEP length strategy.  
? The remaining 67.63% of errors come from 
comparative questions which cannot be cov-
ered by the learned IEPs. 
 
 
 Recall Precision F-score 
Original Patterns 0.689  0. 449 0.544 
+ Specialized 0.731  0.602 0.665 
+ Generalized 0.760  0.776 0.768 
Table 3: Effect of pattern specialization and Generali-
zation in the end-to-end experiments.  
 
Seed patterns # of resulted 
seed pairs 
F-score 
<#start nn/$c vs/cc nn/$c 
?/. #end>  
12,194 0.768 
<#start which/wdt is/vb 
better/jjr , nn/$c or/cc 
nn/$c ?/. #end> 
1,478 0.760 
Table 4: Performance variation over different initial 
seed IEPs in the end-to-end experiments 
 
Set  (# of seed pairs) Recall Precision F-score 
All (12,194) 0.760 0.774 0.768 
Partial (1,000) 0.724 0.763 0.743 
Table 5: Performance variation over different sizes of 
seed pairs generated from a single initial seed IEP 
?<#start nn/$c vs/cc nn/$c ?/. #end>?. 
 
 
Identification only 
(SET-A+SET-B) 
Extraction only 
(SET-B) 
All 
(SET-B) 
J&L (CSR) Our  
Method 
J&L 
(LSR) 
Our  
Method 
J&L Our  
Method SVM NB SVM NB 
Recall 0.601 0.537 0.817* 0.621 0.760* 0.373 0.363 0.760* 
Precision 0.847 0.851 0.833 0.861 0.916* 0.729 0.703 0.776* 
F-score 0.704 0.659 0.825* 0.722 0.833* 0.493 0.479 0.768* 
Table 2: Performance comparison between our method and Jindal and Bing?s Method (denoted as J&L). 
The values with * indicate statistically significant improvements over J&L (CSR) SVM or J&L (LSR) 
according to t-test  at p < 0.01 level. 
 
656
Examples of Comparator Extraction  
By applying our bootstrapping method to the 
entire source data (60M questions), 328,364 
unique comparator pairs were extracted from 
679,909 automatically identified comparative 
questions.  
Table 6 lists top 10 frequently compared enti-
ties for a target item, such as Chanel, Gap, in our 
question archive. As shown in the table, our 
comparator mining method successfully discov-
ers realistic comparators. For example, for ?Cha-
nel?, most results are high-end fashion brands 
such as ?Dior? or ?Louis Vuitton?, while the rank-
ing results for ?Gap? usually contains similar ap-
parel brands for young people, such as ?Old Navy? 
or ?Banana Republic?. For the basketball player 
?Kobe?, most of the top ranked comparators are 
also famous basketball players. Some interesting 
comparators are shown for ?Canon? (the compa-
ny name). It is famous for different kinds of its 
products, for example, digital cameras and prin-
ters, so it can be compared to different kinds of 
companies. For example, it is compared to ?HP?, 
?Lexmark?, or ?Xerox?, the printer manufacturers, 
and also compared to ?Nikon?, ?Sony?, or ?Kodak?, 
the digital camera manufactures.  Besides gener-
al entities such as a brand or company name, our 
method also found an interesting comparable 
entity for a specific item in the experiments. For 
example, our method recommends ?Nikon d40i?, 
?Canon rebel xti?, ?Canon rebel xt?, ?Nikon 
d3000?, ?Pentax k100d?, ?Canon eos 1000d? as 
comparators for the specific camera product ?Ni-
kon 40d?. 
Table 7 can show the difference between our 
comparator mining and query/item recommenda-
tion. As shown in the table, ?Google related 
searches? generally suggests a mixed set of two 
kinds of related queries for a target entity: (1) 
queries specified with subtopics for an original 
query (e.g., ?Chanel handbag? for ?Chanel?) and 
(2) its comparable entities (e.g., ?Dior? for ?Cha-
nel?). It confirms one of our claims that compara-
tor mining and query/item recommendation are 
related but not the same. 
5 Conclusion 
In this paper, we present a novel weakly super-
vised method to identify comparative questions 
and extract comparator pairs simultaneously. We 
rely on the key insight that a good comparative 
question identification pattern should extract 
good comparators, and a good comparator pair 
should occur in good comparative questions to 
bootstrap the extraction and identification 
process. By leveraging large amount of unla-
beled data and the bootstrapping process with 
slight supervision to determine four parameters, 
we found 328,364 unique comparator pairs and 
6,869 extraction patterns without the need of 
creating a set of comparative question indicator 
keywords.  
The experimental results show that our me-
thod is effective in both comparative question 
identification and comparator extraction. It sig-
 Chanel Gap iPod Kobe Canon 
1 Dior Old Navy Zune Lebron Nikon 
2 Louis Vuitton American Eagle mp3 player Jordan Sony 
3 Coach Banana Republic PSP MJ Kodak 
4 Gucci Guess by Marciano cell phone Shaq Panasonic 
5 Prada ACP Ammunition iPhone Wade Casio 
6 Lancome Old Navy brand Creative Zen T-mac Olympus 
7 Versace Hollister Zen Lebron James Hp 
8 LV Aeropostal iPod nano Nash Lexmark 
9 Mac American Eagle outfitters iPod touch KG Pentax 
10 Dooney Guess iRiver Bonds Xerox 
Table 6: Examples of comparators for different entities  
Chanel Gap iPod Kobe Canon 
Chanel handbag Gap coupons iPod nano Kobe Bryant stats Canon t2i 
Chanel sunglass Gap outlet iPod touch Lakers Kobe Canon printers 
Chanel earrings Gap card iPod best buy Kobe espn Canon printer drivers 
Chanel watches Gap careers iTunes Kobe Dallas Mavericks Canon downloads 
Chanel shoes Gap casting call Apple Kobe NBA Canon copiers 
Chanel jewelry Gap adventures iPod shuffle Kobe 2009 Canon scanner 
Chanel clothing Old navy iPod support Kobe san Antonio Canon lenses 
Dior Banana republic iPod classic Kobe Bryant 24 Nikon 
Table 7: Related queries returned by Google related searches for the same target entities in Table 6. The bold 
ones indicate overlapped queries to the comparators in Table 6. 
 
657
nificantly improves recall in both tasks while 
maintains high precision. Our examples show 
that these comparator pairs reflect what users are 
really interested in comparing. 
Our comparator mining results can be used for 
a commerce search or product recommendation 
system. For example, automatic suggestion of 
comparable entities can assist users in their com-
parison activities before making their purchase 
decisions. Also, our results can provide useful 
information to companies which want to identify 
their competitors.  
In the future, we would like to improve extrac-
tion pattern application and mine rare extraction 
patterns. How to identify comparator aliases such 
as ?LV? and ?Louis Vuitton? and how to separate 
ambiguous entities such ?Paris vs. London? as 
location and ?Paris vs. Nicole? as celebrity are 
all interesting research topics. We also plan to 
develop methods to summarize answers pooled 
by a given comparator pair.  
6 Acknowledgement  
This work was done when the first author 
worked as an intern at Microsoft Research Asia. 
References  
Mary Elaine Califf and Raymond J. Mooney. 1999. 
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of AAAI?99 
/IAAI?99. 
Claire Cardie. 1997. Empirical methods in informa-
tion extraction. AI magazine, 18:65?79.  
Dan Gusfield. 1997. Algorithms on strings, trees, and 
sequences: computer science and computational 
biology. Cambridge University Press, New York, 
NY, USA 
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. 
In Proceedings of WWW ?02, pages 517?526. 
Glen Jeh and Jennifer Widom. 2003. Scaling persona-
lized web search. In Proceedings of WWW ?03, 
pages 271?279. 
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings 
of SIGIR ?06, pages 244?251. 
Nitin Jindal and Bing Liu. 2006b. Mining compara-
tive sentences and relations. In Proceedings of 
AAAI ?06. 
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 
2008. Semantic class learning from the web with 
hyponym pattern linkage graphs. In Proceedings of 
ACL-08: HLT, pages 1048?1056.  
Greg Linden, Brent Smith and Jeremy York. 2003. 
Amazon.com Recommendations: Item-to-Item 
Collaborative Filtering. IEEE Internet Computing, 
pages 76-80.  
Raymond J. Mooney and Razvan Bunescu. 2005. 
Mining knowledge from text using information ex-
traction. ACM SIGKDD Exploration Newsletter, 
7(1):3?10. 
Dragomir Radev, Weiguo Fan, Hong Qi, and Harris 
Wu and Amardeep Grewal. 2002. Probabilistic 
question answering on the web. Journal of the 
American Society for Information Science and 
Technology, pages 408?419. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning surface text patterns for a question ans-
wering system. In Proceedings of ACL ?02, pages 
41?47. 
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level 
bootstrapping. In Proceedings of AAAI ?99 
/IAAI ?99, pages 474?479. 
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of 
the 13th National Conference on Artificial Intelli-
gence, pages 1044?1049. 
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233?272.  
 
658
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Nonlinear Evidence Fusion and Propagation 
for Hyponymy Relation Mining 
 
Fan Zhang2*     Shuming Shi1     Jing Liu2     Shuqi Sun3*     Chin-Yew Lin1 
1Microsoft Research Asia 
2Nankai University, China 
3Harbin Institute of Technology, China 
{shumings, cyl}@microsoft.com 
 
 
 
Abstract 
This paper focuses on mining the hypon-
ymy (or is-a) relation from large-scale, 
open-domain web documents. A nonlinear 
probabilistic model is exploited to model 
the correlation between sentences in the 
aggregation of pattern matching results. 
Based on the model, we design a set of ev-
idence combination and propagation algo-
rithms. These significantly improve the 
result quality of existing approaches.  Ex-
perimental results conducted on 500 mil-
lion web pages and hypernym labels for 
300 terms show over 20% performance 
improvement in terms of P@5, MAP and 
R-Precision. 
1 Introduction1 
An important task in text mining is the automatic 
extraction of entities and their lexical relations; this 
has wide applications in natural language pro-
cessing and web search. This paper focuses on 
mining the hyponymy (or is-a) relation from large-
scale, open-domain web documents. From the 
viewpoint of entity classification, the problem is to 
automatically assign fine-grained class labels to 
terms. 
There have been a number of approaches 
(Hearst 1992; Pantel & Ravichandran 2004; Snow 
et al, 2005; Durme & Pasca, 2008; Talukdar et al, 
2008) to address the problem. These methods typi-
cally exploited manually-designed or automatical-
                                                          
* This work was performed when Fan Zhang and Shuqi Sun 
were interns at Microsoft Research Asia 
ly-learned patterns (e.g., ?NP such as NP?, ?NP 
like NP?, ?NP is a NP?). Although some degree of 
success has been achieved with these efforts, the 
results are still far from perfect, in terms of both 
recall and precision. As will be demonstrated in 
this paper, even by processing a large corpus of 
500 million web pages with the most popular pat-
terns, we are not able to extract correct labels for 
many (especially rare) entities. Even for popular 
terms, incorrect results often appear in their label 
lists. 
The basic philosophy in existing hyponymy ex-
traction approaches (and also many other text-
mining methods) is counting: count the number of 
supporting sentences. Here a supporting sentence 
of a term-label pair is a sentence from which the 
pair can be extracted via an extraction pattern. We 
demonstrate that the specific way of counting has a 
great impact on result quality, and that the state-of-
the-art counting methods are not optimal. Specifi-
cally, we examine the problem from the viewpoint 
of probabilistic evidence combination and find that 
the probabilistic assumption behind simple count-
ing is the statistical independence between the ob-
servations of supporting sentences. By assuming a 
positive correlation between supporting sentence 
observations and adopting properly designed non-
linear combination functions, the results precision 
can be improved. 
It is hard to extract correct labels for rare terms 
from a web corpus due to the data sparseness prob-
lem. To address this issue, we propose an evidence 
propagation algorithm motivated by the observa-
tion that similar terms tend to share common hy-
pernyms. For example, if we already know that 1) 
Helsinki and Tampere are cities, and 2) Porvoo is 
similar to Helsinki and Tampere, then Porvoo is 
1159
very likely also a city. This intuition, however, 
does not mean that the labels of a term can always 
be transferred to its similar terms. For example, 
Mount Vesuvius and Kilimanjaro are volcanoes 
and Lhotse is similar to them, but Lhotse is not a 
volcano. Therefore we should be very conservative 
and careful in hypernym propagation. In our prop-
agation algorithm, we first construct some pseudo 
supporting sentences for a term from the support-
ing sentences of its similar terms. Then we calcu-
late label scores for terms by performing nonlinear 
evidence combination based on the (pseudo and 
real) supporting sentences. Such a nonlinear prop-
agation algorithm is demonstrated to perform bet-
ter than linear propagation. 
Experimental results on a publicly available col-
lection of 500 million web pages with hypernym 
labels annotated for 300 terms show that our non-
linear evidence fusion and propagation significant-
ly improve the precision and coverage of the 
extracted hyponymy data. This is one of the tech-
nologies adopted in our semantic search and min-
ing system NeedleSeek2. 
In the next section, we discuss major related ef-
forts and how they differ from our work. Section 3 
is a brief description of the baseline approach. The 
probabilistic evidence combination model that we 
exploited is introduced in Section 4. Our main ap-
proach is illustrated in Section 5. Section 6 shows 
our experimental settings and results. Finally, Sec-
tion 7 concludes this paper. 
2 Related Work 
Existing efforts for hyponymy relation extraction 
have been conducted upon various types of data 
sources, including plain-text corpora (Hearst 1992; 
Pantel & Ravichandran, 2004; Snow et al, 2005; 
Snow et al, 2006; Banko, et al, 2007; Durme & 
Pasca, 2008; Talukdar et al, 2008), semi-
structured web pages (Cafarella  et al, 2008; Shin-
zato & Torisawa, 2004), web search results (Geraci 
et al, 2006; Kozareva et al, 2008; Wang & Cohen, 
2009), and query logs (Pasca 2010). Our target for 
optimization in this paper is the approaches that 
use lexico-syntactic patterns to extract hyponymy 
relations from plain-text corpora. Our future work 
will study the application of the proposed algo-
rithms on other types of approaches. 
                                                          
2 http://research.microsoft.com/en-us/projects/needleseek/ or 
http://needleseek.msra.cn/  
The probabilistic evidence combination model 
that we exploit here was first proposed in (Shi et 
al., 2009), for combining the page in-link evidence 
in building a nonlinear static-rank computation 
algorithm. We applied it to the hyponymy extrac-
tion problem because the model takes the depend-
ency between supporting sentences into 
consideration and the resultant evidence fusion 
formulas are quite simple. In (Snow et al, 2006), a 
probabilistic model was adopted to combine evi-
dence from heterogeneous relationships to jointly 
optimize the relationships. The independence of 
evidence was assumed in their model. In compari-
son, we show that better results will be obtained if 
the evidence correlation is modeled appropriately. 
Our evidence propagation is basically about us-
ing term similarity information to help instance 
labeling. There have been several approaches 
which improve hyponymy extraction with instance 
clusters built by distributional similarity. In (Pantel 
& Ravichandran, 2004), labels were assigned to 
the committee (i.e., representative members) of a 
semantic class and used as the hypernyms of the 
whole class. Labels generated by their approach 
tend to be rather coarse-grained, excluding the pos-
sibility of a term having its private labels (consid-
ering the case that one meaning of a term is not 
covered by the input semantic classes). In contrast 
to their method, our label scoring and ranking ap-
proach is applied to every single term rather than a 
semantic class. In addition, we also compute label 
scores in a nonlinear way, which improves results 
quality. In Snow et al (2005), a supervised ap-
proach was proposed to improve hypernym classi-
fication using coordinate terms. In comparison, our 
approach is unsupervised. Durme & Pasca (2008) 
cleaned the set of instance-label pairs with a 
TF*IDF like method, by exploiting clusters of se-
mantically related phrases. The core idea is to keep 
a term-label pair (T, L) only if the number of terms 
having the label L in the term T?s cluster is above a 
threshold and if L is not the label of too many clus-
ters (otherwise the pair will be discarded). In con-
trast, we are able to add new (high-quality) labels 
for a term with our evidence propagation method. 
On the other hand, low quality labels get smaller 
score gains via propagation and are ranked lower. 
Label propagation is performed in (Talukdar et 
al., 2008; Talukdar & Pereira, 2010) based on mul-
tiple instance-label graphs. Term similarity infor-
mation was not used in their approach. 
1160
Most existing work tends to utilize small-scale 
or private corpora, whereas the corpus that we used 
is publicly available and much larger than most of 
the existing work. We published our term sets (re-
fer to Section 6.1) and their corresponding user 
judgments so researchers working on similar topics 
can reproduce our results. 
 
Type Pattern 
Hearst-I NPL {,} (such as) {NP,}
* {and|or} NP  
Hearst-II 
NPL {,} (include(s) | including) {NP,}
* 
{and|or} NP 
Hearst-III NPL {,} (e.g.|e.g) {NP,}
* {and|or} NP 
IsA-I NP (is|are|was|were|being) (a|an) NPL 
IsA-II NP (is|are|was|were|being) {the, those} NPL 
IsA-III NP (is|are|was|were|being) {another, any} NPL 
Table 1. Patterns adopted in this paper (NP: named 
phrase representing an entity; NPL: label) 
3 Preliminaries 
The problem addressed in this paper is corpus-
based is-a relation mining: extracting hypernyms 
(as labels) for entities from a large-scale, open-
domain document corpus. The desired output is a 
mapping from terms to their corresponding hyper-
nyms, which can naturally be represented as a 
weighted bipartite graph (term-label graph). Typi-
cally we are only interested in top labels of a term 
in the graph. 
Following existing efforts, we adopt pattern-
matching as a basic way of extracting hyper-
nymy/hyponymy relations. Two types of patterns 
(refer to Table 1) are employed, including the pop-
ular ?Hearst patterns? (Hearst, 1992) and the IsA 
patterns which are exploited less frequently in ex-
isting hyponym mining efforts. One or more term-
label pairs can be extracted if a pattern matches a 
sentence. In the baseline approach, the weight of 
an edge T?L (from term T to hypernym label L) in 
the term-label graph is computed as, 
 w(T?L)      ( )       
   
    ( )
 (3.1) 
where m is the number of times the pair (T, L) is 
extracted from the corpus, DF(L) is the number of 
in-links of L in the graph, N is total number of 
terms in the graph, and IDF means the ?inverse 
document frequency?. 
A term can only keep its top-k neighbors (ac-
cording to the edge weight) in the graph as its final 
labels. 
Our pattern matching algorithm implemented in 
this paper uses part-of-speech (POS) tagging in-
formation, without adopting a parser or a chunker. 
The noun phrase boundaries (for terms and labels) 
are determined by a manually designed POS tag 
list. 
4 Probabilistic Label-Scoring Model 
Here we model the hyponymy extraction problem 
from the probability theory point of view, aiming 
at estimating the score of a term-label pair (i.e., the 
score of a label w.r.t. a term) with probabilistic 
evidence combination. The model was studied in 
(Shi et al, 2009) to combine the page in-link evi-
dence in building a nonlinear static-rank computa-
tion algorithm. 
We represent the score of a term-label pair by 
the probability of the label being a correct hyper-
nym of the term, and define the following events, 
AT,L: Label L is a hypernym of term T (the ab-
breviated form A is used in this paper unless it is 
ambiguous). 
Ei: The observation that (T, L) is extracted from 
a sentence Si via pattern matching (i.e., Si is a sup-
porting sentence of the pair). 
Assuming that we already know m supporting 
sentences (S1~Sm), our problem is to compute 
P(A|E1,E2,..,Em), the posterior probability that L is 
a hypernym of term T, given evidence E1~Em. 
Formally, we need to find a function f to satisfy, 
 P(A|E1,?,Em) = f(P(A), P(A|E1)?, P(A|Em) ) (4.1) 
For simplicity, we first consider the case of 
m=2. The case of m>2 is quite similar. 
We start from the simple case of independent 
supporting sentences. That is, 
  (     )   (  )   (  ) (4.2) 
  (       )   (    )   (    ) (4.3) 
By applying Bayes rule, we get, 
 
 (       )  
 (       )   ( )
 (     )
 
          
 (    )   ( )
 (  )
 
 (    )   ( )
 (  )
 
 
 ( )
 
          
 (    )   (    )
 ( )
 
(4.4) 
Then define 
 (   )     
 (   )
 ( )
     ( (   ))     ( ( )) 
1161
Here G(A|E) represents the log-probability-gain 
of A given E, with the meaning of the gain in the 
log-probability value of A after the evidence E is 
observed (or known). It is a measure of the impact 
of evidence E to the probability of event A. With 
the definition of G(A|E), Formula 4.4 can be trans-
formed to, 
  (       )   (    )   (    ) (4.5) 
Therefore, if E1 and E2 are independent, the log-
probability-gain of A given both pieces of evidence 
will exactly be the sum of the gains of A given eve-
ry single piece of evidence respectively. It is easy 
to prove (by following a similar procedure) that the 
above Formula holds for the case of m>2, as long 
as the pieces of evidence are mutually independent. 
Therefore for a term-label pair with m mutually 
independent supporting sentences, if we set every 
gain G(A|Ei) to be a constant value g, the posterior 
gain score of the pair will be ?         . If the 
value g is the IDF of label L, the posterior gain will 
be, 
 G(AT,L|E1?,Em) ?    ( )
 
         ( ) (4.6) 
This is exactly the Formula 3.1. By this way, we 
provide a probabilistic explanation of scoring the 
candidate labels for a term via simple counting. 
 
 Hearst-I IsA-I 
E1: Hearst-I 
E2: IsA-I 
RA: 
 (      )
 (    ) (    )
  66.87 17.30 24.38 
R: 
 (    )
 (  ) (  )
  5997 1711 802.7 
RA/R 0.011 0.010 0.030 
Table 2. Evidence dependency estimation for intra-
pattern and inter-pattern supporting sentences 
In the above analysis, we assume the statistical 
independence of the supporting sentence observa-
tions, which may not hold in reality. Intuitively, if 
we already know one supporting sentence S1 for a 
term-label pair (T, L), then we have more chance to 
find another supporting sentence than if we do not 
know S1. The reason is that, before we find S1, we 
have to estimate the probability with the chance of 
discovering a supporting sentence for a random 
term-label pair. The probability is quite low be-
cause most term-label pairs do not have hyponymy 
relations. Once we have observed S1, however, the 
chance of (T, L) having a hyponymy relation in-
creases. Therefore the chance of observing another 
supporting sentence becomes larger than before. 
Table 2 shows the rough estimation of 
 (      )
 (    ) (    )
 (denoted as RA), 
 (    )
 (  ) (  )
 (denoted 
as R), and their ratios. The statistics are obtained 
by performing maximal likelihood estimation 
(MLE) upon our corpus and a random selection of 
term-label pairs from our term sets (see Section 
6.1) together with their top labels3. The data veri-
fies our analysis about the correlation between E1 
and E2 (note that R=1 means independent). In addi-
tion, it can be seen that the conditional independ-
ence assumption of Formula 4.3 does not hold 
(because RA>1). It is hence necessary to consider 
the correlation between supporting sentences in the 
model. The estimation of Table 2 also indicates 
that, 
 
 (     )
 (  ) (  )
 
 (       )
 (    ) (    )
 (4.7) 
By following a similar procedure as above, with 
Formulas 4.2 and 4.3 replaced by 4.7, we have, 
  (       )   (    )   (    ) (4.8) 
This formula indicates that when the supporting 
sentences are positively correlated, the posterior 
score of label L w.r.t. term T (given both the sen-
tences) is smaller than the sum of the gains caused 
by one sentence only. In the extreme case that sen-
tence S2 fully depends on E1 (i.e. P(E2|E1)=1), it is 
easy to prove that 
  (       )   (    )  
It is reasonable, since event E2 does not bring in 
more information than E1. 
Formula 4.8 cannot be used directly for compu-
ting the posterior gain. What we really need is a 
function h satisfying 
  (         )   ( (    )    (    )) (4.9) 
and 
  (      )  ?   
 
     (4.10) 
Shi et al (2009) discussed other constraints to h 
and suggested the following nonlinear functions, 
   (      )    (  ? ( 
    )    )  (4.11) 
                                                          
3 RA is estimated from the labels judged as ?Good?; whereas 
the estimation of R is from all judged labels. 
1162
   (      )  ??   
  
   
 
           (p>1) (4.12) 
In the next section, we use the above two h func-
tions as basic building blocks to compute label 
scores for terms. 
5 Our Approach 
Multiple types of patterns (Table 1) can be adopted 
to extract term-label pairs. For two supporting sen-
tences the correlation between them may depend 
on whether they correspond to the same pattern. In 
Section 5.1, our nonlinear evidence fusion formu-
las are constructed by making specific assumptions 
about the correlation between intra-pattern sup-
porting sentences and inter-pattern ones. 
Then in Section 5.2, we introduce our evidence 
propagation technique in which the evidence of a 
(T, L) pair is propagated to the terms similar to T. 
5.1 Nonlinear evidence fusion 
For a term-label pair (T, L), assuming K patterns 
are used for hyponymy extraction and the support-
ing sentences discovered with pattern i are, 
                  (5.1) 
where mi is the number of supporting sentences 
corresponding to pattern i. Also assume the gain 
score of Si,j is xi,j, i.e., xi,j=G(A|Si,j). 
Generally speaking, supporting sentences corre-
sponding to the same pattern typically have a high-
er correlation than the sentences corresponding to 
different patterns. This can be verified by the data 
in Table-2. By ignoring the inter-pattern correla-
tions, we make the following simplified assump-
tion: 
Assumption: Supporting sentences correspond-
ing to the same pattern are correlated, while those 
of different patterns are independent. 
According to this assumption, our label-scoring 
function is, 
      (   )  ? (               )
 
   
 (5.2) 
In the simple case that         ( ) , if the h 
function of Formula 4.12 is adopted, then, 
      (   )  (? ?  
 
 
   
)     ( ) (5.3) 
We use an example to illustrate the above for-
mula. 
Example: For term T and label L1, assume the 
numbers of the supporting sentences corresponding 
to the six pattern types in Table 1 are (4, 4, 4, 4, 4, 
4), which means the number of supporting sen-
tences discovered by each pattern type is 4. Also 
assume the supporting-sentence-count vector of 
label L2 is (25, 0, 0, 0, 0, 0). If we use Formula 5.3 
to compute the scores of L1 and L2, we can have 
the following (ignoring IDF for simplicity), 
Score(L1)   ?    ; Score(L2) ?     
One the other hand, if we simply count the total 
number of supporting sentences, the score of L2 
will be larger. 
The rationale implied in the formula is: For a 
given term T, the labels supported by multiple 
types of patterns tend to be more reliable than 
those supported by a single pattern type, if they 
have the same number of supporting sentences. 
5.2 Evidence propagation 
According to the evidence fusion algorithm de-
scribed above, in order to extract term labels relia-
bly, it is desirable to have many supporting 
sentences of different types. This is a big challenge 
for rare terms, due to their low frequency in sen-
tences (and even lower frequency in supporting 
sentences because not all occurrences can be cov-
ered by patterns). With evidence propagation, we 
aim at discovering more supporting sentences for 
terms (especially rare terms). Evidence propaga-
tion is motivated by the following two observa-
tions: 
(I) Similar entities or coordinate terms tend to 
share some common hypernyms. 
(II) Large term similarity graphs are able to be 
built efficiently with state-of-the-art techniques 
(Agirre et al, 2009; Pantel et al, 2009; Shi et al, 
2010). With the graphs, we can obtain the similari-
ty between two terms without their hypernyms be-
ing available. 
The first observation motivates us to ?borrow? 
the supporting sentences from other terms as auxil-
iary evidence of the term. The second observation 
means that new information is brought with the 
state-of-the-art term similarity graphs (in addition 
to the term-label information discovered with the 
patterns of Table 1). 
1163
Our evidence propagation algorithm contains 
two phases. In phase I, some pseudo supporting 
sentences are constructed for a term from the sup-
porting sentences of its neighbors in the similarity 
graph. Then we calculate the label scores for terms 
based on their (pseudo and real) supporting sen-
tences. 
Phase I: For every supporting sentence S and 
every similar term T1 of the term T, add a pseudo 
supporting sentence S1 for T1, with the gain score, 
  (         )       (    )   (      ) (5.5) 
where         is the propagation factor, and 
   (   ) is the term similarity function taking val-
ues in [0, 1]. The formula reasonably assumes that 
the gain score of the pseudo supporting sentence 
depends on the gain score of the original real sup-
porting sentence, the similarity between the two 
terms, and the propagation factor. 
Phase II: The nonlinear evidence combination 
formulas in the previous subsection are adopted to 
combine the evidence of pseudo supporting sen-
tences. 
Term similarity graphs can be obtained by dis-
tributional similarity or patterns (Agirre et al, 
2009; Pantel et al, 2009; Shi et al, 2010). We call 
the first type of graph DS and the second type PB. 
DS approaches are based on the distributional hy-
pothesis (Harris, 1985), which says that terms ap-
pearing in analogous contexts tend to be similar. In 
a DS approach, a term is represented by a feature 
vector, with each feature corresponding to a con-
text in which the term appears. The similarity be-
tween two terms is computed as the similarity 
between their corresponding feature vectors. In PB 
approaches, a list of carefully-designed (or auto-
matically learned) patterns is exploited and applied 
to a text collection, with the hypothesis that the 
terms extracted by applying each of the patterns to 
a specific piece of text tend to be similar. Two cat-
egories of patterns have been studied in the litera-
ture (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009): sentence lexical patterns, 
and HTML tag patterns. An example of sentence 
lexical patterns is ?T {, T}*{,} (and|or) T?. HTML 
tag patterns include HTML tables, drop-down lists, 
and other tag repeat patterns. In this paper, we 
generate the DS and PB graphs by adopting the 
best-performed methods studied in (Shi et al, 
2010). We will compare, by experiments, the prop-
agation performance of utilizing the two categories 
of graphs, and also investigate the performance of 
utilizing both graphs for evidence propagation. 
6 Experiments 
6.1 Experimental setup 
Corpus We adopt a publicly available dataset in 
our experiments: ClueWeb094. This is a very large 
dataset collected by Carnegie Mellon University in 
early 2009 and has been used by several tracks of 
the Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: First, 
it is a corpus large enough for conducting web-
scale experiments and getting meaningful results. 
Second, since it is publicly available, it is possible 
for other researchers to reproduce the experiments 
in this paper. 
Term sets Approaches are evaluated by using 
two sets of selected terms: Wiki200, and Ext100. 
For every term in the term sets, each approach 
generates a list of hypernym labels, which are 
manually judged by human annotators. Wiki200 is 
constructed by first randomly selecting 400 Wik-
ipedia6 titles as our candidate terms, with the prob-
ability of a title T being selected to be     (  
 ( )), where F(T) is the frequency of T in our data 
corpus. The reason of adopting such a probability 
formula is to balance popular terms and rare ones 
in our term set. Then 200 terms are manually se-
lected from the 400 candidate terms, with the prin-
ciple of maximizing the diversity of terms in terms 
of length (i.e., number of words) and type (person, 
location, organization, software, movie, song, ani-
mal, plant, etc.). Wiki200 is further divided into 
two subsets: Wiki100H and Wiki100L, containing 
respectively the 100 high-frequency and low-
frequency terms. Ext100 is built by first selecting 
200 non-Wikipedia-title terms at random from the 
term-label graph generated by the baseline ap-
proach (Formula 3.1), then manually selecting 100 
terms. 
Some sample terms in the term sets are listed in 
Table 3. 
 
                                                          
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
6 http://www.wikipedia.org/  
1164
Term 
Set 
Sample Terms 
Wiki200 
Canon EOS 400D, Disease management, El Sal-
vador, Excellus Blue Cross Blue Shield, F33, 
Glasstron, Indium, Khandala, Kung Fu, Lake 
Greenwood, Le Gris, Liriope, Lionel Barrymore, 
Milk, Mount Alto, Northern Wei, Pink Lady, 
Shawshank, The Dog Island, White flight, World 
War II? 
Ext100 
A2B, Antique gold, GPTEngine, Jinjiang Inn, 
Moyea SWF to Apple TV Converter, Nanny ser-
vice, Outdoor living, Plasmid DNA, Popon, Spam 
detection, Taylor Ho Bynum, Villa Michelle? 
Table 3. Sample terms in our term sets 
 
Annotation For each term in the term set, the 
top-5 results (i.e., hypernym labels) of various 
methods are mixed and judged by human annota-
tors. Each annotator assigns each result item a 
judgment of ?Good?, ?Fair? or ?Bad?. The annota-
tors do not know the method by which a result item 
is generated. Six annotators participated in the la-
beling with a rough speed of 15 minutes per term. 
We also encourage the annotators to add new good 
results which are not discovered by any method. 
The term sets and their corresponding user anno-
tations are available for download at the following 
links (dataset ID=data.queryset.semcat01): 
http://research.microsoft.com/en-us/projects/needleseek/ 
http://needleseek.msra.cn/datasets/ 
Evaluation We adopt the following metrics to 
evaluate the hypernym list of a term generated by 
each method. The evaluation score on a term set is 
the average over all the terms. 
Precision@k: The percentage of relevant (good 
or fair) labels in the top-k results (labels judged as 
?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant labels in the top-
k results to the total number of relevant labels 
R-Precision: Precision@R where R is the total 
number of labels judged as ?Good? 
Mean average precision (MAP): The average of 
precision values at the positions of all good or fair 
results 
Before annotation and evaluation, the hypernym 
list generated by each method for each term is pre-
processed to remove duplicate items. Two hyper-
nyms are called duplicate items if they share the 
same head word (e.g., ?military conflict? and ?con-
flict?). For duplicate hypernyms, only the first (i.e., 
the highest ranked one) in the list is kept. The goal 
with such a preprocessing step is to partially con-
sider results diversity in evaluation and to make a 
more meaningful comparison among different 
methods. Consider two hypernym lists for ?sub-
way?: 
List-1: restaurant; chain restaurant; worldwide chain 
restaurant; franchise; restaurant franchise? 
List-2: restaurant; franchise; transportation; company; 
fast food? 
There are more detailed hypernyms in the first 
list about ?subway? as a restaurant or a franchise; 
while the second list covers a broader range of 
meanings for the term. It is hard to say which is 
better (without considering the upper-layer appli-
cations). With this preprocessing step, we keep our 
focus on short hypernyms rather than detailed ones. 
 
Term Set Method MAP R-Prec P@1 P@5 
Wiki200 
Linear 0.357 0.376 0.783 0.547 
Log 
0.371 
 3.92% 
0.384 
 2.13% 
0.803 
 2.55% 
0.561 
 2.56% 
PNorm 
0.372 
 4.20% 
0.384 
 2.13% 
0.800 
 2.17% 
0.562 
 2.74% 
Wiki100H 
Linear 0.363 0.382 0.805 0.627 
Log 
0.393 
 8.26% 
0.402 
 5.24% 
0.845 
 4.97% 
0.660 
 5.26% 
PNorm 
0.395 
 8.82% 
0.403 
 5.50% 
0.840 
 4.35% 
0.662 
 5.28% 
Table 4. Performance comparison among various 
evidence fusion methods (Term sets: Wiki200 and 
Wiki100H; p=2 for PNorm) 
6.2 Experimental results 
We first compare the evaluation results of different 
evidence fusion methods mentioned in Section 4.1. 
In Table 4, Linear means that Formula 3.1 is used 
to calculate label scores, whereas Log and PNorm 
represent our nonlinear approach with Formulas 
4.11 and 4.12 being utilized. The performance im-
provement numbers shown in the table are based 
on the linear version; and the upward pointing ar-
rows indicate relative percentage improvement 
over the baseline. From the table, we can see that 
the nonlinear methods outperform the linear ones 
on the Wiki200 term set. It is interesting to note 
that the performance improvement is more signifi-
cant on Wiki100H, the set of high frequency terms. 
By examining the labels and supporting sentences 
for the terms in each term set, we find that for 
many low-frequency terms (in Wiki100L), there 
are only a few supporting sentences (corresponding 
1165
to one or two patterns). So the scores computed by 
various fusion algorithms tend to be similar. In 
contrast, more supporting sentences can be discov-
ered for high-frequency terms. Much information 
is contained in the sentences about the hypernyms 
of the high-frequency terms, but the linear function 
of Formula 3.1 fails to make effective use of it. 
The two nonlinear methods achieve better perfor-
mance by appropriately modeling the dependency 
between supporting sentences and computing the 
log-probability gain in a better way. 
The comparison of the linear and nonlinear 
methods on the Ext100 term set is shown in Table 
5. Please note that the terms in Ext100 do not ap-
pear in Wikipedia titles. Thanks to the scale of the 
data corpus we are using, even the baseline ap-
proach achieves reasonably good performance. 
Please note that the terms (refer to Table 3) we are 
using are ?harder? than those adopted for evalua-
tion in many existing papers. Again, the results 
quality is improved with the nonlinear methods, 
although the performance improvement is not big 
due to the reason that most terms in Ext100 are 
rare. Please note that the recall (R@1, R@5) in this 
paper is pseudo-recall, i.e., we treat the number of 
known relevant (Good or Fair) results as the total 
number of relevant ones. 
 
Method MAP R-Prec P@1 P@5 R@1 R@5 
Linear 0.384 0.429 0.665 0.472 0.116 0.385 
Log 
0.395 0.429 0.715 0.472 0.125 0.385 
 2.86%  0%  7.52%  0%  7.76%  0% 
PNorm 
0.390 0.429 0.700 0.472 0.120 0.385 
 1.56%  0%   5.26%  0%  3.45%  0% 
Table 5. Performance comparison among various 
evidence fusion methods (Term set: Ext100; p=2 
for PNorm) 
The parameter p in the PNorm method is related 
to the degree of correlations among supporting 
sentences. The linear method of Formula 3.1 corre-
sponds to the special case of p=1; while p=  rep-
resents the case that other supporting sentences are 
fully correlated to the supporting sentence with the 
maximal log-probability gain. Figure 1 shows that, 
for most of the term sets, the best performance is 
obtained for   [2.0, 4.0]. The reason may be that 
the sentence correlations are better estimated with 
p values in this range. 
 
 
Figure 1. Performance curves of PNorm with dif-
ferent parameter values (Measure: MAP) 
The experimental results of evidence propaga-
tion are shown in Table 6. The methods for com-
parison are, 
Base: The linear function without propagation. 
NL: Nonlinear evidence fusion (PNorm with 
p=2) without propagation. 
LP: Linear propagation, i.e., the linear function 
is used to combine the evidence of pseudo support-
ing sentences. 
NLP: Nonlinear propagation where PNorm 
(p=2) is used to combine the pseudo supporting 
sentences. 
NL+NLP: The nonlinear method is used to 
combine both supporting sentences and pseudo 
supporting sentences. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL 
0.372 0.384 0.800 0.562 0.325 
 4.20%  2.13%  2.17%  2.74%  2.52% 
LP 
0.357 0.376 0.783 0.547 0.317 
 0%  0%  0%  0%  0% 
NLP 
0.396 0.418 0.785 0.605 0.357 
 10.9%  11.2%  0.26%  10.6%  12.6% 
NL+NLP 
0.447 0.461 0.840 0.667 0.404 
 25.2%  22.6%  7.28%  21.9%  27.4% 
Table 6. Evidence propagation results (Term set: 
Wiki200; Similarity graph: PB; Nonlinear formula: 
PNorm) 
In this paper, we generate the DS (distributional 
similarity) and PB (pattern-based) graphs by adopt-
ing the best-performed methods studied in (Shi et 
al., 2010). The performance improvement numbers 
(indicated by the upward pointing arrows) shown 
in tables 6~9 are relative percentage improvement 
1166
over the base approach (i.e., linear function with-
out propagation). The values of parameter   are set 
to maximize the MAP values. 
Several observations can be made from Table 6. 
First, no performance improvement can be ob-
tained with the linear propagation method (LP), 
while the nonlinear propagation algorithm (NLP) 
works quite well in improving both precision and 
recall. The results demonstrate the high correlation 
between pseudo supporting sentences and the great 
potential of using term similarity to improve hy-
pernymy extraction. The second observation is that 
the NL+NLP approach achieves a much larger per-
formance improvement than NL and NLP. Similar 
results (omitted due to space limitation) can be 
observed on the Ext100 term set. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL+NLP 
(PB) 
0.415 0.439 0.830 0.633 0.379 
 16.2%  16.8%  6.00%  15.7%  19.6% 
NL+NLP 
(DS) 
0.456 0.469 0.843 0.673 0.406 
 27.7%  24.7%  7.66%  23.0%  28.1% 
NL+NLP
(PB+DS) 
0.473 0.487 0.860 0.700 0.434 
 32.5%  29.5%  9.83%  28.0%  36.9% 
Table 7. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki200; Nonlin-
ear formula: Log) 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.351 0.370 0.760 0.467 0.317 
NL+NLP 
(PB) 
0.411 0.448 0.770 0.564 0.401 
?17.1% ?21.1% ?1.32% ?20.8% ?26.5% 
NL+NLP 
(DS) 
0.469 0.490 0.815 0.622 0.438 
 33.6%  32.4%  7.24%  33.2%  38.2% 
NL+NLP
(PB+DS) 
0.491 0.513 0.860 0.654 0.479 
 39.9%  38.6%  13.2%  40.0%  51.1% 
Table 8. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki100L) 
Now let us study whether it is possible to com-
bine the PB and DS graphs to obtain better results. 
As shown in Tables 7, 8, and 9 (for term sets 
Wiki200, Wiki100L, and Ext100 respectively, us-
ing the Log formula for fusion and propagation), 
utilizing both graphs really yields additional per-
formance gains. We explain this by the fact that the 
information in the two term similarity graphs tends 
to be complimentary. The performance improve-
ment over Wiki100L is especially remarkable. This 
is reasonable because rare terms do not have ade-
quate information in their supporting sentences due 
to data sparseness. As a result, they benefit the 
most from the pseudo supporting sentences propa-
gated with the similarity graphs. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.384 0.429 0.665 0.472 0.385 
NL+NLP 
(PB) 
0.454 0.479 0.745 0.550 0.456 
 18.3%  11.7%  12.0%  16.5%  18.4% 
NL+NLP 
(DS) 
0.404 0.441 0.720 0.486 0.402 
 5.18%  2.66%  8.27%  2.97%  4.37% 
NL+NLP(P
B+DS) 
0.483 0.518 0.760 0.586 0.492 
 26.0%  20.6%  14.3%  24.2%  27.6% 
Table 9. Combination of PB and DS graphs for 
evidence propagation (Term set: Ext100) 
7 Conclusion 
We demonstrated that the way of aggregating sup-
porting sentences has considerable impact on re-
sults quality of the hyponym extraction task using 
lexico-syntactic patterns, and the widely-used 
counting method is not optimal. We applied a se-
ries of nonlinear evidence fusion formulas to the 
problem and saw noticeable performance im-
provement. The data quality is improved further 
with the combination of nonlinear evidence fusion 
and evidence propagation. We also introduced a 
new evaluation corpus with annotated hypernym 
labels for 300 terms, which were shared with the 
research community. 
Acknowledgments 
We would like to thank Matt Callcut for reading 
through the paper. Thanks to the annotators for 
their efforts in judging the hypernym labels. 
Thanks to Yueguo Chen, Siyu Lei, and the anony-
mous reviewers for their helpful comments and 
suggestions. The first author is partially supported 
by the NSF of China (60903028,61070014), and 
Key Projects in the Tianjin Science and Technolo-
gy Pillar Program. 
 
 
 
 
1167
References  
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas-
ca, and A. Soroa. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-based 
Approaches. In Proc. of NAACL-HLT?2009. 
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proc. of IJCAI?2007. 
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. 
Zhang. 2008. WebTables: Exploring the Power of 
Tables on the Web. In Proceedings of the 34th Con-
ference on Very Large Data Bases (VLDB?2008), 
pages 538?549, Auckland, New Zealand. 
B. Van Durme and M. Pasca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of 
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence. 
F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. 
2006. Cluster Generation and Cluster Labelling for 
Web Snippets: A Fast and Accurate Hierarchical So-
lution. In Proceedings of the 13th Conference on 
String Processing and Information Retrieval 
(SPIRE?2006), pages 25?36, Glasgow, Scotland. 
Z. S. Harris. 1985. Distributional Structure. The Philos-
ophy of Linguistics. New York: Oxford University 
Press. 
M. Hearst. 1992. Automatic Acquisition of Hyponyms 
from Large Text Corpora. In Fourteenth International 
Conference on Computational Linguistics, Nantes, 
France. 
Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic 
Class Learning from the Web with Hyponym Pattern 
Linkage Graphs. In Proc. of ACL'2008. 
P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and 
V. Vyas. 2009. Web-Scale Distributional Similarity 
and Entity Set Expansion. EMNLP?2009. Singapore. 
P. Pantel and D. Ravichandran. 2004. Automatically 
Labeling Semantic Classes. In Proc. of the 2004 Hu-
man Language Technology Conference (HLT-
NAACL?2004), 321?328. 
M. Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search. In Proc. of CIKM?2004. 
M. Pasca. 2010. The Role of Queries in Ranking La-
beled Instances Extracted from Text. In Proc. of 
COLING?2010, Beijing, China. 
S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear 
Static-Rank Computation. In Proc. of CIKM?2009, 
Kong Kong. 
S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpus-
based Semantic Class Mining: Distributional vs. Pat-
tern-Based Approaches. In Proc. of COLING?2010, 
Beijing, China. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypon-
ymy Relations from Web Documents. In Proc. of the 
2004 Human Language Technology Conference 
(HLT-NAACL?2004). 
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Discov-
ery. In Proceedings of the 19th Conference on Neural 
Information Processing Systems. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic 
Taxonomy Induction from Heterogenous Evidence. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(COLING-ACL-06), 801?808. 
P. P. Talukdar and F. Pereira. 2010. Experiments in 
Graph-based Semi-Supervised Learning Methods for 
Class-Instance Acquisition. In 48th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?2010). 
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, 
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised 
Acquisition of Labeled Class Instances using Graph 
Random Walks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP?2008), pages 581?589. 
R.C. Wang. W.W. Cohen. Automatic Set Instance Ex-
traction using the Web. In Proc. of the 47th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP?2009), pages 441?449, Sin-
gapore. 
H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Em-
ploying Topic Models for Pattern-based Semantic 
Class Discovery. In Proc. of the 47th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL-IJCNLP?2009), pages 441?449, Singapore. 
 
1168
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 380?390,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Collective Tweet Wikification based on Semi-supervised Graph
Regularization
Hongzhao Huang
1
, Yunbo Cao
2
, Xiaojiang Huang
2
, Heng Ji
1
, Chin-Yew Lin
2
1
Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
2
Microsoft Research Asia, Beijing 100080, P.R.China
{huangh9,jih}@rpi.edu
1
,
{yunbo.cao,xiaojih,cyl}@microsoft.com
2
Abstract
Wikification for tweets aims to automat-
ically identify each concept mention in a
tweet and link it to a concept referent in
a knowledge base (e.g., Wikipedia). Due
to the shortness of a tweet, a collective
inference model incorporating global ev-
idence from multiple mentions and con-
cepts is more appropriate than a non-
collecitve approach which links each men-
tion at a time. In addition, it is chal-
lenging to generate sufficient high quality
labeled data for supervised models with
low cost. To tackle these challenges, we
propose a novel semi-supervised graph
regularization model to incorporate both
local and global evidence from multi-
ple tweets through three fine-grained re-
lations. In order to identify semantically-
related mentions for collective inference,
we detect meta path-based semantic rela-
tions through social networks. Compared
to the state-of-the-art supervised model
trained from 100% labeled data, our pro-
posed approach achieves comparable per-
formance with 31% labeled data and ob-
tains 5% absolute F1 gain with 50% la-
beled data.
1 Introduction
With millions of tweets posted daily, Twitter en-
ables both individuals and organizations to dis-
seminate information, from current affairs to
breaking news in a timely fashion. In this
work, we study the wikification (Disambiguation
to Wikipedia) task (Mihalcea and Csomai, 2007)
for tweets, which aims to automatically identify
each concept mention in a tweet, and link it to a
concept referent in a knowledge base (KB) (e.g.,
Wikipedia). For example, as shown in Figure 1,
Hawks is an identified mention, and its correct ref-
erent concept in Wikipedia is Atlanta Hawks. An
end-to-end wikification system needs to solve two
sub-problems: (i) concept mention detection, (ii)
concept mention disambiguation.
Wikification is a particularly useful task for
short messages such as tweets because it allows
a reader to easily grasp the related topics and en-
riched information from the KB. From a system-
to-system perspective, wikification has demon-
strated its usefulness in a variety of applica-
tions, including coreference resolution (Ratinov
and Roth, 2012) and classification (Vitale et al,
2012).
Sufficient labeled data is crucial for supervised
models. However, manual wikification annota-
tion for short documents is challenging and time-
consuming (Cassidy et al, 2012). The challenges
are: (i) unlinkability, a valid concept may not ex-
ist in the KB. (ii) ambiguity, it is impossible to
determine the correct concept due to the dearth
of information within a single tweet or multiple
correct answer. For instance, it would be diffi-
cult to determine the correct referent concept for
?Gators? in t
1
in Figure 1. Linking ?UCONN?
in t
3
to University of Connecticut may also be ac-
ceptable since Connecticut Huskies is the athletic
team of the university. (iii) prominence, it is chal-
lenging to select a set of linkable mentions that
are important and relevant. It is not tricky to select
?Fans?, ?slump?, and ?Hawks? as linkable men-
tions, but other mentions such as ?stay up? and
?stay positive? are not prominent. Therefore, it
is challenging to create sufficient high quality la-
beled tweets for supervised models and worth con-
sidering semi-supervised learning with the explo-
ration of unlabeled data.
380
    
Stay up Hawk Fans. We are going 
through a slump now, but we have to 
stay positive. Go Hawks!
Congrats to UCONN and Kemba Walker. 
5 wins in 5 days, very impressive...
Just getting to the Arena, we play the 
Bucks tonight. Let's get it!
Fan (person); Mechanical fan
Slump (geology);  Slump (sports)
Atlanta Hawks;  Hawks (film)
University of Connecticut; Connecticut Huskies
Kemba Walker
Arena; Arena (magazine); Arena (TV series)
Bucks County, Pennsylvania; Milwaukee Bucks
Tweets Concept Candidates
Go Gators!!! Florida Gators football; Florida Gators men's basketballt1
t2
t3
t4
Figure 1: An illustration of Wikification Task for Tweets. Concept mentions detected in tweets are
marked as bold, and correctly linked concepts are underlined. The concept candidates are ranked by
their prior popularity which will be explained in section 4.1, and only top 2 ranked concepts are listed.
However, when selecting semi-supervised
learning frameworks, we noticed another unique
challenge that tweets pose to wikification due
to their informal writing style, shortness and
noisiness. The context of a single tweet usually
cannot provide enough information for prominent
mention detection and similarity computing for
disambiguation. Therefore, a collective inference
model over multiple tweets in the semi-supervised
setting is desirable. For instance, the four tweets
in Figure 1 are posted by the same author within
a short time period. If we perform collective
inference over them we can reliably link am-
biguous mentions such as ?Gators?, ?Hawks?,
and ?Bucks? to basketball teams instead of other
concepts such as the county Bucks County.
In order to address these unique challenges
for wikification for the short tweets, we employ
graph-based semi-supervised learning algorithms
(Zhu et al, 2003; Smola and Kondor, 2003; Blum
et al, 2004; Zhou et al, 2004; Talukdar and
Crammer, 2009) for collective inference by ex-
ploiting the manifold (cluster) structure in both
unlabeled and labeled data. These approaches
normally assume label smoothness over a defined
graph, where the nodes represent a set of labeled
and unlabeled instances, and the weighted edges
reflect the closeness of each pair of instances. In
order to construct a semantic-rich graph capturing
the similarity between mentions and concepts for
the model, we introduce three novel fine-grained
relations based on a set of local features, social
networks and meta paths.
The main contributions of this paper are sum-
marized as follows:
? To the best of our knowledge, this is the first
effort to explore graph-based semi-supervised
learning algorithms for the wikification task.
? We propose a novel semi-supervised graph reg-
ularization model performing collective infer-
ence for joint mention detection and disam-
biguation. Our approach takes advantage of
three proposed principles to incorporate both lo-
cal and global evidence from multiple tweets.
? We propose a meta path-based unified frame-
work to detect both explicitly and implicitly rel-
evant mentions.
2 Preliminaries
Concept and Concept Mention We define a con-
cept c as a Wikipedia article (e.g., Atlanta Hawks),
and a concept mentionm as an n-gram from a spe-
cific tweet. Each concept has a set of textual repre-
sentation fields (Meij et al, 2012), including title
(the title of the article), sentence (the first sentence
of the article), paragraph (the first paragraph of
the article), content (the entire content of the arti-
cle), and anchor (the set of all anchor texts with
incoming links to the article).
Wikipedia Lexicon Construction We first
construct an offline lexicon with each entry as
?m, {c
1
, ..., c
k
}?, where {c
1
, ..., c
k
} is the set of
possible referent concepts for the mention m.
Following the previous work (Bunescu, 2006;
Cucerzan, 2007; Hachey et al, 2013), we extract
the possible mentions for a given concept c using
the following resources: the title of c; the aliases
appearing in the introduction and infoboxes of c
(e.g., The Evergreen State is an alias of Wash-
ington state); the titles of pages redirecting to c
(e.g., State of Washington is a redirecting page of
Washington (state)); the titles of the disambigua-
381
tion pages containing c; and all the anchor texts
appearing in at least 5 pages with hyperlinks to c
(e.g., WA is a mention for the concept Washing-
ton (state) in the text ?401 5th Ave N [[Seattle]],
[[Washington (state)?WA]] 98109 USA?. We also
propose three heuristic rules to extract mentions
(i.e., different combinations of the family name
and given name for a person, the headquarters of
an organization, and the city name for a sports
team).
Concept Mention Extraction Based on the
constructed lexicon, we then consider all n-grams
of size? n (n=7 in this paper) as concept mention
candidates if their entries in the lexicon are not
empty. We first segment @usernames and #hash-
tags into regular tokens (e.g., @amandapalmer is
segmented as amanda palmer and #WorldWater-
Day is split as World Water Day) using the ap-
proach proposed by (Wang et al, 2011). Segmen-
tation assists finding concept candidates for these
non-regular mentions.
3 Principles and Approach Overview
    
R elational Graph Construction
Knowledge Base 
(Wikipedia)
Labeled and 
Unlabeled Tweets
Wikipedia Lex icon Construction
Concept Mention and 
Concept Candidate E x traction
Local Compatibility
(local features, 
cosine similarity)
Coreference
(meta path,
mention 
similarity)
Semantic R elatedness
(meta path, concept 
semantic relatedness)
Semi- Supervised Graph R egularization
<Mention, Concept>
Pairs
Figure 2: Approach Overview.
3.1 Principles
A single tweet may not provide enough evidence
to identify prominent mentions and infer their cor-
rect referent concepts due to the lack of contextual
information. To tackle this problem, we propose to
incorporate global evidence from multiple tweets
and performing collective inference for both men-
tion identification and disambiguation. We first in-
troduce the following three principles that our ap-
proach relies on.
Principle 1 (Local compatibility): Two pairs
of ?m, c? with strong local compatibility tend to
have similar labels. Mentions and their correct
referent concepts usually tend to share a set of
characteristics such as string similarity betweenm
and c (e.g., ?Chicago, Chicago? and ?Facebook,
Facebook?). We define the local compatibility to
model such set of characteristics.
Principle 2 (Coreference): Two coreferential
mentions should be linked to the same concept.
For example, if we know ?nc? and ?North Car-
olina? are coreferential, then they should both be
linked to North Carolina.
Principle 3 (Semantic Relatedness): Two
highly semantically-related mentions are more
likely to be linked to two highly semantically-
related concepts. For instance, when ?Sweet 16?
and ?Hawks? often appear together within rel-
evant contexts, they can be reliably linked to
two baseketball-related concepts NCAA Men?s Di-
vision I Basketball Championship and Atlanta
Hawks, respectively.
3.2 Approach Overview
Given a set of tweets ?t
1
, ..., t
|T |
?, our system first
generates a set of candidate concept mentions, and
then extracts a set of candidate concept referents
for each mention based on the Wikipedia lexicon.
Given a pair of mention and its candidate referent
concept ?m, c?, the remaining task of wikification
is to assign either a positive label if m should be
selected as a prominently linkable mention and c
is its correct referent concept, or otherwise a neg-
ative label. The label assignment is obtained by
our semi-supervised graph regularization frame-
work based on a relational graph, which is con-
structed from local compatibility, coreference, and
semantic relatedness relations. The overview of
our approach is as illustrated in Figure 2.
4 Relational Graph Construction
We first construct the relational graphG = ?V,E?,
where V = {v
1
, ..., v
n
} is a set of nodes and E =
{e
1
, ..., e
m
} is a set of edges. Each v
i
= ?m
i
, c
i
?
represents a tuple of mention m
i
and its referent
concept candidate c
i
. An edge is added between
two nodes v
i
and v
j
if there is a proposed rela-
tion based on the three principles described in sec-
tion 3.1.
4.1 Local Compatibility
We first compute local compatibility (Principle 1)
by considering a set of novel local features to cap-
382
ture the importance and relevance of a mention m
to a tweet t, as well as the correctness of its link-
age to a concept c. We have designed a number
of features which are similar to those commonly
used in wikification and entity linking work (Meij
et al, 2012; Guo et al, 2013; Mihalcea and Cso-
mai, 2007).
Mention Features We define the following fea-
tures based on information from mentions.
? IDF
f
(m) = log(
|C|
df(m)
), where |C| is the total
number of concepts in Wikipedia and df(m) is
the total number of concepts in whichm occurs,
and f indicates the field property, including ti-
tle, content, and anchor.
? Keyphraseness(m) =
|C
a
(m)|
df(m)
to measure
how likely m is used as an anchor in Wikipedia,
where C
a
(m) is the set of concepts where m
appears as an anchor.
? LinkProb(m) =
?
c?C
a
(m)
count(m,c)
?
c?C
count(m,c)
, where
count(m, c) indicates the number of occurrence
of m in c.
? SNIL(m) and SNCL(m) to count the number
of concepts that are equal to or contain a sub-n-
gram of m, respectively (Meij et al, 2012).
Concept Features The concept features are
solely based on Wikipedia, including the number
of incoming and outgoing links for c, and the num-
ber of words and characters in c.
Mention + Concept Features This set of fea-
tures considers information from both mentions
and concepts:
? prior popularity prior(m, c) =
count(m,c)?
c
?
count(m,c
?
)
, where count(m, c) mea-
sures the frequency of the anchor links from m
to c in Wikipedia.
? TF
f
(m, c) =
count
f
(m,c)
|f |
to measure the rela-
tive frequency of m in each field representation
f of c, normalized by the length of f . The fields
include title, sentence, paragraph, content and
anchor.
? NCT (m, c), TCN(m, c), and TEN(m, c) to
measure whether m contains the title of c,
whether the title of c contains m, and whether
m equals to the title of c, respectively.
Context Features This set of features include
(i) Context Capitalization features, which indicate
whether the current mention, the token before, and
the token after are capitalized. (ii) tf-idf based fea-
tures, which include the dot product of two word
vectors v
c
and v
t
, and the average tf-idf value of
common items in v
c
and v
t
, where v
c
and v
t
are
the top 100 tf-idf word vectors in c and t.
Local Compatibility Computation For each
node v
i
= ?m
i
, c
i
?, we collect its local features
as a feature vector F
i
= ?f
1
, f
2
, ..., f
d
?. To avoid
features with large numerical values that domi-
nate other features, the value of each feature is
re-scaled using feature standardization approach.
The cosine similarity is then adopted to compute
the local compatibility of two nodes and construct
a k nearest neighbor (kNN) graph, where each
node is connected to its k nearest neighboring
nodes. We compute the weight matrix that rep-
resents the local compatibility relation as:
W
loc
ij
=
{
cosine(F
i
, F
j
) j ? kNN(i)
0 Otherwise
4.2 Meta Path
    
Mention
Hashtag
Tweet User
post - 1
post
contain - 1
contain
contain - 1 contain
Figure 3: Schema of the Twitter network.
In this subsection, we introduce the concept
meta path which will be used to detect corefer-
ence (section 4.3) and semantic relatedness rela-
tions (section 4.4).
A meta-path is a path defined over a network
and composed of a sequence of relations between
different object types (Sun et al, 2011b). In our
experimental setting, we can construct a natu-
ral Twitter network summarized by the network
schema in Figure 3. The network contains four
types of objects: Mention (M), Tweet (T), User
(U), and Hashtag (H). Tweets and mentions are
connected by links ?contain? and ?contained by?
(denoted as ?contain
?1
?); and other linked rela-
tionships can be described similarly.
We then define the following five types of meta
paths to connect two mentions as:
? ?M - T - M?,
? ?M - T - U - T - M?,
? ?M - T - H - T - M?,
? ?M - T - U - T - M - T - H - T - M?,
? ?M - T - H - T - M - T - U - T - M?.
383
Each meta path represents one particular seman-
tic relation. For instance, the first three paths are
basic ones expressing the explicit relations that
two mentions are from the same tweet, posted by
the same user, and share the same #hashtag, re-
spectively. The last two paths are concatenated
ones which are constructed by concatenating the
first three simple paths to express the implicit rela-
tions that two mentions co-occur with a third men-
tion sharing either the same authorship or #hash-
tag. Such complicated paths can be exploited to
detect more semantically-related mentions from
wider contexts. For example, the relational link
between ?narita airport? and ?Japan? would be
missed without using the path ?narita airport - t
1
- u
1
- t
2
- american - t
3
- h
1
- t
4
- Japan? since they
don?t directly share any authorships or #hashtags.
4.3 Coreference
A coreference relation (Principle 2) usually occurs
across multiple tweets due to the highly redundant
information in Twitter. To ensure high precision,
we propose a simple yet effective approach utiliz-
ing the rich social network relations in Twitter.
We consider two mentions m
i
and m
j
corefer-
ential if m
i
and m
j
share the same surface form
or one is an abbreviation of the other, and at least
one meta path exists betweenm
i
andm
j
. Then we
define the weight matrix representing the corefer-
ential relation as:
W
coref
ij
=
?
?
?
1.0 if m
i
and m
j
are coreferential,
and c
i
= c
j
0 Otherwise
4.4 Semantic Relatedness
Ensuring topical coherence (Principle 3) has been
beneficial for wikification on formal texts (e.g.,
News) by linking a set of semantically-related
mentions to a set of semantically-related concepts
simultaneously (Han et al, 2011; Ratinov et al,
2011; Cheng and Roth, 2013). However, the short-
ness of a single tweet means that it may not pro-
vide enough topical clues. Therefore, it is impor-
tant to extend this evidence to capture semantic re-
latedness information from multiple tweets.
We define the semantic relatedness score be-
tween two mentions as SR(m
i
,m
j
) = 1.0 if at
least one meta path exists between m
i
and m
j
,
otherwise SR(m
i
,m
j
) = 0. In order to compute
the semantic relatedness of two concepts c
i
and
c
j
, we adopt the approach proposed by (Milne and
Witten, 2008a):
SR(c
i
, c
j
) = 1?
logmax(|C
i
|, |C
j
|)? log |C
i
? C
j
|
log(|C|)? logmin(|C
i
|, |C
j
|)
,
where |C| is the total number of concepts in
Wikipedia, and C
i
and C
j
are the set of concepts
that have links to c
i
and c
j
, respectively.
Then we compute a weight matrix representing
the semantic relatedness relation as:
W
rel
ij
=
{
SR(N
i
, N
j
) if SR(N
i
, N
j
) ? ?
0 Otherwise
where SR(N
i
, N
j
) = SR(m
i
,m
j
) ? SR(c
i
, c
j
)
and ? = 0.3, which is optimized from a develop-
ment set.
4.5 The Combined Relational Graph
    
hawks, 
 Atlanta Hawks
uconn, 
Connecticut 
Huskies
bucks, 
Milwaukee 
Bucks
kemba walker, 
Kemba Walker
0 .40 4
gators, 
Florida Gators 
men's basketball
now, 
N ow
days, 
D ay
tonight, 
Tonight
0 .9 32
0 .7 6 4
0 .6 6 5
0 .46 7
0 .56 3 0 .538
0 .447
Figure 4: A example of the relational graph con-
structed for the example tweets in Figure 1. Each
node represents a pair of ?m, c?, separated by a
comma. The edge weight is obtained from the lin-
ear combination of the weights of the three pro-
posed relations. Not all mentions are included due
to the space limitations.
Based on the above three weight matricesW
loc
,
W
coref
, and W
rel
, we first obtain their corre-
sponding transition matrices P
loc
, P
coref
, and
P
rel
, respectively. The entry P
ij
of the transition
matrix P for a weight matrix W is computed as
P
ij
=
W
ij?
k
W
ik
such that
?
k
P
ik
= 1. Then we
obtain the combined graph G with weight matrix
W , where W
ij
= ?P
loc
ij
+ ?P
coref
ij
+ ?P
rel
ij
. ?,
?, and ? are three coefficients between 0 and 1
with the constraint that ?+ ? + ? = 1. They con-
trol the contributions of these three relations in our
semi-supervised graph regularization model. We
choose transition matrix to avoid the domination
of one relation over others. An example graph of
G is shown in Figure 4. Compared to the referent
graph which considers each mention or concept
as a node in previous graph-based re-ranking ap-
proaches (Han et al, 2011; Shen et al, 2013), our
384
novel graph representation has two advantages: (i)
It can easily incorporate more features related to
both mentions and concepts. (ii) It is more appro-
priate for our graph-based semi-supervised model
since it is difficult to assign labels to a pair of men-
tion and concept in the referent graph.
5 Semi-supervised Graph Regularization
Given the constructed relational graph with the
weighted matrix W and the label vector Y of all
nodes, we assume the first l nodes are labeled as
Y
l
and the remaining u nodes (u = n? l) are ini-
tialized with labels Y
0
u
. Then our goal is to refine
Y
0
u
and obtain the final label vector Y
u
.
Intuitively, if two nodes are strongly connected,
they tend to hold the same label. We propose a
novel semi-supervised graph regularization frame-
work based on the graph-based semi-supervised
learning algorithm (Zhu et al, 2003):
Q(Y) = ?
n
?
i=l+1
(y
i
?y
0
i
)
2
+
1
2
?
i,j
W
ij
(y
i
?y
j
)
2
.
The first term is a loss function that incorporates
the initial labels of unlabeled examples into the
model. In our method, we adopt prior popular-
ity (section 4.1) to initialize the labels of the un-
labeled examples. The second term is a regular-
izer that smoothes the refined labels over the con-
structed graph. ? is a regularization parameter that
controls the trade-off between initial labels and the
consistency of labels on the graph. The goal of the
proposed framework is to ensure that the refined
labels of unlabeled nodes are consistent with their
strongly connected nodes, as well as not too far
away from their initial labels.
The above optimization problem can be solved
directly since Q(Y) is convex (Zhu et al, 2003;
Zhou et al, 2004). Let I be an identity matrix
and D
W
be a diagonal matrix with entries D
ii
=
?
j
W
ij
. We can split the weighted matrix W into
four blocks as W =
[
W
ll
W
lu
W
ul
W
uu
]
, where W
mn
is
anm?nmatrix. D
w
is split similarly. We assume
that the vector of the labeled examples Y
l
is fixed,
so we only need to infer the refined label vector of
the unlabeled examples Y
u
. In order to minimize
Q(Y), we need to find Y
?
u
such that
?Q
?Y
u
?
?
?
?
Y
u
=Y
?
u
= (D
uu
+ ?I
uu
)Y
u
?W
uu
Y
u
?
W
ul
Y
l
? ?Y
0
u
= 0.
Therefore, a closed form solution can be derived
as Y
?
u
= (D
uu
+ ?I
uu
?W
uu
)
?1
(W
ul
Y
l
+ ?Y
0
u
).
However, for practical application to a large-
scale data set, an iterative solution would be more
efficient to solve the optimization problem. Let
Y
t
u
be the refined labels after the t
th
iteration, the
iterative solution can be derived as:
Y
t+1
u
= (D
uu
+?I
uu
)
?1
(W
uu
Y
t
u
+W
ul
Y
l
+?Y
0
u
).
The iterative solution is more efficient since
(D
uu
+ ?I
uu
) is a diagonal matrix and its inverse
is very easy to compute.
6 Experiments
In this section we compare our approach with
state-of-the-art methods as shown in Table 1.
6.1 Data and Scoring Metric
For our experiments we use a public data set (Meij
et al, 2012) including 502 tweets posted by 28
verified users. The data set was annotated by two
annotators. We randomly sample 102 tweets for
development and the remaining for evaluation. We
use a Wikipedia dump on May 3, 2013 as our
knowledge base, which includes 30 million pages.
For computational efficiency, we also filter some
mention candidates by applying the preprocess-
ing approach proposed in (Ferragina and Scaiella,
2010), and remove all the concepts with prior pop-
ularity less than 2% from an mention?s concept set
for each mention, similar to (Guo et al, 2013).
A mention and concept pair ?m, c? is judged as
correct if and only if m is linkable and c is the
correct referent concept for m. To evaluate the
performance of a wikification system, we use the
standard precision, recall and F1 measures.
6.2 Experimental Results
The overall performance of various approaches
is shown in Table 2. The results of the super-
vised method proposed by (Meij et al, 2012) are
obtained from 5-fold cross validation. For our
semi-supervised setting, we experimentally sam-
ple 200 tweets for training and use the remain-
ing set as unlabeled and testing sets. In our semi-
supervised regularization model, the matrix W
loc
is constructed by a kNN graph (k = 20). The reg-
ularization parameter ? is empirically set to 0.1,
and the coefficients ?, ?, and ? are learnt from the
development set by considering all the combina-
385
Methods Descriptions
TagMe The same approach that is described in (Ferragina and Scaiella, 2010), which aims to annotate short
texts based on prior popularity and semantic relatedness of concepts. It is basically an unsupervised
approach, except that it needs a development set to tune the probability threshold for linkable mentions.
Meij A state-of-the-art system described in (Meij et al, 2012), which is a supervised approach based on the
random forest model. It performs mention detection and disambiguation jointly, and it is trained from
400 labeled tweets.
SSRegu
1
Our proposed model based on Principle 1, using 200 labeled tweets.
SSRegu
12
Our proposed model based on Principle 1 and 2, using 200 labeled tweets.
SSRegu
13
Our proposed model based on Principle 1 and 3, using 200 labeled tweets.
SSRegu
123
Our proposed full model based on Principle 1, 2 and 3, using 200 labeled tweets.
Table 1: Description of Methods.
Methods Precision Recall F1
TagMe 0.329 0.423 0.370
Meij 0.393 0.598 0.475
SSRegu
1
0.538 0.435 0.481
SSRegu
12
0.638 0.438 0.520
SSRegu
13
0.541 0.457 0.495
SSRegu
123
0.650 0.441 0.525
Table 2: Overall Performance.
tions of values from 0 to 1 at 0.1 intervals
1
. In
order to randomize the experiments and make the
comparison fair, we conduct 20 test runs for each
method and report the average scores across the 20
trials.
The relatively low performance of the baseline
system TagMe demonstrates that only relying on
prior popularity and topical information within a
single tweet is not enough for an end-to-end wik-
ification system for the short tweets. As an exam-
ple, it is difficult to obtain topical clues in order
to link the mention ?Clinton? to Hillary Rodham
Clinton by relying on the single tweet ?wolfblitzer-
cnn: Behind the scenes on Clinton?s Mideast trip
#cnn?. Therefore, the system mistakenly links it
to the most popular concept Bill Clinton.
In comparision with the supervised baseline
proposed by (Meij et al, 2012), our model
SSRegu
1
relying on local compatibility already
achieves comparable performance with 50% of
labeled data. This is because that our model
performs collective inference by making use of
the manifold (cluster) structure of both labeled
and unlabeled data, and that the local compat-
ibility relation is detected with high precision
2
(89.4%). For example, the following three pairs
of mentions and concepts ?pelosi, Nancy Pelosi?,
?obama, Barack Obama?, and ?gaddafi, Muam-
1
These three coefficients are slightly different with differ-
ent training data, a sample of them is: ? = 0.4, ? = 0.5, and
? = 0.1
2
Here we define precision as the percentage of links that
holds the same label.
mar Gaddafi? have strong local compatibility with
each other since they share many similar char-
acteristics captured by the local features such as
string similarity between the mention and the con-
cept. Suppose the first pair is labeled, then its pos-
itive label will be propagated to other unlabeled
nodes through the local compatibility relation, and
correctly predict the labels of other nodes.
Incorporating coreferential or semantic related-
ness relation into SSRegu
1
provides further gains,
demonstrating the effectiveness of these two re-
lations. For instance, ?wh? is correctly linked to
White House by incorporating evidence from its
coreferential mention ?white house?. The corefer-
ential relation (Principle 2) is demonstrated to be
more beneficial than the semantic relatedness re-
lation (Principle 3) because the former is detected
with much higher precision (99.7%) than the latter
(65.4%).
Our full model SSRegu
123
achieves significant
improvement over the supervised baseline (5% ab-
solute F1 gain with 95.0% confidence level by
the Wilcoxon Matched-Pairs Signed-Ranks Test),
showing that incorporating global evidence from
multiple tweets with fine-grained relations is ben-
eficial. For instance, the supervised baseline fails
to link ?UCONN? and ?Bucks? in our examples
to Connecticut Huskies and Milwaukee Bucks, re-
spectively. Our full model corrects these two
wrong links by propagating evidence through the
semantic links as shown in Figure 4 to obtain mu-
tual ranking improvement. The best performance
of our full model also illustrates that the three re-
lations complement each other.
We also study the disambiguation performance
for the annotated mentions, as shown in Table 3.
We can easily see that our proposed approach
using 50% labeled data achieves similar perfor-
mance with the state-of-the-art supervised model
with 100% labeled data. When the mentions are
given, the unpervised approach TagMe has already
386
Methods TagMe Meij SSRegu
123
Accuracy 0.710 0.779 0.772
Table 3: Disambiguation Performance.
Methods Precision Recall F1
SSRegu
12
0.644 0.423 0.510
SSRegu
13
0.543 0.441 0.486
SSRegu
123
0.657 0.419 0.512
Table 4: The Performance of Systems Without Us-
ing Concatenated Meta Paths.
achieved reasonable performance. In fact, mention
detection actually is the performance bottleneck of
a tweet wikification system (Guo et al, 2013). Our
system performs better in identifying the promi-
nent mention.
6.3 Effect of Concatenated Meta Paths
In this work, we propose a unified framework uti-
lizing meta path-based semantic relations to ex-
plore richer relevant context. Beyond the basic
meta paths, we introduce concatenated ones by
concatenating the basic ones. The performance of
the system without using the concatenated meta
paths is shown in Table 4. In comparison with
the system based on all defined meta paths, we
can clearly see that the systems using concate-
nated ones outperform those relying on the sim-
ple ones. This is because the concatenated meta
paths can incorporate more relevant information
with implicit relations into the models by increas-
ing 1.6% coreference links and 9.3% semantic re-
latedness links. For example, the mention ?narita
airport? is correctly disambiguated to the concept
?Narita International Airport? with higher confi-
dence since its semantic relatedness relation with
?Japan? is detected with the concatenated meta
path as described in section 4.2.
6.4 Effect of Labeled Data Size
5 0 1 0 0 1 5 0 2 0 0 2 5 0 3 0 0 3 5 0 4 0 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 L a b e l e d  T w e e t  S i z e S S R e g u 1 2 3 M e i j
Figure 5: The effect of Labeled Tweet Size.
In previous experiments, we experimentally set
the number of labeled tweets to be 200 for over-
all performance comparision with the baselines.
In this subsection, we study the effect of labeled
data size on our full model. We randomly sam-
ple 100 tweets as testing data, and randomly se-
lect 50, 100, 150, 200, 250, and 300 tweets as
labeled data. 20 test runs are conducted and the
average results are reported across the 20 trials,
as shown in Figure 5. We find that as the size
of the labeled data increases, our proposed model
achieves better performance. It is encouraging to
see that our approach, with only 31.3% labeled
tweets (125 out of 400), already achieves a perfor-
mance that is comparable to the state-of-the-art su-
pervised model trained from 100% labeled tweets.
6.5 Parameter Analysis
0 . 1 0 . 5 1 2 5 1 0 2 0 3 0 4 0 5 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 R e g u l a r i z a t i o n  P a r a m e t e r  ? S S R e g u 1 2 3
Figure 6: The effect of parameter ?.
In previous experiments, we empirically set the
parameter ? = 0.1. ? is the regularization pa-
rameter that controls the trade-off between initial
labels and the consistency of labels on the graph.
When ? increases, the model tends to trust more in
the initial labels. Figure 6 shows the performance
of our models by varying ? from 0.02 to 50. We
can easily see that the system performce is stable
when ? < 0.4. However, when ? ? 0.4, the sys-
tem performance dramatically decreases, showing
that prior popularity is not enough for an end-to-
end wikification system.
7 Related Work
The task of linking concept mentions to a knowl-
edge base has received increased attentions over
the past several years, from the linking of concept
mentions in a single text (Mihalcea and Csomai,
2007; Milne and Witten, 2008b; Milne and Witten,
2008a; Kulkarni et al, 2009; He et al, 2011; Rati-
nov et al, 2011; Cassidy et al, 2012; Cheng and
Roth, 2013), to the linking of a cluster of corefer-
387
ent named entity mentions spread throughout dif-
ferent documents (Entity Linking) (McNamee and
Dang, 2009; Ji et al, 2010; Zhang et al, 2010; Ji et
al., 2011; Zhang et al, 2011; Han and Sun, 2011;
Han et al, 2011; Gottipati and Jiang, 2011; He et
al., 2013; Li et al, 2013; Guo et al, 2013; Shen et
al., 2013; Liu et al, 2013).
A significant portion of recent work considers
the two sub-problems mention detection and men-
tion disambiguation separately and focus on the
latter by first defining candidate concepts for a
deemed mention based on anchor links. Men-
tion disambiguation is then formulated as a rank-
ing problem, either by resolving one mention at
each time (non-collective approaches), or by dis-
ambiguating a set of relevant mentions simulta-
neously (collective approaches). Non-collective
methods usually rely on prior popularity and con-
text similarity with supervised models (Mihalcea
and Csomai, 2007; Milne and Witten, 2008b; Han
and Sun, 2011), while collective approaches fur-
ther leverage the global coherence between con-
cepts normally through supervised or graph-based
re-ranking models (Cucerzan, 2007; Milne and
Witten, 2008b; Han and Zhao, 2009; Kulkarni et
al., 2009; Pennacchiotti and Pantel, 2009; Ferrag-
ina and Scaiella, 2010; Fernandez et al, 2010;
Radford et al, 2010; Cucerzan, 2011; Guo et al,
2011; Han and Sun, 2011; Han et al, 2011; Rati-
nov et al, 2011; Chen and Ji, 2011; Kozareva et
al., 2011; Cassidy et al, 2012; Shen et al, 2013;
Liu et al, 2013). Especially note that when apply-
ing the collective methods to short messages from
social media, evidence from other messages usu-
ally needs to be considered (Cassidy et al, 2012;
Shen et al, 2013; Liu et al, 2013). Our method
is a collective approach with the following novel
advancements: (i) A novel graph representation
with fine-grained relations, (ii) A unified frame-
work based on meta paths to explore richer rele-
vant context, (iii) Joint identification and linking
of mentions under semi-supervised setting.
Two most similar methods to ours were pro-
posed by (Meij et al, 2012; Guo et al, 2013)
by performing joint detection and disambiguation
of mentions. (Meij et al, 2012) studied several
supervised machine learning models, but without
considering any global evidence either from a sin-
gle tweet or other relevant tweets. (Guo et al,
2013) explored second order entity-to-entity rela-
tions but did not incorporate evidence from multi-
ple tweets.
This work is also related to graph-based semi-
supervised learning (Zhu et al, 2003; Smola
and Kondor, 2003; Zhou et al, 2004; Talukdar
and Crammer, 2009), which has been success-
fully applied in many Natural Language Process-
ing tasks (Niu et al, 2005; Chen et al, 2006).
We introduce a novel graph that incorporates three
fine-grained relations. Our work is further re-
lated to meta path-based heterogeneous informa-
tion network analysis (Sun et al, 2011b; Sun et
al., 2011a; Kong et al, 2012; Huang et al, 2013),
which has demonstrated advantages over homoge-
neous information network analysis without dif-
ferentiating object types and relational links.
8 Conclusions
We have introduced a novel semi-supervised graph
regularization framework for wikification to si-
multaneously tackle the unique challenges of an-
notation and information shortage in short tweets.
To the best of our knowledge, this is the first work
to explore the semi-supervised collective inference
model to jointly perform mention detection and
disambiguation. By studying three novel fine-
grained relations, detecting semantically-related
information with semantic meta paths, and ex-
ploiting the data manifolds in both unlabeled and
labeled data for collective inference, our work can
dramatically save annotation cost and achieve bet-
ter performance, thus shed light on the challenging
wikification task for tweets.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149, U.S.
DARPA Award No. FA8750-13-2-0041 in the
Deep Exploration and Filtering of Text (DEFT)
Program, IBM Faculty Award, Google Research
Award and RPI faculty start-up grant. The views
and conclusions contained in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
388
References
A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy.
2004. Semi-supervised learning using randomized
mincuts. In Proceedings of the Twenty-first Interna-
tional Conference on Machine Learning, ICML ?04.
Razvan Bunescu. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In EACL,
pages 9?16.
T. Cassidy, H. Ji, L. Ratinov, A. Zubiaga, and
H. Huang. 2012. Analysis and enhancement of wik-
ification for microblogs with context expansion. In
Proceedings of COLING 2012.
Z. Chen and H. Ji. 2011. Collaborative ranking: A
case study on entity linking. In Proc. EMNLP2011.
J. Chen, D. Ji, C Tan, and Z. Niu. 2006. Rela-
tion extraction using label propagation based semi-
supervised learning. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics.
X. Cheng and D. Roth. 2013. Relational inference
for wikification. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL 2007.
S. Cucerzan. 2011. Tac entity linking by performing
full-document entity extraction and disambiguation.
In Proc. TAC 2011 Workshop.
N. Fernandez, J. A. Fisteus, L. Sanchez, and E. Mar-
tin. 2010. Webtlab: A cooccurence-based approach
to kbp 2010 entity-linking task. In Proc. TAC 2010
Workshop.
P. Ferragina and U. Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, CIKM ?10.
S. Gottipati and J. Jiang. 2011. Linking entities to a
knowledge base with query expansion. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing.
Y. Guo, W. Che, T. Liu, and S. Li. 2011. A graph-
based method for entity linking. In Proc. IJC-
NLP2011.
S. Guo, M. Chang, and E. Kiciman. 2013. To link
or not to link? a study on end-to-end tweet entity
linking. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
B. Hachey, W. Radford, J. Nothman, M. Honnibal, and
J. Curran. 2013. Evaluating entity linking with
wikipedia. Artif. Intell.
X. Han and L. Sun. 2011. A generative entity-mention
model for linking entities with knowledge base. In
Proc. ACL2011.
X. Han and J. Zhao. 2009. Named entity disam-
biguation by leveraging wikipedia semantic knowl-
edge. In Proceedings of the 18th ACM conference
on Information and knowledge management, CIKM
2009.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: A graph-based method. In Proc.
SIGIR2011.
J. He, M. de Rijke, M. Sevenster, R. van Ommering,
and Y. Qian. 2011. Generating links to background
knowledge: A case study using narrative radiology
reports. In Proceedings of the 20th ACM inter-
national conference on Information and knowledge
management. ACM.
Z. He, S. Liu, Y. Song, M. Li, M. Zhou, and H. Wang.
2013. Efficient collective entity linking with stack-
ing. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing.
H. Huang, Z. Wen, D. Yu, H. Ji, Y. Sun, J. Han, and
H. Li. 2013. Resolving entity morphs in censored
data. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers).
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. El-
lis. 2010. Overview of the tac 2010 knowledge base
population track. In Text Analysis Conference (TAC)
2010.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview
of the tac 2011 knowledge base population track. In
Text Analysis Conference (TAC) 2011.
X. Kong, P. Yu, Y. Ding, and J. Wild. 2012. Meta
path-based collective classification in heterogeneous
information networks. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management, CIKM ?12.
Z. Kozareva, K. Voevodski, and S. Teng. 2011. Class
label enhancement via related instances. In Proc.
EMNLP2011.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In KDD.
Y. Li, C. Wang, F. Han, J. Han, D. Roth, and X. Yan.
2013. Mining evidences for named entity dis-
ambiguation. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?13.
389
X. Liu, Y. Li, H. Wu, M. Zhou, F. Wei, and Y. Lu.
2013. Entity linking for tweets. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).
P. McNamee and H.T. Dang. 2009. Overview of the
tac 2009 knowledge base population track. In Text
Analysis Conference (TAC) 2009.
E. Meij, W. Weerkamp, and M. de Rijke. 2012.
Adding semantics to microblog posts. In Proceed-
ings of the fifth ACM international conference on
Web search and data mining, WSDM ?12.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking
documents to encyclopedic knowledge. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
?07.
D. Milne and I.H. Witten. 2008a. Learning to link
with wikipedia. In An effective, low-cost measure of
semantic relatedness obtained from wikipedia links.
the Wikipedia and AI Workshop of AAAI.
D. Milne and I.H. Witten. 2008b. Learning to link
with wikipedia. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
Z. Niu, D. Ji, and C. Tan. 2005. Word sense dis-
ambiguation using label propagation based semi-
supervised learning. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
M. Pennacchiotti and P. Pantel. 2009. Entity extraction
via ensemble semantics. In Proc. EMNLP2009.
W. Radford, B. Hachey, J. Nothman, M. Honnibal, and
J. R. Curran. 2010. Cmcrc at tac10: Document-
level entity linking with graph-based re-ranking. In
Proc. TAC 2010 Workshop.
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proc. of the Annual Meeting of
the Association of Computational Linguistics (ACL).
W. Shen, J. Wang, P. Luo, and M. Wang. 2013. Link-
ing named entities in tweets with knowledge base
via user interest modeling. In Proceedings of the
19th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?13.
A. Smola and R. Kondor. 2003. Kernels and regular-
ization on graphs. COLT.
Y. Sun, R. Barber, M. Gupta, C. Aggarwal, and J. Han.
2011a. Co-author relationship prediction in hetero-
geneous bibliographic networks. In Proceedings of
the 2011 International Conference on Advances in
Social Networks Analysis and Mining, ASONAM
?11.
Y. Sun, J. Han, X. Yan, P. Yu, and T. Wu. 2011b. Path-
sim: Meta path-based top-k similarity search in het-
erogeneous information networks. PVLDB, 4(11).
P. Talukdar and K. Crammer. 2009. New regularized
algorithms for transductive learning. In Proceed-
ings of the European Conference on Machine Learn-
ing and Knowledge Discovery in Databases: Part II,
ECML PKDD ?09.
D. Vitale, P. Ferragina, and U. Scaiella. 2012. Clas-
sification of short texts by deploying topical annota-
tions. In ECIR, pages 376?387.
K. Wang, C. Thrasher, and B. Hsu. 2011. Web scale
nlp: A case study on url word breaking. In Proceed-
ings of the 20th International Conference on World
Wide Web, WWW ?11.
W. Zhang, J. Su, C. Tan, and W. Wang. 2010. En-
tity linking leveraging automatically generated an-
notation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010).
W. Zhang, J. Su, and C. L. Tan. 2011. A wikipedia-lda
model for entity linking with batch size changing. In
Proc. IJCNLP2011.
D. Zhou, O. Bousquet, T. Lal, J. Weston, and
B. Sch?olkopf. 2004. Learning with local and global
consistency. In Advances in Neural Information
Processing Systems 16.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In ICML.
390
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 42?49,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
Self-disclosure topic model for Twitter conversations
JinYeong Bak
Department of Computer Science
KAIST
Daejeon, South Korea
jy.bak@kaist.ac.kr
Chin-Yew Lin
Microsoft Research Asia
Beijing 100080, P.R. China
cyl@microsoft.com
Alice Oh
Department of Computer Science
KAIST
Daejeon, South Korea
alice.oh@kaist.edu
Abstract
Self-disclosure, the act of revealing one-
self to others, is an important social be-
havior that contributes positively to inti-
macy and social support from others. It
is a natural behavior, and social scien-
tists have carried out numerous quantita-
tive analyses of it through manual tagging
and survey questionnaires. Recently, the
flood of data from online social networks
(OSN) offers a practical way to observe
and analyze self-disclosure behavior at an
unprecedented scale. The challenge with
such analysis is that OSN data come with
no annotations, and it would be impos-
sible to manually annotate the data for a
quantitative analysis of self-disclosure. As
a solution, we propose a semi-supervised
machine learning approach, using a vari-
ant of latent Dirichlet allocation for au-
tomatically classifying self-disclosure in a
massive dataset of Twitter conversations.
For measuring the accuracy of our model,
we manually annotate a small subset of
our dataset, and we show that our model
shows significantly higher accuracy and
F-measure than various other methods.
With the results our model, we uncover
a positive and significant relationship be-
tween self-disclosure and online conversa-
tion frequency over time.
1 Introduction
Self-disclosure is an important and pervasive so-
cial behavior. People disclose personal informa-
tion about themselves to improve and maintain
relationships (Jourard, 1971; Joinson and Paine,
2007). For example, when two people meet for
the first time, they disclose their names and in-
terests. One positive outcome of self-disclosure
is social support from others (Wills, 1985; Der-
lega et al., 1993), shown also in online social net-
works (OSN) such as Twitter (Kim et al., 2012).
Receiving social support would then lead the user
to be more active on OSN (Steinfield et al., 2008;
Trepte and Reinecke, 2013). In this paper, we seek
to understand this important social behavior using
a large-scale Twitter conversation data, automati-
cally classifying the level of self-disclosure using
machine learning and correlating the patterns with
subsequent OSN usage.
Twitter conversation data, explained in more de-
tail in section 4.1, enable a significantly larger
scale study of naturally-occurring self-disclosure
behavior, compared to traditional social science
studies. One challenge of such large scale study,
though, remains in the lack of labeled ground-
truth data of self-disclosure level. That is,
naturally-occurring Twitter conversations do not
come tagged with the level of self-disclosure in
each conversation. To overcome that challenge,
we propose a semi-supervised machine learning
approach using probabilistic topic modeling. Our
self-disclosure topic model (SDTM) assumes that
self-disclosure behavior can be modeled using a
combination of simple linguistic features (e.g.,
pronouns) with automatically discovered seman-
tic themes (i.e., topics). For instance, an utterance
?I am finally through with this disastrous relation-
ship? uses a first-person pronoun and contains a
topic about personal relationships.
In comparison with various other models,
SDTM shows the highest accuracy, and the result-
ing self-disclosure patterns of the users are cor-
related significantly with their future OSN usage.
Our contributions to the research community in-
clude the following:
? We present a topic model that explicitly in-
cludes the level of self-disclosure in a conver-
sation using linguistic features and the latent
semantic topics (Sec. 3).
42
? We collect a large dataset of Twitter conver-
sations over three years and annotate a small
subset with self-disclosure level (Sec. 4).
? We compare the classification accuracy of
SDTM with other models and show that it
performs the best (Sec. 5).
? We correlate the self-disclosure patterns of
users and their subsequent OSN usage to
show that there is a positive and significant
relationship (Sec. 6).
2 Background
In this section, we review literature on the relevant
aspects of self-disclosure.
Self-disclosure (SD) level: To quantitatively
analyze self-disclosure, researchers categorize
self-disclosure language into three levels: G (gen-
eral) for no disclosure, M for medium disclosure,
and H for high disclosure (Vondracek and Von-
dracek, 1971; Barak and Gluck-Ofri, 2007). Ut-
terances that contain general (non-sensitive) infor-
mation about the self or someone close (e.g., a
family member) are categorized as M. Examples
are personal events, past history, or future plans.
Utterances about age, occupation and hobbies are
also included. Utterances that contain sensitive in-
formation about the self or someone close are cat-
egorized as H. Sensitive information includes per-
sonal characteristics, problematic behaviors, phys-
ical appearance and wishful ideas. Generally,
these are thoughts and information that one would
generally keep as secrets to himself. All other
utterances, those that do not contain information
about the self or someone close are categorized
as G. Examples include gossip about celebrities or
factual discourse about current events.
Classifying self-disclosure level: Prior work
on quantitatively analyzing self-disclosure has re-
lied on user surveys (Trepte and Reinecke, 2013;
Ledbetter et al., 2011) or human annotation (Barak
and Gluck-Ofri, 2007). These methods consume
much time and effort, so they are not suitable for
large-scale studies. In prior work closest to ours,
Bak et al. (2012) showed that a topic model can
be used to identify self-disclosure, but that work
applies a two-step process in which a basic topic
model is first applied to find the topics, and then
the topics are post-processed for binary classifica-
tion of self-disclosure. We improve upon this work
by applying a single unified model of topics and
??
??
??
CTN
??
?
?
?? 3?????? 3
Figure 1: Graphical model of SDTM
self-disclosure for high accuracy in classifying the
three levels of self-disclosure.
Self-disclosure and online social network:
According to social psychology, when someone
discloses about himself, he will receive social sup-
port from those around him (Wills, 1985; Derlega
et al., 1993), and this pattern of self-disclosure
and social support was verified for Twitter con-
versation data (Kim et al., 2012). Social support
is a major motivation for active usage of social
networks services (SNS), and there are findings
that show self-disclosure on SNS has a positive
longitudinal effect on future SNS use (Trepte and
Reinecke, 2013; Ledbetter et al., 2011). While
these previous studies focused on small, qualita-
tive studies, we conduct a large-scale, machine
learning driven study to approach the question of
self-disclosure behavior and SNS use.
3 Self-Disclosure Topic Model
This section describes our model, the self-
disclosure topic model (SDTM), for classifying
self-disclosure level and discovering topics for
each self-disclosure level.
3.1 Model
We make two important assumptions based on our
observations of the data. First, first-person pro-
nouns (I, my, me) are good indicators for medium
level of self-disclosure. For example, phrases such
as ?I live? or ?My age is? occur in utterances that re-
veal personal information. Second, there are top-
ics that occur much more frequently at a particular
SD level. For instance, topics such as physical
appearance and mental health occur frequently at
level H, whereas topics such as birthday and hob-
bies occur frequently at level M.
Figure 1 illustrates the graphical model of
SDTM and how these assumptions are embodied
43
Notation Description
G; M ; H {general; medium; high} SD level
C; T ; N Number of conversations; tweets;
words
K
G
;K
M
;K
H
Number of topics for {G; M; H}
c; ct Conversation; tweet in conversation c
y
ct
SD level of tweet ct, G or M/H
r
ct
SD level of tweet ct, M or H
z
ct
Topic of tweet ct
w
ctn
n
th
word in tweet ct
? Learned Maximum entropy parame-
ters
x
ct
First-person pronouns features
?
ct
Distribution over SD level of tweet ct
pi
c
SD level proportion of conversation c
?G
c
;?M
c
;?H
c
Topic proportion of {G; M; H} in con-
versation c
?G;?M ;?H Word distribution of {G; M; H}
?; ? Dirichlet prior for ?; pi
?G,?M ;?H Dirichlet prior for ?G;?M ;?H
n
cl
Number of tweets assigned SD level l
in conversation c
n
l
ck
Number of tweets assigned SD level l
and topic k in conversation c
n
l
kv
Number of instances of word v as-
signed SD level l and topic k
m
ctkv
Number of instances of word v as-
signed topic k in tweet ct
Table 1: Summary of notations used in SDTM.
in it. The first assumption about the first-person
pronouns is implemented by the observed variable
x
ct
and the parameters ? from a maximum en-
tropy classifier for G vs. M/H level. The second
assumption is implemented by the three separate
word-topic probability vectors for the three lev-
els of SD: ?
l
which has a Bayesian informative
prior ?
l
where l ? {G,M,H}, the three levels
of self-disclosure. Table 1 lists the notations used
in the model and the generative process, Figure 2
describes the generative process.
3.2 Classifying G vs M/H levels
Classifying the SD level for each tweet is done in
two parts, and the first part classifies G vs. M/H
levels with first-person pronouns (I, my, me). In
the graphical model, y is the latent variable that
represents this classification, and ? is the distri-
bution over y. x is the observation of the first-
person pronoun in the tweets, and? are the param-
eters learned from the maximum entropy classifier.
With the annotated Twitter conversation dataset
(described in Section 4.2), we experimented with
several classifiers (Decision tree, Naive Bayes)
and chose the maximum entropy classifier because
it performed the best, similar to other joint topic
models (Zhao et al., 2010; Mukherjee et al., 2013).
1. For each level l ? {G, M, H}:
For each topic k ? {1, . . . ,Kl}:
Draw ?lk ? Dir(?l)2. For each conversation c ? {1, . . . , C}:
(a) Draw ?Gc ? Dir(?)(b) Draw ?Mc ? Dir(?)(c) Draw ?Hc ? Dir(?)(d) Draw pic ? Dir(?)
(e) For each message t ? {1, . . . , T}:
i. Observe first-person pronouns features xct
ii. Draw ?ct ?MaxEnt(xct,?)
iii. Draw yct ? Bernoulli(?ct)
iv. If yct = 0 which is G level:
A. Draw zct ?Mult(?Gc )B. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?Gzct)Else which can be M or H level:
A. Draw rct ?Mult(pic)
B. Draw zct ?Mult(?rctc )C. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?rctzct)
Figure 2: Generative process of SDTM.
3.3 Classifying M vs H levels
The second part of the classification, the M and the
H level, is driven by informative priors with seed
words and seed trigrams.
Utterances with M level include two types:
1) information related with past events and fu-
ture plans, and 2) general information about self
(Barak and Gluck-Ofri, 2007). For the former, we
add as seed trigrams ?I have been? and ?I will?.
For the latter, we use seven types of information
generally accepted to be personally identifiable in-
formation (McCallister, 2010), as listed in the left
column of Table 2. To find the appropriate tri-
grams for those, we take Twitter conversation data
(described in Section 4.1) and look for trigrams
that begin with ?I? and ?my? and occur more than
200 times. We then check each one to see whether
it is related with any of the seven types listed in
the table. As a result, we find 57 seed trigrams for
M level. Table 2 shows several examples.
Type Trigram
Name My name is, My last name
Birthday My birthday is, My birthday party
Location I live in, I lived in, I live on
Contact My email address, My phone number
Occupation My job is, My new job
Education My high school, My college is
Family My dad is, My mom is, My family is
Table 2: Example seed trigrams for identifying M
level of SD. There are 51 of these used in SDTM.
Utterances with H level express secretive wishes
or sensitive information that exposes self or some-
one close (Barak and Gluck-Ofri, 2007). These are
44
Category Keywords
physical
appearance
acne, hair, overweight, stomach, chest,
hand, scar, thighs, chubby, head, skinny
mental/physical
condition
addicted, bulimia, doctor, illness, alco-
holic, disease, drugs, pills, anorexic
Table 3: Example words for identifying H level of
SD. Categories are hand-labeled.
generally keep as secrests. With this intuition, we
crawled 26,523 secret posts from Six Billion Se-
crets
1
site where users post secrets anonymously.
To extract seed words that might express secre-
tive personal information, we compute mutual in-
formation (Manning et al., 2008) with the secret
posts and 24,610 randomly selected tweets. We
select 1,000 words with high mutual information
and filter out stop words. Table 3 shows some of
these words. To extract seed trigrams of secretive
wishes, we again look for trigrams that start with
?I? or ?my?, occur more than 200 times, and select
trigrams of wishful thinking, such as ?I want to?,
and ?I wish I?. In total, there are 88 seed words
and 8 seed trigrams for H.
3.4 Inference
For posterior inference of SDTM, we use col-
lapsed Gibbs sampling which integrates out la-
tent random variables ?,pi,?, and ?. Then we
only need to compute y, r and z for each tweet.
We compute full conditional distribution p(y
ct
=
j
?
, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x) for
tweet ct as follows:
p(y
ct
= 0, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
0
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
g(c, t, l
?
, k
?
)
p(y
ct
= 1, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
1
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
(?
l
?
+ n
(?ct)
cl
?
) g(c, t, l
?
, k
?
)
where z
?ct
, r
?ct
,y
?ct
are z, r,y without tweet
ct, m
ctk
?
(?)
is the marginalized sum over word v of
m
ctk
?
v
and the function g(c, t, l
?
, k
?
) as follows:
g(c, t, l
?
, k
?
) =
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
(?)
)
(
?
k
?
+ n
l
?
(?ct)
ck
?
?
K
k=1
?
k
+ n
l
?
ck
)
V
?
v=1
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
v
)
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
1
http://www.sixbillionsecrets.com
4 Data Collection and Annotation
To answer our research questions, we need a
large longitudinal dataset of conversations such
that we can analyze the relationship between self-
disclosure behavior and conversation frequency
over time. We chose to crawl Twitter because it
offers a practical and large source of conversations
(Ritter et al., 2010). Others have also analyzed
Twitter conversations for natural language and so-
cial media research (Boyd et al., 2010; Danescu-
Niculescu-Mizil et al., 2011), but we collect con-
versations from the same set of dyads over several
months for a unique longitudinal dataset.
4.1 Collecting Twitter conversations
We define a Twitter conversation as a chain of
tweets where two users are consecutively replying
to each other?s tweets using the Twitter reply but-
ton. We identify dyads of English-tweeting users
with at least twenty conversations and collect their
tweets. We use an open source tool for detect-
ing English tweets
2
, and to protect users? privacy,
we replace Twitter userid, usernames and url in
tweets with random strings. This dataset consists
of 101,686 users, 61,451 dyads, 1,956,993 conver-
sations and 17,178,638 tweets which were posted
between August 2007 to July 2013.
4.2 Annotating self-disclosure level
To measure the accuracy of our model, we ran-
domly sample 101 conversations, each with ten
or fewer tweets, and ask three judges, fluent in
English, to annotate each tweet with the level of
self-disclosure. Judges first read and discussed
the definitions and examples of self-disclosure
level shown in (Barak and Gluck-Ofri, 2007), then
they worked separately on a Web-based platform.
Inter-rater agreement using Fleiss kappa (Fleiss,
1971) is 0.67.
5 Classification of Self-Disclosure Level
This section describes experiments and results of
SDTM as well as several other methods for classi-
fication of self-disclosure level.
We first start with the annotated dataset in sec-
tion 4.2 in which each tweet is annotated with SD
level. We then aggregate all of the tweets of a
conversation, and we compute the proportions of
tweets in each SD level. When the proportion of
2
https://github.com/shuyo/ldig
45
tweets at M or H level is equal to or greater than 0.2,
we take the level of the larger proportion and as-
sign that level to the conversation. When the pro-
portions of tweets at M or H level are both less than
0.2, we assign G to the SD level.
We compare SDTM with the following methods
for classifying tweets for SD level:
? LDA (Blei et al., 2003): A Bayesian topic
model. Each conversation is treated as a doc-
ument. Used in previous work (Bak et al.,
2012).
? MedLDA (Zhu et al., 2012): A super-
vised topic model for document classifica-
tion. Each conversation is treated as a doc-
ument and response variable can be mapped
to a SD level.
? LIWC (Tausczik and Pennebaker, 2010):
Word counts of particular categories. Used
in previous work (Houghton and Joinson,
2012).
? Seed words and trigrams (SEED): Occur-
rence of seed words and trigrams which are
described in section 3.3.
? ASUM (Jo and Oh, 2011): A joint model of
sentiment and topic using seed words. Each
sentiment can be mapped to a SD level. Used
in previous work (Bak et al., 2012).
? First-person pronouns (FirstP): Occurrence
of first-person pronouns which are described
in section 3.2. To identify first-person pro-
nouns, we tagged parts of speech in each
tweet with the Twitter POS tagger (Owoputi
et al., 2013).
SEED, LIWC, LDA and FirstP cannot be used
directly for classification, so we use Maximum en-
tropy model with outputs of each of those models
as features. We run MedLDA, ASUM and SDTM
20 times each and compute the average accuracies
and F-measure for each level. We set 40 topics
for LDA, MedLDA and ASUM, 60; 40; 40 top-
ics for SDTM K
G
,K
M
and K
H
respectively, and
set ? = ? = 0.1. To incorporate the seed words
and trigrams into ASUM and SDTM, we initial-
ize ?
G
,?
M
and ?
H
differently. We assign a high
value of 2.0 for each seed word and trigram for
that level, and a low value of 10
?6
for each word
that is a seed word for another level, and a default
Method Acc G F
1
M F
1
H F
1
Avg F
1
LDA 49.2 0.000 0.650 0.050 0.233
MedLDA 43.3 0.406 0.516 0.093 0.338
LIWC 49.2 0.341 0.607 0.180 0.376
SEED 52.0 0.412 0.600 0.178 0.397
ASUM 56.6 0.320 0.704 0.375 0.466
FirstP 63.2 0.630 0.689 0.095 0.472
SDTM 64.5 0.611 0.706 0.431 0.583
Table 4: SD level classification accuracies and F-
measures using annotated data. Acc is accuracy,
and G F
1
is F-measure for classifying the G level.
Avg F
1
is the average value of G F
1
, M F
1
and H
F
1
. SDTM outperforms all other methods com-
pared. The difference between SDTM and FirstP
is statistically significant (p-value < 0.05 for ac-
curacy, < 0.0001 for Avg F
1
).
value of 0.01 for all other words. This approach
is same as other topic model works (Jo and Oh,
2011; Kim et al., 2013).
As Table 4 shows, SDTM performs better than
other methods by accuracy and F-measure. LDA
and MedLDA generally show the lowest perfor-
mance, which is not surprising given these mod-
els are quite general and not tuned specifically
for this type of semi-supervised classification task.
LIWC and SEED perform better than LDA, but
these have quite low F-measure for G and H lev-
els. ASUM shows better performance for classi-
fying H level than others, but not for classifying
the G level. FirstP shows good F-measure for the
G level, but the H level F-measure is quite low,
even lower than SEED. Finally, SDTM has sim-
ilar performance in G and M level with FirstP, but
it performs better in H level than others. Classi-
fying the H level well is important because as we
will discuss later, the H level has the strongest rela-
tionship with longitudinal OSN usage (see Section
6.2), so SDTM is overall the best model for clas-
sifying self-disclosure levels.
6 Self-Disclosure and Conversation
Frequency
In this section, we investigate whether there is a
relationship between self-disclosure and conversa-
tion frequency over time. (Trepte and Reinecke,
2013) showed that frequent or high-level of self-
disclosure in online social networks (OSN) con-
tributes positively to OSN usage, and vice versa.
They showed this through an online survey with
46
Facebook and StudiVZ users. With SDTM, we
can automatically classify self-disclosure level of
a large number of conversations, so we investi-
gate whether there is a similar relationship be-
tween self-disclosure in conversations and subse-
quent frequency of conversations with the same
partner on Twitter. More specifically, we ask the
following two questions:
1. If a dyad displays high SD level in their con-
versations at a particular time period, would
they have more frequent conversations subse-
quently?
2. If a dyad shows high conversation frequency
at a particular time period, would they dis-
play higher SD in their subsequent conver-
sations?
6.1 Experiment Setup
We first run SDTM with all of our Twitter con-
versation data with 150; 120; 120 topics for
SDTM K
G
,K
M
and K
H
respectively. The
hyper-parameters are the same as in section 5. To
handle a large dataset, we employ a distributed al-
gorithm (Newman et al., 2009).
Table 5 shows some of the topics that were
prominent in each SD level by KL-divergence. As
expected, G level includes general topics such as
food, celebrity, soccer and IT devices, M level in-
cludes personal communication and birthday, and
finally, H level includes sickness and profanity.
For comparing conversation frequencies over
time, we divided the conversations into two sets
for each dyad. For the initial period, we include
conversations from the dyad?s first conversation to
60 days later. And for the subsequent period,
we include conversations during the subsequent 30
days.
We compute proportions of conversation for
each SD level for each dyad in the initial and
subsequent periods. Also, we define a new mea-
surement, SD level score for a dyad in the period,
which is a weighted sum of each conversation with
SD levels mapped to 1, 2, and 3, for the levels G,
M, and H, respectively.
6.2 Does self-disclosure lead to more frequent
conversations?
We investigate the effect of the level self-
disclosure on long-term use of OSN. We run lin-
ear regression with the intial SD level score as
1.0 1.5 2.0 2.5 3.0Initial SD level
1.0
0.5
0.0
0.5
1.0
1.5
# C
onv
ersa
ctio
n ch
ang
es p
rop
orti
on o
ver 
tim
e
Figure 3: Relationship between initial SD level
and conversation frequency changes over time.
The solid line is the linear regression line, and the
coefficient is 0.118 with p < 0.001, which shows
a significant positive relationship.
G level M level H level
Coeff (?) 0.094 0.419 0.464
p-value 0.1042 < 0.0001 < 0.0001
Table 6: Relationship between initial SD level
proportions and changes in conversation fre-
quency. For M and H levels, there is significant
positive relationship (p < 0.0001), but for the G
level, there is not (p > 0.1).
the independent variable, and the rate of change
in conversation frequency between initial period
and subsequent period as the dependent variable.
The result of regression is that the independent
variable?s coefficient is 0.118 with a low p-value
(p < 0.001). Figure 3 shows the scatter plot with
the regression line, and we can see that the slope
of regression line is positive.
We also investigate the importance of each SD
level for changes in conversation frequency. We
run linear regression with initial proportions of
each SD level as the independent variable, and
the same dependent variable as above. As ta-
ble 6 shows, there is no significant relationship
between the initial proportion of the G level and
the changes in conversation frequency (p > 0.1).
But for the M and H levels, the initial proportions
show positive and significant relationships with
the subsequent changes to the conversation fre-
quency (p < 0.0001). These results show that M
and H levels are correlated with changes to the fre-
quency of conversation.
47
G level M level H level
101 184 176 36 104 82 113 33 19
chocolate obama league send twitter going ass better lips
butter he?s win email follow party bitch sick kisses
good romney game i?ll tumblr weekend fuck feel love
cake vote season sent tweet day yo throat smiles
peanut right team dm following night shit cold softly
milk president cup address account dinner fucking hope hand
sugar people city know fb birthday lmao pain eyes
cream good arsenal check followers tomorrow shut good neck
Table 5: High ranked topics in each level by comparing KL-divergence with other level?s topics
0 20 40 60 80 100Initial conversation frequency
1.80
1.85
1.90
1.95
2.00
2.05
Sub
seq
uen
t SD
 lev
el
Figure 4: Relationship between initial conversa-
tion frequency and subsequent SD level. The
solid line is the linear regression line, and the co-
efficient is 0.0016 with p < 0.0001, which shows
a significant positive relationship.
6.3 Does high frequency of conversation lead
to more self-disclosure?
Now we investigate whether the initial conversa-
tion frequency is correlated with the SD level in
the subsequent period. We run linear regression
with the initial conversation frequency as the inde-
pendent variable, and SD level in the subsequent
period as the dependent variable.
The regression coefficient is 0.0016 with low p-
value (p < 0.0001). Figure 4 shows the scatter
plot. We can see that the slope of the regression
line is positive. This result supports previous re-
sults in social psychology (Leung, 2002) that fre-
quency of instant chat program ICQ and session
time were correlated to depth of SD in message.
7 Conclusion and Future Work
In this paper, we have presented the self-disclosure
topic model (SDTM) for discovering topics and
classifying SD levels from Twitter conversation
data. We devised a set of effective seed words and
trigrams, mined from a dataset of secrets. We also
annotated Twitter conversations to make a ground-
truth dataset for SD level. With annotated data, we
showed that SDTM outperforms previous methods
in classification accuracy and F-measure.
We also analyzed the relationship between SD
level and conversation frequency over time. We
found that there is a positive correlation between
initial SD level and subsequent conversation fre-
quency. Also, dyads show higher level of SD if
they initially display high conversation frequency.
These results support previous results in social
psychology research with more robust results from
a large-scale dataset, and show importance of
looking at SD behavior in OSN.
There are several future directions for this re-
search. First, we can improve our modeling for
higher accuracy and better interpretability. For
instance, SDTM only considers first-person pro-
nouns and topics. Naturally, there are patterns
that can be identified by humans but not captured
by pronouns and topics. Second, the number of
topics for each level is varied, and so we can
explore nonparametric topic models (Teh et al.,
2006) which infer the number of topics from the
data. Third, we can look at the relationship be-
tween self-disclosure behavior and general online
social network usage beyond conversations.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. Alice Oh was supported by the IT
R&D Program of MSIP/KEIT. [10041313, UX-
oriented Mobile SW Platform]
48
References
JinYeong Bak, Suin Kim, and Alice Oh. 2012. Self-
disclosure and relationship strength in twitter con-
versations. In Proceedings of ACL.
Azy Barak and Orit Gluck-Ofri. 2007. Degree and
reciprocity of self-disclosure in online forums. Cy-
berPsychology & Behavior, 10(3):407?417.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of HICSS.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words!: Lin-
guistic style accommodation in social media. In
Proceedings of WWW.
Valerian J. Derlega, Sandra Metts, Sandra Petronio,
and Stephen T. Margulis. 1993. Self-Disclosure,
volume 5 of SAGE Series on Close Relationships.
SAGE Publications, Inc.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
David J Houghton and Adam N Joinson. 2012.
Linguistic markers of secrets and sensitive self-
disclosure in twitter. In Proceedings of HICSS.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM.
Adam N Joinson and Carina B Paine. 2007. Self-
disclosure, privacy and the internet. The Oxford
handbook of Internet psychology, pages 237?252.
Sidney M Jourard. 1971. Self-disclosure: An experi-
mental analysis of the transparent self.
Suin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012.
Do you feel what i feel? social aspects of emotions
in twitter conversations. In Proceedings of ICWSM.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A hierarchical aspect-sentiment
model for online reviews. In Proceedings of AAAI.
Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeG-
root, Kevin R Meyer, Yuping Mao, and Brian Swaf-
ford. 2011. Attitudes toward online social con-
nection and self-disclosure as predictors of facebook
communication and relational closeness. Communi-
cation Research, 38(1):27?53.
Louis Leung. 2002. Loneliness, self-disclosure, and
icq (? i seek you?) use. CyberPsychology & Behav-
ior, 5(3):241?251.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Erika McCallister. 2010. Guide to protecting the confi-
dentiality of personally identifiable information. DI-
ANE Publishing.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Sharon Meraz. 2013. Public dialogue: Analysis of
tolerance in online discussions. In Proceedings of
ACL.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms
for topic models. Journal of Machine Learning Re-
search, 10:1801?1828.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of HLT-NAACL.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Charles Steinfield, Nicole B Ellison, and Cliff Lampe.
2008. Social capital, self-esteem, and use of on-
line social network sites: A longitudinal analy-
sis. Journal of Applied Developmental Psychology,
29(6):434?445.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the american statistical associ-
ation, 101(476).
Sabine Trepte and Leonard Reinecke. 2013. The re-
ciprocal effects of social network site use and the
disposition for self-disclosure: A longitudinal study.
Computers in Human Behavior, 29(3):1102 ? 1112.
Sarah I Vondracek and Fred W Vondracek. 1971. The
manipulation and measurement of self-disclosure in
preadolescents. Merrill-Palmer Quarterly of Behav-
ior and Development, 17(1):51?58.
Thomas Ashby Wills. 1985. Supportive functions
of interpersonal relationships. Social support and
health, xvii:61?82.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
EMNLP.
Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda:
maximum margin supervised topic models. Journal
of Machine Learning Research, 13:2237?2278.
49
