Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 11?18,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Mining Sentiments from Tweets
Akshat Bakliwal, Piyush Arora, Senthil Madhappan
Nikhil Kapre, Mukesh Singh and Vasudeva Varma
Search and Information Extraction Lab,
International Institute of Information Technology, Hyderabad.
{akshat.bakliwal, piyush.arora}@research.iiit.ac.in,
{senthil.m, nikhil.kapre, mukeshkumar.singh}@students.iiit.ac.in,
vv@iiit.ac.in
Abstract
Twitter is a micro blogging website, where
users can post messages in very short text
called Tweets. Tweets contain user opin-
ion and sentiment towards an object or per-
son. This sentiment information is very use-
ful in various aspects for business and gov-
ernments. In this paper, we present a method
which performs the task of tweet sentiment
identification using a corpus of pre-annotated
tweets. We present a sentiment scoring func-
tion which uses prior information to classify
(binary classification ) and weight various sen-
timent bearing words/phrases in tweets. Us-
ing this scoring function we achieve classifi-
cation accuracy of 87% on Stanford Dataset
and 88% on Mejaj dataset. Using supervised
machine learning approach, we achieve classi-
fication accuracy of 88% on Stanford dataset.
1 Introduction
With enormous increase in web technologies, num-
ber of people expressing their views and opinions
via web are increasing. This information is very
useful for businesses, governments and individuals.
With over 340+ million Tweets (short text messages)
per day, Twitter is becoming a major source of infor-
mation.
Twitter is a micro-blogging site, which is popular
because of its short text messages popularly known
as ?Tweets?. Tweets have a limit of 140 characters.
Twitter has a user base of 140+ million active users1
1As on March 21, 2012. Source:
http://en.wikipedia.org/wiki/Twitter
and thus is a useful source of information. Users
often discuss on current affairs and share their per-
sonals views on various subjects via tweets.
Out of all the popular social media?s like Face-
book, Google+, Myspace and Twitter, we choose
Twitter because 1) tweets are small in length, thus
less ambigious; 2) unbiased; 3) are easily accessible
via API; 4) from various socio-cultural domains.
In this paper, we introduce an approach which can
be used to find the opinion in an aggregated col-
lection of tweets. In this approach, we used two
different datasets which are build using emoticons
and list of suggestive words respectively as noisy la-
bels. We give a new method of scoring ?Popularity
Score?, which allows determination of the popular-
ity score at the level of individual words of a tweet
text. We also emphasis on various types and levels
of pre-processing required for better performance.
Roadmap for rest of the paper: Related work is
discussed in Section 2. In Section 3, we describe
our approach to address the problem of Twitter
sentiment classification along with pre-processing
steps.Datasets used in this research are discussed in
Section 4. Experiments and Results are presented in
Section 5. In Section 6, we present the feature vector
approach to twitter sentiment classification. Section
7 presents as discussion on the methods and we con-
clude the paper with future work in Section 8.
2 Related Work
Research in Sentiment Analysis of user generated
content can be categorized into Reviews (Turney,
2002; Pang et al, 2002; Hu and Liu, 2004), Blogs
(Draya et al, 2009; Chesley, 2006; He et al, 2008),
11
News (Godbole et al, 2007), etc. All these cat-
egories deal with large text. On the other hand,
Tweets are shorter length text and are difficult to
analyse because of its unique language and struc-
ture.
(Turney, 2002) worked on product reviews. Tur-
ney used adjectives and adverbs for performing
opinion classification on reviews. He used PMI-IR
algorithm to estimate the semantic orientation of the
sentiment phrase. He achieved an average accuracy
of 74% on 410 reviews of different domains col-
lected from Epinion. (Hu and Liu, 2004) performed
feature based sentiment analysis. Using Noun-Noun
phrases they identified the features of the products
and determined the sentiment orientation towards
each feature. (Pang et al, 2002) tested various ma-
chine learning algorithms on Movie Reviews. He
achieved 81% accuracy in unigram presence feature
set on Naive Bayes classifier.
(Draya et al, 2009) tried to identify domain spe-
cific adjectives to perform blog sentiment analysis.
They considered the fact that opinions are mainly
expressed by adjectives and pre-defined lexicons fail
to identify domain information. (Chesley, 2006) per-
formed topic and genre independent blog classifica-
tion, making novel use of linguistic features. Each
post from the blog is classified as positive, negative
and objective.
To the best of our knowledge, there is very less
amount of work done in twitter sentiment analy-
sis. (Go et al, 2009) performed sentiment analy-
sis on twitter. They identified the tweet polarity us-
ing emoticons as noisy labels and collected a train-
ing dataset of 1.6 million tweets. They reported an
accuracy of 81.34% for their Naive Bayes classi-
fier. (Davidov et al, 2010) used 50 hashtags and 15
emoticons as noisy labels to create a dataset for twit-
ter sentiment classification. They evaluate the effect
of different types of features for sentiment extrac-
tion. (Diakopoulos and Shamma, 2010) worked on
political tweets to identify the general sentiments of
the people on first U.S. presidential debate in 2008.
(Bora, 2012) also created their dataset based on
noisy labels. They created a list of 40 words (pos-
itive and negative) which were used to identify the
polarity of tweet. They used a combination of
a minimum word frequency threshold and Cate-
gorical Proportional Difference as a feature selec-
tion method and achieved the highest accuracy of
83.33% on a hand labeled test dataset.
(Agarwal et al, 2011) performed three class (pos-
itive, negative and neutral) classification of tweets.
They collected their dataset using Twitter stream
API and asked human judges to annotate the data
into three classes. They had 1709 tweets of each
class making a total of 5127 in all. In their research,
they introduced POS-specific prior polarity features
along with twitter specific features. They achieved
max accuracy of 75.39% for unigram + senti fea-
tures.
Our work uses (Go et al, 2009) and (Bora, 2012)
datasets for this research. We use Naive Bayes
method to decide the polarity of tokens in the tweets.
Along with that we provide an useful insight on how
preprocessing should be done on tweet. Our method
of Senti Feature Identification and Popularity Score
perform well on both the datasets. In feature vec-
tor approach, we show the contribution of individual
NLP and Twitter specific features.
3 Approach
Our approach can be divided into various steps.
Each of these steps are independent of the other but
important at the same time.
3.1 Baseline
In the baseline approach, we first clean the tweets.
We remove all the special characters, targets (@),
hashtags (#), URLs, emoticons, etc and learn the
positive & negative frequencies of unigrams in train-
ing. Every unigram token is given two probability
scores: Positive Probability (Pp) and Negative Prob-
ability (Np) (Refer Equation 1). We follow the same
cleaning process for the test tweets. After clean-
ing the test tweets, we form all the possible uni-
grams and check for their frequencies in the training
model. We sum up the positive and negative proba-
bility scores of all the constituent unigrams, and use
their difference (positive - negative) to find the over-
all score of the tweet. If tweet score is > 0 then it is
12
positive otherwise negative.
Pf = Frequency in Positive Training Set
Nf = Frequency in Negative Training Set
Pp = Positive Probability of the token.
= Pf/(Pf + Nf )
Np = Negative Probability of the token.
= Nf/(Pf + Nf )
(1)
3.2 Emoticons and Punctuations Handling
We make slight changes in the pre-processing mod-
ule for handling emoticons and punctuations. We
use the emoticons list provided by (Agarwal et al,
2011) in their research. This list2 is built from
wikipedia list of emoticons3 and is hand tagged into
five classes (extremely positive, positive, neutral,
negative and extremely negative). In this experi-
ment, we replace all the emoticons which are tagged
positive or extremely positive with ?zzhappyzz? and
rest all other emoticons with ?zzsadzz?. We append
and prepend ?zz? to happy and sad in order to pre-
vent them from mixing into tweet text. At the end,
?zzhappyzz? is scored +1 and ?zzsadzz? is scored -1.
Exclamation marks (!) and question marks (?)
also carry some sentiment. In general, ?!? is used
when we have to emphasis on a positive word and
??? is used to highlight the state of confusion or
disagreement. We replace all the occurrences of ?!?
with ?zzexclaimzz? and of ??? with ?zzquestzz?. We
add 0.1 to the total tweet score for each ?!? and sub-
tract 0.1 from the total tweet score for each ???. 0.1
is chosen by trial and error method.
3.3 Stemming
We use Porter Stemmer4 to stem the tweet words.
We modify porter stemmer and restrict it to step 1
only. Step 1 gets rid of plurals and -ed or -ing.
3.4 Stop Word Removal
Stop words play a negative role in the task of senti-
ment classification. Stop words occur in both pos-
itive and negative training set, thus adding more
ambiguity in the model formation. And also, stop
2http://goo.gl/oCSnQ
3http://en.wikipedia.org/wiki/List of emoticons
4http://tartarus.org/m?artin/PorterStemmer/
words don?t carry any sentiment information and
thus are of no use to us. We create a list of stop
words like he, she, at, on, a, the, etc. and ignore
them while scoring. We also discard words which
are of length ? 2 for scoring the tweet.
3.5 Spell Correction
Tweets are written in random form, without any fo-
cus given to correct structure and spelling. Spell
correction is an important part in sentiment analy-
sis of user- generated content. Users type certain
characters arbitrary number of times to put more em-
phasis on that. We use the spell correction algo-
rithm from (Bora, 2012). In their algorithm, they
replace a word with any character repeating more
than twice with two words, one in which the re-
peated character is placed once and second in which
the repeated character is placed twice. For example
the word ?swwweeeetttt? is replaced with 8 words
?swet?, ?swwet?, ?sweet?, ?swett?, ?swweet?, and so
on.
Another common type of spelling mistakes oc-
cur because of skipping some of characters from the
spelling. like ?there? is generally written as ?thr?.
Such types of spelling mistakes are not currently
handled by our system. We propose to use phonetic
level spell correction method in future.
3.6 Senti Features
At this step, we try to reduce the effect of non-
sentiment bearing tokens on our classification sys-
tem. In the baseline method, we considered all the
unigram tokens equally and scored them using the
Naive Bayes formula (Refer Equation 1). Here, we
try to boost the scores of sentiment bearing words.
In this step, we look for each token in a pre-defined
list of positive and negative words. We use the list of
of most commonly used positive and negative words
provided by Twitrratr5. When we come across a to-
ken in this list, instead of scoring it using the Naive
Bayes formula (Refer Equation 1), we score the to-
ken +/- 1 depending on the list in which it exist. All
the tokens which are missing from this list went un-
der step 3.3, 3.4, 3.5 and were checked for their oc-
currence after each step.
5http://twitrratr.com/
13
3.7 Noun Identification
After doing all the corrections (3.3 - 3.6) on a word,
we look at the reduced word if it is being converted
to a Noun or not. We identify the word as a Noun
word by looking at its part of speech tag in English
WordNet(Miller, 1995). If the majority sense (most
commonly used sense) of that word is Noun, we
discard the word while scoring. Noun words don?t
carry sentiment and thus are of no use in our experi-
ments.
3.8 Popularity Score
This scoring method boosts the scores of the most
commonly used words, which are domain specific.
For example, happy is used predominantly for ex-
pressing the positive sentiment. In this method, we
multiple its popularity factor (pF) to the score of
each unigram token which has been scored in the
previous steps. We use the occurrence frequency of
a token in positive and negative dataset to decide on
the weight of popularity score. Equation 2 shows
how the popularity factor is calculated for each to-
ken. We selected a threshold 0.01 min support as the
cut-off criteria and reduced it by half at every level.
Support of a word is defined as the proportion of
tweets in the dataset which contain this token. The
value 0.01 is chosen such that we cover a large num-
ber of tokens without missing important tokens, at
the same time pruning less frequent tokens.
Pf = Frequency in Positive Training Set
Nf = Frequency in Negative Training Set
if(Pf ?Nf ) > 1000)
pF = 0.9;
elseif((Pf ?Nf ) > 500)
pF = 0.8;
elseif((Pf ?Nf ) > 250)
pF = 0.7;
elseif((Pf ?Nf ) > 100)
pF = 0.5;
elseif((Pf ?Nf < 50))
pF = 0.1;
(2)
Figure 1 shows the flow of our approach.
Figure 1: Flow Chart of our Algorithm
4 Datasets
In this section, we explain the two datasets used in
this research. Both of these datasets are built using
noisy labels.
4.1 Stanford Dataset
This dataset(Go et al, 2009) was built automat-
ically using emoticons as noisy labels. All the
tweets which contain ?:)? were marked positive and
tweets containing ?:(? were marked negative. Tweets
that did not have any of these labels or had both
were discarded. The training dataset has ?1.6 mil-
lion tweets, equal number of positive and negative
tweets. The training dataset was annotated into two
classes (positive and negative) while the testing data
was hand annotated into three classes (positive, neg-
ative and neutral). For our experimentation, we use
only positive and negative class tweets from the test-
ing dataset for our experimentation. Table 1 gives
the details of dataset.
Training Tweets
Positive 800,000
Negative 800,000
Total 1,600,000
Testing Tweets
Positive 180
Negative 180
Objective 138
Total 498
Table 1: Stanford Twitter Dataset
14
4.2 Mejaj
Mejaj dataset(Bora, 2012) was built using noisy la-
bels. They collected a set of 40 words and manually
categorized them into positive and negative. They
label a tweet as positive if it contains any of the pos-
itive sentiment words and as negative if it contains
any of the negative sentiment words. Tweets which
do not contain any of these noisy labels and tweets
which have both positive and negative words were
discarded. Table 2 gives the list of words which were
used as noisy labels. This dataset contains only two
class data. Table 3 gives the details of the dataset.
Positive Labels Negative Labels
amazed, amused,
attracted, cheerful,
delighted, elated,
excited, festive, funny,
hilarious, joyful,
lively, loving,
overjoyed, passion,
pleasant, pleased,
pleasure, thrilled,
wonderful
annoyed, ashamed,
awful, defeated,
depressed,
disappointed,
discouraged,
displeased,
embarrassed, furious,
gloomy, greedy,
guilty, hurt, lonely,
mad, miserable,
shocked, unhappy,
upset
Table 2: Noisy Labels for annotating Mejaj Dataset
Training Tweets
Positive 668,975
Negative 795,661
Total 1,464,638
Testing Tweets
Positive 198
Negative 204
Total 402
Table 3: Mejaj Dataset
5 Experiment
In this section, we explain the experiments carried
out using the above proposed approach.
5.1 Stanford Dataset
On this dataset(Go et al, 2009), we perform a series
of experiments. In the first series of experiments,
we train on the given training data and test on the
testing data. In the second series of experiments,
we perform 5 fold cross validation using the training
data. Table 4 shows the results of each of these ex-
periments on steps which are explained in Approach
(Section 3).
In table 4, we give results for each step emoticons
and punctuations handling, spell correction, stem-
ming and stop word removal mentioned in Approach
Section (Section 3). The Baseline + All Combined
results refers to combination of these steps (emoti-
cons, punctuations, spell correction, Stemming and
stop word removal) performed together. Series 2 re-
sults are average of accuracy of each fold.
5.2 Mejaj Dataset
Similar series of experiments were performed on
this dataset(Bora, 2012) too. In the first series of
experiments, training and testing was done on the
respective given datasets. In the second series of ex-
periments, we perform 5 fold cross validation on the
training data. Table 5 shows the results of each of
these experiments.
In table 5, we give results for each step emoticons
and punctuations handling, spell correction, stem-
ming and stop word removal mentioned in Approach
Section (Section 3). The Baseline + All Combined
results refers to combination of these steps (emoti-
cons, punctuations, spell correction, Stemming and
stop word removal) performed together. Series 2 re-
sults are average of accuracy of each fold.
5.3 Cross Dataset
To validate the robustness of our approach, we ex-
perimented with cross dataset training and testing.
We trained our system on one dataset and tested on
the other dataset. Table 6 reports the results of cross
dataset evaluations.
6 Feature Vector Approach
In this feature vector approach, we form features us-
ing Unigrams, Bigrams, Hashtags (#), Targets (@),
Emoticons, Special Symbol (?!?) and used a semi-
supervised SVM classifier. Our feature vector com-
prised of 11 features. We divide the features into
two groups, NLP features and Twitter specific fea-
tures. NLP features include frequency of positive
15
Method Series 1 (%) Series 2 (%)
Baseline 78.8 80.1
Baseline + Emoticons + Punctuations 81.3 82.1
Baseline + Spell Correction 81.3 81.6
Baseline + Stemming 81.9 81.7
Baseline + Stop Word Removal 81.7 82.3
Baseline + All Combined (AC) 83.5 85.4
AC + Senti Features (wSF) 85.5 86.2
wSF + Noun Identification (wNI) 85.8 87.1
wNI + Popularity Score 87.2 88.4
Table 4: Results on Stanford Dataset
Method Series 1 (%) Series 2 (%)
Baseline 77.1 78.6
Baseline + Emoticons + Punctuations 80.3 80.4
Baseline + Spell Correction 80.1 80.0
Baseline + Stemming 79.1 79.7
Baseline + Stop Word Removal 80.2 81.7
Baseline + All Combined (AC) 82.9 84.1
AC + Senti Features (wSF) 86.8 87.3
wSF + Noun Identification (wNI) 87.6 88.2
wNI + Popularity Score 88.1 88.1
Table 5: Results on Mejaj Dataset
Method Training Dataset Testing Dataset Accuracy
wNI + Popularity Score Stanford Mejaj 86.4%
wNI + Popularity Score Mejaj Stanford 84.7%
Table 6: Results on Cross Dataset evaluation
NLP Unigram (f1) # of positive and negative unigramBigram (f2) # of positive and negative Bigram
Twitter Specific
Hashtags (f3) # of positive and negative hashtags
Emoticons (f4) # of positive and negative emoticons
URLs (f5) Binary Feature - presence of URLs
Targets (f6) Binary Feature - presence of Targets
Special Symbols (f7) Binary Feature - presence of ?!?
Table 7: Features and Description
16
Feature Set Accuracy (Stanford)
f1 + f2 85.34%
f3 + f4 + f7 53.77%
f3 + f4 + f5 + f6 + f7 60.12%
f1 + f2 + f3 + f4 + f7 85.89%
f1 + f2 + f3 + f4 +
f5 + f6 + f7 87.64%
Table 8: Results of Feature Vector Classifier on Stanford
Dataset
unigrams matched, negative unigrams matched, pos-
itive bigrams matched, negative bigrams matched,
etc and Twitter specific features included Emoti-
cons, Targets, HashTags, URLs, etc. Table 7 shows
the features we have considered.
HashTags polarity is decided based on the con-
stituent words of the hashtags. Using the list of pos-
itive and negative words from Twitrratr6, we try to
find if hashtags contains any of these words. If so,
we assign the polarity of that to the hashtag. For
example, ?#imsohappy? contains a positive word
?happy?, thus this hashtag is considered as posi-
tive hashtag. We use the emoticons list provided
by (Agarwal et al, 2011) in their research. This
list7 is built from wikipedia list of emoticons8 and
is hand tagged into five classes (extremely positive,
positive, neutral, negative and extremely negative).
We reduce this five class list to two class by merging
extremely positive and positive class to single posi-
tive class and rest other classes (extremely negative,
negative and neutral) to single negative class. Ta-
ble 8 reports the accuracy of our machine learning
classifier on Stanford dataset.
7 Discussion
In this section, we present a few examples evaluated
using our system. The following example denotes
the effect of incorporating the contribution of emoti-
cons on tweet classification. Example ?Ahhh I can?t
move it but hey w/e its on hell I?m elated right now
:-D?. This tweet contains two opinion words, ?hell?
and ?elated?. Using the unigram scoring method,
this tweet is classified neutral but it is actually posi-
6http://twitrratr.com/
7http://goo.gl/oCSnQ
8http://en.wikipedia.org/wiki/List of emoticons
tive. If we incorporate the effect of emoticon ?:-D?,
then this tweet is tagged positive. ?:-D? is a strong
positive emoticon.
Consider this example, ?Bill Clinton Fail -
Obama Win??. In this example, there are two senti-
ment bearing words, ?Fail? and ?Win?. Ideally this
tweet should be neutral but this is tagged as a posi-
tive tweet in the dataset as well as using our system.
In this tweet, if we calculate the popularity factor
(pF) for ?Win? and ?Fail?, they come out to be 0.9
and 0.8 respectively. Because of the popularity fac-
tor weight, the positive score domniates the negative
score and thus the tweet is tagged as positive. It is
important to identify the context flow in the text and
also how each of these words modify or depend on
the other words of the tweet.
For calculating the system performance, we as-
sume that the dataset which is used here is correct.
Most of the times this assumption is true but there
are a few cases where it fails. For example, this
tweet ?My wrist still hurts. I have to get it looked
at. I HATE the dr/dentist/scary places. :( Time to
watch Eagle eye. If you want to join, txt!? is tagged
as positive, but actually this should have been tagged
negative. Such erroneous tweets also effect the sys-
tem performance.
There are few limitations with the current pro-
posed approach which are also open research prob-
lems.
1. Spell Correction: In the above proposed ap-
proach, we gave a solution to spell correction
which works only when extra characters are en-
tered by the user. It fails when users skip some
characters like ?there? is spelled as ?thr?. We
propose the use of phonetic level spell correc-
tion to handle this problem.
2. Hashtag Segmentation: For handling hashtags,
we looked for the existence of the positive or
negative words9 in the hashtag. But there can
be some cases where it may not work correctly.
For example, ?#thisisnotgood?, in this hashtag
if we consider the presence of positive and neg-
ative words, then this hashtag is tagged posi-
tive (?good?). We fail to capture the presence
and effect of ?not? which is making this hash-
9word list taken from http://twitrratr.com/
17
tag as negative. We propose to devise and use
some logic to segment the hashtags to get cor-
rect constituent words.
3. Context Dependency: As discussed in one of
the examples above, even tweet text which is
limited to 140 characters can have context de-
pendency. One possible method to address this
problem is to identify the objects in the tweet
and then find the opinion towards those objects.
8 Conclusion and Future Work
Twitter sentiment analysis is a very important and
challenging task. Twitter being a microblog suffers
from various linguistic and grammatical errors. In
this research, we proposed a method which incorpo-
rates the popularity effect of words on tweet senti-
ment classification and also emphasis on how to pre-
process the Twitter data for maximum information
extraction out of the small content. On the Stanford
dataset, we achieved 87% accuracy using the scor-
ing method and 88% using SVM classifier. On Me-
jaj dataset, we showed an improvement of 4.77% as
compared to their (Bora, 2012) accuracy of 83.33%.
In future, This work can be extended through in-
corporation of better spell correction mechanisms
(may be at phonetic level) and word sense disam-
biguation. Also we can identify the target and enti-
ties in the tweet and the orientation of the user to-
wards them.
Acknowledgement
We would like to thank Vibhor Goel, Sourav Dutta
and Sonil Yadav for helping us with running SVM
classifier on such a large data.
References
Agarwal, A., Xie, B., Vovsha, I., Rambow, O. and Pas-
sonneau, R. (2011). Sentiment analysis of Twitter
data. In Proceedings of the Workshop on Languages
in Social Media LSM ?11.
Bora, N. N. (2012). Summarizing Public Opinions in
Tweets. In Journal Proceedings of CICLing 2012,
New Delhi, India.
Chesley, P. (2006). Using verbs and adjectives to auto-
matically classify blog sentiment. In In Proceedings
of AAAI-CAAW-06, the Spring Symposia on Compu-
tational Approaches.
Davidov, D., Tsur, O. and Rappoport, A. (2010). En-
hanced sentiment learning using Twitter hashtags and
smileys. In Proceedings of the 23rd International Con-
ference on Computational Linguistics: Posters COL-
ING ?10.
Diakopoulos, N. and Shamma, D. (2010). Characterizing
debate performance via aggregated twitter sentiment.
In Proceedings of the 28th international conference on
Human factors in computing systems ACM.
Draya, G., Planti, M., Harb, A., Poncelet, P., Roche,
M. and Trousset, F. (2009). Opinion Mining from
Blogs. In International Journal of Computer Informa-
tion Systems and Industrial Management Applications
(IJCISIM).
Go, A., Bhayani, R. and Huang, L. (2009). Twitter Sen-
timent Classification using Distant Supervision. In
CS224N Project Report, Stanford University.
Godbole, N., Srinivasaiah, M. and Skiena, S. (2007).
Large-Scale Sentiment Analysis for News and Blogs.
In Proceedings of the International Conference on We-
blogs and Social Media (ICWSM).
He, B., Macdonald, C., He, J. and Ounis, I. (2008). An
effective statistical approach to blog post opinion re-
trieval. In Proceedings of the 17th ACM conference on
Information and knowledge management CIKM ?08.
Hu, M. and Liu, B. (2004). Mining Opinion Features in
Customer Reviews. In AAAI.
Miller, G. A. (1995). WordNet: A Lexical Database for
English. Communications of the ACM 38, 39?41.
Pang, B., Lee, L. and Vaithyanathan, S. (2002). Thumbs
up? Sentiment Classification using Machine Learning
Techniques.
Turney, P. D. (2002). Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. In ACL.
18
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 49?58,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis of Political Tweets: Towards an Accurate Classifier
Akshat Bakliwal1, Jennifer Foster2, Jennifer van der Puil3?,
Ron O?Brien4, Lamia Tounsi2 and Mark Hughes5
1Search and Information Extraction Lab, IIIT-Hyderabad, India
2NCLT/CNGL, School of Computing, Dublin City University, Ireland
3Department of Computer Science and Statistics, Trinity College, Ireland
4Quiddity, Dublin, Ireland
5CLARITY, School of Computing, Dublin City University, Ireland
1akshat.bakliwal@research.iiit.ac.in
2,5{jfoster,ltounsi,mhughes}@computing.dcu.ie
3jvanderp@tcd.ie
4ron@quiddity.ie
Abstract
We perform a series of 3-class sentiment clas-
sification experiments on a set of 2,624 tweets
produced during the run-up to the Irish Gen-
eral Elections in February 2011. Even though
tweets that have been labelled as sarcastic
have been omitted from this set, it still rep-
resents a difficult test set and the highest
accuracy we achieve is 61.6% using super-
vised learning and a feature set consisting
of subjectivity-lexicon-based scores, Twitter-
specific features and the top 1,000 most dis-
criminative words. This is superior to various
naive unsupervised approaches which use sub-
jectivity lexicons to compute an overall senti-
ment score for a <tweet,political party> pair.
1 Introduction
Supervised machine learning using minimal feature
engineering has been shown to work well in binary
positive/negative sentiment classification tasks on
well-behaved datasets such as movie reviews (Pang
et al, 2002). In this paper we describe sentiment
analysis experiments in a more complicated setup:
the task is three-class positive/negative/neutral clas-
sification, the sentiment being classified is not at the
general document level but rather directed towards a
topic, the documents are tweets, and the topic is poli-
tics, specifically the Irish General Election of Febru-
ary 2011.
?Akshat Bakliwal and Jennifer van der Puil carried out their
part of this work while employed as summer interns at the Cen-
tre for Next Generation Localisation(CNGL) in the School of
Computing, DCU.
The dataset used in the experiments contains
tweets which were collected in the run up to the elec-
tion and which were subsequently doubly annotated
as positive, negative or neutral towards a particular
political party or party leader. The annotators also
marked a tweet as sarcastic if its literal sentiment
was different to its actual sentiment. Before explor-
ing the thorny issue of sentiment classification in the
face of sarcasm, we simplify the problem by first try-
ing to establish some sentiment analysis baselines
for those tweets which were not deemed to be sar-
castic.
We first explore a naive approach in which a sub-
jectivity lexicon is used as the primary source of in-
formation in determining whether sentiment towards
a political party or party leader is positive, negative
or neutral. The best version of this method achieves
an accuracy of 58.9, an absolute improvement of 4.9
points over the majority baseline (54%) in which all
tweets are classified as neutral. When these lexi-
con scores are combined with bag-of-word features
and some Twitter-specific features in a supervised
machine learning setup, this accuracy increases to
61.6%.
The paper is organised as follows: related work
is described in Section 2, followed by a brief dis-
cussion of the 2011 Irish General Election in Sec-
tion 3, a description of the dataset in Section 4
and a description of the natural language processing
tools and resources employed in Section 5. In Sec-
tion 6, the unsupervised lexicon-based approach is
presented and its limitations discussed. Section 7 de-
scribes the machine-learning-based experiments and
Section 8 concludes and provides hints towards fu-
49
ture work with this new dataset.
2 Previous Work
The related work can be divided into two groups,
general sentiment analysis research and research
which is devoted specifically to the political domain.
2.1 General Sentiment Analysis
Research in the area of sentiment mining started
with product (Turney, 2002) and movie (Pang et al,
2002) reviews. Turney (2002) used Pointwise Mu-
tual Information (PMI) to estimate the sentiment ori-
entation of phrases. Pang et al (2002) employed
supervised learning with various set of n-gram fea-
tures, achieving an accuracy of almost 83% with un-
igram presence features on the task of document-
level binary sentiment classification. Research on
other domains and genres including blogs (Chesley,
2006) and news (Godbole et al, 2007) followed.
Early sentiment analysis research focused on
longer documents such as movie reviews and blogs.
Microtext on the other hand restricts the writer to a
more concise expression of opinion. Smeaton and
Bermingham (2010) tested the hypothesis that it is
easier to classify sentiment in microtext as compared
to longer documents. They experimented with mi-
crotext from Twitter, microreviews from blippr, blog
posts and movie reviews and concluded that it is eas-
ier to identify sentiment from microtext. However,
as they move from contextually sparse unigrams to
higher n-grams, it becomes more difficult to improve
the performance of microtext sentiment classifica-
tion, whereas higher-order information makes it eas-
ier to perform classification of longer documents.
There has been some research on the use of pos-
itive and negative emoticons and hashtags in tweets
as a proxy for sentiment labels (Go et al, 2009; Pak
and Paroubek, 2010; Davidov et al, 2010; Bora,
2012). Bakliwal et al (2012) emphasized the im-
portance of preprocessing and proposed a set of
features to extract maximum sentiment information
from tweets. They used unigram and bigram fea-
tures along with features which are more associated
with tweets such as emoticons, hashtags, URLs, etc.
and showed that combining linguistic and Twitter-
specific features can boost the classification accu-
racy.
2.2 Political Sentiment Analysis
In recent years, there has been growing interest in
mining online political sentiment in order to pre-
dict the outcome of elections. One of the most in-
fluential papers is that of Tumasjan et al (2010)
who focused on the 2009 German federal election
and investigated whether Twitter can be used to pre-
dict election outcomes. Over one hundred thousand
tweets dating from August 13 to September 19, 2009
containing the names of the six parties represented
in the German parliament were collected. LIWC
2007 (Pennebaker et al, 2007) was then used to ex-
tract sentiment from the tweets. LIWC is a text anal-
ysis software developed to assess emotional, cog-
nitive and structural components of text samples
using a psychometrically validated internal dictio-
nary. Tumasjan et al concluded that the number of
tweets/mentions of a party is directly proportional to
the probability of winning the elections.
O?Connor et al (2010) investigated the extent to
which public opinion polls were correlated with po-
litical sentiment expressed in tweets. Using the Sub-
jectivity Lexicon (Wilson et al, 2005), they estimate
the daily sentiment scores for each entity. A tweet is
defined as positive if it contains a positive word and
vice versa. A sentiment score for that day is calcu-
lated as the ratio of the positive count over the neg-
ative count. They find that their sentiment scores
were correlated with opinion polls on presidential
job approval but less strongly with polls on electoral
outcome.
Choy et al (2011) discuss the application of on-
line sentiment detection to predict the vote percent-
age for each of the candidates in the Singapore pres-
idential election of 2011. They devise a formula to
calculate the percentage vote each candidate will re-
ceive using census information on variables such as
age group, sex, location, etc. They combine this
with a sentiment-lexicon-based sentiment analysis
engine which calculates the sentiment in each tweet
and aggregates the positive and negative sentiment
for each candidate. Their model was able to predict
the narrow margin between the top two candidates
but failed to predict the correct winner.
Wang et al (2012) proposed a real-time sentiment
analysis system for political tweets which was based
on the U.S. presidential election of 2012. They col-
50
lected over 36 million tweets and collected the sen-
timent annotations using Amazon Mechanical Turk.
Using a Naive Bayes model with unigram features,
their system achieved 59% accuracy on the four-
category classification.
Bermingham and Smeaton (2011) are also con-
cerned with predicting electoral outcome, in partic-
ular, the outcome of the Irish General Election of
2011 (the same election that we focused on). They
analyse political sentiment in tweets by means of su-
pervised classification with unigram features and an
annotated dataset different to and larger than the one
we present, achieving 65% accuracy on the task of
positive/negative/neutral classification. They con-
clude that volume is a stronger indicator of election
outcome than sentiment, but that sentiment still has
a role to play.
Gayo-Avello (2012) calls into question the use of
Twitter for election outcome prediction. Previous
works which report positive results on this task using
data from Twitter are surveyed and shortcomings in
their methodology and/or assumptions noted. In this
paper, our focus is not the (non-) predictive nature of
political tweets but rather the accurate identification
of any sentiment expressed in the tweets. If the ac-
curacy of sentiment analysis of political tweets can
be improved (or its limitations at least better under-
stood) then this will likely have a positive effect on
its usefulness as an alternative or complement to tra-
ditional opinion polling.
3 #ge11: The Irish General Election 2011
The Irish general elections were held on February
25, 2011. 165 representatives were elected across 43
constituencies for the Da?il, the main house of parlia-
ment. Eight parties nominated their candidates for
election and a coalition (Fine Gael and Labour) gov-
ernment was formed. The parties in the outgoing
coalition government, Fianna Fa?il and the Greens,
suffered disastrous defeats, the worst defeat of a sit-
ting government since the foundatation of the State
in 1922.
Gallagher and Marsh (2011, chapter 5) discuss the
use of social media by parties, candidates and vot-
ers in the 2011 election and conclude that it had a
much more important role to play in this election
than in the previous one in 2007. On the role of Twit-
ter in particular, they report that ?Twitter was less
widespread among candidates [than Facebook], but
it offered the most diverse source of citizen coverage
during the election, and it has been integrated into
several mainstream media?. They estimated that 7%
of the Irish population had a Twitter account at the
time of the election.
4 Dataset
We compiled a corpus of tweets using the Twitter
search API between 20th and the 25th of January
2011 (one month before the election). We selected
the main political entities (the five biggest politi-
cal parties ? Fianna Fa?il, Fine Gael, Labour, Sinn
Fe?in and the Greens ? and their leaders) and per-
form query-based search to collect the tweets relat-
ing to these entities. The resulting dataset contains
7,916 tweets of which 4,710 are retweets or dupli-
cates, leaving a total of 3,206 tweets.
The tweets were annotated by two Irish annota-
tors with a knowledge of the Irish political land-
scape. Disagreements between the two annotators
were studied and resolved by a third annotator. The
annotators were asked to identify the sentiment as-
sociated with the topic (or entity) of the tweet. An-
notation was performed using the following 6 labels:
? pos: Tweets which carry positive sentiment to-
wards the topic
? neg: Tweets which carry negative sentiment to-
wards the topic
? mix: Tweets which carry both positive and neg-
ative sentiment towards the topic
? neu: Tweets which do not carry any sentiment
towards the topic
? nen: Tweets which were written in languages
other than English.
? non: Tweets which do not have any mention
or relation to the topic. These represent search
errors.
In addition to the above six classes, annotators were
asked to flag whether a tweet was sarcastic.
The dataset which we use for the experiments
described in this paper contains only those tweets
51
Positive Tweets 256 9.75%
Negative Tweets 950 36.22%
Neutral Tweets 1418 54.03%
Total Tweets 2624
Table 1: Class Distribution
that have been labelled as either positive, negative
or neutral, i.e. non-relevant, mixed-sentiment and
non-English tweets are discarded. We also simplify
our task by omitting those tweets which have been
flagged as sarcastic by one or both of the annotators,
leaving a set of 2,624 tweets with a class distribution
as shown in Table 1.
5 Tools and Resources
In the course of our experiments, we use two differ-
ent subjectivity lexicons, one part-of-speech tagger
and one parser. For part-of-speech tagging we use
a tagger (Gimpel et al, 2011) designed specifically
for tweets. For parsing, we use the Stanford parser
(Klein and Manning, 2003). To identify the senti-
ment polarity of a word we use:
1. Subjectivity Lexicon (SL) (Wilson et al,
2005): This lexicon contains 8,221 words
(6,878 unique forms) of which 3,249 are adjec-
tives, 330 are adverbs, 1,325 are verbs, 2,170
are nouns and remaining (1,147) words are
marked as anypos. There are many words
which occur with two or more different part-of-
speech tags. We extend SL with 341 domain-
specific words to produce an extended SL.
2. SentiWordNet 3.0 (SWN) (Baccianella et al,
2010): With over 100+ thousand words, SWN
is far larger than SL but is likely to be noisier
since it has been built semi-automatically. Each
word in the lexicon is associated with both a
positive and negative score, and an objective
score given by (1), i.e. the positive, negative
and objective score sum to 1.
ObjScore = 1?PosScore?NegScore (1)
6 Naive Lexicon-based Classification
In this section we describe a naive approach to sen-
timent classification which does not make use of la-
belled training data but rather uses the information
in a sentiment lexicon to deduce the sentiment ori-
entation towards a political party in a tweet (see
Liu (2010) for an overview of this unsupervised
lexicon-based approach). In Section 6.1, we present
the basic method along with some variants which
improve on the basic method by making use of infor-
mation about part-of-speech, negation and distance
from the topic. In Section 6.2, we examine some
of the cases which remain misclassified by our best
lexicon-based method. In Section 6.3, we discuss
briefly those tweets that have been labelled as sar-
castic.
6.1 Method and Results
Our baseline lexicon-based approach is as follows:
we look up each word in our sentiment lexicon and
sum up the scores to corresponding scalars. The re-
sults are shown in Table 2. Note that the most likely
estimated class prediction is neutral with a probabil-
ity of .5403 (1418/2624).
6.1.1 Which Subjectivity Lexicon?
The first column shows the results that we obtain
when the lexicon we use is our extended version of
the SL lexicon. The results in the second column
are those that result from using SWN. In the third
column, we combine the two lexicons. We define
a combination pattern of Extended-SL and SWN in
which we prioritize Extended-SL because it is man-
ually checked and some domain-specific words are
added. For the words which were missing from
Extended-SL (SWN), we assign them the polarity of
SWN (Extended-SL). Table 3 explains exactly how
the scores from the two lexicons are combined. Al-
though SWN slightly outperforms Extended-SL for
the baseline lexicon-based approach (first row of Ta-
ble 2), it is outperformed by Extended-SL and the
combinaton of the two lexicons for all the variants.
We can conclude from the full set of results in Ta-
ble 2 that SWN is less useful than Extended-SL or
the combination of SWN and Extended-SL.
6.1.2 Filtering by Part-of-Speech
The results in the first row of Table 2 represent
our baseline experiment in which each word in the
tweet is looked up in the sentiment lexicon and
its sentiment score added to a running total. We
achieve a classification accuracy of 52.44% with the
52
Method Extended-SL SWN Combined
3-Class Classification (Pos vs
Neg vs Neu)
Correct Accuracy Correct Accuracy Correct Accuracy
Baseline 1376 52.44% 1379 52.55% 1288 49.09%
Baseline + Adj 1457 55.53% 1449 55.22% 1445 55.07%
Baseline + Adj + S 1480 56.40% 1459 55.60% 1481 56.44%
Baseline + Adj + S + Neg 1495 56.97% 1462 55.72% 1496 57.01%
Baseline + Adj + S + Neg +
Phrases
1511 57.58% 1479 56.36% 1509 57.51%
Baseline + Adj + S + Neg +
Phrases + Than
1533 58.42% 1502 57.24% 1533 58.42%
Distance Based Scoring:
Baseline + Adj + S + Neg +
Phrases + Than
1545 58.88% 1506 57.39% 1547 58.96%
Sarcastic Tweets 87/344 25.29% 81/344 23.55% 87/344 25.29%
Table 2: 3-class classification using the naive lexicon-based approach. The majority baseline is 54.03%.
Extended-
SL
polarity
SWN
Polarity
Combination
Polarity
-1 -1 -2
-1 0 -1
-1 1 -1
0 -1 -0.5
0 0 0
0 1 0.5
1 -1 1
1 0 1
1 1 2
Table 3: Combination Scheme of extended-SL and SWN.
Here 0 represents either a neutral word or a word missing
from the lexicon.
Extended-SL lexicon. We speculate that this low
accuracy is occurring because too many words that
appear in the sentiment lexicon are included in the
overall sentiment score without actually contribut-
ing to the sentiment towards the topic. To refine our
approach one step further, we use part-of-speech in-
formation and consider only adjectives for the clas-
sification of tweets since adjectives are strong in-
dicators of sentiment (Hatzivassiloglou and Wiebe,
2000). We achieve an accuracy improvement of ap-
proximately three absolute points, and this improve-
ment holds true for both sentiment lexicons. This
supports our hypothesis that we are using irrelevant
information for classification in the baseline system.
Our next improvement (third row of Table 2)
comes from mapping all inflected forms to their
stems (using the Porter stemmer). Examples of in-
flected forms that are reduced to their stems are de-
lighted or delightful. Using stemming with adjec-
tives over the baseline, we achieve an accuracy of
56.40% with Extended-SL.
6.1.3 Negation
?Negation is a very common linguistic construc-
tion that affects polarity and, therefore, needs to
be taken into consideration in sentiment analysis?
(Councill et al, 2010). We perform negation han-
dling in tweets using two different approaches. In
the first approach, we first identify negation words
53
and reverse the polarity of sentiment-bearing words
within a window of three words. In the second ap-
proach, we try to resolve the scope of the negation
using syntactic parsing. The Stanford dependency
scheme (de Marneffe and Manning, 2008) has a spe-
cial relation (neg) to indicate negation. We reverse
the sentiment polarity of a word marked via the neg
relation as being in the scope of a negation. Using
the first approach, we see an improvement of 0.6%
in the classification accuracy with the Extended-SL
lexicon. Using the second approach, we see an
improvement of 0.5%. Since there appears to be
very little difference between the two approaches to
negation-handling and in order to reduce the compu-
tational burden of running the Stanford parser each
time to obtain the dependencies, we continue further
experiments with the first method only. Using base-
line + stemming + adjectives + neg we achieve an
accuracy of 56.97% with the Extended-SL lexicon.
6.1.4 Domain-specific idioms
In the context of political tweets we see many
sentiment-bearing idioms and fixed expressions, e.g.
god save us, X for Taoiseach1, wolf in sheep?s cloth-
ing, etc. In our study, we had a total of 89 phrases.
When we directly account for these phrases, we
achieve an accuracy of 57.58% (an absolute im-
provement of 0.6 points over the last step).
6.1.5 Comparative Expressions
Another form of expressing an opinion towards
an entity is by comparing the entity with some other
entity. For example consider the tweet:
Fast Food sounds like a better vote than Fianna Fail.
(2)
In this tweet, an indirect negative sentiment is ex-
pressed towards the political party Fianna Fa?il. In
order to take into account such constructions, the
following procedure is applied: we divide the tweet
into two parts, left and right. The left part contains
the text which comes before the than and the right
part contains the text which comes after than, e.g.
Tweet: ?X is better than Y?
Left: ?X is better?
Right: ?Y?.
1The term Taoiseach refers to the Irish Prime Minister.
We then use the following strategy to calculate the
polarity of the tweet oriented towards the entity:
S left = sentiment score of Left.
S right = sentiment score of Right.
Ent pos left = if entity is left of
?than?, then 1, otherwise ? 1.
Ent pos right = if entity is right of
?than?, then 1, otherwise ? 1.
S(tweet) = Ent pos left ? S left +
Ent pos right ? S right. (3)
So in (2) above the entity, Fianna Fa?il, is to the
right of than meaning that its Ent pos right value
is 1 and its Ent pos left value is -1. This has the
effect of flipping the polarity of the positive word
better. By including the ?than? comparison, we see
an improvement of absolute 0.8% (third last row of
Table 2).
6.1.6 Distance Scoring
To emphasize the topic-oriented nature of our sen-
timent classification, we also define a distance-based
scoring function where we define the overall score
of the tweet as given in (4). Here dis(word) is de-
fined as number of words between the topic (i.e. the
political entity) and the sentiment word.
S(tweet) =
n?
i=1
S(wordi)/dis(wordi). (4)
The addition of the distance information further en-
hanced our system accuracy by 0.45%, taking it to
58.88% (second last row of Table 2). Our highest
overall accuracy (58.96) is achieved in this setting
using the combined lexicon.
It should be noted that this lexicon-based ap-
proach is overfitting to our dataset since the list of
domain-specific phrases and the form of the com-
parative constructions have been obtained from the
dataset itself. This means that we are making a
strong assumption about the representativeness of
this dataset and accuracy on a held-out test set is
likely to be lower.
6.2 Error Analysis
In this section we discuss pitfalls of the naive
lexicon-based approach with the help of some exam-
ples (see Table 4). Consider the first example from
54
the table, @username and u believe people in fianna
fail . What are you a numbskull or a journalist ?
In this tweet, we see that negative sentiment is im-
parted by the question part of the tweet, but actually
there are no sentiment adjectives. The word numb-
skull is contributing to the sentiment but is tagged as
a noun and not as an adjective. This tweet is tagged
as negative by our annotators and as neutral by our
lexicon-based classifier.
Consider the second example from Table 4,
@username LOL . A guy called to our house tonight
selling GAA tickets . His first words were : I?m
not from Fianna Fail . This is misclassified because
there are no sentiment bearing words according to
the sentiment lexicon. The last tweet in the table rep-
resents another example of the same problem. Note
however that the emoticon :/ in the last tweet and the
web acronym LOL in the second tweet are providing
hints which our system is not making use of.
In the third example from Table 4, @username
Such scary words .? Sinn Fein could top the poll ?
in certain constituencies . I feel sick at the thought
of it . ? In this example, we have three sentiment
bearing words: scary, top and sick. Two of the three
words are negative and one word is positive. The
word scary is stemmed incorrectly as scari which
means that it is out of the scope of our lexicons.
If we just count the number of sentiment words re-
maining, then this tweet is labelled as neutral but ac-
tually is negative with respect to the party Sinn Fe?in.
We proposed the use of distance as a measure of re-
latedness to the topic and we observed a minor im-
provement in classification accuracy. However, for
this example, the distance-based approach does not
work. The word top is just two words away from the
topic and thus contributes the maximum, resulting in
the whole tweet being misclassified as positive.
6.3 Sarcastic Tweets
?Political discouse is plagued with humor, double
entendres, and sarcasm; this makes determining po-
litical preference of users hard and inferring voting
intention even harder.?(Gayo-Avello, 2012)
As part of the annotation process, annotators were
asked to indicate whether they thought a tweet ex-
hibited sarcasm. Some examples of tweets that were
annotated as sarcastic are shown in Table 5.
We made the decision to omit these tweets from
the main sentiment classification experiments under
the assumption that they constituted a special case
which would be better handled by a different clas-
sifier. This decision is vindicated by the results in
the last row of Table 2 which show what happens
when we apply our best classifier (Distance-based
Scoring: Baseline+Adj+S+Neg+Phrases+Than) to
the sarcastic tweets ? only a quarter of them are cor-
rectly classified. Even with a very large and highly
domain-tuned lexicon, the lexicon-based approach
on its own will struggle to be of use for cases such
as these, but the situation might be improved were
the lexicon to be used in conjunction with possible
sarcasm indicators such as exclamation marks.
7 Supervised Machine Learning
Although our dataset is small, we investigate
whether we can improve over the lexicon-based ap-
proach by using supervised machine learning. As
our learning algorithm, we employ support vector
machines in a 5-fold cross validation setup. The tool
we use is SVMLight (Joachims, 1999).
We explore two sets of features. The first are the
tried-and-tested unigram presence features which
have been used extensively not only in sentiment
analysis but in other text classification tasks. As we
have only 2,624 training samples, we performed fea-
ture selection by ranking the features using the Chi-
squared metric.
The second feature set consists of 25 features
which are inspired by the work on lexicon-based
classification described in the previous section.
These are the counts of positive, negative, objec-
tive words according to each of the three lexicons
and the corresponding sentiment scores for the over-
all tweets. In total there are 19 such features. We
also employ six Twitter-related presence features:
positive emoticons, negative emoticons, URLs, pos-
itive hashtags, negative hashtags and neutral hash-
tags. For further reference we call this second set of
features our ?hand-crafted? features.
The results are shown in Table 6. We can see
that using the hand-crafted features alone barely im-
proves over the majority baseline of 54.03 but it does
improve over our baseline lexicon-based approach
(see first row of Table 2). Encouragingly, we see
some benefit from using these features in conjunc-
55
Tweet Topic
Manual
Polar-
ity
Calculated
Polarity
Reason for
misclassifica-
tion
@username and u believe people in fianna fail .
What are you a numbskull or a journalist ?
Fianna
Fa?il
neg neu
Focus only on
adjectives
@username LOL . A guy called to our house
tonight selling GAA tickets . His first words were :
I?m not from Fianna Fail .
Fianna
Fa?il
neg neu
No sentiment
words
@username Such scary words .? Sinn Fein could
top the poll ? in certain constituencies . I feel sick
at the thought of it .
Sinn
Fe?in
neg pos
Stemming
and word
distance order
@username more RTE censorship . Why are they
so afraid to let Sinn Fein put their position across .
Certainly couldn?t be worse than ff
Sinn
Fe?in
pos neg
contribution
of afraid
Based on this programme the winners will be Sinn
Fein & Gilmore for not being there #rtefl
Sinn
Fe?in
pos neu
Focus only on
adjectives
#thefrontline pearce Doherty is a spoofer ! Vote
sinn fein and we loose more jobs
Sinn
Fe?in
neg pos
Focus only on
adjectives &
contribution
of phrase Vote
X
@username Tread carefully Conor . BNP
endorsing Sinn Fin etc . etc .
Sinn
Fe?in
neg neu
No sentiment
words
@username ah dude . You made me go to the fine
gael web site ! :/
Fine
Gael
neg neu
No sentiment
words
Table 4: Misclassification Examples
Feature Set # Features Accuracy
# samples = 2624 SVM Light
Hand-crafted 25 54.76
Unigram
7418 55.22
Top 1000 58.92
Top 100 56.86
Unigram + Hand-crafted
7444 54.73
Top 1000 61.62
Top 100 59.53
Table 6: Results of 3-Class Classification using Super-
vised Machine Learning
tion with the unigram features. Our best overall re-
sult of 61.62% is achieved by using the Top 1000 un-
igram features together with these hand-crafted fea-
tures. This result seems to suggest that, even with
only a few thousand training instances, employing
supervised machine learning is still worthwhile.
8 Conclusion
We have introduced a new dataset of political tweets
which will be made available for use by other re-
searchers. Each tweet in this set has been annotated
for sentiment towards a political entity, as well as
for the presence of sarcasm. Omitting the sarcastic
tweets from our experiments, we show that we can
classify a tweet as being positive, negative or neutral
towards a particular political party or party leader
with an accuracy of almost 59% using a simple ap-
proach based on lexicon lookup. This improves over
the majority baseline by almost 5 absolute percent-
age points but as the classifier uses information from
the test set itself, the result is likely to be lower on
a held-out test set. The accuracy increases slightly
when the lexicon-based information is encoded as
features and employed together with bag-of-word
features in a supervised machine learning setup.
Future work involves carrying out further exper-
56
Sarcastic Tweets
Ah bless Brian Cowen?s little cotton socks! He?s staying on as leader of FF because its better for the
country. How selfless!
So now Brian Cowen is now Minister for foreign affairs and Taoiseach? Thats exactly what he needs
more responsibilities http://bbc.in/hJI0hb
Mary Harney is going. Surprise surprise! Brian Cowen is going to be extremely busy with all these
portfolios to administer. Super hero!
Now in its darkest hour Fianna Fail needs. . . Ivor!
Labour and Fine Gael have brought the election forward by 16 days Crisis over Ireland is SAVED!! #vinb
@username Maybe one of those nice Sinn Fein issue boiler suits? #rtefl
I WILL vote for Fine Gael if they pledge to dress James O?Reilly as a leprechaun and send him
to the White House for Paddy?s Day.
Table 5: Examples of tweets which have been flagged as sarcastic
iments on those tweets that have been annotated as
sarcastic, exploring the use of syntactic dependency
paths in the computation of distance between a word
and the topic, examining the role of training set class
bias on the supervised machine learning results and
exploring the use of distant supervision to obtain
more training data for this domain.
Acknowledgements
Thanks to Emmet O Briain, Lesley Ni Bhriain and
the anonymous reviewers for their helpful com-
ments. This research has been supported by En-
terprise Ireland (CFTD/2007/229) and by Science
Foundation Ireland (Grant 07/CE/ I1142) as part of
the CNGL (www.cngl.ie) at the School of Comput-
ing, DCU.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Akshat Bakliwal, Piyush Arora, Senthil Madhappan,
Nikhil Kapre, Mukesh Singh, and Vasudeva Varma.
2012. Mining sentiments from tweets. In Proceedings
of the WASSA?12 in conjunction with ACL?12.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Proceedings of the 19th ACM international
conference on Information and Knowledge Manage-
ment.
Adam Bermingham and Alan Smeaton. 2011. On using
Twitter to monitor political sentiment and predict elec-
tion results. In Proceedings of the Workshop on Sen-
timent Analysis where AI meets Psychology (SAAIP
2011).
Nibir Nayan Bora. 2012. Summarizing public opinions
in tweets. In Journal Proceedings of CICLing 2012.
Paula Chesley. 2006. Using verbs and adjectives to au-
tomatically classify blog sentiment. In Proceedings
of AAAI-CAAW-06, the Spring Symposia on Computa-
tional Approaches.
Murphy Choy, Michelle L. F. Cheong, Ma Nang Laik,
and Koo Ping Shung. 2011. A sentiment analysis
of Singapore Presidential Election 2011 using Twitter
data with census correction. CoRR, abs/1108.5520.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ?10.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation.
Michael Gallagher and Michael Marsh. 2011. How Ire-
land Voted 2011: The Full Story of Ireland?s Earth-
quake Election. Palgrave Macmillan.
Daniel Gayo-Avello. 2012. ?I wanted to predict elec-
tions with Twitter and all I got was this lousy paper?.
57
A balanced survey on election prediction using Twitter
data. CoRR, abs/1204.6441.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
sentiment classification using distant supervision. In
CS224N Project Report, Stanford University.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of COLING.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1.
Bing Liu. 2010. Handbook of natural language pro-
cessing. chapter Sentiment Analysis and Subjectivity.
Chapman and Hall.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the con-
ference on Empirical Methods in Natural Language
Processing - Volume 10.
James W. Pennebaker, Cindy K. Chung, Molly Ireland,
Amy Gonzales, and Roger J. Booth. 2007. The de-
velopment and psychometric properties of liwc2007.
Technical report, Austin,Texas.
Andranik Tumasjan, Timm Oliver Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting elec-
tions with Twitter: What 140 characters reveal about
political sentiment. In Proceedings of the Interna-
tional Conference on Weblogs and Social Media.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics.
Hao Wang, Dogan Can, Abe Kazemzadeh, Franc?ois Bar,
and Shrikanth Narayanan. 2012. A system for real-
time Twitter sentiment analysis of 2012 U.S. presiden-
tial election cycle. In ACL (System Demonstrations).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing.
58
