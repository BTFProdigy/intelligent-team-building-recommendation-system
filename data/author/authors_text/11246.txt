Cross Language Text Categorization Using a Bilingual Lexicon
Ke Wu, Xiaolin Wang and Bao-Liang Lu?
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dong Chuan Rd., Shanghai 200240, China
{wuke,arthur general,bllu}@sjtu.edu.cn
Abstract
With the popularity of the Internet at a phe-
nomenal rate, an ever-increasing number of
documents in languages other than English
are available in the Internet. Cross lan-
guage text categorization has attracted more
and more attention for the organization of
these heterogeneous document collections.
In this paper, we focus on how to con-
duct effective cross language text catego-
rization. To this end, we propose a cross
language naive Bayes algorithm. The pre-
liminary experiments on collected document
collections show the effectiveness of the pro-
posed method and verify the feasibility of
achieving performance close to monolingual
text categorization, using a bilingual lexicon
alone. Also, our algorithm is more efficient
than our baselines.
1 Introduction
Due to the popularity of the Internet, an ever-
increasing number of documents in languages other
than English are available in the Internet. The or-
ganization of these heterogeneous document collec-
tions increases cost of human labor significantly. On
the one hand, experts who know different languages
are required to organize these collections. On the
other hand, maybe there exist a large amount of la-
belled documents in a language (e.g. English) which
are in the same class structure as the unlabelled doc-
uments in another language. As a result, how to ex-
?Corresponding author.
ploit the existing labelled documents in some lan-
guage (e.g. English) to classify the unlabelled doc-
uments other than the language in multilingual sce-
nario has attracted more and more attention (Bel et
al., 2003; Rigutini et al, 2005; Olsson et al, 2005;
Fortuna and Shawe-Taylor, 2005; Li and Shawe-
Taylor, 2006; Gliozzo and Strapparava, 2006). We
refer to this task as cross language text categoriza-
tion. It aims to extend the existing automated text
categorization system from one language to other
languages without additional intervention of human
experts. Formally, given two document collections
{De,Df} from two different languages e and f re-
spectively, we use the labelled document collection
De in the language e to deduce the labels of the doc-
ument collection Df in the language f via an algo-
rithm A and some external bilingual resources.
Typically, some external bilingual lexical re-
sources, such as machine translation system (MT),
large-scale parallel corpora and multilingual ontol-
ogy etc., are used to alleviate cross language text
categorization. However, it is hard to obtain them
for many language pairs. In this paper, we focus on
using a cheap bilingual resource, e.g. bilingual lexi-
con without any translation information, to conduct
cross language text categorization. To my knowl-
edge, there is little research on using a bilingual lex-
icon alone for cross language text categorization.
In this paper, we propose a novel approach for
cross language text categorization via a bilingual
lexicon alone. We call this approach as Cross Lan-
guage Naive Bayes Classifier (CLNBC). The pro-
posed approach consists of two main stages. The
first stage is to acquire a probabilistic bilingual lex-
165
icon. The second stage is to employ naive Bayes
method combined with Expectation Maximization
(EM) (Dempster et al, 1977) to conduct cross lan-
guage text categorization via the probabilistic bilin-
gual lexicon. For the first step, we propose two dif-
ferent methods. One is a naive and direct method,
that is, we convert a bilingual lexicon into a proba-
bilistic lexicon by simply assigning equal translation
probabilities to all translations of a word. Accord-
ingly, the approach in this case is named as CLNBC-
D. The other method is to employ an EM algorithm
to deduce the probabilistic lexicon. In this case, the
approach is called as CLNBC-EM. Our preliminary
experiments on our collected data have shown that
the proposed approach (CLNBC) significantly out-
performs the baselines in cross language case and is
close to the performance of monolingual text cate-
gorization.
The remainder of this paper is organized as fol-
lows. In Section 2, we introduce the naive Bayes
classifier briefly. In Section 3, we present our cross
language naive Bayes algorithm. In Section 4, eval-
uation over our proposed algorithm is performed.
Section 5 is conclusions and future work.
2 The Naive Bayes Classifier
The naive Bayes classifier is an effective known al-
gorithm for text categorization (Domingos and Paz-
zani, 1997). When it is used for text categorization
task, each document d ? D corresponds to an exam-
ple. The naive Bayes classifier estimates the prob-
ability of assigning a class c ? C to a document d
based on the following Bayes? theorem.
P (c|d) ? P (d|c)P (c) (1)
Then the naive Bayes classifier makes two as-
sumptions for text categorization. Firstly, each word
in a document occurs independently. Secondly, there
is no linear ordering of the word occurrences.
Therefore, the naive Bayes classifier can be fur-
ther formalized as follows:
P (c|d) ? P (c)
?
w?d
P (w|c) (2)
The estimates of P (c) and P (w|c) can be referred
to (McCallum and Nigam, 1998)
Some extensions to the naive Bayes classifier with
EM algorithm have been proposed for various text
categorization tasks. The naive Bayes classifier was
combined with EM algorithm to learn the class label
of the unlabelled documents by maximizing the like-
lihood of both labelled and unlabelled documents
(Nigam et al, 2000). In addition, the similar way
was adopted to handle the problem with the positive
samples alone (Liu et al, 2002). Recently, transfer
learning problem was tackled by applying EM algo-
rithm along with the naive Bayes classifier (Dai et
al., 2007). However, they all are monolingual text
categorization tasks. In this paper, we apply a simi-
lar method to cope with cross language text catego-
rization using bilingual lexicon alone.
3 Cross Language Naive Bayes Classifier
Algorithm
In this section, a novel cross language naive Bayes
classifier algorithm is presented. The algorithm con-
tains two main steps below. First, generate a prob-
abilistic bilingual lexicon; second, apply an EM-
based naive Bayes learning algorithm to deduce the
labels of documents in another language via the
probabilistic lexicon.
Table 1: Notations and explanations.
Notations Explanations
e Language of training set
f Language of test set
d Document
De Document collection in language e
Df Document collection in language f
Ve Vocabulary of language e
Vf Vocabulary of language f
L Bilingual lexicon
T ? Ve ? Vf Set of links in L
?? Set of words whose translation is ? in L
E ? Ve Set of words of language e in L
we ? E Word in E
F ? Vf Set of words of language f in L
wf ? F Word in F
|E| Number of distinct words in set E
|F | Number of distinct words in set F
N(we) Word frequency in De
N(wf , d) Word frequency in d in language f
De Data distribution in language e
166
For ease of description, we first define some nota-
tions in Table 1. In the next two sections, we detail
the mentioned-above two steps separately.
3.1 Generation of a probabilistic bilingual
lexicon
To fill the gap between different languages, there are
two different ways. One is to construct the multi-
lingual semantic space, and the other is to transform
documents in one language into ones in another lan-
guage. Since we concentrate on use of a bilingual
lexicon, we adopt the latter method. In this paper,
we focus on the probabilistic model instead of se-
lecting the best translation. That is, we need to cal-
culate the probability of the occurrence of word we
in language e given a document d in language f , i.e.
P (we|d). The estimation can be calculated as fol-
lows:
P (we|d) =
?
wf?d
P (we|wf , d)P (wf |d) (3)
Ignoring the context information in a document
d, the above probability can be approximately esti-
mated as follows:
P (we|d) '
?
wf?d
P (we|wf )P (wf |d) (4)
where P (wf |d) denotes the probability of occur-
rence of wf in d, which can be estimated by relative
frequency of wf in d.
In order to induce P (we|d), we have to know the
estimation of P (we|wf ). Typically, we can obtain a
probabilistic lexicon from a parallel corpus. In this
paper, we concentrate on using a bilingual lexicon
alone as our external bilingual resource. Therefore,
we propose two different methods for cross language
text categorization.
First, a naive and direct method is that we assume
a uniform distribution on a word?s distribution. For-
mally, P (we|wf ) = 1?wf , where (we, wf ) ? T ; oth-
erwise P (we|wf ) = 0.
Second, we can apply EM algorithm to deduce
the probabilistic bilingual lexicon via the bilingual
lexicon L and the training document collection at
hand. This idea is motivated by the work (Li and Li,
2002).
We can assume that each word we in language e
is independently generated by a finite mixture model
as follows:
P (we) =
?
wf?F
P (wf )P (we|wf ) (5)
Therefore we can use EM algorithm to estimate
the parameters of the model. Specifically speaking,
we can iterate the following two step for the purpose
above.
? E-step
P (wf |we) =
P (wf )P (we|wf )
?
w?F P (w)P (we|w)
(6)
? M-step
P (we|wf ) =
(N(we) + 1)P (wf |we)
?
w?E (N(w) + 1) P (wf |w)(7)
P (wf ) = ? ?
?
we?E
P (we)P (wf |we)
+ (1? ?) ? P ?(wf ) (8)
where 0 ? ? ? 1, and
P ?(wf ) =
?
d?Df N(wf , d) + 1
?
wf?F
?
d?Df N(wf , d) + |F |(9)
The detailed algorithm can be referred to Algorithm
1. Furthermore, the probability that each word in
language e occurs in a document d in language f ,
P (we|d), can be calculated according to Equation
(4).
3.2 EM-based Naive Bayes Algorithm for
Labelling Documents
In this sub-section, we present an EM-based semi-
supervised learning method for labelling documents
in different language from the language of train-
ing document collection. Its basic model is naive
Bayes model. This idea is motivated by the transfer
learning work (Dai et al, 2007). For simplicity of
description, we first formalize the problem. Given
the labelled document set De in the source language
and the unlabelled document set Df , the objective is
to find the maximum a posteriori hypothesis hMAP
167
Algorithm 1 EM-based Word Translation Probabil-
ity Algorithm
Input: Training document collectionD(l)e , bilingual
lexicon L and maximum times of iterations T
Output: Probabilistic bilingual lexicon P (we|wf )
1: Initialize P (0)(we|wf ) = 1|?wf | , where
(we, wf ) ? T ; otherwise P (0)(we|wf ) = 0
2: Initialize P (0)(wf ) = 1|F |
3: for t =1 to T do
4: Calculate P (t)(wf |we) based on
P (t?1)(we|wf ) and P (t?1)(wf ) accord-
ing to Equation (6)
5: Calculate P (t)(we|wf ) and P (t)(wf ) based
on P (t)(wf |we) according to Equation (7)
and Equation (8)
6: end for
7: return P (T )(we|wf )
from the hypothesis space H under the data distri-
bution of the language e, De, according to the fol-
lowing formula.
hMAP = arg max
h?H
PDe(h|De,Df ) (10)
Instead of trying to maximize PDe(h|De,Df ) in
Equation (10), we can work with `(h|De,Df ), that
is, log (PDe(h)P (De,Df |h)) . Then, using Equa-
tion (10), we can deduce the following equation.
`(h|De,Df ) ? log PDe(h)
+
?
d?De
log
?
c?C
PDe(d|c)PDe(c|h)
+
?
d?Df
log
?
c?C
PDe(d|c)PDe(c|h)
(11)
EM algorithm is applied to find a local maximum
of `(h|De,Df ) by iterating the following two steps:
? E-step:
PDe(c|d) ? PDe(c)PDe(d|c) (12)
? M-step:
PDe(c) =
?
k?{e,f}
PDe(Dk)PDe(c|Dk) (13)
PDe(we|c) =
?
k?{e,f}
PDe(Dk)PDe(we|c,Dk)
(14)
Algorithm 2 Cross Language Naive Bayes Algo-
rithm
Input: Labelled document collection De, unla-
belled document collection Df , a bilingual lexi-
con L from language e to language f and maxi-
mum times of iterations T .
Output: the class label of each document in Df
1: Generate a probabilistic bilingual lexicon;
2: Calculate P (we|d) according to Equation (4).
3: Initialize P (0)De (c|d) via the traditional naiveBayes model trained from the labelled collec-
tion D(l)e .
4: for t =1 to T do
5: for all c ? C do
6: Calculate P (t)De(c) based on P
(t?1)
De (c|d) ac-cording to Equation (13)
7: end for
8: for all we ? E do
9: Calculate P (t)De(we|c) based on P
(t?1)
De (c|d)and P (we|d) according to Equation (14)
10: end for
11: for all d ? Df do
12: Calculate P (t)De(c|d) based on P
(t)
De(c) and
P (t)De(we|c) according to Equation (12)
13: end for
14: end for
15: for all d ? Df do
16: c = arg max
c?C
P (T )De (c|d)
17: end for
For the ease of understanding, we directly put the
details of the algorithm in cross-language text cate-
gorization algorithmin which we ignore the detail of
the generation algorithm of a probabilistic lexicon.
In Equation (12), PDe(d|c) can be calculated by
PDe(d|c) =
?
{we|we??wf ?wf?d}
PDe(we|c)NDe (we,d)
(15)
where NDe(we, d) = |d|PDe(we|d).
168
In Equation (13), PDe(c|Dk) can be estimated as
follows:
PDe(c|Dk) =
?
d?Dk
PDe(c|d)PDe(d|Dk) (16)
In Equation (14), similar to section 2, we can es-
timate PDe(we|c,Dk) through Laplacian smoothing
as follows:
PDe(we|c,Dk) =
1 + NDe(we, c,Dk)
|Vk|+ NDe(c,Dk)
(17)
where
NDe(we, c,Dk) =
?
d?Dk
|d|PDe(we|d)PDe(c|d)
(18)
NDe(c,Dk) =
?
d?Dk
|d|PDe(c|d) (19)
In addition, in Equation (13) and (14), PDe(Dk)
can be actually viewed as the trade-off parame-
ter modulating the degree to which EM algorithm
weights the unlabelled documents translated from
the language f to the language e via a bilingual lex-
icon. In our experiments, we assume that the con-
straints are satisfied, i.e. PDe(De) + PDe(Df ) = 1
and PDe(d|Dk) = 1|Dk| .
4 Experiments
4.1 Data Preparation
We chose English and Chinese as our experimen-
tal languages, since we can easily setup our exper-
iments and they are rather different languages so
that we can easily extend our algorithm to other
language pairs. In addition, to evaluate the per-
formance of our algorithm, experiments were per-
formed over the collected data set. Standard evalu-
ation benchmark is not available and thus we devel-
oped a test data from the Internet, containing Chi-
nese Web pages and English Web pages. Specifi-
cally, we applied RSS reader1 to acquire the links
to the needed content and then downloaded the Web
pages. Although category information of the con-
tent can be obtained by RSS reader, we still used
three Chinese-English bilingual speakers to organize
these Web pages into the predefined categories. As
a result, the test data containing Chinese Web pages
1http://www.rssreader.com/
and English Web pages from various Web sites are
created. The data consists of news during Decem-
ber 2005. Also, 5462 English Web pages are from
18 different news Web sites and 6011 Chinese Web
pages are from 8 different news Web sites. Data dis-
tribution over categories is shown in Table 2. They
fall into five categories: Business, Education, Enter-
tainment, Science and Sports.
Some preprocessing steps are applied to Web
pages. First we extract the pure texts of all Web
pages, excluding anchor texts which introduce much
noise. Then for Chinese corpus, all Chinese charac-
ters with BIG5 encoding first were converted into
ones with GB2312 encoding, applied a Chinese seg-
menter tool2 by Zhibiao Wu from LDC to our Chi-
nese corpus and removed stop words and words
with one character and less than 4 occurrences; for
English corpus, we used the stop words list from
SMART system (Buckley, 1985) to eliminate com-
mon words. Finally, We randomly split both the En-
glish and Chinese document collection into 75% for
training and 25% for testing.
we compiled a large general-purpose English-
Chinese lexicon, which contains 276,889 translation
pairs, including 53,111 English entries and 38,517
Chinese entries. Actually we used a subset of the
lexicon including 20,754 English entries and 13,471
Chinese entries , which occur in our corpus.
Table 2: Distribution of documents over categories
Categories English Chinese
Sports 1797 2375
Business 951 1212
Science 843 1157
Education 546 692
Entertainment 1325 575
Total 5462 6011
4.2 Baseline Algorithms
To investigate the effectiveness of our algorithms
on cross-language text categorization, three baseline
methods are used for comparison. They are denoted
by ML, MT and LSI respectively.
ML (Monolingual). We conducted text catego-
rization by training and testing the text categoriza-
2http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
169
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 1: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
tion system on document collection in the same lan-
guage.
MT (Machine Translation). We used Systran
premium 5.0 to translate training data into the lan-
guage of test data, since the machine translation sys-
tem is one of the best machine translation systems.
Then use the translated data to learn a model for
classifying the test data.
LSI (Latent Semantic Indexing). We can use
the LSI or SVD technique to deduce language-
independent representations through a bilingual par-
allel corpus. In this paper, we use SVDS command
in MATLAB to acquire the eigenvectors with the
first K largest eigenvalues. We take K as 400 in our
experiments, where best performance is achieved.
In this paper, we use SVMs as the classifier of our
baselines, since SVMs has a solid theoretic founda-
tion based on structure risk minimization and thus
high generalization ability. The commonly used
one-vs-all framework is used for the multi-class
case. SVMs uses the SV M light software pack-
age(Joachims, 1998). In all experiments, the trade-
off parameter C is set to 1.
4.3 Results
In the experiments, all results are averaged on 5 runs.
Results are measured by accuracy, which is defined
as the ratio of the number of labelled correctly docu-
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 2: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
ments to the number of all documents. When inves-
tigating how different training data have effect on
performance, we randomly select the corresponding
number of training samples from the training set 5
times. The results are shown in Figure 1 and Fig-
ure 2. From the two figures, we can draw the fol-
lowing conclusions. First, CLNBC-EM has a stable
and good performance in almost all cases. Also, it
can achieve the best performance among cross lan-
guage methods. In addition, we notice that CLNBC-
D works surprisingly better than CLNBC-EM, when
there are enough test data and few training data. This
may be because the quality of the probabilistic bilin-
gual lexicon derived from CLNBC-EM method is
poor, since this bilingual lexicon is trained from in-
sufficient training data and thus may provide biased
translation probabilities.
To further investigate the effect of varying the
amount of test data, we randomly select the cor-
responding number of test samples from test set 5
times. The results are shown in Figure 3 and Fig-
ure 4, we can draw the following conclusions . First,
with the increasing test data, performance of our two
approaches is improved. Second, CLNBC-EM sta-
tistically significantly outperforms CLNBC-D.
From figures 1 through 4, we also notice that MT
and LSI always achieve some poor results. For MT,
170
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5
0.6
0.7
0.8
0.9
1
Ratio of test data
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 3: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
maybe it is due to the large difference of word usage
between original documents and the translated ones.
For example,   (Qi Shi) has two common trans-
lations, which are cavalier and knight. In sports do-
main, it often means a basketball team of National
Basketball Association (NBA) in U.S. and should
be translated into cavalier. However, the transla-
tion knight is provided by Systran translation system
we use in the experiment. In term of LSI method,
one possible reason is that the parallel corpus is too
limited. Another possible reason is that it is out-of-
domain compared with the domain of the used doc-
ument collections.
From Table 3, we can observe that our algorithm
is more efficient than three baselines. The spent time
are calculated on the machine, which has a 2.80GHz
Dual Pentium CPU.
5 Conclusions and Future Work
In this paper, we addressed the issue of how to con-
duct cross language text categorization using a bilin-
gual lexicon. To this end, we have developed a cross
language naive Bayes classifier, which contains two
main steps. In the first step, we deduce a proba-
bilistic bilingual lexicon. In the second step, we
adopt naive Bayes method combined with EM to
conduct cross language text categorization. We have
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Ratio of test data
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 4: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
proposed two different methods, namely CLNBC-D
and CLNBC-EM, for cross language text categoriza-
tion. The preliminary experiments on collected data
collections show the effectiveness of the proposed
two methods and verify the feasibility of achieving
performance near to monolingual text categorization
using a bilingual lexicon alone.
As further work, we will collect larger compara-
ble corpora to verify our algorithm. In addition, we
will investigate whether the algorithm can be scaled
to more fine-grained categories. Furthermore, we
will investigate how the coverage of bilingual lex-
icon have effect on performance of our algorithm.
Table 3: Comparison of average spent time by dif-
ferent methods, which are used to conduct cross-
language text categorization from English to Chi-
nese.
Methods Preparation Computation
CLNBC-D - ?1 Min
CLNBC-EM - ?2 Min
ML - ?10 Min
MT ?48 Hra ?14 Min
LSI ?90 Minb ?15 Min
aMachine Translation Cost
bSVD Decomposition Cost
171
Acknowledgements. The authors would like to
thank three anonymous reviewers for their valu-
able suggestions. This work was partially sup-
ported by the National Natural Science Founda-
tion of China under the grants NSFC 60375022 and
NSFC 60473040, and the Microsoft Laboratory for
Intelligent Computing and Intelligent Systems of
Shanghai Jiao Tong University.
References
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In ECDL,
pages 126?139.
Chris Buckley. 1985. Implementation of the SMART
information retrieval system. Technical report, Ithaca,
NY, USA.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classifiers for text
classification. In Proceedings of Twenty-Second AAAI
Conference on Artificial Intelligence (AAAI 2007),
pages 540?545, July.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple bayesian classifier under zero-
one loss. Machine Learning, 29(2-3):103?130.
Blaz? Fortuna and John Shawe-Taylor. 2005. The use
of machine translation tools for cross-lingual text min-
ing. In Learning With Multiple Views, Workshop at the
22nd International Conference on Machine Learning
(ICML).
Alfio Massimiliano Gliozzo and Carlo Strapparava.
2006. Exploiting comparable corpora and bilingual
dictionaries for cross-language text categorization. In
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics. The Association for
Computer Linguistics, July.
Thorsten Joachims. 1998. Making large-scale sup-
port vector machine learning practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 343?351.
Yaoyong Li and John Shawe-Taylor. 2006. Using KCCA
for Japanese-English cross-language information re-
trieval and document classification. Journal of Intel-
ligent Information Systems, 27(2):117?133.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.
2002. Partially supervised classification of text doc-
uments. In ICML ?02: Proceedings of the Nineteenth
International Conference on Machine Learning, pages
387?394, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classification.
In Proceedings of AAAI-98, Workshop on Learning for
Text Categorization.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 2000. Text classification from labeled
and unlabeled documents using EM. Machine Learn-
ing, 39(2/3):103?134.
J. Scott Olsson, Douglas W. Oard, and Jan Hajic?. 2005.
Cross-language text classification. In Proceedings of
the 28th Annual international ACM SIGIR Confer-
ence on Research and Development in information Re-
trieval, pages 645?646, New York, NY, August. ACM
Press.
Leonardo Rigutini, Marco Maggini, and Bing Liu. 2005.
An EM based training algorithm for cross-language
text categorization. In Proceedings of Web Intelligence
Conference (WI-2005), pages 529?535, Compie`gne,
France, September. IEEE Computer Society.
172
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199?204,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mr. MIRA: Open-Source Large-Margin Structured Learning on
MapReduce
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{eidelman,wuke,fture,resnik,jimmylin}@umd.edu
Abstract
We present an open-source framework
for large-scale online structured learning.
Developed with the flexibility to handle
cost-augmented inference problems such
as statistical machine translation (SMT),
our large-margin learner can be used with
any decoder. Integration with MapReduce
using Hadoop streaming allows efficient
scaling with increasing size of training
data. Although designed with a focus
on SMT, the decoder-agnostic design of
our learner allows easy future extension to
other structured learning problems such as
sequence labeling and parsing.
1 Introduction
Structured learning problems such as sequence la-
beling or parsing, where the output has a rich in-
ternal structure, commonly arise in NLP. While
batch learning algorithms adapted for structured
learning such as CRFs (Lafferty et al, 2001)
and structural SVMs (Joachims, 1998) have re-
ceived much attention, online methods such as
the structured perceptron (Collins, 2002) and a
family of Passive-Aggressive algorithms (Cram-
mer et al, 2006) have recently gained promi-
nence across many tasks, including part-of-speech
tagging (Shen, 2007), parsing (McDonald et
al., 2005) and statistical machine translation
(SMT) (Chiang, 2012), due to their ability to deal
with large training sets and high-dimensional in-
put representations.
Unlike batch learners, which must consider all
examples when optimizing the objective, online
learners operate in rounds, optimizing using one
example or a handful of examples at a time. This
online nature offers several attractive properties,
facilitating scaling to large training sets while re-
maining simple and offering fast convergence.
Mr. MIRA, the open source system1 de-
scribed in this paper, implements an online large-
margin structured learning algorithm based on
MIRA (?2.1), for cost-augmented online large-
scale training in high-dimensional feature spaces.
Our contribution lies in providing the first pub-
lished decoder-agnostic parallelization of MIRA
with Hadoop for structured learning.
While the current demonstrated application fo-
cuses on large-scale discriminative training for
machine translation, the learning algorithm is gen-
eral with respect to the inference algorithm em-
ployed. We are able to decouple our learner en-
tirely from the MT decoder, allowing users to
specify their own inference procedure through a
simple text communication protocol (?2.2). The
learner only requires k-best output with feature
vectors, as well as the specification of a cost func-
tion. Standard automatic evaluation metrics for
MT, such as BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006), have already been imple-
mented. Furthermore, our system can be extended
to other structured learning problems with a min-
imal amount of effort, simply by implementing a
task-specific cost function and specifying an ap-
propriate decoder.
Through Hadoop streaming, our system can
take advantage of commodity clusters to handle
large-scale training (?3), while also being capable
of running in environments ranging from a single
machine to a PBS-managed batch cluster. Experi-
mental results (?4) show that it scales linearly and
makes fast parameter tuning on large tuning sets
for SMT practical.
2 Learning and Inference
2.1 Online Large-Margin Learning
MIRA is a popular online large-margin structured
learning method for NLP tasks (McDonald et al,
2005; Chiang et al, 2009; Chiang, 2012). The
1https://github.com/kho/mr-mira
199
main intuition is that we want our model to enforce
a margin between the correct and incorrect out-
puts of a sentence that agrees with our cost func-
tion. This is done by making the smallest update
we can to our parameters, w, on every sentence,
that will ensure that the difference in model scores
?fi(y?) = w>(f(xi, y+) ? f(xi, y?)) between the
correct output y+ and incorrect output y? is at least
as large as the cost, ?i(y?), incurred by predicting
the incorrect output:2
wt+1 = arg minw
1
2 ||w ?wt||
2 + C?i
s.t. ?y? ? Y(xi), ?fi(y?) ? ?i(y?)? ?i
where Y(xi) is the space of possible structured
outputs we are able to produce from xi, and
C is a regularization parameter that controls the
size of the update. In practice, we can de-
fine Y(xi) to be the k-best output. With a
passive-aggressive (PA) update, the ?y? constraint
above can be approximated by selecting the sin-
gle most violated constraint, which maximizes
y? ? arg maxy?Y(xi) w>f(xi, y) + ?i(y). This
optimization problem is attractive because it re-
duces to a simple analytical solution, essentially
performing a subgradient descent step with the
step size adjusted based on each example:
?? min
(
C, ?i(y
?)? ?fi(y?)
?f(xi, y+)? f(xi, y?)?2
)
w? w + ??
(
f(xi, y+)? f(xi, y?)
)
The user-defined cost function is a task-specific
external measure of quality that relays how bad se-
lecting y? truly is on the task we care about. The
cost can take any form as long as it decomposes
across the local parts of the structure, just as the
feature functions. For instance, it could be the
Hamming loss for sequence labeling, F-score for
parsing, or an approximate BLEU score for SMT.
Cost-augmented Inference For most struc-
tured prediction problems in machine learning,
yi ? Y(xi), that is, the model is able to produce,
and thus score, the correct output structure, mean-
ing y+ = yi. However, for certain NLP prob-
lems this may not be the case. For instance in
SMT, our model may not be able to produce or
reach the correct reference translation, which pro-
hibits our model from scoring it. This problem
2For a more formal description we refer the reader
to (Crammer et al, 2006; Chiang, 2012).
necessitates cost-augmented inference, where we
select y+ ? arg maxy?Y(xi) w>f(xi, y)??i(y)
from the space of structures our model can pro-
duce, to stand in for the correct output in optimiza-
tion. Our system was developed to handle both
cases, with the decoder providing the k-best list
to the learner, specifying whether to perform cost-
augmented selection.
Sparse Features While utilizing sparse features
is a primary motivation for performing large-scale
discriminative training, which features to use and
how to learn their weights can have a large im-
pact on the potential benefit. To this end, we in-
corporate `1/`2 regularization for joint feature se-
lection in order to improve efficiency and counter
overfitting effects (Simianer et al, 2012). Further-
more, the PA update has a single learning rate ?
for all features, which specifies how much the fea-
ture weights can change at each update. How-
ever, since dense features (e.g., language model)
are observed far more frequently than sparse fea-
tures (e.g., rule id), we may instead want to use
a per-feature learning rate that allows larger steps
for features that do not have much support. Thus,
we allow setting an adaptive per-feature learning
rate (Green et al, 2013; Crammer et al, 2009;
Duchi et al, 2011).
2.2 Learner/Decoder Communication
Training requires communication between the de-
coder and the learner. The decoder needs to re-
ceive weight updates and the input sentence from
the learner; and the learner needs to receive k-best
output with feature vectors from the decoder. This
is essentially all the required communication be-
tween the learner and the decoder. Below, we de-
scribe a simple line-based text protocol.
Input sentence and weight updates Follow-
ing common practice in machine translation, the
learner encodes each input sentence as a single-
line SGML entry named seg and sends it to the
decoder. The first line of Figure 1 is an exam-
ple sentence in this format. In addition to the
required sentence ID (useful in parallel process-
ing), an optional delta field is used to encode
the weight updates, as a sparse vector indexed
by feature names. First, for each name and up-
date pair, a binary record consisting of a null-
terminated string (name) and a double-precision
floating point number in native byte order (up-
date) is created. Then, all binary records are con-
200
<seg id="123" delta="TE0AexSuR+F6hD8="> das ist ein kleine haus </seg>
<seg id="124"> ein kleine haus </seg>\tein kleine ||| a small\thaus ||| house
Figure 1: Example decoder input in SGML
5
123 ||| 5 ||| this is a small house ||| TE0AAAAA... <base64> ||| 120.3
123 ||| 5 ||| this is the small house ||| <base64> ||| 118.4
123 ||| 5 ||| this was small house ||| <base64> ||| 110.5
<empty>
<empty>
Figure 2: Example k-best output
catenated and encoded in base64. In the example
above, the value of delta is the base64 encod-
ing of 0x4c 0x4d 0x00 0x7b 0x14 0xae 0x47
0xe1 0x7a 0x84 0x3f. The first 3 bytes store the
feature name (LM) and the next 8 bytes is its update
(0.01), to be added to the decoder?s current value
of the corresponding feature weight.
The learner also allows the user to pass any ad-
ditional information to the decoder, as long as it
can be encoded as a single-line text string. Such
information, if given, is appended after the seg en-
try, with a leading tab character as the delimiter.
For example, the second line of Figure 1 passes
two phrase translation rules to the decoder.
k-best output The decoder reads from standard
input and outputs the k-best output for one input
sentence before consuming the next line. For the
k-best output, the decoder first outputs to standard
output a line consisting of a single integerN . Next
the decoder outputs N lines where each line can
be either empty or an actual hypothesis. When the
line is an actual hypothesis, it consists of the fol-
lowing parts:
SID ||| LEN ||| TOK ||| FEAT [ REST ]
SID is the sentence ID of the corresponding input;
LEN is the length of source sentence;3 TOK contains
the tokens of the hypothesis sentence separated by
spaces; FEAT is the feature vector, encoded in the
same way as the weight updates, delimited by a
whitespace. Everything after FEAT until the end of
the line is discarded. See Figure 2 for an example
of k-best output. Note the scores after the last |||
are discarded by the learner.
Overall workflow The learner reads lines from
standard input in the following tab-delimited for-
mat:
3This is used in computing the smoothed cost. Usually
this is identical for all hypotheses if the input is a plain sen-
tence. But in applications such as lattice-based translation,
each hypothesis can be produced from different source sen-
tences, resulting in different lengths.
SRC<tab>REF<tab>REST
SRC is the actual input sentence as a seg entry; REF
is the gold output for the input sentence, for ex-
ample, reference translations in MT;4 REST is the
additional information that will be appended after
the seg entry and passed to the decoder.
The learner creates a sub-process for the de-
coder and connects to the sub-process? standard
input and output with pipes. Then it processes the
input lines one by one. For each line, it first sends
a composed input message to the decoder, combin-
ing the input sentence, weight updates, and user-
supplied information. Next it collects the k-best
output from the decoder, solves the QP problem to
obtain weight updates and repeats.
The learner produces two types of output. First,
the 1-best hypothesis for each input sentence, in
the following format:
SID<tab>TOK
Second, when there are no more input lines, the
learner outputs final weights and the number of
lines processed, in the following format:
-1<tab>NUM ||| WEIGHTS
The 1-best hypotheses can be scored against ref-
erences to obtain an estimate of cost. The final
weights are stored in a way convenient for averag-
ing in a parallel setting, as we shall discuss next.
3 Large-Scale Discriminative Training
3.1 MapReduce
With large amounts of data available today,
distributed computations have become essen-
tial. MapReduce (Dean and Ghemawat, 2004)
has emerged as a popular distributed process-
ing framework for commodity clusters that has
gained widespread adoption in both industry and
academia, thanks to its simplicity and the avail-
ability of the Hadoop open-source implementa-
tion. MapReduce provides a higher level of
4There can be multiple references, separated by |||.
201
abstraction for designing distributed algorithms
compared to, say, MPI or pthreads, by hiding
system-level details (e.g., deadlock, race condi-
tions, machine failures) from the developer.
A single MapReduce program begins with a
map phase, where mapper processes input key-
value pairs to produce an arbitrary number of in-
termediate key-value pairs. The mappers execute
in parallel, consuming data splits independently.
Following the map phase, all key-value pairs emit-
ted by the mappers are sorted by key and dis-
tributed to the reducers, such that all pairs shar-
ing the same key are guaranteed to arrive at the
same reducer. Finally, in the reduce phase, each
reducer processes the intermediate key-value pairs
it receives and emits final output key-value pairs.
3.2 System Architecture
Algorithm design We use Hadoop streaming to
parallelize the training process. Hadoop stream-
ing allows any arbitrary executable to serve as the
mapper or reducer, as long as it handles key-value
pairs properly.5 One iteration of training is im-
plemented as a single Hadoop streaming job. In
the map step, our learner can be directly used as
the mapper. Each mapper loads the same initial
weights, processes a single split of data and pro-
duces key-value pairs: the one-best hypothesis of
each sentence is output with the sentence ID as
the key (non-negative); the final weights with re-
spect to the split are output with a special negative
key. In the reduce step, a single reducer collects all
key-value pairs, grouped and sorted by keys. The
one-best hypotheses are output to disk in the or-
der they are received, so that the order matches the
reference translation set. The reducer also com-
putes the feature selection and weighted average
of final weights received from all of the mappers.
Assuming mapper i produces the final weights wi
after processing ni sentences, the weighted aver-
aged is defined as w? =
?
iwi?ni?
i ni
. Although aver-
aging yields different result from running a single
learner over the entire data, we have found the dif-
ference to be quite small in terms of convergence
and quality of tuned weights in practice.
After the reducer finishes, the averaged weights
are extracted and used as the initial weights for the
next iteration; the emitted hypotheses are scored
5By default, each line is treated as a key-value pair en-
coded in text, where the key and the value are separated by a
<tab>.
against the references, which allows us to track the
learning curve and the progress of convergence.
Scalability In an application such as SMT, the
decoder requires access to the translation gram-
mar and language model to produce translation hy-
potheses. For small tuning sets, which have been
typical in MT research, having these files trans-
ferred across the network to individual servers
(which then load the data into memory) is not
a problem. However, for even modest input on
the order of tens of thousands of sentences, this
creates a challenge. For example, distributing
thousands of per-sentence grammar files to all the
workers in a Hadoop cluster is time-consuming,
especially when this needs to be performed prior
to every iteration.
To benefit from MapReduce, it is essential to
avoid dependencies on ?side data? as much as
possible, due to the challenges explained above
with data transfer. To address this issue, we ap-
pend the per-sentence translation grammar as user-
supplied additional information to each input sen-
tence. This results in a large input file (e.g., 75 gi-
gabytes for 50,000 sentences), but this is not an is-
sue since the data reside on the Hadoop distributed
file system and MapReduce optimizes for data lo-
cality when scheduling mappers.
Unfortunately, it is much more difficult to ob-
tain per-sentence language models that are small
enough to handle in this same manner. Currently,
the best solution we have found is to use Hadoop?s
distributed cache to ship the single large language
model to each worker.
4 Evaluation
We evaluated online learning in Hadoop Map-
Reduce by applying it to German-English ma-
chine translation, using our hierarchical phrase-
based translation system with cdec as the de-
coder (Dyer et al, 2010). The parallel training
data consist of the Europarl and News Commen-
tary corpora from the WMT12 translation task,6
containing 2.08M sentences. A 5-gram language
model was trained on the English side of the bi-
text along with 750M words of news using SRILM
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996).
We experimented with two feature sets: (1) a
small set with standard MT features, including
6http://www.statmt.org/wmt12/translation-task.html
202
Tuning set size Time/iteration # splits # features Tuning BLEU Test
(corpus) (on disk, GB) (in seconds) BLEU TER
dev 3.3 119 120 16 22.38 22.69 60.61
5k 7.8 289 120 16 32.60 22.14 59.60
10k 15.2 432 120 16 33.16 22.06 59.43
25k 37.2 942 300 16 32.48 22.21 59.54
50k 74.5 1802 600 16 32.21 22.21 59.39
dev 3.3 232 120 85k 23.08 23.00 60.19
5k 7.8 610 120 159k 33.70 22.26 59.26
10k 15.2 1136 120 200k 34.00 22.12 59.24
25k 37.2 2395 300 200k 32.96 22.35 59.29
50k 74.5 4465 600 200k 32.86 22.40 59.15
Table 1: Evaluation of our Hadoop implementation of MIRA, showing running time as well as BLEU
and TER values for tuning and testing data.
dev test 5k 10k 25k 50k
Sentences 3003 3003 5000 10000 25000 50000
Tokens en 75k 74k 132k 255k 634k 1258k
Tokens de 74k 73k 133k 256k 639k 1272k
Table 2: Corpus statistics
phrase and lexical translation probabilities in both
directions, word and arity penalties, and language
model scores; and (2) a large set containing the top
200k sparse features that might be useful to train
on large numbers of instances: rule id and shape,
target bigrams, insertions and deletions, and struc-
tural distortion features.
All experiments were conducted on a Hadoop
cluster (running Cloudera?s distribution, CDH
4.2.1) with 16 nodes, each with two quad-core 2.2
GHz Intel Nehalem Processors, 24 GB RAM, and
three 2 TB drives. In total, the cluster is configured
for a capacity of 128 parallel workers, although
we do not have direct control over the number
of simultaneous mappers, which depends on the
number of input splits. If the number of splits is
smaller than 128, then the cluster is under-utilized.
To note this, we report the number of splits for
each setting in our experimental results (Table 1).
We ran MIRA on a number of tuning sets, de-
scribed in Table 2, in order to test the effective-
ness and scalability of our system. First, we used
the standard development set from WMT12, con-
sisting of 3,003 sentences from news domain. In
order to show the scaling characteristics of our ap-
proach, we then used larger portions of the train-
ing bitext directly to tune parameters. In order to
avoid overfitting, we used a jackknifing method
to split the training data into n = 10 folds, and
built a translation system on n ? 1 folds, while
adjusting the sampling rate to sample sentences
from the other fold to obtain tuning sets ranging
from 5,000 sentences to 50,000 sentences. Table 1
shows details of experimental results for each set-
ting. The second column shows the space each
tuning set takes up on disk when we include refer-
ence translations and grammar files along with the
sentences. The reported tuning BLEU is from the
iteration with best performance, and running times
are reported from the top-scoring iteration as well.
Even though our focus in this evaluation is to
show the scalability of our implementation to large
input and feature sets, it is also worthwhile to men-
tion the effectiveness aspect. As we increase the
tuning set size by sampling sentences from the
training data, we see very little improvement in
BLEU and TER with the smaller feature set. This
is not surprising, since sparse features are more
likely to gain from additional tuning instances. In-
deed, tuning scores for all sets improve substan-
tially with sparse features, accompanied by small
increases on test.
While tuning on dev data results in better BLEU
on test data than when tuning on the larger sets, it
is important to note that although we are able to
tune more features on the larger bitext tuning sets,
they are not composed of the same genre as the
dev and test sets, resulting in a domain mismatch.
203
Therefore, we are actually comparing a smaller in-
domain tuning set with a larger out-of-domain set.
While this domain adaptation is problematic (Had-
dow and Koehn, 2012), the ability to discrimina-
tively tune on larger sets remains highly desirable.
In terms of running time, we observe that the al-
gorithm scales linearly with respect to input size,
regardless of the feature set. With more features,
running time increases due to a more complex
translation model, as well as larger intermediate
output (i.e., amount of information passed from
mappers to reducers). The scaling characteristics
point out the strength of our system: our scalable
MIRA implementation allows one to tackle learn-
ing problems where there are many parameters,
but also many training instances.
Comparing the wall clock time of paralleliza-
tion with Hadoop to the standard mode of 10?20
learner parallelization (Haddow et al, 2011; Chi-
ang et al, 2009), for the small 25k feature set-
ting, after one iteration, which takes 4625 sec-
onds using 15 learners on our PBS cluster, the tun-
ing score is 19.5 BLEU, while in approximately
the same time, we can perform five iterations
with Hadoop and obtain 30.98 BLEU. While this
is not a completely fair comparison, as the two
clusters utilize different resources and the num-
ber of learners, it suggests the practical benefits
that Hadoop can provide. Although increasing the
number of learners on our PBS cluster to the num-
ber of mappers used in Hadoop would result in
roughly equivalent performance, arbitrarily scal-
ing out learners on the PBS cluster to handle larger
training sets can be challenging since we?d have to
manually coordinate the parallel processes in an
ad-hoc manner. In contrast, Hadoop provides scal-
able parallelization in a manageable framework,
providing data distribution, synchronization, fault
tolerance, as well as other features, ?for free?.
5 Conclusion
In this paper, we presented an open-source
framework that allows seamlessly scaling struc-
tured learning to large feature-rich problems with
Hadoop, which lets us take advantage of large
amounts of data as well as sparse features. The
development of Mr. MIRA has been motivated pri-
marily by application to SMT, but we are planning
to extend our system to other structured prediction
tasks in NLP such as parsing, as well as to facili-
tate its use in other domains.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship. Any opinions, findings, con-
clusions, or recommendations expressed are those
of the authors and do not necessarily reflect views
of the sponsors.
References
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In ACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new fea-
tures for statistical machine translation. In NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative training
of statistical translation models. JMLR, 13:1159?1187.
M. Collins. 2002. Ranking algorithms for named-entity ex-
traction: boosting and the voted perceptron. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive
regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blun-
som, H. Setiawan, V. Eidelman, and P. Resnik. 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL
System Demonstrations.
S. Green, S. Wang, D. Cer, and C. Manning. 2013. Fast and
adaptive online training of feature-rich translation models.
In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect of out-
of-domain data on smt systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. In WMT.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features. In
ECML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL.
L. Shen. 2007. Guided learning for bidirectional sequence
classification. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-scale
discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
204
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 128?133,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Towards Efficient Large-Scale Feature-Rich Statistical Machine
Translation
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{vlad,wuke,fture,resnik,jimmylin}@umiacs.umd.edu
Abstract
We present the system we developed to
provide efficient large-scale feature-rich
discriminative training for machine trans-
lation. We describe how we integrate with
MapReduce using Hadoop streaming to
allow arbitrarily scaling the tuning set and
utilizing a sparse feature set. We report our
findings on German-English and Russian-
English translation, and discuss benefits,
as well as obstacles, to tuning on larger
development sets drawn from the parallel
training data.
1 Introduction
The adoption of discriminative learning methods
for SMT that scale easily to handle sparse and lex-
icalized features has been increasing in the last
several years (Chiang, 2012; Hopkins and May,
2011). However, relatively few systems take full
advantage of the opportunity. With some excep-
tions (Simianer et al, 2012), most still rely on
tuning a handful of common dense features, along
with at most a few thousand others, on a relatively
small development set (Cherry and Foster, 2012;
Chiang et al, 2009). While more features tuned
on more data usually results in better performance
for other NLP tasks, this has not necessarily been
the case for SMT.
Thus, our main focus in this paper is to improve
understanding into the effective use of sparse fea-
tures, and understand the benefits and shortcom-
ings of large-scale discriminative training. To
this end, we conducted experiments for the shared
translation task of the 2013 Workshop on Statis-
tical Machine Translation for the German-English
and Russian-English language pairs.
2 Baseline system
We use a hierarchical phrase-based decoder im-
plemented in the open source translation system
cdec1 (Dyer et al, 2010). For tuning, we use
Mr. MIRA2 (Eidelman et al, 2013), an open
source decoder agnostic implementation of online
large-margin learning in Hadoop MapReduce. Mr.
MIRA separates learning from the decoder, allow-
ing the flexibility to specify the desired inference
procedure through a simple text communication
protocol. The decoder receives input sentences
and weight updates from the learner, while the
learner receives k-best output with feature vectors
from the decoder.
Hadoop MapReduce (Dean and Ghemawat,
2004) is a popular distributed processing frame-
work that has gained widespread adoption, with
the advantage of providing scalable parallelization
in a manageable framework, taking care of data
distribution, synchronization, fault tolerance, as
well as other features. Thus, while we could oth-
erwise achieve the same level of parallelization, it
would be in a more ad-hoc manner.
The advantage of online methods lies in their
ability to deal with large training sets and high-
dimensional input representations while remain-
ing simple and offering fast convergence. With
Hadoop streaming, our system can take advantage
of commodity clusters to handle parallel large-
scale training while also being capable of running
on a single machine or PBS-managed batch clus-
ter.
System design To efficiently encode the infor-
mation that the learner and decoder require (source
sentence, reference translation, grammar rules) in
a manner amenable to MapReduce, i.e. avoiding
dependencies on ?side data? and large transfers
across the network, we append the reference and
1http://cdec-decoder.org
2https://github.com/kho/mr-mira
128
per-sentence grammar to each input source sen-
tence. Although this file?s size is substantial, it is
not a problem since after the initial transfer, it re-
sides on Hadoop distributed file system, and Map-
Reduce optimizes for data locality when schedul-
ing mappers.
A single iteration of training is performed as
a Hadoop streaming job. Each begins with a
map phase, with every parallel mapper loading the
same initial weights and decoding and updating
parameters on a shard of the data. This is followed
by a reduce phase, with a single reducer collect-
ing final weights from all mappers and computing
a weighted average to distribute as initial weights
for the next iteration.
Parameter Settings We tune our system toward
approximate sentence-level BLEU (Papineni et al,
2002),3 and the decoder is configured to use cube
pruning (Huang and Chiang, 2007) with a limit
of 200 candidates at each node. For optimiza-
tion, we use a learning rate of ?=1, regularization
strength of C=0.01, and a 500-best list for hope
and fear selection (Chiang, 2012) with a single
passive-aggressive update for each sentence (Ei-
delman, 2012).
Baseline Features We used a set of 16 stan-
dard baseline features: rule translation relative
frequency P (e|f), lexical translation probabilities
Plex(e|f) and Plex(f |e), target n-gram language
model P (e), penalties for source and target words,
passing an untranslated source word to the tar-
get side, singleton rule and source side, as well
as counts for arity-0,1, or 2 SCFG rules, the total
number of rules used, and the number of times the
glue rule is used.
2.1 Data preparation
For both languages, we used the provided Eu-
roparl and News Commentary parallel training
data to create the translation grammar neces-
sary for our model. For Russian, we addi-
tionally used the Common Crawl and Yandex
data. The data were lowercased and tokenized,
then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many align-
ments in both directions and symmetrized sing the
grow-diag-final-and method (Koehn et al, 2003).
3We approximate corpus BLEU by scoring sentences us-
ing a pseudo-document of previous 1-best translations (Chi-
ang et al, 2009).
We constructed a 5-gram language model us-
ing SRILM (Stolcke, 2002) from the provided
English monolingual training data and parallel
data with modified Kneser-Ney smoothing (Chen
and Goodman, 1996), which was binarized using
KenLM (Heafield, 2011). The sentence-specific
translation grammars were extracted using a suffix
array rule extractor (Lopez, 2007).
For German, we used the 3,003 sentences in
newstest2011 as our Dev set, and report results
on the 3,003 sentences of the newstest2012 Test
set using BLEU and TER (Snover et al, 2006).
For Russian, we took the first 2,000 sentences of
newstest2012 for Dev, and report results on the re-
maining 1,003. For both languages, we selected
1,000 sentences from the bitext to be used as an
additional testing set (Test2).
Compound segmentation lattices As German
is a morphologically rich language with produc-
tive compounding, we use word segmentation lat-
tices as input for the German translation task.
These lattices encode alternative segmentations of
compound words, allowing the decoder to auto-
matically choose which segmentation is best. We
use a maximum entropy model with recommended
settings to create lattices for the dev and test sets,
as well as for obtaining the 1-best segmentation of
the training data (Dyer, 2009).
3 Evaluation
This section describes the experiments we con-
ducted in moving towards a better understanding
of the benefits and challenges posed by large-scale
high-dimensional discriminative tuning.
3.1 Sparse Features
The ability to incorporate sparse features is the pri-
mary reason for the recent move away from Min-
imum Error Rate Training (Och, 2003), as well as
for performing large-scale discriminative training.
We include the following sparse Boolean feature
templates in our system in addition to the afore-
mentioned baseline features: rule identity (for ev-
ery unique rule in the grammar), rule shape (map-
ping rules to sequences of terminals and nontermi-
nals), target bigrams, lexical insertions and dele-
tions (for the top 150 unaligned words from the
training data), context-dependent word pairs (for
the top 300 word pairs in the training data), and
structural distortion (Chiang et al, 2008).
129
Dev Test Test2 5k 10k 25k 50k
en 75k 74k 27k 132k 255k 634k 1258k
de 74k 73k 26k 133k 256k 639k 1272k
Table 1: Corpus statistics in tokens for German.
Dev Test Test2 15k
ru 46k 24k 24k 350k
en 50k 27k 25k 371k
Table 2: Corpus statistics in tokens for
Russian.
Set # features Tune Test
?BLEU ?BLEU ?TER
de-en 16 22.38 22.69 60.61
+sparse 108k 23.86 23.01 59.89
ru-en 16 30.18 29.89 49.05
+sparse 77k 32.40 30.81 48.40
Table 3: Results with the addition of sparse fea-
tures for German and Russian.
All of these features are generated from the
translation rules on the fly, and thus do not have
to be stored as part of the grammar. To allow for
memory efficiency while scaling the training data,
we hash all the lexical features from their string
representation into a 64-bit integer.
Altogether, these templates result in millions of
potential features, thus how to select appropriate
features, and how to properly learn their weights
can have a large impact on the potential benefit.
3.2 Adaptive Learning Rate
The passive-aggressive update used in MIRA has a
single learning rate ? for all features, which along
with ? limits the amount each feature weight can
change at each update. However, since the typical
dense features (e.g., language model) are observed
far more frequently than sparse features (e.g., rule
identity), it has been shown to be advantageous
to use an adaptive per-feature learning rate that
allows larger steps for features that do not have
much support (Green et al, 2013; Duchi et al,
2011). Essentially, instead of having a single pa-
rameter ?,
?? min
(
C, cost(y
?)?w>(f(y+)? f(y?))
?f(y+)? f(y?)?2
)
w? w + ??
(
f(y+)? f(y?)
)
we instead have a vector ? with one entry for each
feature weight:
??1 ? ??1 + ?diag
(
ww>
)
w? w + ??1/2
(
f(y+)? f(y?)
)
?=1 
?=0.01 
?=0.1 
22.2 
22.4 
22.6 
22.8 
23 
23.2 
23.4 
23.6 
23.8 
24 
BLE
U 
Iteration 
Figure 1: Learning curves for tuning when using
a single step size (?) versus different per-feature
learning rates.
In practice, this update is very similar to that of
AROW (Crammer et al, 2009; Chiang, 2012).
Figure 1 shows learning curves for sparse mod-
els with a single learning rate, and adaptive learn-
ing with ?=0.01 and ?=0.1, with associated re-
sults on Test in Table 4.4 As can be seen, using
a single ? produces almost no gain on Dev. How-
ever, while both settings using an adaptive rate fare
better, the proper setting of ? is important. With
?=0.01 we observe 0.5 BLEU gain over ?=0.1 in
tuning, which translates to a small gain on Test.
Henceforth, we use an adaptive learning rate with
?=0.01 for all experiments.
Table 3 presents baseline results for both lan-
guages. With the addition of sparse features, tun-
ing scores increase by 1.5 BLEU for German, lead-
ing to a 0.3 BLEU increase on Test, and 2.2 BLEU
for Russian, with 1 BLEU increase on Test. The
majority of active features for both languages are
rule id (74%), followed by target bigrams (14%)
and context-dependent word pairs (11%).
3.3 Feature Selection
As the tuning set size increases, so do the num-
ber of active features. This may cause practi-
cal problems, such as reduced speed of computa-
tion and memory issues. Furthermore, while some
4All sparse models are initialized with the same tuned
baseline weights. Learning rates are local to each mapper.
130
Adaptive # feat. Tune Test
?BLEU ?BLEU ?TER
none 74k 22.75 22.87 60.19
?=0.01 108k 23.86 23.01 59.89
?=0.1 62k 23.32 22.92 60.09
Table 4: Results with different ? settings for using a per-feature learning rate with sparse features.
Set # feat. Tune Test
?BLEU ?BLEU ?TER
all 510k 32.99 22.36 59.26
top 200k 200k 32.96 22.35 59.29
all 373k 34.26 28.84 49.29
top 200k 200k 34.45 28.98 49.30
Table 5: Comparison of using all features versus
top k selection.
sparse features will generalize well, others may
not, thereby incurring practical costs with no per-
formance benefit. Simianer et al (2012) recently
explored `1/`2 regularization for joint feature se-
lection for SMT in order to improve efficiency and
counter overfitting effects. When performing par-
allel learning, this allows for selecting a reduced
set of the top k features at each iteration that are
effective across all learners.
Table 5 compares selecting the top 200k fea-
tures versus no selection for a larger German and
Russian tuning set (?3.4). As can be seen, we
achieve the same performance with the top 200k
features as we do when using double that amount,
while the latter becomes increasing cumbersome
to manage. Therefore, we use a top 200k selection
for the remainder of this work.
3.4 Large-Scale Training
In the previous section, we saw that learning
sparse features on the small development set leads
to substantial gains in performance. Next, we
wanted to evaluate if we can obtain further gains
by scaling the tuning data to learn parameters di-
rectly on a portion of the training bitext. Since the
bitext is used to learn rules for translation, using
the same parallel sentences for grammar extrac-
tion as well as for tuning feature weights can lead
to severe overfitting (Flanigan et al, 2013). To
avoid this issue, we used a jackknifing method to
split the training data into n = 10 folds, and built
a translation system on n?1 folds, while sampling
sentences from the News Commentary portion of
the held-out fold to obtain tuning sets from 5,000
to 50,000 sentences for German, and 15,000 sen-
tences for Russian.
Results for large-scale training for German are
presented in Table 6. Although we cannot com-
pare the tuning scores across different size sets,
we can see that tuning scores for all sets improve
substantially with sparse features. Unfortunately,
with increasing tuning set size, we see very little
improvement in Test BLEU and TER with either
feature set. Similar findings for Russian are pre-
sented in Table 7. Introducing sparse features im-
proves performance on each set, respectively, but
Dev always performs better on Test.
While tuning on Dev data results in better BLEU
on Test than when tuning on the larger sets, it is
important to note that although we are able to tune
more features on the larger bitext tuning sets, they
are not composed of the same genre as the Tune
and Test sets, resulting in a domain mismatch.
This phenomenon is further evident in German
when testing each model on Test2, which is se-
lected from the bitext, and is thus closer matched
to the larger tuning sets, but is separate from both
the parallel data used to build the translation model
and the tuning sets. Results on Test2 clearly show
significant improvement using any of the larger
tuning sets versus Dev for both the baseline and
sparse features. The 50k sparse setting achieves
almost 1 BLEU and 2 TER improvement, showing
that there are significant differences between the
Dev/Test sets and sets drawn from the bitext.
For Russian, we amplified the effects by select-
ing Test2 from the portion of the bitext that is sepa-
rate from the tuning set, but is among the sentences
used to create the translation model. The effects of
overfitting are markedly more visible here, as there
is almost a 7 BLEU difference between tuning on
Dev and the 15k set with sparse features. Further-
more, it is interesting to note when looking at Dev
that using sparse features has a significant nega-
tive impact, as the baseline tuned Dev performs
131
Tuning Test
?BLEU ?TER
5k 22.81 59.90
10k 22.77 59.78
25k 22.88 59.77
50k 22.86 59.76
Table 8: Results for German with 2 iterations of
tuning on Dev after tuning on larger set.
reasonably well, while the introduction of sparse
features leads to overfitting the specificities of the
Dev/Test genre, which are not present in the bitext.
We attempted two strategies to mitigate this
problem: combining the Dev set with the larger
bitext tuning set from the beginning, and tuning
on a larger set to completion, and then running 2
additional iterations of tuning on the Dev set using
the learned model. Results for tuning on Dev and a
larger set together are presented in Table 7 for Rus-
sian and Table 6 for German. As can be seen, the
resulting model improves somewhat on the other
genre and strikes a middle ground, although it is
worse on Test than Dev.
Table 8 presents results for tuning several ad-
ditional iterations after learning a model on the
larger sets. Although this leads to gains of around
0.5 BLEU on Test, none of the models outperform
simply tuning on Dev. Thus, neither of these two
strategies seem to help. In future work, we plan
to forgo randomly sampling the tuning set from
the bitext, and instead actively select the tuning
set based on similarity to the test set.
4 Conclusion
We explored strategies for scaling learning for
SMT to large tuning sets with sparse features.
While incorporating an adaptive per-feature learn-
ing rate and feature selection, we were able to
use Hadoop to efficiently take advantage of large
amounts of data. Although discriminative training
on larger sets still remains problematic, having the
capability to do so remains highly desirable, and
we plan to continue exploring methods by which
to leverage the power of the bitext effectively.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship.
References
S. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
ACL.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A decoder, alignment, and
learning framework for finite-state and context-free
translation models. In ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA: Open-
source large-margin structured learning on map-
reduce. In ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In WMT.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In NAACL.
S. Green, S. Wang, D. Cer, and C. Manning. 2013.
Fast and adaptive online training of feature-rich
translation models. In ACL.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
132
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 22.38 22.69 60.61 29.31 54.26
5k 120 16 32.60 22.14 59.60 29.69 52.96
10k 120 16 33.16 22.06 59.43 29.93 52.37
Dev+10k 120 16 19.40 22.32 59.37 30.17 52.45
25k 300 16 32.48 22.21 59.54 30.03 51.71
50k 600 16 32.21 22.21 59.39 29.94 52.55
Dev 120 108k 23.86 23.01 59.89 29.65 53.86
5k 120 159k 33.70 22.26 59.26 30.53 51.84
10k 120 200k 34.00 22.12 59.24 30.51 51.71
Dev+10k 120 200k 19.62 22.42 59.17 30.26 52.21
25k 300 200k 32.96 22.35 59.29 30.39 52.14
50k 600 200k 32.86 22.40 59.15 30.54 51.88
Table 6: German evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 30.18 29.89 49.05 57.14 32.56
15k 200 16 34.65 28.60 49.63 59.64 30.65
Dev+15k 200 16 33.97 28.88 49.37 58.24 31.81
Dev 120 77k 32.40 30.81 48.40 52.90 36.85
15k 200 200k 35.05 28.34 49.69 59.81 30.59
Dev+15k 200 200k 34.45 28.98 49.30 57.61 32.71
Table 7: Russian evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint fea-
ture selection in distributed stochastic learning for
large-scale discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP.
133
