Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1758?1763,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Large-scale Reordering Model for Statistical Machine Translation
using Dual Multinomial Logistic Regression
Abdullah Alrajeh
ab
and Mahesan Niranjan
b
a
Computer Research Institute, King Abdulaziz City for Science and Technology (KACST)
Riyadh, Saudi Arabia, asrajeh@kacst.edu.sa
b
School of Electronics and Computer Science, University of Southampton
Southampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.uk
Abstract
Phrase reordering is a challenge for statis-
tical machine translation systems. Posing
phrase movements as a prediction prob-
lem using contextual features modeled by
maximum entropy-based classifier is su-
perior to the commonly used lexicalized
reordering model. However, Training
this discriminative model using large-scale
parallel corpus might be computationally
expensive. In this paper, we explore recent
advancements in solving large-scale clas-
sification problems. Using the dual prob-
lem to multinomial logistic regression, we
managed to shrink the training data while
iterating and produce significant saving in
computation and memory while preserv-
ing the accuracy.
1 Introduction
Phrase reordering is a common problem when
translating between two grammatically different
languages. Analogous to speech recognition sys-
tems, statistical machine translation (SMT) sys-
tems relied on language models to produce more
fluent output. While early work penalized phrase
movements without considering reorderings aris-
ing from vastly differing grammatical structures
across language pairs like Arabic-English (Koehn,
2004a), many researchers considered lexicalized
reordering models that attempted to learn orienta-
tion based on the training corpus (Tillmann, 2004;
Kumar and Byrne, 2005; Koehn et al., 2005).
Building on this, some researchers have bor-
rowed powerful ideas from the machine learning
literature, to pose the phrase movement problem
as a prediction problem using contextual input fea-
tures whose importance is modeled as weights of
a linear classifier trained by entropic criteria. The
approach (so called maximum entropy classifier
or simply MaxEnt) is a popular choice (Zens and
Ney, 2006; Xiong et al., 2006; Nguyen et al.,
2009; Xiang et al., 2011). Max-margin structure
classifiers were also proposed (Ni et al., 2011).
Alternatively, Cherry (2013) proposed recently us-
ing sparse features optimize the translation quality
with the decoder instead of training a classifier in-
dependently.
While large-scale parallel corpus is advanta-
geous for improving such reordering model, this
improvement comes at a price of computational
complexity. This issue is particularly pronounced
when discriminative models are considered such
as maximum entropy-based model due to the re-
quired iterative learning.
Advancements in solving large-scale classifica-
tion problems have been shown to be effective
such as dual coordinate descent method for linear
support vector machines (Hsieh et al., 2008). Sim-
ilarly, Yu et al. (2011) proposed a two-level dual
coordinate descent method for maximum entropy
classifier.
In this work we explore the dual problem to
multinomial logistic regression for building large-
scale reordering model (section 3). One of the
main advantages of solving the dual problem is
providing a mechanism to shrink the training data
which is a serious issue in building such large-
scale system. We present empirical results com-
paring between the primal and the dual problems
(section 4). Our approach is shown to be fast and
memory-efficient.
2 Baseline System
In statistical machine translation, the most likely
translation e
best
of an input sentence f can be
found by maximizing the probability p(e|f), as
follows:
e
best
= arg max
e
p(e|f). (1)
1758
A log-linear combination of different models
(features) is used for direct modeling of the poste-
rior probability p(e|f) (Papineni et al., 1998; Och
and Ney, 2002):
e
best
= arg max
e
n
?
i=1
?
i
h
i
(f , e) (2)
where the feature h
i
(f , e) is a score function
over sentence pairs. The translation model and the
language model are the main features in any sys-
tem although additional features h(.) can be inte-
grated easily (such as word penalty). State-of-the-
art systems usually have around ten features.
The language model, which ensures fluent
translation, plays an important role in reordering;
however, it has a bias towards short translations
(Koehn, 2010). Therefore, a need for developing a
specific model for the reordering problem.
2.1 Lexicalized Reordering Model
Adding a lexicalized reordering model consis-
tently improved the translation quality for sev-
eral language pairs (Koehn et al., 2005). Re-
ordering modeling involves formulating phrase
movements as a classification problem where each
phrase position considered as a class (Tillmann,
2004). Some researchers classified phrase move-
ments into three categories (monotone, swap, and
discontinuous) but the classes can be extended to
any arbitrary number (Koehn and Monz, 2005). In
general, the distribution of phrase orientation is:
p(o
k
|
?
f
i
, e?
i
) =
1
Z
h(
?
f
i
, e?
i
, o
k
) . (3)
This lexicalized reordering model is estimated
by relative frequency where each phrase pair
(
?
f
i
, e?
i
) with such an orientation (o
k
) is counted
and then normalized to yield the probability as fol-
lows:
p(o
k
|
?
f
i
, e?
i
) =
count(
?
f
i
, e?
i
, o
k
)
?
o
count(
?
f
i
, e?
i
, o)
. (4)
The orientation of a current phrase pair is de-
fined with respect to the previous target phrase.
Galley and Manning (2008) extended the model to
tackle long-distance reorderings. Their hierarchi-
cal model enables phrase movements that are more
complex than swaps between adjacent phrases.
3 Multinomial Logistic Regression
Multinomial logistic regression (MLR), also
known as maximum entropy classifier (Zens and
Ney, 2006), is a probabilistic model for the multi-
class problem. The class probability is given by:
p(o
k
|
?
f
i
, e?
i
) =
exp(w
>
k
?(
?
f
i
, e?
i
))
?
k
?
exp(w
>
k
?
?(
?
f
i
, e?
i
))
, (5)
where ?(
?
f
i
, e?
i
) is the feature vector of the i-th
phrase pair. An equivalent notation to w
>
k
?(
?
f
i
, e?
i
)
is w
>
f(?(
?
f
i
, e?
i
), o
k
) where w is a long vector
composed of all classes parameters (i.e. w
>
=
[w
>
1
. . .w
>
K
] ) and f(., .) is a joint feature vec-
tor decomposed via the orthogonal feature rep-
resentation (Rousu et al., 2006). This repre-
sentation simply means there is no crosstalk be-
tween two different feature vectors. For example,
f(?(
?
f
i
, e?
i
), o
1
)
>
= [?(
?
f
i
, e?
i
)
>
0 . . . 0].
The model?s parameters can be estimated by
minimizing the following regularized negative
log-likelihood P(w) as follows (Bishop, 2006):
min
w
1
2?
2
K
?
k=1
?w
k
?
2
?
N
?
i=1
K
?
k=1
p?
ik
log p(o
k
|
?
f
i
, e?
i
)
(6)
Here ? is a penalty parameter and p? is the em-
pirical distribution where p?
ik
equals zero for all
o
k
6= o
i
.
Solving the primal optimization problem (6) us-
ing the gradient:
?P(w)
?w
k
=
w
k
?
2
?
N
?
i=1
(
p?
ik
? p(o
k
|
?
f
i
, e?
i
)
)
?(
?
f
i
, e?
i
),
(7)
do not constitute a closed-form solution. In our
experiments, we used stochastic gradient decent
method (i.e. online learning) to estimate w which
is shown to be fast and effictive for large-scale
problems (Bottou, 2010). The method approxi-
mates (7) by a gradient at a single randomly picked
phrase pair. The update rule is:
w
?
k
= w
k
? ?
i
?
k
P
i
(w), (8)
where ?
i
is a positive learning rate.
1759
3.1 The Dual Problem
Lebanon and Lafferty (2002) derived an equiva-
lent dual problem to (6). Introducing Lagrange
multipliers ?, the dual becomes
min
w
1
2?
2
K
?
k=1
?w
k
(?)?
2
+
N
?
i=1
K
?
k=1
?
ik
log?
ik
,
s.t.
K
?
k=1
?
ik
= 1 and ?
ik
? 0 ,?i, k, (9)
where
w
k
(?) = ?
2
N
?
i=1
(p?
ik
? ?
ik
)?(
?
f
i
, e?
i
) (10)
As mentioned in the introduction, Yu et al.
(2011) proposed a two-level dual coordinate de-
scent method to minimize D(?) in (9) but it has
some numerical difficulties. Collins et al. (2008)
proposed simple exponentiated gradient (EG) al-
gorithm for Conditional Random Feild (CRF). The
algorithm is applicable to our problem, a special
case of CRF. The rule update is:
?
?
ik
=
?
ik
exp(??
i
?
ik
D(?))
?
k
?
?
ik
?
exp(??
i
?
ik
?
D(?))
(11)
where
?
ik
D(?) ?
?D(?)
??
ik
= 1 + log?
ik
+
(
w
y
(?)
>
?(
?
f
i
, e?
i
)?w
k
(?)
>
?(
?
f
i
, e?
i
)
)
.
(12)
Here y represents the true class (i.e. o
y
= o
i
).
To improve the convergence, ?
i
is adaptively ad-
justed for each example. If the objective function
(9) did not decrease, ?
i
is halved for number of tri-
als (Collins et al., 2008). Calculating the function
difference below is the main cost in EG algorithm,
D(?
?
)?D(?) =
K
?
k=1
(
?
?
ik
log?
?
ik
? ?
ik
log?
ik
)
?
K
?
k=1
(?
?
ik
? ?
ik
)w
k
(?)
>
?(
?
f
i
, e?
i
)
+
?
2
2
??(
?
f
i
, e?
i
)?
2
K
?
k=1
(?
?
ik
? ?
ik
)
2
. (13)
Clearly, the cost is affordable because w
k
(?) is
maintained throughout the algorithm as follows:
w
k
(?
?
) = w
k
(?)??
2
(?
?
ik
??
ik
)?(
?
f
i
, e?
i
) (14)
Following Yu et al. (2011), we initialize ?
ik
as
follows:
?
ik
=
{
(1? ) if o
k
= o
i
;

K?1
else.
(15)
where  is a small positive value. This is because
the objective function (9) is not well defined at
?
ik
= 0 due to the logarithm appearance.
Finally, the optimal dual variables are achieved
when the following condition is satisfied for all ex-
amples (Yu et al., 2011):
max
k
?
ik
D(?) = min
k
?
ik
D(?) (16)
This condition is the key to accelerate EG al-
gorithm. Unlike the primal problem (6), the dual
variables ?
ik
are associated with each example
(i.e. phrase pair) therefore a training example can
be disregarded once its optimal dual variables ob-
tained. More data shrinking can be achieved by
tolerating a small difference between the two val-
ues in (16). Algorithm 1 presents the overall pro-
cedure (shrinking step is from line 6 to 9).
Algorithm 1 Shrinking stochastic exponentiated
gradient method for training the dual problem
Require: training set S = {?(
?
f
i
, e?
i
), o
i
}
N
i=1
1: Given ? and the corresponding w(?)
2: repeat
3: Randomly pick i from S
4: Claculate?
ik
D(?) ?k by (12)
5: v
i
= max
k
?
ik
D(?)?min
k
?
ik
D(?)
6: if v
i
?  then
7: Remove i from S
8: Continue from line 3
9: end if
10: ? = 0.5
11: for t = 1 to maxTrial do
12: Calculate ?
?
ik
?k by (11)
13: if D(?
?
)?D(?) ? 0 then
14: Update ? and w(?) by (14)
15: Break
16: end if
17: ? = 0.5 ?
18: end for
19: until v
i
?  ?i
1760
4 Experiments
We used MultiUN which is a large-scale parallel
corpus extracted from the United Nations website
(Eisele and Chen, 2010). We have used Arabic
and English portion of MultiUN where the English
side is about 300 million words.
We simplify the problem by classifying phrase
movements into three categories (monotone,
swap, discontinuous). To train the reordering
models, we used GIZA++ to produce word align-
ments (Och and Ney, 2000). Then, we used the
extract tool that comes with the Moses toolkit
(Koehn et al., 2007) in order to extract phrase pairs
along with their orientation classes.
As shown in Table 1, each extracted phrase pair
is represented by linguistic features as follows:
? Aligned source and target words in a phrase
pair. Each word alignment is a feature.
? Words within a window around the source
phrase to capture the context. We choose ad-
jacent words of the phrase boundary.
The extracted phrase pairs after filtering are
47,227,789. The features that occur more than 10
times are 670,154.
Sentence pair:
f : f
1
f
2
1
f
3
f
4
f
5
2
f
6
3
.
e : e
1
1
e
2
e
3
3
e
4
e
5
2
.
Extracted phrase pairs (
?
f , e?) :
?
f
i
||| e?
i
||| o
i
||| alignment ||| context
f
1
f
2
||| e
1
||| mono ||| 0-0 1-0 ||| f
3
f
3
f
4
f
5
||| e
4
e
5
||| swap ||| 0-1 2-0 ||| f
2
f
6
f
6
||| e
2
e
3
||| other ||| 0-0 0-1 ||| f
5
All linguistic features:
1. f
1
&e
1
2. f
2
&e
1
3. f
3
4. f
3
&e
5
5. f
5
&e
4
6. f
2
7. f
6
8. f
6
&e
2
9. f
6
&e
3
10. f
5
Bag-of-words representation:
a phrase pair is represented as a vector where each feature
is a discrete number (0=not exist).
?(
?
f
i
, e?
i
) 1 2 3 4 5 6 7 8 9 10
?(
?
f
1
, e?
1
) = 1 1 1 0 0 0 0 0 0 0
?(
?
f
2
, e?
2
) = 0 0 0 1 1 1 1 0 0 0
?(
?
f
3
, e?
3
) = 0 0 0 0 0 0 1 1 1 1
Table 1: A generic example of the process of
phrase pair extraction and representation.
4.1 Classification
We trained our reordering models by both primal
and dual classifiers for 100 iterations. For the dual
MLR, different shrinking levels have been tried by
varying the parameter () in Algorithm 1. Table 2
reports the training time and classification error
rate of these models.
Training the dual MLR with moderate shrinking
level (i.e.  = 0.1) is almost four times faster than
training the primal one. Choosing larger value for
() leads to faster training but might harm the per-
formance as shown below.
Classifier Training Time Error Rate
Primal MLR 1 hour 9 mins 17.81%
Dual MLR :0.1 18 minutes 17.95%
Dual MLR :1.0 13 minutes 21.13%
Dual MLR :0.01 22 minutes 17.89%
Table 2: Performance of the primal and dual MLR
based on held-out data.
Figure 1 shows the percentage of active set dur-
ing training dual MLR with various shrinking lev-
els. Interestingly, the dual MLR could disregard
more than 99% of the data after a couple of iter-
ations. For very large corpus, the data might not
fit in memory and training primal MLR will take
long time due to severe disk-swapping. In this sit-
uation, using dual MLR is very beneficial.
2 4 6 8 10 12 14 16 18 200
10
20
30
40
50
60
70
80
90
100
Training iteration
Per
cen
tag
e o
f ac
tive
 ph
ras
e p
airs
 
 
? = 0.1
? = 1.0
? = 0.01
Figure 1: Percentage of active set in dual MLR.
As the data size decreases, each iteration takes far
less computation time (see Table 2 for total time).
1761
4.2 Translation
We used the Moses toolkit (Koehn et al., 2007)
with its default settings to build three phrase-based
translation systems. They differ in how their re-
ordering models were estimated. The language
model is a 5-gram with interpolation and Kneser-
Ney smoothing (Kneser and Ney, 1995). We tuned
the system by using MERT technique (Och, 2003).
As commonly used in statistical machine trans-
lation, we evaluated the translation performance
by BLEU score (Papineni et al., 2002). The test
sets are NIST MT06 and MT08 where the En-
glish sides are 35,481 words (1056 sentences) and
116,840 words (3252 sentences), respectively. Ta-
ble 3 shows the BLEU scores for the translation
systems. We also computed statistical significance
for the models using the paired bootstrap resam-
pling method (Koehn, 2004b).
Translation System MT06 MT08
Baseline + Lexical. model 30.86 34.22
Baseline + Primal MLR 31.37* 34.85*
Baseline + Dual MLR :0.1 31.36* 34.87*
Table 3: BLEU scores for Arabic-English transla-
tion systems with different reordering models (*:
better than the lexicalized model with at least 95%
statistical significance).
5 Conclusion
In training such system with large data sizes and
big dimensionality, computational complexity be-
come a serious issue. In SMT, maximum entropy-
based reordering model is often introduced as a
better alternative to the commonly used lexical-
ized one. However, training this discriminative
model using large-scale corpus might be compu-
tationally expensive due to the iterative learning.
In this paper, we propose training the model
using the dual MLR with shrinking method. It
is almost four times faster than the primal MLR
(also know as MaxEnt) and much more memory-
efficient. For very large corpus, the data might not
fit in memory and training primal MLR will take
long time due to severe disk-swapping. In this sit-
uation, using dual MLR is very beneficial. The
proposed method is also useful for many classi-
fication problems in natural language processing
that require large-scale data.
References
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secau-
cus, NJ, USA.
L?eon Bottou. 2010. Large-scale machine learning
with stochastic gradient descent. In Yves Lecheval-
lier and Gilbert Saporta, editors, Proceedings of
the 19th International Conference on Computa-
tional Statistics (COMPSTAT?2010), pages 177?
187, Paris, France, August. Springer.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 22?
31, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponen-
tiated gradient algorithms for conditional random
fields and max-margin markov networks. Journal
of Machine Learning Research, 9:1775?1822, June.
Andreas Eisele and Yu Chen. 2010. Multiun:
A multilingual corpus from united nation docu-
ments. In Daniel Tapias, Mike Rosner, Ste-
lios Piperidis, Jan Odjik, Joseph Mariani, Bente
Maegaard, Khalid Choukri, and Nicoletta Calzo-
lari (Conference Chair), editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation, pages 2868?2872. Euro-
pean Language Resources Association (ELRA), 5.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Hawaii, October. Association
for Computational Linguistics.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
dual coordinate descent method for large-scale lin-
ear svm. In Proceedings of the 25th International
Conference on Machine Learning, ICML ?08, pages
408?415.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. IEEE
International Conference on Acoustics, Speech and
Signal Processing, pages 181?184.
Philipp Koehn and Christof Monz. 2005. Shared
task: Statistical machine translation between euro-
pean languages. In Proceedings of ACL Workshop
on Building and Using Parallel Texts, pages 119?
124. Association for Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
1762
for the 2005 IWSLT speech translation evaluation.
In Proceedings of International Workshop on Spo-
ken Language Translation, Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ond?rej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL 2007 Demo
and Poster Sessions, pages 177?180.
Philipp Koehn. 2004a. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of 6th Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA), pages 115?124, Washington DC.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
161?168, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Guy Lebanon and John D. Lafferty. 2002. Boosting
and maximum likelihood for exponential models. In
T.G. Dietterich, S. Becker, and Z. Ghahramani, ed-
itors, Advances in Neural Information Processing
Systems 14, pages 447?454. MIT Press.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving a lex-
icalized hierarchical reordering model using maxi-
mum entropy. In Proceedings of the Twelfth Ma-
chine Translation Summit (MT Summit XII). Inter-
national Association for Machine Translation.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2011. Exploitation of machine
learning techniques in modelling phrase movements
for machine translation. Journal of Machine Learn-
ing Research, 12:1?30, February.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, and Todd Ward.
1998. Maximum likelihood and discriminative
training of direct translation models. In Proceedings
of ICASSP, pages 189?192.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning of
hierarchical multilabel classification models. Jour-
nal of Machine Learning Research, pages 1601?
1626.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL: Short Papers, pages 101?
104.
Bing Xiang, Niyu Ge, and Abraham Ittycheriah. 2011.
Improving reordering for statistical machine transla-
tion with smoothed priors and syntactic features. In
Proceedings of SSST-5, Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
pages 61?69, Portland, Oregon, USA. Association
for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL,
pages 521?528, Sydney, July. Association for Com-
putational Linguistics.
Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy models. Machine
Learning, 85(1-2):41?75, October.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings on the Workshop on Statis-
tical Machine Translation, pages 55?63, New York
City, June. Association for Computational Linguis-
tics.
1763
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 477?485,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Bayesian Reordering Model with Feature Selection
Abdullah Alrajeh
ab
and Mahesan Niranjan
b
a
Computer Research Institute, King Abdulaziz City for Science and Technology (KACST)
Riyadh, Saudi Arabia, asrajeh@kacst.edu.sa
b
School of Electronics and Computer Science, University of Southampton
Southampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.uk
Abstract
In phrase-based statistical machine trans-
lation systems, variation in grammatical
structures between source and target lan-
guages can cause large movements of
phrases. Modeling such movements is cru-
cial in achieving translations of long sen-
tences that appear natural in the target lan-
guage. We explore generative learning
approach to phrase reordering in Arabic
to English. Formulating the reordering
problem as a classification problem and
using naive Bayes with feature selection,
we achieve an improvement in the BLEU
score over a lexicalized reordering model.
The proposed model is compact, fast and
scalable to a large corpus.
1 Introduction
Currently, the dominant approach to machine
translation is statistical, starting from the math-
ematical formulations and algorithms for param-
eter estimation (Brown et al., 1988), further ex-
tended in (Brown et al., 1993). These early mod-
els, widely known as the IBM models, were word-
based. Recent extensions note that a better ap-
proach is to group collections of words, or phrases,
for translation together, resulting in a significant
focus these days on phrase-based statistical ma-
chine translation systems.
To deal with the alignment problem of one-
to-many word alignments in the IBM model
formulation, whereas phrase-based models may
have many-to-many translation relationships, IBM
models are trained in both directions, source to tar-
get and target to source, and their word alignments
are combined (Och and Ney, 2004).
While phrase-based systems are a significant
improvement over word-based approaches, a par-
ticular issue that emerges is long-range reorder-
ings at the phrase level (Galley and Manning,
2008). Analogous to speech recognition systems,
translation systems relied on language models to
produce more fluent translation. While early work
penalized phrase movements without considering
reorderings arising from vastly differing grammat-
ical structures across language pairs like Arabic-
English, many researchers considered lexical re-
ordering models that attempted to learn orienta-
tion based on content (Tillmann, 2004; Kumar
and Byrne, 2005; Koehn et al., 2005). These
approaches may suffer from the data sparseness
problem since many phrase pairs occur only once
(Nguyen et al., 2009).
As an alternative way of exploiting function ap-
proximation capabilities offered by machine learn-
ing methods, there is recent interest in formulating
a learning problem that aims to predict reorder-
ing from linguistic features that capture their con-
text. An example of this is the maximum entropy
method used by (Xiang et al., 2011; Nguyen et al.,
2009; Zens and Ney, 2006; Xiong et al., 2006).
In this work we apply a naive Bayes classifier,
combined with feature selection to address the re-
ordering problem. To the best of our knowledge,
this simple model of classification has not been
used in this context previously. We present em-
pirical results comparing our work and previously
proposed lexicalized reordering model. We show
that our model is scalable to large corpora.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work in the
field and how that is related to our paper. Section 3
gives an overview of the baseline translation sys-
tem. Section 4 introduces the Bayesian reorder-
ing model and gives details of different inference
methods, while, Section 5 describes feature selec-
tion method. Section 6 presents the experiments
and reports the results evaluated as classification
and translation problems. Finally, we end the pa-
per with a summary of our conclusions and per-
spectives.
477
Symbol Notation
f/e a source / target sentence (string)
?
f/e? a source / target phrase sequence
N the number of examples
K the number of classes
(
?
f
n
, e?
n
) the n-th phrase pair in (
?
f , e?)
o
n
the orientation of (
?
f
n
, e?
n
)
?(
?
f
n
, e?
n
) the feature vector of (
?
f
n
, e?
n
)
Table 1: Notation used in this paper.
2 Related Work
The phrase reordering model is a crucial compo-
nent of any translation system, particularly be-
tween language pairs with different grammatical
structures (e.g. Arabic-English). Adding a lex-
icalized reordering model consistently improved
the translation quality for several language pairs
(Koehn et al., 2005). The model tries to predict the
orientation of a phrase pair with respect to the pre-
vious adjacent target words. Ideally, the reorder-
ing model would predict the right position in the
target sentence given a source phrase, which is dif-
ficult to achieve. Therefore, positions are grouped
into limited orientations or classes.The orientation
probability for a phrase pair is simply based on the
relative occurrences in the training corpus.
The lexicalized reordering model has been ex-
tended to tackle long-distance reorderings (Gal-
ley and Manning, 2008). This takes into account
the hierarchical structure of the sentence when
considering such an orientation. Certain exam-
ples are often used to motivate syntax-based sys-
tems were handled by this hierarchical model, and
this approach is shown to improve translation per-
formance for several translation tasks with small
computational cost.
Despite the fact that the lexicalized reordering
model is always biased towards the most frequent
orientation for such a phrase pair, it may suffer
from a data sparseness problem since many phrase
pairs occur only once. Moreover, the context of
a phrase might affect its orientation, which is not
considered as well.
Adopting the idea of predicting orientation
based on content, it has been proposed to represent
each phrase pair by linguistic features as reorder-
ing evidence, and then train a classifier for predic-
tion. The maximum entropy classifier is a popu-
lar choice among many researchers (Zens and Ney,
2006; Xiong et al., 2006; Nguyen et al., 2009; Xi-
ang et al., 2011). Max-margin structure classifiers
were also proposed (Ni et al., 2011). Recently,
Cherry (2013) proposed using sparse features op-
timize BLEU with the decoder instead of training
a classifier independently.
We distinguish our work from the previous ones
in the following. We propose a fast reordering
model using a naive Bayes classifier with feature
selection. In this study, we undertake a compari-
son between our work and lexicalized reordering
model.
3 Baseline System
In statistical machine translation, the most likely
translation e
best
of an input sentence f can be
found by maximizing the probability p(e|f), as
follows:
e
best
= arg max
e
p(e|f). (1)
A log-linear combination of different models
(features) is used for direct modeling of the poste-
rior probability p(e|f) (Papineni et al., 1998; Och
and Ney, 2002):
e
best
= arg max
e
n
?
i=1
?
i
h
i
(f , e) (2)
where the feature h
i
(f , e) is a score function
over sentence pairs. The translation model and the
language model are the main features in any sys-
tem although additional features h(.) can be inte-
grated easily (such as word penalty). State-of-the-
art systems usually have around ten features (i.e.
n = 10).
In phrase-based systems, the translation model
can capture the local meaning for each source
phrase. However, to capture the whole meaning
of a sentence, its translated phrases need to be in
the correct order. The language model, which en-
sures fluent translation, plays an important role in
reordering; however, it prefers sentences that are
grammatically correct without considering their
actual meaning. Besides that, it has a bias towards
short translations (Koehn, 2010). Therefore, de-
veloping a reordering model will improve the ac-
curacy particularly when translating between two
grammatically different languages.
3.1 Lexicalized Reordering Model
Phrase reordering modeling involves formulat-
ing phrase movements as a classification problem
478
where each phrase position considered as a class
(Tillmann, 2004). Some researchers classified
phrase movements into three categories (mono-
tone, swap, and discontinuous) but the classes can
be extended to any arbitrary number (Koehn and
Monz, 2005). In general, the distribution of phrase
orientation is:
p(o
k
|
?
f
n
, e?
n
) =
1
Z
h(
?
f
n
, e?
n
, o
k
) . (3)
This lexicalized reordering model is estimated
by relative frequency where each phrase pair
(
?
f
n
, e?
n
) with such an orientation (o
k
) is counted
and then normalized to yield the probability as fol-
lows:
p(o
k
|
?
f
n
, e?
n
) =
count(
?
f
n
, e?
n
, o
k
)
?
o
count(
?
f
n
, e?
n
, o)
. (4)
The orientation class of a current phrase pair is
defined with respect to the previous target word
or phrase (i.e. word-based classes or phrase-based
classes). In the case of three categories (mono-
tone, swap, and discontinuous): monotone is the
previous source phrase (or word) that is previ-
ously adjacent to the current source phrase, swap
is the previous source phrase (or word) that is next-
adjacent to the current source phrase, and discon-
tinuous is not monotone or swap.
Galley and Manning (2008) extended the lex-
icalized reordering mode to tackle long-distance
phrase reorderings. Their hierarchical model en-
ables phrase movements that are more complex
than swaps between adjacent phrases.
4 Bayesian Reordering Model
Many feature-based reordering models have been
proposed to replace the lexicalized reordering
model. The reported results showed consistent im-
provement in terms of various translation metrics.
Naive Bayes method has been a popular clas-
sification model of choice in many natural lan-
guage processing problems (e.g. text classifica-
tion). Naive Bayes is a simple classifier that ig-
nores correlation between features, but has the ap-
peal of computational simplicity. It is a generative
probabilistic model based on Bayes? theorem as
below:
p(o
k
|
?
f
n
, e?
n
) =
p(
?
f
n
, e?
n
|o
k
)p(o
k
)
?
o
p(
?
f
n
, e?
n
|o)p(o)
. (5)
The class prior can be estimated easily as a rel-
ative frequency (i.e. p(o
k
) =
N
k
N
). The likeli-
hood distribution p(
?
f
n
, e?
n
|o
k
) is defined based on
the type of data. The classifier will be naive if we
assume that feature variables are conditionally in-
dependent. The naive assumption simplifies our
distribution and hence reduces the parameters that
have to be estimated. In text processing, multi-
nomial is used as a class-conditional distribution
(Rogers and Girolami, 2011). The distribution is
defined as:
p(
?
f
n
, e?
n
|q) = C
?
m
q
?
m
(
?
f
n
,e?
n
)
m
(6)
where C is a multinomial coefficient,
C =
(
?
m
?
m
(
?
f
n
, e?
n
))!
?
m
?
m
(
?
f
n
, e?
n
)!
, (7)
and q are a set of parameters, each of which is a
probability. Estimating these parameters for each
class by maximum likelihood,
arg max
q
k
N
k
?
n
p(
?
f
n
, e?
n
|q
k
), (8)
will result in (Rogers and Girolami, 2011):
q
km
=
?
N
k
n
?
m
(
?
f
n
, e?
n
)
?
M
m
?
?
N
k
n
?
m
?
(
?
f
n
, e?
n
)
. (9)
MAP estimate It is clear that q
km
might be
zero which means the probability of a new phrase
pair with nonzero feature ?
m
(
?
f
n
, e?
n
) is always
zero because of the product in (6). Putting a prior
over q is one smoothing technique. A conjugate
prior for the multinomial likelihood is the Dirich-
let distribution and the MAP estimate for q
km
is
(Rogers and Girolami, 2011):
q
km
=
?? 1 +
?
N
k
n
?
m
(
?
f
n
, e?
n
)
M(?? 1) +
?
M
m
?
?
N
k
n
?
m
?
(
?
f
n
, e?
n
)
(10)
where M is the feature vector?s length or the
feature dictionary size and ? is a Dirichlet param-
eter with a value greater than one. The derivation
is in Appendix A.
Bayesian inference Instead of using a point es-
timate of q as shown previously in equation (10),
Bayesian inference is based on the whole param-
eter space in order to incorporate uncertainty into
our multinomial model. This requires a posterior
479
probability distribution over q as follows:
p(
?
f
n
, e?
n
|o
k
) =
?
p(
?
f
n
, e?
n
|q
k
)p(q
k
|?
k
) dq
k
=C
? (
?
m
?
km
)
?
m
?(?
km
)
?
m
?(?
km
+ ?
m
(
?
f
n
, e?
n
))
?
(
?
m
?
km
+ ?
m
(
?
f
n
, e?
n
)
)
.
(11)
Here ?
k
are new hyperparameters of the pos-
terior derived by means of Bayes theorem as fol-
lows:
p(q
k
|?
k
) =
p(q
k
|?)
?
N
k
n
p(
?
f
n
, e?
n
|q
k
)
?
p(q
k
|?)
?
N
k
n
p(
?
f
n
, e?
n
|q
k
)dq
k
.
(12)
The solution of (11) will result in:
?
k
= ? +
N
k
?
n
?(
?
f
n
, e?
n
). (13)
For completeness we give a summary of deriva-
tions of equations (11) and (13) in Appendix B,
more detailed discussions can be found in (Barber,
2012).
5 Feature Selection
In several high dimensional pattern classification
problems, there is increasing evidence that the
discriminant information may be in small sub-
spaces, motivating feature selection (Li and Niran-
jan, 2013). Having irrelevant or redundant fea-
tures could affect the classification performance
(Liu and Motoda, 1998). They might mislead the
learning algorithms or overfit them to the data and
thus have less accuracy.
The aim of feature selection is to find the op-
timal subset features which maximize the ability
of prediction, which is the main concern, or sim-
plify the learned results to be more understand-
able. There are many ways to measure the good-
ness of a feature or a subset of features; however
the criterion will be discussed is mutual informa-
tion.
5.1 Mutual Information
Information criteria are based on the concept of
entropy which is the amount of randomness. The
distribution of a fair coin, for example, is com-
pletely random so the entropy of the coin is very
high. The following equation calculates the en-
tropy of a variable X (MacKay, 2002):
H (X) = ?
?
x
p(x) log p(x). (14)
The mutual information of a feature X can be mea-
sured by calculating the difference between the
prior uncertainty of the class variable Y and the
posterior uncertainty after using the feature as fol-
lows (MacKay, 2002):
I(X;Y ) = H(Y )?H(Y |X) (15)
=
?
x,y
p(x, y) log
p(x, y)
p(x)p(y)
.
The advantage of mutual Information over other
criteria is the ability to detect nonlinear patterns.
The disadvantage is its bias towards higher ar-
bitrary features; however this problem can be
solved by normalizing the information as follows
(Est?evez et al., 2009):
I
norm
(X;Y ) =
I(X;Y )
min(H(X), H(Y ))
. (16)
6 Experiments
The corpus used in our experiments is MultiUN
which is a large-scale parallel corpus extracted
from the United Nations website
1
(Eisele and
Chen, 2010). We have used Arabic and English
portion of MultiUN. Table 2 shows the general
statistics.
Statistics Arabic English
Sentence Pairs 9.7 M
Running Words 255.5 M 285.7 M
Word/Line 22 25
Vocabulary Size 677 K 410 K
Table 2: General statistics of Arabic-English Mul-
tiUN (M: million, K: thousand).
We simplify the problem by classifying phrase
movements into three categories (monotone,
swap, discontinuous). To train the reordering
models, we used GIZA++ to produce word align-
ments (Och and Ney, 2000). Then, we used the
extract tool that comes with the Moses
2
toolkit
(Koehn et al., 2007) in order to extract phrase pairs
along with their orientation classes.
Each extracted phrase pair is represented by lin-
guistic features as follows:
? Aligned source and target words in a phrase
pair. Each word alignment is a feature.
1
http://www.ods.un.org/ods/
2
Moses is an open source toolkit for statistical machine
translation (www.statmt.org/moses/).
480
? Words within a window around the source
phrase to capture the context. We choose ad-
jacent words of the phrase boundary.
Most researchers build one reordering model
for the whole training set (Zens and Ney, 2006;
Xiong et al., 2006; Nguyen et al., 2009; Xiang
et al., 2011). Ni et al. (Ni et al., 2011) simpli-
fied the learning problem to have as many sub-
models as source phrases. Training data were di-
vided into small independent sets where samples
having the same source phrase are considered a
training set. In our experiments, we have chosen
the first method.
We compare lexicalized and Bayesian reorder-
ing models in two phases. In the classification
phase, we see the performance of the models as
a classification problem. In the translation phase,
we test the actual impact of these reordering mod-
els in a translation system.
6.1 Classification
We built naive Bayes classifier with both MAP es-
timate and Bayesian inference. We also used mu-
tual Information in order to select the most infor-
mative features for our classification task.
Table 3 reports the error rate of the reorder-
ing models compared to the lexicalized reorder-
ing model. All experiments reported here were
repeated three times to evaluate the uncertainties
in our results. The results shows that there is no
advantage to using Bayesian inference instead of
MAP estimate.
Classifier Error Rate
Lexicalized model 25.2%
Bayes-MAP estimate 19.53%
Bayes-Bayesian inference 20.13%
Table 3: Classification error rate of both lexical-
ized and Bayesian models.
The feature selection process reveals that many
features have low mutual information. Hence they
are not related to the classification task and can be
excluded from the model. Figure 1 shows the nor-
malized mutual information for all extracted fea-
tures.
A ranking threshold for selecting features based
on their mutual information is specified experi-
mentally. In Figure 2, we tried different thresh-
olds ranging from 0.001 to 0.05 and measure the
error rate after each reduction. Although there
is no much gain in terms of performance but the
Bayesian model maintains low error rate when the
proportion of selected features is low. The model
with almost half of the feature space is as good as
the one with full feature space.
Figure 1: Normalized mutual information for all
extracted features (ranked from lowest to highest).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 119
19.5
20
20.5
21
21.5
22
22.5
Percentage of Feature Reductuion
Erro
r Ra
te
Figure 2: Classification error rate of the Baysien
model with different levels of feature reduction.
6.2 Translation
6.2.1 Experimental Design
We used the Moses toolkit (Koehn et al., 2007)
with its default settings. The language model
is a 5-gram with interpolation and Kneser-Ney
smoothing (Kneser and Ney, 1995). We tuned the
system by using MERT technique (Och, 2003).
We built four Arabic-English translation sys-
tems. Three systems differ in how their reordering
models were estimated and the fourth system is a
481
baseline system without reordering model. In all
cases, orientation extraction is hierarchical-based
since it is the best approach while orientations are
monotone, swap and discontinuous. The model is
trained in Moses by specifying the configuration
string hier-msd-backward-fe.
As commonly used in statistical machine trans-
lation, we evaluated the translation performance
by BLEU score (Papineni et al., 2002). The test
sets are NIST MT06 and NIST MT08. Table 4
shows statistics of development and test sets. We
also computed statistical significance for the pro-
posed models using the paired bootstrap resam-
pling method (Koehn, 2004).
Evaluation Set Arabic English
Development sentences 696 696
words 19 K 21 K
NIST MT06 sentences 1797 7188
words 49 K 223 K
NIST MT08 sentences 813 3252
words 25 K 117 K
Table 4: Statistics of development and test sets.
The English side in NIST is larger because there
are four translations for each Arabic sentence.
6.2.2 Results
We first demonstrate in Table 5 a general com-
parison of the proposed model and the lexicalized
model in terms of disc size and average speed in a
translation system. The size of Bayesian model is
far smaller. The lexicalized model is slightly faster
than the Bayesian model because we have over-
head computational cost to extract features and
compute the orientation probabilities. However,
the disc size of our model is much smaller which
makes it more efficient practically for large-scale
tasks.
Model Size (MB) Speed (s/sent)
Lexicalized model 604 2.2
Bayesian model 18 2.6
Table 5: Disc size and average speed of the re-
ordering models in a translation system.
Table 6 shows the BLEU scores for the transla-
tion systems according to two test sets. The base-
line system has no reordering model. In the two
test sets, our Bayesian reordering model is better
than the lexicalized one with at least 95% statis-
tical significance. As we have seen in the clas-
sification section, Bayes classifier with Bayesian
inference has no advantage over MAP estimate.
Translation System MT06 MT08
Baseline 28.92 32.13
BL+ Lexicalized model 30.86 34.22
BL+ Bayes-MAP estimate 31.21* 34.72*
BL+ Bayes-Baysien inference 31.20 34.69
Table 6: BLEU scores for Arabic-English trans-
lation systems (*: better than the baseline with at
least 95% statistical significance).
7 Conclusion
In this paper, we have presented generative mod-
eling approach to phrase reordering in machine
translation. We have experimented with trans-
lation from Arabic to English and shown im-
provements over the lexicalized model of estimat-
ing probabilities as relative frequencies of phrase
movements. Our proposed Bayesian model with
feature selection is shown to be superior. The
training time of the model is as fast as the lexical-
ized model. Its storage requirement is many times
smaller which makes it more efficient practically
for large-scale tasks.
The feature selection process reveals that many
features have low mutual information. Hence they
are not related to the classification task and can be
excluded from the model. The model with almost
half of the feature space is as good as the one with
full feature space.
Previously proposed discriminative models
might achieve higher score than the reported re-
sults. However, our model is scalable to large-
scale systems since parameter estimation require
only one pass over the data with limited memory
(i.e. no iterative learning). This is a critical advan-
tage over discriminative models.
Our current work focuses on three issues. The
first is improving the translation speed of the pro-
posed model. The lexicalized model is slightly
faster. The second is using more informative fea-
tures. We plan to explore part-of-speech informa-
tion, which is more accurate in capturing content.
Finally, we will explore different feature selection
methods. In our experiments, feature reduction is
based on univariate ranking which is riskier than
multivariate ranking. This is because useless fea-
ture can be useful with others.
482
References
D. Barber. 2012. Bayesian Reasoning and Machine
Learning. Cambridge University Press.
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, R. Mercer, and P. Roossin. 1988. A sta-
tistical approach to language translation. In 12th In-
ternational Conference on Computational Linguis-
tics (COLING), pages 71?76.
P. Brown, V. Pietra, S. Pietra, and R. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
C. Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 22?
31, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
A. Eisele and Y. Chen. 2010. Multiun: A multilingual
corpus from united nation documents. In Daniel
Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik,
Joseph Mariani, Bente Maegaard, Khalid Choukri,
and Nicoletta Calzolari (Conference Chair), editors,
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation, pages
2868?2872. European Language Resources Associ-
ation (ELRA), 5.
P. Est?evez, M. Tesmer, C. Perez, and J. Zurada. 2009.
Normalized mutual information feature selection.
Trans. Neur. Netw., 20(2):189?201, February.
M. Galley and C. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
848?856, Hawaii, October. Association for Compu-
tational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pages 181?184.
P. Koehn and C. Monz. 2005. Shared task: Sta-
tistical machine translation between european lan-
guages. In Proceedings of ACL Workshop on Build-
ing and Using Parallel Texts, pages 119?124. Asso-
ciation for Computational Linguistics.
P. Koehn, A. Axelrod, A. Mayne, C. Callison-Burch,
M. Osborne, and D. Talbot. 2005. Edinburgh sys-
tem description for the 2005 IWSLT speech trans-
lation evaluation. In Proceedings of International
Workshop on Spoken Language Translation, Pitts-
burgh, PA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages
177?180.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
P. Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 161?168, Van-
couver, British Columbia, Canada, October. Associ-
ation for Computational Linguistics.
Hongyu Li and M. Niranjan. 2013. Discriminant sub-
spaces of some high dimensional pattern classifica-
tion problems. In IEEE International Workshop on
Machine Learning for Signal Processing (MLSP),
pages 27?32.
H. Liu and H. Motoda. 1998. Feature Selection for
Knowledge Discovery and Data Mining. Kluwer
Academic Publishers, Norwell, MA, USA.
D. MacKay. 2002. Information Theory, Inference &
Learning Algorithms. Cambridge University Press,
New York, NY, USA.
V. Nguyen, A. Shimazu, M. Nguyen, and T. Nguyen.
2009. Improving a lexicalized hierarchical reorder-
ing model using maximum entropy. In Proceed-
ings of the Twelfth Machine Translation Summit (MT
Summit XII). International Association for Machine
Translation.
Y. Ni, C. Saunders, S. Szedmak, and M. Niranjan.
2011. Exploitation of machine learning techniques
in modelling phrase movements for machine transla-
tion. Journal of Machine Learning Research, 12:1?
30, February.
F. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting of the Association of Computational Lin-
guistics (ACL).
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
483
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics - Volume 1, ACL ?03, pages 160?167,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
K. Papineni, S. Roukos, and T. Ward. 1998. Max-
imum likelihood and discriminative training of di-
rect translation models. In Proceedings of ICASSP,
pages 189?192.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 311?318, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. Rogers and M. Girolami. 2011. A First Course in
Machine Learning. Chapman & Hall/CRC, 1st edi-
tion.
C. Tillmann. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of
HLT-NAACL: Short Papers, pages 101?104.
B. Xiang, N. Ge, and A. Ittycheriah. 2011. Improving
reordering for statistical machine translation with
smoothed priors and syntactic features. In Proceed-
ings of SSST-5, Fifth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 61?
69, Portland, Oregon, USA. Association for Com-
putational Linguistics.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum en-
tropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages
521?528, Sydney, July. Association for Computa-
tional Linguistics.
R. Zens and H. Ney. 2006. Discriminative reorder-
ing models for statistical machine translation. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 55?63, New York City, June. As-
sociation for Computational Linguistics.
A MAP Estimate Derivation
Multinomial distribution is defined as:
p(x|q) = C
?
m
q
x
m
m
(17)
where C is a multinomial coefficient,
C =
(
?
m
x
m
)!
?
m
x
m
!
, (18)
and q
m
is an event probability (
?
m
q
m
= 1).
A maximum a posteriori probability (MAP) es-
timate requires a prior over q. Dirichlet distribu-
tion is a conjugate prior and is defined as:
p(q|?) =
? (
?
m
?
m
)
?
m
?(?
m
)
?
m
q
?
m
?1
m
(19)
where ?
m
is is a parameter with a positive value.
Finding the MAP estimate for q given a data is
as follows:
q
?
= arg max
q
p(q|?,X)
= arg max
q
{p(q|?)p(X|q)}
= arg max
q
{
p(q|?)
?
n
p(x
n
|q)
}
= arg max
q
{
?
m
q
?
m
?1
m
?
n,m
q
x
nm
m
}
= arg max
q
{
?
m
log q
?
m
?1
m
+
?
n,m
log q
x
nm
m
}
.
(20)
Since our function is subject to constraints
(
?
m
q
m
= 1), we introduce Lagrange multiplier
as follows:
f(q) =
?
m
log q
?
m
?1
m
+
?
n,m
log q
x
nm
m
??(
?
m
q
m
?1).
(21)
Now we can find q
?
by taking the partial deriva-
tive with respect to one variable q
m
:
?f(q)
?q
m
=
?
m
? 1 +
?
n
x
nm
q
m
? ?
q
m
=
?
m
? 1 +
?
n
x
nm
?
. (22)
Finally, we sum both sides over M to find ? :
?
?
m
q
m
=
?
m
(
?
m
? 1 +
?
n
x
nm
)
? =
?
m
(?
m
? 1) +
?
n,m
x
nm
. (23)
The solution can be simplified by choosing the
same value for each ?
m
which will result in:
q
m
=
?? 1 +
?
n
x
nm
M(?? 1) +
?
n,m
?
x
nm
?
. (24)
484
B Bayesian Inference Derivation
In Appendix A, the inference is based on a single
point estimate of q that has the highest posterior
probability. However, it can be based on the whole
parameter space to incorporate uncertainty. The
probability of a new data point marginalized over
the posterior as follows:
p(x|?,X) =
?
p(x|q)p(q|?,X) dq, (25)
p(q|?,X) =
p(q|?)p(X|q)
?
p(q|?)p(X|q)dq
. (26)
Since Dirichlet and Multinomial distributions
are conjugate pairs, they form the same density as
the prior. Therefore the posterior is also Dirichlet.
Now we can expand the posterior expression and
re-arrange it to look like a Dirichlet as follows:
p(q|?,X) ? p(q|?)
?
n
p(x
n
|q)
?
?
m
q
?
m
?1
m
?
n
?
m
q
x
nm
m
?
?
m
q
(?
m
+
?
n
x
nm
)?1
m
. (27)
The new hyperparameters of the posterior is:
?
?
m
= ?
m
+
?
n
x
nm
. (28)
Finally, we expand and re-arrange Dirichlet and
multinomial distributions inside the integral in
(25) as follows:
p(x|?,X) =
?
C
?
m
q
x
m
m
? (
?
m
?
?
m
)
?
m
?(?
?
m
)
?
m
q
?
?
m
?1
m
dq
=C
? (
?
m
?
?
m
)
?
m
?(?
?
m
)
?
?
m
q
?
?
m
+x
m
?1
m
dq. (29)
Note that inside the integral looks a Dirichlet
without a normalizing constant. If we multiply
and divide by its normalizing constant (i.e. Beta
function), the integral is going to be one because
it is a density function, resulting in:
p(x|?,X) = C
? (
?
m
?
?
m
)
?
m
?(?
?
m
)
B(?
?
+ x)
?
1
B(?
?
+ x)
?
m
q
?
?
m
+x
m
?1
m
dq
c
=C
? (
?
m
?
?
m
)
?
m
?(?
?
m
)
B(?
?
+ x)
=C
? (
?
m
?
?
m
)
?
m
?(?
?
m
)
?
m
?(?
?
m
+ x
m
)
? (
?
m
(?
?
m
+ x
m
))
. (30)
485
