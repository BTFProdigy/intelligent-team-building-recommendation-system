Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 169?173,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Data Driven Language Transfer Hypotheses
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
Language transfer, the preferential second
language behavior caused by similarities
to the speaker?s native language, requires
considerable expertise to be detected by
humans alone. Our goal in this work is to
replace expert intervention by data-driven
methods wherever possible. We define a
computational methodology that produces
a concise list of lexicalized syntactic pat-
terns that are controlled for redundancy
and ranked by relevancy to language trans-
fer. We demonstrate the ability of our
methodology to detect hundreds of such
candidate patterns from currently available
data sources, and validate the quality of
the proposed patterns through classifica-
tion experiments.
1 Introduction
The fact that students with different native lan-
guage backgrounds express themselves differ-
ently in second language writing samples has
been established experimentally many times over
(Tetreault et al., 2013), and is intuitive to most
people with experience learning a new language.
The exposure and understanding of this process
could potentially enable the creation of second
language (L2) instruction that is tailored to the na-
tive language (L1) of students.
The detectable connection between L1 and L2
text comes from a range of sources. On one end of
the spectrum are factors such as geographic or cul-
tural preference in word choice, which are a pow-
erful L1 indicator. On the other end lie linguistic
phenomena such as language transfer, in which the
preferential over-use or under-use of structures in
the L1 is reflected in the use of corresponding pat-
terns in the L2. We focus on language transfer in
this work, based on our opinion that such effects
are more deeply connected to and effectively uti-
lized in language education.
The inherent challenge is that viable language
transfer hypotheses are naturally difficult to con-
struct. By the requirement of contrasting different
L1 groups, hypothesis formulation requires deep
knowledge of multiple languages, an ability re-
served primarily for highly trained academic lin-
guists. Furthermore, the sparsity of any particular
language pattern in a large corpus makes it diffi-
cult even for a capable multilingual scholar to de-
tect the few patterns that evidence language trans-
fer. This motivates data driven methods for hy-
pothesis formulation.
We approach this as a representational problem,
requiring the careful definition of a class of lin-
guistic features whose usage frequency can be de-
termined for each L1 background in both L1 and
L2 text (e.g. both German and English written
by Germans). We claim that a feature exhibiting
a sufficiently non-uniform usage histogram in L1
that is mirrored in L2 data is a strong language
transfer candidate, and provide a quantified mea-
sure of this property.
We represent both L1 and L2 sentences in a
universal constituent-style syntactic format and
model language transfer hypotheses with con-
tiguous syntax sub-structures commonly known
as Tree Substitution Grammar (TSG) fragments
(Post and Gildea, 2009)(Cohn and Blunsom,
2010). With these features we produce a concise
ranked list of candidate language transfer hypothe-
ses and their usage statistics that can be automati-
cally augmented as increasing amounts of data be-
come available.
169
2 Related Work
This work leverages several recently released data
sets and analysis techniques, with the primary
contribution being the transformations necessary
to combine these disparate efforts. Our analy-
sis methods are closely tied to those described
in Swanson and Charniak (2013), which con-
trasts techniques for the discovery of discrimina-
tive TSG fragments in L2 text. We modify and
extend these methods to apply to the universal de-
pendency treebanks of McDonald et al. (2013),
which we will refer to below to as the UTB. Bilin-
gual lexicon construction (Haghighi et al., 2008)
is also a key component, although previous work
has focused primarily on nouns while we focus on
stopwords. We also transform the UTB into con-
stituent format, in a manner inspired by Carroll
and Charniak (1992).
There is a large amount of related research in
Native Language Identification (NLI), the task of
predicting L1 given L2 text. This work has culmi-
nated in a well attended shared task (Tetreault et
al., 2013), whose cited report contains an excellent
survey of the history of this task. In NLI, however,
L1 data is not traditionally used, and patterns are
learned directly from L2 text that has been anno-
tated with L1 labels. One notable outlier is Brooke
and Hirst (2012), which attempts NLI using only
L1 data for training using large online dictionar-
ies to tie L2 English bigrams and collocations to
possible direct translations from native languages.
Jarvis and Crossley (2012) presents another set of
studies that use NLI as a method to form language
transfer hypotheses.
3 Methodology
The first of the four basic requirements of our pro-
posed method is the definition of a class of features
F such that a single feature F ? F is capable
of capturing language transfer phenomenon. The
second is a universal representation of both L1 and
L2 data that allows us to count the occurrences of
any F in an arbitrary sentence. Third, as any suf-
ficiently expressive F is likely to be very large, a
method is required to propose an initial candidate
list C ? F . Finally, we refine C into a ranked list
H of language transfer hypotheses, where H has
also been filtered to remove redundancy.
In this work we define F to be the set of Tree
Substitution Grammar (TSG) fragments in our
data, which allows any connected syntactic struc-
ture to be used as a feature. As such, our universal
representation of L1/L2 data must be a constituent
tree structure of the general form used in syntactic
parsing experiments on the Penn Treebank. The
UTB gets us most of the way to our goal, defining
a dependency grammar with a universal set of part
of speech (POS) tags and dependency arc labels.
Two barriers remain to the use of standard TSG
induction algorithms. The first is to define a map-
ping from the dependency tree format to con-
stituency format. We use the following depen-
dency tree to illustrate our transformation.
ROOT DT NN VBZ PRP
The poodle chews it
root
det
nsubj
dobj
Under our transformation, the above dependency
parse becomes
ROOT
root
VBZ-L
nsubj
NN-L
det
DT
the
NN
poodle
VBZ
chews
VBZ-R
dobj
PRP
it
We also require a multilingual lexicon in the form
of a function M
L
(w) for each language L that
maps words to clusters representing their meaning.
In order to avoid cultural cues and reduce noise
in our mapping, we restrict ourselves to clusters
that correspond to a list of L2 stopwords. Any L2
words that do not appear on this list are mapped
to the unknown ?UNK? symbol, as are all for-
eign words that are not good translations of any
L2 stopword. Multiple words from a single lan-
guage can map to the same cluster, and it is worth
noting that this is true for L2 stopwords as well.
To determine the mapping functions M
L
we
train IBM translation models in both directions be-
tween the L2 and each L1. We create a graph in
which nodes are words, either the L2 stopwords or
any L1 word with some translation probability to
170
or from one of the L2 stopwords. The edges in this
graph exist only between L2 and L1 words, and
are directed with weight equal to the IBM model?s
translation probability of the edge?s target given
its source. We construct M
L
by removing edges
with weight below some threshold and calculating
the connected components of the resulting graph.
We then discard any cluster that does not contain
at least one word from each L1 and at least one L2
stopword.
To propose a candidate list C, we use the TSG
induction technique described in Swanson and
Charniak (2013), which simultaneously induces
multiple TSGs from data that has been partitioned
into labeled types. This method permits linguisti-
cally motivated constraints as to which grammars
produce each type of data. For an experimental
setup that considers n different L1s, we use 2n+1
data types; Figure 1 shows the exact layout used
in our experiments. Besides the necessary n data
types for each L1 in its actual native language form
and n in L2 form, we also include L2 data from
L2 native speakers. We also define 2n + 1 gram-
mars. We begin with n grammars that can each
be used exclusively by one native language data
type, representing behavior that is unique to each
native language (grammars A-C in Figure 1) . This
is done for the L2 as well (grammar G). Finally,
we create an interlanguage grammar for each of
our L1 types that can be used in derivation of both
L1 and L2 data produced by speakers of that L1
(grammars D-F).
The final step is to filter and rank the TSG frag-
ments produced in C, where filtering removes re-
dundant features and ranking provides some quan-
tification of our confidence in a feature as a lan-
guage transfer hypothesis. Swanson and Char-
niak (2013) provides a similar method for pure L2
data, which we modify for our purposes. For re-
dundancy filtering no change is necessary, and we
use their recommended Symmetric Uncertainty
method. For a ranking metric of how well a frag-
ment fits the profile of language transfer we adopt
the expected per feature loss (or risk) also de-
scribed in their work. For an arbitrary feature F ,
this is defined as
R(F ) =
1
|T
F
|
?
t?T
F
P
F
(L 6= L
?
t
)
where T
F
is the subset of the test data that contains
the feature F , and L
?
t
is the gold label of test da-
L2
Data
L1
Data
DE DE
FR FR
ES ES
EN
A
B
C
D
E
F
G
Figure 1: The multi-grammar induction setup used
in our experiments. Squares indicate data types,
and circles indicate grammars. Data type labels
indicate the native language of the speaker, and all
L2 data is in English.
tum t. While in their work the predictive distribu-
tion P
F
(L) is determined by the observed counts
of F in L2 training data, we take our estimates
directly from the L1 data of the languages under
study. This metric captures the extent to which the
knowledge of a feature F ?s L1 usage can be used
to predict its usage in L2.
The final result is a ranked and filtered list of hy-
potheses H . The elements of H can be subjected
to further investigation by experts and the accom-
panying histogram of counts contains the relevant
empirical evidence. As more data is added, the
uncertainty in the relative proportions of these his-
tograms and their corresponding R is decreased.
One additional benefit of our method is that TSG
induction is a random process, and repeated runs
of the sampling algorithm can produce different
features. Since redundancy is filtered automati-
cally, these different feature lists can be combined
and processed to potentially find additional fea-
tures given more computing time.
4 Results
Limited by the intersection of languages across
data sets, we take French, Spanish, and German
as our set of L1s with English as the L2. We use
the UTB for our native language data, which pro-
vides around 4000 sentences of human annotated
text for each L1. For our L2 data we use the ETS
Corpus of Non-Native English (Blanchard et al.,
2013), which consists of over 10K sentences per
L1 label drawn from TOEFL
r
exam essays. Fi-
171
nally, we use the Penn Treebank as our source of
native English data, for a total of seven data types;
four in English, and one in each L1.
When calculating metrics such as redundancy
and R(F ) we use all available data. For TSG
sampling, we balance our data sets to 4000 sen-
tences from each data type and sample using the
Enbuske sampler that was released with Swanson
and Charniak (2013). To construct word clusters,
we use Giza++ (Och and Ney, 2003) and train on
the Europarl data set (Koehn, 2005), using .25 as
a threshold for construction on connected compo-
nents.
We encourage the reader to peruse the full list
of results
1
, in which each item contains the infor-
mation in the following example.
advcl
VERB-L
mark
VERB
110
VERB-R
ES DE FR
L1 4.2 0.0 0.0
L2 2.3 0.3 0.3
This fragment corresponds to an adverbial
clause whose head is a verb in the cluster 110,
which contains the English word ?is? and its vari-
ous translations. This verb has a single left depen-
dent, a clause marker such as ?because?, and at
least one right dependent. Its prevalence in Span-
ish can explained by examining the translations of
the English sentence ?I like it because it is red?.
ES Me gusta porque es rojo.
DE Ich mag es, weil es rot ist.
FR Je l?aime parce qu?il est rouge.
Only in the Spanish sentence is the last pronoun
dropped, as in ?I like it because is red?. This
observation, along with the L1/L2 profile which
shows the count per thousand sentences in each
language provides a strong argument that this pat-
tern is indeed a form of language transfer.
Given our setup of three native languages, a fea-
ture with R(F ) < .66 is a candidate for language
transfer. However, several members of our filtered
list have R(F ) > .66, which is to say that their
1
bllip.cs.brown.edu/download/interlanguage corpus.pdf
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0  10  20  30  40  50  60  70  80  90C
las
sif
ica
tio
n A
cc
ura
cy
 (%
)
Sentences Per Test Case
Figure 2: Creating test cases that consist of sev-
eral sentences mediates feature sparsity, providing
clear evidence for the discriminative power of the
chosen feature set.
L2 usage does not mirror L1 usage. This is to be
expected in some cases due to noise, but it raises
the concern that our features withR(F ) < .66 are
also the result of noise in the data. To address this,
we apply our features to the task of cross language
NLI using only L1 data for training. If the varia-
tion ofR(F ) around chance is simply due to noise
then we would expect near chance (33%) classifi-
cation accuracy. The leftmost point in Figure 2
shows the initial result, using boolean features in
a log-linear classification model, where a test case
involves guessing an L1 label for each individual
sentence in the L2 corpus. While the accuracy
does exceed chance, the margin is not very large.
One possible explanation for this small margin
is that the language transfer signal is sparse, as it
is likely that language transfer can only be used to
correctly label a subset of L2 data. We test this by
combining randomly sampled L2 sentences with
the same L1 label, as shown along the horizontal
axis of Figure 2. As the number of sentences used
to create each test case is increased, we see an in-
crease in accuracy that supports the argument for
sparsity; if the features were simply weak predic-
tors, this curve would be flat. The resulting margin
is much larger, providing evidence that a signifi-
cant portion of our features with R(F ) < .66 are
not selected due to random noise in R and are in-
deed connected to language transfer.
The number and strength of these hypotheses is
easily augmented with more data, as is the number
of languages under consideration. Our results also
motivate future work towards automatic genera-
tion of L1 targeted language education exercises,
and the fact that TSG fragments are a component
of a well studied generative language model makes
them well suited to such generation tasks.
172
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013. Toefl11:
A corpus of non-native english. Technical report,
Educational Testing Service.
Julian Brooke and Graeme Hirst. 2012. Measur-
ing Interlanguage: Native Language Identification
with L1-influence Metrics. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation (LREC-2012), pages 779?
784, Istanbul, Turkey, May. European Language Re-
sources Association (ELRA). ACL Anthology Iden-
tifier: L12-1016.
Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Technical Report CS-92-16,
Brown University, Providence, RI, USA.
Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225?230. Association for Computational Linguis-
tics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach, volume 64. Multilingual Matters Limited,
Bristol, UK.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT Summit.
Ryan T. McDonald, Joakim Nivre, Yvonne
Quirmbach-Brundage, Yoav Goldberg, Dipan-
jan Das, Kuzman Ganchev, Keith Hall, Slav Petrov,
Hao Zhang, Oscar T?ackstr?om, Claudia Bedini,
N?uria Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal dependency annotation for multilingual
parsing. In ACL (2), pages 92?97.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45?48. Association for Computational Linguistics.
Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language ac-
quisition. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 85?94, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, Atlanta, GA, USA, June.
Association for Computational Linguistics.
173
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 357?361,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Correction Detection and Error Type Selection as an ESL Educational Aid
Ben Swanson
Brown University
chonger@cs.brown.edu
Elif Yamangil
Harvard University
elif@eecs.harvard.edu
Abstract
We present a classifier that discriminates be-
tween types of corrections made by teachers
of English in student essays. We define a set
of linguistically motivated feature templates
for a log-linear classification model, train this
classifier on sentence pairs extracted from
the Cambridge Learner Corpus, and achieve
89% accuracy improving upon a 33% base-
line. Furthermore, we incorporate our classi-
fier into a novel application that takes as input
a set of corrected essays that have been sen-
tence aligned with their originals and outputs
the individual corrections classified by error
type. We report the F-Score of our implemen-
tation on this task.
1 Introduction
In a typical foreign language education classroom
setting, teachers are presented with student essays
that are often fraught with errors. These errors can
be grammatical, semantic, stylistic, simple spelling
errors, etc. One task of the teacher is to isolate these
errors and provide feedback to the student with cor-
rections. In this body of work, we address the pos-
sibility of augmenting this process with NLP tools
and techniques, in the spirit of Computer Assisted
Language Learning (CALL).
We propose a step-wise approach in which a
teacher first corrects an essay and then a computer
program aligns their output with the original text and
separates and classifies independent edits. With the
program?s analysis the teacher would be provided
accurate information that could be used in effective
lesson planning tailored to the students? strengths
and weaknesses.
This suggests a novel NLP task with two compo-
nents: The first isolates individual corrections made
by the teacher, and the second classifies these cor-
rections into error types that the teacher would find
useful. A suitable corpus for developing this pro-
gram is the Cambridge Learner Corpus (CLC) (Yan-
nakoudakis et al, 2011). The CLC contains approxi-
mately 1200 essays with error corrections annotated
in XML within sentences. Furthermore, these cor-
rections are tagged with linguistically motivated er-
ror type codes.
To the best of our knowledge our proposed task
is unexplored in previous work. However, there is
a significant amount of related work in automated
grammatical error correction (Fitzgerald et al, 2009;
Gamon, 2011; West et al, 2011). The Helping
Our Own (HOO) shared task (Dale and Kilgarriff,
2010) also explores this issue, with Rozovskaya et
al. (2011) as the best performing system to date.
While often addressing the problem of error type
selection directly, previous work has dealt with the
more obviously useful task of end to end error detec-
tion and correction. As such, their classification sys-
tems are crippled by poor recall of errors as well as
the lack of information from the corrected sentence
and yield very low accuracies for error detection and
type selection, e.g. Gamon (2011).
Our task is fundamentally different as we assume
the presence of both the original and corrected text.
While the utility of such a system is not as obvi-
ous as full error correction, we note two possible
applications of our technique. The first, mentioned
357
above, is as an analytical tool for language teach-
ers. The second is as a complementary tool for au-
tomated error correction systems themselves. Just
as tools such as BLAST (Stymne, 2011) are useful
in the development of machine translation systems,
our system can produce accurate summaries of the
corrections made by automated systems even if the
systems themselves do not involve such fine grained
error type analysis.
In the following, we describe our experimental
methodology (Section 2) and then discuss the fea-
ture set we employ for classification (Section 3) and
its performance. Next, we outline our application
(Section 4), its heuristic correction detection strat-
egy and empirical evaluation. We finish by dis-
cussing the implications for real world systems (Sec-
tion 5) and avenues for improvement.
2 Methodology
Sentences in the CLC contain one or more error cor-
rections, each of which is labeled with one of 75
error types (Nicholls, 2003). Error types include
countability errors, verb tense errors, word order er-
rors, etc. and are often predicated on the part of
speech involved. For example, the category AG
(agreement) is augmented to form AGN (agreement
of a noun) to tag an error such as ?here are some
of my opinion?. For ease of analysis and due to
the high accuracy of state-of-the-art POS tagging,
in addition to the full 75 class problem we also
perform experiments using a compressed set of 15
classes. This compressed set removes the part of
speech components of the error types as shown in
Figure 1.
We create a dataset of corrections from the CLC
by extracting sentence pairs (x, y) where x is the
original (student?s) sentence and y is its corrected
form by the teacher. We create multiple instances
out of sentence pairs that contain multiple correc-
tions. For example, consider the sentence ?With this
letter I would ask you if you wuld change it?. This
consists of two errors: ?ask? should be replaced with
?like to ask? and ?wuld? is misspelled. These are
marked separately in the CLC, and imply the cor-
rected sentence ?With this letter I would like to ask
you if you would change it?. Here we extract two
instances consisting of ?With this letter I would ask
you if you would change it? and ?With this letter I
would like to ask if you wuld change it?, each paired
with the fully corrected sentence. As each correc-
tion in the CLC is tagged with an error type t, we
then form a dataset of triples (x, y, t). This yields
45080 such instances. We use these data in cross-
validation experiments with the feature based Max-
Ent classifier in the Mallet (McCallum, 2002) soft-
ware package.
3 Feature Set
We use the minimum unweighted edit distance path
between x and y as a source of features. The edit dis-
tance operations that compose the path are Delete,
Insert, Substitute, and Equal. To illustrate, the op-
erations we would get from the sentences above
would be (Insert, ?like?), (Insert, ?to?), (Substitute,
?wuld?, ?would?), and (Equal, w, w) for all other
words w.
Our feature set consists of three main categories
and a global category (See Figure 2). For each edit
distance operation other than Equal we use an indi-
cator feature, as well as word+operation indicators,
for example ?the word w was inserted? or ?the word
w1 was substituted with w2?. The POS Context fea-
tures encode the part of speech context of the edit,
recording the parts of speech immediately preced-
ing and following the edit in the corrected sentence.
For all POS based features we use only tags from the
corrected sentence y, as our tags are obtained auto-
matically.
For a substitution of w2 for w1 we use several
targeted features. Many of these are self explana-
tory and can be calculated easily without outside li-
braries. The In Dictionary? feature is indexed by
two binary values corresponding to the presence of
the words in the WordNet dictionary. For the Same
Stem? feature we use the stemmer provided in the
freely downloadable JWI (Java Wordnet Interface)
library. If the two words have the same stem then
we also trigger the Suffixes feature, which is in-
dexed by the two suffix strings after the stem has
been removed. For global features, we record the
total number of non-Equal edits as well as a feature
which fires if one sentence is a word-reordering of
the other.
358
Description (Code) Sample and Correction Total # % Accuracy
Unnecessary (U)
July is the period of time that suits me best
5237 94.0
July is the time that suits me best
Incorrect verb tense (TV)
She gave me autographs and talk really nicely.
2752 85.2
She gave me autographs and talked really nicely.
Countability error (C)
Please help them put away their stuffs.
273 65.2
Please help them put away their stuff.
Incorrect word order (W)
I would like to know what kind of clothes should I bring.
1410 76.0
I would like to know what kind of clothes I should bring.
Incorrect negative (X)
We recommend you not to go with your friends.
124 18.5
We recommend you don?t go with your friends.
Spelling error (S)
Our music lessons are speccial.
4429 90.0
Our music lessons are special.
Wrong form used (F)
In spite of think I did well, I had to reapply.
2480 82.0
In spite of thinking I did well, I had to reapply.
Agreement error (AG)
I would like to take some picture of beautiful scenery.
1743 77.9
I would like to take some pictures of beautiful scenery.
Replace (R)
The idea about going to Maine is common.
14290 94.6
The idea of going to Maine is common.
Missing (M)
Sometimes you surprised when you check the balance.
9470 97.6
Sometimes you are surprised when you check the balance.
Incorrect argument structure (AS)
How much do I have to bring the money?
191 19.4
How much money do I have to bring?
Wrong Derivation (D)
The arrive of every student is a new chance.
1643 58.6
The arrival of every student is a new chance.
Wrong inflection (I)
I enjoyded it a lot.
590 58.6
I enjoyed it a lot.
Inappropriate register (L)
The girls?d rather play table tennis or badminton.
135 23.0
The girls would rather play table tennis or badminton.
Idiomatic error (ID)
The level of life in the USA is similar to the UK.
313 15.7
The cost of living in the USA is similar to the UK.
Figure 1: Error types in the collapsed 15 class set.
3.1 Evaluation
We perform five-fold cross-validation and achieve
a classification accuracy of 88.9% for the 15 class
problem and 83.8% for the full 75 class problem.
The accuracies of the most common class base-
lines are 33.3% and 7.8% respectively. The most
common confusion in the 15 class case is between
D (Derivation), R (Replacement) and S (Spelling).
These are mainly due to context-sensitive spelling
corrections falling into the Replace category or noise
in the mark-up of derivation errors. For the 75 class
case the most common confusion is between agree-
ment of noun (AGN) and form of noun (FN). This is
unsurprising as we do not incorporate long distance
features which would encode agreement.
To check against over-fitting we performed an ex-
periment where we take away the strongly lexical-
ized features (such as ?word w is inserted?) and
observed a reduction from 88.9% to 82.4% for 15
class classification accuracy. The lack of a dramatic
reduction demonstrates the generalization power of
our feature templates.
4 An Educational Application
As mentioned earlier, we incorporate our classifier
in an educational software tool. The input to this
tool is a group of aligned sentence pairs from orig-
inal and teacher edited versions of a set of essays.
This tool has two components devoted to (1) isola-
tion of individual corrections in a sentence pair, and
(2) classification of these corrections. This software
could be easily integrated in real world curriculum
as it is natural for the teacher to produce corrected
versions of student essays without stopping to label
and analyze distribution of correction types.
We devise a family of heuristic strategies to
separate independent corrections from one another.
Heuristic hi allows at most i consecutive Equal edit
distance operations in a single correction. This im-
plies that hn+1 would tend to merge more non-
Equal edits than hn. We experimented with i ?
{0, 1, 2, 3, 4}. For comparison we also implemented
359
? Insert
? Insert
? Insert(w)
? POS Context
? Delete
? Delete
? Delete(w)
? POS Context
? Substitution
? Substitution
? Substitution(w1,w2)
? Character Edit Distance
? Common Prefix Length
? In Dictionary?
? Previous Word
? POS of Substitution
? Same Stem?
? Suffixes
? Global
? Same Words?
? Number Of Edits
Figure 2: List of features used in our classifier.
a heuristic h? that treats every non-Equal edit as
an individual correction. This is different than h0,
which would merge edits that do not have an in-
tervening Equal operation. F-scores (using 5 fold
cross-validation) obtained by different heuristics are
reported in Figure 3 for the 15 and 75 class prob-
lems. For these F-scores we attempt to predict both
the boundaries and the labels of the corrections. The
unlabeled F-score (shown as a line) evaluates the
heuristic itself and provides an upper bound for the
labeled F-score of the overall application. We see
that the best upper bound and F-scores are achieved
with heuristic h0 which merges consecutive non-
Equal edits.
5 Future Work
There are several directions in which this work could
be extended. The most obvious is to replace the
correction detection heuristic with a more robust al-
gorithm. Our log-linear classifier is perhaps better
suited for this task than other discriminative clas-
sifiers as it can be extended in a larger framework
which maximizes the joint probability of all correc-
tions. Our work shows that h0 will provide a strong
baseline for such experiments.

	

     









	
Proceedings of NAACL-HLT 2013, pages 85?94,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Extracting the Native Language Signal
for Second Language Acquisition
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
We develop a method for effective extraction
of linguistic patterns that are differentially ex-
pressed based on the native language of the
author. This method uses multiple corpora
to allow for the removal of data set specific
patterns, and addresses both feature relevancy
and redundancy. We evaluate different rel-
evancy ranking metrics and show that com-
mon measures of relevancy can be inappro-
priate for data with many rare features. Our
feature set is a broad class of syntactic pat-
terns, and to better capture the signal we ex-
tend the Bayesian Tree Substitution Grammar
induction algorithm to a supervised mixture of
latent grammars. We show that this extension
can be used to extract a larger set of relevant
features.
1 Introduction
Native Language Identification (NLI) is a classifi-
cation task in which a statistical signal is exploited
to determine an author?s native language (L1) from
their writing in a second language (L2). This aca-
demic exercise is often motivated not only by fraud
detection or authorship attribution for which L1 can
be an informative feature, but also by its potential to
assist in Second Language Acquisition (SLA).
Our work focuses on the latter application and on
the observation that the actual ability to automati-
cally determine L1 from text is of limited utility in
the SLA domain, where the native language of a stu-
dent is either known or easily solicited. Instead, the
likely role of NLP in the context of SLA is to pro-
vide a set of linguistic patterns that students with
certain L1 backgrounds use with a markedly unusual
frequency. Experiments have shown that such L1
specific information can be incorporated into lesson
plans that improve student performance (Laufer and
Girsai, 2008; Horst et al 2008).
This is essentially a feature selection task with the
additional caveat that features should be individually
discriminative between native languages in order to
facilitate the construction of focused educational ex-
cersizes. With this goal, we consider metrics for
data set dependence, relevancy, and redundancy. We
show that measures of relevancy based on mutual in-
formation can be inappropriate in problems such as
ours where rare features are important.
While the majority of the methods that we con-
sider generalize to any of the various feature sets
employed in NLI, we focus on the use of Tree Sub-
stitution Grammar rules as features. Obtaining a
compact feature set is possible with the well known
Bayesian grammar induction algorithm (Cohn and
Blunsom, 2010), but its rich get richer dynamics can
make it difficult to find rare features. We extend the
induction model to a supervised mixture of latent
grammars and show how it can be used to incorpo-
rate linguistic knowledge and extract discriminative
features more effectively.
The end result of this technique is a filtered list of
patterns along with their usage statistics. This pro-
vides an enhanced resource for SLA research such
as Jarvis and Crossley (2012) which tackles the man-
ual connection of highly discriminative features with
plausible linguistic transfer explanations. We output
a compact list of language patterns that are empiri-
cally associated with native language labels, avoid-
85
ing redundancy and artifacts from the corpus cre-
ation process. We release this list for use by the
linguistics and SLA research communities, and plan
to expand it with upcoming releases of L1 labeled
corpora1.
2 Related Work
Our work is closely related to the recent surge of re-
search in NLI. Beginning with Koppel et al(2005),
several papers have proposed different feature sets
to be used as predictors of L1 (Tsur and Rappa-
port, 2007; Wong and Dras, 2011a; Swanson and
Charniak, 2012). However, due to the ubiquitous
use of random subsamples, different data prepara-
tion methods, and severe topic and annotation biases
of the data set employed, there is little consensus on
which feature sets are ideal or sufficient, or if any
reported accuracies reflect some generalizable truth
of the problem?s difficulty. To combat the bias of
a single data set, a new strain of work has emerged
in which train and test documents come from dif-
ferent corpora (Brooke and Hirst, 2012; Tetreault et
al, 2012; Bykh and Meurers, 2012). We follow this
cross corpus approach, as it is crucial to any claims
of feature relevance.
Feature selection itself is a well studied problem,
and the most thorough systems address both rele-
vancy and redundancy. While some work tackles
these problems by optimizing a metric over both si-
multaneously (Peng et al 2005), we decouple the
notions of relevancy and redundancy to allow ad-hoc
metrics for either, similar to the method of Yu and
Liu (2004). The measurement of feature relevancy
in NLI has to this point been handled primarily with
Information Gain, and elimination of feature redun-
dancy has not been considered.
Tree Substitution Grammars have recently been
successfully applied in several domains using the
induction algorithm presented by Cohn and Blun-
som (2010). Our hierarchical treatment builds on
this work by incorporating supervised mixtures over
latent grammars into this induction process. Latent
mixture techniques for NLI have been explored with
other feature types (Wong and Dras, 2011b; Wong
and Dras, 2012), but have not previously led to mea-
surable empirical gains.
1bllip.cs.brown.edu/download/nli corpus.pdf
3 Corpus Description
We first make explicit our experimental setup in or-
der to provide context for the discussion to follow.
We perform analysis of English text from Chinese,
German, Spanish, and Japanese L1 backgrounds
drawn from four corpora. The first three consist of
responses to essay prompts in educational settings,
while the fourth is submitted by users in an internet
forum.
The first corpus is the International Corpus of
Learner English (ICLE) (Granger et al 2002), a
mainstay in NLI that has been shown to exhibit a
large topic bias due to correlations between L1 and
the essay prompts used (Brooke and Hirst, 2011).
The second is the International Corpus of Crosslin-
guistic Interlanguage (ICCI) (Tono et al 2012),
which is annotated with sentence boundaries and has
yet to be used in NLI. The third is the public sample
of the Cambridge International Corpus (FCE), and
consists of short prompted responses. One quirk of
the FCE data is that several responses are written in
the form of letters, leading to skewed distributions
of the specialized syntax involved with use of the
second person. The fourth is the Lang8 data set in-
troduced by Brooke and Hirst (2011). This data set
is free of format, with no prompts or constraints on
writing aids. The samples are often very short and
are qualitatively the most noisy of the four data sets.
One distinctive experimental decision is to treat
each sentence as an individual datum. As document
length can vary dramatically, especially across cor-
pora, this gives increased regularity to the number
of features per data item. More importantly, this
creates a rough correspondence between feature co-
occurrence and the expression of the same under-
lying linguistic phenomenon, which is desirable for
automatic redundancy metrics.
We automatically detect sentence boundaries
when they are not provided, and parse all corpora
with the 6-split Berkeley Parser. As in previous NLI
work, we then replace all word tokens that do not oc-
cur in a list of 614 common words with an unknown
word symbol, UNK.
While these are standard data preprocessing steps,
from our experience with this problem we propose
additional practical considerations. First, we filter
the parsed corpora, retaining only sentences that are
86
parsed to a Clause Level2 tag. This is primarily due
to the fact that automatic sentence boundary detec-
tors must be used on the ICLE, Lang8, and FCE data
sets, and false positives lead to sentence fragments
that are parsed as NP, VP, FRAG, etc. The wild inter-
net text found in the Lang8 data set alo yields many
non-Clause Level parses from non-English text or
emotive punctuation. Sentence detection false neg-
atives, on the other hand, lead to run-on sentences,
and so we additionally remove sentences with more
than 40 words.
We also impose a simple preprocessing step for
better treatment of proper nouns. Due to the geo-
graphic distribution of languages, the proper nouns
used in a writer?s text naturally present a strong L1
signal. The obvious remedy is to replace all proper
nouns with UNK, but this is unfortunately insuffi-
cient as the structure of the proper noun itself can
be a covert signal of these geographical trends. To
fix this, we also remove all proper noun left sisters
of proper nouns. We choose to retain the rightmost
sister node in order to preserve the plurality of the
noun phrase, as the rightmost noun is most likely
the lexical head.
From these parsed, UNKed, and filtered corpora
we draw 2500 sentences from each L1 background
at random, for a total of 10000 sentences per corpus.
The exception is the FCE corpus, from which we
draw 1500 sentences per L1 due to its small size.
4 Tree Substitution Grammars
A Tree Substitution Grammar (TSG) is a model
of parse tree derivations that begins with a sin-
gle ROOT nonterminal node and iteratively rewrites
nonterminal leaves until none remain. A TSG
rewrite rule is a tree of any depth, as illustrated in
Figure 1, and can be used as a binary feature of a
parsed sentence that is triggered if the rule appears
in any derivation of that sentence.
Related NLI work compares a plethora of sug-
gested feature sets, ranging from character n-grams
to latent topic activations to labeled dependency
arcs, but TSG rules are best able to represent com-
plex lexical and syntactic behavior in a homoge-
neous feature type. This property is summed up
nicely by the desire for features that capture rather
2S, SINV, SQ, SBAR, or SBARQ
ROOT
S
NP VP
VBZ
loves
NP
NP
DT
the
NN
NN
man
NN
woman
Figure 1: A Tree Substitution Grammar capable of de-
scribing the feelings of people of all sexual orientations.
than cover linguistic phenomena (Johnson, 2012);
while features such as character n-grams, POS tag
sequences, and CFG rules may provide a usable L1
signal, each feature is likely covering some compo-
nent of a pattern instead of capturing it in full. TSG
rules, on the other hand, offer remarkable flexibil-
ity in the patterns that they can represent, potentially
capturing any contiguous parse tree structure.
As it is intractable to rank and filter the entire set
of possible TSG rules given a corpus, we start with
the large subset produced by Bayesian grammar in-
duction. The most widely used algorithm for TSG
induction uses a Dirichlet Process to choose a subset
of frequently reoccurring rules by repeatedly sam-
pling derivations for a corpus of parse trees (Cohn
and Blunsom, 2010). The rich get richer dynamic of
the DP leads to the use of a compact set of rules
that is an effective feature set for NLI (Swanson
and Charniak, 2012). However, this same property
makes rare rules harder to find.
To address this weakness, we define a general
model for TSG induction in labeled documents that
combines a Hierarchical Dirichlet Process (Teh et al
2005), with supervised labels in a manner similar to
upstream supervised LDA (Mimno and McCallum,
2008). In the context of our work the document label
? indicates both its authors native language L and
data set D. Each ? is associated with an observed
Dirichlet prior ??, and a hidden multinomial ?? over
grammars is drawn from this prior. The traditional
grammatical model of nonterminal expansion is aug-
mented such that to rewrite a symbol we first choose
a grammar from the document?s ?? and then choose
a rule from that grammar.
For those unfamiliar with these models, the basic
idea is to jointly estimate a mixture distribution over
grammars for each ?, as well as the parameters of
these grammars. The HDP is necessary as the size
87
of each of these grammars is essentially infinite. We
can express the generative model formally by defin-
ing the probability of a rule r expanding a symbol s
in a sentence labeled ? as
?? ? Dir(??)
zi? ?Mult(??)
Hs ? DP (?, P0(?|s))
Gks ? DP (?s, Hs)
ri?s ? Gzi?s
This is closely related to the application of the
Hierarchical Pitman Yor Process used in (Blunsom
and Cohn, 2010) and (Shindo et al 2012), which
interpolates between multiple coarse and fine map-
pings of the data items being clustered to deal with
sparse data. While the underlying Chinese Restau-
rant Process sampling algorithm is quite similar, our
approach differs in that it models several different
distributions with the same support that share a com-
mon prior.
By careful choice of the number of grammars K,
the Dirichlet priors ?, and the backoff concentration
parameter ?, a variety of interesting models can eas-
ily be defined, as demonstrated in our experiments.
5 Feature Selection
5.1 Dataset Independence
The first step in our L1 signal extraction pipeline
controls for patterns that occur too frequently in cer-
tain combinations of native language and data set.
Such patterns arise primarily from the reuse of es-
say prompts in the creation of certain corpora, and
we construct a hard filter to exclude features of this
type.
A simple first choice would be to rank the rules
in order of dependence on the corpus, as we expect
an irregularly represented topic to be confined to a
single data set. However, this misses the subtle but
important point that corpora have different qualities
such as register and author proficiency. Instead we
treat the set of sentences containing an arbitrary fea-
ture X as a set of observations of a pair of categor-
ical random variables L and D, representing native
language and data set respectively.
To see why this treatment is superior, consider the
outcomes for the two hypothetical features shown
L1 L2
D1 1000 500
D2 100 50
L1 L2
D1 1000 500
D2 750 750
Figure 2: Two hypothetical feature profiles that illustrate
the problems with filtering only on data set independence,
which prefers the right profile over the left. Our method
has the opposite preference.
in Figure 2. The left table has a high data set de-
pendence but exhibits a clean twofold preference for
L1 in both data sets, making it a desirable feature to
retain. Conversely, the right table shows a feature
where the distribution is uniform over data sets, but
has language preference in only one. This is a sign
of either a large variance in usage or some data set
specific tendency, and in either case we can not make
confident claims as to this feature?s association with
any native language.
The L-D dependence can be measured with Pear-
son?s ?2 test, although the specifics of its use as
a filter deserve some discussion. As we eliminate
the features for which the null hypothesis of inde-
pendence is rejected, our noisy data will cause us
to overzealously reject. In order to prevent the un-
neccesary removal of interesting patterns, we use a
very small p value as a cutoff point for rejection. In
all of our experiments the ?2 value corresponding to
p < .001 is in the twenties; we use ?2 > 100 as our
criteria for rejection.
Another possible source of error is the sparsity of
some features in our data. To avoid making pre-
dictions of rules for which we have not observed
a sufficient number of examples, we automatically
exclude any rule with a count less than five for any
L-D combination ?. This also satisfies the common
requirements for validity of the ?2 test that require
a minimum number of 5 expected counts for every
outcome.
5.2 Relevancy
We next rank the features in terms of their ability to
discriminate between L1 labels. We consider three
relevancy ranking metrics: Information Gain (IG),
Symmetric Uncertainty (SU), and ?2 statistic.
88
IG SU ?2
r .84 .72 .15
Figure 3: Sample Pearson correlation coefficients be-
tween different ranking functions and feature frequency
over a large set of TSG features.
IG(L,Xi) = H(L)?H(L|Xi)
SU(L,Xi) = 2
IG(L,Xi)
H(L) +H(Xi)
?2(Xi) =
?
m
(nim ?
Ni
M )
2
Ni
M
We define L as the Multinomial distributed L1 la-
bel taking values in {1, ...,M} andXi as a Bernoulli
distributed indicator of the presence or absence of
the ith feature, which we represent with the events
X+i and X
?
i respectively. We use the Maximum
Likelihood estimates of these distributions from the
training data to compute the necessary entropies for
IG and SU. For the ?2 metric we use nim, the count
of sentences with L1 labelm that contain featureXi,
and their sum over classes Ni.
While SU is often preferred over IG in feature se-
lection for several reasons, their main difference in
the context of selection of binary features is the addi-
tion of H(Xi) in the denominator, leading to higher
values for rare features under SU. This helps to
counteract a subtle preference for common features
that these metrics can exhibit in data such as ours, as
shown in Figure 3. The source of this preference is
the overwhelming contribution of p(X?i )H(L|X
?
i )
in IG(L,Xi) for rare features, which will be essen-
tially the maximum value of log(M). In most clas-
sification problems a frequent feature bias is a desir-
able trait, as a rare feature is naturally less likely to
appear and contribute to decision making.
We note that binary features in sentences are
sparsely observed, as the opportunity for use of the
majority of patterns will not exist in any given sen-
tence. This leads to a large number of rare features
that are nevertheless indicative of their author?s L1.
The ?2 statistic we employ is better suited to retain
such features as it only deals with counts of sen-
tences containing Xi.
The ranking behavior of these metrics is high-
lighted in Figure 4. We expect that features with
profiles like Xa and Xb will be more useful than
those like Xd, and only ?2 ranks these features ac-
cordingly. Another view of the difference between
the metrics is taken in Figure 5. As shown in the
left plot, IG and SU are nearly identical for the
most highly ranked features and significantly differ-
ent from ?2.
L1 L2 L3 L4 IG SU ?2
Xa 20 5 5 5 .0008 .0012 19.29
Xb 40 20 20 20 .0005 .0008 12.0
Xc 2000 500 500 500 .0178 .0217 385.7
Xd 1700 1800 1700 1800 .0010 .0010 5.71
Figure 4: Four hypothetical features in a 4 label clas-
sification problem, with the number of training items
from each class using the feature listed in the first four
columns. The top three features under each ranking are
shown in bold.
 0
 10
 20
 30
 40
 50
 0  10  20  30  40  50
# 
of
 sh
ar
ed
 fe
at
ur
es
Top n features
X-IG
X-SU
SU-IG
 0
 50
 100
 150
 200
 250
 300
 0  50  100  150  200  250  300
Top n features
X-IG
X-SU
SU-IG
Figure 5: For all pairs of relevancy metrics, we show the
number of features that appear in the top n of both. The
result for low n is highlighted in the left plot, showing a
high similarity between SU and IG.
5.3 Redundancy
The second component of thorough feature selection
is the removal of redundant features. From an ex-
perimental point of view, it is inaccurate to compare
feature selection systems under evaluation of the top
n features or the number of features with ranking
statistic at or beyond some threshold if redundancy
has not been taken into account. Furthermore, as
our stated goal is a list of discriminative patterns,
multiple representations of the same pattern clearly
89
degrade the quality of our output. This is especially
necessary when using TSG rules as features, as it is
possible to define many slightly different rules that
essentially represent the same linguistic act.
Redundancy detection must be able to both deter-
mine that a set of features are redundant and also
select the feature to retain from such a set. We use
a greedy method that allows us to investigate differ-
ent relevancy metrics for selection of the representa-
tive feature for a redundant set (Yu and Liu, 2004).
The algorithm begins with a list S containing the
full list of features, sorted by an arbitrary metric of
relevancy. While S is not empty, the most relevant
feature X? in S is selected for retention, and all fea-
tures Xi are removed from S if R(X?, Xi) > ? for
some redundancy metric R and some threshold ?.
We consider two probabilistic metrics for redun-
dancy detection, the first being SU, as defined in
the previous section. We contrast this metric with
Normalized Pointwise Mutual Information (NPMI)
which uses only the events A = X+a and B = X
+
b
and has a range of [-1,1].
NPMI(Xa, Xb) =
log(P (A|B))? log(P (A))
? log(P (A,B))
Another option that we explore is the structural
redundancy between TSG rules themselves. We de-
fine a 0-1 redundancy metric such that R(Xa, Xb) is
one if there exists a fragment that contains both Xa
and Xb with a total number of CFG rules less than
the sum of the number of CFG rules in Xa and Xb.
The latter constraint ensures that Xa and Xb overlap
in the containing fragment. Note that this is not the
same as a nonempty set intersection of CFG rules,
as can be seen in Figure 6.
S
NP
NN
VP
S
NP
PRP
VP
S
NP VP
VBZ
Figure 6: Three similar fragments that highlight the be-
havior of the structural redundancy metric; the first two
fragments are not considered redundant, while the third
is made redundant by either of the others.
6 Experiments
6.1 Relevancy Metrics
The traditional evaluation criterion for a feature se-
lection system such as ours is classification accuracy
or expected risk. However, as our desired output is
not a set of features that capture a decision bound-
ary as an ensemble, a per feature risk evaluation bet-
ter quantifies the performance of a system for our
purposes. We plot average risk against number of
predicted features to view the rate of quality degra-
dation under a relevancy measure to give a picture
of a each metric?s utility.
The per feature risk for a feature X is an eval-
uation of the ML estimate of PX(L) = P (L|X+)
from the training data on TX , the test sentences that
contain the feature X . The decision to evaluate only
sentences in which the feature occurs removes an
implicit bias towards more common features.
We calculate the expected risk R(X) using a 0-1
loss function, averaging over TX .
R(X) =
1
|TX |
?
t?TX
PX(L 6= L
?
t )
where L?t is the gold standard L1 label of test item
t. This metric has two important properties. First,
given any true distribution over class labels in TX ,
the best possible PX(L) is the one that matches
these proportions exactly, ensuring that preferred
features make generalizable predictions. Second, it
assigns less risk to rules with lower entropy, as long
as their predictions remain generalizable. This cor-
responds to features that find larger differences in
usage frequency across L1 labels.
The alternative metric of per feature classifica-
tion accuracy creates a one to one mapping between
features and native languages. This unnecessarily
penalizes features that are associated with multiple
native languages, as well as features that are selec-
tively dispreferred by certain L1 speakers. Also, we
wish to correctly quantify the distribution of a fea-
ture over all native languages, which goes beyond
correct prediction of the most probable.
Using cross validation with each corpus as a fold,
we plot the average R(X) for the best n features
against n for each relevancy metric in Figure 7. This
clearly shows that for highly ranked features ?2 is
90
 0.69
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0  20  40  60  80  100  120  140  160  180  200
A
ve
ra
ge
 E
xp
ec
te
d 
Lo
ss
Top n features
X2
IG
SU
Figure 7: Per-feature Average Expected Loss plotted
against top N features using ?2, IG, and SU as a rele-
vancy metric
able to best single out the type of features we de-
sire. Another point to be taken from the plot is
that it is that the top ten features under SU are
remarkably inferior. Inspection of these rules re-
veals that they are precisely the type of overly fre-
quent but only slightly discriminative features that
we predicted would corrupt feature selection using
IG based measures.
6.2 Redundancy Metrics
We evaluate the redundancy metrics by using the top
n features retained by redundancy filtering for en-
semble classification. Under this evaluation, if re-
dundancy is not being effectively eliminated perfor-
mance should increase more slowly with n as the
set of test items that can be correctly classified re-
mains relatively constant. Additionally, if the metric
is overzealous in its elimination of redundancy, use-
ful patterns will be eliminated leading to diminished
increase in performance. Figure 8 shows the tradeoff
between Expected Loss on the test set and the num-
ber of features used with SU, NPMI, and the overlap
based structural redundancy metric described above.
We performed a coarse grid search to find the opti-
mal values of ? for SU and NPMI.
Both the structural overlap hueristic and SU per-
form similarly, and outperform NPMI. Analysis re-
veals that NPMI seems to overstate the similarity of
large fragments with their small subcomponents. We
choose to proceed with SU, as it is not only faster in
our implementation but also can generalize to fea-
ture types beyond TSG rules.
 0.66
 0.67
 0.68
 0.69
 0.7
 0.71
 0.72
 0.73
 0  50  100  150  200  250  300
Ex
pe
ct
ed
 L
os
s
Top n features
overlap
SU
NPMI
Figure 8: The effects of redundancy filtering on classi-
fication performance using different redundancy metrics.
The cutoff values (?) used for SU and NPMI are .2 and .7
respectively.
6.3 TSG Induction
We demonstrate the flexibility and effectiveness of
our general model of mixtures of TSGs for labeled
data by example. The tunable parameters are the
number of grammars K, the Dirichlet priors ?? over
grammar distributions for each label ?, and the con-
centration parameter ? of the smoothing DP.
For a first baseline we set the number of grammars
K = 1, making the Dirichlet priors ? irrelevant.
With a large ? = 1020, we essentially recover the
basic block sampling algorithm of Cohn and Blun-
som (2010). We refer to this model as M1. Our
second baseline model, M2, sets K to the number of
native language labels, and sets the ? variables such
that each ? is mapped to a single grammar by its L1
label, creating a naive Bayes model. For M2 and
the subsequent models we use ? = 1000 to allow
moderate smoothing.
We also construct a model (M3) in which we set
K = 9 and ?? is such that three grammars are likely
for any single ?; one shared by all ? with the same
L1 label, one shared by all ? with the same corpus
label, and one shared by all ?. We compare this with
another K = 9 model (M4) where the ? are set to
be uniform across all 9 grammars.
We evaluate these systems on the percent of their
resulting grammar that rejects the hypothesis of lan-
guage independence using a ?2 test. Slight adjust-
ments were made to ? for these models to bring
their output grammar size into the range of approxi-
mately 12000 rules. We average our results for each
model over single states drawn from five indepen-
91
p < .1 p < .05 p < .01 p < .001
M1 56.5(3.1) 54.5(3.0) 49.8(2.7) 45.1(2.5)
M2 55.3(3.7) 53.7(3.6) 49.1(3.3) 44.7(3.0)
M3 59.0(4.1) 57.2(4.1) 52.4(3.6) 48.4(3.3)
M4 58.9(3.8) 57.0(3.7) 51.9(3.4) 47.2(3.1)
Figure 9: The percentage of rules from each model that
reject L1 independence at varying levels of statistical sig-
nificance. The first number is with respect to the number
rules that pass the L1/corpus independence and redun-
dancy tests, and the second is in proportion to the full list
returned by grammar induction.
dent Markov chains.
Our results in Figure 9 show that using a mixture
of grammars allows the induction algorithm to find
more patterns that fit arbitrary criteria for language
dependence. The intuition supporting this is that in
simpler models a given grammar must represent a
larger amount of data that is better represented with
more vague, general purpose rules. Dividing the re-
sponsibility among several grammars lets rare pat-
terns form clusters more easily. The incorporation of
informed structure in M3 further improves the per-
formance of this latent mixture technique.
7 Discussion
Using these methods, we produce a list of L1 as-
sociated TSG rules that we release for public use.
We perform grammar induction using model M3,
apply our data dependence and redundancy filters,
rank for relevancy using ?2 and filter at the level of
p < .1 statistical significance for relevancy. Each
entry consists of a TSG rule and its matrix of counts
with each ?. We provide the total for each L1 la-
bel, which shows the overall prediction of the pro-
portional use of that item. We also provide the ?2
statistics for L1 dependence and the dependence of
L1 and corpus.
It is speculative to assign causes to the discrimi-
native rules we report, and we leave quantification
of such statements to future work. However, the
strength of the signal, as evidenced by actual counts
in data, and the high level interpretation that can be
easily assigned to the TSG rules is promising. As
understanding the features requires basic knowledge
of Treebank symbols, we provide our interpretations
for some of the more interesting rules and summa-
rize their L1 distributions. Note that by describing a
rule as being preferred by a certain set of L1 labels,
our claim is relative to the other labels only; the true
cause could also be a dispreference in the comple-
ment of this set.
One interesting comparison made easy by our
method is the identification of similar structures that
have complementary L1 usage. An example is the
use of a prepositional phrase just before the first
noun phrase in a sentence, which is preferred in Ger-
man and Spanish, especially in the former. However,
German speakers disprefer a prepositional phrase
followed by a comma at the beginning of the sen-
tence, and Chinese speakers use this pattern more
frequently than the other L1s. Another contrastable
pair is the use of the word ?because? with upper or
lower case, signifying sentence initial or medial use.
The former is preferred in Chinese and Japanese
text, while the latter is preferred in German and even
more so in Spanish L1 data.
As these examples suggest, the data shows a
strong division of preference between European
and Asian languages, but many patterns exist that
are uniquely preferred in single languages as well.
Japanese speakers are seen to frequently use a per-
sonal pronoun as the subject of the sentence, while
Spanish speakers use the phrase ?the X of Y?, the
verb ?go?, and the determiner ?this? with markedly
higher frequency. Germans tend to begin sentences
with adverbs, and various modal verb constructions
are popular with Chinese speakers. We suspect these
patterns to be evidence of preference in the speci-
fied language, rather than dispreference in the other
three.
Our strategy in regard to the hard filters for L1-
corpus dependence and redundancy has been to pre-
fer recall to precision, as false positives can be easily
ignored through subsequent inspection of the data
we supply. This makes the list suitable for human
qualitative analysis, but further work is required for
its use in downstream automatic systems.
8 Conclusion
This work contributes to the goal of leveraging NLI
data in SLA applications. We provide evidence for
92
our hypothesis that relevancy metrics based on mu-
tual information are ill-suited for this task, and rec-
ommend the use of the ?2 statistic for rejecting the
hypothesis of language independence. Explicit con-
trols for dependence between L1 and corpus are
proposed, and redundancy between features are ad-
dressed as well. We argue for the use of TSG rules as
features, and develop an induction algorithm that is
a supervised mixture of hierarchical grammars. This
generalizable formalism is used to capture linguistic
assumptions about the data and increase the amount
of relevant features extracted at several thresholds.
This project motivates continued incorporation of
more data and induction of TSGs over these larger
data sets. This will improve the quality and scope of
the resulting list of discriminative syntax, allowing
broader use in linguistics and SLA research. The
prospect of high precision and recall in the extrac-
tion of such patterns suggests several interesting av-
enues for future work, such as determination of the
actual language transfer phenomena evidenced by an
arbitrary count profile. To achieve the goal of auto-
matic detection of plausible transfer the native lan-
guages themselves must be considered, as well as a
way to distinguish between preference and dispref-
erence based on usage statistics. Another exciting
application of such a refined list of patterns is the
automatic integration of its features in L1 targeted
SLA software.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. Empirical Methods in Natural Lan-
guage Processing.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. Conference of
Learner Corpus Research.
Julian Brooke and Graeme Hirst. 2012. Measuring In-
terlanguage: Native Language Identification with L1-
influence Metrics. LREC
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. COLING.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification Using Recurring N-grams - Inves-
tigating Abstraction and Domain Dependence. COL-
ING.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing Compact but Accurate Tree-
Substitution Grammars. In Proceedings NAACL.
Trevor Cohn, and Phil Blunsom. 2010. Blocked infer-
ence in Bayesian tree substitution grammars. Associa-
tion for Computational Linguistics.
Gilquin, Gae?tanelle and Granger, Sylviane. 2011. From
EFL to ESL: Evidence from the International Corpus
of Learner English. Exploring Second-Language Va-
rieties of English and Learner Englishes: Bridging a
Paradigm Gap (Book Chapter).
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al chapter 8..
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus of Learner English, (ICLE).
Horst M., White J., Bell P. 2010. First and second lan-
guage knowledge in the language classroom. Interna-
tional Journal of Bilingualism.
Scott Jarvis and Scott Crossley 2012. Approaching Lan-
guage Transfer through Text Classification.
Mark Johnson 2011. How relevant is linguistics to com-
putational linguistics?. Linguistic Issues in Language
Technology.
Ekaterina Kochmar. 2011. Identification of a writer?s
native language by error analysis. Master?s Thesis.
Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir.
2005. Determining an author?s native language by
mining a text for errors. Proceedings of the eleventh
ACM SIGKDD international conference on Knowl-
edge discovery in data mining.
Laufer, B and Girsai, N. 2008. Form-focused Instruction
in Second Language Vocabulary Learning: A Case for
Contrastive Analysis and Translation. Applied Lin-
guistics.
David Mimno and Andrew McCallum. 2008. Topic
Models Conditioned on Arbitrary Features with
Dirichlet-multinomial Regression. UAI.
Hanchuan Peng and Fuhui Long and Chris Ding. 2005.
Feature selection based on mutual information cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. Association for Compu-
tational Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. Association for Com-
putational Linguistics.
Tono, Y., Kawaguchi, Y. & Minegishi, M. (eds.) . 2012.
Developmental and Cross-linguistic Perspectives in
Learner Corpus Research..
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language on
the choice of written second language words. CACLA.
93
Shindo, Hiroyuki and Miyao, Yusuke and Fujino, Aki-
nori and Nagata, Masaaki 2012. Bayesian Symbol-
Refined Tree Substitution Grammars for Syntactic
Parsing. Association for Computational Linguistics.
Ben Swanson and Eugene Charniak. 2012. Native
Language Detection with Tree Substitution Grammars.
Association for Computational Linguistics.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2005. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, Beata
Beigman-Klebanov and Martin Chodorow. 2012. Na-
tive Tongues, Lost and Found: Resources and Em-
pirical Evaluations in Native Language Identification.
COLING.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing Parse Structures for Native Language Identifica-
tion. Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing.
Sze-Meng Jojo Wong and Mark Dras. 2011. Topic Mod-
eling for Native Language Identification. Proceedings
of the Australasian Language Technology Association
Workshop.
Sze-Meng Jojo Wong, Mark Dras, Mark Johnson. 2012.
Exploring Adaptor Grammars for Native Language
Identification. EMNLP-CoNLL.
Lei Yu and Huan Liu. 2004. Efficient Feature Selection
via Analysis of Relevance and Redundancy. Journal
of Machine Learning Research.
94
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 302?310,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Context Free TAG Variant
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Elif Yamangil
Harvard University
Cambridge, MA
elif@eecs.harvard.edu
Stuart Shieber
Harvard University
Cambridge, MA
shieber@eecs.harvard.edu
Abstract
We propose a new variant of Tree-
Adjoining Grammar that allows adjunc-
tion of full wrapping trees but still bears
only context-free expressivity. We provide
a transformation to context-free form, and
a further reduction in probabilistic model
size through factorization and pooling of
parameters. This collapsed context-free
form is used to implement efficient gram-
mar estimation and parsing algorithms.
We perform parsing experiments the Penn
Treebank and draw comparisons to Tree-
Substitution Grammars and between dif-
ferent variations in probabilistic model de-
sign. Examination of the most probable
derivations reveals examples of the lin-
guistically relevant structure that our vari-
ant makes possible.
1 Introduction
While it is widely accepted that natural language
is not context-free, practical limitations of ex-
isting algorithms motivate Context-Free Gram-
mars (CFGs) as a good balance between model-
ing power and asymptotic performance (Charniak,
1996). In constituent-based parsing work, the pre-
vailing technique to combat this divide between
efficient models and real world data has been to
selectively strengthen the dependencies in a CFG
by increasing the grammar size through methods
such as symbol refinement (Petrov et al, 2006).
Another approach is to employ a more power-
ful grammatical formalism and devise constraints
and transformations that allow use of essential ef-
ficient algorithms such as the Inside-Outside al-
gorithm (Lari and Young, 1990) and CYK pars-
ing. Tree-Adjoining Grammar (TAG) is a natural
starting point for such methods as it is the canoni-
cal member of the mildly context-sensitive family,
falling just above CFGs in the hierarchy of for-
mal grammars. TAG has a crucial advantage over
CFGs in its ability to represent long distance in-
teractions in the face of the interposing variations
that commonly manifest in natural language (Joshi
and Schabes, 1997). Consider, for example, the
sentences
These pretzels are making me thirsty.
These pretzels are not making me thirsty.
These pretzels that I ate are making me thirsty.
Using a context-free language model with
proper phrase bracketing, the connection between
the words pretzels and thirsty must be recorded
with three separate patterns, which can lead to
poor generalizability and unreliable sparse fre-
quency estimates in probabilistic models. While
these problems can be overcome to some extent
with large amounts of data, redundant representa-
tion of patterns is particularly undesirable for sys-
tems that seek to extract coherent and concise in-
formation from text.
TAG allows a linguistically motivated treatment
of the example sentences above by generating the
last two sentences through modification of the
first, applying operations corresponding to nega-
tion and the use of a subordinate clause. Un-
fortunately, the added expressive power of TAG
comes with O(n6) time complexity for essential
algorithms on sentences of length n, as opposed to
O(n3) for the CFG (Schabes, 1990). This makes
TAG infeasible to analyze real world data in a rea-
sonable time frame.
In this paper, we define OSTAG, a new way to
constrain TAG in a conceptually simple way so
302
SNP VP NP
NP
DT
the
NN
lack
NP
NNS
computers
VP
VBP
do
RB
not
VP
NP
NP PP
NP
PRP
I
PP
IN
of
PRP
them
VP
VB
fear
Figure 1: A simple Tree-Substitution Grammar using S as its start symbol. This grammar derives the
sentences from a quote of Isaac Asimov?s - ?I do not fear computers. I fear the lack of them.?
that it can be reduced to a CFG, allowing the use of
traditional cubic-time algorithms. The reduction is
reversible, so that the original TAG derivation can
be recovered exactly from the CFG parse. We pro-
vide this reduction in detail below and highlight
the compression afforded by this TAG variant on
synthetic formal languages.
We evaluate OSTAG on the familiar task of
parsing the Penn Treebank. Using an automati-
cally induced Tree-Substitution Grammar (TSG),
we heuristically extract an OSTAG and estimate
its parameters from data using models with var-
ious reduced probabilistic models of adjunction.
We contrast these models and investigate the use
of adjunction in the most probable derivations of
the test corpus, demonstating the superior model-
ing performance of OSTAG over TSG.
2 TAG and Variants
Here we provide a short history of the relevant
work in related grammar formalisms, leading up
to a definition of OSTAG. We start with context-
free grammars, the components of which are
?N,T,R, S?, where N and T are the sets of non-
terminal and terminal symbols respectively, and S
is a distinguished nonterminal, the start symbol.
The rules R can be thought of as elementary trees
of depth 1, which are combined by substituting a
derived tree rooted at a nonterminalX at some leaf
node in an elementary tree with a frontier node
labeled with that same nonterminal. The derived
trees rooted at the start symbol S are taken to be
the trees generated by the grammar.
2.1 Tree-Substitution Grammar
By generalizing CFG to allow elementary trees in
R to be of depth greater than or equal to 1, we
get the Tree-Substitution Grammar. TSG remains
in the family of context-free grammars, as can be
easily seen by the removal of the internal nodes
in all elementary trees; what is left is a CFG that
generates the same language. As a reversible al-
ternative that preserves the internal structure, an-
notation of each internal node with a unique index
creates a large number of deterministic CFG rules
that record the structure of the original elementary
trees. A more compact CFG representation can be
obtained by marking each node in each elemen-
tary tree with a signature of its subtree. This trans-
form, presented by Goodman (2003), can rein in
the grammar constant G, as the crucial CFG algo-
rithms for a sentence of length n have complexity
O(Gn3).
A simple probabilistic model for a TSG is a set
of multinomials, one for each nonterminal in N
corresponding to its possible substitutions in R. A
more flexible model allows a potentially infinite
number of substitution rules using a Dirichlet Pro-
cess (Cohn et al, 2009; Cohn and Blunsom, 2010).
This model has proven effective for grammar in-
duction via Markov Chain Monte Carlo (MCMC),
in which TSG derivations of the training set are re-
peatedly sampled to find frequently occurring el-
ementary trees. A straightforward technique for
induction of a finite TSG is to perform this non-
parametric induction and select the set of rules that
appear in at least one sampled derivation at one or
several of the final iterations.
2.2 Tree-Adjoining Grammar
Tree-adjoining grammar (TAG) (Joshi, 1985;
Joshi, 1987; Joshi and Schabes, 1997) is an exten-
sion of TSG defined by a tuple ?N,T,R,A, S?,
and differs from TSG only in the addition of a
303
VP
always VP
VP* quickly
+ S
NP VP
runs
? S
NP VP
always VP
VP
runs
quickly
Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle)
to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is
denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node
only, as it is along the spine of the auxiliary tree.
set of auxiliary trees A and the adjunction oper-
ation that governs their use. An auxiliary tree ?
is an elementary tree containing a single distin-
guished nonterminal leaf, the foot node, with the
same symbol as the root of ?. An auxiliary tree
with root and foot node X can be adjoined into an
internal node of an elementary tree labeled with
X by splicing the auxiliary tree in at that internal
node, as pictured in Figure 2. We refer to the path
between the root and foot nodes in an auxiliary
tree as the spine of the tree.
As mentioned above, the added power afforded
by adjunction comes at a serious price in time
complexity. As such, probabilistic modeling for
TAG in its original form is uncommon. However,
a large effort in non-probabilistic grammar induc-
tion has been performed through manual annota-
tion with the XTAG project(Doran et al, 1994).
2.3 Tree Insertion Grammar
Tree Insertion Grammars (TIGs) are a longstand-
ing compromise between the intuitive expressivity
of TAG and the algorithmic simplicity of CFGs.
Schabes and Waters (1995) showed that by re-
stricting the form of the auxiliary trees in A and
the set of auxiliary trees that may adjoin at par-
ticular nodes, a TAG generates only context-free
languages. The TIG restriction on auxiliary trees
states that the foot node must occur as either the
leftmost or rightmost leaf node. This introduces
an important distinction between left, right, and
wrapping auxiliary trees, of which only the first
two are allowed in TIG. Furthermore, TIG disal-
lows adjunction of left auxiliary trees on the spines
of right auxiliary trees, and vice versa. This is
to prevent the construction of wrapping auxiliary
trees, whose removal is essential for the simplified
complexity of TIG.
Several probabilistic models have been pro-
posed for TIG. While earlier approaches such as
Hwa (1998) and Chiang (2000) relied on hueristic
induction methods, they were nevertheless sucess-
ful at parsing. Later approaches (Shindo et al,
2011; Yamangil and Shieber, 2012) were able to
extend the non-parametric modeling of TSGs to
TIG, providing methods for both modeling and
grammar induction.
2.4 OSTAG
Our new TAG variant is extremely simple. We al-
low arbitrary initial and auxiliary trees, and place
only one restriction on adjunction: we disallow
adjunction at any node on the spine of an aux-
iliary tree below the root (though we discuss re-
laxing that constraint in Section 4.2). We refer to
this variant as Off Spine TAG (OSTAG) and note
that it allows the use of full wrapping rules, which
are forbidden in TIG. This targeted blocking of
recursion has similar motivations and benefits to
the approximation of CFGs with regular languages
(Mohri and jan Nederhof, 2000).
The following sections discuss in detail the
context-free nature of OSTAG and alternative
probabilistic models for its equivalent CFG form.
We propose a simple but empirically effective
heuristic for grammar induction for our experi-
ments on Penn Treebank data.
3 Transformation to CFG
To demonstrate that OSTAG has only context-
free power, we provide a reduction to context-free
grammar. Given an OSTAG ?N,T,R,A, S?, we
define the set N of nodes of the corresponding
CFG to be pairs of a tree inR orA together with an
304
?: S
T
x
T
y
?: T
a T* a
?: T
b T* b
S ? X Y S ? X Y
X ? x X ? x
Y ? y Y ? y
X ? A
X ? B
Y ? A?
Y ? B?
A ? a X ? a X ? a X a
A? ? a Y ? a Y ? a Y a
X ? ? X
Y ? ? Y
B ? b X ?? b X ? b X b
B? ? b Y ?? b Y ? b Y b
X ?? ? X
Y ?? ? Y
(a) (b) (c)
Figure 3: (a) OSTAG for the language wxwRvyvR where w, v ? {a|b}+ and R reverses a string. (b) A
CFG for the same language, which of necessity must distinguish between nonterminalsX and Y playing
the role of T in the OSTAG. (c) Simplified CFG, conflating nonterminals, but which must still distinguish
between X and Y .
address (Gorn number (Gorn, 1965)) in that tree.
We take the nonterminals of the target CFG gram-
mar to be nodes or pairs of nodes, elements of the
setN +N ?N . We notate the pairs of nodes with
a kind of ?applicative? notation. Given two nodes
? and ??, we notate a target nonterminal as ?(??).
Now for each tree ? and each interior node ?
in ? that is not on the spine of ? , with children
?1, . . . , ?k, we add a context-free rule to the gram-
mar
? ? ?1 ? ? ? ?k (1)
and if interior node ? is on the spine of ? with
?s the child node also on the spine of ? (that is,
dominating the foot node of ? ) and ?? is a node (in
any tree) where ? is adjoinable, we add a rule
?(??)? ?1 ? ? ? ?s(??) ? ? ? ?k . (2)
Rules of type (1) handle the expansion of a node
not on the spine of an auxiliary tree and rules of
type (2) a spinal node.
In addition, to initiate adjunction at any node ??
where a tree ? with root ? is adjoinable, we use a
rule
?? ? ?(??) (3)
and for the foot node ?f of ? , we use a rule
?f (?)? ? (4)
The OSTAG constraint follows immediately
from the structure of the rules of type (2). Any
child spine node ?s manifests as a CFG nonter-
minal ?s(??). If child spine nodes themselves al-
lowed adjunction, we would need a type (3) rule
of the form ?s(??) ? ?s(??)(???). This rule itself
would feed adjunction, requiring further stacking
of nodes, and an infinite set of CFG nonterminals
and rules. This echoes exactly the stacking found
in the LIG reduction of TAG .
To handle substitution, any frontier node ? that
allows substitution of a tree rooted with node ??
engenders a rule
? ? ?? (5)
This transformation is reversible, which is to
say that each parse tree derived with this CFG im-
plies exactly one OSTAG derivation, with substi-
tutions and adjunctions coded by rules of type (5)
and (3) respectively. Depending on the definition
of a TAG derivation, however, the converse is not
necessarily true. This arises from the spurious am-
biguity between adjunction at a substitution site
(before applying a type (5) rule) versus the same
adjunction at the root of the substituted initial tree
(after applying a type (5) rule). These choices
lead to different derivations in CFG form, but their
TAG derivations can be considered conceptually
305
identical. To avoid double-counting derivations,
which can adversely effect probabilistic modeling,
type (3) and type (4) rules in which the side with
the unapplied symbol is a nonterminal leaf can be
omitted.
3.1 Example
The grammar of Figure 3(a) can be converted to
a CFG by this method. We indicate for each CFG
rule its type as defined above the production arrow.
All types are used save type (5), as substitution
is not employed in this example. For the initial
tree ?, we have the following generated rules (with
nodes notated by the tree name and a Gorn number
subscript):
? 1?? ?1 ?2 ?1 3?? ?(?1)
?1 1?? x ?1 3?? ?(?1)
?2 1?? y ?2 3?? ?(?2)
?2 3?? ?(?2)
For the auxiliary trees ? and ? we have:
?(?1) 2?? a ?1(?1) a
?(?2) 2?? a ?1(?2) a
?1(?1) 4?? ?1
?1(?2) 4?? ?2
?(?1) 2?? b ?1(?1) b
?(?2) 2?? b ?1(?2) b
?1(?1) 4?? ?1
?1(?2) 4?? ?2
The grammar of Figure 3(b) is simply a renaming
of this grammar.
4 Applications
4.1 Compact grammars
The OSTAG framework provides some leverage in
expressing particular context-free languages more
compactly than a CFG or even a TSG can. As
an example, consider the language of bracketed
palindromes
Pal = aiw aiwR ai
1 ? i ? k
w ? {bj | 1 ? j ? m}?
containing strings like a2 b1b3 a2 b3b1 a2. Any
TSG for this language must include as substrings
some subpalindrome constituents for long enough
strings. Whatever nonterminal covers such a
string, it must be specific to the a index within
it, and must introduce at least one pair of bs as
well. Thus, there are at least m such nontermi-
nals, each introducing at least k rules, requiring at
least km rules overall. The simplest such gram-
mar, expressed as a CFG, is in Figure 4(a). The
ability to use adjunction allows expression of the
same language as an OSTAG with k +m elemen-
tary trees (Figure 4(b)). This example shows that
an OSTAG can be quadratically smaller than the
corresponding TSG or CFG.
4.2 Extensions
The technique in OSTAG can be extended to ex-
pand its expressiveness without increasing gener-
ative capacity.
First, OSTAG allows zero adjunctions on each
node on the spine below the root of an auxiliary
tree, but any non-zero finite bound on the num-
ber of adjunctions allowed on-spine would simi-
larly limit generative capacity. The tradeoff is in
the grammar constant of the effective probabilis-
tic CFG; an extension that allows k levels of on
spine adjunction has a grammar constant that is
O(|N |(k+2)).
Second, the OSTAG form of adjunction is con-
sistent with the TIG form. That is, we can extend
OSTAG by allowing on-spine adjunction of left- or
right-auxiliary trees in keeping with the TIG con-
straints without increasing generative capacity.
4.3 Probabilistic OSTAG
One major motivation for adherence to a context-
free grammar formalism is the ability to employ
algorithms designed for probabilistic CFGs such
as the CYK algorithm for parsing or the Inside-
Outside algorithm for grammar estimation. In this
section we present a probabilistic model for an OS-
TAG grammar in PCFG form that can be used in
such algorithms, and show that many parameters
of this PCFG can be pooled or set equal to one and
ignored. References to rules of types (1-5) below
refer to the CFG transformation rules defined in
Section 3. While in the preceeding discussion we
used Gorn numbers for clarity, our discussion ap-
plies equally well for the Goodman transform dis-
cussed above, in which each node is labeled with a
signature of its subtree. This simply redefines ? in
the CFG reduction described in Section 3 to be a
subtree indicator, and dramatically reduces redun-
dancy in the generated grammar.
306
S ? ai Ti ai
Ti ? bj Ti bj
Ti ? ai
?i | 1 ? i ? k: S
ai T
ai
ai
?j | 1 ? j ? m: T
bj T* bj
(a) (b)
Figure 4: A CFG (a) and more compact OSTAG (b) for the language Pal
The first practical consideration is that CFG
rules of type (2) are deterministic, and as such
we need only record the rule itself and no asso-
ciated parameter. Furthermore, these rules employ
a template in which the stored symbol appears in
the left-hand side and in exactly one symbol on
the right-hand side where the spine of the auxil-
iary tree proceeds. One deterministic rule exists
for this template applied to each ?, and so we may
record only the template. In order to perform CYK
or IO, it is not even necessary to record the index
in the right-hand side where the spine continues;
these algorithms fill a chart bottom up and we can
simply propagate the stored nonterminal up in the
chart.
CFG rules of type (4) are also deterministic and
do not require parameters. In these cases it is not
necessary to record the rules, as they all have ex-
actly the same form. All that is required is a check
that a given symbol is adjoinable, which is true for
all symbols except nonterminal leaves and applied
symbols. Rules of type (5) are necessary to cap-
ture the probability of substitution and so we will
require a parameter for each.
At first glance, it would seem that due to the
identical domain of the left-hand sides of rules of
types (1) and (3) a parameter is required for each
such rule. To avoid this we propose the follow-
ing factorization for the probabilistic expansion of
an off spine node. First, a decision is made as to
whether a type (1) or (3) rule will be used; this cor-
responds to deciding if adjunction will or will not
take place at the node. If adjunction is rejected,
then there is only one type (1) rule available, and
so parameterization of type (1) rules is unneces-
sary. If we decide on adjunction, one of the avail-
able type (3) rules is chosen from a multinomial.
By conditioning the probability of adjunction on
varying amounts of information about the node,
alternative models can easily be defined.
5 Experiments
As a proof of concept, we investigate OSTAG in
the context of the classic Penn Treebank statistical
parsing setup; training on section 2-21 and testing
on section 23. For preprocessing, words that oc-
cur only once in the training data are mapped to
the unknown categories employed in the parser of
Petrov et al (2006). We also applied the annota-
tion from Klein and Manning (2003) that appends
?-U? to each nonterminal node with a single child,
drastically reducing the presence of looping unary
chains. This allows the use of a coarse to fine
parsing strategy (Charniak et al, 2006) in which
a sentence is first parsed with the Maximum Like-
lihood PCFG and only constituents whose prob-
ability exceeds a cutoff of 10?4 are allowed in
the OSTAG chart. Designed to facilitate sister ad-
junction, we define our binarization scheme by ex-
ample in which the added nodes, indicated by @,
record both the parent and head child of the rule.
NP
@NN-NP
@NN-NP
DT @NN-NP
JJ NN
SBAR
A compact TSG can be obtained automatically
using the MCMC grammar induction technique of
Cohn and Blunsom (2010), retaining all TSG rules
that appear in at least one derivation in after 1000
iterations of sampling. We use EM to estimate the
parameters of this grammar on sections 2-21, and
use this as our baseline.
To generate a set of TAG rules, we consider
each rule in our baseline TSG and find all possi-
307
All 40 #Adj #Wrap
TSG 85.00 86.08 ? ?
TSG? 85.12 86.21 ? ?
OSTAG1 85.42 86.43 1336 52
OSTAG2 85.54 86.56 1952 44
OSTAG3 85.86 86.84 3585 41
Figure 5: Parsing F-Score for the models under
comparison for both the full test set and sentences
of length 40 or less. For the OSTAG models, we
list the number of adjunctions in the MPD of the
full test set, as well as the number of wrapping
adjunctions.
ble auxiliary root and foot node pairs it contains.
For each such root/foot pair, we include the TAG
rule implied by removal of the structure above the
root and below the foot. We also include the TSG
rule left behind when the adjunction of this auxil-
iary tree is removed. To be sure that experimental
gains are not due to this increased number of TSG
initial trees, we calculate parameters using EM for
this expanded TSG and use it as a second base-
line (TSG?). With our full set of initial and aux-
iliary trees, we use EM and the PCFG reduction
described above to estimate the parameters of an
OSTAG.
We investigate three models for the probabil-
ity of adjunction at a node. The first uses a con-
servative number of parameters, with a Bernoulli
variable for each symbol (OSTAG1). The second
employs more parameters, conditioning on both
the node?s symbol and the symbol of its leftmost
child (OSTAG2).The third is highly parameterized
but most prone to data sparsity, with a separate
Bernoulli distribution for each Goodman index ?
(OSTAG3). We report results for Most Probable
Derivation (MPD) parses of section 23 in Figure
5.
Our results show that OSTAG outperforms both
baselines. Furthermore, the various parameteri-
zations of adjunction with OSTAG indicate that,
at least in the case of the Penn Treebank, the
finer grained modeling of a full table of adjunction
probabilities for each Goodman index OSTAG3
overcomes the danger of sparse data estimates.
Not only does such a model lead to better parsing
performance, but it uses adjunction more exten-
sively than its more lightly parameterized alterna-
tives. While different representations make direct
comparison inappropriate, the OSTAG results lie
in the same range as previous work with statistical
TIG on this task, such as Chiang (2000) (86.00)
and Shindo et al (2011) (85.03).
The OSTAG constraint can be relaxed as de-
scribed in Section 4.2 to allow any finite number of
on-spine adjunctions without sacrificing context-
free form. However, the increase to the grammar
constant quickly makes parsing with such models
an arduous task. To determine the effect of such a
relaxation, we allow a single level of on-spine ad-
junction using the adjunction model of OSTAG1,
and estimate this model with EM on the training
data. We parse sentences of length 40 or less in
section 23 and observe that on-spine adjunction is
never used in the MPD parses. This suggests that
the OSTAG constraint is reasonable, at least for
the domain of English news text.
We performed further examination of the MPD
using OSTAG for each of the sentences in the test
corpus. As an artifact of the English language, the
majority have their foot node on the left spine and
would also be usable by TIG, and so we discuss
the instances of wrapping auxiliary trees in these
derivations that are uniquely available to OSTAG.
We remove binarization for clarity and denote the
foot node with an asterisk.
A frequent use of wrapping adjunction is to co-
ordinate symbols such as quotes, parentheses, and
dashes on both sides of a noun phrase. One com-
mon wrapping auxiliary tree in our experiments is
NP
? NP* ? PP
This is used frequently in the news text of
the Wall Street Journal for reported speech when
avoiding a full quotation. This sentence is an ex-
ample of the way the rule is employed, using what
Joshi and Schabes (1997) referred to as ?factoring
recursion from linguistic constraints? with TAG.
Note that replacing the quoted noun phrase and
its following prepositional phrase with the noun
phrase itself yields a valid sentence, in line with
the linguistic theory underlying TAG.
Another frequent wrapping rule, shown below,
allows direct coordination between the contents of
an appositive with the rest of the sentence.
308
NP
NP , CC
or
NP* ,
This is a valuable ability, as it is common to
use an appositive to provide context or explanation
for a proper noun. As our information on proper
nouns will most likely be very sparse, the apposi-
tive may be more reliably connected to the rest of
the sentence. An example of this from one of the
sentences in which this rule appears in the MPD is
the phrase ?since the market fell 156.83, or 8 %,
a week after Black Monday?. The wrapping rule
allows us to coordinate the verb ?fell? with the pat-
tern ?X %? instead of 156.83, which is mapped to
an unknown word category.
These rules highlight the linguistic intuitions
that back TAG; if their adjunction were undone,
the remaining derivation would be a valid sen-
tence that simply lacks the modifying structure of
the auxiliary tree. However, the MPD parses re-
veal that not all useful adjunctions conform to this
paradigm, and that left-auxiliary trees that are not
used for sister adjunction are susceptible to this
behavior. The most common such tree is used to
create noun phrases such as
P&G?s share of [the Japanese market]
the House?s repeal of [a law]
Apple?s family of [Macintosh Computers]
Canada?s output of [crude oil]
by adjoining the shared unbracketed syntax onto
the NP dominating the bracketed text. If adjunc-
tion is taken to model modification, this rule dras-
tically changes the semantics of the unmodified
sentence. Furthermore, in some cases removing
the adjunction can leave a grammatically incorrect
sentence, as in the third example where the noun
phrase changes plurality.
While our grammar induction method is a crude
(but effective) heuristic, we can still highlight the
qualities of the more important auxiliary trees
by examining aggregate statistics over the MPD
parses, shown in Figure 6. The use of left-
auxiliary trees for sister adjunction is a clear trend,
as is the predominant use of right-auxiliary trees
for the complementary set of ?regular? adjunc-
tions, which is to be expected in a right branch-
ing language such as English. The statistics also
All Wrap Right Left
Total 3585 (1374) 41 (26) 1698 (518) 1846 (830)
Sister 2851 (1180) 17 (11) 1109 (400) 1725 (769)
Lex 2244 (990) 28 (19) 894 (299) 1322 (672)
FLex 1028 (558) 7 (2) 835 (472) 186 (84)
Figure 6: Statistics for MPD auxiliary trees us-
ing OSTAG3. The columns indicate type of aux-
iliary tree and the rows correspond respectively to
the full set found in the MPD, those that perform
sister adjunction, those that are lexicalized, and
those that are fully lexicalized. Each cell shows
the number of tokens followed by the number of
types of auxiliary tree that fit its conditions.
reflect the importance of substitution in right-
auxiliary trees, as they must capture the wide va-
riety of right branching modifiers of the English
language.
6 Conclusion
The OSTAG variant of Tree-Adjoining Grammar
is a simple weakly context-free formalism that
still provides for all types of adjunction and is
a bit more concise than TSG (quadratically so).
OSTAG can be reversibly transformed into CFG
form, allowing the use of a wide range of well
studied techniques in statistical parsing.
OSTAG provides an alternative to TIG as a
context-free TAG variant that offers wrapping ad-
junction in exchange for recursive left/right spine
adjunction. It would be interesting to apply both
OSTAG and TIG to different languages to deter-
mine where the constraints of one or the other are
more or less appropriate. Another possibility is the
combination of OSTAG with TIG, which would
strictly expand the abilities of both approaches.
The most important direction of future work for
OSTAG is the development of a principled gram-
mar induction model, perhaps using the same tech-
niques that have been successfully applied to TSG
and TIG. In order to motivate this and other re-
lated research, we release our implementation of
EM and CYK parsing for OSTAG1. Our system
performs the CFG transform described above and
optionally employs coarse to fine pruning and re-
laxed (finite) limits on the number of spine adjunc-
tions. As a TSG is simply a TAG without adjunc-
tion rules, our parser can easily be used as a TSG
estimator and parser as well.
1bllip.cs.brown.edu/download/bucketparser.tar
309
References
Eugene Charniak, Mark Johnson, Micha Elsner,
Joseph L. Austerweil, David Ellis, Isaac Hax-
ton, Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Eugene Charniak. 1996. Tree-bank grammars. In As-
sociation for the Advancement of Artificial Intelli-
gence, pages 1031?1036.
David Chiang. 2000. Statistical parsing with
an automatically-extracted tree adjoining grammar.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225?230. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Association for Computational Linguistics.
Christy Doran, Dania Egedi, Beth Ann Hockey, Banga-
lore Srinivas, and Martin Zaidel. 1994. XTAG sys-
tem: a wide coverage grammar for English. pages
922?928. Association for Computational Linguis-
tics.
J. Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. Bod et al 2003.
Saul Gorn. 1965. Explicit definitions and linguistic
dominoes. In Systems and Computer Science, pages
77?115.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 557?563. Association for Computational Lin-
guistics.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer.
Aravind K Joshi. 1985. Tree adjoining grammars:
How much context-sensitivity is required to provide
reasonable structural descriptions? University of
Pennsylvania.
Aravind K Joshi. 1987. An introduction to tree ad-
joining grammars. Mathematics of Language, pages
87?115.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. pages 423?430. Associ-
ation for Computational Linguistics.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
pages 35?56.
Mehryar Mohri and Mark jan Nederhof. 2000. Regu-
lar approximation of context-free grammars through
transformation. In Robustness in language and
speech technology.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: a cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, (4):479?513.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for bayesian tree substi-
tution grammars. pages 206?211. Association for
Computational Linguistics.
Elif Yamangil and Stuart M. Shieber. 2012. Estimat-
ing compact yet rich tree insertion grammars. pages
110?114. Association for Computational Linguis-
tics.
310
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 146?151,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Exploring Syntactic Representations
for Native Language Identification
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Abstract
Tree Substitution Grammar rules form a large
and expressive class of features capable of rep-
resenting syntactic and lexical patterns that
provide evidence of an author?s native lan-
guage. However, this class of features can
be applied to any general constituent based
model of grammar and previous work has
done little to explore these options, relying
primarily on the common Penn Treebank an-
notation standard. In this work we contrast
the performance of syntactic features for Na-
tive Language Indentification using five dif-
ferent formalisms. The use of different for-
malisms captures complementary information
from second language data, and can be used
in combination to yield classification perfor-
mance superior to any formalism taken on its
own.
1 Introduction
Native Language Identification, the automatic deter-
mination of an author?s native language (L1) from
their writing in a second language (L2), follows a
general trend of supervised classification using fea-
tures extracted from text. These systems can be opti-
mized by both classification algorithm selection and
the integration of diverse feature sets, and in this
work we focus on the latter.
Syntactic features have been shown to provide
a strong discriminative signal of an author?s na-
tive language (Wong and Dras, 2011; Swanson and
Charniak, 2012), but little work has been done to ex-
plore the various options for representation of syn-
tax of learner text. Many such representations ex-
ist, and are routinely employed to improve perfor-
mance on the widely studied task of parsing the Penn
Treebank. Furthermore, most techniques that prove
widely successful at this task have publicly available
implementations, making them very feasible options
for NLI systems.
In this work we investigate the use of Tree Sub-
stitution Grammars as features for NLI, focusing on
the implication of syntactic paradigm (constituent vs
dependency grammar) and the addition of annota-
tions that have proved useful in statistical parsing.
A Tree Substitution Grammar (TSG) is an intuitive
extension of the Context Free Grammar (CFG) that
allows rewrite rules of arbitrary tree structure. Alter-
natively, a CFG can be seen as a TSG in which the
rewrite rules obey the constraint that each is a tree
structure of unit depth.
While a collection of parsed data can be poten-
tially generated by a TSG that is exponential in the
length of the text, recent techniques allow for the ef-
ficient induction of compact grammars (Cohn and
Blunsom, 2010). At a high level, this technique
employs the rich-get-richer dynamics of a Dirich-
let Process to sample derivations for the trees in the
training corpus: the more that a rule is used in other
derivations, the more likely it is that we will choose
it when sampling a derivation.
We follow previous work in stylometry with
TSGs for the NLI in that we parse the entirety of
the training data and use it to induce a compact TSG
using the method described above.1 We then use the
1An alternative method of note that we do not consider in
this work is to induce TSG rules on hand-annotated data such
as the Penn Treebank, as in Bergsma et al (2012).
146
S@S
PP
IN
Without
NP
NN
deviation
NP
progress
VP
@VP
VBZ
is
RB
not
NP
JJ
possible
S-5
@S-2
PP-3
IN-10
Without
NP-12
NN-13
deviation
NP-15
progress
VP-2
@VP-1
VBZ-3
is
RB-5
not
NP-6
JJ-2
possible
Figure 1: Sample parse trees produced by the Berkeley Parser. An example of what the tree might look like with split
symbol annotations is shown on the right.
TSG rules as binary features for supervised classi-
fication such that the feature for a TSG rule is trig-
gered on a document if that rule appears in the parse
of some derivation of any of its sentences. This de-
scription purposefully treats the parsing of text as a
black box whose input is plain text and whose out-
put is any valid tree structure. Our work considers
five alternatives for this black box, and evaluates the
effect of this choice on the NLI Shared Task at the
BEA Workshop of NAACL 2013 (Tetreault et al,
2013).
2 Syntactic Representations
We investigate five variations on the output of the
parsing process. All five are easily produced by
freely available Java software; two with the Berkeley
Parser, two with the Stanford Parser, and one with a
combination of both software packages.
2.1 Berkeley Constituent Parses
Our first representation reproduces previous work by
using the output of the Berkeley Parser (Petrov et
al., 2006), one of highest performing systems on the
benchmark Penn Treebank task. The basic motivat-
ing principle involved is that the traditional nonter-
minal symbols used in Penn Treebank parsing are
too coarse to satisfy the context free assumption of
a CFG. To combat this, hierarchical latent annota-
tions are induced that split a symbol into several
subtypes, and a larger CFG is estimated on this set
of split nonterminals. A sentence is parsed using
this large CFG and each resulting symbol is mapped
back to its original unsplit supertype to produce the
final parse.
One important subtlety of the Berkeley Parser is
its default binarization, which we leave intact in our
downstream use of its parses. While binarization is
normally motivated by the desired cubic complexity
of parsing algorithms, it also benefits syntactic sty-
lometry. Consider the nugget of wisdom from the
great Frank Zappa shown on the left in Figure 1, in
which artificially introduced binarization nodes are
marked with the @ symbol.
The use of binarization allows us to capture pat-
terns such as verb phrases that begin with ?is not?
independent of the following child constituents. The
capabilities of TSG rules makes the use of binariza-
tion even more apt, as we can easily choose to re-
cover the unbinarized pattern with a slightly larger
fragment. This choice will be made in TSG induc-
tion based on the frequency with which the combi-
nation occurs, which intuitively aligns with our goal
of choosing representative features.
The second form that we investigate is identical
to the normal Berkeley Parser output, but with the
split annotations used in parsing left intact, as shown
the right of Figure 1. This parsed sentence shows
how each nonterminal is annotated with a split cat-
egory, and illustrates the potential advantages that
this method affords. For example, consider the @VP
node in the left-hand tree, whose subtree is gen-
erated with a CFG by first choosing to produce a
VBZ and RB, and then by lexicalizing each inde-
pendently. These two lexicalizations are not in fact
independent, as can be seen by the combination of
?is? with the RB ?may?, which is impossible al-
147
though each are independently quite likely. Splitting
the symbols as shown on the right allows us to cre-
ate a special RB node that is most likely to produce
?not? and VBZ node likely to produce ?is?. Their
likely co-occurrence can then be modeled as shown
by a rule with both specialized tags as children.
It is worth noting that this particular ability of
split symbol grammars to coordinate lexical items
is easily captured with the TSG rules that we induce
on these parses, regardless of the presence of split
symbols. The more orthogonal quality of these split
grammars is their ability to categorize symbols that
appear in similar syntactic situations. Consider that
some adjectives are more likely to appear in ?X is Y?
sentences in the ?Y? position, while some are more
likely to be used directly to the left of nouns. A split
symbol grammar handily captures this trait with a
split POS tag, while a TSG cannot associate patterns
containing different lexical items on its own.
2.2 Stanford Dependency Parses
The third and fourth syntactic models we employ
are derived from dependency parses produced by the
Stanford parser(Marneffe et al, 2006). In its stan-
dard form, a dependency parse is a directed tree in
which each word except the special ROOT node has
exactly one incoming edge and zero to many outgo-
ing edges, where edges represent syntactic depen-
dence. Arcs are labeled with the type of syntactic
dependence that they indicate. Following conven-
tion, we represent each word in combination with its
part of speech tag, as shown in the following exam-
ple dependency parse.
ROOT DT NN VBZ PRP
The poodle chews it
root
det nsubj dobj
In order to apply the techniques of TSG induction
to dependency parsed data, we implement a conver-
sion from dependency tree to constituent form. The
mechanics of this conversion are simple and illus-
trated in full by the following conversion of the de-
pendency tree shown above, and are similar to trans-
forms used in previous work in unsupervised depen-
dency parsing(Carroll and Charniak, 1992).
ROOT
VBZ-L
nsubj
NN-L
det
DT
the
NN
poodle
VBZ
chews
VBZ-R
dobj
PRP
it
Note that it is always the case that the arc labels
from the dependency parses are always produced
by unary rules. This allows the simple removal of
the nodes corresponding to arc labels, yielding our
fourth syntactic model.
ROOT
VBZ-L
NN-L
DT
the
NN
poodle
VBZ
chews
VBZ-R
PRP
it
Those familiar with the Stanford Parser may be
concerned that the dependency parses used here are
determined by a deterministic transform of a con-
stituent parse of Penn Treebank style, and then sim-
ply transformed back into constituent form. This is
especially concerning when considering the second
form in which arc labels have been removed; this
form can be constructed directly from the Berkeley
Parse form used above, and contains no additional
information. Our motivation in the investigation of
dependency parses is not that they offer new infor-
mation, but that they are organized differently than
constituent parses. When inducing a TSG, our abil-
ity to find a useful connections is impeded by phys-
ical distance between structures. In particular, in a
dependency parse, the head of the subject and the
verb are always contained in some TSG fragment
148
made up of small number of CFG rules, five or four
depending on the presence of arc labels. In con-
stituent parses, the presence of modifying phrases
can arbitrarily increase this distance.
2.3 Stanford Heuristic Annotations
Our final variation uses the annotations internal to
the Stanford Penn Treebank parser, as presented in
Klein and Manning (2003). These annotations are
motivated in the same way as Berkeley Parser split
states, but are deterministically applied to parse trees
using linguistic motivations. Besides handling ex-
plicit tracking of binarization and parent annotation,
several additional annotations are applied, such as
the splitting of certain POS tags into useful cate-
gories and annotation of some nodes with their num-
ber of children or siblings.
For ease of implementation, we do not use the
Stanford Parser itself to produce our trees, instead
we used our results from the Berkeley Parser. The
Stanford Parser annotations were then applied to
these trees after binarization symbols were first col-
lapsed. The following tree is an example of the
actual annotations applied by this process, and in-
cludes a fair subset of the many annotation types
that are used. The original symbol in each case is
the leftmost string of capital letters in the resulting
symbol strings shown.
ROOT
S-v
NP-B
NNP?NP
Ace
VP-VBF-v
VBZ?VB-BE
is
PP
IN?PP
in
NP-B
DT?NP
the
NN?NP
house
3 Experiments
We contrast the syntactic formalisms on the NLI
shared task experimental setup for the NAACL 2013
BEA workshop. This new data set (Blanchard et al,
2013) consists of TOEFL essays drawn from speak-
ers of 11 different L1 backgrounds. 9900 Essays
were supplied as a training set, with an additional
1100 development set essays and 1100 test essays.
Previous work in NLI has relied heavily on the
International Corpus of Learner English, but due to
significant topic biases along L1 lines in this data
set the explicit use of word tokens was frequently
limited to a predetermined set of stopwords. With
this in mind, the data set for the shared task was bal-
anced across TOEFL essay prompts and proficiency
levels. The result was that the participants in this
task were not forced to limit the word tokens explic-
itly employed, with the hopes that mitigating factors
had been minimized.
We prepared the data in the five forms described
above and induced TSGs on each version of the
parsed training set with the blocked sampling algo-
rithm of Cohn and Blunsom (2010). The resulting
rules were used as binary feature functions over doc-
uments indicating the presence of the rule in some
derivation of sentence in that document. We used
the Mallet implementation of a log-linear (MaxEnt)
classifier with a zero mean Gaussian prior with vari-
ance .1 on the classifier?s weights. Our results on the
development set are shown in Figure 3.
While a range of performance is achieved, when
we construct a classifier that simply averages the
predictive distributions of all five methods we get
better accuracy than any model on its own. We ob-
served further evidence of the orthogonality of these
methods by looking at pairs of formalisms and ob-
serving how many development set items were pre-
dicted correctly by one formalism and incorrectly by
another. This was routinely around 10 percent of the
development set in each direction for a given pair,
implying that gains of up to at least 20 percent classi-
fication accuracy are possible with an expert system
that approaches oracle selection of which formalism
to use.
As our submission to the shared task, we used the
Berkeley Parser output in isolation, the average of
the five classifiers, and the weighted average of the
classifiers using the optimal weights on the devel-
opment set. The former two models use the devel-
opment set as additional training data, which is one
possible explanation of the slightly higher perfor-
mance of the equally weighted average model. An-
149
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR P R F
ARA 76 2 4 1 2 2 2 1 4 3 3 76.8 76.0 76.4
CHI 2 86 0 1 1 0 4 4 1 0 1 81.1 86.0 83.5
FRE 2 1 77 3 2 6 2 1 5 1 0 82.8 77.0 79.8
GER 0 1 1 91 1 1 0 0 2 0 3 86.7 91.0 88.8
HIN 2 2 1 2 71 0 0 0 0 20 2 73.2 71.0 72.1
ITA 2 0 2 1 1 84 0 1 7 0 2 79.2 84.0 81.6
JPN 3 4 0 1 0 0 83 7 1 0 1 74.1 83.0 78.3
KOR 1 6 1 1 1 0 20 65 2 1 2 69.1 65.0 67.0
SPA 4 2 4 3 2 12 0 3 66 0 4 71.7 66.0 68.8
TEL 1 2 0 0 16 0 0 0 0 81 0 76.4 81.0 78.6
TUR 6 0 3 1 0 1 1 12 4 0 72 80.0 72.0 75.8
Figure 2: Confusion Matrix and per class results on the final test set evaluation using the evenly averaged model.
other explanation of note is that while the weight
optimization was carried out with EM over the like-
lihood of the development set labels, this did not
in correlate positively with classification accuracy;
even as we optimized on the development set the ac-
curacy in absolute classification of these items de-
creased slightly.
The confusion matrix for the evenly averaged
model, our best performing system, is shown in Fig-
ure 2. The most frequently confused L1 pairs were
Hindi and Telegu, Japanese and Korean, and Span-
ish and Italian. The similarity between Hindi and
Telegu is particularly troubling, as they come from
two completely different language families and their
most obvious similarity is that they are both spoken
primarily in India. This suggests that even though
the TOEFL corpus has been balanced by topic that
there is a strong geographical signal that is corre-
lated with but not caused by native language.
BP BPS DP DPA KM AVG
Acc 74.5 69.3 72.4 73.5 73.5 77.3
Figure 3: The resulting classification accuracies on the
development set for the various syntactic forms that we
considered. The forms used are plain Berkeley Parses
(BP), Berkeley Parses with split symbols (BPS), depen-
dency parses (DP), dependency parses without arc la-
bels (DPA), and the heuristic annotations from (Klein and
Manning, 2003) (KM). When the predictive distributions
of the five models are averaged (AVG), a higher accuracy
is achieved.
BP AVG AVG-EM
Acc 74.7 77.5 77.0
Figure 4: The classification accuracies obtained on the
test data using the Berkeley parser output alone (BP), the
arithmetic mean of all five predictive distributions (AVG)
and the weighted mean using the optimal weights from
the development set as determined with EM (AVG-EM)
4 Conclusion
In this work we open investigation of a generally un-
considered variable in syntactic stylometry: the ac-
tual syntactic formalism. We examine five poten-
tial candidates of which only one has been previ-
ously presented in the context of TSG features for
NLI. These five formalisms cover both constituent
and dependency grammars, and explore the possi-
bility of split state annotations for constituent gram-
mars and the inclusion of arc labels for dependency
grammars. We find that the use of different grammar
formalisms captures orthogonal information about
an author?s native language. Furthermore, the com-
bination of different formalisms can be used to in-
crease classification accuracy.
While our results are intriguing, they primarily
serve as a proof of concept that syntactic stylome-
try can benefit from a range of representations and
should not be taken as an exhaustive search for the
best representations to use. Other syntactic forms
exist, and even in our methods there are additional
variables that can be adjusted.
One such variable is the number of splits used in
150
the Berkeley Parser when split states are included;
the default number that we use in this work is 6,
the optimal value for the parsing task, but this may
be suboptimal as a representation for feature extrac-
tion. Binarization is another easily adjusted variable,
with several available options in the literature. For
example, binarization can be done that is aware of
head attachment. Another option is to binarize more
heavily, increasing the ability of TSG fragments to
separate sister nodes and find frequent patterns.
Alternative syntactic forms not explored in this
work are also available. These include well stud-
ied grammars such as Hierarchical Phrase Structure
Grammars and Combinatory Categorial Grammars,
and transforms that rearrange the tree such as the
Left Corner Transform used in Roark and Johnson
(1999). Furthermore, the use of the TSG as a fea-
ture extractor itself has the potential for extension
to more powerful systems such as Tree Adjoining
Grammars or Tree Insertion Grammars.
References
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric Analysis of Scientific Articles. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 327?
337, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. Toefl11: A cor-
pus of non-native english. Technical report, Educa-
tional Testing Service.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. Technical Report CS-92-16, Brown
University, Providence, RI, USA.
Trevor Cohn and Phil Blunsom. 2010. Blocked inference
in bayesian tree substitution grammars. In ACL (Short
Papers), pages 225?230.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In In
Proc. Intl Conf. on Language Resources and Evalua-
tion (LREC, pages 449?454.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Brian Roark and Mark Johnson. 1999. Efficient proba-
bilistic top-down and left-corner parsing. In ACL.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
151
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 124?133,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Natural Language Generation with Vocabulary Constraints
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Elif Yamangil
Google Inc.
Mountain View, CA
leafer@google.com
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
We investigate data driven natural lan-
guage generation under the constraints
that all words must come from a fixed vo-
cabulary and a specified word must ap-
pear in the generated sentence, motivated
by the possibility for automatic genera-
tion of language education exercises. We
present fast and accurate approximations
to the ideal rejection samplers for these
constraints and compare various sentence
level generative language models. Our
best systems produce output that is with
high frequency both novel and error free,
which we validate with human and auto-
matic evaluations.
1 Introduction
Freeform data driven Natural Language Genera-
tion (NLG) is a topic explored by academics and
artists alike, but motivating its empirical study is a
difficult task. While many language models used
in statistical NLP are generative and can easily
produce sample sentences by running their ?gen-
erative mode?, if all that is required is a plausible
sentence one might as well pick a sentence at ran-
dom from any existing corpus.
NLG becomes useful when constraints exist
such that only certain sentences are valid. The
majority of NLG applies a semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their specific
meaning.
We study two constraints concerning the words
that are allowed in a sentence. The first sets a
fixed vocabulary such that only sentences where
all words are in-vocab are allowed. The second
demands not only that all words are in-vocab,
but also requires the inclusion of a specific word
somewhere in the sentence.
These constraints are natural in the construction
of language education exercises, where students
have small known vocabularies and exercises that
reinforce the knowledge of arbitrary words are re-
quired. To provide an example, consider a Chi-
nese teacher composing a quiz that asks students
to translate sentences from English to Chinese.
The teacher cannot ask students to translate words
that have not been taught in class, and would like
ensure that each vocabulary word from the current
book chapter is included in at least one sentence.
Using a system such as ours, she could easily gen-
erate a number of usable sentences that contain a
given vocab word and select her favorite, repeat-
ing this process for each vocab word until the quiz
is complete.
The construction of such a system presents two
primary technical challenges. First, while highly
parameterized models trained on large corpora are
a good fit for data driven NLG, sparsity is still
an issue when constraints are introduced. Tradi-
tional smoothing techniques used for prediction
based tasks are inappropriate, however, as they lib-
erally assign probability to implausible text. We
investigate smoothing techniques better suited for
NLG that smooth more precisely, sharing proba-
bility only between words that have strong seman-
tic connections.
The second challenge arises from the fact that
both vocabulary and word inclusion constraints
are easily handled with a rejection sampler that re-
peatedly generates sentences until one that obeys
the constraints is produced. Unfortunately, for
124
models with a sufficiently wide range of outputs
the computation wasted by rejection quickly be-
comes prohibitive, especially when the word in-
clusion constraint is applied. We define models
that sample directly from the possible outputs for
each constraint without rejection or backtracking,
and closely approximate the distribution of the
true rejection samplers.
We contrast several generative systems through
both human and automatic evaluation. Our best
system effectively captures the compositional na-
ture of our training data, producing error-free text
with nearly 80 percent accuracy without wasting
computation on backtracking or rejection. When
the word inclusion constraint is introduced, we
show clear empirical advantages over the simple
solution of searching a large corpus for an appro-
priate sentence.
2 Related Work
The majority of NLG focuses on the satisfaction
of a communicative goal, with examples such as
Belz (2008) which produces weather reports from
structured data or Mitchell et al. (2013) which gen-
erates descriptions of objects from images. Our
work is more similar to NLG work that concen-
trates on structural constraints such as generative
poetry (Greene et al., 2010) (Colton et al., 2012)
(Jiang and Zhou, 2008) or song lyrics (Wu et al.,
2013) (Ramakrishnan A et al., 2009), where spec-
ified meter or rhyme schemes are enforced. In
these papers soft semantic goals are sometimes
also introduced that seek responses to previous
lines of poetry or lyric.
Computational creativity is another subfield of
NLG that often does not fix an a priori meaning in
its output. Examples such as
?
Ozbal et al. (2013)
and Valitutti et al. (2013) use template filling tech-
niques guided by quantified notions of humor or
how catchy a phrase is.
Our motivation for generation of material for
language education exists in work such as Sumita
et al. (2005) and Mostow and Jang (2012), which
deal with automatic generation of classic fill in the
blank questions. Our work is naturally comple-
mentary to these efforts, as their methods require a
corpus of in-vocab text to serve as seed sentences.
3 Freeform Generation
For clarity in our discussion, we phrase the sen-
tence generation process in the following general
terms based around two classes of atomic units :
contexts and outcomes. In order to specify a gen-
eration system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o)? List[c ? C]
4. M : derivation tree sentence
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. Be-
ginning with a unique root context, a derivation
tree is created by repeatedly choosing an outcome
o for a leaf context c and expanding c to the new
leaf contexts specified by I(c, o). M converts be-
tween derivation tree and sentence text form.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of symbols, O be the
rules of the CFG, and I(c, o) return a list of the
symbols on the right hand side of the rule o. To de-
fine an n-gram model, a context is a list of words,
an outcome a single word, and I(c, o) can be pro-
cedurally defined to drop the first element of c and
append o.
To perform the sampling required for derivation
tree construction we must define P (o|c). Using
M, we begin by converting a large corpus of sen-
tence segmented text into a training set of deriva-
tion trees. Maximum likelihood estimation of
P (o|c) is then as simple as normalizing the counts
of the observed outcomes for each observed con-
text. However, in order to obtain contexts for
which the conditional independence assumption
of P (o|c) is appropriate, it is necessary to con-
dition on a large amount of information. This
leads to sparse estimates even on large amounts of
training data, a problem that can be addressed by
smoothing. We identify two complementary types
of smoothing, and illustrate them with the follow-
ing sentences.
The furry dog bit me.
The cute cat licked me.
An unsmoothed bigram model trained on this
data can only generate the two sentences verba-
tim. If, however, we know that the tokens ?dog?
and ?cat? are semantically similar, we can smooth
by assuming the words that follow ?cat? are also
likely to follow ?dog?. This is easily handled with
125
traditional smoothing techniques that interpolate
between distributions estimated for both coarse,
P (w|w
?1
=[animal]), and fine, P (w|w
?1
=?dog?),
contexts. We refer to this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
dog [animal]
bit
[action]
4
2
1
3
5
7
6
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a semantic goal but instead ask only that the
output be considered a valid sentence, seeking a
model that captures the variability of language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. We also require the
existence of a single unique root context, and refer
to the result of repeated sampling of outcomes for
contexts as a derivation tree. Finally, a mapping
from derivation tree to surface form is required to
produce actual text.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of nonterminals, O be
the rules of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dropping the first word of the previous
context and appending the outcome to the end.
This formulation is well suited to data driven
estimation from a corpus of derivation trees.
While our methods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estimation of P (o|c) is then as simple as nor-
malizing the counts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
formation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry dog bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle word, an unsmoothed model tr ined on this
data can only generate the two sentences verba-
tim. Imagine we have some way of knowing that
the tokens ?dog? and ?cat? are similar and would
like to leverage this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely to follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for both coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), wher specified meter or rhyme schemes
are enforce .
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a semantic goal but in tead ask only that the
output b co sidered a valid sentence, seeking
m del that captures the variability of language.
For clarity in our discussion, we phrase the
generation proce s in the following general ter s
based around two classes of atomic unit : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
wher I(c, o) d fines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. W lso require the
exist nce of a single un que r ot context, and refer
to the result of repeated sampling of outcomes f r
contexts as a derivation tree. Fi a ly, a mapping
f om derivation tree to s rface form is r quired to
produce actual text.
Thi is simply a co veni nt rephrasing of the
Context Free Grammar formalism, and as such
the systems w describe all have some equiv lent
CFG nterpretat on. Indeed, to describe a tradi-
tional CFG let C be the set of nonterminals, O be
the r le of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rul
o and does ot d pend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The C ntext-Outcome term can be re natu-
ral wh n describing other models where we do no
want to expl cit y define the space of onterm nals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally d -
fined to pr duce a list containg a single context
made by dropping the first word of the previous
con ext and appending the outcome to the end.
This formulation is well suited to data driven
estim f om a corpus of derivation trees.
While our methods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estim tion of P (o|c) is then as simple as nor-
malizing th counts of the observed outcomes for
each bs rved context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assump ion of P (o|c) is appropriate, it is
n cessary t condition on a large amount of in-
for ion. This leads to sparse estimates even on
la ge a ounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry dog bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle wor , an unsmooth d model trained n this
data can only generate the two sentences verba-
tim. Imagine we have some way of knowing that
the toke s ?d g? and ?cat? are similar and would
like to leverag this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely t follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for both coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome sm thing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a s mantic goal but instead ask only that the
output be onsidered a valid sentence, seeking a
model that captures the variability of language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of cont xts c
2. the set O of outco es o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) define the fur r contexts impli d
by the choice of outcome o for the context . This
model can be made probabi istic by the definition
of P (o|c), wher each outcome is sampled inde-
pend ntly give its context. We also r quire the
existe ce of a single unique ro t context, and refer
to esult of r peated samp ing of utcomes for
contexts as a derivati n tree. Finally, a mapping
from derivation tree to sur ac form is required to
produce actual text.
T is is simply a convenient rephrasing of the
Co t xt Free Grammar formalism, and as suc
the systems we describe all hav some equivalent
CFG interpret tion. Indeed, to describe a tradi-
tional CFG, let C be the set of onterminals, O be
the rules of the CFG, and I( , o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dropping the first word of the pr ious
c ntext nd appe ding the outcome to the end.
This f rmul tion is w ll suited to data driven
estimation from a corpus f derivation trees.
While our methods are eas ly extended to mul-
tiple derivations for eac single se tence, in this
w rk we assum cces to a single derivation for
each sentence i our data set. Maximum lik li-
hood s imation of P (o|c) is then as simple as nor-
malizing the cou ts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
formation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry d g bit me.
The cut cat licked me.
Assuming a simple bigram model wher con-
text is the previous word and the outcome a sin-
gle word, an u smoothed model trained on this
at can only generate the two sentences verba-
tim. Imagine we have som way of knowing that
the tokens ?dog? and ?cat? a e sim lar and wou d
like to leverage this fact . In our bigram model,
this amounts to the clai that e words that follow
?cat? e perhap al o likely to ollow ?dog?. Thi
is easily handled wit traditional smo thi tech-
niques, which interpolate between distributions
estimated for both coar , P (w|w
?1
=[is-a imal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to apture the in-
tuition that words which can be followed by ?dog?
can also be f llowed by ?cat?, which we will all
out m smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Gene ation
We first addr ss th problem of freeform data
driven language g neration directly. We do not
set a semantic goal but instead ask only that the
output b considered a valid s ntenc , seeking a
model that captures the vari bility f language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. We also require the
existence of a single unique root context, and refer
to the result of repeated sampling of outcomes for
contexts as a derivation tree. Finally, a mapping
from derivation tree to surface form is required to
produce actual text.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of nonterminals, O be
the rules of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dro ping the first word of the previous
context and appending the outcome to the end.
This formulation is well suited to data driven
estimation from a corpus of derivation trees.
While our me hods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estimation of P (o|c) is then as simple as nor-
malizing the counts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
f rmation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry d g bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle word, an unsmoothed model trained on this
data can only gener e the two senten es verba-
tim. Imagine we have some way of knowing that
the tokens ?dog? and ?cat? are similar and would
like to leverage this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely to follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for bot coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
Figure 1: A flow chart depicting the decisions
made when choosing an outcome for a context.
The large circles show the set of items associated
with each decision, and contain examples items
for a bigram model where S
C
and S
O
map words
(e.g. dog) to semantic classes (e.g. [animal]).
We describe the smoothed generative process
with the flowchart shown in Figure 1. In order to
choose an outcome for a given context, two deci-
sions must be made. First, we must decide which
context we will employ, the true context or the
smooth context, marked by edges 1 or 2 respec-
tively. Next, we choose to generate a true outcome
or a smooth outcome, and if we select the latter
we use edge 6 to choose a true outcome given the
smooth outcome. The decision between edges 1
and 2 can be sampled from a Bernoulli random
variable with parameter ?
c
, with one variable es-
timated for each context c. The decision between
edges 5 and 3 and the one between 4 and 7 can also
be made with Bernoulli random variables, with pa-
rameter sets ?
c
and ?
c?
respectively.
This yields the full form of the unconstrained
probabilistic generative model as follows
P (o|c) = ?
c
P
1
(o|c) + (1? ?
c
)P
2
(o|S
C
(c))
P
1
(o|c) = ?
c
P
5
(o|c)+
(1? ?
c
)P
7
(o|o?)P
3
(o?|c) (1)
P
2
(o|c?) = ?
c?
P
6
(o|c)+
(1? ?
c?
)P
7
(o|o?)P
4
(o?|c?)
requiring estimation of the ? and ? variables as
well as the five multinomial distributions P
3?7
.
This can be done with a straightforward applica-
tion of EM.
4 Limiting Vocabulary
A primary concern in the generation of language
education exercises is the working vocabulary of
the students. If efficiency were not a concern, the
natural solution to the vocabulary constraint would
be rejection sampling: simply generate sentences
until one happens to obey the constraint. In this
section we show how to generate a sentence di-
rectly from this constrained set with a distribution
closely approximating that of the rejection sam-
pler.
4.1 Pruning
The first step is to prune the space of possible sen-
tences to those that obey the vocabulary constraint.
For the models we investigate there is a natural
predicate V (o) that is true if and only if an out-
come introduces a word that is out of vocab, and
so the vocabulary constraint is equivalent to the
requirement that V (o) is false for all possible out-
comes o. Considering transitions along edges in
Figure 1, the removal of all transitions along edges
5,6, and 7 that lead to outcomes where V (o) is true
satisfies this property.
Our remaining concern is that the generation
process does not reach a failure case. Again
considering transitions in Figure 1, failure occurs
when we require P (o|c) for some c and there is no
transition to c on edge 1 or S
C
(c) along edge 2.
We refer to such a context as invalid. Our goal,
which we refer to as consistency, is that for all
126
valid contexts c, all outcomes o that can be reached
in Figure 1 satisfy the property that all members of
I(c, o) are valid contexts.
To see how we might end up in failure, consider
a trigram model on POS/word pairs for which S
C
is the identity function and S
O
backs off to the
POS tag. Given a context c = (
(
t
?2
w
?2
)
,
(
t
?1
w
?1
)
) if
we generate along a path using edge 6 we will
choose a smooth outcome t
0
that we have seen
following c in the data and then independenently
choose a w
0
that has been observed with tag t
0
.
This implies a following context (
(
t
?1
w
?1
)
,
(
t
0
w
0
)
). If
we have estimated our model with observations
from data, there is no guarantee that this context
ever appeared, and if so there will be no available
transition along edges 1 or 2.
Let the list
?
I(c, o) be the result of the mapped
application of S
C
to each element of I(c, o). In
order to define an efficient algorithm, we require
the following property D referring to the amount
of information needed to determine
?
I(c, o). Sim-
ply put, D states if the smoothed context and out-
come are fixed, then the implied smooth contexts
are determined.
D {S
C
(c), S
O
(o)} ?
?
I(c, o)
To highlight the statement D makes, consider the
trigram POS/word model described above, but let
S
C
also map the POS/word pairs in the context
to their POS tags alone. D holds here because
given S
C
(c) = (t
?2
, t
?1
) and S
O
(o) = t
0
from
the outcome, we are able to determine the implied
smooth context (t
?1
, t
0
). If context smoothing in-
stead produced S
C
(c) = (t
?2
),D would not hold.
If D holds then we can show consistency based
on the transitions in Figure 1 alone as any com-
plete path through Figure 1 defines both c? and o?.
By D we can determine
?
I(c, o) for any path and
verify that all its members have possible transi-
tions along edge 2. If the verification passes for
all paths then the model is consistent.
Algorithm 1 produces a consistent model by
verifying each complete path in the manner just
described. One important feature is that it pre-
serves the invariant that if a context c can be
reached on edge 1, then S
C
(c) can be reached on
edge 2. This means that if the verification fails
then the complete path produces an invalid con-
text, even though we have only checked the mem-
bers of
?
I(c, o) against path 2.
If a complete path produces an invalid con-
text, some transition along that path must be re-
Algorithm 1 Pruning Algorithm
Initialize with all observed transitions
for all out of vocab o do
remove ?? o from edges 5,6, and 7
end for
repeat
for all paths in flow chart do
if ?c? ?
?
I(c, o) s.t. c? is invalid then
remove transition from edge 5,7,3 or 4
end if
end for
Run FIXUP
until edge 2 transitions did not change
moved. It is never optimal to remove transitions
from edges 1 or 2 as this unnecessarily removes
all downstream complete paths as well, and so for
invalid complete paths along 1-5 and 2-7 Algo-
rithm 1 removes the transitions along edges 5 and
7. The choice is not so simple for the complete
paths 1-3-6 and 2-4-6, as there are two remaining
choices. Fortunately, D implies that breaking the
connection on edge 3 or 4 is optimal as regardless
of which outcome is chosen on edge 6,
?
I(c, o) will
still produce the same invalid c?.
After removing transitions in this manner, some
transitions on edges 1-4 may no longer have any
outgoing transitions. The subroutine FIXUP re-
moves such transitions, checking edges 3 and 4
before 1 and 2. If FIXUP does not modify edge 2
then the model is consistent and Algorithm 1 ter-
minates.
4.2 Estimation
In order to replicate the behavior of the rejection
sampler, which uses the original probability model
P (o|c) from Equation 1, we must set the probabil-
ities P
V
(o|c) of the pruned model appropriately.
We note that for moderately sized vocabularies it
is feasible to recursively enumerate C
V
, the set of
all reachable contexts in the pruned model. In
further discussion we simplify the representation
of the model to a standard PCFG with C
V
as its
symbol set and its PCFG rules indexed by out-
comes. This also allows us to construct the reach-
ability graph for C
V
, with an edge from c
i
to c
j
for each c
j
? I(c
i
, o). Such an edge is given
weight P (o|c), the probability under the uncon-
strained model, and zero weight edges are not in-
cluded.
Our goal is to retain the form of the stan-
127
dard incremental recursive sampling algorithm for
PCFGs. The correctness of this algorithm comes
from the fact that the probability of a rule R ex-
panding a symbolX is precisely the probability of
all trees rooted atX whose first rule isR. This im-
plies that the correct sampling distribution is sim-
ply the distribution over rules itself. When con-
straints that disallow certain trees are introduced,
the probability of all trees whose first rule is R
only includes the mass from valid trees, and the
correct sampling distribution is the renormaliza-
tion of these values.
Let the goodness of a contextG(c) be the proba-
bility that a full subtree generated from c using the
unconstrained model obeys the vocabulary con-
straint. Knowledge of G(c) for all c ? C
V
al-
lows the calculation of probabilities for the pruned
model with
P
V
(o|c) ? P (o|c)
?
c
?
?I(c,o)
G(c
?
) (2)
While G(c) can be defined recursively as
G(c) =
?
o?O
P (o|c)
?
c
?
?I(c,o)
G(c
?
) (3)
its calculation requires that the reachability graph
be acyclic. We approximate an acyclic graph by
listing all edges in order of decreasing weight and
introducing edges as long as they do not create cy-
cles. This can be done efficiently with a binary
search over the edges by weight. Note that this ap-
proximate graph is used only in recursive estima-
tion of G(c), and the true graph can still be used
in Equation 2.
5 Generating Up
In this section we show how to efficiently gener-
ate sentences that contain an arbitrary word w
?
in
addition to the vocabulary constraint. We assume
the ability to easily find C
w
?
, a subset of C
V
whose
use guarantees that the resulting sentence contains
w
?
. Our goal is once again to efficiently emulate
the rejection sampler, which generates a derivation
tree T and accepts if and only if it contains at least
one member of C
w
?
.
Let T
w
?
be the set of derivation trees that would
be accepted by the rejection sampler. We present
a three stage generative model and its associated
probability distribution P
w
?
(?) over items ? for
which there is a functional mapping into T
w
?
.
In addition to the probabilities P
V
(o|c) from the
previous section, we require an estimate of E(c),
the expected number of times each context c ap-
pears in a single tree. This can be computed effi-
ciently using the mean matrix, described in Miller
and Osullivan (1992). This |C
V
| ? |C
V
| matri x M
has its entries defined as
M(i, j) =
?
o?O
P (o|c
i
)#(c
j
, c
i
, o) (4)
where the operator # returns the number of times
context c
j
appears I(c
i
, o). Defining a 1 ? |C
V
|
start state vector z
0
that is zero everywhere and 1
in the entry corresponding to the root context gives
E(z) =
?
?
i=0
z
0
M
i
which can be iteratively computed with sparse ma-
trix multiplication. Note that the ith term in the
sum corresponds to expected counts at depth i in
the derivation tree. With definitions of context and
outcome for which very deep derivations are im-
probable, it is reasonable to approximate this sum
by truncation.
Our generation model operates in three phases.
1. Chose a start context c
0
? C
w
?
2. Generate a spine S of contexts and outcomes
connecting c
0
to the root context
3. Fill in the full derivation tree T below all re-
maining unexpanded contexts
In the first phase, c
0
is sampled from the multi-
nomial
P
1
(c
0
) =
E(c
0
)
?
c?C
w
?
E(c)
(5)
The second step produces a spine S, which is
formally an ordered list of triples. Each element
of S records a context c
i
, an outcome o
i
, and the
index k in I(c
i
, o
i
) of the child along which the
spine progresses. The members of S are sampled
independantly given the previously sampled con-
text, starting from c
0
and terminating when the
root context is reached. Intuitively this is equiv-
alent to generating the path from the root to c
0
in
a bottom up fashion.
We define the probability P
?
of a triple
(c
i
, o
i
, k) given a previously sampled context c
j
128
as
P
?
({c
i
, o
i
, k}|c
j
) ?
{
E(c
i
)P
V
(o
i
|c
i
) I(c
i
, o
i
)[k] = c
j
0 otherwise
(6)
Let S = (c
1
, o
1
, k
1
) . . . (c
n
, o
n
, k
n
) be the re-
sults of this recursive sampling algorithm, where
c
n
is the root context, and c
1
is the parent context
of c
0
. The total probability of a spine S is then
P
2
(S|c
0
) =
|S|
?
i=1
E(c
i
)P
V
(o
i
|c
i
)
Z
i?1
(7)
Z
i?1
=
?
(c,o)?I
c
i?1
E(c)P
V
(o|c)#(c
i?1
, c, o)
(8)
where I
c?1
is the set of all (c, o) for which
P
?
(c, o, k|c
i?1
) is non-zero for some k. A key
observation is that Z
i?1
= E(c
i?1
), which can-
cels nearly all of the expected counts from the full
product. Along with the fact that the expected
count of the root context is one, the formula sim-
plifies to
P
2
(S|c
0
) =
|S|
?
i=1
P
V
(o
i
|c
i
)
E(c
0
)
(9)
The third step generates a final tree T by fill-
ing in subtrees below unexpanded contexts on the
spine S using the original generation algorithm,
yielding results with probability
P
3
(T |S) =
?
(c,o)?T/S
P
V
(o|c) (10)
where the set T/S includes all contexts that are
not ancestors of c
0
, as their outcomes are already
specified in S.
We validate this algorithm by considering its
distrubution over complete derivation trees T ?
T
w
?
. The algorithm generates ? = (T, S, c
0
) and
has a simple functional mapping into T
w
?
by ex-
tracting the first member of ? .
Combining the probabilities of our three steps
gives
P
w
?
(?) =
E(c
0
)
?
c?C
w
?
E(c)
|S|
?
i=1
P
V
(o
i
|c
i
)
E(c
0
)
?
(c,o)?T/S
P
V
(o|c)
P
w
?
(?) =
P
V
(T )
?
c?C
w
?
E(c)
=
1
?
P
V
(T ) (11)
where ? is a constant and
P
V
(T ) =
?
(c,o)?T
P
V
(o|c)
is the probability of T under the original model.
Note that several ? may map to the same T by
using different spines, and so
P
w
?
(T ) =
?(T )
?
P
V
(T ) (12)
where ?(T ) is the number of possible spines, or
equivalently the number of contexts c ? C
w
?
in T .
Recall that our goal is to efficiently emulate the
output of a rejection sampler. An ideal system P
w
?
would produce the complete set of derivation trees
accepted by the rejection sampler using P
V
, with
probabilities of each derivation tree T satisfying
P
w
?
(T ) ? P
V
(T ) (13)
Consider the implications of the following as-
sumption
A each T ? T
w
?
contains exactly one c ? C
w
?
A ensures that ?(T ) = 1 for all T , unifying Equa-
tions 12 and 13. A does not generally hold in prac-
tice, but its clear exposition allows us to design
models for which it holds most of the time, lead-
ing to a tight approximation.
The most important consideration of this type is
to limit redundancy in C
w
?
. For illustration con-
sider a dependency grammar model with parent
annotation where a context is the current word and
its parent word. When specifying C
w
?
for a partic-
ular w
?
, we might choose all contexts in which w
?
appears as either the current or parent word, but
a better choice that more closely satisfies A is to
choose contexts where w
?
appears as the current
word only.
129
END
END
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivat ng its empiri-
cal study is a difficult task. While many language
models used in statistical NLP ar generative and
can easily produce sample sentences from distri-
bu ions estim ed rom d ta, if all that is required
is a plausible sentence one might as well pick one
at random from any exi ting corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freefor Generation fro a Fixed Vocabulary
bstract
e investigate data driven natural lan-
guage generation under the constraint that
all ords ust co e fro a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified ord
ust also appear in the sentence. e
present fast approxi ations to the ideal re-
jection sa plers and increase variability in
generated text through controlled s ooth-
ing.
I tr cti
JJ
s li s i s
li
l
JJ
big
JJ S
big dogs
ata driven atural anguage eneration
( ) is a fascinating topic explored by aca-
de ics and artists alike, but otivating its e piri-
cal st is a iffic lt tas . ile a la a e
els se i statistical are e erati e a
ca easil r ce sa le se te ces fr istri-
ti s sti t r t , if ll t t is r ir
is l si l s t i t s ll i
t r fr i ti r .
i f l tr i t r li
t l rt i l i l t r li .
j it li t ti t i t
t t , i t it
i ti l . t i ti
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations t the ideal re-
jection samplers and increase variability in
generated text thro gh controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she d gs
ROOT VBZ NNS
lik s dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-v cab, but specifies the inclusion of a single ar-
bitrary wo d somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
he dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
c n easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when co straints are applied such
that only certain plausible sentences are valid. The
ajority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximatio s to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at r ndom from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first s ts a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, w re students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabul ry. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific c nstraints concern-
i g the word that are all wed in a s ntence. The
first sets a fixed vocabulary such that only sen-
tences where ll words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge f arbitrary words are required. This use
Freeform Generatio fro a Fixed Vocabulary
Abstract
We inves igate data driven natural lan-
guage g ne ation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specifi d word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 In roduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We inves gate data driven natural l -
guage generation u der the constraint tha
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natur l Language Generation
(NLG) is a fascinati g topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is us ful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The seco d demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Figure 2: The generation system SPINEDEP draws on ep n ency tree sy t x where we use the term
node to refer to a POS/word pair. Contexts consist of a nod , its parent ode, and grandparent POS tag,
as shown in squares. Outcomes, shown in squares with rounded right s des, are full lists of dependents
or the END symbol. The shaded rectangles contain the results o I(c, o) from the indicated (c, o) pair.
6 Experiments
We train our models on sentences drawn from the
Simple English Wikipedia
1
. We obtained these
sentences from a data dump which we liberally fil-
tered to remove items such as lists and sentences
longer than 15 words or shorter then 3 words. We
parsed this data with the recently updated Stanford
Parser (Socher et al., 2013) to Penn Treebank con-
stituent form, and removed any sentence that did
not parse to a top level S containing at least one
NP and one VP child. Even with such strong fil-
ters, we retained over 140K sentences for use as
training data, and provide this exact set of parse
trees for use in future work.
2
Inspired by the application in language educa-
tion, for our vocabulary list we use the English Vo-
cabulary Profile (Capel, 2012), which predicts stu-
dent vocabulary at different stages of learning En-
glish as a second language. We take the most ba-
sic American English vocabulary (the A1 list), and
retrieve all inflections for each word using Sim-
pleNLG (Gatt and Reiter, 2009), yielding a vocab-
ulary of 1226 simple words and punctuation.
To mitigate noise in the data, we discard any
pair of context and outcome that appears only once
in the training data, and estimate the parameters of
the unconstrained model using EM.
6.1 Model Comparison
We experimented with many generation models
before converging on SPINEDEP, described in
Figure 2, which we use in these experiments.
1
http://simple.wikipedia.org
2
data url anon for review
Corr(%) % uniq
SPINEDEP unsmoothed 87.6 5.0
SPINEDEP WordNe 78.3 32.5
SPINEDEP word2vec 5000 72.6 52.9
SPINEDEP word2vec 500 65.3 60.2
KneserNey-5 64.0 25.8
DMV 33.7 71.2
Figure 3: System comparison based on human
judged correctness and the percentage of unique
sentences in a sample of 100K.
SPINEDEP uses dependency grammar elements,
with parent and grandparent information in the
contexts to capture such distinctions as that be-
tween main and clausal verbs. Its outcomes are
full configurations of dependents, capturing co-
ordinations such as subject-object pairings. This
specificity greatly increases the size of the model
and in turn reduces the speed of the true rejection
sampler, which fails over 90% of the time to pro-
duce an in-vocab sentence.
We found that large amounts of smoothing
quickly diminishes the amount of error free out-
put, and so we smooth very cautiously, map-
ping words in the contexts and outcomes to
fine semantic classes. We compare the use
of human annotated hypernyms from Word-
net (Miller, 1995) with automatic word clusters
from word2vec (Mikolov et al., 2013), based on
vector space word embeddings, evaluating both
500 and 5000 clusters for the latter.
We compare these models against several base-
line alternatives, shown in Figure 3. To determine
130
correctness, used Amazon Mechanical Turk, ask-
ing the question: ?Is this sentence plausible??. We
further clarified this question in the instructions
with alternative definitions of plausibility as well
as both positive and negative examples. Every sen-
tence was rated by five reviewers and its correct-
ness was determined by majority vote, with a .496
Fleiss kappa agreement. To avoid spammers, we
limited our hits to Turkers with an over 95% ap-
proval rating.
Traditional language modeling techniques such
as such as the Dependency Model with Va-
lence (Klein and Manning, 2004) and 5-gram
Kneser Ney (Chen and Goodman, 1996) perform
poorly, which is unsurprising as they are designed
for tasks in recognition rather than generation. For
n-gram models, accuracy can be greatly increased
by decreasing the amount of smoothing, but it be-
comes difficult to find long n-grams that are com-
pletely in-vocab and results become redundant,
parroting the few completely in-vocab sentences
from the training data. The DMV is more flex-
ible, but makes assumptions of conditional inde-
pendence that are far too strong. As a result it
is unable to avoid red flags such as sentences not
ending in punctuation or strange subject-object co-
ordinations. Without smoothing, SPINEDEP suf-
fers from a similar problem as unsmoothed n-gram
models; high accuracy but quickly vanishing pro-
ductivity.
All of the smoothed SPINEDEP systems show
clear advantages over their competitors. The
tradeoff between correctness and generative ca-
pacity is also clear, and our results suggest that the
number of clusters created from the word2vec em-
beddings can be used to trace this curve. As for the
ideal position in this tradeoff, we leave such deci-
sions which are particular to specific application to
future work, arbitrarily using SPINEDEP WordNet
for our following experiments.
6.2 Fixed Vocabulary
To show the tightness of the approximation pre-
sented in Section 4.2, we evaluate three settings
for the probabilities of the pruned model. The first
is a weak baseline that sets all distributions to uni-
form. For the second, we simply renormalize the
true model?s probabilities, which is equivalent to
setting G(c) = 1 for all c in Equation 2. Finally,
we use our proposed method to estimate G(c).
We show in Figure 4 that our estimation method
Corr(%) -LLR
True RS 79.3 ?
Uniform 47.3 96.2
G(c) = 1 77.0 25.0
G(c) estimated 78.3 1.0
Figure 4: A comparison of our system against both
a weak and a strong baseline based on correctness
and the negative log of the likelihood ratio mea-
suring closeness to the true rejection sampler.
more closely approximates the distribution of the
rejection sampler by drawing 500K samples from
each model and comparing them with 500K sam-
ples from the rejection sampler itself. We quantify
this comparison with the likelihood ratio statistic,
evaluating the null hypothesis that the two sam-
ples were drawn from the same distribution. Not
only does our method more closely emulate that of
the rejection sampler, be we see welcome evidence
that closeness to the true distribution is correlated
with correctness.
6.3 Word Inclusion
To explore the word inclusion constraint, for each
word in our vocabulary list we sample 1000 sen-
tences that are constrained to include that word
using both unsmoothed and WordNet smoothed
SPINEDEP. We compare these results to the ?Cor-
pus? model that simply searches the training data
and uniformly samples from the existing sentences
that satisfy the constraints. This corpus search ap-
proach is quite a strong baseline, as it is trivial to
implement and we assume perfect correctness for
its results.
This experiment is especially relevant to our
motivation of language education. The natural
question when proposing any NLG approach is
whether or not the ability to automatically produce
sentences outweighs the requirement of a post-
process to ensure goal-appropriate output. This
is a challenging task in the context of language
education, as most applications such as exam or
homework creation require only a handful of sen-
tences. In order for an NLG solution to be appro-
priate, the constraints must be so strong that a cor-
pus search based method will frequently produce
too few options to be useful. The word inclusion
constraint highlights the strengths of our method
as it is not only highly plausible in a language ed-
131
# < 10 # > 100 Corr(%)
Corpus 987 26 100
Unsmooth 957 56 89.0
Smooth 544 586 79.0
Figure 5: Using systems that implement the word
inclusion constraint, this table shows the number
of words for which the amount of unique sentences
out of 1000 samples was less than 10 or greater
than 100, along with the correctness of each sys-
tem.
ucation setting but difficult to satisfy by chance in
large corpora.
Figure 5 shows that the corpus search approach
fails to find more than ten sentences that obey the
word inclusion constraints for most target words.
Moreover, it is arguably the case that unsmoothed
SPINEDEP is even worse due to its inferior cor-
rectness. With the addition of smoothing, how-
ever, we see a drastic shift in the number of words
for which a large number of sentences can be pro-
duced. For the majority of the vocabulary words
this model generates over 100 sentences that obey
both constraints, of which approximately 80% are
valid English sentences.
7 Conclusion
In this work we address two novel NLG con-
straints, fixed vocabulary and fixed vocabulary
with word inclusion, that are motivated by lan-
guage education scenarios. We showed that un-
der these constraints a highly parameterized model
based on dependency tree syntax can produce a
wide range of accurate sentences, outperforming
the strong baselines of popular generative lan-
guage models. We developed a pruning and es-
timation algorithm for the fixed vocabulary con-
straint and showed that it not only closely approx-
imates the true rejection sampler but also that the
tightness of approximation is correlated with hu-
man judgments of correctness. We showed that
under the word inclusion constraint, precise se-
mantic smoothing produces a system whose abili-
ties exceed the simple but powerful alternative of
looking up sentences in large corpora.
SPINEDEP works surprisingly well given the
widely held stigma that freeform NLG produces
either memorized sentences or gibberish. Still, we
expect that better models exist, especially in terms
of definition of smoothing operators. We have pre-
sented our algorithms in the flexible terms of con-
text and outcome, and clearly stated the properties
that are required for the full use of our methodol-
ogy. We have also implemented our code in these
general terms
3
, which performs EM based param-
eter estimation as well as efficient generation un-
der the constraints discussed above. All systems
used in this work with the exception of 5-gram in-
terpolated Kneser-Ney were implemented in this
way, are included with the code, and can be used
as templates.
We recognize several avenues for continued
work on this topic. The use of form-based con-
straints such as word inclusion has clear applica-
tion in language education, but many other con-
straints are also desirable. The clearest is perhaps
the ability to constrain results based on a ?vocab-
ulary? of syntactic patterns such as ?Not only ...
but also ...?. Another extension would be to incor-
porate the rough communicative goal of response
to a previous sentence as in Wu et al. (2013) and
attempt to produce in-vocab dialogs such as are
ubiquitous in language education textbooks.
Another possible direction is in the improve-
ment of the context-outcome framework itself.
While we have assumed a data set of one deriva-
tion tree per sentence, our current methods eas-
ily extend to sets of weighted derivations for each
sentence. This suggests the use of techinques that
have proved effective in grammar estimation that
reason over large numbers of possible derivations
such as Bayesian tree substitution grammars or un-
supervised symbol refinement.
References
Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
A. Capel. 2012. The english vocabulary profile.
http://vocabulary.englishprofile.org/.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full face poetry generation. In Proceedings of the
3
url anon for review
132
Third International Conference on Computational
Creativity, pages 95?102.
Albert Gatt and Ehud Reiter. 2009. Simplenlg: A re-
alisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation, ENLG ?09, pages 90?93,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry with
applications to generation and translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 524?533, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Long Jiang and Ming Zhou. 2008. Generating chi-
nese couplets using a statistical mt approach. In
Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1, pages 377?
384. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proceed-
ings of the 42Nd Annual Meeting on Association for
Computational Linguistics, ACL ?04, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Michael I. Miller and Joseph A. Osullivan. 1992. En-
tropies and combinatorics of random branching pro-
cesses and context-free languages. IEEE Transac-
tions on Information Theory, 38.
George A. Miller. 1995. Wordnet: A lexical database
for english. COMMUNICATIONS OF THE ACM,
38:39?41.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013. Generating expressions that refer to visible
objects. In HLT-NAACL, pages 1174?1184.
Jack Mostow and Hyeju Jang. 2012. Generating di-
agnostic multiple choice comprehension cloze ques-
tions. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 136?146. Association for Computational Lin-
guistics.
G?ozde
?
Ozbal, Daniele Pighin, and Carlo Strapparava.
2013. Brainsup: Brainstorming support for creative
sentence generation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1446?
1455, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Ananth Ramakrishnan A, Sankar Kuppan, and
Sobha Lalitha Devi. 2009. Automatic generation
of tamil lyrics for melodies. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, pages 40?46. Association for Compu-
tational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In ACL.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring non-native speak-
ers? proficiency of english by using a test with
automatically-generated fill-in-the-blank questions.
In Proceedings of the second workshop on Building
Educational Applications Using NLP, pages 61?68.
Association for Computational Linguistics.
Alessandro Valitutti, Hannu Toivonen, Antoine
Doucet, and Jukka M. Toivanen. 2013. ?let every-
thing turn well in your wife?: Generation of adult
humor using lexical constraints. In ACL (2), pages
243?248.
Dekai Wu, Karteek Addanki, Markus Saers, and
Meriem Beloucif. 2013. Learning to freestyle: Hip
hop challenge-response induction via transduction
rule segmentation. In EMNLP, pages 102?112.
133
