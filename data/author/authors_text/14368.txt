Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1136?1145,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Gender Inference of Twitter Users in Non-English Contexts
Morgane Ciot
School of Computer Science
McGill University
Montreal, Quebec, Canada
morgane.ciot@mail.mcgill.ca
Morgan Sonderegger
Department of Linguistics
McGill University
Montreal, Quebec, Canada
morgan.sonderegger@mcgill.ca
Derek Ruths
School of Computer Science
McGill University
Montreal, Quebec, Canada
derek.ruths@mcgill.ca
Abstract
While much work has considered the problem
of latent attribute inference for users of social
media such as Twitter, little has been done on
non-English-based content and users. Here,
we conduct the first assessment of latent at-
tribute inference in languages beyond English,
focusing on gender inference. We find that
the gender inference problem in quite diverse
languages can be addressed using existing ma-
chinery. Further, accuracy gains can be made
by taking language-specific features into ac-
count. We identify languages with complex
orthography, such as Japanese, as difficult for
existing methods, suggesting a valuable direc-
tion for future research.
1 Introduction
A 2012 study reported that US-based Twitter users
now account for only 28% of all active accounts on
the platform (Semiocast, 2012). Brazil, Japan, India,
and Indonesia all rank in the top 10, each with over
5% of all users. These and other findings confirm
that Twitter enjoys widespread international popu-
larity and usage. This is also reflected in the multi-
national community of researchers who study hu-
man behavior on Twitter and related platforms, e.g.
(Sakaki et al, 2010; Tumasjan et al, 2010; Kim and
Park, 2012).
It is remarkable, then, that advances in la-
tent attribute inference on social media have been
largely confined to English content, e.g. (Liu and
Ruths, 2013; Zamal et al, 2012; Pennacchiotti and
Popescu, 2011; Conover et al, 2011a). This bias
may be partially explained in the context of the re-
search being conducted largely by anglophone re-
searchers. Nonetheless, it has created a notable
silence in the literature concerning the large-scale
analysis of languages, cultures, and people on social
media who do not employ English.
In this paper, we examine the problem of latent at-
tribute inference outside the English-language con-
text. To our knowledge, this is the first such study
ever conducted. Here we specifically focus on gen-
der inference, as it has been the basis for significant
work in recent years (Liu et al, 2012; Zamal et al,
2012; Pennacchiotti and Popescu, 2011; Rao et al,
2010; Burger et al, 2011). Our work makes two
contributions. First, we quantify the extent to which
established gender inference methods can be used
with non-English Twitter content. Second, we ex-
plore the capacity for unique features of other lan-
guages (besides English) to improve inference ac-
curacy. This second aspect, in particular, acknowl-
edges the fact that latent attribute inference may be
easier in some languages due not to conventions in
word usage, but to syntactic structure.
In order to assess the extent to which existing gen-
der inference machinery works for users who use
languages other than English, we assembled Twit-
ter datasets for languages that are both prevalent on
Twitter and representative of diverse language fam-
ilies: Japanese, Indonesian, Turkish, and French.
Each dataset consisted of approximately 1000 users
who tweeted primarily in a given language. We used
Amazon Mechanical Turk to manually label each
user with their gender, using a language-agnostic la-
beling strategy (Liu and Ruths, 2013). For classi-
fication, we employed a performant support vector
machine-based (SVM) technique that has been used
in a range of studies, e.g. (Rao et al, 2010; Burger
et al, 2011; Zamal et al, 2012).
1136
We found that, without any modification to the
types of features given to the SVM, the classifier ac-
curacy was comparable on English, French, and In-
donesian. Turkish actually performed much better,
achieving 87% on average. Gender in Japanese, in
contrast, could not be reliably inferred with any rea-
sonable accuracy (61% on average) despite numer-
ous attempts to preprocess the tweets and tune the
classifier to accommodate the language?s complex
orthography. This indicates that existing approaches
may not generalize well to language systems with
thousands of distinct unigrams (as opposed to tens
or hundreds in the other languages considered).1
To evaluate the extent to which language-specific
features might be used to boost the accuracy of
the SVM classifier further, we focused on French.
French is a valuable case study because, unlike En-
glish, it has a number of syntax-based mechanisms
that can encode the gender of the speaker. The
most common instantiation of gender marking is the
modification of adjective and some past participle
endings to match the gender of the subject in con-
structions beginning with ?je suis? (trans. ?I am?)
constructions. A classifier based on this insight
achieved average accuracy of 90% on the vast ma-
jority of French users, surpassing the accuracy of
standard techniques on English or French.
Overall, our results show that, with little modifi-
cation, existing gender inference machinery can per-
form comparably to English on several other lan-
guages. There are clear areas for substantial im-
provement: incorporating language-specific features
and, in the case of Japanese, finding better ways
of accommodating the complex orthography. These
findings identify promising directions for future re-
search and will, hopefully, call attention to an im-
portant area in the latent attribute inference domain
in need of further work.
2 Background
Non-English Twitter data mining and studies.
Existing work on non-English Twitter content can
been divided into two groups: surveys of the use of
several languages on the platform and studies of a
social phenomenon in a non-English body of tweets.
1In this work, unigram, bigram, and k-gram refer to one,
two, and k-character sequences in a language?s written form.
To our knowledge, only a handful of the former va-
riety exist. One recent paper characterized the re-
lationship between language and geography (Mo-
canu et al, 2012). Another measured how high-level
tweet features (i.e., link, mention, and hashtag fre-
quencies) vary across languages (Weerkamp et al,
2011). These papers show that tweet structure and
content can differ widely across languages.
More work has been done in the latter category:
analysis of social phenomena in a non-English con-
text. A well-known study evaluated usage of Twit-
ter in the aftermath of the 2010 earthquake in Japan
(Sakaki et al, 2010). Another Japanese-oriented
study evaluated the impact of television on tweeted
content (Akioka et al, 2010). Within this cate-
gory, another recurring research topic is the anal-
ysis of political discussion and elections. Outside
of English-based analysis, some attention has been
given to European and East Asian elections, e.g.
(Tumasjan et al, 2010; Giglietto, 2012; Kim and
Park, 2012). However, few of these studies have
considered measures beyond simple hashtag fre-
quencies, relative mention counts among politicians,
and retweet counts. The only study using more
complex features for computational text analysis in-
volved sentiment analysis of a set of German tweets
(Tumasjan et al, 2010). However, the tweets in
this study (conducted at a German university) were
translated into English prior to analysis, a step which
underscores the significant bias towards English in
the literature on analyzing microtext, and the tools
available to researchers in this domain.
Gender inference methods. Gender inference is a
field of research situated with the broader area of la-
tent attribute inference. The majority of recent work
in this area has focused on Twitter users (Rao et al,
2010; Pennacchiotti and Popescu, 2011; Conover et
al., 2011b; Burger et al, 2011; Rao and Yarowsky,
2010; Liu and Ruths, 2013; Liu et al, 2012; Zamal
et al, 2012). Classifiers have been built, predomi-
nantly, using support vector machines, e.g. (Rao et
al., 2010; Pennacchiotti and Popescu, 2011; Burger
et al, 2011; Zamal et al, 2012), though boosted de-
cision trees and latent dirichlet alocation systems
have also been evaluated, e.g. (Pennacchiotti and
Popescu, 2011; Conover et al, 2011b). With one
exception, gender inference accuracy has been re-
1137
ported between 80% and 85%. The one study which
reported 90% accuracy involved the use of a dataset
which has been shown to be quite different from typ-
ical anglophone Twitter users (Burger et al, 2011).
This same study did involve non-English Twitter
users, but did not analyze the performance of the
classifier on different languages (e.g. break down
performance by language, examine to what extent
its results were due to better performance on some
languages), or indeed discuss fully which languages
were present in their sample. Thus, little can be in-
ferred from Burger et al?s study about the relative
performance of attribute inference methods on dif-
ferent languages, which is the focus of our paper.
Language families. Human languages can be
classified into different language families, defined
as a set of languages which are all descended from a
single, ancient parent language. Languages which
are genetically related (in the same family), how-
ever distantly, tend to share many more character-
istics than languages from different families.
Each language considered in this paper belongs
to a different language family: French to Indo-
European, Turkish to Altaic, Japanese to Japonic,
and Indonesian to Austronesian. Thus, these lan-
guages are completely genetically unrelated, by def-
inition. Further, they are both geographically and
culturally dispersed. While they all have some loan-
words from English, these constitute a tiny fraction
of each language?s vocabulary. This selection of lan-
guages allows us to conduct the most far-reaching
survey of non-English latent attribute inference per-
formance to date.
A variety of features make each language se-
lected interesting within the gender inference con-
text. French is noteworthy for its grammatical gen-
der. All nouns, including people, are grammati-
cally ?male? or ?female.? English, in contrast, has
separate pronouns for people of different genders
(e.g., ?he?, ?she?), but does not have grammati-
cal gender. (Besides a handful of exceptions like
?waiter?/?waitress?, there are no words besides pro-
nouns which have different ?masculine? and ?femi-
nine? forms.) Indonesian, Turkish, and Japanese are
all so-called genderless languages. Like many lan-
guages of the world, they do not have distinct male
and female pronouns (like English and French), or
grammatical gender (like French).
3 General Gender Inference
In order to evaluate the extent to which existing gen-
der inference machinery can be used on users whose
tweets are in languages other than English, we de-
veloped gender-labeled datasets of Twitter users for
each language and then evaluated the performance
of a classifier on each.
3.1 Data
The core data for this project consisted of four
datasets of content from Twitter users who tweeted
predominantly in one of four languages?French,
Indonesian, Turkish, and Japanese?collected using
the methods described below.
Data collection. In order to identify users for can-
didate inclusion in a particular language?s (hereafter
the target language) dataset, we walked the stream-
ing output of the Twitter firehose and evaluated the
language of each tweet using the language mod-
els provided by the Natural Language Toolkit (Bird,
2006). Users associated with tweets written in the
target language were added to a list. 5000 such users
were identified for each language. The latest 1000
tweets for each user were downloaded. This com-
prised the base for the target language dataset.
Assigning gender labels. In prior work, e.g. (Rao
et al, 2010; Pennacchiotti and Popescu, 2011; Za-
mal et al, 2012), the dominant way of obtain-
ing datasets consisting of Twitter users with high-
confidence gender-labels is to use gender-name as-
sociations. The use of name-gender associations are
problematic when non-English content is considered
because databases of anglophone name-gender asso-
ciations are no longer useful (Mislove et al, 2011).
We instead used Amazon Mechanical Turk workers
to identify the gender of the person shown in the pro-
file picture associated with a user?s account (Liu and
Ruths, 2013). In our datasets, each user?s profile pic-
ture was coded by 5 separate workers. Users with
non-photographic or celebrity-based profile pictures
was discarded, as well as any users with profile pic-
tures where the gender could not be confidently as-
sessed (less than 4 out of 5 votes for one gender).
Table 1 shows the final composition of each
dataset. In Japanese and Indonesian, we observed
1138
Table 1: The composition of the different language
datasets used in this study.
Language # Males # Females Total Size
French 437 506 943
Indonesian 977 2260 3237
Turkish 1672 1937 3609
Japanese 309 520 829
a notable difference in the number of males and fe-
males in the dataset. Measures were taken to ensure
that classifier results were not biased by these differ-
ences within the datasets.
3.2 Methods
The majority of prior work in gender inference (and
latent inference in general) has used support vector
machines (SVMs). We followed prior work in this
regard, particularly since our intent here is to eval-
uate the relevance of existing gender inference ma-
chinery on other languages. For the present study,
we adopted an SVM-based classifier, described in
(Zamal et al, 2012), that incorporated nearly all fea-
tures used in prior work and showed comparable
(and sometimes better) accuracy than other methods.
Parameter values and kernel choices for the SVM
are discussed in the source paper.
Feature set. SVM classifiers require that
each object to be classified be represented by
a fixed-length feature vector. The features we
employed were: k-top words, k-top digrams
and trigrams, k-top hashtags, k-top mentions,
tweet/retweet/hashtag/link/mention frequencies,
and out/in-neighborhood size. Note that ?k-top X
features? (e.g., k-top hashtags) refers to the k most
discriminating items of that type for each label (i.e.,
Male/Female). Thus, k-top words is actually 2k
features: the k words most associated with males
and the k words most associated with females.
This list of features is the same set of features used
in (Zamal et al, 2012), except that k-top stems and
k-top co-stems were both dropped in our version.
Both of these feature types are specific to English.
Of course, word stems do exist in other languages,
however we found that stemmers (the algorithms
that identify and extract the appropriate stem from
a word) were not available across the whole bank
of languages. Therefore, we omitted these stem and
co-stem features. We also added features for the us-
age frequencies of Eastern-style and Western-style
emoticons but saw no discernible change in accu-
racy; thus, these features are not discussed further.
It is important to note that all features included
in our classifier are language-agnostic. An n-gram
is simply an n-character sequence drawn from the
alphabet and additional symbols (numbers, punctu-
ation, etc.) present in tweets written in the target
language. Words are sequences of characters that
are bounded by whitespace or punctuation. Hash-
tags are words proceeded by a pound (?#?) character,
mentions by an ?@? symbol. A system that properly
supports unicode strings can implement all of these
notions without knowing anything about the target
language it is operating on.
Tokenization of Japanese. While all the defini-
tions provided above for the SVM features are op-
erational, there is a glaring disconnect between the
whitespace-border definition of a word and written
conventions in Japanese. Specifically, in Japanese
words are generally not separated by whitespace.
We used a tokenizer to insert whitespace into
Japanese text to break up words. Tokenization
was done using Kuromoji, the software Japanese
morphological analyzer used and supported by the
Apache software Foundation (Atilika, 2012). No-
tably, this tool tokenizes the mixed character sets
that are often used in informal Japanese writing.
As tokenization does involves some language-
specific processing, its use here somewhat under-
mines the objective set out for this project. Thus, we
report the accuracy achieved for both untokenized
and tokenized Japanese tweets. Curiously, tokeniza-
tion was found to not make a difference in overall
average accuracy.
3.3 Results
For each dataset, 5-fold cross validation was used
to assess the classifier?s performance. The value of
k = 20 was used for all k-top features, though the
results reported are robust to changes of this value
within reason (between 10 and 30). If the num-
bers of male and female users were unbalanced in
a dataset, the larger set was subsampled randomly
to obtain a set of users the same size as the smaller
labeled set. During the training process, the actual
1139
Table 2: The accuracy of the SVM-based classifier on
each of the language datasets. In the case of Japanese,
the performance is given for both the tokenized and un-
tokenized versions of the dataset. (Note that tokenization
did not affect overall accuracy.)
Language Male Female Overall
French 0.79 0.73 0.76
Indonesian 0.87 0.80 0.83
Turkish 0.89 0.85 0.87
Japanese (t) 0.50 0.76 0.63
Japanese (u) 0.58 0.68 0.63
values of the features were extracted from the train-
ing users (e.g., the k-top differentiating words for
males and females were identified). In this way,
the gender model implemented by the SVM was
language-specific, in the sense that a particular lan-
guage?s gender model contained a different set of
features. We call our method language-agnostic on
the grounds that, given a labeled set of users and
tweets drawn from a particular language, a model
can be built without any knowledge of the structure
or content of the language itself.
Tables in Supplementary Material show the fea-
tures for the classifier built over each language?s en-
tire dataset. Note that to conduct the cross-fold eval-
uation, new models (and hence different features)
were recomputed for each fold. As a result, the fea-
tures reported are slightly different from those that
might have appeared in the models for a given fold.
Manual inspection, however, revealed that differ-
ences were slight. The features reported in the Sup-
plementary Material can be safely considered a con-
sensus among the models for the individual folds.
The accuracy of the classifier for each language
is shown in Table 2. Overall, the classifier demon-
strated good performance on all languages ex-
cept for Japanese. Below, we consider the re-
sults for each of the four languages in turn. In
each case we discuss language-specific trends in
which words were most informative for inferring
user gender, and thus help explain the classifier?s
performance. Throughout, we omit discussion of
non-alphanumeric ?words? (such as punctuation or
emoticons), and call the k-top discriminating words
for male and female users the k-top male words and
k-top female words.
French. The k-top words for men and women are
of very different grammatical types. Most male
words are prepositions or articles (16/25; e.g. de
?of?, un ?a/one?); a few others are basic grammati-
cal words (ne ?[part of] not?, et ?and?), or pronouns
or verb forms referring to a single person or object
(he/she/it), as well as one noun (France). In con-
trast, many female words (11/25) are pronouns or
basic verb forms referring to the speaker or a single
addressee (je ?I?, mon/mes/ma ?my?, tu ?you?, j?ai
?I have?). Others are pronouns or basic verbs re-
fer to a single person or object (elle, ?she/it?, c?est
?it?s?), as well as a few other frequent words (trop
?too much?, pas ?[part of] not?, oui ?yes?). The most
salient pattern is that use of words (pronouns, basic
verbs) associated with talking about the speaker or
addressee indicates a tweet is more likely to be from
a female user. Heavy use of other common function
words, specifically prepositions and articles, sug-
gests a male user. These patterns reflect known gen-
der differences in word usage by male and female
French speakers (Witsen, 1981).
Indonesian. Indonesian achieved performance
closest to the inference accuracy for English re-
ported in the literature. The k-top lists for men and
women give some justification for why the classifier
performed well. Some differences can be tentatively
linked to general trends in how men and women use
language differently across cultures. 5/25 of men?s
k-top words are nouns which are either related to
soccer (vs ?versus?, chelsea ?[name of UK soccer
team]?, pemain ?player?) or which could be related
to soccer (jakarta, indonesia, malam ?night?); in
contrast, no women?s words are nouns. It seems
plausible that men tweet about soccer significantly
more than women. In such a situation, a reasonable
concern is that our classifier discriminated soccer
from non-soccer enthusiasts rather than males from
females. To address this, we confirmed that these
topic-based words were not required for accurate
classification: a classifier in which soccer words
were explicitly removed performed just as well
(83.8% vs. 83.3%).
More interestingly, many of the k-top words cor-
respond to men and women using different terms
of address and self-reference. Among the k-top
words, 7/25 for men and 4/25 for women are terms
1140
of address or self-reference. The terms men use are
mostly highly informal, including the slang term lu
(you) and the English borrowing bro; the address
terms women use are mostly medium-formality,
such as aku (I) and kamu (you). Thus, women seem
to be using ?more polite? self-reference and address
terms than men on average on Indonesian Twitter,
in line with the more general tendency for women
to use polite forms more frequently than men cross-
culturally (Holmes, 1995).
Turkish. Turkish achieved notably high accuracy:
the highest of all four languages considered. In
fact, to our knowledge, this is the highest accuracy
achieved in the entire Twitter gender inference lit-
erature on a dataset drawn from the Twitter gen-
eral population. The k-top lists of male and female
words again give some justification for the classi-
fier?s performance. Many differences between the
male and female lists can be linked to men and
women talking about different topics, or to differ-
ent people. Several of the male words refer to soc-
cer (gol ?goal?, galatasaray ?popular Istanbul team?,
mac? ?match?, at ?[part of imperative for] score?),
which men plausibly tweet about more. As with In-
donesian, a concern is that topics represent a biased
sample of the population. Thus, we tested a classi-
fier with soccer-specific terms removed, and again
found no difference in accuracy (86% vs. 87%).
Many other k-top words are familiar terms of ad-
dress for men (lan, abi, karde sim, adam, kanka) or
a greeting used mainly between men (eyvallah), sug-
gesting that male users are addressing or discussing
men more often than female users are. In contrast,
9/25 of the k-top female words are pronouns refer-
ring to the speaker, a familiar addressee, or a third
party (he/she/it), while none of the k-top male words
are, suggesting female users are more often talking
directly about themselves or to others. Finally, 2/25
of the k-top male words are profanity (amk, ulan),
while none of the female k-top words are, suggest-
ing male users swear more.
Japanese. Beyond the Japanese classifier?s gener-
ally poor accuracy, it is striking that tokenization
did not improve overall accuracy. This indicates
that once words were properly tokenized, no ad-
ditional gender-distinguishing signal could be ex-
tracted. This may be an indication that word-based
features carry little information in languages with
complex orthography, such as Japanese (with many
thousands of unigrams).
Despite the classifier?s poor performance, the k-
top discriminating words for male and female users
differ in interesting ways. Some differences can be
understood as resulting from known general trends
in how Japanese men and women?s use language.
Japanese speakers have a choice of many first-
person singular pronouns (equivalent to ?I?), which
signal different levels of politeness and of male ver-
sus female speech. The pronoun boku (?) is asso-
ciated with informal male speech; accordingly, it is
among the k-top male words. Japanese also uses an
extensive system of verb forms corresponding to dif-
ferent levels of politeness, and honorifics (affixes for
names used when referring to others). Women tend
to use polite verb forms and honorifics more fre-
quently than men in Japanese speech (Peng, 1981).
In agreement with this pattern, several polite verb
forms (-masu, -mashi) and a polite honorific (o-) are
among the k-top female words, as is a diminutive
honorific often used to refer to women (-chan).
4 Language-specific Features and
Inference
While the classifier performed well across a diverse
set of languages, recall that all features used by the
SVM were language-agnostic. A natural question
concerns the extent to which language-specific fea-
tures relevant to the attribute of interest (e.g., gen-
der) might improve the classifier?s performance.
We examine this question within the context of
French. Where gender inference is concerned,
French is quite interesting because information
about the gender of nouns (including the speaker) is
often obligatorily marked in the syntax: many words
have different ?masculine? and ?feminine? forms for
referring to male and female nouns, including the
speaker. Thus, it is in principle often possible to in-
fer the gender of the speaker by which form they use,
although it is not clear a priori that this method will
work for Twitter data.
4.1 Method
French grammar dictates that which forms of words
are used often reflects the gender of the speaker.
1141
Adjectives and past participles all have masculine
and feminine forms, which are often spelled dif-
ferently, and in addition often pronounced differ-
ently.Adjectives must agree in gender with the noun
they refer to. For example, ?I am happy? would
be je suis heureuse for a female speaker and je suis
heureux for a male speaker (literally ?I-am-happy?);
heureuse and heureux are the feminine and mascu-
line singular forms of the adjective, and are pro-
nounced differently. Past participles of verbs also
agree with the gender of the subject or object of the
verb, for certain verbs and constructions. For ex-
ample, ?I went? would be je suis alle?e for a female
speaker and je suis alle? for a male speaker (here suis
is used to form the simple past of the verb aller,
?to go?); alle? and alle?e are the masculine and fem-
inine forms of the past participle of aller, and are
pronounced the same.
Note that the phrase je suis (?I am?) occurs in both
the adjectival and verbal constructions referring to
the speaker; however, the function of suis differs be-
tween the two. suis is the first-person singular form
of the verb e?tre (?to be?), and functions as a cop-
ula when followed by an adjective (?I am happy?)
but as an auxiliary verb to mark the past tense, when
followed by the past participle of certain verbs (?I
went?). For our purposes, what is important is that,
in both cases, a following adjective or past participle
will take on the gender of the speaker.
When this construction occurs in a tweet, it is
likely that je is referring to the author of the tweet,
and the rules of French grammar dictate that the
gender of the associated adjective or past partici-
ple should reflect the gender of the tweet?s author.
We implemented a classifier that used this logic to
classify the gender of francophone Twitter users. It
is worth emphasizing that the existence of adjec-
tives and participles which reflect the speaker?s gen-
der does not automatically make gender identifica-
tion in French tweets a trivial task. First, given the
prevalence of non-standard spelling and grammar
on Twitter and other online platforms, French users
may sometimes not use the ?correct? gender marked
form reflecting their actual gender?especially given
that the male and female forms for a given adjective
or participle are often pronounced the same. Sec-
ond, even if gender-marked constructions are used
correctly, they may not occur sufficiently often in
Table 3: The set of patterns that were considered to be
suis-constructions when encountered in a tweet.
jn suis pas, jm suis, jmsuis, jnmsuis pas, jnsuis pas,
je ne suis pas, je suis pas, jsuis, jensuis pas, jemsuis,
jnesuis pas, jmesuis, je me suis, je ne me suis pas
tweets to be a reliably used for speaker gender iden-
tification. Both of these concerns are borne out in
our French dataset, as described further below; the
question addressed in the experiment is how useful
the signal provided by gender-marked forms is, de-
spite these two sources of noise.
Unlike the probabilistic SVM classifier, the suis-
construction classifier can be made entirely deter-
ministic. For a given user, the set of tweets con-
taining a suis-construction are identified, Tsuis(u).
Of these, we can identify the number of those tweets
that involve an adjective or past participle with a fe-
male ending TFsuis(u) ? Tsuis(u). Labeling a user
involves selecting a threshold based on TFsuis(u) and
Tsuis(u) below which a user receives one label and
above which the user receives the other label.
Detecting suis-constructions. As expected, cur-
sory inspection of tweets revealed that Twitter
users often employed shorthand forms of the suis-
construction. We accounted for this by conduct-
ing a manual survey of the shorthand forms of
the suis-construction. A catalog of regular expres-
sions was drawn up that matched the different suis-
construction forms we identified, shown in Table 3.
Recognizing the gender of the adjective or past
participle involved in a suis-construction required
a second processing stage. The Lexique lexical
database was used to tag the word trailing the suis-
construction (New and Landing, 2012). If the tag
was not an adjective or verb, the construction was
discarded as it would not contain a gender indica-
tion. If the word was recognized as an adjective or
verb, Lexique would also return the gender, which
would be returned as the gender indication for that
particular suis-construction.
Threshold selection. We evaluated a number of
policies for assigning the user?s gender based on
the relative values of TFsuis(u) and Tsuis(u). In the
end, however, the best performing threshold was
TFsuis(u) > 1: simply labeling as female any user
1142
Table 4: The component-wise and overall accuracy of the
combined suis-construction and SVM classifier.
Component # Male Female Overall
users Acc Acc Acc
suis-const. 723 0.91 0.90 0.90
SVM 220 0.70 0.54 0.62
Overall 943 0.86 0.82 0.83
who employed the female construction even once.
This threshold makes sense given the plausible intu-
ition that females will (almost always) be the only
users to employ a female suis-construction; how-
ever, it is quite sensitive to uses of female suis-
constructions by males.
Mixed classifier. Since not all users had tweets
which contained suis-constructions, we combined
the SVM-based classifier used previously with the
suis-construction-based classifier. The SVM com-
ponent was applied to any users who lacked suis-
constructions entirely in their tweet history. Any
user who used even one suis-construction would be
labeled according to the TFsuis(u) > 1 threshold.
4.2 Results
We ran our classifier on the French dataset, obtain-
ing the results shown in Table 4.
Coverage of the suis-construction. In spite of
our concerns over the occurrence frequency and de-
tectability of the suis-construction in tweets, our
results show that suis-constructions were found in
tweets belonging to nearly 75% of all users in the
dataset. This suggests that the suis-construction
classifier has quite broad coverage of the popula-
tion. Of course, given the essential role of the verb
?e?tre? in French (like the role of ?to be? in En-
glish), its frequent use is expected. Nonetheless,
the flexible use of grammar and spelling in Twitter
and other online contexts raised a genuine concern
that occurrences of the suis-construction might not
be detected. In fact, when we looked through the
tweets of users who were flagged as not having use-
ful suis-constructions in their tweets, we discovered
that many actually did. The issue was that they em-
ployed highly irregular spellings that our implemen-
tation was not able to pick up. Thus, with additional
refinement, it may be possible to improve the suis-
construction coverage further, well beyond 80%.
Performance of the suis-construction classifier.
On the set of users for which the suis-construction
was detected, the classifier did very well, achiev-
ing an average accuracy of 90%. Recall that the
threshold used to generate the results in Table 4 was
TFsuis(u) > 1. We tested other (larger) thresholds
and found that the performance of the method dra-
matically and monotonically decreased. This was
largely due to female users being misclassified as
males, indicating that females do not exclusively
use female suis-constructions (this was confirmed
via manual inspection of a number of female tweet
histories). This is different from males, most of
whom are quite strict about using only male suis-
constructions. Since forming the female form of
an adjective or participle typically requires adding
an additional character (or more) to the base of the
word, this may reflect a tendency towards dropping
gender modifiers in favor of typing less.
Performance of the SVM classifier. While the
suis-construction classifier performed well, the
SVM component did not do nearly as well on the
Twitter users that could not be labeled using the suis-
construction, achieving an average performance of
62%. At this level of accuracy, the classifier is per-
forming barely better than a random classifier, which
would have achieved around 50% accuracy on the
label-balanced testing data. This result stands in
opposition to our earlier finding that French users
could be labeled with 75% accuracy. This disparity
suggests that the non-suis-construction users com-
prise a particularly difficult-to-classify group.
The suis-construction as a filter. The finding that
the SVM classifier performed poorly in the com-
bined classification setting suggests that the suis-
construction classifier is acting as a very effective fil-
ter for users that are hard for it to classify. While we
might have preferred better classification accuracy
all around, this result is still interesting and useful.
Such filters can decrease classification error by sim-
ply flagging those users who cannot be easily clas-
sified, leaving them to be handled more carefully by
more powerful classifiers or human coding. This is
precisely the function that the suis-construction clas-
sifier appears to play (in addition to classifying the
1143
other users).
This result suggests a question for future work:
whether it is possible to build classifiers that accu-
rately label the sets of users that are discarded by
the suis-construction classifier.
Performance of the combined classifier. Despite
the relatively poor performance of the SVM com-
ponent, the accuracy of the combined classifier im-
proved on the original SVM-only classifier by 8%,
which is a substantial increase in accuracy. With
some additional focus on classifying the difficult
users who could not be labeled by suis-construction
usage, we feel that this accuracy can be increased
upwards of 90%.
5 Discussion
In this project, we have extended, for the first time,
the latent attribute inference problem to users who
tweet primarily in languages other than English. Our
study offers several notable insights.
Existing approaches generalize. While accuracy
levels certainly vary across languages, overall an ex-
isting SVM-based classifier, when trained on users
from a given language, can classify the gender of
other users from that same language with accuracy
comparable to performance reported for English.
We suspect that this result will generalize to the in-
ference of other demographic characteristics (e.g.,
age and political orientation), though this must be
explored in future work.
Complex orthography creates unique issues.
Japanese stands out as being utterly unclassifiable
using existing SVM-based approaches and feature
sets. Even efforts to bridge some of the orthographic
disconnects between the Japanese language and the
assumptions made by the SVM failed to improve
performance. This stands out as a clear direction for
future work, particularly since apparent issues with
the large number of unigrams used by Japanese will
create issues for handling (Mandarin) Chinese, the
world?s most-spoken language.
Language-specific features boost performance.
While unsurprising that customizing a classifier to
the peculiarities of a given language boosts perfor-
mance, our use of the suis-construction in French
highlights how particular linguistic features may be
uniquely well suited to the inference of particular
attributes. The results obtained for French stand in
contrast to various, relatively unsuccessful attempts
to boost gender inference by incorporating syntac-
tic features of English into the classifier (e.g., us-
ing stems and co-stems). It seems that some lan-
guages have features better suited for certain classi-
fication tasks. Identifying and leveraging such fea-
tures will be an interesting and fruitful direction for
future work.
Classifiers as a linguist?s tool. In each language,
a number of the k-top words align with or sug-
gest gender-specific conventions in that particular
language. That a language-agnostic classifier pro-
vided such insights highlights its potential for ex-
ploring language-specific word usage patterns and
nuances. For example, sociolinguistics (a subfield of
linguistics) has long studied the different ways men
and women use language, especially in spontaneous
speech (Eckert and McConnell-Ginet, 2003); recent
work has begun to examine how language is used
differently by men and women online as well (Bam-
man et al, 2012). Such studies could be radically
scaled up in terms of the number of languages con-
sidered using a language-agnostic gender classifier.
6 Conclusion
Though there has been relatively little investigation
into latent attribute inference outside of English-
language content, we consider it both a fruitful
and important area for future research. Here, we
have evaluated the capacity for existing inference
methods to be used outside their intended English-
language context. Furthermore, we have shown how
language-specific features might be incorporated in
order to boost classifier accuracy further. The posi-
tive results suggest that latent attribute inference in
the non-English context as a research direction wor-
thy of further attention.
7 Acknowledgements
The authors gratefully acknowledge three anony-
mous reviewers whose feedback improved the clar-
ity and correctness of the manuscript. The study
was supported by grants from the Social Sciences
1144
and Humanities and Natural Sciences and Engi-
neering Research Councils of Canada (SSHRC In-
sight Grant #435-2012-1802 and NSERC Discovery
Grant #125517855) and the Public Safety Canada
Kanishka Program.
References
S. Akioka, N. Kato, Y. Muraoka, and H. Yamana. 2010.
Cross-media impact on Twitter in Japan. In Proceed-
ings of the International Workshop on Search andMin-
ing User-generated Contents.
Atilika. 2012. Kuromoji morphological analyzer.
http://www.atilika.org.
D. Bamman, J. Eisenstein, and T. Schnoebelen. 2012.
Gender in Twitter: Styles, stances, and social net-
works. arXiv preprint arXiv:1210.4567.
S Bird. 2006. Nltk: the natural language toolkit. In Pro-
ceedings of the COLING/ACL Interactive Presentation
Sessions.
J.D. Burger, J. Henderson, and G. Zarrella. 2011. Dis-
criminating gender on Twitter. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
M. Conover, B. Gonc?alves, J. Ratkiewicz, A. Flammini,
and F. Menczer. 2011a. Predicting the politial align-
ment of Twitter users. In Proceedings of the Interna-
tional Conference on Social Computing.
M.D. Conover, J. Ratkiewicz, M. Francisco,
B. Gonc?alves, F Menczer, and A Flammini. 2011b.
Political polarization on Twitter. In Proceedings of
the International Conference on Weblogs and Social
Media.
P. Eckert and S. McConnell-Ginet. 2003. Language and
gender. Cambridge University Press, Cambridge.
F. Giglietto. 2012. If likes were votes: An empirical
study of the 2011 Italian administrative elections. In
Proceedings of the International Conference on We-
blogs and Social Media.
J. Holmes. 1995. Women, men and politeness. Long-
man, London.
M. Kim and H.W. Park. 2012. e-measuring Twitter-
based political participation and deliberation in the
South Korean context by using social network and
Triple Helix indicators. Scientometrics, 90(1):121?
140.
W. Liu and D. Ruths. 2013. What?s in a name? Using
first names as features for gender inference in Twit-
ter. In Analyzing Microtext: 2013 AAAI Spring Sym-
posium.
W. Liu, F.A. Zamal, and D. Ruths. 2012. Using so-
cial media to infer gender composition from commuter
populations. In Proceedings of the When the City
Meets the Citizen Worksop.
A. Mislove, S. Lehmann, Y.Y. Ahn, J.P. Onnela, and J.N.
Rosenquist. 2011. Understanding the demographics
of Twitter users. In Proceedings of the International
Conference on Weblogs and Social Media.
D. Mocanu, A. Baronchelli, B. Gonc?alves, N. Perra, and
A. Vespignani. 2012. The Twitter of Babel: Map-
ping world languages through microblogging plat-
forms. ArXiv e-prints, December.
B. New and C. Landing. 2012. Lexique 3.
http://www.lexique.org/telLexique.php.
F.C.C. Peng, editor. 1981. Male/female differences in
Japanese. The East-West Sign Language Association,
Tokyo.
M. Pennacchiotti and A.M. Popescu. 2011. A machine
learning approach to Twitter user classification. In
Proceedings of the International Conference on We-
blogs and Social Media.
D. Rao and D. Yarowsky. 2010. Detecting latent user
properties in social media. In Proceedings of the NIPS
workshop on Machine Learning for Social Networks.
D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta. 2010.
Classifying latent user attributes in Twitter. In Pro-
ceedings of the International Workshop on Search and
Mining User-generated Contents.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes Twitter users: Real-time event detection
by social sensors. In Proceedings of the International
World Wide Web Conference.
Semiocast. 2012. Brazil becomes the 2nd country on
Twitter, Japan 3rd, Netherlands most active country.
http://semiocast.com/publications/
2012 01 31 Brazil becomes 2nd country on
Twitter superseds Japan.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with Twitter: What
140 characters reveal about political sentiment. In
Proceedings of the International Conference on We-
blogs and Social Media.
W. Weerkamp, S. Carter, and M. Tsagkias. 2011. How
people use Twitter in different languages. In Proceed-
ings of the Web Science Conference.
R. Schenk-Van Witsen. 1981. Les diffe?rences sex-
uelles dans le franc?ais parle?: une e?tude-pilote des
diffe?rences lexicales entre hommes et femmes. Lan-
gage et socie?te?, 17(1):59?78.
F.A. Zamal, W. Liu, and D. Ruths. 2012. Homophily and
latent attribute inference: Inferring latent attributes of
Twitter users from neighbors. In Proceedings of the
International Conference on Weblogs and Social Me-
dia.
1145
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1019?1029,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Combining data and mathematical models of language change
Morgan Sonderegger
University of Chicago
Chicago, IL, USA.
morgan@cs.uchicago.edu
Partha Niyogi
University of Chicago
Chicago, IL, USA.
niyogi@cs.uchicago.edu
Abstract
English noun/verb (N/V) pairs (contract,
cement) have undergone complex patterns
of change between 3 stress patterns for
several centuries. We describe a longitu-
dinal dataset of N/V pair pronunciations,
leading to a set of properties to be ac-
counted for by any computational model.
We analyze the dynamics of 5 dynamical
systems models of linguistic populations,
each derived from a model of learning by
individuals. We compare each model?s dy-
namics to a set of properties observed in
the N/V data, and reason about how as-
sumptions about individual learning affect
population-level dynamics.
1 Introduction
The fascinating phenomena of language evolution
and language change have inspired much work
from computational perspectives in recent years.
Research in this field considers populations of lin-
guistic agents, and asks how the population dy-
namics are related to the behavior of individual
agents. However, most such work makes little
contact with empirical data (de Boer and Zuidema,
2009).1 As pointed out by Choudhury (2007),
most computational work on language change
deals with data from cases of change either not at
all, or at a relatively high level.2
Recent computational work has addressed ?real
world? data from change in several languages
(Mitchener, 2005; Choudhury et al, 2006; Choud-
hury et al, 2007; Pearl and Weinberg, 2007; Da-
land et al, 2007; Landsbergen, 2009). In the same
1However, among language evolution researchers there
has been significant recent interest in behavioral experiments,
using the ?iterated learning? paradigm (Griffiths and Kalish,
2007; Kalish et al, 2007; Kirby et al, 2008).
2We do not review the literature on computational studies
of change due to space constraints; see (Baker, 2008; Wang
et al, 2005; Niyogi, 2006) for reviews.
spirit, we use data from an ongoing stress shift
in English noun/verb (N/V) pairs. Because stress
has been listed in dictionaries for several centuries,
we are able to trace stress longitudinally and at
the level of individual words, and observe dynam-
ics significantly more complicated than in changes
previously considered in the computational litera-
ture. In ?2, we summarize aspects of the dynamics
to be accounted for by any computational model of
the stress shift. We also discuss proposed sources
of these dynamics from the literature, based on ex-
perimental work by psychologists and linguists.
In ?3?4, we develop models in the mathemati-
cal framework of dynamical systems (DS), which
over the past 15 years has been used to model
the interaction between language learning and lan-
guage change in a variety of settings (Niyogi
and Berwick, 1995; Niyogi and Berwick, 1996;
Niyogi, 2006; Komarova et al, 2001; Yang, 2001;
Yang, 2002; Mitchener, 2005; Pearl and Weinberg,
2007).
We interpret 6 aspects of the N/V stress dy-
namics in DS terms; this gives a set of 6 desired
properties to which any DS model?s dynamics can
be compared. We consider 5 models of language
learning by individuals, based on the experimen-
tal findings relevant to the N/V stress shift, and
evaluate the population-level dynamics of the dy-
namical system model resulting from each against
the set of desired properties. We are thus able to
reason about which theories of the source of lan-
guage change ? considered as hypotheses about
how individuals learn ? lead to the population-
level patterns observed in change.
2 Data: English N/V pairs
The data considered here are the stress patterns of
English homographic, disyllabic noun/verb pairs
(Table 1); we refer to these throughout as ?N/V
pairs?. Each of the N and V forms of a pair can
have initial (???: co?nvict, n.) or final (???: conv??ct,
1019
N V
{1, 1} ??? ??? (exile, anchor, fracture)
{1, 2} ??? ??? (consort, protest, refuse)
{2, 2} ??? ??? (cement, police, review)
Table 1: Attested N/V pair stress patterns.
v.) stress. We use the notation {Nstress,Vstress}
to denote the stress of an N/V pair, with 1=???,
2=???. Of the four logically possible stress pat-
terns, all current N/V pairs follow one of the 3
patterns shown in Table 1: {1,1}, {1,2}, {2,2}.3
No pair follows the fourth possible pattern, {2,1}.
N/V pairs have been undergoing variation and
change between these 3 patterns since Middle En-
glish (ME, c. 1066-1470), especially change to
{1,2}. The vast majority of stress shifts occurred
after 1570 (Minkova, 1997), when the first dictio-
nary listing English word stresses was published
(Levens, 1570). Many dictionaries from the 17th
century on list word stresses, making it possible to
trace change in the stress of individual N/V pairs
in considerable detail.
2.1 Dynamics
Expanding on dictionary pronunciation data col-
lected by Sherman (1975) for the period 1570?
1800, we have collected a corpus of pronunci-
ations of 149 N/V pairs, as listed in 62 British
dictionaries, published 1570?2007. Variation and
change in N/V pair stress can be visualized by
plotting stress trajectories: the moving average of
N and V stress vs. time for a given pair. Some
examples are shown in Fig. 1. The corpus is
described in detail in (Sonderegger and Niyogi,
2010); here we summarize the relevant facts to be
accounted for in a computational model.4
Change Four types of clear-cut change between
the three stress patterns are observed:
{2,2}?{1,2} (Fig.1(a)) {1,2}?{1,1}
{1,1}?{1,2} (Fig. 1(b)) {1,2}?{2,2}
However, change to {1,2} is much more com-
mon than change from {1,2}; in particular,
{2,2}?{1,2} is the most common change. When
3However, as variation and change in N/V pair stress
is ongoing, a few pairs (e.g. perfume) currently have vari-
able stress. By ?stress?, we always mean ?primary stress?.
All present-day pronunciations are for British English, from
CELEX (Baayen et al, 1996).
4The corpus is available on the first author?s home page
(currently, people.cs.uchicago.edu/?morgan).
change occurs, it is often fairly sudden, as in
Figs. 1(a), 1(b). Finally, change never occurs di-
rectly between {1,1} and {2,2}.
Stability Previous work on stress in N/V pairs
(Sherman, 1975; Phillips, 1984) has emphasized
change, in particular {2,2}?{1,2} (the most com-
mon change). However, an important aspect of
the diachronic dynamics of N/V pairs is stability:
most N/V pairs do not show variation or change.
The 149 N/V pairs, used both in our corpus and
in previous work, were chosen by Sherman (1975)
as those most likely to have undergone change,
and thus are not suitable for studying how stable
the three attested stress patterns are. In a ran-
dom sample of N/V pairs (not the set of 149) in
use over a fixed time period (1700?2007), we find
that only 12% have shown variation or change in
stress (Sonderegger and Niyogi, 2010). Most pairs
maintain the {1,1}, {2,2}, or {1,2} stress pattern
for hundreds of years. A model of the diachronic
dynamics of N/V pair stress must explain how it
can be the case both that some pairs show varia-
tion and change, and that many do not.
Variation N/V pair stress patterns show both
synchronic and diachronic variation.
Synchronically, there is variation at the pop-
ulation level in the stress of some N/V pairs at
any given time; this is reflected by the inclusion
of more than one pronunciation for some N/V
pairs in many dictionaries. An important question
for modeling is whether there is variation within
individual speakers. We show in (Sonderegger
and Niyogi, 2010) that there is, for present-day
American English speakers, using a corpus of ra-
dio speech. For several N/V pairs which have
currently variable pronunciation, 1/3 of speakers
show variation in the stress of the N form. Metrical
evidence from poetry suggests that individual vari-
ation also existed in the past; the best evidence is
for Shakespeare, who shows variation in the stress
of over 20 N/V pairs (Ko?keritz, 1953).
Diachronically, a relevant question for mod-
eling is whether all variation is short-lived, or
whether stable variation is possible. A particu-
lar type of stable variation is in fact observed rela-
tively often in the corpus: either the N or V form
stably vary (Fig. 1(c)), but not both at once. Stable
variation where both N and V forms vary almost
never occurs (Fig. 1(d)).
Frequency dependence Phillips (1984) hypoth-
1020
1700 1800 1900 20001
1.21.4
1.61.8
2 concert
YearMovin
g average
 of stress 
placemen
t
(a) concert
1700 1800 1900 20001
1.21.4
1.61.8
2 combat
YearMovin
g average
 of stress 
placemen
t
(b) combat
1700 1800 1900 20001
1.21.4
1.61.8
2 exile
YearMovin
g average
 of stress 
placemen
t
(c) exile
1850 1900 1950 20001
1.21.4
1.61.8
2 rampage
YearMovin
g average
 of stress 
placemen
t
(d) rampage
Figure 1: Example N/V pair stress trajectories. Moving averages (60-year window) of stress placement
(1=???, 2=???). Solid lines=nouns, dashed lines=verbs.
esizes that N/V pairs with lower frequencies
(summed N+V word frequencies) are more likely
to change to {1,2}. Sonderegger (2010) shows
that this is the case for the most common change,
{2,2}?{1,2}: among N/V pairs which were
{2,2} in 1700 and are either {2,2} or {1,2} today,
those which have undergone change have signif-
icantly lower frequencies, on average, than those
which have not. In (Sonderegger and Niyogi,
2010), we give preliminary evidence from real-
time frequency trajectories (for <10 N/V pairs)
that it is not lower frequency per se which triggers
change to {1,2}, but falling frequency. For exam-
ple, change in combat from {1,1}?{1,2} around
1800 (Fig. 1(b)) coincides with falling word fre-
quency from 1775?present.
2.2 Sources of change
The most salient facts about English N/V pair
stress are that (a) change is most often to {1,2}
(b) the {2,1} pattern never occurs. We summa-
rize two types of explanation for these facts from
the experimental literature, each of which exem-
plifies a commonly-proposed type of explanation
for phonological change. In both cases, there is ex-
perimental evidence for biases in present-day En-
glish speakers reflecting (a?b). We assume that
these biases have been active over the course of
the N/V stress shift, and can thus be seen as pos-
sible sources of the diachronic dynamics of N/V
pairs.5
5This type of assumption is necessary for any hypothesis
about the sources of a completed or ongoing change, based
on present-day experimental evidence, and is thus common in
the literature. In the case of N/V pairs, it is implicitly made in
Kelly?s (1988 et seq) account, discussed below. Both biases
discussed here stem from facts about English (Ross? Gener-
alization; rhythmic context) that we believe have not changed
over the time period considered here (?1600?present), based
on general accounts of English historical phonology during
this period (Lass, 1992; MacMahon, 1998). We leave more
careful verification of this claim to future work.
Analogy/Lexicon In historical linguistics, ana-
logical changes are those which make ?...related
forms more similar to each other in their phonetic
(and morphological) structure? (Hock, 1991).6
Proposed causes for analogical change thus often
involve a speaker?s production and perception of
a form being influenced by similar forms in their
lexicon.
The English lexicon shows a broad tendency,
which we call Ross? generalization, which could
be argued to be driving analogical change to {1,2},
and acting against the unobserved stress pattern
{2,1}: ?primary stress in English nouns is farther
to the left than primary stress in English verbs?
(Ross, 1973). Change to {1,2} could be seen
as motivated by Ross? generalization, and {2,1}
made impossible by it.
The argument is lent plausibility by experimen-
tal evidence that Ross? Generalization is reflected
in production and perception. English listeners
strongly prefer the typical stress pattern (N=??? or
V=???) in novel English disyllables (Guion et al,
2003), and process atypical disyllables (N=??? or
V=???) more slowly than typical ones (Arciuli and
Cupples, 2003).
Mistransmission An influential line of research
holds that many phonological changes are based
in asymmetric transmission errors: because of ar-
ticulatory or perceptual factors, listeners systemat-
ically mishear some sound ? as ?, but rarely mis-
hear ? as ?.7 We call such effects mistransmis-
sion. Asymmetric mistransmission (by individu-
6?Forms? here means any linguistic unit; e.g. sounds,
words, or paradigms, such as an N/V pair?s stress pattern.
7A standard example is final obstruent devoicing, a com-
mon change cross-linguistically. There are several articula-
tory and perceptual reasons why final voiced obstruents could
be heard as unvoiced, but no motivation for the reverse pro-
cess (final unvoiced obstruents heard as voiced) (Blevins,
2006).
1021
als) is argued to be a necessary condition for the
change ??? at the population level, and an ex-
planation for why the change ??? is common,
while the change ??? is rarely (or never) ob-
served. Mistransmission-based explanations were
pioneered by Ohala (1981, et seq.), and are the
subject of much recent work (reviewed by Hans-
son, 2008)
For English N/V pairs, M. Kelly and collabo-
rators have shown mistransmission effects which
they propose are responsible for the directionality
of the most common type of N/V pair stress shifts
({1,1}, {2,2}?{1,2}), based on ?rhythmic con-
text? (Kelly, 1988; Kelly and Bock, 1988; Kelly,
1989). Word stress is misperceived more often
as initial in ?trochaic-biasing? contexts, where the
preceding syllable is weak or the following syl-
lable is heavy; and more often as final in anal-
ogously ?iambic-biasing? contexts. Nouns occur
more frequently in trochaic contexts, and verbs
more frequently in iambic contexts; there is thus
pressure for the V forms of {1,1} pairs to be mis-
perceived as ???, and for the N forms of {2,2} pairs
to be misperceived as ???.
3 Modeling preliminaries
We first describe assumptions and notation for
models developed below (?4).
Because of the evidence for within-speaker
variation in N/V pair stress (?2.1), in all models
described below, we assume that what is learned
for a given N/V pair are the probabilities of using
the ??? form for the N and V forms.
We also make several simplifying assumptions.
There are discrete generations Gt, and learners in
Gt learn from Gt?1. Each example a learner in Gt
hears is equally likely to come from any member
of Gt?1. Each learner receives an identical num-
ber of examples, and each generation has infinitely
many members.
These are idealizations, adopted here to keep
models simple enough to analyze; the effects of
relaxing some of these assumptions have been ex-
plored by Niyogi (2006) and Sonderegger (2009).
The infinite-population assumption in particular
makes the dynamics fully deterministic; this rules
out the possibility of change due to drift (or sam-
ple variation), where a form disappears from the
population because no examples of it are encoun-
tered by learners in Gt in the input from Gt?1.
Notation For a fixed N/V pair, a learner in Gt
hears N1 examples of the N form, of which kt1 are
??? and (N1-kt1) are ???; N2 and k
t
2 are similarly
defined for V examples. Each example is sampled
i.i.d. from a random member of Gt?1. The Ni are
fixed (each learner hears the same number of ex-
amples), while the kti are random variables (over
learners in Gt). Each learner applies an algorithm
A to the N1+N2 examples to learn ??t, ??t ? [0, 1],
the probabilities of producing N and V examples
as ???. ?t, ?t are the expectation of ??t and ??t over
members of Gt: ?t = E(??t), ?t = E(??t). ??t and
??t are thus random variables (over learners in Gt),
while ?t, ?t ? [0, 1] are numbers.
Because learners in Gt draw examples at ran-
dom from members of Gt?1, the distributions
of ??t and ??t are determined by (?t?1, ?t?1).
(?t, ?t), the expectations of ??t and ??t, are thus
determined by (?t?1, ?t?1) via an iterated map f :
f : [0, 1]2 ? [0, 1]2, f(?t, ?t) = (?t+1, ?t+1).
3.1 Dynamical systems
We develop and analyze models of populations of
language learners in the mathematical framework
of (discrete) dynamical systems (DS) (Niyogi and
Berwick, 1995; Niyogi, 2006). This setting allows
us to determine the diachronic, population-level
consequences of assumptions about the learning
algorithm used by individuals, as well as assump-
tions about population structure or the input they
receive.
Because it is in general impossible to solve a
given iterated map as a function of t, the dynam-
ical systems viewpoint is to understand its long-
term behavior by finding its fixed points and bi-
furcations: changes in the number and stability of
fixed points as system parameters vary.
Briefly, ?? is a fixed point (FP) of f if f(??) =
??; it is stable if lim
t??
?t = ?? for ?0 sufficiently
near ??, and unstable otherwise; these are also
called stable states and unstable states. Intuitively,
?? is stable iff the system is stable under small per-
turbations from ??.8
In the context of a linguistic population, change
from state ? (100% of the population uses {1,1})
to state ? (100% of the population uses {1,2})
corresponds to a bifurcation, where some system
parameter (N ) passes a critical value (N0). For
8See (Strogatz, 1994; Hirsch et al, 2004) for introduc-
tions to dynamical systems in general, and (Niyogi, 2006) for
the type of models considered here.
1022
N<N0, ? is stable. For N>N0, ? is unstable,
and ? is stable; this triggers change from ? to ?.
3.2 DS interpretation of observed dynamics
Below, we describe 5 DS models of linguistic pop-
ulations. To interpret whether each model has
properties consistent with the N/V dataset, we
translate the observations about the dynamics of
N/V stress made above (?2.1) into DS terms. This
gives a list of desired properties against which to
evaluate the properties of each model.
1. ?{2,1}: {2,1} is not a stable state.
2. Stability of {1,1}, {1,2}, {2,2}: These stress
patterns correspond to stable states (for some
system parameter values).
3. Observed stable variation: Stable states are
possible (for some system parameter values)
corresponding to variation in the N or V
form, but not both.
4. Sudden change: Change from one stress pat-
tern to another corresponds to a bifurcation,
where the fixed point corresponding to the
old stress pattern becomes unstable.
5. Observed changes: There are bifurcations
corresponding to each of the four observed
changes ({1,1}?? {1,2}, {2,2}?? {1,2}).
6. Observed frequency dependence: Change to
{1,2} corresponds to a bifurcation in fre-
quency (N ), where {2,2} or {1,1} loses sta-
bility as N is decreased.
4 Models
We now describe 5 DS models, each correspond-
ing to a learning algorithm A used by individual
language learners. Each A leads to an iterated
map, f(?t, ?t) = (?t+1, ?t+1), which describes
the state of the population of learners over succes-
sive generations. We give these evolution equa-
tions for each model, then discuss their dynamics,
i.e. bifurcation structure. Each model?s dynam-
ics are evaluated with respect to the set of desired
properties corresponding to patterns observed in
the N/V data. Derivations have been mostly omit-
ted for reasons of space, but are given in (Son-
deregger, 2009).
The models differ along two dimensions, cor-
responding to assumptions about the learning al-
gorithm (A): whether or not it is assumed that
the stress of examples is possibly mistransmitted
(Models 1, 3, 5), and how the N and V probabil-
ities acquired by a given learner are coupled. In
Model 1 there is no coupling (??t and ??t learned
independently), in Models 2?3 coupling takes the
form of a hard constraint corresponding to Ross?
generalization, and in Models 4?5 different stress
patterns have different prior probabilities.9
4.1 Model 1: Mistransmission
Motivated by the evidence for asymmetric mis-
perception of N/V pair stress (?2.2), suppose the
stress of N=??? and V=??? examples may be mis-
perceived (as N=??? and V=???), with mistrans-
mission probabilities p and q.
Learners are assumed to simply probability
match: ??t = kt1/N1, ??t = k
t
2/N2, where k
t
1 is the
number of N and V examples heard as ??? (etc.)
The probabilities pN,t & pV,t of hearing an N or V
example as final stressed at t are then
pN,t = ?t?1(1? p), pV,t = ?t?1 + (1? ?t?1)q (1)
kt1 and k
t
2 are binomially-distributed:
PB(k
t
1, k
t
2) ?
(
N1
kt1
)
pN,t
kt1(1? pN,t)
N1?kt1
?
(
N2
kt2
)
pV,t
kt2(1? pV,t)
N2?kt2 (2)
?t and ?t, the probability that a random member
of Gt produces N and V examples as ???, are the
ensemble averages of ??t and ??t over all members
of Gt. Because we have assumed infinitely many
learners per generation, ?t=E(??t) and ?t=E(??t).
Using (1), and the formula for the expectation of a
binomially-distributed random variable:
?t = ?t?1(1? p) (3)
?t = ?t?1 + (1? ?t?1)q (4)
these are the evolution equations for Model 1.
Due to space constraints we do not give the (more
lengthy) derivations of the evolution equations in
Models 2?5.
Dynamics There is a single, stable fixed point
of evolution equations (3?4): (??, ??) = (0, 1),
corresponding to the stress pattern {1,2}. This
model thus shows none of the desired properties
discussed in ?3.2, except that {1,2} corresponds
to a stable state.
9The sixth possible model (no coupling, no mistransmis-
sion) is a special case of Model 1, resulting in the identity
map: ?t+1 = ?t, ?t+1 = ?t.
1023
4.2 Model 2: Coupling by constraint
Motivated by the evidence for English speak-
ers? productive knowledge of Ross? Generaliza-
tion (?2.2), we consider a second learning model
in which the learner attempts to probability match
as above, but the (??t, ??t) learned must satisfy the
constraint that ??? stress be more probable in the
V form than in the N form.
Formally, the learner chooses (??t, ??t) satisfying
a quadratic optimization problem:
minimize [(??
kt1
N1
)2 + (? ?
kt2
N2
)2] s.t. ? ? ?
This corresponds to the following algorithm, A2:
1. If k
t
1
N1
< k
t
2
N2
, set ??t =
kt1
N1
, ??t =
kt2
N2
.
2. Otherwise, set ??t = ??t = 12 (
kt1
N1
+ k
t
2
N2
)
The resulting evolution equations can be shown to
be
?t+1 = ?t +
A
2
, ?t+1 = ?t ?
A
2
(5)
where A =
?
k1
N1
> k2N2
PB(k
t
1, k
t
2)(
kt1
N1
?
kt2
N2
).
Dynamics Adding the equations in (5)
gives that the (?t, ?t) trajectories are lines
of constant ?t + ?t (Fig. 2). All (0, x)
and (x, 1) (x?[0, 1]) are stable fixed points.
1.0
1.00
0
Figure 2: Dynamics
of Model 2
This model thus has sta-
ble FPs corresponding to
{1,1}, {1,2}, and {2,2},
does not have {2,1} as
a stable FP (by construc-
tion), and allows for sta-
ble variation in exactly
one of N or V. It does
not have bifurcations, or
the observed patterns of
change and frequency de-
pendence.
4.3 Model 3: Coupling by constraint, with
mistransmission
We now assume that each example is subject to
mistransmission, as in Model 1; the learner then
applies A2 to the heard examples. The evolution
equations are thus the same as in (5), but with ?t?1
and ?t?1 changed to pN,t, pV,t (Eqn. 1).
Dynamics There is a single, stable fixed point,
corresponding to stable variation in both N and V.
This model thus shows none of the desired prop-
erties, except that {2,1} is not a stable FP (by con-
struction).
4.4 Model 4: Coupling by priors
The type of coupling assume in Models 2?3 ? a
constraint on the relative probability of ??? stress
for N and V forms ? has the drawback that there
is no way for the rest of the lexicon to affect a
pair?s N and V stress probabilities: there can be no
influence of the stress of other N/V pairs, or in the
lexicon as a whole, on the N/V pair being learned.
Models 4?5 allow such influence by formalizing a
simple intuitive explanation for the lack of {2, 1}
N/V pairs: learners cannot hypothesize a {2, 1}
pair because there is no support for this pattern in
their lexicons.
We now assume that learners compute the prob-
abilities of each possible N/V pair stress pattern,
rather than separate probabilities for the N and V
forms. We assume that learners keep two sets of
probabilities (for {1, 1}, {1, 2}, {2, 1}, {2, 2}):
1. Learned probabilities:
~P=(P11, P12, P22, P21), where
P11 =
N1?kt1
N1
N2?kt2
N2
, P12 =
N1?kt1
N1
kt2
N2
P22 =
kt1
N1
kt2
N2
, P21 =
kt1
N1
N2?kt2
N2
2. Prior probabilities: ~? = (?11, ?12, ?21, ?22),
based on the support for each stress pattern in
the lexicon.
The learner then produces N forms as follows:
1. Pick a pattern {n1, v1} according to ~P .
2. Pick a pattern {n2, v2} according to ~?
3. Repeat 1?2 until n1=n2, then produce N=n1.
V forms are produced similarly, but checking
whether v1 = v2 at step 3. Learners? production of
an N/V pair is thus influenced by both their learn-
ing experience (for the particular N/V pair) and by
how much support exists in their lexicon for the
different stress patterns.
We leave the exact interpretation of the ?ij am-
biguous; they could be the percentage of N/V pairs
already learned which follow each stress pattern,
for example. Motivated by the absence of {2,1}
N/V pairs in English, we assume that ?21 = 0.
1024
By following the production algorithm above,
the learner?s probabilities of producing N and V
forms as ??? are:
??t = ??(k
t
1, k
t
2) =
?22P22
?11P11 + ?12P12 + ?22P22
(6)
??t = ??(k
t
1, k
t
2) =
?12P12 + ?22P22
?11P11 + ?12P12 + ?22P22
(7)
Eqns. 6?7 are undefined when (kt1, k
t
2)=(N1, 0); in
this case we set ??(N1, 0) = ?22 and ??(N1, 0) =
?12 + ?22.
The evolution equations are then
?t = E(??t) =
N1?
k1=0
N2?
k2=0
PB(k1, k2)??(k1, k2) (8)
?t = E(??t) =
N1?
k1=0
N2?
k2=0
PB(k1, k2)??(k1, k2) (9)
Dynamics The fixed points of (8?9) are (0, 0),
(0, 1), and (1, 1); their stabilities depend on N1,
N2, and ~?. Define
R =
(
N2
1 + (N2 ? 1)
?12
?11
)(
N1
1 + (N1 ? 1)
?12
?22
)
(10)
There are 6 regions of parameter space in which
different FPs are stable:
1. ?11, ?22 < ?12: (0, 1) stable
2. ?22 > ?12, R < 1: (0, 1), (1, 1) stable
3. ?11 < ?12 < ?22, R > 1: (1, 1) stable
4. ?11, ?22 > ?12: (0, 0), (1, 1) stable
5. ?22 < ?12 < ?11, R > 1: (0, 0) stable
6. ?11 > ?12, R < 1: (0, 0), (0, 1) stable
The parameter space is split into these regimes
by three hyperplanes: ?11=?12, ?22=?12, and
R=1. Given that ?21=0, ?12 = 1 ? ?11 ?
?22, and the parameter space is 4-dimensional:
(?11, ?22, N1, N2). Fig. 3 shows An example
phase diagram in (?11, ?2), with N1 and N2 fixed.
The bifurcation structure implies all 6 possi-
ble changes between the three FPs ({1,1}??{1,2},
{1,2}??{2,2}, {2,2}??{1,2}). For example, sup-
pose the system is at stable FP (1, 1) (correspond-
ing to {2,2}) in region 2. As ?22 is decreased, we
move into region 1, (1, 1) becomes unstable, and
the system shifts to stable FP (0, 1). This transi-
tion corresponds to change from {2,2} to {1,2}.
Note that change to {1,2} entails crossing the
hyperplanes ?12=?22 and ?12=?11. These hy-
perplanes do not change as N1 and N2 vary, so
0.0 0.2 0.4 0.6 0.8 1.0?110.0
0.2
0.4
0.6
0.8
1.0
? 22
1
23 4
56
Figure 3: Example phase diagram in (?11, ?22) for
Model 4, with N1 = 5, N2 = 10. Numbers are
regions of parameter space (see text).
change to {1,2} is not frequency-dependent. How-
ever, change from {1,2} entails crossing the hy-
perplane R=1, which does change as N1 and N2
vary (Eqn. 10), so change from {1,2} is frequency-
dependent. Thus, although there is frequency de-
pendence in this model, it is not as observed in
the diachronic data, where change to {1,2} is
frequency-dependent.
Finally, no stable variation is possible: in every
stable state, all members of the population cate-
gorically use a single stress pattern. {2,1} is never
a stable FP, by construction.
4.5 Model 5: Coupling by priors, with
mistransmission
We now suppose that each example from a
learner?s data is possibly mistransmitted, as in
Model 1; the learner then applies the algorithm
from Model 4 to the heard examples (instead of
using kt1, k
t
2) . The evolution equations are thus
the same as (8?9), but with ?t?1 and ?t?1 changed
to pN,t, pV,t (Eqn. 1).
Dynamics (0, 1) is always a fixed point. For
some regions of parameter space, there can be one
fixed point of the form (?, 1), as well as one fixed
point of the form (0, ?), where ?, ? ? (0, 1). De-
fine R? = (1? p)(1? q)R, ??12 = ?12, and
??11 = ?11(1?q
N2
N2 ? 1
), ??22 = ?22(1?p
N1
N1 ? 1
)
There are 6 regions of parameter space corre-
sponding to different stable FPs, identical to the
6 regions in Model 4, with the following substitu-
1025
0 2 4 6 8 10N10.0
0.2
0.4
0.6
0.8
1.0
Stable 
? t fixed
 point lo
cation
Figure 4: Example of fallingN1 triggering change
from (1, 1) to (0, 1) for Model 5. Dashed line =
stable FP of the form (?, 1), solid line = stable FP
(0, 1). ForN1 > 4, there is a stable FP near (1, 1).
For N1 < 2, (0, 1) is the only stable FP. ?22 =
0.58, ?12 = 0.4, N2 = 10, p = q = 0.05.
tions made: R ? R?, ?ij ? ??ij , (0, 0) ? (0, ?),
(1, 1)? (?, 1).
The parameter space is again split into these
regions by three hyperplanes: ??11=?
?
12, ?
?
22=?
?
12,
andR?=1. As in Model 4, the bifurcation structure
implies all 6 possible changes between the three
FPs. However, change to {1,2} entails crossing
the hyperplanes ??11=?
?
12 and ?
?
2=?
?
12, and is thus
now frequency dependent.
In particular, consider a system at a stable FP
(?, 1), for some N/V pair. This FP becomes un-
stable if ??22 becomes smaller than ?
?
12. Assuming
that the ?ij are fixed, this occurs only if N1 falls
below a critical value, N?1 = (1?
?22
?12
(1? p))?1;
the system would then transition to (0, 1), the only
stable state. By a similar argument, falling fre-
quency can lead to change from (0, ?) to (0, 1).
Falling frequency can thus cause change to {1,2}
in this model, as seen in the N/V data; Fig. 4 shows
an example.
Unlike in Model 4, stable variation of the type
seen in the N/V stress trajectories ? one of N or V
stably varying, but not both ? is possible for some
parameter values. (0, 0) and (1, 1) (corresponding
to {1,1} and {2,2}) are technically never possible,
but effectively occur for FPs of the form (?, 0) and
(?, 1) when ? or ? are small. {2,1} is never a sta-
ble FP, by construction.
This model thus arguably shows all of the de-
sired properties seen in the N/V data.
Property
Model
1 2 3 4 5
?{2,1} ! ! ! ! !
{1,1}, {1,2}, {2,2} % ! % ! !
Obs. stable variation % ! % % !
Sudden change % % % ! !
Observed changes % % % ! !
Obs. freq. depend. % % % % !
Table 2: Summary of model properties
4.6 Models summary, observations
Table 2 lists which of Models 1?5 show each of
the desired properties (from ?3.2), corresponding
to aspects of the observed diachronic dynamics of
N/V pair stress.
Based on this set of models, we are able to
make some observations about the effect of dif-
ferent assumptions about learning by individuals
on population-level dynamics. Models including
asymmetric mistransmission (1, 3, 5) generally do
not lead to stable states in which the entire pop-
ulation uses {1,1} or {2,2}. (In Model 5, sta-
ble variation very near {1,1} or {2,2} is possi-
ble.) However, {1,1} and {2,2} are diachroni-
cally very stable stress patterns, suggesting that at
least for this model set, assuming mistransmission
in the learner is problematic. Models 2?3, where
analogy is implemented as a hard constraint based
on Ross? generalization, do not give most desired
properties. Models 4?5, where analogy is imple-
mented as prior probabilities over N/V stress pat-
terns, show crucial aspects of the observed dynam-
ics: bifurcations corresponding to the changes ob-
served in the stress data. Model 5 shows change
to {1,2} triggered by falling frequency, a pattern
observed in the stress data, and an emergent prop-
erty of the model dynamics: this frequency effect
is not present in Models 1 or 4, but is present in
Model 5, where the learner combines mistransmis-
sion (Model 1) with coupling by priors (Model 4).
5 Discussion
We have developed 5 dynamical systems models
for a relatively complex diachronic change, found
one successful model, and were able to reason
about the source of model behavior. Each model
describes the diachronic, population-level conse-
quences of assuming a particular learning algo-
rithm for individuals. The algorithms considered
1026
were motivated by different possible sources of
change, from linguistics and psychology (?2.2).
We discuss novel contributions of this work, and
future directions.
The dataset used here shows more complex dy-
namics, to our knowledge, than in changes previ-
ously considered in the computational literature.
By using a detailed, longitudinal dataset, we were
able to strongly constrain the desired behavior of
a computational model, so that the task of model
building is not ?doomed to success?. While all
models show some patterns observed in the data,
only one shows all such properties. We believe de-
tailed datasets are potentially very useful for eval-
uating and differentiating between proposed com-
putational models of change.
This paper is a first attempt to integrate detailed
data with a range of DS models. We have only
considered some schematic properties of the dy-
namics observed in our dataset, and used these
to qualitatively compare each model?s predictions
to the dynamics. Future work should consider
the dynamics in more detail, develop more com-
plex models (for example, by relaxing the infinite-
population assumption, allowing for stochastic dy-
namics), and quantitatively compare model pre-
dictions and observed dynamics.
We were able to reason about how assump-
tions about individual learning affect population
dynamics by analyzing a range of simple, related
models. This approach is pursued in more depth
in the larger set of models considered in (Son-
deregger, 2009). Our use of model comparison
contrasts with most recent computational work on
change, where a small number (1?2) of very com-
plex models are analyzed, allowing for much more
detailed models of language learning and usage
than those considered here (e.g. Choudhury et al,
2006; Minett & Wang, 2008; Baxter et al, 2009;
Landsbergen, 2009). An advantage of our ap-
proach is an enhanced ability to evaluate a range of
proposed causes for a particular case of language
change.
By using simple models, we were able to con-
sider a range of learning algorithms correspond-
ing to different explanations for the observed di-
achronic dynamics. What makes this a useful ex-
ercise is the fundamentally non-trivial map, illus-
trated by Models 1?5, between individual learn-
ing and population-level dynamics. Although the
type of individual learning assumed in each model
was chosen with the same patterns of change in
mind, and despite the simplicity of the models
used, the resulting population-level dynamics dif-
fer greatly. This is an important point given that
proposed explanations for change (e.g., mistrans-
mission and analogy) operate at the level of in-
dividuals, while the phenomena being explained
(patterns of change, or particular changes) are as-
pects of the population-level dynamics.
Acknowledgments
We thank Max Bane, James Kirby, and three
anonymous reviewers for helpful comments.
References
J. Arciuli and L. Cupples. 2003. Effects of stress typ-
icality during speeded grammatical classification.
Language and Speech, 46(4):353?374.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1996.
CELEX2 (CD-ROM). Linguistic Data Consortium,
Philadelphia.
A. Baker. 2008. Computational approaches to the
study of language change. Language and Linguis-
tics Compass, 2(3):289?307.
G.J. Baxter, R.A. Blythe, W. Croft, and A.J. McK-
ane. 2009. Modeling language change: An evalu-
ation of Trudgill?s theory of the emergence of New
Zealand English. Language Variation and Change,
21(2):257?296.
J. Blevins. 2006. A theoretical synopsis of Evolution-
ary Phonology. Theoretical Linguistics, 32(2):117?
166.
M. Choudhury, A. Basu, and S. Sarkar. 2006. Multi-
agent simulation of emergence of schwa deletion
pattern in Hindi. Journal of Artificial Societies and
Social Simulation, 9(2).
M. Choudhury, V. Jalan, S. Sarkar, and A. Basu. 2007.
Evolution, optimization, and language change: The
case of Bengali verb inflections. In Proceedings
of the Ninth Meeting of the ACL Special Interest
Group in Computational Morphology and Phonol-
ogy, pages 65?74.
M. Choudhury. 2007. Computational Models of Real
World Phonological Change. Ph.D. thesis, Indian
Institute of Technology Kharagpur.
R. Daland, A.D. Sims, and J. Pierrehumbert. 2007.
Much ado about nothing: A social network model
of Russian paradigmatic gaps. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 936?943.
1027
B. de Boer and W. Zuidema. 2009. Models of lan-
guage evolution: Does the math add up? ILLC
Preprint Series PP-2009-49, University of Amster-
dam.
T.L. Griffiths and M.L. Kalish. 2007. Language evolu-
tion by iterated learning with bayesian agents. Cog-
nitive Science, 31(3):441?480.
S.G. Guion, J.J. Clark, T. Harada, and R.P. Wayland.
2003. Factors affecting stress placement for English
nonwords include syllabic structure, lexical class,
and stress patterns of phonologically similar words.
Language and Speech, 46(4):403?427.
G.H. Hansson. 2008. Diachronic explanations of
sound patterns. Language & Linguistics Compass,
2:859?893.
M.W. Hirsch, S. Smale, and R.L. Devaney. 2004. Dif-
ferential Equations, Dynamical Systems, and an In-
troduction to Chaos. Academic Press, Amsterdam,
2nd edition.
H.H. Hock. 1991. Principles of Historical Linguistics.
Mouton de Gruyter, Berlin, 2nd edition.
M.L. Kalish, T.L. Griffiths, and S. Lewandowsky.
2007. Iterated learning: Intergenerational knowl-
edge transmission reveals inductive biases. Psycho-
nomic Bulletin and Review, 14(2):288.
M.H. Kelly and J.K. Bock. 1988. Stress in time. Jour-
nal of Experimental Psychology: Human Perception
and Performance, 14(3):389?403.
M.H. Kelly. 1988. Rhythmic alternation and lexical
stress differences in English. Cognition, 30:107?
137.
M.H. Kelly. 1989. Rhythm and language change in
English. Journal of Memory & Language, 28:690?
710.
S. Kirby, H. Cornish, and K. Smith. 2008. Cumula-
tive cultural evolution in the laboratory: An experi-
mental approach to the origins of structure in human
language. Proceedings of the National Academy of
Sciences, 105(31):10681?10686.
S. Klein, M.A. Kuppin, and K.A. Meives. 1969.
Monte Carlo simulation of language change in
Tikopia & Maori. In Proceedings of the 1969 Con-
ference on Computational Linguistics, pages 1?27.
ACL.
S. Klein. 1966. Historical change in language us-
ing monte carlo techniques. Mechanical Translation
and Computational Linguistics, 9:67?82.
S. Klein. 1974. Computer simulation of language
contact models. In R. Shuy and C-J. Bailey, ed-
itors, Toward Tomorrows Linguistics, pages 276?
290. Georgetown University Press, Washington.
H. Ko?keritz. 1953. Shakespeare?s Pronunciation.
Yale University Press, New Haven.
N.L. Komarova, P. Niyogi, and M.A. Nowak. 2001.
The evolutionary dynamics of grammar acquisition.
Journal of Theoretical Biology, 209(1):43?60.
F. Landsbergen. 2009. Cultural evolutionary modeling
of patterns in language change: exercises in evolu-
tionary linguistics. Ph.D. thesis, Universiteit Lei-
den.
R. Lass. 1992. Phonology and morphology. In R.M.
Hogg, editor, The Cambridge History of the English
Language, volume 3: 1476?1776, pages 23?156.
Cambridge University Press.
P. Levens. 1570. Manipulus vocabulorum. Henrie
Bynneman, London.
M. MacMahon. 1998. Phonology. In S. Romaine,
editor, The Cambridge History of the English Lan-
guage, volume 4: 1476?1776, pages 373?535. Cam-
bridge University Press.
J.W. Minett and W.S.Y. Wang. 2008. Modelling en-
dangered languages: The effects of bilingualism and
social structure. Lingua, 118(1):19?45.
D. Minkova. 1997. Constraint ranking in Middle En-
glish stress-shifting. English Language and Linguis-
tics, 1(1):135?175.
W.G. Mitchener. 2005. Simulating language change
in the presence of non-idealized syntax. In Pro-
ceedings of the Second Workshop on Psychocom-
putational Models of Human Language Acquisition,
pages 10?19. ACL.
P. Niyogi and R.C. Berwick. 1995. The logical prob-
lem of language change. AI Memo 1516, MIT.
P. Niyogi and R.C. Berwick. 1996. A language learn-
ing model for finite parameter spaces. Cognition,
61(1-2):161?193.
P. Niyogi. 2006. The Computational Nature of Lan-
guage Learning and Evolution. MIT Press, Cam-
bridge.
J.J. Ohala. 1981. The listener as a source of sound
change. In C.S. Masek, R.A. Hendrick, and M.F.
Miller, editors, Papers from the Parasession on Lan-
guage and Behavior, pages 178?203. Chicago Lin-
guistic Society, Chicago.
L. Pearl and A. Weinberg. 2007. Input filtering in syn-
tactic acquisition: Answers from language change
modeling. Language Learning and Development,
3(1):43?72.
B.S. Phillips. 1984. Word frequency and the actuation
of sound change. Language, 60(2):320?342.
J.R. Ross. 1973. Leftward, ho! In S.R. Anderson and
P. Kiparsky, editors, Festschrift for Morris Halle,
pages 166?173. Holt, Rinehart and Winston, New
York.
1028
D. Sherman. 1975. Noun-verb stress alternation: An
example of the lexical diffusion of sound change in
English. Linguistics, 159:43?71.
M. Sonderegger and P. Niyogi. 2010. Variation and
change in English noun/verb pair stress: Data, dy-
namical systems models, and their interaction. Ms.
To appear in A.C.L. Yu, editor, Origins of Sound
Patterns: Approaches to Phonologization. Oxford
University Press.
M. Sonderegger. 2009. Dynamical systems models of
language variation and change: An application to an
English stress shift. Masters paper, Department of
Computer Science, University of Chicago.
M. Sonderegger. 2010. Testing for frequency and
structural effects in an English stress shift. In Pro-
ceedings of the Berkeley Linguistics Society 36. To
appear.
S. Strogatz. 1994. Nonlinear Dynamics and Chaos.
Addison-Wesley, Reading, MA.
W.S.Y. Wang, J. Ke, and J.W. Minett. 2005. Compu-
tational studies of language evolution. In C. Huang
and W. Lenders, editors, Computational Linguistics
and Beyond, pages 65?108. Institute of Linguistics,
Academia Sinica, Taipei.
C. Yang. 2001. Internal and external forces in lan-
guage change. Language Variation and Change,
12(3):231?250.
C. Yang. 2002. Knowledge and Learning in Natural
Language. Oxford University Press.
1029
