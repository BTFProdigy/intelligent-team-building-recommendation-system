Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 494?498,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
LABR: A Large Scale Arabic Book Reviews Dataset
Mohamed Aly
Computer Engineering Department
Cairo University
Giza, Egypt
mohamed@mohamedaly.info
Amir Atiya
Computer Engineering Department
Cairo University
Giza, Egypt
amir@alumni.caltech.edu
Abstract
We introduce LABR, the largest sentiment
analysis dataset to-date for the Arabic lan-
guage. It consists of over 63,000 book
reviews, each rated on a scale of 1 to 5
stars. We investigate the properties of the
the dataset, and present its statistics. We
explore using the dataset for two tasks:
sentiment polarity classification and rat-
ing classification. We provide standard
splits of the dataset into training and test-
ing, for both polarity and rating classifica-
tion, in both balanced and unbalanced set-
tings. We run baseline experiments on the
dataset to establish a benchmark.
1 Introduction
The internet is full of platforms where users can
express their opinions about different subjects,
from movies and commercial products to books
and restaurants. With the explosion of social me-
dia, this has become easier and more prevalent
than ever. Mining these troves of unstructured text
has become a very active area of research with
lots of applications. Sentiment Classification is
among the most studied tasks for processing opin-
ions (Pang and Lee, 2008; Liu, 2010). In its ba-
sic form, it involves classifying a piece of opinion,
e.g. a movie or book review, into either having a
positive or negative sentiment. Another form in-
volves predicting the actual rating of a review, e.g.
predicting the number of stars on a scale from 1 to
5 stars.
Most of the current research has focused on
building sentiment analysis applications for the
English language (Pang and Lee, 2008; Liu, 2010;
Korayem et al, 2012), with much less work on
other languages. In particular, there has been
little work on sentiment analysis in Arabic (Ab-
basi et al, 2008; Abdul-Mageed et al, 2011;
Abdul-Mageed et al, 2012; Abdul-Mageed and
Diab, 2012b; Korayem et al, 2012), and very
few, considerably small-sized, datasets to work
with (Rushdi-Saleh et al, 2011b; Rushdi-Saleh et
al., 2011a; Abdul-Mageed and Diab, 2012a; Elar-
naoty et al, 2012). In this work, we try to address
the lack of large-scale Arabic sentiment analysis
datasets in this field, in the hope of sparking more
interest in research in Arabic sentiment analysis
and related tasks. Towards this end, we intro-
duce LABR, the Large-scale Arabic Book Review
dataset. It is a set of over 63K book reviews, each
with a rating of 1 to 5 stars.
We make the following contributions: (1)
We present the largest Arabic sentiment analy-
sis dataset to-date (up to our knowledge); (2)
We provide standard splits for the dataset into
training and testing sets. This will make
comparing different results much easier. The
dataset and the splits are publicly available at
www.mohamedaly.info/datasets; (3) We explore
the structure and properties of the dataset, and per-
form baseline experiments for two tasks: senti-
ment polarity classification and rating classifica-
tion.
2 Related Work
A few Arabic sentiment analysis datasets have
been collected in the past couple of years, we men-
tion the relevant two sets:
OCA Opinion Corpus for Arabic (Rushdi-Saleh
et al, 2011b) contains 500 movie reviews in Ara-
bic, collected from forums and websites. It is di-
vided into 250 positive and 250 negative reviews,
although the division is not standard in that there is
no rating for neutral reviews i.e. for 10-star rating
systems, ratings above and including 5 are con-
sidered positive and those below 5 are considered
negative.
AWATIF is a multi-genre corpus for Mod-
ern Standard Arabic sentiment analysis (Abdul-
494
Number of reviews 63,257
Number of users 16,486
Avg. reviews per user 3.84
Median reviews per user 2
Number of books 2,131
Avg. reviews per book 29.68
Median reviews per book 6
Median tokens per review 33
Max tokens per review 3,736
Avg. tokens per review 65
Number of tokens 4,134,853
Number of sentences 342,199
Table 1: Important Dataset Statistics.
1 2 3 4 5Rating
0
5000
10000
15000
20000
25000
Num
ber 
of r
evie
ws
2,939 5,285
12,201
19,054
23,778
Figure 1: Reviews Histogram. The plot shows
the number of reviews for each rating.
Mageed and Diab, 2012a). It consists of about
2855 sentences of news wire stories, 5342 sen-
tences from Wikipedia talk pages, and 2532
threaded conversations from web forums.
3 Dataset Collection
We downloaded over 220,000 reviews from the
book readers social network www.goodreads.com
during the month of March 2013. These reviews
were from the first 2143 books in the list of Best
Arabic Books. After harvesting the reviews, we
found out that over 70% of them were not in Ara-
bic, either because some non-Arabic books exist in
the list, or because of existing translations of some
of the books in other languages. After filtering out
the non-Arabic reviews, and performing several
pre-processing steps to clean up HTML tags and
other unwanted content, we ended up with 63,257
Arabic reviews.
4 Dataset Properties
The dataset contains 63,257 reviews that were sub-
mitted by 16,486 users for 2,131 different books.
Task Training Set Test Set
1. Polarity Classification B 13,160 3,288U 40,845 10,211
2. Rating Classification B 11,760 2,935U 50,606 12,651
Table 2: Training and Test sets. B stands for bal-
anced, and U stands for Unbalanced.
Table 1 contains some important facts about the
dataset and Fig. 1 shows the number of reviews
for each rating. We consider as positive reviews
those with ratings 4 or 5, and negative reviews
those with ratings 1 or 2. Reviews with rating 3
are considered neutral and not included in the po-
larity classification. The number of positive re-
views is much larger than that of negative reviews.
We believe this is because the books we got re-
views for were the most popular books, and the
top rated ones had many more reviews than the the
least popular books.
The average user provided 3.84 reviews with the
median being 2. The average book got almost 30
reviews with the median being 6. Fig. 2 shows
the number of reviews per user and book. As
shown in the Fig. 2c, most books and users have
few reviews, and vice versa. Figures 2a-b show
a box plot of the number of reviews per user and
book. We notice that books (and users) tend to
have (give) positive reviews than negative reviews,
where the median number of positive reviews per
book is 5 while that for negative reviews is only 2
(and similarly for reviews per user).
Fig. 3 shows the statistics of tokens and sen-
tences. The reviews were tokenized and ?rough?
sentence counts were computed (by looking for
punctuation characters). The average number of
tokens per review is 65.4, the average number of
sentences per review is 5.4, and the average num-
ber of tokens per sentence is 12. Figures 3a-b
show that the distribution is similar for positive
and negative reviews. Fig. 3c shows a plot of the
frequency of the tokens in the vocabulary in a log-
log scale, which conforms to Zipf?s law (Manning
and Sch?tze, 2000).
5 Experiments
We explored using the dataset for two tasks: (a)
Sentiment polarity classification: where the goal
is to predict if the review is positive i.e. with rating
4 or 5, or is negative i.e. with rating 1 or 2; and (b)
495
All Pos Neg1
10
100
#re
view
s / u
ser
(a) Users
All Pos Neg1
10
100
1000
#re
view
s / b
ook
(b) Books
100 101 102 103 104# reviews
100
101
102
103
104
# o
f us
ers
/bo
oks
(c) Number of users/books
UsersBooks
Figure 2: Users and Books Statistics. (a) Box plot of the number of reviews per user for all, positive,
and negative reviews. The red line denotes the median, and the edges of the box the quartiles. (b) the
number of reviews per book for all, positive, and negative reviews. (c) the number of books/users with a
given number of reviews.
All Pos Neg
50
100
150
200
#to
ken
s / r
evie
w
(a) Tokens
All Pos Neg
5
10
15
20
#se
nte
nce
s / r
evie
w
(b) Sentences
100 101 102 103 104 105 106vocabulary token
100
101
102
103
104
105
106
freq
uen
cy
(c) Vocabulary
Figure 3: Tokens and Sentences Statistics. (a) the number of tokens per review for all, positive, and
negative reviews. (b) the number of sentences per review. (c) the frequency distribution of the vocabulary
tokens.
Rating classification: where the goal is to predict
the rating of the review on a scale of 1 to 5.
To this end, we divided the dataset into separate
training and test sets, with a ratio of 8:2. We do
this because we already have enough training data,
so there is no need to resort to cross-validation
(Pang et al, 2002). To avoid the bias of having
more positive than negative reviews, we explored
two settings: (a) a balanced split where the num-
ber of reviews from every class is the same, and
is taken to be the size of the smallest class (where
larger classes are down-sampled); (b) an unbal-
anced split where the number of reviews from ev-
ery class is unrestricted, and follows the distribu-
tion shown in Fig. 1. Table 2 shows the number of
reviews in the training and test sets for each of the
two tasks for the balanced and unbalanced splits,
while Fig. 4 shows the breakdown of these num-
bers per class.
Tables 3-4 show results of the experiments for
both tasks in both balanced/unbalanced settings.
We tried different features: unigrams, bigrams,
and trigrams with/without tf-idf weighting. For
classifiers, we used Multinomial Naive Bayes,
Bernoulli Naive Bayes (for binary counts), and
Support Vector Machines. We report two mea-
sures: the total classification accuracy (percentage
of correctly classified test examples) and weighted
F1 measure (Manning and Sch?tze, 2000). All
experiments were implemented in Python using
scikit-learn (Pedregosa et al, 2011) and Qalsadi
(available at pypi.python.org/pypi/qalsadi).
We notice that: (a) The total accuracy and
weighted F1 are quite correlated and go hand-in-
hand. (b) Task 1 is much easier than task 2, which
is expected. (c) The unbalanced setting seems eas-
496
Features Tf-Idf Balanced UnbalancedMNB BNB SVM MNB BNB SVM
1g No 0.801 / 0.801 0.807 / 0.807 0.766 / 0.766 0.887 / 0.879 0.889 / 0.876 0.880 / 0.877Yes 0.809 / 0.808 0.529 / 0.417 0.801 / 0.801 0.838 / 0.765 0.838 / 0.766 0.903 / 0.895
1g+2g No 0.821 / 0.821 0.821 / 0.821 0.789 / 0.789 0.893 / 0.877 0.891 / 0.873 0.892 / 0.888Yes 0.822 / 0.822 0.513 / 0.368 0.818 / 0.818 0.838 / 0.765 0.837 / 0.763 0.910 / 0.901
1g+2g+3g No 0.821 / 0.821 0.823 / 0.823 0.786 / 0.786 0.889 / 0.869 0.886 / 0.863 0.893 / 0.888Yes 0.827 / 0.827 0.511 / 0.363 0.821 / 0.820 0.838 / 0.765 0.837 / 0.763 0.910 / 0.901
Table 3: Task 1: Polarity Classification Experimental Results. 1g means using the unigram model,
1g+2g is using unigrams + bigrams, and 1g+2g+3g is using trigrams. Tf-Idf indicates whether tf-idf
weighting was used or not. MNB is Multinomial Naive Bayes, BNB is Bernoulli Naive Bayes, and SVM
is the Support Vector Machine. The numbers represent total accuracy / weighted F1 measure. See Sec.
5.
Features Tf-Idf Balanced UnbalancedMNB BNB SVM MNB BNB SVM
1g No 0.393 / 0.392 0.395 / 0.396 0.367 / 0.365 0.465 / 0.445 0.464 / 0.438 0.460 / 0.454Yes 0.402 / 0.405 0.222 / 0.128 0.387 / 0.384 0.430 / 0.330 0.379 / 0.229 0.482 / 0.472
1g+2g No 0.407 / 0.408 0.418 / 0.421 0.383 / 0.379 0.487 / 0.460 0.487 / 0.458 0.472 / 0.466Yes 0.419 / 0.423 0.212 / 0.098 0.411 / 0.407 0.432 / 0.325 0.379 / 0.217 0.501 / 0.490
1g+2g+3g No 0.405 / 0.408 0.417 / 0.420 0.384 / 0.381 0.487 / 0.457 0.484 / 0.452 0.474 / 0.467Yes 0.426 / 0.431 0.211 / 0.093 0.410 / 0.407 0.431 / 0.322 0.379 / 0.216 0.503 / 0.491
Table 4: Task 2: Rating Classification Experimental Results. See Table 3 and Sec. 5.
Negative Positive0
10000
20000
30000
40000
50000
# re
view
s
1,644
6,580
1,670
6,554
1,644
6,580
8,541
34,291
(a) Polarity Classification
Training - balancedTesting - balancedTraining - unbalancedTesting - unbalanced
1 2 3 4 5Rating
0
5000
10000
15000
20000
25000
# re
view
s
5872,352 6022,337 5872,352
1,088
4,197 5872,352
2,360
9,841 5872,352
3,838
15,216
5872,352
4,763
19,015
(b) Rating Classification
Training - balancedTesting - balancedTraining - unbalancedTesting - unbalanced
Figure 4: Training-Test Splits. (a) Histogram of
the number of training and test reviews for the po-
larity classification task for balanced (solid) and
unbalanced (hatched) cases. (b) The same for the
rating classification task. In the balanced set, all
classes have the same number of reviews as the
smallest class, which is done by down-sampling
the larger classes.
ier than the balanced one. This might be because
the unbalanced sets contain more training exam-
ples to make use of. (d) SVM does much better
in the unbalanced setting, while MNB is slightly
better than SVM in the balanced setting. (e) Using
more ngrams helps, and especially combined with
tf-idf weighting, as all the best scores are with tf-
idf.
6 Conclusion and Future Work
In this work we presented the largest Arabic sen-
timent analysis dataset to-date. We explored its
properties and statistics, provided standard splits,
and performed several baseline experiments to es-
tablish a benchmark. Although we used very sim-
ple features and classifiers, task 1 achieved quite
good results (~90% accuracy) but there is much
room for improvement in task 2 (~50% accuracy).
We plan next to work more on the dataset to
get sentence-level polarity labels, and to extract
Arabic sentiment lexicon and explore its poten-
tial. Furthermore, we also plan to explore using
Arabic-specific and more powerful features.
497
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Transactions on Information Systems
(TOIS).
Muhammad Abdul-Mageed and Mona Diab. 2012a.
Awatif: A multi-genre corpus for modern standard
arabic subjectivity and sentiment analysis. In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation.
Muhammad Abdul-Mageed and Mona Diab. 2012b.
Toward building a large-scale arabic sentiment lexi-
con. In Proceedings of the 6th International Global
Word-Net Conference.
Muhammad Abdul-Mageed, Mona Diab, and Mo-
hammed Korayem. 2011. Subjectivity and senti-
ment analysis of modern standard arabic. In 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
Muhammad Abdul-Mageed, Sandra K?bler, and Mona
Diab. 2012. Samar: A system for subjectivity and
sentiment analysis of arabic social media. In Pro-
ceedings of the 3rd Workshop in Computational Ap-
proaches to Subjectivity and Sentiment Analysis.
Mohamed Elarnaoty, Samir AbdelRahman, and Aly
Fahmy. 2012. A machine learning approach for
opinion holder extraction in arabic language. arXiv
preprint arXiv:1206.1011.
Mohammed Korayem, David Crandall, and Muham-
mad Abdul-Mageed. 2012. Subjectivity and sen-
timent analysis of arabic: A survey. In Advanced
Machine Learning Technologies and Applications.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing.
Christopher D. Manning and Hinrich Sch?tze. 2000.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2:1?135.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: Sentiment classification using machine learn-
ing techniques. In EMNLP.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011. Scikit-learn: Machine Learning in
Python . Journal of Machine Learning Research,
12:2825?2830.
M. Rushdi-Saleh, M. Mart?n-Valdivia, L. Ure?a-L?pez,
and J. Perea-Ortega. 2011a. Bilingual experiments
with an arabic-english corpus for opinion mining.
In Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP).
M. Rushdi-Saleh, M. Mart?n-Valdivia, L. Ure?a-L?pez,
and J. Perea-Ortega. 2011b. Oca: Opinion corpus
for arabic. Journal of the American Society for In-
formation Science and Technology.
498
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 121?126,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Arabic Spelling Correction using Supervised Learning
Youssef Hassan
Dept Computer Engineering
Cairo University
Giza, Egypt
youssefhassan13@gmail.com
Mohamed Aly
Dept Computer Engineering
Cairo University
Giza, Egypt
mohamed@mohamedaly.info
Amir Atiya
Dept Computer Engineering
Cairo University
Giza, Egypt
amir@alumni.caltech.edu
Abstract
In this work, we address the problem
of spelling correction in the Arabic lan-
guage utilizing the new corpus provided
by QALB (Qatar Arabic Language Bank)
project which is an annotated corpus of
sentences with errors and their corrections.
The corpus contains edit, add before, split,
merge, add after, move and other error
types. We are concerned with the first four
error types as they contribute more than
90% of the spelling errors in the corpus.
The proposed system has many models to
address each error type on its own and then
integrating all the models to provide an
efficient and robust system that achieves
an overall recall of 0.59, precision of 0.58
and F1 score of 0.58 including all the error
types on the development set. Our system
participated in the QALB 2014 shared task
?Automatic Arabic Error Correction? and
achieved an F1 score of 0.6, earning the
sixth place out of nine participants.
1 Introduction
The Arabic language is a highly inflected natural
language that has an enormous number of possi-
ble words (Othman et al., 2003). And although it
is the native language of over 300 million people,
it suffers from the lack of useful resources as op-
posed to other languages, specially English and
until now there are no systems that cover the wide
range of possible spelling errors. Fortunately the
QALB corpus (Zaghouani et al., 2014) will help
enrich the resources for Arabic language generally
and the spelling correction specifically by provid-
ing an annotated corpus with corrected sentences
from user comments, native student essays, non-
native data and machine translation data. In this
work, we are trying to use this corpus to build an
error correction system that can cover a range of
spelling errors.
This paper is a system description paper that is
submitted in the EMNLP 2014 conference shared
task ?Automatic Arabic Error Correction? (Mohit
et al., 2014) in the Arabic NLP workshop. The
challenges that faced us while working on this sys-
tem was the shortage of contribution in the area
of spelling correction in the Arabic language. But
hopefully the papers and the work in this shared
task specifically and in the workshop generally
will enrich this area and flourish it.
Our system targets four types of spelling errors,
edit errors, add before errors, merge errors and
split errors. For each error type, A model is built
to correct erroneous words detected by the error
detection technique. Edit errors and add before
errors are corrected using classifiers with contex-
tual features, while the merge and split errors are
corrected by inserting or omitting a space between
words and choosing the best candidate based on
the language model score of each candidate.
The rest of this paper is structured as follows.
In section 2, we give a brief background on re-
lated work in spelling correction. In section 3, we
introduce our system for spelling correction with
the description of the efficient models used in the
system. In section 4, we list some experimental re-
sults on the development set. In section 5, we give
some concluding remarks.
2 Related Work
The work in the field of spelling correction in the
Arabic language is not yet mature and no sys-
tem achieved a great error correction efficiency.
Even Microsoft Word, the most widely used Ara-
bic spelling correction system, does not achieve
good results. Our work was inspired by a num-
ber of papers. (Shaalan et al., 2012) addressed
the problem of Arabic Word Generation for spell
checking and they produced an open source and
121
large coverage word list for Arabic containing 9
million fully inflected surface words and applied
language models and Noisy Channel Model and
knowledge-based rules for error correction. This
word list is used in our work besides using lan-
guage models and Noisy Channel Model.
(Shaalan et al., 2010) proposed another sys-
tem for cases in which the candidate genera-
tion using edit algorithm only was not enough,
in which candidates were generated based on
transformation rules and errors are detected using
BAMA (Buckwalter Arabic Morphological Ana-
lyzer)(Buckwalter, 2002).
(Khalifa et al., 2011) proposed a system for text
segmentation. The system discriminates between
waw wasl and waw fasl, and depending on this
it can predict if the sentence to be segmented at
this position or not, they claim that they achieved
97.95% accuracy. The features used in this work
inspired us with the add before errors correction.
(Schaback, 2007) proposed a system for the En-
glish spelling correction, that is addressing the edit
errors on various levels: on the phonetic level us-
ing Soundex algorithm, on the character level us-
ing edit algorithm with one operation away, on the
word level using bigram language model, on the
syntactic level using collocation model to deter-
mine how fit the candidate is in this position and
on the semantic level using co-occurrence model
to determine how likely a candidate occurs within
the given context, using all the models output of
candidate word as features and using SVM model
to classify the candidates, they claim reaching re-
call ranging from 90% for first candidate and 97%
for all five candidates presented and outperform-
ing MS Word, Aspell, Hunspell, FST and Google.
3 Proposed System
We propose a system for detecting and correct-
ing various spelling errors, including edit, split,
merge, and add before errors. The system consists
of two steps: error detection and error correction.
Each word is tested for correctness. If the word
is deemed incorrect, it is passed to the correction
step, otherwise it remains unchanged. The correc-
tion step contains specific handling for each type
of error, as detailed in subsection 3.3.
3.1 Resources
Dictionary: Arabic wordlist for spell checking
1
is a free dictionary containing 9 million Ara-
bic words. The words are automatically generated
from the AraComLex
2
open-source finite state
transducer.
The dictionary is used in the generation
of candidates and using a special version of
MADAMIRA
3
(Pasha et al., 2014) created for the
QALB shared task using a morphological database
based on BAMA 1.2.1
4
(Buckwalter, 2002). Fea-
tures are extracted for each word of the dictionary
to help in the proposed system in order that each
candidate has features just like the words in the
corpus.
Stoplist: Using stop words list available on
sourceforge.net
5
. This is used in the collocation
algorithm described later.
Language Model: We use SRILM (Stolcke,
2002) to build a language model using the Ajdir
Corpora
6
as a corpus with the vocabulary from
the dictionary stated above. We train a language
model containing unigrams, bigrams, and trigrams
using modified Kneser-Ney smoothing (James,
2000).
QALB Corpus: QALB shared task offers a
new corpus for spelling correction. The corpus
contains a large dataset of manually corrected Ara-
bic sentences. Using this corpus, we were able
to implement a spelling correction system that
targets the most frequently occurring error types
which are (a) edit errors where a word is replaced
by another word, (b) add before errors where
a word was removed, (c) merge errors where a
space was inserted mistakenly and finally (d) split
errors where a space was removed mistakenly.
The corpus provided also has three other error
types but they occur much less frequently happen
which are (e) add after errors which is like the
add before but the token removed should be put af-
ter the word, (f) move errors where a word should
be moved to other place within the sentence and
(g) other errors where any other error that does
1
http://sourceforge.net/projects/
arabic-wordlist/
2
http://aracomlex.sourceforge.net/
3
MADAMIRA-release-20140702-1.0
4
AraMorph 1.2.1 - http://sourceforge.net/
projects/aramorph/
5
http://sourceforge.net/projects/
arabicstopwords/
6
http://aracorpus.e3rab.com/
argistestsrv.nmsu.edu/AraCorpus/
122
not lie in the six others is labeled by it.
3.2 Error Detection
The training set, development set and test set pro-
vided by QALB project come with the ?columns
file? and contains very helpful features generated
by MADAMIRA. Using the Buckwalter morpho-
logical analysis (Buckwalter, 2002) feature, we
determine if a word is correct or not. If the word
has no analysis, we consider the word as incorrect
and pass it through the correction process.
3.3 Edit Errors Correction
The edit errors has the highest portion of total er-
rors in the corpus. It amounts to more than 55% of
the total errors. To correct this type of errors, we
train a classifier with features like the error model
probability, collocation and co-occurrence as fol-
lows:
Undiacriticized word preprocessed: Utilizing
the MADAMIRA features of each word, the undi-
acriticized word fixes some errors like hamzas, the
pair of haa and taa marboutah and the pair of yaa
and alif maqsoura.
We apply some preprocessing on the undiacrit-
icized word to make it more useful and fix the is-
sues associated with it. For example we remove
the incorrect redundant characters from the word
e.g (?@@ @ Ag
.
Q? @ ? ?Ag
.
Q? @, AlrjAAAAl ? AlrjAl).
We also replace the Roman punctuation marks by
the Arabic ones e.g (? ? ?).
Language Model: For each candidate, A un-
igram, bigram and trigram values from the lan-
guage model trained are retrieved. In addition to a
feature that is the product of the unigram, bigram
and trigram values.
Likelihood Model: The likelihood model is
trained by iterating over the training sentences
counting the occurrences of each edit with the
characters being edited and the type of edit. The
output of this is called a confusion matrix.
The candidate score is based on the Noisy
Channel Model (Kernighan et al., 1990) which is
the multiplication of probabilty of the proposed
edit using the confusion matrix trained which is
called the error model, and the language model
score of that word. The language model used is
unigram, bigram and trigram with equal weights.
Add-1 smoothing is used for both models in the
counts.
Score = p(x|w).p(w)
where x is the wrong word and w is the candidate
correction.
For substitution edit candidates, we give higher
score for substitution of a character that is close on
the keyboard or the substitution pair belongs to the
same group of letter groups (Shaalan et al., 2012)
by multiplying the score by a constant greater than
one.
,(h
.
, h , p) ,(H
.
,

H ,

H ,
	
? , ?


) ,(@ ,

@ , @

,

@)
,(? ,
	
?) ,(? ,
	
?) ,(? ,

?) ,(P ,
	
P) ,(X ,
	
X)
.(?


, ?) ,(? ,

?) ,( ? ,

?) ,(
	
? ,

?) ,(? ,
	
?)
(|, < , >, A), (y, n, v, t, b), (x, H, j), (*, d), (z, r),
($, s), (D, S), (Z, T), (g, E), (q, f), (p h), (&, w),
(Y, y)
For each candidate , the likelihood score is com-
puted and added to the feature vector of the candi-
date.
Collocation: The collocation model targets the
likelihood of the candidate inside the sentence.
This is done using the lemma of the word and the
POS tags of words in the sentence.
We use the algorithm in (Schaback, 2007) for
training the collocation model. Specifically, by re-
trieving the 5,000 most occurring lemmas in the
training corpus and put it in list L. For each lemma
in L, three lists are created, each record in the list
is a sequence of three POS tags around the target
lemma. For training, we shift a window of three
POS tags over the training sentence. If a lemma
belongs to L, we add the surrounding POS tags to
the equivalent list of the target lemma depending
on the position of the target lemma within the three
POS tags.
Given a misspelled word in a sentence, for each
candidate correction, if it is in the L list, we count
the number of occurrences of the surrounding POS
tags in each list of the three depending on the po-
sition of of the candidate.
The three likelihoods are stored in the feature
vector of the candidate in addition to the product
of them.
Co-occurrence: Co-occurrence is used to mea-
sure how likely a word fits inside a context. Where
L is the same list of most frequent lemmata from
collocation.
We use the co-occurrence algorithm in (Sch-
aback, 2007). Before training the model, we trans-
form each word of our training sentence into its
lemma form and remove stop-words. For exam-
ple, consider the original text:
123
A?
	
E

@ A?
?
.

?J


?Am
?
'@

????m
?
'@? PA??

J?B@
	
?



K
.

?Q
	
?

B

IJ


k
Hyv l>frq byn AlAstEmAr wAlHkwmp
AlHAlyp bmA >nhA
After removing stop-words and replacing the
remaining words by their lemma form we end up
with:
?


?Ag

????k PA??

J?@

?Q
	
?

@
>frq AstEmAr Hkwmp HAly
which forms C.
From that C, we get all lemmata that appear in
the radius of 10 words around the target lemma
b where b belongs to L. We count the number of
occurrences of each lemma in that context C.
By using the above model, three distances are
calculated for target lemma b: d
1
, the ratio of ac-
tually found context words in C and possibly find-
able context words. This describes how similar the
trained context and the given context are for can-
didate b; d
2
considers how significant the found
context lemmata are by summing the normalized
frequencies of the context lemmata. As a third fea-
ture; d
3
(b) that simply measures how big the vec-
tor space model for lemma b is.
For each candidate, the model is applied and the
three distances are calculated and added to the fea-
ture vector of that candidate.
The Classifier: After generating the candidate
corrections within 1 and 2 edit operations (insert,
delete, replace and transpose) distance measured
by Levenshtein distance (Levenshtein, 1966), we
run them through a Naive-Bayes classifier using
python NLTK?s implementation to find out which
one is the most likely to be the correction for the
incorrect word.
The classifier is trained using the training set
provided by QALB project. For each edit correc-
tion in the training set, all candidates are gener-
ated for the incorrect word and a feature vector
(as shown in table1) is calculated using the tech-
niques aforementioned. If the candidate is the cor-
rect one, the label for the training feature vector is
correct else it is incorrect.
Then using the trained classifier, the same is
done on the development set or the test set where
we replace the incorrect word with the word sug-
gested by the classifier.
3.4 Add before Errors Correction
The add before errors are mostly punctuation er-
rors. A classifier is trained on the QALB training
Table 1: The feature set used by the edit errors
classifier.
Feature name
Likelihood model probability
unigram probability
previous bigram probability
next bigram probability
trigram probability
language model product
collocation left
collocation right
collocation mid
collocation product
cooccurrence distance 1
cooccurrence distance 2
cooccurrence distance 3
previous gender
previous number
next gender
next number
corpus. A classifier is implemented with contex-
tual features C. C is a 4-gram around the token be-
ing investigated. Each word of these four has the
two features: The token itself and Part-of-speech
tag and for the next word only pregloss because
if the word?s pregloss is ?and? it is more prob-
able that a new sentence began. Those features
are available thanks to MADAMIRA features pro-
vided with the corpus and the generated for dictio-
nary words.
The classifier is trained on the QALB training
set. We iterate over all the training sentences word
by word and getting the aforementioned features
(as shown in table 2) and label the training with
the added before token if there was a matching add
before correction for this word or the label will be
an empty string.
For applying the model, the same is done on the
QALB development sentences after removing all
punctuations as they are probably not correct and
the output of the classifier is either empty or sug-
gested token to add before current word.
3.5 Merge Errors Correction
The merge errors occurs due to the insertion of
a space between two words by mistake. The ap-
proach is simply trying to attach every word with
its successor word and checking if it is a valid
124
Table 2: The feature set used by the add before
errors classifier.
Feature name
before previous word
before previous word POS tag
previous word
previous word POS tag
next word
next word POS tag
next word pregloss
after next word
after next POS tag
Arabic word and rank it with the language model
score.
3.6 Split Errors Correction
The split errors occurs due to the deletion of a
space between two words. The approach is sim-
ply getting all the valid partitions of the word and
try to correct both partitions and give them a rank
using the language model score. The partition is at
least two characters long.
4 Experimental Results
In order to know the contribution of each error
type models to the overall system performance, we
adopted an incremental approach of the models.
We implemented the system using python
7
and
NLTK
8
(Loper and Bird, 2002) toolkit. The mod-
els are trained on the QALB corpus training set
and the results are obtained by applying the trained
models on the development set. Our goal was to
achieve high recall but without losing too much
precision. The models were evaluated using M2
scorer (Dahlmeier and Ng, 2012).
First, we start with only the preprocessed un-
diacriticized word, then we added our edit error
classifier. Adding the add before classifier was a
great addition to the system as the system was able
to increase the number of corrected errors signif-
icantly, notably the add before classifier proposed
too many incorrect suggestions that decreased the
precision. Then we added the merging correction
technique. Finally we added the split error cor-
rection technique. The system corrects 9860 errors
versus 16659 golden error corrections and pro-
7
https://www.python.org/
8
http://www.nltk.org/
posed 17057 correction resulting in the final sys-
tem recall of 0.5919, precision of 0.5781 and F1
score of 0.5849. Details are shown in Table 3.
Table 3: The incremental results after adding each
error type model and applying them on the devel-
opment set.
Model name Recall Precision F1 score
Undiacriticized 0.32 0.833 0.4715
+ Edit 0.3515 0.7930 0.5723
+ Add before 0.5476 0.5658 0.5567
+ Merge 0.5855 0.5816 0.5836
+ Split 0.5919 0.5781 0.5849
We tried other combinations of the models by
removing one or more of the components to get the
best results possible. Noting that all the systems
results are using the undiacriticized word. Details
are shown in Table 4
Table 4: The results of some combinations of the
models and applying them on the development set.
The models are abbreviated as Edit E, Merge M,
Split S, and Add before A.
Model name Precision Recall F1 score
M Only 0.8441 0.3724 0.5167
S Only 0.7838 0.338 0.5167
A Only 0.6008 0.4887 0.539
E Only 0.8143 0.3472 0.4868
M & S 0.8121 0.3814 0.5191
E & S 0.62 0.3542 0.4508
M & E 0.6184 0.5403 0.5767
S & M & A 0.6114 0.5396 0.5733
M & E & A 0.6186 0.5404 0.5768
E & S & A 0.5955 0.507 0.5477
E & S & M 0.6477 0.3969 0.4922
E & S & M & A 0.5919 0.5781 0.5849
5 Conclusion and Future Work
We propose an all-in-one system for error detec-
tion and correction. The system addresses four
types of spelling errors (edit, add before, merge
and split errors). The system achieved promis-
ing results by successfully getting corrections for
about 60% of the spelling errors in the develop-
ment set. Also, There is still a big room for im-
provements in all types of error correction models.
We are planning to improve the current system
by incorporating more intelligent techniques and
models for split and merge. Also, the add before
classifier needs much work to improve the cov-
erage as the errors are mostly missing punctua-
tion marks. For the edit classifier, real-word errors
need to be addressed.
125
References
Tim Buckwalter. 2002. Buckwalter arabic morpholog-
ical analyzer version 1.0. November.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 568?572, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Frankie James. 2000. Modified kneser-ney smoothing
of n-gram models. RIACS.
Mark D. Kernighan, Kenneth W. Church, and
William A. Gale. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proceed-
ings of the 13th Conference on Computational Lin-
guistics - Volume 2, COLING ?90, pages 205?210,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Iraky Khalifa, Zakareya Al Feki, and Abdelfatah
Farawila. 2011. Arabic discourse segmentation
based on rhetorical methods.
VI Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. vol-
ume 10, page 707.
Edward Loper and Steven Bird. 2002. NLTK: The
Natural Language Toolkit.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Eman Othman, Khaled Shaalan, and Ahmed Rafea.
2003. A chart parser for analyzing modern standard
arabic sentence. In To appear in In proceedings of
the MT Summit IX Workshop on Machine Transla-
tion for Semitic Languages: Issues and Approaches,
Louisiana, U.S.A.
Pasha, Arfath, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan M. Roth.
2014. Madamira: A fast, comprehensive tool for
morphological analysis and disambiguation of ara-
bic. In In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
Johannes Schaback. 2007. Multi-level feature extrac-
tion for spelling correction. Hyderabad, India.
K. Shaalan, R. Aref, and A Fahmy. 2010. An approach
for analyzing and correcting spelling errors for non-
native arabic learners. In Informatics and Systems
(INFOS), 2010 The 7th International Conference on,
pages 1?7, March.
Khaled Shaalan, Mohammed Attia, Pavel Pecina,
Younes Samih, and Josef van Genabith. 2012. Ara-
bic word generation and modelling for spell check-
ing. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur
Doan, Bente Maegaard, Joseph Mariani, Asuncion
Moreno, Jan Odijk, and Stelios Piperidis, editors,
Proceedings of the Eight International Conference
on Language Resources and Evaluation (LREC?12),
Istanbul, Turkey, may. European Language Re-
sources Association (ELRA).
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, Denver,U.S.A.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale arabic error annotation: Guidelines and
framework. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?14), Reykjavik, Iceland, May. Euro-
pean Language Resources Association (ELRA).
126
