Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 26?27,
Vancouver, October 2005.
Japanese Speech Understanding Using Grammar Specialization
Manny Rayner, Nikos Chatzichrisafis, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
mrayner@riacs.edu
{Pierrette.Bouillon,Nikolaos.Chatzichrisafis}@issco.unige.ch
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Beth Ann Hockey
UCSC/NASA Ames Research Center
Moffet Field, CA 94035
bahockey@riacs.edu
Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Marianne.Santaholma@eti.unige.ch
Marianne.Starlander@eti.unige.ch
The most common speech understanding archi-
tecture for spoken dialogue systems is a combination
of speech recognition based on a class N-gram lan-
guage model, and robust parsing. For many types
of applications, however, grammar-based recogni-
tion can offer concrete advantages. Training a
good class N-gram language model requires sub-
stantial quantities of corpus data, which is gen-
erally not available at the start of a new project.
Head-to-head comparisons of class N-gram/robust
and grammar-based systems also suggest that users
who are familiar with system coverage get better re-
sults from grammar-based architectures (Knight et
al., 2001). As a consequence, deployed spoken dia-
logue systems for real-world applications frequently
use grammar-based methods. This is particularly
the case for speech translation systems. Although
leading research systems like Verbmobil and NE-
SPOLE! (Wahlster, 2000; Lavie et al, 2001) usu-
ally employ complex architectures combining sta-
tistical and rule-based methods, successful practical
examples like Phraselator and S-MINDS (Phrasela-
tor, 2005; Sehda, 2005) are typically phrasal trans-
lators with grammar-based recognizers.
Voice recognition platforms like the Nuance
Toolkit provide CFG-based languages for writing
grammar-based language models (GLMs), but it is
challenging to develop and maintain grammars con-
sisting of large sets of ad hoc phrase-structure rules.
For this reason, there has been considerable inter-
est in developing systems that permit language mod-
els be specified in higher-level formalisms, normally
some kind of unification grammar (UG), and then
compile these grammars down to the low-level plat-
form formalisms. A prominent early example of this
approach is the Gemini system (Moore, 1998).
Gemini raises the level of abstraction signifi-
cantly, but still assumes that the grammars will be
domain-dependent. In the Open Source REGULUS
project (Regulus, 2005; Rayner et al, 2003), we
have taken a further step in the direction of increased
abstraction, and derive all recognizers from a sin-
gle linguistically motivated UG. This derivation pro-
cedure starts with a large, application-independent
UG for a language. An application-specific UG is
then derived using an Explanation Based Learning
(EBL) specialization technique. This corpus-based
specialization process is parameterized by the train-
ing corpus and operationality criteria. The training
corpus, which can be relatively small, consists of ex-
amples of utterances that should be recognized by
the target application. The sentences of the corpus
are parsed using the general grammar, then those
parses are partitioned into phrases based on the op-
erationality criteria. Each phrase defined by the
operationality criteria is flattened, producing rules
of a phrasal grammar for the application domain.
This application-specific UG is then compiled into
26
a CFG, formatted to be compatible with the Nuance
recognition platform. The CFG is compiled into the
runtime recognizer using Nuance tools.
Previously, the REGULUS grammar specialization
programme has only been implemented for English.
In this demo, we will show how we can apply the
same methodology to Japanese. Japanese is struc-
turally a very different language from English, so it
is by no means obvious that methods which work
for English will be applicable in this new context:
in fact, they appear to work very well. We will
demo the grammars and resulting recognizers in the
context of Japanese ? English and Japanese ?
French versions of the Open Source MedSLT medi-
cal speech translation system (Bouillon et al, 2005;
MedSLT, 2005).
The generic problem to be solved when building
any sort of recognition grammar is that syntax alone
is insufficiently constraining; many of the real con-
straints in a given domain and use situation tend to
be semantic and pragmatic in nature. The challenge
is thus to include enough non-syntactic constraints
in the grammar to create a language model that can
support reliable domain-specific speech recognition:
we sketch our solution for Japanese.
The basic structure of our current general
Japanese grammar is as follows. There are four main
groups of rules, covering NP, PP, VP and CLAUSE
structure respectively. The NP and PP rules each as-
sign a sortal type to the head constituent, based on
the domain-specific sortal constraints defined in the
lexicon. VP rules define the complement structure
of each syntactic class of verb, again making use of
the sortal features. There are also rules that allow
a VP to combine with optional adjuncts, and rules
which allow null constituents, in particular null sub-
jects and objects. Finally, clause-level rules form a
clause out of a VP, an optional subject and optional
adjuncts. The sortal features constrain the subject
and the complements combining with a verb, but the
lack of constraints on null constituents and optional
adjuncts still means that the grammar is very loose.
The grammar specialization mechanism flattens the
grammar into a set of much simpler structures, elim-
inating the VP level and only permitting specific pat-
terns of null constituents and adjuncts licenced by
the training corpus.
We will demo several different versions of the
Japanese-input medical speech translation system,
differing with respect to the target language and
the recognition architecture used. In particular, we
will show a) that versions based on the specialized
Japanese grammar offer fast and accurate recogni-
tion on utterances within the intended coverage of
the system (Word Error Rate around 5%, speed un-
der 0.1?RT), b) that versions based on the original
general Japanese grammar are much less accurate
and more than an order of magnitude slower.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
A. Lavie, C. Langley, A. Waibel, F. Pianesi, G. Lazzari,
P. Coletti, L. Taddei, and F. Balducci. 2001. Ar-
chitecture and design considerations in NESPOLE!:
a speech translation system for e-commerce applica-
tions. In Proceedings of HLT: Human Language Tech-
nology Conference, San Diego, California.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 9 June 2005.
R. Moore. 1998. Using natural language knowledge
sources in speech recognition. In Proceedings of the
NATO Advanced Studies Institute.
Phraselator, 2005. http://www.phraselator.com/. As of 9
June 2005.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 9 June 2005.
Sehda, 2005. http://www.sehda.com/. As of 9 June 2005.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
27
Evaluating Task Performance for a Unidirectional Controlled Language 
Medical Speech Translation System 
 
 
Nikos Chatzichrisafis, Pierrette Bouillon, Manny Rayner, Marianne Santaholma, 
Marianne Starlander 
University of Geneva, TIM/ISSCO 
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland 
 
Nikos.Chatzichrisafis@vozZup.com, Pierrette.Bouillon@issco.unige.ch, 
Emmanuel.Rayner@issco.unige.ch, Marianne.Santaholma@eti.unige.ch, 
Marianne.Starlander@eti.unige.ch  
 
Beth Ann Hockey 
UCSC 
NASA Ames Research Center 
Moffett Field, CA 94035 
bahockey@email.arc.nasa.gov 
 
  
Abstract 
We present a task-level evaluation of the 
French to English version of MedSLT, a 
medium-vocabulary unidirectional con-
trolled language medical speech transla-
tion system designed for doctor-patient 
diagnosis interviews. Our main goal was 
to establish task performance levels of 
novice users and compare them to expert 
users. Tests were carried out on eight 
medical students with no previous expo-
sure to the system, with each student us-
ing the system for a total of three 
sessions. By the end of the third session, 
all the students were able to use the sys-
tem confidently, with an average task 
completion time of about 4 minutes. 
1 Introduction 
Medical applications have emerged as one of the 
most promising application areas for spoken lan-
guage translation, but there is still little agreement 
about the question of architectures. There are in 
particular two architectural dimensions which we 
will address: general processing strategy (statistical 
or grammar-based), and top-level translation func-
tionality (unidirectional or bidirectional transla-
tion). Given the current state of the art in 
recognition and machine translation technology, 
what is the most appropriate combination of 
choices along these two dimensions? 
Reflecting current trends, a common approach 
for speech translation systems is the statistical one. 
Statistical translation systems rely on parallel cor-
pora of source and target language texts, from 
which a translation model is trained. However, this 
is not necessarily the best alternative in safety-
critical medical applications. Anecdotally, many 
doctors express reluctance to trust a translation 
device whose output is not readily predictable, and 
most of the speech translation systems which have 
reached the stage of field testing rely on various 
types of grammar-based recognition and rule-based 
translation (Phraselator, 2006; S-MINDS, 2006; 
MedBridge, 2006). Even though statistical systems 
exhibit many desirable properties (purely data-
driven, domain independence), grammar-based 
systems utilizing probabilistic context-free gram-
mar tuning appear to deliver better results when 
training data is sparse (Rayner et al, 2005a). 
One drawback of grammar-based systems is that 
out-of-coverage utterances will be neither recog-
nized nor translated, an objection that critics have 
sometimes painted as decisive. It is by no means 
obvious, however, that restricted coverage is such 
a serious problem. In text processing, work on sev-
eral generations of controlled language systems has 
developed a range of techniques for keeping users 
within the bounds of system coverage (Kittredge, 
2003; Mitamura, 1999). If these techniques work 
for text processing, it is surely not inconceivable 
that variants of them will be equally successful for 
spoken language applications. Users are usually 
able to adapt to a controlled language system given 
enough time. The critical questions are how to 
provide efficient support to guide them towards the 
system's coverage, and how much time they will 
then need before they have acclimatized. 
With regard to top-level translation functional-
ity, the choice is between unidirectional and bidi-
rectional systems. Bidirectional systems are 
certainly possible today1, but the arguments in fa-
vor of them are not as clear-cut as might first ap-
pear. Ceteris paribus, doctors would certainly 
prefer bidirectional systems; in particular, medical 
students are trained to conduct examination dia-
logues using ?open questions? (WH-questions), 
and to avoid leading the patient by asking YN-
questions. 
The problem with a bidirectional system is, 
however, that open questions only really work well 
if the system can reliably handle a broad spectrum 
of replies from the patients, which is over-
optimistic given the current state of the art. In prac-
tice, the system's coverage is always more or less 
restricted, and some experimentation is required 
before the user can understand what language it is 
capable of handling. A doctor, who uses the system 
regularly, will acquire the necessary familiarity. 
The same might be true for a few patients, if spe-
cial circumstances mean that they encounter 
speech translation applications reasonably fre-
quently. Most patients, however, will have had no 
previous exposure to the system, and may be un-
willing to use a type of technology which they 
have trouble understanding.  
A unidirectional system, in which the doctor 
mostly asks YN-questions, will never be ideal. If, 
                                                          
1
 For example, the S-MINDS system (S-MINDS, 2006) 
offers bidirectional translation. 
however, the doctor can become proficient in using 
it, it may still be very much better than the alterna-
tive of no translation assistance at all.  
To summarize, today?s technology definitely 
lets us build unidirectional grammar-based medical 
speech translation systems which work for regular 
users who have had time to adapt to their limita-
tions. While bidirectional systems are possible, the 
case for them is less obvious, since users on the 
patient side may not in practice be able to use them 
effectively. 
In this paper, we will empirically investigate the 
ability of medical students to adapt to the coverage 
of unidirectional spoken language translation sys-
tem. We report a series of experiments, carried out 
using a French to English speech translation sys-
tem, in which medical students with no previous 
experience to the system were asked to use it to 
carry out a series of verbal examinations on sub-
jects who were simulating the symptoms of various 
types of medical conditions. Evaluation will be 
focused on usability. We primarily want to know 
how quickly subjects learn to use the system, and 
how their performance compares to that of expert 
users. 
2 The MedSLT system 
MedSLT (MedSLT, 2005; Bouillon et al, 2005) 
is a unidirectional, grammar-based medical speech 
translation system intended for use in doctor-
patient diagnosis dialogues. The system is built on 
top of Regulus (Regulus, 2006), an Open Source 
platform for developing grammar-based speech 
applications. Regulus supports rapid construction 
of complex grammar-based language models using 
an example-based method (Rayner et al, 2003; 
Rayner et al, 2006), which extracts most of the 
structure of the model from a general linguistically 
motivated resource grammar. Regulus-based rec-
ognizers are reasonably easy to maintain, and 
grammar structure is shared automatically across 
different subdomains. Resource grammars are now 
available for several languages, including English, 
Japanese (Rayner et al, 2005b), French (Bouillon 
et al, 2006) and Spanish. 
MedSLT includes a help module, whose purpose 
is to add robustness to the system and guide the 
user towards the supported coverage. The help 
module uses a second backup recognizer, equipped 
with a statistical language model; it matches the 
results from this second recognizer against a cor-
pus of utterances, which are within system cover-
age and have already been judged to give correct 
translations. In previous studies (Rayner et al, 
2005a; Starlander et al, 2005), we showed that the 
grammar-based recognizer performs much better 
than the statistical one on in-coverage utterances, 
and rather worse on out-of-coverage ones. We also 
found that having the help module available ap-
proximately doubled the speed at which subjects 
learned to use the system, measured as the average 
difference in semantic error rate between the re-
sults for their first quarter-session and their last 
quarter-session. It is also possible to recover from 
recognition errors by selecting one of the displayed 
help sentences; in the cited studies, we found that 
this increased the number of acceptably processed 
utterances by about 10%. 
The version of MedSLT used for the experi-
ments described in the present paper was config-
ured to translate from spoken French into spoken 
English in the headache subdomain. Coverage is 
based on standard headache-related examination 
questions obtained from a doctor, and consists 
mostly of yes/no questions. WH-questions and el-
liptical constructions are also supported. A typical 
short session with MedSLT might be as follows: 
- is the pain in the side of the head? 
- does the pain radiate to the neck? 
- to the jaw? 
- do you usually have headaches in the morn-
ing ?  
The recognizer?s vocabulary is about 1000 sur-
face words; on in-grammar material, Word Error 
Rate is about 8% and semantic error rate (per ut-
terance) about 10% (Bouillon et al, 2006). Both 
the main grammar-based recognizer and the statis-
tical recognizer used by the help system were 
trained from the same corpus of about 975 utter-
ances. Help sentences were also taken from this 
corpus. 
3 Experimental Setup 
In previous work, we have shown how to build a 
robust and extendable speech translation system. 
We have focused on performance metrics defined 
in terms of recognition and translation quality, and 
tested the system on na?ve users without any medi-
cal background (Bouillon et al, 2005; Rayner et 
al., 2005a; Starlander et al, 2005). 
In this paper, our primary goal was rather to fo-
cus on task performance evaluation using plausible 
potential users. The basic methodology used is 
common in evaluating usability in software sys-
tems in general, and spoken language systems in 
particular (Cohen et. al 2000). We defined a simu-
lated situation, where a French-speaking doctor 
was required to carry out a verbal examination of 
an English-speaking patient who claimed to be suf-
fering from a headache, using the MedSLT system 
to translate all their questions. The patients were 
played by members of the development team, who 
had been trained to answer questions consistently 
with the symptoms of different medical conditions 
which could cause headaches. We recruited eight 
native French-speaking medical students to play 
the part of the doctor. All of the students had com-
pleted at least four years of medical school; five of 
them were already familiar with the symptoms of 
different types of headaches, and were experienced 
in real diagnosis situations. 
The experiment was designed to study how well 
users were able to perform the task using the 
MedSLT system. In particular, we wished to de-
termine how quickly they could adapt to the re-
stricted language and limited coverage of the 
system. As a comparison point, representing near-
perfect performance, we also carried out the same 
test on two developers who had been active in im-
plementing the system, and were familiar with its 
coverage. 
Since it seemed reasonable to assume that most 
users would not interact with the system on a daily 
basis, we conducted testing in three sessions, with 
an interval of two days between each session. At 
the beginning of the first session, subjects were 
given a standardized 10-minute introduction to the 
system. This consisted of instruction on how to set 
up the microphone, a detailed description of the 
MedSLT push-to-talk interface, and a video clip 
showing the system in action. At the end of the 
presentation, the subject was given four sample 
sentences to get familiar with the system. 
After the training was completed, subjects were 
asked to play the part of a doctor, and conduct an 
examination through the system. Their task was to 
identify the headache-related condition simulated 
by the ?patient?, out of nine possible conditions. 
Subjects were given definitions of the simulated 
headache types, which included conceptual infor-
mation about location, duration, frequency, onset 
and possible other symptoms the particular type of 
headache might exhibit. 
Subjects were instructed to signal the conclusion 
of their examination when they were sure about the 
type of simulated headache. The time required to 
reach a conclusion was noted in the experiment 
protocols by the experiment supervisor. 
The subjects repeated the same diagnosis task on 
different predetermined sets of simulated condi-
tions during the second and third sessions. The ses-
sions were concluded either when a time limit of 
30 minutes was reached, or when the subject com-
pleted three headache diagnoses. At the end of the 
third session, the subject was asked to fill out a 
questionnaire. 
4 Results 
Performance of a speech translation system is 
best evaluated by looking at system performance 
as a whole, and not separately for each subcompo-
nent in the systems processing pipeline (Rayner et. 
al. 2000, pp. 297-pp. 312). In this paper, we conse-
quently focus our analysis on objective and subjec-
tive usability-oriented measures. 
In Section 4.1, we present objective usability 
measures obtained by analyzing user-system inter-
actions and measuring task performance. In Sec-
tion 4.2, we present subjective usability figures and 
a preliminary analysis of translation quality. 
4.1 Objective Usability Figures 
4.1.1 Analysis of User Interactions 
Most of our analysis is based on data from the 
MedSLT system log, which records all interactions 
between the user and the system. An interaction is 
initiated when the user presses the ?Start Recogni-
tion? button. The system then attempts to recog-
nize what the user says. If it can do so, it next 
attempts to show the user how it has interpreted the 
recognition result, by first translating it into the 
Interlingua, and then translating it back into the 
source language (in this case, French). If the user 
decides that the back-translation is correct, they 
press the ?Translate? button. This results in the 
system attempting to translate the Interlingua rep-
resentation into the target language (in this case, 
English), and speak it using a Text-To-Speech en-
gine. The system also displays a list of ?help sen-
tences?, consisting of examples that are known to 
be within coverage, and which approximately 
match the result of performing recognition with the 
statistical language model. The user has the option 
of choosing a help sentence from the list, using the 
mouse, and submitting this to translation instead.  
We classify each interaction as either ?success-
ful? or ?unsuccessful?. An interaction is defined to 
be unsuccessful if either 
i) the user re-initiates recognition without 
asking the system for a translation, or 
ii) the system fails to produce a correct 
translation or back translation. 
Our definition of ?unsuccessful interaction? in-
cludes instances where users accidentally press the 
wrong button (i.e. ?Start Recognition? instead of 
?Translate?), press the button and then say nothing, 
or press the button and change their minds about 
what they want to ask half way through. We ob-
served all of these behaviors during the tests. 
Interactions where the system produced a trans-
lation were counted as successful, irrespective of 
whether the translation came directly from the 
user?s spoken input or from the help list. In at least 
some examples, we found that when the translation 
came from a help sentence it did not correspond 
directly to the sentence the user had spoken; to our 
surprise, it could even be the case that the help sen-
tence expressed the directly opposite question to 
the one the user had actually asked. This type of 
interaction was usually caused by some deficiency 
in the system, normally bad recognition or missing 
coverage. Our informal observation, however, was 
that, when this kind of thing happened, the user 
perceived the help module positively: it enabled 
them to elicit at least some information from the 
patient, and was less frustrating than being forced 
to ask the question again. 
Table I to Table III show the number of total in-
teractions per session, the proportion of successful 
interactions, and the proportion of interactions 
completed by selecting a sentence from the help 
list. The total number of interactions required to 
complete a session decreased over the three ses-
sions, declining from an average of 98.6 interac-
tions in the first session to 63.4 in the second (36% 
relative) and 53.9 in the third (45% relative). It is 
interesting to note that interactions involving the 
help system did not decrease in frequency, but re-
mained almost constant over the first two sessions 
(15.5% and 14.0%), and were in fact most com-
mon during the third session (21.7%). 
 
Session 1 
Subject Interactions % Successful % Help 
User 1 57 56.1% 0.0% 
User 2 98 52.0% 25.5% 
User 3 91 63.7% 15.4% 
User 4 156 69.9% 10.3% 
User 5 86 64.0% 22.1% 
User 6 134 47.0% 19.4% 
User 7 56 53.6% 5.4% 
User 8 111 63.1% 26.1% 
AVG 98.6 58.7% 15.5% 
Table I Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 1st session 
 
Session 2 
Subject Interactions % Successful % Help 
User 1 50 74.0% 2.0% 
User 2 63 55.6% 27.0% 
User 3 34 88.2% 23.5% 
User 4 96 57.3% 17.7% 
User 5 64 65.6% 21.9% 
User 6 93 68.8% 10.8% 
User 7 48 60.4% 4.2% 
User 8 59 79.7% 5.1% 
AVG 63.4 68.7% 14.0% 
Table II Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 2nd session 
 
Session 3 
Subject Interactions % Successful % Help 
User 1 33 90.9% 33.3% 
User 2 57 56.1% 22.8% 
User 3 48 72.9% 29.2% 
User 4 67 70.2% 16.4% 
User 5 68 73.5% 27.9% 
User 6 60 70.0% 6.7% 
User 7 41 65.9% 14.6% 
User 8 57 56.1% 22.8% 
AVG 53.9 69.5% 21.7% 
Table III Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 3rd session 
In order to establish a performance baseline, we 
also analyzed interaction data for two expert users, 
who performed the same experiment. The expert 
users were two native French-speaking system de-
velopers, which were both familiar with the diag-
nosis domain. Table IV summarizes the results of 
those users. One of our expert users, listed as Ex-
pert 2, is the French grammar developer, and had 
no failed interactions. This confirms that recogni-
tion is very accurate for users who know the cov-
erage. 
 
Session 1 / Expert Users 
Subject Interactions % Successful % Help 
Expert 1 36 77.8% 13.9% 
Expert 2 30 100.0% 3.3% 
AVG 33 88.9% 8.6% 
Table IV Number of interactions, and percentages 
of successful interactions, and interactions 
involving the help component 
 
The expert users were able to complete the ex-
periment using an average of 33 interaction rounds. 
Similar performance levels were achieved by some 
subjects during the second and third session, which 
suggests that it is possible for at least some new 
users to achieve performance close to expert level 
within a few sessions. 
4.1.2 Task Level Performance 
One of the important performance indicators for 
end users is how long it takes to perform a given 
task. During the experiments, the instructors noted 
completion times required to reach a definite diag-
nosis in the experiment log. Table VI shows task 
completion times, categorized by session (col-
umns) and task within the session (rows).  
 Session 1 Session 2 Session 3 
Diagnosis 1 17:00 min 11:00 min 7:54 min 
Diagnosis 2 11:00 min 6:18 min 5:34 min 
Diagnosis 3 7:54 min 4:10 min 4:00 min 
Table V Average time required by subjects to 
complete diagnoses 
 
In the last two sessions, after subjects had ac-
climatized to the system, a diagnosis takes an aver-
age of about four minutes to complete. This 
compares to a three-minute average required to 
complete a diagnosis by our expert users. 
4.1.3 System coverage 
Table VI shows the percentage of in-coverage 
sentences uttered by the users on interactions that 
did not involve invocation of the help component. 
 
 IN-COVERAGE SENTENCES 
Session 1 54.9% 
Session 2 60.7% 
Session 3 64.6% 
Table VI Percentage of in-coverage sentences 
 
This indicates that subjects learn and adapt to 
the system coverage as they use the system more. 
The average proportion of in-coverage utterances 
is 10 percent higher during the third session than 
during the first session. 
4.2 Subjective Usability Measures 
4.2.1 Results of Questionnaire 
After finishing the third session, subjects were 
asked to fill in a short questionnaire, where re-
sponses were on a five-point scale ranging from 1 
(?strongly disagree?) to 5 (?strongly agree?). The 
results are presented in Table VIII. 
 
STATEMENT SCORE 
I quickly learned how to use the system. 4.4 
System response times were generally 
satisfactory. 
4.5 
When the system did not understand me, 
the help system usually showed me an-
other way to ask the question. 
4.6 
When I knew what I could say, the sys-
tem usually recognized me correctly. 
4.3 
I was often unable to ask the questions I 
wanted. 
3.8 
I could ask enough questions that I was 
sure of my diagnosis. 
4.3 
This system is more effective than non-
verbal communication using gestures. 
4.3 
I would use this system again in a simi-
lar situation. 
4.1 
Table VIII Subject responses to questionnaire. 
Scores are on a 5-point scale, averaged over all 
answers. 
 
Answers were in general positive, and most of 
the subjects were clearly very comfortable with the 
system after just an hour and a half of use. Interest-
ingly, even though most of the subjects answered 
?yes? to the question ?I was often unable to ask the 
questions I wanted?, the good performance of the 
help system appeared to compensate adequately for 
missing coverage. 
4.2.2 Translation Performance 
In order to evaluate the translation quality of the 
newly developed French-to-English system, we 
conducted a preliminary performance evaluation, 
similar to the evaluation method described in 
(Bouillon 2005). 
We performed translation judgment in two 
rounds. In the first round, an English-speaking 
judge was asked to categorize target utterances as 
comprehensible or not without looking at corre-
sponding source sentences. 91.1% of the sentences 
were judged as comprehensible. The remaining 
8.9% consisted of sentences where the terminology 
used was not familiar to the judge and of sentences 
where the translation component failed to produce 
a sufficiently good translation. An example sen-
tence is 
- Are the headaches better when you experi-
ence dark room? 
which stems from the French source sentence 
- Vos maux de t?te sont ils soulag?s par obs-
curit?? 
In the second round, English-speaking judges, 
sufficiently fluent in French to understand source 
language utterances, were shown the French source 
utterance, and asked to decide whether the target 
language utterance correctly reflected the meaning 
of the source language utterance. They were also 
asked to judge the style of the target language ut-
terance. Specifically, judges were asked to classify 
sentences as ?BAD? if the meaning of the English 
sentence did not reflect the meaning of the French 
sentence. Sentences were categorized as ?OK? if 
the meaning was transferred correctly and the sen-
tence was comprehensible, but the style of the re-
sulting English sentence was not perfect. Sentences 
were judged as ?GOOD? when they were compre-
hensible, and both meaning and style were consid-
ered to be completely correct. Table VIII 
summarizes results of two judges. 
 
 Good OK Bad  
Judge 1 15.8% 73.80% 10.3% 
Judge 2 46.6% 47.1% 6.3% 
Table VIII Judgments of the quality of the transla-
tions of 546 utterances 
 
It is apparent that translation judging is a highly 
subjective process. When translations were marked 
as ?bad?, the problem most often seemed to be re-
lated to lexical items where it was challenging to 
find an exact correspondence between French and 
English. Two common examples were ?troubles de 
la vision?, which was translated as ?blurred vi-
sion?, and ?faiblesse musculaire?, which was trans-
lated as ?weakness?. It is likely that a more careful 
choice of lexical translation rules would deal with 
at least some of these cases. 
5 Summary 
We have presented a first end-to-end evaluation 
of the MedSLT spoken language translation sys-
tem. The medical students who tested it were all 
able to use the system well, with performance in 
some cases comparable to that of that of system 
developers after only two sessions. At least for the 
fairly simple type of diagnoses covered by our sce-
nario, the system?s performance appeared clearly 
adequate for the task.  
This is particularly encouraging, since the 
French to English version of the system is quite 
new, and has not yet received the level of attention 
required for a clinical system. The robustness 
added by the help system was sufficient to com-
pensate for that, and in most cases, subjects were 
able to find ways to maneuver around coverage 
holes and other problems. It is entirely reasonable 
to hope that performance, which is already fairly 
good, would be substantially better with another 
couple of months of development work. 
In summary, we feel that this study shows that 
the conservative architecture we have chosen 
shows genuine potential for use in medical diagno-
sis situations. Before the end of 2006, we hope to 
have advanced to the stage where we can start ini-
tial trials with real doctors and patients. 
 
 
Acknowledgments 
We would like to thank Agnes Lisowska, Alia 
Rahal, and Nancy Underwood for being impartial 
judges over our system?s results. 
This work was funded by the Swiss National 
Science Foundation. 
References 
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. 
Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain 
medical speech translation. In Proceedings of the 
10th Conference of the European Association for 
Machine Translation (EAMT), Budapest, Hungary. 
P. Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. San-
taholma, M. Starlander, and N. Chatzichrisafis. 2006. 
Une grammaire multilingue partag?e pour la recon-
naissance et la g?n?ration. In Proceedings of TALN 
2006, Leuwen, Belgium. 
M. Cohen, J. Giangola, and J. Balogh. 2004, Voice User 
Interface Design. Addison Wesley Publishing. 
R. I. Kittredge. 2003. Sublanguages and comtrolled 
languages. In R. Mitkov, editor, The Oxford Hand-
book of Computational Linguistics, pages 430?447. 
Oxford University Press. 
MedBridge, 2006. http://www.medtablet.com/. As of 
15th March 2006. 
MedSLT, 2005. http://sourceforge.net/projects/medslt/. 
As of 15th March 2006. 
T. Mitamura. 1999. Controlled language for multilin-
gual machine translation. In Proceedings of Machine 
Translation Summit VII, Singapore. 
Phraselator, 2006. http://www.phraselator.com. As of 
15 February 2006. 
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An 
open source environment for compiling typed unifi-
cation grammars into speech recognisers. In Pro-
ceedings of the 10th EACL (demo track), Budapest, 
Hungary. 
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma,M. Starlander, H. Isahara, 
K. Kankazi, and Y. Nakao. 2005a. A methodology for 
comparing grammar-based and robust approaches to 
speech understanding. In Proceedings of the 9th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Lisboa, Portugal. 
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. 
Wir?n. 2000. The Spoken Language Translator, 
Cambridge University Press.  
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Put-
ting Linguistics into Speech Recognition: The 
Regulus Grammar Compiler. CSLI Press, Chicago.  
Regulus, 2006. http://sourceforge.net/projects/regulus/. 
As of 15 March 2006. 
S-MINDS, 2006. http://www.sehda.com/. As of 15 
March 2006. 
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. San-
taholma, M. Rayner, B.A. Hockey, H. Isahara, K. 
Kanzaki, and Y. Nakao. 2005. Practicing controlled 
language through a help system integrated into the 
medical speech translation system (MedSLT). In Pro-
ceedings of the MT Summit X, Phuket, Thailand 
 
MedSLT: A Limited-Domain Unidirectional Grammar-Based Medical
Speech Translator
Manny Rayner, Pierrette Bouillon, Nikos Chatzichrisafis, Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Marianne.Starlander@eti.unige.ch
Beth Ann Hockey
UCSC/NASA Ames Research Center, Moffet Field, CA 94035
bahockey@email.arc.nasa.gov
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Abstract
MedSLT is a unidirectional medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several differ-
ent language pairs and subdomains. Vo-
cabulary ranges from about 350 to 1000
surface words, depending on the language
and subdomain. We will demo both the
system itself and the development envi-
ronment, which uses a combination of
rule-based and data-driven methods to
construct efficient recognisers, generators
and transfer rule sets from small corpora.
1 Overview
The mainstream in speech translation work is for the
moment statistical, but rule-based systems are still a
very respectable alternative. In particular, nearly all
systems which have actually been deployed are rule-
based. Prominent examples are (Phraselator, 2006;
S-MINDS, 2006; MedBridge, 2006).
MedSLT (MedSLT, 2005; Bouillon et al, 2005)
is a unidirectional medical speech translation system
for use in doctor-patient diagnosis dialogues, which
covers several different language pairs and subdo-
mains. Recognition is performed using grammar-
based language models, and translation uses a rule-
based interlingual framework. The system, includ-
ing the development environment, is built on top of
Regulus (Regulus, 2006), an Open Source platform
for developing grammar-based speech applications,
which in turn sits on top of the Nuance Toolkit.
The demo will show how MedSLT can be used
to carry out non-trivial diagnostic dialogues. In par-
ticular, we will demonstrate how an integrated intel-
ligent help system counteracts the brittleness inher-
ent in rule-based processing, and rapidly leads new
users towards the supported system coverage. We
will also demo the development environment, and
show how grammars and sets of transfer rules can be
efficiently constructed from small corpora of a few
hundred to a thousand examples.
2 The MedSLT system
The MedSLT demonstrator has already been exten-
sively described elsewhere (Bouillon et al, 2005;
Rayner et al, 2005a), so this section will only
present a brief summary. The main components are
a set of speech recognisers for the source languages,
a set of generators for the target languages, a transla-
tion engine, sets of rules for translating to and from
interlingua, a simple discourse engine for dealing
with context-dependent translation, and a top-level
which manages the information flow between the
other modules and the user.
MedSLT also includes an intelligent help mod-
ule, which adds robustness to the system and guides
the user towards the supported coverage. The help
module uses a backup recogniser, equipped with a
statistical language model, and matches the results
from this second recogniser against a corpus of utter-
ances which are within system coverage and trans-
late correctly. In previous studies, we showed that
the grammar-based recogniser performs much bet-
ter than the statistical one on in-coverage utterances,
but worse on out-of-coverage ones. Having the help
system available approximately doubled the speed
at which subjects learned, measured as the average
difference in semantic error rate between the results
for their first quarter-session and their last quarter-
session (Rayner et al, 2005a). It is also possible to
recover from recognition errors by selecting a dis-
played help sentence; this typically increases the
number of acceptably processed utterances by about
10% (Starlander et al, 2005).
We will demo several versions of the system, us-
ing different source languages, target languages and
subdomains. Coverage is based on standard exami-
nation questions obtained from doctors, and consists
mainly of yes/no questions, though there is also sup-
port for WH-questions and elliptical utterances. Ta-
ble 1 gives examples of the coverage in the English-
input headache version, and Table 2 summarises
recognition performance in this domain for the three
main input languages. Differences in the sizes of the
recognition vocabularies are primarily due to differ-
ences in use of inflection. Japanese, with little in-
flectional morphology, has the smallest vocabulary;
French, which inflects most parts of speech, has the
largest.
3 The development environment
Although the MedSLT system is rule-based, we
would, for the usual reasons, prefer to acquire these
rules from corpora using some well-defined method.
There is, however, little or no material available for
most medical speech translation domains, including
ours. As noted in (Probst and Levin, 2002), scarcity
of data generally implies use of some strategy to ob-
tain a carefully structured training corpus. If the cor-
pus is not organised in this way, conflicts between
alternate learned rules occur, and it is hard to in-
Where?
?do you experience the pain in your jaw?
?does the pain spread to the shoulder?
When?
?have you had the pain for more than a month?
?do the headaches ever occur in the morning?
How long?
?does the pain typically last a few minutes?
?does the pain ever last more than two hours?
How often?
?do you get headaches several times a week?
?are the headaches occurring more often?
How?
?is it a stabbing pain?
?is the pain usually severe?
Associated symptoms?
?do you vomit when you get the headaches?
?is the pain accompanied by blurred vision?
Why?
?does bright light make the pain worse?
?do you get headaches when you eat cheese?
What helps?
?does sleep make the pain better?
?does massage help?
Background?
?do you have a history of sinus disease?
?have you had an e c g?
Table 1: Examples of English MedSLT coverage
duce a stable set of rules. As Probst and Levin sug-
gest, one obvious way to attack the problem is to
implement a (formal or informal) elicitation strat-
egy, which biases the informant towards translations
which are consistent with the existing ones. This is
the approach we have adopted in MedSLT.
The Regulus platform, on which MedSLT
is based, supports rapid construction of com-
plex grammar-based language models; it uses an
example-based method driven by small corpora
of disambiguated parsed examples (Rayner et al,
2003; Rayner et al, 2006), which extracts most of
the structure of the model from a general linguis-
tically motivated resource grammar. The result is
a specialised version of the general grammar, tai-
lored to the example corpus, which can then be com-
piled into an efficient recogniser or into a genera-
Language Vocab WER SemER
English 441 6% 18%
French 1025 8% 10%
Japanese 347 4% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognisers.
?Vocab? = number of surface words in source lan-
guage recogniser vocabulary; ?WER? = Word Error
Rate for source language recogniser, on in-coverage
material; ?SemER? = semantic error rate for source
language recogniser, on in-coverage material.
tion module. Regulus-based recognisers and gen-
erators are easy to maintain, and grammar struc-
ture is shared automatically across different subdo-
mains. Resource grammars are available for several
languages, including English, Japanese, French and
Spanish.
Nuance recognisers derived from the resource
grammars produce both a recognition string and a
semantic representation. This representation con-
sists of a list of key/value pairs, optionally including
one level of nesting; the format of interlingua and
target language representations is similar. The for-
malism is sufficiently expressive that a reasonable
range of temporal and causal constructions can be
represented (Rayner et al, 2005b). A typical exam-
ple is shown in Figure 1. A translation rule maps
a list of key/value pairs to a list of key/value pairs,
optionally specifying conditions requiring that other
key/value pairs either be present or absent in the
source representation.
When developing new coverage for a given lan-
guage pair, the developer has two main tasks. First,
they need to add new training examples to the
corpora used to derive the specialised grammars
used for the source and target languages; second,
they must add translation rules to handle the new
key/value pairs. The simple structure of the Med-
SLT representations makes it easy to support semi-
automatic acquisition of both of these types of in-
formation. The basic principle is to attempt to find
the minimal set of new rules that can be added to the
existing set, in order to cover the new corpus exam-
ple; this is done through a short elicitation dialogue
with the developer. We illustrate this with a simple
example.
Suppose we are developing coverage for the En-
glish ? Spanish version of the system, and that
the English corpus sentence ?does the pain occur at
night? fails to translate. The acquisition tool first
notes that processing fails when converting from in-
terlingua to Spanish. The interlingua representation
is
[[utterance_type,ynq],
[pronoun,you],
[state,have_symptom],
[symptom,pain],[tense,present],
[prep,in_time],[time,night]]
Applying Interlingua ? Spanish rules, the result is
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
failed:[time,night]]
where the tag failed indicates that the element
[time,night] could not be processed. The tool
matches the incomplete transferred representation
against a set of correctly translated examples, and
shows the developer the English and Spanish strings
for the three most similar ones, here
does it appear in the morning
-> tiene el dolor por la man?ana
does the pain appear in the morning
-> tiene el dolor por la man?ana
does the pain come in the morning
-> tiene el dolor por la man?ana
This suggests that a translation for ?does the pain
occur at night? consistent with the existing rules
would be ?tiene el dolor por la noche?. The devel-
oper gives this example to the system, which parses
it using both the general Spanish resource grammar
and the specialised grammar used for generation in
the headache domain. The specialised grammar fails
to produce an analysis, while the resource grammar
produces two analyses,
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[[utterance_type,ynq],[pronoun,you],[state,have_symptom],
[tense,present],[symptom,headache],[sc,when],
[[clause,[[utterance_type,dcl],[pronoun,you],
[action,drink],[tense,present],[cause,coffee]]]]
Figure 1: Representation of ?do you get headaches when you drink coffee?
[tense,present],
[prep,por_temporal],
[temporal,noche]]
and
[[utterance_type,dcl],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
[temporal,noche]]
The first of these corresponds to the YN-question
reading of the sentence (?do you have the pain at
night?), while the second is the declarative reading
(?you have the pain at night?). Since the first (YN-
question) reading matches the Interlingua represen-
tation better, the acquisition tool assumes that it is
the intended one. It can now suggest two pieces of
information to extend the system?s coverage.
First, it adds the YN-question reading of ?tiene
el dolor por la noche? to the corpus used to train
the specialised generation grammar. The piece
of information acquired from this example is that
[temporal,noche] should be realised in this
domain as ?la noche?. Second, it compares the cor-
rect Spanish representation with the incomplete one
produced by the current set of rules, and induces a
new Interlingua to Spanish translation rule. This will
be of the form
[time,night] -> [temporal,noche]
In the demo, we will show how the development
environment makes it possible to quickly add new
coverage to the system, while also checking that old
coverage is not broken.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
MedBridge, 2006. http://www.medtablet.com/index.html.
As of 15 March 2006.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 15 March 2005.
Phraselator, 2006. http://www.phraselator.com. As of 15
March 2006.
K. Probst and L. Levin. 2002. Challenges in automatic
elicitation of a controlled bilingual corpus. In Pro-
ceedings of the 9th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kankazi,
and Y. Nakao. 2005a. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. In Proceedings of the 9th International
Conference on Spoken Language Processing (ICSLP),
Lisboa, Portugal.
M. Rayner, P. Bouillon, M. Santaholma, and Y. Nakao.
2005b. Representational and architectural issues in a
limited-domain medical speech translator. In Proceed-
ings of TALN/RECITAL, Dourdan, France.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2006. http://sourceforge.net/projects/regulus/.
As of 15 March 2006.
S-MINDS, 2006. http://www.sehda.com. As of 15
March 2006.
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santa-
holma, M. Rayner, B.A. Hockey, H. Isahara, K. Kan-
zaki, and Y. Nakao. 2005. Practicing controlled lan-
guage through a help system integrated into the medi-
cal speech translation system (MedSLT). In Proceed-
ings of the MT Summit X, Phuket, Thailand.
Proceedings of SPEECHGRAM 2007, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Bidirectional Grammar-Based Medical Speech Translator
Pierrette Bouillon1, Glenn Flores2, Marianne Starlander1, Nikos Chatzichrisafis1
Marianne Santaholma1, Nikos Tsourakis1, Manny Rayner1,3, Beth Ann Hockey4
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Pierrette.Bouillon@issco.unige.ch
Marianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch
2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226
gflores@mcw.edu
3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107
manny@powerset.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
Abstract
We describe a bidirectional version of the
grammar-based MedSLT medical speech
system. The system supports simple medi-
cal examination dialogues about throat pain
between an English-speaking physician and
a Spanish-speaking patient. The physician?s
side of the dialogue is assumed to consist
mostly of WH-questions, and the patient?s of
elliptical answers. The paper focusses on the
grammar-based speech processing architec-
ture, the ellipsis resolution mechanism, and
the online help system.
1 Background
There is an urgent need for medical speech trans-
lation systems. The world?s current population
of 6.6 billion speaks more than 6,000 languages
(Graddol, 2004). Language barriers are associated
with a wide variety of deleterious consequences in
healthcare, including impaired health status, a lower
likelihood of having a regular physician, lower rates
of mammograms, pap smears, and other preven-
tive services, non-adherence with medications, a
greater likelihood of a diagnosis of more severe psy-
chopathology and leaving the hospital against med-
ical advice among psychiatric patients, a lower like-
lihood of being given a follow-up appointment af-
ter an emergency department visit, an increased risk
of intubation among children with asthma, a greater
risk of hospital admissions among adults, an in-
creased risk of drug complications, longer medical
visits, higher resource utilization for diagnostic test-
ing, lower patient satisfaction, impaired patient un-
derstanding of diagnoses, medications, and follow-
up, and medical errors and injuries (Flores, 2005;
Flores, 2006). Nevertheless, many patients who
need medical interpreters do not get them. For ex-
ample, in the United States, where 52 million peo-
ple speak a language other than English at home
and 23 million people have limited English profi-
ciency (LEP) (Census, 2007), one study found that
about half of LEP patients presenting to an emer-
gency department were not provided with a medical
interpreter (Baker et al, 1996). There is thus a sub-
stantial gap between the need for and availability of
language services in health care, a gap that could be
bridged through effective medical speech translation
systems.
An ideal system would be able to interpret ac-
curately and flexibly between patients and health
care professionals, using unrestricted language and
a large vocabulary. A system of this kind is, un-
fortunately, beyond the current state of the art.
It is, however, possible, using today?s technol-
ogy, to build speech translation systems for specific
scenarios and language-pairs, which can achieve
acceptable levels of reliability within the bounds
41
of a well-defined controlled language. MedSLT
(Bouillon et al, 2005) is an Open Source system
of this type, which has been under construction at
Geneva University since 2003. The system is built
on top of Regulus (Rayner et al, 2006), an Open
Source platform which supports development of
grammar-based speech-enabled applications. Regu-
lus has also been used to build several other systems,
including NASA?s Clarissa (Rayner et al, 2005b).
The most common architecture for speech trans-
lation today uses statistical methods to perform both
speech recognition and translation, so it is worth
clarifying why we have chosen to use grammar-
based methods. Even though statistical architec-
tures exhibit many desirable properties (purely data-
driven, domain independent), this is not necessar-
ily the best alternative in safety-critical medical ap-
plications. Anecdotally, many physicians express
reluctance to trust a translation device whose out-
put is not readily predictable, and most of the
speech translation systems which have reached the
stage of field testing rely on various types of
grammar-based recognition and rule-based transla-
tion (Phraselator, 2007; Fluential, 2007).
Statistical speech recognisers can achieve impres-
sive levels of accuracy when trained on enough data,
but it is a daunting task to collect training mate-
rial in the requisite quantities (usually, tens of thou-
sands of high-quality utterances) when trying to
build practical systems. Considering that the medi-
cal speech translation applications we are interested
in constructing here need to work for multiple lan-
guages and subdomains, the problem becomes even
more challenging. Our experience is that grammar-
based systems which also incorporate probabilistic
context-free grammar tuning deliver better results
than purely statistical ones when training data are
sparse (Rayner et al, 2005a).
Another common criticism of grammar-based
systems is that out-of-coverage utterances will
neither be recognized nor translated, an objec-
tion that critics have sometimes painted as de-
cisive. It is by no means obvious, however,
that restricted coverage is such a serious prob-
lem. In text processing, work on several gener-
ations of controlled language systems has devel-
oped a range of techniques for keeping users within
the bounds of system coverage (Kittredge, 2003;
Mitamura, 1999), and variants of these methods can
also be adapted for spoken language applications.
Our experiments with MedSLT show that even a
quite simple help system is enough to guide users
quickly towards the intended coverage of a medium-
vocabulary grammar-based speech translation appli-
cation, with most users appearing confident after just
an hour or two of exposure (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
Until recently, the MedSLT system only sup-
ported unidirectional processing in the physician
to patient direction. The assumption was that the
physician would mostly ask yes/no questions, to
which the patient would respond non-verbally, for
example by nodding or shaking their head. A uni-
directional architecture is easier to make habitable
than a bidirectional one. It is reasonable to as-
sume that the physician will use the system regu-
larly enough to learn the coverage, but most patients
will not have used the system before, and it is less
clear that they will be able to acclimatize within the
narrow window at their disposal. These consider-
ations must however be balanced against the fact
that a unidirectional system does not allow for a
patient-centered interaction characterized by mean-
ingful patient-clinician communication or shared de-
cision making. Multiple studies in the medical lit-
erature document that patient-centeredness, effec-
tive patient-clinician communication, and shared de-
cision making are associated with significant im-
provements in patient health outcomes, including
reduced anxiety levels, improved functional sta-
tus, reduced pain, better control of diabetes melli-
tus, blood pressure reduction among hypertensives,
improved adherence, increased patient satisfaction,
and symptom reduction for a variety of conditions
(Stewart, 1995; Michie et al, 2003). A bidirectional
system is considered close to essential from a health-
care perspective, since it appropriately addresses the
key issues of patient centeredness and shared de-
cision making. For these reasons, we have over
the last few months developed a bidirectional ver-
sion of MedSLT, initially focussing on a throat pain
scenario with an English-speaking physician and a
Spanish-speaking patient. The physician uses full
sentences, while the patient answers with short re-
sponses.
One of the strengths of the Regulus approach is
42
that it is very easy to construct parallel versions of
a grammar; generally, all that is required is to vary
the training corpus. (We will have more to say about
this soon). We have exploited these properties of
the platform to create two different configurations
of the bidirectional system, so that we can compare
competing approaches to the problem of accommo-
dating patients unfamiliar with speech technology.
In Version 1 (less restricted), the patient is allowed
to answer using both elliptical utterances and short
sentences, while in Version 2 (more restricted) they
are only permitted to use elliptical utterances. Thus,
for example, if the physician asks the question ?How
long have you had a sore throat??, Version 1 allows
the patient to respond both ?Desde algunos d??as?
(?For several days?) and ?Me ha dolido la garganta
desde algunos d??as? (?I have had a sore throat for
several days?), while Version 2 only allows the first
of these. Both the short and the long versions are
translated uniformly, with the short version resolved
using the context from the preceding question.
In both versions, if the patient finds it too chal-
lenging to use the system to answer WH-questions
directly, it is possible to back off to the earlier di-
alogue architecture in which the physician uses Y-
N questions and the patient responds with simple
yes/no answers, or nonverbally. Continuing the ex-
ample, if the patient is unable to find an appro-
priate way to answer the physician?s question, the
physician could ask ?Have you had a sore throat for
more than three days??; if the patient responds nega-
tively, they could continue with the follow-on ques-
tion ?More than a week??, and so on.
In the rest of the paper, we first describe the
system top-level (Section 2), the way in which
grammar-based processing is used (Section 3), the
ellipsis processing mechanism (Section 4), and the
help system (Section 5). Section 6 presents an ini-
tial evaluation, and the final section concludes.
2 Top-level architecture
The system is operated through the graphical user
interface (GUI) shown in Figures 1 and 2. In
accordance with the basic principles of patient-
centeredness and shared decision-making outlined
in Section 1, the patient and the physician each have
their own headset, use their own mouse, and share
the same view of the screen. This is in sharp contrast
to the majority of the medical speech translation sys-
tems described in the literature (Somers, 2006).
As shown in the screenshots, the main GUI win-
dow is separated into two tabbed panes, marked
?Doctor? and ?Patient?. Initially, the ?Doctor? view
(the one shown in Figure 1) is active. The physician
presses the ?Push to talk? button, and speaks into
the headset microphone. If recognition is success-
ful, the GUI displays four separate results, listed on
the right side of the screen. At the top, immediately
under the heading ?Question?, we can see the actual
words returned by speech recognition. Here, these
words are ?Have you had rapid strep test?. Below,
we have the help pane: this displays similar ques-
tions taken from the help corpus, which are known to
be within system coverage. The pane marked ?Sys-
tem understood? shows a back-translation, produced
by first translating the recognition result into inter-
lingua, and then translating it back into English. In
the present example, this corrects the minor mistake
the recogniser has made, missing the indefinite ar-
ticle ?a?, and confirms that the system has obtained
a correct grammatical analysis and interpretation at
the level of interlingua. At the bottom, we see the
target language translation. The left-hand side of the
screen logs the history of the conversation to date, so
that both sides can refer back to it.
If the physician decides that the system has cor-
rectly understood what they said, they can now press
the ?Play? button. This results in the system produc-
ing a spoken output, using the Vocalizer TTS engine.
Simultaneously with speaking, the GUI shifts to the
?Patient? configuration shown in Figure 2. This dif-
fers from the ?Doctor? configuration in two respects:
all text is in the patient language, and the help pane
presents its suggestions immediately, based on the
preceding physician question. The various process-
ing components used to support these functionalities
are described in the following sections.
3 Grammar-based processing
Grammar-based processing is used for source-
language speech recognition and target-side genera-
tion. (Source-language analysis is part of the recog-
nition process, since grammar-based recognition in-
cludes creating a parse). All of these functionalities
43
Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed
the ?Play? button. The help pane shows similar queries known to be within coverage.
Figure 2: Screenshot showing the state of the GUI after the physician has pressed the ?Play? button. The
help pane shows known valid responses to similar questions.
44
are implemented using the Regulus platform, with
the task-specific grammars compiled out of general
feature grammar resources by the Regulus tools. For
both recognition and generation, the first step is
to extract a domain-specific feature grammar from
the general one, using a version of the Explanation
Based Learning (EBL) algorithm.
The extraction process is driven by a corpus of ex-
amples and a set of ?operationality criteria?, which
define how the rules in the original resource gram-
mar are recombined into domain-specific ones. It is
important to realise that the domain-specific gram-
mar is not merely a subset of the resource grammar;
a typical domain-specific grammar rule is created by
merging two to five resource grammar rules into a
single ?flatter? rule. The result is a feature gram-
mar which is less general than the original one, but
more efficient. For recognition, the grammar is then
processed further into a CFG language model, using
an algorithm which alternates expansion of feature
values and filtering of the partially expanded gram-
mar to remove irrelevant rules. Detailed descrip-
tions of the EBL learning and feature grammar ?
CFG compilation algorithms can be found in Chap-
ters 8 and 10 of (Rayner et al, 2006). Regulus fea-
ture grammars can also be compiled into generators
using a version of the Semantic Head Driven algo-
rithm (Shieber et al, 1990).
The English (physician) side recogniser is com-
piled from the large English resource grammar de-
scribed in Chapter 9 of (Rayner et al, 2006), and
was constructed in the same way as the one de-
scribed in (Rayner et al, 2005a), which was used for
a headache examination task. The operationality cri-
teria are the same, and the only changes are a differ-
ent training corpus and the addition of new entries
to the lexicon. The same resources, with a differ-
ent training corpus, were used to build the English
language generator. It is worth pointing out that, al-
though a uniform method was used to build these
various grammars, the results were all very differ-
ent. For example, the recognition grammar from
(Rayner et al, 2005a) is specialised to cover only
second-person questions (?Do you get headaches
in the mornings??), while the generator grammar
used in the present application covers only first-
person declarative statements (?I visited the doctor
last Monday.?). In terms of structure, each gram-
mar contains several important constructions that the
other lacks. For example, subordinate clauses are
central in the headache domain (?Do the headaches
occur when you are stressed??) but are not present
in the sore throat domain; this is because the stan-
dard headache examination questions mostly focus
on generic conditions, while the sore throat exami-
nation questions only relate to concrete ones. Con-
versely, relative clauses are important in the sore
throat domain (?I have recently been in contact with
someone who has strep throat?), but are not suffi-
ciently important in the headache domain to be cov-
ered there.
On the Spanish (patient) side, there are four
grammars involved. For recognition, we have
two different grammars, corresponding to the two
versions of the system; the grammar for Ver-
sion 2 is essentially a subset of that for Version
1. For generation, there are two separate and
quite different grammars: one is used for trans-
lating the physician?s questions, while the other
produces back-translations of the patient?s ques-
tions. All of these grammars are extracted from
a general shared resource grammar for Romance
languages, which currently combines rules for
French, Spanish and Catalan (Bouillon et al, 2006;
Bouillon et al, to appear 2007b).
One interesting consequence of our methodology
is related to the fact that Spanish is a prodrop lan-
guage, which implies that many sentences are sys-
tematically ambiguous between declarative and Y-N
question readings. For example, ?He consultado un
me?dico? could in principle mean either ?I visited a
doctor? or ?Did I visit a doctor??. When training the
specialised Spanish grammars, it is thus necessary to
specify which readings of the training sentences are
to be used. Continuing the example, if the sentence
occurred in training material for the answer gram-
mar, we would specify that the declarative reading
was the intended one1.
4 Ellipsis processing and contextual
interpretation
In Version 1 of the system, the patient is per-
mitted to answer using elliptical phrases; in Ver-
1The specification can be formulated as a preference that
applies uniformly to all the training examples in a given group.
45
sion 2, she is obliged to do so. Ability to pro-
cess elliptical responses makes it easier to guide the
patient towards the intended coverage of the sys-
tem, without degrading the quality of recognition
(Bouillon et al, to appear 2007a). The downside is
that ellipses are also harder to translate than full sen-
tences. Even in a limited domain like ours, and in a
closely related language-pair, ellipsis can generally
not be translated word for word, and it is necessary
to look at the preceding context if the rules are to
be applied correctly. In examples 1 and 2 below,
the locative phrase ?In your stomach? in the English
source becomes the subject in the Spanish transla-
tion. This implies that the translation of the ellipsis
in the second physician utterance needs to change
syntactic category: ?In your head? (PP) becomes
?La cabeza? (NP).
(1) Doctor: Do you have a pain in your
stomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
(Trans): *En la cabeza?
Since examples like this are frequent, our sys-
tem implements a solution in which the patient?s
replies are translated in the context of the preced-
ing utterance. If the patient-side recogniser?s output
is classified as an ellipsis (this can done fairly reli-
ably thanks to use of suitably specialised grammars;
cf. Section 3), we expand the incomplete phrase
into a full sentence structure by adding appropriate
structural elements from the preceding physician-
side question; the expanded semantic structure is the
one which is then translated into interlingual form,
and thence back to the physician-side language.
Since all linguistic representations, including
those of elliptical phrases and their contexts, are rep-
resented as flat attribute-value lists, we are able to
implement the resolution algorithm very simply in
terms of list manipulation. In YN-questions, where
the elliptical answer intuitively adds information to
the question (?Did you visit the doctor??; ?El lunes?
? ?I visited the doctor on Monday?), the repre-
sentations are organised so that resolution mainly
amounts to concatenation of the two lists2. In WH-
questions, where the answer intuitively substitutes
the elliptical answer for the WH-phrase (?What is
2It is also necessary to replace second-person pronouns with
first-person counterparts.
your temperature??; ?Cuarenta grados?? ?My tem-
perature is forty degrees?), resolution substitutes the
representation of the elliptical phrase for that of a
semantically similar element in the question.
The least trivial aspect of this process is provid-
ing a suitable definition of ?semantically similar?.
This is done using a simple example-based method,
in which the grammar developer writes a set of dec-
larations, each of which lists a set of semantically
similar NPs. At compile-time, the grammar is used
to parse each NP, and extract a generalised skele-
ton, in which specific lexical information is stripped
away; at run-time, two NPs are held to be semanti-
cally similar if they can each be unified with skele-
tons in the same equivalence class. This ensures that
the definition of the semantic similarity relation is
stable across most changes to the grammar and lex-
icon. The issues are described in greater detail in
(Bouillon et al, to appear 2007a).
5 Help system
Since the performance of grammar-based speech un-
derstanding is only reliable on in-coverage mate-
rial, systems based on this type of architecture must
necessarily use a controlled language approach, in
which it is assumed that the user is able to learn the
relevant coverage. As previously noted, the Med-
SLT system addresses this problem by incorporat-
ing an online help system (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
On the physician side, the help system offers, af-
ter each recognition event, a list of related ques-
tions; similarly, on the patient side, it provides ex-
amples of known valid answers to the current ques-
tion. In both cases, the help examples are extracted
from a precompiled corpus of question-answer pairs,
which have been judged for correctness by system
developers. The process of selecting the examples
is slightly different on the two sides. For questions
(physician side), the system performs a second par-
allel recognition of the input speech, using a sta-
tistical recogniser. It then compares the recogni-
tion result, using an N-gram based metric, against
the set of known correct in-coverage questions from
the question-answer corpus, to extract the most sim-
ilar ones. For answers (patient side), the help sys-
tem searches the question-answer corpus to find the
46
questions most similar to the current one, and shows
the list of corresponding valid answers, using the
whole list in the case of Version 1 of the system, and
only the subset consisting of elliptical phrases in the
case of Version 2.
6 Evaluation
In previous studies, we have evaluated speech
recognition and speech understanding per-
formance for physician-side questions in
English (Bouillon et al, 2005) and Spanish
(Bouillon et al, to appear 2007b), and investi-
gated the impact on performance of the help system
(Rayner et al, 2005a; Starlander et al, 2005). We
have also carried out recent evaluations designed to
contrast recognition performance on elliptical and
full versions of the same utterance; here, our results
suggest that elliptical forms of (French-language)
MedSLT utterances are slightly easier to recognise
in terms of semantic error rate than full sentential
forms (Bouillon et al, to appear 2007a). Our initial
evaluation studies on the bidirectional system have
focussed on a specific question which has particular
relevance to this new version of MedSLT. Since
we are assuming that the patient will respond
using elliptical utterances, and that these utterances
will be translated in the context of the preceding
physician-side question, how confident can we
be that this context-dependent translation will be
correct?
In order to investigate these issues, we performed
a small data-collection using Version 2 of the sys-
tem, whose results we summarise here. One of the
authors of the paper played the role of an English-
speaking physician, in a simulated medical exam-
ination scenario where the goal was to determine
whether or not the ?patient? was suffering from a
viral throat infection. The six subjects playing the
role of the patient were all native speakers of Span-
ish, and had had no previous exposure to the system,
or indeed any kind of speech technology. They were
given cards describing the symptoms they were sup-
posed to be displaying, on which they were asked
to based their answers. From a total of 92 cor-
rectly recognised patient responses, we obtained 50
yes/no answers and 42 examples of real elliptical ut-
terances. Out of these, 36 were judged to have been
translated completely correctly, and a further 3 were
judged correct in terms of meaning, but less than flu-
ent. Only 3 examples were badly translated: of these
two were caused by problems in a translation rule,
and one by incorrect treatment of ellipsis resolution.
We show representative exchanges below; the last of
these is the one in which ellipsis processing failed to
work correctly.
(3) Doctor: For how long have you
had your sore throat?
Patient: Desde hace ma?s de
una semana
(Trans): I have had a sore
throat for more than one week
(4) Doctor: What were the results?
Patient: Negativo
(Trans): The results were negative
(5) Doctor: Have you seen a doctor
for your sore throat?
Patient: S?? el lunes
(Trans): I visited the doctor
for my sore throat monday
(6) Doctor: Have you been with anyone
recently who has a strep throat?
Patient: Si ma?s de dos semanas
(Trans): I was in contact with someone
more than two weeks recently
who had strep throat
7 Conclusions
We have presented a bidirectional grammar-based
English ? Spanish medical speech translation sys-
tem built using a linguistically motivated archi-
tecture, where all linguistic information is ulti-
mately derived from two resource grammars, one
for each language. We have shown how this en-
ables us to derive the multiple grammars needed,
which differ both with respect to function (recog-
nition/generation) and to domain (physician ques-
tions/patient answers). The system is currently un-
dergoing initial lab testing; we hope to advance to
initial trials on real patients some time towards the
end of the year.
References
[Baker et al1996] D.W. Baker, R.M. Parker, M.V.
Williams, W.C. Coates, and Kathryn Pitkin. 1996.
47
Use and effectiveness of interpreters in an emer-
gency department. Journal of the American Medical
Association, 275:783?8.
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Bouillon et al2006] P. Bouillon, M. Rayner, B. Novel-
las Vall, Y. Nakao, M. Santaholma, M. Starlander, and
N. Chatzichrisafis. 2006. Une grammaire multilingue
partage?e pour la traduction automatique de la parole.
In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et alto appear 2007a] P. Bouillon, M. Rayner,
M. Santaholma, and M. Starlander. to appear 2007a.
Les ellipses dans un syste`me de traduction automa-
tique de la parole. In Proceedings of TALN 2006,
Toulouse, France.
[Bouillon et alto appear 2007b] P. Bouillon, M. Rayner,
B. Novellas Vall, Y. Nakao, M. Santaholma, M. Star-
lander, and N. Chatzichrisafis. to appear 2007b. Une
grammaire partage?e multi-ta?che pour le traitement de
la parole : application aux langues romanes. Traite-
ment Automatique des Langues.
[Census2007] U.S. Census, 2007. Selected Social Char-
acteristics in the United States: 2005. Data Set: 2005
American Community Survey. Available here.
[Chatzichrisafis et al2006] N. Chatzichrisafis, P. Bouil-
lon, M. Rayner, M. Santaholma, M. Starlander, and
B.A. Hockey. 2006. Evaluating task performance for
a unidirectional controlled language medical speech
translation system. In Proceedings of the HLT-NAACL
International Workshop on Medical Speech Transla-
tion, pages 9?16, New York.
[Flores2005] G. Flores. 2005. The impact of medical in-
terpreter services on the quality of health care: A sys-
tematic review. Medical Care Research and Review,
62:255?299.
[Flores2006] G. Flores. 2006. Language barriers to
health care in the united states. New England Journal
of Medicine, 355:229?231.
[Fluential2007] Fluential, 2007.
http://www.fluentialinc.com. As of 24 March
2007.
[Graddol2004] D. Graddol. 2004. The future of lan-
guage. Science, 303:1329?1331.
[Kittredge2003] R. I. Kittredge. 2003. Sublanguages and
comtrolled languages. In R. Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
[Michie et al2003] S. Michie, J. Miles, and J. Weinman.
2003. Patient-centeredness in chronic illness: what is
it and does it matter? Patient Education and Counsel-
ing, 51:197?206.
[Mitamura1999] T. Mitamura. 1999. Controlled lan-
guage for multilingual machine translation. In Pro-
ceedings of Machine Translation Summit VII, Singa-
pore.
[Phraselator2007] Phraselator, 2007.
http://www.voxtec.com/. As of 24 March 2007.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. A methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the International
Space Station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
[Shieber et al1990] S. Shieber, G. van Noord, F.C.N.
Pereira, and R.C. Moore. 1990. Semantic-head-driven
generation. Computational Linguistics, 16(1).
[Somers2006] H. Somers. 2006. Language engineering
and the path to healthcare: a user-oriented view. In
Proceedings of the HLT-NAACL International Work-
shop on Medical Speech Translation, pages 32?39,
New York.
[Starlander et al2005] M. Starlander, P. Bouillon,
N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A.
Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.
Practising controlled language through a help system
integrated into the medical speech translation system
(MedSLT). In Proceedings of MT Summit X, Phuket,
Thailand.
[Stewart1995] M.A. Stewart. 1995. Effective physician-
patient communication and health outcomes: a review.
Canadian Medical Association Journal, 152:1423?
1433.
48
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 29?32, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Voice Enabled Procedure Browser
for the International Space Station
Manny Rayner, Beth Ann Hockey, Nikos Chatzichrisafis, Kim Farrell
ICSI/UCSC/RIACS/NASA Ames Research Center
Moffett Field, CA 94035?1000
mrayner@riacs.edu, bahockey@email.arc.nasa.gov
Nikos.Chatzichrisafis@web.de, kfarrell@email.arc.nasa.gov
Jean-Michel Renders
Xerox Research Center Europe
6 chemin de Maupertuis, Meylan, 38240, France
Jean-Michel.Renders@xrce.xerox.com
Abstract
Clarissa, an experimental voice enabled
procedure browser that has recently been
deployed on the International Space Sta-
tion (ISS), is to the best of our knowl-
edge the first spoken dialog system in
space. This paper gives background
on the system and the ISS procedures,
then discusses the research developed to
address three key problems: grammar-
based speech recognition using the Regu-
lus toolkit; SVM based methods for open
microphone speech recognition; and ro-
bust side-effect free dialogue management
for handling undos, corrections and con-
firmations.
1 Overview
Astronauts on the International Space Station (ISS)
spend a great deal of their time performing com-
plex procedures. Crew members usually have to
divide their attention between the task and a pa-
per or PDF display of the procedure. In addition,
since objects float away in microgravity if not fas-
tened down, it would be an advantage to be able
to keep both eyes and hands on the task. Clarissa,
an experimental speech enabled procedure navigator
(Clarissa, 2005), is designed to address these prob-
lems. The system was deployed on the ISS on Jan-
uary 14, 2005 and is scheduled for testing later this
year; the initial version is equipped with five XML-
encoded procedures, three for testing water quality
and two for space suit maintenance. To the best of
our knowledge, Clarissa is the first spoken dialogue
application in space.
The system includes commands for navigation:
forward, back, and to arbitrary steps. Other com-
mands include setting alarms and timers, record-
ing, playing and deleting voice notes, opening and
closing procedures, querying system status, and in-
putting numerical values. There is an optional mode
that aggressively requests confirmation on comple-
tion of each step. Open microphone speech recog-
nition is crucial for providing hands free use. To
support this, the system has to discriminate between
speech that is directed to it and speech that is not.
Since speech recognition is not perfect, and addi-
tional potential for error is added by the open micro-
phone task, it is also important to support commands
for undoing or correcting bad system responses.
The main components of the Clarissa system are
a speech recognition module, a classifier for exe-
cuting the open microphone accept/reject decision,
a semantic analyser, and a dialogue manager. The
rest of this paper will briefly give background on the
structure of the procedures and the XML representa-
tion, then describe the main research content of the
system.
2 Voice-navigable procedures
ISS procedures are formal documents that typically
represent many hundreds of person hours of prepa-
ration, and undergo a strict approval process. One
requirement in the Clarissa project was that the pro-
cedures should be displayed visually exactly as they
29
Figure 1: Adding voice annotations to a group of
steps
appear in the original PDF form. However, reading
these procedures verbatim would not be very useful.
The challenge is thus to let the spoken version di-
verge significantly from the written one, yet still be
similar enough in meaning that the people who con-
trol the procedures can be convinced that the two
versions are in practice equivalent.
Figure 1 illustrates several types of divergences
between the written and spoken versions, with
?speech bubbles? showing how procedure text is ac-
tually read out. In this procedure for space suit main-
tenance, one to three suits can be processed. The
group of steps shown cover filling of a ?dry LCVG?.
The system first inserts a question to ask which suits
require this operation, and then reads the passage
once for each suit, specifying each time which suit is
being referred to; if no suits need to be processed, it
jumps directly to the next section. Step 51 points the
user to a subprocedure. The spoken version asks if
the user wants to execute the steps of the subproce-
dure; if so, it opens the LCVG Water Fill procedure
and goes directly to step 6. If the user subsequently
goes past step 17 of the subprocedure, the system
warns that the user has gone past the required steps,
and suggests that they close the procedure.
Other important types of divergences concern en-
try of data in tables, where the system reads out an
appropriate question for each table cell, confirms the
value supplied by the user, and if necessary warns
about out-of-range values.
Rec Patterns Errors
Reject Bad Total
Text LF 3.1% 0.5% 3.6%
Text Surface 2.2% 0.8% 3.0%
Text Surface+LF 0.8% 0.8% 1.6%
SLM Surface 2.8% 7.4% 10.2%
GLM LF 1.4% 4.9% 6.3%
GLM Surface 2.9% 4.8% 7.7%
GLM Surface+LF 1.0% 5.0% 6.0%
Table 1: Speech understanding performance on six
different configurations of the system.
3 Grammar-based speech understanding
Clarissa uses a grammar-based recognition architec-
ture. At the start of the project, we had two main rea-
sons for choosing this approach over the more popu-
lar statistical one. First, we had no available training
data. Second, the system was to be designed for ex-
perts who would have time to learn its coverage, and
who moreover, as former military pilots, were com-
fortable with the idea of using controlled language.
Although there is not much to be found in the litera-
ture, an earlier study in which we had been involved
(Knight et al, 2001) suggested that grammar-based
systems outperformed statistical ones for this kind
of user. Given that neither of the above arguments is
very strong, we wanted to implement a framework
which would allow us to compare grammar-based
methods with statistical ones, and retain the option
of switching from a grammar-based framework to a
statistical one if that later appeared justified. The
Regulus and Alterf platforms, which we have devel-
oped under Clarissa and other earlier projects, are
designed to meet these requirements.
The basic idea behind Regulus (Regulus, 2005;
Rayner et al, 2003) is to extract grammar-based lan-
guage models from a single large unification gram-
mar, using example-based methods driven by small
corpora. Since grammar construction is now a
corpus-driven process, the same corpora can be used
to build statistical language models, facilitating a di-
rect comparison between the two methodologies.
On its own, however, Regulus only permits com-
parison at the level of recognition strings. Alterf
(Rayner and Hockey, 2003) extends the paradigm to
30
ID Rec Features Classifier Error rates
Classification Task
In domain Out Av
Good Bad
1 SLM Confidence Threshold 5.5% 59.1% 16.5% 11.8% 10.1%
2 GLM Confidence Threshold 7.1% 48.7% 8.9% 9.4% 7.0%
3 SLM Confidence + Lexical Linear SVM 2.8% 37.1% 9.0% 6.6% 7.4%
4 GLM Confidence + Lexical Linear SVM 2.8% 48.5% 8.7% 6.3% 6.2%
5 SLM Confidence + Lexical Quadratic SVM 2.6% 23.6% 8.5% 5.5% 6.9%
6 GLM Confidence + Lexical Quadratic SVM 4.3% 28.1% 4.7% 5.5% 5.4%
Table 2: Performance on accept/reject classification and the top-level task, on six different configurations.
the semantic level, by providing a trainable seman-
tic interpretation framework. Interpretation uses a
set of user-specified patterns, which can match ei-
ther the surface strings produced by both the statisti-
cal and grammar-based architectures, or the logical
forms produced by the grammar-based architecture.
Table 1 presents the result of an evaluation, car-
ried out on a set of 8158 recorded speech utterances,
where we compared the performance of a statisti-
cal/robust architecture (SLM) and a grammar-based
architecture (GLM). Both versions were trained off
the same corpus of 3297 utterances. We also show
results for text input simulating perfect recognition.
For the SLM version, semantic representations are
constructed using only surface Alterf patterns; for
the GLM and text versions, we can use either sur-
face patterns, logical form (LF) patterns, or both.
The ?Error? columns show the proportion of ut-
terances which produce no semantic interpretation
(?Reject?), the proportion with an incorrect seman-
tic interpretation (?Bad?), and the total.
Although the WER for the GLM recogniser is
only slightly better than that for the SLM recogniser
(6.27% versus 7.42%, 15% relative), the difference
at the level of semantic interpretation is considerable
(6.3% versus 10.2%, 39% relative). This is most
likely accounted for by the fact that the GLM ver-
sion is able to use logical-form based patterns, which
are not accessible to the SLM version. Logical-form
based patterns do not appear to be intrinsically more
accurate than surface (contrast the first two ?Text?
rows), but the fact that they allow tighter integration
between semantic understanding and language mod-
elling is intuitively advantageous.
4 Open microphone speech processing
The previous section described speech understand-
ing performance in terms of correct semantic inter-
pretation of in-domain input. However, open micro-
phone speech processing implies that some of the in-
put will not be in-domain. The intended behaviour
for the system is to reject this input. We would
also like it, when possible, to reject in-domain input
which has not been correctly recognised.
Surface output from the Nuance speech recog-
niser is a list of words, each tagged with a confidence
score; the usual way to make the accept/reject deci-
sion is by using a simple threshold on the average
confidence score. Intuitively, however, we should be
able to improve the decision quality by also taking
account of the information in the recognised words.
By thinking of the confidence scores as weights,
we can model the problem as one of classifying doc-
uments using a weighted bag of words model. It
is well known (Joachims, 1998) that Support Vec-
tor Machine methods are very suitable for this task.
We have implemented a version of the method de-
scribed by Joachims, which significantly improves
on the naive confidence score threshold method.
Performance on the accept/reject task can be eval-
uated directly in terms of the classification error. We
can also define a metric for the overall speech under-
standing task which includes the accept/reject deci-
sion, as a weighted loss function over the different
types of error. We assign weights of 1 to a false re-
ject of a correct interpretation, 2 to a false accept of
an incorrectly interpreted in-domain utterance, and 3
to a false accept of an out-of-domain utterance. This
31
captures the intuition that correcting false accepts is
considerably harder than correcting false rejects, and
that false accepts of utterances not directed at the
system are worse than false accepts of incorrectly
interpreted utterances.
Table 2 summarises the results of experiments
comparing performance of different recognisers and
accept/reject classifiers on a set of 10409 recorded
utterances. ?GLM? and ?SLM? refer respectively to
the best GLM and SLM recogniser configurations
from Table 1. ?Av? refers to the average classi-
fier error, and ?Task? to a normalised version of the
weighted task metric. The best SVM-based method
(line 6) outperforms the best naive threshold method
(line 2) by 5.4% to 7.0% on the task metric, a relative
improvement of 23%. The best GLM-based method
(line 6) and the best SLM-based method (line 5) are
equally good in terms of accept/reject classification
accuracy, but the GLM?s better speech understand-
ing performance means that it scores 22% better on
the task metric. The best quadratic kernel (line 6)
outscores the best linear kernel (line 4) by 13%. All
these differences are significant at the 5% level ac-
cording to the Wilcoxon matched-pairs test.
5 Side-effect free dialogue management
In an open microphone spoken dialogue application
like Clarissa, it is particularly important to be able
to undo or correct a bad system response. This
suggests the idea of representing discourse states
as objects: if the complete dialogue state is an ob-
ject, a move can be undone straightforwardly by
restoring the old object. We have realised this idea
within a version of the standard ?update seman-
tics? approach to dialogue management (Larsson
and Traum, 2000); the whole dialogue management
functionality is represented as a declarative ?update
function? relating the old dialogue state, the input
dialogue move, the new dialogue state and the out-
put dialogue actions.
In contrast to earlier work, however, we include
task information as well as discourse information in
the dialogue state. Each state also contains a back-
pointer to the previous state. As explained in detail
in (Rayner and Hockey, 2004), our approach per-
mits a very clean and robust treatment of undos, cor-
rections and confirmations, and also makes it much
simpler to carry out systematic regression testing of
the dialogue manager component.
Acknowledgements
Work at ICSI, UCSC and RIACS was supported
by NASA Ames Research Center internal fund-
ing. Work at XRCE was partly supported by the
IST Programme of the European Community, un-
der the PASCAL Network of Excellence, IST-2002-
506778. Several people not credited here as co-
authors also contributed to the implementation of
the Clarissa system: among these, we would par-
ticularly like to mention John Dowding, Susana
Early, Claire Castillo, Amy Fischer and Vladimir
Tkachenko. This publication only reflects the au-
thors? views.
References
Clarissa, 2005. http://www.ic.arc.nasa.gov/projects/clarissa/.
As of 26 April 2005.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the 10th European Conference on
Machine Learning, Chemnitz, Germany.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Dia-
logue Systems Engineering, pages 323?340.
M. Rayner and B.A. Hockey. 2003. Transparent com-
bination of rule-based and data-driven approaches in a
speech understanding architecture. In Proceedings of
the 10th EACL (demo track), Budapest, Hungary.
M. Rayner and B.A. Hockey. 2004. Side effect free
dialogue management in a voice enabled procedure
browser. In Proceedings of INTERSPEECH 2004, Jeju
Island, Korea.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL, Budapest, Hungary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 26 April 2005.
32
