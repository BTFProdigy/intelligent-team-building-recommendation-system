In Question Answering, Two Heads Are Better Than One
Jennifer Chu-Carroll Krzysztof Czuba John Prager Abraham Ittycheriah
IBM T.J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598, U.S.A.
jencc,kczuba,jprager,abei@us.ibm.com
Abstract
Motivated by the success of ensemble methods
in machine learning and other areas of natu-
ral language processing, we developed a multi-
strategy and multi-source approach to question
answering which is based on combining the re-
sults from different answering agents searching
for answers in multiple corpora. The answer-
ing agents adopt fundamentally different strate-
gies, one utilizing primarily knowledge-based
mechanisms and the other adopting statistical
techniques. We present our multi-level answer
resolution algorithm that combines results from
the answering agents at the question, passage,
and/or answer levels. Experiments evaluating
the effectiveness of our answer resolution algo-
rithm show a 35.0% relative improvement over
our baseline system in the number of questions
correctly answered, and a 32.8% improvement
according to the average precision metric.
1 Introduction
Traditional question answering (QA) systems typically
employ a pipeline approach, consisting roughly of ques-
tion analysis, document/passage retrieval, and answer se-
lection (see e.g., (Prager et al, 2000; Moldovan et al,
2000; Hovy et al, 2001; Clarke et al, 2001)). Although a
typical QA system classifies questions based on expected
answer types, it adopts the same strategy for locating po-
tential answers from the same corpus regardless of the
question classification. In our own earlier work, we de-
veloped a specialized mechanism called Virtual Annota-
tion for handling definition questions (e.g., ?Who was
Galileo?? and ?What are antibiotics??) that consults,
in addition to the standard reference corpus, a structured
knowledge source (WordNet) for answering such ques-
tions (Prager et al, 2001). We have shown that better
performance is achieved by applying Virtual Annotation
and our general purpose QA strategy in parallel. In this
paper, we investigate the impact of adopting such a multi-
strategy and multi-source approach to QA in a more gen-
eral fashion.
Our approach to question answering is additionally
motivated by the success of ensemble methods in ma-
chine learning, where multiple classifiers are employed
and their results are combined to produce the final output
of the ensemble (for an overview, see (Dietterich, 1997)).
Such ensemble methods have recently been adopted in
question answering (Chu-Carroll et al, 2003b; Burger
et al, 2003). In our question answering system, PI-
QUANT, we utilize in parallel multiple answering agents
that adopt different processing strategies and consult dif-
ferent knowledge sources in identifying answers to given
questions, and we employ resolution mechanisms to com-
bine the results produced by the individual answering
agents.
We call our approach multi-strategy since we com-
bine the results from a number of independent agents im-
plementing different answer finding strategies. We also
call it multi-source since the different agents can search
for answers in multiple knowledge sources. In this pa-
per, we focus on two answering agents that adopt fun-
damentally different strategies: one agent uses predomi-
nantly knowledge-based mechanisms, whereas the other
agent is based on statistical methods. Our multi-level
resolution algorithm enables combination of results from
each answering agent at the question, passage, and/or an-
swer levels. Our experiments show that in most cases
our multi-level resolution algorithm outperforms its com-
ponents, supporting a tightly-coupled design for multi-
agent QA systems. Experimental results show signifi-
cant performance improvement over our single-strategy,
single-source baselines, with the best performing multi-
level resolution algorithm achieving a 35.0% relative im-
provement in the number of correct answers and a 32.8%
improvement in average precision, on a previously un-
seen test set.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 24-31
                                                         Proceedings of HLT-NAACL 2003
Answering Agents
KSP
SemanticSearch
KeywordSearch
Question
WordNet
Answer
Cyc
QFrame
QuestionAnalysis QGoals
Knowledge-BasedAnswering Agent
StatisticalAnswering Agent
Aquaintcorpus
TRECcorpus
EB
AnswerResolution
Definition QAnswering Agent
KSP-BasedAnswering Agent
Knowledge Sources
Figure 1: PIQUANT?s Architecture
2 A Multi-Agent QA Architecture
In order to enable a multi-source and multi-strategy ap-
proach to question answering, we developed a modu-
lar and extensible QA architecture as shown in Figure 1
(Chu-Carroll et al, 2003a; Chu-Carroll et al, 2003b).
With a consistent interface defined for each component,
this architecture allows for easy plug-and-play of individ-
ual components for experimental purposes.
In our architecture, a question is first processed by the
question analysis component. The analysis results are
represented as a QFrame, which minimally includes a set
of question features that help activate one or more an-
swering agents. Each answering agent takes the QFrame
and generates its own set of requests to a variety of
knowledge sources. This may include performing search
against a text corpus and extracting answers from the re-
sulting passages, or performing a query against a struc-
tured knowledge source, such as WordNet (Miller, 1995)
or Cyc (Lenat, 1995). The (intermediate) results from
the individual answering agents are then passed on to the
answer resolution component, which combines and re-
solves the set of results, and either produces the system?s
final answers or feeds the intermediate results back to the
answering agents for further processing.
We have developed multiple answering agents, some
general purpose and others tailored for specific ques-
tion types. Figure 1 shows the answering agents cur-
rently available in PIQUANT. The knowledge-based and
statistical answering agents are general-purpose agents
that adopt different processing strategies and consult a
number of different text resources. The definition-Q
agent targets definition questions (e.g., ?What is peni-
cillin?? and ?Who is Picasso??) with a technique called
Virtual Annotation using the external knowledge source
WordNet (Prager et al, 2001). The KSP-based answer-
ing agent focuses on a subset of factoid questions with
specific logical forms, such as capital(?COUNTRY) and
state tree(?STATE). The answering agent sends requests
to the KSP (Knowledge Sources Portal), which returns, if
possible, an answer from a structured knowledge source
(Chu-Carroll et al, 2003a).
In the rest of this paper, we briefly describe our two
general-purpose answering agents. We then focus on a
multi-level answer resolution algorithm, applicable at dif-
ferent points in the QA process of these two answering
agents. Finally, we discuss experiments conducted to dis-
cover effective methods for combining results from mul-
tiple answering agents.
3 Component Answering Agents
We focus on two end-to-end answering agents designed
to answer short, fact-seeking questions from a collection
of text documents, as motivated by the requirements of
the TREC QA track (Voorhees, 2003). Both answer-
ing agents adopt the classic pipeline architecture, con-
sisting roughly of question analysis, passage retrieval,
and answer selection components. Although the answer-
ing agents adopt fundamentally different strategies in
their individual components, they have performed quite
comparably in past TREC QA tracks (Voorhees, 2001;
Voorhees, 2002).
3.1 Knowledge-Based Answering Agent
Our first answering agent utilizes a primarily knowledge-
driven approach, based on Predictive Annotation (Prager
et al, 2000). A key characteristic of this approach is that
potential answers, such as person names, locations, and
dates, in the corpus are predictively annotated. In other
words, the corpus is indexed not only with keywords, as
is typical for most search engines, but also with the se-
mantic classes of these pre-identified potential answers.
During the question analysis phase, a rule-based mech-
anism is employed to select one or more expected an-
swer types, from a set of about 80 classes used in the
predictive annotation process, along with a set of ques-
tion keywords. A weighted search engine query is then
constructed from the keywords, their morphological vari-
ations, synonyms, and the answer type(s). The search en-
gine returns a hit list of typically 10 passages, each con-
sisting of 1-3 sentences. The candidate answers in these
passages are identified and ranked based on three criteria:
1) match in semantic type between candidate answer and
expected answer, 2) match in weighted grammatical rela-
tionships between question and answer passages, and 3)
frequency of answer in candidate passages (redundancy).
The answering agent returns the top n ranked candidate
answers along with a confidence score for each answer.
3.2 Statistical Answering Agent
The second answering agent takes a statistical approach
to question answering (Ittycheriah, 2001; Ittycheriah et
al., 2001). It models the distribution p(c|q, a), which
measures the ?correctness? (c) of an answer (a) to a ques-
tion (q), by introducing a hidden variable representing the
answer type (e) as follows:
p(c|q, a) =
?
e p(c, e|q, a)
=
?
e p(c|e, q, a)p(e|q, a)
p(e|q, a) is the answer type model which predicts, from
the question and a proposed answer, the answer type they
both satisfy. p(c|e, q, a) is the answer selection model.
Given a question, an answer, and the predicted answer
type, it seeks to model the correctness of this configura-
tion. These distributions are modeled using a maximum
entropy formulation (Berger et al, 1996), using training
data which consists of human judgments of question an-
swer pairs. For the answer type model, 13K questions
were annotated with 31 categories. For the answer selec-
tion model, 892 questions from the TREC 8 and TREC 9
QA tracks were used, along with 4K trivia questions.
During runtime, the question is first analyzed by the
answer type model, which selects one out of a set of 31
types for use by the answer selection model. Simultane-
ously, the question is expanded using local context anal-
ysis (Xu and Croft, 1996) with an encyclopedia, and the
top 1000 documents are retrieved by the search engine.
From these documents, the top 100 passages are chosen
that 1) maximize the question word match, 2) have the
desired answer type, 3) minimize the dispersion of ques-
tion words, and 4) have similar syntactic structures as the
question. From these passages, candidate answers are ex-
tracted and ranked using the answer selection model. The
top n candidate answers are then returned, each with an
associated confidence score.
4 Answer Resolution
Given two answering agents with the same pipeline archi-
tecture, there are multiple points in the process at which
(intermediate) results can be combined, as illustrated in
Figure 2. More specifically, it is possible for one answer-
ing agent to provide input to the other after the question
analysis, passage retrieval, and answer selection phases.
In PIQUANT, the knowledge based agent may accept in-
put from the statistical agent after each of these three
phases.1 The contributions from the statistical agent are
taken into consideration by the knowledge based answer-
ing agent in a phase-dependent fashion. The rest of this
section details our combination strategies for each phase.
4.1 Question-Level Combination
One of the key tasks of the question analysis component
is to determine the expected answer type, such as PERSON
for ?Who discovered America?? and DATE for ?When
did World War II end?? This information is taken into ac-
count by most existing QA systems when ranking candi-
date answers, and can also be used in the passage retrieval
process to increase the precision of candidate passages.
We seek to improve the knowledge-based agent?s
performance in passage retrieval and answer selection
through better answer type identification by consulting
the statistical agent?s expected answer type. This task,
however, is complicated by the fact that QA systems em-
ploy different sets of answer types, often with different
granularities and/or with overlapping types. For instance,
while one system may generate ROYALTY for the ques-
tion ?Who was the King of France in 1702??, another
system may produce PERSON as the most specific an-
swer type in its repertoire. This is quite a serious problem
for us as the knowledge based agent uses over 80 answer
types while the statistical agent adopts only 31 categories.
In order to distinguish actual answer type discrepan-
cies from those due to granularity differences, we first
manually created a mapping between the two sets of an-
swer types. This mapping specifies, for each answer type
used by the statistical agent, a set of possible correspond-
ing types used by the knowledge-based agent. For exam-
ple, the GEOLOGICALOBJ class is mapped to a set of finer
grained classes: RIVER, MOUNTAIN, LAKE, and OCEAN.
At processing time, the statistical agent?s answer type
is mapped to the knowledge-based agent?s classes (SA-
1Although it is possible for the statistical agent to receive
input from the knowledge based agent as well, we have not pur-
sued that option because of implementation issues.
Question
Analysis 1
Passage
Retrieval 1
Answer
Selection 1
passages answers
Question
Analysis 2
Passage
Retrieval 2
Answer
Selection 2
Agent 1 (Knowledge-Based)
Agent 2 (Statistical)
question
typeQFrame
Figure 2: Answer Resolution Strategies
types), which are then merged with the answer type(s) se-
lected by the knowledge-based agent itself (KBA-types)
as follows:
1. If the intersection of KBA-types and SA-types is
non-null, i.e., the two agents produced consistent an-
swer types, then the merged set is KBA-types.
2. Otherwise, the two sets of answer types are truly
in disagreement, and the merged set is the union of
KBA-types and SA-types.
The merged answer types are then used by the
knowledge-based agent in further processing.
4.2 Passage-Level Combination
The passage retrieval component selects, from a large text
corpus, a small number of short passages from which an-
swers are identified. Oftentimes, multiple passages that
answer a question are retrieved. Some of these passages
may be better suited than others for the answer selection
algorithm employed downstream. For example, consider
?When was Benjamin Disraeli prime minister??, whose
answer can be found in both passages below:
1. Benjamin Disraeli, who had become prime minister
in 1868, was born into Judaism but was baptized a
Christian at the age of 12.
2. France had a Jewish prime minister in 1936, Eng-
land in 1868, and Spain, of all countries, in 1835,
but none of them, Leon Blum, Benjamin Disraeli or
Juan Alvarez Mendizabel, were devoutly observant,
as Lieberman is.
Although the correct answer, 1868, is present in both
passages, it is substantially easier to identify the answer
from the first passage, where it is directly stated, than
from the second passage, where recognition of parallel
constructs is needed to identify the correct answer.
Because of strategic differences in question analysis
and passage retrieval, our two answering agents often re-
trieve different passages for the same question. Thus, we
perform passage-level combination to make a wider va-
riety of passages available to the answer selection com-
ponent, as shown in Figure 2. The potential advantages
are threefold. First, passages from agent 2 may contain
answers absent in passages retrieved by agent 1. Sec-
ond, agent 2 may have retrieved passages better suited for
the downstream answer selection algorithm than those re-
trieved by agent 1. Third, passages from agent 2 may con-
tain additional occurrences of the correct answer, which
boosts the system?s confidence in the answer through the
redundancy measure.2
Our passage-level combination algorithm adds to the
passages extracted by the knowledge-based agent the top-
ranked passages from the statistical agent that contain
candidate answers of the right type. More specifically,
the statistical agent?s passages are semantically annotated
and the top 10 passages containing at least one candidate
of the expected answer type(s) are selected.3
4.3 Answer-Level Combination
The answer selection component identifies, from a set
of passages, the top n answers for the given question,
with their associated confidence scores. An answer-level
combination algorithm takes the top answer(s) from the
individual answering agents and determines the overall
best answer(s). Of our three combination algorithms, this
most closely resembles traditional ensemble methods, as
voting takes place among the end results of individual an-
2On the other hand, such redundancy may result in error
compounding, as discussed in Section 5.3.
3We selected the top 10 passages so that the same number
of passages are considered from both answering agents.
swering agents to determine the final output of the ensem-
ble.
We developed two answer-level combination algo-
rithms, both utilizing a simple confidence-based voting
mechanism, based on the premise that answers selected
by both agents with high confidence are more likely to
be correct than those identified by only one agent.4 In
both algorithms, named entity normalization is first per-
formed on all candidate answers considered. In the first
algorithm, only the top answer from each agent is taken
into account. If the two top answers are equivalent, the
answer is selected with the combined confidence from
both agents; otherwise, the more confident answer is se-
lected.5 In the second algorithm, the top 5 answers from
each agent are allowed to participate in the voting pro-
cess. Each instance of an answer votes with a weight
equal to its confidence value and the weights of equiv-
alent answers are again summed. The answer with the
highest weight, or confidence value, is selected as the
system?s final answer. Since in our evaluation, the second
algorithm uniformly outperforms the first, it is adopted as
our answer-level combination algorithm in the rest of the
paper.
5 Performance Evaluation
5.1 Experimental Setup
To assess the effectiveness of our multi-level answer res-
olution algorithm, we devised experiments to evaluate the
impact of the question, passage, and answer-level combi-
nation algorithms described in the previous section.
The baseline systems are the knowledge-based and sta-
tistical agents performing individually against a single
reference corpus. In addition, our earlier experiments
showed that when employing a single answer finding
strategy, consulting multiple text corpora yielded better
performance than using a single corpus. We thus con-
figured a version of our knowledge-based agent to make
use of three available text corpora,6 the AQUAINT cor-
pus (news articles from 1998-2000), the TREC corpus
(news articles from 1988-1994),7 and a subset of the En-
cyclopedia Britannica. This multi-source version of the
knowledge-based agent will be used in all answer resolu-
tion experiments in conjunction with the statistical agent.
We configured multiple versions of PIQUANT to eval-
uate our question, passage, and answer-level combination
4In future work we will be investigating weighted voting
schemes based on question features.
5The confidence values from both answering agents are nor-
malized to be between 0 and 1.
6The statistical agent is currently unable to consult multiple
corpora.
7Both the AQUAINT and TREC corpora are available from
the Linguistics Data Consortium, http://www.ldc.org.
algorithms individually and cumulatively. For cumula-
tive effects, we 1) combined the algorithms pair-wise,
and 2) employed all three algorithms together. The two
test sets were selected from the TREC 10 and 11 QA
track questions (Voorhees, 2002; Voorhees, 2003). For
both test sets, we eliminated those questions that did not
have known answers in the reference corpus. Further-
more, from the TREC 10 test set, we discarded all defini-
tion questions,8 since the knowledge-based agent adopts
a specialized strategy for handling definition questions
which greatly reduces potential contributions from other
answering agents. This results in a TREC 10 test set of
313 questions and a TREC 11 test set of 453 questions.
5.2 Experimental Results
We ran each of the baseline and combined systems on the
two test sets. For each run, the system outputs its top
answer and its confidence score for each question. All
answers for a run are then sorted in descending order of
the confidence scores. Two established TREC QA eval-
uation metrics are adopted to assess the results for each
run as follows:
1. % Correct: Percentage of correct answers.
2. Average Precision: A confidence-weighted score
that rewards systems with high confidence in cor-
rect answers as follows, where N is the number of
questions:
1
N
N?
i=1
# correct up to question i/i
Table 1 shows our experimental results. The top sec-
tion shows the comparable baseline results from the sta-
tistical agent (SA-SS) and the single-source knowledge-
based agent (KBA-SS). It also includes results for the
multi-source knowledge-based agent (KBA-MS), which
improve upon those for its single-source counterpart
(KBA-SS).
The middle section of the table shows the answer
resolution results, including applying the question, pas-
sage, and answer-level combination algorithms individu-
ally (Q, P, and A, respectively), applying them pair-wise
(Q+P, P+A, and Q+A), and employing all three algo-
rithms (Q+P+A). Finally, the last row of the table shows
the relative improvement by comparing the best perform-
ing system configuration (highlighted in boldface) with
the better performing single-source, single-strategy base-
line system (SA-SS or KBA-SS, in italics).
Overall, PIQUANT?s multi-strategy and multi-source
approach achieved a 35.0% relative improvement in the
8Definition questions were intentionally excluded by the
track coordinator in the TREC 11 test set.
TREC 10 (313) TREC 11 (453)
% Corr Avg Prec % Corr Avg Prec
SA-SS 36.7% 0.569 32.9% 0.534
KBA-SS 39.6% 0.595 32.5% 0.531
KBA-MS 43.8% 0.641 38.2% 0.622
Q 44.7% 0.647 38.9% 0.632
P 49.5% 0.661 40.0% 0.627
A 49.5% 0.712 43.5% 0.704
Q+P 48.9% 0.656 41.1% 0.640
P+A 51.1% 0.711 44.2% 0.686
Q+A 49.8% 0.716 43.9% 0.709
Q+P+A 50.8% 0.706 44.4% 0.690
rel. improv. 29.0% 20.3% 35.0% 32.8%
Table 1: Experimental Results
number of correct answers and a 32.8% improvement in
average precision on the TREC 11 data set. Of the com-
bined improvement, approximately half was achieved by
the multi-source aspect of PIQUANT, while the other half
was obtained by PIQUANT?s multi-strategy feature. Al-
though the absolute average precision values are com-
parable on both test sets and the absolute percentage of
correct answers is lower on the TREC 11 data, the im-
provement is greater on TREC 11 in both cases. This
is because the TREC 10 questions were taken into ac-
count for manual rule refinement in the knowledge-based
agent, resulting in higher baselines on the TREC 10 test
set. We believe that the larger improvement on the previ-
ously unseen TREC 11 data is a more reliable estimate of
PIQUANT?s performance on future test sets.
We applied an earlier version of our combination algo-
rithms, which performed between our current P and P+A
algorithms, in our submission to the TREC 11 QA track.
Using the average precision metric, that version of PI-
QUANT was among the top 5 best performing systems
out of 67 runs submitted by 34 groups.
5.3 Discussion and Analysis
A cursory examination of the results in Table 1 allows
us to draw two general conclusions about PIQUANT?s
performance. First, all three combination algorithms ap-
plied individually improved upon the baseline using both
evaluation metrics on both test sets. In addition, overall
performance is generally better the later in the process
the combination occurs, i.e., the answer-level combina-
tion algorithm outperformed the passage-level combina-
tion algorithm, which in turn outperformed the question-
level combination algorithm. Second, the cumulative im-
provement from multiple combination algorithms is in
general greater than that from the components. For in-
stance, the Q+A algorithm uniformly outperformed the Q
and A algorithms alone. Note, however, that the Q+P+A
algorithm achieved the highest performance only on the
TREC 11 test set using the % correct metric. We believe
KBA
TREC 10 (313) TREC 11 (453)
+ - + -
SA + 185 43 254 58
- 24 61 41 100
Table 2: Passage Retrieval Analysis
that this is because of compounding errors that occurred
during the multiple combination process.
In ensemble methods, the individual components must
make different mistakes in order for the combined sys-
tem to potentially perform better than the component sys-
tems (Dietterich, 1997). We examined the differences
in results between the two answering agents from their
question analysis, passage retrieval, and answer selection
components. We focused our analysis on the potential
gain/loss from incorporating contributions from the sta-
tistical agent, and how the potential was realized as actual
performance gain/loss in our end-to-end system.
At the question level, we examined those questions
for which the two agents proposed incompatible answer
types. On the TREC 10 test set, the statistical agent in-
troduced correct answer types in 6 cases and incorrect
answer types in 9 cases. As a result, in some cases the
question-level combination algorithm improved system
performance (comparing A and Q+A) and in others it
degraded performance (comparing P and Q+P). On the
other hand, on the TREC 11 test set, the statistical agent
introduced correct and incorrect answer types in 15 and
6 cases, respectively. As a result, in most cases perfor-
mance improved when the question-level combination al-
gorithm was invoked. The difference in question analysis
performance again reflects the fact that TREC 10 ques-
tions were used in question analysis rule refinement in
the knowledge-based agent.
At the passage level, we examined, for each ques-
tion, whether the candidate passages contained the cor-
rect answer. Table 2 shows the distribution of ques-
tions for which correct answers were (+) and were not
(-) present in the passages for both agents. The bold-
faced cells represent questions for which the statistical
agent retrieved passages with correct answers while the
knowledge-based agent did not. There were 43 and 58
such questions in the TREC 10 and TREC 11 test sets, re-
spectively, and employing the passage-level combination
algorithm resulted only in an additional 18 and 8 correct
answers on each test set. This is because the statistical
agent?s proposes in its 10 passages, on average, 29 candi-
date answers, most of which are incorrect, of the proper
semantic type per question. As the downstream answer
selection component takes redundancy into account in an-
swer ranking, incorrect answers may reinforce one an-
other and become top ranked answers. This suggests that
KBA
TREC 10 (313) TREC 11 (453)
1st 2-5th none 1st 2-5th none
SA 1st 66 22 26 93 21 35
2-5th 26 9 13 29 19 22
none 45 14 92 51 21 162
Table 3: Answer Voting Analysis
the relative contributions of our answer selection features
may not be optimally tuned for our multi-agent approach
to QA. We plan to investigate this issue in future work.
At the answer level, we analyzed each agent?s top 5
answers, used in the combination algorithm?s voting pro-
cess. Table 3 shows the distribution of questions for
which an answer was found in 1st place, in 2nd-5th place,
and not found in top 5. Since we employ a linear vot-
ing strategy based on confidence scores, we classify the
cells in Table 3 as follows based on the perceived likeli-
hood that the correct answers for questions in each cell
wins in the voting process. The boldfaced and underlined
cells contain highly likely candidates, since a correct an-
swer was found in 1st place by both agents.9 The bold-
faced cells consist of likely candidates, since a 1st place
correct answer was supported by a 2nd-5th place answer.
The italicized and underlined cells contain possible can-
didates, while the rest of the cells cannot produce correct
1st place answers using our current voting algorithm. On
TREC 10 data, 194 questions fall into the highly likely,
likely, and possible categories, out of which the voting al-
gorithm successfully selected 155 correct answers in 1st
place. On TREC 11 data, 197 correct answers were se-
lected out of 248 questions that fall into these categories.
These results represent success rates of 79.9% and 79.4%
for our answer-level combination algorithm on the two
test sets.
6 Related Work
There has been much work in employing ensemble meth-
ods to increase system performance in machine learning.
In NLP, such methods have been applied to tasks such
as POS tagging (Brill and Wu, 1998), word sense dis-
ambiguation (Pedersen, 2000), parsing (Henderson and
Brill, 1999), and machine translation (Frederking and
Nirenburg, 1994).
In question answering, a number of researchers have
investigated federated systems for identifying answers to
questions. For example, (Clarke et al, 2003) and (Lin et
al., 2003) employ techniques for utilizing both unstruc-
9These cells are not marked as definite because in a small
number of cases, the two answers are not equivalent. For exam-
ple, for the TREC 9 question, ?Who is the emperor of Japan??,
Hirohito, Akihito, and Taisho are all considered correct answers
based on the reference corpus.
tured text and structured databases for question answer-
ing. However, the approaches taken by both these sys-
tems differ from ours in that they enforce an order be-
tween the two strategies by attempting to locate answers
in structured databases first for select question types and
falling back to unstructured text when the former fails,
while we explore both options in parallel and combine
the results from multiple answering agents.
The multi-agent approach to question answering most
similar to ours is that by Burger et al (2003). They
applied ensemble methods to combine the 67 runs sub-
mitted to the TREC 11 QA track, using an unweighted
centroid method for selecting among the 67 proposed an-
swers for each question. However, their combined sys-
tem did not outperform the top scoring system(s). Fur-
thermore, their approach differs from ours in that they fo-
cused on combining the end results of a large number of
systems, while we investigated a tightly-coupled design
for combining two answering agents.
7 Conclusions
In this paper, we introduced a multi-strategy and multi-
source approach to question answering that enables com-
bination of answering agents adopting different strategies
and consulting multiple knowledge sources. In partic-
ular, we focused on two answering agents, one adopt-
ing a knowledge-based approach and one using statistical
methods. We discussed our answer resolution component
which employs a multi-level combination algorithm that
allows for resolution at the question, passage, and answer
levels. Best performance using the % correct metric was
achieved by the three-level algorithm that combines af-
ter each stage, while highest average precision was ob-
tained by a two-level algorithm merging at the question
and answer levels, supporting a tightly-coupled design
for multi-agent question answering. Our experiments
showed that our best performing algorithms achieved a
35.0% relative improvement in the number of correct an-
swers and a 32.8% improvement in average precision on
a previously unseen test set.
Acknowledgments
We would like to thank Dave Ferrucci, Chris Welty, and
Salim Roukos for helpful discussions, Diane Litman and
the anonymous reviewers for their comments on an ear-
lier draft of this paper. This work was supported in
part by the Advanced Research and Development Ac-
tivity (ARDA)?s Advanced Question Answering for In-
telligence (AQUAINT) Program under contract number
MDA904-01-C-0988.
References
Adam L. Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Eric Brill and Jun Wu. 1998. Classifier combination for
improved lexical disambiguation. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics, pages 191?195.
John D. Burger, Lisa Ferro, Warren Greiff, John Hender-
son, Marc Light, and Scott Mardis. 2003. MITRE?s
Qanda at TREC-11. In Proceedings of the Eleventh
Text Retrieval Conference. To appear.
Jennifer Chu-Carroll, David Ferrucci, John Prager, and
Christopher Welty. 2003a. Hybridization in ques-
tion answering systems. In Working Notes of the AAAI
Spring Symposium on New Directions in Question An-
swering, pages 116?121.
Jennifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003b. A
multi-strategy and multi-source approach to question
answering. In Proceedings of the Eleventh Text Re-
trieval Conference. To appear.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
C.L.A. Clarke, G.V. Cormack, G. Kemkes, M. Laszlo,
T.R. Lynam, E.L. Terra, and P.L. Tilker. 2003. Statis-
tical selection of exact answers. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Thomas G. Dietterich. 1997. Machine learning research:
Four current directions. AI Magazine, 18(4):97?136.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Fourth
Conference on Applied Natural Language Processing.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the 4th Conference on Em-
pirical Methods in Natural Language Processing.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2001. Question answering
in Webclopedia. In Proceedings of the Ninth Text RE-
trieval Conference, pages 655?664.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. Question answering using
maximum entropy components. In Proceedings of the
2nd Conference of the North American Chapter of the
Association for Computational Linguistics, pages 33?
39.
Abraham Ittycheriah. 2001. Trainable Question Answer-
ing Systems. Ph.D. thesis, Rutgers - The State Univer-
sity of New Jersey.
Douglas B. Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11).
Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory Mar-
ton, and Stefanie Tellex. 2003. Extracting an-
swers from the web using knowledge annotation and
knowledge mining techniques. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
George Miller. 1995. Wordnet: A lexical database for
English. Communications of the ACM, 38(11).
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 563?570.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive Bayesian classifiers for word sense
disambiguation. In Proceedings of the 1st Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 63?69.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive anno-
tation. In Proceedings of the 23rd SIGIR Conference,
pages 184?191.
John Prager, Dragomir Radev, and Krzysztof Czuba.
2001. Answering what-is questions by virtual anno-
tation. In Proceedings of Human Language Technolo-
gies Conference, pages 26?30.
Ellen M. Voorhees. 2001. Overview of the TREC-9
question answering track. In Proceedings of the 9th
Text Retrieval Conference, pages 71?80.
Ellen M. Voorhees. 2002. Overview of the TREC 2001
question answering track. In Proceedings of the 10th
Text Retrieval Conference, pages 42?51.
Ellen M. Voorhees. 2003. Overview of the TREC
2002 question answering track. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of the 19th SIGIR Conference, pages 4?11.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33?36,
New York, June 2006. c?2006 Association for Computational Linguistics
Answering the Question You Wish They Had Asked:
The Impact of Paraphrasing for Question Answering
Pablo Ariel Duboue
IBM T.J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532, USA
duboue@us.ibm.com
Jennifer Chu-Carroll
IBM T.J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532, USA
jencc@us.ibm.com
Abstract
State-of-the-art Question Answering (QA)
systems are very sensitive to variations
in the phrasing of an information need.
Finding the preferred language for such
a need is a valuable task. We investi-
gate that claim by adopting a simple MT-
based paraphrasing technique and evalu-
ating QA system performance on para-
phrased questions. We found a potential
increase of 35% in MRR with respect to
the original question.
1 Introduction
In a typical Question Answering system, an input
question is analyzed to formulate a query to re-
trieve relevant documents from a target corpus (Chu-
Carroll et al, 2006; Harabagiu et al, 2006; Sun
et al, 2006). This analysis of the input question
affects the subset of documents that will be exam-
ined and ultimately plays a key role in determining
the answers the system chooses to produce. How-
ever, most existing QA systems, whether they adopt
knowledge-based, statistical, or hybrid methods, are
very sensitive to small variations in the question
form, often yielding substantially different answers
for questions that are semantically equivalent. For
example, our system?s answer to ?Who invented the
telephone?? is ?Alexander Graham Bell;? how-
ever, its top answer to a paraphrase of the above
question ?Who is credited with the invention of the
telephone?? is ?Gutenberg,? who is credited with
the invention of the printing press, while ?Alexander
Graham Bell,? who is credited with the invention of
the telephone, appears in rank four.
To demonstrate the ubiquity of this phenomenon,
we asked the aforementioned two questions to sev-
eral QA systems on the web, including LCC?s Pow-
erAnswer system,1 MIT?s START system,2 Answer-
Bus,3 and Ask Jeeves.4 All systems exhibited dif-
ferent behavior for the two phrasings of the ques-
tion, ranging from minor variations in documents
presented to justify an answer, to major differences
such as the presence of correct answers in the answer
list. For some systems, the more complex question
form posed sufficient difficulty that they chose not
to answer it.
In this paper we focus on investigating a high risk
but potentially high payoff approach, that of improv-
ing system performance by replacing the user ques-
tion with a paraphrased version of it. To obtain can-
didate paraphrases, we adopt a simple yet powerful
technique based on machine translation, which we
describe in the next section. Our experimental re-
sults show that we can potentially achieve a 35% rel-
ative improvement in system performance if we have
an oracle that always picks the optimal paraphrase
for each question. Our ultimate goal is to automat-
ically select from the set of candidates a high po-
tential paraphrase using a component trained against
the QA system. In Section 3, we present our ini-
tial approach to paraphrase selection which shows
that, despite the tremendous odds against selecting
performance-improving paraphrases, our conserva-
tive selection algorithm resulted in marginal im-
provement in system performance.
1http://www.languagecomputer.com/demos
2http://start.csail.mit.edu
3http://www.answerbus.com
4http://www.ask.com
33
(A)
What toxins are most
hazardous to expectant
mothers?
en?it Che tossine sono pi? peri-
colose alle donne incinte?
it?en
Which toxins are more
dangerous to the preg-
nant women?
(B)
Find out about India?s
nuclear weapons pro-
gram.
en?es
Descubra sobre el pro-
grama de las armas nu-
cleares de la India.
es?en
Discover on the program
of the nuclear weapons
of India.
Figure 1: Example of lexical and syntactical paraphrases via MT-paraphrasing using Babelfish.
2 MT-Based Automatic Paraphrasing
To measure the impact of paraphrases on QA sys-
tems, we seek to adopt a methodology by which
paraphrases can be automatically generated from a
user question. Inspired by the use of parallel trans-
lations to mine paraphrasing lexicons (Barzilay and
McKeown, 2001) and the use of MT engines for
word sense disambiguation (Diab, 2000), we lever-
age existing machine translation systems to generate
semantically equivalent, albeit lexically and syntac-
tically distinct, questions.
Figure 1 (A) illustrates how MT-based paraphras-
ing captures lexical paraphrasing, ranging from ob-
taining simple synonyms such as hazardous and
dangerous to deriving more complex equivalent
phrases such as expectant mother and pregnant
woman. In addition to lexical paraphrasing, some
two-way translations achieve structural paraphras-
ing, as illustrated by the example in Figure 1 (B).
Using multiple MT engines can help paraphrase
diversity. For example, in Figure 1 (B), if we use the
@promt translator5 for English-to-Spanish transla-
tion and Babelfish6 for Spanish-to-English transla-
tion, we get ?Find out on the nuclear armament
program of India? where both lexical and struc-
tural paraphrasings are observed.
The motivation of generating an array of lexically
and structurally distinct paraphrases is that some of
these paraphrases may better match the processing
capabilities of the underlying QA system than the
original question and are thus more likely to pro-
duce correct answers. Our observation is that while
the paraphrase set contains valuable performance-
improving phrasings, it also includes a large num-
ber of ungrammatical sentences which need to be fil-
5http://www.online-translator.com
6http://babelfish.altavista.com
Q&A
System
Question
Answer List
Paraphrase
Selection
Feature
Extractor
paraphrase
...
paraphrase
paraphrase
paraphrase
...
paraphrase
paraphrase
paraphrase
MT
Paraphraser
Figure 2: System Architecture.
tered out to reduce negative impact on performance.
3 Using Automatic Paraphrasing in
Question Answering
We use a generic architecture (Figure 2) that treats
a QA system as a black box that is invoked after a
paraphrase generation module, a feature extraction
module, and a paraphrase selection module are exe-
cuted. The preprocessing modules identifies a para-
phrase of the original question, which could be the
question itself, to send as input to the QA system.
A key advantage of treating the core QA system as
a black box is that the preprocessing modules can
be easily applied to improve the performance of any
QA system.7
We described the paraphrase generation module
in the previous section and will discuss the remain-
ing two modules below.
Feature Extraction Module. For each possible
paraphrase, we compare it against the original ques-
tion and compute the features shown in Table 1.
These are a subset of the features that we have ex-
perimented with and have found to be meaningful
for the task. All of these features are required in or-
7In our earlier experiments, we adopted an approach that
combines answers to all paraphrases through voting. These ex-
periments proved unsuccessful: in most cases, the answer to the
original question was amplified, both when right and wrong.
34
Feature Description Intuition
Sum
IDF
The sum of the IDF scores for all terms in
the original question and the paraphrase.
Paraphrases with more informative terms for
the corpus at hand should be preferred.
Lengths Number of query terms for each of the para-phrase and the original question.
We expect QA systems to prefer shorter para-
phrases.
Cosine
Distance
The distance between the vectors of both
questions, IDF-weighted.
Certain paraphrases diverge too much from the
original.
Answer
Types
Whether answer types, as predicted by our
question analyzer, are the same or overlap.
Choosing a paraphrase that does not share an
answer type with the original question is risky.
Table 1: Our features, computed for each paraphrase by comparing it against the original question.
der not to lower the performance with respect to the
original question. They are ordered by their relative
contributions to the error rate reduction.
Paraphrase Selection Module. To select a para-
phrase, we used JRip, the Java re-implementation of
ripper (Cohen, 1996), a supervised rule learner in
the Weka toolkit (Witten and Frank, 2000).
We initially formulated paraphrase selection as a
three-way classification problem, with an attempt to
label each paraphrase as being ?worse,? the ?same,?
or ?better? than the original question. Our objective
was to replace the original question with a para-
phrase labeled ?better.? However, the priors for
these classes are roughly 30% for ?worse,? 65% for
?same,? and 5% for ?better?. Our empirical evi-
dence shows that successfully pinpointing a ?better?
paraphrase improves, on average, the reciprocal rank
for a question by 0.5, while erroneously picking a
?worse? paraphrase results in a 0.75 decrease. That
is to say, errors are 1.5 times more costly than suc-
cesses (and five times more likely). This scenario
strongly suggests that a high precision algorithm is
critical for this component to be effective.
To increase precision, we took two steps. First,
we trained a cascade of two binary classifiers. The
first one classifies ?worse? versus ?same or better,?
with a bias for ?worse.? The second classifier has
classes ?worse or same? versus ?better,? now with a
bias towards ?better.? The second step is to constrain
the confidence of the classifier and only accept para-
phrases where the second classifier has a 100% con-
fidence. These steps are necessary to avoid decreas-
ing performance with respect to the original ques-
tion, as we will show in the next section.
4 Experimental Results
We trained the paraphrase selection module us-
ing our QA system, PIQUANT (Chu-Carroll et al,
2006). Our target corpus is the AQUAINT corpus,
employed in the TREC QA track since 2002.
As for MT engines, we employed Babelfish
and Google MT,8 rule-based systems developed by
SYSTRAN and Google, respectively. We adopted
different MT engines based on the hypothesis that
differences in their translation rules will improve the
effectiveness of the paraphrasing module.
To measure performance, we trained and tested by
cross-validation over 712 questions from the TREC
9 and 10 datasets. We paraphrased the questions us-
ing the four possible combinations of MT engines
with up to 11 intermediate languages, obtaining a
total of 15,802 paraphrases. These questions were
then fed to our system and evaluated per TREC an-
swer key. We obtained a baseline MRR (top five
answers) of 0.345 running over the original ques-
tions. An oracle run, in which the best paraphrase
(or the original question) is always picked would
yield a MRR of 0.48. This potential increase is sub-
stantial, taking into account that a 35% improve-
ment separated the tenth participant from the sec-
ond in TREC-9. Our three-fold cross validation us-
ing the features and algorithm described in Section 3
yielded a MRR of 0.347. Over 712 questions, it re-
placed 14, two of which improved performance, the
rest stayed the same. On the other hand, random
selection of paraphrases decreased performance to
0.156, clearly showing the importance of selecting a
good paraphrase.
8http://translate.google.com
35
5 Related Work
Most of the work in QA and paraphrasing focused
on folding paraphrasing knowledge into the question
analyzer or the answer locator (Rinaldi et al, 2003;
Tomuro, 2003). Our work, on the contrary, focuses
on question paraphrasing as an external component,
independent of the QA system architecture.
Some authors (Dumais et al, 2002; Echihabi et
al., 2004) considered the query sent to a search en-
gine as a ?paraphrase? of the original natural lan-
guage question. For instance, Echihabi et al (2004)
presented a large number of ?reformulations? that
transformed the query into assertions that could
match the answers in text. Here we understand a
question paraphrase as a reformulation that is itself
a question, not a search engine query.
Other efforts in using paraphrasing for QA
(Duclaye et al, 2003) focused on using the Web
to obtain different verbalizations for a seed relation
(e.g., Author/Book); however, they have yet to apply
their learned paraphrases to QA.
Recently, there has been work on identifying para-
phrases equivalence classes for log analysis (Hed-
strom, 2005). Hedstrom used a vector model from
Information Retrieval that inspired our cosine mea-
sure feature described in Section 3.
6 Conclusions
The work presented here makes contributions at
three different levels. First, we have shown that po-
tential impact of paraphrasing with respect to QA
performance is significant. Replacing a question
with a more felicitously worded question can poten-
tially result in a 35% performance increase.
Second, we performed our experiments by tap-
ping into a readily available paraphrase resource:
MT engines. Our results speak of the usefulness of
the approach in producing paraphrases. This tech-
nique of obtaining a large, although low quality,
set of paraphrases can be easily employed by other
NLP practitioners wishing to investigate the impact
of paraphrasing on their own problems.
Third, we have shown that the task of selecting a
better phrasing is amenable to learning, though more
work is required to achieve its full potential. In that
respect, the features and architecture discussed in
Section 3 are a necessary first step in that direction.
In future work, we are interested in developing
effective filtering techniques to reduce our candidate
set to a small number of high precision paraphrases,
in experimenting with state-of-the-art paraphrasers,
and in using paraphrasing to improve the stability of
the QA system.
Acknowledgments
The authors would like to thank Nelson Correa and
Annie Ying for helpful discussions and comments.
This work was supported in part by the Disruptive
Technology Office (DTO)?s Advanced Question An-
swering for Intelligence (AQUAINT) Program un-
der contract number H98230-04-C-1577.
References
Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a
parallel corpus. In Proceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL-EACL 2001), Toulouse, France, July.
Jennifer Chu-Carroll, Pablo A. Duboue, John M. Prager, and Krzysztof Czuba.
2006. IBM?s piquant II in TREC 2005. In E. M. Voorhees and Lori P. Buck-
land, editors, Proceedings of the Fourthteen Text REtrieval Conference Pro-
ceedings (TREC 2005), Gaithersburg, MD, USA.
William Cohen. 1996. Learning trees and rules with set-valued features. In
Proceedings of the 14th joint American Association for Artificial Intelligence
and IAAI Conference (AAAI/IAAI-96), pages 709?716. American Association
for Artificial Intelligence.
Mona Diab. 2000. An unsupervised method for word sense tagging using parallel
corpora: A preliminary investigation. In Special Interest Group in Lexical
Semantics (SIGLEX) Workshop, Association for Computational Linguistics,
Hong Kong, China, October.
Florence Duclaye, Francois Yvon, and Olivier Collin. 2003. Learning para-
phrases to improve a question-answering system. In EACL 2003, 11th Con-
ference of the European Chapter of the Association for Computational Lin-
guistics, Workshop in NLP for QA, Budapest, Hungary, April.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering:
is more always better? In Proc. SIGIR ?02, pages 291?298, New York, NY,
USA. ACM Press.
A. Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, and D. Ravichandran.
2004. Multiple-engine question answering in textmap. In Proc. TREC 2003.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2006.
Employing two question answering systems. In Proc. TREC 2005.
Anna Hedstrom. 2005. Question categorization for a question answering system
using a vector space model. Master?s thesis, Department of Linguistics and
Philology (Language Technology Programme) Uppsala University, Uppsala,
Sweden.
Fabio Rinaldi, James Dowdall, Kaarel Kaljurand, Michael Hess, and Diego Moll?.
2003. Exploiting paraphrases in a question answering system. In Proceedings
of the Second International Workshop on Paraphrasing, pages 25?32, July.
R. Sun, J. Jiang, Y.F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. 2006. Using
syntactic and semantic relation analysis in question answering. In Proc. TREC
2005.
Noriko Tomuro. 2003. Interrogative reformulation patterns and acquisition of
question paraphrases. In Proceedings of the Second International Workshop
on Paraphrasing, pages 33?40, July.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning
Tools and Techniques with Java Implementations. Morgan Kaufmann Pub-
lishers.
36
Question Answering using Constraint Satisfaction:                                       
QA-by-Dossier-with-Constraints  
John Prager  
T.J. Watson Research Ctr. 
Yorktown Heights 
N.Y. 10598 
jprager@us.ibm.com 
Jennifer Chu-Carroll 
T.J. Watson Research Ctr. 
Yorktown Heights  
N.Y. 10598 
jencc@us.ibm.com 
Krzysztof Czuba  
T.J. Watson Research Ctr. 
Yorktown Heights  
N.Y. 10598 
kczuba@us.ibm.com 
 
 
 
Abstract 
QA-by-Dossier-with-Constraints is a new ap-
proach to Question Answering whereby candi-
date answers? confidences are adjusted by 
asking auxiliary questions whose answers con-
strain the original answers.  These constraints 
emerge naturally from the domain of interest, 
and enable application of real-world knowledge 
to QA.  We show that our approach signifi-
cantly improves system performance (75% rela-
tive improvement in F-measure on select 
question types) and can create a ?dossier? of in-
formation about the subject matter in the origi-
nal question. 
1 Introduction 
Traditionally, Question Answering (QA) has 
drawn on the fields of Information Retrieval, Natural 
Language Processing (NLP), Ontologies, Data Bases 
and Logical Inference, although it is at heart a prob-
lem of NLP.  These fields have been used to supply 
the technology with which QA components have 
been built.  We present here a new methodology 
which attempts to use QA holistically, along with 
constraint satisfaction, to better answer questions, 
without requiring any advances in the underlying 
fields. 
Because NLP is still very much an error-prone 
process, QA systems make many mistakes; accord-
ingly, a variety of methods have been developed to 
boost the accuracy of their answers.  Such methods 
include redundancy (getting the same answer from 
multiple documents, sources, or algorithms), deep 
parsing of questions and texts (hence improving the 
accuracy of confidence measures), inferencing 
(proving the answer from information in texts plus 
background knowledge) and sanity-checking (veri-
fying that answers are consistent with known facts).  
To our knowledge, however, no QA system deliber-
ately asks additional questions in order to derive 
constraints on the answers to the original questions.  
We have found empirically that when our own 
QA system?s (Prager et al, 2000; Chu-Carroll et al, 
2003) top answer is wrong, the correct answer is 
often present later in the ranked answer list.  In other 
words, the correct answer is in the passages re-
trieved by the search engine, but the system was un-
able to sufficiently promote the correct answer 
and/or deprecate the incorrect ones.  Our new ap-
proach of QA-by-Dossier-with-Constraints (QDC) 
uses the answers to additional questions to provide 
more information that can be used in ranking candi-
date answers to the original question.  These auxil-
iary questions are selected such that natural 
constraints exist among the set of correct answers.  
After issuing both the original question and auxiliary 
questions, the system evaluates all possible combi-
nations of the candidate answers and scores them by 
a simple function of both the answers? intrinsic con-
fidences, and how well the combination satisfies the 
aforementioned constraints.  Thus we hope to im-
prove the accuracy of an essentially NLP task by 
making an end-run around some of the more diffi-
cult problems in the field. 
We describe QDC and experiments to evaluate its 
effectiveness. Our results show that on our test set, 
substantial improvement is achieved by using con-
straints, compared with our baseline system, using 
standard evaluation metrics. 
2 Related Work 
Logic and inferencing have been a part of Ques-
tion-Answering since its earliest days.  The first 
such systems employed natural-language interfaces 
to expert systems, e.g.  SHRDLU (Winograd, 1972), 
or to databases e.g. LUNAR (Woods, 1973) and 
LIFER/LADDER (Hendrix et al 1977).  CHAT-80 
(Warren & Pereira, 1982) was a DCG-based NL-
query system about world geography, entirely in 
Prolog.  In these systems, the NL question is trans-
formed into a semantic form, which is then proc-
essed further; the overall architecture and system 
operation is very different from today?s systems, 
however, primarily in that there is no text corpus to 
process. 
Inferencing is used in at least two of the more 
visible systems of the present day.  The LCC system 
(Moldovan & Rus, 2001) uses a Logic Prover to 
establish the connection between a candidate answer 
passage and the question.  Text terms are converted 
to logical forms, and the question is treated as a goal 
which is ?proven?, with real-world knowledge being 
provided by Extended WordNet.  The IBM system 
PIQUANT (Chu-Carroll et al, 2003) uses Cyc (Le-
nat, 1995) in answer verification.  Cyc can in some 
cases confirm or reject candidate answers based on 
its own store of instance information; in other cases, 
primarily of a numerical nature, Cyc can confirm 
whether candidates are within a reasonable range 
established for their subtype.   
At a more abstract level, the use of constraints 
discussed in this paper can be viewed as simply an 
example of finding support (or lack of it) for candi-
date answers.  Many current systems (see, e.g. 
(Clarke et al, 2001), (Prager et al, 2004)) employ 
redundancy as a significant feature of operation:  if 
the same answer appears multiple times in an inter-
nal top-n list, whether from multiple sources or mul-
tiple algorithms/agents, it is given a confidence 
boost, which will affect whether and how it gets re-
turned to the end-user. 
 Finally, our approach is somewhat reminiscent of 
the scripts introduced by Schank (Schank et al, 
1975, and see also Lehnert, 1978). In order to gener-
ate meaningful auxiliary questions and constraints, 
we need a model (?script?) of the situation the ques-
tion is about.  Among others, we have identified one 
such script modeling the human life cycle that seems 
common to different question types regarding peo-
ple.   
3 Introducing QDC 
QA-by-Dossier-with-Constraints is an extension 
of on-going work of ours called QA-by-Dossier 
(QbD) (Prager et al, 2004).  In the latter, defini-
tional questions of the form ?Who/What is X? are 
answered by asking a set of specific factoid ques-
tions about properties of X.  So if X is a person, for 
example, these auxiliary questions may be about 
important dates and events in the person?s life-cycle, 
as well as his/her achievement.  Likewise, question 
sets can be developed for other entities such as or-
ganizations, places and things.  
QbD employs the notion of follow-on questions.  
Given an answer to a first-round question, the sys-
tem can ask more specific questions based on that 
knowledge.  For example, on discovering a person?s 
profession, it can ask occupation-specific follow-on 
questions: if it finds that people are musicians, it can 
ask what they have composed, if it finds they are 
explorers, then what they have discovered, and so 
on. 
QA-by-Dossier-with-Constraints extends this ap-
proach by capitalizing on the fact that a set of an-
swers about a subject must be mutually consistent, 
with respect to constraints such as time and geogra-
phy.   The essence of the QDC approach is to ini-
tially return instead of the best answer to 
appropriately selected factoid questions, the top n 
answers (we use n=5), and to choose out of this top 
set the highest confidence answer combination that 
satisfies consistency constraints. 
We illustrate this idea by way of the example, 
?When did Leonardo da Vinci paint the Mona 
Lisa??.  Table 1 shows our system?s top answers to 
this question, with associated scores in the range    
0-1. 
 
 Score Painting Date 
1 .64 2000 
2 .43 1988 
3 .34 1911 
4 .31 1503 
5 .30 1490 
 
Table 1.  Answers for ?When did Leonardo da 
Vinci paint the Mona Lisa?? 
 
The correct answer is ?1503?, which is in 4th 
place, with a low confidence score.  Using QA-by-
Dossier, we ask two related questions ?When was 
Leonardo da Vinci born?? and ?When did Leonardo 
da Vinci die??  The answers to these auxiliary ques-
tions are shown in Table 2.   
Given common knowledge about a person?s life 
expectancy and that a painting must be produced 
while its author is alive, we observe that the best 
dates proposed in Table 2 consistent with one an-
other are that Leonardo da Vinci was born in 1452, 
died in 1519, and painted the Mona Lisa in 1503.  
[The painting date of 1490 also satisfies the con-
straints, but with a lower confidence.]  We will ex-
amine the exact constraints used a little later.  This 
example illustrates how the use of auxiliary ques-
tions helps constrain answers to the original ques-
tion, and promotes correct answers with initial low 
confidence scores.  As a side-effect, a short dossier 
is produced. 
 
 
 Score Born  Score Died 
1 .66 1452  .99 1519 
2 .12 1519  .98 1989 
3 .04 1920  .96 1452 
4 .04 1987  .60 1988 
5 .04 1501  .60 1990 
Table 2.  Answers for auxiliary questions ?When 
was Leonardo da Vinci born?? and ?When did Leo-
nardo da Vinci die??. 
3.1 Reciprocal Questions 
QDC also employs the notion of reciprocal ques-
tions.  These are a type of follow-on question used 
solely to provide constraints, and do not add to the 
dossier.  The idea is simply to double-check the an-
swer to a question by inverting it, substituting the 
first-round answer and hoping to get the original 
subject back.  For example, to double-check ?Sac-
ramento? as the answer to ?What is the capital of 
California?? we would ask ?Of what state is Sacra-
mento the capital??.  The reciprocal question would 
be asked of all of the candidate answers, and the 
confidences of the answers to the reciprocal ques-
tions would contribute to the selection of the opti-
mum answer.  We will discuss later how this 
reciprocation may be done automatically.  In a sepa-
rate study of reciprocal questions (Prager et al, 
2004), we demonstrated an increase in precision 
from .43 to .95, with only a 30% drop in recall. 
Although the reciprocal questions seem to be 
symmetrical and thus redundant, their power stems 
from the differences in the search for answers inher-
ent in our system. The search is primarily based on 
the expected answer type (STATE vs. CAPITAL in 
the above example). This results in different docu-
ment sets being passed to the answer selection mod-
ule. Subsequently, the answer selection module 
works with a different set of syntactic and semantic 
relationships, and the process of asking a reciprocal 
question ends up looking more like the process of 
asking an independent one. The only difference be-
tween this and the ?regular? QDC case is in the type 
of constraint applied to resolve the resulting answer 
set. 
3.2 Applying QDC 
In order to automatically apply QDC during ques-
tion answering, several problems need to be ad-
dressed.  First, criteria must be developed to 
determine when this process should be invoked.  
Second, we must identify the set of question types 
that would potentially benefit from such an ap-
proach, and, for each question type, develop a set of 
auxiliary questions and appropriate constraints 
among the answers.  Third, for each question type, 
we must determine how the results of applying con-
straints should be utilized.  
3.2.1 When to apply QDC 
To address these questions we must distinguish 
between ?planned? and ?ad-hoc? uses of QDC.  For 
answering definitional questions (?Who/what is 
X??) of the sort used in TREC2003, in which collec-
tions of facts can be gathered by QA-by-Dossier, we 
can assume that QDC is always appropriate.  By 
defining broad enough classes of entities for which 
these questions might be asked (e.g. people, places, 
organizations and things, or major subclasses of 
these), we can for each of these classes manually 
establish once and for all a set of auxiliary questions 
for QbD and constraints for QDC.  This is the ap-
proach we have taken in the experiments reported 
here.  We are currently working on automatically 
learning effective auxiliary questions for some of 
these classes. 
In a more ad-hoc situation, we might imagine that 
a simple variety of QDC will be invoked using 
solely reciprocal questions whenever the difference 
between the scores of the first and second answer is 
below a certain threshold.    
3.2.2 How to apply QDC 
We will posit three methods of generating auxil-
iary question sets: 
o By hand 
o Through a structured repository, such as a 
knowledge-base of real-world information 
o Through statistical techniques tied to a machine-
learning algorithm, and a text corpus. 
We think that all three methods are appropriate, 
but we initially concentrate on the first for practical 
reasons.  Most TREC-style factoid questions are 
about people, places, organizations, and things, and 
we can generate generic auxiliary question sets for 
each of these classes.  Moreover, the purpose of this 
paper is to explain the QDC methodology and to 
investigate its value.   
3.2.3 Constraint Networks 
The constraints that apply to a given situation can 
be naturally represented in a network, and we find it 
useful for visualization purposes to depict the con-
straints graphically.  In such a graph the entities and 
values are represented as nodes, and the constraints 
and questions as edges.   
It is not clear how possible, or desirable, it is to 
automatically develop such constraint networks 
(other than the simple one for reciprocal questions), 
since so much real-world knowledge seems to be 
required.  To illustrate, let us look at the constraints 
required for the earlier example.  A more complex 
constraint system is used in our experiments de-
scribed later.  For our Leonardo da Vinci example, 
the set of constraints applied can be expressed as 
follows1: 
 
Date(Died) <= Date(Born) + 100 
Date(Painting) >=  Date(Born) + 7 
Date(Painting) <=  Date(Died) 
 
The corresponding graphical representation is in 
Figure 1.  Although the numerical constants in these 
constraints betray a certain arbitrariness, we found it 
a useful practice to find a middle ground between 
absolute minima or maxima that the values can 
achieve and their likely values.  Furthermore, al-
though these constraints are manually derived for 
our prototype system, they are fairly general for the 
human life-cycle and can be easily reused for other, 
similar questions, or for more complex dossiers, as 
described below. 
 
 
 
Figure 1.  Constraint Network for Leonardo ex-
ample.  Dashed lines represent question-answer 
pairs, solid lines constraints between the answers. 
We also note that even though a constraint net-
work might have been inspired by and centered 
around a particular question, once the network is 
established, any question employed in it could be the 
end-user question that triggers it. 
There exists the (general) problem of when more 
than one set of answers satisfies our constraints.  
Our approach is to combine the first-round scores of 
the individual answers to provide a score for the 
dossier as a whole.  There are several ways to do 
this, and we found experimentally that it does not 
appear critical exactly how this is done.  In the ex-
ample in the evaluation we mention one particular 
combination algorithm. 
3.2.4 Kinds of constraint network 
There are an unlimited number of possible con-
straint networks that can be constructed.  We have 
experimented with the following: 
Timelines.  People and even artifacts have life-
cycles.  The examples in this paper exploit these. 
                                                        
1 Painting is only an example of an activity in these constraints. 
Any other achievement that is usually associated with adulthood 
can be used. 
Geographic (?Where is X?).  Neighboring entities 
are in the same part of the world.   
Kinship (?Who is married to X?).  Most kinship 
relationships have named reciprocals e.g. husband-
wife, parent-child, and cousin-cousin.  Even though 
these are not in practice one-one relationships, we 
can take advantage of sufficiency even if necessity is 
not entailed. 
Definitional (?What is X??, ?What does XYZ stand 
for??)   For good definitions, a term and its defini-
tion are interchangeable. 
Part-whole.  Sizes of parts are no bigger than sizes 
of wholes.  This fact can be used for populations, 
areas, etc. 
3.2.5 QDC potential 
We performed a manual examination of the 500 
TREC2002 questions2 to see for how many of these 
questions the QDC framework would apply.  Being 
a manual process, these numbers provide an upper 
bound on how well we might expect a future auto-
matic process to work.  
We noted that for 92 questions (18%) a non-
trivial constraint network of the above kinds would 
apply.  For a total of 454 questions (91%), a simple 
reciprocal constraint could be generated.  However, 
for 61 of those, the reciprocal question was suffi-
ciently non-specific that the sought reciprocal an-
swer was unlikely to be found in a reasonably-sized 
hit-list.  For example, the reciprocal question to 
?How did Mickey Mantle die?? would be ?Who died 
of cancer??  However, we can imagine using other 
facts in the dossier to craft the question, giving us 
?What famous baseball player (or Yankees player) 
died of cancer??, giving us a much better chance of 
success.  For the simple reciprocation, though, sub-
tracting these doubtful instances leaves 79% of the 
questions appearing to be good candidates for QDC. 
4 Experimental Setup 
4.1 Test set generation 
To evaluate QDC, we had our system develop 
dossiers of people in the creative arts, unseen in pre-
vious TREC questions.  However, we wanted to use 
the personalities in past TREC questions as inde-
pendent indicators of appropriate subject matter.  
Therefore we collected all of the ?creative? people 
in the TREC9 question set, and divided them up into 
classes by profession, so we had, for example, male 
singers Bob Marley, Ray Charles, Billy Joel and 
Alice Cooper; poets William Wordsworth and 
Langston Hughes; painters Picasso, Jackson Pollock 
                                                        
2 This set did not contain definition questions, which, by our 
inspection, lend themselves readily to reciprocation. 
Birthdate 
Deathdate 
Leonardo Painting 
and Vincent Van Gogh, etc. ? twelve such groupings 
in all.  For each set, we entered the individuals in the 
?Google Sets? interface 
(http://labs.google.com/sets), which finds ?similar? 
entities to the ones entered.  For example, from our 
set of male singers it found: Elton John, Sting, Garth 
Brooks, James Taylor, Phil Collins, Melissa 
Etheridge, Alanis Morissette, Annie Lennox, Jack-
son Browne, Bryan Adams, Frank Sinatra and Whit-
ney Houston. 
Altogether, we gathered 276 names of creative 
individuals this way, after removing duplicates, 
items that were not names of individuals, and names 
that did not occur in our test corpus (the AQUAINT 
corpus).  We then used our system manually to help 
us develop ?ground truth? for a randomly selected 
subset of 109 names.  This ground truth served both 
as training material and as an evaluation key.  We 
split the 109 names randomly into a set of 52 for 
training and 57 for testing.  The training process 
used a hill-climbing method to find optimal values 
for three internal rejection thresholds.  In developing 
the ground truth we might have missed some in-
stances of assertions we were looking for, so the 
reported recall (and hence F-measure) figures should 
be considered to be upper bounds, but we believe the 
calculated figures are not far from the truth. 
4.2 QDC Operation 
The system first asked three questions for each 
subject X: 
 
 In what year was X born? 
 In what year did X die? 
 What compositions did X have? 
 
The third of these triggers our named-entity type 
COMPOSITION that is used for all kinds of titled 
works ? books, films, poems, music, plays and so 
on, and also quotations.  Our named-entity recog-
nizer has rules to detect works of art by phrases that 
are in apposition to ?the film ? ? or the ?the book 
? ? etc., and also captures any short phrase in quotes 
beginning with a capital letter.  The particular ques-
tion phrasing we used does not commit us to any 
specific creative verb.  This is of particular impor-
tance since it very frequently happens in text that 
titled works are associated with their creators by 
means of a possessive or parenthetical construction, 
rather than subject-verb-object. 
The top five answers, with confidences, are re-
turned for the born and died questions (subject to 
also passing a confidence threshold test).  The com-
positions question is treated as a list question, mean-
ing that all answers that pass a certain threshold are 
returned.  For each such returned work Wi, two addi-
tional questions are asked: 
 What year did X have Wi? 
 Who had Wi? 
 
The top 5 answers to each of these are returned, 
again as long as they pass a confidence threshold.  
We added a sixth answer ?NIL? to each of the date 
sets, with a confidence equal to the rejection thresh-
old.  (NIL is the code used in TREC ever since 
TREC10 to indicate the assertion that there is no 
answer in the corpus.)  We used a two stage con-
straint-satisfaction process: 
Stage 1:  For each work Wi for subject X, we 
added together its original confidence to the confi-
dence of the answer X in the answer set of the recip-
rocal question (if it existed ? otherwise we added 
zero).  If the total did not exceed a learned threshold 
(.50) the work was rejected. 
Stage 2.  For each subject, with the remaining 
candidate works we generated all possible combina-
tions of the date answers.  We rejected any combina-
tion that did not satisfy the following constraints: 
 
 DIED >= BORN + 7 
 DIED <= BORN + 100 
 WORK >= BORN + 7 
 WORK <= BORN + 100 
 WORK <= DIED 
 DIED <= WORK + 100 
 
The apparent redundancy here is because of the 
potential NIL answers for some of the date slots.  
We also rejected combinations of works whose 
years spanned more than 100 years (in case there 
were no BORN or DIED dates).  In performing these 
constraint calculations, NIL satisfied every test by 
fiat.  The constraint network we used is depicted in 
Figure 2. 
 
 
 
Figure 2.  Constraint Network for evaluation ex-
ample.  Dashed lines represent question-answer 
pairs, solid lines constraints between the answers. 
We used as a test corpus the AQUAINT corpus 
used in TREC-QA since 2002.  Since this was not 
the same corpus from which the test questions were 
generated (the Web), we acknowledged that there 
might be some difference in the most common spell-
ing of certain names, but we made no attempt to cor-
rect for this.  Neither did we attempt to normalize, 
translate or aggregate names of the titled works that 
were returned, so that, for example, ?Well-
Birthdate of X 
Deathdate of X 
Work Wi 
Author X Date of Wi 
Xi = Author of Wi 
Tempered Klavier? and ?Well-Tempered Clavier? 
were treated as different.  Since only individuals 
were used in the question set, we did not have in-
stances of problems we saw in training, such as 
where an ensemble (such as The Beatles) created a 
certain piece, which in turn via the reciprocal ques-
tion was found to have been written by a single per-
son (Paul McCartney).  The reverse situation was 
still possible, but we did not handle it.  We foresee a 
future version of our system having knowledge of 
ensembles and their composition, thus removing this 
restriction.  In general, a variety of ontological rela-
tionships could occur between the original individ-
ual and the discovered performer(s) of the work. 
We generated answer keys by reading the pas-
sages that the system had retrieved and from which 
the answers were generated, to determine ?truth?.  In 
cases of absent information in these passages, we 
did our own corpus searches.  This of course made 
the issue of evaluation of recall only relative, since 
we were not able to guarantee we had found all ex-
isting instances. 
We encountered some grey areas, e.g., if a paint-
ing appeared in an exhibition or if a celebrity en-
dorsed a product, then should the exhibition?s or 
product?s name be considered an appropriate ?work? 
of the artist?  The general perspective adopted was 
that we were not establishing or validating the nature 
of the relationship between an individual and a crea-
tive work, but rather its existence.  We answered 
?yes? if we subjectively felt the association to be 
both very strong and with the individual?s participa-
tion ? for example, Pamela Anderson and Playboy.  
However, books/plays about a person or dates of 
performances of one?s work were considered incor-
rect.  As we shall see, these decisions would not 
have a big impact on the outcome.   
4.3 Effect of Constraints 
The answers collected from these two rounds of 
questions can be regarded as assertions about the 
subject X.  By applying constraints, two possible 
effects can occur to these assertions: 
1. Some works can get thrown out. 
2. An asserted date (which was the top candidate 
from its associated question) can get replaced by 
a candidate date originally in positions 2-6 
(where sixth place is NIL) 
Effect #1 is expected to increase precision at the 
risk of worsening recall; effect #2 can go either way.  
We note that NIL, which is only used for dates, can 
be the correct answer if the desired date assertion is 
absent from the corpus; NIL is considered a ?value? 
in this evaluation. 
By inspection, performances and other indirect 
works (discussed in the previous section) were usu-
ally associated with the correct artist, so our decision 
to remove them from consideration resulted in a de-
crease in both the numerator and denominator of the 
precision and recall calculations, resulting in a 
minimal effect. 
The results of applying QDC to the 57 test indi-
viduals are summarized in Table 3.  The baseline 
assertions for individual X were: 
o Top-ranking birthdate/NIL   
o Top-ranking deathdate/NIL   
o Set of works Wi that passed threshold 
o Top-ranking date for Wi /NIL 
 
The sets of baseline assertions (by individual) are 
in effect the results of QA-by-Dossier WITHOUT 
Constraints (QbD). 
 
  Assertions Micro-Average Macro-Average 
  Total Cor-
rect 
Tru-
th 
Prec Rec F Prec Rec F 
Base-
line 
1671 517 933 .309 .554 .396 .331 .520 .386 
QDC 1417 813 933 .573 .871 .691 .603 .865 .690 
 
Table 3.  Results of Performance Evaluation.  
Two calculations of P/R/F are made, depending on 
whether the averaging is done over the whole set, or 
first by individual; the results are very similar.   
The QDC assertions were the same as those for 
QbD, but reflecting the following effects: 
o Some {Wi, date} pairs were thrown out (3 out of 
14 on average) 
o Some dates in positions 2-6 moved up (applica-
ble to birth, death and work dates) 
The results show improvement in both precision 
and recall, in turn determining a 75-80% relative 
increase in F-measure. 
5 Discussion 
This exposition of QA-by-Dossier-with-
Constraints is very short and undoubtedly leaves 
may questions unanswered.  We have not presented 
a precise method for computing the QDC scores. 
One way to formalize this process would be to treat 
it as evidence gathering and interpret the results in a 
Bayesian-like fashion. The original system confi-
dences would represent prior probabilities reflecting 
the system?s belief that the answers are correct.  As 
more evidence is found, the confidences would be 
updated to reflect the changed likelihood that an an-
swer is correct.  
We do not know a priori how much ?slop? should 
be allowed in enforcing the constraints, since auxil-
iary questions are as likely to be answered incor-
rectly as the original ones.  A further problem is to 
determine the best metric for evaluating such ap-
proaches, which is a question for QA in general.   
The task of generating auxiliary questions and 
constraint sets is a matter of active research.  Even 
for simple questions like the ones considered here, 
the auxiliary questions and constraints we looked at 
were different and manually chosen. Hand-crafting a 
large number of such sets might not be feasible, but 
it is certainly possible to build a few for common 
situations, such as a person?s life-cycle. More gener-
ally, QDC could be applied to situations in which a 
certain structure is induced by natural temporal (our 
Leonardo example) and/or spatial constraints, or by 
properties of the relation mentioned in the question 
(evaluation example). Temporal and spatial con-
straints appear general to all relevant question types, 
and include relations of precedence, inclusion, etc. 
For certain relationships, there are naturally-
occurring reciprocals (if X is married to Y, then Y is 
married to X; if X is a child of Y then Y is a parent 
of X; compound-term to acronym and vice versa).  
Transitive relationships (e.g. greater-than, located-
in, etc.) offer the immediate possibility of con-
straints, but this avenue has not yet been explored. 
5.1 Automatic Generation of Reciprocal Ques-
tions 
While not done in the work reported here, we are 
looking at generating reciprocal questions automati-
cally.  Consider the following transformations: 
 
?What is the capital of California?? -> ?Of what 
state is <candidate> the capital?? 
 
?What is Frank Sinatra?s nickname?? -> 
?Whose (or what person?s) nickname is <can-
didate>?? 
 
?How deep is Crater Lake?? -> ?What (or what 
lake) is <candidate> deep?? 
 
?Who won the Oscar for best actor in 1970??  
-> ?In what year did <candidate> win the 
Oscar for best actor?? (and/or ?What award 
did <candidate> win in 1970??) 
 
These are precisely the transformations necessary 
to generate the auxiliary reciprocal questions from 
the given original questions and candidate answers 
to them.  Such a process requires identifying an en-
tity in the question that belongs to a known class, 
and substituting the class name for the entity.  This 
entity is made the subject of the question, the previ-
ous subject (or trace) being replaced by the candi-
date answer.  We are looking at parse-tree rather 
than string transformations to achieve this.  This 
work will be reported in a future paper.  
5.2 Final Thoughts 
Despite these open questions, initial trials with 
QA-by-Dossier-with-Constraints have been very 
encouraging, whether it is by correctly answering 
previously missed questions, or by improving confi-
dences of correct answers.  An interesting question 
is when it is appropriate to apply QDC.  Clearly, if 
the base QA system is too poor, then the answers to 
the auxiliary questions will be useless; if the base 
system is highly accurate, the increase in accuracy 
will be negligible.  Thus our approach seems most 
beneficial to middle-performance levels, which, by 
inspection of TREC results for the last 5 years, is 
where the leading systems currently lie. 
We had initially thought that use of constraints 
would obviate the need for much of the complexity 
inherent in NLP.  As mentioned earlier, with the 
case of ?The Beatles? being the reciprocal answer to 
the auxiliary composition question to ?Who is Paul 
McCartney??, we see that structured, ontological 
information would benefit QDC.  Identifying alter-
nate spellings and representations of the same name 
(e.g. Clavier/Klavier, but also taking care of varia-
tions in punctuation and completeness) is also nec-
essary.  When we asked ?Who is Ian Anderson??, 
having in mind the singer-flautist for the Jethro Tull 
rock band, we found that he is not only that, but also 
the community investment manager of the English 
conglomerate Whitbread, the executive director of 
the U.S. Figure Skating Association, a writer for 
New Scientist, an Australian medical advisor to the 
WHO, and the general sales manager of Houseman, 
a supplier of water treatment systems.  Thus the 
problem of word sense disambiguation has returned 
in a particularly nasty form.  To be fully effective, 
QDC must be configured not just to find a consistent 
set of properties, but a number of independent sets 
that together cover the highest-confidence returned 
answers3.  Altogether, we see that some of the very 
problems we aimed to skirt are still present and need 
to be addressed.  However, we have shown that even 
disregarding these issues, QDC was able to provide 
substantial improvement in accuracy. 
6 Summary 
We have presented a method to improve the accu-
racy of a QA system by asking auxiliary questions 
for which natural constraints exist.  Using these con-
straints, sets of mutually consistent answers can be 
generated.  We have explored questions in the bio-
graphical areas, and identified other areas of appli-
cability.  We have found that our methodology 
exhibits a double advantage:  not only can it im-
                                                        
3 Possibly the smallest number of sets that provide such cover-
age.   
prove QA accuracy, but it can return a set of mutu-
ally-supporting assertions about the topic of the 
original question.  We have identified many open 
questions and areas of future work, but despite these 
gaps, we have shown an example scenario where 
QA-by-Dossier-with-Constraints can improve the F-
measure by over 75%. 
7 Acknowledgements 
We wish to thank Dave Ferrucci, Elena Filatova 
and Sasha Blair-Goldensohn for helpful discussions.  
This work was supported in part by the Advanced 
Research and Development Activity (ARDA)'s Ad-
vanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 
MDA904-01-C-0988. 
References 
Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci.  ?A Multi-Strategy and Multi-Source 
Approach to Question Answering?, Proceedings 
of the 11th TREC,  2003. 
Clarke, C., Cormack, G., Kisman, D.. and Lynam, T.  
?Question answering by passage selection 
(Multitext experiments for TREC-9)? in Proceed-
ings of the 9th TREC, pp. 673-683, 2001. 
Hendrix, G., E. Sacerdoti, D. Sagalowicz, J. Slocum: 
Developing a Natural Language Interface to Com-
plex Data. VLDB 1977: 292  
Lehnert, W.  The Process of Question Answering. A 
Computer Simulation of Cognition. Lawrence 
Erlbaum Associates, Publishers, 1978.  
Lenat, D. 1995.  "Cyc: A Large-Scale Investment in 
Knowledge Infrastructure." Communications of 
the ACM 38, no. 11. 
Moldovan, D. and V. Rus, ?Logic Form Transfor-
mation of WordNet and its Applicability to Ques-
tion Answering?, Proceedings of the ACL, 2001. 
Prager, J., E. Brown, A. Coden, and D. Radev. 2000. 
"Question-Answering by Predictive Annotation?.  
In Proceedings of SIGIR 2000, pp. 184-191.  
Prager, J., J. Chu-Carroll and K. Czuba, "A Multi-
Agent Approach to using Redundancy and Rein-
forcement in Question Answering" in New Direc-
tions in Question-Answering, Maybury, M. (Ed.),  
to appear in 2004. 
Schank, R. and R. Abelson. ?Scripts, Plans and 
Knowledge?, Proceedings of IJCAI?75. 
Voorhees, E. ?Overview of the TREC 2002 Ques-
tion Answering Track?, Proceedings of the 11th 
TREC, 2003. 
Warren, D., and F. Pereira "An efficient easily 
adaptable system for interpreting natural language 
queries," Computational Linguistics, 8:3-4, 110-
122, 1982.  
Winograd, T. Procedures as a representation for data 
in a computer program for under-standing natural 
language. Cognitive Psychology, 3(1), 1972. 
Woods, W. Progress in natural language understand-
ing --- an application in lunar geology. Proceed-
ings of the 1973 National Computer Conference, 
AFIPS Conference Proceedings, Vol. 42, 441--
450, 1973. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1073?1080,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving QA Accuracy by Question Inversion  
John Prager  
IBM T.J. Watson Res. Ctr. 
Yorktown Heights 
N.Y. 10598 
jprager@us.ibm.com 
Pablo Duboue 
IBM T.J. Watson Res. Ctr. 
Yorktown Heights  
N.Y. 10598 
duboue@us.ibm.com 
Jennifer Chu-Carroll 
IBM T.J. Watson Res. Ctr. 
Yorktown Heights  
N.Y. 10598 
jencc@us.ibm.com 
 
Abstract 
This paper demonstrates a conceptually simple 
but effective method of increasing the accuracy 
of QA systems on factoid-style questions.  We 
define the notion of an inverted question, and 
show that by requiring that the answers to the 
original and inverted questions be mutually con-
sistent, incorrect answers get demoted in confi-
dence and correct ones promoted.  Additionally, 
we show that lack of validation can be used to 
assert no-answer (nil) conditions.  We demon-
strate increases of performance on TREC and 
other question-sets, and discuss the kinds of fu-
ture activities that can be particularly beneficial 
to approaches such as ours.  
1 Introduction 
Most QA systems nowadays consist of the following 
standard modules:  QUESTION PROCESSING, to de-
termine the bag of words for a query and the desired 
answer type (the type of the entity that will be of-
fered as a candidate answer); SEARCH, which will 
use the query to extract a set of documents or pas-
sages from a corpus; and ANSWER SELECTION, 
which will analyze the returned documents or pas-
sages for instances of the answer type in the most 
favorable contexts. Each of these components im-
plements a set of heuristics or hypotheses, as de-
vised by their authors (cf. Clarke et al 2001, Chu-
Carroll et al 2003). 
 
When we perform failure analysis on questions in-
correctly answered by our system, we find that there 
are broadly speaking two kinds of failure.  There are 
errors (we might call them bugs) on the implementa-
tion of the said heuristics: errors in tagging, parsing, 
named-entity recognition; omissions in synonym 
lists; missing patterns, and just plain programming 
errors.  This class can be characterized by being fix-
able by identifying incorrect code and fixing it, or 
adding more items, either explicitly or through train-
ing.  The other class of errors (what we might call 
unlucky) are at the boundaries of the heuristics; 
situations were the system did not do anything 
?wrong,? in the sense of bug, but circumstances con-
spired against finding the correct answer. 
 
Usually when unlucky errors occur, the system gen-
erates a reasonable query and an appropriate answer 
type, and at least one passage containing the right 
answer is returned.  However, there may be returned 
passages that have a larger number of query terms 
and an incorrect answer of the right type, or the 
query terms might just be physically closer to the 
incorrect answer than to the correct one.  ANSWER 
SELECTION modules typically work either by trying 
to prove the answer is correct (Moldovan & Rus, 
2001) or by giving them a weight produced by 
summing a collection of heuristic features (Radev et 
al., 2000); in the latter case candidates having a lar-
ger number of matching query terms, even if they do 
not exactly match the context in the question, might 
generate a larger score than a correct passage with 
fewer matching terms. 
 
To be sure, unlucky errors are usually bugs when 
considered from the standpoint of a system with a 
more sophisticated heuristic, but any system at any 
point in time will have limits on what it tries to do; 
therefore the distinction is not absolute but is rela-
tive to a heuristic and system. 
 
It has been argued (Prager, 2002) that the success of 
a QA system is proportional to the impedance match 
between the question and the knowledge sources 
available.  We argue here similarly. Moreover, we 
believe that this is true not only in terms of the cor-
rect answer, but the distracters,1 or incorrect answers 
too.  In QA, an unlucky incorrect answer is not usu-
ally predictable in advance; it occurs because of a 
coincidence of terms and syntactic contexts that 
cause it to be preferred over the correct answer.  It 
has no connection with the correct answer and is 
only returned because its enclosing passage so hap-
pens to exist in the same corpus as the correct an-
swer context.  This would lead us to believe that if a 
                                                     
1 We borrow the term from multiple-choice test design. 
1073
different corpus containing the correct answer were 
to be processed, while there would be no guarantee 
that the correct answer would be found, it would be 
unlikely (i.e. very unlucky) if the same incorrect an-
swer as before were returned. 
 
We have demonstrated elsewhere (Prager et al 
2004b) how using multiple corpora can improve QA 
performance, but in this paper we achieve similar 
goals without using additional corpora. We note that 
factoid questions are usually about relations between 
entities, e.g. ?What is the capital of France??, where 
one of the arguments of the relationship is sought 
and the others given.  We can invert the question by 
substituting the candidate answer back into the ques-
tion, while making one of the given entities the so-
called wh-word, thus ?Of what country is Paris the 
capital??  We hypothesize that asking this question 
(and those formed from other candidate answers) 
will locate a largely different set of passages in the 
corpus than the first time around.  As will be ex-
plained in Section 3, this can be used to decrease the 
confidence in the incorrect answers, and also in-
crease it for the correct answer, so that the latter be-
comes the answer the system ultimately proposes. 
 
This work is part of a continuing program of demon-
strating how meta-heuristics, using what might be 
called ?collateral? information, can be used to con-
strain or adjust the results of the primary QA system.   
 
In the next Section we review related work.  In Sec-
tion 3 we describe our algorithm in detail, and in 
Section 4 present evaluation results.  In Section 5 we 
discuss our conclusions and future work. 
2 Related Work 
Logic and inferencing have been a part of Question-
Answering since its earliest days.  The first such 
systems were natural-language interfaces to expert 
systems, e.g., SHRDLU (Winograd, 1972), or to 
databases, e.g., LIFER/LADDER (Hendrix et al 
1977).  CHAT-80 (Warren & Pereira, 1982), for in-
stance, was a DCG-based NL-query system about 
world geography, entirely in Prolog.  In these 
systems, the NL question is transformed into a se-
mantic form, which is then processed further.  Their 
overall architecture and system operation is very 
different from today?s systems, however, primarily 
in that there was no text corpus to process. 
 
Inferencing is a core requirement of systems that 
participate in the current PASCAL Recognizing 
Textual Entailment (RTE) challenge (see 
http://www.pascal-network.org/Challenges/RTE and 
.../RTE2).   It is also used in at least two of the more 
visible end-to-end QA systems of the present day.  
The LCC system (Moldovan & Rus, 2001) uses a 
Logic Prover to establish the connection between a 
candidate answer passage and the question.  Text 
terms are converted to logical forms, and the ques-
tion is treated as a goal which is ?proven?, with real-
world knowledge being provided by Extended 
WordNet.  The IBM system PIQUANT (Chu-
Carroll et al, 2003) used Cyc (Lenat, 1995) in an-
swer verification.  Cyc can in some cases confirm or 
reject candidate answers based on its own store of 
instance information; in other cases, primarily of a 
numerical nature, Cyc can confirm whether candi-
dates are within a reasonable range established for 
their subtype.   
 
At a more abstract level, the use of inversions dis-
cussed in this paper can be viewed as simply an ex-
ample of finding support (or lack of it) for candidate 
answers.  Many current systems (see, e.g. (Clarke et 
al., 2001; Prager et al 2004b)) employ redundancy 
as a significant feature of operation:  if the same an-
swer appears multiple times in an internal top-n list, 
whether from multiple sources or multiple algo-
rithms/agents, it is given a confidence boost, which 
will affect whether and how it gets returned to the 
end-user. 
 
The work here is a continuation of previous work 
described in (Prager et al 2004a,b).  In the former 
we demonstrated that for a certain kind of question, 
if the inverted question were given, we could im-
prove the F-measure of accuracy on a question set 
by 75%.  In this paper, by contrast, we do not manu-
ally provide the inverted question, and in the second 
evaluation presented here we do not restrict the 
question type. 
3 Algorithm 
3.1 System Architecture 
A simplified block-diagram of our PIQUANT sys-
tem is shown in Figure 1.  The outer block on the 
left, QS1, is our basic QA system, in which the 
QUESTION PROCESSING (QP), SEARCH (S) and 
ANSWER SELECTION (AS) subcomponents are indi-
cated.  The outer block on the right, QS2, is another 
QA-System that is used to answer the inverted ques-
tions.  In principle QS2 could be QS1 but parameter-
ized differently, or even an entirely different system, 
but we use another instance of QS1, as-is.  The 
block in the middle is our Constraints Module CM, 
which is the subject of this paper.  
 
1074
The Question Processing component of QS2 is not 
used in this context since CM simulates its output by 
modifying the output of QP in QS1, as described in 
Section 3.3. 
3.2 Inverting Questions 
Our open-domain QA system employs a named-
entity recognizer that identifies about a hundred 
types.  Any of these can be answer types, and there 
are corresponding sets of patterns in the QUESTION 
PROCESSING module to determine the answer type 
sought by any question.  When we wish to invert a 
question, we must find an entity in the question 
whose type we recognize; this entity then becomes 
the sought answer for the inverted question.  We call 
this entity the inverted or pivot term. 
 
Thus for the question: 
(1) ?What was the capital of Germany in 1985?? 
Germany is identified as a term with a known type 
(COUNTRY).  Then, given the candidate answer 
<CANDANS>, the inverted question becomes  
(2) ?Of what country was < CANDANS> the capital 
in 1985?? 
Some questions have more than one invertible term.  
Consider for example:  
(3) ?Who was the 33rd president of the U.S.?? 
This question has 3 inversion points: 
(4) ?What number president of the U.S. was 
<CANDANS>?? 
(5) ?Of what country was <CANDANS> the 33rd 
president?? 
(6) ?<CANDANS> was the 33rd what of the U.S.?? 
 
Having more than one possible inversion is in theory 
a benefit, since it gives more opportunity for enforc-
ing consistency, but in our current implementation 
we just pick one for simplicity.  We observe on 
training data that, in general, the smaller the number 
of unique instances of an answer type, the more 
likely it is that the inverted question will be correctly 
answered.  We generated a set NELIST of the most 
frequently-occurring named-entity types in ques-
tions; this list is sorted in order of estimated cardi-
nality. 
 
It might seem that the question inversion process can 
be quite tricky and can generate possibly unnatural 
phrasings, which in turn can be difficult to reparse.  
However, the examples given above were simply 
English renditions of internal inverted structures ? as 
we shall see the system does not need to use a natu-
ral language representation of the inverted questions. 
 
Some questions are either not invertible, or, like 
?How did X die?? have an inverted form (?Who died 
of cancer??) with so many correct answers that we 
know our algorithm is unlikely to benefit us.  How-
ever, as it is constituted it is unlikely to hurt us ei-
ther, and since it is difficult to automatically identify 
such questions, we don?t attempt to intercept them.  
As reported in (Prager et al 2004a), an estimated 
79% of the questions in TREC question sets can be 
inverted meaningfully.  This places an upper limit 
on the gains to be achieved with our algorithm, but 
is high enough to be worth pursuing. 
Figure 1.  Constraints Architecture.  QS1 and QS2 are (possibly identical) QA systems. 
Answers
Question 
QS1 
QA system 
QP 
 question proc. 
S 
 search 
AS 
answer selection 
QS2 
QA system 
QP 
 question proc. 
S 
 search 
AS 
answer selection 
CM 
constraints 
module 
 
1075
3.3 Inversion Algorithm 
As shown in the previous section, not all questions 
have easily generated inverted forms (even by a hu-
man).  However, we do not need to explicate the 
inverted form in natural language in order to process 
the inverted question. 
 
In our system, a question is processed by the 
QUESTION PROCESSING module, which produces a 
structure called a QFrame, which is used by the sub-
sequent SEARCH and ANSWER SELECTION modules.  
The QFrame contains the list of terms and phrases in 
the question, along with their properties, such as 
POS and NE-type (if it exists), and a list of syntactic 
relationship tuples.  When we have a candidate an-
swer in hand, we do not need to produce the inverted 
English question, but merely the QFrame that would 
have been generated from it.  Figure 1 shows that 
the CONSTRAINTS MODULE takes the QFrame as one 
of its inputs, as shown by the link from QP in QS1 
to CM.  This inverted QFrame can be generated by a 
set of simple transformations, substituting the pivot 
term in the bag of words with a candidate answer 
<CANDANS>, the original answer type with the type 
of the pivot term, and in the relationships the pivot 
term with its type and the original answer type with 
<CANDANS>.  When relationships are evaluated, a 
type token will match any instance of that type.  Fig-
ure 2 shows a simplified view of the original 
QFrame for ?What was the capital of Germany in 
1945??, and Figure 3 shows the corresponding In-
verted QFrame.  COUNTRY is determined to be a 
better type to invert than YEAR, so ?Germany? be-
comes the pivot.  In Figure 3, the token 
<CANDANS> might take in turn ?Berlin?, ?Mos-
cow?, ?Prague? etc. 
 
Figure 2. Simplified QFrame 
 
Figure 3. Simplified Inverted QFrame.   
The output of QS2 after processing the inverted 
QFrame is a list of answers to the inverted question, 
which by extension of the nomenclature we call ?in-
verted answers.?  If no term in the question has an 
identifiable type, inversion is not possible. 
3.4 Profiting From Inversions 
Broadly speaking, our goal is to keep or re-rank the 
candidate answer hit-list on account of inversion 
results.  Suppose that a question Q is inverted 
around pivot term T, and for each candidate answer 
Ci, a list of ?inverted? answers {Cij} is generated as 
described in the previous section.  If T is on one of 
the {Cij}, then we say that Ci is validated.  Valida-
tion is not a guarantee of keeping or improving Ci?s 
position or score, but it helps.  Most cases of failure 
to validate are called refutation; similarly, refutation 
of Ci is not a guarantee of lowering its score or posi-
tion.   
 
It is an open question how to adjust the results of the 
initial candidate answer list in light of the results of 
the inversion.  If the scores associated with candi-
date answers (in both directions) were true prob-
abilities, then a Bayesian approach would be easy to 
develop.  However, they are not in our system.  In 
addition, there are quite a few parameters that de-
scribe the inversion scenario. 
 
Suppose Q generates a list of the top-N candidates 
{Ci}, with scores {Si}.  If this inversion method 
were not to be used, the top candidate on this list, 
C1, would be the emitted answer.  The question gen-
erated by inverting about T and substituting Ci is 
QTi.  The system is fixed to find the top 10 passages 
responsive to QTi, and generates an ordered list Cij 
of candidate answers found in this set. 
 
Each inverted question QTi is run through our sys-
tem, generating inverted answers {Cij}, with scores 
{Sij}, and whether and where the pivot term T shows 
up on this list, represented by a list of positions {Pi}, 
where Pi is defined as: 
 
 Pi  =  j    if Cij = T, for some j 
 Pi  =  -1 otherwise 
 
We added to the candidate list the special answer 
nil, representing ?no answer exists in the corpus.? 
 
As described earlier, we had observed from training 
data that failure to validate candidates of certain 
types (such as Person) would not necessarily be a 
real refutation, so we established a set of types 
SOFTREFUTATION which would contain the broadest 
of our types.  At the other end of the spectrum, we 
observed that certain narrow candidate types such as 
UsState would definitely be refuted if validation 
didn?t occur.  These are put in set MUSTCONSTRAIN. 
Our goal was to develop an algorithm for recomput-
ing all the original scores {Si} from some combina-
tion (based on either arithmetic or decision-trees) of 
Keywords: {1945, <CANDANS>, capital} 
AnswerType: COUNTRY 
Relationships: {(COUNTRY, capital), (capital, 
<CANDANS>), (capital, 1945)} 
Keywords: {1945, Germany, capital} 
AnswerType: CAPITAL 
Relationships: {(Germany, capital), (capital, 
CAPITAL), (capital, 1945)} 
1076
{Si} and {Sij} and membership of SOFTREFUTATION 
and MUSTCONSTRAIN.  Reliably learning all those 
weights, along with set membership, was not possi-
ble given only several hundred questions of training 
data.  We therefore focused on a reduced problem. 
 
We observed that when run on TREC question sets, 
the frequency of the rank of our top answer fell off 
rapidly, except with a second mode when the tail 
was accumulated in a single bucket.  Our numbers 
for TRECs 11 and 12 are shown in Table 1. 
 
Top answer rank TREC11 TREC12 
1 170 108 
2 35 32 
3 23 14 
4 7 7
5 14 9
elsewhere 251 244 
% correct 34 26 
Table 1.  Baseline statistics for TREC11-12. 
 
We decided to focus on those questions where we 
got the right answer in second place (for brevity, 
we?ll call these second-place questions).  Given that 
TREC scoring only rewards first-place answers, it 
seemed that with our incremental approach we 
would get most benefit there.  Also, we were keen to 
limit the additional response time incurred by our 
approach.  Since evaluating the top N answers to the 
original question with the Constraints process re-
quires calling the QA system another N times per 
question, we were happy to limit N to 2.  In addition, 
this greatly reduced the number of parameters we 
needed to learn.  
 
For the evaluation, which consisted of determining if 
the resulting top answer was right or wrong, it meant 
ultimately deciding on one of three possible out-
comes:  the original top answer, the original second 
answer, or nil.  We hoped to promote a significant 
number of second-place finishers to top place and 
introduce some nils, with minimal disturbance of 
those already in first place. 
 
We used TREC11 data for training, and established 
a set of thresholds for a decision-tree approach to 
determining the answer, using Weka (Witten & 
Frank, 2005).  We populated sets SOFTREFUTATION 
and MUSTCONSTRAIN by manual inspection.   
 
The result is Algorithm A, where (i ? {1,2}) and 
o The Ci are the original candidate answers 
o The ak are learned parameters (k ? {1..13}) 
o Vi means the ith answer was validated 
o Pi was the rank of the validating answer to ques-
tion QTi 
o Ai was the score of the validating answer to QTi. 
Algorithm A. Answer re-ranking using con-
straints validation data. 
1. If C1 = nil and V2,    return C2 
2. If V1 and A1 > a1,     return C1 
3. If not V1 and not V2 and  
 type(T) ? MUSTCONSTRAIN,  
    return nil 
4. If  not V1 and not V2 and  
 type(T) ?SOFTREFUTATION, 
if S1 > a2,, return C1 else nil 
5. If not V2,    return C1 
6. If not V1 and V2 and  
A2 > a3 and P2 < a4 and  
S1-S2 < a5 and S2 > a6, return C2 
7. If V1 and V2 and  
(A2 - P2/a7) > (A1 - P1/a7) and  
A1 < a8 and P1 > a9 and  
A2 < a10 and P2 > a11 and  
S1-S2 < a12  and (S2 - P2/a7) > a13,  
    return C2 
8. else return C1 
 
4 Evaluation 
Due to the complexity of the learned algorithm, we 
decided to evaluate in stages.  We first performed an 
evaluation with a fixed question type, to verify that 
the purely arithmetic components of the algorithm 
were performing reasonably.  We then evaluated on 
the entire TREC12 factoid question set. 
4.1 Evaluation 1 
We created a fixed question set of 50 questions of 
the form ?What is the capital of X??, for each state 
in the U.S.  The inverted question ?What state is Z 
the capital of?? was correctly generated in each 
case.  We evaluated against two corpora: the 
AQUAINT corpus, of a little over a million news-
wire documents, and the CNS corpus, with about 
37,000 documents from the Center for Nonprolifera-
tion Studies in Monterey, CA.  We expected there to 
be answers to most questions in the former corpus, 
so we hoped there our method would be useful in 
converting 2nd place answers to first place.  The lat-
ter corpus is about WMDs, so we expected there to 
be holes in the state capital coverage2, for which nil 
identification would be useful.3   
                                                     
2 We manually determined that only 23 state capitals were at-
tested to in the CNS corpus, compared with all in AQUAINT. 
3 We added Tbilisi to the answer key for ?What is the capi-
tal of Georgia??, since there was nothing in the question to 
disambiguate Georgia.  
1077
The baseline is our regular search-based QA-System 
without the Constraint process.  In this baseline sys-
tem there was no special processing for nil ques-
tions, other than if the search (which always 
contained some required terms) returned no docu-
ments.  Our results are shown in Table 2. 
 
 AQUAINT 
baseline 
AQUAINT 
w/con-
straints 
CNS 
baseline 
CNS 
w/con-
straints 
Firsts 
(non-nil) 
39/50 43/50 7/23 4/23 
Total 
nils 
0/0 0/0 0/27 16/27 
Total 
firsts 
39/50 43/50 7/50 20/50 
%  
correct 
78 86 14 40 
Table 2.  Evaluation on AQUAINT and CNS 
corpora. 
 
On the AQUAINT corpus, four out of seven 2nd 
place finishers went to first place.  On the CNS cor-
pus 16 out of a possible 26 correct no-answer cases 
were discovered, at a cost of losing three previously 
correct answers.  The percentage correct score in-
creased by a relative 10.3% for AQUAINT and 
186% for CNS.  In both cases, the error rate was 
reduced by about a third. 
4.2 Evaluation 2 
For the second evaluation, we processed the 414 
factoid questions from TREC12.  Of special interest 
here are the questions initially in first and second 
places, and in addition any questions for which nils 
were found. 
 
As seen in Table 1, there were 32 questions which 
originally evaluated in rank 2.  Of these, four ques-
tions were not invertible because they had no terms 
that were annotated with any of our named-entity 
types, e.g. #2285 ?How much does it cost for gas-
tric bypass surgery?? 
 
Of the remaining 28 questions, 12 were promoted to 
first place.  In addition, two new nils were found.  
On the down side, four out of 108 previous first 
place answers were lost.  There was of course 
movement in the ranks two and beyond whenever 
nils were introduced in first place, but these do not 
affect the current TREC-QA factoid correctness 
measure, which is whether the top answer is correct 
or not.  These results are summarized in Table 3.  
 
While the overall percentage improvement was 
small, note that only second?place answers were 
candidates for re-ranking, and 43% of these were 
promoted to first place and hence judged correct.  
Only 3.7% of originally correct questions were 
casualties.  To the extent that these percentages are 
stable across other collections, as long as the size of 
the set of second-place answers is at least about 1/10 
of the set of first-place answers, this form of the 
Constraint process can be applied effectively. 
 
 Baseline Constraints 
Firsts (non-nil) 105 113 
nils 3 5 
Total firsts 108 118 
% correct 26.1 28.5 
 
Table 3.  Evaluation on TREC12 Factoids. 
5 Discussion  
The experiments reported here pointed out many 
areas of our system which previous failure analysis 
of the basic QA system had not pinpointed as being 
too problematic, but for which improvement should 
help the Constraints process.  In particular, this work 
brought to light a matter of major significance, term 
equivalence, which we had not previously focused 
on too much (and neither had the QA community as 
a whole).  We will discuss that in Section 5.4. 
 
Quantitatively, the results are very encouraging, but 
it must be said that the number of questions that we 
evaluated were rather small, as a result of the com-
putational expense of the approach. 
 
From Table 1, we conclude that the most mileage is 
to be achieved by our QA-System as a whole by ad-
dressing those questions which did not generate a 
correct answer in the first one or two positions.  We 
have performed previous analyses of our system?s 
failure modes, and have determined that the pas-
sages that are output from the SEARCH component 
contain the correct answer 70-75% of the time.  The 
ANSWER SELECTION module takes these passages 
and proposes a candidate answer list. Since the CON-
STRAINTS MODULE?s operation can be viewed as a 
re-ranking of the output of ANSWER SELECTION, it 
could in principle boost the system?s accuracy up to 
that 70-75% level.  However, this would either re-
quire a massive training set to establish all the pa-
rameters and weights required for all the possible re-
ranking decisions, or a new model of the answer-list 
distribution.    
5.1 Probability-based Scores 
Our ANSWER SELECTION component assigns scores 
to candidate answers on the basis of the number of 
terms and term-term syntactic relationships from the 
1078
original question found in the answer passage 
(where the candidate answer and wh-word(s) in the 
question are identified terms).  The resulting num-
bers are in the range 0-1, but are not true probabili-
ties (e.g. where answers with a score of 0.7 would be 
correct 70% of the time).  While the generated 
scores work well to rank candidates for a given 
question, inter-question comparisons are not gener-
ally meaningful.  This made the learning of a deci-
sion tree (Algorithm A) quite difficult, and we 
expect that when addressed, will give better per-
formance to the Constraints process (and maybe a 
simpler algorithm).  This in turn will make it more 
feasible to re-rank the top 10 (say) original answers, 
instead of the current 2. 
5.2 Better confidences 
Even if no changes to the ranking are produced by 
the Constraints process, then the mere act of valida-
tion (or not) of existing answers can be used to ad-
just confidence scores.  In TREC2002 (Voorhees, 
2003), there was an evaluation of responses accord-
ing to systems? confidences in their own answers, 
using the Average Precision (AP) metric.  This is an 
important consideration, since it is generally better 
for a system to say ?I don?t know? than to give a 
wrong answer.  On the TREC12 questions set, our 
AP score increased 2.1% with Constraints, using the 
algorithm we presented in (Chu-Carroll et al 2002).    
5.3 More complete NER 
Except in pure pattern-based approaches, e.g. (Brill, 
2002), answer types in QA systems typically corre-
spond to the types identifiable by their named-entity 
recognizer (NER). There is no agreed-upon number 
of classes for an NER system, even approximately.  
It turns out that for best coverage by our 
CONSTRAINTS MODULE, it is advantageous to have a 
relatively large number of types.  It was mentioned 
in Section 4.2 that certain questions were not invert-
ible because no terms in them were of a recogniz-
able type.  Even when questions did have typed 
terms, if the types were very high-level then creating 
a meaningful inverted question was problematic.  
For example, for QA without Constraints it is not 
necessary to know the type of ?MTV? in ?When 
was MTV started??, but if it is only known to be a 
Name then the inverted question ?What <Name> 
was started in 1980?? could be too general to be ef-
fective. 
5.4 Establishing Term Equivalence 
The somewhat surprising condition that emerged 
from this effort was the need for a much more com-
plete ability than had previously been recognized for 
the system to establish the equivalence of two terms.  
Redundancy has always played a large role in QA 
systems ? the more occurrences of a candidate an-
swer in retrieved passages the higher the answer?s 
score is made to be. Consequently, at the very least, 
a string-matching operation is needed for checking 
equivalence, but other techniques are used to vary-
ing degrees. 
 
It has long been known in IR that stemming or lem-
matization is required for successful term matching, 
and in NLP applications such as QA, resources such 
as WordNet (Miller, 1995) are employed for check-
ing synonym and hypernym relationships; Extended 
WordNet (Moldovan & Novischi, 2002) has been 
used to establish lexical chains between terms.  
However, the Constraints work reported here has 
highlighted the need for more extensive equivalence 
testing. 
 
In direct QA, when an ANSWER SELECTION module 
generates two (or more) equivalent correct answers 
to a question (e.g. ?Ferdinand Marcos? vs. ?Presi-
dent Marcos?; ?French? vs. ?France?), and fails to 
combine them, it is observed that as long as either 
one is in first place then the question is correct and 
might not attract more attention from developers.  It 
is only when neither is initially in first place, but 
combining the scores of correct candidates boosts 
one to first place that the failure to merge them is 
relevant.  However, in the context of our system, we 
are comparing the pivot term from the original ques-
tion to the answers to the inverted questions, and 
failure here will directly impact validation and hence 
the usefulness of the entire approach. 
 
As a consequence, we have identified the need for a 
component whose sole purpose is to establish the 
equivalence, or generally the kind of relationship, 
between two terms.  It is clear that the processing 
will be very type-dependent ? for example, if two 
populations are being compared, then a numerical 
difference of 5% (say) might not be considered a 
difference at all; for ?Where? questions, there are 
issues of granularity and physical proximity, and so 
on.  More examples of this problem were given in 
(Prager et al 2004a).  Moriceau (2006) reports a 
system that addresses part of this problem by trying 
to rationalize different but ?similar? answers to the 
user, but does not extend to a general-purpose 
equivalence identifier.   
6 Summary 
We have extended earlier Constraints-based work 
through the method of question inversion.  The ap-
proach uses our QA system recursively, by taking 
candidate answers and attempts to validate them 
through asking the inverted questions. The outcome 
1079
is a re-ranking of the candidate answers, with the 
possible insertion of nil (no answer in corpus) as the 
top answer.   
 
While we believe the approach is general, and can 
work on any question and arbitrary candidate lists, 
due to training limitations we focused on two re-
stricted evaluations.  In the first we used a fixed 
question type, and showed that the error rate was 
reduced by 36% and 30% on two very different cor-
pora.  In the second evaluation we focused on ques-
tions whose direct answers were correct in the 
second position.  43% of these questions were sub-
sequently judged correct, at a cost of only 3.7% of 
originally correct questions.  While in the future we 
would like to extend the Constraints process to the 
entire answer candidate list, we have shown that ap-
plying it only to the top two can be beneficial as 
long as the second-place answers are at least a tenth 
as numerous as first-place answers.  We also showed 
that the application of Constraints can improve the 
system?s confidence in its answers. 
 
We have identified several areas where improve-
ment to our system would make the Constraints 
process more effective, thus getting a double benefit.  
In particular we feel that much more attention 
should be paid to the problem of determining if two 
entities are the same (or ?close enough?). 
7 Acknowledgments 
This work was supported in part by the Disruptive 
Technology Office (DTO)?s Advanced Question 
Answering for Intelligence (AQUAINT) Program 
under contract number H98230-04-C-1577.   We 
would like to thank the anonymous reviewers 
for their helpful comments. 
References 
Brill, E., Dumais, S. and Banko M. ?An analysis of 
the AskMSR question-answering system.? In Pro-
ceedings of EMNLP 2002. 
Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci.  ?A Multi-Strategy and Multi-Source 
Approach to Question Answering?, Proceedings 
of the 11th TREC, 2003. 
Clarke, C., Cormack, G., Kisman, D. and Lynam, T.  
?Question answering by passage selection 
(Multitext experiments for TREC-9)? in Proceed-
ings of the 9th TREC, pp. 673-683, 2001. 
Hendrix, G., Sacerdoti, E., Sagalowicz, D., Slocum 
J.: Developing a Natural Language Interface to 
Complex Data. VLDB 1977: 292  
Lenat, D. 1995.  "Cyc: A Large-Scale Investment in 
Knowledge Infrastructure." Communications of 
the ACM 38, no. 11. 
Miller, G. ?WordNet: A Lexical Database for Eng-
lish?, Communications of the ACM 38(11) pp. 
39-41, 1995. 
Moldovan, D. and Novischi, A, ?Lexical Chains for 
Question Answering?, COLING 2002. 
Moldovan, D. and Rus, V., ?Logic Form Transfor-
mation of WordNet and its Applicability to Ques-
tion Answering?, Proceedings of the ACL, 2001. 
Moriceau, V. ?Numerical Data Integration for Co-
operative Question-Answering?, in EACL Work-
shop on Knowledge and Reasoning for Language 
Processing (KRAQ?06), Trento, Italy, 2006. 
Prager, J.M., Chu-Carroll, J. and Czuba, K. "Ques-
tion Answering using Constraint Satisfaction: 
QA-by-Dossier-with-Constraints", Proc. 42nd 
ACL, pp. 575-582, Barcelona, Spain, 2004(a). 
Prager, J.M., Chu-Carroll, J. and Czuba, K. "A 
Multi-Strategy, Multi-Question Approach to 
Question Answering" in New Directions in Ques-
tion-Answering, Maybury, M. (Ed.), AAAI Press, 
2004(b). 
Prager, J., "A Curriculum-Based Approach to a QA 
Roadmap"' LREC 2002 Workshop on Question 
Answering: Strategy and Resources, Las Palmas, 
May 2002. 
Radev, D., Prager, J. and Samn, V. "Ranking Sus-
pected Answers to Natural Language Questions 
using Predictive Annotation", Proceedings of 
ANLP 2000, pp. 150-157, Seattle, WA. 
Voorhees, E. ?Overview of the TREC 2002 Ques-
tion Answering Track?, Proceedings of the 11th 
TREC, Gaithersburg, MD, 2003. 
Warren, D., and F. Pereira "An efficient easily 
adaptable system for interpreting natural language 
queries," Computational Linguistics, 8:3-4, 110-
122, 1982.  
Winograd, T. Procedures as a representation for data 
in a computer program for under-standing natural 
language. Cognitive Psychology, 3(1), 1972. 
Witten, I.H. & Frank, E. Data Mining.  Practical 
Machine Learning Tools and Techniques.  El-
sevier Press, 2005. 
 
1080
A Hybrid Approach to Natural Language Web Search
Jennifer Chu-Carroll, John Prager, Yael Ravin and Christian Cesar
IBM T.J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598, U.S.A.
{jencc,jprager,ravin,cesar}@us.ibm.com
Abstract
We describe a hybrid approach to improv-
ing search performance by providing a
natural language front end to a traditional
keyword-based search engine. The key
component of the system is iterative query
formulation and retrieval, in which one or
more queries are automatically formulated
from the user?s question, issued to the
search engine, and the results accumulated
to form the hit list. New queries are gener-
ated by relaxing previously-issued queries
using transformation rules, applied in an
order obtained by reinforcement learning.
This statistical component is augmented
by a knowledge-driven hub-page identi-
fier that retrieves a hub-page for the most
salient noun phrase in the question, if
possible. Evaluation on an unseen test
set over the www.ibm.com public web-
site with 1.3 million webpages shows that
both components make substantial contri-
bution to improving search performance,
achieving a combined 137% relative im-
provement in the number of questions cor-
rectly answered, compared to a baseline
of keyword queries consisting of two noun
phrases.
1 Introduction
Keyword-based search engines have been one of the
most highly utilized internet tools in recent years.
Nevertheless, search performance remains unsatis-
factory at most e-commerce sites (Hagen et al,
2000). Librarians and search professionals have tra-
ditionally favored Boolean keyword search systems,
which, when successful, return a small set of rele-
vant hits. However, the success of these systems crit-
ically depends on the choice of the right keywords
and the appropriate Boolean operators. As the pop-
ulation of search engine users has grown beyond a
small dedicated search professional community and
as these new users are less familiar with the contents
they are searching, it has become harder for them to
formulate successful keyword queries. To improve
search performance, one can improve search engine
accuracy with respect to fixed keyword queries, or
provide the search engine with better queries, those
more likely to retrieve good results. While there is
much on-going work in the IR community on the
former topic, we have taken the latter approach by
providing a natural language search interface and
automatically generating keyword queries that uti-
lize advanced search features typically unused by
end users. We believe that natural language ques-
tions are easier for users to construct than keyword
queries, thus shifting the burden of optimal query
formulation from the user to the system. Such ques-
tions also eliminate much of the ambiguity of key-
word queries that often leads to poor results. Fur-
thermore, the methodology we describe may be ap-
plied to different search engines with only minor
modification.
To transform natural language input into a search
query, the system must identify information perti-
nent for search and utilize it to formulate keyword
queries likely to retrieve relevant answers. We de-
scribe and evaluate a hybrid system, RISQUE, that
adopts an iterative approach to query formulation
and retrieval for search on the www.ibm.com pub-
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 180-187.
                         Proceedings of the Conference on Empirical Methods in Natural
lic website with 1.3 million webpages. RISQUE
may issue multiple queries per question, where a
new query is generated by relaxing a previously is-
sued query via transformation rule application, in
an order obtained by reinforcement learning. In ad-
dition, RISQUE identifies a hub-page for the most
salient noun phrase in the question, if possible,
utilizing traditional knowledge-driven mechanisms.
Evaluation on an unseen test set showed that both
the machine-learned and knowledge-driven compo-
nents made substantial contribution to improving
RISQUE?s performance, resulting in a combined
137% relative improvement in the number of ques-
tions correctly answered, compared to a baseline ob-
tained by queries consisting of two noun phrases
(2NP baseline).
2 Related Work
The popularity of natural language search is evi-
denced by the growing number of search engines,
such as AskJeeves, Electric Knowledge, and North-
ern Light,1 that offer such functionality. For most
sites, we were only able to perform a cursory ex-
amination of their proprietary techniques. Adopt-
ing a similar approach as FAQFinder (Hammond
et al, 1995), AskJeeves maintains a database of
questions and webpages that provide answers to
them. User questions are compared against those in
the database, and links to webpages for the closest
matches are returned. Similar to our approach, Elec-
tric Knowledge transforms a natural language ques-
tion into a series of increasingly more general key-
word queries (Bierner, 2001). However, their query
formulation process utilizes hard-crafted regular ex-
pressions, while we adopt a more general machine
learning approach for transformation rule applica-
tion.
Our work is also closely related to question an-
swering in the question analysis component (e.g.,
(Harabagui et al, 2001; Prager et al, 2000; Clarke
et al, 2001; Ittycheriah et al, 2001)). In partic-
ular, Harabagui et al(2001) also iteratively refor-
mulate queries based partly on the search results.
However, their mechanism for query reformulation
is heuristic-based. We utilized machine learning to
1www.askjeeves.com, www.electricknowledge.com, and
www.northernlight.com, respectively.
optimize the query formulation process.
3 Data Analysis
To generate optimal keyword queries from natural
language questions, we first analyzed a set of 502
questions related to the purchasing and support of
ThinkPads (notebook computers) and their acces-
sories, such as ?How do I set up hibernation for my
ThinkPad?? and ?Show me all p3 laptops.? Our
analysis focused on three tasks. First, we attempted
to identify an exhaustive set of correct webpages for
each question, where a correct webpage is one that
contains either an answer to the question or a hyper-
link to such a page. Second, we manually formu-
lated successful keyword queries from the question,
i.e., queries which retrieved at least one correct web-
page. Third, we attempted to discover general pat-
terns in how the natural language questions may be
transformed into successful keyword queries.
Our analysis eliminated 110 questions for which
no correct webpage was found. Of the remaining
392 questions, we identified, on average, 4.37 cor-
rect webpages and 1.58 successful queries per ques-
tion. We found that the characteristics of success-
ful queries varied greatly. In the simplest case, a
successful query may contain all the content bear-
ing NPs in the question, such as thinkpad AND
?answering machine? for ?Can I use my ThinkPad
as an answering machine??2 In the vast majority
of cases, however, more complex transformations
were applied to the question to result in a successful
query. For instance, a successful query for ?How do
I hook an external mouse to my laptop?? is (mouse
OR mice) AND thinkpad AND +url:support. In
this case, the head noun mouse was inflected,3 the
premodifier external was dropped, hook was deleted,
laptop was replaced by thinkpad, and a URL con-
straint was applied.
We observed that in our corpus, most success-
ful queries can be derived by applying one or
more transformation rules to the NPs and verbs
in the questions. Table 1 shows the manually in-
2Our search engine (www.alltheweb.com) accepts a con-
junction of terms (a word, quoted phrase, or disjunction of
words/phrases), and inclusion/exclusion of text strings in the
URL, such as +url:support.
3Many commercial search engines purposely do not inflect
search words to avoid overgeneralization of queries.
Rule Function
ConstrainURL Apply URL constraints
RelaxNP Relax phrase to conjunction of words
DropNP Remove least salient NP
DropModifier Remove premodifiers of nouns
DropVerb Remove verb
ApplySynonyms Add synonyms of NPs
Inflect Inflect head nouns and verb
Table 1: Query Transformation Rules
duced commonly-used transformation rules based
on our corpus analysis. Though the rules were quite
straightforward to identify, the order in which they
should be applied to yield optimal overall perfor-
mance was non-intuitive. In fact, the best order
we manually derived did not yield sufficient per-
formance improvement over our baseline (see Sec-
tion 7). We further hypothesize that the optimal rule
application sequence may be dependent on ques-
tion characteristics. For example, DropVerb may
be a higher priority rule for buy questions than for
support questions, since the verbs indicative of buy
questions (typically ?buy? or ?sell?) are often ab-
sent in the target product pages. Therefore, we in-
vestigated a machine learning approach to automat-
ically obtain the optimal rule application sequence.
4 A Reinforcement Learning Approach to
Query Formulation
Our problem consists of obtaining an optimal strat-
egy for choosing transformation rules to generate
successful queries. A key feature of this problem is
that feedback during training is often delayed, i.e.,
the positive effect of applying a rule may not be ap-
parent until a successful query is constructed after
the application of other rules. Thus, we adopt a rein-
forcement learning approach to obtain this optimal
strategy.
4.1 Q Learning
We adopted the Q learning paradigm (Watkins,
1989; Mitchell, 1997) to model our problem as a set
of possible states, S, and a set of actions, A, which
can be performed to alter the current state. While
in state s ? S and performing action a ? A, the
learner receives a reward r(s, a), and advances to
state s? = ?(s, a).
To learn an optimal control strategy that maxi-
mizes the cumulative reward over time, an evalua-
tion function Q(s, a) is defined as follows:
Q(s, a) ? r(s, a) + ? maxa?Q(?(s, a), a?) (1)
In other words, Q(s, a) is the immediate reward,
r(s, a), plus the discounted maximum future reward
starting from the new state ?(s, a).
The Q learning algorithm iteratively selects an ac-
tion and updates Q?, an estimate of Q, as follows:
Q?n(s, a) ? (1? ?n)Q?n?1(s, a) + (2)
?n(r(s, a) + maxa?Q?n?1(s?, a?))
where s? = ?(s, a) and ?n is inversely proportional
to the number of times a state/action pair <s,a> has
been visited up to the nth iteration of the algorithm.4
Once the system learns Q?, it can select from the
possible actions in state s based on Q?(s, ai).
4.2 Query Formulation Using Q Learning
To formulate our problem in the Q learning
paradigm, we represent a state as a 6-tuple,
<qtype, url constraint, np phrase, num nps,
num modifiers, num verbs>, where:
? qtype is buy or support depending on question
classification.
? url constraint is true or false, and determines
if manually predefined URL restrictions will be
applied in the query.
? np phrase is true or false, and determines
whether each NP will be searched for as a
phrase or a conjunction of words.
? num nps is an integer between 1 and 3, and
determines how many NPs will be included in
the query.
? num modifiers is an integer between 0 and 2,
and indicates the maximum number of premod-
ifiers in each NP.
? num verbs is 0 or 1, and determines if the verb
will be included in the query.
4Equation (2) modifies (1) by taking a decaying weighted
average of the current Q? value and the new value to guarantee
convergence of Q? in non-deterministic environments. We ex-
plain in the next section why our query formulation problem in
the Q learning framework is non-deterministic.
This representation is chosen based on the rules
identified in Section 3. The actions, A, include the
first 5 actions in Table 1, and the ?undo? counterpart
for each action.5 Except for qtype, which remains
static for a question, each remaining element in the
tuple can be altered by one of the 5 pairs of actions
in a straightforward manner. The state, s, and the
question, q, generate a unique keyword query which
results in a hit list, h(s, q). The hit list is evaluated
for correctness, whose result is used to define the
reward function as follows:
r(s, a) =
?
??????
??????
1 if h(s?, q) contains at least one
correct webpage
0 if h(s?, q) has no correct page &
|h(s?, q)| < 10
?1 otherwise
where s? = ?(s, a). Note that our system operates in
a non-deterministic environment because the reward
is dependent not only on s and a, but also on q.6
Having defined S, A, ?, and r, Q? is determined by
applying the Q learning algorithm, using the update
function in (2), to our corpus of 392 questions. For
each question, an initial state is randomly selected
within the bounds of the question. The system then
iteratively selects and applies actions, and updates
Q? until a successful query is generated or the maxi-
mum number of iterations is reached (in our imple-
mentation, 15). The training algorithm iterates over
all questions in the training set and terminates when
Q? converges.
5 RISQUE: A Hybrid System for Natural
Language Search
5.1 System Overview
In addition to motivating machine learning based
query transformation as our central approach to nat-
ural language search, our analysis revealed the need
for several other key system components. As shown
in Figure 1, RISQUE adopts a hybrid architecture
5Morphological and synonym expansions are applied at the
outset, which was shown to result in better performance than
optional application of those rules.
6It is theoretically possible to encode pertinent information
in q in the state representation, thus making the environment
deterministic. However, data sparseness problems associated
with such a representation makes it impractical.
Hit List Accumulation
Ontology
Hub-Page Identifier
Natural Language Question
Question Pre-Processing
Top n Hits for Question
Question Understanding
NP Sequencing
and Retrieval
Query Formulation
Figure 1: RISQUE Architecture
that combines the utility of traditional knowledge-
based methods and statistical approaches. Given
a question, RISQUE first performs question analy-
sis by extracting pertinent information to be used in
query formulation, such as the NPs, VPs, and ques-
tion type, and then orders the NPs in terms of their
relative salience. This information is then used for
hit list construction by two modules. The first com-
ponent is the hub-page identifier, which retrieves, if
possible, a hub page for the most salient NP in the
question. The second component is the Q learning
based query formulation and retrieval module that
iteratively generates queries via transformation rule
application and issues them to the search engine.
The results from both processes are combined and
accumulated until n distinct hits are retrieved.
In addition to the above components, RISQUE
employs an ontology for the ThinkPad domain,
which consists of 1) a hierarchy of about 500 do-
main objects, 2) nearly 400 instances of relation-
ships, such as isa and accessory-of, between objects,
and 3) a synonym dictionary containing about 1000
synsets. The ontology was manually constructed
and took approximately 2 person-months for cov-
erage in the ThinkPad domain. It provides perti-
nent information to the question pre-processing and
query formulation modules, which we will describe
in the next sections.
5.2 Question Pre-Processing
5.2.1 Question Understanding
RISQUE?s question understanding component is
based primarily on a rule-driven parser in the slot
grammar framework (McCord, 1989). The result-
ing parse tree is first analyzed for NP/VP extrac-
tion. Each NP includes the head noun and up to
two premodifiers, which covers most NPs in our do-
main. The NPs are further processed by a named-
entity recognizer (Prager et al, 2000; Wacholder et
al., 1997), with reference to domain-specific proper
names in our ontology. Recognized compound
terms, such as ?hard drive?, are treated as single en-
tities, rather than as head nouns (?drive?) with pre-
modifiers (?hard?). This prevents part of the com-
pound term from being dropped when the DropMod-
ifier transformation rule is applied.
The parse tree is also used to classify the question
as buy or support. The classifier utilizes a set of rules
based on lexical and part-of-speech information. For
example, ?how? tagged as a adverb (as in ?How do
I ...?) suggests a support question, while ?buy/sell?
used as a verb indicates a buy question. These rules
were manually derived based on our training data.
5.2.2 NP Sequencing
Our analysis showed that when a successful query
is to contain fewer NPs than in the question, it is
not straightforward to determine which NPs to elim-
inate, as it requires both domain and content knowl-
edge. However, we observed that less salient NPs
are often removed first, where salience indicates the
importance of the term in the search process. The
relative salience of NPs in this context can, for the
most part, be determined based on the ontological
relationship between the NPs and knowledge about
the website organization. For instance, if A is an
accessory-of B, then A is more salient than B since,
on our website, accessories typically have their own
webpages with significantly more information than
pages about, for instance, the ThinkPads with which
they are compatible.
Our NP sequencer utilizes a rule-based reasoning
mechanism to rank a set of NPs based on their rel-
ative salience, as determined by their relationship
in the ontology.7 Objects not present in the ontol-
7We are aware that factors involving deeper question under-
ogy are considered less important than those present.
This process produces a list of NPs ranked in de-
creasing order of salience.
5.3 Hub-Page Identifier
As with most websites, the ThinkPad pages on
www.ibm.com are organized hierarchically, with a
dozen or so hub-pages that serve as good starting
points for specific sub-topics, such as mobile acces-
sories and personal printers. However, since these
hub-pages are typically not content-rich, they often
do not receive high scores from the search engine
(over which we have no control). Thus, we devel-
oped a mechanism to explicitly retrieve these hub-
pages when appropriate, and to combine its results
with the outcome of the actual search process.
The hub-page identifier consists of a mapping
from a subset of the named entities in the ontology to
their corresponding hub-pages.8 For each question,
the hub-page identifier retrieves the hub-page for the
most salient NP, if possible, which is presented as
the first entry in the hit list.
5.4 Reinforcement Learning Based Query
Formulation
This main component of RISQUE iteratively formu-
lates queries, issues them to the search engine, and
accumulates the results to construct the hit list. The
query formulation process starts with the most con-
strained query, and each new query is a relaxation of
a previously issued query, obtained by applying one
or more transformation rules to the current query.
The transformation rules are applied in the order ob-
tained by the Q learning algorithm as described in
Section 4.2.
The initial state of the query formulation process
is as follows: url constraint and np phrase are set
to true, while the other attributes are set to their re-
spective maximum values based on the outcome of
the question understanding process. This initial state
represents the most constrained query possible for
the given question, and allows for subsequent relax-
ation via the application of transformation rules.
standing come into play in determining relative salience. We
leave investigation of such features as future work.
8For reasons of robustness, we actually map a named entity
to manually selected keywords which, when issued to the search
engine, retrieves the desired hub-page as the first hit.
When a state s, is visited, a query is generated
based on s and the question. The query terms
are instantiated based on the values of np phrase,
num nps, num modifiers, and num verbs in s and
the question itself, while URL constraints may be
applied based on url constraint and qtype. Finally,
synonyms expansion is applied based on the syn-
onym dictionary in the ontology, while morphologi-
cal expansion is performed on all NPs using a rule-
based inflection procedure.
After a query is issued, the search results are
incorporated into the hit list, and duplicate hits
are removed. A transformation rule amax =
argmaxaQ?(s, a) is applied to yield the new state.
Q?(s, amax) is then decreased to remove it from fur-
ther consideration. This iterative process continues
until the hit list contains 10 or more elements.
6 Example
To illustrate RISQUE?s end-to-end operation, con-
sider the question ?Do you sell a USB hub for a
ThinkPad??
The question is classified as a buy question, given
presence of the verb sell. In addition, two NPs are
identified:
NP1: head = USB hub
NP2: head = ThinkPad
Note that ?USB hub? is identified as a compound
noun by our named-entity recognizer. The NP se-
quencer determines that USB hub is more salient
than ThinkPad since the former is an accessory of
the latter.
The hub-page identifier finds the networking de-
vices hub-page for USB hub, presented as the first
entry in the hit list in Figure 2, where correct web-
pages are boldfaced.
Next, RISQUE invokes its iterative query for-
mulation process to populate the remaining hit
list entries. The initial state is <qtype = buy,
url constraint = true, np phrase = true, num nps
= 2, num modifiers = 0, num verbs = 0>. This
state generates the query shown as ?Query 2? in Fig-
ure 2, which results in 6 hits, of which 4 are correct.
RISQUE selects the optimal transformation rule
for the current state, which is ReinstateModifier.
Since neither NP has any modifier, a second rule,
RelaxNP is attempted, which resulted in a new query
that did not retrieve any previously unseen hits.
Next, RISQUE selects ConstrainNP and RelaxURL,
resulting in the query shown as ?Query 3? in Fig-
ure 2.9 Note that relaxing the URL constraint results
in retrieval of USB hub support pages.
7 Performance Evaluation and Analysis
We evaluated RISQUE?s performance on 102 ques-
tions in the ThinkPad domain previously unseen
to both RISQUE?s knowledge-based and statistical
components. The top 10 hits returned by RISQUE
for each question were manually evaluated for cor-
rectness as in Section 3. A 2NP baseline was ob-
tained by extracting up to two most salient NPs in
each question, searching for the conjunction of all
words in the NPs, and manually evaluating the 10
top hits returned.
We selected the 2NP baseline based on statistics
of keyword query logs on our website, which show
that 98.2% of all queries contain 4 keywords or less.
Furthermore, most three and four-word queries con-
tain two distinct noun phrases, such as ?visualage for
java? and ?application framework for e-business?.
Thus, we use the 2NP baseline as an approximation
of user keyword search performance for our natural
language questions.10
We compared RISQUE?s performance to the
baseline using three metrics:11
1. Total correct: number of questions for which
at least one correct webpage is retrieved.
2. Average correct: average number of correct
webpages retrieved per question.
3. Average rank: average rank of the first correct
webpage in the hit list.
The evaluation results are summarized in Table 2,
where the first and last rows show the 2NP base-
line and RISQUE?s performance, respectively. The
9A set of negative URL constraints is applied at all times to
best exclude parts of the website unrelated to ThinkPads.
10This is likely too high an estimate for current keyword
search performance, since the majority of user queries employ
only one noun phrase.
11We chose not to evaluate our results using the traditional
IR recall measure because for our task, it is often sufficient to
return one page that answers the question instead of attempting
to retrieve all relevant pages.
Question: Do you have a USB hub for a ThinkPad?
Query 1: hub-page identifier
1 Communications, Networking, Input/Output Devices ...
Query 2: (thinkpad thinkpads laptop laptops notebook notebooks) (?usb hub? ?usb hubs?)
+url: (commerce accessories proven products thinkpad)
-url: research press rs6000 eserver ...
2 Mobile Accessories ...
3 4-Port Ultra-Mini USB Hub
4 ThinkPad TransNote Port Extender
5 Belkin ExpressBus 7-Port USB Hub
6 Portable Drive Bay 2000
7 Belkin BusStation 7 Port Modular USB Hub
Query 3: (thinkpad thinkpads laptop laptops notebook notebooks) (?usb hub? ?usb hubs?)
-url: research press rs6000 eserver ...
8 Java technology for the universal serial bus
9 Multiport USB Hub - Printer compatibility list
10 Multi-Port USB Hub - Overview
Figure 2: RISQUE Results for Sample Question
Total Average Average
Correct Correct Rank
2NPs 30 0.63 4.0
Fixed Order 45 1.24 2.71
RISQUE w/o
hub identifier 56 1.67 2.25
RISQUE 71 1.87 2.11
Table 2: RISQUE Evaluation Results
results show that RISQUE correctly answered 71
questions, a 137% relative improvement over the
baseline. Furthermore, the average number of cor-
rect answers found nearly tripled, while, on average,
the rank of the first correct answer improved from
4.0 to 2.11.
Table 2 further shows performance figures that
evaluate the individual contribution of RISQUE?s
two main components, the hub-page identifier and
the iterative query formulation module. Comparison
between the last two rows in Table 2 shows the effec-
tiveness of the hub-page identifier, which substan-
tially increased the number of questions correctly
answered, but resulted in only minor gain using the
other two performance metrics. To assess the effec-
tiveness of the query formulation module, we used
the best manually-derived rule application sequence
obtained in Section 3. We compared these fixed or-
der performance figures to those for RISQUE w/o
hub identifier which shows that applying Q learning
to derive an optimal state-dependent rule application
order resulted in fairly substantial improvement us-
Maxq 10 5 3 2 1
RISQUE 5.07 4.47 2.89 1.98 1
RISQUE w/o hpi 4.26 3.86 2.80 1.93 1
Table 3: Average Queries Issued for Select Maxqs
20
30
40
50
60
70
0 1 2 3 4 5 6
Nu
mb
er o
f Qu
esti
ons
 An
swe
red
Average Number of Queries Issued
RISQUERISQUE w/o hpi2NP baseline
Figure 3: # Queries Issued vs. System Performance
ing all three metrics.
One of RISQUE?s parameters, maxq, specifies the
maximum number of distinct queries it can issue to
the search engine for each question. Table 3 shows
the average number of queries actually issued for
select values of maxq.12 Figure 3 shows how per-
formance degrades when fewer queries are issued
as a result of lowering maxq for both RISQUE and
RISQUE without the hub-page identifier. It shows
that, with the exception of RISQUE?s performance
12Maxq is 10 for the results in Table 2.
when only one query is issued,13 the number of
questions answered have a near-linear relationship
with the number of queries issued for both sys-
tems. Notice that without the hub-page identifier,
RISQUE?s performance when issuing an average of
1.93 queries per question is nearly the same as that
of the 2NP baseline, while it performs worse than
the baseline when issuing only one query per ques-
tion. This is because our iterative query formula-
tion process intentionally begins with the most con-
strained query, resulting in an empty hit list in many
cases.
8 Conclusions and Future Work
In this paper, we described and evaluated RISQUE,
a hybrid system for performing natural language
search on a large company public website. RISQUE
utilizes a two-pronged approach to generate hit lists
for answering natural language questions. On the
one hand, RISQUE employs a hub-page identi-
fier to retrieve, when possible, a hub-page for the
most salient NP in the question. On the other
hand, RISQUE adopts a statistical iterative query
formulation and retrieval mechanism that generates
new queries by applying transformation rules to
previously-issued queries. By employing these two
components in parallel, RISQUE takes advantages
of both knowledge-driven and machine learning ap-
proaches, and achieves an overall 137% relative im-
provement in the number of questions correctly an-
swered on an unseen test set, compared to a baseline
of 2NP keyword queries.
In our current work, we are focusing on expand-
ing system coverage to other domains. In particu-
lar, we plan to investigate semi-automatic methods
for extracting ontological knowledge from existing
webpages and databases.
Acknowledgments
We would like to thank Dave Ferrucci and Nanda
Kambhatla for helpful discussions, Ruchi Kalra and
Jerry Cwiklik for data preparation, Eric Brown and
the anonymous reviewers for helpful comments on
an earlier draft of this paper, as well as Mike Moran
13In most cases, this query is issued by the hub-page identi-
fier, which has a higher success rate than the queries generated
by the query formulation module.
and Alex Holt for providing technical assistance
with ibm.com search.
References
G. Bierner. 2001. Alternative phrases and natural lan-
guage information retrieval. In Proc. 39th ACL, pages
58?65.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploit-
ing redundancy in question answering. In Proc. 24th
SIGIR, pages 358?365.
P. Hagen, H. Manning, and Y. Paul. 2000. Must search
stink? The Forrester Report, June.
K. Hammond, R. Burke, C. Martin, and S. Lytinen. 1995.
Faq finder: A case-based approach to knowledge nav-
igation. In AAAI Spring Symposium on Information
Gathering in Heterogeneous Environments, pages 69?
73.
S. Harabagui, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and
P. Morarescu. 2001. The role of lexico-semantic feed-
back in open-domain textual question-answering. In
Proc. 39th ACL, pages 274?281.
A. Ittycheriah, M. Franz, W-J. Zhu, and A. Ratnaparkhi.
2001. Question answering using maximum entropy
components. In Proc. 2nd NAACL, pages 33?39.
M. McCord. 1989. Slot grammar: A system for simpler
construction of practical natural language grammars.
In Natural Language and Logic, pages 118?145.
T. Mitchell. 1997. Machine Learning. McGraw Hill.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question-answering by predictive annotation. In Proc.
23rd SIGIR, pages 184?191.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text. In Proc. 5th ANLP.
C. Watkins. 1989. Learning from Delayed Rewards.
Ph.D. thesis, King?s College.
   
		
	 ffMIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for 
Information Queries 
Jennifer Chu-Carroll 
Lucent Technologies Bell Laboratories 
600 Mountain Avenue 
Murray Hill, NJ 07974, U.S.A. 
jencc @research.bell-labs.corn 
Abstract 
This paper describes MIMIC, an adaptive mixed initia- 
tive spoken dialogue system that provides movie show- 
time information. MIMIC improves upon previous 
dialogue systems in two respects. First, it employs 
initiative-oriented strategy adaptation to automatically 
adapt response generation strategies based on the cumu- 
lative effect of information dynamically extracted from 
user utterances during the dialogue. Second, MIMIC's 
dialogue management architecture decouples its initia- 
tive module from the goal and response strategy selec- 
tion processes, providing ageneral framework for devel- 
oping spoken dialogue systems with different adaptation 
behavior. 
1 Introduction 
In recent years, speech and natural anguage technolo- 
gies have matured enough to enable the development of
spoken dialogue systems in limited domains. Most ex- 
isting systems employ dialogue strategies pre-specified 
during the design phase of the dialogue manager with- 
out taking into account characteristics of actual dialogue 
interactions. More specifically, mixed initiative systems 
typically employ rules that specify conditions (generally 
based on local dialogue context) under which initiative 
may shift from one agent o the other. Previous research, 
on the other hand, has shown that changes in initiative 
strategies inhuman-human dialogues can be dynamically 
modeled in terms of characteristics of the user and of 
the on-going dialogue (Chu-Carroll and Brown, 1998) 
and that adaptability of initiative strategies in dialogue 
systems leads to better system performance (Litman and 
Pan, 1999). However, no previous dialogue system takes 
into account these dialogue characteristics or allows for 
initiative-oriented adaptation of dialogue strategies. 
In this paper, we describe MIMIC, a voice-enabled 
telephone-based dialogue system that provides movie 
showtime information, emphasizing its dialogue man- 
agement aspects. MIMIC improves upon previous ys- 
tems along two dimensions. First, MIMIC automat- 
ically adapts dialogue strategies based on participant 
roles, characteristics of the current utterance, and dia- 
logue history. This automatic adaptation allows appro- 
priate dialogue strategies to be employed based on both 
local dialogue context and dialogue history, and has been 
shown to result in significantly better performance than 
non-adaptive systems. Second, MIMIC employs an ini- 
tiative module that is decoupled from the goal selection 
process in the dialogue manager, while allowing the out- 
come of both components ojointly determine the strate- 
gies chosen for response generation. As a result, MIMIC 
can exhibit drastically different dialogue behavior with 
very minor adjustments o parameters in the initiative 
module, allowing for rapid development and comparison 
of experimental prototypes and resulting in general and 
portable dialogue systems. 
2 Adaptive Mixed Initiative Dialogue 
Management 
2.1 Motivation 
In naturally occurring human-human dialogues, peakers 
often adopt different dialogue strategies based on hearer 
characteristics, dialogue history, etc. For instance, the 
speaker may provide more guidance if the hearer is hav- 
ing difficulty making progress toward task completion, 
while taking a more passive approach when the hearer 
is an expert in the domain. Our main goal is to enable 
a spoken dialogue system to simulate such human be- 
havior by dynamically adapting dialogue strategies dur- 
ing an interaction based on information that can be au- 
tomatically detected from the dialogue. Figure 1 shows 
an excerpt from a dialogue between MIMIC and an ac- 
tual user where the user is attempting to find the times 
at which the movie Analyze This playing at theaters in 
Montclair. S and U indicate system and user utterances, 
respectively, and the italicized utterances are the output 
of our automatic speech recognizer. In addition, each 
system turn is annotated with its task and dialogue ini- 
tiative holders, where task initiative tracks the lead in the 
process toward achieving the dialogue participants' do- 
main goal, while dialogue initiative models the lead in 
determining the current discourse focus (Chu-Carroll and 
Brown, 1998). In our information query application do- 
main, the system has task (and thus dialogue) initiative if 
its utterances provide helpful guidance toward achieving 
the user's domain goal, as in utterances (6) and (7) where 
MIMIC provided valid response choices to its query in- 
tending to solicit a theater name, while the system has 
97 
dialogue but not task initiative if its utterances only spec- 
ify the current discourse goal, as in utterance (4). i 
This dialogue illustrates everal features of our adap- 
tive mixed initiative dialogue manager. First, MIMIC au- 
tomatically adapted the initiative distribution based on 
information extracted from user utterances and dialogue 
history. More specifically, MIMIC took over task initia- 
tive in utterance (6), after failing to obtain a valid an- 
swer to its query soliciting a missing theater name in (4). 
It retained task initiative until utterance (12), after the 
user implicitly indicated her intention to take over task 
initiative by providing a fully-specified query (utterance 
(11)) to a limited prompt (utterance (10)). Second, ini- 
tiative distribution may affect the strategies MIMIC se- 
lects to achieve its goals. For instance, in the context 
of soliciting missing information, when MIMIC did not 
have task initiative, a simple information-seeking query 
was generated (utterance (4)). On the other hand, when 
MIMIC had task initiative, additional guidance was pro- 
vided (in the form of valid response choices in utterance 
(6)), which helped the user successfully respond to the 
system's query. In the context of prompting the user for 
a new query, when MIMIC had task initiative, a lim- 
ited prompt was selected to better constrain the user's 
response (utterance (10)), while an open-ended prompt 
was generated to allow the user to take control of the 
problem-solving process otherwise (utterances (1) and 
(13)). 
In the next section, we briefly review a framework for 
dynamic initiative modeling. In Section 3, we discuss 
how this framework was incorporated into the dialogue 
management component of a spoken dialogue system to 
allow for automatic adaptation of dialogue strategies. Fi- 
nally, we outline experiments evaluating the resulting 
system and show that MIMIC's automatic adaptation ca- 
pabilities resulted in better system performance. 
2.2 An Evidential Framework for Modeling 
Initiative 
In previous work, we proposed a framework for mod- 
eling initiative during dialogue interaction (Chu-Carroll 
and Brown, 1998). This framework predicts task and dia- 
logue initiative holders on a turn-by-turn basis in human- 
human dialogues based on participant roles (such as each 
dialogue agent's level of expertise and the role that she 
plays in the application domain), cues observed in the 
current dialogue turn, and dialogue history. More specif- 
ically, we utilize the Dempster-Shafer theory (Shafer, 
1976; Gordon and Shortliffe, 1984), and represent the 
current initiative distribution as two basic probability as- 
signments (bpas) which indicate the amount of support 
for each dialogue participant having the task and dia- 
logue initiatives. For instance, the bpa mt-cur({S}) =
l Although the dialogues we collected in our experiments (Sec- 
tion 5) include cases in which MIMIC has neither initiative, such cases 
are rare in this application domain, and will not be discussed further in 
this paper. 
0.3, mt-c~,r({U}) = 0.7 indicates that, with all evidence 
taken into account, there is more support (to the degree 
0.7) for the user having task initiative in the current urn 
than for the system. At the end of each turn, the bpas 
are updated based on the effects that cues observed ur- 
ing that turn have on changing them, and the new bpas 
are used to predict he next task and dialogue initiative 
holders. 
In this framework, cues that affect initiative distribu- 
tion include NoNewlnfo, triggered when the speaker sim- 
ply repeats or rephrases an earlier utterance, implicitly 
suggesting that the speaker may want to give up initia- 
tive, AmbiguousActions, triggered when the speaker pro- 
poses an action that is ambiguous in the application do- 
main, potentially prompting the hearer to take over ini- 
tiative to resolve the detected ambiguity, etc. The ef- 
fects that each cue has on changing the current bpas 
are also represented asbpas, which were determined by 
an iterative training procedure using a corpus of tran- 
scribed ialogues where each turn was annotated with the 
task/dialogue initiative holders and the observed cues. 
The bpas for the next turn are computed by combining 
the bpas representing the current initiative distribution 
and the bpas representing the effects of cues observed 
during the current urn, using Dempster's combination 
rule (Gordon and Shortliffe, 1984). The task and dia- 
logue initiative holders are then predicted based on the 
new bpas. This framework was evaluated using anno- 
tated dialogues from four task-oriented domains, achiev- 
ing, on average, a correct prediction rate of 97% and 88% 
for task and dialogue initiative holders, respectively. In 
Section 3.2, we discuss how this predictive model is con- 
verted into a generative model by enabling the system to 
automatically detect cues that were previously labelled 
manually. We further discuss how the model is used 
by the dialogue manager for dynamic dialogue strategy 
adaptation. 
3 MIMIC :  M ixed  Init iative Mov ie  
In fo rmat ion  Consu l tant  
MIMIC is a telephone-based dialogue system that pro- 
vides movie showtime information. It consists of the fol- 
lowing main components, implemented on a distributed, 
client-server a chitecture (Zhou et al, 1997): 
1. 
. 
Telephony server: this component detects rings 
and hang-ups, and enables treaming of audio data 
on channels of a telephony board. 
Speech recognizer: the recognizer receives audio 
data from the telephony server and generates the 
word string hypothesis that best matches the audio 
input. We used the Lucent Automatic Speech Rec- 
ognizer (Reichl and Chou, 1998; Ortmanns et al, 
1999), configured to use class-based probabilistic n- 
gram language models to allow for rapid updates of 
movie/theater/town names. 
_ql:l 
98
Utterance 
(1) S: Hello, this is MIMIC, the movie information system. 
(2) How can I help you? 
(3) U: What time is Analyze This playing in Montclair? 
\[what time is analyze this playing in montclair\] 
(4) S: What theater would you like? User 
(5) U: Montclair. 
\[montclair\] 
(6) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System 
(7) What theater would you like? 
(8) U: Clearviews Screening Zone. 
\[urn movie screening zone\] 
(9) S: Analyze This is playing at Clearviews Screening Zone in Montclair at l:15pm, 4:10pm, System 
and 6:30pm. 
(10) Please say the name of the movie or theater or town you would like information about. 
(11) U: What time is Analyze This playing at the Welimont Theater? 
\[what time is analyze this playing at the wellmont theater\] 
(12) S: Analyze This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, User 
and lO:lOpm. 
(13) Can I help you with anything else? 
Task 
Initiative 
User 
Dialogue 
Initiative 
System 
System 
System 
System 
System 
Figure 1: Excerpt of an Adaptive Mixed Initiative Dialogue 
3. NLP/Dialogue component: this main application- 
dependent component receives a user utterance hy- 
pothesis from the speech recognizer, and generates 
system utterance(s) in response. Three major tasks 
are carried out by this component: 1) semantic in- 
terpretation, which constructs frame-based seman- 
tic representations from user utterances, 2)dialogue 
management, where response strategies are selected 
based on the semantic representation f the user's 
utterance, system's domain knowledge, and initia- 
tive distribution, and 3) utterance generation, where 
utterance templates are chosen and instantiated to 
realize the selected response strategies. These three 
tasks will be discussed in further detail in the rest of 
this section. 
4. Text-to-speech engine: the TTS system receives 
the word string comprising the system's response 
from the dialogue component and converts the text 
into speech for output over the telephone. We used 
the Bell Labs TTS system (Sproat, 1998), which 
in addition to converting plain text into speech, ac- 
cepts text strings annotated to override default pitch 
height, accent placement, speaking rate, etc. 2 
3.1 Semantic Interpretation 
MIMIC utilizes a non-recursive frame-based semantic 
representation commonly used in spoken dialogue sys- 
tems (e.g. (Seneff et al, 1991; Lamel, 1998)), which 
represents an utterance as a set of attribute-value pairs. 
Figure 2(a) shows the frame-based semantic representa- 
tion for the utterance "What time is Analyze This playing 
2 See (Nakatani and Chu-Carroll, 2000) for how MIMIC's dialogue- 
level knowledge is used to override default prosodic assignments for 
concept-to-speech generation. 
Question-Type: When 
Movie: Analyze This 
Theater: null 
Town: Montclair 
(a) Semantic Representation 
Question-Type: When 
Movie: mandatory 
Theater: mandatory 
Town: optional 
(b) Task Specification 
Figure 2: Semantic Representation a d Task Specifica- 
tion 
in Montclair?" 
MIMIC's semantic representation is constructed by 
first extracting, for each attribute, a set of keywords from 
the user utterance. Using a vector-based topic identifi- 
cation process (Salton, 1971; Chu-Carroll and Carpen- 
ter, 1999), these keywords are used to determine a set 
of likely values (including null) for that attribute. Next, 
the utterance is interpreted with respect o the dialogue 
history and the system's domain knowledge. This al- 
lows MIMIC to handle elliptical sentences and anaphoric 
references, as well as automatically infer missing values 
and detect inconsistencies in the current representation. 
This semantic representation allows for decoupling 
of domain-dependent task specifications and domain- 
99
independent dialogue management s rategies. Each 
query type is specified by a template indicating, for each 
attribute, whether a value must, must not, or can option- 
ally be provided in order for a query to be considered 
well-formed. Figure 2(b) shows that to solicit movie 
showtime information (question type when), a movie 
name and a theater name must be provided, whereas a
town may optionally be provided. These specifications 
are determined based on domain semantics, and must be 
reconstructed when porting the system to a new domain. 
3.2 Dialogue Management 
Given a semantic representation, the dialogue history and 
the system's domain knowledge, the dialogue manager 
selects a set of strategies that guides MIMIC's response 
generation process. This task is carried out by three 
subprocesses: 1) initiative modeling, which determines 
the initiative distribution for the system's dialogue turn, 
2) goal selection, which identifies a goal that MIMIC's 
response attempts to achieve, and 3) strategy selection, 
which chooses, based on the initiative distribution, a set 
of dialogue acts that MIMIC will adopt in its attempt to 
realize the selected goal. 
3.2.1 Initiative Modeling 
MIMIC's initiative module determines the task and di- 
alogue initiative holders for each system turn in order 
to enable dynamic strategy adaptation. It automatically 
detects cues triggered uring the current user turn, and 
combines the effects of these cues with the current ini- 
tiative distribution to determine the initiative holders for 
the system's turn. 
Cue Detection The cues and the bpas representing 
their effects are largely based on a subset of those de- 
scribed in (Chu-Carroll and Brown, 1998), 3 as shown 
in Figures 3(a) and 3(b). Figure 3(a) shows that obser- 
vation of TakeOverTask supports a task initiative shift 
to the speaker to the degree .35. The remaining sup- 
port is assigned to O, the set of all possible conclusions 
(i.e., {speaker,hearer}), indicating that to the degree .65, 
observation of this cue does not commit to identifying 
which dialogue participant should have task initiative in 
the next dialogue turn. 
The cues used in MIMIC are classified into two cate- 
gories, discourse cues and analytical cues, based on the 
types of knowledge needed to detect hem: 
I. Discourse cues, which can be detected by consider- 
ing the semantic representation f the current utter- 
ance and dialogue history: 
? TakeOverTask, an implicit indication that the 
user wants to take control of the problem- 
solving process, triggered when the user pro- 
vides more information than the discourse x- 
pectation. 
3We selected only those cues that can be automatically detected in 
a spoken dialogue system with speech recognition errors and limited 
semantic interpretation capabilities. 
? NoNewlnfo, an indication that the user is un- 
able to make progress toward task completion, 
triggered when the semantic representations of 
two consecutive user turns are identical (a re- 
sult of the user not knowing what to say or the 
speech recognizer failing to recognize the user 
utterances). 
2. Analytical cues, which can only be detected by tak- 
ing into account MIMIC's domain knowledge: 
? lnvalidAction, an indication that the user made 
an invalid assumption about the domain, trig- 
gered when the system database lookup based 
on the user's query returns null. 
? lnvalidActionResolved, triggered when the 
previous invalid assumption is corrected. 
? AmbiguousAction, an indication that the user 
query is not well-formed, triggered when a 
mandatory attribute is unspecified or when 
more than one value is specified for an at- 
tribute. 
? AmbiguousActionResolved, triggered when the 
attribute in question is uniquely instantiated. 
Computing Initiative Distribution To determine the 
initiative distribution, the bpas representing the effects 
of cues detected in the current user utterance are instan- 
tiated (i.e., speaker~hearer in Figure 3 are instantiated as 
system~user accordingly). These effects are then inter- 
preted with respect o the current initiative distribution 
by applying Dempster's combination rule (Gordon and 
Shortliffe, 1984) to the bpas representing the current ini- 
tiative distribution and the instantiated bpas. This results 
in two new bpas representing the task and dialogue initia- 
tive distributions for the system's turn. The dialogue par- 
ticipant with the greater degree of support for having the 
task/dialogue initiative in these bpas is the task/dialogue 
initiative holder for the system's turn 4 (see Section 4 for 
an example). 
3.2.2 Goal Selection 
The goal selection module selects a goal that MIMIC at- 
tempts to achieve in its response by utilizing informa- 
tion from analytical cue detection as shown in Figure 4. 
MIMIC's goals focus on two aspects of cooperative di- 
alogue interaction: 1) initiating subdialogues to resolve 
anomalies that occur during the dialogue by attempting 
to instantiate an unspecified attribute, constraining an at- 
tribute for which multiple values have been specified, or 
correcting an invalid assumption i  the case of invalid 
41n practice, this is the preferred initiative holder since practical 
reasons may prevent the dialogue participant from actually holding the 
initiative. For instance, if having task initiative dictates inclusion of 
additional helpful information, this can only be realized if M1M1C's 
knowledge base provides uch information. 
"INN 100
Cue Class 
Discourse 
Analytical 
Cue 
TakeOverTask 
NoNewlnfo 
InvalidAction 
lnvalidActionResolved 
AmbiguousAction 
AmbiguousActionResolved 
BPA 
mt-tot({speaker}) = 0.35; mr-tot(O) = 0.65 
mt-,~ni({hearer}) = 0.35; mt-nn~(O) = 0.65 
mt-i~({hearer}) = 0.35; mt- ia(O) = 0.65 
mt-iar({hearer}) = 0.35; mt- iar(O)  = 0.65 
mt-aa({hearer}) = 0.35; mt-a~(O) = 0.65 
mt . . . .  ({speaker}) = 0.35; mt . . . .  (O) = 0.65 
(a)Task Initiative 
Cue Class 
Discourse 
Analytical 
Cue 
TakeOverTask 
NoNewlnfo 
lnvalidAction 
InvalidActionResolved 
AmbiguousAction 
AmbiguousActionResolved 
BPA 
md-tot({speaker}) = 0.35; ma-tot(O) = 0.65 
md-nni({hearer}) = 0.35; md-nni(O) -~- 0.65 
md-ia ({hearer}) = 0.7; md-ia (O) = 0.3 
ma-iar({hearer}) = 0.7; ma-iar(O) = 0.3 
ma-aa({hearer}) = 0.7; md_a~(O) = 0.3 
ma . . . .  ({speaker}) = 0.7; md . . . .  (O) = 0.3 
(b)Dialogue Initiative 
Figure 3: Cues and BPAs for Modeling Initiative in MIMIC 
Seleet-Goal(SemRep): 
(1) IfAmbiguousAction detected 
(2) ambiguous-attr +--get-ambiguous(SemRep) 
/* get name of ambiguous attribute */ 
(3) If (number-values(ambiguous-attr) == 0) 
/* attribute unspecified *,1 
(4) Instantiate(ambiguous-attr) 
(5) Else/* more than one value specified */ 
(6) Constrain(ambiguous-attr) 
(7) Else if lnvalidAction detected 
(8) ProvideNegativeAnswer(SemRep) 
(9) Else/* well-formed query */ 
(10) answer +-- database-query(SemRep) 
(11 ) ProvideAnswer(answer) 
Figure 4: Goal Selection Algorithm 
user queries (steps 1-8) 5 (van Beeket  al., 1993; Raskutti 
and Zukerman, 1993; Qu and Beale, 1999), and 2) pro- 
viding answers to well-formed queries (steps 9-11). 
3.2.3 Strategy Selection 
Previous work has argued that initiative affects the de- 
gree of control an agent has in the dialogue interaction 
(Whittaker and Stenton, 1988; Walker and Whittaker, 
1990; Chu-Carroll and Brown, 1998). Thus, a cooper- 
ative system may adopt different strategies to achieve the 
same goal depending on the initiative distribution. Since 
task initiative models contribution to domain/problem- 
solving goals, while dialogue initiative affects the cur- 
5An alternative strategy to step (4) is to perform adatabase lookup 
based on the ambiguous query and summarize the results (Litman et 
al., 1998), which we leave for future work. 
rent discourse goal, we developed alternative strategies 
for achieving the goals in Figure 4 based on initiative 
distribution, as shown in Table 1. 
The strategies employed when MIMIC has only dia- 
logue initiative are similar to the mixed initiative dia- 
logue strategies employed by many existing spoken di- 
alogue systems (e.g., (Bennacef et al, 1996; Stent et 
al., 1999)). To instantiate an attribute, MIMIC adopts 
the lnfoSeek dialogue act to solicit the missing informa- 
tion. In contrast, when MIMIC has both initiatives, it 
plays a more active role by presenting the user with addi- 
tional information comprising valid instantiations of the 
attribute (GiveOptions). Given an invalid query, MIMIC 
notifies the user of the failed query and provides an open- 
ended prompt when it only has dialogue initiative. When 
MIMIC has both initiatives, however, in addition to No- 
tifyFailure, it suggests an alternative close to the user's 
original query and provides a limited prompt. Finally, 
when MIMIC has neither initiative, it simply adopts No- 
tifyFailure, allowing the user to determine the next dis- 
course goal. 
3.3 Utterance Generat ion  
MIMIC employs a simple template-driven utterance gen- 
eration approach. Templates are associated with dialogue 
acts as shown in Table 2.6 The generation component re- 
ceives from the dialogue manager the selected ialogue 
acts and the parameters needed to instantiate the tem- 
plates. It then generates the system response, which is 
sent to the TTS module for spoken output synthesis. 
6In most cases, there is a one-to-one-mapping between dialogue 
acts and templates. The exceptions are Answer, NotifyFailure, and 
SuggestAlternative, whose templates vary based on the question type. 
101 
Task + Dialogue Dialogue None 
Constrain Clarify Clarify Acknowledge 
Instantiate GiveOptions lnfoSeek Acknowledge 
InfoSeek 
ProvideAnswer Answer Answer Answer 
LimitedPrompt OpenPrompt 
ProvideNegativeAnswer NotifyFailure NotifyFailure NotifyFailure 
SuggestAltemative OpenPrompt 
LimitedPrompt 
Table 1: Strategy Selection Based on Goal and Initiative Distribution 
Dialogue Act 
Clarify 
Acknowledge 
GiveOptions 
InfoSeek 
Answer 
OpenPrompt 
LimitedPrompt 
NotifyFailure 
SuggestAltemative 
Template 
"Did you say < valuel > .... or < valuen >.9" 
"Uh-huh." 
"Choices for < attribute > are < valuex > ... < value, >7 
"What < attribute > would you like?" 
E.g., "< movie > is playing at < theater > at < time1 > ... < time,, >" 
"Can I help you with anything elseT' 
"Please say the name of the movie or theater or town you would like information about." 
E.g., "< movie > is not playing at < theater >. 
E.g., "< movie > is playing at < alternativetheater > at < timex > ... < timen >" 
Table 2: Mappings Between Dialogue Acts and Utterance Templates 
4 Examples 
To illustrate MIMIC's adaptation capabilities, we return 
to the dialogue in Figure 1, which is repeated in Figure 5 
and annotated with the cues detected in each user turn 
(in boldfaced italics) and the dialogue acts employed for 
response generation i each system turn (in boldface). 
The bpas representing the initiative distribution for ut- 
terance (3) are the initial bpas, which, based on MIMIC's 
role as an information provider, are 
mt-(3)({S}) = 0.3, mt-(3)({U}) = 0.7; 
= 0.6, md- (3 ) ({V})  = 0.4. 
The cue AmbiguousAction is detected in utterance (3) 
because the mandatory attribute theater was not specified 
and cannot be inferred (since the town of Montclair has 
multiple theaters). The bpas representing its effect are 
instantiated as follows (Figure 3): 
mt-,,({S}) = 0.35, mt_ , , (O)  = 0.65; 
md-aa({S}) = 0.7, md-aa(O) = 0.3. 
Combining the current bpas with the effects of the ob- 
served cue, we obtain the following new bpas: 
mt-(4)({S}) = 0.4, mt_(a)({U}) = 0.6; 
md_(4)({S}) = 0.83, md_(4)({U}) = 0.17. 
The updated bpas indicate that MIMIC should have dia- 
logue but not task initiative when attempting to resolve 
the detected ambiguity in utterance (4). 
MIMIC selects Instantiate as its goal to be achieved 
(Figure 4), which, based on the initiative distribution, 
leads it to select he InfoSeek action (Table I) and gener- 
ate the query "What heater would you like?" 
The user's response in (5) again triggers Ambiguous- 
Action, as well as NoNewlnfo since the semantic repre- 
sentations of (3) and (5) are identical, given the dialogue 
context. When the effects of these cues are taken into 
account, we have the following initiative distribution for 
utterance (6): 
mt-(6)({S}) = 0.62, mt_(6)({U}) = 0.38; 
md-(6)({S}) = 0.96, rnd_(6)({V}) = 0.04. 
Although Instaatiate is again selected as the goal, 
MIMIC now has both task and dialogue initiatives; thus 
it selects both GiveOptions and lnfoSeek to achieve this 
goal and generates utterances (6) and (7). The addi- 
tional information, in the form of valid theater choices, 
helps the user provide the missing value in (8), allow- 
ing MIMIC to answer the query in (9) and prompt for 
the next query. However, despite the limited prompt, 
the user provides a well-formed query in (11), trigger- 
ing TakeOverTask. Thus, MIMIC answers the query and 
switches to an open-ended prompt in (13), relinquishing 
task initiative to the user. 
In addition to its automatic adaptation capabilities, an- 
other advantage of MIMIC is the ease of modifying its 
adaptation behavior, enabled by the decoupling of the 
initiative module from the goal and strategy selection 
processes. For instance, a system-initiative rsion of 
MIMIC can be achieved by setting the initial bpas as fol- 
lows: 
mt-initial({S}) = 1; 
md--initial({S}) -~1. 
102 
(1) S: Hello, this is MIMIC, the movie information system. 
(2) How can I help you? 
(3) U: What time is Analyze This playing in Montclair? 
\[what time is analyze this playing in montclair\] 
(4) S: What theater would you like? 
(5) U: Montclair. 
\[montclair\] 
(6) S: Choices for theater are Wellmont heater and Clearviews Screening Zone 
(7) What theater would you like? 
(8) U: Clearviews Screening Zone. 
\[um movie screening zone\] 
(9) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 
4:10pm, and 6:30pm. 
(10) Please say the name of the movie or theater or town you would like information 
about. 
(11) U: What time is Analyze This playing at the Wellmont heater? 
\[what time is analyze this playing at the wellmont theater\] 
(12) S: Analyze this is playing at Wellmont heater in Montclair at 3:15pm, 5:45pm, 
8:20pm, and 10:10pm. 
(13) Can I help you with anything else? 
\[AmbiguousAction\] 
\[lnfoSeek\] 
\[AmbiguousAction, NoNewlnfo\] 
\[GiveOptions\] 
\[InfoSeek\] 
\[AmbiguousActionResolved\] 
\[Answer\] 
\[LimitedPrompt\] 
\[TakeOverTask\] 
\[Answer\] 
\[OpenPrompt\] 
Figure 5: Annotated Dialogue Shown in Figure 1 
This is because in the Dempster-Shafer theory, if the 
initial bpas or the bpas for a cue provide definite evi- 
dence for drawing a certain conclusion, then no subse- 
quent cue has any effect on changing that conclusion. 
Thus, MIMIC will retain both initiatives throughout the 
dialogue. Alternatively, versions of MIMIC with differ- 
ent adaptation behavior can be achieved by tailoring the 
initial bpas and/or the bpas for each cue based on the ap- 
plication. For instance, for an electronic sales agent, the 
effect oflnvalidAction can be increased so that when the 
user orders an out-of-stock item, the system will always 
take over task initiative and suggest an alternative item. 
5 System Evaluation 
We conducted two experiments oevaluate MIMIC's au- 
tomatic adaptation capabilities. We compared MIMIC 
with two control systems: MIMIC-SI, a system-initiative 
version of MIMIC in which the system retains both ini- 
tiatives throughout the dialogue, and MIMIC-MI, a non- 
adaptive mixed-initiative version of MIMIC that resem- 
bles the behavior of many existing dialogue systems. In 
this section we summarize these experiments and their 
results. A companion paper describes the evaluation pro- 
cess and results in further detail (Chu-Carroll and Nick- 
erson, 2000). 
Each experiment involved eight users interacting with 
MIMIC and MIMIC-SI or MIMIC-MI to perform aset of 
tasks, each requiring the user to obtain specific movie in- 
formation. User satisfaction was assessed by asking the 
subjects to fill out a questionnaire after interacting with 
each version of the system. Furthermore, a number of 
performance f atures, largely based on the PARADISE 
dialogue valuation scheme (Walker et al, 1997), were 
automatically logged, derived, or manually annotated. In 
addition, we logged the cues automatically detected in 
each user utterance, as well as the initiative distribution 
for each turn and the dialogue acts selected to generate 
each system response. 
The features gathered from the dialogue interactions 
were analyzed along three dimensions: system perfor- 
mance, discourse features (in terms of characteristics 
of the resulting dialogues, such as the cues detected in 
user utterances), and initiative distribution. Our results 
show that MIMIC's adaptation capabilities 1) led to bet- 
ter system performance in terms of user satisfaction, dia- 
logue efficiency (shorter dialogues), and dialogue quality 
(fewer ASR timeouts), and 2) better matched user expec- 
tations (by giving up task initiative when the user intends 
to have control of the dialogue interaction) and more effi- 
ciently resolved ialogue anomalies (by taking over task 
initiative to provide guidance when no progress is made 
in the dialogue, or to constrain user utterances when ASR 
performance is poor). 
6 Conclusions 
In this paper, we discussed MIMIC, an adaptive mixed- 
initiative spoken dialogue system. MIMIC's automatic 
adaptation capabilities allow it to employ appropriate 
strategies based on the cumulative ffect of information 
dynamically extracted from user utterances during dia- 
logue interactions, enabling MIMIC to provide more co- 
operative and satisfactory responses than existing non- 
adaptive systems. Furthermore, MIMIC was imple- 
mented as a general framework for information query 
systems by decoupling its initiative module from the 
goal selection process, while allowing the outcome of 
both processes to jointly determine the response strate- 
gies employed. This feature nables easy modification to 
MIMIC's adaptation behavior, thus allowing the frame- 
work to be used for rapid development and comparisons 
103 
of experimental prototypes of spoken dialogue systems. 
Acknowledgments 
The author would like to thank Egbert Ammicht, An- 
toine Saad, Qiru Zhou, Wolfgang Reichl, and Stefan 
Ortmanns for their help on system integration and on 
ASR/telephony server development, Jill Nickerson for 
conducting the evaluation experiments, and Bob Carpen- 
ter, Diane Litman, Christine Nakatani, and Jill Nickerson 
for their comments on an earlier draft of this paper. 
References 
S. Bennacef, L. Devillers, S. Rosset, and L. Lamel. 
1996. Dialog in the RAILTEL telephone-based sys- 
tem. In Proceedings of the 4th International Confer- 
ence on Spoken Language Processing. 
Jennifer Chu-Carroll and Michael K. Brown. 1998. An 
evidential model for tracking initiative in collabora- 
tive dialogue interactions. User Modeling and User- 
Adapted Interaction, 8(3-4):215-253. 
Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector- 
based natural anguage call routing. Computational 
Linguistics, 25(3):361-388. 
Jennifer Chu-Carroll and Jill S. Nickerson. 2000. Evalu- 
ating automatic dialogue strategy adaptation for a spo- 
ken dialogue system. In Proceedings of the 1st Con- 
ference of the North American Chapter of the Associ- 
ation for Computational Linguistics. To appear. 
Jean Gordon and Edward H. Shortliffe. 1984. The 
Dempster-Shafer theory of evidence. In Bruce 
Buchanan and Edward Shortliffe, editors, Rule-Based 
Expert Systems: The MYCIN Experiments of the 
Stanford Heuristic Programming Project, chapter 13, 
pages 272-292. Addison-Wesley. 
Lori Lamel. 1998. Spoken language dialog system de- 
velopment and evaluation at LIMSI. In Proceedings 
of the International Symposium on Spoken Dialogue, 
pages 9-17. 
Diane J. Litman and Shimei Pan. 1999. Empirically 
evaluating an adaptable spoken dialogue system. In 
Proceedings of the 7th International Conference on 
User Modeling, pages 55-64. 
Diane J. Litman, Shimei Pan, and Marilyn A. Walker. 
1998. Evaluating response strategies in a web-based 
spoken dialogue agent. In Proceedings of the 36th 
Annual Meeting of the Association for Computational 
Linguistics, pages 780-786. 
Christine H. Nakatani and Jennifer Chu-Carroll. 2000. 
Using dialogue representations forconcept-to-speech 
generation. In Proceedings of the ANLP-NAACL 
Workshop on Conversational Systems. 
Stefan Ortmanns, Wolfgang Reichl, and Wu Chou. 1999. 
An efficient decoding method for real time speech 
recognition. In Proceedings of the 5th European Con- 
ference on Speech Communication a d Technology. 
Yan Qu and Steve Beale. 1999. A constraint-based 
model for cooperative r sponse generation i informa- 
tion dialogues. In Proceedings of the Sixteenth Na- 
tional Conference on Artificial Intelligence. 
Bhavani Raskutti and Ingrid Zukerman. 1993. Elicit- 
ing additional information during cooperative consul- 
tations. In Proceedings of the 15th Annual Meeting of 
the Cognitive Science Society. 
Wolfgang Reichl and Wu' Chou. 1998. Decision tree 
state tying based on segmental c ustering for acoustic 
modeling. In Proceedings of the International Confer- 
ence on Acoustics, Speech, and Signal Processing. 
Gerald Salton. 1971. The SMART Retrieval System. 
Prentice Hall, Inc. 
Stephanie Seneff, James Glass, David Goddeau, David 
Goodine, Lynette Hirschman, Hong Leung, Michael 
Phillips, Joseph Polifroni, and Victor Zue. 1991. De- 
velopment and preliminary evaluation of the MIT 
ATIS system. In Proceedings of the DARPA Speech 
and Natural Language Workshop, ages 88-93. 
Glenn Shafer. 1976. A Mathematical Theory of Evi- 
dence. Princeton University Press. 
Richard Sproat, editor. 1998. Multilingual Text-to- 
Speech Synthesis: The Bell Labs Approach. Kluwer, 
Boston, MA. 
Amanda Stent, John Dowding, Jean Mark Gawron, Eliz- 
abeth Owen Bratt, and Robert Moore. 1999. The 
CommandTalk spoken dialogue system. In Proceed- 
ings of the 37th Annual Meeting of the Association for 
Computational Linguistics, pages 183-190. 
Peter van Beek, Robin Cohen, and Ken Schmidt. 1993. 
From plan critiquing to clarification dialogue for co- 
operative response generation. Computational Intelli- 
gence, 9(2):132-154. 
Marilyn Walker and Steve Whittaker. 1990. Mixed ini- 
tiative in dialogue: An investigation i to discourse 
segmentation. In Proceedings of the 28th Annual 
Meeting of the Association for Computational Lin- 
guistics, pages 70-78. 
Marilyn A. Walker, Diane J. Litman, Candance A. 
Kamm, and Alicia Abella. 1997. PARADISE: A 
framework for evaluating spoken dialogue agents. In 
Proceedings of the 35th Annual Meeting of the Associ- 
ation for Computational Linguistics, pages 271-280. 
Steve Whittaker and Phil Stenton. 1988. Cues and con- 
trol in expert-client dialogues. In Proceedings of the 
26th Annual Meeting of the Association for Computa- 
tional Linguistics, pages 123-130. 
Qiru Zhou, Chin-Hui Lee, Wu Chou, and Andrew Pargel- 
lis. 1997. Speech technology integration and research 
platform: A system study. In Proceedings of the 5th 
European Conference on Speech Communication and 
Technology. 
104 
Evaluating Automatic Dialogue Strategy Adaptation for a 
Spoken Dialogue System 
Jennifer Chu-Carroll 
Lucent  Technologies Bell Laborator ies 
600 Mounta in  Avenue 
Murray Hill, NJ 07974, U.S.A. 
jencc @research.bel l - labs.corn 
Jill Suzanne Nickerson 
Harvard University 
Cambr idge,  MA 02138, U.S.A.  
nickerso @eecs.harvard.edu 
Abstract 
In this paper, we describe an empirical evaluation of an 
adaptive mixed initiative spoken dialogue system. We 
conducted two sets of experiments toevaluate the mixed 
initiative and automatic adaptation aspects of the system, 
and analyzed the resulting dialogues along three dimen- 
sions: performance factors, discourse features, and ini- 
tiative distribution. Our results show that 1) both the 
mixed initiative and automatic adaptation aspects led 
to better system performance in terms of user satisfac- 
tion and dialogue fficiency, and 2) the system's adap- 
tation behavior better matched user expectations, more 
efficiently resolved ialogue anomalies, and resulted in 
higher overall dialogue quality. 
1 Introduction 
Recent advances in speech technologies have enabled 
spoken dialogue systems to employ mixed initiative di- 
alogue strategies (e.g. (Allen et al, 1996; Sadek et al, 
1996; Meng et al, 1996)). Although these systems inter- 
act with users in a manner more similar to human-human 
interactions than earlier systems employing system ini- 
tiative strategies, their response strategies are typically 
selected using only local dialogue context, disregarding 
dialogue history. Therefore, their gain in naturalness and 
performance under optimal conditions i often overshad- 
owed by their inability to cope with anomalies in dia- 
logues by automatically adapting dialogue strategies. In 
contrast, Figure 1 shows a dialogue in which the sys- 
tem automatically adapts dialogue strategies based on 
the current user utterance and dialogue history. 1 Af- 
ter failing to obtain a valid response to an information- 
seeking query in utterance (4), the system adapted ia- 
logue strategies to provide additional information i  (6) 
that assisted the user in responding to the query. Further- 
more, after the user esponded toa limited system prompt 
in (10) with a fully-specified query in (11), implicitly 
indicating her intention to take charge of the problem- 
IS and U indicate system and user utterances, respectively. The 
words appearing in square brackets are the output from the Lucent 
Automatic Speech Recognizer (Reichl and Chou, 1998; Ortmanns et 
al., 1999), configured to use class-based probabilistic n-gram language 
models. The task and dialogue initiative annotations are explained in 
Section 2.1. 
solving process, the system again adapted strategies, 
hence providing an open-ended prompt in (13). 
Previous work has shown that dialogue systems in 
which users can explicitly change the system's dia- 
logue strategies result in better performance than non- 
adaptable systems (Litman and Pan, 1999). However, 
no earlier system allowed for initiative-oriented auto- 
matic strategy adaptation based on information dynam- 
ically extracted from the user's spoken input. In this 
paper, we briefly introduce MIMIC, a mixed initiative 
spoken dialogue system that automatically adapts dia- 
logue strategies. We then describe two experiments that 
evaluated the effectiveness of MIMIC's mixed initiative 
and automatic adaptation capabilities. Our results how 
that, when analyzed along the performance dimension, 
MIMIC's mixed initiative and automatic adaptation fea- 
tures lead to more efficient dialogues and higher user sat- 
isfaction. Moreover, when analyzed along the discourse 
and initiative dimensions, MIMIC's adaptation capabil- 
ities result in dialogues in which system behavior better 
matches user expectations and dialogue anomalies are re- 
solved more efficiently. 
2 M IMIC :  An  Adaptive Mixed Initiative 
Dialogue System 
MIMIC is a voice-enabled, telephone-based dialogue 
system that provides movie showtime information. Two 
main features distinguish MIMIC from other existing 
spoken dialogue systems. First, MIMIC automatically 
adapts dialogue strategies based on dialogue character- 
istics and thus tailors its behavior to the dialogue con- 
text. Second, MIMIC's initiative module is decoupled 
from the dialogue manager's goal and strategy selection 
processes, thereby enabling the system's adaptation be- 
havior to be easily modified. We outline characteristics 
of MIMIC relevant to our experiments and analyses. For 
further details about he system, see (Chu-Carroll, 2000). 
2.1 Adaptation of Dialogue Strategies 
Figure 1 showed an actual interaction between MIMIC 
and a user. The task and dialogue initiative holders 
annotated for each system turn are the factors that af- 
fect MIMIC's dialogue strategy adaptation. Following 
(Chu-Carroll and Brown, 1998), task initiative tracks the 
202 
Utterance 
(1) S: Hello, this is MIMIC, the movie information system. 
(2) How can I help you? 
(3) U: What time is Analyze This playing in Montclair? 
\[what time is analyze this playing in montclair\] 
(4) S: What theater would you like? User 
(5) U: Montclair. 
\[montclair\] 
(6) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System 
(7) What theater would you like? 
(8) U: Clearviews Screening Zone. 
lure movie screening zone\] 
(9) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pro, System 
and 6:30pm. 
(10) Please say the name of the movie or theater or town you would like information about. 
(11) U: What time is Analyze This playing at the Wellmont Theater? 
\[what time is analyze this playing at the wellmont theater\] 
(12) S: Analyze this is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, User 
and 10:10pm. 
(13) Can I help you with anything else? 
Task Dialogue 
Initiative Initiative 
User System 
System 
System 
System 
System 
Figure 1: An Adaptive Mixed Initiative Dialogue with MIMIC 
lead in the process toward achieving the dialogue partici- 
pants' domain goal, while dialogue initiative models the 
lead in determining the current discourse focus. In our 
information query application domain, MIMIC has task 
(and thus dialogue) initiative when its utterances are in- 
tended to provide helpful guidance toward achieving the 
user's domain goal, while it has dialogue but not task 
initiative if its utterances only specify the current dis- 
course goal. 2 For example, as a result of MIMIC taking 
over task initiative in (6), helpful guidance, in the form 
of valid response choices, was provided in its attempt 
to obtain a theater name after the user failed to answer 
an earlier question intended to solicit this information. 
In (4), MIMIC specified the current discourse goal (re- 
questing information about a missing theater) but did not 
suggest valid response choices ince it only had dialogue 
initiative. 
MIMIC's ability to automatically adapt dialogue 
strategies i achieved by employing an initiative mod- 
ule that determines initiative distribution based on par- 
ticipant roles, cues detected uring the current user ut- 
terance, and dialogue history (Chu-Carroll and Brown, 
1998). This initiative framework utilizes the Dempster- 
Shafer theory (Shafer, 1976; Gordon and Shortliffe, 
1984), and represents he current initiative distribution as 
two basic probability assignments (bpas) that signify the 
overall amount of evidence supporting each agent hav- 
ing task and dialogue initiatives. The effects that a cue 
has on changing the current ask and dialogue initiative 
distribution are also represented asbpas, obtained using 
an iterative training procedure on a corpus of transcribed 
21n the dialogues collected inour experiments, which are described 
in Section 3, there are system turns in which MIMIC had neither task 
nor dialogue initiative. However, such cases are rare in this domain and 
will not be discussed in this paper. 
and annotated human-human dialogues. At the end of 
each user turn, the bpas representing the effects of cues 
detected uring that turn are combined with the bpas rep- 
resenting the current initiative distribution to obtain the 
initiative distribution for the system's next turn. 
In Figure 1, utterance (3) triggered the cue Ambigu- 
ousAction since the town of Montclair has multiple the- 
aters. Although AmbiguousAction contributed to the sys- 
tem having both initiatives in the next turn, the effect of 
this cue, when combined with the initiative distribution 
of (3), resulted in MIMIC having dialogue but not task 
initiative in (4). However, in addition to triggering Am- 
biguousAction, utterance (5) also triggered NoNewlnfo, 
since it did not contribute any new information. The ef- 
fect of these two cues, combined with the initiative dis- 
tribution in (5), was enough to lead MIMIC to take over 
task initiative in (6). 
2.2 Modification of Adaptation Behavior 
MIMIC employs an initiative module which is decou- 
pied from its goal selection process, while allowing the 
outcome of both processes to guide its response strat- 
egy selection. A major advantage of this design is that 
changes in the system's trategy selection behavior, and 
thus its adaptation behavior, can be achieved by modi- 
fying parameters in the initiative module, while leaving 
the goal and strategy selection processes intact. The ini- 
tiative module determines initiative distribution based on 
participant roles, cues observed in the current user ut- 
terance, and dialogue history. Although all three factors 
may play a role in modifying MIMIC's adaptation be- 
havior, we discuss how the effects of participant roles 
and dialogue history are modified to create systems used 
as bases of comparison i our experiments. 
Participant roles affect the default initiative distribu- 
203 
Utterance 
(14) S: Hello, this is MIMIC, the movie information system. 
(15) Please say the name of the movie or theater or town you would like information about. 
(16) U: Analyze This. 
\[analyze this1 
(17) S: Choices for task are when and where. System 
(18) What would you like to know about Analyze This? 
(19) U: When. 
\[when is\] 
(20) S: Choices for theater are Middlebrook Galeria, Loews East Hanover, Montgomery Cinemas, System 
Atlantic Triplex Cinema, and others. 
(21) What theater would you like? 
(22) U: Montclair. 
\[montclair\] 
(23) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System 
(24) What theater would you like? 
(25) U: Wellmont. 
\[wellmontl 
(26) S: Analyze This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, System 
and 10:10pm. 
(27) Please say the name of the movie or theater or town you would like information about. 
(28) U: Analyze This in Montclair. 
\[analyze this in montclair\] 
(29) S: Choices for task are location, what, where, and when. System 
(30) What would you like to know about Montclair? 
(31) U: When. 
(32) \[when\] 
(33) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone System 
(34) What theater would you like? 
(35) U: Clearviews Screening Zone. 
\[screening zone\] 
(36) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, System 
and 6:30pm. 
(37) Please say the name of the movie or theater or town you would like information about. 
Figure 2: A System Initiative Dialogue with MIMIC-SI 
Task Dialogue 
Initiative Initiative 
System System 
System 
System 
System 
System 
System 
System 
Sys~m 
tion for MIMIC's initial utterance. For naive users, it 
may be more appropriate for MIMIC to employ system- 
initiative strategies. This system, MIMIC-SI, can be 
achieved by setting the default initiative distribution to 
provide definite evidence for the system having both task 
and dialogue initiatives) Figure 2 shows a dialogue 
in which a user interacts with MIMIC-SI to obtain the 
same information as in Figure 1. MIMIC-SI prompts the 
user for one piece of information at a time and provides 
(sometimes unnecessary) assistance, in the form of valid 
dialogue moves, during each turn. 
Taking into account dialogue history allows MIMIC 
to determine the initiative distribution based on the cu- 
mulative effect of previously observed cues. This ef- 
fect can be disabled so that only local dialogue con- 
text affects the initiative distribution for the next turn. 
Based on MIMIC's parameter settings, this adjustment 
results in MIMIC-MI, a system which always has dia- 
3Based on the Dempster-Shafer theory, if the bpas of the default 
initiative distribution or those of a detected cue provide definite vi- 
dence for drawing acertain conclusion, then no subsequent cues have 
any effect on changing that conclusion. 
logue initiative but never has task initiative. MIMIC-MI 
behaves imilarly to many existing non-adaptive mixed 
initiative dialogue systems (e.g., (Bennacef et al, 1996; 
Papineni et' al., 1999)). Figure 3 shows a dialogue in- 
volving MIMIC-MI in which the user again attempts to 
acquire the same information as in the previous two dia- 
logues. Notice that (41)-(49) constitute a failed attempt 
to solicit a theater name from the user. Instead of provid- 
ing helpful information as in (6) in Figure 1, MIMIC-MI 
relied on the user to change her problem-solving strategy 
in (50) to obtain the information eeded to answer the 
system's question. 
3 Experimental  Design 
Our main goal in evaluating MIMIC is to determine 
whether users find the mixed initiative and automatic 
adaptation aspects of its dialogue strategies useful. We 
compared MIMIC to two control systems: MIMIC-SI 
and MIMIC-MI, since they employ dialogue manage- 
ment strategies imilar to those in many existing sys- 
tems. The comparison between MIMIC and MIMIC-SI 
204 
Utterance 
(38) S: 
(39) 
(40) U: 
(41) S: 
(42) U: 
(43) S: 
(44) U: 
(45) S: 
(46) U: 
(47) S: 
(48) U: 
(49) S: 
(50) U: 
(51) S: 
(52) U: 
(53) S: 
(54) 
(55) 
(56) 
(57) 
U: 
S: 
Hello, this is MIMIC, the movie information system. 
How can I help you? 
When is Analyze This playing in Montclair? 
\[when analyze this playing in montclair\] 
What theater would you like? 
I1 
What theater would you like? 
Um. 
lure\] 
What theater would you like? 
Montclair. 
\[montclairl 
What theater would you like? 
Any. 
\[any\] 
What theater would you like? 
Where in Montclair is Analyze This playing? 
\[where in montclair is analyze this playing\] 
Analyze This is playing at Wellmont Theater and Clearviews Screening Zone in Montclair. 
When is Analyze This playing at Wellmont Theater? 
\[when is analyze this playing at wellmont theater in\] 
Analyze This is playing at Wellmont Theater in Montclair at 3:15pro, 5:45pm, 8:20pro, 
and 10:10pro. 
Can I help you with anything else? 
When is it playing at Clearviews Screening Zone? 
\[when is it playing at movie screening zone\] 
Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, 
and 6:30pm. 
Can I help you with anything else? 
Figure 3: A Non-Adaptive Mixed Initiative Dialogue with MIMIC-MI 
focused on the contribution of  mixed-initiative dialogue 
management, while the comparison between MIMIC and 
MIMIC-MI emphasized the contribution of  automatic 
strategy adaptation. The following three factors were 
controlled in our experiments: 
Town Theater 
(if playing) 
Hoboken 
Task Dialogue 
Initiative Initiative 
User System 
User System 
User System 
User System 
User System 
User System 
User System 
User System 
User System 
Movie Times after 5:10pm 
(if playing) 
Antz 
(a) Easy Task 
1. System version: For each experiment, wo systems 
were used: MIMIC and a control system. In the first 
experiment MIMIC was compared with MIMIC-SI, 
and in the second experiment, with MIMIC-MI. 
2. Order:  For each experiment, all subjects were ran- 
domly divided into two groups. One group per- 
formed tasks using MIMIC first, and the other group 
used the control system first. 
Town Theater Movie Two Times 
(if playing) (if playing) 
Millbum Analyze This 
Berkeley Hgts 
Mountainside 
Analyze This 
Analyze This 
Madison True Crime 
Hoboken True Crime 
(b) Difficult Task 
3. Task difficulty: 3-4 tasks which highlighted iffer- 
ences between systems were used for each experi- 
ment. Based on the amount of information to be ac- 
quired, we divided the tasks into two groups: easy 
and difficult; an example of each is shown in Fig- 
ure 4. 
Figure 4: Sample Tasks for Evaluation Experiments 
Eight subjects 4 participated in each experiment. Each 
of the subjects interacted with both systems to perform 
4The subjects were Bell Labs researchers, summer students, and 
their friends. Most of them are computer scientists, electrical engi- 
205 
all tasks. The subjects completed one task per call so 
that the dialogue history for one task did not affect the 
next task. Once they had completed all tasks in sequence 
using one system, they filled out a questionnaire to as- 
sess user satisfaction by rating 8-9 statements, imilar 
to those in (Walker et al, 1997), on a scale of  1-5, where 
5 indicated highest satisfaction. Approximately two days 
later, they attempted the same tasks using the other sys- 
tem. 5 These experiments resulted in 112 dialogues with 
approximately 2,800 dialogue turns. 
In addition to user satisfaction ratings, we automat- 
ically logged, derived, and manually annotated a num- 
ber of  features (shown in boldface below). For each 
task/subject/system triplet, we computed the task suc- 
cess rate based on the percentage of slots correctly filled 
in on the task worksheet, and counted the # of calls 
needed to complete ach task. 6 For each call, the user- 
side of the dialogue was recorded, and the elapsed time 
of the call was automatically computed. All user ut- 
terances were logged as recognized by our automatic 
speech recognizer (ASR) and manually transcribed from 
the recordings. We computed the ASR word  er ror  rate,  
ASR reject ion rate, and ASR t imeout rate,  as well as 
# of user turns and average sentence length for each 
task/subject/system triplet. Additionally, we recorded 
the cues that the system automatically detected from 
each user utterance. All system utterances were also 
logged, along with the init iative d istr ibut ion for each 
system turn and the dialogue acts selected to generate 
each system response. 
4 Results and Discussion 
Based on the features described above, we com- 
pared MIMIC and the control systems, MIMIC-SI  and 
MIMIC-MI,  along three dimensions: performance fea- 
tures, in which comparisons were made using previously 
proposed features relevant o system performance (e.g., 
(Price et al, 1992; Simpson and Fraser, 1993; Danieli 
and Gerbino, 1995; Walker et al, 1997)); discourse fea- 
tures, in which comparisons were made using character- 
istics of the resulting dialogues; and initiative distribu- 
tion, where initiative characteristics of all dialogues in- 
volving MIMIC from both experiments were examined. 
4.1 Performance Features 
For our performance evaluation, we first applied a three- 
way analysis of  variance (ANOVA) (Cohen, 1995) to 
each feature using three factors: system version, order, 
neers, or linguists, and none had prior knowledge of MIMIC. 
SWe used the exact same set of tasks rather than designing tasks of 
similar difficulty levels because we intended to compare all available 
features between the two system versions, including ASR word error 
rate, which would have been affected by the choice of movie/theater 
names in the tasks. 
6Although the vast majority of tasks were completed in one call, 
some subjects, when unable to make progress, did not change strategies 
as in (41)-(49) in Figure 3; instead, they hung up and started the task 
over .  
Performance Feature MIMIC 
# of user turns 10.3 
Elapsed time (see.) 229.5 
ASR timeout (%) 12.5 
User satisfaction (n=8) 21.9 
ASR rejection (%) 514 
Task success (%) 100 
# of calls I 1.0 
28.1 ASR word error (%) 
Sl 
13.6 
277.5 
6.9 
19.8 
8.1 
98.8 
1.1 
31.1 
(a) MIMIC vs. MIMIC-SI (n=32) 
P 
0.0075 
0.0162 
0.0239 
0.0447 
0.1911 
0.3251 
0.572 
0.8475 
Performance Feature MIMIC 
ASR timeout (%) 5.7 
# of user turns 10.3 
User satisfaction (n=8) 29.5 
Elapsed time (see.) 200.6 
ASR word error (%) 23.0 
Task success (%) 100 
# of calls 1.21 
8.4 ASR rejection (%) 
MI p 
15.6 0.001 
14.3 0.0199 
24.4 0.0364 
246.4 0.0457 
30.6 0.0588 
98.4 0.1639 
1.21 0.5 
7.7 0.8271 
(b) MIMIC vs. MIMIC-MI (n=24) 
Table 1: Comparison of Performance Features 
and task difficulty. 7 If no interaction effects emerged, we 
compared system versions using paired sample t-tests. 8 
Following the PARADISE evaluation scheme (Walker 
et al, 1997), we divided performance f atures into four 
groups: 
? Task success: task success rate, # of calls. 
? Dialogue quality: ASR rejection rate, ASR timeout 
rate, ASR word error rate. 
? Dialogue efficiency: # of user turns, elapsed time. 
? System usability: user satisfaction. 
For both experiments, the ANOVAs showed no inter- 
action effects among the controlled factors. Tables l(a) 
and l(b) summarize the results of the paired sample t- 
tests based on performance f atures, where features that 
differed significantly between systems are shown in ital- 
ics. 9 These results how that, when compared with either 
7User satisfaction was a per subject as opposed to a per task per- 
formance feature; thus, we performed a two-way ANOVA using the 
factors ystem version and order. 
8This paper focuses on evaluating the effect of MIMIC's mixed ini- 
tiative and automatic adaptation capabilities. We assess these ffects 
based on comparisons between system version when no interaction ef- 
fects emerged from the ANOVA tests using the factors ystem version, 
order, and task difficulty. Effects based on system order and task diffi- 
culty alone are beyond the scope of this paper. 
9Typically p<0.05 is considered statistically significant (Cohen, 
1995). 
206 
control system, users were more satisfied with MIMIC t? 
and that MIMIC helped users complete tasks more effi- 
ciently. Users were able to complete tasks in fewer turns 
and in a more timely manner using MIMIC. 
When comparing MIMIC and MIMIC-MI, dialogues 
involving MIMIC had a lower timeout rate. When 
MIMIC detected cues signaling anomalies in the dia- 
logue, it adapted strategies to provide assistance, which 
in addition to leading to fewer timeouts, saved users time 
and effort when they did not know what to say. In con- 
trast, users interacting with MIMIC-MI had to iteratively 
reformulate questions until they obtained the desired in- 
formation from the system, leading to more timeouts 
(see (41)-(49) in Figure 3). However, when comparing 
MIMIC and MIMIC-SI, even though users accomplished 
tasks more efficiently with MIMIC, the resulting dia- 
logues contained more timeouts. As opposed to MIMIC- 
SI, which always prompted users for one piece of infor- 
mation at a time, MIMIC typically provided more open- 
ended prompts when the user had task initiative. Even 
though this required more effort on the user's part in for- 
mulating utterances and led to more timeouts, MIMIC 
quickly adapted strategies to assist users when recog- 
nized cues indicated that they were having trouble. 
To sum up, our experiments show that both MIMIC's 
mixed initiative and automatic adaptation aspects re- 
sulted in better performance along the dialogue efficiency 
and system usability dimensions. Moreover, its adap- 
tation capabilities contributed to better performance in 
terms of dialogue quality. MIMIC, however, did not con- 
tribute to higher performance in the task success dimen- 
sion. In our movie information domain, the tasks were 
sufficiently simple; thus, all but one user in each experi- 
ment achieved a 100% task success rate. 
4.2 Discourse Features 
Our second evaluation dimension concerns characteris- 
tics of resulting dialogues. We analyzed features of  user 
utterances in terms of utterance l ngth and cues observed 
and features of system utterances in terms of dialogue 
acts. For each feature, we again applied a three-way 
ANOVA test, and if no interaction effects emerged, we 
performed a paired sample t-test to compare system ver- 
sions. 
The cues detected in user utterances provide insight 
into both user intentions and system capabilities. The 
cues that MIMIC automatically detects are a subset of  
those discussed in (Chu-Carroll and Brown, 1998): il 
? TakeOverTask: triggered when the user provides 
more information than expected; an implicit indi- 
cation that the user wants to take control of the 
l?The range of user satisfaction scores was 8-40 for experiment one 
and 9-45 for experiment two. 
l t A subset of these cues corresponds loosely to previously proposed 
evaluation metrics (e.g., (Danieli and Gerbino, 1995)). However, our 
system automatically detects hese features instead of requiring manual 
annotation by experts. 
Discourse Feature MIMIC 
Cue: TakeOverTask 1.84 
Cue: AmbiguousActResolved 1.69 
'Cue: AmbiguousAction 3 
Avg sentence l ngth (words) 6.82 
Cue: InvalidAction 1.16 
Cue: NoNewInfo 1.28 
Sl 
5 
4.59 
6.59 
5.45 
0.94 
1.38 
(a) MIMIC vs. MIMIC-SI (n=32) 
P 
0 
0 
0.0008 
0.0016 
0.1738 
0.766 
Discourse Feature MIMIC 
Cue: TakeOverTask 2.33 
Cue: InvalidAction 2.04 
Cue: NoNewlnfo 2.25 
Cue: AmbiguousActResolved 2.08 
Avg sentence length (words) 5.26 
Cue: AmbiguousAction 4.13 
MI 
0 
3.75 
4.79 
1.13 
5.63 
4.38 
(b) MIMIC vs. MIMIC-MI (n=24) 
P 
0 
0.0011 
0.0161 
0.0297 
0.1771 
0.8767 
Table 2: Comparison of User Utterance Features 
problem-solving process. 
? NoNewlnfo: triggered when the user is unable to 
make progress toward task completion, either when 
the user does not know what to say or the ASR en- 
gine fails to recognize the user's utterance. 
? lnvalidAction/InvalidActionResolved: triggered 
when the user utterance makes an invalid as- 
sumption about the domain and when the invalid 
assumption is corrected, respectively. 
? AmbiguousAction/AmbiguousActionResolved: trig- 
gered when the user query is ambiguous and when 
the ambiguity is resolved, respectively. 
Tables 2(a) and (b) summarize the results of the paired 
sample t-tests based on user utterance features where fea- 
tures whose numbers of occurrences were significantly 
different according to system version used are shown in 
italics. 12 Table 2(a) shows that users expected the system 
to adapt its strategies when they attempted to take control 
of the dialogue. Even though MIMIC-SI did not behave 
as expected, the users continued their attempts, resulting 
in significantly more occurrences of  TakeOverTask in di- 
alogues with MIMIC-SI than with MIMIC. Furthermore, 
the average sentence length in dialogues with MIMIC 
was only 1.5 words per turn longer than in dialogues 
with MIMIC-SI, providing further evidence that users 
~2Since system dialogue acts are often selected based on cues de- 
tected in user utterances, we only discuss results of our user utterance 
feature analysis, using dialogue act analysis results as additional sup- 
port for our conclusions. 
207 
preferred to provide free-formed queries, regardless of 
system version used. 
Table 2(b) shows that MIMIC was more effec- 
tive at resolving dialogue anomalies than MIMIC-MI. 
More specifically, there were significantly fewer oc- 
currences of NoNewlnfo in dialogues with MIMIC 
than with MIMIC-MI. In addition, while the number 
of occurrences of AmbiguousAction was not signifi- 
cantly different for the two systems, the number that 
were resolved (AmbiguousActionResolved) was signif- 
icantly higher in interactions with MIMIC than with 
MIMIC-MI. Since NoNewlnfo and AmbiguousAction 
both prompted MIMIC to adapt strategies and, as a re- 
suit, provide additional useful information, the user was 
able to quickly resolve the problem at hand. This is fur- 
ther supported by the higher frequency of the system dia- 
logue act GiveOptions in MIMIC (p=0), which provides 
helpful information based on dialogue context. 
In sum, the results of our discourse feature analysis 
further confirm the usefulness of MIMIC's adaptation 
capabilities. Comparisons with MIMIC-SI provide ev- 
idence that MIMIC's ability to give up initiative better 
matched user expectations. Moreover, comparisons with 
MIMIC-MI show that MIMIC's ability to opportunisti- 
cally take over initiative resulted in dialogues in which 
anomalies were more efficiently resolved and progress 
toward task completion was more consistently made. 
4.3 Initiative Analysis 
Our final analysis concerns the task initiative distri- 
bution in our adaptive system in relation to the fea- 
tures previously discussed. For each dialogue involving 
MIMIC, we computed the percentage of turns in which 
MIMIC had task initiative and the correlation coefficient 
(r) between the initiative percentage and each perfor- 
mance/discourse feature. To determine if this correlation 
was significant, we performed Fisher' s r to z transform, 
upon which a conventional Z test was performed (Cohen, 
1995). 
Tables 3(a) and (b) summarize the correlation between 
the performance and discourse features and the percent- 
age of turns in which MIMIC has task initiative, respec- 
tively. 13 Again, those correlations which are statistically 
significant are shown in italics. Table 3(a) shows a strong 
positive correlation between task initiative distribution 
and the number of user turns as well as the elapsed time 
of the dialogues. Although earlier results (Table l(a)) 
show that dialogues in which the system always had task 
initiative tended to be longer, we believe that this corre- 
lation also suggests that MIMIC took over task initiative 
more often in longer dialogues, those in which the user 
was more likely to be having difficulty. Table 3(a) fur- 
ther shows moderate correlation between task initiative 
distribution and ASR rejection rate as well as ASR word 
error rate. It is possible that such a correlation exists 
13This test was not performed for user satisfaction, since user saris- 
faction was a per subject and not a per dialogue f ature. 
Performance Feature r p 
# of user turns 0,71 0 
ASR rejection 0.55 0 
Elapsed time 0.51 0.00002 
ASR word error 0.46 0.00012 
~# of calls 0.15 0.1352 
! ASR timeout -0.003 0.4911 
Task success rate 0 0.5 
(a) Performance F atures 
Discourse Feature r p 
Cue: AmbiguousActionResolved 0.61 0 
Cue: NoNewlnfo 0.59 0 
Cue: TakeOverTask 0.44 0.00028 
Cue: lnvalidAction 0.42 0.00057 
Average sentence l ngth -0.40 0.00099 
Cue: AmbiguousAction 0.38 0.00169 
(b) Discourse Features 
Table 3: Correlation Between Task Initiative Distribution 
and Features (n=56) 
because ASR performance worsens when MIMIC takes 
over task initiative. However, in that case, we would have 
expected the results in Section 4.1 to show that the ASR 
rejection and word error rates for MIMIC-SI are signif- 
icantly greater than those for MIMIC, which are in turn 
significantly greater than those for MIMIC-MI, since in 
MIMIC-SI the system always had task initiative and in 
MIMIC-MI the system never took over task initiative. 
To the contrary, Tables l(a) and l(b) showed that the 
differences in ASR rejection rate and ASR word error 
rate were not significant between system versions, and 
Table l(b) showed that ASR word error rate for MIMIC- 
MI was in fact quite substantially higher than that for 
MIMIC. This suggests that the causal relationship is the 
other way around, i.e., MIMIC's adaptation capabilities 
allowed it to opportunistically take over task initiative 
when ASR performance was poor. 
Table 3(b) shows that all cues are positively correlated 
with task initiative distribution. For AmbiguousAction, 
lnvalidAction, and NoNewlnfo, this correlation exists be- 
cause observation of these cues contributed to MIMIC 
having task initiative. However, note that AmbiguousAc- 
tionResolved has a stronger positive correlation with task 
initiative distribution than does AmbiguousAction, again 
indicating that MIMIC's adaptive strategies contributed 
to more efficient resolution of ambiguous actions. 
In brief, our initiative analysis lends additional sup- 
port to the conclusions drawn in our performance and 
discourse feature analyses and provides new evidence 
for the advantages of MIMIC's adaptation capabilities. 
208 
In addition to taking over task initiative when previously 
identified ialogue anomalies were encountered (e.g., de- 
tection of ambiguous or invalid actions), our analysis 
shows that MIMIC took over task initiative when ASR 
performance was poor, allowing the system to better con- 
strain user utterances, t4 
5 Conclusions 
This paper described an empirical evaluation of MIMIC, 
an adaptive mixed initiative spoken dialogue system. We 
conducted two experiments hat focused on evaluating 
the mixed initiative and automatic adaptation aspects of 
MIMIC and analyzed the results along three dimensions: 
performance f atures, discourse features, and initiative 
distribution. Our results showed that both the mixed 
initiative and automatic adaptation aspects of the sys- 
tem led to better performance in terms of user satisfac- 
tion and dialogue fficiency. In addition, we found that 
MIMIC's adaptation behavior better matched user expec- 
tations, more efficiently resolved anomalies in dialogues, 
and led to higher overall dialogue quality. 
Acknowledgments 
We would like to thank Bob Carpenter and Christine 
Nakatani for their help on experimental design, Jan van 
Santen for discussion on statistical analysis, and Bob 
Carpenter for his comments on an earlier draft of this pa- 
per. Support for the second author is provided by an NSF 
graduate fellowship and a Lucent Technologies GRPW 
grant. 
References 
James F. Allen, Bradford W. Miller, Eric K. Ringger, 
and Teresa Sikorski. 1996. A robust system for nat- 
ural spoken dialogue. In Proceedings of the 34th An- 
nual Meeting of the Association for Computational 
Linguistics, pages 62-70. 
S. Bennacef, L. Devillers, S. Rosset, and L. Lamel. 
1996. Dialog in the RAILTEL telephone-based sys- 
tem. In Proceedings of the 4th International Confer- 
ence on Spoken Language Processing. 
Jennifer Chu-Carroll and Michael K. Brown. 1998. An 
evidential model for tracking initiative in collabora- 
tive dialogue interactions. User Modeling and User- 
Adapted Interaction, 8(3-4):215-253. 
Jennifer Chu-Carroll. 2000. MIMIC: An adaptive mixed 
initiative spoken dialogue system for information 
queries. In Proceedings of the 6th ACL Conference on 
Applied Natural Language Processing. To appear. 
Paul R. Cohen. 1995. Empirical Methods for Artificial 
Intelligence. MIT Press. 
Morena Danieli and Elisabetta Gerbino. 1995. Metrics 
for evaluating dialogue strategies ina spoken language 
laAlthough not currently utilized, the ability to adapt dialogue strate- 
gies when ASR performance is poor enables the system to employ dia- 
logue strategy specific language models for ASR. 
system. In Proceedings of the AAAI Spring Sympo- 
sium on Empirical Methods in Discourse Interpreta- 
tion and Generation, pages 34-39. 
Jean Gordon and Edward H. Shortliffe. 1984. The 
Dempster-Shafer theory of evidence. In Bruce 
Buchanan and Edward Shortliffe, editors, Rule-Based 
Expert Systems: The MYCIN Experiments of the 
Stanford Heuristic Programming Project, chapter 13, 
pages 272-292. Addison-Wesley. 
Diane J. Litman and Shimei Pan. 1999. Empirically 
evaluating an adaptable spoken dialogue system. In 
Proceedings of the 7th International Conference on 
User Modeling, pages 55-64. 
H. Meng, S. Busayaponchai, J. Glass, D. Goddeau, 
L. Hetherington, E. Hurley, C. Pao, J. Polifroni, 
S. Seneff, and V. Zue. 1996. WHEELS: A conversa- 
tional system in the automobile classifieds domain. In 
Proceedings of the International Conference on Spo- 
ken Language Processing, pages 542-545. 
Stefan Ortmanns, Wolfgang Reichl, and Wu Chou. 1999. 
An efficient decoding method for real time speech 
recognition. In Proceedings of the 5th European Con- 
ference on Speech Communication and Technology. 
K.A. Papineni, S. Roukos, and R.T. Ward. 1999. Free- 
flow dialog management using forms. In Proceedings 
of the 6th European Conference on Speech Communi- 
cation and Technology, pages 1411-1414. 
Patti Price, Lynette Hirschman, Elizabeth Shriberg, and 
Elizabeth Wade. 1992. Subject-based valuation mea- 
sures for interactive spoken language systems. In Pro- 
ceedings of the DARPA Speech and Natural Language 
Workshop, pages 34-39. 
Wolfgang Reichl and Wu Chou. 1998. Decision tree 
state tying based on segmental c ustering for acoustic 
modeling. In Proceedings of the International Confer- 
ence on Acoustics, Speech, and Signal Processing. 
M.D. Sadek, A. Ferrieux, A. Cozannet, P. Bretier, 
E Panaget, and J. Simonin. 1996. Effective human- 
computer cooperative spoken dialogue: The AGS 
demonstrator. In Proceedings of the International 
Conference on Spoken Language Processing. 
Glenn Shafer. 1976. A Mathematical Theory of Evi- 
dence. Princeton University Press. 
Andrew Simpson and Norman M. Fraser. 1993. Black 
box and glass box evaluation of the SUNDIAL system. 
In Proceedings of the 3rd European Conference on 
Speech Communication a d Technology, pages 1423- 
1426. 
Gert Veldhuijzen van Zanten. 1999. User modelling in 
adaptive dialogue management. In Proceedings of the 
6th European Conference on Speech Communication 
and Technology, pages 1183-1186. 
Marilyn A. Walker, Diane J. Litman, Candance A. 
Kamm, and Alicia Abella. 1997. PARADISE: A 
framework for evaluating spoken dialogue agents. In 
Proceedings of the 35th Annual Meeting of the Associ- 
ation for Computational Linguistics, pages 271-280. 
209 
Using Dialogue Representations for Concept-to-Speech 
Generation 
Christ ine H. Nakatani 
Jennifer Chu-Carroll  
Abstract 
We present an implemented concept-to-speech 
(CTS) syst@n'~J tl~at offers original proposals for 
certain couplings-oi r dialogue computation with 
prosodic computation. Specifically, the semantic in- 
terpretation, task modeling and dialogue strategy 
modules in a working spoken dialogue system are 
used to generate prosodic features to better convey 
the meaning of system replies. The new CTS system 
embodies and extends theoretical work on intona- 
tional meaning in a more general, robust and rigor- 
ous way than earlier approaches, by reflecting com- 
positional aspects of both dialogue and intonation 
interepretation i  an original computational frame- 
work for prosodic generation. 
1 In t roduct ion  
Conversational systems that use speech as the input 
and output modality are often realized by architec- 
tures that decouple speech processing components 
from language processing components. In this pa- 
per, we show how speech generation can be more 
closely coupled with the dialogue manager of a work- 
ing mixed-initiative spoken dialogue system. In par- 
ticular, we use representations from the semantic in- 
terpretation, task model and dialogue strategy mod- 
ules to better communicate the meaning of system 
replies through prosodically appropriate synthetic 
speech. 
While dialogue prosody has been a topic of much 
study, our implemented concept-to-speech (CTS) 
system offers original proposals for specific couplings 
of dialogue computation with prosodic omputation. 
Further, it embodies and extends theoretical work 
on intonational meaning in a more general, robust 
and rigorous way than earlier CTS systems, in an 
architecture that reflects compositional aspects of 
dialogue and intonation interpretation. 
2 Theoretical Foundations 
In this work, we implement and extend the com- 
positional theory of intonational meaning proposed 
by Pierrehumbert and Hirschberg (1986; 1990), 
who sought o identify correspondences between the 
Bell Laboratories, Lucent Technologies 
600 Mountain Avenue 
Murray Hill, NJ 07974 USA 
{chn I j encc}?research, bell-labs, com 
Grosz and Sidner (1986) computational model of dis- 
course interpretation and Pierrehumbert's prosodic 
grammar for American English (1980). 
In the present work, certain aspects of the orig- 
inal theories are modified and adapted to the ar- 
chitecture of the dialogue system in which the CTS 
component is embedded. Below, we present he im- 
portant fundamental definitions and principles of in- 
tonation underlying our CTS system. 
2.1 Intonat ional  System 
In our CTS system, the prosodic elements that are 
computed are based on the intonational system of 
Pierrehumbert (1980), who defined a formal lan- 
guage for describing American English intonation 
using the following regular grammar: 
Inton Phrase ---~ (Interm Phrase) + Bndry Tone 
Interm Phrase ~ (Pitch Acc)+ Phrase Ace 
Major phrases, or inlonational phrases, are made 
up of one or more minor phrases, or inlermediale 
phrases. Melodic movements in intermediate and 
intonational phrases are in turn expressed by three 
kinds of tonal elements. These include six pilch ac- 
cents: a low pitch excursion (L*), a high pitch excur- 
sion (H*), or a combination of both low and high ex- 
cursions (L*+H, L+H*, H*+L, It+L*); two phrase 
accents: a high (H-) or low (L-) tonal target that 
guides the interpolation of the melodic ontour from 
final pitch accent o intermediate phrase nding; and 
two boundary lones: a high (H%) or low (L%) tonal 
target that guides interpolation from phrase accent 
to intonational phrase ending. 
2.2 Intonat ional  Meaning 
Theoretical work on intonational meaning has at- 
tempted to relate the grammatical e ements of Pier- 
rehumbert's system - -  pitch accent, phrase accent 
and boundary tone, to interpretive processes at dif- 
ferent levels of discourse and dialogue structure. 
Hirschberg and Pierrehumbert (1986) conjectured 
that the absence or presence of accentuation conveys 
discourse focus status, while the tonal properties of 
the accent itself (i.e. pitch accent ype) convey se- 
mantic focus information. 
48 
MIMIC: 
User: 
MIMIC: 
User: 
MIMIC: 
hello this is mimic the movie information system 
how can I help you 
where in hoboken is october sky playing 
october sky is playing at hoboken cinema in hoboken 
can I help you with anything else 
when is it playing there 
october sky is playing at hoboken cinema in hoboken at 3:45pm, 5:50pm, 7:lOpm, and lOpm 
can i help you with anything else 
Figure 1: A MIMIC dialogue. 
In later work, pitch accent type was said to 
express whether the accented information was in- 
tended by the speaker to be "predicated" or not by 
the hearer (Pierrehumbert and Hirschberg, 1990). 
Nompredicated-~forernation was said to bear low- 
star accentuation (L*-, L*+H, H+L*), while predi- 
cated information would be marked by high-star ac- 
cents (H*, L+H*, H*+L). The theory further stated 
that L*+H conveys uncertainty or lack of speaker 
commitment to the expressed propositional content, 
while L+H* marks correction or contrast. The com- 
plex accent, H*+L, was said to convey that an infer- 
ence path was required to support he predication; 
usage of H+L* similarly was said to imply an in- 
ference path, but did not suggest a predication of a 
mutual belief. Finally, phrase accents and bound- 
ary tones were said to reflect aspects of discourse 
structure. 
3 Systems Foundations 
Our task is to improve the communicative compe- 
tence of a spoken dialogue agent, by making re- 
course to our knowledge of intonational meaning, di- 
alogue processing and relations between the two. Of 
course, a worthwhile CTS system must also outper- 
form out-of-the-box text-to-speech (TTS) systems 
that may determine prosodic mark-up in linguisti- 
cally sophisticated ways. As in (Nakatani, 1998), we 
take the prosodic output of an advanced research 
system that implements he Pierrehumbert theory 
of intonation, namely the Bell Labs TTS system, 
as our baseline xperimental system to be enhanced 
by CTS algorithms. We embed the CTS system in 
MIMIC, a working spoken dialogue system repre- 
senting state-of-the-art dialogue management prac- 
tices, to develop CTS algorithms that can be eventu- 
ally realistically evaluated using task-based perfor- 
mance metrics. 
3.1 Dialogue System: Mixed-Init iat ive 
Movie Information Consultant 
(MIMIC)  
The dialogue system whose baseline speech gen- 
eration capabilities we enhance is the Mixed- 
Initiative Movie Information Consultant (MIMIC) 
(Chu-Carroll, 2000). MIMIC" provides movie list- 
ing information involving knowledge about towns, 
theaters, movies and showtimes, as demonstrated 
in Figure 1. MIMIC currently utilizes template- 
driven text generation, and passes on text strings 
to a stand-alone TTS system. In the version of 
MIMIC enhanced with concept-to-speech capabili- 
ties, MIMIC-CTS, contextual knowledge is used to 
modify the prosodic features of the slot and filler 
material in the templates; we are currently integrat- 
ing the algorithms in MIMIC-CTS with a grammar- 
driven generation system. Further details of MIMIC 
are presented in the relevant sections below, but see 
(Chu-Carroll, 2000) for a complete overview. 
3.2 TTS: The Bell Labs System 
For default prosodic processing and speech synthe- 
sis realization, we use a research version of the 
Bell Labs TTS System, circa 1992 (Sproat, 1997), 
that generates intonational contours based on Pier- 
rehumbert's intonation theory (1980), as described 
in (Pierrehumbert, 1981). Of relevance is the fact 
that various pitch accent ypes, phrase accent and 
boundary tones in Pierrehumbert's theory are di- 
rectly implemented in this system, so that by gener- 
ating a Pierrehumbert-style prosodic transcription, 
the work of the CTS system is done. More pre- 
cisely, MIMIC-CTS computes prosodic annotations 
that override the default prosodic processing that is 
performed by the Bell Labs TTS system. 
To our knowledge, the intonation component of 
the Bell Labs TTS system utilizes more linguistic 
knowledge to compute prosodic annotations than 
any other unrestricted TTS system, so it is reason- 
able to assume that improvements upon it are mean- 
ingful in practice as well as in theory. 
4 MIMIC's Concept-to-Speech 
Component  (MIMIC-CTS) 
In MIMIC-CTS, the MIMIC dialogue system is en- 
hanced with a CTS component to better communi- 
cate the meaning of system replies through contex- 
tually conditioned prosodic features. MIMIC-CTS 
makes use of three distinct levels of dialogue rep- 
resentations to convey meaning through intonation. 
MIMIC's semantic representations allow MIMIC- 
CTS to decide which information to prosodically 
49 
highlight. MIMIC's task model in turn determines 
how to prosodically highlight selected information, 
based on the pragmatic properties of the system 
reply. MIMIC's dialogue strategy selection process 
informs various choices in prosodic contour and ac- 
centing that convey logico-semantic aspects of mean- 
ing, such as contradiction. 
4.1 Highl ighting Informat ion using 
Semantic Representat ions 
MIMIC employs a statistically-driven semantic in- 
terpretation engine to "spot" values for key at- 
tributes that make up a valid MIMIC query in a 
robust fashion) To simplify matters, for each ut- 
terance, MIMIC computes an attribute-value ma- 
trix (AVM)-~epresentation, identifying important 
pieces of information for accomplishing a given set 
of tasks. The AVM created from the following ut- 
terance, "When is October Sky playing at Hoboken 
Cinema in Hoboken?", for example, is given in Fig- 
ure 2. 
Attribute 11 Value 
Task 
Movie 
Theatre 
Town 
Time 
when 
October Sky 
Hoboken Cinema 
Hoboken 
Figure 2: Attribute Value Matrix (AVM), computed 
by MIMIC's semantic interpreter. 
Attribute names and attribute values are critical 
to the task at hand. In MIMIC-CTS, attribute 
names and values that occur in templates are typed, 
so that MIMIC-CTS can highlight hese items in 
the following way: 
1. All lexical items realizing attribute values are 
accented. 
2. Attribute values are synthesized at a slower 
speaking rate. 
3. Attribute values are set off by phrase bound- 
aries. 
4. Attribute names are always accented. 
These modifications are entirely rule-based, given a 
list of attribute names and typed attribute values. 
1 Specifically, MIMIC uses an n-dimensional call router 
front-end (Chu-Carroll, 2000), which is a generalization of 
the vector-based call-routing paradigm of semantic interpre- 
tation (Chu-CarroU and Carpenter, 1999); that is, instead of 
detecting one concept per utterance, MIMIC's semantic in- 
terpretation engine detects multiple (n) concepts or classes 
conveyed by a single utterance, by using n call touters in 
parallel. 
Even such minimal use of dialogue information 
can make a difference. For example, changing the 
default accent for the following utterance highlights 
the kind of information that the system is seeking, 
instead of highlighting the semantically vacuous 
main verb, like: 2 
Default TTS: what movie would you LIKE 
MIMIC-CTS: what MOVIE would you like 
4.2 Conveying In format ion Status using 
the Task Model 
MIMIC performs a set of information-giving tasks, 
i.e. what, where, when, location, that are concisely 
defined by a task model. MIMIC processes the 
AVM for each utterance and then evaluates whether 
it should perform a database query based on the 
task specifications given in Figure 3. The task 
mode\] defines which attribute values must be filled 
in (Y), must not 56 filled in (N), or may optionally 
be filled in (-), to "license" a database query action. 
If no task is "specified" by the current AVM state, 
Task Movie Theater Town 
Figure 3: Task Specifications for MIMIC. 
MIMIC employs various strategies to progress 
toward a complete and valid task specification. 
For example, in response to the follgwing user 
utterance, MIMIC initiates an information-seeking 
subdialogue to instantiate the theater attribute 
value to accomplish a when task: 
User: when is october sky playing 
in hoboken 
MIMIC-CTS: what THEATER would you like 
To better convey the structure of the task model, 
which is learned by the user through interaction 
with the system, we define four information statuses 
based on properties of the task model, which align 
on a scale of given and new in the following order: 
OLD INFERRABLE KEY HEARER-NEW 
\[given\] \[new\] 
KEY information is that which is necessary to 
formulate a valid database query, and is exchanged 
and (implicitly or explicitly) confirmed between 
the system and user. INFERRABLE information is
not explicitly exchanged between the system and 
2In the examples, small capitalization denotes a word is 
accented. 
50 
Task Specification Status 
Required (Y) 
Optional (-) 
Not allowed (N) 
Information Status Pitch Accent 
KEY L+H* 
INFERRABLE/OLD L*+H/L* 
HEARER-NEW H* 
Table 1: Highlighting relevance of information based on task model (and discourse history). 
User: 
MIMIC: 
where in montclair is analyze this playing 
ana lyze  this  is p lay ing  at we lhnont  theat re  and c learv iews sc reen ing  zone 
in mont  clair  
ANALYZE THIS is PLAYING at WELLIMONT THEATER 
LWH* LWH* L* - H* H* L-H% 
CLEARVIEWS SCREENING ZONE in MONTCLAIR 
~= - H* H* H* L-H% LWH* L-L% 
and 
Figure 4: Above, dialogue excerpt of MIMIC performing a where task. Below, the modified version of the 
bold-faced reply string, generated by MIMIC-CTS. 
user, but is derived by MIMIC's limited inference 
engine that seeks to instantiate as many attribute 
values as possible. For instance, a theater name 
may be inferred given a town name, if there is only 
one theater in the given town. OLD information 
is inherited from the discourse history, based on 
updating rules relying on confidence scores for 
attribute values. HEARER-NEW information (c.f. 
(Prince, 1988)) is that which is requested by the 
user, and constitutes the only new information on 
the scale. But note that KEY information, while 
given, is still clearly in discourse focus, along with 
HEARER-NEW information. 
The next step is to map the information statuses, 
ordered from given to new, to a scale of pitch 
accent, or accent melodies, ordered from given to 
new as follows: 
L* L*?H L+H* H* 
&ivan\] \[new\] 
Table 1 summarizes this original mapping of infor- 
mation statuses to pitch accent melodies, and Fig- 
ure 4 illustrates the use of this mapping in an ex- 
ample. It obeys the general principle of Pierrehum- 
bert and Hirschberg's work, that low tonality sig- 
nifies discourse givenness and high tonality signifies 
discourse newness, but extends this principle beyond 
its vague definition in terms of predication of mutual 
beliefs. Instead, the principle is operationalized here 
in a practically motivated manner that is consistent 
with and perhaps illuminating of the theory. 
4.3 Ass igning "D ia logue  Prosody"  us ing 
Dia logue St rateg ies  
As in earlier CTS systems, special logico-semantic 
relations, such as contrast or correction, are effec- 
tively conveyed in MIMIC-CTS by prosodic ues. In 
MIMIC-CTS, however, these situations are not stip- 
ulated in an ad hoc manner, but can be determined 
to a large degree by MIMIC's dialogue strategy se- 
lection process that identifies appropriate dialogue 
acts to realize a dialogue goal. a 
For example, the dialogue act ANSWER may be 
selected to achieve the dialogue goal of providing an 
answer to a successful user query, while the dialogue 
act NOTIFYFAILURE may be performed to achieve 
the dialogue goal of providing an answer in situations 
where no movie listing in the database matches the 
user query. The template associated with the di- 
alogue act, NOTIFYFAILURE, when compared with 
that for ANSWER, contains an additional negative 
auxiliary associated with the key attribute responsi- 
ble for the query failure, in an utterance conveying a
contradiction i beliefs between the user and system 
(namely, the presupposition on the part of the user 
that the query can be satisfied). 
Theoretical work on intonational interpretation 
leads us to prosodically mark the negative auxil- 
iary, as well as the associated focus position (Rooth, 
1985). We choose to mark the negative auxiliary not 
with the L+H* pitch accent to convey correction, 
while marking the material in the associated focus 
position with the L*+H pitch accent to convey (the 
3Importantly, MIMIC's adaptive dialogue strategy selec- 
tion algorithm takes into account the outcome of an initia- 
tive tracking module that we do not discuss here (see (Chu- 
Carroll, 2000)). 
51 
User: 
MIMIC: 
where is the corruptor playing in cranford 
the corruptor  is not playing in cranford 
the corruptor is playing at lincoln cinemas in arlington 
THE CORRUPTOR is NOT playing in CRANFORD 
L+H* L+H* LWH* !H* L*+H L-H% 
Figure 5: Above, dialogue xcerpt of MIMIC performing a NOTIFYFAILURE dialogue act. Below, the modified 
version of the bold-faced reply string, generated by MIMIC-CTS. Note the diacritic "!" denotes a downstepped 
accent (see (Pierrehumbert, 1980)). 
system's) lack of commitment to the (user's) pre- 
supposition at hand. Finally, the NOTIFYFAILURE 
dialogue act is conveyed by assigning the so-called 
rise-fall-risd-cbntfadiction c tour, L*+tt L-H%, to 
the utterance-at large (c.f. (Hirschberg and Ward, 
1991)). An example generated by MIMIC-CTS ap- 
pears in Figure 5. Note that pitch accent types for 
the remaining attribute values are assigned using the 
task model, as described in section 4.2. Thus in Fig- 
ure 5, the movie title is treated as KEY information, 
marked by the L+H* pitch accent. 
MIMIC-CTS contains additional prosodic rules 
for logical connectives, and clarification and confir- 
mation suhdialogues. 
5 Related Work 
Although a number of earlier CTS systems have 
captured linguistic phenomena that we address in 
our work, the computation ofprosody from dialogue 
representations is often not as rigorous, detailed or 
complete as in MIMIC-CTS. Further, while several 
systems use given/new information status to decide 
whether to accent or deaccent a lexical item, no sys- 
tem has directly implemented general rules for pitch 
accent type assignment. Together, MIMIC-CTS's 
computation of accentuation, pitch accent ype and 
dialogue prosody constitutes the most general and 
complete implementation f a compositional theory 
of intonational meaning in a CTS system to date. 
Nevertheless, elements of a handful of previ- 
ous CTS systems support the approaches taken 
in MIMIC-CTS toward conveying semantic, task 
and dialogue level meaning. For example, the Di- 
rection Assistant system (Davis and Hirschberg, 
1988) mapped a hand-crafted route grammar to a 
discourse structure for generated irections. The 
discourse structure determined accentuation, with 
deaccenting ofdiscourse-old entities realized (by lex- 
ically identical morphs) in the current or previous 
discourse segment. Other material was assigned ac- 
centuation based on lexical category information, 
with the exception that certain contrastive cases of 
accenting, such as left versus right, were stipulated 
for the domain. 
Accent assignment in the SUNDIAL travel infor- 
mation system (House and Yond, 1990) also relied 
on discourse and task models. Mutually known en- 
tities, said to be in negative focus, were deaccented; 
entities in the current task space, in referring focus, 
received (possibly contrastive) accenting; and enti- 
ties of the same type as a previously mentioned ob- 
ject, were classified-as in either referring or emphatic 
focus, depending on the dialogue act~ in the cases 
of corrective situations or repeated system-intitiated 
queries, the contrasting or corrective items were em- 
phatically accented. 
The BRIDGE project on speech generation 
(Zacharski etal., 1992) identified four main factors 
affecting accentability: linear 0rder, lexical category, 
semantic weight and givenness. In relatedwork 
(Monaghan, 1994), word accentability was quanti- 
tatively scored by hand-crafted rules based on infor- 
mation status, semantic focus and Word class. The 
givenness hierarchy of Gundel and'colleagues (1989), 
which associates lexical forms of expression with in- 
formation statuses, was divided into four intervals, 
with scores assigned to each. A binary semantic fo- 
cus score was based on whether the word occurred 
in the topic or comment of a sentence. Finally, lex- 
ical categories determined word class scores. These 
scores were combined, and metrical phonological 
rules then referred to final acce'ntability scores to 
assign a final accenting pattern. 
To summarize, all of the above CTS systems em- 
ploy either hand-crafted or heuristic techniques for 
representing semantic and discourse focus informa- 
tion. Further, only SUNDIAL makes use of dialogue 
acts. 
6 Conc lus ion  and Future  Work  
We are presently carrying out evaluations ofMIMIC- 
CTS. An initial corpus-based analysis compares 
the prosodic annotations assigned to three ac- 
tual MIMIC dialogues, which were previously col- 
lected during an overall system evaluation (Chu- 
Carroll and Nickerson, 2000). The corpus of di- 
alogues is made up of 37 system/user turns, in- 
cluding 40 system-generated sentences. Three ver- 
sions of the MIMIC dialogues are being analysed, 
with prosodic features arising from three differ- 
52 
ent sources: MIMIC-CTS, MIMIC operating with 
default Bell Labs TTS, and a professional voice 
talent who read the dialogue scripts in context. 
This corpus-based assessment - -  comparing the 
prosody of CTS-generated, TTS-generated, and hu- 
man speech, will enable more domain-dependent 
tuning of the MIMIC-CTS algorithms, as well as the 
refinement of general prosodic patterns for linguis- 
tic structures, uch as lists and conjunctive phrases. 
Ultimately; the value of MIMIC-CTS must be mea- 
sured based on its contribution to overall task pefor- 
mance by real MIMIC users. Such a study is under 
design, following (Chu-Carroll and Nickerson, 2000). 
In conclusion, we have shown how prosodic com- 
putation can be conditioned on various dialogue 
representations, for robust and domain-independent 
CTS synthesis. -While some rules for prosody as- 
signment depend on the task model, others must be 
tied closely to the particular choices of content in 
the replies, at the level of dialogue goals and dia- 
logue acts. At this level as well, however, linguis- 
tic principles of intonation interpretation can be ap- 
plied to determine the mappings. In sum, the lesson 
learned is that a unitary notion of "concept" from 
which we generate a unitary prosodic structure, does 
not apply to state-of-the-art spoken dialogue gener- 
ation. Instead, the representation f dialogue mean- 
ing in experimental rchitectures, such as MIMIC's, 
is compositional tosome degree, and we take advan- 
tage of this fact to implement a compositional theory 
of intonational meaning in a new concept-to-speech 
system, MIMIC-CTS. 
Re ferences  
Jennifer Chu-Carroll and Bob Carpenter. 1999. 
Vector:based natural language call routing. Com- 
putational Linguistics, 25(3):361-388. 
Jennifer Chu-Carroll and Jill S. Nickerson. 2000. 
Evaluating automatic dialogue strategy adapta- 
tion for a spoken dialogue system. In Proceed- 
ings of the 1st Conference of the North Ameri- 
can Chapter of the Association for Computational 
Linguistics, Seattle. 
Jennifer Chu-Carroll. 2000. Mimic: an adaptive 
mixed initiative spoken dialogue system for infor- 
mation queries. In Proceedings of the 6th Con- 
ference on Applied Natural Language Processing, 
Seattle. 
J. R. Davis and :l. Hirschberg. 1988. Assigning into- 
national features in synthesized spoken directions. 
In Proceedings of the 26th Annual Meeting of the 
Association for Computational Linguistics, pages 
187-193, Buffalo. 
Barbara Grosz and Candace Sidner. 1986. Atten- 
tion, intentions, and the structure of discourse. 
Computational Linguistics, 12(3):175-204. 
J. Gundel, N. Hedberg, and R. Zacharski. 1989. 
Givenness, implicature and demonstrative expres- 
sions in English discourse. In Proceedings ofCLS- 
25, Parasession on Language in Context, pages 
89-103. Chicago Linguistics Society. 
Julia Hirschberg and Janet Pierrehumbert. 1986. 
The intonational structuring of discourse. In Pro- 
ceedings of the 2~lh Annual Meeting of the Asso- 
ciation for Computational -Linguistics, New York. 
J. Hirschberg and G. Ward. 1991. The influence of 
pitch range, duration, amplitude, and spectral fea- 
tures on the interpretation f l*+h I h%. Journal 
of Phonetics. 
Jill House and Nick Youd. 1990. Contextually ap- 
propriate intonation in speech synthesis. In Pro- 
ceedings of the European Speech Communication 
Association Workshop on Speech Synthesis, pages 
185-188, Autrans. 
A. I. C. Monaghan. 1,994. Intonation accent place- 
ment in a concept-to-dialogue system. In Proceed- 
ings of the ESCA/IEEE Workshop on Speech Syn- 
thesis, pages 171-174, New Paltz, NY. 
C. H. Nakatani. 1998. Constituent-based accent 
prediction. In Proceedings of the 36th Annual 
Meeting of the Association for Computational 
Linguistics, Montreal. 
J. Pierrehumbert and J. Hirschberg. 1990. The 
meaning of intonational contours in the interpre- 
tation of discourse. In Intentions in Communica- 
tion. MIT Press, Cambridge, MA. 
Janet Pierrehumbert. 1980. The Phonology and 
Phonetics of English Intonation. Ph.D. thesis, 
Massachusetts Institute of Technology, Septem- 
ber. Distributed by the Indiana University Lin- 
guistics Club. 
J. Pierrehumbert. 1981. Synthesising intonation. 
Journal of the Acoustical Society of America, 
70(4):985-995. 
Ellen Prince. 1988. The ZPG letter: subjects, defi- 
niteness, and information status. In S. Thompson 
and W. Mann, editors, Discourse Description: Di- 
verse Analyses of a Fund Raising Text. Elsevier 
Science Publishers, Amsterdam. 
Mats Rooth. 1985. Association with Focus. Ph.D. 
thesis, University of Massachusetts, Amherst MA. 
Richard Sproat, editor. 1997. Multilingual Text- 
to-Speech Synthesis: The Bell Labs Approach. 
Kluwer Academic, Boston. 
Ron Zacharski, A. I. C. Monaghan, D. R. Ladd, 
and Judy Delin. 1992. BaIDGE: Basic research 
on intonation for dialogue generation. Technical 
report, University of Edinburgh. 
53 
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712?724,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Using Syntactic and Semantic Structural Kernels for
Classifying Definition Questions in Jeopardy!
Alessandro Moschitti? Jennifer Chu-Carroll? Siddharth Patwardhan?
James Fan? Giuseppe Riccardi?
?Department of Information Engineering and Computer Science
University of Trento, 38123 Povo (TN), Italy
{moschitti,riccardi}@disi.unitn.it
?IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598, U.S.A.
{jencc,siddharth,fanj}@us.ibm.com
Abstract
The last decade has seen many interesting ap-
plications of Question Answering (QA) tech-
nology. The Jeopardy! quiz show is certainly
one of the most fascinating, from the view-
points of both its broad domain and the com-
plexity of its language. In this paper, we study
kernel methods applied to syntactic/semantic
structures for accurate classification of Jeop-
ardy! definition questions. Our extensive em-
pirical analysis shows that our classification
models largely improve on classifiers based on
word-language models. Such classifiers are
also used in the state-of-the-art QA pipeline
constituting Watson, the IBM Jeopardy! sys-
tem. Our experiments measuring their impact
on Watson show enhancements in QA accu-
racy and a consequent increase in the amount
of money earned in game-based evaluation.
1 Introduction
Question Answering (QA) is an important research
area of Information Retrieval applications, which re-
quires the use of core NLP capabilities, such as syn-
tactic and semantic processing for a more effective
user experience. While the development of most
existing QA systems are driven by organized eval-
uation efforts such as TREC (Voorhees and Dang,
2006), CLEF (Giampiccolo et al, 2007), and NT-
CIR (Sasaki et al, 2007), there exist efforts that
leverage data from popular quiz shows, such as Who
Wants to be a Millionaire (Clarke et al, 2001; Lam
et al, 2003) and Jeopardy! (Ferrucci et al, 2010), to
demonstrate the generality of the technology.
Jeopardy! is a popular quiz show in the US which
has been on the air for 27 years. In each game, three
contestants compete for the opportunity to answer
60 questions in 12 categories of 5 questions each.
Jeopardy! questions cover an incredibly broad do-
main, from science, literature, history, to popular
culture. We are drawn to Jeopardy! as a test bed
for open-domain QA technology due to its broad do-
main, complex language, as well as the emphasis on
accuracy, confidence, and speed during game play.
While the vast majority of Jeopardy! questions
are factoid questions, we find several other types
of questions in the Jeopardy! data, which can ben-
efit from specialized processing in the QA system.
The additional processing in these questions com-
plements that of the factoid questions to achieve im-
proved overall QA performance. Among the various
types of questions handled by the system are defini-
tion questions shown in the examples below:
(1) GON TOMORROW: It can be the basket
below a hot-air balloon or a flat-bottomed
boat used on a canal (answer: gondola);
(2) I LOVE YOU, ?MIN?: Overbearing (an-
swer: domineering);
(3) INVEST: From the Latin for ?year?, it?s
an investment or retirement fund that pays
out yearly (answer: an annuity)
where the upper case text indicates the Jeop-
ardy! category for each question1.
Several characteristics of this class of questions
warrant special processing: first, the clue (question)
1A Jeopardy! category indicates a theme is common among
its 5 questions.
712
often aligns well with dictionary entries, making
dictionary resources potentially effective. Second,
these clues often do not indicate an answer type,
which is an important feature for identifying cor-
rect answers in factoid questions (in the examples
above, only (3) provided an answer type, ?fund?).
Third, definition questions are typically shorter in
length than the average factoid question. These dif-
ferences, namely the shorter clue length and the lack
of answer types, make the use of a specialized ma-
chine learning model potentially promising for im-
proving the overall system accuracy. The first step
for handling definitions is, of course, the automatic
separation of definitions from other question types,
which is not a simple task in the Jeopardy! domain.
For instance, consider the following example which
is a variation of (3) above:
(4) INVEST: From the Latin for ?year?,
an annuity is an investment or retirement
fund that pays out this often (answer:
yearly)
Even though the clue is nearly identical to (3), the
clue does not provide a definition for the answer
yearly, although at first glance we may have been
misled. The source of complexity is given by the fact
that Jeopardy! clues are not phrased in interrogative
form as questions typically are. This complicates the
design of definition classifiers since we cannot di-
rectly use either typical structural patterns that char-
acterize definition/description questions, or previous
approaches, e.g. (Ahn et al, 2004; Kaisser and Web-
ber, 2007; Blunsom et al, 2006). Given the com-
plexity and the novelty of the task, we found it use-
ful to exploit the kernel methods technology. This
has shown state-of-the-art performance in Question
Classification (QC), e.g. (Zhang and Lee, 2003;
Suzuki et al, 2003; Moschitti et al, 2007) and it
is very well suited for engineering feature represen-
tations for novel tasks.
In this paper, we apply SVMs and kernel meth-
ods to syntactic/semantic structures for modeling
accurate classification of Jeopardy! definition ques-
tions. For this purpose, we use several levels of lin-
guistic information: word and POS tag sequences,
dependency, constituency and predicate argument
structures and we combined them using state-of-
the-art structural kernels, e.g. (Collins and Duffy,
2002; Shawe-Taylor and Cristianini, 2004; Mos-
chitti, 2006). The extensive empirical analysis of
several advanced models shows that our best model,
which combines different kernels, improves the F1
of our baseline model by 67% relative, from 40.37
to 67.48. Surprisingly, with respect to previous find-
ings on standard QC, e.g. (Zhang and Lee, 2003;
Moschitti, 2006), the Syntactic Tree Kernel (Collins
and Duffy, 2002) is not effective whereas the ex-
ploitation of partial tree patterns proves to be es-
sential. This is due to the different nature of Jeop-
ardy! questions, which are not expressed in the usual
interrogative form.
To demonstrate the benefit of our question clas-
sifier, we integrated it into our Watson by coupling
it with search and candidate generation against spe-
cialized dictionary resources. We show that in end-
to-end evaluations, Watson with kernel-based defi-
nition classification and specialized definition ques-
tion processing achieves statistically significant im-
provement compared to our baseline systems.
In the reminder of this paper, Section 2 describes
Watson by focusing on the problem of definition
question classification, Section 3 describes our mod-
els for such classifiers, Section 4 presents our exper-
iments on QC, whereas Section 5 shows the final im-
pact on Watson. Finally, Section 6 discusses related
work and Section 7 derives the conclusions.
2 Watson: The IBM Jeopardy! System
This section gives a quick overview of Watson and
the problem of classification of definition questions,
which is the focus of this paper.
2.1 Overview
Watson is a massively parallel probabilistic
evidence-based architecture for QA (Ferrucci et
al., 2010). It consists of several major stages for
underlying sub-tasks, including analysis of the
question, retrieval of relevant content, scoring and
ranking of candidate answers, as depicted in Figure
1. In the rest of this section, we provide an overview
of Watson, focusing on the task of answering
definitional questions.
Question Analysis: The first stage of the pipeline,
it applies several analytic components to identify
key characteristics of the question (such as answer
713
Figure 1: Overview of Watson
type, question classes, etc.) used by later stages of
the Watson pipeline. Various general purpose NLP
components, such as a parser and named entity de-
tector, are combined with task-specific modules for
this analysis.
The task-specific analytics include several QC
components, which determine if the question be-
longs to one or more broad ?question classes?.
These question classes can influence later stages of
the Watson pipeline. For instance, a question de-
tected as an abbreviation question can invoke spe-
cialized candidate generators to produce possible ex-
pansions of the abbreviated term in the clue. Simi-
larly, the question classes can impact the methods
for answer scoring and the machine learning mod-
els used for ranking candidate answers. The focus
of this paper is on the definition class, which is de-
scribed in the next section.
Hypothesis Generation: Following question anal-
ysis, the Watson pipeline searches its document col-
lection for relevant documents and passages that are
likely to contain the correct answer to the question.
This stage of the pipeline generates search queries
based on question analysis results, and obtains a
ranked list of documents and passages most relevant
to the search queries. A variety of candidate gen-
eration techniques are then applied to the retrieved
results to produce a set of candidate answers.
Information obtained from question analysis can
be used to influence the search and candidate gener-
ation processes. The question classes detected dur-
ing question analysis can focus the search towards
specific subsets of the corpus. Similarly, during can-
didate generation, strategies used to generate the set
of candidate answers are selected based on the de-
tected question classes.
Hypothesis and Evidence Scoring: A wide variety
of answer scorers are then used to gather evidence
supporting each candidate answer as the correct an-
swer to the given question. The scorers include both
context dependent as well as context independent
scorers, relying on various structured and unstruc-
tured resources for their supporting evidence.
Candidate Ranking: Finally, machine learning
models are used to weigh the gathered evidence and
rank the candidate answers. The models generate a
ranked list of answers each with an associated con-
fidence. The system can also choose to refrain from
answering a question if it has low confidence in all
candidates. This stage of the pipeline employs sev-
eral machine learning models specially trained to
handle various types of questions. These models are
trained using selected feature sets based on question
classes and candidate answers are ?routed? to the
appropriate model according to the question classes
detected during question analysis.
2.2 Answering Definition Questions
Among the many question classes that Watson iden-
tifies and leverages for special processing, of partic-
ular interest for this paper is the class we refer to
as definition questions. These are questions whose
clue texts contain one or more definitions of the cor-
rect answer. For instance, in example (3), the main
clause in the question corresponds to a dictionary
definition of the correct answer (annuity). Looking
up this definition in dictionary resources could en-
able us to answer this question correctly and with
high confidence. This suggests that special process-
714
ing of such definition questions could allow us to
hone in on the correct answer through processes dif-
ferent from those used for other types of questions.
This paper explores strategies for definition ques-
tion processing to improve overall question answer-
ing performance. A key challenge we have to ad-
dress is that of accurate recognition of such ques-
tions. Given an input question the Watson question
analysis stage uses a definition question recognizer
to detect this specific class of questions. We explore
several approaches for recognition, including a rule
based approach and a variety of statistical models.
Questions that are recognized as definition ques-
tions invoke search processes targeted towards
dictionary-like sources in our system. We use a va-
riety of such sources, such as standard English dic-
tionaries, Wiktionary, WordNet, etc. After gather-
ing supporting evidence for candidate answers ex-
tracted from these sources, our system routes the
candidates to definition-specific candidate ranking
models, which have been trained with selected fea-
ture sets.
The following sections present a description and
evaluation of our approach for identifying and an-
swering definition questions.
3 Kernel Models for Question
Classification
Previous work (Zhang and Lee, 2003; Suzuki et al,
2003; Blunsom et al, 2006; Moschitti et al, 2007)
as shown that syntactic structures are essential for
QC. Given the novelty of both the domain and the
type of our classification items, we rely on kernel
methods to study and design effective representa-
tions. Indeed, these are excellent tools for auto-
matic feature engineering, especially for unknown
tasks and domains. Our approach consists of using
SVMs and kernels for structured data applied to sev-
eral types of structural lexical, syntactic and shallow
semantic information.
3.1 Tree and Sequence Kernels
Kernel functions are implicit scalar products be-
tween data examples (i.e. questions in our case)
in the very high dimensional space of substructures,
where each of the latter is a component of the im-
plicit vectors associated with the examples.
ROOT
SBARQ
WHADVP
WRB
When
S
VP
VBN
hit
PP
IN
by
NP
NNS
electrons
,
,
NP
DT
a
NN
phosphor
VP
VBZ
gives
PRP
RP
off
NP
NP
JJ
electromagnetic
NN
energy
PP
IN
in
NP
DT
this
NN
form
1
Figure 2: Constituency Tree
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
A2
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
ROOT
VBZ
OBJ
NN
NMOD
IN
PMOD
NN
formNMOD
DT
this
in
energyNMOD
JJ
electromag.
PRT
RP
off
givesSBJ
NN
phosphorNMOD
DT
a
P
,
TMP
VBN
LGS
IN
PMOD
NNS
electrons
by
hitTMP
WRB
when
1
Figure 3: Dependency Tree
negative mistake STK, ok PTK
NP
ADJP
JJ
conceited
CC
or
JJ
arrogant
NP
NN
meaning
NN
adjective
NN
5-letter
NN
fowl
positive mistake STK, ok PTK
NP
VP
PP
NP
NN
field
VBG
playing
DT
a
IN
on
VBN
used
NP
NN
grass
JJ
green
JJ
artificial
NP
VP
PP
NP
NN
canal
DT
a
IN
on
VBN
used
NP
NN
boat
JJ
flat-bottomed
DT
a
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
AM-MNR
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
1
Figure 4: A tree encoding a Predicate Argument Structure Set
Although several kernels for structured data have
been developed (see Section 6), the main distinc-
tions in terms of feature spaces is given by the fol-
lowing three different kernels:
? Sequence Kernels (SK); we implemented the
discontinuous string kernels described in (Shawe-
Taylor and Cristianini, 2004). This allows for rep-
resenting a string of symbols in terms of its possi-
ble substrings with gaps, i.e. an arbitrary number of
symbols can be skipped during the generation of a
substring. The symbols we used in the sequential de-
scriptions of questions are words and part-of-speech
tags (in two separate sequences). Consequently, all
possible multiwords with gaps are features of the im-
plicitly generated vector space.
715
? Syntactic Tree Kernel (STK) (Collins and Duffy,
2002) applied to constituency parse trees. This gen-
erates all possible tree fragments as features with
the conditions that sibling nodes from the original
trees cannot be separated. In other words, substruc-
tures are composed by atomic building blocks cor-
responding to nodes along with all their direct chil-
dren. These, in case of a syntactic parse tree, are
complete production rules of the associated parser
grammar2.
? Partial Tree Kernel (PTK) (Moschitti, 2006) ap-
plied to both constituency and dependency parse
trees. This generates all possible tree fragments, as
above, but sibling nodes can be separated (so they
can be part of different tree fragments). In other
words, a fragment is any possible tree path, from
whose nodes other tree paths can depart. Conse-
quently, an extremely rich feature space is gener-
ated. Of course, PTK subsumes STK but sometimes
the latter provides more effective solutions as the
number of irrelevant features is smaller as well.
When applied to sequences and tree structures, the
kernels discussed above produce many different
kinds of features. Therefore, the design of appro-
priate syntactic/semantic structures determines the
representational power of the kernels. Hereafter, we
show the models we used.
3.2 Syntactic Semantic Structures
We applied the above kernels to different structures.
These can be divided in sequences of words (WS)
and part of speech tags (PS) and different kinds of
trees. For example, given the non-definition Jeop-
ardy! question:
(5) GENERAL SCIENCE: When hit by elec-
trons, a phosphor gives off electromag-
netic energy in this form. (answer: light
or photons),
we use the following sequences:
WS: [when][hit][by][electrons][,][a][phosphor][gives]
[off][electromagnetic][energy][in][this][form]
PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in]
[dt][nn]
Additionally, we use constituency trees (CTs), see
2From here the name syntactic tree kernels
Figure 2 and dependency structures converted into
the dependency trees (DTs), e.g. shown in Figure
3. Note that, the POS-tags are central nodes, the
grammatical relation label is added as a father
node and all the relations with the other nodes are
described by means of the connecting edges. Words
are considered additional children of the POS-tag
nodes (in this case the connecting edge just serves
to add a lexical feature to the target POS-tag node).
Finally, we also use predicate argument structures
generated by verbal and nominal relations accord-
ing to PropBank (Palmer et al, 2005) and NomBank
(Meyers et al, 2004). Given the target sentence, the
set of its predicates are extracted and converted into
a forest, then a fake root node, PAS, is used to con-
nect these trees. For example, Figure 4 illustrates a
Predicate Argument Structures Set (PASS) encoding
two relations, give and hit, as well as the nominaliza-
tion energy along with all their arguments.
4 Experiments on Definition Question
Classification
In these experiments, we study the role of kernel
technology for the design of accurate classification
of definition questions. We build several classifiers
based on SVMs and kernel methods. Each classi-
fier uses advanced syntactic/semantic structural fea-
tures and their combination. We carry out an exten-
sive comparison in terms of F1 between the different
models on the Jeopardy! datasets.
4.1 Experimental Setup
Corpus: the data for our QC experiments consists
of a randomly selected set of 33 Jeopardy! games3.
These questions were manually annotated based on
whether or not they are considered definitional. This
resulted in 306 definition and 4964 non-definition
clues. Each test set is stored in a separate file con-
sisting of one line per question, which contains tab-
separated clue information and the Jeopardy! cate-
gory, e.g. INVEST in example (4).
Tools: for SVM learning, we used the SVMLight-
TK software4, which includes structural kernels in
SVMLight (Joachims, 1999)5. For generating con-
3Past Jeopardy! games can be downloaded from
http://www.j-archive.com.
4Available at http://dit.unitn.it/?moschitt
5http://svmlight.joachims.org
716
stituency trees, we used the Charniak parser (Char-
niak, 2000). We also used the syntactic?semantic
parser by Johansson and Nugues (2008) to gener-
ate dependency trees (Mel?c?uk, 1988) and predicate
argument trees according to the PropBank (Palmer
et al, 2005) and NomBank (Meyers et al, 2004)
frameworks.
Baseline Model: the first model that we used as a
baseline is a rule-based classifier (RBC). The RBC
leverages a set of rules that matches against lexical
and syntactic information in the clue to make a bi-
nary decision on whether or not the clue is consid-
ered definitional. The rule set was manually devel-
oped by a human expert, and consists of rules that
attempt to identify roughly 70 different constructs
in the clues. For instance, one of the rules matches
the parse tree structure for ?It?s X or Y?, which will
identify example (1) as a definition question.
Kernel Models: we apply the kernels described
in Section 3 to the structures extracted from Jeop-
ardy! clues. In particular, we design the following
models: BOW, i.e. linear kernel on bag-of-words
from the clues; WSK, PSK and CSK, i.e. SK applied
to the word and POS-tag sequences from the clues,
and the word sequence taken from the question cat-
egories, respectively; STK-CT, i.e. STK applied to
CTs of the clue; PTK-CT and PTK-DT, i.e. PTK
applied to CTs and DTs of the clues, respectively;
PASS, i.e. PTK applied to the Predicate Argument
Structure Set extracted from the clues; and RBC, i.e.
a linear kernel applied to the vector only constituted
by the 1/0 output of RBC.
Learning Setting: there is no particular parameteri-
zation. Since there is an imbalance between positive
and negative examples, we used a Precision/Recall
trade-off parameter in SVM-Light-TK equal to 5.6
Measures: the performance is measured with Pre-
cision, Recall and F1-measure. We estimated them
by means of Leave-One-Out7 (LOO) on the question
set.
4.2 Results and Discussion
Table 1 shows the performance obtained using dif-
ferent kernels (feature spaces) with SVMs. We note
6We have selected 5 as a reasonable value, which kept bal-
anced Precision and Recall on a validation set.
7LOO applied to a corpus ofN instances consists in training
on N ? 1 examples and testing on the single held-out example.
This process is repeated for all instances.
Kernel Space Prec. Rec. F1
RBC 28.27 70.59 40.38
BOW 47.67 46.73 47.20
WSK 47.11 50.65 48.82
STK-CT 50.51 32.35 39.44
PTK-CT 47.84 57.84 52.37
PTK-DT 44.81 57.84 50.50
PASS 33.50 21.90 26.49
PSK 39.88 45.10 42.33
CSK 39.07 77.12 51.86
Table 1: Kernel performance using leave-one-out cross-
validation.
that: first, RBC has good Recall but poor Precision.
This is interesting since, on one hand, these results
validate the complexity of the task: in order to cap-
ture the large variability of the positive examples,
the rules developed by a skilled human designer are
unable to be sufficiently precise to limit the recog-
nition to those examples. On the other hand, RBC,
being a rather different approach from SVMs, can be
successfully exploited in a joint model with them.
Second, BOW yields better F1 than RBC but it
does not generalize well since its F1 is still low.
When n-grams are also added to the model by
means of WSK, the F1 improves by about 1.5 ab-
solute points. As already shown in (Zhang and Lee,
2003; Moschitti et al, 2007), syntactic structures are
needed to improve generalization.
Third, surprisingly with respect to previous work,
STK applied to CT8 provides accuracy lower than
BOW, about 8 absolute points. The reason is due to
the different nature of the Jeopardy! questions: large
syntactic variability reduces the probability of find-
ing general and well formed patterns, i.e. structures
generated by entire production rules. This suggests
that PTK, which can capture patterns derived from
partial production rules, can be more effective. In-
deed, PTK-CT achieves the highest F1, outperform-
ing WSK also when used with a different syntactic
paradigm, i.e. PTK-DT.
Next, PSK and PASS provide a lower accuracy
but they may be useful in kernel combinations as
they can complement the information captured by
the other models. Interestingly, CSK alone is rather
effective for classifying definition questions. We be-
8Applying it to DT does not make much sense as already
pointed out in (Moschitti, 2006).
717
?Figure 5: Similarity according to PTK and STK
lieve this is because definition questions are some-
times clustered into categories such as 4-LETTER
WORDS or BEGINS WITH ?B?.
Moreover, we carried out qualitative error analy-
sis on the PTK and STK outcome, which supported
our initial hypothesis. Let us consider the bottom
tree in Figure 5 in the training set. The top tree is
a test example correctly classified by PTK but in-
correctly classified by STK. The dashed line in the
top tree contains the largest subtree matched by PTK
(against the bottom tree), whereas the dashed line in
the bottom tree indicates the largest subtree matched
by STK (against the top tree). As the figure shows,
PTK can exploit a larger number of partial patterns.
Finally, the above points suggest that different
kernels produce complementary information. It is
thus promising to experiment with their combina-
tions. The joint models can be simply built by
summing kernel functions together. The results are
shown in Table 2. We note that: (i) CSK comple-
ments the WSK information, achieving a substan-
tially better result, i.e. 62.95; (ii) PTK-CT+CSK
performs even better than WSK+CSK (as PTK out-
performs WSK); and (iii) adding RBC improves
further on the above combinations, i.e. 68.11 and
67.32, respectively. This evidently demonstrates
that RBC captures complementary information. Fi-
nally, more complex kernels, especially the overall
kernel summation, do not seem to improve the per-
Kernel Space Prec. Rec. F1
WSK+CSK 70.00 57.19 62.95
PTK-CT+CSK 69.43 60.13 64.45
PTK-CT+WSK+CSK 68.59 62.09 65.18
CSK+RBC 47.80 74.51 58.23
PTK-CT+CSK+RBC 59.33 74.84 65.79
BOW+CSK+RBC 60.65 73.53 66.47
PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32
PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56
WSK+CSK+RBC 69.26 66.99 68.11
ALL 61.42 67.65 64.38
Table 2: Performance of Kernel Combinations using
leave-one-out cross-validation.
formance. This is also confirmed by the PASS re-
sults derived in (Moschitti et al, 2007) on TREC
QC.
5 Experiments on the Jeopardy System
Since the kernel-based classifiers perform substan-
tially better than RBC, we incorporate the PTK-
CT+WSK+CSK model9 into Watson for definition
classification and evaluated the QA performance
against two baseline systems. For the end-to-end ex-
periments, we used Watson?s English Slot Grammar
parser (McCord, 1980) to generate the constituency
trees. The component level evaluation shows that
we achieved comparable performance as previously
discussed with ESG.
5.1 Experimental Setup
We integrated the classifier into the question analy-
sis module, and incorporated additional components
to search against dictionary resources and extract
candidate answers from these search results when a
question is classified as definitional. In the final ma-
chine learning models, a separate model is trained
for definition questions to enable scoring tailored to
the specific characteristics of those questions.
Based on our manually annotated gold standard,
less than 10% of Jeopardy! questions are classified
as definition questions. Due to their relatively low
frequency we conduct two types of evaluations. The
first is definition-only evaluation, in which we apply
our definition question classifier to identify a large
9Since we aim to compare a purely statistical approach to
the rule-based approach, we did not experiment with the model
that uses RBC as a feature in our end-to-end experiments.
718
set of definition questions and evaluate the end-to-
end system?s performance on this large set of ques-
tions. These results enable us to draw statistically
significant conclusions about our approach to ad-
dressing definition questions.
The second type of evaluation is game-based
evaluation, which assesses the impact of our defi-
nition question processing on Watson performance
while preserving the natural distribution of these
question types in Jeopardy! data. Game-based eval-
uations situate the system?s performance on defini-
tion questions relative to other types of questions,
and enable us to gauge the component?s contribu-
tions in a game-based setting.
For both evaluation settings, three configurations
of Watson are used as follows:
? the NoDef system, in which Watson is config-
ured without definition classification and pro-
cessing, thereby treating all definition ques-
tions as regular factoid questions;
? the StatDef system, which leverages the sta-
tistical classifier and subsequent definition spe-
cific search and candidate generation compo-
nents as described above; and
? the RuleDef system, in which Watson adopts
RBC and employs the same additional defini-
tion search and candidate generation compo-
nents as the StatDef system.
For the definition-only evaluation, we selected all
questions recognized as definitional by the statistical
classifier from roughly 1000 unseen games (60000
questions), resulting in a test set of 1606 questions.
Due to the size of the initial set, it is impractical to
manually create a gold standard for measuring Pre-
cision and Recall of the classifier. Instead, we com-
pare the StatDef system against the NoDef on these
1606 questions using two metrics: accuracy, defined
as the percentage of questions correctly answered,
and p@70, the system?s Precision when answering
only the top 70% most confident questions. P@70 is
an important metric in Jeopardy! game play as well
as in real world applications where the system may
refrain from answering a question when it is not con-
fident about any of its answers. Since RBC identifies
significantly more definition questions, we started
NoDef StatDef NoDef RuleDef
# Questions 1606 1606 1875 1875
Accuracy 63.76% 65.57% 56.64% 57.51%
P@70 82.22% 84.53% 72.73% 74.87%
Table 3: Definition-Only Evaluation Results
with an initial set of roughly 300 games, from which
the RBC identified 1875 questions as definitional.
We compared the RuleDef system?s performance on
these questions against the NoDef baseline using the
accuracy and p@70 metrics.
For the game-based evaluation, we randomly se-
lected 66 unseen Jeopardy! games, consisting of
3546 questions after excluding audio/visual ques-
tions.10 We contrast the StatDef system perfor-
mance against that of NoDef and RuleDef along
several dimensions: accuracy and p@70, described
above, as well as earnings, the average amount of
money earned for each game.
5.2 Definition-Only Evaluation
For the definition-only evaluation, we compared the
StatDef system against the NoDef system on a set of
1606 questions that the StatDef system classified as
definitional. The results are shown in the first two
columns in Table 3. To contrast the gain obtained
by the StatDef system against that achieved by the
RuleDef system, we ran the RuleDef system over
the 1875 questions identified as definitional by the
rule-based classifier. We contrast the RuleDef sys-
tem performance with that of the NoDef system, as
shown in the last two columns in Table 3.
Our results show that based on both evaluation
metrics, StatDef improved upon the NoDef baseline
more than RuleDef improved on the same baseline
system. Furthermore, for the accuracy metric where
all samples are paired and independent, the differ-
ence in performance between the StatDef and NoDef
systems is statistically significant at p<0.05, while
that between the RuleDef and NoDef systems is not.
5.3 Game-Based Evaluation
The game-based evaluation was carried out on 66
unseen games (roughly 3500 questions). Of these
10Audio/visual questions are those accompanied by either an
image or an audio clip. The text portions of these questions are
often insufficient for identifying the correct answers.
719
# Def Q?s Accuracy P@70 Earnings
NoDef 0 69.71% 86.79% $24,818
RuleDef 480 69.23% 86.31% $24,397
StatDef 131 69.85% 87.19% $25,109
Table 4: Game-Based Evaluation Results
questions, the StatDef system classified 131 of them
as definitional while the RuleDef system identified
480 definition questions. Both systems were com-
pared against the NoDef system using the accuracy,
p@70, and earnings metric computed over all ques-
tions, as shown in Table 4.
Our results show that even though in the
definition-only evaluation both the RuleDef and
StatDef systems outperformed the NoDef baseline,
in our game-based evaluation, the RuleDef system
performed worse than the NoDef baseline. The low-
ered performance is due to the fact that the Preci-
sion of the RBC is much lower than that of the sta-
tistical classifier, and the special definition process-
ing applied to questions that are erroneously clas-
sified as definitional was harmful. Our evaluation
of this false positive set showed that its accuracy
dropped by 6% compared to the NoDef system. On
the other hand, the StatDef system outperformed the
two other systems, and its accuracy improvement
upon the RuleDef system is statistically significant
at p<0.05.
6 Related Work
Our paper studies the use of advanced representa-
tion for QC in the Jeopardy! domain. As previously
mentioned Jeopardy! questions are stated as affir-
mative sentences, which are different from the typ-
ical QA questions. For the design of our models,
we have carefully taken into account previous work.
This shows that semantics and syntax are essential
to retrieve precise answers, e.g (Hickl et al, 2006;
Voorhees, 2004; Small et al, 2004).
We focus on definition questions, which typically
require more complex processing than factoid ques-
tions (Blair-Goldensohn et al, 2004; Chen et al,
2006; Shen and Lapata, 2007; Bilotti et al, 2007;
Moschitti et al, 2007; Surdeanu et al, 2008; Echi-
habi and Marcu, 2003). For example, language mod-
els were applied to definitional QA in (Cui et al,
2005) to learn soft pattern models based on bigrams.
Other related work, such as (Sasaki, 2005; Suzuki
et al, 2002), was also very tied to bag-of-words
features. Predicate argument structures have been
mainly used for reranking (Shen and Lapata, 2007;
Bilotti et al, 2007; Moschitti et al, 2007; Surdeanu
et al, 2008).
Our work and methods are similar to (Zhang and
Lee, 2003; Moschitti et al, 2007), which achieved
the state-of-the-art in QC by applying SVMs along
with STK-CT. The results were derived by experi-
menting with a TREC dataset11(Li and Roth, 2002),
reaching an accuracy of 91.8%. However, such data
refers to typical instances from QA, whose syntactic
patterns can be easily generalized by STK. In con-
trast, we have shown that STK-CT is not effective
for our domain, as it presents very innovative ele-
ments: questions in affirmative and highly variable
format. Thus, we employed new methods such as
PTK, dependency structures, multiple sequence ker-
nels including category information and many com-
binations.
Regarding the use of Kernel Methods, there is
a considerably large body of work in Natural Lan-
guage Processing, e.g. regarding syntactic parsing
(Collins and Duffy, 2002; Kudo et al, 2005; Shen
et al, 2003; Kudo and Matsumoto, 2003; Titov and
Henderson, 2006; Toutanova et al, 2004), named
entity recognition and chunking (Cumby and Roth,
2003; Daume? III and Marcu, 2004), relation extrac-
tion (Zelenko et al, 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005; Zhang et al,
2005; Bunescu, 2007; Nguyen et al, 2009a), text
categorization (Cancedda et al, 2003), word sense
disambiguation (Gliozzo et al, 2005) and seman-
tic role labeling (SRL), e.g. (Kazama and Torisawa,
2005; Che et al, 2006a; Moschitti et al, 2008).
However, ours is the first study on the use of sev-
eral combinations of kernels applied to several struc-
tures on very complex data from the Jeopardy! do-
main.
7 Final Remarks and Conclusion
In this paper we have experimented with advanced
structural kernels applied to several kinds of syntac-
tic/semantic linguistic structures for the classifica-
tion of questions in a new application domain, i.e.
Jeopardy!. Our findings are summarized hereafter:
11Available at http://cogcomp.cs.illinois.
edu/Data/QA/QC/
720
First, it should be noted that basic kernels, such
as STK, PTK and SK, when applied to new repre-
sentations, i.e. syntactic/semantic structures, con-
stitute new kernels. Thus structural representations
play a major role and, from this perspective, our pa-
per makes a significant contribution.
Second, the experimental results show that the
higher variability of Jeopardy! questions prevents us
from achieving generalization with typical syntactic
patterns even if they are derived by powerful meth-
ods such as STK. In contrast, partial patterns, such
as those provided by PTK applied to constituency
(or dependency) trees, prove to be effective.
In particular, STK has been considered as the best
kernel for exploiting syntactic information in con-
stituency trees, e.g. it is state-of-the-art in: QC
(Zhang and Lee, 2003; Moschitti et al, 2007; Mos-
chitti, 2008); SRL, (Moschitti et al, 2008; Mos-
chitti et al, 2005; Che et al, 2006b); pronominal
coreference resolution (Yang et al, 2006; Versley
et al, 2008) and Relation Extraction (Zhang et al,
2006; Nguyen et al, 2009b). We showed that, in
the complex domain of Jeopardy!, STK surprisingly
provides low accuracy whereas PTK is rather ef-
fective and greatly outperforms STK. We have also
provided an explanation of such behavior by means
of error analysis: in contrast with traditional ques-
tion classification, which focuses on basic syntactic
patterns (e.g. ?what?, ?where?, ?who? and ?how?).
Figure 5 shows that PTK captures partial patterns
that are important for more complex questions like
those in Jeopardy!
Third, we derived other interesting findings for
NLP related to this novel domain, e.g.: (i) the im-
pact of dependency trees is similar to the one of
constituency trees. (ii) A simple computational rep-
resentation of shallow semantics, i.e. PASS (Mos-
chitti, 2008), does not work in Jeopardy!. (iii) Se-
quence kernels on category cues, i.e., higher level of
lexical semantics, improve question classification.
(iv) RBC jointly used with statistical approaches is
helpful to tackle the Jeopardy! complexity.
Next, our kernel models improve up to 20 abso-
lute percent points over n-grams based approaches,
reaching a significant accuracy of about 70%. Wat-
son, exploiting such a classifier, improved previ-
ous versions using RBC and no definition classifica-
tion both in definition-only evaluations and in game-
based evaluations.
Finally, we point out that:
? Jeopardy! has a variety of different special ques-
tion types that are handled differently. We focus on
kernel methods for definition question for two rea-
sons. First, their recognition relies heavily on parse
structures and is therefore more amenable to the ap-
proach proposed in the paper than the recognition
of other question types. Second, definition is by far
the most frequent special question type in Jeopardy!;
therefore, we can obtain sufficient data for training
and testing.
? We were unable to address the whole QC prob-
lem using a statistical model due to the lack of suffi-
cient training data for most special question classes.
Furthermore, we focused only on the definition clas-
sification and its impact on system performance due
to space reasons.
? Our RBC has a rather imbalanced trade-off be-
tween Precision and Recall. This may not be the
best operating point, but the optimal point is diffi-
cult to obtain empirically for an RBC, which is a
strong motivation of the work in this paper. We ex-
perimented with tuning the trade-off between Preci-
sion and Recall with the RBC, but since RBC uses
hand-crafted rules and does not have a parameter for
that, ultimately the statistical approach proved more
effective.
In future work, we plan to extend the current re-
search by investigating models capable of exploit-
ing predicate argument structures for question clas-
sification and answer reranking. The use of syntac-
tic/semantic kernels is a promising research direc-
tion (Basili et al, 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). In this
perspective kernel learning is a very interesting re-
search line, considering the complexity of represen-
tation and classification problems in which our ker-
nels operate.
Acknowledgements
This work has been supported by the IBM?s Open
Collaboration Research (OCR) awards program. We
are deeply in debt with Richard Johansson, who pro-
duced the earlier syntactic/semantic representations
of the Jeopardy! questions from the text format.
721
References
Kisuh Ahn, Johan Bos, Stephen Clark, James R. Cur-
ran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B.
Smillie, and Bonnie Webber. 2004. Question an-
swering with qed and wee at trec-2004. In E. M.
Voorhees and L. P. Buckland, editors, The Thirteenth
Text REtrieval Conference, TREC 2004, pages 595?
599, Gaitersburg, MD.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1?8, Ann Arbor, Michigan. Association
for Computational Linguistics.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of ACM SIGIR.
S. Blair-Goldensohn, K. R. McKeown, and A. H.
Schlaikjer. 2004. Answering definitional questions:
A hybrid approach. In M. Maybury, editor, Proceed-
ings of AAAI 2004. AAAI Press.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear models.
In SIGIR ?06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 615?616, New
York, NY, USA. ACM.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006a. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 73?
80, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006b. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 73?80, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
Proceedings of ACL.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern
models for definitional QA. In Proceedings of SIGIR,
Salvador, Brazil. ACM.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proceedings of ACL.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Danilo Giampiccolo, Pamela Froner, Anselmo Pen?as,
Christelle Ayache, Dan Cristea, Valentin Jijkoun,
Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, and
Richard Suteliffe. 2007. Overview of the CLEF 2007
multilingual question anwering track. In Proceedings
of the Cross Language Evaluation Forum.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lcc chaucer
at trec 2006. In Proceedings of TREC.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ? Sup-
port Vector Learning, 13.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
722
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings of
the Workshop on Deep Linguistic Processing, DeepLP
?07, pages 41?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP?05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Shyong Lam, David Pennock, Dan Cosley, and Steve
Lawrence. 2003. 1 billion pages = 1 milllion dollars?
mining the web to pay ?who wants to be a millionaire?
In Proceedings of the 19th Conference on Uncertainty
in AI.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL.
Michael C. McCord. 1980. Slot grammars. Computa-
tional Linguistics.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31, Boston, United
States.
Alessandro Moschitti, Bonaventura Coppola, Ana-Maria
Giuglea, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In CoNLL 2005 shared task.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ?08, NY, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009a. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378?1387, Singapore, August.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009b. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1378?1387, Morristown,
NJ, USA. Association for Computational Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?105.
Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, and
Hsin-Hsi Chen. 2007. Overview of the NTCIR-6
cross-lingual question answering (CLQA) task. In
Proceedings of the 6th NTCIR Workshop on Evalua-
tion of Information Access Technologies.
Y. Sasaki. 2005. Question answering as question-biased
term extraction: A new approach toward multilingual
qa. In Proceedings of ACL, pages 215?222.
John Shawe-Taylor and Nello Cristianini. 2004. LaTeX
User?s Guide and Document Reference Manual. Ker-
nel Methods for Pattern Analysis, Cambridge Univer-
sity Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL.
L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In Proceedings of
EMNLP, Sapporo, Japan.
S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin,
N. Shimizu, P. Kantor, D. Kelly, and N. Wacholder.
2004. Hitiqa: Towards analytical question answering.
In Proceedings of COLING.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT, Columbus, Ohio.
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. Svm an-
swer selection for open-domain question answering.
In Proceedings of Coling, pages 974?980.
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku
Maeda. 2003. Question classification using hdag ker-
nel. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answering,
pages 61?68, Sapporo, Japan, July. Association for
Computational Linguistics.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing?08), Manchester, England.
723
Ellen M. Voorhees and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering track.
In Proceedings of the TREC 2005 Conference.
E. M. Voorhees. 2004. Overview of the trec 2004 ques-
tion answering track. In Proceedings of TREC 2004.
Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM Press.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP?2005, Lecture Notes in Computer Science (LNCS
3651), pages 378?389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
724
