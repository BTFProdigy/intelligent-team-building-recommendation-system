Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Assessment of Spoken Modern Standard Arabic 
Jian Cheng, Jared Bernstein, Ulrike Pado, Masanori Suzuki 
Pearson Knowledge Technologies 
299 California Ave, Palo Alto, CA 94306 
jian.cheng@pearson.com
 
 
 
Abstract 
 Proficiency testing is an important ingredient 
in successful language teaching. However, re-
peated testing for course placement, over the 
course of instruction or for certification can be 
time-consuming and costly. We present the 
design and validation of the Versant Arabic 
Test, a fully automated test of spoken Modern 
Standard Arabic, that evaluates test-takers' fa-
cility in listening and speaking. Experimental 
data shows the test to be highly reliable (test-
retest r=0.97) and to strongly predict perform-
ance on the ILR OPI (r=0.87), a standard in-
terview test that assesses oral proficiency. 
1 Introduction 
Traditional high-stakes testing of spoken profi-
ciency often evaluates the test-taker's ability to ac-
complish communicative tasks in a conversational 
setting. For example, learners may introduce them-
selves, respond to requests for information, or ac-
complish daily tasks in a role-play. 
Testing oral proficiency in this way can be 
time-consuming and costly, since at least one 
trained interviewer is needed for each student. For 
example, the standard oral proficiency test used by 
the United States government agencies (the Inter-
agency Language Roundtable Oral Proficiency 
Interview or ILR OPI) is usually administered by 
two certified interviewers for approximately 30-45 
minutes per candidate. 
The great effort involved in oral proficiency in-
terview (OPI) testing makes automated testing an 
attractive alternative. Work has been reported on 
fully automated scoring of speaking ability (e.g., 
Bernstein & Barbier, 2001; Zechner et al, 2007, 
for English; Balogh & Bernstein, 2007, for English 
and Spanish). Automated testing systems do not 
aim to simulate a conversation with the test-taker 
and therefore do not directly observe interactive 
human communication. Bernstein and Barbier 
(2001) describe a system that might be used in 
qualifying simultaneous interpreters; Zechner et al 
(2007) describe an automated scoring system that 
assesses performance according to the TOEFL iBT 
speaking rubrics. Balogh and Bernstein (2007) fo-
cus on evaluating facility in a spoken language, a 
separate test construct that relates to oral profi-
ciency. 
?Facility in a spoken language? is defined as 
?the ability to understand a spoken language on 
everyday topics and to respond appropriately and 
intelligibly at a native-like conversational pace? 
(Balogh & Bernstein, 2007, p. 272). This ability is 
assumed to underlie high performance in commu-
nicative settings, since learners have to understand 
their interlocutors correctly and efficiently in real 
time to be able to respond. Equally, learners have 
to be able to formulate and articulate a comprehen-
sible answer without undue delay. Testing for oral 
proficiency, on the other hand, conventionally in-
cludes additional aspects such as correct interpreta-
tion of the pragmatics of the conversation, socially 
and culturally appropriate wording and content and 
knowledge of the subject matter under discussion. 
In this paper, we describe the design and valida-
tion of the Versant Arabic Test (VAT), a fully 
automated test of facility with spoken Modern 
Standard Arabic (MSA). Focusing on facility 
rather than communication-based oral proficiency 
enables the creation of an efficient yet informative 
automated test of listening and speaking ability. 
The automated test can be administered over the 
telephone or on a computer in approximately 17 
minutes. Despite its much shorter format and con-
strained tasks, test-taker scores on the VAT 
1
strongly correspond to their scores from an ILR 
Oral Proficiency Interview. 
The paper is structured as follows: After re-
viewing related work, we describe Modern Stan-
dard Arabic and introduce the test construct (i.e., 
what the test is intended to measure) in detail (Sec-
tion 3). We then describe the structure and devel-
opment of the VAT in Section 4 and present 
evidence for its reliability and validity in Section 5. 
2 Related Work 
The use of automatic speech recognition appeared 
earliest in pronunciation tutoring systems in the 
field of language learning. Examples include SRI's 
AUTOGRADER (Bernstein et al, 1990), the CMU 
FLUENCY system (Eskenazi, 1996; Eskenazi & 
Hansma, 1998) and SRI's commercial EduSpeak 
system (Franco et al, 2000). In such systems, 
learner speech is typically evaluated by comparing 
features like phone duration, spectral characteris-
tics of phones and rate-of-speech to a model of 
native speaker performances. Systems evaluate 
learners? pronunciation and give some feedback. 
Automated measurement of more comprehen-
sive speaking and listening ability was first re-
ported by Townshend et al (1998), describing the 
early PhonePass test development at Ordinate. The 
PhonePass tests returned five diagnostic scores, 
including reading fluency, repeat fluency and lis-
tening vocabulary. Ordinate?s Spoken Spanish Test 
also included automatically scored passage re-
tellings that used an adapted form of latent seman-
tic analysis to estimate vocabulary scores.  
More recently at ETS, Zechner et al (2007) de-
scribe experiments in automatic scoring of test-
taker responses in a TOEFL iBT practice environ-
ment, focusing mostly on fluency features. Zechner 
and Xi (2008) report work on similar algorithms to 
score item types with varying degrees of response 
predictability, including items with a very re-
stricted range of possible answers (e.g., reading 
aloud) as well as item types with progressively less 
restricted answers (e.g., describing a picture ? rela-
tively predictable, or stating an opinion ? less pre-
dictable). The scoring mechanism in Zechner and 
Xi (2008) employs features such as the average 
number of word types or silences for fluency esti-
mation, the ASR HMM log-likelihood for pronun-
ciation or a vector-based similarity measure to 
assess vocabulary and content. Zechner and Xi 
present correlations of machine scores with human 
scores for two tasks: r=0.50 for an opinion task and 
r=0.69 for picture description, which are compara-
ble to the modest human rater agreement figures in 
this data. 
Balogh and Bernstein (2007) describe opera-
tional automated tests of spoken Spanish and Eng-
lish that return an overall ability score and four 
diagnostic subscores (sentence mastery, vocabu-
lary, fluency, pronunciation). The tests measure a 
learner's facility in listening to and speaking a for-
eign language. The facility construct can be tested 
by observing performance on many kinds of tasks 
that elicit responses in real time with varying, but 
generally high, predictability. More predictable 
items have two important advantages: As with do-
main restricted speech recognition tasks in general, 
the recognition of response content is more accu-
rate, but a higher precision scoring system is also 
possible as an independent effect beyond the 
greater recognition accuracy. Scoring is based on 
features like word stress, segmental form, latency 
or rate of speaking for the fluency and pronuncia-
tion subscores, and on response fidelity with ex-
pected responses for the two content subscores.  
Balogh and Bernstein report that their tests are 
highly reliable (r>0.95 for both English and Span-
ish) and that test scores strongly predict human 
ratings of oral proficiency based on Common 
European Framework of Reference language abil-
ity descriptors (r=0.88 English, r=0.90 Spanish).  
3 Versant Arabic Test: Facility in Mod-
ern Standard Arabic 
We describe a fully operational test of spoken 
MSA that follows the tests described in Balogh and 
Bernstein (2007) in structure and method, and in 
using the facility construct. There are two impor-
tant dimensions to the test's construct: One is the 
definition of what comprises MSA, and the other 
the definition of facility. 
3.1 Target Language: Modern Standard 
Arabic 
Modern Standard Arabic is a non-colloquial lan-
guage used throughout the Arabic-speaking world 
for writing and in spoken communication within 
public, literary, and educational settings. It differs 
from the colloquial dialects of Arabic that are spo-
ken in the countries of North Africa and the Mid-
2
dle East in lexicon and in syntax, for example in 
the use of explicit case and mood marking. 
Written MSA can be identified by its specific 
syntactic style and lexical forms. However, since 
all short vowels are omitted in normal printed ma-
terial, the word-final short vowels indicating case 
and mood are provided by the speaker, even when 
reading MSA aloud. This means that a text that is 
syntactically and lexically MSA can be read in a 
way that exhibits features of the regional dialect of 
the speaker if case and mood vowels are omitted or 
phonemes are realized in regional pronunciations. 
Also, a speaker's dialectal and educational back-
ground may influence the choice of lexical items 
and syntactic structures in spontaneous speech. 
The MSA spoken on radio and television in the 
Arab world therefore shows a significant variation 
of syntax, phonology, and lexicon. 
3.2 Facility 
We define facility in spoken MSA as the ability to 
understand and speak contemporary MSA as it is 
used in international communication for broadcast, 
for commerce, and for professional collaboration. 
Listening and speaking skills are assessed by ob-
serving test-taker performance on spoken tasks that 
demand understanding a spoken prompt, and for-
mulating and articulating a response in real time. 
Success on the real-time language tasks de-
pends on whether the test-taker can process spoken 
material efficiently. Automaticity is an important 
underlying factor in such efficient language proc-
essing (Cutler, 2003). Automaticity is the ability to 
access and retrieve lexical items, to build phrases 
and clause structures, and to articulate responses 
without conscious attention to the linguistic code 
(Cutler, 2003; Jescheniak et al, 2003; Levelt, 
2001). If processing is automatic, the lis-
tener/speaker can focus on the communicative con-
tent rather than on how the language code is 
structured. Latency and pace of the spoken re-
sponse can be seen as partial manifestation of the 
test-taker?s automaticity.  
Unlike the oral proficiency construct that coor-
dinates with the structure and scoring of OPI tests, 
the facility construct does not extend to social 
skills, higher cognitive functions (e.g., persuasion), 
or world knowledge. However, we show below 
that test scores for language facility predict almost 
all of the reliable variance in test scores for an in-
terview-based test of language and communication.  
4 Versant Arabic Test 
The VAT consists of five tasks with a total of 69 
items. Four diagnostic subscores as well as an 
overall score are returned. Test administration and 
scoring is fully automated and utilizes speech 
processing technology to estimate features of the 
speech signal and extract response content. 
4.1 Test Design 
The VAT items were designed to represent core 
syntactic constructions of MSA and probe a wide 
range of ability levels. To make sure that the VAT 
items used realistic language structures, texts were 
adapted from spontaneous spoken utterances found 
in international televised broadcasts with the vo-
cabulary altered to contain common words that a 
learner of Arabic may have encountered.  
Four educated native Arabic speakers wrote the 
items and five dialectically distinct native Arabic 
speakers (Arabic linguist/teachers) independently 
reviewed the items for correctness and appropri-
ateness of content.  Finally, fifteen educated native 
Arabic speakers (eight men and seven women) 
from seven different countries recorded the vetted 
items at a conversational pace, providing a range 
of native accents and MSA speaking styles in the 
item prompts.  
4.2 Test Tasks and Structure 
The VAT has five task types that are arranged in 
six sections (Parts A through F): Readings, Repeats 
(presented in two sections), Short Answer Ques-
tions, Sentence Builds, and Passage Retellings. 
These item types provide multiple, fully independ-
ent measures that underlie facility with spoken 
MSA, including phonological fluency, sentence 
construction and comprehension, passive and ac-
tive vocabulary use, and pronunciation of rhythmic 
and segmental units. 
Part A: Reading (6 items) In this task, test-
takers read six (out of eight) printed sentences, one 
at a time, in the order requested by the examiner 
voice. Reading items are printed in Arabic script 
with short vowels indicated as they would be in a 
basal school reader. Test-takers have the opportu-
nity to familiarize themselves with the reading 
items before the test begins. The sentences are 
relatively simple in structure and vocabulary, so 
they can be read easily and fluently by people edu-
3
cated in MSA.  For test-takers with little facility in 
spoken Arabic but with some reading skills, this 
task provides samples of pronunciation and oral 
rea
rly 
aut
the 
dem
                                                          
ding fluency. 
Parts B and E: Repeats (2x15 items) Test-
takers hear sentences and are asked to repeat them 
verbatim. The sentences were recorded by native 
speakers of Arabic at a conversational pace. Sen-
tences range in length from three words to at most 
twelve words, although few items are longer than 
nine words. To repeat a sentence longer than about 
seven syllables, the test-taker has to recognize the 
words as produced in a continuous stream of 
speech (Miller & Isard, 1963). Generally, the abil-
ity to repeat material is constrained by the size of 
the linguistic unit that a person can process in an 
automatic or nearly automatic fashion. The ability 
to repeat longer and longer items indicates more 
and more advanced language skills ? particula
omaticity with phrase and clause structures.  
Part C: Short Answer Questions (20 items) 
Test-takers listen to spoken questions in MSA and 
answer each question with a single word or short 
phrase. Each question asks for basic information or 
requires simple inferences based on time, se-
quence, number, lexical content, or logic. The 
questions are designed not to presume any special-
ist knowledge of specific facts of Arabic culture or 
other subject matter. An English example1 of a 
Short Answer Question would be ?Do you get milk 
from a bottle or a newspaper?? To answer the 
questions, the test-taker needs to identify the words 
in phonological and syntactic context, infer 
and proposition and formulate the answer. 
Part D: Sentence Building (10 items) Test-
takers are presented with three short phrases. The 
phrases are presented in a random order (excluding 
the original, naturally occurring phrase order), and 
the test-taker is asked to respond with a reasonable 
sentence that comprises exactly the three given 
phrases. An English example would be a prompt of 
?was reading - my mother - her favorite maga-
zine?, with the correct response: ?My mother was 
reading her favorite magazine.? In this task, the 
test-taker has to understand the possible meanings 
of each phrase and know how the phrases might be 
combined with the other phrasal material, both 
with regard to syntax and semantics. The length 
and complexity of the sentence that can be built is 
 (e.g., a syllable, a word 
or 
ly, 
 scored in this test. 
e within 
mpleted. 
 of facility with spoken MSA. The sub-
sc s
? 
 phrases and clauses in 
? 
ontext and 
? 
tructing, reading and re-
? 
 in a native-like manner 
1 See Pearson (2009) for Arabic example items. 
constrained by the size of the linguistic units with 
which the test-taker represents the prompt phrases 
in verbal working memory
a multi-word phrase). 
Part F: Passage Retelling (3 items) In this fi-
nal task, test-takers listen to a spoken passage 
(usually a story) and then are asked to retell the 
passage in their own words. Test-takers are en-
couraged to retell as much of the passage as they 
can, including the situation, characters, actions and 
ending. The passages are from 19 to 50 words 
long.  Passage Retellings require listening compre-
hension of extended speech and also provide addi-
tional samples of spontaneous speech. Current
this task is not automatically
4.3 Test Administration 
Administration of the test takes about 17 minutes 
and the test can be taken over the phone or via a 
computer. A single examiner voice presents all the 
spoken instructions in either English or Arabic and 
all the spoken instructions are also printed verba-
tim on a test paper or displayed on the computer 
screen. Test items are presented in Arabic by na-
tive speaker voices that are distinct from the exam-
iner voice. Each test administration contains 69 
items selected by a stratified random draw from a 
large item pool. Scores are available onlin
a few minutes after the test is co
4.4 Scoring Dimensions 
The VAT provides four diagnostic subscores that 
indicate the test-taker's ability profile over various 
dimensions
ore  are 
Sentence Mastery: Understanding, recalling,   
and producing MSA
complete sentences. 
Vocabulary: Understanding common words 
spoken in continuous sentence c
producing such words as needed. 
Fluency: Appropriate rhythm, phrasing and 
timing when cons
peating sentences. 
Pronunciation: Producing consonants, vow-
els, and lexical stress
in sentence context. 
4
The VAT also reports an Overall score, which 
is a weighted average of the four subscores (Sen-
, Vocabulary 20%, 
tion 20%). 
m was trained 
on 
ent 
val
onse networks for each 
ite
wo
 answers with ob-
ser
 the can-
did
linear 
mo
 Sentence 
Building items and Vocabulary is based on re-
n rt Answer Questions. 
inconsistent measure-
me
tence Mastery contributes 30%
Fluency 30%, and Pronuncia
4.5 Automated Scoring 
The VAT?s automated scoring syste
native and non-native responses to the test items 
as well as human ability judgments. 
Data Collection For the development of the 
VAT, a total of 246 hours of speech in response to 
the test items was collected from natives and learn-
ers and was transcribed by educated native speak-
ers of Arabic. Subsets of the response data were 
also rated for proficiency. Three trained native 
speakers produced about 7,500 judgments for each 
of the Fluency and the Pronunciation subscores (on 
a scale from 1-6, with 0 indicating missing data). 
The raters agreed well with one another at r?0.8 
(r=0.79 for Pronunciation, r=0.83 for Fluency). All 
test administrations included in the concurr
idation study (cf. Section 5 below) were ex-
cluded from the training of the scoring system. 
Automatic Speech Recognition Recognition is 
performed by an HMM-based recognizer built us-
ing the HTK toolkit (Young et al, 2000). Three-
state triphone acoustic models were trained on 130 
hours of non-native and 116 hours of native MSA 
speech. The expected resp
m were induced from the transcriptions of native 
and non-native responses. 
Since standard written Arabic does not mark 
short vowels, the pronunciation and meaning of 
written words is often ambiguous and words do not 
show case and mood markings. This is a challenge 
to Arabic ASR, since it complicates the creation of 
pronunciation dictionaries that link a word's sound 
to its written form. Words were represented with 
their fully voweled pronunciation (cf., Vergyri et 
al., 2008; Soltau et al, 2007).  We relied on hand-
corrected automatic diacritization of the standard 
written transcriptions to create fully-voweled 
rds from which phonemic representations were 
automatically created. 
The orthographic transcript of a test-taker utter-
ance in standard, unvoweled form is still ambigu-
ous with regard to the actual words uttered, since 
the same consonant string can have different mean-
ings depending on the vowels that are inserted. 
Moreover, the different words written in this way 
are usually semantically related, making them po-
tentially confusable for language learners. There-
fore, for system development, we transcribed 
words with full vowel marks whenever a vowel 
change would cause a change of meaning. This 
partial voweling procedure deviates from the stan-
dard way of writing, but it facilitated system-
internal comparison of target
ved test-taker utterances since the target pro-
nunciation was made explicit. 
Scoring Methods The Sentence Mastery and 
Vocabulary scores are derived from the accuracy 
of the test-taker's response (in terms of number of 
words inserted, deleted, or substituted by
ate), and the presence or absence of expected 
words in correct sequences, respectively. 
The Fluency and Pronunciation subscores are 
calculated by measuring the latency of the re-
sponse, the rate of speaking, the position and 
length of pauses, the stress and segmental forms of 
the words, and the pronunciation of the segments 
in the words within their lexical and phrasal con-
text. The final subscores are based on a non-linear 
combination of these features. The non-
del is trained on feature values and human 
judgments for native and non-native speech. 
Figure 1 shows how each subscore draws on re-
sponses from the different task types to yield a sta-
ble estimate of test-taker ability. The Pronunciation 
score is estimated from responses to Reading, Re-
peat and Sentence Build items. The Fluency score 
uses the same set of responses as for Pronuncia-
tion, but a different set of acoustic features are ex-
tracted and combined in the score. Sentence 
Mastery is derived from Repeat and
spo ses to the Sho
5 Evaluation 
For any test to be meaningful, two properties are 
crucial: Reliability and validity. Reliability repre-
sents how consistent and replicable the test scores 
are. Validity represents the extent to which one can 
justify making certain inferences or decisions on 
the basis of test scores. Reliability is a necessary 
condition for validity, since 
nts cannot support inferences that would justify 
real-world decision making. 
To investigate the reliability and the validity of 
the VAT, a concurrent validation study was con-
ducted in which a group of test-takers took both 
5
the VAT and the ILR OPI.  If the VAT scores are 
parable t  traditional 
his will be a 
tive functioning in the target language. 
Th
ning Arabic 
co .S., and at least 11 were gradu-
ter for Arabic Studies Abroad 
between one rater and 
the
e taker took the VAT twice, we can 
                                                          
com o scores from a reliable
measure of oral proficiency in MSA, t
piece of evidence that the VAT indeed captures 
important aspects of test-takers' abilities in using 
spoken MSA. 
As additional evidence to establish the validity 
of the VAT, we examined the performance of the 
native and non-native speaker groups. Since the 
test claims to measure facility in understanding and 
speaking MSA, most educated native speakers 
should do quite well on the test, whereas the scores 
of the non-native test-takers should spread out ac-
cording to their ability level. Furthermore, one 
would also expect that educated native speakers 
would perform equally well regardless of specific 
national dialect backgrounds and no important 
score differences among different national groups 
of educated native speakers should be observed.  
5.1 Concurrent Validation Study 
ILR OPIs.  The ILR Oral Proficiency Interview is 
a well-established test of spoken language per-
formance, and serves as the standard evaluation 
tool used by United States government agencies 
(see www.govtilr.org). The test is a structured in-
terview that elicits spoken performances that are 
graded according to the ILR skill levels. These 
levels describe the test-taker?s ability in terms of 
communica
e OPI test construct is therefore different from 
that of the VAT, which measures facility with spo-
ken Arabic, and not communicative ability, as 
such. 
Concurrent Sample. A total of 118 test-takers 
(112 non-natives and six Arabic natives) took two 
VATs and two ILR OPIs. Each test-taker com-
pleted all four tests within a 15 day window. The 
mean age of the test-takers was 27 years old (SD = 
7) and the male-to-female split was 60-to-58. Of 
the non-native speakers in this concurrent testing 
sample, at least 20 test-takers were lear
at a llege in the U
ates from the Cen
program. Nine test-takers were recruited at a lan-
guage school in Cairo, Egypt, and the remainder 
were current or former students of Arabic recruited 
in the US. 
Seven active government-certified oral profi-
ciency interviewers conducted the ILR OPIs over 
the telephone. Each OPI was administered by two 
interviewers who submitted the performance rat-
ings independently after each interview. The aver-
age inter-rater correlation 
 average score given by the other two raters 
administering the same test-taker's other interview 
was 0.90. 
The test scores used in the concurrent study are 
the VAT Overall score, reported here in a range 
from 10 to 90, and the ILR OPI scores with levels 
{0, 0+, 1, 1+, 2, 2+, 3, 3+, 4, 4+, 5}2. 
5.2 Reliability 
Sinc each test-
estimate the VAT?s reliability using the test-retest 
method (e.g., Crocker & Algina, 1986: 133). The 
2 All plus ratings (e.g., 1+, 2+, etc) were converted with 0.5 
(e.g, 1.5, 2.5, etc) in the analysis reported in this paper. 
Figure 1: Relation of su
 
bscores to item types. 
6
correlation between the scores from the first ad-
ministration and the scores from the second ad-
mi
 are reliable at r=0.91 
(th
ased test of oral proficiency in 
M .
the V  MSA 
sp k
h
with ILR OPI scores, despite the difference in con-
dict native performance)  
score distributions of test-taker 
nistration was found to be at r=0.97, indicating 
high reliability of the VAT test. The scores from 
one test administration explain 0.972=94% of the 
score variance in another test administration to the 
same group of test-takers. 
We also compute the reliability of the ILR OPI 
scores for each test taker by correlating the aver-
ages of the ratings for each of the two test admini-
strations. The OPI scores
us 83% of the variance in the test scores are 
shared by the scores of another administration). 
This indicates that the OPI procedure implemented 
in the validation study was relatively consistent. 
5.3 Validity 
Evidence here for VAT score validity comes from 
two sources: the prediction of ILR OPI scores (as-
sumed for now to be valid) and the performance 
distribution of native and non-native test takers. 
Prediction of ILR OPI Test Scores.  For the 
comparison of the VAT to the ILR OPI, a scaled 
average OPI score was computed for each test-
taker from all the available ILR OPI ratings. The 
scaling was performed using a computer program, 
FACETS, which takes into account rater severity 
and test-taker ability and therefore produces a 
fairer estimate than a simple average (Linacre et 
al., 1990; Linacre, 2003).  
Figure 2 is a scatterplot of the ILR OPI scores 
and VAT scores for the concurrent validation sam-
ple (N=118). IRT scaling of the ILR scores allows 
a mapping of the scaled OPI scores and the VAT 
scores onto the original OPI levels, which are 
given on the inside of the plot axes. The correlation 
coefficient of the two test scores is r=0.87. This is 
roughly in the same range as both the ILR OPI re-
liability and the average ILR OPI inter-rater corre-
lation. The test scores on the VAT account for 76% 
of the variation in the ILR OPI scores (in contrast 
to 83% accounted for by another ILR OPI test ad-
ministration and 81% accounted for by one other 
ILR OPI interviewer). 
The VAT accounts for most of the variance in 
the interview-b
SA  This is one form of confirming evidence that 
AT captures important aspects of
ea ing and listening ability. 
T e close correspondence of the VAT scores 
struct, may come about because candidates easily 
transfer basic social and communicative skills ac-
quired in their native language, as long as they are 
able to correctly and efficiently process (i.e., com-
prehend and produce) the second language. Also, 
highly proficient learners have most likely ac-
quired their skills at least to some extent in social 
interaction with native speakers of their second 
language and therefore know how to interact ap-
propriately. 
Group Performance.  Finally, we examine the 
score distributions for different groups of test-
takers to investigate whether three basic expecta-
tions are met:  
? Native speakers all perform well, while non-
natives show a range of ability levels 
? Non-native speakers spread widely across 
the scoring scale (the test can distinguish 
well between a range of non-native ability   
levels)  
? Native speakers from different countries per-
form similarly (national origin does not pre-
We compare the 
groups in the training data set, which contains 
1309 native and 1337 non-native tests. For each 
test in the data set, an Overall score is computed by 
the trained scoring system on the basis of the re-
corded responses. Figure 3 presents cumulative 
distribution functions of the VAT overall scores, 
showing for each score which percentage of test-
takers performs at or below that level. This figure 
compares two speaker groups: Educated native 
speakers of Arabic and learners of Arabic. The 
Figure 2: Test-takers' ILR OPI scores as a function 
of VAT scores (r=0.87; N=118). 
 
7
score distributions of the native speakers and the 
learner sample are clearly different. For example, 
fewer than 5% of the native speakers score below  
70, while fewer than 10% of the learners score 
above 70. Further, the shape of the learner curve 
bution of scores, suggesting 
that the VAT discriminates well in the range of 
. The Mo-
ccan speakers are slightly separate from the other 
native speakers, but only a negligible number of 
them scores lower than 70, a score that less than 
10% of learners achieve.  This finding supports the 
notion that the VAT scores reflect a speaker's facil-
ity in spoken MSA, irrespective of the speaker's 
country of origin. 
6 Conclusion 
We have presented an automatically scored test of 
facility with spoken Modern Standard Arabic 
(MSA). The test yields an ability profile over four 
subscores, Fluency and Pronunciation (manner-of-
speaking) as well as Sentence Mastery and Vo-
cabulary (content), and generates a single Overall 
score as the weighted average of the subscores. We 
have presented data from a validation study with 
native and non-native test-takers that shows the 
VAT to be highly reliable (test-retest r=0.97). We 
also have presented validity evidence for justifying 
the use of VAT scores as a measure of oral profi-
ciency in MSA.  While educated native speakers of 
Arabic can score high on the test regardless of their 
country of origin because they all possess high fa-
cility in spoken MSA, learners of Arabic score dif-
ferently according to their ability levels; the VAT 
test scores account for most of the variance in the 
interview-based ILR OPI for MSA, indicating that 
the VAT captures a major feature of oral profi-
ciency.  
In summary, the empirical validation data sug-
gests that the VAT can be an efficient, practical 
alternative to interview-based proficiency testing 
in many settings, and that VAT scores can be used 
to inform decisions in which a person?s listening 
and speaking ability in Modern Standard Arabic 
should play a part. 
Acknowledgments 
The reported work was conducted under contract 
W912SU-06-P-0041 from the U.S. Dept. of the 
Army.  The authors thank Andy Freeman for pro-
viding diacritic markings, and to Waheed Samy, 
Naima Bousofara Omar, Eli Andrews, Mohamed 
Al-Saffar, Nazir Kikhia, Rula Kikhia, and Linda 
Istanbulli for support with item development and 
data collection/transcription in Arabic.  
 
Figure 4: Score distributions for native speakers 
of different countries of origin. 
indicates a wide distri
abilities of learners of Arabic as a foreign lan-
guage.  
Figure 4 is also a cumulative distribution func-
tions, but it shows score distributions for native 
speakers by country of origin (showing only coun-
tries with at least 40 test-takers). The curves for 
Egyptian, Syrian, Iraqi, Palestinian, Saudi and 
Yemeni speakers are indistinguishable
ro
Figure 3: Score distributions for native and non-
native speakers. 
8
References 
Jennifer Balogh and Jared Bernstein. 2007. Workable 
models of standard performance in English and 
Spanish. In Y. Matsumoto, D. Oshima, O. Robinson, 
and P. Sells, editors, Diversity in Language: Per-
spectives and Implications (CSLI Lecture Notes, 
176), 271-292. CSLI, Stanford, CA. 
Jared Bernstein and Isabella Barbier. 2001. Design and 
development parameters for a rapid automatic 
screening test for prospective simultaneous inter-
preters. Interpreting, International Journal of Re-
search and Practice in Interpreting, 5(2): 221-238. 
Jared Bernstein, Michael Cohen, Hy Murveit, Dmitry 
Rtischev, and Mitch Weintraub. 1990. Automatic 
evaluation and training in English pronunciation. In 
Proceedings of ICSLP, 1185-1188. 
Linda Crocker and James Algina. 1986. Introduction to 
Classical & Modern Test Theory. Harcourt Brace 
Jovanovich, Orland, FL. 
Anne Cutler. 2003. Lexical access. In L. Nadel, editor, 
Encyclopedia of Cognitive Science, volume 2, pp. 
858-864. Nature Publishing Group. 
Maxine Eskenazi. 1996. Detection of foreign speakers? 
pronunciation errors for second language training ? 
preliminary results. In Proceedings of ICSLP ?96. 
Maxine Eskenazi and Scott Hansma. 1998. The fluency 
pronunciation trainer. In Proceedings of the STiLL 
Workshop.  
Horacio Franco, Victor Abrash, Kristin Precoda, Harry 
Bratt, Raman Rao, John Butzberger, Romain Ross-
ier, and Federico Cesar. 2000. The SRI EduSpeak 
system: Recognition and pronunciation scoring for 
language learning. In Proceedings of InSTiLL, 123-
128. 
J?rg Jescheniak, Anja Hahne, and Herbert Schriefers. 
2003. Information flow in the mental lexicon during 
speech planning: Evidence from event-related poten-
tials. Cognitive Brain Research, 15(3):858-864. 
Willem Levelt. 2001. Spoken word production: A the-
ory of lexical access. Proceedings of the National 
Academy of Sciences, 98(23):13464-13471. 
John Linacre. 2003. FACETS Rasch measurement com-
puter program. Winstep, Chicago, IL.  
John Linacre, Benjamin Wright, and Mary Lunz. 1990. 
A Facets model for judgmental scoring. Memo 61. 
MESA Psychometric Laboratory. University of Chi-
cago. Retrieved April 14, 2009, from http:// 
http://www.rasch.org/memo61.htm. 
George Miller and Stephen Isard. 1963. Some percep-
tual consequences of linguistic rules. Journal of 
Verbal Learning and Verbal Behavior, 2:217-228. 
 
 
 
Pearson. 2009. Versant Arabic test ? test description 
and validation summary. Pearson. Retrieved April 
14, 2009, from 
http://www.ordinate.com/technology/VersantArabic
TestValidation.pdf. 
Hagen Soltau, George Saon, Daniel Povy, Lidia Mangu, 
Brian Kingsbury, Jeff Kuo, Mohamed Omar, and 
Geoffrey Zweig. 2007. The IBM 2006 GALE Arabic 
ASR system. In Proceedings of ICASSP 2007, 349-
352. 
Brent Townshend, Jared Bernstein, Ognjen Todic & 
Eryk Warren. 1998. Estimation of Spoken Language 
Proficiency. In STiLL: Speech Technology in Lan-
guage Learning, 177-180.  
Dimitra Vergyri, Arindam Mandal, Wen Wang, An-
dreas Stolcke, Jing Zheng, Martin Graciarena, David 
Rybach, Christian Gollan, Ralf Schl?ter, Karin 
Kirchhoff, Arlo Faria, and Nelson Morgan. 2008. 
Development of the SRI/Nightingale Arabic ASR 
system. In Proceedings of Interspeech 2008, 1437-
1440. 
Steve Young, Dan Kershaw, Julian Odell, Dave Ol-
lason, Valtcho Valtchev, and Phil Woodland. 2000. 
The HTK Book Version 3.0. Cambridge University 
Press, Cambridge, UK. 
Klaus Zechner and Xiaoming Xi. 2008. Towards auto-
matic scoring of a test of spoken language with het-
erogeneous task types. In Proceedings of the Third 
Workshop on Innovative Use of NLP for Building 
Educational Applications, 98-106. 
Klaus Zechner, Derrick Higgins, and Xiaoming Xi. 
2007. SpeechRater?: A construct-driven approach 
to score spontaneous non-native speech. In Proceed-
ings of the Workshop of the ISCA SIG on Speech and 
Language Technology in Education. 
9
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46?55,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
 
Performance of Automated Scoring for Children?s Oral Reading 
 
 
Ryan Downey,  David Rubin,  Jian Cheng,  Jared Bernstein 
Pearson Knowledge Technologies 
299 S. California Ave. 
Palo Alto, California 94306 
 
Ryan.Downey@Pearson.com 
 
  
 
Abstract 
For adult readers, an automated system can 
produce oral reading fluency (ORF) scores 
(e.g., words read correctly per minute) that are 
consistent with scores provided by human 
evaluators (Balogh et al, 2005, and in press).  
Balogh?s work on NAAL materials used 
passage-specific data to optimize statistical 
language models and scoring performance.  The 
current study investigates whether or not an 
automated system can produce scores for young 
children?s reading that are consistent with 
human scores.  A novel aspect of the present 
study is that text-independent rule-based 
language models were employed (Cheng and 
Townshend, 2009) to score reading passages 
that the system had never seen before.  Oral 
reading performances were collected over cell 
phones from 1st, 2nd, and 3rd grade children (n = 
95) in a classroom environment. Readings were 
scored 1) in situ by teachers in the classroom, 
2) later by expert scorers, and 3) by an 
automated system. Statistical analyses provide 
evidence that machine Words Correct scores 
correlate well with scores provided by teachers 
and expert scorers, with all (Pearson?s 
correlation coefficient) r?s > 0.98 at the 
individual response level, and all r?s > 0.99 at 
the ?test? level (i.e., median scores out of 3). 
1 Introduction 
Oral reading fluency (ORF), defined as ?the ability 
to read a text quickly, accurately, and with proper 
expression? (National Reading Panel, 2000; p. 
3.5), is a reflection of readers? decoding ability.  
Skilled readers can recognize words effortlessly 
(Rasinski and Hoffman, 2003), due to 
?automaticity? of processing (LaBerge and 
Samuels, 1974) whereby a reader?s attention is no 
longer focused on ?lower level? processing (e.g., 
letter to phoneme correspondence, word 
identification, etc.).  Instead, attention can be 
devoted to ?higher level? functions such as 
comprehension and expression (LaBerge and 
Samuels, 1974).  As a means of assessing general 
reading ability, oral reading fluency performance is 
also a predictor of student success in academic 
areas such as reading and math (e.g., Crawford, 
Tindal, and Stieber, 2001).  Oral reading fluency is 
one of the key basic skills identified in the Reading 
First initiative used to satisfy the standards of the 
No Child Left Behind Act (NCLB, 2001). 
Although oral reading fluency is comprised of 
several abilities, due to practical constraints the 
most commonly reported reflection of oral reading 
fluency is reading rate, specifically, the words read 
correctly per minute (WCPM).  Typically, ORF 
performance is measured by a classroom teacher 
who sits alongside a student, marking and 
annotating ? in real time ? the student?s reading on 
a sheet of paper containing the passage to be read.  
Classroom testing is time-consuming and requires 
a teacher?s full attention.  In practice, teaching time 
is often sacrificed to ?testing time? to satisfy local 
and federal reporting standards (e.g., NCLB).  
ORF scoring guidelines are specific to particular 
publishers; teachers must undergo training to 
become familiar with these guidelines, and cost, 
availability, and quality of training varies.  Finally, 
despite good-faith attempts to score accurately, 
teachers may impose errors and inconsistencies in 
46
 scoring ORF performances due to unavoidable 
factors such as classroom distractions, varying 
experience with different accents/dialects, varying 
experience with scoring conventions, and 
differences in training, among others.   
To address the need for a rapid and reliable way 
to assess oral reading fluency, a growing body of 
research has supported the use of automated 
approaches.  Beginning with work by Bernstein et 
al. (1990) and Mostow et al (1994), prototype 
systems for automatic measurement of basic 
components of reading have appeared.  Recent 
projects have addressed finer event classification in 
reading aloud (Black, Tepperman, Lee, Price, and 
Narayanan, 2007), and word level reading 
(Tepperman et al, 2007), among others.  Research 
has increasingly focused on systems to score 
passage-level reading performances (e.g., Balogh 
et al, 2005; Zechner, Sabatini, and Chen, 2009; 
Cheng and Townshend, 2009).  Eskenazi (2009) 
presents a general historical perspective on speech 
processing applications in language learning, 
including reading. 
The present automated ORF assessment was 
developed to deliver and score tests of oral reading 
fluency, allowing teachers to spend less time 
testing and more time teaching, while at the same 
time improving score consistency across time and 
location.  Automated ORF tests are initiated by a 
click in a web-based class roster.  Once a test is 
initiated, a call is placed to a local phone number 
and the test begins when the phone is answered.  
Instructions presented through the handset direct 
the student to read passages out loud into the cell 
phone, and these readings are sent to the automated 
ORF system for processing and scoring. 
2 Present Study 
The scoring models used by the automated ORF 
test (see Method below) were originally developed 
based on adult readings, and then optimized on 
large sets of data collected from students reading 
passages produced by AIMSweb, a publisher of 
Reading Curriculum-Based Measurement (R-
CBM) oral reading fluency passages 
(www.aimsweb.com).  AIMSweb passages are 
leveled and normed across large samples of 
students.  Previous validation studies found that 
when the system was optimized using data from 
students reading AIMSweb passages, machine 
scores correlated with trained human expert score 
with r = 0.95 to 0.98, depending on the grade level 
of the student readers.  
The primary question that the present studies 
attempt to answer is whether the automated scoring 
system can score newly inserted content ? in this 
case, ORF passages offered by Sopris called 
?Dynamic Indicators of Basic Early Literacy 
Skills?, or DIBELS (www.dibels.com) ? accurately 
and at a high level of reliability.  This is an 
evaluation of text-independent Rule Based 
Language Models (RBLMs) that were developed 
with training data from other readers performing 
on other passages and then applied to the new 
passages.   
A secondary question of interest involves how 
different types of scorers may assign Words 
Correct scores differently.  Two groups of human 
scorers were recruited:  1) teachers who were 
recently trained in DIBELS scoring methods who 
would perform scoring in the classroom, and 2) 
expert scorers with the ability to score reading 
recordings carefully and at their convenience, 
without classroom distractions.  Answering the 
first part of the question involves comparing 
machine Words Correct scores to human scores 
when teachers make ratings in the classroom 
environment as the student reads into the phone.  
This analysis reveals if the machine and teachers 
produce systematically different scores when 
testing is performed in a ?live? classroom with the 
typical attentional demands placed on a teacher 
scoring an ORF passage.  Answering the second 
part of the question involves comparing machine 
Words Correct scores to a ?consensus?, or median 
Words Correct value, from expert scorers.  These 
three experts, with over 14 years of combined 
experience scoring DIBELS passages, listened to 
recordings of the same readings made in the 
classroom.  Because the recordings were digitally 
preserved in a database, the expert scorers were 
able to replay any part(s) of the recordings to 
determine whether each word was read correctly.  
The benefit of being able to replay recordings is 
that such scores obtained are, in theory, closer to 
capturing the ?truth? of the student?s performance, 
unaffected by biases or distractions encountered by 
scorers performing a ?live? rating.  
47
 2.1 Method 
2.1.1 Rule Based Language Models 
The scoring models used by the automated ORF 
system are RBLMs such as those described by 
Cheng and Townshend (2009). Such models out-
perform traditional n-gram language models 
(Cheng and Shen, 2010), in part by adding 
intuitively simple rules such as allowing a long 
silence as an alternative to a short pause after every 
word, leading to improvements in accuracy. Also, 
rules like those described by Cheng and 
Townshend (2009) consider much longer 
sequential dependencies. The basic idea for this 
kind of language model is that each passage gets a 
simple directed graph with a path from the first 
word to the last word. Different arcs are added to 
represent different common errors made by the 
readers, such as skipping, repeating, inserting, and 
substituting words. For each arc, a probability is 
assigned to represent the chance that the arc will be 
chosen.  Knowledge of performance on other 
readings produces linguistic rules, such as she can 
substitute for he, a single noun can replace a plural 
noun, the reader may skip from any place to the 
end, etc. All the rules used in RBLMs can be 
classified into five broad groups:  
1. skip/repeat rules 
2. rules using part-of-speech (POS) tagging 
information 
3. rules accommodating for insertion of 
partial words  
4. general word level rules 
5. hesitation and mouth noise rules  
A detailed analysis of the role of rules in RBLMs 
was described in Cheng and Shen (2010). 
The language rules are extrapolated from 
transcriptions of oral reading responses to passages 
using four base rules: any word substitutes for any 
word with a low probability; any word is inserted 
after any word with a low probability; any word is 
skipped with a low probability; any word is 
repeated immediately with a low probability. 
Following Cheng and Townshend (2009), the first 
two are the only rules that allow out-of-vocabulary 
words and their probabilities are fixed to the lowest 
level, so their arcs will never be traversed unless 
there is no other choice. 
General language model rules for reading can 
be inferred from clustering traversals of the basic 
models and proposing further rules that can be 
applied to new reading passages and used to infer 
underlying knowledge about the reading. Arcs are 
added to represent commonly observed non-
canonic readings. Further analysis of rule-firing 
details may provide diagnostic linguistic 
information about children?s reading habits that 
can be reported and analyzed. 
In the present automated scoring system, new 
passages are automatically tagged for part-of-
speech (POS) using the Penn Tree Tagger (Marcus, 
Santorini, and Marcinkiewicz, 1993).  POS tags 
allow specification of certain general rules based 
on linguistic properties, such as: 
? NN (noun, singular or mass) can become NNS 
(noun, plural);  
? VBZ (verb, 3rd person singular present) can 
become VBP (verb, non-3rd person singular 
present); and so on.  
These patterns occur quite frequently in real 
responses and can therefore be accounted for by 
rules. Sentence, clause, and end-of-line boundaries 
are tagged manually. Marked up passages are then 
inserted into the ORF scoring system, providing 
data regarding places in the reading that may result 
in pauses, hesitations, corrections, etc.  If the 
expected response to a reading passage is highly 
constrained, the system can verify the occurrence 
of the correct lexical content in the correct 
sequence.  It is expected that the system, using 
previously trained data coupled with the RBLMs 
from the newly inserted passages, will be able to 
produce Words Correct scores with high accuracy 
(i.e., consistent with human Words Correct scores). 
Here, we make a final note on the use of Words 
Correct instead of words correct per minute 
(WCPM), when WCPM is the most common 
measure for quantifying oral reading performance.  
The automated system presents students with a 60-
second recording window to read each passage, but 
it calculates a truer WCPM by trimming leading 
and trailing silence.  Human scorers simply 
reported the number of words correct, on the 
assumption that the reading time is the recording 
window duration.  Thus, Words Correct scores are 
the appropriate comparison values, with a fixed 60-
second nominal reading time. 
2.1.2 Participants 
A total of 95 students were recruited from the San 
Jose Unified School District in San Jose, 
48
 California.  The students were 20 first graders, 20 
second graders, and 55 third graders, all enrolled in 
a summer school program.  Students with known 
speech disorders were included in the study, as was 
one student with a hearing impairment.  Roughly 
half of the participants were male and half were 
female.  A number of English Language Learners 
are known to have been included in the sample, 
though language status was not recorded as a 
variable for this study.  It is not known whether 
any of the students had been diagnosed with 
reading disabilities.  
Four Teachers were trained to administer and 
score DIBELS ORF passages by an official 
DIBELS trainer, over the course of a two day 
training session.  All Teachers were reading 
experts or teachers with experience in reading 
education.  They were trained to navigate a web 
application that triggers delivery of tests over cell 
phones under classroom testing conditions.  
Evaluator qualifications are summarized in Table 
1. 
Evaluator Highest degree, or relevant certification 
Years 
assessing 
reading 
Teacher 1 MA Education 8 
Teacher 2 MA Education 7 
Teacher 3 Reading Credential 15 
Teacher 4 BA Education 12 
Expert 1 MS, Statistics 5 
Expert 2 EdS, Education 2 
Expert 3 MA Education 20 
Table 1.  Evaluator qualifications 
2.1.3 Procedure 
First, nine passages ? three for each of the three 
grades, presented together in a single test ? were 
drawn from the DIBELS Benchmark test materials.  
Each DIBELS passage was tagged for parts of 
speech and formatting (e.g., line breaks) and 
inserted into the automated scoring system.  Rule-
based language models were produced for each 
passage. 
During data collection, each student read the 
grade-appropriate DIBELS Benchmark test (3 
passages) into a cellular telephone in the 
classroom.  With three passages per student, this 
process yielded 285 individual reading 
performances.   
Once a test was initiated, Teachers allowed the 
test to run independently and scored manually 
alongside the student reading into the phone.  
According to standard DIBELS scoring 
conventions, the students were allowed to read 
each passage for one minute.  Teachers calculated 
and recorded the Words Correct score on a 
worksheet for each passage.  Teachers returned the 
annotated score sheets for analysis. 
Later, three Expert scorers logged in to a web-
based interface via the Internet, where they listened 
to the digitized recordings of the readings.  All 
three Expert scorers had extensive experience with 
DIBELS rating.  One Expert was the DIBELS 
trainer who provided the DIBELS training to the 
Teachers for this study.  Experts scored students? 
performance manually using score sheets with the 
instruction to use standard DIBELS scoring 
conventions.  Each Expert entered a Words Correct 
score for each passage using the web interface, and 
the score sheets were returned for analysis.   
2.1.4 Automated scoring 
Incoming spoken responses were digitally 
recorded and sent to a speech processing system 
that is optimized for both native and non-native 
speech.  Recognition was performed by an HMM-
based recognizer built using the HTK toolkit 
(Young, et al, 2000).  Acoustic models, 
pronunciation dictionaries, and expected-response 
networks were developed in-house using data from 
previous training studies involving many 
thousands of responses.  The words, pauses, 
syllables, phones, and even some subphonemic 
events can be located in the recorded signal, and 
?words recognized? are compared with ?words 
expected? to produce a recognized response and 
word count. 
The acoustic models for the speech recognizer 
were developed using data from a diverse sample 
of non-native speakers of English.  In addition, 
recordings from 57 first-grade children were used 
to optimize the automated scoring system to 
accommodate for characteristics specific to young 
children?s voices and speech patterns.  These 
participants produced 136 usable, individual 
reading samples.  These samples were each rated 
by two expert human raters.  Using this final 
training set, the scoring models were refined to the 
point that the correlation between human and 
machine scoring was 0.97 for WCPM.  
49
 2.1.5 Human scoring 
During data preparation, it was noted that many 
of the teacher scores were several words longer 
than would be expected based on the machine 
scores.  Further investigation revealed that teachers 
would occasionally continue scoring after the one 
minute point at which the system stopped 
recording a passage, perhaps because they hadn?t 
heard the notification that the reading was 
complete.  A total of 31 out of 285 instances 
(~10.8%) were found where teachers continued 
scoring for more than 3 words beyond the 1 minute 
recording window, leading to artificially inflated 
Teacher scores.  This artifact of the testing 
apparatus/environment warranted making a careful 
correction, whereby all Teacher scores were 
adjusted to account for what the machine ?heard?.  
That is, words and errors which Teachers scored 
after the automated system stopped recording (i.e., 
to which the automated system did not have 
access) were subtracted from the original Teacher 
Words Correct scores.  All Teacher Words Correct 
scores reported hereafter are thus ?corrected?. 
For purposes of finding a ?consensus? Expert 
score, the median of the 3 expert human scores for 
each passage was obtained and is referred to as 
ExpertM in the following analyses.   
Nine readings from eight separate students 
received no scores from teachers.  Information was 
not provided by the teachers regarding why they 
failed to complete the scoring process for these 
readings.  However, we made the following 
observations based on the teachers? marked-up 
scoring sheets. For three readings, the teacher?s 
final score was blank when the student appeared to 
have skipped lines in the passage.  It is possible 
that, despite recent scoring training, the teacher 
was uncertain how to score skipped lines in the 
readings and left the final score blank pending 
confirmation.  For one reading, the teacher made a 
note that the system stopped recording well before 
one minute had expired because the child?s reading 
was too quiet to be picked up, and the teacher did 
not record the final score on the score sheet.  For 
one reading, the student did not hear the prompt to 
begin reading (confirmed by listening to the 
response recording) and therefore did not read the 
entire passage; the teacher did not enter a final 
score.  For the four remaining readings, the teacher 
annotated the performance but did not write down 
the final score for unclear reasons. 
We might have elected to fill in the teachers? 
final scores for these 9 readings prior to subjecting 
the data to analysis, especially in the cases where a 
teacher annotated the reading correctly on the 
score sheet but simply failed to record the final 
Words Correct score, perhaps due to oversight or 
not knowing how to handle unusual events (e.g., 
entire line of reading skipped).  Excluding such 
readings from the analysis ensured that the 
teachers? scores reflected ?their own? scoring ? 
including any errors they might make ? rather than 
our interpretation of what the Teachers probably 
would have written.  In addition, to maintain the 
most conservative approach, whenever a single 
reading passage from a student lacked a teacher?s 
score, all 3 of that student?s readings were 
excluded.  The decision to exclude all readings 
from students with only a single passage missing 
was made because relevant analyses reported 
below involve reporting median scores, and a 
median score for students lacking one or two 
passage scores would not be possible.1  The final 
set of graded responses thus consisted of 261 
responses from 87 students.2 
2.2 Results  
2.2.1 Score Group Comparisons 
Words Correct scores from Teachers, ExpertM, and 
machine are displayed in Table 2.  Repeated 
measures ANOVA with Scorer Type (machine, 
Teacher, ExpertM) as the repeated measure and 
Score Group as the between-subjects factor 
revealed a main effect of group for the 261 
                                                          
1 The excluded 8 students produced 15 readings with all three 
(Machine, Teacher, Expert) scores.  Machine scores vs. 
Teacher scores and Machine scores vs. ExpertM scores for these 15 individual responses yielded correlations of 
(Pearson?s) r = 0.9949 and 0.9956, respectively.  Thus, 
excluding these responses from the larger dataset is unlikely to 
have significantly affected the overall results. 
2 In production, such a system would not commit these errors 
of omission.  Readings that are unscorable for technical 
reasons can trigger a ?Median score not be calculated? 
message and request a teacher to manually score a recording 
or re-administer the assessment.  Also, anomalous 
performances where Words Correct on one passage is very 
different from Words Correct on the two other passages could 
return a message. 
50
 readings3, F(2, 520) = 9.912, p < .01, ?2 < .001.  
Post-hoc pairwise comparisons 4  showed that 
Words Correct scores from Teachers were higher 
on average than both the machine and ExpertM 
scores (higher by 1.559 and 0.923 words correct, 
respectively; both p?s < .05).  On the other hand, 
Machine and ExpertM scores did not differ 
significantly from each other (diff = 0.636).  
Although the ANOVA showed that the means 
In the above analysis were significantly different, 
the effect size was negligible: ?2 was = .0002, 
indicating that Score Group by itself accounted for 
less than 1% of the overall variance in scores.  
These results indicate that, for all 261 passages, the 
ExpertM and machine scores were statistically 
comparable (e.g., within 1 word correct of each 
other), while Teachers tended to assign slightly ? 
but not meaningfully ? higher scores, on average. 
Next, comparisons were made using the median 
value of each student?s three readings.  Median 
Words Correct scores for the 87 individual 
students were subjected to repeated measures 
ANOVA with the same factor (Scorer Group).  
Teachers? Words Correct scores were again higher 
than ExpertM scores (diff = 1.115) and Machine 
scores (diff = 0.851), but this was not statistically 
significant in the main analysis, F(2, 172) = 3.11, p 
> .05, ?2 < .001.  Machine Words Correct scores 
were, on average, 0.264 words higher than ExpertM 
scores, but this, too, was not statistically 
significant.  These results support the previous 
comparisons, in that machine scores fall well 
within ~1 word correct of scores from careful 
experts, while teachers tended to give scores of 
about 1 word correct higher than both experts and 
machine. 
2.2.2 Scorer performance 
To compare reliability, the Pearson?s Product 
Moment coefficient (r) was used to estimate the 
correlation between paired human and machine 
scores, and between pairs of human raters.  Two 
types of analyses are reported.  First, analyses of 
Words Correct scores were conducted across 
scorers.  Next, analyses were conducted on the 
basis of the median Words Correct score for each 
                                                          
3  For both ANOVAs, uncorrected degrees of freedom are 
reported but reported F values are corrected using Huynh-
Feldt estimates of sphericity. 
4 Using Bonferroni adjustment for multiple comparisons. 
student?s readings (i.e., the median score across all 
three passages).  This score reflects the ?real-life? 
score of DIBELS ORF tests because the median 
score is the one that is ultimately reported 
according to DIBELS scoring/reporting 
conventions.    
2.2.2.1.  Intra-rater reliability 
Each Teacher scored each reading once during the 
live grading; intra-rater reliability could thus not be 
 
 Words Correct 
Score 
Type 
261 readings 
Mean (SD) 
87 students  
Mean (SD) 
Teacher 84.3 (42.5) 84.0 (42.1) 
ExpertM 83.4 (42.3) 82.9 (41.8) 
Machine 82.8 (39.6) 83.2 (39.3) 
Table 2.  Mean Words Correct for all readings and 
all students. 
 
estimated for the Teacher group.  During Expert 
rating, a randomly selected 5% of the passages 
were presented again for rating to each scorer.  
Overall Expert intra-rater reliability was 0.9998, 
with intra-rater reliability scores for Expert 1, 
Expert 2, and Expert 3 at 0.9996, 1.0, and 1.0, 
respectively.  These results indicate that Expert 
human scorers are extremely consistent when 
asked to provide Words Correct scores for reading 
passages when given the opportunity to listen to 
the passages at a careful, uninterrupted pace.  The 
automated scoring system would produce the exact 
same score (reliability = 1.0) every time it scored 
the same recordings, making its reliability 
comparable. 
2.2.2.2.  Inter-rater reliability 
Pearson?s r was used to estimate the inter-rater 
reliability.  All three Experts scored all passages, 
whereas any particular Teacher scored only a 
subset of the passages; thus, the Teacher?s score 
was used without consideration of which teacher 
provided the score.  Inter-rater reliability results 
are summarized in Table 3.   
 
 
 
 
 
 
 
51
 Reliability  (N = 261) 
 Teacher Expert 1 Expert 2 
Expert 1 0.998   
Expert 2 0.999 0.999  
Expert 3 0.998 0.999 0.999 
Table 3.  Inter-rater reliability estimates for Expert 
scorers. 
 
To provide a measure of a ?consensus? expert 
score, the median score from all 3 Experts was 
derived for each passage, and then compared with 
the Teacher score.  This comparison (Teacher vs. 
ExpertM) yielded a reliability of 0.999, p < .01.  As 
shown in Table 3, all inter-rater reliability 
estimates are extremely high, indicating, in part, 
that teachers in the classroom produce scores that 
do not differ systematically from those given by 
careful experts.   
2.2.3 Human-machine performance 
Pearson?s r was computed to estimate the 
correlations.  The different scorer groups (i.e., 
ExpertM, Teacher, and Machine) provided similarly 
consistent scoring, as evidenced by high 
correlations between scores from the three groups.  
These correlations were maintained even when 
data were broken down into individual grades.  
Table 4 reveals correlations between Words 
Correct scores provided by all 3 scoring groups, 
for each grade individually, for all three grades 
combined, and finally for the median scores for all 
87 students. 
 
Grade level (N) Machine ~ Teacher 
Machine ~ 
ExpertM 
Teacher ~ 
ExpertM 
1st grade (54) 0.990 0.990 0.996 
2nd grade (60) 0.990 0.991 0.999 
3rd grade (147) 0.964 0.962 0.997 
Grades 1-3 (261) 0.989 0.988 0.999 
Only medians 87 0.994 0.994 0.999 
Table 4.  Correlations between Words Correct scores 
by Experts, Teachers, and machine. 
 
All correlations are 0.96 or higher.  Correlations 
are highest between Teacher and ExpertM, but 
correlations between machine and both human 
groups are consistently 0.96 or above.  The 
relatively lower correlations between human and 
machine scores seen in the third grade data may be 
attributed in large part to two outliers noted in the 
Figures below.  If these outliers are excluded from 
the analysis, both correlations between human and 
machine scores in the third grade rise to 0.985.  
(See below for discussion of these outliers.) 
2.2.4.1.  Teacher vs. Machine performance 
Pearson?s r was used to estimate the correlation 
between Teacher and Machine scores.  First, the 
Teacher-generated Words Correct score and 
Machine-generated Words Correct scores were 
obtained for each of the 261 individual recordings, 
where the correlation was found to be r = 0.989, p 
< .01.   
 
Words Correct scores
Teacher vs. Machine 
r = 0.989
n = 261
0
20
40
60
80
100
120
140
160
180
0 20 40 60 80 100 120 140 160 180
Teacher WC
Ma
ch
ine
 W
C
 Figure 1.  Words Correct (WC) scores from Teachers 
and Machine; response level (n = 261 responses) 
 
Figure 1 shows a small number of outliers in the 
scatterplot (circled in red).  One outlier (human = 
3, machine = 21) came from a student whose low 
level of reading skill required him to sound out the 
letters as he read; machine scores were high for all 
3 recordings from this reader.  One outlier (human 
= 21, machine = 10) occurred because the reader 
had an unusually high pitched voice quality which 
posed a particular challenge to the recognizer.  
Two outliers (human = 141, machine = 76; human 
= 139, machine = 104) suffered from a similar 
recording quality-based issue whereby only some 
of the words were picked up by the system because 
the student read rapidly but quietly, making it 
difficult for the system to consistently pick up their 
voices.  That is, for these calls the Teacher was 
close enough to hear the students? entire reading 
52
 but the machine picked up only some of the words 
due to distance from the telephone handset.5   
Next, median Words Correct scores for each 
student were computed.  Median scores derived 
from machine and Teachers correlated at 0.994, p 
< .01 for the 87 students.  These scores are 
presented in Figure 2. 
 
Words Correct scores
Teacher vs. Machine
r = 0.994
n = 87
0
30
60
90
120
150
180
0 30 60 90 120 150 180
Teacher WC
Ma
ch
ine
 W
C
 
Figure 2.  Words Correct (WC) scores from Teachers 
and Machine scoring at the reader level (n = 87). 
 
Figure 2 shows that some of the outliers visible in 
the individual recording data disappear when the 
median score is computed for each student?s 
reading performance, as would be expected.     
2.2.4.2. Expert vs. Machine performance 
The median of the 3 expert human scores for each 
passage (ExpertM) was compared to the Machine 
score.  The correlation between machine-generated 
Words Correct scores and ExpertM-generated 
Words Correct scores was 0.988, p < .01, for the 
261 individual readings, and 0.999, p < .01, for the 
median (student-level) scores.  These results are 
displayed in Figure 3. 
Figure 3 shows that two notable outliers present 
in the Teacher analysis were also present in the 
ExpertM analysis.  This may be due to the fact that 
while the recordings were of a low enough volume 
to present a challenge to the automated scoring 
system, they were of a sufficient quality for expert 
human scorers to ?fill in the blanks? by listening 
                                                          
5 In a production version, these recordings would return an 
instruction to re-administer the readings with better recording 
conditions or to score the recordings. 
repeatedly (e.g., with the ability to turn up the 
volume), and in some cases giving the student 
credit for a word spoken correctly even though 
they, the scorers, were not completely confident of 
having heard every portion of the word correctly.  
Though conjectural, it is reasonable to expect that 
the human listeners were able to interpolate the 
words in a ?top down? fashion in a way that the 
machine was not.  
 
Words Correct scores
Expert (consensus) vs. Machine
r = 0.988
n = 261
0
20
40
60
80
100
120
140
160
180
0 20 40 60 80 100 120 140 160 180
Expert WC
Ma
ch
ine
 W
C
 
Words Correct scores
Expert (consensus) vs. Machine
r = 0.994
n = 87
0
30
60
90
120
150
180
0 30 60 90 120 150 180
Expert WC
Ma
ch
ine
 W
C
 
 Figure 3.  Words Correct (WC) Machine scores vs. 
Expert scores for all 261 individual responses 
(top) and for 87 students at test level (bottom).   
2.2.5. Scoring Precision 
It is reasonable to assume that careful expert 
scorers provide the closest possible representation 
of how a reading should be scored, particularly if 
the Expert score represents a ?consensus? of expert 
opinions.  Given the impracticality of having a 
team of experts score every passage read by a child 
53
 in the classroom, automated machine scoring 
might provide the preferred alternative if its scores 
can be shown to be consistent with expert scores.  
To explore the consistency between scores from 
Teachers and scores from the machine with scores 
provided by Experts, Teacher and Machine scores 
were compared against the median Expert score for 
each call using linear regression.   
The standard error of the estimate (SEE) for the 
two human groups was computed.  The SEE may 
be considered a measure of the accuracy of the 
predictions made for Teacher and Machine scores 
based on the (median, ?consensus?) Expert scores.  
Figure 4 below shows a scatterplot of the data, 
along with the R2 and SEE measures for both 
Teacher and machine scores based on ExpertM 
scores. 
Scores from Teachers and Machine produce very 
similar regression lines and coefficients of 
determination (R2 = 0.998 and 0.988 for Teachers 
and Machine, respectively).  The figure also shows 
that, compared with the Machine scores, Teachers? 
scores approximate the predicted ExpertMed scores 
more closely (SEE for Teachers = 1.80 vs. 4.25 for 
machine).  This disparity appears to be driven by 
diverging scores at the upper and lower end of the 
distribution, as might be expected due to relatively 
smaller numbers of scores at the ends of the 
distribution. 
 
Median Words Correct Scores
Teacher vs Expert ; Machine vs Expert
R2 = 0.988, S.E.E. = 4.25 
R2 = 0.998, S.E.E. = 1.80
n = 87
0
20
40
60
80
100
120
140
160
180
0 20 40 60 80 100 120 140 160 180
Expert WC
Te
ac
he
r/M
ac
hin
e W
C
 
Figure 4.  Median Words Correct scores from 
Machine (red squares) and Teachers (blue 
triangles) plotted against median Expert scores 
for 87 students.  S.E.E. = Standard error of 
estimate. 
 
3 Summary/Discussion 
Correlations between human- and machine-based 
Words Correct scores were found to be above 0.95 
for both individual reading passages and for 
median scores per student.  The machine scoring 
was consistent with human scoring performed by 
teachers following along with the readings in real 
time (r = 0.989), and was also consistent with 
human scoring when performed by careful expert 
scorers who had the ability to listen to recorded 
renditions repeatedly (r = 0.988).  Correlations 
were consistent with those between expert scorers 
(all r?s between 0.998 and 0.999) and between 
Teachers and Experts (r = 0.999 and 0.988, 
respectively).   
These results demonstrate that text-independent  
machine scoring of Words Correct for children?s 
classroom reading predicts human scores 
extremely well (almost always within a word or 
two).   
Acknowledgments 
The authors acknowledge useful feedback from the 
anonymous reviewers that improved this paper. 
54
 References  
Jennifer Balogh, Jared Bernstein, Jian Cheng & Brent Townshend. 
2005. Ordinate Scoring of FAN in NAAL Phase III: Accuracy 
Analysis. Ordinate Corporation: Menlo Park, California.   
Jennifer Balogh, Jared Bernstein, Jian Cheng, Alistair Van Moere, 
Brent Townshend, Masanori Suzuki.  In press. Validation of 
automated scoring of oral reading. Educational and Psychological 
Measurement.  
Jared Bernstein, Michael Cohen, Hy Murveit, Dmitry Rtischev, 
Dmitry, and Mitch Weintraub. 1990. Automatic evaluation and 
training in English pronunciation. In: Proc. ICSLP-90: 1990 
Internat. Conf. on Spoken Language Processing, Kobe, Japan, pp. 
1185?1188. 
Matthew Black, Joseph Tepperman, Sungbok Lee, Patti Price, and 
Shrikanth Narayanan.  2006.  Automatic detection and 
classification of disfluent reading miscues in young children?s 
speech for the purpose of assessment.  Proc. In 
INTERSPEECH/ICSLP, Antwerp, Belgium. 
Jian Cheng & Jianqiang Shen. 2010. Towards Accurate Recognition 
for Children's Oral Reading Fluency. IEEE-SLT 2010, 91-96. 
Jian Cheng & Brent Townshend. 2009.  A rule-based language model 
for reading recognition. SLaTE 2009. 
 
Lindy Crawford, Gerald Tindal, & Steve Stieber. 2001.  Using Oral 
Reading Rate to Predict Student Performance on Statewide 
Achievement Tests.  Educational Assessment, 7(4), 303-323. 
 
Maxine Eskanazi.  2009.  An overview of spoken language technology 
for education.  Speech Communication, 51, 832-844. 
 
David LaBerge & S. Jay Samuels.  1974. Toward a theory of 
automatic information processing in reading. Cognitive 
Psychology, 6(2), 293-323. 
Mitchell P. Marcus, Beatrice Santorini, & Mary Ann Marcinkiewicz. 
1993. Building a large annotated corpus of English: The Penn 
Treebank. Computational Linguistics 19(2):313-330. 
Jack Mostow, Steven F. Roth, Alexander G. Hauptmann, & Matthew 
Kane. 1994. A prototype reading coach that listens. In Proc. of 
AAAI-94, 785?792. 
National Institute of Child Health and Human Development, ?Report 
of the national reading panel. Teaching children to read: An 
evidence-based assessment of the scientific research literature on 
reading and its implications for reading instruction,? Tech. Rep. 
NIH Publication No. 00-4769, U.S. Government Printing Office, 
2000. 
Timothy V. Rasinski & . James V. Hoffman. 2003. Theory and 
research into practice: Oral reading in the school literacy 
curriculum. Reading Research Quarterly, 38, 510-522. 
Joseph Tepperman, Matthew Black, Patti Price, Sungbok Lee, Abe 
Kazemzadeh, Matteo Gerosa, Margaret Heritage, Abeer Always, 
and Shrikanth Narayanan.  2007.  A Bayesian network classifier 
for word-level reading assessment.  Proceedings of ICSLP, 
Antwerp, Belgium. 
Steve Young, D. Ollason, V. Valtchev, & Phil Woodland. 2002.  The 
HTK Book (for HTK Version 3.2). Cambridge University 
Engineering Department. 
Klaus Zechner, John, Sabatini, & Lei Chen. 2009.  Automatic scoring 
of children?s read-aloud text passages and word lists.  Proceedings 
of the NAACL-HLT Workshop on Innovative Use of NLP for 
Building Educational Applications. Boulder, Colorado. 
55
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 12?21,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automatic Assessment of the Speech of Young English Learners
Jian Cheng
1
, Yuan Zhao D?Antilio
1
, Xin Chen
1
, Jared Bernstein
2
1
Knowledge Technologies, Pearson, Menlo Park, California, USA
2
Tasso Partners LLC, Palo Alto, California, USA
jian.cheng@pearson.com
Abstract
This paper introduces some of the research
behind automatic scoring of the speak-
ing part of the Arizona English Language
Learner Assessment, a large-scale test now
operational for students in Arizona. Ap-
proximately 70% of the students tested are
in the range 4-11 years old. We cover the
methods used to assess spoken responses
automatically, considering both what the
student says and the way in which the stu-
dent speaks. We also provide evidence
for the validity of machine scores. The
assessments include 10 open-ended item
types. For 9 of the 10 open item types,
machine scoring performed at a similar
level or better than human scoring at the
item-type level. At the participant level,
correlation coefficients between machine
overall scores and average human overall
scores were: Kindergarten: 0.88; Grades
1-2: 0.90; Grades 3-5: 0.94; Grades 6-8:
0.95; Grades 9-12: 0.93. The average cor-
relation coefficient was 0.92. We include
a note on implementing a detector to catch
problematic test performances.
1 Introduction
Arizona English Language Learner Assessment
(AZELLA) (Arizona Department of Education,
2014) is a test administered in the state of Arizona
to all students from kindergarten up to grade 12
(K-12) who had been previously identified as En-
glish learners (ELs). AZELLA is used to place EL
students into an appropriate level of instructional
and to reassess EL students on an annual basis to
monitor their progress. AZELLA was originally
a fully human-delivered paper-pencil test cover-
ing four domains: listening, speaking, reading and
writing. The Arizona Department of Education
chose to automate the delivery and scoring of the
speaking parts of the test, and further decided that
test delivery via speakerphone would be the most
efficient and universally accessible mode of ad-
ministration. During the first field test (Nov. 7 -
Dec. 2, 2011) over 31,000 tests were administered
to 1st to 12th graders on speakerphones in Arizona
schools. A second field test in April 2012 deliv-
ered over 13,000 AZELLA tests to kindergarten
students. This paper reports research results based
on analysis of data sets from the 44,000 students
tested in these two administrations.
2 AZELLA speaking tests
AZELLA speaking tests are published in five
stages (Table 1), one for each of five grade ranges
or student levels. Each stage has four fixed test
forms. Table 1 presents the total number of field
tests delivered for each stage, or level.
Table 1: Stages, grades, and number of field tests
Stage I II III IV V
Grade K 1-2 3-5 6-8 9-12
N 13184 10646 9369 6439 5231
Fourteen different speaking exercises (item-
types) were included in the various level-specific
forms of the test. Some item-types were accom-
panied by images; some only had audio prompts.
Note, however, that before the change to automatic
administration and scoring, test forms had only in-
cluded speaking item-types from a set of thirteen
different types, of which ten were not designed to
constrain the spoken responses. On the contrary,
these ten item-types were designed to elicit rela-
tively open-ended displays of speaking ability, and
most test forms included one or two items of most
types. A Repeat Sentence item type was added
to the test designs (10 Repeat items per test form
at every level), yielding test forms with around
12
27 items total, including Repeats. Table 2 lists
all the speaking item types that are presented in
one AZELLA test form for Stage III (Grades 3-
5). Some items such as Questions on Image, Sim-
ilarities & Differences, Ask Qs about a Statement,
and Detailed Response to Topic are presented as a
sequence of two related questions and the two re-
sponses are human-rated together to produce one
holistic score.
Table 2: Stage III (Grades 3-5) items.
Descriptions items/test Score-Points
Repeat Sentence 10 0-4
Read-by-Syllables 3 0-1
Read-Three-Words 3 0-1
Questions on Image 3 0-4
Similarities & Differences 2 0-4
Give Directions from Map 1 0-4
Ask Qs about a Statement 1 0-4
Give Instructions 1 0-4
Open Question on Topic 1 0-4
Detailed Response to Topic 1 0-4
Table 3: Item types used in AZELLA speaking
field tests.
Description (restriction) Score-Points
Naming (Stage I) 0-1
Short Response (Stage I) 0-2
Open Question (Stage I) 0-2
Read-by-Syllables 0-1
Read-Three-Words 0-1 or 0-3
Repeat Sentence 0-4
Questions on Image 0-4
Similarities & Differences (III) 0-4
Give Directions from Map 0-4
Ask Qs about a Thing (II) 0-2
Ask Qs about a Statement (III) 0-4
Give Instructions 0-4
Open Questions on Topic 0-4
Detailed Response to Topic 0-4
All the speaking item-types used at any level
in the AZELLA field tests are listed in Table
3. Item-types used at only one stage (level) are
noted. From Table 3 we can see that, except for
Naming, Repeat Sentence, Read-by-Syllables, and
Read-Three-Words, all the items are fairly uncon-
strained questions. Engineering considerations did
not guide the design of these items to make them
be more suitable for machine learning and auto-
matic scoring, and they were, indeed, a challenge
to score.
By tradition and by design, human scoring of
AZELLA responses is limited to a single holistic
score, guided by sets of Score-Point rubrics defin-
ing scores at 2, 3, 4, or 5 levels. The column Score-
Points specifies the number of categories used in
holistic scoring. One set of five abbreviated holis-
tic rubrics for assigning points by human rating is
presented below in Table 4. For the Repeat Sen-
tence items only, separate human ratings were col-
lected under a pronunciation rubric and a fluency
rubric.
Table 4: Example AZELLA abbreviated holistic
rubric (5 Score-Points).
Points Descriptors
4 Correct understandable English us-
ing two or more sentences. 1. Com-
plete declarative or interrogative sen-
tences. 2. Grammar (or syntax) er-
rors are not evident and do not im-
pede communication. 3. Clear and
correct pronunciation. 4. Correct
syntax.
3 Understandable English using two or
more sentences. 1. Complete declar-
ative or interrogative sentences. 2.
Minor grammatical (or syntax) er-
rors. 3. Clear and correct pronuncia-
tion.
2 An intelligible English response. 1.
Less than two complete declarative
or interrogative sentences. 2. Errors
in grammar (or syntax). 3. Attempt
to respond with clear and correct pro-
nunciation.
1 Erroneous responses. 1. Not com-
plete declarative or interrogative sen-
tences. 2. Significant errors in gram-
mar (or syntax). 3. Not clear and cor-
rect pronunciation.
0 Non-English or silence.
3 Development and validation data
From the data in the first field test (Stages II, III,
IV, V), for each AZELLA Stage, we randomly
sampled 300 tests (75 tests/form x 4 forms) as a
validation set and 1, 200 tests as a development
set. For the data in the second field test (Stage
I), we randomly sampled 167 tests from the four
forms as the validation set and 1, 200 tests as the
13
development set. No validation data was used for
model training.
3.1 Human transcriptions and scoring
In the development sets, we needed from 100 to
300 responses per item to be transcribed, depend-
ing on the complexity of the item type. In the val-
idation sets, all responses were fully transcribed.
Depending on the item type, we got single or dou-
ble transcriptions, as necessary.
All responses from the tests were scored by
trained professional raters according to predefined
rubrics (Arizona Department of Education, 2012),
such as those in Table 4. Departing from usual
practice in production settings, we used the aver-
age score from different raters as the final score
during machine learning. The responses in each
validation set were double rated (producing two
final scores) for use in validation. Note that five of
the 1,367 tests in the validation sets had no human
transcriptions and ratings, and so were excluded
from the final validation results.
4 Machine scoring methods
Previous research on automatic assessment of
spoken responses can be found in Bernstein et
al. (2000; 2010), Cheng (2011) and Higgins
et al. (2011). Past work on automatic assess-
ment of children?s oral reading fluency has been
reported at the passage-level (Cheng and Shen,
2010; Downey et al., 2011) and at the word-level
(Tepperman et al., 2007). A comprehensive review
of spoken language technologies for education can
be found in Eskanazi (2009). The following sub-
sections summarize the methods we have used for
scoring AZELLA tests. Those methods with cita-
tions have been previously discussed in research
papers. Other methods described are novel modi-
fications or extensions of known methods.
Both the linguistic content and the manner of
speaking are scored. Our machine scoring meth-
ods include a combination of automatic speech
recognition (ASR), speech processing, statistical
modeling, linguistics, word vectors, and machine
learning. The speech processing technology was
built to handle the different rhythms and varied
pronunciations used by a range of natives and
learners. In addition to recognizing the words
spoken, the system also aligns the speech signal,
i.e., it locates the part of the signal containing
relevant segments, syllables, and words, allowing
the system to assign independent scores based on
the content of what is spoken and the manner in
which it is said. Thus, we derive scores based on
the words used, as well as the pace, fluency, and
pronunciation of those words in phrases and sen-
tences. For each response, base measures are then
derived from the linguistic units (segments, sylla-
bles, words), with reference to statistical models
built from the spoken performances of natives and
learners. Except for the Repeat items, the system
produces only one holistic score per item from a
combination of base measures.
4.1 Acoustic models
We tried various sets of recorded responses to train
GMM-HMM acoustic models as implemented in
HTK (Young et al., 2000). Performance im-
proved by training acoustic models on larger sets
of recordings, including material from students
out of the age range being tested. For exam-
ple, training acoustic models using only the Stage
II transcriptions to recognize other Stage II re-
sponses was significantly improved by using more
data from outside the Stage II data set, such as
other AZELLA field data. We observed that the
more child speech data, the better the automatic
scoring. The final acoustic models used for recog-
nition were trained on all transcribed AZELLA
field data, except the data in the validation sets,
plus data from an unrelated set of children?s oral
reading of passages (Cheng and Shen, 2010), and
the data collected during the construction of the
Versant Junior English tests for use by young chil-
dren in Asia (Bernstein and Cheng, 2007). Thus,
the acoustic models were built using any and all
relevant data available: totaling about 380 hours
of data (or around 176, 000 responses). The word
error rate (WER) over all the validation sets using
the final acoustic models is around 35%.
For machine scoring (after recognition and
alignment), native acoustic models are used to
compute native likelihoods of producing the ob-
served base measures. Human listeners classified
student recordings from Stage II (grades 1-2) as
native or non-native. For example, in Stage II
data, 287 subjects were identified as native and the
recordings from these 287 subjects plus the native
recordings from the Versant Junior English tests
were used to build native acoustic models for grad-
ing. (approximately 66 hours of speech data, or
39, 000 responses).
14
4.2 Language models
Item-specific bigram language models were built
using the human transcription of the development-
set as described in Section 3.1.
4.3 Content modeling
"Content" refers to the linguistic material (words,
phrases, and semantic elements) in the spoken re-
sponse. Appropriate response content reflects the
speaker?s productive control of English vocabu-
lary and also indicates how well the test-taker un-
derstood the prompt. Previous work on scoring
linguistic content in the speech domain includes
Bernstein et al. (2010) and Xie et al. (2012).
Except for the four relatively closed-response-
form items (Naming, Repeat, Read-by-Syllables
and Read-Three-Words), we produced a
word_vector score for each response (Bern-
stein et al., 2010). The value of the word_vector
score is calculated by scaling the weighted sum of
the occurrence of a large set of expected words
and word sequences available in an item-specific
response scoring model. An automatic process
assigned weights to the expected words and word
sequences according to their semantic relation to
known good responses using a method similar to
latent semantic analysis (Landauer et al., 1998).
The word_vector score is generally the most
powerful feature used to predict the final human
scores.
Note that a recent competition to develop accu-
rate scoring algorithms for student-written short-
answer responses (Kaggle, 2012) focused on a
similar problem to the content scoring task for
AZELLA open-ended responses. We assume that
the methods used by the prize-winning teams,
for example Tandalla (2012) and Zbontar (2012),
should work well for the AZELLA open-ended
material too, although we did not try these meth-
ods.
For the responses to Naming, Read-by-
Syllables, and Read-Three-Words items, the ma-
chine scoring makes binary decisions based on the
occurrence of a correct sequence of syllables or
words (keywords). In Stage II forms, for first
and second grade students, the responses to Read-
Three-Words items were human-rated in four cat-
egories. For this stage, the machine counted the
number of words read correctly.
For the responses to Repeat items, the recog-
nized string is compared to the word string re-
cited in the prompt, and the number of word er-
rors (word_errors) is calculated as the minimum
number of substitutions, deletions, and/or inser-
tions required to find a best string match in the
response. This matching algorithm ignores hes-
itations and filled or unfilled pauses, as well as
any leading or trailing material in the response
(Bernstein et al., 2010). A verbatim repetition
would have zero word errors. For Repeat re-
sponses, the percentage of words repeated cor-
rectly (percent_correct) was used as an addi-
tional feature.
4.4 Duration modeling
Phone-level duration statistics contribute to ma-
chine scores of test-takers? pronunciation and flu-
ency. Native-speakers segment duration statis-
tics from Versant Junior English tests (Bernstein
and Cheng, 2007) were used to compute the
log-likelihood of phone durations produced by
test-takers. No data from AZELLA tests con-
tributed to the duration models. We calculated the
phoneme duration log-likelihood: log_seg_prob
and the inter-word silence duration log-likelihood:
iw_log_seg_prob (Cheng, 2011).
Assume in a recognized response that the se-
quence of phonemes and their corresponding du-
rations are p
i
and D
i
, i = 1...N , then the
log likelihood segmental probability for phonemes
(log_seg_prob) was computed as:
log_seg_prob =
1
N ? 2
N?1
?
i=2
log(Pr(D
i
)), (1)
where Pr(D
i
) was the probability that a native
would produce phoneme p
i
with the observed du-
ration D
i
in the context found. The first and last
phonemes in the response were not used for the
calculation of the log_seg_prob because durations
of these phonemes as determined by the ASR were
more likely to be incorrect. The log likelihood
segmental probability for inter-word silence dura-
tions, iw_log_seg_prob, was calculated the same
way (Cheng, 2011).
4.5 Spectral modeling
To construct scoring models for pronunciation
and fluency, we computed several spectral likeli-
hood features with reference to native and learner
segment-specific models applied to the recogni-
tion alignment, computing the phone-level poste-
rior probabilities given the acoustic observation X
15
that is recognized as p
i
:
P (p
i
|X) =
P (X|p
i
)P (p
i
)
?
m
k=1
P (X|p
k
)P (p
k
)
(2)
where k runs over all the potential phonemes. In
a real-world ASR system, it is extremely difficult
to estimate
?
m
k=1
P (X|p
k
)P (p
k
) precisely. So
approximations are used, such as substituting a
maximum for the summation, etc. Formula 2 is
the general framework for pronunciation diagno-
sis (Witt and Young, 1997; Franco et al., 1999;
Witt and Young, 2000) and pronunciation assess-
ment (Witt and Young, 2000; Franco et al., 1997;
Neumeyer et al., 1999; Bernstein et al., 2010).
Various authors use different approximations to
suit the particulars of their data and their applica-
tions.
In the AZELLA spectral scoring, we approx-
imated Formula 2 with the following procedure.
After the learner acoustic models produce a recog-
nition result, we force-align the utterance on the
recognized word string, but using the native mono-
phone acoustic models, producing acoustic log-
likelihood, duration and time boundaries for ev-
ery phone. For each such phone, again using the
native monophone time alignment, we perform
an all-phone recognition using the native mono-
phone acoustic models. The recognizer calculates
a log-likelihood for every phone and picks the
best match from all possible phones over that time
frame. For each phone-of-interest in a response,
we calculated the average spectral score difference
as:
spectral_1 =
1
N
N
?
i=1
lp
fa
i
? lp
ap
i
d
i
(3)
where the variables are:
? lp
fa
i
is the log-likelihood corresponding to
the i-th phoneme by using the forced align-
ment method;
? lp
ap
i
is the log-likelihood by using the all-
phone recognition method;
? d
i
is its duration;
? N is the number of phonemes of interest in a
response.
In calculating spectral_1, all possible
phonemes are included. We define another
variable, spectral_2, that only accumulates
the log-likelihood for a target set of phonemes
that learners often have difficulty with. We call
the percentage of phones from the all-phone
recognition that match the phones from the forced
alignment the percent phone match, or ppm.
We take Formula 3 as the average log of the
approximate posterior probabilities that phones
were produced by a native.
4.6 Confidence modeling
After finishing speech recognition, we can assign
speech confidence scores to words and phonemes
(Cheng and Shen, 2011). Then for every response,
we can compute the average confidence, the per-
centage of words or phonemes whose confidences
are lower than a threshold value as features to pre-
dict test-takers? performance.
4.7 Final models
AZELLA holistic score rubrics (Arizona Depart-
ment of Education, 2012), such as those shown
in Table 4, consider both the answer content and
the manner of speaking used in the response. The
automatic scoring should consider both too. Fea-
tures word_vector, keywords, word_errors,
percent_correct can represent content scores
based on what is spoken. Features log_seg_prob,
iw_log_seg_prob, spectral_1, spectral_2, ppm
can represent both the rhythmic and segmental as-
pects of the performance as native likelihoods of
producing the observed base measures. By feed-
ing these features to models, we can effectively
predict human holistic scores, as well as human
pronunciation and fluency ratings, although we did
not model grammar errors in the way they are
specifically described in the rubrics, e.g. in Table
4.
For each item, a specific combination of base
scores was selected. So, on an item-by-item basis,
we tried two methods of combination: (i) multiple
linear regression and (ii) neural networks with one
hidden layer trained by back propagation. Then
we selected the one that was more accurate for
that item. For almost all items, the neural network
model worked better.
4.8 Unscorable test detection
Many factors can render a test unscorable: poor
sound quality (recording noise, mouth too close
to the microphone, too soft, etc.), gibberish (non-
sense words, noise, or a foreign language), off-
topic (off topic, but intelligible English), unintelli-
gible English (e.g. a good-faith attempt to respond
16
in English, but is so unintelligible and/or disfluent
that it cannot be understood confidently).
There have been several approaches to dealing
with this issue (Cheng and Shen, 2011; Chen and
Mostow, 2011; Yoon et al., 2011). Some un-
scorable tests can be identified easily by a hu-
man listener, and we reported research on a speci-
fied unscorable category (off-topic) before (Cheng
and Shen, 2011). Dealing with a specified cat-
egory could be significantly easier than dealing
with wide-open items as in AZELLA. Also, be-
cause we did not collect human ?unscorable" rat-
ings for this data, we worked on predicting the ab-
solute overall difference between human and ma-
chine scores; which is like predicting outliers. If
the difference is expected to exceed a threshold,
the test should be sent for human grading.
Many problems were due to low volume record-
ings made by shy kids, so we identified features to
deal with low-volume tests. These included max-
imum energy, the number of frames with funda-
mental frequency, etc., using many features men-
tioned in Cheng and Shen (2011). The method
used to detect off-topic responses did not work
well here, but features based on lattice confidence
seemed to work fairly well. If we define an un-
scorable test as one with an overall difference be-
tween human and machine scores greater than or
equal to 3 (within the score range 0-14), our final
unscorable test detector achieves an equal-error
rate of 16.5% in validation sets; or when fixing the
false rejection rate at 6%, the false acceptance rate
is 44%. We are actively investigating better meth-
ods to achieve acceptable performance for use in
real tests.
5 Experimental results
All results presented in this section used the vali-
dation data sets, while the recognition and scoring
models were built from completely separate mate-
rial. The participant-level speaking scores were
designed not to consider the scores from Read-
by-Syllables and Read-Three-Words. For each
test, the system produced holistic scores for Re-
peat items and for non-Repeat items. For every
Repeat item, the machine generated pronuncia-
tion, fluency and accuracy scores mapped into the
0 to 4 score-point range. Both human and machine
holistic scores for a Repeat response are equal
to: 50% ? Accuracy + 25% ? Pronunciation +
25% ? Fluency. Accuracy scores were scaled
as percent_correct times four. Human accuracy
scores were based on human transcriptions instead
of ASR transcriptions. Holistic scores for Repeat
items at the participant level were the simple aver-
age of the corresponding item-level scores.
For every non-Repeat item, we generated one
holistic score that considered pronunciation, flu-
ency and content together. The non-Repeat holis-
tic scores at the participant level were the sim-
ple average of the corresponding item level scores
after normalizing them to the same scale. The
final generated holistic scores for Repeats were
scaled to a 0 ? 4 range and non-Repeat holis-
tic scores were scaled to a 0 ? 10 range to sat-
isfy an AZELLA design requirement that Repeat
items count for 4 points and non-Repeats count
for 10 points. The overall participant level scores
are the sum of the Repeat holistic scores and the
non-Repeat holistic scores (maximum 14). All
machine-generated scores are continuous values.
In the following tables, H-H r stands for the
human-human correlation and M-H r stands for
the correlation between machine-generated scores
and average human scores.
Table 5: Human rating reliabilities and Machine-
human correlations by item type. Third column
gives mean and standard deviation of words per
response.
S Item types
Words/response
H-H r M-H r
?? ?
I Naming 2.5? 2.5 0.83 0.67
I Short Response 5.7? 3.8 0.71 0.73
I Open Question 8.7? 7.9 0.70 0.76
I Repeat Sentence 5.0? 2.5 0.91 0.83
II Questions on Image 14.0? 10.8 0.87 0.86
II Give Directions from Map 10.9? 9.7 0.82 0.84
II Ask Qs about a Thing 6.8? 5.9 0.83 0.64
II Open Question on Topic 11.6? 10.6 0.75 0.72
II Give Instructions 11.5? 10.0 0.83 0.80
II Repeat Sentence 6.1? 2.9 0.95 0.85
III Questions on Image 14.5? 10.2 0.87 0.77
III Similarities & Differences 19.5? 11.6 0.75 0.75
III Give Directions from Map 16.3? 11.2 0.74 0.85
III Ask Qs about a Statement 16.7? 13.4 0.79 0.82
III Give Instructions 17.0? 12.8 0.77 0.81
III Open Question on Topic 13.9? 11.1 0.85 0.85
III Detailed Response to Topic 13.8? 10.5 0.81 0.80
III Repeat Sentence 6.4? 3.2 0.97 0.88
IV Questions on Image 13.9? 11.8 0.84 0.84
IV Give Directions from Map 13.7? 13.3 0.84 0.90
IV Open Question on Topic 17.2? 15.2 0.82 0.82
IV Detailed Response to Topic 13.9? 11.4 0.85 0.87
IV Give Instructions 16.5? 15.7 0.87 0.90
IV Repeat Sentence 6.9? 3.2 0.96 0.89
V Questions on Image 17.3? 12.0 0.80 0.76
V Open Question on Topic 18.7? 14.9 0.84 0.82
V Detailed Response to Topic 17.7? 15.2 0.88 0.87
V Give Instructions 17.2? 16.6 0.90 0.90
V Give Directions from Map 22.4? 16.8 0.86 0.85
V Repeat Sentence 6.4? 3.5 0.95 0.89
17
We summarize the psychometric properties of
different item types that contribute to the final
scores in Table 5. For each item-type and each
stage, the third column in Table 5 presents the
mean and standard deviation of the words-per-
response produced by students, showing that older
students generally produce more spoken material.
We found that the number of words spoken is a
better measure than speech signal duration to rep-
resent the amount of material produced, because
young English learners often emit long silences
while speaking. The difference between the two
measures in columns 4 and 5 is statistically signif-
icant (two-tailed, p < 0.05) for item types Nam-
ing (Stage I), Ask Qs about a Thing (Stage II),
Questions on Image (Stage III), and Repeat Sen-
tence (all Stages), in which machine scoring does
not match human; and for item types Give Direc-
tions from Map (Stage III, IV), in which machine
is better than a single human score. For almost
all open-ended items, machine scoring is similar
to or better than human scoring. We noticed that
machine scoring of one open-ended item type, Ask
Qs about a Thing used in Stage II test forms, was
significantly worse than human scoring, leading us
to identify problems specific to the item type itself,
both in the human rating rubric and in the machine
grading approach. Arizona is not using this item
type in operational tests.
Figures 1, 2, 3, 4, 5 present scatter plots of over-
all scores at the participant level comparing hu-
man and machine scores for test in each AZELLA
stage. Figure 6 shows the averaged human holistic
score distribution for participants in the validation
set for Stage V. The human holistic score distribu-
tions for participants in other AZELLA stages are
similar to those in Figure 6, except the means shift
somewhat.
We identified several participants for whom the
difference between human and machine scores is
bigger than 4 in Figures 1, 2, 3, 4, 5. Listen-
ing to the recordings of these tests, we concluded
that the most important factor was low Signal-to-
Noise Ratio (SNR). Either the background noise
was very high (in 6 of 1,362 tests in the validation
set), or speech volume was low (in 3 of 1,362 tests
in the validation set). Either condition can make
recognition difficult. With very low voice ampli-
tude and high background noise levels, the SNR of
some outlier response recordings is so low that hu-
man raters refuse to affirm that they understand the
Figure 1: Overall human vs. machine scores at the
participant level for Stage I (Grade K). Mean and
standard deviation for human scores: (8.74, 3.1).
Figure 2: Overall human vs. machine scores at
the participant level for Stage II (Grades 1-2).
Mean and standard deviation for human scores:
(7.1, 2.5).
Figure 3: Overall human vs. machine scores at
the participant level for Stage III (Grades 3-5).
Mean and standard deviation for human scores:
(9.6, 2.3).
18
Figure 4: Overall human vs. machine scores at
the participant level for Stage IV (Grades 6-8).
Mean and standard deviation for human scores:
(8.3, 2.9).
Figure 5: Overall human vs. machine scores at
the participant level for Stage V (Grades 9-12).
Mean and standard deviation for human scores:
(8.9, 2.9).
Figure 6: Distribution of average human holistic
score for participants in the validation set for Stage
V (Grades 9-12).
content of the response or rate its pronunciation.
Since many young children in kindergarten and
early elementary school speak softly, the youngest
children?s speech is substantially harder to recog-
nize (Li and Russell, 2002; Lee et al., 1999). This
probably contributes to the lower reliabilities in
Stage I and II. When setting the total rejection rate
at 6%, our unscorable test detector identifies only
7 of the 13 outlier tests.
Table 6: Reliability of human scores and Human-
Machine correlations of overall test scores by
stage.
Stage H-H r M-H r
I 0.91 0.88
II 0.96 0.90
III 0.97 0.94
IV 0.98 0.95
V 0.98 0.93
Average 0.96 0.92
Table 6 summarizes the reliabilities of the tests
in different stages. At the participant level, the av-
erage inter-rater reliability coefficient across the
five stages was 0.96, suggesting that the well-
trained human raters agree with each other with
high consistency when ratings are combined over
all the material in all the responses in a whole
test; the average correlation coefficient between
machine-generated overall scores and average hu-
man overall scores was 0.92. This suggests that
the machine grading may be sufficiently reliable
for most purposes.
Table 7: Test reliability by stage, separating non-
Repeat holistic scores and Repeat holistic scores.
Stage
H-H r M-H r H-H r M-H r
NonRptH NonRptH RptH RptH
I 0.85 0.83 0.99 0.94
II 0.93 0.89 0.99 0.90
III 0.95 0.92 0.99 0.92
IV 0.96 0.95 0.99 0.94
V 0.96 0.91 0.99 0.93
Average 0.93 0.90 0.99 0.93
Table 7 summarizes the reliabilities of test
scores in the different stages considering the non-
Repeat holistic scores and Repeat holistic scores
separately to check the effect of adding the Re-
peat items. Repeat items improve the machine re-
19
liability in Stage I significantly, but not so much
for other stages. This difference may relate to the
difficulty in eliciting sufficient speech samples in
non-Repeat items from the young EL students in
Stage I. Eliciting spoken materials in Repeat items
is more straightforward. Consideration of Table
7 suggests that using only open-ended item-types
can also achieve sufficiently reliable results.
6 Discussion and future work
We believe that we can improve this system fur-
ther by scoring Repeat items using a partial credit
Rasch model (Masters, 1982) instead of the av-
erage of percent_correct, which should improve
the reliability of the Repeat item type. We may
also be able to train a better native acoustic model
by using a larger sample of native data from
AZELLA, if we are given access to the test-taker
demographic information.
The original item selection and assignment of
items to forms was quite simple and had room for
improvement. Currently in the AZELLA testing
program, test forms go through a post-pilot re-
vision, so that the operational tests only include
good items in the final test forms. This post-pilot
selection and arrangement of items into forms
should improve human-machine correlations be-
yond the values reported here. If we effectively
address the problem of shy-kids-talking-softly, the
scoring performance will definitely improve even
more. Getting young students to talk louder is
probably something that can be best done at the
testing site (by instruction or by example); and
it may solve several problems. We are happy
to report that the first operational AZELLA test
with automatic speech scoring took place between
January 14 and February 26, 2013, with approxi-
mately 140, 700 tests delivered.
Recent progress in machine learning has ap-
plied deep neural networks (DNNs) to many
long-standing pattern recognition and classifica-
tion problems. Many groups have now applied
DNNs to the task of building better acoustic mod-
els for speech recognition (Hinton et al., 2012).
DNNs have repeatedly been shown to work better
than Gaussian mixture models (GMMs) for ASR
acoustic modeling (Hinton et al., 2012; Dahl et al.,
2012). We are actively exploring the use of DNNs
for use in recognition of children?s speech. We
expect that DNN acoustic models can overcome
some of the recognition difficulties mentioned in
this paper (e.g. low SNR in responses and short
response item types like Naming) and boost the fi-
nal assessment accuracy significantly.
7 Conclusions
We have reported an evaluation of the automatic
methods that are currently used to assess spo-
ken responses to test tasks that occur in Ari-
zona?s AZELLA test for young English learners.
The methods score both the content of the re-
sponses and the quality of the speech produced
in the responses. Although most of the speak-
ing item types in the AZELLA tests are uncon-
strained and open-ended, machine scoring accu-
racy is similar to or better than human scoring for
most item types. We presented basic validity evi-
dence for machine-generated scores, including an
average correlation coefficient between machine-
generated overall scores and human overall scores
derived from subscores that are based on multi-
ple human ratings. Further, we described the de-
sign, implementation and evaluation of a detec-
tor to catch problematic, unscorable tests. We be-
lieve that near-term re-optimization of some scor-
ing process elements may further improve ma-
chine scoring accuracy.
References
Arizona Department of Education. 2012.
AZELLA update. http://www.azed.gov/
standards-development-assessment/
files/2012/12/12-12-12-update-
v5.pdf. [Accessed 19-March-2014].
Arizona Department of Education. 2014. Arizona
English Language Learner Assessment (AZELLA).
http://www.azed.gov/standards-
development-assessment/arizona-
english-language-learner-
assessment-azella. [Accessed 19-March-
2014].
J. Bernstein and J. Cheng. 2007. Logic and valida-
tion of a fully automatic spoken English test. In
V. M. Holland and F. P. Fisher, editors, The Path
of Speech Technologies in Computer Assisted Lan-
guage Learning, pages 174?194. Routledge, New
York.
J. Bernstein, J. De Jong, D. Pisoni, and B. Townshend.
2000. Two experiments on automatic scoring of spo-
ken language proficiency. In Proc. of STIL (Integrat-
ing Speech Technology in Learning), pages 57?61.
J. Bernstein, A. Van Moere, and J. Cheng. 2010. Vali-
dating automated speaking tests. Language Testing,
27(3):355?377.
20
W. Chen and J. Mostow. 2011. A tale of two tasks: De-
tecting children?s off-task speech in a reading tutor.
In Interspeech 2011, pages 1621?1624.
J. Cheng and J. Shen. 2010. Towards accurate recogni-
tion for children?s oral reading fluency. In IEEE-SLT
2010, pages 91?96.
J. Cheng and J. Shen. 2011. Off-topic detection in
automated speech assessment applications. In Inter-
speech 2011, pages 1597?1600.
J. Cheng. 2011. Automatic assessment of prosody
in high-stakes English tests. In Interspeech 2011,
pages 1589?1592.
G. Dahl, D. Yu, L. Deng, and A. Acero. 2012.
Context-dependent pretrained deep neural networks
for large vocabulary speech recognition. IEEE
Transactions on Speech and Audio Processing, Spe-
cial Issue on Deep Learning for Speech and Lang.
Processing, 20(1):30?42.
R. Downey, D. Rubin, J. Cheng, and J. Bernstein.
2011. Performance of automated scoring for chil-
dren?s oral reading. In Proceedings of the Sixth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 46?55.
M. Eskanazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51:832?844.
H. Franco, L. Neumeyer, Y. Kim, and O. Ronen. 1997.
Automatic pronunciation scoring for language in-
struction. In ICASSP 1997, pages 1471?1474.
H. Franco, L. Neumeyer, M. Ramos, and H. Bratt.
1999. Automatic detection of phone-level mispro-
nunciation for language learning. In Eurospeech
1999, pages 851?854.
D. Higgins, X. Xi, K. Zechner, and D. Williamson.
2011. A three-stage approach to the automated scor-
ing of spontaneous spoken responses. Computer
Speech and Language, 25:282?306.
G. Hinton, L. Deng, Y. Dong, G. Dahl, A. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, and B. Kingsbury. 2012. Deep neu-
ral networks for acoustic modeling in speech recog-
nition: The shared views of four research groups.
IEEE Signal Processing Magazine, 29(6):82?97.
Kaggle. 2012. The Hewlett Foundation: Short
answer scoring. http://www.kaggle.com/
c/asap-sas;http://www.kaggle.com/
c/asap-sas/details/winners. [Accessed
20-April-2014].
T. K. Landauer, P. W. Foltz, and D. Laham. 1998.
Introduction to latent semantic analysis. Discourse
Processes, 25:259?284.
S. Lee, A. Potamianos, and S. Narayanan. 1999.
Acoustics of children?s speech: developmental
changes of temporal and spectral parameters. Jour-
nal of Acoustics Society of American, 105:1455?
1468.
Q. Li and M. Russell. 2002. An analysis of the causes
of increased error rates in children?s speech recogni-
tion. In ICSLP 2002, pages 2337?2340.
G. N. Masters. 1982. A Rasch model for partial credit
scoring. Psychometrika, 47(2):149?174.
L. Neumeyer, H. Franco, V. Digalakis, and M. Wein-
traub. 1999. Automatic scoring of pronunciation
quality. Speech Communication, 30:83?93.
L. Tandalla. 2012. ASAP Short Answer
Scoring Competition System Description:
Scoring short answer essays. https:
//kaggle2.blob.core.windows.net/
competitions/kaggle/2959/media/
TechnicalMethodsPaper.pdf. [Accessed
20-April-2014].
J. Tepperman, M. Black, P. Price, S. Lee,
A. Kazemzadeh, M. Gerosa, M. Heritage, A. Al-
wan, and S. Narayanan. 2007. A Bayesian network
classifier for word-level reading assessment. In
Interspeech 2007, pages 2185?2188.
S. M. Witt and S. J. Young. 1997. Language learn-
ing based on non-native speech recognition. In Eu-
rospeech 1997, pages 633?636.
S. M. Witt and S. J. Young. 2000. Phone-level pro-
nunciation scoring and assessment for interactive
language learning. Speech Communication, 30:95?
108.
S. Xie, K. Evanini, and K. Zechner. 2012. Explor-
ing content features for automated speech scoring.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 103?111.
S.-Y. Yoon, K. Evanini, and K. Zechner. 2011. Non-
scorable response detection for automated speaking
proficiency assessment. In Proceedings of the Sixth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 152?160.
S. J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2000. The HTK
Book Version 3.0. Cambridge University, Cam-
bridge, England.
J. Zbontar. 2012. ASAP Short Answer Scoring Com-
petition System Description: Short answer scoring
by stacking. https://kaggle2.blob.core.
windows.net/competitions/kaggle/
2959/media/jzbontar.pdf. [Accessed
20-April-2014].
21
