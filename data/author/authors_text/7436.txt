Proceedings of NAACL HLT 2007, Companion Volume, pages 185?188,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Kernel Regression Based Machine Translation
Zhuoran Wang and John Shawe-Taylor
Department of Computer Science
University College London
London, WC1E 6BT
United Kingdom
{z.wang, jst}@cs.ucl.ac.uk
Sandor Szedmak
School of Electronics and Computer Science
University of Southampton
Southampton, SO17 1BJ
United Kingdom
ss03v@ecs.soton.ac.uk
Abstract
We present a novel machine translation
framework based on kernel regression
techniques. In our model, the translation
task is viewed as a string-to-string map-
ping, for which a regression type learning
is employed with both the source and the
target sentences embedded into their ker-
nel induced feature spaces. We report the
experiments on a French-English transla-
tion task showing encouraging results.
1 Introduction
Fig. 1 illustrates an example of phrase alignment
for statistical machine translation (SMT). A rough
linear relation is shown by the co-occurences of
phrases in bilingual sentence pairs, which motivates
us to introduce a novel study on the SMT task:
If we define the feature space Hx of our source
language X as all its possible phrases (i.e. informa-
tive blended word n-grams), and define the mapping
?x : X ? Hx, then a sentence x ? X can be ex-
pressed by its feature vector ?x(x) ? Hx. Each
component of ?x(x) is indexed by a phrase with the
value being the frequency of it in x. The definition
of the feature space Hy of our target language Y can
be made in a similar way, with corresponding map-
ping ?y : Y ? Hy. Now in the machine translation
task, given S = {(xi, yi) : xi ? X , yi ? Y, i =
1, . . . ,m}, a set of sample sentence pairs where yi
is the translation of xi, we are trying to learn W a
matrix represented linear operator, such that:
?y(y) = f(x) = W?x(x) (1)
we return
to marked
questions
marqu?es
nous
revenous
aux
questions
Figure 1: Phrase alignment in SMT
to predict the translation y for a new sentence x.
Comparing with traditional methods, this model
gives us a theoretical framework to capture higher-
dimensional dependencies within the sentences. To
solve the multi-output regression problem, we inves-
tigate two models, least squares regression (LSR)
similar to the technique presented in (Cortes et al,
2005), and maximum margin regression (MMR) in-
troduced in (Szedmak et al, 2006).
The rest of the paper is organized as follows. Sec-
tion 2 gives a brief review of the regression models.
Section 3 details the solution to the pre-image prob-
lem. We report the experimental results in Section
4, with discussions in Section 5.
2 Kernel Regression with Vector Outputs
2.1 Kernel Induced Feature Space
In the practical learning process, only the inner prod-
ucts of the feature vectors are needed (see Section
2.2, 2.3 and 3), so we can perform the so-called
kernel trick to avoid dealing with the very high-
dimensional feature vectors explicitly. That is, for
x, z ? X , a kernel function is defined as:
?x(x, z) = ??x(x),?x(z)? = ?x(x)??x(z) (2)
185
Similarly, a kernel function ?y(?, ?) is defined in Hy.
In our case, the blended n-spectrum string ker-
nel (Lodhi et al, 2002) that compares two strings
by counting how many (contiguous) substrings of
length from 1 up to n they have in common, is a good
choice for the kernel function to induce our feature
spaces Hx and Hy implicitly, even though it brings
in some uninformative features (word n-grams) as
well, when compared to our original definition.
2.2 Least Squares Regression
A basic method to solve the problem in Eq. 1 is least
squares regression that seeks the matrix W mini-
mizing the squared loss in Hy on the training set S:
min ?WMx ?My?2F (3)
where Mx = [?x(x1), ...,?x(xm)], My =
[?y(y1), ...,?y(ym)], and ? ? ?F denotes the Frobe-
nius norm.
Differentiating the expression and setting it to
zero gives:
2WMxM?x ? 2MyM?x = 0
? W = MyK?1x M?x (4)
where Kx = M?x Mx = (?x(xi, xj)1?i,j?m) is the
Gram matrix.
2.3 Maximum Margin Regression
An alternative solution to our regression learn-
ing problem is proposed in (Szedmak et al,
2006), called maximum margin regression. If L2-
normalized feature vectors are used in Eq. 1, de-
noted by ??x(?) and ??y(?), MMR solves the follow-
ing optimization:
min 12?W?
2
F + C
m
?
i=1
?i (5)
s.t. ???y(yi),W??x(xi)?Hy ? 1? ?i,
?i > 0, i = 1, . . . ,m.
where C > 0 is the regularization coefficient, and
?i are the slack variables. The Lagrange dual form
with dual variables ?i gives:
min
m
?
i,j=1
?i?j ??x(xi, xj)??y(yi, yj)?
m
?
i=1
?i
s.t. 0 ? ?i ? C, i = 1, . . . ,m. (6)
where ??x(?, ?) and ??y(?, ?) denote the kernel func-
tions associated to the respective normalized feature
vectors.
This dual problem can be solved efficiently with
a perceptron algorithm based on an incremental
subgradient method, of which the bounds on the
complexity and achievable margin can be found in
(Szedmak et al, 2006).
Then according to Karush-Kuhn-Tucker theory,
W is expressed as:
W =
m
?
i=1
?i??y(yi)??x(xi)? (7)
In practice, MMR works better when the distribu-
tion of the training points are symmetrical. So we
center the data before normalizing them. If ?Sx =
1
m
?m
i=1 ?x(xi) is the centre of mass of the source
sentence sample set {xi} in the feature space, the
new feature map is given by ??x(?) = ?x(?) ? ?Sx .
The similar operation is performed on ?y(?) to ob-
tain ??y(?). Then the L2-normalizations of ??x(?) and
??y(?) yield our final feature vectors ??x(?) and ??y(?).
3 Pre-image Solution
To find the pre-image sentence y = f?1(x) can be
achieved by seeking yt that has the minimum loss
between its feature vector ?y(yt) and our prediction
f(x). That is (Eq. 8: LSR, Eq. 9: MMR):
yt = arg min
y?Y(x)
?W?x(x)? ?y(y)?2
= arg min
y?Y(x)
?y(y, y)? 2ky(y)K?1x kx(x) (8)
yt = arg min
y?Y(x)
1? ???y(y),W??x(x)?Hy
= arg max
y?Y(x)
m
?
i=1
?i??y(yi, y)??x(xi, x) (9)
where Y(x) ? Y is a finite set covering all po-
tential translations for the given source sentence
x, and kx(?) = (?x(?, xi)1?i?m) and ky(?) =
(?y(?, yi)1?i?m) are m? 1 column matrices.
A proper Y(x) can be generated according to a
lexicon that contains possible translations for every
component (word or phrase) in x. But the size of it
will grow exponentially with the length of x, which
poses implementation problem for a decoding algo-
rithm.
186
In earlier systems, several heuristic search meth-
ods were developed, of which a typical example
is Koehn (2004)?s beam search decoder for phrase-
based models. However, in our case, because of the
?y(y, y) item in Eq. 8 and the normalization opera-
tion in MMR, neither the expression in Eq. 8 nor
the one in Eq. 9 can be decomposed into a sum
of subfunctions each involving feature components
in a local area only. It means we cannot estimate
exactly how well a part of the source sentence is
translated, until we obtain a translation for the entire
sentence, which prevents us doing a straightforward
beam search similar to (Koehn, 2004).
To simplify the situation, we restrict the reorder-
ing (distortion) of phrases that yield the output sen-
tences by only allowing adjacent phrases to ex-
change their positions. (The discussion of this strat-
egy can be found in (Tillmann, 2004).) We use x[i:j]
and y[i:j] to denote the substrings of x and y that be-
gin with the ith word and end with the jth. Now, if
we go back to the implementation of a beam search,
the current distortion restriction guarantees that in
each expansion of the search states (hypotheses) we
have x[1:lx] translated to a y[1:ly], either like state (a)
or like state (b) in Fig. 2, where lx is the number of
words translated in the source sentence, and ly is the
number of words obtained in the translation.
We assume that if y is a good translation of x,
then y[1:ly] is a good translation of x[1:lx] as well. So
we can expect that the squared loss ?W?x(x[1:lx])?
?y(y[1:ly])?2 in the LSR is small, or the inner prod-
uct ???y(y[1:ly]),W??x(x[1:lx])?Hy in the MMR is
large, for the hypothesis yielding a good translation.
According to Eq. 8 and Eq. 9, the hypotheses in the
search stacks can thus be reranked with the follow-
ing score functions (Eq. 10: LSR, Eq. 11: MMR):
Score(x[1:lx], y[1:ly]) = (10)
?y(y[1:ly], y[1:ly])? 2ky(y[1:ly])K?1x kx(x[1:lx])
Score(x[1:lx], y[1:ly]) =
m
?
i=1
?i??y(yi, y[1:ly])??x(xi, x[1:lx]) (11)
Therefore, to solve the pre-image problem, we
just employ the same beam search algorithm as
(Koehn, 2004), except we limit the derivation of new
hypotheses with the distortion restriction mentioned
nous revenous aux questions
we return to questions
marqu?es ?(a)
(b)
marked
?nous revenous aux questions
we return to questions
marqu?es
Figure 2: Search states with the limited distortion.
above. However, our score functions will bring
more runtime complexities when compared with tra-
ditional probabilistic methods. The time complexity
of a naive implementation of the blended n-spectrum
string kernel between two sentences si and sj is
O(n|si||sj|), where |?| denotes the length of the sen-
tence. So the score function in Eq. 11 results in an
average runtime complexity of O(mnlyl), where l is
the average length of the sentences yi in the training
set. Note here ??x(x[1:lx], xi) can be pre-computed
for lx from 1 to |x| before the beam search, which
calls for O(m|x|) space. The average runtime com-
plexity of the score function in Eq. 10 will be the
same if we pre-compute K?1x kx(x[1:lx]).
4 Experimental Results
4.1 Resource Description
Baseline System To compare with previous work,
we take Pharaoh (Koehn, 2004) as a baseline system,
with its default settings (translation table size 10,
beam size 100). We train a trigram language model
with the SRILM toolkit (Stocke, 2002). Whilst, the
parameters for the maximum entropy model are de-
veloped based on the minimum error rate training
method (Och, 2003).
In the following experiments, to facilitate com-
parison, each time we train our regression models
and the language model and translation model for
Pharaoh on a common corpus, and use the same
phrase translation table as Pharaoh?s to decode our
systems. According to our preliminary experiments,
with the beam size of 100, the search errors of our
systems can be limited within 1.5%.
Corpora To evaluate our models, we randomly
take 12,000 sentences from the French-English por-
tion of the 1996?2003 Europarl corpus (Koehn,
2005) for scaling-up training, 300 for test (Test), and
300 for the development of Pharaoh (Dev). Some
187
Vocabulary Words Perplexity
Fr En Fr En Dev Test
4k 5084 4039 43k 39k 32.25 31.92
6k 6426 5058 64k 59k 30.81 29.03
8k 7377 5716 85k 79k 29.91 28.94
10k 8252 6339 106k 98k 27.55 27.09
12k 9006 6861 127k 118k 27.19 26.41
Table 1: Statistics of the corpora.
characteristics of the corpora are summarized in Ta-
ble 1.
4.2 Results
Based on the 4k training corpus, we test the per-
formance of the blended n-spectrum string kernel in
LSR and MMR using BLEU score, with n increas-
ing from 2 to 7. Fig. 3 shows the results. It can be
found that the performance becomes stable when n
reaches a certain value. Finally, we choose the 3-
spectrum for LSR, and the 5-spectrum for MMR.
Then we scale up the training set, and compare the
performance of our models with Pharaoh in Fig. 4.
We can see that the LSR model performs almost as
well as Pharaoh, whose differences of BLEU score
are within 0.5% when the training set is larger than
6k. But MMR model performs worse than the base-
line. With the training set of 12k, it is outperformed
by Pharaoh by 3.5%.
5 Discussions
Although at this stage the main contribution is
still conceptual, the capability of our approach to
be applied to machine translation is still demon-
strated. Comparable performance to previous work
is achieved by the LSR model.
But a main problem we face is to scale-up the
training set, as in practice the training set for SMT
will be much larger than several thousand sentences.
A method to speed up the training is proposed in
(Cortes et al, 2005). By approximating the Gram
matrix with a n ? m (n ? m) low-rank matrix,
the time complexity of the matrix inversion opera-
tion can be reduced from O(m3) to O(n2m). But
the space complexity of O(nm) in their algorithm is
still too expensive for SMT tasks. Subset selection
techniques could give a solution to this problem, of
2 3 4 5 6 7
26
28
30
32
34
36
38
40
42
MMR
LSR
Figure 3: BLEU(%) versus n-spectrum
4000 6000 8000 10000 12000
30
32
34
36
38
40
42
44
Pharaoh
LSR
MMR
Figure 4: BLEU(%) versus training set size
which we will leave the further exploration to future
work.
Acknowledgements
The authors acknowledge the support of the EU un-
der the IST project No. FP6-033917.
References
C. Cortes, M. Mohri, and J. Weston. 2005. A general re-
gression technique for learning transductions. In Proc.
of ICML?05.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. of AMTA 2004.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit X.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. J. Mach. Learn. Res., 2:419?444.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL?03.
A. Stocke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. of ICSLP?02.
S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.
2006. Learning via linear operators: Maximum mar-
gin regression; multiclass and multiview learning at
one-class complexity. Technical report, PASCAL,
Southampton, UK.
C. Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
NAACL?04.
188
Proceedings of the Third Workshop on Statistical Machine Translation, pages 155?158,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Kernel Regression Framework for Machine Translation: UCL System
Description for WMT 2008 Shared Translation Task
Zhuoran Wang
University College London
Dept. of Computer Science
Gower Street, London, WC1E 6BT
United Kingdom
z.wang@cs.ucl.ac.uk
John Shawe-Taylor
University College London
Dept. of Computer Science
Gower Street, London, WC1E 6BT
United Kingdom
jst@cs.ucl.ac.uk
Abstract
The novel kernel regression model for SMT
only demonstrated encouraging results on
small-scale toy data sets in previous works due
to the complexities of kernel methods. It is
the first time results based on the real-world
data from the shared translation task will be
reported at ACL 2008 Workshop on Statisti-
cal Machine Translation. This paper presents
the key modules of our system, including the
kernel ridge regression model, retrieval-based
sparse approximation, the decoding algorithm,
as well as language modeling issues under this
framework.
1 Introduction
This paper follows the work in (Wang et al, 2007;
Wang and Shawe-Taylor, 2008) which applied the
kernel regression method with high-dimensional
outputs proposed originally in (Cortes et al, 2005)
to statistical machine translation (SMT) tasks. In our
approach, the machine translation problem is viewed
as a string-to-string mapping, where both the source
and the target strings are embedded into their re-
spective kernel induced feature spaces. Then ker-
nel ridge regression is employed to learn the map-
ping from the input feature space to the output one.
As a kernel method, this model offers the potential
advantages of capturing very high-dimensional cor-
respondences among the features of the source and
target languages as well as easy integration of ad-
ditional linguistic knowledge via selecting particu-
lar kernels. However, unlike the sequence labeling
tasks such as optical character recognition in (Cortes
et al, 2005), the complexity of the SMT problem it-
self together with the computational complexities of
kernel methods significantly complicate the imple-
mentation of the regression technique in this field.
Our system is actually designed as a hybrid of
the classic phrase-based SMT model (Koehn et al,
2003) and the kernel regression model as follows:
First, for each source sentence a small relevant set of
sentence pairs are retrieved from the large-scale par-
allel corpus. Then, the regression model is trained
on this small relevant set only as a sparse approx-
imation of the regression hyperplane trained on the
entire training set, as proposed in (Wang and Shawe-
Taylor, 2008). Finally, a beam search algorithm is
utilized to decode the target sentence from the very
noisy output feature vector we predicted, with the
support of a pre-trained phrase table to generate pos-
sible hypotheses (candidate translations). In addi-
tion, a language model trained on a monolingual cor-
pus can be integrated either directly into the regres-
sion model or during the decoding procedure as an
extra scoring function.
Before describing each key component of our sys-
tem in detail, we give a block diagram overview in
Figure 1.
2 Problem Formulation
Concretely, the machine translation problem in our
method is formulated as follows. If we define a fea-
ture space Hx of our source language X , and define
the mapping ? : X ? Hx, then a sentence x ? X
can be expressed by its feature vector ?(x) ? Hx.
The definition of the feature space Hy of our target
language Y can be made in a similar way, with cor-
155
AlignmentParallel
Corpus
Retriever
Phrase Table
Phrase
Extraction
Kernel
Regression Decoder
Monolingual
Corpus
Language
Modeling
N-gram
Model
Target Text
Relevant Set
Source Text
Figure 1: System overview. The processes in gray blocks
are pre-performed for the whole system, while the white
blocks are online processes for each input sentence. The
two dash-line arrows represent two possible ways of lan-
guage model integration in our system described in Sec-
tion 6.
responding mapping ? : Y ? Hy. Now in the ma-
chine translation task, we are trying to seek a matrix
represented linear operator W, such that:
?(y) = W?(x) (1)
to predict the translation y for an arbitrary source
sentence x.
3 Kernel Ridge Regression
Based on a set of training samples, i.e. bilingual
sentence pairs S = {(xi,yi) : xi ? X ,yi ? Y, i =
1, . . . ,m.}, we use ridge regression to learn the W
in Equation (1), as:
min ?WM? ? M??2F + ??W?2F (2)
where M? = [?(x1), ...,?(xm)], M? =
[?(y1), ...,?(ym)], ? ? ?F denotes the Frobenius
norm that is a matrix norm defined as the square root
of the sum of the absolute squares of the elements in
that matrix, and ? is a regularization coefficient.
Differentiating the expression and setting it to
zero gives the explicit solution of the ridge regres-
sion problem:
W = M?(K? + ?I)?1M?? (3)
where I is the identity matrix, and K? =
M??M? = (??(xi,xj)1?i,j?m). Note here, we use
the kernel function:
??(xi,xj) = ??(xi),?(xj)? = ?(xi)??(xj) (4)
to denote the inner product between two feature vec-
tors. If the feature spaces are properly defined, the
?kernel trick? will allow us to avoid dealing with
the very high-dimensional feature vectors explicitly
(Shawe-Taylor and Cristianini, 2004).
Inserting Equation (3) into Equation (1), we ob-
tain our prediction as:
?(y) = M?(K? + ?I)?1k?(x) (5)
where k?(x) = (??(x,xi)1?i?m) is an m ? 1 col-
umn matrix. Note here, we will use the exact matrix
inversion instead of iterative approximations.
3.1 N -gram String Kernel
In the practical learning and prediction processes,
only the inner products of feature vectors are re-
quired, which can be computed with the kernel func-
tion implicitly without evaluating the explicit coor-
dinates of points in the feature spaces. Here, we de-
fine our features of a sentence as its word n-gram
counts, so that a blended n-gram string kernel can
be used. That is, if we denote by xi:j a substring
of sentence x starting with the ith word and ending
with the jth, then for two sentences x and z, the
blended n-gram string kernel is computed as:
?(x, z) =
n
?
p=1
|x|?p+1
?
i=1
|z|?p+1
?
j=1
[[xi:i+p?1 = zj:j+p?1]]
(6)
Here, | ? | denotes the length of the sentence, and
[[?]] is the indicator function for the predicate. In our
system, the blended tri-gram kernel is used, which
means we count the n-grams of length up to 3.
4 Retrieval-based Sparse Approximation
For SMT, we are not able to use the entire training
set that contains millions of sentences to train our
regression model. Fortunately, it is not necessary ei-
ther. Wang and Shawe-Taylor (2008) suggested that
a small set of sentences whose source is relevant to
the input can be retrieved, and the regression model
can be trained on this small-scale relevant set only.
156
Src n? y a-t-il pas ici deux poids , deux mesures
Rlv pourquoi y a-t-il deux poids , deux mesures
pourquoi deux poids et deux mesures
peut-e?tre n? y a-t-il pas d? e?pide?mie non
plus
pourquoi n? y a-t-il pas urgence
cette directive doit exister d? ici deux mois
Table 1: A sample input (Src) and some of the retrieved
relevant examples (Rlv).
In our system, we take each sentence as a docu-
ment and use the tf-idf metric that is frequently used
in information retrieval tasks to retrieve the relevant
set. Preliminary experiments show that the size of
the relevant set should be properly controlled, as if
many sentences that are not very close to the source
text are involved, they will correspond to adding
noise. Hence, we use a threshold of the tf-idf score
to filter the relevant set. On average, around 1500
sentence pairs are extracted for each source sen-
tence. Table 1 shows a sample input and some of
its top relevant sentences retrieved.
5 Decoding
After the regression, we have a prediction of the
target feature vector as in Equation (1). To ob-
tain the target sentence, a decoding algorithm is still
required to solve the pre-image problem. This is
achieved in our system by seeking the sentence y?
whose feature vector has the minimum Euclidean
distance to the prediction, as:
y? = arg min
y?Y(x)
?W?(x) ? ?(y)? (7)
where Y(x) ? Y denotes a finite set covering all
potential translations for the given source sentence
x. To obtain a smaller search space and more re-
liable translations, Y(x) is generated with the sup-
port of a phrase table extracted from the whole train-
ing set. Then a modified beam search algorithm
is employed, in which we restricted the distortion
of the phrases by only allowing adjacent phrases to
exchange their positions, and rank the search states
in the beams according to Equation (7) but applied
directly to the partial translations and their corre-
sponding source parts. A more detailed explanation
of the decoding algorithm can be found in (Wang
et al, 2007). In addition, Wang and Shawe-Taylor
(2008) further showed that the search error rate of
this algorithm is acceptable.
6 Language Model Integration
In previous works (Wang et al, 2007; Wang and
Shawe-Taylor, 2008), there was no language model
utilized in the regression framework for SMT, as
similar function can be achieved by the correspon-
dences among the n-gram features. It was demon-
strated to work well on small-scale toy data, how-
ever, real-world data are much more sparse and
noisy, where a language model will help signifi-
cantly.
There are two ways to integrate a language model
in our framework. First, the most straightforward so-
lution is to add a weight to adjust the strength of the
regression based translation scores and the language
model score during the decoding procedure. Alter-
natively, as language model is n-gram-based which
matches the definition of our feature space, we can
add a langauge model loss to the objective function
of our regression model as follows. We define our
language score for a target sentence y as:
LM(y) = V??(y) (8)
where V is a vector whose components Vy??y?y will
typically be log-probabilities logP (y|y??y?), and y,
y? and y?? are arbitrary words. Note here, in or-
der to match our blended tri-gram induced feature
space, we can make V of the same dimension as
?(y), while zero the components corresponding to
uni-grams and bi-grams. Then the regression prob-
lem can be defined as:
min ?WM??M??2F +?1?W?2F ??2V?WM?1
(9)
where ?2 is a coefficient balancing between the pre-
diction being close to the target feature vector and
being a fluent target sentence, and 1 denotes a vec-
tor with components 1. By differentiating the ex-
pression with respect to W and setting the result to
zero, we can obtain the explicit solution as:
W = (M? + ?2V1?)(K? + ?1I)?1M?? (10)
7 Experimental Results
Preliminary experiments are carried out on the
French-English portion of the Europarl corpus. We
157
System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER (%)
Kernel Regression 26.59 7.00 52.63 55.98 60.52 43.20
Moses 31.15 7.48 56.80 55.14 59.85 42.79
Table 3: Evaluations based on different metrics with comparison to Moses.
train our regression model on the training set, and
test the effects of different language models on the
development set (test2007). The results evaluated
by BLEU score (Papineni et al, 2002) is shown in
Table 2.
It can be found that integrating the language
model into the regression framework works slightly
better than just using it as an additional score com-
ponent during decoding. But language models of
higher-order than the n-gram kernel cannot be for-
mulated to the regression problem, which would be
a drawback of our system. Furthermore, the BLEU
score performance suggests that our model is not
very powerful, but some interesting hints can be
found in Table 3 when we compare our method with
a 5-gram language model to a state-of-the-art system
Moses (Koehn and Hoang, 2007) based on various
evaluation metrics, including BLEU score, NIST
score (Doddington, 2002), METEOR (Banerjee and
Lavie, 2005), TER (Snover et al, 2006), WER and
PER. It is shown that our system?s TER, WER and
PER scores are very close to Moses, though the
gaps in BLEU, NIST and METEOR are significant,
which suggests that we would be able to produce ac-
curate translations but might not be good at making
fluent sentences.
8 Conclusion
This work is a novel attempt to apply the advanced
kernel method to SMT tasks. The contribution at this
stage is still preliminary. When applied to real-world
data, this approach is not as powerful as the state-of-
the-art phrase-based log-linear model. However, in-
teresting prospects can be expected from the shared
translation task.
Acknowledgements
This work is supported by the European Commis-
sion under the IST Project SMART (FP6-033917).
no-LM LM13gram LM
2
3gram LM
1
5gram
BLEU 23.27 25.19 25.66 26.59
Table 2: BLEU score performance of different language
models. LM1 denotes adding the language model dur-
ing decoding process, while LM2 represents integrating
the language model into the regression framework as de-
scribed in Problem (9).
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2005. A general regression technique for learning
transductions. In Proc. of ICML?05.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. of HLT?02, pages 138?145.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proc. of EMNLP-CoNLL?07.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HAACL-HLT?03, pages 48?54.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proc. of ACL?02.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA?06.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel-
based machine translation. In Cyril Goutte, Nicola
Cancedda, Marc Dymetman, and George Foster, edi-
tors, Learning Machine Translation. MIT Press, to ap-
pear.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine transla-
tion. In Proc. of NAACL-HLT?07, Short Paper Volume,
pages 185?188.
158
