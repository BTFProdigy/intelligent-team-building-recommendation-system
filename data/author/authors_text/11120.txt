Proceedings of the Third Workshop on Statistical Machine Translation, pages 35?43,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Performance of a Machine Translation System: a Statistical and
Computational Analysis
Marco Turchi Tijl De Bie
Dept. of Engineering Mathematics
University of Bristol,
Bristol, BS8 1TR, UK
{Marco.Turchi, Tijl.DeBie}@bristol.ac.uk
nello@support-vector.net
Nello Cristianini
Abstract
We present an extensive experimental study
of a Statistical Machine Translation system,
Moses (Koehn et al, 2007), from the point
of view of its learning capabilities. Very ac-
curate learning curves are obtained, by us-
ing high-performance computing, and extrap-
olations are provided of the projected perfor-
mance of the system under different condi-
tions. We provide a discussion of learning
curves, and we suggest that: 1) the represen-
tation power of the system is not currently a
limitation to its performance, 2) the inference
of its models from finite sets of i.i.d. data
is responsible for current performance limita-
tions, 3) it is unlikely that increasing dataset
sizes will result in significant improvements
(at least in traditional i.i.d. setting), 4) it is un-
likely that novel statistical estimation methods
will result in significant improvements. The
current performance wall is mostly a conse-
quence of Zipf?s law, and this should be taken
into account when designing a statistical ma-
chine translation system. A few possible re-
search directions are discussed as a result of
this investigation, most notably the integra-
tion of linguistic rules into the model inference
phase, and the development of active learning
procedures.
1 Introduction and Background
The performance of every learning system is the re-
sult of (at least) two combined effects: the repre-
sentation power of the hypothesis class, determin-
ing how well the system can approximate the target
behaviour; and statistical effects, determining how
well the system can approximate the best element of
the hypothesis class, based on finite and noisy train-
ing information. The two effects interact, with richer
classes being better approximators of the target be-
haviour but requiring more training data to reliably
identify the best hypothesis. The resulting trade-
off, equally well known in statistics and in machine
learning, can be expressed in terms of bias variance,
capacity-control, or model selection. Various theo-
ries on learning curves have been proposed to deal
with it, where a learning curve is a plot describing
performance as a function of some parameters, typ-
ically training set size.
In the context of Statistical Machine Translation
(SMT), where large bilingual corpora are used to
train adaptive software to translate text, this task is
further complicated by the peculiar distribution un-
derlying the data, where the probability of encoun-
tering new words or expressions never vanishes. If
we want to understand the potential and limitations
of the current technology, we need to understand the
interplay between these two factors affecting perfor-
mance. In an age where the creation of intelligent
behaviour is increasingly data driven, this is a ques-
tion of great importance to all of Artificial Intelli-
gence.
These observations lead us to an analysis of learn-
ing curves in machine translation, and to a number of
related questions, including an analysis of the flexi-
bility of the representation class used, an analysis of
the stability of the models with respect to perturba-
tions of the parameters, and an analysis of the com-
putational resources needed to train these systems.
Using the open source package Moses (Koehn et
35
al., 2007) and the Spanish-English Europarl corpus
(Koehn, 2005) we have performed a complete inves-
tigation of the influence of training set size on the
quality of translations and on the cost of training; the
influence of several design choices; the role of data
sizes in training various components of the system.
We use this data to inform a discussion about learn-
ing curves. An analysis of learning curves has pre-
viously been proposed by (Al-Onaizan et al, 1999).
Recent advances in software, data availability and
computing power have enabled us to undertake the
present study, where very accurate curves are ob-
tained on a large corpus.
Since our goal was to obtain high accuracy learn-
ing curves, that can be trusted both for compar-
ing different system settings, and to extrapolate per-
formance under unseen conditions, we conducted a
large-scale series of tests, to reduce uncertainty in
the estimations and to obtain the strongest possible
signals. This was only possible, to the degree of ac-
curacy needed by our analysis, by the extensive use
of a high performance computer cluster over several
weeks of computation.
One of our key findings is that the current per-
formance is not limited by the representation power
of the hypothesis class, but rather by model estima-
tion from data. And that increasing of the size of
the dataset is not likely to bridge that gap (at least
not for realistic amounts in the i.i.d. setting), nor is
the development of new parameter estimation prin-
ciples. The main limitation seems to be a direct
consequence of Zipf?s law, and the introduction of
constraints from linguistics seems to be an unavoid-
able step, to help the system in the identification of
the optimal models without resorting to massive in-
creases in training data, which would also result in
significantly higher training times, and model sizes.
2 Statistical Machine Translation
What is the best function class to map Spanish doc-
uments into English documents? This is a question
of linguistic nature, and has been the subject of a
long debate. The de-facto answer came during the
1990?s from the research community on Statistical
Machine Translation, who made use of statistical
tools based on a noisy channel model originally de-
veloped for speech recognition (Brown et al, 1994;
Och and Weber, 1998; R.Zens et al, 2002; Och and
Ney, 2001; Koehn et al, 2003). A Markovian lan-
guage model, based on phrases rather than words,
coupled with a phrase-to-phrase translation table are
at the heart of most modern systems. Translating a
text amounts to computing the most likely transla-
tion based on the available model parameters. Infer-
ring the parameters of these models from bilingual
corpora is a matter of statistics. By model inference
we mean the task of extracting all tables, parameters
and functions, from the corpus, that will be used to
translate.
How far can this representation take us towards
the target of achieving human-quality translations?
Are the current limitations due to the approximation
error of this representation, or to lack of sufficient
training data? How much space for improvement
is there, given new data or new statistical estima-
tion methods or given different models with differ-
ent complexities?
We investigate both the approximation and the es-
timation components of the error in machine transla-
tion systems. After analysing the two contributions,
we focus on the role of various design choices in
determining the statistical part of the error. We in-
vestigate learning curves, measuring both the role of
the training set and the optimization set size, as well
as the importance of accuracy in the numeric param-
eters.
We also address the trade-off between accuracy
and computational cost. We perform a complete
analysis of Moses as a learning system, assessing the
various contributions to its performance and where
improvements are more likely, and assessing com-
putational and statistical aspects of the system.
A general discussion of learning curves in Moses-
like systems and an extrapolation of performance
are provided, showing that the estimation gap is un-
likely to be closed by adding more data in realistic
amounts.
3 Experimental Setup
We have performed a large number of detailed ex-
periments. In this paper we report just a few, leaving
the complete account of our benchmarking to a full
journal version (Turchi et al, In preparation). Three
experiments allow us to assess the most promis-
36
ing directions of research, from a machine learning
point of view.
1. Learning curve showing translation perfor-
mance as a function of training set size, where
translation is performed on unseen sentences.
The curves, describing the statistical part of the
performance, are seen to grow very slowly with
training set size.
2. Learning curve showing translation perfor-
mance as a function of training set size, where
translation is performed on known sentences.
This was done to verify that the hypothesis
class is indeed capable of representing high
quality translations in the idealized case when
all the necessary phrases have been observed
in training phase. By limiting phrase length
to 7 words, and using test sentences mostly
longer than 20 words, we have ensured that this
was a genuine task of decoding. We observed
that translation in these idealized conditions is
worse than human translation, but much better
than machine translation of unseen sentences.
3. Plot of performance of a model when the nu-
meric parameters are corrupted by an increas-
ing amount of noise. This was done to simu-
late the effect of inaccurate parameter estima-
tion algorithms (due either to imprecise objec-
tive functions, or to lack of sufficient statistics
from the corpus). We were surprised to observe
that accurate estimation of these parameters ac-
counts for at most 10% of the final score. It is
the actual list of phrases that forms the bulk of
the knowledge in the system.
We conclude that the availability of the right mod-
els in the system would allow the system to have a
much higher performance, but these models will not
come from increased datasets or estimation proce-
dures. Instead, they will come from the results of ei-
ther the introduction of linguistic knowledge, or the
introduction of query algorithms, themselves result-
ing necessarily from confidence estimation meth-
ods. Hence these appear to be the two most pressing
questions in this research area.
3.1 Software
Moses (Koehn et al, 2007) is a complete translation
toolkit for academic purposes. It provides all the
components needed to create a machine translation
system from one language to another. It contains dif-
ferent modules to preprocess data, train the language
models and the translation models. These mod-
els can be tuned using minimum error rate training
(Och, 2003). Moses uses standard external tools for
some of these tasks, such as GIZA++ (Och and Ney,
2003) for word alignments and SRILM (Stolcke,
2002) for language modeling. Notice that Moses is a
very sophisticated system, capable of learning trans-
lation tables, language models and decoding param-
eters from data. We analyse the contribution of each
component to the overall score.
Given a parallel training corpus, Moses prepro-
cesses it removing long sentences, lowercasing and
tokenizing sentences. These sentences are used to
train the language and translation models. This
phase requires several steps as aligning words, com-
puting the lexical translation, extracting phrases,
scoring the phrases and creating the reordering
model. When the models have been created, the de-
velopment set is used to run the minimum error rate
training algorithm to optimize their weights. We re-
fer to that step as the optimization step in the rest of
the paper. Test set is used to evaluate the quality of
models on the data. The translated sentences are em-
bedded in a sgm format, such that the quality of the
translation can be evaluated using the most common
machine translation scores. Moses provides BLEU
(K.Papineni et al, 2001) and NIST (Doddington,
2002), but Meteor (Banerjee and Lavie, 2005; Lavie
and Agarwal, 2007) and TER (Snover et al, 2006)
can easily be used instead. NIST is used in this paper
as evaluation score after we observed its high corre-
lation to the other scores on the corpus (Turchi et al,
In preparation).
All experiments have been run using the default
parameter configuration of Moses. It means that
Giza++ has used IBM model 1, 2, 3, and 4 with
number of iterations for model 1 equal to 5, model
2 equal to 0, model 3 and 4 equal to 3; SRILM
has used n-gram order equal to 3 and the Kneser-
Ney smoothing algorithm; Mert has been run fix-
ing to 100 the number of nbest target sentence for
37
each develop sentence, and it stops when none of
the weights changed more than 1e-05 or the nbest
list does not change.
The training, development and test set sentences
are tokenized and lowercased. The maximum num-
ber of tokens for each sentence in the training pair
has been set to 50, whilst no limit is applied to the
development or test set. TMs were limited to a
phrase-length of 7 words and LMs were limited to
3.
3.2 Data
The Europarl Release v3 Spanish-English corpus
has been used for the experiments. All the pairs of
sentences are extracted from the proceedings of the
European Parliament.
This dataset is made of three sets of pairs of sen-
tences. Each of them has a different role: training,
development and test set. The training set contains
1,259,914 pairs, while there are 2,000 pairs for de-
velopment and test sets.
This work contains several experiments on differ-
ent types and sizes of data set. To be consistent
and to avoid anomalies due to overfitting or par-
ticular data combinations, each set of pairs of sen-
tences have been randomly sampled. The number of
pairs is fixed and a software selects them randomly
from the whole original training, development or test
set using a uniform distribution (bootstrap). Redun-
dancy of pairs is allowed inside each subset.
3.3 Hardware
All the experiments have been run on a cluster ma-
chine, http://www.acrc.bris.ac.uk/acrc/hpc.htm. It
includes 96 nodes each with two dual-core opteron
processors, 8 GB of RAM memory per node (2 GB
per core); 4 thick nodes each with four dual-core
opteron processors, 32 GB of RAM memory per
node (4 GB per core); ClearSpeed accelerator boards
on the thick nodes; SilverStorm Infiniband high-
speed connectivity throughout for parallel code mes-
sage passing; General Parallel File System (GPFS)
providing data access from all the nodes; storage -
11 terabytes. Each experiment has been run using
one core and allocating 4Gb of RAM.
4 Experiments
4.1 Experiment 1: role of training set size on
performance on new sentences
In this section we analyse how performance is af-
fected by training set size, by creating learning
curves (NIST score vs training set size).
We have created subsets of the complete corpus
by sub-sampling sentences from a uniform distribu-
tion, with replacement. We have created 10 random
subsets for each of the 20 chosen sizes, where each
size represents 5%, 10%, etc of the complete cor-
pus. For each subset a new instance of the SMT
system has been created, for a total of 200 mod-
els. These have been optimized using a fixed size
development set (of 2,000 sentences, not included
in any other phase of the experiment). Two hun-
dred experiments have then been run on an indepen-
dent test set (of 2,000 sentences, also not included in
any other phase of the experiment). This allowed us
to calculate the mean and variance of NIST scores.
This has been done for the models with and without
the optimization step, hence producing the learning
curves with error bars plotted in Figure 1, represent-
ing translation performance versus training set size,
in the two cases.
The growth of the learning curve follows a typi-
cal pattern, growing fast at first, then slowing down
(traditional learning curves are power laws, in theo-
retical models). In this case it appears to be grow-
ing even slower than a power law, which would be
a surprise under traditional statistical learning the-
ory models. In any case, the addition of massive
amounts of data from the same distribution will re-
sult into smaller improvements in the performance.
The small error bars that we have obtained also al-
low us to neatly observe the benefits of the optimiza-
tion phase, which are small but clearly significant.
4.2 Experiment 2: role of training set size on
performance on known sentences
The performance of a learning system depends both
on the statistical estimation issues discussed in the
previous subsection, and on functional approxima-
tion issues: how well can the function class repro-
duce the desired behaviour? In order to measure this
quantity, we have performed an experiment much
like the one described above, with one key differ-
38
0 2 4 6 8 10 12 14
x 105
6.8
6.9
7
7.1
7.2
7.3
7.4
7.5
7.6
Nist Score vs Training Size
Training Size
N
is
t S
co
re
 
 
Optimized
Not Optimized
Figure 1: ?Not Optimized? has been obtained using a
fixed test set and no optimization phase. ?Optimized?
using a fixed test set and the optimization phase.
ence: the test set was selected randomly from the
training set (after cleaning phase). In this way we
are guaranteed that the system has seen all the nec-
essary information in training phase, and we can as-
sess its limitations in these very ideal conditions.
We are aware this condition is extremely idealized
and it will never happen in real life, but we wanted
to have an upper bound on the performance achiev-
able by this architecture if access to ideal data was
not an issue. We also made sure that the perfor-
mance on translating training sentences was not due
to simple memorization of the entire sentence, ver-
ifying that the vast majority of the sentences were
not present in the translation table (where the max-
imal phrase size was 7), not even in reduced form.
Under these favourable conditions, the system ob-
tained a NIST score of around 11, against a score
of about 7.5 on unseen sentences. This suggests
that the phrase-based Markov-chain representation
is sufficiently rich to obtain a high score, if the nec-
essary information is contained in the translation and
language models.
For each model to be tested on known sentences,
we have sampled ten subsets of 2,000 sentences each
from the training set.
The ?Optimized, Test on Training Set? learn-
ing curve, see figure 2, represents a possible upper
bound on the best performance of this SMT sys-
tem, since it has been computed in favourable con-
ditions. It does suggest that this hypothesis class
has the power of approximating the target behaviour
more accurately than we could think based on per-
formance on unseen sentences. If the right informa-
tion has been seen, the system can reconstruct the
sentences rather accurately. The NIST score com-
puted using the reference sentences as target sen-
tences is around 15, we identify the relative curve as
?Human Translation?. At this point, it seems likely
that the process with which we learn the necessary
tables representing the knowledge of the system is
responsible for the performance limitations.
The gap between the ?Optimized, Test on Train-
ing Set? and the ?Optimized? curves is even more in-
teresting if related to the slow growth rate in the pre-
vious learning curve: although the system can repre-
sent internally a good model of translation, it seems
unlikely that this will ever be inferred by increasing
the size of training datasets in realistic amounts.
The training step results in various forms of
knowledge: translation table, language model and
parameters from the optimization. The internal
models learnt by the system are essentially lists
of phrases, with probabilities associated to them.
Which of these components is mostly responsible
for performance limitations?
4.3 Experiment 3: effect on performance of
increasing noise levels in parameters
Much research has focused on devising improved
principles for the statistical estimation of the pa-
rameters in language and translation models. The
introduction of discriminative graphical models has
marked a departure from traditional maximum like-
lihood estimation principles, and various approaches
have been proposed.
The question is: how much information is con-
tained in the fine grain structure of the probabilities
estimated by the model? Is the performance improv-
ing with more data because certain parameters are
estimated better, or just because the lists are grow-
ing? In the second case, it is likely that more sophis-
ticated statistical algorithms to improve the estima-
tion of probabilities will have limited impact.
In order to simulate the effect of inaccurate esti-
mation of the numeric parameters, we have added
increasing amount of noise to them. This can either
represent the effect of insufficient statistics in esti-
mating them, or the use of imperfect parameter esti-
39
0 2 4 6 8 10 12 14
x 105
6
7
8
9
10
11
12
13
14
15
16
Nist Score vs Training Size
Training Size
N
is
t S
co
re
 
 
Not Optimized
Optimized
Optimized, Test On Training Set
Human Translation
Figure 2: Four learning curves have been compared. ?Not Optimized? has been obtained using a fixed test set and no
optimization phase. ?Optimized? using a fixed test set and the optimization phase. ?Optimized Test On Training Set?
a test set selected by the training set for each training set size and the optimization phase. ?Human Translation? has
been obtained by computing NIST using the reference English sentence of the test set as target sentences.
mation biases. We have corrupted the parameters in
the language and translation models, by adding in-
creasing levels of noise to them, and measured the
effect of this on performance.
One model trained with 62,995 pairs of sentences
has been chosen from the experiments in Section
4.1. A percentage of noise has been added to each
probability in the language model, including condi-
tional probability and back off, translation model,
bidirectional translation probabilities and lexical-
ized weighting. Given a probability p and a percent-
age of noise, pn, a value has been randomly selected
from the interval [-x,+x], where x = p * pn, and
added to p. If this quantity is bigger than one it has
been approximated to one. Different values of per-
centage have been used. For each value of pn, five
experiment have been run. The optimization step
has not been run.
We see from Figure 3 that the performance does
not seem to depend crucially on the fine structure of
the parameter vectors, and that even a large addition
of noise (100%) produces a 10% decline in NIST
score. This suggests that it is the list itself, rather
0 10 20 30 40 50 60 70 80 90 100 110
6.6
6.65
6.7
6.75
6.8
6.85
"Perturbed" Nist Score vs Percentage of Perturbation
Percentage of Perturbation
"
Pe
rtu
rb
ed
" N
ist
 S
co
re
Figure 3: Each probability of the language and translation
models has been perturbed adding a percentage of noise.
This learning curve reports the not optimized NIST score
versus the percentage of perturbation applied. These re-
sults have been obtained using a fixed training set size
equal to 62,995 pairs of sentences.
40
0 2 4 6 8 10 12 14
x 105
0
500
1000
1500
2000
2500
CPU Computational Time in minutes vs Training Size
Training Size
CP
U 
Co
m
pu
ta
tio
na
l T
im
e 
in
 m
in
ut
es
 
 
Training Time
Tuning Time
Figure 4: Training and tuning user time vs training set
size. Time quantities are expressed in minutes.
than the probabilities in it, that controls the perfor-
mance. Different estimation methods can produce
different parameters, but this does not seem to mat-
ter very much. The creation of a more complete list
of words, however, seems to be the key to improve
the score. Combined with the previous findings, this
would mean that neither more data nor better statis-
tics will bridge the performance gap. The solution
might have to be found elsewhere, and in our Dis-
cussion section we outline a few possible avenues.
5 Computational Cost
The computational cost of models creation and
development-phase has been measured during the
creation of the learning curves. Despite its efficiency
in terms of data usage, the development phase has a
high cost in computational terms, if compared with
the cost of creating the complete language and trans-
lation models.
For each experiment, the user CPU time is com-
puted as the sum of the user time of the main process
and the user time of the children.
These quantities are collected for training, devel-
opment, testing and evaluation phases. In figure 4,
training and tuning user times are plotted as a func-
tion of the training set size. It is evident that increas-
ing the training size causes an increase in training
time in a roughly linear fashion.
It is hard to find a similar relationship for the tun-
ing time of the development phase. In fact, the tun-
ing time is strictly connected with the optimization
algorithm and the sentences in the development set.
We can also see in figure 4 that even a small devel-
opment set size can require a large amount of tun-
ing time. Each point of the tuning time curve has a
big variance. The tuning phase involves translating
the development set many times and hence its cost
depends very weakly on the training set size, since a
large training set leads to larger tables and these lead
to slightly longer test times.
6 Discussion
The impressive capability of current machine trans-
lation systems is not only a testament to an incredi-
bly productive and creative research community, but
can also be seen as a paradigm for other Artificial In-
telligence tasks. Data driven approaches to all main
areas of AI currently deliver the state of the art per-
formance, from summarization to speech recogni-
tion to machine vision to information retrieval. And
statistical learning technology is central to all ap-
proaches to data driven AI.
Understanding how sophisticated behaviour can
be learnt from data is hence not just a concern for
machine learning, or to individual applied commu-
nities, such as Statistical Machine Translation, but
rather a general concern for modern Artificial Intelli-
gence. The analysis of learning curves, and the iden-
tification of the various limitations to performance
is a crucial part of the machine learning method,
and one where statistics and algorithmics interact
closely.
In the case of Statistical Machine Translation, the
analysis of Moses suggests that the current bottle-
neck is the lack of sufficient data, not the function
class used for the representation of translation sys-
tems. The clear gap between performance on train-
ing and testing set, together with the rate of the
learning curves, suggests that improvements may be
possible but not by adding more data in i.i.d. way as
done now. The perturbation analysis suggests that
improved statistical principles are unlikely to make
a big difference either.
Since it is unlikely that sufficient data will be
available by simply sampling a distribution, one
needs to address a few possible ways to transfer
large amounts of knowledge into the system. All of
them lead to open problems either in machine learn-
41
ing or in machine translation, most of them having
been already identified by their respective communi-
ties as important questions. They are actively being
worked on.
The gap between performances on training and
on test sets is typically affected by model selection
choices, ultimately controlling the trade off between
overfitting and underfitting. In these experiments the
system used phrases of length 7 or less. Changing
this parameter might reflect on the gap and this is
the focus of our current work.
A research programme naturally follows from
our analysis. The first obvious approach is an ef-
fort to identify or produce datasets on demand (ac-
tive learning, where the learning system can request
translations of specific sentences, to satisfy its infor-
mation needs). This is a classical machine learning
question, that however comes with the need for fur-
ther theoretical work, since it breaks the traditional
i.i.d. assumptions on the origin of data. Further-
more, it would also require an effective way to do
confidence estimation on translations, as traditional
active learning approaches are effectively based on
the identification (or generation) of instances where
there is low confidence in the output (Blatz et al,
2004; Ueffing and Ney, 2004; Ueffing and Ney,
2005b; Ueffing and Ney, 2005a).
The second natural direction involves the intro-
duction of significant domain knowledge in the form
of linguistic rules, so to dramatically reduce the
amount of data needed to essentially reconstruct
them by using statistics. These rules could take the
form of generation of artificial training data, based
on existing training data, or a posteriori expansion of
translation and language tables. Any way to enforce
linguistic constraints will result in a reduced need
for data, and ultimately in more complete models,
given the same amount of data (Koehn and Hoang,
2007).
Obviously, it is always possible that the identifi-
cation of radically different representations of lan-
guage might introduce totally different constraints
on both approximation and estimation error, and this
might be worth considering.
What is not likely to work. It does not seem that
the introduction of more data will change the situ-
ation significantly, as long as the data is sampled
i.i.d. from the same distribution. It also does not
seem that more flexible versions of Markov mod-
els would be likely to change the situation. Finally,
it does not seem that new and different methods to
estimate probabilities would make much of a differ-
ence. Our perturbation studies show that significant
amounts of noise in the parameters result into very
small variations in the performance. Note also that
the current algorithm is not even working on refin-
ing the probability estimates, as the rate of growth of
the tables suggests that new n-grams are constantly
appearing, reducing the proportion of time spent re-
fining probabilities of old n-grams.
It does seem that the control of the performance
relies on the length of the translation and language
tables. Ways are needed to make these tables grow
much faster as a function of training set size; they
can either involve active selection of documents to
translate, or the incorporation of linguistic rules to
expand the tables without using extra data.
It is important to note that many approaches sug-
gested above are avenues currently being actively
pursued, and this analysis might be useful to decide
which one of them should be given priority.
7 Conclusions
We have started a series of extensive experimental
evaluations of performance of Moses, using high
performance computing, with the goal of under-
standing the system from a machine learning point
of view, and use this information to identify weak-
nesses of the system that can lead to improvements.
We have performed many more experiments that
cannot be reported in this workshop paper, and will
be published in a longer report (Turchi et al, In
preparation). In general, our goal is to extrapolate
the performance of the system under many condi-
tions, to be able to decide which directions of re-
search are most likely to deliver improvements in
performance.
Acknowledgments
Marco Turchi is supported by the EU Project
SMART. The authors thank Callum Wright, Bris-
tol HPC Systems Administrator, and Moses mailing
list.
42
References
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,
D. Melamed, F. J. Och, D. Purdy, N. A. Smith, and
D. Yarowsky. 1999. Statistical machine translation:
Final report. Technical report, Johns Hopkins Univer-
sity 1999 Summer Workshop on Language Engineer-
ing, Center for Speech and Language Processing.
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved correla-
tion with human judgments. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confi-
dence estimation for machine translation. In COLING
?04: Proceedings of the 20th international conference
on Computational Linguistics, page 315, Morristown,
NJ, USA. Association for Computational Linguistics.
P. F. Brown, S. Della Pietra, V.t J. Della Pietra, and R. L.
Mercer. 1994. The mathematic of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of the second international con-
ference on Human Language Technology Research,
pages 138?145, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceedings
of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology, pages 48?54, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In the Annual Meet-
ing of the Association for Computational Linguistics,
demonstration session.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Machine Translation
Summit X, pages 79?86.
K.Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL ?02, pages 311?
318, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
A. Lavie and A. Agarwal. 2007. Meteor: An automatic
metric for mt evaluation with high levels of correla-
tion with human judgments. In ACL ?07: Proceedings
of 45th Annual Meeting of the Association for Com-
putational Linguistics. Association for Computational
Linguistics.
F. J. Och and H. Ney. 2001. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL ?02, pages 295?
302, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och and H. Weber. 1998. Improving statistical nat-
ural language translation with categories and rules. In
COLING-ACL, pages 985?989.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL ?03,
pages 160?167, Morristown, NJ, USA. Association for
Computational Linguistics.
R.Zens, F. J.Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In KI ?02: Proceedings
of the 25th Annual German Conference on AI, pages
18?32, London, UK. Springer-Verlag.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of the
7th Conference of the Association for Machine Trans-
lation in the Americas, pages 223?231. Association for
Machine Translation in the Americas.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Intl. Conf. on Spoken Language Pro-
cessing.
M. Turchi, T. De Bie, and N. Cristianini. In preparation.
Learning analysis of a machine translation system.
N. Ueffing and H. Ney. 2004. Bayes decision rules
and confidence measures for statistical machine trans-
lation. In EsTAL-2004, pages 70?81.
N. Ueffing and H. Ney. 2005a. Application of word-level
confidence measures in interactive statistical machine
translation. In EAMT-2005, pages 262?270.
N. Ueffing and H. Ney. 2005b. Word-level confidence
estimation for machine translation using phrase-based
translation models. In Proceedings of HLT ?05, pages
763?770, Morristown, NJ, USA. Association for Com-
putational Linguistics.
43
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 409?420, Dublin, Ireland, August 23-29 2014.
Machine Translation Quality Estimation Across Domains
Jos
?
e G. C. de Souza
University of Trento
Fondazione Bruno Kessler
Trento, Italy
desouza@fbk.eu
Marco Turchi
Fondazione Bruno Kessler
Trento, Italy
turchi@fbk.eu
Matteo Negri
Fondazione Bruno Kessler
Trento, Italy
negri@fbk.eu
Abstract
Machine Translation (MT) Quality Estimation (QE) aims to automatically measure the quality
of MT system output without reference translations. In spite of the progress achieved in re-
cent years, current MT QE systems are not capable of dealing with data coming from different
train/test distributions or domains, and scenarios in which training data is scarce. We investigate
different multitask learning methods that can cope with such limitations and show that they over-
come current state-of-the-art methods in real-world conditions where training and test data come
from different domains.
1 Introduction
Machine Translation (MT) Quality Estimation (QE) aims to automatically predict the quality of MT
output without using reference translations (Blatz et al., 2003; Specia et al., 2009). QE systems usually
employ supervised machine learning models that use different information extracted from (source, target)
sentence pairs as features along with quality scores as labels. The notion of quality that these models
measure can be indicated by different scores. Some examples are the average number of edits required
to post-edit the MT output, i.e., human translation edit rate
1
(HTER (Snover et al., 2006)), and the time
(in seconds) required to post-edit a translation produced by an MT system (Specia, 2011).
Research on QE has received a strong boost in recent years due to the increase in the usage of MT
systems in real-world applications. Automatic and reference-free MT quality prediction demonstrated
to be useful for different applications, such as: deciding whether the translation output can be published
without post-editing (Soricut and Echihabi, 2010), filtering out low-quality translation suggestions that
should be rewritten from scratch (Specia et al., 2009), selecting the best translation output from a pool
of MT systems (Specia et al., 2010), and informing readers of the translation whether it is reliable or not
(Turchi et al., 2012). Another example is the computer-assisted translation (CAT) scenario, in which it
might be necessary to predict the quality of translation suggestions generated by different MT systems
to support the activity of post editors working with different genres of text.
The dominant QE framework presents some characteristics that can limit models? applicability in
such real-world scenarios. First, the scores used as training labels (HTER, time) are costly to obtain
because they are derived from manual post-editions of MT output. Such requirement makes it difficult
to develop models for domains in which there is a limited amount of labeled data. Second, the learning
methods currently used (for instance in the framework of QE shared evaluation campaigns)
2
assume that
training and test data are sampled from the same distribution. Though reasonable as a first evaluation
setting to promote research in the field, this controlled scenario is not realistic as different data in real-
world applications might be post-edited by different translators, the translations might be generated by
different MT systems and the documents being translated might belong to different domains or genres. To
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the
number of words in the reference. Lower HTER values indicate better translations.
2
In the last two editions of the yearly Workshop on Machine Translation, several QE shared tasks have been proposed
(Callison-Burch et al., 2012; Bojar et al., 2013).
409
overcome these limitations a plausible research objective is to exploit techniques that: (i) allow domains
and distributions of features to be different between training and test data, and (ii) that cope with the
scarce amount of training labels by sharing information across domains, a common scenario for transfer
learning.
In this paper we investigate the use of techniques that can exploit the training instances from different
domains to learn a QE model for a specific target domain for which there is a small amount of labeled
data. In particular, we are interested in approaches that allow not only learning from one single source
domain but also from multiple source domains simultaneously, by leveraging the labels from all available
data to improve results in a target domain.
Given these requirements, we experiment with different multitask learning techniques that perform
transfer learning via a common task structure (domain relatedness). Furthermore, we employ an approach
based on feature augmentation that has been successfully used in other natural language processing tasks.
We present a series of experiments over three domains with increasing amounts of training data, showing
that our adaptive approaches outperform competitive baselines.
The contributions of our work are: (i) a first exploration of techniques that overcome the limitation
of current QE learning methods when dealing with data with different training and test distributions and
domains, and (ii) an empirical verification of the amount of training data required by such techniques to
outperform competitive baselines on different target domains. To the best of our knowledge, this is the
first work addressing the challenges posed by domain adaptation in MT QE.
2 Related Work
Quality estimation has recently gained increasing attention, also boosted by two evaluation campaigns
organized within the Workshop on Machine Translation (WMT) (Callison-Burch et al., 2012; Bojar et
al., 2013). The bulk of work done so far has focused on the controlled WMT evaluation framework and,
in particular, on two major aspects of the problem: feature engineering and machine learning methods.
Feature engineering accounts for linguistically-based predictors that aim to model different perspec-
tives of the quality estimation problem. The research ranges from identifying indicators that approximate
the complexity of translating the source sentence and designing features that model the fluency of the
automatically generated translation, to linguistically motivated measures that estimate how adequate the
translation is in comparison to the source sentence in terms of meaning (Blatz et al., 2003; Mehdad et
al., 2012; Hardmeier et al., 2012; Rubino et al., 2012; Specia et al., 2012; de Souza et al., 2013a).
State-of-the-art QE explores different supervised linear or non-linear learning methods for regression
or classification such as Support Vector Machines (SVM), different types of Decision Trees, Neural
Networks, Elastic-Net, Gaussian Processes, Naive Bayes, among others (Specia et al., 2009; Buck,
2012; Beck et al., 2013; Souza et al., 2014). Another aspect related to the learning methods that has
received attention is the optimal selection of features in order to overcome issues related with the high-
dimensionality of the feature space (Soricut et al., 2012; de Souza et al., 2013a; Beck et al., 2013; de
Souza et al., 2013b).
Despite constant improvements, such learning methods have limitations. The main one is that they
assume that both training and test data are independently and identically distributed. As a consequence,
when they are applied to data from a different distribution or domain they show poor performance. This
limitation harms the performance of QE systems for several real-world applications, such as CAT envi-
ronments. Advanced CAT systems currently integrate suggestions obtained from MT engines with those
derived from translation memories (TMs). In such framework, the compelling need to speed up the trans-
lation process and reduce its costs by presenting human translators with good-quality suggestions raises
interesting research challenges for the QE community. In such environments, translation jobs come from
different domains that might be translated by different MT systems and are routed to professional transla-
tors with different idiolect, background and quality standards (Turchi et al., 2013). Such variability calls
for flexible and adaptive QE solutions by investigating two directions: (i) modeling translator behaviour
(Turchi et al., 2014) and (ii) maximize the learning capabilities from all the available data. The second
research objective motivates our investigation on methods that allow the training and test domains and
410
the distributions to be different.
Recent work in QE focused on aspects that are problematic even in the controlled WMT scenario, and
are closely related to the flexibility/adaptability issue. Focusing on the first of the two aforementioned
directions (i.e. modeling translators? behaviour), Cohn and Specia (2013) propose a Multitask Gaussian
Process method that jointly learns a series of annotator-specific models and that outperforms models
trained for each annotator. Our work differs from theirs in that we are interested in the latter research
direction (i.e. coping with domain and distribution diversity) and we use in and out-of-domain data to
learn robust in-domain models. Our scenario represents a more challenging setting than the one tackled
in (Cohn and Specia, 2013), which does not consider different domains.
In transfer learning there are many techniques suitable to fulfill our requirements. The aim of transfer
learning is to extract the knowledge from one or more source tasks and apply it to a target task (Pan
and Yang, 2010). One type of transfer learning is multitask learning (MTL), which uses domain-specific
training signals of related tasks to improve model generalization (Caruana, 1997). Although it was not
originally thought for transferring knowledge to a new task, MTL can be used to achieve this objective
due to its capability to capture task relatedness, which is important knowledge that can be applied to a
new task (Jiang, 2009).
Domain adaptation is a kind of transfer learning in which source and target domains (i.e. training and
test) are different but the tasks are the same (Pan and Yang, 2010). The domain adaptation techniques
that inspire our work have been successfully applied to a variety of NLP tasks (Blitzer et al., 2006;
Jiang and Zhai, 2007). For instance, an effective solution for supervised domain adaptation, EasyAdapt
(SVR FEDA henceforth), was proposed in (Daum?e III, 2007) and applied to named entity recognition,
part-of-speech tagging and shallow parsing. The approach transforms the domain adaptation problem
into a standard learning problem by augmenting the source and target feature set. The feature space is
transformed to be a cross-product of the features of the source and target domains augmented with the
original target domain features. In supervised domain adaptation one has access to out-of-domain labels
and wants to leverage a small amount of available in-domain labeled data to train a model (Daum?e III,
2007), the case of this study. This is different from the semi-supervised case in which in-domain labels
are not available.
3 Adaptation for QE
An important assumption in MTL is that different tasks (domains in our case) are correlated via a certain
structure. Examples of such structures are the hidden layers in a neural network (Caruana, 1997) and
shared feature representation (Argyriou et al., 2007) among others. This common structure allows for
knowledge transfer among tasks and has been demonstrated to improve model generalization over single
task learning (STL) for different problems in different areas. Under this scenario, several assumptions
can be made about the relatedness among the tasks, leading to different transfer structures. We explore
three approaches to MTL that deal with task relatedness in different ways. These are the ?Dirty? approach
to MTL (Jalali et al., 2010), Sparse Trace MTL (Chen et al., 2012) and Robust MTL (Chen et al., 2011).
The three approaches use different regularization techniques that capture task relatedness using norms
over the weights of the features.
Before describing the three approaches, we introduce some basic notation similar to (Chen
et al., 2011). In MTL there are T tasks and each task t ? T has m training samples
{(x
(t)
1
, y
(t)
1
), . . . , (x
(t)
m
, y
(t)
m
)}, with x
(t)
i
? R
d
where d is the number of features and y
(t)
i
? R is the
output (the response variable or label). The input features and labels are stacked together to form two
different matrices X
(t)
= [x
(t)
1
, . . . , x
(t)
m
] and Y
(t)
= [x
(t)
1
, . . . , x
(t)
m
], respectively. The weights of the
features for each task are represented by W , where each column corresponds to a task and each row
corresponds to a feature.
The ?Dirty? approach to MTL follows the idea that different tasks may share the same discriminative
features (Argyriou et al., 2007). However, it also considers that different tasks might have different
discriminative features that are inherent to each task. Therefore, the method encourages shared-sparsity
among tasks and among features in each task. It decomposes W into two components, one is a row-
411
sparsed matrix that corresponds to the features shared among the tasks and the other is an element-wise
sparse matrix that corresponds to the non-shared features that are important for each task independently.
More formally, the ?Dirty? approach is explained by Equation 1.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
s
||S||
1
+ ?
b
||B||
1,?
subject to: W = S +B (1)
where ||(W
(t)
X
(t)
? Y
(t)
)||
2
2
is the least squares loss function, S is the regularization term that en-
courages element-wise sparsity and B is the block-structured row-sparsity regularizer. The ||.||
2
is the
l
2
-norm (Euclidean distance), ||.||
1
is the l
1
-norm (given by
?
i=1
|x
i
|) and ||.||
1,?
is the row grouped l
1
-
norm. The ?
s
and ?
b
are non-negative trade-off parameters that control the amount of regularization
applied to S and B, respectively.
Sparse Trace MTL considers the problem of learning incoherent sparse and low-rank patterns from
multiple related tasks. This approach captures task relationship via a shared low-rank structure of the
weight matrix W . As computing the low-rank structure of a matrix leads to a NP-hard optimization
problem, Chen et al. (2012) proposed to compute the trace norm as a surrogate, making the optimization
problem tractable. In addition to learning the low-rank patterns, this method also considers the fact that
different tasks may have different inherent discriminative features. It decomposes W into two compo-
nents: S, which models element-wise sparsity, and Q, which captures task relationship via the trace
norm. The convex problem minimized by Sparse Trace is given in Equation 2.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
s
||S||
1
subject to: W = S +Q, ||Q||
?
< ?
p
(2)
where ||.||
?
is the trace norm, given by the sum of the singular values ?
i
of W , i.e., ||W ||
?
=
?
i=1
?
i
(W ). Here, ?
p
controls the rank of Q and ?
s
controls the sparsity of S.
The key assumption in MTL is that tasks are related in some way. However, this assumption might not
hold for a series of real-world problems. In situations in which tasks are not related a negative transfer
of information among tasks might occur, harming the generalization of the model. One way to deal
with this problem is to: (i) group related tasks in one structure and share knowledge among them, and
(ii) identify irrelevant tasks maintaining them in a different group that does not share information with
the first group. This is the idea of Robust MTL (RMTL henceforth). The algorithm approximates task
relatedness via a low-rank structure like Sparse Trace and identifies outlier tasks using a group-sparse
structure (column-sparse, at task level). Robust MTL is described by Equation 3. It employs a non-
negative linear combination of the trace norm (the task relatedness component L) and a column-sparse
structure induced by the l
1,2
-norm (the outlier task detection component S). If a task is an outlier it will
have non-zero entries in S.
min
W
T
?
t=1
||(W
(t)
X
(t)
? Y
(t)
)||
2
2
+ ?
l
||L||
?
+ ?
s
||S||
1,2
subject to: W = L+ S (3)
where ||S||
1,2
is the group regularizer that induces sparsity on the tasks.
4 Experimental Setting
In this section we describe the data used for our experiments, the features extracted, the set up of the
learning methods, the baselines used for comparison and the evaluation of the models. The goal of our
experiments is to show that the methods presented in Section 3 outperform competitive baselines and
standard QE learning methods that are not capable of adapting to different domains. We experiment with
three different domains of comparable size and evaluate the performance of the adaptive methods and the
standard techniques with different amounts of training data. The MTL models described in section 3 are
trained with the Malsar toolkit implementation (Zhou et al., 2012). The hyper-parameters are optimized
412
using 5-fold cross-validation in a grid search procedure. The parameter values are searched in an interval
ranging from 10
?3
to 10
3
.
4.1 Data
Our experiments focus on the English-French language pair and encompass three very different domains:
newswire text (henceforth News), transcriptions of Technology Entertainment Design talks (TED) and
Information Technology manuals (IT). Such domains are a challenging combination for adaptive systems
since they come from very different sources spanning speech and written discourse (TED and News/IT,
respectively) as well as a very well defined and controlled vocabulary in the case of IT.
Each domain is composed of 363 tuples formed by the source sentence in English, the French trans-
lation produced by an MT system and a human post-edition of the translated sentence. For each pair
(translation, post-edition) we use as labels the HTER score computed with TERCpp
3
. For the three do-
mains we use half of the data for training (181 instances) and half of the data for testing (182 instances).
The limited amount of instances for training contrasts with the 800 or more instances of the WMT evalu-
ation campaigns and is closer to real-world applications where the availability of large and representative
training sets is far from being guaranteed (e.g. the CAT scenario).
The sentence tuples for the first two domains are randomly sampled from the Trace corpus
4
. The
translations were generated by two different MT systems, a state-of-the-art phrase-based statistical MT
system and a commercial rule-based system. Furthermore, the translations were post-edited by up to four
different translators, as described in (Wisniewski et al., 2013).
Domain No. of tokens Vocab. size Avg. sent. length
TED source 6858 1659 19
TED target 7016 1828 19
IT source 3310 1004 9
IT target 3134 1049 8
News source 7605 2273 21
News target 8230 2346 23
Table 1: Datasets statistics for each domain.
The TED talks domain is formed by subtitles of several talks in a range of topics presented in the TED
conferences. The complete dataset has been used for MT and automatic speech recognition systems
evaluation within the International Workshop on Spoken Language Translation (IWSLT). The News
domain is formed by newswire text used in WMT translation campaigns and covers different topics. The
IT texts come from a software user manual translated by a statistical MT system based on the state-of-
the-art phrase-based Moses toolkit (Koehn et al., 2007) trained on about 2M parallel sentences. The
post-editions were collected from one professional translator operating on the Matecat
5
CAT tool in
real working conditions. Table 1 provides macro-indicators (number of tokens, vocabulary size, average
sentence length) that evidence the large difference between the domains addressed by our experiments
and give an idea of the difficulty of the task.
A peculiarity of the TED domain is that it is formed by manual transcriptions of speech translated by
different MT systems, configuring a different type of discourse than News and IT. In TED, the vocabulary
size in the source and target sentences is lower than that of the News domain but higher than IT. News
presents the most varied vocabulary, which is an evidence of the more varied lexical choice represented
by the several topics that compose the domain. Moreover, News has the highest average sentence length,
a characteristic of non-technical written discourse, which tends to have longer sentences than spoken
discourse and domains dominated by technical jargon. Such a characteristic is exactly what differentiates
IT from the other two domains. IT sentences are technical and present a reduced average number of
3
http://sourceforge.net/projects/tercpp/
4
http://anrtrace.limsi.fr/trace_postedit.tar.bz2
5
www.matecat.com
413
words, as evidenced by the vocabulary size (the smallest among the three domains). These numbers
suggest a divergence between IT and the other two domains, possibly making adaptation more difficult.
4.2 Features
For all the experiments we use the same feature set composed of seventeen features proposed in (Specia et
al., 2009). The set is formed by features that model the complexity of translating the source sentence (e.g.
the average source token length or the number of tokens in the source sentence), and the fluency of the
translated sentence produced by the MT system (e.g. the language model probability of the translation).
The decision to use this feature set is motivated by the fact that it demonstrated to be robust across
language pairs, MT systems and text domains (Specia et al., 2009). The 17 features are:
? number of tokens in the source sentence and in the generated translation;
? average source token length;
? average number of occurences of the target word within the generated translation;
? language model probability of the source sentence and generated translation;
? average number of translations per source word in the sentence: as given by IBM 1 model thresh-
olded so that P (t|s) > 0.2 weighted by the inverse frequency of each word in the source side of the
SMT training corpus?;
? average number of translations per source word in the sentence: as given by IBM 1 model thresh-
olded so that P (t|s) > 0.01 weighted by the inverse frequency of each word in the source side of
the SMT training corpus;
? percentage of unigrams?, bigrams and trigrams? in the first quartile of frequency (lower fre-
quency words) in a corpus of the source language;
? percentage of unigrams?, bigrams and trigrams in the fourth quartile of frequency (higher fre-
quency words) in a corpus of the source language;
? percentage of unigrams in the source sentence seen in the source side of the SMT training corpus;
? number of punctuation marks in the source sentence and in the hypothesis translation;
4.3 Baselines
As a term of comparison, we consider these baselines in our experiments. A simple to implement but
difficult to beat baseline when dealing with regression on tasks with different distributions is to compute
the mean of the training labels and use it as the prediction for each testing point (Rubino et al., 2013).
Hereafter we refer to this baseline as ?. Since supervised domain adaptation techniques should outper-
form models that are trained only on the available in-domain data, we also use as baseline the regressor
built only on the available in-domain data (SVR in-domain). Furthermore, as a third baseline, we train a
regressor by pooling together training data of all domains, combining source and target data without any
kind of task relationship mechanism (SVR Pooling).
The baselines are trained on the feature set described earlier in Section 4.2 with an SVM regression
(SVR) method using the implementation of Scikit-learn (Pedregosa et al., 2011). The radial basis func-
tion (RBF) kernel is used for all baselines. The hyper-parameters of the model are optimized using
randomized search optimization process with 50 iterations as described in (Bergstra and Bengio, 2012)
and used previously for QE in (de Souza et al., 2013a). The best parameters are found using 5-fold
cross-validation on the training data and , ? and C are sampled from exponential distributions scaled at
0.1 for the first two parameters and scaled at 100 for the last one. It is important to notice that the SVR
with RBF kernel methods learn non-linear models that have been shown to perform better than linear
models on the set of features used for predicting HTER. On the contrary, the MTL methods presented in
Section 3 are methods that do not explore kernels or any other kind of non-linear learning method.
414
Source / Target IT
tgt
News
tgt
TED
tgt
IT
src
0.2081 0.2341 0.2232
News
src
0.2368 0.1690 0.2130
TED
src
0.2183 0.2263 0.1928
Table 2: Results of the SVR in-domain baseline trained and evaluated in each domain (average of 50
different shuffles). Rows represent the domain data used to train the model and columns represent the
domain data used to evaluate the model. Scores are MAE.
4.4 Evaluation
The accuracy of the models is evaluated with the mean absolute error (MAE), which was also used in
previous work and in the WMT QE shared tasks (Bojar et al., 2013). MAE is the average of the absolute
difference between the prediction y?
i
of a model and the gold standard response y
i
(Equation 4). As it is
an error measure, lower values mean better performance.
MAE =
1
m
m
?
i=1
|y?
i
? y
i
| (4)
To test the statistical significance of our results we need to perform comparisons of multiple models.
In addition, we would like to test the significance over different training amounts. Given these require-
ments we need to perform multiple hypothesis tests instead of paired tests. It has been shown that for
comparisons of multiple machine learning models, the recommended approach is to use a non-parametric
multiple hypothesis test followed by a post-hoc analysis that compares each pair of hypothesis (Dem?sar,
2006). In our experiments we use the Friedman test (Friedman, 1937; Friedman, 1940) followed by a
post-hoc analysis of the pairs of regressors using Holm?s procedure (Holm, 1979) to perform the pairwise
comparisons when the null hypothesis is rejected. All tests for both Friedman and post-hoc analysis are
run with ? = 0.05. For more details about these methods, we refer the reader to (Dem?sar, 2006; Garcia
and Herrera, 2008) which provide a complete review about the application of multiple hypothesis testing
to machine learning methods.
5 Results and Discussion
Our experiments are organized as follows. First, we evaluate the performance of single task learning
methods on different cross-domain experiments. Then, we report the evaluation for the multitask learning
methods and discuss the results.
5.1 Single Task Learning
With the objective of having an insight about the difference between the domains, we train the SVR
in-domain baseline with all available training data for each domain and evaluate its performance on the
same domain and in the two remaining domains.
Results are reported in Table 2, where the diagonal shows the figures for the in-domain evaluation.
These numbers suggest that the IT domain configures a more difficult challenge for the learning algo-
rithm. The IT in-domain model (IT
src
-IT
tgt
) presents a performance 21% inferior to News and 8%
inferior to TED. For all models trained on a source domain different than the target domain there is a
drop in performance, as it is expected from a system that assumes that training and test data are sampled
from the same distribution. In addition, when predicting IT using the model trained on News, we have a
perfomance drop of 13% whereas using the model trained on TED the performance drops up to 4%.
5.2 Multitask learning
We run the baselines described in Section 4.3 and the methods described in Section 3 on different
amounts of training data, ranging from 18 to 181 instances (10% and 100%, respectively). The mo-
tivation is to verify how much training data is required by the MTL methods to outperform the baselines
for a target domain. Table 3 presents the results for the three domains with models trained on 30, 50 and
415
100% of the training data (54, 90 and 181 instances, respectively). Each method was run on 50 different
train/test splits of the data in order to account for the variability of points in each split.
Method TED News IT
30 % of training data (54 instances)
mean 0.1951 0.1711 0.2174
SVR In-Domain 0.2013 0.1753 0.2235
SVR Pooling 0.1962 0.1899 0.2201
SVR FEDA 0.1952 0.1839 0.2193
MTL Dirty 0.1954 0.1708 0.2193
MTL SparseTrace 0.1976 0.1743 0.2222
MTL RMTL 0.1946 0.1685 0.2162
50% of training data (90 instances)
mean 0.1943 0.1707 0.2170
SVR In-Domain 0.1976 0.1711 0.2183
SVR Pooling 0.1951 0.1865 0.2191
SVR FEDA 0.1937 0.1806 0.2161
MTL Dirty 0.1927 0.1678 0.2148
MTL SparseTrace 0.1922 0.1672 0.2157
MTL RMTL 0.1878 0.1653 0.2119
100% of training data (181 instances)
mean 0.1936 0.1690 0.2162
SVR In-Domain 0.1928 0.1690 0.2081
SVR Pooling 0.1927 0.1849 0.2203
SVR FEDA 0.1908 0.1757 0.2107
MTL Dirty 0.1878 0.1666 0.2083
MTL SparseTrace 0.1881 0.1661 0.2094
MTL RMTL 0.1846 0.1653 0.2075
Table 3: Average performance of fifty runs
of the models on different train and test splits
with 30, 50 and 100 percent of training data.
The average scores reported are the MAE.
Figure 1: Visualization of the RMTL task outlier
model when trained on all the 181 instances of
training data. Cells with darker shades are closer
to zero. Cells with lighter shades are closer to one.
Columns with only black entries are considered in-
lier tasks (domains). From left to right, columns
correspond to News, TED and IT domains. The
first 17 rows correspond to the features used to
train the model and the last row in corresponds to
the bias term.
For all three domains, a general trend is that MTL RMTL is the method that reaches the lowest MAE
when compared to all the other models. Given the difference among the domains, it is very likely that
MTL Dirty and MTL SparseTrace suffer from the negative transfer problem (the assumption that all
tasks are similar does not hold). MTL RMTL is the only method among the methods presented here that
copes with negative transfer among tasks. The significance tests indicate that MTL RMTL improvements
are statistically significant with respect to all baselines depending on the range of training data used to
compute the test.
? For TED, the Friedman test rejects the null hypothesis with p = 4.62
?5
. Post-hoc analysis indicates
that there are differences statistically significant between MTL RMTL and all the three baselines
with p ? 0.002.
? For News, the Friedman test measures significant differences with p = 1.14
?9
and the post-hoc
analysis indicates that MTL RMTL is statistically significant with respect to SVR in-domain and
SVR Pooling with p = 0.002 for varying amounts of training data from 10 to 100%. As can be seen
in Figure 2, MTL RMTL starts with a very high MAE using 10% of the data (approximately 0.21
MAE) but improves dramatically with 20% of the data. Calculating the significance test with 20 to
100% of training data, MTL RMTL is significantly better than all baselines with p ? 2.89
?10
.
? For IT, in a similar situation to the News domain, RMTL is significantly better than all baselines
416
trained on 30% to 100% of the training data (Friedman test?s p = 2.86
?4
and post-hoc analysis?
p ? 3.73
?7
).
Another observed trend is that the MTL models benefit from increasing amounts of training data.
MTL RMTL has an improvement in performance of 5.13% for TED, 4% for News and 1.85% for IT
when trained on 100% of the training data in comparison with the model trained on 30% of training data.
18 36 54 72 90 108 126 144 162 181
Training data points
0.16
0.17
0.18
0.19
0.20
0.21
0.22
M
A
E
news as target domain
Mean
SVR RBF in-domain
SVR Pooling
SVR FEDA
MTL Dirty
MTL SparseTrace
MTL RMTL
Figure 2: Learning curves for the News domain.
The results for the IT domain are in line with the in-domain experiments in which we observed that
IT is a more challenging domain in comparison to TED and News. The MAE of IT is always higher
than for the other domains on in-domain and MTL experiments. Another evidence of this is the model
learned by the RMTL method when using all training data and run on one of the 50 training/test splits. A
graphic representation of the RMTL outlier task detection component (described in Section 3) is shown
in Figure 1.
From left to right, each column represents News, TED, and IT domains, respectively, while each row is
the instantiation of a feature in the corresponding task. Columns with non-black entries represent outlier
tasks. The highest number of entries with lighter shades is in the third column, IT. Several features in
this task are considered outliers with respect to the same features in the other tasks. Consequently, the
learning method takes the weights into consideration to a greater extent when learned with the outlier
model for the IT domain. Entries with the lightest shades in the IT domain correspond to the features
marked with? in Section 4.2. These outlier features are directly affected by the length of the sentences
on which they are computed (source or target) given that the number of tokens influences the final value
of the feature. This outcome goes in the same direction of our analysis of the three domains (Section 4.1)
that indicates a very different vocabulary size and average sentence length for IT when compared to the
other two domains.
To a lesser extent than IT, News and TED domains also present a few lighter-shaded entries in the
outlier component (1st and 2nd column). This suggests that MTL RMTL was capable of transfering
information among the domains in a more efficient way than the other MTL methods analyzed.
417
Overall the experiments presented show encouraging results in the direction of coping with QE data
coming from different domains/genres, translated by different MT systems and post-edited by different
translators. Results show that even in such difficult conditions, the methods investigated are capable of
outperforming competitive baselines based on non-linear models on different domains. As a rationale,
models that consider not only similarity between the domains but also deal with some sort of dissimilarity
should be considered. This is the case of the best performing method, MTL RMTL, which identifies
outlier tasks in order to avoid negative transfer among tasks.
6 Conclusion
In this work we presented an investigation of methods that overcome limitations presented by current
MT QE state-of-the-art systems when applied to real world conditions. In such scenarios (e.g. CAT
environment) the requirements are two-fold: (i) learning in the presence of different train/test feature
and label distributions and across different domains/genres, and (ii) the capability of learning with scarce
training data. In our experiments, we explored transfer learning methods, in particular multitask learning,
and we showed that such methods can cope with the needs of real-world scenarios.
We showed that multitask learning methods are capable to learn robust models for three different
domains that perform better than three strong baselines trained on the same amount of data. The methods
explored here benefit from increasing amounts of training data but also perform well when operating
with very limited amounts of data. We believe that the results obtained in this first exploration of model
adaptation for the problem can encourage the MT QE community to shift the focus from controlled
scenarios to more applicable, real-world contexts that require more robust methods.
Acknowledgements
This work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).
References
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. 2007. Multi-task feature learning. In Advances
in neural information processing systems, volume 19.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite: When less is more for translation
quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342.
James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Ma-
chine Learning Research, 13:281?305.
Joseph John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. In 20th COLING, pages
315?321.
John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence
learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
120?128, Morristown, NJ, USA. Association for Computational Linguistics.
Ondej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, pages 1?44.
Christian Buck. 2012. Black Box Features for the WMT 2012 Quality Estimation Shared Task. In Proceedings of
the 7th Workshop on Statistical Machine Translation, pages 91?95.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical
Machine Translation, pages 10?51, Montr{?e}al, Canada, June. Association for Computational Linguistics.
Rich Caruana. 1997. Multitask Learning. Machine learning, 28(1):41?75.
418
Jianhui Chen, Jiayu Zhou, and Jieping Ye. 2011. Integrating low-rank and group-sparse structures for robust multi-
task learning. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining - KDD ?11, page 42, New York, New York, USA. ACM Press.
Jianhui Chen, Ji Liu, and Jieping Ye. 2012. Learning incoherent sparse and low-rank patterns from multiple tasks.
ACM Transactions on Knowledge Discovery from Data, 5(4):22, February.
Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An applica-
tion to Machine Translation Quality Estimation. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 32?42.
Hal Daum?e III. 2007. Frustratingly Easy Domain Ddaptation. In Conference of the Association for Computational
Linguistics (ACL).
Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013a. FBK-UEdin participation to
the WMT13 Quality Estimation shared-task. In Proceedings of the Eighth Workshop on Statistical Machine
Translation, pages 352?358.
Jos?e G.C. de Souza, Miquel Espl`a-Gomis, Marco Turchi, and Matteo Negri. 2013b. Exploiting qualitative infor-
mation from automatic word alignment for cross-lingual nlp tasks. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 2: Short Papers), pages 771?776, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Janez Dem?sar. 2006. Statistical Comparisons of Classifiers over Multiple Data Sets. The Journal of Machine
Learning Research, 7:1?30, December.
Milton Friedman. 1937. The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of
Variance. Journal of the American Statistical Association, 32(200):675?701.
Milton Friedman. 1940. A Comparison of Alternative Tests of Significance for the Problem of m Rankings. The
Annals of Mathematical Statistics, 11(1):86?92.
Salvador Garcia and Francisco Herrera. 2008. An Extension on ?Statistical Comparisons of Classifiers over
Multiple Data Sets? for all Pairwise Comparisons. Journal of Machine Learning Research, 9:2677?2694.
Christian Hardmeier, Joakim Nivre, and Jorg Tiedemann. 2012. Tree Kernels for Machine Translation Quality
Estimation. In Proceedings of the 7th Workshop on Statistical Machine Translation, number 2011, pages 109?
113.
Sture Holm. 1979. A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian Journal of Statistics,
6(2):pp. 65?70.
Ali Jalali, PD Ravikumar, S Sanghavi, and C Ruan. 2010. A Dirty Model for Multi-task Learning. In Advances in
Neural Information Processing Systems (NIPS) 23.
Jing Jiang and Chengxiang Zhai. 2007. Instance Weighting for Domain Adaptation in NLP. In Proceedings of the
45th Annual Meeting of the Association for Computational Linguistics, number June, pages 264?271.
Jing Jiang. 2009. Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction. In ACL ?09 Proceed-
ings of the Joint Conference of the 47th Annual Meeting of the ACL, number August, pages 1012?1020.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zenz, Chris Dyer, Ondej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007 Demo and Poster
Sessions, number June, pages 177?180.
Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee : Evaluating MT Adequacy
without Reference Translations. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages
171?180.
Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359, October.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Mathieu Brucher, Mathieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn : Machine Learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
419
Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasul Samad Zadeh Kaljahi, and Fred Hol-
lowood. 2012. DCU-Symantec Submission for the WMT 2012 Quality Estimation Task. In Proceedings of the
Seventh Workshop on Statistical Machine Translation, pages 138?144, Montr{?e}al, Canada, June. Association
for Computational Linguistics.
Raphael Rubino, Jos?e G. C. de Souza, Jennifer Foster, and Lucia Specia. 2013. Topic Models for Translation
Quality Estimation for Gisting Purposes. In Machine Translation Summit (MT Summit) XIV, pages 295?302.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Trans-
lation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas.
Radu Soricut and A Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, number July, pages 612?621.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012. The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages
145?151.
Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri. 2014. FBK-UPV-
UEdin participation in the WMT14 Quality Estimation shared-task. In Proceedings of the Ninth Workshop on
Statistical Machine Translation, Baltimore, MD, USA, June.
Lucia Specia, Marco Turchi, Nello Cristianini, Nicola Cancedda, and Marc Dymetman. 2009. Estimating the
Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the
EAMT, number May, pages 28?35.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50, May.
Lucia Specia, Stafford Street, Regent Court, and Mariano Felice. 2012. Linguistic Features for Quality Estimation.
In Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96?103.
Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings
of the European Association for Machine Translation, number May, pages 73?80.
Marco Turchi, Josef Steinberger, and Lucia Specia. 2012. Relevance ranking for translated texts. In Proceedings
of the 16th Annual Conference of the European Association for Machine Translation, number May, pages 153?
160.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the Subjectivity of Human Judgements
in MT Quality Estimation. In Proceedings of the 8th Workshop on Statistical Machine Translation (WMT?13),
Sofia, Bulgaria, August.
Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics.
Guillaume Wisniewski, Anil Kumar Singh, Natalia Segal, and Franc?ois Yvon. 2013. Design and Analysis of
a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-
Edition. In Machine Translation Summit XIV, pages 117?124.
Jiayu Zhou, Jianhui Chen, and Jieping Ye. 2012. MALSAR: Multi-tAsk Learning via StructurAl Regularization.
420
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1813?1823, Dublin, Ireland, August 23-29 2014.
Quality Estimation for Automatic Speech Recognition
Matteo Negri
(1)
Marco Turchi
(1)
Jos
?
e G. C. de Souza
(1,2)
Daniele Falavigna
(1)
(1)
FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2)
University of Trento, Italy
{negri,turchi,desouza,falavi}@fbk.eu
Abstract
We address the problem of estimating the quality of Automatic Speech Recognition (ASR) out-
put at utterance level, without recourse to manual reference transcriptions and when information
about system?s confidence is not accessible. Given a source signal and its automatic transcription,
we approach this problem as a regression task where the word error rate of the transcribed utter-
ance has to be predicted. To this aim, we explore the contribution of different feature sets and
the potential of different algorithms in testing conditions of increasing complexity. Results show
that our automatic quality estimates closely approximate the word error rate scores calculated
over reference transcripts, outperforming a strong baseline in all the testing conditions.
1 Introduction
In recent years, the increasing usage of large vocabulary continuous speech recognition (LVCSR) systems
to transcribe audio recordings from different sources (e.g. Youtube videos, TV programs, DVD movies,
meetings, etc) has sparked the need of accurate, fast and cost-effective methods to estimate the quality
of ASR output. This need contrasts with the fact that, after decades of progress in ASR research, the
established evaluation protocol is based on computing word error rate scores (WER)
1
over large test
sets of hand-crafted reference transcriptions. Indeed, despite its reliability, reference-based performance
assessment has an evident drawback represented by the cost of acquiring manual transcripts. Besides
increasing the cost-effectiveness of ASR evaluation routines, bypassing this bottleneck has several other
motivations. From an application perspective, for instance, reference-free quality estimation methods
could be used to: i) decide at run-time whether a given input signal has been properly recognized (e.g.
if a user spoken utterance needs to be repeated in a dialogue application), ii) decide if an automatic
transcription is acceptable as is (e.g. if manual revision is needed in an automatic subtitling application),
or iii) select the best transcription among options from multiple ASR systems.
When information about the inner workings of the system used to produce the transcriptions is acces-
sible, current reference-free confidence estimation methods can supply ASR applications with reliable
indicators about output reliability. This condition, however, does not always hold in the aforementioned
scenarios. A clear motivating example is provided by the exponential growth of captioned TED Talks
and Youtube videos,
2
for which no information is available about how transcriptions have been pro-
duced. In this case, neither reference-based methods, nor standard confidence measures can be applied
to obtain useful quality estimates. Nevertheless, in this scenario, supplying reliable indicators of tran-
scription quality has a huge market potential (e.g. to reduce the costs of manual revision/translation)
which motivates our research.
Focusing on these compelling needs, this paper investigates the automatic prediction of ASR out-
put quality when: i) manual reference transcripts are not available and ii) information about the
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The word error rate is the minimum edit distance between an hypothesis and the reference transcription. Edit distance is
calculated as the number of edits (word insertions, deletions, substitutions) divided by the number of words in the reference.
2
Since 2009, Youtube videos in English can be automatically captioned. In 2012, for the 72 hours of video uploaded per
minute, such functionality was already available for 10 languages. Currently, more than 200 million Youtube videos have either
automatic or human-created captions (source: http://goo.gl/9swYSS).
1813
inner workings of the ASR system is not accessible. Casting the problem as a supervised regression
task, we experiment in a range of testing conditions on a well-known LVCSR setting (i.e. the automatic
transcription of TED talks). In this framework, we analyse the performance of various models (i.e. their
capability to predict utterance-level WER scores) as a function of the different learning algorithms used,
the proposed features, and the amount of training data available.
Our features are categorized according to the type of information they aim to capture. Since the na-
ture of the proposed features is a relevant aspect for the applicability of our approach, an important
distinction is made between ?glass-box? and ?black-box? features, which are respectively informed and
agnostic about systems? internal decoding strategies. The former can play an important role when all the
intermediate processing steps are accessible (e.g. in the selection of the best possible transcription hy-
pothesis). In contrast, black-box features have a wider applicability to situations where such information
is not available (e.g. to estimate the quality of online video subtitles).
Another important aspect relevant to our study is the relation between the accuracy of utterance-level
quality predictions and the degree of homogeneity of training and test data. Indeed, as in any supervised
learning framework, the similarity between training and test data has a direct impact on (classification
and regression) results. In order to fully understand the potential of our approach, we hence measure
performance variations under different levels of similarity between the data used to train the regressor
and the data used for evaluation. To this aim, our experiments account for a range of possible conditions.
These vary from the situation in which training and test are fully homogeneous (i.e. same dataset, with
training instances produced by the same ASR system) to the more challenging situation where training
and test are not homogeneous (i.e. different datasets, with training instances produced by different ASR
systems). Our results, obtained with two different state-of-the-art algorithms for regression, demonstrate
that in all such variable conditions our ASR quality estimation models lead to accurate predictions (i.e.
close the word error rate scores calculated over reference transcripts).
To the best of our knowledge, this paper represents the first extensive investigation on reference-
free and system-agnostic automatic estimation of ASR output quality. Along this direction, our main
contributions can be summarized as follows:
1. We propose a supervised, application-oriented approach to ASR quality estimation that bypasses
the need of manual reference transcriptions and is system-independent.
2. We evaluate our method with different learning algorithms and in different conditions, showing that
its estimates closely approximate the WER scores calculated over reference transcripts.
3. We perform feature analysis, isolating the contribution of each feature set in all the testing condi-
tions.
4. We analyse the learning curves of our best models, investigating the relation between performance
results and the amount of data needed for training.
Overall, these contributions provide useful insights about the feasibility of automatic ASR quality esti-
mation, opening interesting research avenues relevant for system development and for ASR applications.
2 Related Work
As a reference-free automatic evaluation method, our work introduces a valid application-oriented alter-
native to the standard evaluation protocols used within current ASR evaluation campaigns such as IWSLT
(Federico et al., 2011; Federico et al., 2012; Cettolo et al., 2013).
3
Besides that, our approach to ASR
quality estimation (QE) also differs from the well-established confidence estimation (CE) techniques
proposed in previous ASR literature (Sukkar and Lee, 1996; Evermann and Woodland, 2000; Wessel et
al., 2001; Sanchis et al., 2012; Seigel, 2013, inter alia). Such difference firstly relies in the fact that,
while in CE is the system itself that provides an indicator of the reliability of its output transcriptions,
QE aims to provide an external and more objective measure of goodness through WER predictions. A
3
See http://www.iwslt2013.org/ for details about the last edition of the IWSLT Workshop held in 2013.
1814
second (related) difference is that, in contrast with previous CE methods that heavily rely on information
about the internal behaviour of the ASR system, our technique does not necessarily depend on the access
to such information. This extends its applicability to scenarios (out of the scope of CE research) where
the quality of transcriptions produced by (possibly unknown) ASR systems has to be evaluated/compared
solely based on information about the input audio signals and the output transcriptions.
An interesting approach exploiting ASR word accuracy estimates to automatically score the profi-
ciency of non-native English speakers has been proposed by Yoon et al. (2010). To our knowledge this
work is the most similar to the one presented here, although it differs in the application domain and sev-
eral other aspects. First of all, similar to CE methods, it makes some use of glass-box features derived
from knowledge about the ASR internal workings (e.g. word confidence and acoustic/language model
probabilities). Secondly, the domain addressed is constrained to responses to prompted utterances, while
in this paper we address a large unconstrained domain, namely the automatic transcription of lectures
(TED talks) covering different topics. Finally, (Yoon et al., 2010) is based on a rather simple model
whose performance is not carefully analysed from the learning point of view (e.g. by comparing the
contribution different state-of-the-art algorithms) as we do here.
The problem of automating system evaluation without a gold standard has been addressed also in other
NLP areas. For instance, (Louis and Nenkova, 2013) recently addressed the assessment of machine-
generated summaries without model summaries. The strongest parallelism with our work, however,
can be found in the Machine Translation (MT) evaluation field, where the goal of bypassing the need
of manually-created reference translations has motivated a large body of research.
4
Quality estimation
for MT and ASR have a number of commonalities. First, they both deal with a ?source? (respectively
a sentence in a language L and an acoustic utterance) and an ?hypothesis? whose quality has to be
estimated without references (respectively a translation in a language L1 and an automatic transcription
of the audio signal). Second, they can be addressed at various granularities. Indeed, ASR output quality
estimation is similar to its MT counterpart where research focused on quality predictions at word level
(Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012)
and document level (Soricut and Echihabi, 2010). Third, both tasks are suitable for supervised machine
learning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia et
al., 2010; Specia, 2011). Finally, both tasks motivate efforts in designing features capable to capture
the difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) the
confidence of the decoding process (Felice, 2012; Rubino et al., 2013b).
3 Approach
We approach the automatic estimation of ASR output quality as a supervised regression problem. Given
a training set of (signal, transcription, WER) instances, the task is to predict the WER of each instance
in a test set of unseen (signal, transcription) pairs.
Features. As shown in Table 1, the features used in our experiments (68 in total) can be categorized in
four main groups. The first group (ASR features) includes several glass-box features proposed in previous
literature on ASR confidence estimation (Litman et al., 2000; Gabsdil and Lemon, 2004; Goldwater et al.,
2010; Higgins et al., 2011). These features are suitable only for the ideal situation in which information
about systems? internal decoding strategies is available (as in the experiments discussed in ?4.1). We use
them as a term of comparison to evaluate the usefulness of the other three groups (signal, hybrid and
textual), which belong to the black-box type. These features, which are totally uninformed about the
decoding process, have wider applicability to the system-independent ASR quality estimation tasks that
represent our target scenario (see Sections 4.2 and 4.3). More in detail:
? ASR features aim to capture the confidence of the speech recognizer and the reliability of the whole
decoding process. In our experiments, as we do not have access to decoders of other systems, they
are computed only for the ASR system developed in our labs (Falavigna et al., 2013). These features
4
For a complete overview of the current approaches to MT quality estimation we refer the reader to the WMT12 and WMT13
shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013).
1815
are extracted both from word graphs (WGs) and n-best lists (n=100). In Table 1 ?Total probabil-
ity? is the weighted sum of log Language Model (LM) and log Acoustic Model (AM) probabilities.
LM probability is computed with a 4-gram backoff LM, trained over about 5 billion words using
the IRSTLM toolkit (Federico et al., 2008) and the modified shift-beta smoothing method. AM
probability is computed using a set of tied-state triphone Hidden Markov Models having, as output
state density, a mixture of Gaussian probability densities with diagonal covariance matrices. ?Mean
probability? is obtained dividing the total probability by the number of hypothesized ASR output
items (words + silences). Confidence scores are computed averaging time posterior word proba-
bilities (Evermann and Woodland, 2000). ?Proportion of low confidence words? is the fraction of
words having confidence values ? 0.5. The remaining ASR features are directly extracted from
word graphs and n-best lists scores.
? Signal features aim to capture the difficulty to transcribe a given input looking at the signal as a
whole. They are computed from raw vectors extracted through frame analysis (we employ 20ms
analysis window and 10ms analysis step). For each analysed window, 12 Mel Frequency Cepstral
Coefficients (MFCCs) are evaluated plus log energy. Then, for each given segment, minimum,
maximum and mean values of raw energy, as well as the mean MFCCs values and total segment
duration, are computed to form the signal feature vector.
? Hybrid features provide a more fine-grained way to capture the difficulty of transcribing the signal.
This is done by considering information about word and silence/noise regions, as well as their
respective duration. These features are computed after having performed forced alignment between
the input audio signal and the corresponding automatic hypotheses. Forced alignment is carried
out with our ASR system (Falavigna et al., 2013), in order to detect audio segments related to
words, hesitations and silences in the hypothesis. Pitch features have been computed with the Praat
software tool (Boersma and Weenink, 2005).
? Textual features aim to capture the plausibility (i.e. the fluency) of an output transcription. To
this aim, we consider surface information (such as the number of words and the percentage of
numbers/content-words/nouns/verbs in the hypothesis) as well as information about LM perplexity
and probability of the hypothesis (both at the level of words and parts of speech)
5
.
Feature selection is performed throughout all our experiments to maximize results and, at the same
time, analyse the contribution of the proposed features. To this aim, we use Randomized Lasso, or
stability selection (Meinshausen and B?uhlmann, 2010), which re-samples the training data several times
and fits a Lasso regression model on each sample. Features that appear in a given number of samples are
considered more informative for the task at hand, and hence retained (those marked in bold in Table 1
are the most informative ones based on the experiments described in Sections 4.2 and 4.3).
Learning algorithms. To build our regression models we experimented with two non-parametric learn-
ing approaches: Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Extremely
Randomized Trees (XT) (Geurts et al., 2006). SVMs are non-parametric deterministic algorithms that
have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various
tasks. Extra-Trees are a tree-based ensemble method for supervised classification and regression that
were also successfully used for MT quality estimation (de Souza et al., 2013; de Souza et al., 2014a). In
XTs each tree can be parametrized differently. When a tree is built, the node splitting step is done at ran-
dom by picking the best split among a random subset of the input features. The results of the individual
trees are combined by averaging their predictions. Hyper-parameter optimization of the SVM (with Ra-
dial basis function kernel ? RBF) and XT models was performed using randomized search (Bergstra and
Bengio, 2012). We used both learning methods as implemented in the Scikit-learn package (Pedregosa
et al., 2011).
5
The PoS LM has been obtained by processing with the TreeTagger (Schmid, 1995) the same data used for the word LM.
6
Hesitations, such as ?uhm?, ?eh? and ?ah? are found through matches with a predefined list. Consecutive repeated words
in the same utterance are also considered as hesitations.
1816
ASR (16)
Total probability of ASR output (w ? logP
LM
+ logP
AM
), mean probability, total
acoustic probability, mean acoustic probability, mean confidence score, Std of confi-
dence scores, confidence scores per second, proportion of low-confidence words, WG
node density, WG transition density, Mean/Std/Min n-best probability, Mean/Std/Min
n-best acoustic probability.
Signal (16)
Total segment duration (sec), Mean/Min/Max raw energy (dB), mean MFCC[1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 11,12].
Hybrid (26)
SNR (dB), mean noise energy (dB), Mean/Min/Max word energy (dB), Min/Max
noise energy (dB), (max word - min noise) energy (dB), # silences, ratio of silences
and words, # words per second, # silences per second, total duration of words
(sec), total duration of silences (sec), mean duration of words (sec), mean duration
of silences (sec), ratio of (tot duration silences) and (tot duration words), Std of word
duration (sec), Std of silence duration (sec), (tot duration words) - (tot duration
silences), Mean/Std/Min./Max. pitch (Hz), # hesitations,
6
frequency of hesitations.
Textual (10)
Number of words, LM log probability of the hypothesis, LM log probability of
POS of the hypothesis, LM log perplexity of POS of the hypothesis, Perplexity of
the hypothesis, % of numbers in the hypothesis, % of tokens in the hypothesis which
do not contain only a-z, % of content words in the hypothesis, % of nouns in the
hypothesis, % of verbs in the hypothesis
Table 1: Full list of the 68 features used in our experiments, divided into four groups. The most predictive
black-box features (resulting from feature selection in the ?4.3 experiments) are marked in bold.
4 Experiments
To evaluate our approach we carried out three sets of experiments. In each set our feature groups are
analysed: i) with the two learning algorithms, ii) in combination/isolation, iii) with/without feature se-
lection. The three sets differ in terms of the difficulty of the quality estimation task from the learning
point of view. To experiment with situations of increasing complexity, we alternate conditions in which
all the features (glass-box and black-box) can be used, training and test sets are non-/homogeneous, the
quality estimator is trained on transcriptions generated by the same/different ASR systems.
Data. The data used in the experiments consists of the audio recordings delivered for the IWSLT 2013
evaluation campaign (Cettolo et al., 2013). One of the tasks of IWSLT 2013 is the automatic tran-
scription of English TED talks, a global set of conferences whose audio/video recordings are publicly
available. The main challenges for ASR in these talks include: the large variability of topics (hence
a large, unconstrained vocabulary), the presence of non-native speakers and a rather informal speaking
style. Each IWSLT participant submitted one primary ASR output run for each of the talks included in
the test set plus some optional contrastive ASR outputs. In addition, participants sent submissions for
the ASR tracks delivered for the 2012 evaluation campaign. Our experiments have been carried out on
the primary submissions, sent by 8 participants, related to the 2012 (consisting in 11 different talks) and
2013 (28 different talks) test sets. The 2012 test set has a total duration of around 1h45sec, it contains
1,118 reference sentences and 18,613 running words. On such dataset, participants? primary submissions
achieved a mean utterance WER ranging from 10.5% to 18.4% (in this work a WER score is computed
for each reference sentence, and mean utterance WER represents the average of sentence WERs). The
2013 test set has a total duration of around 3h55sec, it contains 2,238 reference sentences and 41,545 run-
ning words. On this dataset, primary participants? submissions achieve a mean utterance WER ranging
from 15.9% to 30.8%.
In our experiments, we always use 1,118 utterances for training the regressor and 1,120 for testing. To
this aim, the IWSLT 2013 data is randomly sampled three times in training and test sets of such dimen-
sions. While for the 2012 test set manual utterance segmentation has been provided by the organizers, for
the 2013 data the participants had to employ their own automatic segmentation systems before decoding
the audio tracks (thus resulting in a different number of ASR sentence hypotheses for each team). Hence,
1817
to ensure that each participant has the same number of ASR sentence hypotheses, an alignment with the
reference manual segmentation has been performed in our experiments.
Evaluation. Our evaluation is carried out in terms of Mean Absolute Error (MAE), a standard metric
for regression problems. The MAE is the average of the absolute errors e
i
= |f
i
? y
i
|, where f
i
is
the prediction of the model and y
i
is the actual WER for the i
th
test instance. WER is calculated with
the NIST SCLITE Scoring Package.
7
As it is a measure of error, lower MAE scores indicate that our
predictions are closer to the real WER calculated for each test instance against the reference transcripts.
For each experiment, we report the mean and the standard deviation of the MAE achieved by the best
performing QE models on the IWSLT 2013 test sets.
Baseline. Besides measuring performance in terms of global MAE, each model is compared against a
common baseline for regression tasks. This baseline, which is particularly relevant in settings featuring
different data distributions between training and test sets, is calculated by labelling each test instance
with the mean WER score calculated on the training set. Previous works, also in MT quality estimation,
demonstrated that its results can be particularly hard to beat (Rubino et al., 2013a).
4.1 Experiment 1
In the first set of experiments we consider the easiest situation from the learning perspective. In this
setting we predict the WER of transcriptions produced by our ASR system (denoted by X), whose inner
workings are known (thus enabling the use of glass-box features). To investigate the relation between
prediction accuracy and the degree of homogeneity of training and test data, we experiment both with
similar datasets (disjoint training and test sampled from IWSLT13) and different datasets (IWSLT12
for training and samples from IWSLT13 for test). Results are reported in Table 2, where the notation
?LetterYear - LetterYear? indicates the systems and the datasets used for training and test (respectively
our system X, and data from IWSLT12 and/or IWSLT13).
Train - Test ALL (glass-box + BB COMB) ASR (glass-box) BB COMB (Signal+Hybrid+Textual) Baseline
X13 - X13 11.56?0.29 SVR 12.11?0.29 XT 15.17?0.06 XT 19.84?0.06
X12 - X13 12.61?0.13 XT 13.78?0.16 XT 16.78?0.18 XT 19.06?0.12
Train - Test Signal Hybrid Textual Baseline
X13 - X13 16.42?0.1 XT 17.61?0.12 XT 17.42?0.15 SVR 19.84?0.06
X12 - X13 18.85?0.09
?
XT 18.39?0.22 XT 17.58?0.15 XT 19.06?0.12
Table 2: MAE results using the same system on different datasets, with and without glass-box features.
As can be seen from the table, the two models using ALL the features achieve the largest improvements
over the strong baseline used for comparison (up to 8.2 MAE points in the X13 - X13 setting). This is
not surprising if we consider the high predictive power of ASR (glass-box) features that, when used in
isolation, lead to a considerably lower MAE with respect to the other three groups. However, it?s worth
observing that also the combination of only the black-box features (BB COMB) allows the QE predictors
to significantly outperform the baseline (up to 4.67 MAE points in X13 - X13). Such improvements come
from the joint contribution of each of the three groups, which achieve good results also in isolation.
Indeed, except in one case where the gain over the baseline is not significant
8
(X12 - X13 with the Signal
features), their MAE reduction ranges between 0.67 (X12 - X13 Hybrid) and 3.42 MAE points (X13 -
X13 Signal). The good prediction capability of the black-box features is also shown by the fact that, when
combined with the glass-box features, they lead to improvements between 0.55 and 1.17 MAE points
over the ASR features alone. Considering the privileged condition of the (system-informed) glass-box
features, this is a remarkable result that suggests some complementarity between the two groups.
In general, our supervised approach is sensitive to the similarity between training and test. This is
evidenced by higher MAE results when non-homogeneous datasets (i.e. X12 - X13) are processed. In
7
http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm
8
Statistical significance is measured by considering the overlap of confidence intervals defined by the standard deviation
range around the mean. In our tables, the results marked with the ??? symbol are not significantly better than the baseline.
1818
terms of algorithms, XT generally performs better than SVR, in particular when the QE model is trained
and tested on non-homogeneous data. This can be explained by their higher generalization capability
due to variance reductions as explained in (Hastie et al., 2009, Chapter 15).
4.2 Experiment 2
In this set of experiments we consider a situation of intermediate difficulty from the learning perspective.
Our objective is to evaluate, on homogeneous datasets (sampled from IWSLT13), the output of ASR
systems whose inner workings are not known (hence only black-box features can be used). To make
our analysis more complete, we also evaluate the performance of models trained on a given ASR system
to predict the WER of hypotheses produced by a different one. This situation is closer to application
scenarios in which the evaluated ASR system is unknown and different from the one used to train the
quality estimator. Two systems with very different performance are considered for this purpose: the best
and the worst according to the official IWSLT 2013 ranking (respectively denoted by A and Z).
Train - Test BB COMB Signal Hybrid Textual Baseline
A13 - A13 11.18?0.22 SVR 11.91?0.23 SVR 12.76?0.18 SVR 12.57?0.13 SVR 14.35?0.1
Z13 - A13 16.01?0.23 SVR 18.04?0.22 SVR 17.24?0.22 SVR 18.01?0.2 XT 21.58?0.15
Z13 - Z13 15.52?0.6 XT 16.94?0.41 XT 17.04?0.56 SVR 17.84?0.4 XT 19.65?0.43
A13 - Z13 17.36?0.43 XT 18.7?0.53 XT 18.21?0.45 XT 19.38?0.45 XT 21.03?0.51
Table 3: MAE results using different systems on the same dataset, without glass-box features.
The results reported in Table 3 confirm that: i) the combination of black-box features (BB COMB)
always leads to the best QE models, which significantly outperform the baseline, ii) the same holds
also when each single group is used in isolation, iii) with less homogeneous training and test data, XT
performs generally better than SVR.
In addition, it?s worth noting that when a QE model is trained and tested on data transcribed by
the same ASR system the results are significantly better (the MAE is always about 1.0 - 6.0 points
lower). Indeed, as also shown by the same behaviour of our baseline, this condition is simpler and more
suitable for supervised learning methods. This depends on the fact that each ASR system has its own
coherent behaviour, which results in transcriptions with similar characteristics that supervised models
are able to learn (e.g. recurring errors, similar WER distributions). In contrast, when training and test
data are produced by different ASR systems, supervised learning becomes more difficult and the output
predictions less reliable. Each feature group is affected by this situation, but it is interesting to note that
the Hybrid features are more robust than the other two groups to less homogeneous datasets. This can be
explained by the fact that they are extracted after applying forced alignment by means of a third system,
which is likely to normalise and reduce the difference between training and test data. Overall, also in
this more complex scenario where the glass-box features cannot be used, our results demonstrate a good
prediction capability of the QE models, which are still able to beat a strong baseline.
4.3 Experiment 3
In the third set of experiments we consider the hardest case from the learning point of view. In this setting
the evaluated ASR systems are unknown and training/test data are non homogeneous (i.e. training from
IWSLT12, test from samples of IWSLT13). Results are reported in Table 4.
Train - Test BB COMB Signal Hybrid Textual Baseline
A12 - A13 12.81?0.08 XT 13.57?0.13
?
XT 12.85?0.1 XT 13.25?0.23
?
XT 13.65?0.17
Z12 - A13 14.78?0.1 SVR 15.66?0.09
?
XT 13.56?0.09 SVR 13.63?0.24 SVR 15.51?0.35
Z12 - Z13 17.16?0.4 XT 19.34?0.32
?
XT 17.68?0.3 XT 19.59?0.11
?
XT 19.98?0.29
A12 - Z13 19.83?0.23 XT 21.85?0.2 XT 20.68?0.13 XT 22.62?0.08 XT 23.04?0.18
Table 4: MAE results using different systems on different dataset, without glass-box features.
Also in the most challenging scenario our results substantially confirm the previous findings. Indeed,
except in one case (Z12 - A13), the following observations still hold: i) when used in combination, the
1819
0 10 20 30 40 50 60 70 80 90 10012
14
16
18
20
22
24
26
% of Training Data
MA
E
 
 
A12 ? A13
A12 ? Z13
Z12 ? Z13
Z12 ? A13
Figure 1: Learning curves for the best systems of ?Experiment 3? (using BB COMB features).
black-box features (BB COMB) lead to the best QE models, which significantly outperform the baseline,
ii) this holds also when each single group is used in isolation (although not significantly in 5 out of 12
settings), iii) with less homogeneous training and test data, XT performs generally better than SVR.
Unsurprisingly, as also observed in the previous set of experiments, the low homogeneity of training
and test data has an impact on the accuracy of the predictions. The effect of training and testing on
less homogeneous data produced by different systems is now clearly visible. Except for the more robust
Hybrid features, which in the Z12 - A13 setting produce the best model, the results obtained with the two
other groups decreased to the point that their improvement over the baseline is often not significant. Nev-
ertheless, even under the challenging conditions posed by this realistic and application-oriented scenario,
reference-free and system-agnostic ASR evaluation remains a feasible task.
5 Feature Analysis and Learning Curves
In order to gain additional insights about the effectiveness of our method, we performed a further analysis
of the ?Experiment 3? results. In such challenging scenario, the most interesting from the application
perspective, we first identified the most predictive features among those in the BB COMB set. To this
aim, we collected the features that are always chosen by the feature selection algorithm proposed in ?3.
The resulting list contains features from all the three black-box groups (marked in bold in Table 1). This
confirms their complementarity in predicting the quality of a transcribed utterance.
In the same setting, we also investigated the relation between the amount of data used to train our
models and the accuracy of their predictions. To this aim, we measured performance variations when
the same models (i.e. those obtained with the BB COMB set) are trained on different amounts of data.
For each training set, nine subsets were created (with 10%, 20%,..., 90% of the data) by sub-sampling
sentences from a uniform distribution. The process was iterated 5 times. Each subset was used to build
the relative QE regressor, which was then evaluated on our test sets. Figure 1 shows the resulting learning
curves (each point is the average result of the 5 runs on each test set; the error bars show ?1std). As
can be seen from all the curves, after an initial fluctuation of the MAE, performance results with 40% of
the training data are comparable with those obtained using the whole training set. Moreover, it?s worth
remarking that in three out of four cases the models trained with such amount of data already outperform
the baseline (for Z12 - A13 the MAE is only 0.01 point higher). This suggests that reference-free, system-
independent models for ASR quality estimation are able to provide informative predictions even with a
limited amount (?400 manual transcripts) of training instances.
1820
6 Conclusion
We investigated the problem of automatically predicting the word error rate of an automatically-
transcribed utterance in a large vocabulary continuous speech recognition setting. In such scenario,
we proposed a supervised regression approach that bypasses the need of manual reference transcriptions
and does not necessarily depend on information about system?s confidence (first contribution of the pa-
per). Then, by evaluating models obtained with different state-of-the-art learning algorithms, we showed
that our automatic predictions outperform a strong baseline and closely approximate the WER scores
calculated over reference transcripts (second contribution). Different feature groups have been proposed
and their contribution has been analysed in a range of testing conditions of increasing difficulty (third
contribution). This made possible to isolate informative features that significantly contribute to the per-
formance of our quality estimation models, and to get useful insights about the potential of our approach
when different sources of information (glass-box, black box features) are available. Finally, analysing
the relation between prediction performance and the size of the training set, we showed that the results
obtained with 40% of the data are already comparable to our best MAE (fourth contribution).
Our analysis revealed a dependency between the performance of the quality estimation models and
the degree of homogeneity between training and test data. This aspect is particularly relevant from the
application perspective since in real working conditions the availability of large amounts of representa-
tive training instances is far from being guaranteed. In quality estimation for machine translation (a task
featuring strong similarities with ours), these issues have recently motivated studies on domain adapta-
tion and online learning techniques (de Souza et al., 2014b; Turchi et al., 2014). This suggests, as a first
direction for future work, the investigation of approaches capable to better exploit the available training
data and mitigate the impact of large differences between training and test instances.
Acknowledgements
This work has been partially funded by the European project EU-BRIDGE (FP7-287658) and by the
Autonomous Province of Trento, Italy, under the project Wikivoice (L.P. 6/1999).
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: a Method for Measuring Machine Translation
Confidence. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 211?219. The
Association for Computer Linguistics.
James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Ma-
chine Learning Research, 13:281?305.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence Estimation for Machine Translation. Summer workshop final report,
JHU/CLSP.
Paul Boersma and David Weenink. 2005. Praat: Doing Phonetics by Computer (Version 4.3.01). Retrieved from
http://www.praat.org/.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical
Machine Translation (WMT?12), pages 10?51, Montr?eal, Canada.
Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th
IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation,
Heidelberg, Germany.
1821
Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013. FBK-UEdin participation to the
WMT13 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Trans-
lation, pages 352?358, Sofia, Bulgaria, August. Association for Computational Linguistics.
Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri. 2014a. FBK-UPV-
UEdin participation in the WMT14 Quality Estimation shared-task. In Proceedings of the Ninth Workshop on
Statistical Machine Translation, Baltimore, MD, USA, June.
Jos?e G. C. de Souza, Marco Turchi, and Matteo Negri. 2014b. Predicting Machine Translation Quality Esti-
mation Across Domains. In Proceedings of the 25th International Conference on Computational Linguistics,
COLING?14, Dublin, Ireland.
Gunnar Evermann and Philip C. Woodland. 2000. Large Vocabulary Decoding and Confidence Estimation Using
Word Posterior Probabilities. In Proc. of ICASSP, pages 2366?2369, Istanbul, Turkey, June.
Daniele Falavigna, Roberto Gretter, Fabio Brugnara, Diego Giuliani, and Romain Serizel. 2013. FBK@IWSLT
2013 - ASR Tracks. In Proceedings of the IWSLT 2013 workshop, Heidelberg, Germany.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an Open Source Toolkit for Handling
Large Scale Language Models. pages 1618?1621, Brisbane, Australia, September.
Marcello Federico, Luisa Bentivogli, Michael Paul, and Sebastian St?uker. 2011. Overview of the IWSLT 2011
Evaluation Campaign. In International Workshop on Spoken Language Translation, pages 11?27.
Marcello Federico, Mauro Cettolo, Luisa Bentivogli, Michael Paul, and Sebastian St?uker. 2012. Overview of the
IWSLT 2012 Evaluation Campaign. In Proc. of the International Workshop on Spoken Language Translation,
Hong Kong, HK, December.
Mariano Felice. 2012. Linguistic Indicators for Quality Estimation of Machine Translations. Master?s thesis,
University of Wolverhampton, UK.
Malte Gabsdil and Oliver Lemon. 2004. Combining acoustic and pragmatic features to predict recognition perfor-
mance in spoken dialogue systems. pages 344?351.
Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Mach. Learn., 63(1):3?42,
April.
Sharon Goldwater, Dan Jurafsky, and Christopher Manning. 2010. Which words are hard to recognize? Prosodic,
lexical, and disfluency factors that increase speech recognition error rates. 52(3):181?200.
Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. 2009. The elements
of statistical learning, volume 2. Springer.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and David Williamson. 2011. A three-stage approach for auto-
mated scoring of spontaneous responses. (25):282?306.
Diane J. Litman, Julia B. Hirschberg, and Marc Swerts. 2000. Predicting Automatic Speech Recognition Perfor-
mance Using Prosodic Cues. In Proceedings of NAACL, pages 218?225.
Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard.
Computational Linguistics, 39(2):267?300.
Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee: Evaluating MT Adequacy
without Reference Translations. In Proceedings of the Machine Translation Workshop (WMT2012).
Nicolai Meinshausen and Peter B?uhlmann. 2010. Stability selection. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 72(4):417?473.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn : Machine Learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Christopher B. Quirk. 2004. Training a Sentence-Level Machine Translation Confidence Measure. In Proceedings
of LREC 2004.
Raphael Rubino, Jos?e GC de Souza, Jennifer Foster, and Lucia Specia. 2013a. Topic Models for Translation
Quality Estimation for Gisting Purposes. In Proceedings of the Machine Translation Summit XIV.
1822
Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hol-
lowood. 2013b. DCU-Symantec at the WMT 2013 Quality Estimation Shared Task. In Proceedings of the
Eighth Workshop on Statistical Machine Translation, pages 392?397.
Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2012. A Word-Based Naive Bayes Classifier for Confidence
Estimation in Speech Recognition. 20(12):565?574.
Helmut Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin, Ireland.
Matthew Stephen Seigel. 2013. Condence Estimation for Automatic Speech Recognition Hypotheses. University
of Cambridge. PhD Thesis.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel methods for pattern analysis. Cambridge university press.
Radu Soricut and Abdessamad Echihabi. 2010. TrustRank: Inducing Trust in Automatic Translations via Ranking.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages
612?621, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the
Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the
European Association for Machine Translation (EAMT?09), pages 28?35, Barcelona, Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Machine Translation Evaluation versus Quality Estimation.
Machine translation, 24(1):39?50.
Lucia Specia. 2011. Exploiting Objective Annotations for Measuring Translation Post-editing Effort. Proceedings
of the 15th Conference of the European Association for Machine Translation, pages 73?80.
Rafic Antoon Sukkar and Chin-Hui Lee. 1996. Vocabulary Independent Discriminative Utterance Verification for
Nonkeyword Rejection in Subword Based Speech Recognition. 6(6):420?429.
Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics, ACL?14, Baltimore, MD, USA. Association for Computational Linguistics.
Nicola Ueffing and Hermann Ney. 2007. Word-Level Confidence Estimation for Machine Translation. Comput.
Linguist., 33(1):9?40, March.
Frank Wessel, Ralf Schl?uter, Klaus Macherey, and Hermann Ney. 2001. Confidence Measures for Large Vocabu-
lary Continuous Speech Recognition. 9(3):288?298.
Su-Youn Yoon, Lei Chen, and Klaus Zechner. 2010. Predicting word accuracy for the automatic speech recogni-
tion of non-native speech. In Proc. of INTERSPEECH, pages 773?776, Makuhari,Chiba, Japan.
1823
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 129?132, Dublin, Ireland, August 23-29 2014.
THE MATECAT TOOL
M. Federico and N. Bertoldi and M. Cettolo and M. Negri and M. Turchi
Fondazione Bruno Kessler, Trento (Italy)
M. Trombetti and A. Cattelan and A. Farina and
D. Lupinetti and A. Martines and A. Massidda
Translated Srl, Roma (Italy)
H. Schwenk and L. Barrault and F. Blain
Universit?e du Maine, Le Mans (France)
P. Koehn and C. Buck and U. Germann
The University of Edinburgh (United Kingdom)
www.matecat.com
Abstract
We present a new web-based CAT tool providing translators with a professional work environ-
ment, integrating translation memories, terminology bases, concordancers, and machine transla-
tion. The tool is completely developed as open source software and has been already successfully
deployed for business, research and education. The MateCat Tool represents today probably the
best available open source platform for investigating, integrating, and evaluating under realistic
conditions the impact of new machine translation technology on human post-editing.
1 Introduction
The objective of MateCat
1
is to improve the integration of machine translation (MT) and human transla-
tion within the so-called computer aided translation (CAT) framework. CAT tools represent nowadays the
dominant technology in the translation industry. They provide translators with text editors that can man-
age several document formats and suitably arrange their content into text segments ready to be translated.
Most importantly, CAT tools provide access to translation memories (TMs), terminology databases, con-
cordance tools and, more recently, to machine translation (MT) engines. A TM is basically a repository
of translated segments. During translation, the CAT tool queries the TM to search for exact or fuzzy
matches of the current source segment. These matches are proposed to the user as translation sugges-
tions. Once a segment is translated, its source and target texts are added to the TM for future queries. The
integration of suggestions from an MT engine as a complement to TM matches is motivated by recent
studies (Federico et al., 2012; Green et al., 2013; L?aubli et al., 2013), which have shown that post-editing
MT suggestions can substantially improve the productivity of professional translators. MateCat lever-
ages the growing interest and expectations in statistical MT by advancing the state-of-the-art along three
directions:
? Self-tuning MT, i.e. methods to train statistical MT for specific domains or translation projects;
? User adaptive MT, i.e. methods to quickly adapt statistical MT from user corrections and feedback;
? Informative MT, i.e. supply more information to enhance users? productivity and work experience.
Research along these three directions has converged into a new generation CAT software, which is
both an enterprise level translation workbench (currently used by several hundreds of professional trans-
lators) as well as an advanced research platform for integrating new MT functions, running post-editing
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/.
1
MateCat, acronym of Machine Translation Enhanced Computer Assisted Translation, is a 3-year research project (11/2011-
10/2014) funded by the European Commission under FP7 (grant agreement no 287688). The project consortium is led by FBK
(Trento, Italy) and includes the University of Edinburgh (United Kingdom), Universit?e du Maine (Le Mans, France), and
Translated Srl (Rome, Italy).
129
Figure 1: The MateCat Tool editing page.
experiments and measuring user productivity. The MateCat Tool, which is distributed under the LGPL
open source license, combines features of the most advanced systems (either commercial, like the pop-
ular SDL Trados Workbench,
2
or free like OmegaT
3
) with new functionalities. These include: i) an
advanced API for the Moses Toolkit,
4
customizable to languages and domains, ii) ease of use through a
clean and intuitive web interface that enables the collaboration of multiple users on the same project, iii)
concordancers, terminology databases and support for customizable quality estimation components and
iv) advanced logging functionalities.
2 The MateCat Tool in a Nutshell
Overview. The MateCat Tool runs as a web-server accessible through Chrome, Firefox and Safari. The
CAT web-server connects with other services via open APIs: the TM server MyMemory
5
, the commer-
cial Google Translate (GT) MT server, and a list of Moses-based servers specified in a configuration file.
While MyMemory?s and GT?s servers are always running and available, customized Moses servers have
to be first installed and set-up. Communication with the Moses servers extends the GT API in order to
support self-tuning, user-adaptive and informative MT functions. The natively supported document for-
mat of MateCat Tool is XLIFF,
6
although its configuration file makes it possible to specify external file
converters. The tool supports Unicode (UTF-8) encoding, including non latin alphabets and right-to-left
languages, and handles texts embedding mark-up tags.
How it works. The tool is intended both for individual translators or managers of translation projects
involving one or more translators. A translation project starts by uploading one or more documents and
specifying the desired translation direction. Then the user can optionally select a MT engine from an
available list and/or a new or existing private TM in MyMemory, by specifying its private key. Notice
that the public MyMemory TM and the GT MT services are assumed by default. The following step is
the volume analysis of the document, which reports statistics about the words to be actually translated
based on the coverage provided by the TM. At this stage, long documents can be also split into smaller
portions to be for instance assigned to different translators or translated at different times. The following
step starts the actual translation process by opening the editing window. All source segments of the
2
http://www.translationzone.com/
3
http://www.omegat.org/
4
http://www.statmt.org/moses/
5
http://mymemory.translated.net
6
http://docs.oasis-open.org/xliff/v1.2/os/xliff-core.html
130
document and their corresponding target segments are arranged side-by-side on the screen. By selecting
one segment, an editing pane opens (Figure 1) including an editable field that is initialized with the best
available suggestion or with the last post-edit. Translation hints are shown right below together with
their origin (MT or TM). Their ranking is based on the TM match score or the MT confidence score. MT
hints with no confidence score are assigned a default score. Tag consistency is automatically checked
during translation and warnings are possibly shown in the editing window. An interesting feature of the
MateCat Tool is that each translation project is uniquely identified by its URL page which also includes
the currently edited segment. This permits for instance more users to simultaneously access and work on
the same project. Moreover, to support simultaneous team work on the same project, translators can mark
the status (draft, translated, approved, rejected) of each segment with a corresponding color (see Figure
1, right blue bar). The user interface is enriched with search and replace functions, a progress report at
the bottom of the page, and several shortcut commands for the skilled users. Finally, the tool embeds a
concordance tool to search for terms in the TM, and a glossary where each user can upload, query and
update her terminology base. Users with a Google account can access a project management page which
permits then to manage all their projects, including storage, deletion, and access to the editing page.
MT support. The tool supports Moses-based servers able to provide an enhanced CAT-MT commu-
nication. In particular, the GT API is augmented with feedback information provided to the MT engine
every time a segment is post-edited as well as enriched MT output, including confidence scores, word
lattices, etc. The developed MT server supports multi-threading to serve multiple translators, properly
handles text segments including tags, and instantly adapts from the post-edits performed by each user
(Bertoldi et al., 2013).
Edit Log. During post-editing the tool collects timing information for each segment, which is updated
every time the segment is opened and closed. Moreover, for each segment, information is collected about
the generated suggestions and the one that has actually been post-edited. This information is accessible at
any time through a link in the Editing Page, named Editing Log. The Editing Log page (Figure 2) shows
a summary of the overall editing performed so far on the project, such as the average translation speed
and post-editing effort and the percentage of top suggestions coming from MT or the TM. Moreover,
for each segment, sorted from the slowest to the fastest in terms of translation speed, detailed statistics
about the performed edit operations are reported. This information, with even more details, can be also
downloaded as a CSV file to perform a more detailed post-editing analysis. While the information shown
in the Edit Log page is very useful to monitor progress of a translation project in real time, the CSV file
is a fundamental source of information for detailed productivity analyses once the project is ended.
3 Applications.
The MateCat Tool has been exploited by the MateCat project to investigate new MT functions (Bertoldi
et al., 2013; Cettolo et al., 2013; Turchi et al., 2013; Turchi et al., 2014) and to evaluate them in a real
professional setting, in which translators have at disposal all the sources of information they are used
to work with. Moreover, taking advantage of its flexibility and ease of use, the tool has been recently
exploited for data collection and education purposes (a course on CAT technology for students in trans-
lation studies). An initial version of the tool has also been leveraged by the Casmacat project
7
to create
a workbench (Alabau et al., 2013), particularly suitable for investigating advanced interaction modalities
such as interactive MT, eye tracking, and handwritten input. Currently the tool is employed by Trans-
lated for their internal translation projects and is being tested by several international companies, both
language service providers and IT companies. This has made possible to collect continuous feedback
from hundreds of translators, which besides helping us to improve the robustness of the tool is also
influencing the way new MT functions will be integrated to supply the best help to the final user.
7
http://www.casmacat.eu
131
Figure 2: The MateCat Tool edit log page.
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael Carl, Francisco Casacuberta, Mercedes Garca-Mart??nez,
Jes?us Gonz?alez, Philipp Koehn, Luis Leiva, Bartolom?e Mesa-Lao, Daniel Oriz, Herv?e Saint-Amand, Germ?an
Sanchis, and Chara Tsiukala. 2013. Advanced computer aided translation with a web-based workbench. In
Proceedings of Workshop on Post-editing Technology and Practice, pages 55?62.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico. 2013. Cache-based Online Adaptation for Machine
Translation Enhanced Computer Assisted Translation. In Proceedings of the MT Summit XIV, pages 35?42,
Nice, France, September.
Mauro Cettolo, Christophe Servan, Nicola Bertoldi, Marcello Federico, Lo??c Barrault, and Holger Schwenk. 2013.
Issues in Incremental Adaptation of Statistical MT from Human Post-edits. In Proceedings of the MT Summit
XIV Workshop on Post-editing Technology and Practice (WPTP-2), pages 111?118, Nice, France, September.
Marcello Federico, Alessandro Cattelan, and Marco Trombetti. 2012. Measuring user productivity in machine
translation enhanced computer assisted translation. In Proceedings of the Tenth Conference of the Association
for Machine Translation in the Americas (AMTA).
Spence Green, Jeffrey Heer, and Christopher D Manning. 2013. The efficacy of human post-editing for language
translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439?
448. ACM.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. Assessing Post-
Editing Efficiency in a Realistic Translation Environment. In Michel Simard Sharon O?Brien and Lucia Specia
(eds.), editors, Proceedings of MT Summit XIVWorkshop on Post-editing Technology and Practice, pages 83?91,
Nice, France.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements
in MT quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
240?251, Sofia, Bulgaria, August. Association for Computational Linguistics.
Marco Turchi, Antonios Anastasopoulos, Jos?e G.C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (ACL ?14). Association for Computational Linguistics.
132
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1259?1269,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet
Marco Guerini
Trento RISE
Via Sommarive 18
38123 Povo in Trento, Italy
m.guerini@trentorise.eu
Lorenzo Gatti
Trento RISE
Via Sommarive 18
38123 Povo in Trento, Italy
l.gatti@trentorise.eu
Marco Turchi
Fondazione Bruno Kessler
Via Sommarive 18
38123 Povo in Trento, Italy
turchi@fbk.eu
Abstract
Assigning a positive or negative score to a
word out of context (i.e. a word?s prior polar-
ity) is a challenging task for sentiment analy-
sis. In the literature, various approaches based
on SentiWordNet have been proposed. In this
paper, we compare the most often used tech-
niques together with newly proposed ones and
incorporate all of them in a learning frame-
work to see whether blending them can fur-
ther improve the estimation of prior polarity
scores. Using two different versions of Sen-
tiWordNet and testing regression and classifi-
cation models across tasks and datasets, our
learning approach consistently outperforms
the single metrics, providing a new state-of-
the-art approach in computing words? prior
polarity for sentiment analysis. We conclude
our investigation showing interesting biases
in calculated prior polarity scores when word
Part of Speech and annotator gender are con-
sidered.
1 Introduction
Many approaches to sentiment analysis make use of
lexical resources ? i.e. lists of positive and neg-
ative words ? often deployed as baselines or as
features for other methods (usually machine learn-
ing based) for sentiment analysis research (Liu and
Zhang, 2012). In these lexica, words are associated
with their prior polarity, i.e. if that word out of con-
text evokes something positive or something nega-
tive. For example, wonderful has a positive connota-
tion ? prior polarity ? while horrible has a negative
one. These approaches have the advantage of not
needing deep semantic analysis or word sense dis-
ambiguation to assign an affective score to a word
and are domain independent (they are thus less pre-
cise but more portable).
SentiWordNet (henceforth SWN) is one of these
resources and has been widely adopted since it pro-
vides a broad-coverage lexicon ? built in a semi-
automatic manner ? for English (Esuli and Sebas-
tiani, 2006). Given that SWN provides polarities
scores for each word sense (also called ?posterior
polarities?), it is necessary to derive prior polarities
from the posteriors. For example, the word cold has
a posterior polarity for the meaning ?having a low
temperature? ? like in ?cold beer? ? that is different
from the one in ?cold person? which refers to ?being
emotionless?. This information must be considered
when reconstructing the prior polarity of cold.
Several formulae to compute prior polarities start-
ing from posterior polarities scores have been used
in the literature. However, their performance varies
significantly depending on the adopted variant. We
show that researchers have not paid sufficient atten-
tion to this posterior-to-prior polarity issue. Indeed,
we show that some variants outperform others on
different datasets and can represent a fairer state-of-
the-art approach using SWN. On top of this, we at-
tempt to outperform the state-of-the-art formula us-
ing a learning framework that combines the various
formulae together.
In detail, we will address five main research
questions: (i) is there any relevant difference in
the posterior-to-prior polarity formulae performance
(both in regression and classification tasks), (ii) is
there any relevant variation in prior polarity values
1259
if we use different releases of SWN (i.e. SWN1 or
SWN3), (iii) can a learning framework boost per-
formance of such formulae, (iv) considering word
Part of Speech (PoS), is there any relevant difference
in formulae performance, (v) considering the gender
dimension of the annotators (male/female) and the
sentiment dimension (positive/negative), is there any
relevant difference in SWN performance.
In Section 2 we briefly describe our approach and
how it differentiates from similar sentiment analysis
tasks. Then, in Sections 3 and 4, we present Sen-
tiWordNet and overview various posterior-to-prior
polarity formulae based on this resource that ap-
peared in the literature (included some new ones
we identified as potentially relevant). In Section 5
we describe the learning approach adopted on prior-
polarity formulae. In Section 6 we introduce the
ANEW and General Inquirer resources that will be
used as gold standards. Finally, in the two last sec-
tions, we present a series of experiments, both in
regression and classification tasks, that give an an-
swer to the aforementioned research questions. The
results support the hypothesis that using a learning
framework we can improve on state-of-the-art per-
formance and that there are some interesting phe-
nomena connected to PoS and annotator gender.
2 Proposed Approach
In the broad field of Sentiment Analysis we will fo-
cus on the specific problem of posterior-to-prior po-
larity assessment, using both regression and classifi-
cation experiments. A general overview on the field
and possible approaches can be found in (Pang and
Lee, 2008) or (Liu and Zhang, 2012).
For the regression task, we tackled the problem
of assigning affective scores (along a continuum be-
tween -1 and 1) to words using the posterior-to-prior
polarity formulae. For the classification task (assess-
ing whether a word is either positive or negative) we
used the same formulae, but considering just the sign
of the result. In these experiments we will also use a
learning framework which combines the various for-
mulae together. The underlying hypothesis is that by
blending these formulae, and looking at the same in-
formation from different perspectives (i.e. the pos-
terior polarities provided by SWN combined in var-
ious ways), we can give a better prediction.
The regression task is harder than binary classifi-
cation, since we want to assess not only that pretty,
beautiful and gorgeous are positive words, but also
to define a partial or total order so that gorgeous
is more positive than beautiful which, in turn, is
more positive than pretty. This is fundamental for
tasks such as affective modification of existing texts,
where words? polarity together with their score are
necessary for creating multiple graded variations of
the original text (Guerini et al, 2008). Some of
the work that addresses the problem of sentiment
strength are presented in (Wilson et al, 2004; Pal-
toglou et al, 2010), however, their approach is mod-
eled as a multi-class classification problem (neutral,
low, medium or high sentiment) at the sentence level,
rather than a regression problem at the word level.
Other works such as (Neviarouskaya et al, 2011)
use a fine grained classification approach too, but
they consider emotion categories (anger, joy, fear,
etc.), rather than sentiment strength categories. On
the other hand, even if approaches that go beyond
pure prior polarities ? e.g. using word bigram fea-
tures (Wang and Manning, 2012) ? are better for
sentiment analysis tasks, there are tasks that are in-
trinsically based on the notion of words? prior polar-
ity. Consider copywriting, where evocative names
are a key element to a successful product (O?zbal and
Strapparava, 2012; O?zbal et al, 2012). In such cases
no context is given and the brand name alone, with
its perceived prior polarity, is responsible for stating
the area of competition and evoking semantic asso-
ciations. For example Mitsubishi changed the name
of one of its SUV for the Spanish market, since the
original name Pajero had a very negative prior po-
larity, as it meant ?wanker? in Spanish (Piller, 2003).
To our knowledge, the only work trying to address
the SWN posterior-to-prior polarity issue, compar-
ing some of the approaches appeared in the literature
is (Gatti and Guerini, 2012). However, in our previ-
ous study we only considered a regression frame-
work, we did not use machine learning and we only
tested SWN1. So, we took this work as a starting
point for our analysis and expanded on it.
3 SentiWordNet
SentiWordNet (Esuli and Sebastiani, 2006) is a
lexical resource in which each entry is a set of
1260
lemma-PoS pairs sharing the same meaning, called
?synset?. Each synset s is associated with the nu-
merical scores Pos(s) and Neg(s), which range
from 0 to 1. These scores ? automatically as-
signed starting from a bunch of seed terms ? rep-
resent the positive and negative valence (or pos-
terior polarity) of the synset and are inherited by
each lemma-PoS in the synset. According to the
structure of SentiWordNet, each pair can have more
than one sense and each of them takes the form of
lemma#PoS#sense-number, where the small-
est sense-number corresponds to the most frequent
sense.
Obviously, different senses can have different po-
larities. In Table 1, the first 5 senses of cold#a
present all possible combinations, included mixed
scores (cold#a#4), where positive and negative
valences are assigned to the same sense. Intuitively,
mixed scores for the same sense are acceptable, as
in ?cold beer? (positive) vs. ?cold pizza? (negative).
PoS Offset Pos(s) Neg(s) SynsetTerms
a 1207406 0.0 0.75 cold#a#1
a 1212558 0.0 0.75 cold#a#2
a 1024433 0.0 0.0 cold#a#3
a 2443231 0.125 0.375 cold#a#4
a 1695706 0.625 0.0 cold#a#5
Table 1: First five SentiWordNet entries for cold#a
In our experiments we use two different versions
of SWN: SentiWordNet 1.0 (SWN1), the first re-
lease of SWN, and its updated version SentiWord-
Net 3.0 (Baccianella et al, 2010) ? SWN3. In
SWN3 the annotation algorithm used in SWN1
was revised, leading to an increase in the accuracy
of posterior polarities over the previous version.
4 Prior Polarities Formulae
In this section we review the main strategies for
computing prior polarities used in previous stud-
ies. All the proposed approaches try to estimate
the prior polarity score from the posterior polari-
ties of all the senses for a single lemma-PoS. Given
a lemma-PoS with n senses (lemma#PoS#n), ev-
ery formula f is independently applied to all the
Pos(s) and Neg(s) . This produces two scores,
f(posScore) and f(negScore), for each lemma-
PoS. To obtain a unique prior polarity for each
lemma-PoS, f(posScore) and f(negScore) can be
mapped according to different strategies:
fm =
?
??
??
f(posScore) if f(posScore) ?
f(negScore)
?f(negScore) otherwise
fd = f(posScore)? f(negScore)
where fm computes the absolute maximum of
the two scores, while fd computes the difference
between them. It is worth noting that f(negScore)
is always positive by construction. To obtain
a final prior polarity that ranges from -1 to 1,
the negative sign is imposed. So, consider-
ing the first 5 senses of cold#a in Table 1,
f(posScore) will be derived from the Pos(s) val-
ues <0.0, 0.0, 0.0, 0.125, 0.625>, while f(negScore)
from <0.750, 0.750, 0.0, 0.375, 0.0>. Then, the fi-
nal polarity strength returned will be either fm or fd.
The formulae (f ) we tested are the following:
fs. In this formula only the first (and thus
most frequent) sense is considered for the given
lemma#PoS. This is equivalent to considering only
the SWN score for lemma#PoS#1. Based on
(Neviarouskaya et al, 2009; Agrawal and Siddiqui,
2009; Guerini et al, 2008; Chowdhury et al, 2013),
this is the most basic form of prior polarities.
mean. It calculates the mean of the positive
and negative scores for all the senses of the given
lemma#PoS. This formula has been used in (Thet
et al, 2009; Denecke, 2009; Devitt and Ahmad,
2007; Sing et al, 2012).
uni. Based on (Neviarouskaya et al, 2009), it
considers only those senses that have a Pos(s)
greater than or equal to the corresponding Neg(s),
and greater than 0 (the stronglyPos set). In case
posScore is equal to negScore, the one with the
highest weight is returned, where weights are de-
fined as the cardinality of stronglyPos divided by
the total number of senses. The same applies for the
negative senses. This is the only method, together
with rnd, for which we cannot apply fd, as it returns
a positive or negative score according to the weight.
uniw. Like uni but without the weighting system.
w1. This formula weighs each sense with a geo-
metric series of ratio 1/2. The rationale behind this
choice is based on the assumption that more frequent
1261
senses should bear more ?affective weight? than rare
senses when computing the prior polarity of a word.
The system presented in (Chaumartin, 2007) uses a
similar approach of weighted mean.
w2. Similar to the previous one, this formula
weighs each lemma with a harmonic series, see for
example (Denecke, 2008).
On top of these formulae, we implemented some
new formulae that were relevant to our task and
have not been implemented before. These for-
mulae mimic the ones discussed previously, but
they are built under a different assumption: that
the saliency (Giora, 1997) of a word?s prior polar-
ity might be more related to its posterior polari-
ties score, rather than to sense frequencies. Thus
we ordered posScore and negScore by strength,
giving more relevance to ?valenced? senses. For
instance, in Table 1, posScore and negScore
for cold#a become<0.625, 0.125, 0.0, 0.0, 0.0> and
<0.750, 0.750, 0.375, 0.0, 0.0> respectively.
w1s and w1n. Like w1 and w2, but senses are
ordered by strength (sorting Pos(s) and Neg(s) in-
dependently).
w1n and w2n. Like w1 and w2 respectively, but
without considering senses that have a 0 score for
both Pos(s) and Neg(s). Our motivation is that
?empty? senses are mostly noise.
w1sn and w2sn. Like w1s and w2s, but with-
out considering senses that have a 0 score for both
Pos(s) and Neg(s).
median: return the median of the senses ordered
by polarity score.
All these prior polarities formulae are compared
against two gold standards (one for regression, one
for classification) both one by one, as in the works
mentioned above, and combined together in a learn-
ing framework (to see whether combining these fea-
tures ? that capture different aspect of prior polari-
ties ? can further improve the results).
Finally, we implemented two variants of a prior
polarity random baseline to asses possible advan-
tages of approaches using SWN:
rnd. This formula represents the basic baseline
random approach. It simply returns a random num-
ber between -1 and 1 for any given lemma#PoS.
swnrnd. This formula represents an advanced
random approach that incorporates some ?knowl-
edge? from SWN. It takes the scores of a random
sense for the given lemma#PoS. We believe this
is a fairer baseline than rnd since SWN informa-
tion can possibly constrain the values. A similar ap-
proach has been used in (Qu et al, 2008).
5 Learning Algorithms
We used two non-parametric learning approaches,
Support Vector Machines (SVMs) (Shawe-Taylor
and Cristianini, 2004) and Gaussian Processes (GPs)
(Rasmussen and Williams, 2006), to test the perfor-
mance of all the metrics in conjunction. SVMs are
non-parametric deterministic algorithms that have
been widely used in several fields, in particular in
NLP where they are the state-of-the-art for various
tasks. GPs, on the other hand, are an extremely flex-
ible non-parametric probabilistic framework able to
explicitly model uncertainty, that, despite being con-
sidered state-of-the-art in regression, have rarely
been used in NLP. To our knowledge only two pre-
vious works did so (Polajnar et al, 2011; Cohn and
Specia, 2013).
Both methods take advantage of the kernel trick,
a technique used to embed the original feature space
into an alternative space where data may be linearly
separable. This is performed by the kernel func-
tion that transforms the input data in a new structure,
called kernel. How it is used to produce the predic-
tion is one of the main differences between SVMs
and GPs. In classification SVMs use the geomet-
ric mean to discriminate between the positive and
negative classes, while the GP model uses the pos-
terior probability distribution over each class. Both
frameworks support learning algorithms for regres-
sion and classification. An exhaustive explanation
of the two methodologies can be found in (Shawe-
Taylor and Cristianini, 2004) and (Rasmussen and
Williams, 2006).
In the SVM experiments, we use C-SVM and -
SVM implemented in the LIBSVM toolbox (Chang
and Lin, 2011). The selection of the kernel (linear,
polynomial, radial basis function and sigmoid) and
the optimization of the parameters are carried out
through grid search in 10-fold cross-validation.
GP regression models with Gaussian noise are a
rare exception where the exact inference with like-
lihood functions is tractable, see ?2 in (Rasmussen
1262
and Williams, 2006). Unfortunately, this is not valid
for the classification task ? see ?3 in (Rasmussen and
Williams, 2006) ? where an approximation method
is required. In this work, we use the Laplace ap-
proximation method proposed in (Williams and Bar-
ber, 1998). Different kernels are tested (covariance
for constant functions, linear with and without au-
tomatic relevance determination (ARD)1, Matern,
neural network, etc.2) and the linear logistic (lll)
and probit regression (prl) likelihood functions are
evaluated in classification. In our classification ex-
periments we tried all possible combinations of ker-
nels and likelihood functions, while in the regression
tests we ranged only on different kernels. All the GP
models were implemented using the GPML Matlab
toolbox3. Unlike SVMs, the optimization of the ker-
nel parameters can be performed without using grid
search, but the optimal parameters can be obtained
iteratively, by maximizing the marginal likelihood
(or in classification, the Laplace approximation of
the marginal likelihood). We fix at 100 the maxi-
mum number of iterations.
An interesting property of the GPs is their capa-
bility of weighting the features differently accord-
ing to their importance in the data. This is referred
to as the automatic variance determination kernel.
As demonstrated in (Weston et al, 2000), SVMs
can benefit from the application of feature selec-
tion techniques especially when there are highly re-
dundant features. Since the prior polarities formu-
lae tend to cluster in groups that provide similar re-
sults (Gatti and Guerini, 2012) ? creating noise for
the learner ? we want to understand whether feature
selection approaches can boost the performance of
SVMs. For this reason, we also test feature selection
prior to the SVM training. For that we used Ran-
domized Lasso, or stability selection (Meinshausen
and Bu?hlmann, 2010). Re-sampling of the training
data is performed several times and a Lasso regres-
sion model is fit on each sample. Features that ap-
pear in a given number of samples are retained. Both
the fraction of the data to be sampled and the thresh-
old to select the features can be configured. In our
1linone and linard in the result tables, respectively.
2More detailed information on the available kernels are in
?4 (Rasmussen and Williams, 2006)
3http://www.gaussianprocess.org/gpml/
code/matlab/doc/
experiments we set the sampling fraction to 75%,
the selection threshold to 25% and the number of re-
samples to 1,000. We refer to these as SVMfs.
6 Gold Standards
To assess how well prior polarity formulae perform,
a gold standard with word polarities provided by hu-
man annotators is needed. There are many such re-
sources in the literature, each with different cover-
age and annotation characteristics. ANEW (Bradley
and Lang, 1999) rates the valence score of 1,034
words, which were presented in isolation to anno-
tators. The SO-CAL entries (Taboada et al, 2011)
were collected from corpus data and then manu-
ally tagged by a small number of annotators with
a multi-class label. These ratings were further vali-
dated through crowdsourcing. Other resources, such
as the General Inquirer lexicon (Stone et al, 1966),
provide a binomial classification (either positive or
negative) of sentiment-bearing words. The resource
presented in (Wilson et al, 2005) uses a similar bi-
nomial annotation for single words; another inter-
esting resource is WordNetAffect (Strapparava and
Valitutti, 2004) but it labels words senses and it can-
not be used for the prior polarity validation task.
In the following we describe in detail the two
resources we used for our experiments, namely
ANEW for the regression experiments and the Gen-
eral Inquirer (GI) for the classification ones.
6.1 ANEW
ANEW (Bradley and Lang, 1999) is a resource de-
veloped to provide a set of normative emotional rat-
ings for a large number of words (roughly 1 thou-
sand) in the English language. It contains a set of
words that have been rated in terms of pleasure (af-
fective valence), arousal, and dominance. In par-
ticular for our task we considered the valence di-
mension. Since words were presented to subjects
in isolation (i.e. no context was provided) this re-
source represents a human validation of prior polar-
ities scores for the given words, and can be used as a
gold standard. For each word ANEW provides two
main metrics: anew?, which correspond to the av-
erage of annotators votes, and anew?, which gives
the variance in annotators scores for the given word.
In the same way these metrics are also provided for
1263
the male/female annotator groups.
6.2 General Inquirer
The Harvard General Inquirer dictionary is a widely
used resource, built for automatic text analysis
(Stone et al, 1966). Its latest revision4 contains
11789 words, tagged with 182 semantic and prag-
matic labels, as well as with their part of speech.
Words and their categories were initially taken
from the Harvard IV-4 Psychosociological Dictio-
nary (Dunphy et al, 1974) and the Lasswell Value
Dictionary (Lasswell and Namenwirth, 1969). For
this paper we consider the Positiv and Negativ
categories (1,915 words the former, 2,291 words the
latter, for a total of 4,206 affective words).
7 Experiments
In order to use the ANEW dataset to measure
prior polarities formulae performance, we had to
assign a PoS to all the words to obtain the SWN
lemma#PoS format. To do so, we proceeded as
follows: for each word, check if it is present among
both SWN1 and SWN3 lemmas; if not, lemmatize
the word with the TextPro tool suite (Pianta et al,
2008) and check if the lemma is present instead5.
If it is not found (i.e., the word cannot be aligned
automatically), remove the word from the list (this
was the case for 30 words of the 1,034 present in
ANEW). The remaining 1,004 lemmas were then
associated with all the PoS present in SWN to get
the final lemma#PoS. Note that a lemma can have
more than one PoS, for example, writer is present
only as a noun (writer#n), while yellow is present
as a verb, a noun and an adjective (yellow#v,
yellow#n, yellow#a). This gave us a list of
1,484 words in the lemma#PoS format.
In a similar way we pre-processed the GI words
that uses the generic modif label to indicate ei-
ther adjective or adverb (noun and verb PoS were
instead consistently used). Finally, all the sense-
disambiguated words in the lemma#PoS#n format
were discarded (1,114 words out of the 4,206 words
with positive or negative valence).
4http://www.wjh.harvard.edu/?inquirer/
5We did not lemmatize everything to avoid duplications (for
example, if we lemmatize the ANEW entry addicted, we obtain
addict, which is already present in ANEW).
After the two datasets were built this way, we
removed the words for which the posScore and
negScore contained all 0 in both SWN1 and
SWN3 (523 lemma#PoS for ANEW and 484 for
the GI dataset), since these words are not informa-
tive for our experiments. The final dataset included
961 entries for ANEW and 2,557 for GI. For each
lemma#PoS in GI and ANEW, we then applied the
prior polarity formulae described in Section 4, using
both SWN1 and SWN3 and annotated the results.
According to the nature of the human labels (real
numbers or -1/1), we ran several regression and clas-
sification experiments. In both cases, each dataset
was randomly split into 70% for training and the re-
maining for test. This process was repeated 5 times
to generate different splits. For each partition, opti-
mization of the learning algorithm parameters was
performed on the training data (in 10-fold cross-
validation for SVMs). Training and test sets were
normalized using the z-score.
To evaluate the performance of our regression ex-
periments on ANEW we used the Mean Absolute
Error (MAE), that averages the error over a given
test set. Accuracy was used for the classification ex-
periments on GI instead. We opted for accuracy ?
rather than F1 ? since for us True Negatives have
same importance as True Positives. For each experi-
ments we reported the average performance and the
standard deviation over the 5 random splits. In the
following sections, to check if there was a statisti-
cally significant difference in the results, we used
Student?s t-test for regression experiments, while
an approximate randomization test (Yeh, 2000) was
used for the classification experiments.
In Tables 2 and 3, the results of regression exper-
iments over the ANEW dataset, using SWN1 and
SWN3, are presented. The results of the classifica-
tion experiments over the GI dataset, using SWN1
and SWN3 are shown in Tables 4 and 5. For the
sake of interpretability, results are divided accord-
ing to the main approaches: randoms, posterior-to-
prior formulae, learning algorithms. Note that for
classification we report the generics f and not the
fm and fd variants. In fact, both versions always
return the same classification answer (we are clas-
sifying according to the sign of f result and not its
strength). For the GPs, we report the two best con-
figurations only.
1264
MAE ? MAE ?
rnd 0.652 0.026
swnrndm 0.427 0.011
swnrndd 0.426 0.009
uniwm 0.420 0.009
maxm 0.419 0.009
fsd 0.413 0.011
fsm 0.412 0.009
uni 0.410 0.010
uniwd 0.406 0.007
w1snm 0.405 0.011
maxd 0.404 0.005
w2snm 0.402 0.011
mediand 0.401 0.014
w1d 0.401 0.010
w1nd 0.399 0.008
meand 0.398 0.010
w2d 0.398 0.010
medianm 0.397 0.015
w1snd 0.397 0.008
w2snd 0.397 0.008
w2nd 0.397 0.008
w1sm 0.396 0.010
w1m 0.396 0.010
w1nm 0.394 0.009
meanm 0.393 0.011
w2sd 0.393 0.008
w1sd 0.393 0.009
w2sm 0.392 0.010
w2m 0.391 0.011
w2nm 0.391 0.012
GPlinard 0.398 0.014
GPlinone 0.398 0.014
SVM 0.367 0.010
SVMfs 0.366 0.011
AVERAGE 0.398 0.010
Table 2: MAE results for metrics using SWN1
8 General Discussion
In this section we sum up the main results of our
analysis, providing an answer to the various ques-
tions we introduced at the beginning of the paper:
SentiWordNet improves over random. One of
the first things worth noting ? in Tables 2, 3, 4 and
5 ? is that the random approach (rnd), as expected,
is the worst performing metric, while all other ap-
proaches, based on SWN, have statistically signif-
icant improvements both for MAE and for Accu-
racy (p < 0.001). So, using SWN for posterior-
to-prior polarity computation brings benefits, since
it increases the performance above the baseline in
words? prior polarity assessment.
SWN3 is better than SWN1. With respect to
MAE ? MAE ?
rnd 0.652 0.026
swnrndd 0.404 0.013
swnrndm 0.402 0.010
maxm 0.393 0.009
fsd 0.382 0.008
uniwm 0.382 0.015
fsm 0.381 0.010
medianm 0.377 0.008
uniwd 0.377 0.012
mediand 0.377 0.011
uni 0.376 0.010
maxd 0.372 0.011
meand 0.371 0.010
w1snm 0.371 0.011
w2snm 0.369 0.010
w1d 0.368 0.010
w2d 0.367 0.010
meanm 0.367 0.010
w1m 0.365 0.010
w2snd 0.364 0.011
w1snd 0.364 0.010
w1sm 0.363 0.009
w1nd 0.362 0.009
w2sd 0.362 0.010
w2m 0.362 0.010
w1sd 0.362 0.009
w1nm 0.362 0.007
w2nd 0.361 0.010
w2sm 0.360 0.009
w2nm 0.359 0.009
GPlinone 0.356 0.008
GPlinard 0.355 0.008
SVM 0.333 0.004
SVMfs 0.333 0.003
AVERAGE 0.366 0.009
Table 3: MAE results for regression using SWN3
SWN1, using SWN3 enhances performance, both
in regression (MAE ? 0.398 vs. 0.366, p < 0.001)
and classification (Accuracy ? 0.710 vs. 0.771,
p < 0.001) tasks. Since many of the approaches
described in the literature use SWN1 their results
should be revised and SWN3 should be used as
standard. This difference in performance can be
partially explained by the fact that, even after pre-
processing, for the ANEW dataset 137 lemma#PoS
have all senses equal to 0 in SWN1, while in SWN3
they are just 48. In the GI lexicon the numbers are
233 for SWN1 and 69 for SWN3.
Not all formulae are created equal. The formu-
lae described in Section 4 have very different results,
along a continuum. While inspecting every differ-
1265
Acc. ? Acc. ?
rnd 0.447 0.019
swn rndm 0.639 0.026
swn rndd 0.646 0.021
fs m 0.659 0.020
uni 0.684 0.017
median 0.686 0.022
uniw 0.702 0.019
max 0.710 0.022
w1 0.712 0.021
w1n 0.713 0.022
w2n 0.714 0.023
w2 0.715 0.021
mean 0.718 0.023
w2s 0.719 0.023
w2sn 0.719 0.023
w1s 0.719 0.023
w1sn 0.719 0.023
GP llllinard 0.721 0.026
GP prllinard 0.722 0.025
SVM 0.733 0.021
SVMfs 0.743 0.021
Average 0.710 0.022
Table 4: Accuracy results for classification using SWN1
ence in performance is out of the scope of the present
paper, we can see that there is a strong difference be-
tween best and worst performing formulae both in
regression (in Table 2 w2nm is better than uniwm,
in Table 3 w2nm is better than maxm) and classifi-
cation (in Table 4 w1snm is better than fsm,in Ta-
ble 5 w2m is better than fsm) and these differences
are all statistically significant (p < 0.001). Again,
these results indicate that the previous experiments
in the literature that use SWN as a baseline should
be revised to take these results into account. Further-
more, the new formulae we introduced, based on the
?posterior polarities saliency? hypothesis, proved to
be among the best performing in all experiments.
This entails that there is room for inspecting new
formulae variants other than those already proposed
in the literature.
Selecting just one sense is not a good choice.
On a side note, the approaches that rely on only one
sense polarity (namely fs, median and max) have
similar results which do not differ significantly from
swnrnd (for maxm, fsd and fsm in Table 2, and
for maxm in Table 3). These same approaches are
also far from the best performing formulae: in Ta-
ble 3, mediand differs from w2nm (p < 0.05), as
do maxm, maxd, fsm and fsd (p < 0.001); in Ta-
Acc. ? Acc. ?
rnd 0.447 0.019
swn rndd 0.700 0.030
swn rndm 0.706 0.034
fs 0.723 0.014
medianm 0.742 0.016
uni 0.750 0.015
uniw 0.762 0.023
max 0.769 0.019
w2s 0.777 0.017
w2sn 0.777 0.017
w1s 0.777 0.017
w1sn 0.777 0.017
w1n 0.780 0.021
w2n 0.780 0.022
mean 0.781 0.018
w1 0.781 0.021
w2 0.781 0.021
SVM 0.779 0.016
GPl 0.779 0.018
GPg 0.781 0.018
SVMfs 0.792 0.014
Average 0.771 0.018
Table 5: Accuracy results for classification using SWN3
ble 3, fs, max and median in both their fm and fd
variants are significantly different from the best per-
forming w2nm (p < 0.001). For classification, in
Table 4 and 5 the difference between the correspond-
ing best performing formula and the single senses
formulae is always significant (at least p < 0.01).
Among other things, this finding entails, surpris-
ingly, that taking the first sense of a lemma#PoS in
some cases has no improvement over taking a ran-
dom sense, and that in all cases it is one of the worst
approaches with SWN . This is surprising since in
many NLP tasks, such as word sense disambigua-
tion, algorithms based on most frequent sense repre-
sent a very strong baseline6.
Learning improvements. Combining the formu-
lae in a learning framework further improves the
results over the best performing formulae, both in
regression (MAE? with SWN1 0.366 vs. 0.391,
p < 0.001; MAE? with SWN3 0.333 vs. 0.359,
p < 0.001) and in classification (Accuracy? for
SWN1 is 0.743 vs. 0.719, p < 0.001; Accuracy?
for SWN3 is 0.792 vs. 0.781, not significant p =
0.07). Another thing worth noting is that, in re-
gression, GPs are outperformed by both versions of
6In SemEval 2010, only 5 participants out of 29 performed
better than the most frequent threshold (Agirre et al, 2010).
1266
SVM (p < 0.001), see Tables 2 and 3. This is in
contrast with the results presented in (Cohn and Spe-
cia, 2013), where GPs on the single task are on av-
erage better than SVMs. In classification, GPs have
similar performance to SVM without feature selec-
tion, and in some cases (see Table 5) even slightly
better. Analyzing the selected kernels for GPs and
SVMs, we notice that in most of the splits SVMs
prefer the radial based function, while the best per-
formance with the GPs are obtained with linear ker-
nels with and without ARD. There is no significant
difference in using linear logistic and probit regres-
sion likelihoods. In all our experiments, SVM with
feature selection leads to the best performance. This
is not surprising due the high level of redundancy
in the formulae scores. Interestingly, inspecting the
most frequent selected features by SVMfs, we see
that features from different groups are selected, and
even the worst performing formulae can add infor-
mation, confirming the idea that viewing the same
information from different perspectives (i.e. the pos-
terior polarities provided by SWN combined in var-
ious ways) can give better predictions.
To sum up: the new state-of-the-art performance
level in prior-polarity computation is represented
by the SVMfs approach using SWN3, and this
should be used as the reference from now on.
9 PoS and Gender Experiments
Next, we wanted to understand if the performance of
our approach, using SWN3, was consistent across
word PoS. In Table 6 we report the results for the
best performing formulae and learning algorithm on
the GI PoS classes. In particular for ADJ there are
1,073 words, 922 for NOUN and 508 for VERB. We
discarded adverbs since the class was too small to
allow reliable evaluation and efficient learning (only
54 instances). The results show a greater accuracy
for adjectives (p < 0.01), while performance for
nouns and verbs are similar.
SVMfs best f
Acc. ? Acc. ? Acc. ? Acc. ?
ADJ 0.829 0.019 0.821 0.016
NOUN 0.784 0.021 0.765 0.023
VERBS 0.782 0.052 0.744 0.046
Table 6: Accuracy results for PoS using SWN3
Finally we test against the male and female ratings
provided by ANEW. As can be seen from Table 7,
SWN approaches are far more precise in predicting
Male judgments rather than Female ones (MAE?
goes from 0.392 to 0.323 with the best formula and
from 0.369 to 0.292 with SVMfs, both differences
are significant p < 0.001). Instead, in Table 8 ?
which displays the results along gender and polarity
dimensions ? there is no statistically significant dif-
ference in MAE on positive words between male
and female, while there is a strong statistical signifi-
cance for negative words (p < 0.001).
Interestingly, there is also a large difference be-
tween positive and negative affective words (both
for male and female dimensions). This difference
is maximum for male scores on positive words com-
pared to female scores on negative words (0.283 vs.
0.399, p < 0.001). Recent work by Warriner et al
(2013) inspected the differences in prior polarity as-
sessment due to gender.
At this stage we can only note that prior polari-
ties calculated with SWN are closer to ANEW male
annotations than female ones. Understanding why
this happens would require an accurate examination
of the methods used to create WordNet and SWN
(which will be the focus of our future work).
Male female
MAE ? MAE ? MAE ? MAE ?
SVMfs 0.292 0.020 0.369 0.008
best f 0.323 0.022 0.392 0.010
Table 7: MAE results for Male vs Female using SWN3
Male female
MAE ? MAE ? MAE ? MAE ?
Pos 0.283 0.022 0.340 0.009
Neg 0.301 0.029 0.399 0.013
Table 8: MAE for Male/Female - Pos/Neg using SWN3
10 Conclusions
We have presented a study on the posterior-to-prior
polarity issue, i.e. the problem of computing words?
prior polarity starting from their posterior polarities.
Using two different versions of SentiWordNet and
30 different approaches that have been proposed in
the literature, we have shown that researchers have
not paid sufficient attention to this issue. Indeed, we
1267
showed that the better variants outperform the oth-
ers on different datasets both in regression and clas-
sification tasks, and that they can represent a fairer
state-of-art baseline approach using SentiWordNet.
On top of this, we also showed that these state-of-
the-art formulae can be further outperformed using
a learning framework that combines the various for-
mulae together. We conclude our analysis with some
experiments investigating the impact of word PoS
and annotator gender in gold standards, showing in-
teresting phenomena that requires further investiga-
tion.
Acknowledgments
The authors thanks Jose? Camargo De Souza for his
help with feature selection. This work has been par-
tially supported by the Trento RISE PerTe project.
References
E. Agirre, O.L. De Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation (IWSE ?10), pages 75?80, Uppsala, Sweden.
S. Agrawal and T.J. Siddiqui. 2009. Using syntactic and
contextual information for sentiment polarity analysis.
In Proceedings of the 2nd International Conference on
Interaction Sciences: Information Technology, Culture
and Human (ICIS ?09), pages 620?623, Seoul, Repub-
lic of Korea.
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Senti-
WordNet 3.0: An enhanced lexical resource for sen-
timent analysis and opinion mining. In Proceed-
ings of the 7th Conference on International Language
Resources and Evaluation (LREC ?10), pages 2200?
2204, Valletta, Malta.
M.M. Bradley and P.J. Lang. 1999. Affective norms for
English words (ANEW): Instruction manual and af-
fective ratings. Technical Report C-1, University of
Florida.
C.C. Chang and C.J. Lin. 2011. LIBSVM: A library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2:27:1?27:27.
F.R. Chaumartin. 2007. UPAR7: A knowledge-based
system for headline sentiment tagging. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (IWSE ?07), pages 422?425, Prague, Czech Re-
public.
F.M. Chowdhury, M. Guerini, S. Tonelli, and A. Lavelli.
2013. Fbk: Sentiment analysis in twitter with tweet-
sted. In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM): Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval ?13), volume 2, pages 466?470, Atlanta,
Georgia, USA, June.
T. Cohn and L. Specia. 2013. Modelling annotator bias
with multi-task gaussian processes: An application to
machine translation quality estimation. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (ACL ?13), pages 32?
42, Sofia, Bulgaria.
K. Denecke. 2008. Accessing medical experiences
and information. In Proceedings of the 18th Euro-
pean Conference on Artificial Intelligence, Workshop
on Mining Social Data (MSoDa ?08), Patras, Greece.
K. Denecke. 2009. Are SentiWordNet scores suited for
multi-domain sentiment classification? In Proceed-
ings of the 4th International Conference on Digital
Information Management (ICDIM ?09), pages 32?37,
Ann Arbor, MI, USA.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based ap-
proach. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics (ACL
?07), pages 984?991, Prague, Czech Republic.
D.C. Dunphy, C.G. Bullard, and E.E.M. Crossing. 1974.
Validation of the General Inquirer Harvard IV Dictio-
nary. Paper presented at the Pisa Conference on Con-
tent Analysis.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Inter-
national Language Resources and Evaluation (LREC
?06), pages 417?422, Genova, Italy.
L. Gatti and M. Guerini. 2012. Assessing sentiment
strength in words prior polarities. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING ?12), pages 361?370, Mumbai,
India.
R. Giora. 1997. Understanding figurative and literal lan-
guage: The graded salience hypothesis. Cognitive Lin-
guistics, 8:183?206.
M. Guerini, O. Stock, and C. Strapparava. 2008.
Valentino: A tool for valence shifting of natural lan-
guage texts. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC ?08), pages 243?246, Marrakech, Morocco.
H.D. Lasswell and J.Z. Namenwirth. 1969. The Lasswell
value dictionary. New Haven.
B. Liu and L. Zhang. 2012. A survey of opinion mining
and sentiment analysis. Mining Text Data, pages 415?
463.
1268
N. Meinshausen and P. Bu?hlmann. 2010. Stability selec-
tion. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 72(4):417?473.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2009. Sentiful: Generating a reliable lexicon for
sentiment analysis. In Proceedings of the 3rd Affec-
tive Computing and Intelligent Interaction (ACII ?09),
pages 363?368, Amsterdam, Netherlands.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2011. Affect analysis model: novel rule-based ap-
proach to affect sensing from text. Natural Language
Engineering, 17(1):95.
G. O?zbal and C. Strapparava. 2012. A computational ap-
proach to the automation of creative naming. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL ?12), pages 703?
711, Jeju Island, Korea.
G. O?zbal, C. Strapparava, and M. Guerini. 2012. Brand
Pitt: A corpus to explore the art of naming. In Pro-
ceedings of the 8th International Conference on Lan-
guage Resources and Evaluation (LREC ?12), pages
1822?1828, Istanbul, Turkey.
G. Paltoglou, M. Thelwall, and K. Buckley. 2010. On-
line textual communications annotated with grades of
emotion strength. In Proceedings of the 3rd Interna-
tional Workshop of Emotion: Corpora for research on
Emotion and Affect (satellite of LREC ?10), pages 25?
31, Valletta, Malta.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro
tool suite. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC ?08), pages 2603?2607, Marrakech, Morocco.
I. Piller. 2003. Advertising as a site of language contact.
Annual Review of Applied Linguistics, 23:170?183.
T. Polajnar, S. Rogers, and M. Girolami. 2011. Protein
interaction detection in sentences via gaussian pro-
cesses: a preliminary evaluation. International jour-
nal of data mining and bioinformatics, 5(1):52?72.
L. Qu, C. Toprak, N. Jakob, and I. Gurevych. 2008.
Sentence level subjectivity and sentiment analysis ex-
periments in NTCIR-7 MOAT challenge. In Proceed-
ings of the 7th NTCIR Workshop Meeting (NTCIR ?08),
pages 210?217, Tokyo, Japan.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaussian
processes for machine learning. MIT Press.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel meth-
ods for pattern analysis. Cambridge university press.
J.K. Sing, S. Sarkar, and T.K. Mitra. 2012. Devel-
opment of a novel algorithm for sentiment analysis
based on adverb-adjective-noun combinations. In Pro-
ceedings of the 3rd National Conference on Emerging
Trends and Applications in Computer Science (NC-
ETACS ?12), pages 38?40, Shillong, India.
P.J. Stone, D.C. Dunphy, and M.S. Smith. 1966. The
General Inquirer: A Computer Approach to Content
Analysis. MIT press.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC ?04), pages 1083 ?
1086, Lisbon, Portugal.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2011. Lexicon-based methods for senti-
ment analysis. Computational linguistics, 37(2):267?
307.
T.T. Thet, J.C. Na, C.S.G. Khoo, and S. Shakthikumar.
2009. Sentiment analysis of movie reviews on dis-
cussion boards using a linguistic approach. In Pro-
ceedings of the 1st international CIKM workshop on
Topic-sentiment analysis for mass opinion (TSA ?09),
pages 81?84, Hong Kong.
S. Wang and C.D. Manning. 2012. Baselines and bi-
grams: Simple, good sentiment and topic classifica-
tion. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (ACL ?12),
pages 90?94, Jeju Island, Korea.
A.B. Warriner, V. Kuperman, and M. Brysbaert. 2013.
Norms of valence, arousal, and dominance for 13,915
english lemmas. Behavior research methods, pages 1?
17.
J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Pog-
gio, and V. Vapnik. 2000. Feature selection for SVMs.
In Proceedings of the 14th Conference on Neural In-
formation Processing Systems (NIPS ?00), pages 668?
674, Denver, CO, USA.
C.K.I. Williams and D. Barber. 1998. Bayesian clas-
sification with gaussian processes. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(12):1342?1351.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the 19th National Conference on Artifi-
cial Intelligence (AAAI ?04), pages 761?769, San Jose,
CA, USA.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP ?05), pages
347?354, Vancouver, Canada.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In Proceedings of the
18th International Conference on Computational Lin-
guistics (COLING ?00), pages 947?953, Saarbru?cken,
Germany.
1269
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1643?1653,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Assessing the Impact of Translation Errors
on Machine Translation Quality with Mixed-effects Models
Marcello Federico, Matteo Negri, Luisa Bentivogli, Marco Turchi
FBK - Fondazione Bruno Kessler
Via Sommarive 18, 38123 Trento, Italy
{federico,negri,bentivogli,turchi}@fbk.eu
Abstract
Learning from errors is a crucial aspect of
improving expertise. Based on this no-
tion, we discuss a robust statistical frame-
work for analysing the impact of different
error types on machine translation (MT)
output quality. Our approach is based on
linear mixed-effects models, which allow
the analysis of error-annotated MT out-
put taking into account the variability in-
herent to the specific experimental setting
from which the empirical observations are
drawn. Our experiments are carried out
on different language pairs involving Chi-
nese, Arabic and Russian as target lan-
guages. Interesting findings are reported,
concerning the impact of different error
types both at the level of human perception
of quality and with respect to performance
results measured with automatic metrics.
1 Introduction
The dominant statistical approach to machine
translation (MT) is based on learning from large
amounts of parallel data and tuning the result-
ing models on reference-based metrics that can
be computed automatically, such as BLEU (Pap-
ineni et al., 2001), METEOR (Banerjee and Lavie,
2005), TER (Snover et al., 2006), GTM (Turian
et al., 2003). Despite the steady progress in the
last two decades, especially for few well resourced
translation directions having English as target lan-
guage, this way to approach the problem is quickly
reaching a performance plateau. One reason is
that parallel data are a source of reliable informa-
tion but, alone, limit systems knowledge to ob-
served positive examples (i.e. how a sentence
should be translated) without explicitly modelling
any notion of error (i.e. how a sentence should
not be translated). Another reason is that, as a
development and evaluation criterion, automatic
metrics provide a holistic view of systems? be-
haviour without identifying the specific issues of a
translation. Indeed, the global scores returned by
MT evaluation metrics depend on comparisons be-
tween translation hypotheses and reference trans-
lations, where the causes and the nature of the dif-
ferences between them are not identified.
To cope with these issues and define system
improvement priorities, the focus of MT evalua-
tion research is gradually shifting towards profil-
ing systems? behaviour with respect to various ty-
pologies of errors (Vilar et al., 2006; Popovi?c and
Ney, 2011; Farr?us et al., 2012, inter alia). This
shift has enriched the traditional MT evaluation
framework with a new element, that is the actual
errors done by a system. Until now, most of the
research has focused on the relationship (i.e. the
correlation) between two elements of the frame-
work: humans and automatic evaluation metrics.
As a new element of the framework, which be-
comes a sort of ?evaluation triangle?, the analy-
sis of error annotations opens interesting research
problems related to the relationships between: i)
error types and human perception of MT quality
and ii) error types and the sensitivity of automatic
metrics.
Besides motivating further investigation on met-
rics featuring high correlation with human judge-
ments (a well-established MT research sub-field,
which is out of the scope of this paper), connecting
the vertices of this triangle raises new challenging
questions such as:
(1) Which types of MT errors have the high-
est impact on human perception of translation
quality? Surprisingly, little prior work focused
on this side of the triangle. Error annotations
have been considered to highlight strengths and
weaknesses of MT engines or to investigate the
influence of different error types on post-editors?
work. However, the direct connection between er-
1643
rors and users? preferences has been only partially
understood, mainly from a descriptive standpoint
and through rudimentary techniques unsuitable to
draw clear-cut conclusions or reliable inferences.
(2) To which types of errors are different MT
evaluation metrics more sensitive? This side of
the triangle has been even less explored. For in-
stance, little has been done to understand which
automatic metric is more suitable to assess sys-
tem improvements with respect to a specific issue
(e.g. word order or morphology) or to shed light
on the joint impact of different error types on per-
formance results calculated with different metrics.
To answer these questions, we propose a ro-
bust statistical framework to analyse the im-
pact of different error types, alone and in com-
bination, both on human perception of quality and
on MT evaluation metrics? results. Our analysis
is carried out by employing linear mixed-effects
models, a generalization of linear regression mod-
els suited to model responses with fixed and ran-
dom effects. Experiments are performed on data
covering three translation directions (English to
Chinese, Arabic and Russian). For each direc-
tion, two automatic translations were collected for
around 400 sentences and were manually evalu-
ated by expert translators through absolute quality
judgements and error annotation.
Building on the advantages offered by linear
mixed-effects models, our main contributions in-
clude:
? A rigorous method, novel to MT error anal-
ysis research, to relate MT issues to human
preferences and MT metrics? results;
? The application of such method to three
translation directions having English as
source and different languages as target;
? A number of findings, specific to each lan-
guage direction, which are out of the reach of
the few simpler methods proposed so far.
Overall, our study has clear practical implica-
tions for MT systems? development and evalu-
ation. Indeed, the proposed statistical analysis
framework represents an ideal instrument to: i)
identify translation issues having the highest im-
pact on human perception of quality and ii) choose
the most appropriate evaluation metric to measure
progress towards their solution.
2 Related Work
Error analysis, as a way to identify systems? weak-
nesses and define priorities for their improvement,
is gaining increasing interest in the MT com-
munity (Popovi?c and Ney, 2011; Popovic et al.,
2013). Along this direction, the initial efforts to
develop error taxonomies covering different levels
of granularity (Flanagan, 1994; Vilar et al., 2006;
Farr?us Cabeceran et al., 2010; Stymne and Ahren-
berg, 2012; Lommel et al., 2014) have been re-
cently complemented by investigations on how to
exploit error annotations for diagnostic purposes.
Error annotations of sentences produced by differ-
ent MT systems, in different target languages and
domains, have been used to determine the qual-
ity of translations according to the amount of er-
rors encountered (Popovic et al., 2013), to design
new automatic metrics that take into considera-
tion human annotations (Popovic, 2012; Bojar et
al., 2013), and to train classifiers that can auto-
matic identify fine-grained errors in the MT output
(Popovi?c and Ney, 2011). The impact of edit op-
erations on post-editors? productivity, which im-
plicitly connects the severity of different errors to
human activity, has also been studied (Temnikova,
2010; O?Brien, 2011; Blain et al., 2011), but
few attempts have been made to explicitly model
how fine-grained errors impact on human quality
judgements and automatic metrics.
Recently, the relation between different error
types, their frequency, and human quality judge-
ments has been investigated from a descriptive
standpoint in (Lommel et al., 2014; Popovi?c et al.,
2014). In both works, however, the underlying as-
sumption that the most frequent error has also the
largest impact on quality perception is not verified
(in general and, least of all, across language pairs,
domains, MT systems and post-editors). Another
limitation of the proposed (univariate) analysis lies
in the fact that it exclusively focuses on error types
taken in isolation. This simplification excludes the
possibility that humans, when assigning a global
quality score to a translation, may be influenced
not only by the error types but also by their inter-
action. The implications of such possibility call
for a multivariate analysis capable to model also
error interactions.
In (Kirchhoff et al., 2013), a statistically-
grounded approach based on conjoint analysis has
been used to investigate users? reactions to dif-
ferent types of translation errors. According to
1644
their results, word order is the most dispreferred
error type, and the count of the errors in a sen-
tence is not a good predictor of users? prefer-
ences. Though more sophisticated than methods
based on rough error counts, the conjoint model
is bound to several constraints that limit its us-
ability. In particular, the application of conjoint
analysis in this context requires to: i) operate with
semi-automatically created (hence artificial) data
instead of real MT output, ii) manually define dif-
ferent levels of severity for each error type (e.g.
high/medium/low), and iii) limit the number of er-
ror types considered to avoid the explosion of all
possible combinations. Finally, the conjoint anal-
ysis framework is not able to explicitly model vari-
ance in the translated sentences, the human anno-
tators, and the SMT systems used to translate the
source sentences. Our claim is that avoiding any
possible bias introduced by these factors should be
a priority in the analysis of empirical observations
in a given experimental setting.
So far, the relation between errors and auto-
matic metrics has been analysed by measuring the
correlation between single or total error frequen-
cies and automatic scores (Popovi?c and Ney, 2011;
Farr?us et al., 2012). Using two different error tax-
onomies, both works show that the sum of the er-
rors has a high correlation with BLEU and TER
scores. Similar to the aforementioned works ad-
dressing the impact of MT errors on human per-
ception, these studies disregard error interactions,
and their possible impact on automatic scores.
To overcome these issues, we propose a ro-
bust statistic analysis framework based on mixed-
effects models, which have been successfully ap-
plied to several NLP problems such as sentiment
analysis (Greene and Resnik, 2009), automatic
speech recognition (Goldwater et al., 2010), and
spoken language translation (Ruiz and Federico,
2014). Despite their effectiveness, the use of
mixed-effects models in the MT field is rather re-
cent and limited to the analysis of human post-
editions (Green et al., 2013; L?aubli et al., 2013).
In both studies, the goal was to evaluate the im-
pact of post-editing on the quality and productivity
of human translation assuming an ANOVA mixed
model for a between-subject design, in which hu-
man translators either post-edited or translated the
same texts. Our scenario is rather different as we
employ mixed models to measure the influence of
different MT error types - expressed as continu-
ous fixed effects - on quality judgements and auto-
matic quality metrics. Mixed models, having the
capability to absorb random variability due to the
specific experimental set-up, provide a robust mul-
tivariate method to efficiently analyse the impor-
tance of error types.
Finally, differently from all previous works, our
analysis is run on language pairs having English
as source and languages distant from English (in
term of morphology and word-order) as target.
3 Mixed-effects Models
Mixed-effects models - or simply mixed models
- like any regression model, express the relation-
ship between a response variable and some co-
variates and/or contrast factors. They enhance
conventional models by complementing fixed ef-
fects with so-called random effects. Random ef-
fects are introduced to absorb random variability
inherent to the specific experimental setting from
which the observations are drawn. In general, ran-
dom effects correspond to covariates that are not -
or cannot be - exhaustively observed in an experi-
ment, e.g. the human annotators and the evaluated
systems. Hence, mixed models permit to elegantly
cope with experimental design aspects that hinder
the applicability of conventional regression mod-
els. These are, in particular, the use of repeated
and/or clustered observations that introduce corre-
lations in the response variable that clearly violate
the independence and homoscedasticity assump-
tions of conventional linear, ANOVA, and logis-
tic regression models. Significance testing with
mixed models is in general more powerful, i.e. less
prone to Type II Errors, and also permits to reduce
the chance of Type I Errors in within-subject de-
signs, which are prone to the ?fallacy of language-
as-a-fixed-effect? (Clark, 1973).
Random effects can be directly associated to
the regression model parameters, as random in-
tercepts and random slopes, and have the same
form of the generic error component of the model,
i.e. normally distributed with zero mean and un-
known variance. As random effects introduce hid-
den variables, mixed models are trained with Ex-
pectation Maximization, while significance testing
is performed via likelihood-ratio (LR) tests.
In this work we employ mixed linear models to
measure the influence of different MT error types,
expressed as continuous fixed effects, on quality
1645
judgements or on automatic quality metrics.
1
We illustrate mixed linear models (Baayen et
al., 2008) by referring to our analysis, which ad-
dresses the relationships between a quality metric
(y) and different types of errors (e.g. A, B, and
C)
2
observed at the sentence level. For the sake of
simplicity, we assume to have balanced repeated
observations for one single crossed effect. That is,
we have i ? {1, . . . , I} MT systems (our groups)
each of which translated the same j ? {1, . . . , J}
test sentences. Our response variable y
ij
- a nu-
meric quality score - is computed on each (sen-
tence, system) pair, and we aim to investigate its
relationship with error statistics available for each
MT output, namely A
ij
, B
ij
and C
ij
. A (possible)
linear mixed model for our study would be:
y
ij
= ?
0
+ ?
1
A
ij
+ ?
2
B
ij
+ ?
3
C
ij
+ (1)
b
0,i
+ b
1,i
A
ij
+ b
2,i
B
ij
+ b
3,i
C
i
+ 
ij
The model is split into two lines on purpose. The
first line shows the fixed effect component, that is
intercept (?
0
) and slopes (?
1
, ?
2
, ?
3
) for each error
type. The second line specifies the random struc-
ture of the model, which includes random inter-
cept and slopes for each MT system and the resid-
ual error. Borrowing the notation from (Green
et al., 2013), we conveniently rewrite (1) in the
group-wise arranged matrix notation:
y
i
= x
T
i
? + z
T
i
b
i
+ 
i
(2)
where y
i
is the J ? 1 vector of responses, x
i
is the
J?p design matrix of covariates (including the in-
tercept) with fixed coefficients ? ? R
p?1
, z is the
random structure matrix defined by J ? q covari-
ates with random coefficients b
i
? R
q?1
, and 
i
is
the vector of residuals (in our example, p = 4 and
q = 4). By packing together vectors and matrices
indexed over groups i, we can rewrite the model
in a general form (Baayen et al., 2008), which can
represent any possible crossed-effects and random
structures defined over them allowing, at the same
time, for a compact model specification:
y = X
T
? + Z
T
b+  (3)
 ? N (0, ?
2
I), b ? N (0, ?
2
?), b ? 
1
Although mixed ordinal models (Tutz and Hennevogl,
1996) are in principle more appropriate to target quality
judgements, in our preliminary investigations mixed linear
models showed a significantly higher predictive power.
2
Here, A, B and C represent three generic error classes.
Their actual number in a given experimental setting will de-
pend on the granularity of the reference error taxonomy.
where ? is the relative variance-covariance q ? q
matrix of the random effects (now q = 4I), ?
2
is the variance of the per-observation term , the
symbol ? denotes independence of random vari-
ables, andN indicates the multivariate normal dis-
tribution. While b, ?, and ? are estimated via max-
imum likelihood, the single random intercept and
slope values for each group are calculated subse-
quently. They are referred to as Best Linear Un-
biased Predictors (BLUPS) and, formally, are not
parameters of the model.
The significance of the contribution of each sin-
gle parameter (e.g. single entries of ?) to the
goodness of fit can be tested via likelihood ratio.
In this way, both the fixed and random effect struc-
ture of the model can be investigated with respect
to its actual necessity to the model.
4 Dataset
For our analysis we used a dataset that covers
three translation directions, corresponding to En-
glish to Chinese, Arabic, and Russian. An inter-
national organization provided us a set of English
sentences together with their translation produced
by two anonymous MT systems. For each evalu-
ation item (source sentence and two MT outputs)
three experts were asked to assign quality scores to
the MT outputs, and a fourth expert was asked to
annotate translation errors. The four experts, who
were all professional translators native in the ex-
amined target languages, were carefully trained to
get acquainted with the evaluation guidelines and
the annotation tool specifically developed for these
evaluation tasks (Girardi et al., 2014). The anno-
tation process was carried out in parallel by all an-
notators over one week, resulting in a final dataset
composed of 312 evaluation items for the ENZH
direction, 393 for ENAR, and 437 for ENRU.
4.1 Quality Judgements
Quality judgements were collected by asking the
three experts to rate each automatic translation
according to a 1-5 Likert scale, where 1 means
?incomprehensible translation? and 5 means ?per-
fect translation?. The distribution of the collected
annotations with respect to each quality score is
shown in Figure 1. As we can see, this distri-
bution reflects different levels of perceived qual-
ity across languages. ENZH, for instance, has the
highest number of low quality scores (1 and 2),
while ENRU has the highest number of high qual-
1646
0%	 ?
20%	 ?
40%	 ?
60%	 ?
80%	 ?
100%	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
5	 ?
4	 ?
3	 ?
2	 ?
1	 ?
Figure 1: Distribution of quality scores.
ity scores (4 and 5).
Table 1 shows the average of all the qual-
ity scores assigned by each annototator as well
as the average score obtained for each MT sys-
tem. These values demonstrate the variability
of annotators and systems. A particularly high
variability among human judges is observed for
the ENAR language direction (also reflected by
the inter-annotator agreement scores discussed be-
low), while ENZH shows the highest variability
between systems. As we will see in ?5.1, we suc-
cessfully cope with this variability by considering
systems and annotators as random effects, which
allow the regression models to abstract from these
differences.
Ann1 Ann2 Ann3 Sys1 Sys2
ENZH 2.38 2.69 2.21 2.29 2.56
ENAR 2.76 2.77 1.84 2.39 2.53
ENRU 2.82 2.72 2.96 2.87 2.79
Table 1: Average quality scores per annotator and
per system.
Inter-annotator agreement was computed using
the Fleiss? kappa coefficient (Fleiss, 1971), and re-
sulted in 22.70% for ENZH, 5.24% for ENAR, and
21.80% for ENRU. While for ENZH and ENRU
the results fall in the range of ?fair? agreement
(Landis and Koch, 1977), for ENAR only ?slight?
agreement is reached, reflecting the higher anno-
tators? variability evidenced in Table 1.
A more fine-grained agreement analysis is pre-
sented in Figure 2, where the kappa values are
given for each score class. In general we no-
tice a lower agreement on the intermediate quality
scores, while annotators tend to agree on very bad
and, even more, on good translations. In partic-
ular, we see that the agreement for ENAR is sys-
tematically lower than the values measured for the
other languages on all the score classes.
-??0.1	 ?
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
FLE
ISS
'	 ?KA
PPA
	 ?
QUALITY	 ?SCORES	 ?
ENZH	 ?
ENAR	 ?
ENRU	 ?
Figure 2: Class specific inter-annotator agreement.
4.2 Error Annotation
This evaluation task was carried out by one ex-
pert for each language direction, who was asked to
identify the type of errors present in the MT output
and to mark their position in the text. Since the fo-
cus of our work is the analysis method rather than
the definition of an ideal error taxonomy, for the
difficult language directions addressed we opted
for the following general error classes, partially
overlapping with (Vilar et al., 2006): i) reordering
errors, ii) lexicon errors (including wrong lexical
choices and extra words), iii) missing words, iv)
morphology errors.
Figure 3 shows the distribution of the errors in
terms of affected tokens (words) for each error
type. Since token counts for Chinese are not word-
based but character-based, for readability purposes
the number of errors counted for Chinese trans-
lations have been divided by 2.5. Note also that
morphological errors annotated for ENZH involve
only 13 characters and thus are not visible in the
plot. The total number of errors amounts to 16,320
characters for ENZH, 4,926 words for ENAR, and
5,965 words for ENRU.
This distribution highlights some differences
between languages directions. For example, trans-
lations into Arabic and Russian present several
morphology errors, while word reordering is the
most frequent issue for translations into Chinese.
As we will see in ?5.1, error frequency does not
give a direct indication of their impact on trasla-
tion quality judgements.
4.3 Automatic Metrics
In our investigation we consider three popular au-
tomatic metrics: sentence-level BLEU (Lin and
Och, 2004), TER (Snover et al., 2006), and GTM
(Turian et al., 2003). We compute all automatic
scores by relying on a single reference and by
1647
0	 ?
500	 ?
1000	 ?
1500	 ?
2000	 ?
2500	 ?
3000	 ?
3500	 ?
4000	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
LEX	 ?
MISS	 ?
MORPH	 ?
REO	 ?
Figure 3: Distribution of error types.
means of standard packages. In particular, auto-
matic scores on Chinese are computed at the char-
acter level. Moreover, as we use metrics as re-
sponse variables for our regression models, we
compute all metrics at the sentence level. The
overall mean scores for all systems and languages
are reported in Table 2. Differences in systems?
performance can be observed for all language
pairs; as we will observe in ?5.2 such variability
explains the effectiveness of considering the MT
systems as a random effect.
BLEU TER GTM
Sys1 Sys2 Sys1 Sys2 Sy1 Sys2
ENZH 27.95 44.11 64.52 48.13 62.15 72.30
ENAR 19.63 25.25 68.83 63.99 47.20 52.33
ENRU 27.10 31.07 60.89 54.41 53.74 56.41
Table 2: Overall automatic scores per system.
5 Experiments
To assess the impact of translation errors on MT
quality we perform two sets of experiments. The
first set (?5.1) addresses the relation between er-
rors and human quality judgements. The sec-
ond set (?5.2) focuses on the relation between er-
rors and automatic metrics. In both cases, be-
fore measuring the impact of different errors on
the response variable (respectively quality judge-
ments and metrics), we validate the effectiveness
of mixed linear models by comparing their predic-
tion capability with other methods.
In all experiments, error counts of each category
were normalized into percentages with respect to
the sentence length and mapped in a logarithmic
scale. In this way, we basically assume that the
impact of errors tends to saturate above a given
threshold, hypothesis that also results in better fits
by our models.
3
Notice that while the chosen log-
3
In other words, we assume that human sensitivity to er-
10 base is easy to interpret, linear models can im-
plicitly adjust it. Our analysis makes use of mixed
linear models incorporating, as fixed effects, the
four types of errors (lex, miss, morph and reo) and
their pairwise interactions (the product of the sin-
gle error log counts), while their random struc-
ture depends on each specific experiment. For
the experiments we rely on the R language (R
Core Team, 2013) implementation of linear mixed
model in the lme4 library (Bates et al., 2014).
We assess the quality of our mixed linear mod-
els (MLM) by comparing their prediction capabil-
ity with a sequence of simpler linear models in-
cluding only fixed effects. In particular, we built
five univariate models and two multivariate mod-
els. The univariate models use as covariates, re-
spectively, the sum of all error types (baseline),
and each of the four types of errors (lex, miss,
morph and reo). The two multivariate models in-
clude all the four error types, considering them
without interactions (FLM w/o Interact.) and with
interactions (FLM).
Prediction performance is computed in terms of
Mean Absolute Error (MAE),
4
which we estimate
by averaging over 1,000 random splits of the data
in 90% training and 10% test. In particular, for the
human quality classes we pick the integer between
1-5 that is closest to the predicted value.
5.1 Errors vs. Quality Judgements
The response variable we target in this experiment
is the quality score produced by human annotators.
Our measurements follow a typical within-subject
design in which all the 3 annotators are exposed
to the same conditions (levels of the independent
variables), corresponding in our case to perfectly
balanced observations from 2 MT systems and N
sentences. This setting results in repeated or clus-
tered observations (thus violating independence)
corresponding to groups which naturally identify
possible random effects,
5
namely the annotators
(3 levels with 2xN observations each), the systems
(2 levels and 3xN observations each), and the sen-
rors follows a log-scale law: e.g. more sensitive to variations
in the interval [1-10] that in the interval [30-40].
4
MAE is calculated as the average of the absolute errors
|f
i
? y
i
|, where f
i
is the prediction of the model and y
i
the
true value for the i
th
instance. As it is a measure of error,
lower MAE scores indicate that our predictions are closer to
the true values of each test instance.
5
In all our experiments, random effects are limited to ran-
dom shifts since preliminary experiments also including ran-
dom slopes did not provide consistent results.
1648
Model ENZH ENAR ENRU
baseline 0.58 0.73 0.67
lex 0.67 0.78 0.72
miss 0.72 0.89 0.74
morph 0.72 0.89 0.74
reo 0.70 0.82 0.76
FLM w/o Interact. 0.59 0.77 0.65
FLM 0.57 0.72 0.63
MLM 0.53 0.61 0.61
Table 3: Prediction capability of human judge-
ments (MAE).
tences (N levels with 6 observations each). In prin-
ciple, such random effects permit to remove sys-
tematic biases of individual annotators, single sys-
tems and even single sentences, which are mod-
elled as random variables sampled from distinct
populations.
Table 3 shows a comparison of the prediction
capability of the mixed model
6
with simpler ap-
proaches. While the good performance achieved
by our strong baseline cannot be outperformed
by separately counting the number of errors of a
single type, lower MAE results are obtained by
methods based on multivariate analysis. Among
them, FLM brings the first consistent improve-
ments over the baseline by considering error in-
teractions, while MLM leads to the lowest MAE
due to the addition of random effects. The impor-
tance of random effects is particularly evidenced
by ENAR (12 points below the baseline). Indeed,
as discussed in ?4.1, for this language combina-
tion human annotators show the lowest agreement
score. This variability, which hides the smaller
differences in systems? behaviour, demonstrates
the importance of accounting for the erratic fac-
tors that might influence empirical observations in
a given setting. The good performance achieved
by MLM, combined with their high descriptive
power,
7
motivates their adoption in our study.
Concerning the analysis of error impact, Ta-
ble 4 shows the statistically significant coefficients
for the full-fledged MLM models for each trans-
lation direction. By default, all reported coeffi-
cients have p-values ? 10
?4
, while those marked
with ? and ? have respectively p-values ? 10
?3
and ? 10
?2
. Slope coefficients basically show
6
Note that the mixed model used in prediction does not in-
clude the random effect on sentences since the training sam-
ples do not guarantee sufficient observations for each test sen-
tence.
7
Note that the strong baseline used for comparison is not
capable to describe the contribution of the different error
types.
Error ENZH ENAR ENRU
Intercept 4.29 3.79
?
4.21
lex -1.27 -0.96 -1.12
miss -1.76 -0.90 -1.30
morph -0.48
?
-0.83 -0.51
reo -1.01 -0.75 -0.18
lex:miss 1.00 0.39 0.68
lex:morph - 0.29 0.32
lex:reo 0.50 0.21 -
miss:morph - 0.35 -
miss:reo 0.54 0.33 -
morph:reo - 0.37 -
Table 4: Effect of translation errors on MT qual-
ity perception on all judged sentences. Reported
coefficients (?) are all statistically significant with
p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
and
?
(p ? 10
?2
).
the impact of different error types (alone and in
combination) on human quality scores. Those that
are not statistically significant are omitted as they
do not increase the fitting capability of our model.
As can be seen from the table, such impact varies
across the different language combinations. While
for ENZH and ENRU miss is the error having
the highest impact (highest decrement with respect
to the intercept), the most problematic error for
ENAR is lex. It is interesting to observe that pos-
itive values for error combinations indicate that
their combined impact is lower that the sum of the
impact of the single errors. For instance, while for
ENZH a one-step increment in lex and miss errors
would respectively cause a reduction in the human
judgement of 1.27 and 1.76, their occurrence in
the same sentence would be discounted by 1.00.
This would result in a global judgement of 2.26
(4.29 -1.27 -1.76 +1.00) instead of 1.26. While
for ENAR this phenomenon can be observed for
all error combinations, such discount effects are
not always significant for the other two language
pairs. The existence of discount effects of various
magnitude associated to the different error com-
binations is a novel finding made possible by the
adoption of mixed-effect models.
Another interesting observation is that, in con-
trast with the common belief that the most fre-
quent errors have the highest impact on human
quality judgements, our experiments do not re-
veal such strict correlation (at least for the exam-
ined language pairs). For instance, for ENZH and
ENRU the impact of miss errors is higher than the
impact of other more frequent issues.
1649
BLEU score TER GTM
Model ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
baseline 12.4 9.8 12.2 15.7 13.4 14.4 9.8 10.6 11.5
lex 12.9 10.4 13.0 16.3 13.8 14.9 9.7 10.9 12.1
miss 13.8 10.5 14.1 17.3 14.2 16.4 10.5 11.1 13.2
morph 13.9 10.3 13.6 17.5 13.8 16.3 10.5 10.9 13.1
reo 13.7 10.5 14.0 17.4 14.1 16.3 10.4 11.1 13.1
FLM w/o Interact. 12.9 9.9 12.2 16.3 13.5 14.4 9.7 10.7 11.7
FLM 12.3 9.7 12.1 15.6 13.4 14.3 9.4 10.6 11.6
MLM 10.8 9.5 12.0 14.7 13.0 14.2 8.9 10.5 11.6
Table 5: Prediction capability of BLEU score, TER and GTM (MAE).
5.2 Errors vs. Automatic Metrics
In this experiment, the response variable is an au-
tomatic metric which is computed on a sample of
MT outputs (which are again perfectly balanced
over systems and sentences) and a set of reference
translations. As no subjects are involved in the ex-
periment, random variability is assumed to come
from the involved systems, the tested sentences,
and the unknown missing link between the covari-
ates (error types) and the response variable which
is modelled by the residual noise. Notice that,
in this case, the random effect on the sentences
also incorporates in some sense the randomness
of the corresponding reference translations, which
are themselves representatives of larger samples.
The prediction capability of the mixed model,
in comparison with the simpler ones, is reported
in Table 5. Also in this case, the low MAE
achieved by the baseline is out of the reach of uni-
variate methods. Again, small improvements are
brought by FLM when considering error interac-
tions, whereas the most visible gains are achieved
by MLM due to their control of random effects.
This is more evident for some language combina-
tions and can be explained by the differences in
systems? performance, a variability factor easily
absorbed by random effects. Indeed, the largest
MAE decrements over the baseline are always ob-
served for ENZH (for which the overall mean re-
sults reported in Table 2 show the largest dif-
ferences) and the smallest decrements relate to
language/metric combinations where systems? be-
haviour is more similar (e.g. ENRU/GTM).
Concerning the analysis of error impact, Table
6 shows how different error types (alone and in
combination) influence performance results mea-
sured with automatic metrics. To ease interpre-
tation of the reported figures we also show Pear-
son and Spearman correlations of each set of coef-
ficients (excluding intercept estimates) with their
corresponding coefficients reported in Table 4. In
fact, our primary interest in this experiment is to
see which metrics show a sensitivity to specific er-
ror types similar to human perception. As we can
see, the coefficients for each metric significantly
vary depending on the language, for the simple
reason that also the distribution and co-occurrence
of errors vary significantly across the different lan-
guages and MT systems. Remarkably, for some
translation directions, some of the metrics show
a sensitivity to errors that is very similar to that
of human judges. In particular, BLEU for ENZH
and ENAR, and GTM for ENZH show a very high
correlation with the human sensitivity to transla-
tion errors, with Pearson correlation coefficient ?
0.97. For ENRU, the best Pearson correlation is
instead achieved by TER (-0.78).
Besides these general observations, a closer
look at the reported scores brings additional find-
ings. In three cases (BLEU for ENZH, GTM for
ENZH and ENAR) the analysed metrics are most
sensitive to the same error type that has the high-
est influence on human judgements (according to
Table 4, these are miss for ENZH and ENRU, lex
for ENAR). On the contrary, in one case (TER for
ENZH) the analysed metric is insensitive to the er-
ror type (miss) which has the highest impact on hu-
man quality scores. From a practical point of view,
these remarks provide useful indications about the
appropriateness of each metric to highlight the de-
ficiencies of a specific system and to measure im-
provements targeting specific issues. As a rule of
thumb, for instance, to measure improvements of
an ENZH system with respect to missing words,
it would be more advisable to use BLEU or GTM
instead of TER.
8
8
Note that this conclusion holds for our data sample, in
which different types of errors co-occur and only one refer-
ence translation is available. In such conditions, our regres-
sion model shows that TER is not influenced by miss errors in
a statistically significant way. This does not mean that TER
is insensitive to missing words when occurring in isolation,
1650
BLEU score TER GTM
Error ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
Intercept 60.55
2
38.45
?
51.73 32.41
2
52.25
?
33.4
?
83.57
?
60.11
?
75.38
lex -18.78 -9.25 -16.57 16.87 9.66 18.45 -13.63 -7.60 -16.13
miss -23.20 -10.41 -6.75 - - 8.24 -14.87 - -5.98
morph - -9.97 -12.65 - 8.90 11.41 - -6.60 -10.42
reo -13.27 -7.62 -10.57 14.44 9.81 6.39 -7.29 -5.50 -7.03
lex:miss 14.37 4.97
?
- - - - 8.24
?
- -
lex:morph - - 5.27
?
- - -5.22
?
- - 4.92
lex:reo 8.57 3.57
?
5.40
?
-7.24
?
-4.35
?
- 5.46 3.22
?
3.65
2
miss:morph - 4.44
?
- - - - - - -
miss:reo 6.74
?
- 4.30 - - -6.38
?
5.07
?
- 4.71
?
morph:reo - 3.81
?
- - -4.97
?
- - 2.57
?
-
Pearson 0.98 0.97 0.70 -0.58 -0.78 -0.78 0.98 0.78 0.74
Spearman 0.97 0.91 0.73 -0.57 -0.59 -0.80 0.97 0.59 0.76
Table 6: Effect of translation errors on BLEU score, TER and GTM on all judged sentences and correla-
tion with their corresponding effects on human quality scores (from Table 4). Reported coefficients (?)
are statistically significant with p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
?
(p ? 10
?2
) and
2
(p ? 10
?1
).
Similar considerations also apply to the analysis
of the impact of error combinations. The same dis-
count effects that we noticed when analysing the
impact of errors? co-occurrence on human percep-
tion (?5.1) are evidenced, with different degrees of
sensitivity, by the automatic metrics. While some
of them substantially reflect human response (e.g.
BLEU and GTM for ENZH), in some cases we
observe either the insensitivity to specific combi-
nations (mostly for ENAR), or a higher sensitivity
compared to the values measured for human as-
sessors (mostly for ENRU, where the impact of
miss:reo combinations is discounted - hence un-
derestimated - by all the metrics).
Despite such small differences, the coherence of
our results with previous findings (?5.1) suggests
the reliability of the applied method. Complet-
ing the picture along the side of the MT evalua-
tion triangle which connects error annotations and
automatic metrics, our findings contribute to shed
light on the existing relationships between transla-
tion errors, their interaction, and the sensitivity of
widely used automatic metrics.
6 Conclusion
We investigated the MT evaluation triangle (hav-
ing as corners automatic metrics, human quality
judgements and error annotations) along the two
less explored sides, namely: i) the relation be-
tween MT errors and human quality judgements
but that TER becomes less sensitive to such errors when they
co-occur with other types of errors. Overall, our experiments
show that when MT outputs contain more than one error type,
automatic metrics show different levels of sensitivity to each
specific error type.
and ii) the relation between MT errors and auto-
matic metrics. To this aim we employed a ro-
bust statistical analysis framework based on lin-
ear mixed-effects models (the first contribution of
the paper), which have a higher descriptive power
than simpler methods based on the raw count of
translation errors and are less artificial compared
to previous statistically-grounded approaches.
Working on three translation directions having
Chinese, Arabic and Russian as target (our second
contribution), we analysed error-annotated trans-
lations considering the impact of specific errors
(alone and in combination) and accounting for the
variability of the experimental set-up that origi-
nated our empirical observations. This led us to
interesting findings specific to each language pair
(third contribution). Concerning the relation be-
tween MT errors and quality judgements, we have
shown that: i) the frequency of errors of a given
type does not correlate with human preferences,
ii) errors having the highest impact can be pre-
cisely isolated and iii) the impact of error inter-
actions is often subject to measurable and previ-
ously unknown ?discount? effects. Concerning the
relation between MT errors and automatic met-
rics (BLEU, TER and GTM), our analysis evi-
denced significant differences in the sensitivity of
each metric to different error types. Such differ-
ences provide useful indications about the most
appropriate metric to assess system improvements
with respect to specific weaknesses. If learning
from errors is a crucial aspect of improving exper-
tise, our method and the resulting empirical find-
ings represent a significant contribution towards a
1651
more informed approach to system development,
improvement and evaluation.
Acknowledgements
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
References
Harald R. Baayen, Douglas J. Davidson, and Dou-
glas M. Bates. 2008. Mixed-effects modeling with
crossed random effects for subjects and items. Jour-
nal of memory and language, 59(4):390?412.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Douglas Bates, Martin Maechler, Ben Bolker, and
Steven Walker, 2014. lme4: Linear mixed-effects
models using Eigen and S4. R package version 1.1-
6.
Fr?ed?eric Blain, Jean Senellart, Holger Schwenk, Mirko
Plitt, and Johann Roturier. 2011. Qualitative analy-
sis of post-editing for high quality machine transla-
tion. In Asia-Pacific Association for Machine Trans-
lation (AAMT), editor, Machine Translation Summit
XIII, Xiamen (China), 19-23 sept.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Herbert H. Clark. 1973. The language-as-fixed-effect
fallacy: A critique of language statistics in psycho-
logical research. Journal of verbal learning and ver-
bal behavior, 12(4):335?359.
Mireia Farr?us, Marta R. Costa-juss`a, and Maja
Popovi?c. 2012. Study and correlation analysis of
linguistic, perceptual, and automatic machine trans-
lation evaluations. J. Am. Soc. Inf. Sci. Technol.,
63(1):174?184, January.
Mireia Farr?us Cabeceran, Marta Ruiz Costa-Juss`a,
Jos?e Bernardo Mari?no Acebal, Jos?e Adri?an
Rodr??guez Fonollosa, et al. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th
Annual Conference of the European Association for
Machine Translation (EAMT).
Mary Flanagan. 1994. Error classification for mt eval-
uation. In Technology Partnerships for Crossing the
Language Barrier: Proceedings of the First Confer-
ence of the Association for Machine Translation in
the Americas, pages 65?72.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5).
Christian Girardi, Luisa Bentivogli, Mohammad Amin
Farajian, and Marcello Federico. 2014. Mt-equal:
a toolkit for human assessment of machine trans-
lation output. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 120?
123, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.
Sharon Goldwater, Daniel Jurafsky, and Christopher D.
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181?200.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 439?448. ACM.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 503?511, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Katrin Kirchhoff, Daniel Capurro, and Anne M. Turner.
2013. A conjoint analysis framework for evaluating
user preferences in machine translation. Machine
Translation, pages 1?17.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33 (1):159?174.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. Assess-
ing Post-Editing Efficiency in a Realistic Translation
Environment. In Michel Simard Sharon O?Brien
and Lucia Specia (eds.), editors, Proceedings of MT
Summit XIV Workshop on Post-editing Technology
and Practice, pages 83?91, Nice, France.
Chin-Yew Lin and Franz Josef Och. 2004. Orange:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of Col-
ing 2004, pages 501?507, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Arle Lommel, Aljoscha Burchardt, Maja Popovi?c, Kim
Harris, Eleftherios Avramidis, and Hans Uszkoreit.
1652
2014. Using a new analytic measure for the anno-
tation and analysis of mt errors on real data. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Sharon O?Brien. 2011. Cognitive Explorations of
Translation. Bloomsbury Studies in Translation.
Bloomsbury Academic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Research Report
RC22176, IBM Research Division, Thomas J. Wat-
son Research Center.
Maja Popovi?c and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Comput. Linguist., 37(4):657?688, December.
Maja Popovic, Eleftherios Avramidis, Aljoscha Bur-
chardt, Sabine Hunsicker, Sven Schmeier, Cindy
Tscherwinka, David Vilar, and Hans Uszkoreit.
2013. Learning from human judgments of machine
translation output. In Proceedings of the MT Summit
XIV. Proceedings of MT Summit XIV.
Maja Popovi?c, Arle Lommel, Aljoscha Burchardt,
Eleftherios Avramidis, and Hans Uszkoreit. 2014.
Relations between different types of post-editing op-
erations, cognitive effort and temporal effort. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71?75, Montr?eal, Canada, June. Associ-
ation for Computational Linguistics.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Nick Ruiz and Marcello Federico. 2014. Assessing the
Impact of Speech Recognition Errors on Machine
Translation Quality. In 11th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), Vancouver, BC, Canada.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In 5th Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, Mas-
sachusetts, August.
Sara Stymne and Lars Ahrenberg. 2012. On
the practice of error analysis for machine trans-
lation evaluation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
Irina Temnikova. 2010. Cognitive evaluation approach
for a controlled language post-editing experiment.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Joseph P. Turian, I. Dan Melamed, and Luke Shen.
2003. Evaluation of machine translation and its
evaluation. In Proceedings of the MT Summit IX.
Gerhard Tutz and Wolfgang Hennevogl. 1996. Ran-
dom effects in ordinal regression models. Computa-
tional Statistics & Data Analysis, 22(5):537?557.
David Vilar, Jia Xu, Luis Fernando dHaro, and Her-
mann Ney. 2006. Error analysis of statistical ma-
chine translation output. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC?06), pages 697?702.
1653
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 25?30,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
ONTS: ?Optima? News Translation System
Marco Turchi?, Martin Atkinson?, Alastair Wilcox+, Brett Crawley,
Stefano Bucci+, Ralf Steinberger? and Erik Van der Goot?
European Commission - Joint Research Centre (JRC), IPSC - GlobeSec
Via Fermi 2749, 21020 Ispra (VA) - Italy
?[name].[surname]@jrc.ec.europa.eu
+[name].[surname]@ext.jrc.ec.europa.eu
brettcrawley@gmail.com
Abstract
We propose a real-time machine translation
system that allows users to select a news
category and to translate the related live
news articles from Arabic, Czech, Danish,
Farsi, French, German, Italian, Polish, Por-
tuguese, Spanish and Turkish into English.
The Moses-based system was optimised for
the news domain and differs from other
available systems in four ways: (1) News
items are automatically categorised on the
source side, before translation; (2) Named
entity translation is optimised by recog-
nising and extracting them on the source
side and by re-inserting their translation in
the target language, making use of a sep-
arate entity repository; (3) News titles are
translated with a separate translation sys-
tem which is optimised for the specific style
of news titles; (4) The system was opti-
mised for speed in order to cope with the
large volume of daily news articles.
1 Introduction
Being able to read news from other countries and
written in other languages allows readers to be
better informed. It allows them to detect national
news bias and thus improves transparency and
democracy. Existing online translation systems
such as Google Translate and Bing Translator1
are thus a great service, but the number of docu-
ments that can be submitted is restricted (Google
will even entirely stop their service in 2012) and
submitting documents means disclosing the users?
interests and their (possibly sensitive) data to the
service-providing company.
1http://translate.google.com/ and http:
//www.microsofttranslator.com/
For these reasons, we have developed our
in-house machine translation system ONTS. Its
translation results will be publicly accessible as
part of the Europe Media Monitor family of ap-
plications, (Steinberger et al 2009), which gather
and process about 100,000 news articles per day
in about fifty languages. ONTS is based on
the open source phrase-based statistical machine
translation toolkit Moses (Koehn et al 2007),
trained mostly on freely available parallel cor-
pora and optimised for the news domain, as stated
above. The main objective of developing our in-
house system is thus not to improve translation
quality over the existing services (this would be
beyond our possibilities), but to offer our users a
rough translation (a ?gist?) that allows them to get
an idea of the main contents of the article and to
determine whether the news item at hand is rele-
vant for their field of interest or not.
A similar news-focused translation service is
?Found in Translation? (Turchi et al 2009),
which gathers articles in 23 languages and trans-
lates them into English. ?Found in Translation? is
also based on Moses, but it categorises the news
after translation and the translation process is not
optimised for the news domain.
2 Europe Media Monitor
Europe Media Monitor (EMM)2 gathers a daily
average of 100,000 news articles in approximately
50 languages, from about 3,400 hand-selected
web news sources, from a couple of hundred spe-
cialist and government websites, as well as from
about twenty commercial news providers. It vis-
its the news web sites up to every five minutes to
2http://emm.newsbrief.eu/overview.html
25
search for the latest articles. When news sites of-
fer RSS feeds, it makes use of these, otherwise
it extracts the news text from the often complex
HTML pages. All news items are converted to
Unicode. They are processed in a pipeline struc-
ture, where each module adds additional informa-
tion. Independently of how files are written, the
system uses UTF-8-encoded RSS format.
Inside the pipeline, different algorithms are im-
plemented to produce monolingual and multilin-
gual clusters and to extract various types of in-
formation such as named entities, quotations, cat-
egories and more. ONTS uses two modules of
EMM: the named entity recognition and the cate-
gorization parts.
2.1 Named Entity Recognition and Variant
Matching.
Named Entity Recognition (NER) is per-
formed using manually constructed language-
independent rules that make use of language-
specific lists of trigger words such as titles
(president), professions or occupations (tennis
player, playboy), references to countries, regions,
ethnic or religious groups (French, Bavarian,
Berber, Muslim), age expressions (57-year-old),
verbal phrases (deceased), modifiers (former)
and more. These patterns can also occur in
combination and patterns can be nested to capture
more complex titles, (Steinberger and Pouliquen,
2007). In order to be able to cover many different
languages, no other dictionaries and no parsers or
part-of-speech taggers are used.
To identify which of the names newly found
every day are new entities and which ones are
merely variant spellings of entities already con-
tained in the database, we apply a language-
independent name similarity measure to decide
which name variants should be automatically
merged, for details see (Pouliquen and Stein-
berger, 2009). This allows us to maintain a
database containing over 1,15 million named en-
tities and 200,000 variants. The major part of
this resource can be downloaded from http:
//langtech.jrc.it/JRC-Names.html
2.2 Category Classification across
Languages.
All news items are categorized into hundreds of
categories. Category definitions are multilingual,
created by humans and they include geographic
regions such as each country of the world, organi-
zations, themes such as natural disasters or secu-
rity, and more specific classes such as earthquake,
terrorism or tuberculosis,
Articles fall into a given category if they sat-
isfy the category definition, which consists of
Boolean operators with optional vicinity opera-
tors and wild cards. Alternatively, cumulative
positive or negative weights and a threshold can
be used. Uppercase letters in the category defi-
nition only match uppercase words, while lower-
case words in the definition match both uppercase
and lowercase words. Many categories are de-
fined with input from the users themselves. This
method to categorize the articles is rather sim-
ple and user-friendly, and it lends itself to dealing
with many languages, (Steinberger et al 2009).
3 News Translation System
In this section, we describe our statistical machine
translation (SMT) service based on the open-
source toolkit Moses (Koehn et al 2007) and its
adaptation to translation of news items.
Which is the most suitable SMT system for
our requirements? The main goal of our system
is to help the user understand the content of an ar-
ticle. This means that a translated article is evalu-
ated positively even if it is not perfect in the target
language. Dealing with such a large number of
source languages and articles per day, our system
should take into account the translation speed, and
try to avoid using language-dependent tools such
as part-of-speech taggers.
Inside the Moses toolkit, three different
statistical approaches have been implemented:
phrase based statistical machine translation (PB-
SMT) (Koehn et al 2003), hierarchical phrase
based statistical machine translation (Chiang,
2007) and syntax-based statistical machine trans-
lation (Marcu et al 2006). To identify the
most suitable system for our requirements, we
run a set of experiments training the three mod-
els with Europarl V4 German-English (Koehn,
2005) and optimizing and testing on the News
corpus (Callison-Burch et al 2009). For all of
them, we use their default configurations and they
are run under the same condition on the same ma-
chine to better evaluate translation time. For the
syntax model we use linguistic information only
on the target side. According to our experiments,
in terms of performance the hierarchical model
26
performs better than PBSMT and syntax (18.31,
18.09, 17.62 Bleu points), but in terms of transla-
tion speed PBSMT is better than hierarchical and
syntax (1.02, 4.5, 49 second per sentence). Al-
though, the hierarchical model has the best Bleu
score, we prefer to use the PBSMT system in our
translation service, because it is four times faster.
Which training data can we use? It is known
in statistical machine translation that more train-
ing data implies better translation. Although, the
number of parallel corpora has been is growing
in the last years, the amounts of training data
vary from language pair to language pair. To
train our models we use the freely available cor-
pora (when possible): Europarl (Koehn, 2005),
JRC-Acquis (Steinberger et al 2006), DGT-
TM3, Opus (Tiedemann, 2009), SE-Times (Ty-
ers and Alperen, 2010), Tehran English-Persian
Parallel Corpus (Pilevar et al 2011), News
Corpus (Callison-Burch et al 2009), UN Cor-
pus (Rafalovitch and Dale, 2009), CzEng0.9 (Bo-
jar and Z?abokrtsky?, 2009), English-Persian paral-
lel corpus distributed by ELRA4 and two Arabic-
English datasets distributed by LDC5. This re-
sults in some language pairs with a large cover-
age, (more than 4 million sentences), and other
with a very small coverage, (less than 1 million).
The language models are trained using 12 model
sentences for the content model and 4.7 million
for the title model. Both sets are extracted from
English news.
For less resourced languages such as Farsi and
Turkish, we tried to extend the available corpora.
For Farsi, we applied the methodology proposed
by (Lambert et al 2011), where we used a large
language model and an English-Farsi SMT model
to produce new sentence pairs. For Turkish we
added the Movie Subtitles corpus (Tiedemann,
2009), which allowed the SMT system to in-
crease its translation capability, but included sev-
eral slang words and spoken phrases.
How to deal with Named Entities in transla-
tion? News articles are related to the most impor-
tant events. These names need to be efficiently
translated to correctly understand the content of
an article. From an SMT point of view, two main
issues are related to Named Entity translation: (1)
such a name is not in the training data or (2) part
3http://langtech.jrc.it/DGT-TM.html
4http://catalog.elra.info/
5http://www.ldc.upenn.edu/
of the name is a common word in the target lan-
guage and it is wrongly translated, e.g. the French
name ?Bruno Le Maire? which risks to be trans-
lated into English as ?Bruno Mayor?. To mitigate
both the effects we use our multilingual named
entity database. In the source language, each news
item is analysed to identify possible entities; if
an entity is recognised, its correct translation into
English is retrieved from the database, and sug-
gested to the SMT system enriching the source
sentence using the xml markup option 6 in Moses.
This approach allows us to complement the train-
ing data increasing the translation capability of
our system.
How to deal with different language styles
in the news? News title writing style contains
more gerund verbs, no or few linking verbs,
prepositions and adverbs than normal sentences,
while content sentences include more preposi-
tion, adverbs and different verbal tenses. Starting
from this assumption, we investigated if this phe-
nomenon can affect the translation performance
of our system.
We trained two SMT systems, SMTcontent
and SMTtitle, using the Europarl V4 German-
English data as training corpus, and two dif-
ferent development sets: one made of content
sentences, News Commentaries (Callison-Burch
et al 2009), and the other made of news ti-
tles in the source language which were trans-
lated into English using a commercial transla-
tion system. With the same strategy we gener-
ated also a Title test set. The SMTtitle used a
language model created using only English news
titles. The News and Title test sets were trans-
lated by both the systems. Although the perfor-
mance obtained translating the News and Title
corpora are not comparable, we were interested
in analysing how the same test set is translated
by the two systems. We noticed that translat-
ing a test set with a system that was optimized
with the same type of data resulted in almost 2
Blue score improvements: Title-TestSet: 0.3706
(SMTtitle), 0.3511 (SMTcontent); News-TestSet:
0.1768 (SMTtitle), 0.1945 (SMTcontent). This
behaviour was present also in different language
pairs. According to these results we decided
to use two different translation systems for each
language pair, one optimized using title data
6http://www.statmt.org/moses/?n=Moses.
AdvancedFeatures#ntoc4
27
and the other using normal content sentences.
Even though this implementation choice requires
more computational power to run in memory two
Moses servers, it allows us to mitigate the work-
load of each single instance reducing translation
time of each single article and to improve transla-
tion quality.
3.1 Translation Quality
To evaluate the translation performance of ONTS,
we run a set of experiments where we translate a
test set for each language pair using our system
and Google Translate. Lack of human translated
parallel titles obliges us to test only the content
based model. For German, Spanish and Czech we
use the news test sets proposed in (Callison-Burch
et al 2010), for French and Italian the news test
sets presented in (Callison-Burch et al 2008),
for Arabic, Farsi and Turkish, sets of 2,000 news
sentences extracted from the Arabic-English and
English-Persian datasets and the SE-Times cor-
pus. For the other languages we use 2,000 sen-
tences which are not news but a mixture of JRC-
Acquis, Europarl and DGT-TM data. It is not
guarantee that our test sets are not part of the train-
ing data of Google Translate.
Each test set is translated by Google Translate
- Translator Toolkit, and by our system. Bleu
score is used to evaluate the performance of both
systems. Results, see Table 1, show that Google
Translate produces better translation for those lan-
guages for which large amounts of data are avail-
able such as French, German, Italian and Spanish.
Surprisingly, for Danish, Portuguese and Polish,
ONTS has better performance, this depends on
the choice of the test sets which are not made of
news data but of data that is fairly homogeneous
in terms of style and genre with the training sets.
The impact of the named entity module is ev-
ident for Arabic and Farsi, where each English
suggested entity results in a larger coverage of
the source language and better translations. For
highly inflected and agglutinative languages such
as Turkish, the output proposed by ONTS is poor.
We are working on gathering more training data
coming from the news domain and on the pos-
sibility of applying a linguistic pre-processing of
the documents.
Source L. ONTS Google T.
Arabic 0.318 0.255
Czech 0.218 0.226
Danish 0.324 0.296
Farsi 0.245 0.197
French 0.26 0.286
German 0.205 0.25
Italian 0.234 0.31
Polish 0.568 0.511
Portuguese 0.579 0.424
Spanish 0.283 0.334
Turkish 0.238 0.395
Table 1: Automatic evaluation.
4 Technical Implementation
The translation service is made of two compo-
nents: the connection module and the Moses
server. The connection module is a servlet im-
plemented in Java. It receives the RSS files,
isolates each single news article, identifies each
source language and pre-processes it. Each news
item is split into sentences, each sentence is to-
kenized, lowercased, passed through a statisti-
cal compound word splitter, (Koehn and Knight,
2003), and the named entity annotator module.
For language modelling we use the KenLM im-
plementation, (Heafield, 2011).
According to the language, the correct Moses
servers, title and content, are fed in a multi-
thread manner. We use the multi-thread version
of Moses (Haddow, 2010). When all the sentences
of each article are translated, the inverse process
is run: they are detokenized, recased, and untrans-
lated/unknown words are listed. The translated ti-
tle and content of each article are uploaded into
the RSS file and it is passed to the next modules.
The full system including the translation mod-
ules is running in a 2xQuad-Core with In-
tel Hyper-threading Technology processors with
48GB of memory. It is our intention to locate
the Moses servers on different machines. This is
possible thanks to the high modularity and cus-
tomization of the connection module. At the mo-
ment, the translation models are available for the
following source languages: Arabic, Czech, Dan-
ish, Farsi, French, German, Italian, Polish, Por-
tuguese, Spanish and Turkish.
28
Figure 1: Demo Web site.
4.1 Demo
Our translation service is currently presented on
a demo web site, see Figure 1, which is available
at http://optima.jrc.it/Translate/.
News articles can be retrieved selecting one of the
topics and the language. All the topics are as-
signed to each article using the methodology de-
scribed in 2.2. These articles are shown in the left
column of the interface. When the button ?Trans-
late? is pressed, the translation process starts and
the translated articles appear in the right column
of the page.
The translation system can be customized from
the interface enabling or disabling the named
entity, compound, recaser, detokenizer and un-
known word modules. Each translated article is
enriched showing the translation time in millisec-
onds per character and, if enabled, the list of un-
known words. The interface is linked to the con-
nection module and data is transferred using RSS
structure.
5 Discussion
In this paper we present the Optima News Trans-
lation System and how it is connected to Eu-
rope Media Monitor application. Different strate-
gies are applied to increase the translation perfor-
mance taking advantage of the document struc-
ture and other resources available in our research
group. We believe that the experiments described
in this work can result very useful for the develop-
ment of other similar systems. Translations pro-
duced by our system will soon be available as part
of the main EMM applications.
The performance of our system is encouraging,
but not as good as the performance of web ser-
vices such as Google Translate, mostly because
we use less training data and we have reduced
computational power. On the other hand, our in-
house system can be fed with a large number of
articles per day and sensitive data without includ-
ing third parties in the translation process. Per-
formance and translation time vary according to
the number and complexity of sentences and lan-
guage pairs.
The domain of news articles dynamically
changes according to the main events in the world,
while existing parallel data is static and usually
associated to governmental domains. It is our in-
tention to investigate how to adapt our translation
system updating the language model with the En-
glish articles of the day.
Acknowledgments
The authors thank the JRC?s OPTIMA team for
its support during the development of ONTS.
References
O. Bojar and Z. Z?abokrtsky?. 2009. CzEng0.9: Large
Parallel Treebank with Rich Annotation. Prague
Bulletin of Mathematical Linguistics, 92.
C. Callison-Burch and C. Fordyce and P. Koehn and
C. Monz and J. Schroeder. 2008. Further Meta-
Evaluation of Machine Translation. Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, pages 70?106. Columbus, US.
C. Callison-Burch, and P. Koehn and C. Monz and J.
Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 1?28. Athens, Greece.
C. Callison-Burch, and P. Koehn and C. Monz and K.
Peterson and M. Przybocki and O. Zaidan. 2009.
Findings of the 2010 Joint Workshop on Statisti-
cal Machine Translation and Metrics for Machine
Translation. Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 17?53. Uppsala, Sweden.
D. Chiang. 2005. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2): pages 201?
228. MIT Press.
B. Haddow. 2010. Adding multi-threaded decoding to
moses. The Prague Bulletin of Mathematical Lin-
guistics, 93(1): pages 57?66. Versita.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. Proceedings of the Sixth
Workshop on Statistical Machine Translation, Ed-
inburgh, UK.
29
P. Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of
the Machine Translation Summit X, pages 79-86.
Phuket, Thailand.
P. Koehn and F. J. Och and D. Marcu. 2003. Statistical
phrase-based translation. Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 48?54. Edmon-
ton, Canada.
P. Koehn and K. Knight. 2003. Empirical methods
for compound splitting. Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics, pages 187?193. Bu-
dapest, Hungary.
P. Koehn and H. Hoang and A. Birch and C. Callison-
Burch and M. Federico and N. Bertoldi and B.
Cowan and W. Shen and C. Moran and R. Zens
and C. Dyer and O. Bojar and A. Constantin and E.
Herbst 2007. Moses: Open source toolkit for sta-
tistical machine translation. Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, demonstration session, pages 177?180.
Columbus, Oh, USA.
P. Lambert and H. Schwenk and C. Servan and S.
Abdul-Rauf. 2011. SPMT: Investigations on Trans-
lation Model Adaptation Using Monolingual Data.
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 284?293. Edinburgh,
Scotland.
D. Marcu and W. Wang and A. Echihabi and K.
Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language phrases.
Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
48?54. Edmonton, Canada.
M. Pilevar and H. Faili and A. Pilevar. 2011. TEP:
Tehran English-Persian Parallel Corpus. Compu-
tational Linguistics and Intelligent Text Processing,
pages 68?79. Springer.
B. Pouliquen and R. Steinberger. 2009. Auto-
matic construction of multilingual name dictionar-
ies. Learning Machine Translation, pages 59?78.
MIT Press - Advances in Neural Information Pro-
cessing Systems Series (NIPS).
A. Rafalovitch and R. Dale. 2009. United nations
general assembly resolutions: A six-language par-
allel corpus. Proceedings of the MT Summit XIII,
pages 292?299. Ottawa, Canada.
R. Steinberger and B. Pouliquen. 2007. Cross-lingual
named entity recognition. Lingvistic? Investiga-
tiones, 30(1) pages 135?162. John Benjamins Pub-
lishing Company.
R. Steinberger and B. Pouliquen and A. Widiger and
C. Ignat and T. Erjavec and D. Tufis? and D. Varga.
2006. The JRC-Acquis: A multilingual aligned par-
allel corpus with 20+ languages. Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 2142?2147. Genova,
Italy.
R. Steinberger and B. Pouliquen and E. van der Goot.
2009. An Introduction to the Europe Media Monitor
Family of Applications. Proceedings of the Infor-
mation Access in a Multilingual World-Proceedings
of the SIGIR 2009 Workshop, pages 1?8. Boston,
USA.
J. Tiedemann. 2009. News from OPUS-A Collection
of Multilingual Parallel Corpora with Tools and
Interfaces. Recent advances in natural language
processing V: selected papers from RANLP 2007,
pages 309:237.
M. Turchi and I. Flaounas and O. Ali and T. DeBie
and T. Snowsill and N. Cristianini. 2009. Found in
translation. Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discov-
ery in Databases, pages 746?749. Bled, Slovenia.
F. Tyers and M.S. Alperen. 2010. South-East Euro-
pean Times: A parallel corpus of Balkan languages.
Proceedings of the LREC workshop on Exploita-
tion of multilingual resources and tools for Central
and (South) Eastern European Languages, Valletta,
Malta.
30
Proceedings of the ACL 2010 Conference Short Papers, pages 382?386,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Wrapping up a Summary:
from Representation to Generation
Josef Steinberger and Marco Turchi and
Mijail Kabadjov and Ralf Steinberger
EC Joint Research Centre
21027, Ispra (VA), Italy
{Josef.Steinberger, Marco.Turchi,
Mijail.Kabadjov, Ralf.Steinberger}
@jrc.ec.europa.eu
Nello Cristianini
University of Bristol,
Bristol, BS8 1UB, UK
nello@support-vector.net
Abstract
The main focus of this work is to investi-
gate robust ways for generating summaries
from summary representations without re-
curring to simple sentence extraction and
aiming at more human-like summaries.
This is motivated by empirical evidence
from TAC 2009 data showing that human
summaries contain on average more and
shorter sentences than the system sum-
maries. We report encouraging prelimi-
nary results comparable to those attained
by participating systems at TAC 2009.
1 Introduction
In this paper we adopt the general framework
for summarization put forward by Spa?rck-Jones
(1999) ? which views summarization as a three-
fold process: interpretation, transformation and
generation ? and attempt to provide a clean in-
stantiation for each processing phase, with a par-
ticular emphasis on the last, summary-generation
phase often omitted or over-simplified in the main-
stream work on summarization.
The advantages of looking at the summarization
problem in terms of distinct processing phases are
numerous. It not only serves as a common ground
for comparing different systems and understand-
ing better the underlying logic and assumptions,
but it also provides a neat framework for devel-
oping systems based on clean and extendable de-
signs. For instance, Gong and Liu (2002) pro-
posed a method based on Latent Semantic Anal-
ysis (LSA) and later J. Steinberger et al (2007)
showed that solely by enhancing the first source
interpretation phase, one is already able to pro-
duce better summaries.
There has been limited work on the last sum-
mary generation phase due to the fact that it is
unarguably a very challenging problem. The vast
amount of approaches assume simple sentence se-
lection, a type of extractive summarization, where
often the summary representation and the end
summary are, indeed, conflated.
The main focus of this work is, thus, to in-
vestigate robust ways for generating summaries
from summary representations without recurring
to simple sentence extraction and aiming at more
human-like summaries. This decision is also mo-
tivated by empirical evidence from TAC 2009 data
(see table 1) showing that human summaries con-
tain on average more and shorter sentences than
the system summaries. The intuition behind this is
that, by containing more sentences, a summary is
able to capture more of the important content from
the source.
Our initial experimental results show that our
approach is feasible, since it produces summaries,
which when evaluated against the TAC 2009 data1
yield ROUGE scores (Lin and Hovy, 2003) com-
parable to the participating systems in the Sum-
marization task at TAC 2009. Taking into account
that our approach is completely unsupervised and
language-independent, we find our preliminary re-
sults encouraging.
The remainder of the paper is organised as fol-
lows: in the next section we briefly survey the
related work, in ?3 we describe our approach to
summarization, in ?4 we explain how we tackle
the generation step, in ?5 we present and discuss
our experimental results and towards the end we
conclude and give pointers to future work.
2 Related Work
There is a large body of literature on summariza-
tion (Hovy, 2005; Erkan and Radev, 2004; Kupiec
et al, 1995). The most closely related work to the
approach presented hereby is work on summariza-
tion attempting to go beyond simple sentence ex-
1http://www.nist.gov/tac/
382
traction and to a lesser degree work on sentence
compression. We survey below work along these
lines.
Although our approach is related to sentence
compression (Knight and Marcu, 2002; Clarke
and Lapata, 2008), it is subtly different. Firstly, we
reduce the number of terms to be used in the sum-
mary at a global level, not at a local per-sentence
level. Secondly, we directly exploit the resulting
structures from the SVD making the last genera-
tion step fully aware of previous processing stages,
as opposed to tackling the problem of sentence
compression in isolation.
A similar approach to our sentence reconstruc-
tion method has been developed by Quirk et al
(2004) for paraphrase generation. In their work,
training and test sets contain sentence pairs that
are composed of two different proper English sen-
tences and a paraphrase of a source sentence is
generated by finding the optimal path through a
paraphrases lattice.
Finally, it is worth mentioning that we are aware
of the ?capsule overview? summaries proposed by
Boguraev and Kennedy (1997) which is similar to
our TSR (see below), however, as opposed to their
emphasis on a suitable browsing interface rather
than producing a readable summary, we precisely
attempt the latter.
3 Three-fold Summarization:
Interpretation, Transformation and
Generation
We chose the LSA paradigm for summarization,
since it provides a clear and direct instantiation of
Spa?rck-Jones? three-stage framework.
In LSA-based summarization the interpreta-
tion phase takes the form of building a term-by-
sentence matrix A = [A1, A2, . . . , An], where
each column Aj = [a1j , a2j , . . . , anj ]T represents
the weighted term-frequency vector of sentence j
in a given set of documents. We adopt the same
weighting scheme as the one described in (Stein-
berger et al, 2007), as well as their more general
definition of term entailing not only unigrams and
bigrams, but also named entities.
The transformation phase is done by applying
singular value decomposition (SVD) to the initial
term-by-sentence matrix defined as A = U?V T .
The generation phase is where our main contri-
bution comes in. At this point we depart from stan-
dard LSA-based approaches and aim at produc-
ing a succinct summary representation comprised
only of salient terms ? Term Summary Represen-
tation (TSR). Then this TSR is passed on to an-
other module which attempts to produce complete
sentences. The module for sentence reconstruc-
tion is described in detail in section 4, in what fol-
lows we explain the method for producing a TSR.
3.1 Term Summary Representation
To explain how a term summary representation
(TSR) is produced, we first need to define two con-
cepts: salience score of a given term and salience
threshold. Salience score for each term in matrix
A is given by the magnitude of the corresponding
vector in the matrix resulting from the dot product
of the matrix of left singular vectors with the diag-
onal matrix of singular values. More formally, let
T = U ? ? and then for each term i, the salience
score is given by |~Ti|. Salience threshold is equal
to the salience score of the top kth term, when all
terms are sorted in descending order on the basis
of their salience scores and a cutoff is defined as a
percentage (e.g., top 15%). In other words, if the
total number of terms is n, then 100?k/n must be
equal to the percentage cutoff specified.
The generation of a TSR is performed in two
steps. First, an initial pool of sentences is selected
by using the same technique as in (Steinberger and
Jez?ek, 2009) which exploits the dot product of the
diagonal matrix of singular values with the right
singular vectors: ? ? V T .2 This initial pool of sen-
tences is the output of standard LSA approaches.
Second, the terms from the source matrix A are
identified in the initial pool of sentences and those
terms whose salience score is above the salience
threshold are copied across to the TSR. Thus, the
TSR is formed by the most (globally) salient terms
from each one of the sentences. For example:
? Extracted Sentence: ?Irish Prime Minister Bertie
Ahern admitted on Tuesday that he had held a series of
private one-on-one meetings on the Northern Ireland
peace process with Sinn Fein leader Gerry Adams, but
denied they had been secret in any way.?
? TSR Sentence at 10%: ?Irish Prime Minister
Bertie Ahern Tuesday had held one-on-one meetings
Northern Ireland peace process Sinn Fein leader Gerry
Adams?3
2Due to space constraints, full details on that step are
omitted here, see (Steinberger and Jez?ek, 2009).
3The TSR sentence is stemmed just before feeding it to
the reconstruction module discussed in the next section.
383
Average Human System At 100% At 15% At 10% At 5% At 1%
number of: Summaries Summaries
Sentences/summary 6.17 3.82 3.8 3.95 4.39 5.18 12.58
Words/sentence 15.96 25.01 26.24 25.1 22.61 19.08 7.55
Words/summary 98.46 95.59 99.59 99.25 99.18 98.86 94.96
Table 1: Summary statistics on TAC?09 data (initial summaries).
Metric LSAextract At 100% At 15% At 10% At 5% At 1%
ROUGE-1 0.371 0.361 0.362 0.365 0.372 0.298
ROUGE-2 0.096 0.08 0.081 0.083 0.083 0.083
ROUGE-SU4 0.131 0.125 0.126 0.128 0.131 0.104
Table 2: Summarization results on TAC?09 data (initial summaries).
4 Noisy-channel model for sentence
reconstruction
This section describes a probabilistic approach to
the reconstruction problem. We adopt the noisy-
channel framework that has been widely used in a
number of other NLP applications. Our interpre-
tation of the noisy channel consists of looking at a
stemmed string without stopwords and imagining
that it was originally a long string and that some-
one removed or stemmed some text from it. In our
framework, reconstruction consists of identifying
the original long string.
To model our interpretation of the noisy chan-
nel, we make use of one of the most popular
classes of SMT systems: the Phrase Based Model
(PBM) (Zens et al, 2002; Och and Ney, 2001;
Koehn et al, 2003). It is an extension of the noisy-
channel model and was introduced by Brown et al
(1994), using phrases rather than words. In PBM,
a source sentence f is segmented into a sequence
of I phrases f I = [f1, f2, . . . fI ] and the same is
done for the target sentence e, where the notion of
phrase is not related to any grammatical assump-
tion; a phrase is an n-gram. The best translation
ebest of f is obtained by:
ebest = argmaxe p(e|f) = argmaxe
I?
i=1
?(fi|ei)
??
d(ai ? bi?1)
?d
|e|?
i=1
pLM (ei|e1 . . . ei?1)
?LM
where ?(fi|ei) is the probability of translating
a phrase ei into a phrase fi. d(ai ? bi?1) is
the distance-based reordering model that drives
the system to penalize substantial reorderings of
words during translation, while still allowing some
flexibility. In the reordering model, ai denotes the
start position of the source phrase that was trans-
lated into the ith target phrase, and bi?1 denotes
the end position of the source phrase translated
into the (i?1th) target phrase. pLM (ei|e1 . . . ei?1)
is the language model probability that is based on
the Markov chain assumption. It assigns a higher
probability to fluent/grammatical sentences. ??,
?LM and ?d are used to give a different weight to
each element (for more details see (Koehn et al,
2003)).
In our reconstruction problem, the difference
between the source and target sentences is not in
terms of languages, but in terms of forms. In fact,
our source sentence f is a stemmed sentence with-
out stopwords, while the target sentence e is a
complete English sentence. ?Translate? means to
reconstruct the most probable sentence e given f
inserting new words and reproducing the inflected
surface forms of the source words.
4.1 Training of the model
In Statistical Machine Translation, a PBM system
is trained using parallel sentences, where each sen-
tence in a language is paired with another sentence
in a different language and one is the translation of
the other.
In the reconstruction problem, we use a set, S1
of 2,487,414 English sentences extracted from the
news. This set is duplicated, S2, and for each sen-
tence in S2, stopwords are removed and the re-
maining words are stemmed using Porter?s stem-
mer (Porter, 1980). Our stopword list contains 488
words. Verbs are not included in this list, because
they are relevant for the reconstruction task. To
optimize the lambda parameters, we select 2,000
pairs as development set.
384
An example of training sentence pair is:
? Source Sentence: ?royal mail ha doubl profit 321
million huge fall number letter post?
? Target Sentence: ?royal mail has doubled its prof-
its to 321 million despite a huge fall in the number of
letters being posted?
In this work we use Moses (Koehn et al, 2007),
a complete phrase-based translation toolkit for
academic purposes. It provides all the state-of-the-
art components needed to create a phrase-based
machine translation system. It contains different
modules to preprocess data, train the Language
Models and the Translation Models.
5 Experimental Results
For our experiments we made use of the TAC
2009 data which conveniently contains human-
produced summaries against which we could eval-
uate the output of our system (NIST, 2009).
To begin our inquiry we carried out a phase
of exploratory data analysis, in which we mea-
sured the average number of sentences per sum-
mary, words per sentence and words per summary
in human vs. system summaries in the TAC 2009
data. Additionally, we also measured these statis-
tics of summaries produced by our system at five
different percentage cutoffs: 100%, 15%, 10%,
5% and 1%. 4 The results from this exploration
are summarised in table 1. The most notable thing
is that human summaries contain on average more
and shorter sentences than the system summaries
(see 2nd and 3rd column from left to right). Sec-
ondly, we note that as the percentage cutoff de-
creases (from 4th column rightwards) the charac-
teristics of the summaries produced by our system
are increasingly more similar to those of the hu-
man summaries. In other words, within the 100-
word window imposed by the TAC guidelines, our
system is able to fit more (and hence shorter) sen-
tences as we decrease the percentage cutoff.
Summarization performance results are shown
in table 2. We used the standard ROUGE evalu-
ation (Lin and Hovy, 2003) which has been also
used for TAC. We include the usual ROUGE met-
rics: R1 is the maximum number of co-occurring
unigrams, R2 is the maximum number of co-
occurring bigrams and RSU4 is the skip bigram
measure with the addition of unigrams as counting
4Recall from section ?3 that the salience threshold is a
function of the percentage cutoff.
unit. The last five columns of table 2 (from left to
right) correspond to summaries produced by our
system at various percentage cutoffs. The 2nd col-
umn, LSAextract, corresponds to the performance
of our system at producing summaries by sentence
extraction only.5
In the light of the above, the decrease in per-
formance from column LSAextract to column ?At
100%? can be regarded as reconstruction error.6
Then, as we decrease the percentage cutoff (from
4th column rightwards) we are increasingly cover-
ing more of the content comprised by the human
summaries (as far as the ROUGE metrics are able
to gauge this, of course). In other words, the im-
provement of content coverage makes up for the
reconstruction error, and at 5% cutoff we already
obtain ROUGE scores comparable to LSAextract.
This suggests that if we improve the quality of our
sentence reconstruction we would potentially end
up with a better performing system than a typical
LSA system based on sentence selection. Hence,
we find these results very encouraging.
Finally, we admittedly note that by applying a
percentage cutoff on the initial term set and further
performing the sentence reconstruction we gain in
content coverage, to a certain extent, on the ex-
pense of sentence readability.
6 Conclusion
In this paper we proposed a novel approach to
summary generation from summary representa-
tion based on the LSA summarization framework
and on a machine-translation-inspired technique
for sentence reconstruction.
Our preliminary results show that our approach
is feasible, since it produces summaries which re-
semble better human summaries in terms of the av-
erage number of sentences per summary and yield
ROUGE scores comparable to the participating
systems in the Summarization task at TAC 2009.
Bearing in mind that our approach is completely
unsupervised and language-independent, we find
our results promising.
In future work we plan on working towards im-
proving the quality of our sentence reconstruction
step in order to produce better and more readable
sentences.
5These are, effectively, what we called initial pool of sen-
tences in section 3, before the TSR generation.
6The only difference between the two types of summaries
is the reconstruction step, since we are including 100% of the
terms.
385
References
B. Boguraev and C. Kennedy. 1997. Salience-
based content characterisation of text documents. In
I. Mani, editor, Proceedings of the Workshop on In-
telligent and Scalable Text Summarization at the An-
nual Joint Meeting of the ACL/EACL, Madrid.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1994. The mathematic of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 31:273?318.
G. Erkan and D. Radev. 2004. LexRank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analy-
sis. In Proceedings of ACM SIGIR, New Orleans,
US.
E. Hovy. 2005. Automated text summarization. In
Ruslan Mitkov, editor, The Oxford Handbook of
Computational Linguistics, pages 583?598. Oxford
University Press, Oxford, UK.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
?03, pages 48?54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of ACL ?07, demonstration session.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proceedings of the ACM
SIGIR, pages 68?73, Seattle, Washington.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, Edmonton, Canada.
NIST, editor. 2009. Proceeding of the Text Analysis
Conference, Gaithersburg, MD, November.
F. Och and H. Ney. 2001. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL ?02, pages
295?302, Morristown, NJ, USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation.
In Proceedings of EMNLP, volume 149. Barcelona,
Spain.
K. Spa?rck-Jones. 1999. Automatic summarising: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization.
MIT Press.
J. Steinberger and K. Jez?ek. 2009. Update summariza-
tion based on novel topic distribution. In Proceed-
ings of the 9th ACM DocEng, Munich, Germany.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jez?ek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management,
43(6):1663?1680. Special Issue on Text Summari-
sation (Donna Harman, ed.).
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of KI
?02, pages 18?32, London, UK. Springer-Verlag.
386
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 771?776,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploiting Qualitative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks
Jose? G.C. de Souza
FBK-irst,
University of Trento
Trento, Italy
desouza@fbk.eu
Miquel Espla`-Gomis
Universitat d?Alacant
Alacant, Spain
mespla@dlsi.ua.es
Marco Turchi
FBK-irst
Trento, Italy
turchi@fbk.eu
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Abstract
The use of automatic word alignment to
capture sentence-level semantic relations
is common to a number of cross-lingual
NLP applications. Despite its proved
usefulness, however, word alignment in-
formation is typically considered from a
quantitative point of view (e.g. the number
of alignments), disregarding qualitative
aspects (the importance of aligned terms).
In this paper we demonstrate that integrat-
ing qualitative information can bring sig-
nificant performance improvements with
negligible impact on system complexity.
Focusing on the cross-lingual textual en-
tailment task, we contribute with a novel
method that: i) significantly outperforms
the state of the art, and ii) is portable, with
limited loss in performance, to language
pairs where training data are not available.
1 Introduction
Meaning representation, comparison and projec-
tion across sentences are major challenges for a
variety of cross-lingual applications. So far, de-
spite the relevance of the problem, research on
multilingual applications has either circumvented
the issue, or proposed partial solutions.
When possible, the typical approach builds on
the reduction to a monolingual task, burdening the
process with dependencies from machine transla-
tion (MT) components. For instance, in cross-
lingual question answering and cross-lingual tex-
tual entailment (CLTE), intermediate MT steps
are respectively performed to ease answer re-
trieval/presentation (Parton, 2012; Tanev et al,
2006) and semantic inference (Mehdad et al,
2010). Direct solutions that avoid such pivot-
ing strategies typically exploit similarity measures
that rely on bag-of-words representations. As an
example, most supervised approaches to MT qual-
ity estimation (Blatz et al, 2003; Callison-Burch
et al, 2012) and CLTE (Wa?schle and Fendrich,
2012) include features that consider the amount of
equivalent terms that are found in the input sen-
tence pairs. Such simplification, however, disre-
gards the fact that semantic equivalence is not only
proportional to the number of equivalent terms,
but also to their importance. In other words, in-
stead of checking what of a given sentence can be
found in the other, current approaches limit the
analysis to the amount of lexical elements they
share, under the rough assumption that the more
the better.
In this paper we argue that:
(1) Considering qualitative aspects of word align-
ments to identify sentence-level semantic relations
can bring significant performance improvements
in cross-lingual NLP tasks.
(2) Shallow linguistic processing techniques (of-
ten a constraint in real cross-lingual scenarios due
to limited resources availability) can be leveraged
to set up portable solutions that still outperform
current bag-of-words methods.
To support our claims we experiment with the
CLTE task, which allows us to perform exhaus-
tive comparative experiments due to the availabil-
ity of comparable benchmarks for different lan-
guage pairs. In the remainder of the paper, we:
(1) Prove the effectiveness of our method over
datasets for four language combinations;
(2) Assess the portability of our models across lan-
guages in different testing conditions.
2 Objectives and Method
We propose a supervised learning approach for
identifying and classifying semantic relations be-
tween two sentences T1 and T2 written in different
languages. Beyond semantic equivalence, which
is relevant to applications such as MT quality es-
771
(a) (c)(b)
Word alignment 
model for L1-L2
Parallel data 
for L1-L2
Unlabeled 
CLTE data 
for L1-L2
Word alignment 
algorithm
CLTE 
annotation 
Learning 
algorithm
CLTE model 
for L1-L2
Labeled 
CLTE data 
for L1-L2
Word alignment 
model for L3-L4
Parallel data 
for L3-L4
Unlabeled 
CLTE data 
for L3-L4
Word alignment 
algorithm
CLTE 
annotation 
CLTE model 
for L1-L2
Word alignment 
model for L3-L4
Parallel data 
for L3-L4
Unlabeled 
CLTE data 
for L3-L4
Word alignment 
algorithm
CLTE 
annotation 
CLTE model 
for L1-L2
CLTE model 
for L5-L6
CLTE model 
for L7-L8
Combination
Figure 1: System architecture in different training/evaluation conditions. (a): parallel data and CLTE
labeled data are available for language pair L1-L2. (b): the L1-L2 CLTE model is used to cope with the
unavailability of labeled data for L3-L4. (c): the same problem is tackled by combining multiple models.
timation (Mehdad et al, 2012b),1 we aim to cap-
ture a richer set of relations potentially relevant to
other tasks. For instance, recognizing unrelated-
ness, forward and backward entailment relations,
represents a core problem in cross-lingual docu-
ment summarization (Lenci et al, 2002) and con-
tent synchronization (Monz et al, 2011; Mehdad
et al, 2012a). CLTE, as proposed within the Se-
mEval evaluation exercises (Negri et al, 2012;
Negri et al, 2013), represents an ideal framework
to evaluate such capabilities. Within this frame-
work, our goal is to automatically identify the fol-
lowing entailment relations between T1 and T2:
forward (T1 ? T2), backward (T1 ? T2), bidi-
rectional (T1 ? T2) and no entailment.
Our approach (see Figure 1) involves two core
components: i) a word alignment model, and ii) a
CLTE classifier. The former is trained on a par-
allel corpus, and associates equivalent terms in T1
and T2. The information about word alignments
is used to extract quantitative (amount and dis-
tribution of the alignments) and qualitative fea-
tures (importance of the aligned terms) to train the
CLTE classifier. Although in principle both com-
ponents need training data (respectively a paral-
lel corpus and labeled CLTE data), our goal is to
develop a method that is also portable across lan-
guages. To this aim, while the parallel corpus is
necessary to train the word aligner for any lan-
guage pair we want to deal with, the CLTE clas-
1A translation has to be semantically equivalent to the
source sentence.
sifier can be designed to learn from features that
capture language independent knowledge.2 This
allows us to experiment in different testing con-
ditions, namely: i) when CLTE training data are
available for a given language pair (Figure 1a),
and ii) when CLTE training data are missing, and
a model trained on other language pairs has to be
reused (Figure 1b-c).
Features. Considering word alignment informa-
tion, we extract three different groups of features:
AL, POS, and IDF.
The AL group provides quantitative informa-
tion about the aligned/unaligned words in each
sentence T? of the pair. These features are:
1. proportion of aligned words in T?. We use
this indicator as our baseline (B henceforth);
2. number of sequences of unaligned words,
normalized by the length of T?;
3. length of the longest a) sequence of aligned
words, and b) sequence of unaligned words,
both normalized by the length of T?;
4. average length of a) the aligned word se-
quences, and b) the unaligned word se-
quences;
5. position of a) the first unaligned word, and
b) the last unaligned word, both normalized
by the lenght of T?;
6. proportion of word n-grams in T? contain-
ing only aligned words (the feature was com-
2For instance, the fact that aligning all nouns and the most
relevant terms in T1 and T2 is a good indicator of semantic
equivalence.
772
puted separately for values of n = 1 . . . 5).
The POS group considers the part of speech
(PoS) of the words in T? as a source of qualitative
information about their importance. To compute
these features we use the TreeTagger (Schmid,
1995), manually mapping the fine-grained set of
assigned PoS labels into a more general set of tags
(P ) based on the universal PoS tag set by Petrov
et al (2012). POS features differentiate between
aligned words (words in T1 that are aligned to one
or more words in T2) and alignments (the edges
connecting words in T1 and T2). Features consid-
ering the aligned words in T? are:
7. for each PoS tag p ? P , proportion of aligned
words in T? tagged with p;
8. proportion of words in T1 aligned with words
with the same PoS tag in T2 (and vice-versa);
9. for each PoS tag p ? P , proportion of words
in T1 tagged as p which are aligned to words
with the same tag in T2 (and vice-versa).
Features considering the alignments are:
10. proportion of alignments connecting words
with the same PoS tag p;
11. for each PoS tag p ? P , proportion of align-
ments connecting two words tagged as p.
IDF, the last feature, uses the inverse docu-
ment frequency (Salton and Buckley, 1988) as an-
other source of qualitative information under the
assumption that rare words (and, therefore, with
higher IDF) are more informative:
12. summation of all the IDF scores of the
aligned words in T? over the summation of
the IDF scores of all words in T?.
3 Experiments
Our experiments cover two different scenarios.
First, the typical one, in which the CLTE model
is trained on labeled data for the same pair of lan-
guages L1?L2 of the test set. Then, simulating
the less favorable situation in which labeled train-
ing data for L1?L2 are missing, we investigate the
possibility to use existing CLTE models trained on
labeled data for a different language pair L3?L4.
The SemEval 2012 CLTE datasets used in our
experiments are available for four language pairs:
Es?En, De?En, Fr?En, and It?En. Each dataset
was created with the crowdsourcing-based method
described in Negri et al (2011), and consists of
1000 T1?T2 pairs (500 for training, 500 for test).
To train the word alignment models we used
the Europarl parallel corpus (Koehn, 2005), con-
catenated with the News Commentary corpus3
for three language pairs: De?En (2,079,049
sentences), Es?En (2,123,036 sentences), Fr?En
(2,144,820 sentences). For It?En we only used
the parallel data available in Europarl (1,909,115
sentences) since this language pair is not covered
by the News Commentary corpus. IDF values for
the words in each language were calculated on the
monolingual part of these corpora, using the aver-
age IDF value of each language for unseen terms.
To build the word alignment models we used the
MGIZA++ package (Gao and Vogel, 2008). Ex-
periments have been carried out with the hidden
Markov model (HMM) (Vogel et al, 1996) and
IBM models 3 and 4 (Brown et al, 1993).4 We also
explored three symmetrization techniques (Koehn
et al, 2005): union, intersection, and grow-diag-
final-and. A greedy feature selection process on
training data, with different combinations of word
alignment models and symmetrization methods,
indicated HMM/intersection as the best perform-
ing combination. For this reason, all our experi-
ments use this setting.
The SVM implementation of Weka (Hall et
al., 2009) was used to build the CLTE model.5
Two binary classifiers were trained to separately
check T1 ? T2 and T1 ? T2, merging
their output to obtain the 4-class judgments (e.g.
yes/yes=bidirectional, yes/no=forward).
3.1 Evaluation with CLTE training data
Figure 2 shows the accuracy obtained by the dif-
ferent feature groups.6 For the sake of compari-
son, state-of-the-art results achieved for each lan-
guage combination at SemEval 2012 are also re-
ported. As regards Es?En (63.2% accuracy) and
De?En (55.8%), the top scores were obtained by
the system described in (Wa?schle and Fendrich,
2012), where a combination of binary classifiers
for each entailment direction is trained with a mix-
3http://www.statmt.org/wmt11/
translation-task.html#download
4Five iterations of HMM, and three iterations of IBM
models 3 and 4 have been performed on the training corpora.
5The polynomial kernel was used with parameters empir-
ically estimated on the training set (C = 2.0, and d = 1)
6In Figures 2 and 3, the ?*? indicates statistically signif-
icant improvements over the state of the art at p ? 0.05,
calculated with approximate randomization (Pado?, 2006).
773
ture of monolingual (i.e. with the input sentences
translated in the same language using Google
Translate7) and cross-lingual features. Although
such system exploits word-alignment information
to some extent, this is only done at quantitative
level (e.g. number of unaligned words, percentage
of aligned words, length of the longest unaligned
subsequence). As regards It?En, the state of the
art (56.6%) is represented by the system described
in (Jimenez et al, 2012), which uses a pure pivot-
ing method (using Google Translate) and adaptive
similarity functions based on ?soft? cardinality for
flexible term comparisons. The two systems ob-
tained the same result on Fr?En (57.0%).
 50
 55
 60
 65
 70
 75
Es-En De-En Fr-En It-En
Ac
cu
rac
y (
%) *
* * * * * *
*
state-of-the-art
BB+ALB+AL+IDFB+AL+POSB+AL+IDF+POS
Figure 2: Accuracy obtained by each feature
group on four language combinations.
As can be seen in Figure 2, the combination of
all our features outperforms the state of the art
for each language pair. The accuracy improve-
ment ranges from 6.6% for Es?En (from 63.2% to
67.4%) to 14.6% for De?En (from 55.8% to 64%).
Except for Es?En, that has very competitive state-
of-the-art results, the combination of AL with POS
or IDF feature groups always outperforms the best
systems. Furthermore, the performance increase
with qualitative features (POS and IDF) shows co-
herent trends across all language pairs. It is worth
noting that, while we rely on a pure cross-lingual
approach, both the state-of-the-art CLTE systems
include features from the translation of T1 into the
language of T2. For De?En, quantitative features
alone achieve lower results compared to the other
languages. This can be motivated by the higher
difficulty in aligning De?En pairs (this hypothesis
is supported by the fact that the average number
of alignments per sentence pair is 18 for De?En,
and >22 for the other combinations). Neverthe-
less, qualitative features lead to results comparable
7http://translate.google.com/
with the other language pairs.
The selection of the best performing features
for each language pair produces further improve-
ments of varying degrees in Es?En (from 67.4%
to 68%), De?En (64% ? 64.8%) and It?En (63.4%
? 66.8%), while performance remains stable for
Fr?En (63%). All these configurations include
the IDF feature (12) and the proportion of aligned
words for each PoS category (7), proving the ef-
fectiveness of qualitative word alignment features.
The fact that HMM/intersection is the best com-
bination of alignment model and symmetrization
method is interesting, since it contradicts the gen-
eral notion that IBM models 3 and 4 perform bet-
ter than HMM (Och and Ney, 2003). A possible
explanation is that, while word alignment models
are usually trained on parallel corpora, the major-
ity of CLTE sentence pairs are not parallel. In
this setting, where producing reliable alignments
is more difficult, IBM models are less effective for
at least two reasons. First, including a word fertil-
ity model, IBM 3 and 4 limit (typically to the half
of the source sentence length) the number of tar-
get words that can be aligned with the null word.
Therefore, when such limit is reached, these mod-
els tend to force low probability, hence less reli-
able, word alignments. Second, in IBM model 4,
the larger distortion limit makes it possible to align
distant words. In the case of non-parallel sen-
tences, this often results in wrong or noisy align-
ments that affect final results. For these reasons,
CLTE data seem more suitable for the simpler and
more conservative HMM model, and a precision-
oriented symmetrization method like intersection.
3.2 Evaluation without CLTE training data
The goal of our second round of experiments is to
investigate if, and to what extent, our approach can
be considered as language-independent. Confirm-
ing this would allow to reuse models trained for
a given language pair in situations where CLTE
training data is missing. This is a rather realistic
situation since, while bitexts to train word aligners
are easier to find, the availability of labeled CLTE
data is far from being guaranteed.
Our experiments have been carried out, over the
same SemEval datasets, with two methods that do
not use labeled data for the target language com-
bination. The first one (method b in Figure 1)
uses a CLTE model trained for a language pair
L1?L2 for which labeled training data are avail-
774
able, and applies this model to a language pair
L3?L4 for which only parallel corpora are avail-
able. The second method (c in Figure 1) addresses
the same problem, but exploits a combination of
CLTE models trained for different language pairs.
For each test set, the models trained for the other
three language pairs are used in a voting scheme,
in order to check whether they can complement
each other to increase final results.
All the experiments have been performed using
the best CLTE model for each language pair, com-
paring results with those presented in Section 3.1.
 50
 55
 60
 65
 70
 75
 80
 85
Es-En De-En Fr-En It-En
Ac
cu
rac
y (
%) full s
ys.
full 
sys
.
full 
sys
. full 
sys
.
*
**
*
* * *
state-of-the-art
Es-EnDe-EnFr-EnIt-EnVoting
Figure 3: Accuracy obtained by reusing CLTE
models (alone and in a voting scheme).
As shown in Figure 3, reusing models for a new
language pair leads to results that still outperform
the state of the art.6 Remarkably, when used for
other language combinations, the Es?En, It?En,
and Fr?En models always lead to results above,
or equal to the state of the art. For similar lan-
guages such as Spanish, French, and Italian, the
accuracy increase over the state of the art is up to
14.8% (from 56.6% to 65.0%) and 13.4% (from
56.6% to 64.2%) when the Fr?En and Es?En mod-
els are respectively used to label the It?En dataset.
Although not always statistically significant and
below the performance obtained in the ideal sce-
nario where CLTE training data are available (full
sys.), such improvements suggest that our features
can be re-used, at least to some extent, across dif-
ferent language settings. As expected, the major
incompatibilities arise between German and the
other languages due to the linguistic differences
between this language and the others. However, it
is interesting to note that: i) at least in one case
(i.e. when tested on It?En) the De?En model still
achieves results above the state of the art, and ii)
on the De?En evaluation setting the worst model
(Fr?En) still achieves state of the art results.
The results obtained with the voting scheme
suggest that our models can complement each
other when used on a new language pair. Although
statistically significant only over It?En data, vot-
ing results both outperform the state of the art and
the results achieved by single models.
4 Conclusion
We investigated the usefulness of qualitative infor-
mation from automatic word alignment to iden-
tify semantic relations between sentences in dif-
ferent languages. With coherent results in CLTE,
we demonstrated that features considering the im-
portance of aligned terms can successfully inte-
grate the quantitative evidence (number and pro-
portion of aligned terms) used by previous su-
pervised learning approaches. A study on the
portability across languages of the learned mod-
els demonstrated that word alignment information
can be exploited to train reusable models for new
language combinations where bitexts are available
but CLTE labeled data are not.
Acknowledgments
This work has been partially supported by the EC-
funded projects CoSyne (FP7-ICT-4-248531) and
MateCat (ICT-2011.4.2?287688), and by Span-
ish Government through projects TIN2009-14009-
C02-01 and TIN2012-32615.
References
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 10?51, Montre?al, Canada.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, USA.
775
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: an Up-
date. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality + ML: Learning Adap-
tive Similarity Functions for Cross-lingual Textual
Entailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012),
pages 684?688, Montre?al, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philip Koehn. 2005. Europarl: a Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Alessandro Lenci, Roberto Bartolini, Nicoletta Cal-
zolari, Ana Agua, Stephan Busemann, Emmanuel
Cartier, Karine Chevreau, and Jose? Coch. 2002.
Multilingual summarization by integrating linguistic
resources in the MLIS-MUSI Project. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC?02), pages
1464?1471, Las Palmas de Gran Canaria, Spain.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
In Proceedings of the Eleventh Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2010),
pages 321?324, Los Angeles, California, USA.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012a. Detecting Semantic Equivalence and Infor-
mation Disparity in Cross?lingual Documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?12),
pages 120?124, Jeju Island, Korea.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012b. Match without a Referee: Evaluating
MT Adequacy without Reference Translations. In
Proceedings of the Machine Translation Workshop
(WMT2012), Montre?al, Canada.
Christoph Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
CoSyne: a Framework for Multilingual Content
Synchronization of Wikis. In Proceedings of Wik-
iSym 2011, the International Symposium on Wikis
and Open Collaboration, pages 217?218, Mountain
View, California, USA.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad,
Danilo Giampiccolo, and Alessandro Marchetti.
2011. Divide and Conquer: Crowdsourcing the Cre-
ation of Cross-Lingual Textual Entailment Corpora.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), Edinburgh, Scotland.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 Task 8: Cross-Lingual Textual En-
tailment for Content Synchronization. In Proceed-
ings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), pages 399?407,
Montre?al, Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2013.
Semeval-2013 Task 8: Cross-Lingual Textual En-
tailment for Content Synchronization. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, GA.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Kristen Parton. 2012. Lost and Found in Transla-
tion: Cross-Lingual Question Answering with Result
Translation. Ph.D. thesis, Columbia University.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), pages 2089?
2096, Istanbul, Turkey.
Gerard Salton and Christopher Buckley. 1988.
Term-weighting Approaches in Automatic Text Re-
trieval. Information Processing and Management,
24(5):513?523.
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50, Dublin, Ireland.
Hristo Tanev, Milen Kouylekov, Bernardo Magnini,
Matteo Negri, and Kiril Simov. 2006. Exploit-
ing Linguistic Indices and Syntactic Structures for
Multilingual Question Answering: ITC-irst at CLEF
2005. Accessing Multilingual Information Reposito-
ries, pages 390?399.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics
(ACL?96), pages 836?841, Copenhagen, Denmark.
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Fea-
tures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), pages
467?471, Montre?al, Canada.
776
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 710?720,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Adaptive Quality Estimation for Machine Translation
Marco Turchi
(1)
Antonios Anastasopoulos
(3)
Jos
?
e G. C. de Souza
(1,2)
Matteo Negri
(1)
(1)
FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
(2)
University of Trento, Italy
(3)
National Technical University of Athens, Greece
{turchi,desouza,negri}@fbk.eu
anastasopoulos.ant@gmail.com
Abstract
The automatic estimation of machine
translation (MT) output quality is a hard
task in which the selection of the appro-
priate algorithm and the most predictive
features over reasonably sized training sets
plays a crucial role. When moving from
controlled lab evaluations to real-life sce-
narios the task becomes even harder. For
current MT quality estimation (QE) sys-
tems, additional complexity comes from
the difficulty to model user and domain
changes. Indeed, the instability of the sys-
tems with respect to data coming from dif-
ferent distributions calls for adaptive so-
lutions that react to new operating con-
ditions. To tackle this issue we propose
an online framework for adaptive QE that
targets reactivity and robustness to user
and domain changes. Contrastive exper-
iments in different testing conditions in-
volving user and domain changes demon-
strate the effectiveness of our approach.
1 Introduction
After two decades of steady progress, research
in statistical machine translation (SMT) started to
cross its path with translation industry with tan-
gible mutual benefit. On one side, SMT research
brings to the industry improved output quality and
a number of appealing solutions useful to increase
translators? productivity. On the other side, the
market needs suggest concrete problems to solve,
providing real-life scenarios to develop and eval-
uate new ideas with rapid turnaround. The evolu-
tion of computer-assisted translation (CAT) envi-
ronments is an evidence of this trend, shown by
the increasing interest towards the integration of
suggestions obtained from MT engines with those
derived from translation memories (TMs).
The possibility to speed up the translation pro-
cess and reduce its costs by post-editing good-
quality MT output raises interesting research chal-
lenges. Among others, these include deciding
what to present as a suggestion, and how to do it
in the most effective way.
In recent years, these issues motivated research
on automatic QE, which addresses the problem
of estimating the quality of a translated sentence
given the source and without access to reference
translations (Blatz et al, 2003; Specia et al, 2009;
Mehdad et al, 2012). Despite the substantial
progress done so far in the field and in success-
ful evaluation campaigns (Callison-Burch et al,
2012; Bojar et al, 2013), focusing on concrete
market needs makes possible to further define the
scope of research on QE. For instance, moving
from controlled lab testing scenarios to real work-
ing environments poses additional constraints in
terms of adaptability of the QE models to the vari-
able conditions of a translation job. Such variabil-
ity is due to two main reasons:
1. The notion of MT output quality is highly
subjective (Koponen, 2012; Turchi et al,
2013; Turchi and Negri, 2014). Since the
quality standards of individual users may
vary considerably (e.g. according to their
knowledge of the source and target lan-
guages), the estimates of a static QE model
trained with data collected from a group of
post-editors might not fit with the actual
judgements of a new user;
2. Each translation job has its own specifici-
ties (domain, complexity of the source text,
average target quality). Since data from a
new job may differ from those used to train
the QE model, its estimates on the new in-
stances might result to be biased or uninfor-
mative.
The ability of a system to self-adapt to the be-
710
haviour of specific users and domain changes is
a facet of the QE problem that so far has been
disregarded. To cope with these issues and deal
with the erratic conditions of real-world trans-
lation workflows, we propose an adaptive ap-
proach to QE that is sensitive and robust to dif-
ferences between training and test data. Along this
direction, our main contribution is a framework in
which QE models can be trained and can continu-
ously evolve over time accounting for knowledge
acquired from post editors? work.
Our approach is based on the online learning
paradigm and exploits a key difference between
such framework and the batch learning methods
currently used. On one side, the QE models ob-
tained with batch methods are learned exclusively
from a predefined set of training examples under
the assumption that they have similar characteris-
tics with respect to the test data. This makes them
suitable for controlled evaluation scenarios where
such condition holds. On the other side, online
learning techniques are designed to learn in a step-
wise manner (either from scratch, or by refining an
existing model) from new, unseen test instances
by taking advantage of external feedback. This
makes them suitable for real-life scenarios where
the new instances to be labelled can considerably
differ from the data used to train the QE model.
To develop our approach, different online algo-
rithms have been embedded in the backbone of
a QE system. This required the adaptation of its
standard batch learning workflow to:
1. Perform online feature extraction from a
source?target pair (i.e. one instance at a time
instead of processing an entire training set);
2. Emit a prediction for the input instance;
3. Gather user feedback for the instance (i.e.
calculating a ?true label? based on the
amount of user post-editions);
4. Send the true label back to the model to up-
date its predictions for future instances.
Focusing on the adaptability to user and domain
changes, we report the results of comparative ex-
periments with two online algorithms and the stan-
dard batch approach. The evaluation is carried out
by measuring the global error of each algorithm
on test sets featuring different degrees of similar-
ity with the data used for training. Our results
show that the sensitivity of online QE models to
different distributions of training and test instances
makes them more suitable than batch methods for
integration in a CAT framework.
Our adaptive QE infrastructure has been re-
leased as open source. Its C++ implementation is
available at http://hlt.fbk.eu/technologies/
aqet.
2 Related work
QE is generally cast as a supervised machine
learning task, where a model trained from a col-
lection of (source, target, label) instances is used
to predict labels
1
for new, unseen test items (Spe-
cia et al, 2010).
In the last couple of years, research in the field
received a strong boost by the shared tasks orga-
nized within the WMT workshop on SMT,
2
which
is also the framework of our first experiment in
?5. Current approaches to the tasks proposed at
WMT have mainly focused on three main direc-
tions, namely: i) feature engineering, as in (Hard-
meier et al, 2012; de Souza et al, 2013a; de Souza
et al, 2013b; Rubino et al, 2013b), ii) model
learning with a variety of classification and regres-
sion algorithms, as in (Bicici, 2013; Beck et al,
2013; Soricut et al, 2012), and iii) feature selec-
tion as a way to overcome sparsity and overfitting
issues, as in (Soricut et al, 2012).
Being optimized to perform well on specific
WMT sub-tasks and datasets, current systems re-
flect variations along these directions but leave im-
portant aspects of the QE problem still partially
investigated or totally unexplored.
3
Among these,
the necessity to model the diversity of human qual-
ity judgements and correction strategies (Kopo-
nen, 2012; Koponen et al, 2012) calls for solu-
tions that: i) account for annotator-specific be-
haviour, thus being capable of learning from inher-
ently noisy datasets produced by multiple annota-
tors, and ii) self-adapt to changes in data distribu-
tion, learning from user feedback on new, unseen
test items.
1
Possible label types include post-editing effort scores
(e.g. 1-5 Likert scores indicating the estimated percentage
of MT output that has to be corrected), HTER values (Snover
et al, 2006), and post-editing time (e.g. seconds per word).
2
http://www.statmt.org/wmt13/
3
For a comprehensive overview of the QE approaches
proposed so far we refer the reader to the WMT12 and
WMT13 QE shared task reports (Callison-Burch et al, 2012;
Bojar et al, 2013).
711
These interconnected issues are particularly rel-
evant in the CAT framework, where translation
jobs from different domains are routed to pro-
fessional translators with different idiolect, back-
ground and quality standards.
The first aspect, modelling annotators? individ-
ual behaviour and interdependences, has been ad-
dressed by Cohn and Specia (2013), who explored
multi-task Gaussian Processes as a way to jointly
learn from the output of multiple annotations. This
technique is suitable to cope with the unbalanced
distribution of training instances and yields better
models when heterogeneous training datasets are
available.
The second problem, the adaptability of QE
models, has not been explored yet. A common
trait of all current approaches, in fact, is the re-
liance on batch learning techniques, which assume
a ?static? nature of the world where new unseen
instances that will be encountered will be similar
to the training data.
4
However, similarly to trans-
lation memories that incrementally store translated
segments and evolve over time incorporating users
style and terminology, all components of a CAT
tool (the MT engine and the mechanisms to assign
quality scores to the suggested translations) should
take advantage of translators feedback.
On the MT system side, research on adaptive
approaches tailored to interactive SMT and CAT
scenarios explored the online learning protocol
(Littlestone, 1988) to improve various aspects of
the decoding process (Cesa-Bianchi et al, 2008;
Ortiz-Mart??nez et al, 2010; Mart??nez-G?omez et
al., 2011; Mart??nez-G?omez et al, 2012; Mathur
et al, 2013; Bertoldi et al, 2013).
As regards QE models, our work represents the
first investigation on incremental adaptation by ex-
ploiting users feedback to provide targeted (sys-
tem, user, or project specific) quality judgements.
3 Online QE for CAT environments
When operating with advanced CAT tools, transla-
tors are presented with suggestions (either match-
ing fragments from a translation memory or auto-
matic translations produced by an MT system) for
each sentence of a source document. Before being
approved and published, translation suggestions
may require different amounts of post-editing op-
erations depending on their quality.
4
This assumption holds in the WMT evaluation scenario,
but it is not necessarily valid in real operating conditions.
Each post-edition brings a wealth of dynamic
knowledge about the whole translation process
and the involved actors. For instance, adaptive QE
components could exploit information about the
distance between automatically assigned scores
and the quality standards of individual translators
(inferred from the amount of their corrections) to
?profile? their behaviour.
The online learning paradigm fits well with this
research objective. In the online framework, dif-
ferently from the batch mode, the learning al-
gorithm sequentially processes an unknown se-
quence of instances X = x
1
, x
2
, ..., x
n
, returning
a prediction p(x
i
) as output at each step. Differ-
ences between p(x
i
) and the true label p?(x
i
) ob-
tained as feedback are used by the learner to refine
the next prediction p(x
i+1
).
In our experiments on adaptive QE we aim to
predict the quality of the suggested translations
in terms of HTER, which measures the minimum
edit distance between the MT output and its man-
ually post-edited version in the [0,1] interval.
5
In
this scenario:
? The set of instances X is represented by
(source, target) pairs;
? The prediction p(x
i
) is the automatically es-
timated HTER score;
? The true label p?(x
i
) is the actual HTER score
calculated over the target and its post-edition.
At each step of the process, the goal of the learner
is to exploit user post-editions to reduce the differ-
ence between the predicted HTER values and the
true labels for the following (source, target) pairs.
As depicted in Figure 1, this is done as follows:
1. At step i, an unlabelled (source, target) pair
x
i
is sent to a feature extraction component.
To this aim, we used an adapted version
(Shah et al, 2014) of the open-source QuEst
6
tool (Specia et al, 2013). The tool, which im-
plements a large number of features proposed
by participants in the WMT QE shared tasks,
has been modified to process one sentence at
a time as requested for integration in a CAT
environment;
5
Edit distance is calculated as the number of edits (word
insertions, deletions, substitutions, and shifts) divided by the
number of words in the reference. Lower HTER values indi-
cate better translations.
6
http://www.quest.dcs.shef.ac.uk/
712
Figure 1: Online QE workflow. <src>, <trg> and <pe> respectively stand for the source sentence, the
target translation and the post-edited target.
2. The extracted features are sent to an on-
line regressor, which returns a QE prediction
score p(x
i
) in the [0,1] interval (set to 0 at the
first round of the iteration);
3. Based on the post-edition done by the user,
the true HTER label p?(x
i
) is calculated by
means of the TERCpp
7
open source tool;
4. The true label is sent back to the online al-
gorithm for a stepwise model improvement.
The updated model is then ready to process
the following instance x
i+1
.
This new paradigm for QE makes it possible
to: i) let the QE system learn from one point at
a time without complete re-training from scratch,
ii) customize the predictions of an existing QE
model with respect to a specific situation (post-
editor or domain), or even iii) build a QE model
from scratch when training data is not available.
For the sake of clarity it is worth observing that,
at least in principle, a model built in a batch fash-
ion could also be adapted to new test data. For in-
stance, this could be done by running periodic re-
training routines once a certain amount of new la-
belled instances has been collected (de facto mim-
icking an online process). Such periodic updates,
however, would not represent a viable solution in
the CAT framework where post-editors? work can-
not be slowed by time-consuming procedures to
re-train core system components from scratch.
7
goo.gl/nkh2rE
4 Evaluation framework
To measure the adaptation capability of different
QE models, we experiment with a range of condi-
tions defined by variable degrees of similarity be-
tween training and test data.
The degree of similarity depends on several fac-
tors: the MT engine used, the domain of the docu-
ments to be translated, and the post-editing style of
individual translators. In our experiments, the de-
gree of similarity is measured in terms of ?HTER,
which is computed as the absolute value of the dif-
ference between the average HTER of the training
and test sets. Large values indicate a low simi-
larity between training and test data and a more
challenging scenario for the learning algorithms.
4.1 Experimental setup
In the range of possible evaluation scenarios, our
experiments cover:
? One artificial setting (?5) obtained from the
WMT12 QE shared task data, in which train-
ing/test instances are arranged to reflect ho-
mogeneous distributions of the HTER labels.
? Two settings obtained from data collected
with a CAT tool in real working condi-
tions, in which different facets of the adap-
tive QE problem interact with each other.
In the first (user change, ?6.1), train-
ing and test data from the same domain are
obtained from different users. In the sec-
713
ond (user+domain change, ?6.2), train-
ing and test data are obtained from different
users and domains.
For each setting, we compare an adaptive and
an empty model against a system trained in batch
mode. The adaptive model is built on top of an
existing model created from the training data and
exploits the new test instances to refine its predic-
tions in a stepwise manner. The empty model only
learns from the test set, simulating the worst con-
dition where training data is not available. The
batch model is built by learning only from the
training data and is evaluated on the test set with-
out exploiting information from the test instances.
Each model is also compared against a common
baseline for regression tasks, which is particularly
relevant in settings featuring different data distri-
butions between training and test sets. This base-
line (? henceforth) is calculated by labelling each
instance of the test set with the mean HTER score
of the training set. Previous works (Rubino et al,
2013a) demonstrated that its results can be partic-
ularly hard to beat.
4.2 Performance indicator and feature set
To measure the adaptability of our model to a
given test set we compute the Mean Absolute Er-
ror (MAE), a metric for regression problems also
used in the WMT QE shared tasks. The MAE is
the average of the absolute errors e
i
= |f
i
? y
i
|,
where f
i
is the prediction of the model and y
i
is
the true value for the i
th
instance.
As our focus is on the algorithmic aspect, in all
experiments we use the same feature set, which
consists of the seventeen features proposed in
(Specia et al, 2009). This feature set, fully de-
scribed in (Callison-Burch et al, 2012), takes into
account the complexity of the source sentence
(e.g. number of tokens, number of translations per
source word) and the fluency of the target trans-
lation (e.g. language model probabilities). The
results of previous WMT QE shared tasks have
shown that these baseline features are particularly
competitive in the regression task (with only few
systems able to beat them at WMT12).
4.3 Online algorithms
In our experiments we evaluate two online algo-
rithms, OnlineSVR (Parrella, 2007)
8
and Passive-
8
http://www2.imperial.ac.uk/
?
gmontana/
onlinesvr.htm
Aggressive Perceptron (Crammer et al, 2006),
9
by
comparing their performance with a batch learning
strategy based on the Scikit-learn implementation
of Support Vector Regression (SVR).
10
The choice of the OnlineSVR and Passive-
Aggressive (OSVR and PA henceforth) is moti-
vated by different considerations. From a perfor-
mance point of view, as an adaptation of -SVR
which proved to be one of the top performing algo-
rithms in the regression QE tasks at WMT, OSVR
seems to be the best candidate. For this reason,
we use the online adaptation of -SVR proposed
by (Ma et al, 2003). The goal of OnlineSVR is to
find a way to add each new sample to one of three
sets (support, empty, error) maintaining the con-
sistency of a set of conditions known as Karush-
Kuhn Tucker (KKT) conditions. For each new
point, OSVR starts a cycle where the samples are
moved across the three sets until the KKT condi-
tions are verified and the new point is assigned to
one of the sets. If the point is identified as a sup-
port vector, the parameters of the model are up-
dated. This allows OSVR to benefit from the pre-
diction capability of -SVR in an online setting.
From a practical point of view, providing the
best trade off between accuracy and computational
time (He and Wang, 2012), PA represents a good
solution to meet the demand of efficiency posed
by the CAT framework. For each instance i, after
emitting a prediction and receiving the true label,
PA computes the -insensitive hinge loss function.
If its value is larger than the tolerance parameter
(), the weights of the model are updated as much
as the aggressiveness parameter C allows. In con-
trast with OSVR, which keeps track of the most
important points seen in the past (support vectors),
the update of the weights is done without consid-
ering the previously processed i-1 instances. Al-
though it makes PA faster than OSVR, this is a
riskier strategy because it may lead the algorithm
to change the model to adapt to outlier points.
5 Experiments with WMT12 data
The motivations for experiments with training and
test data featuring homogeneous label distribu-
tions are twofold. First, since in this artificial sce-
nario adaptation capabilities are not required for
the QE component, batch methods operate in the
ideal conditions (as training and test are indepen-
9
https://code.google.com/p/sofia-ml/
10
http://scikit-learn.org/
714
WMT Dataset
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg. MAE Alg.
200 754 0.39 13.7 13.2 13.2
?
OSVR 13.5
?
OSVR
600 754 1.32 13.8 12.7 12.9
?
OSVR 13.5
?
OSVR
1500 754 1.22 13.8 12.7 12.8
?
OSVR 13.5
?
OSVR
Table 1: MAE of the best performing batch, adaptive and empty models on WMT12 data. Training sets
of different size and the test set have been arranged to reflect homogeneous label distributions.
dent and identically distributed). This makes pos-
sible to obtain from batch models the best possible
performance to compare with. Second, this sce-
nario provides the fairest conditions for such com-
parison because, in principle, online algorithms
are not favoured by the possibility to learn from
the diversity of the test instances.
For our controlled experiments we use the
WMT12 English-Spanish corpus, which consists
of 2,254 source-target pairs (1,832 for training,
422 for test). The HTER labels for our regression
task are calculated from the post-edited version
and the target sentences provided in the dataset.
To avoid biases in the label distribution, the
WMT12 training and test data have been merged,
shuffled, and eventually separated to generate
three training sets of different size (200, 600, and
1500 instances), and one test set with 754 in-
stances. For each algorithm, the training sets are
used for learning the QE models, optimizing pa-
rameters (i.e. C, , the kernel and its parame-
ters for SVR and OSVR; tolerance and aggressive-
ness for PA) through grid search in 10-fold cross-
validation.
Evaluation is carried out by measuring the per-
formance of the batch (learning only from the
training set), the adaptive (learning from the train-
ing set and adapting to the test set), and the empty
(learning from scratch from the test set) models in
terms of global MAE scores on the test set.
Table 1 reports the results achieved by the
best performing algorithm for each type of model
(batch, adaptive, empty). As can be seen, close
MAE values show a similar behaviour for the three
types of models.
11
With the same amount of train-
ing data, the performance of the batch and the
adaptive models (in this case always obtained with
OSVR) is almost identical. This demonstrates
that, as expected, the online algorithms do not take
11
Results marked with the ?
?
? symbol are NOT statisti-
cally significant compared to the corresponding batch model.
The others are always statistically significant at p?0.005, cal-
culated with approximate randomization (Yeh, 2000).
advantage of test data with a label distribution sim-
ilar to the training set. All the models outper-
form the baseline, even if the minimal differences
confirm the competitiveness of such a simple ap-
proach.
Overall, these results bring some interesting in-
dications about the behaviour of the different on-
line algorithms. First, the good results achieved
by the empty models (less than one MAE point
separates them from the best ones built on the
largest training set) suggest their high potential
when training data are not available. Second,
our results show that OSVR is always the best
performing algorithm for the adaptive and empty
models. This suggests a lower capability of PA to
learn from instances similar to the training data.
6 Experiments with CAT data
To experiment with adaptive QE in more realis-
tic conditions we used a CAT tool
12
to collect
two datasets of (source, target, post edited tar-
get) English-Italian tuples.The source sentences in
the datasets come from two documents from dif-
ferent domains, respectively legal (L) and infor-
mation technology (IT). The L document, which
was extracted from a European Parliament resolu-
tion published on the EUR-Lex platform,
13
con-
tains 164 sentences. The IT document, which was
taken from a software user manual, contains 280
sentences. The source sentences were translated
with two SMT systems built by training the Moses
toolkit (Koehn et al, 2007) on parallel data from
the two domains (about 2M sentences for IT and
1.5M for L). Post-editions were collected from
eight professional translators (four for each docu-
ment) operating with the CAT tool in real working
conditions.
According to the way they are created, the two
datasets allow us to evaluate the adaptability of
different QE models with respect to user changes
12
MateCat ? http://www.matecat.com/
13
http://eur-lex.europa.eu/
715
user change
Legal Domain
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg. MAE Alg.
rad cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
cons rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
sim1 sim2 3.3 14.7 12.2 12.6
?
OSVR 12.9
?
OSVR
sim2 sim1 3.2 13.4 13.3 13.9
?
OSVR 15.2
?
OSVR
IT Domain
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg MAE Alg
cons rad 12.8 19.2 19.8 17.5
?
OSVR 16.6 OSVR
rad cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
sim2 sim1 3.3 14.7 14.4 15
?
OSVR 15.5
?
OSVR
sim1 sim2 1.1 15 13.9 14.4
?
OSVR 16.1
?
OSVR
Table 2: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users in the same domain.
within the same domain (?6.1), as well as user and
domain changes at the same time (?6.2).
For each document D (L or IT), these two sce-
narios are obtained by dividing D into two parts
of equal size (80 instances for L and 140 for IT).
The result is one training set and one test set for
each post-editor within the same domain. For the
user change experiments, training and test sets
are selected from different post-editors within the
same domain. For the user+domain change
experiments, training and test sets are selected
from different post-editors in different domains.
On each combination of training and test sets,
the batch, adaptive, and empty models are trained
and evaluated in terms of global MAE scores on
the test set.
6.1 Dealing with user changes
Among the possible combinations of training and
test data from different post-editors in the same
domain, Table 2 refers to two opposite scenarios.
For each domain, these respectively involve the
most dissimilar and the most similar post-editors
according to the ?HTER. Also in this case, for
each model (batch, adaptive and empty) we only
report the MAE of the best performing algorithm.
The first scenario defines a challenging situation
where two post-editors (rad and cons) are charac-
terized by opposite behaviour. As evidenced by
the high ?HTER values, one of them (rad) is the
most ?radical? post-editor (performing more cor-
rections) while the other (cons) is the most ?con-
servative? one. As shown in Table 2, global MAE
scores for the online algorithms (both adaptive and
empty) indicate their good adaptation capabilities.
This is evident from the significant improvements
both over the baseline (?) and the batch models.
Interestingly, the best results are always achieved
by the empty models (with MAE reductions up to
10 points when tested on rad in the L domain,
and 3.2 points when tested on rad in the IT do-
main). These results (MAE reductions are always
statistically significant) suggest that, when deal-
ing with datasets with very different label distri-
butions, the evident limitations of batch methods
are more easily overcome by learning from scratch
from the feedback of a new post-editor. This also
holds when the amount of test points to learn from
is limited, as in the L domain where the test set
contains only 80 instances. From the application-
oriented perspective that motivates our work, con-
sidering the high costs of acquiring large and rep-
resentative QE training data, this is an important
finding.
The second scenario defines a less challeng-
ing situation where the two post-editors (sim1 and
sim2) are characterized by the most similar be-
haviour (small ?HTER). This scenario is closer to
the situation described in Section ?5. Also in this
case MAE results for the adaptive and empty mod-
els are slightly worse, but not significantly, than
those of the batch models and the baseline. How-
ever, considering the very small amount of ?unin-
formative? instances to learn from (especially for
the empty models), these lower results are not sur-
prising.
A closer look at the behaviour of the online al-
gorithms in the two domains leads to other obser-
vations. First, OSVR always outperforms PA for
the empty models and when post-editors have sim-
716
user+domain change
Train Test
? ? Batch Adaptive Empty
HTER MAE MAE MAE Alg MAE Alg
L cons IT rad 24.5 26.4 27 18.2 OSVR 16.6 OSVR
IT rad L cons 24.0 24.9 25.4 19.7 OSVR 12.5 OSVR
L rad L cons 20.5 21.4 20.6 14.5 PA 12.5 OSVR
L cons L rad 19.4 21.2 21.3 16.1 PA 11.3 OSVR
IT cons L cons 13.5 17.3 17.5 15.7 OSVR 12.5 OSVR
IT cons IT rad 12.8 19.2 19.8 17.5 OSVR 16.6 OSVR
L cons IT cons 12.7 17.6 17.6 15.1 OSVR 15.5 OSVR
IT rad IT cons 9.6 16.8 16.6 15.6 PA 15.5 OSVR
IT cons L rad 8.3 12.3 13 10.7 OSVR 11.3 OSVR
L rad IT rad 6.8 17 16.9 16.2 OSVR 16.6 OSVR
L rad IT cons 5.0 15.4 16.2 14.7 OSVR 15.5 OSVR
IT rad L rad 2.2 10.6 10.8 10.5 OSVR 11.3 OSVR
Table 3: MAE of the best performing batch, adaptive and empty models on CAT data collected from
different users and domains.
ilar behaviour, which are situations where the al-
gorithm does not have to quickly adapt or react to
sudden changes.
Second, PA seems to perform better for the
adaptive models when the post-editors have sig-
nificantly different behaviour and a quick adapta-
tion to the incoming points is required. This can
be motivated by the fact that PA relies on a simpler
and less robust learning strategy that does not keep
track of all the information coming from the previ-
ously processed instances, and can easily modify
its weights taking into consideration the last seen
point (see Section ?3). For OSVR the addition of
new points to the support set may have a limited
effect on the whole model, in particular if the num-
ber of points in the set is large. This also results
in a different processing time for the two algo-
rithms.
14
For instance, in the empty configurations
on IT data, OSVR devotes 6.0 ms per instance to
update the model, while PA devotes 4.8ms, which
comes at the cost of lower performance.
6.2 Dealing with user and domain changes
In the last round of experiments we evaluate the
reactivity of different online models to simultane-
ous user and domain changes. To this aim, our
QE models are created using a training set coming
from one domain (L or IT), and then used to pre-
dict the HTER labels for the test instances coming
from the other domain (e.g. training on L, testing
on IT).
Among the possible combinations of training
14
Their complexity depends on the number of features (f )
and the number of previously seen instances (n). While for
PA it is linear in f, i.e. O(f), for OSVR it is quadratic in n, i.e.
O(n
2
*f).
and test data, Table 3 refers to scenarios involv-
ing the most conservative and radical post-editors
in each domain (previously identified with cons
and rad)
15
. In the table, results are ordered ac-
cording to the ?HTER computed between the se-
lected post-editor in the training domain (e.g. L
cons) and the selected post-editor in the test do-
main (e.g. IT rad). For the sake of comparison,
we also report (grey rows) the results of the ex-
periments within the same domain presented in
?6.1. For each type of model (batch, adaptive and
empty) we only show the MAE obtained by the
best performing algorithm.
Intuitively, dealing with simultaneous user and
domain changes represents a more challenging
problem compared to the previous setting where
only post-editors changes were considered. Such
intuition is confirmed by the results of the adaptive
models that outperform both the baseline (?) and
the batch models even for low ?HTER values. Al-
though in these cases the distance between train-
ing and test data is comparable to the experiments
with similar post-editors working in the same do-
main (sim1 and sim2), here the predictive power
of the batch models seems in fact to be lower. The
same holds also for the empty models except in
two cases where the ?HTER is the smallest (2.2
and 5.0). This is a strong evidence of the fact that,
in case of domain changes, online models can still
learn from new test instances even if they have a
label distribution similar to the training set.
When the distance between training and test in-
creases, our results confirm our previous findings
15
For brevity, we omit the results for the other post-editors
which, however, show similar trends with respect to the pre-
vious experiments.
717
about the potential of the empty models. The ob-
served MAE reductions range in fact from 10.4
to 12.9 points for the two combinations with the
highest ?HTER.
From the algorithmic point of view, our results
indicate that OSVR achieves the best performance
for all the combinations involving user and domain
changes. This contrasts with the results of most of
the combinations involving only user changes with
post-editors characterized by opposite behaviour
(grey rows in Table 3). However, it has to be re-
marked that in the case of heterogeneous datasets
the difference between the two algorithms is al-
ways very high. In our experiments, when PA out-
performs OSVR, its MAE results are significantly
lower and vice-versa (respectively up to 1.5 and
1.7 MAE points). This suggests that, although PA
is potentially capable of achieving higher results
and better adapt to the new test points, its instabil-
ity makes it less reliable for practical use.
As a final analysis of our results, we investi-
gated how the performance of the different types
of models (batch, adaptive, empty) relates to the
distance between training and test sets. To this
aim, we computed the Pearson correlation be-
tween the ?HTER (column 3 in Table 3) and the
MAE of each model (columns 5, 6 and 8), which
respectively resulted in 0.9 for the batch, 0.63 for
the adaptive and -0.07 for the empty model. These
values confirm that batch models are heavily af-
fected by the dissimilarity between training and
test data: large differences in the label distribution
imply higher MAE results and vice-versa. This
is in line with our previous findings about batch
models that, learning only from the training set,
cannot leverage possible dissimilarities of the test
set. The lower correlation observed for the adap-
tive models also confirms our intuitions: adapting
to the new test points, these models are in fact
more robust to differences with the training data.
As expected, the results of the empty models are
completely uncorrelated with the ?HTER since
they only use the test set.
This analysis confirms that, even when dealing
with different domains, the similarity between the
training and test data is one of the main factors that
should drive the choice of the QE model. When
this distance is minimal, batch models can be a
reasonable option, but when the gap between train-
ing and test data increases, adaptive or empty mod-
els are a preferable choice to achieve good results.
7 Conclusion
In the CAT scenario, each translation job can be
seen as a complex situation where the user (his
personal style and background), the source doc-
ument (the language and the domain) and the un-
derlying technology (the translation memory and
the MT engine that generate translation sugges-
tions) contribute to make the task unique. So far,
the adaptability to such specificities (a major chal-
lenge for CAT technology) has been mainly sup-
ported by the evolution of translation memories,
which incrementally store translated segments in-
corporating the user style. The wide adoption of
translation memories demonstrates the importance
of capitalizing on such information to increase
translators productivity.
While this lesson recently motivated research
on adaptive MT decoders that learn from user cor-
rections, nothing has been done to develop adap-
tive QE components. In the first attempt to ad-
dress this problem, we proposed the application
of the online learning protocol to leverage users
feedback and to tailor QE predictions to their qual-
ity standards. Besides highlighting the limitations
of current batch methods to adapt to user and
domain changes, we performed an application-
oriented analysis of different online algorithms fo-
cusing on specific aspects relevant to the CAT sce-
nario. Our results show that the wealth of dynamic
knowledge brought by user corrections can be ex-
ploited to refine in a stepwise fashion the qual-
ity judgements in different testing conditions (user
changes as well as simultaneous user and domain
changes).
As an additional contribution, to spark further
research on this facet of the QE problem, our adap-
tive QE infrastructure (integrating all the compo-
nents and the algorithms described in this paper)
has been released as open source. Its C++ im-
plementation is available at http://hlt.fbk.eu/
technologies/aqet.
Acknowledgements
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
References
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite: When less is more for
translation quality estimation. In Proceedings of the
718
8th
Workshop on Statistical Machine Translation,
Sofia, Bulgaria, August.
Nicola Bertoldi, Mauro Cettolo, and Federico Mar-
cello. 2013. Cache-based Online Adaptation
for Machine Translation Enhanced Computer As-
sisted Translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 1147?1162, Nice,
France.
Ergun Bicici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the 8
th
Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of the 8
th
Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7
th
Work-
shop on Statistical Machine Translation (WMT?12),
pages 10?51, Montr?eal, Canada.
Nicol`o Cesa-Bianchi, Gabriel Reverberi, and Sandor
Szedmak. 2008. Online Learning Algorithms for
Computer-Assisted Translation. Deliverable D4.2,
SMART: Statistical Multilingual Analysis for Re-
trieval and Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of the 51
st
Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL-2013, pages 32?42, Sofia, Bulgaria.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms. J. Mach. Learn.
Res., 7:551?585, December.
Jos?e G.C. de Souza, Christian Buck, Marco Turchi, and
Matteo Negri. 2013a. FBK-UEdin participation to
the WMT13 quality estimation shared task. In Pro-
ceedings of the 8
th
Workshop on Statistical Machine
Translation, Sofia, Bulgaria, August.
Jos?e G.C. de Souza, Miquel Espl`a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting Quali-
tative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks. In Proceedings of the
51
st
Annual Meeting of the Association for Compu-
tational Linguistics - Short Papers, pages 771?776,
Sofia, Bulgaria.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Tree Kernels for Machine Transla-
tion Quality Estimation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 109?113, Montr?eal, Canada.
Zhengyan He and Houfeng Wang. 2012. A Com-
parison and Improvement of Online Learning Al-
gorithms for Sequence Labeling. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1147?
1162, Mumbai, India.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45
th
Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing Time as a Mea-
sure of Cognitive Effort. In Proceedings of the
AMTA 2012 Workshop on Post-editing Technology
and Practice (WPTP 2012), San Diego, California.
Maarit Koponen. 2012. Comparing Human Percep-
tions of Post-editing Effort with Post-editing Op-
erations. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 181?190,
Montr?eal, Canada.
Nick Littlestone. 1988. Learning Quickly when Irrel-
evant Attributes Abound: A New Linear-Threshold
Algorithm. In Machine Learning, pages 285?318.
Junshui Ma, James Theiler, and Simon Perkins. 2003.
Accurate Online Support Vector Regression. Neural
Computation, 15:2683?2703.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2011. Online Learning via
Dynamic Reranking for Computer Assisted Transla-
tion. In Proceedings of the 12th international con-
ference on Computational linguistics and intelligent
text processing - Volume Part II, CICLing?11.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193?
3203, September.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online Learning Approaches in Com-
puter Assisted Translation. In Proceedings of the
8
th
Workshop on Statistical Machine Translation,
Sofia, Bulgaria.
719
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Match without a Referee: Evaluating MT
Adequacy without Reference Translations. In Pro-
ceedings of the 7
th
Workshop on Statistical Machine
Translation, pages 171?180, Montr?eal, Canada.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ?10, pages
546?554, Stroudsburg, PA, USA.
Francesco Parrella. 2007. Online support vector re-
gression. Master?s Thesis, Department of Informa-
tion Science, University of Genoa, Italy.
Raphael Rubino, Jos?e G.C. de Souza, Jennifer Fos-
ter, and Lucia Specia. 2013a. Topic Models for
Translation Quality Estimation for Gisting Purposes.
In Proceedings of the Machine Translation Summit
XIV, Nice, France.
Raphael Rubino, Antonio Toral, S Cort?es Va??llo, Jun
Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.
2013b. The CNGL-DCU-Prompsit translation sys-
tems for WMT13. In Proceedings of the 8
th
Work-
shop on Statistical Machine Translation, pages 211?
216, Sofia, Bulgaria.
Kashif Shah, Marco Turchi, and Lucia Specia. 2014.
An Efficient and User-friendly Tool for Machine
Translation Quality Estimation. In Proceedings of
the 9
t
h International Conference on Language Re-
sources and Evaluation, Reykjavik, Iceland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231, Cambridge,
Massachusetts, USA.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the 7
th
Workshop on Statistical Machine Translation
(WMT?12), pages 145?151, Montr?eal, Canada.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13
th
Annual Con-
ference of the European Association for Machine
Translation (EAMT?09), pages 28?35, Barcelona,
Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine translation, 24(1):39?50.
Lucia Specia, Kashif Shah, Jos?e G.C. de Souza, and
Trevor Cohn. 2013. QuEst - A Translation Qual-
ity Estimation Framework. In Proceedings of the
51
st
Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, ACL-
2013, pages 79?84, Sofia, Bulgaria.
Marco Turchi and Matteo Negri. 2014. Automatic An-
notation of Machine Translation Datasets with Bi-
nary Quality Judgements. In Proceedings of the 9
th
International Conference on Language Resources
and Evaluation, Reykjavik, Iceland.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the Subjectivity of Human
Judgements in MT Quality Estimation. In Proceed-
ings of the 8
th
Workshop on Statistical Machine
Translation, pages 240?251, Sofia, Bulgaria.
Alexander Yeh. 2000. More Accurate Tests for the
Statistical Significance of Result Differences. In
Proceedings of the 18th conference on Computa-
tional linguistics (COLING 2000) - Volume 2, pages
947?953, Saarbrucken, Germany.
720
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 128?132, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ALTN: Word Alignment Features for Cross-lingual Textual Entailment
Marco Turchi and Matteo Negri
Fondazione Bruno Kessler
Trento, Italy
{turchi,negri}@fbk.eu
Abstract
We present a supervised learning approach to
cross-lingual textual entailment that explores
statistical word alignment models to predict
entailment relations between sentences writ-
ten in different languages. Our approach
is language independent, and was used to
participate in the CLTE task (Task#8) or-
ganized within Semeval 2013 (Negri et al,
2013). The four runs submitted, one for
each language combination covered by the test
data (i.e. Spanish/English, German/English,
French/English and Italian/English), achieved
encouraging results. In terms of accuracy,
performance ranges from 38.8% (for Ger-
man/English) to 43.2% (for Italian/English).
On the Italian/English and Spanish/English
test sets our systems ranked second among
five participants, close to the top results (re-
spectively 43.4% and 45.4%).
1 Introduction
Cross-lingual textual entailment (CLTE) is an ex-
tension of the Textual Entailment task (Dagan and
Glickman, 2004) that consists in deciding, given
two texts T and H written in different languages
(respectively called text and hypothesis), if H can
be inferred from T (Mehdad et al, 2010). In the
case of SemEval 2013, the task is formulated as
a multi-class classification problem in which there
are four possible relations between T and H: for-
ward (T ? H), backward (T ? H), bidirectional
(T ? H) and ?no entailment?.
Targeting the identification of semantic equiva-
lence and information disparity between topically
related sentences, CLTE recognition can be seen as a
core task for a number of cross-lingual applications.
Among others, multilingual content synchronization
has been recently proposed as an ideal framework
for the exploitation of CLTE components and the in-
tegration of semantics and machine translation (MT)
technology (Mehdad et al, 2011; Mehdad et al,
2012b; Bronner et al, 2012; Monz et al, 2011).
In the last few years, several methods have been
proposed for CLTE. These can be roughly divided
in two main groups (Negri et al, 2012): i) those us-
ing a pivoting strategy by translating H into the lan-
guage of T and then using monolingual TE compo-
nents1, and those directly using cross-lingual strate-
gies. Among this second group, several sources of
cross-lingual knowledge have been used, such as
dictionaries (Kouylekov et al, 2012; Perini, 2012),
phrase and paraphrase tables (Mehdad et al, 2012a),
GIZA++ (Och and Ney, 2003) word alignment mod-
els (Wa?schle and Fendrich, 2012), MT of sub-
segments (Espla`-Gomis et al, 2012), or semantic
Wordnets (Castillo, 2011).
In this work we propose a CLTE detection method
based on a new set of features using word align-
ment as a source of cross-lingual knowledge. This
set, which is richer than the one by (Wa?schle and
Fendrich, 2012), is aimed not only at grasping infor-
mation about the proportion of aligned words, but
also about the distribution of the alignments in both
1In the first CLTE evaluation round at Semeval 2012, for
instance, the system described in (Meng et al, 2012) used the
open source EDITS system (Kouylekov and Negri, 2010; Negri
et al, 2009) to calculate similarity scores between monolingual
English pairs.
128
H and T . This set of features is later used by two
support vector machine (SVM) classifiers for detect-
ing CLTE separately in both directions (T ? H and
T ? H). We use the combined output of both clas-
sifiers for performing the CLTE detection.
The paper is organized as follows: Section 2
describes the features used and the classification
method; Section 3 explains the experimental frame-
work and the results obtained for the different
language-pair sets; finally, the conclusions obtained
from the results are summarised in Section 4.
2 ALTN System
In our approach we have implemented a system
based on supervised learning. It takes an unlabeled
sentence pair as input (T and H) and labels it au-
tomatically with one of the possible four valid en-
tailment relations. The architecture is depicted in
Figure 1.
A key component to our approach is the word
alignment model. In a preprocessing step it is
trained on a set of parallel texts for the target lan-
guage pair. Next, different features based on the
word alignment are extracted. Taking the features
and the target language pair labels as input, a su-
pervised learning algorithm is run to fit a model to
the data. The last step is to use the model to au-
tomatically label unseen instances with entailment
relations.
2.1 Features
What characterizes our submission is the use of
word alignment features to capture entailment rela-
tions. We extract the following features from a word
alignment model for a given sentence pair (all fea-
tures are calculated for both T and H):
? proportion of aligned words in the sentence
(baseline);
? number of unaligned sequences of words nor-
malized by the length of the sentence;
? length of the longest sequence of aligned words
normalized by the length of the sentence;
? length of the longest sequence of unaligned
words normalized by the length of the sentence;
Figure 1: System architecture
? average length of the aligned word sequences;
? average length of the unaligned word se-
quences;
? position of the first unaligned word normalized
by the length of the sentence;
? position of the last unaligned word normalized
by the lenght of the sentence;
? proportion of aligned n-grams in the sentence
(n varying from 1 to 5).
These features are language independent as they
are obtained from statistical models that take as in-
put a parallel corpus. Provided that there exist paral-
lel data for a given language pair, the only constraint
in terms of resources, the adoption of these features
makes our approach virtually portable across lan-
guages with limited effort.
2.2 CLTE Model
Our CLTE model is composed by two supervised bi-
nary classifiers that predict whether there is entail-
ment between the T and H . One classifier checks
129
for forward entailment (T ? H) and the other
checks for backward entailment (T ? H). The out-
put of both classifiers is combined to form the four
valid entailment decisions:
? forward and backward classifier output true:
?bidirectional? entailment;
? forward is true and backward is false:
?forward? entailment;
? forward is false and backward is true:
?backward? entailment;
? both forward and backward output false: ?no
entailment? relation.
Both binary classifiers were implemented using
the SVM implementation of Weka (Hall et al,
2009).
3 Experiments
In our submission we experimented with three stan-
dard word alignment algorithms: the hidden Markov
model (HMM) (Vogel et al, 1996) and IBM models
3 and 4 (Brown et al, 1993). They are implemented
in the MGIZA++ package (Gao and Vogel, 2008).
Building on a probabilistic lexical model to establish
mappings between words in two languages, these
models compute alignments between the word po-
sitions in two input sentences S1 and S2. The mod-
els are trained incrementally: HMM is the base for
IBM model 3, which is the base for IBM model 4.
To train our models, we used 5 iterations of HMM,
and 3 iterations of IBM models 3 and 4.
Word alignments produced by these models are
asymmetric (S1 ? S2 6= S2 ? S1). To cope
with this, different heuristics (Koehn et al, 2005)
have been proposed to obtain symmetric alignments
from two asymmetric sets (S1 ? S2). We ex-
perimented with three symmetrization heuristics,
namely: union, intersection, and grow-diag-final-
and, a more complex symmetrization method which
combines intersection with some alignments from
the union.
To train the word alignment models we used
the Europarl parallel corpus (Koehn, 2005) con-
catenated with the News Commentary corpus2 for
2http://www.statmt.org/wmt11/
translation-task.html#download
three language pairs: English-German (2,079,049
sentences), English-Spanish (2,123,036 sentences),
English-French (2,144,820 sentences). For English-
Italian we only used the parallel data available in Eu-
roparl (1,909,115 sentences) since this language pair
is not covered by the News Commentary corpus.
For our submitted run the SVM classifiers were
trained using the whole training set. Such dataset
consists of 1,000 pairs for each of the four language
combinations, resulting from a concatenation of the
training and test sets used for the first round of eval-
uation at SemEval 2012 (Negri et al, 2012; Negri et
al., 2011). We have set a polynomial kernel with pa-
rameters empirically estimated on the training set:
C = 2.0, and d = 1. After some preliminary ex-
periments we have concluded that the HMM model
in conjunction with the intersection symmetrization
provides the best results.
Our results, calculated over the 500 test pairs pro-
vided for each language combination, are presented
in Table 3. As can be seen from the table, our system
consistently outperforms the best average run of all
participants and is the second best system for Span-
ish/English and Italian/English. For the other two
languages, French/English and German/English, it
is the 3rd best system with a larger distance from top
results. The motivations for such lower results, cur-
rently under investigation, might be related to lower
performance in terms of word alignment, the core
of our approach. The first step of our analysis will
hence address, and in case try to cope with, signifi-
cant differences in word alignment performance af-
fecting results.
Overall, considering the small distance from top
results, and the fact that our approach does not re-
quire deep linguistic processing to be reasonably ef-
fective for any language pair for which parallel cor-
pora are available, our results are encouraging and
motivate further research along such direction.
4 Conclusion
In this paper we presented the participation of the
Fondazione Bruno Kessler in the Semeval 2013
Task#8 on Cross-lingual Textual Entailment for
Content Synchronization. To identify entailment re-
lations between texts in different languages, our sys-
tem explores the use of word alignment features
130
Features / Language pair German/English Spanish/English French/English Italian/English
Avg best runs 0.378 0.404 0.407 0.405
ALTN 0.388 0.428 0.420 0.432
Best system 0.452 0.434 0.458 0.454
Table 1: Accuracy results for the language pairs evaluated for the average of the best runs of the participating systems,
our submission and the best systems.
within a supervised learning setting. In our ap-
proach, word alignment models obtained by statis-
tical methods from parallel corpora leverage infor-
mation about the number, the proportion, and the
distribution of aligned terms in the input sentences.
In terms of accuracy results over the SemEval 2013
CLTE test data, performance ranges from 38.8%
(for German/English) to 43.2% (for Italian/English).
On the Italian/English and Spanish/English test sets
our systems ranked second among five participants,
close to the top results (respectively 43.4% and
45.4%). Such results suggest that the use of word
alignment models to capture sentence-level seman-
tic relations in different language settings represents
a promising research direction.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531).
References
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. CoSyne: Synchro-
nizing Multilingual Wiki Content. In Proceedings of
the Eighth Annual International Symposium on Wikis
and Open Collaboration, WikiSym ?12, pages 33:1?
33:4, New York, NY, USA. ACM.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Julio J. Castillo. 2011. A WordNet-based Semantic Ap-
proach to Textual Entailment and Cross-lingual Tex-
tual Entailment. International Journal of Machine
Learning and Cybernetics, 2(3):177?189.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining, Grenoble, France.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. UAlacant: Using Online
Machine Translation for Cross-Lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), pages
472?476, Montre?al, Canada.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
USA.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: an Update.
SIGKDD Explorations, 11(1):10?18.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of the International Workshop on Spo-
ken Language Translation, Pittsburgh, Pennsylvania,
USA.
Philip Koehn. 2005. Europarl: a Parallel Corpus for
Statistical Machine Translation. In Proceedings of
Machine Translation Summit X, pages 79?86, Phuket,
Thailand.
Milen Kouylekov and Matteo Negri. 2010. An Open-
source Package for Recognizing Textual Entailment.
In Proceedings of the ACL 2010 System Demonstra-
tions.
Milen Kouylekov, Luca Dini, Alessio Bosca, and Marco
Trevisan. 2012. CELI: an Experiment with Cross
Language Textual Entailment. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), pages 696?700, Montre?al, Canada.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
131
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Jose? Guilherme C.
de Souza. 2012a. FBK: cross-lingual textual entail-
ment without translation. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 701?705, Montre?al, Canada.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012b. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Fandong Meng, Hao Xiong, and Qun Liu. 2012. ICT:
A Translation based Cross-lingual Textual Entailment.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Christoph Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
CoSyne: a Framework for Multilingual Content Syn-
chronization of Wikis. In Proceedings of Wikisym
2011, the International Symposium on Wikis and Open
Collaboration, pages 217?218, Mountain View, Cali-
fornia, USA.
Matteo Negri, Milen Ognianov Kouylekov, Bernardo
Magnini, Yashar Mehdad, and Elena Cabrio. 2009.
Towards Extensible Textual Entailment Engines: the
EDITS Package. In AI*IA 2009: XIth International
Conference of the Italian Association for Artificial In-
telligence.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 Task 8: Cross-Lingual Textual Entail-
ment for Content Synchronization. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), pages 399?407, Montre?al,
Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2013.
Semeval-2013 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Alpa?r Perini. 2012. DirRelCond3: detecting textual en-
tailment across languages with conditions on direc-
tional text relatedness scores. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), pages 710?714, Montre?al, Canada.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th International Con-
ference on Computational Linguistics (ACL?96), pages
836?841, Copenhagen, Denmark.
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Features.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), pages 467?471,
Montre?al, Canada.
132
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 19?27,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation for Multilingual Summary Content Evaluation
Josef Steinberger and Marco Turchi
Joint Research Centre,
European Commission,
Via E. Fermi 2749,
21027 Ispra (VA), Italy
[name].[surname]@jrc.ec.europa.eu
Abstract
The multilingual summarization pilot task at
TAC?11 opened a lot of problems we are fac-
ing when we try to evaluate summary qual-
ity in different languages. The additional lan-
guage dimension greatly increases annotation
costs. For the TAC pilot task English arti-
cles were first translated to other 6 languages,
model summaries were written and submit-
ted system summaries were evaluated. We
start with the discussion whether ROUGE can
produce system rankings similar to those re-
ceived from manual summary scoring by mea-
suring their correlation. We study then three
ways of projecting summaries to a different
language: projection through sentence align-
ment in the case of parallel corpora, sim-
ple summary translation and summarizing ma-
chine translated articles. Building such sum-
maries gives opportunity to run additional ex-
periments and reinforce the evaluation. Later,
we investigate whether an evaluation based on
machine translated models can perform close
to an evaluation based on original models.
1 Introduction
Evaluation of automatically produced summaries in
different languages is a challenging problem for the
summarization community, because human efforts
are multiplied to create model summaries for each
language. Unavailability of parallel corpora suitable
for news summarization adds even another annota-
tion load because documents need to be translated to
other languages. At the last TAC?11 campaign, six
research groups spent a lot of work on creating eval-
uation resources in seven languages (Giannakopou-
los et al, 2012). Thus compared to the monolingual
evaluation, which requires writing model summaries
and evaluating outputs of each system by hand, in
the multilingual setting we need to obtain transla-
tions of all documents into the target language, write
model summaries and evaluate the peer summaries
for all the languages.
In the last fifteen years, research on Machine
Translation (MT) has made great strides allowing
human beings to understand documents written in
various languages. Nowadays, on-line services such
as Google Translate and Bing Translator1 can trans-
late text into more than 50 languages showing that
MT is not a pipe-dream.
In this paper we investigate how machine trans-
lation can be plugged in to evaluate quality of sum-
marization systems, which would reduce annotation
efforts. We also discuss projecting summaries to dif-
ferent languages with the aim to reinforce the evalu-
ation procedure (e.g. obtaining additional peers for
comparison in different language or studying their
language-independence).
This paper is structured as follows: after dis-
cussing the related work in section 2, we give a
short overview of the TAC?11 multilingual pilot task
(section 3). We compare average model and system
manual scores and we also study ROUGE correla-
tion to the manual scores. We run our experiments
on a subset of languages of the TAC multilingual
task corpus (English, French and Czech). Section
4 introduces our translation system. We mention its
1http://translate.google.com/ and http://
www.microsofttranslator.com/
19
translation quality for language pairs used later in
this study. Then we move on to the problem of pro-
jecting summaries to different languages in section
5. We discuss three approaches: projecting sum-
mary through sentence alignment in a parallel cor-
pus, translating a summary, and summarizing trans-
lated source texts. Then, we try to answer the ques-
tion whether using translated models produces sim-
ilar system rankings as when using original models
(section 6), accompanied by a discussion of discrim-
inative power difference and cross-language model
comparison.
2 Related work
Attempts of using machine translation in different
natural language processing tasks have not been
popular due to poor quality of translated texts, but
recent advance in Machine Translation has mo-
tivated such attempts. In Information Retrieval,
Savoy and Dolamic (2009) proposed a comparison
between Web searches using monolingual and trans-
lated queries. On average, the results show a limited
drop in performance, around 15% when translated
queries are used.
In cross-language document summarization, Wan
et al (2010) and Boudin et al (2010) combined the
MT quality score with the informativeness score of
each sentence to automatically produce summary in
a target language. In Wan et al (2010), each sen-
tence of the source document is ranked according to
both scores, the summary is extracted and then the
selected sentences translated to the target language.
Differently, in Boudin et al (2010), sentences are
first translated, then ranked and selected. Both ap-
proaches enhance the readability of the generated
summaries without degrading their content.
Automatic evaluation of summaries has been
widely investigated in the past. In the task of
cross-lingual summarization evaluation Saggion et
al. (2002) proposed different metrics to assess the
content quality of a summary. Evaluation of sum-
maries without the use of models has been intro-
duced by Saggion et al (2010). They showed that
substituting models by full document in the com-
putation of the Jensen-Shannon divergence measure
can produce reliable rankings. Yeloglu et al (2011)
concluded that the pyramid method partially re-
flects the manual inspection of the summaries and
ROUGE can only be used when there is a manually
created summary. A method, and related resources,
which allows saving precious annotation time and
that makes the evaluation results across languages
directly comparable was introduced by Turchi et
al. (2010). This approach relies on parallel data and
it is based on the manual selection of the most im-
portant sentences in a cluster of documents from a
sentence-aligned parallel corpus, and by projecting
the sentence selection to various target languages.
Our work addresses the same problem of reducing
annotation time and generating models, but from a
different prospective. Instead of using parallel data
and annotation projection or full documents, we in-
vestigate the use of machine translation at different
level of summary evaluation. While the aproach of
Turchi et al (2010) is focussed on sentence selection
evaluation our strategy can also evaluate generative
summaries, because it works on summary level.
3 TAC?11 Multilingual Pilot
The Multilingual task of TAC?11 (Giannakopoulos
et al, 2012) aimed to evaluate the application of
(partially or fully) language-independent summa-
rization algorithms on a variety of languages. The
task was to generate a representative summary (250
words) of a set of 10 related news articles.
The task included 7 languages (English, Czech,
French, Hebrew, Hindi, Greek and Arabic). Anno-
tation of each language sub-corpus was performed
by a different group. English articles were manu-
ally translated to the target languages, 3 model sum-
maries were written for each topic.
8 groups (systems) participated in the task, how-
ever, not all systems produced summaries for all lan-
guages. In addition there were 2 baselines: Cen-
troid Baseline ? the start of the centroid article and
GA Topline ? summary based on genetic algorithm
using model summary information, which should
serve as an upper bound.
Human annotators scored each summary, both
models and peers, on the 5-to-1 scale (5 = the best, 1
= the worst) ? human grades. The score corresponds
to the overall responsiveness of the main TAC task ?
equal weight of content and readability. 2
2In this article we focus on raw human grades. The task
20
English French Czech average English French Czech average
Manual grades Manual grades
average model 4.06 4.03 4.73 4.27 4.06 4.03 4.73 4.27
average peer 2.73 2.18 2.56 2.50 2.73 2.18 2.56 2.50
ROUGE-2 ROUGE-SU4
average model .194 .222 .206 .207 .235 .255 .237 .242
average peer .139 .167 .182 .163 .183 .207 .211 .200
correlation to manual grading ? peers and models not stemmed
peers only .574 .427 .444 .482 .487 .362 .519 .456
(p-value) (< .1)
models & peers .735 .702 .484 .640 .729 .703 .549 .660
(p-value) (< .01) (< .02) (< .02) (< .02)
correlation to manual grading ? peers and models stemmed
Peers only .573 .445 .500 .506 .484 .336 .563 .461
(p-value) (< .1)
models & peers .744 .711 .520 .658 .723 .700 .636 .686
(p-value) (< .01) (< .01) (< .02) (< .02) (< .1)
Table 1: Average ROUGE-2 and ROUGE-SU4 scores for models and peers, and their correlation to the manual
evaluation (grades). We report levels of significance (p) for two-tailed test. Cells with missing p-values denote non-
significant correlations (p > .1).
3.1 Manual Evaluation
When we look at the manually assigned grades we
see that there is a clear gap between human and au-
tomatic summaries (see the first two rows in table
1). While the average grade for models were always
over 4, peers were graded lower by 33% for English
and by 54% for French and Czech. However, there
were 5 systems for English and 1 system for French
which were not significantly worse than at least one
model.
3.2 ROUGE
The first question is: can an automatic metric rank
the systems similarly as manual evaluation? This
would be very useful when we test different config-
urations of our systems, in which case manual scor-
ing is almost impossible. Another question is: can
the metric distinguish well the gap between mod-
els and peers? ROUGE is widely used because of
its simplicity and its high correlation with manually
assigned content quality scores on overall system
rankings, although per-case correlation is lower.
We investigated how the two most common
ROUGE scores (ROUGE-2 and ROUGE-SU4) cor-
overview paper (Giannakopoulos et al, 2012) discusses, in ad-
dition, scaling down the grades of shorter summaries to avoid
assigning better grades to shorter summaries.
relate with human grades. Although using n-grams
with n greater than 1 gives limited possibility to
reflect readability in the scores when compared to
reference summaries, ROUGE is considered mainly
as a content evaluation metric. Thus we cannot
expect a perfect correlation because half of the
grade assigned by humans reflects readability issues.
ROUGE could not also evaluate properly the base-
lines. The centroid baseline contains a continuous
text (the start of an article) and it thus gets higher
grades by humans because of its good readability,
but from the ROUGE point of view the baseline is
weak. On the other hand, the topline used informa-
tion from models and it is naturally more similar to
them when evaluated by ROUGE. Its low readabil-
ity ranked it lower in the case of human evaluation.
Because of these problems we include in the correla-
tion figures only the submitted systems, neither the
baseline nor the topline.
Table 1 compares average model and peer
ROUGE scores for the three analyzed languages. It
adds two correlations3 to human grades: for mod-
els+systems and for systems only. The first case
should answer the question whether the automatic
metric can distinguish between human and auto-
matic summaries. The second settings could show
3We used the classical Pearson correlation.
21
whether the automatic metric accurately evaluates
the quality of automatic summaries. To ensure a fair
comparison of models and non-models, each model
summary is evaluated against two other models, and
each non-model summary is evaluated three times,
each time against a different couple of models, and
these three scores are averaged out (the jackknif-
ing procedure).4 The difference of the model and
system ROUGE scores is significant, although it is
not that distinctive as in the case of human grades.
The distinction results in higher correlations when
we include models than in the more difficult systems
only case. This is shown by both correlation figures
and their confidence. The only significant correla-
tion for the systems only case was for English and
ROUGE-2. Other correlations did not cross the 90%
confidence level. If we run ROUGE for morpholog-
ically rich languages (e.g. Czech), stemming plays
more important role than in the case of English. In
the case of French, which stands in between, we
found positive effect of stemming only for ROUGE-
2. ROUGE-2 vs. ROUGE-SU4: for English and
French we see better correlation with ROUGE-2 but
the free word ordering in Czech makes ROUGE-
SU4 correlate better.
4 In-house Translator
Our translation service (Turchi et al, 2012) is
based on the most popular class of Statistical Ma-
chine Translation systems (SMT): the Phrase-Based
model (Koehn et al, 2003). It is an extension of
the noisy channel model introduced by Brown et
al. (1993), and uses phrases rather than words. A
source sentence f is segmented into a sequence of
I phrases f I = {f1, f2, . . . fI} and the same is
done for the target sentence e, where the notion of
phrase is not related to any grammatical assumption;
a phrase is an n-gram. The best translation ebest of
f is obtained by:
ebest = arg maxe p(e|f) = arg maxe p(f |e)pLM (e)
4In our experiments we used the same ROUGE settings as at
TAC. The summaries were truncated to 250 words. For English
we used the Porter stemmer included in the ROUGE package,
for Czech the aggressive version from http://members.
unine.ch/jacques.savoy/clef/index.html and
for French http://jcs.mobile-utopia.com/jcs/
19941\_FrenchStemmer.java.
= arg max
e
I
?
i=1
?(fi|ei)??d(ai ? bi?1)?d
|e|
?
i=1
pLM(ei|e1 . . . ei?1)?LM
where ?(fi|ei) is the probability of translating a
phrase ei into a phrase fi. d(ai ? bi?1) is the
distance-based reordering model that drives the sys-
tem to penalize significant word reordering during
translation, while allowing some flexibility. In the
reordering model, ai denotes the start position of
the source phrase that is translated into the ith tar-
get phrase, and bi?1 denotes the end position of
the source phrase translated into the (i ? 1)th target
phrase. pLM (ei|e1 . . . ei?1) is the language model
probability that is based on the Markov?s chain as-
sumption. It assigns a higher probability to flu-
ent/grammatical sentences. ??, ?LM and ?d are
used to give a different weight to each element. For
more details see (Koehn et al, 2003). In this work
we use the open-source toolkit Moses (Koehn et al,
2007).
Furthermore, our system takes advantage of a
large in-house database of multi-lingual named and
geographical entities. Each entity is identified in the
source language and its translation is suggested to
the SMT system. This solution avoids the wrong
translation of those words which are part of a named
entity and also common words in the source lan-
guage, (e.g. ?Bruno Le Maire? which can be
wrongly translated to ?Bruno Mayor?), and enlarges
the source language coverage.
We built four models covering the following lan-
guage pairs: En-Fr, En-Cz, Fr-En and Cz-En. To
train them we use the freely available corpora: Eu-
roparl (Koehn, 2005), JRC-Acquis (Steinberger et
al., 2006), CzEng0.9 (Bojar and ?Zabokrtsky?, 2009),
Opus (Tiedemann, 2009), DGT-TM5 and News Cor-
pus (Callison-Burch et al, 2010), which results
in more than 4 million sentence pairs for each
model. Our system was tested on the News test set
(Callison-Burch et al, 2010) released by the orga-
nizers of the 2010 Workshop on Statistical Machine
Translation. Performance was evaluated using the
Bleu score (Papineni et al, 2002): En-Fr 0.23, En-
Cz 0.14, Fr-En 0.26 and Cz-En 0.22. The Czech
5http://langtech.jrc.it/DGT-TM.html
22
language is clearly more challenging than French for
the SMT system, this is due to the rich morphology
and the partial free word order. These aspects are
more evident when we translate to Czech, for which
we have poor results.
5 Mapping Peers to Other Languages
When we want to generate a summary of a set of ar-
ticles in a different language we have different pos-
sibilities. The first case is when we have articles in
the target language and we run our summarizer on
them. This was done in the Multilingual TAC task.
If we have parallel corpora we can take advantage of
projecting a sentence-extractive summary from one
language to another (see Section 5.1).
If we do not have the target language articles we
can apply machine translation to get them and run
the summarizer on them (see Section 5.3). If we
miss a crucial resource for running the summarizer
for the target language we can simply translate the
summaries (see Section 5.2).
In the case of the TAC Multilingual scenario these
projections can also give us summaries for all lan-
guages from the systems which were applied only
on some languages.
5.1 Aligned Summaries
Having a sentence-aligned (parallel) corpus gives
access to additional experiments. Because the cur-
rent trend is still on the side of pure sentence extrac-
tion we can investigate whether the systems select
the same sentences across the languages. While cre-
ating the TAC corpus each research group translated
the English articles into their language, thus the re-
sulting corpus was close to be parallel. However,
sentences are not always aligned one-to-one because
a translator may decide, for stylistic or other reasons,
to split a sentence into two or to combine two sen-
tences into one. Translations and original texts are
never perfect, so that it is also possible that the trans-
lator accidentally omits or adds some information,
or even a whole sentence. For these reasons, align-
ers such as Vanilla6, which implements the Gale and
Church algorithm (Gale and Church, 1994), typi-
cally also allow two-to-one, one-to-two, zero-to-one
and one-to-zero sentence alignments. Alignments
6http://nl.ijs.si/telri/Vanilla/
other than one-to-one thus present a challenge for
the method of aligning two text, in particular one-
to-two and two-to-one alignments. We used Vanilla
to align Czech and English article sentences, but be-
cause of high error rate we corrected the alignment
by hand.
The English summaries were then aligned to
Czech (and the opposite direction as well) accord-
ing to the following approach. Sentences in a source
language system summary were split. For each sen-
tence we found the most similar sentence in the
source language articles based on 3-gram overlap.
The alignment information was used to select sen-
tences for the target language summary. Some sim-
plification rules were applied: if the most similar
sentence found in the source articles was aligned
with more sentences in the target language articles,
all the projected sentences were selected (one-to-two
alignment); if the sentence to be projected covered
only a part of sentences aligned with one target lan-
guage sentence, the target language sentence was se-
lected (two-to-one alignment).
The 4th row in table 2 shows average peer
ROUGE scores of aligned summaries.7 When com-
paring the scores to the peers in original language
(3rd row) we notice that the average peer score is
slightly better in the case of English (cz?en projec-
tion) and significantly worse for Czech (en?cz pro-
jection) indicating that Czech summaries were more
similar to English models than English summaries
to Czech models.
Having the alignment we can study the overlap
of the same sentences selected by a summarizer in
different languages. The peer average for the en-
cz language pair was 31%, meaning that only a bit
less than one third of sentences was selected both to
English and Czech summaries by the same system.
The percentage differed a lot from a summarizer to
another one, from 13% to 57%. This number can be
seen as an indicator of summarizer?s language inde-
pendence.
However, the system rankings of aligned sum-
maries did not correlate well with human grades.
There are many inaccuracies in the alignment sum-
mary creation process. At first, finding the sentence
7Models are usually not sentence-extractive and thus align-
ing them would not make much sense.
23
ROUGE-2 ROUGE-SU4
fr?en cz?en en?fr en?cz avg. fr?en cz?en en?fr en?cz avg.
average ROUGE scores
orig. model .194 .194 .222 .206 .207 .235 .235 .255 .237 .242
transl. model .128 .162 .187 .123 .150 .184 .217 .190 .160 .188
orig. peer .139 .139 .167 .182 .163 .183 .183 .207 .211 .200
aligned peer .148 .146 .147 .175 .140 .180
transl. peer .100 .119 .128 .102 .112 .155 .174 .179 .140 .162
correlation to source language manual grading for translated summaries
peers only .411 .483 .746 .456 .524 .233 .577 .754 .571 .534
(p-value) (< .05) (< .05)
models & peers .622 .717 .835 .586 .690 .581 .777 .839 .620 .704
(p-value) (< .05) (< .05) (< .01) (< .1) (< .05) (< .02) (< .01) (< .05)
correlation to target language manual grading for translated summaries
peers only .685 .708 .555 .163 .528 .516 .754 .529 .267 .517
(p-value) (< .1)
Table 2: ROUGE results of translated summaries, evaluated against target language models (e.g., cz?en against
English models).
in the source data that was probably extracted is
strongly dependent on the sentence splitting each
summarizer used. At second, alignment relations
different from one-to-one results in selecting con-
tent with different length compared to the original
summary. And since ROUGE measures recall, and
truncates the summaries, it introduces another inac-
curacy. There were also relations one-to-zero (sen-
tences not translated to the target language). In that
case no content was added to the target summary.
5.2 Translated Summaries
The simplest way to obtain a summary in a different
language is to apply machine translation software on
summaries. Here we investigate (table 2) whether
machine translation errors affect the system order
by correlation to human grades again. In this case
we have two reference human grade sets: one for
the source language (from which we translate) and
one for the target language (to which we translate).
Since there were different models for each language
we can include models only in computing the corre-
lation against source language manual grading.
At first, we can see that ROUGE scores are af-
fected by the translation errors. Average model
ROUGE-2 score went down by 28% and average
peer ROUGE-2 by 31%. ROUGE-SU4 seems to be
more robust to deal with the translation errors: mod-
els went down by 21%, peers by 19%. The gap be-
tween models and peers is still distinguishable, sys-
tem ranking correlation to human grades holds sim-
ilar levels although less statistically significant cor-
relations can be seen. Clearly, quality of the trans-
lator affects these results because our worst transla-
tor (en?cz) produced the worst summaries. Cor-
relation to the source language manual grades in-
dicates how the ranking of the summarizers is af-
fected (changed) by translation errors. For exam-
ple it compares ranking for English based on man-
ual grades with ranking computed on the same sum-
maries translated from English to French. The sec-
ond scenario (correlation to target language scores)
shows how similar is the ranking of summarizers
based on translated summaries with the target lan-
guage ranking based on original summaries. If we
omit translation inaccuracies, low correlation in the
latter case indicates qualitatively different output of
participating peers (e.g. en and cz summaries).
5.3 Summarizing Translated Articles
To complete the figure we tested the configuration
in which we first translate the full articles to the
target language and then apply a summarizer. As
we have at disposal an implementation of system
3 from the TAC multilingual task we used it on 4
translated document sets (en?cz, cz?en, fr?en,
en?fr). This system was the best according to hu-
man grades in all three discussed languages.
24
method ROUGE-2 ROUGE-SU4
en .177 .209
cz ? en alignment .200 .235
cz ? en translation .142 .194
en from (cz ? en source translation) .132 .181
fr ? en translation .120 .172
en from (fr ? en source translation) .129 .185
fr .214 .241
en ? fr translation .167 .212
fr from (en ? fr source translation) .156 .202
cz .204 .225
en ? cz alignment .176 .196
en ? cz translation .115 .150
cz from (en ? cz source translation) .138 .178
Table 3: ROUGE results of different variants of summaries produced by system 3. The first line shows the ROUGE
scores of the original English summaries submitted by system 3. The second line gives average scores of the cz?en
aligned summaries (see Section 5.1), in the 3rd and 5th lines there are figures of cz?en and fr?en translated sum-
maries, and 4th and 6th lines show scores when the summarizer was applied on translated source texts (cz?en and
fr?en). Similarly, lines further down show performance for French and Czech.
The system is based on the latent semantic anal-
ysis framework originally proposed by Gong and
Liu (2002) and later improved by J. Steinberger
and Jez?ek (2004). It first builds a term-by-sentence
matrix from the source articles, then applies Singu-
lar Value Decomposition (SVD) and finally uses the
resulting matrices to identify and extract the most
salient sentences. SVD finds the latent (orthogonal)
dimensions, which in simple terms correspond to the
different topics discussed in the source (for details
see (Steinberger et al, 2011)).
Table 3 shows all results of summaries generated
by the summarizer. The first part compares English
summaries. We see that when projecting the sum-
mary through alignment from Czech, see Section
5.1, a better summary was obtained. When using
translation the summaries are always significantly
worse compared to original (TAC) summaries, with
the lowest performing en?cz translation. It is in-
teresting that in the case of this low-performing
translator it was significantly better to translate the
source articles and to use the summarizer afterwards.
The advantage of this configuration is that the core
of the summarizer (LSA) treats all terms the same
way, thus even English terms that were not trans-
lated work well for sentence selection. On the other
hand, when translating the summary ROUGE will
not match the English terms in Czech models.
6 Using Translated Models
With growing number of languages the annotation
effort rises (manual creation of model summaries).
Now we investigate whether we can produce models
in one pivot language (e.g., English) and translate
them automatically to all other languages. The fact
that in the TAC corpus we have manual summaries
for each language gives us opportunity to reinforce
the evaluation by translating all model summaries
to a common language and thus obtaining a larger
number of models. This way we can also evaluate
similarity among models coming from different lan-
guages and it lowers the annotators? subjectivity.
6.1 Evaluation Against Translated Models
Table 4 shows ROUGE figures when peers were
evaluated against translated models. We discuss also
the case when English peer summaries (and mod-
els as well) are evaluated against both French and
Czech models translated to English. We can see
again lower ROUGE scores caused by translation er-
rors, however, there is more or less the same gap
between peers and models and the correlation holds
similar levels as when using the original target lan-
guage models. Exceptions are using English models
translated to French and Czech models translated to
English in combination with the systems only cor-
relation. If we used both French and Czech mod-
25
ROUGE-2 ROUGE-SU4
peers from en fr cz avg. en fr cz avg.
models tr. from fr cz fr / cz en en fr cz fr / cz en en
average model .144 .167 .155 .165 .144 .155 .207 .221 .206 .215 .190 .208
average peer .110 .111 .104 .135 .125 .117 .170 .162 .153 .186 .172 .169
correlation to target language manual grading
peers only .639 .238 .424 .267 .541 .422 .525 .136 .339 .100 .624 .345
(p-value) < .1
models & peers .818 .717 .782 .614 .520 .690 .785 .692 .759 .559 .651 .793
(p-value) < .01 < .02 < .01 < .05 < .01 < .02 < .01 < .1 < .1
Table 4: ROUGE results of using translated model summaries, which evaluate both peer and model summaries in the
particular language.
els translated to English, higher correlation of En-
glish peers with translated French models was av-
eraged out by lower correlation with Czech models.
And because the TAC Multilingual task contained 7
languages the experiment can be extended to using
translated models from 6 languages. However, our
results rather indicate that using the best translator is
better choice.
Given the small scale of the experiment we cannot
draw strong conclusions on discriminative power8
when using translated models. However, our ex-
periments indicate that by using translated sum-
maries we are partly loosing discriminative power
(i.e. ROUGE finds fewer significant differences be-
tween systems).
6.2 Comparing Models Across Languages
By translating both Czech and French models to
English we could compare all models against each
other. For each topic we had 9 models: 3 original
English models, 3 translated from French and 3 from
Czech. In this case we reached slightly better cor-
relations for the models+systems case: ROUGE-2:
.790, ROUGE-SU4: .762. It was mainly because of
the fact that this time also models only rankings from
ROUGE correlated with human grades (ROUGE-2:
.475, ROUGE-SU4: .445). When we used only En-
glish models, the models ranking did not correlate at
all (? -0.1). Basically, one English model was less
similar to the other two, but it did not mean that it
was worse which was shown by adding models from
8Discriminative power measures how successful the auto-
matic measure is in finding the same significant differences be-
tween systems as manual evaluation.
other languages. If we do not have enough reference
summaries this could be a way to lower subjectivity
in the evaluation process.
7 Conclusion
In this paper we discuss the synergy between ma-
chine translation and multilingual summarization
evaluation. We show how MT can be used to obtain
both peer and model evaluation data.
Summarization evaluation mostly aims to achieve
two main goals a) to identify the absolute perfor-
mance of each system and b) to rank all the sys-
tems according to their performances. Our results
show that the use of translated summaries or mod-
els does not alter much the overall system ranking.
It maintains a fair correlation with the source lan-
guage ranking although without statistical signifi-
cance in most of the systems only cases given the
limited data set. A drop in ROUGE score is evident,
and it strongly depends on the translation perfor-
mance. The use of aligned summaries, which lim-
its the drop, requires high quality parallel data and
alignments, which are not always available and have
a significant cost to be created.
The study leaves many opened questions: What
is the required translation quality which would let
us substitute target language models? Are transla-
tion errors averaged out when using translated mod-
els from more languages? Can we add a new lan-
guage to the TAC multilingual corpus just by using
MT having in mind lower quality (? lower scores)
and being able to quantify the drop? Experiment-
ing with a larger evaluation set could try to find the
answers.
26
References
O. Bojar and Z. ?Zabokrtsky?. 2009. CzEng0.9: Large Par-
allel Treebank with Rich Annotation. Prague Bulletin
of Mathematical Linguistics, 92. in print.
F. Boudin, S. Huet, J.M. Torres-Moreno, and J.M. Torres-
Moreno. 2010. A graph-based approach to cross-
language multi-document summarization. Research
journal on Computer science and computer engineer-
ing with applications (Polibits), 43:113?118.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational linguis-
tics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 17?53. Associa-
tion for Computational Linguistics.
W.A. Gale and K.W. Church. 1994. A program for align-
ing sentences in bilingual corpora. Computational lin-
guistics, 19.
G. Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,
J. Steinberger, and V. Varma. 2012. Tac 2011 multil-
ing pilot overview. In Proceedings of TAC?11. NIST.
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analysis.
In Proceedings of ACM SIGIR, New Orleans, US.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology-Volume 1, pages 48?54. Asso-
ciation for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180. Association for Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of the MT
summit, volume 5.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
H. Saggion, D. Radev, S. Teufel, W. Lam, and S.M.
Strassel. 2002. Developing infrastructure for the eval-
uation of single and multi-document summarization
systems in a cross-lingual environment. In Proceed-
ings of LREC 2002, pages 747?754.
H. Saggion, J.M. Torres-Moreno, I. Cunha, and E. San-
Juan. 2010. Multilingual summarization evaluation
without human models. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, pages 1059?1067. Association for Computa-
tional Linguistics.
J. Savoy and L. Dolamic. 2009. How effective is
google?s translation service in search? Communica-
tions of the ACM, 52(10):139?143.
J. Steinberger and K. Jez?ek. 2004. Text summarization
and singular value decomposition. In Proceedings of
the 3rd ADVIS conference, Izmir, Turkey.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, D. Tufis, and D. Varga. 2006. The jrc-acquis:
A multilingual aligned parallel corpus with 20+ lan-
guages. Arxiv preprint cs/0609058.
J. Steinberger, M. Kabadjov, R. Steinberger, H. Tanev,
M. Turchi, and V. Zavarella. 2011. Jrcs participation
at tac 2011: Guided and multilingual summarization
tasks. In Proceedings of the Text Analysis Conference
(TAC).
J. Tiedemann. 2009. News from opus-a collection of
multilingual parallel corpora with tools and interfaces.
In Proceedings of the Recent Advances in Natural Lan-
guage Processing Conference, volume 5, pages 237?
248. John Benjamins Amsterdam.
M. Turchi, J. Steinberger, M. Kabadjov, and R. Stein-
berger. 2010. Using parallel corpora for multilin-
gual (multi-document) summarisation evaluation. In
Proceedings of the Multilingual and Multimodal Infor-
mation Access Evaluation Conference, pages 52?63.
Springer.
M. Turchi, M. Atkinson, A. Wilcox, B. Crawley,
S. Bucci, R. Steinberger, and E. Van der Goot. 2012.
Onts:optima news translation system. In Proceedings
of EACL 2012, page 25.
X. Wan, H. Li, and J. Xiao. 2010. Cross-language
document summarization based on machine transla-
tion quality prediction. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 917?926. Association for Computa-
tional Linguistics.
O. Yeloglu, E. Milios, and N. Zincir-Heywood. 2011.
Multi-document summarization of scientific corpora.
In Proceedings of the 2011 ACM Symposium on Ap-
plied Computing, pages 252?258. ACM.
27
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 52?60,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Multilingual Sentiment Analysis using Machine Translation?
Alexandra Balahur and Marco Turchi
European Commission Joint Research Centre
Institute for the Protection and Security of the Citizen
Via E. Fermi 2749, Ispra, Italy
alexandra.balahur, marco.turchi@jrc.ec.europa.eu
Abstract
The past years have shown a steady growth
in interest in the Natural Language Process-
ing task of sentiment analysis. The research
community in this field has actively proposed
and improved methods to detect and classify
the opinions and sentiments expressed in dif-
ferent types of text - from traditional press ar-
ticles, to blogs, reviews, fora or tweets. A less
explored aspect has remained, however, the
issue of dealing with sentiment expressed in
texts in languages other than English. To this
aim, the present article deals with the prob-
lem of sentiment detection in three different
languages - French, German and Spanish - us-
ing three distinct Machine Translation (MT)
systems - Bing, Google and Moses. Our ex-
tensive evaluation scenarios show that SMT
systems are mature enough to be reliably em-
ployed to obtain training data for languages
other than English and that sentiment analysis
systems can obtain comparable performances
to the one obtained for English.
1 Introduction
Together with the increase in the access to tech-
nology and the Internet, the past years have shown
a steady growth of the volume of user-generated
contents on the Web. The diversity of topics cov-
ered by this data (mostly containing subjective and
opinionated content) in the new textual types such
as blogs, fora, microblogs, has been proven to be
of tremendous value to a whole range of applica-
tions, in Economics, Social Science, Political Sci-
ence, Marketing, to mention just a few. Notwith-
standing these proven advantages, the high quan-
tity of user-generated contents makes this informa-
tion hard to access and employ without the use of
automatic mechanisms. This issue motivated the
rapid and steady growth in interest from the Natural
Language Processing (NLP) community to develop
computational methods to analyze subjectivity and
sentiment in text. Different methods have been pro-
posed to deal with these phenomena for the distinct
types of text and domains, reaching satisfactory lev-
els of performance for English. Nevertheless, for
certain applications, such as news monitoring, the
information in languages other than English is also
highly relevant and cannot be disregarded. Addi-
tionally, systems dealing with sentiment analysis in
the context of monitoring must be reliable and per-
form at similar levels as the ones implemented for
English.
Although the most obvious solution to these is-
sues of multilingual sentiment analysis would be to
use machine translation systems, researchers in sen-
timent analysis have been reluctant to using such
technologies due to the low performance they used
to have. However, in the past years, the performance
of Machine Translation systems has steadily im-
proved. Open access solutions (e.g. Google Trans-
late1, Bing Translator2) offer more and more accu-
rate translations for frequently used languages.
Bearing these thoughts in mind, in this article
we study the manner in which sentiment analysis
can be done for languages other than English, using
Machine Translation. In particular, we will study
1http://translate.google.it/
2http://www.microsofttranslator.com/
52
this issue in three languages - French, German and
Spanish - using three different Machine Translation
systems - Google Translate, Bing Translator and
Moses (Koehn et al, 2007).
We employ these systems to obtain training and
test data for these three languages and subsequently
extract features that we employ to build machine
learning models using Support Vector Machines Se-
quential Minimal Optimization. We additionally
employ meta-classifiers to test the possibility to min-
imize the impact of noise (incorrect translations) in
the obtained data.
Our experiments show that machine translation
systems are mature enough to be employed for mul-
tilingual sentiment analysis and that for some lan-
guages (for which the translation quality is high
enough) the performance that can be attained is sim-
ilar to that of systems implemented for English.
2 Related Work
Most of the research in subjectivity and sentiment
analysis was done for English. However, there were
some authors who developed methods for the map-
ping of subjectivity lexicons to other languages. To
this aim, (Kim and Hovy, 2006) use a machine trans-
lation system and subsequently use a subjectivity
analysis system that was developed for English to
create subjectivity analysis resources in other lan-
guages. (Mihalcea et al, 2009) propose a method
to learn multilingual subjective language via cross-
language projections. They use the Opinion Finder
lexicon (Wilson et al, 2005) and use two bilin-
gual English-Romanian dictionaries to translate the
words in the lexicon. Since word ambiguity can ap-
pear (Opinion Finder does not mark word senses),
they filter as correct translations only the most fre-
quent words. The problem of translating multi-word
expressions is solved by translating word-by-word
and filtering those translations that occur at least
three times on the Web. Another approach in obtain-
ing subjectivity lexicons for other languages than
English was explored by Banea et al (Banea et al,
2008b). To this aim, the authors perform three dif-
ferent experiments, obtaining promising results. In
the first one, they automatically translate the anno-
tations of the MPQA corpus and thus obtain subjec-
tivity annotated sentences in Romanian. In the sec-
ond approach, they use the automatically translated
entries in the Opinion Finder lexicon to annotate a
set of sentences in Romanian. In the last experi-
ment, they reverse the direction of translation and
verify the assumption that subjective language can
be translated and thus new subjectivity lexicons can
be obtained for languages with no such resources.
Further on, another approach to building lexicons
for languages with scarce resources is presented by
Banea et al (Banea et al, 2008a). In this research,
the authors apply bootstrapping to build a subjectiv-
ity lexicon for Romanian, starting with a set of seed
subjective entries, using electronic bilingual dictio-
naries and a training set of words. They start with
a set of 60 words pertaining to the categories of
noun, verb, adjective and adverb from the transla-
tions of words in the Opinion Finder lexicon. Trans-
lations are filtered using a measure of similarity to
the original words, based on Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) scores. Yet
another approach to mapping subjectivity lexica to
other languages is proposed by Wan (2009), who
uses co-training to classify un-annotated Chinese re-
views using a corpus of annotated English reviews.
He first translates the English reviews into Chinese
and subsequently back to English. He then performs
co-training using all generated corpora. (Kim et al,
2010) create a number of systems consisting of dif-
ferent subsystems, each classifying the subjectivity
of texts in a different language. They translate a cor-
pus annotated for subjectivity analysis (MPQA), the
subjectivity clues (Opinion finder) lexicon and re-
train a Nave Bayes classifier that is implemented in
the Opinion Finder system using the newly gener-
ated resources for all the languages considered. Fi-
nally, (Banea et al, 2010) translate the MPQA cor-
pus into five other languages (some with a similar
ethimology, others with a very different structure).
Subsequently, they expand the feature space used in
a Nave Bayes classifier using the same data trans-
lated to 2 or 3 other languages. Their conclusion is
that by expanding the feature space with data from
other languages performs almost as well as training
a classifier for just one language on a large set of
training data.
Attempts of using machine translation in differ-
ent natural language processing tasks have not been
widely used due to poor quality of translated texts,
53
but recent advances in Machine Translation have
motivated such attempts. In Information Retrieval,
(Savoy and Dolamic, 2009) proposed a comparison
between Web searches using monolingual and trans-
lated queries. On average, the results show a drop
in performance when translated queries are used,
but it is quite limited, around 15%. For some lan-
guage pairs, the average result obtained is around
10% lower than that of a monolingual search while
for other pairs, the retrieval performance is clearly
lower. In cross-language document summarization,
(Wan et al, 2010; Boudin et al, 2010) combined
the MT quality score with the informativeness score
of each sentence in a set of documents to automat-
ically produce summary in a target language using
a source language texts. In (Wan et al, 2010), each
sentence of the source document is ranked accord-
ing both the scores, the summary is extracted and
then the selected sentences translated to the target
language. Differently, in (Boudin et al, 2010), sen-
tences are first translated, then ranked and selected.
Both approaches enhance the readability of the gen-
erated summaries without degrading their content.
3 Motivation and Contribution
The main motivation for the experiments we present
in this article is the known lack of resources and ap-
proaches for sentiment analysos in languages other
than English. Although, as we have seen in the
Related Work section, a few attempts were made
to build systems that deal with sentiment analysis
in other languages, they mostly employed bilingual
dictionaries and used unsupervised approaches. The
very few that employed supervised learning using
translated data have, in change, concentrated only
on the issue of sentiment classification and have dis-
regarded the impact of the translation quality and
the difference that the use of distinct translation sys-
tems can make in this settings. Moreover, such ap-
proaches have usually employed only simple ma-
chine learning algorithms. No attempt has been
made to study the use of meta-classifiers to enhance
the performance of the classification through the re-
moval of noise in the data.
Our main contribution in this article is the com-
parative study of multilingual sentiment analysis
performance using distinct machine translation sys-
tems, with varying levels of translation quality. In
this sense, we employ three different systems - Bing
Translator, Google Translate and Moses to translate
data from English to three languages - French, Ger-
man and Spanish. We subsequently study the perfor-
mance of classifying sentiment from the translated
data and different methods to minimize the effect of
noise in the data.
Our comparative results show, on the one hand,
that machine translation can be reliably used for
multilingual sentiment analysis and, on the other
hand, which are the main characteristics of the data
for such approaches to be successfully employed.
4 Dataset Presentation and Analysis
For our experiments, we employed the data provided
for English in the NTCIR 8 Multilingual Opinion
Analysis Task (MOAT)3. In this task, the organiz-
ers provided the participants with a set of 20 top-
ics (questions) and a set of documents in which sen-
tences relevant to these questions could be found,
taken from the New York Times Text (2002-2005)
corpus. The documents were given in two differ-
ent forms, which had to be used correspondingly,
depending on the task to which they participated.
The first variant contained the documents split into
sentences (6165 in total) and had to be used for
the task of opinionatedness, relevance and answer-
ness. In the second form, the sentences were also
split into opinion units (6223 in total) for the opin-
ion polarity and the opinion holder and target tasks.
For each of the sentences, the participants had to
provide judgments on the opinionatedness (whether
they contained opinions), relevance (whether they
are relevant to the topic). For the task of polar-
ity classification, the participants had to employ the
dataset containing the sentences that were also split
into opinion units (i.e. one sentences could contain
two/more opinions, on two/more different targets or
from two/more different opinion holders).
For our experiments, we employed the latter rep-
resentation. From this set, we randomly chose 600
opinion units, to serve as test set. The rest of opin-
ion units will be employed as training set. Subse-
quently, we employed the Google Translate, Bing
3http://research.nii.ac.jp/ntcir/ntcir-
ws8/permission/ntcir8xinhua-nyt-moat.html
54
Translator and Moses systems to translate, on the
one hand, the training set and on the other hand
the test set, to French, German and Spanish. Ad-
ditionally, we employed the Yahoo system to trans-
late only the test set into these three languages. Fur-
ther on, this translation of the test set by the Yahoo
service has been corrected by a person for all the
languages. This corrected data serves as Gold Stan-
dard4. Most of these sentences, however, contained
no opinion (were neutral). Due to the fact that the
neutral examples are majoritary and can produce a
large bias when classifying, we decided to eliminate
these examples and employ only the positive and
negative sentences in both the training, as well as
the test sets. After this elimination, the training set
contains 943 examples (333 positive and 610 nega-
tive) and the test set and Gold Standard contain 357
examples (107 positive and 250 negative).
5 Machine Translation
During the 1990?s the research community on Ma-
chine Translation proposed a new approach that
made use of statistical tools based on a noisy chan-
nel model originally developed for speech recogni-
tion (Brown et al, 1994). In the simplest form, Sta-
tistical Machine Translation (SMT) can be formu-
lated as follows. Given a source sentence written
in a foreign language f , the Bayes rule is applied
to reformulate the probability of translating f into a
sentence e written in a target language:
ebest = argmax
e
p(e|f) = argmax
e
p(f |e)pLM (e)
where p(f |e) is the probability of translating e to f
and pLM (e) is the probability of producing a fluent
sentence e. For a full description of the model see
(Koehn, 2010).
The noisy channel model was extended in differ-
ent directions. In this work, we analyse the most
popular class of SMT systems: PBSMT. It is an ex-
tension of the noisy channel model using phrases
rather than words. A source sentence f is segmented
4Please note that each sentence may contain more than one
opinion unit. In order to ensure a contextual translation, we
translated the whole sentences, not the opinion units separately.
In the end, we eliminate duplicates of sentences (due to the fact
that they contained multiple opinion units), resulting in around
400 sentences in the test and Gold Standard sets and 5700 sen-
tences in the training set
into a sequence of I phrases f I = {f1, f2, . . . fI}
and the same is done for the target sentence e, where
the notion of phrase is not related to any grammat-
ical assumption; a phrase is an n-gram. The best
translation ebest of f is obtained by:
ebest = argmax
e
p(e|f) = argmax
e
p(f |e)pLM (e)
= argmax
e
I?
i=1
?(fi|ei)
??d(ai ? bi?1)
?d
|e|?
i=1
pLM (ei|e1 . . . ei?1)
?LM
where ?(fi|ei) is the probability of translating a
phrase ei into a phrase fi. d(ai ? bi?1) is the
distance-based reordering model that drives the sys-
tem to penalise significant reorderings of words dur-
ing translation, while allowing some flexibility. In
the reordering model, ai denotes the start position
of the source phrase that is translated into the ith
target phrase, and bi?1 denotes the end position of
the source phrase translated into the (i ? 1)th target
phrase. pLM (ei|e1 . . . ei?1) is the language model
probability that is based on the Markov?s chain as-
sumption. It assigns a higher probability to flu-
ent/grammatical sentences. ??, ?LM and ?d are
used to give a different weight to each element. For
more details see (Koehn et al, 2003).
Three different SMT systems were used to trans-
late the human annotated sentences: two existing
online services such as Google Translate and Bing
Translator5 and an instance of the open source
phrase-based statistical machine translation toolkit
Moses (Koehn et al, 2007).
To train our models based on Moses we used the
freely available corpora: Europarl (Koehn, 2005),
JRC-Acquis (Steinberger et al, 2006), Opus (Tiede-
mann, 2009), News Corpus (Callison-Burch et al,
2009). This results in 2.7 million sentence pairs for
English-French, 3.8 for German and 4.1 for Span-
ish. All the modes are optimized running the MERT
algorithm (Och, 2003) on the development part of
the News Corpus. The translated sentences are re-
cased and detokonized (for more details on the sys-
tem, please see (Turchi et al, 2012).
5http://translate.google.com/ and http://
www.microsofttranslator.com/
55
Performances of a SMT system are automati-
cally evaluated comparing the output of the system
against human produced translations. Bleu score
(Papineni et al, 2001) is the most used metric and it
is based on averaging n-gram precisions, combined
with a length penalty which penalizes short transla-
tions containing only sure words. It ranges between
0 and 1, and larger value identifies better translation.
6 Sentiment Analysis
In the field of sentiment analysis, most work has
concentrated on creating and evaluating methods,
tools and resources to discover whether a specific
?target?or ?object? (person, product, organization,
event, etc.) is ?regarded? in a positive or negative
manner by a specific ?holder? or ?source? (i.e. a per-
son, an organization, a community, people in gen-
eral, etc.). This task has been given many names,
from opinion mining, to sentiment analysis, review
mining, attitude analysis, appraisal extraction and
many others.
The issue of extracting and classifying sentiment
in text has been approached using different methods,
depending on the type of text, the domain and the
language considered. Broadly speaking, the meth-
ods employed can be classified into unsupervised
(knowledge-based), supervised and semi-supervised
methods. The first usually employ lexica or dictio-
naries of words with associated polarities (and val-
ues - e.g. 1, -1) and a set of rules to compute the
final result. The second category of approaches em-
ploy statistical methods to learn classification mod-
els from training data, based on which the test data
is then classified. Finally, semi-supervised methods
employ knowledge-based approaches to classify an
initial set of examples, after which they use different
machine learning methods to bootstrap new training
examples, which they subsequently use with super-
vised methods.
The main issue with the first approach is that ob-
taining large-enough lexica to deal with the vari-
ability of language is very expensive (if it is done
manually) and generally not reliable (if it is done
automatically). Additionally, the main problem of
such approaches is that words outside contexts are
highly ambiguous. Semi-supervised approaches, on
the other hand, highly depend on the performance of
the initial set of examples that is classified. If we are
to employ machine translation, the errors in translat-
ing this small initial set would have a high negative
impact on the subsequently learned examples. The
challenge of using statistical methods is that they re-
quire training data (e.g. annotated corpora) and that
this data must be reliable (i.e. not contain mistakes
or ?noise?). However, the larger this dataset is, the
less influence the translation errors have.
Since we want to study whether machine transla-
tion can be employed to perform sentiment analy-
sis for different languages, we employed statistical
methods in our experiments. More specifically, we
used Support Vector Machines Sequential Minimal
Optimization (SVM SMO) since the literature in the
field has confirmed it as the most appropriate ma-
chine learning algorithm for this task.
In the case of statistical methods, the most impor-
tant aspect to take into consideration is the manner
in which texts are represented - i.e. the features that
are extracted from it. For our experiments, we repre-
sented the sentences based on the unigrams and the
bigrams that were found in the training data. Al-
though there is an ongoing debate on whether bi-
grams are useful in the context of sentiment classi-
fication, we considered that the quality of the trans-
lation can also be best quantified in the process by
using these features (because they give us a measure
of the translation correctness, both regarding words,
as well as word order). Higher level n-grams, on the
other hand, would only produce more sparse feature
vectors, due to the high language variability and the
mistakes in the traslation.
7 Experiments
In order to test the performance of sentiment classi-
fication when using translated data, we performed a
series of experiments:
? In the first set of experiments, we trained an
SVM SMO classifier on the training data ob-
tained for each language, with each of the three
machine translations, separately (i.e. we gen-
erated a model for each of the languages con-
sidered, for each of the machine translation
systems employed). Subsequently, we tested
the models thus obtained on the correspond-
ing test set (e.g. training on the Spanish train-
56
ing set obtained using Google Translate and
testing on the Spanish test set obtained using
Google Translate) and on the Gold Standard for
the corresponding language (e.g. training on
the Spanish training set obtained using Google
Translate and testing on the Spanish Gold Stan-
dard). Additionally, in order to study the man-
ner in which the noise in the training data can
be removed, we employed two meta-classifiers
- AdaBoost and Bagging (with varying sizes of
the bag).
? In the second set of experiments, we combined
the translated data from all three machine trans-
lation systems for the same language and cre-
ated a model based on the unigram and bigram
features extracted from this data (e.g. we cre-
ated a Spanish training model using the uni-
grams and bigrams present in the training sets
generated by the translation of the training set
to Spanish by Google Translate, Bing Trans-
lator and Moses). We subsequently tested the
performance of the sentiment classification us-
ing the Gold Standard for the corresponding
language, represented using the features of this
model.
Table 1 presents the number of unigram and bi-
gram features employed in each of the cases.
In the following subsections, we present the re-
sults of these experiments.
7.1 Individual Training with Translated Data
In the first experiment, we translated the training
and test data from English to all the three other
languages considered, using each of the three ma-
chine translation systems. Subsequently, we rep-
resented, for each of the languages and translation
systems, the sentences as vectors, whose features
marked the presence/absence (1 or 0) of the uni-
grams and bigrams contained in the corresponding
trainig set (e.g. we obtained the unigrams and bi-
grams in all the sentences in the training set ob-
tained by translating the English training data to
Spanish using Google and subsequently represented
each sentence in this training set, as well as the test
set obtained by translating the test data in English to
Spanish using Google marking the presence of the
unigram and bigram features). In order to test the
approach on the Gold Standard (for each language),
we represented this set using the corresponding un-
igram and bigram features extracted from the cor-
responding training set (for the example given, we
represented each sentence in the Gold Standard by
marking the presence/absence of the unigrams and
bigrams from the training data for Spanish using
Google Translate).
The results of these experiments are presented in
Table 2, in terms of weighted F1 measure.
7.2 Joint Training with Translated Data
In the second set of experiments, we added together
all the translations of the training data obtained for
the same language, with the three different MT sys-
tems. Subsequently, we represented, for each lan-
guage in part, each of the sentences in the joint train-
ing corpus as vectors, whose features represented
the presence/absence of the unigrams and bigrams
contained in this corpus. In order to test the perfor-
mance of the sentiment classification, we employed
the Gold Standard for the corresponding language,
representing each sentence it contains according to
the presence or absence of the unigrams and bigrams
in the corresponding joint training corpus for that
language. Finally, we applied SVM SMO to classify
the sentences according to the polarity of the senti-
ment they contained. Additionally, we applied the
AdaBoost and Bagging meta-classifiers to test the
possibilities to minimize the impact of noise in the
data. The results are presented in Tables 3 and 4,
again, in terms of weighter F1 measure.
Language SMO AdaBoost M1 Bagging
To German 0.565? 0.563? 0.565?
To Spanish 0.419 0.494 0.511
To French 0.25 0.255 0.23
Table 3: For each language, each classifier has been
trained merging the translated data coming form differ-
ent SMT systems, and tested using the Gold Standard.
?Classifier is not able to discriminate between positive
and negative classes, and assigns most of the test points
to one class, and zero to the other.
8 Results and Discussion
Generally speaking, from our experiments using
SVM, we could see that incorrect translations imply
57
Bing Google T. Moses
To German 0.57? 0.572? 0.562?
To Spanish 0.392 0.511 0.448
To French 0.612? 0.571? 0.575?
Table 4: For each language, the SMO classifiers have
been trained merging the translated data coming form dif-
ferent SMT systems, and tested using independently the
translated test sets. ?Classifier is not able to discriminate
between positive and negative classes, and assigns most
of the test points to one class, and zero to the other.
an increment of the features, sparseness and more
difficulties in identifying a hyperplane which sepa-
rates the positive and negative examples in the train-
ing phase. Therefore, a low quality of the translation
leads to a drop in performance, as the features ex-
tracted are not informative enough to allow for the
classifier to learn.
From Table 2, we can see that:
a) There is a small difference between performances
of the sentiment analysis system using the English
and translated data, respectively. In the worst case,
there is a maximum drop of 8 percentages.
b) Adaboost is sensitive to noisy data, and it is
evident in our experiments where in general it does
not modify the SMO performances or there is a
drop. Vice versa, Bagging, reducing the variance
in the estimated models, produces a positive effect
on the performances increasing the F-score. These
improvements are larger using the German data,
this is due to the poor quality of the translated data,
which increases the variance in the data.
Looking at the results in Tables 3 and 4, we can
see that:
a) Adding all the translated training data together
drastically increases the noise level in the training
data, creating harmful effects in terms of clas-
sification performance: each classifier loses its
discriminative capability.
b) At language level, clearly the results depend
on the translation performance. Only for Spanish
(for which we have the highest Bleu score), each
classifies is able to properly learn from the training
data and try to properly assign the test samples. For
the other languages, translated data are so noisy
that the classifier is not able to properly learn the
correct information for the positive and the negative
classes, this results in the assignment of most of
the test points to one class and zero to the other. In
Table 3, for the French language we have significant
drop in performance, but the classifier is still able
to learn something from the training and assign the
test points to both the classes.
c) The results for Spanish presented in Table 3
confirm the capability of Bagging to reduce the
model variance and increase the performance in
classification.
d) At system level in Table 4, there is no evidence
that better translated test set alows better classifica-
tion performance.
9 Conclusions and Future Work
In this work we propose an extensive evaluation of
the use of translated data in the context of sentiment
analysis. Our findings show that SMT systems are
mature enough to produce reliably training data for
languages other than English. The gap in classifi-
cation performance between systems trained on En-
glish and translated data is minimal, with a maxi-
mum of 8
Working with translated data implies an incre-
ment number of features, sparseness and noise in the
data points in the classification task. To limit these
problems, we test three different classification ap-
proaches showing that bagging has a positive impact
in the results.
In future work, we plan to investigate different
document representations, in particular we believe
that the projection of our documents in space where
the features belong to a sentiment lexical and in-
clude syntax information can reduce the impact of
the translation errors. As well we are interested to
evaluate different term weights such as tf-idf.
Acknowledgments
The authors would like to thank Ivano Azzini, from
the BriLeMa Artificial Intelligence Studies, for the
advice and support on using meta-classifiers. We
would also like to thank the reviewers for their use-
ful comments and suggestions on the paper.
58
References
Turchi, M. and Atkinson, M. and Wilcox, A. and Craw-
ley, B. and Bucci, S. and Steinberger, R. and Van der
Goot, E. 2012. ONTS: ?Optima? News Translation
System.. Proceedings of EACL 2012.
Banea, C., Mihalcea, R., and Wiebe, J. 2008. A boot-
strapping method for building subjectivity lexicons for
languages with scarce resources.. Proceedings of the
Conference on Language Resources and Evaluations
(LREC 2008), Maraakesh, Marocco.
Banea, C., Mihalcea, R., Wiebe, J., and Hassan, S.
2008. Multilingual subjectivity analysis using ma-
chine translation. Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), 127-135, Honolulu, Hawaii.
Banea, C., Mihalcea, R. and Wiebe, J. 2010. Multilin-
gual subjectivity: are more languages better?. Pro-
ceedings of the International Conference on Computa-
tional Linguistics (COLING 2010), p. 28-36, Beijing,
China.
Boudin, F. and Huet, S. and Torres-Moreno, J.M. and
Torres-Moreno, J.M. 2010. A Graph-based Ap-
proach to Cross-language Multi-document Summa-
rization. Research journal on Computer science
and computer engineering with applications (Polibits),
43:113?118.
P. F. Brown, S. Della Pietra, V. J. Della Pietra and R. L.
Mercer. 1994. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation, Computa-
tional Linguistics 19:263?311.
C. Callison-Burch, and P. Koehn and C. Monz and J.
Schroeder. 2009. Findings of the 2009 Workshop on
Statistical Machine Translation. Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 1?28. Athens, Greece.
Deerwester, S., Dumais, S., Furnas, G. W., Landauer, T.
K., and Harshman, R. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 3(41).
Kim, S.-M. and Hovy, E. 2006. Automatic identification
of pro and con reasons in online reviews. Proceedings
of the COLING/ACL Main Conference Poster Ses-
sions, pages 483490.
Kim, J., Li, J.-J. and Lee, J.-H. 2006. Evaluating
Multilanguage-Comparability of Subjectivity Analysis
Systems. Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
595603, Uppsala, Sweden, 11-16 July 2010.
P. Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
Machine Translation Summit X, pages 79-86. Phuket,
Thailand.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn and F. J. Och and D. Marcu. 2003. Statistical
Phrase-Based Translation, Proceedings of the North
America Meeting on Association for Computational
Linguistics, 48?54.
P. Koehn and H. Hoang and A. Birch and C. Callison-
Burch and M. Federico and N. Bertoldi and B. Cowan
and W. Shen and C. Moran and R. Zens and C. Dyer
and O. Bojar and A. Constantin and E. Herbst 2007.
Moses: Open source toolkit for statistical machine
translation. Proceedings of the Annual Meeting of the
Association for Computational Linguistics, demon-
stration session, pages 177?180. Columbus, Oh, USA.
Mihalcea, R., Banea, C., and Wiebe, J. 2009. Learn-
ing multilingual subjective language via cross-lingual
projections. Proceedings of the Conference of the An-
nual Meeting of the Association for Computational
Linguistics 2007, pp.976-983, Prague, Czech Repub-
lic.
F. J. Och 2003. Minimum error rate training in statisti-
cal machine translation. Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 160?167. Sapporo, Japan.
K. Papineni and S. Roukos and T. Ward and W. J. Zhu
2001. BLEU: a method for automatic evaluation of
machine translation. Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318. Philadelphia, Pennsylvania.
J. Savoy, and L. Dolamic. 2009. How effective is
Google?s translation service in search?. Communi-
cations of the ACM, 52(10):139?143.
R. Steinberger and B. Pouliquen and A. Widiger and C.
Ignat and T. Erjavec and D. Tufis? and D. Varga. 2006.
The JRC-Acquis: A multilingual aligned parallel cor-
pus with 20+ languages. Proceedings of the 5th Inter-
national Conference on Language Resources and Eval-
uation, pages 2142?2147. Genova, Italy.
J. Tiedemann. 2009. News from OPUS-A Collection of
Multilingual Parallel Corpora with Tools and Inter-
faces. Recent advances in natural language processing
V: selected papers from RANLP 2007, pages 309:237.
Wan, X. and Li, H. and Xiao, J. 2010. Cross-language
document summarization based on machine transla-
tion quality prediction. Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 917?926.
Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. Proceedings of HLT-EMNLP 2005, pp.347-354,
Vancouver, Canada.
59
Language SMT system Nr. of unigrams Nr. of bigrams
French
Bing 7441 17870
Google 7540 18448
Moses 6938 18814
Bing+Google+Moses 9082 40977
German
Bing 7817 16216
Google 7900 16078
Moses 7429 16078
Bing+Google+Moses 9371 36556
Spanish
Bing 7388 17579
Google 7803 18895
Moses 7528 18354
Bing+Google+Moses 8993 39034
Table 1: Features employed.
Language SMT Test Set SMO AdaBoost M1 Bagging Bleu Score
English GS 0.685 0.685 0.686
To German
Bing
GS 0.641 0.631 0.648
Tr 0.658 0.636 0.662 0.227
To German
Google T.
GS 0.646 0.623 0.674
Tr 0.687 0.645 0.661 0.209
To German
Moses
GS 0.644 0.644 0.676
Tr 0.667 0.667 0.674 0.17
To Spanish
Bing
GS 0.656 0.658 0.646
Tr 0.633 0.633 0.633 0.316
To Spanish
Google T.
GS 0.653 0.653 0.665
Tr 0.636 0.667 0.636 0.341
To Spanish
Moses
GS 0.664 0.664 0.671
Tr 0.649 0.649 0.663 0.298
To French
Bing
GS 0.644 0.645 0.664
Tr 0.644 0.649 0.652 0.243
To French
Google T.
GS 0.64 0.64 0.659
Tr 0.652 0.652 0.678 0.274
To French
Moses
GS 0.633 0.633 0.645
Tr 0.666 0.666 0.674 0.227
Table 2: Results obtained using the individual training sets obtained by translating with each of the three considered
MT systems, to each of the three languages considered.
60
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 240?251,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Coping with the Subjectivity of Human Judgements
in MT Quality Estimation
Marco Turchi Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{turchi|negri|federico}@fbk.eu
Abstract
Supervised approaches to NLP tasks rely
on high-quality data annotations, which
typically result from expensive manual la-
belling procedures. For some tasks, how-
ever, the subjectivity of human judgements
might reduce the usefulness of the an-
notation for real-world applications. In
Machine Translation (MT) Quality Esti-
mation (QE), for instance, using human-
annotated data to train a binary classifier
that discriminates between good (useful
for a post-editor) and bad translations is
not trivial. Focusing on this binary task,
we show that subjective human judge-
ments can be effectively replaced with an
automatic annotation procedure. To this
aim, we compare binary classifiers trained
on different data: the human-annotated
dataset from the 7th Workshop on Statis-
tical Machine Translation (WMT-12), and
an automatically labelled version of the
same corpus. Our results show that human
labels are less suitable for the task.
1 Introduction
With the steady progress in the field of Statistical
Machine Translation (SMT), the translation indus-
try is now faced with the possibility of significant
productivity increases (i.e. amount of publishable
output per unit of time). One way to achieve this
goal, in Computer Assisted Translation (CAT) en-
vironments, is the integration of (precise, but of-
ten partial) suggestions obtained through ?fuzzy
matches? from a Translation Memory (TM), with
(complete, but potentially less precise) translations
produced by an MT system. Such integration can
loosely consist in presenting translators with un-
ranked suggestions obtained from the MT and the
TM, or rely on tighter combination strategies. For
instance, MT and TM translations can be automat-
ically ranked to ease the selection of the most suit-
able one for post-editing (He et al, 2010), or the
TM can be used to constrain and improve MT sug-
gestions (Ma et al, 2011). In all cases, the ef-
fectiveness of the integration is conditioned by:
i) the quality of MT, and ii) the accuracy in au-
tomatically predicting such quality. Higher pro-
ductivity increases depend on the capability of the
MT system to output useful material that is close
to be publishable ?as is? (Denkowski and Lavie,
2012), and the capability to automatically identify
and present to human translators only such sug-
gestions.
Recognizing good translations falls in the scope
of research on automatic MT Quality Estimation
(QE), which addresses the problem of estimating
the quality of a translated sentence at run-time,
without access to reference translations (Specia et
al., 2009; Soricut and Echihabi, 2010; Bach et al,
2011; Specia, 2011; Mehdad et al, 2012b). In
recent years QE gained increasing interest in the
MT community, resulting in several datasets avail-
able for training and evaluation (Callison-Burch et
al., 2012), the definition of features showing good
correlation with human judgements (Soricut et al,
2012), and the release of open-source software.1
The proposed solutions to the QE problem rely
on supervised methods that strongly depend on the
availability of labelled data. While early works
(Blatz et al, 2003) exploited annotations obtained
with automatic MT evaluation metrics like BLEU
(Papineni et al, 2002), the current trend is to
rely on human annotations, which seem to lead
to more accurate models (Quirk, 2004; Specia et
al., 2009). Along this direction, the QE task con-
sists in predicting scores that reflect human quality
judgements, by learning from manually annotated
datasets (e.g. collections of source-target pairs la-
1http://www.quest.dcs.shef.ac.uk/
240
belled according to an n-point Likert scale or with
real numbers in a given interval). Within this dom-
inant supervised framework, we explore different
ways to obtain labelled data for training a bi-
nary QE classifier suitable for integration in a
CAT tool. Since, to the best of our knowledge,
labelled data with binary judgements are currently
not available, we consider two alternative options.
The first option is to adapt an existing dataset,
checking whether it can be partitioned in a way
that reflects the distinction between good (use-
ful for the translator, suitable for post editing)
and bad translations (that need complete rewrit-
ing).2 To this aim we experiment with the QE
data released within the 7th Workshop on Ma-
chine Translation (WMT-12). The corpus con-
sists of source-target pairs annotated with manual
QE labels (1-5 scores) indicating the post-editing
needed to correct the translations. Besides explicit
human judgements, the availability of post-edited
translations makes also possible to calculate the
actual HTER values (Snover et al, 2009), indicat-
ing the minimum edit distance between the ma-
chine translation and its manually post-edited ver-
sion in the [0,1] interval.
The second option is to automatically re-
annotate the same dataset, trying to produce labels
that reflect an objective and more reliable binary
distinction based on empirical observations.
Our analysis aims to answer the following ques-
tions:
1. Are human labels reliable and coherent
enough to train accurate binary models?
2. Are arbitrarily-set thresholds useful to parti-
tion QE data for this task?
3. Is it possible to obtain reliable binary annota-
tions from an automatic procedure?
Negative answers to the first two questions would
respectively call into question: i) the intuitive idea
that human labels are the most reliable for a super-
vised approach to binary QE, and ii) the possibility
that thresholds on a single metric (e.g. the HTER)
can be set to capture the subtle differences separat-
ing useful from useless translations. A positive an-
swer to the third question would open to the possi-
bility to create training datasets in a more coherent
2In the remainder of the paper we will consider as ?good?
translations those for which post-editing requires a smaller
effort than translation from scratch. Conversely, we will label
as ?bad? the translations that need complete rewriting.
and replicable way compared to current data anno-
tation methods. By answering these questions, this
paper provides the following main contributions:
? We show that training a binary classifier on
arbitrary partitions of an existing dataset is
difficult. Our experiments with the WMT-
12 corpus demonstrate that neither following
standard indications (e.g. ?if more than 70%
of the MT output needs to be edited, a trans-
lation from scratch is necessary?)3, nor con-
sidering arbitrary HTER thresholds, it is pos-
sible to obtain accurate binary classifiers suit-
able for integration in a CAT environment;
? We propose a replicable automatic (hence
non subjective) method to re-annotate an ex-
isting dataset in a way that the resulting bi-
nary classifier outperforms those trained with
human labels.
? We show that, with our method, a smaller
amount of training data is sufficient to ob-
tain similar or better performance compared
to that of the human-annotated dataset used
for comparison.
2 Binary QE for CAT environments
QE has been mainly addressed as a classification
or regression task, where a quality score (respec-
tively an integer or a real value) has to be automat-
ically assigned to MT output sentences given their
source (Specia et al, 2010). Casting the problem
in this way, the integration of a QE component
in a CAT environment makes possible to present
translators with estimates of the expected quality
of each MT suggestion. Such intuitive solution,
however, disregards the fact that even precise QE
scores would not alleviate translators from the ef-
fort of reading useless MT output (or at least the
associated score).
A more effective alternative is to use the esti-
mated QE scores to filter out poor MT suggestions,
presenting only those worth for post-editing. Bi-
nary classification, however, has to confront with
the problem of setting reasonable cut-off criteria.
The arbitrary thresholds, used in several previous
works (Quirk, 2004; Specia et al, 2010; Specia
et al, 2011) are in fact hard to justify, and even
harder to learn from human-labelled training data.
3This was a guideline for the professional trans-
lators involved in the annotation of a previous ver-
sion of the dataset used for the WMT-12 evalua-
tion (see http://www.statmt.org/wmt12/
quality-estimation-task.html).
241
On one side, for instance, there is no evi-
dence that the 70% HTER threshold used in some
datasets yields the optimal separation between ac-
ceptable and totally useless suggestions. Such ar-
bitrary criterion, based on the raw count of post-
editing operations, is likely to reflect a partial view
on a complex problem, disregarding important as-
pects such as the distribution of the corrections in
the MT output. However, in some cases, having
the first 30% of words correctly translated might
take less post-editing effort than having 50% of
correctly translated terms scattered throughout the
whole sentence. In these cases, a 70% HTER
threshold would wrongly consider useless trans-
lations as positive instances and vice-versa.
On the other side, when arbitrary thresholds are
used as annotation guidelines (Callison-Burch et
al., 2012), the moderate agreement between hu-
man judges might make manual labels ill-suited to
learn accurate models.
Under the constraints posed by a CAT envi-
ronment, where only useful suggestions can lead
to a significant productivity increase, the ideal
model should maximize the number of true posi-
tives (useful translations recognized as good) min-
imizing, at the same time, the number of false pos-
itives (useless translations recognized as good). To
this aim, the more the training data are partitioned
according to objective criteria, the higher the ex-
pected reliability of the corresponding cut-off and,
in turn, the higher the expected performance of the
binary classifier.
Focusing on these issues, the following sections
discuss various methods to obtain training data for
binary QE geared to the integration in a CAT en-
vironment. Partitions based on human judgements
from the WMT-12 dataset will be compared with
an automatic method to re-annotate the same cor-
pus. The suitability of the resulting training sets
for binary classification will be assessed by mea-
suring the performance of classifiers built from
each training set. Metrics sensitive to the number
of false positives will be used for this purpose.
3 Partitioning the WMT-12 dataset
Due to the lack of datasets annotated with ex-
plicit binary (good, bad) judgements about transla-
tion quality, the most intuitive way to obtain train-
ing data for our QE classifier is to adapt exist-
ing manually-labelled data. The reasonable size
of the WMT-12 dataset makes it a good candidate
for our purposes. The corpus consists of 2,254
English-Spanish news sentences (1,832 for train-
ing, 422 for test) produced by the Moses phrase-
based SMT system (Koehn et al, 2007) trained
on Europarl (Koehn, 2005) and News Commen-
taries corpora,4 along with their source sentences,
reference translations and post-edited translations.
Training and test instances have been annotated by
professional translators with scores (1 to 5) indi-
cating the estimated post-editing effort (percent-
age of MT output that has to be corrected). Ac-
cording to the proposed scheme, the highest score
indicates lowest effort (MT output requires little or
no editing), while the lowest score indicates that
the MT output needs to be translated from scratch.
To cope with systematic biases among the anno-
tators,5 the judgements were combined in a final
score obtained from their weighted average, re-
sulting in a labelled dataset with real numbers in
the [1, 5] interval as effort scores.
In order to obtain suitable data for binary QE,
the WMT-12 training set (1,832 instances) has
been partitioned in different ways, leaving the test
set for evaluation (see Section 5). The goal, for
each partition strategy, was to label as bad (the as-
signed label is -1) only the translations that need
complete rewriting, keeping all the other transla-
tions as good instances (labelled with +1). Consid-
ering the averaged effort scores, the actual human
judgements, and the HTER values calculated be-
tween the translations and the corresponding post-
edited version, we experimented with the follow-
ing three partition criteria.
Average effort scores (AES). Three partitions
have been generated based on the effort scores
of 2, 2.5, and 3, labelling the WMT-12 train-
ing instances with scores below or equal to each
threshold as negative examples (-1), and the in-
stances with scores above the threshold as posi-
tive examples (+1). Partitions with thresholds be-
low 2 were also considered, including the most
intuitive partition with cut-off set to 1. However,
the resulting number of negative instances, if any,
was too scarce, and the overall dataset too unbal-
anced, to make standard supervised learning meth-
ods effective The creation of highly unbalanced
data is a recurring issue for all the partition meth-
4http://www.statmt.org/wmt11/
translation-task.html#download
5Such biases support the idea that labelling translations
with quality scores is per se a highly subjective task.
242
ods we applied to the WMT-12 corpus. Together
with the low homogeneity of human labels (even
for very poor translations the three judges do not
agree in assigning the lowest score), in most of
the cases the small number of low-quality transla-
tions in the dataset makes the negative class con-
siderably smaller than the positive one. This can
be observed in Table 1, which provides the to-
tal number of positive and negative instances for
each partition method. For instance, with our low-
est AES threshold (2) the total number of nega-
tive instances is 113, while the positive ones are
1,719. Although considering different cut-off cri-
teria aims to make our investigation more com-
plete, it?s also worth remarking that the higher the
threshold, the higher the distance of the result-
ing experimental setting from our target scenario.
While 2, as an effort score threshold, is likely
to reflect a reasonable separation between useless
and post-editable translations, higher values are in
principle more appropriate for ?soft? separations
into worse versus better translations.
Human scores (HS). Five partitions have been
generated using the actual labels assigned by the
three annotators to each translation instead of the
average effort scores. In particular, we considered
the following score combinations (?X? stands for
any integer between 1 and 5): 1-X-X, 2-2-2, 2-
2-X, 2-3-3, 3-3-3. Also in this case, as shown
in Table 1, partitions based on lower scores lead
to highly unbalanced datasets of limited usability,
while those based on higher scores are increas-
ingly more distant to our application scenario.6
HTER scores (HTER). Seven partitions have
been generated considering the following HTER
thresholds: 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45.
In this case, being the HTER an error measure,
training instances with scores above or equal to
the threshold were labelled as negative examples
(-1), while instances with lower scores were la-
belled as positive examples (+1). Similar to the
other partition criteria, some of our threshold val-
ues reflect our task more closely than others, but
result in more unbalanced datasets. In particular,
thresholds around 0.7 substantially adhere to the
WMT-12 annotation guidelines (as far as transla-
tions that need complete rewriting are concerned)
6The partition most closely related to our task (i.e. 1-1-1)
was impossible to produce since none of the examples was
labelled with 1 by all the annotators. Even for 1-1-X, the
negative class contains only one example.
and produce training data with fewer negative in-
stances. Other thresholds, which is still worth ex-
ploring since we do not know the optimal cut-off
value, are in principle less suitable to our task but
produce more balanced training data.
Training instances
Average effort scores (AES) Positive Negative
2 1,719 113
2.5 1,475 357
3 1,194 638
Human scores (HS) Positive Negative
1-X-X 1,736 96
2-2-2 1,719 113
2-2-X 1,612 220
2-3-3 1,457 375
3-3-3 1,360 472
HTER scores (HTER) Positive Negative
0.75 1,798 34
0.7 1,786 46
0.65 1,756 76
0.6 1,708 124
0.55 1,653 179
0.5 1,531 301
0.45 1,420 412
Table 1: Number of positive/negative instances for
each partition of the WMT-12 training set.
4 Re-annotating the WMT-12 dataset
As an alternative to partitioning methods, we in-
vestigated the possibility to re-annotate the WMT-
12 training set with an automatic procedure.
4.1 Approach
Our approach, which does not involve subjec-
tive human judgements, is based on the observa-
tion of similarities and dissimilarities between an
automatic translation (TGT), its post-edited ver-
sion (PE) and the corresponding reference trans-
lation (RT). Such comparisons provide useful in-
dications about the behaviour of a post-editor
when correcting automatic translations and, in
turn, about MT output quality.
Typically, the PE version of a good-quality TGT
preserves some characteristics (e.g. lexical, struc-
tural) that indicate a moderate correction activity
by the post editor. Conversely, in the PE ver-
sion of a low-quality TGT, such characteristics
are more difficult to observe, indicating an in-
tense correction activity. At the two extremes, the
PE of a perfect TGT preserves all its characteris-
tics, while the PE of a useless TGT looses most
of them. In the first case TGT and PE are iden-
243
tical, and their similarity is the highest possible
(i.e. sim(TGT, PE) = 1). In the second case,
TGT and PE show a degree of similarity close to
that of TGT and a completely rewritten transla-
tion featuring different lexical choices and struc-
ture. This is where reference translations come
into play: considering RT as a good example of
rewritten sentence,7 for low-quality TGT we will
have sim(TGT, PE) ? sim(TGT,RT ).
In light of these considerations, we hypothe-
size that the automatic re-annotation of WMT-12
training data can take advantage of a classifier that
learns a similarity threshold T such that:
? a PE sentence with sim(TGT, PE) ? T
will be considered as a rewritten translation
(hence TGT is useless, and the correspond-
ing source-TGT pair a negative example to
be labelled as ?-1?);
? a PE sentence with sim(TGT, PE) > T
will be considered as a real post-edition
(hence TGT is useful for the post-editor, and
the corresponding source-TGT pair a positive
example to be labelled as ?+1?).
Based on this hypothesis, to perform our au-
tomatic re-annotation procedure we: 1) create a
training set Z of positive and negative examples
(i.e. [TGT, correct translation] pairs, where cor-
rect translation is either a post-editing or a rewrit-
ten translation); 2) design a feature set capable
to capture different aspects of the similarity be-
tween TGT and correct translation; 3) build a bi-
nary classifier using Z; 4) use the classifier to label
the [TGT, PE] pairs as instances of post-editings
or rewritings; 5) assess the quality of the resulting
annotation.
4.2 Building the classifier
Training corpus. To build a classifier capable
of labelling PE sentences as rewritten/post-edited
material, we first created a set of positive and neg-
ative instances from the WMT-12 training set. For
each tuple [source, TGT, PE, RT] of the dataset,
one positive and one negative instance have been
respectively obtained as the combination of [TGT,
PE] and [TGT, RT]. Figure 1, which plots the dis-
tribution of positive and negative instances against
HTER, shows a fairly good separation between the
7Such assumption is supported by the fact that reference
sentences are, by definition, free translations manually pro-
duced without any influence from the target.
0 500 1000 1500 2000 2500 3000 3500 40000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Sentences
HTE
R
 
 TGT?PE sentencesTGT?RT sentences
Figure 1: Distribution of [TGT, PE] and [TGT,
RT] pairs plotted against the HTER.
two classes. This indicates that our use of the
references as examples of rewritten translations
builds on a reasonable assumption.
Features. Crucial to our classification task, a
number of features can be used to estimate sen-
tence similarity. Differently from the binary QE
task, where the possibility to catch common char-
acteristics between two sentences is limited by
language barriers, in our re-annotation task all the
features are extracted by comparing two monolin-
gual sentences (i.e. TGT and a correct translation,
either a PE or a RT). Although the problem of
measuring sentence similarity can be addressed
in many ways, the solutions should not overlook
the specificities of the task. In our case, for in-
stance, the scarce importance of the semantic as-
pect (TGT, PE and RT typically show a high se-
mantic similarity) makes features used for other
tasks (e.g. based on distributional similarity) less
effective than shallow features looking at the sur-
face form of the input sentences. Our problem
presents some similarities with the plagiarism de-
tection task, where subtle lexical and structural
similarities have to be identified to spot suspicious
plagiarized texts (Potthast et al, 2010). For this
reason, part of our features (e.g. ROUGE scores)
are inspired by research in such field (Chen et al,
2010), while others have been designed ad-hoc,
based on the specific requirements of our task. The
resulting feature set aims to capture text similar-
ity by measuring word/n-gram matches, as well as
the level of sparsity and density of the common
words as a shallow indicator of structural similar-
ity. In total, from each [TGT, correct translation]
244
pair, the following 22 features are extracted:
? Human-targeted Translation Error Rate ?
HTER. The editing operations considered
are: shift, insertion, substitution and deletion.
? Number of words in common.
? Number of words in common, normalized by
TGT length and correct translation length (2
features).
? Number of words in TGT and in the cor-
rect translation (2 features).
? Size of the longest common subsequence.
? Size of the longest common subsequence,
normalized by TGT length.
? Aligned word density: total number of
aligned words,8 divided by the number of
aligned blocks (more than 1 aligned word).
? Unaligned word density: total number of un-
aligned words, divided by the number of un-
aligned blocks (more than 1 unaligned word).
? Normalized number of aligned blocks: total
number of aligned blocks, divided by TGT
length.
? Normalized number of unaligned blocks: to-
tal number of unaligned blocks, divided by
TGT length.
? Normalized density difference: difference
between aligned word density and unaligned
word density, divided by TGT length.
? Modified Lesk score (Lesk, 1986): sum of
the squares of the length of n-gram matches,
normalized by the product of the sentence
lengths.
? ROUGE-1/2/3/4: n-gram recall with n=1,...,4
(4 features).9
? ROUGE-L: size of longest common
subsequence, normalized by the cor-
rect translation length.
? ROUGE-W: the ROUGE-L using different
weights for consecutive matches of length L
(default weight = 1.2).
? ROUGE-S: the ROUGE-L allowing for the
presence of skip-bigrams (pairs of words,
even not adjacent, in their sentence order).
? ROUGE-SU: the extension of ROUGE-S
adding unigrams as counting unit.
8Monolingual stem-to-stem exact matches between TGT
and correct translation are inferred by computing the HTER,
as in (Blain et al, 2012).
9All ROUGE scores, described in (Lin, 2004), have been
calculated using the software available at http://www.
berouge.com.
To increase the capability of identifying simi-
lar sentences, all sentences are tokenized, lower-
cased and stemmed using the Snowball algorithm
(Porter, 2001).
Classifier. On the resulting corpus, an SVM
classifier has been trained using the LIBSVM tool-
box (Chang and Lin, 2011). The selection of the
kernel (linear) and the optimization of the param-
eters (C=0.8) were carried out through grid search
in 5-fold cross-validation.
Labelling the dataset. Using the best parameter
setting obtained, [TGT, PE] and [TGT, RT] pairs
have been re-labelled as post-editings or rewrit-
ings through 5 rounds of cross-validation. The fi-
nal label of each instance was set to the mode of
the predictions produced by each cross-validation
round. Since we assume that the quality of the tar-
get sentence can be inferred from the amount of
correction activity done by the post-editor, the la-
bels assigned to the [TGT, PE] pairs represent the
result of our re-annotation of the corpus into posi-
tive and negative instances.
At the end of the process, of the 1,832 [TGT,
PE] pairs of the WMT 2012 training set, 1.394 are
labelled as examples of post-editing (TGT is use-
ful), and 438 as examples of complete rewriting
(TGT is useless). Compared to the distribution
of positive and negative instances obtained with
most of the partition methods described in Section
3, our automatic annotation produces a fairly bal-
anced dataset. The resulting proportion of nega-
tive examples (?1:3) is similar to what could be
reached only by partitions reflecting a ?soft? sep-
aration into worse versus better translations rather
than a strict separation into useless versus useful
translations.10 In Figure 2, the labelling results
plotted against the HTER show that there is a quite
clear separation between [TGT, PE] pairs marked
as post-editings (lower HTER values) and pairs
marked as rewritings (higher HTER values). Such
separation corresponds to an HTER value around
0.4, which is significantly lower than the thresh-
old of 0.7 proposed by the WMT-12 guidelines as
a criterion to label sentences for which ?a trans-
lation from scratch is necessary?. This confirms
that our separation differs from those produced by
partition methods based on human annotations or
arbitrary HTER thresholds. Furthermore, our au-
10Such partitions are: average effort scores = 3, human
scores = 3-3-3, HTER score = 0.45.
245
0 200 400 600 800 1000 1200 1400 1600 1800 20000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Sentences
HTE
R
 
 TGT?PE labelled as PETGT?PE labelled as RT
Figure 2: TGT-PE classification in post-editings
and rewritings.
tomatic annotation procedure relies on the contri-
bution of features designed to capture different as-
pects of the similarity between the TGT and a cor-
rect translation, while some of the partition meth-
ods discussed in Section 3 rely on thresholds set on
a single score (e.g. HTER). Considering the many
facets of the binary QE problem, we expect that
our features are more effective to deal with latent
aspects disregarded by such thresholds.
5 Experiments and results
At this point, the question is: are the automatically
labelled data more suitable than partitions based
on human labels to train a binary QE classifier?
To answer this question, all the proposed separa-
tions of the WMT-12 training set have been eval-
uated on different test sets. For each separation
we trained a binary classifier able to assign a label
(good or bad) to unseen source-target pairs. Since
the classifiers use the same algorithm and feature
set, differences in performance will mainly depend
on the quality of the training data on which they
are built. Using task-oriented metrics sensitive to
the number of false positives, results highlighting
such differences will indicate the best separation.
5.1 Experimental Setting
Binary QE classifier. Each separation of the
WMT-12 training data was used to train a binary
SVM classifier. Different kernels and parameters
were optimized through a grid search in 5-fold
cross-validation on each training set. Being the
number of positive and negative training instances
highly unbalanced, the best models were selected
optimizing a metric that takes into account the
number of true and false positives (see below).
Seventeen features proposed in (Specia et al,
2009) were extracted from each source-target pair.
This feature set, fully described in (Callison-
Burch et al, 2012), mainly takes into account the
complexity of the source sentence (e.g. number
of tokens, number of translations per source word)
and the fluency of the target translation (e.g. lan-
guage model probabilities). Results of the WMT
2012 QE task shown that these ?baseline? features
are particularly competitive in the regression task,
with only few systems able to beat them. All the
features are extracted using the Quest software11
and the model files released by the organizers of
the WMT 2013 workshop.
Test sets. To obtain different separations be-
tween good and bad translations, artificial test sets
have been created using arbitrary thresholds on
the HTER (the same used to partition the train-
ing set on a HTER basis) and the post-editing time
(PET).12 Two different datasets were split: i) the
WMT-12 test (422 source, target, post-edited and
reference sentences); ii) the WMT-13 training set
for Task 1.3 (800 source, target and post-edited
sentences labelled with PET). The first dataset, the
most similar to the WMT-12 training set, should
better reflect (and reward) the HTER-based parti-
tions proposed in Section 3. The WMT-13 dataset
contains sentences translated with a different con-
figuration (data and parameters) of the SMT en-
gine. This can result in different HTER-based par-
titions in good and bad, useful to test the portabil-
ity of our automatic re-annotation method across
different datasets. Finally, testing on data parti-
tions based on PET allows us to check the stability
of the automatic re-annotation method when eval-
uated on a test set divided according to a different
concept of translation quality. In the end, the com-
bination of different partition methods, thresholds
and datasets results in 21 different test sets (see
Table 2).
Evaluation metrics. F-score and accuracy are
the classic evaluation metrics used in classifica-
tion. In our evaluation, however, they would al-
ways result in high uninformative values due to
the unbalanced nature of the test sets (positive in-
stances  negative instances). In order to bet-
11http://www.quest.dcs.shef.ac.uk/
12PET is the time spent by a post-editor to transform the
target into a publishable sentence.
246
Test instances
WMT-12 HTER Positive Negative
0.45 289 133
0.5 319 103
0.55 352 70
0.6 371 51
0.65 386 36
0.70 398 24
0.75 406 16
WMT-13 Task 1.3 HTER Positive Negative
0.45 582 218
0.5 622 178
0.55 695 105
0.6 724 76
0.65 748 52
0.70 763 37
0.75 773 27
WMT-13 Task 1.3 PET Positive Negative
4 499 301
4.16? 517 283
4.50 554 246
5 594 206
6 659 141
7 698 102
8 727 73
Table 2: Number of positive and negative in-
stances for each partition of the WMT-12 test set
and WMT-13 training set. ?*?: Average PET com-
puted on all the instances in the WMT-13 dataset.
ter understand the real quality of the classifica-
tion, we hence opted for two task-oriented evalua-
tion metrics sensitive to the number of false posi-
tives (the main issue in a CAT environment, where
false positives and true positives should be re-
spectively minimized and maximized). These are:
i) the weighted combination of the false positive
rate (FPR) and false discovery rate (FDR) (Ben-
jamini and Hochberg, 1995), and ii) the weighed
average of sensitivity and specificity (also called
balanced/weighted accuracy). FPR measures the
level of false positives, but does not provide infor-
mation about the number of true positives. For this
reason, we combined it with FDR (1-precision),
which indirectly controls the level of true posi-
tives. FPR and FDR were equally weighted in
the average; lower values indicate good perfor-
mance. Furthermore, in our scenario it is desir-
able to have a classifier with high prediction ac-
curacy over the minority class (specificity), while
maintaining reasonable accuracy for the majority
class (sensitivity). Weighted accuracy is useful in
such situations. To better asses the performance on
the minority (negative) class, we hence gave more
importance to specificity (0.7 vs 0.3). As regards
weighted accuracy higher values in indicate bet-
ter performance. Penalizing majority voting clas-
sifiers, both metrics are particularly appropriate in
our framework. Besides evaluation, the weighted
average of FPR and FDR was also used to tune the
parameters of the SVM classifier.
5.2 Results
Table 3 presents the results achieved by classifiers
trained on different datasets, on the 21 splits pro-
duced from the test sets used for evaluation.
Although the total number of classifiers tested
is 16 (15 resulting from partitions based on human
labels, and 1 obtained with our automatic annota-
tion method), most of them are not present in the
table since they predict the majority class for all
the test points. These are, in general, trained on
highly unbalanced training sets where the number
of negative samples is really small. However, it
is interesting to note that increasing the number
of instances in the negative class does not always
result in a better classifier. For instance, the classi-
fier built on an HTER separation with threshold at
0.55 performs majority voting even if it is built on
a more balanced (but probably more noisy) train-
ing set than the classifier obtained with threshold
at 0.6. This suggests that the quality of the sep-
aration is as important as the actual proportion of
positive and negative instances.
On all test sets, and for both the evaluation met-
rics used, the results achieved by the classifier built
from the automatically annotated training set (AA)
produces lower error rates (Weighted FPR-FDR)
and higher accuracy (Weighted Accuracy), outper-
forming all the other classifiers. The effective-
ness of the automatic annotation is confirmed by
the fact that classifiers 3 (based on the average
of effort scores - AES) and 3-3-3 (based on the
actual human scores - HS), which are trained on
more balanced training sets, achieve worse perfor-
mances than the AA classifier.13
Results on the WMT-13 PET test set are not as
good as in the other two test sets. This shows that
test data labelled in terms of time are more dif-
ficult to be correctly classified compared to those
based on the HTER. This can be explained consid-
ering the intrinsic differences between the HTER
and the PET as approximations of the post-editing
13The distribution of positive/negative instances in the
training sets is: 1194/638 for classifier 3, 1360/472 for clas-
sifier 3-3-3, 1394/438 for classifier AA.
247
Weighted Training: WMT-12 Separations
FPR-FDR 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AA
AES HS HS HS HTER HTER
Tes
t:W
MT
-12
HT
ER 0.45 0.61 0.66 0.66 0.66 0.66 0.66 0.550.5 0.57 0.62 0.62 0.62 0.62 0.62 0.49
0.55 0.52 0.58 0.58 0.58 0.58 0.58 0.42
0.6 0.5 0.56 0.56 0.56 0.56 0.56 0.4
0.65 0.5 0.54 0.54 0.54 0.54 0.54 0.39
0.7 0.49 0.53 0.53 0.53 0.53 0.53 0.39
0.75 0.49 0.52 0.52 0.52 0.52 0.52 0.35
Tes
t:W
MT
-13
HT
ER 0.45 0.59 0.63 0.63 0.64 0.64 0.63 0.540.5 0.57 0.6 0.6 0.61 0.61 0.6 0.5
0.55 0.51 0.56 0.56 0.57 0.57 0.56 0.41
0.6 0.49 0.54 0.54 0.55 0.55 0.54 0.37
0.65 0.47 0.53 0.53 0.53 0.53 0.53 0.33
0.7 0.44 0.52 0.52 0.52 0.52 0.52 0.29
0.75 0.44 0.52 0.52 0.52 0.52 0.52 0.28
Tes
t:W
MT
-13
PET
4 0.61 0.68 0.68 0.69 0.69 0.68 0.58
4.16 0.61 0.67 0.67 0.67 0.67 0.67 0.56
4.5 0.58 0.65 0.64 0.65 0.65 0.65 0.54
5 0.55 0.63 0.62 0.63 0.63 0.62 0.51
6 0.49 0.58 0.58 0.58 0.58 0.58 0.45
7 0.45 0.55 0.55 0.56 0.56 0.55 0.43
8 0.45 0.54 0.54 0.54 0.54 0.54 0.41
Weighted Training: WMT-12 Separations
Accuracy 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AA
AES HS HS HS HTER HTER
Tes
t:W
MT
-12
HT
ER 0.45 0.35 0.3 0.3 0.3 0.3 0.3 0.410.5 0.35 0.3 0.3 0.3 0.3 0.3 0.44
0.55 0.37 0.3 0.3 0.3 0.3 0.3 0.48
0.6 0.37 0.3 0.3 0.3 0.3 0.3 0.49
0.65 0.35 0.3 0.3 0.3 0.3 0.3 0.47
0.7 0.35 0.3 0.3 0.3 0.3 0.3 0.45
0.75 0.33 0.3 0.3 0.3 0.3 0.3 0.49
Tes
t:W
MT
-13
HT
ER 0.45 0.33 0.31 0.31 0.3 0.3 0.31 0.40.5 0.34 0.31 0.31 0.3 0.3 0.31 0.42
0.55 0.35 0.31 0.31 0.3 0.3 0.31 0.48
0.6 0.35 0.31 0.31 0.3 0.3 0.31 0.51
0.65 0.36 0.3 0.3 0.3 0.3 0.3 0.54
0.7 0.39 0.3 0.3 0.3 0.3 0.3 0.56
0.75 0.38 0.3 0.3 0.3 0.3 0.3 0.59
Tes
t:W
MT
-13
PET
4 0.37 0.3 0.31 0.3 0.3 0.3 0.4
4.16 0.37 0.3 0.31 0.3 0.3 0.3 0.4
4.5 0.37 0.3 0.31 0.3 0.3 0.3 0.4
5 0.38 0.31 0.31 0.3 0.3 0.31 0.41
6 0.41 0.31 0.31 0.3 0.3 0.31 0.43
7 0.42 0.31 0.31 0.3 0.3 0.31 0.44
8 0.4 0.31 0.31 0.3 0.3 0.31 0.43
Table 3: Weighted FPR-FDR (left table) and weighted Accuracy (right table) obtained by the binary QE
classifiers trained on different separations of the WMT-12 training set. Several arbitrary partitions of the
WMT-12 Test set and WMT-13 Training set are considered.
effort, as pointed out by several recent works (Spe-
cia, 2011; Koponen, 2012).
Comparing the results calculated with the two
metrics, we note that weighted accuracy seems to
be less sensible to small variations in terms of true
and false negatives returned by the classifier, even
if the specificity (accuracy on our minority class)
is weighted more than sensitivity (accuracy on our
majority class). This often results in scores very
close (differences ? 10?3) to the accuracy ob-
tained by majority voting classification (0.3).
Overall, our experiments demonstrate that the
proposed automatic separation method is more ef-
fective than arbitrary partitions of datasets anno-
tated with subjective human judgements.
5.3 Learning Curve
Our automatic re-annotation approach requires
post-edited and reference sentences. Although all
the datasets annotated for QE include post-edited
sentences, this is not always true for the refer-
ences. The cost of having both resources is in
fact not negligible. For this reason, we investi-
gated the minimal number of training data needed
to re-annotate the WMT-12 training set without
altering performance on binary classification. To
this aim, we selected two of the test sets on which
our re-annotation method produces classifiers with
high performance results (WMT-13 HTER 0.6 and
0.75), and measured score variations with increas-
ing amounts of data.
Nine subsets of the WMT-12 training set cor-
pus were created (with 10%, 20%,..., 100% of the
dataset) by sub-sampling sentences from a uni-
form distribution. The process was iterated 10
times. Then, for each subset, a new re-annotation
process was run, the resulting training set was used
to build the relative binary QE classifier, which
was eventually evaluated on the test set in terms of
weighted FPR-FDR. Figures 3 and 4 show the ob-
tained learning curves. Each point is the average
result of the 10 runs; the error bars show ?1std.
As can be seen from both curves, performance
results with 60% of the training data are already
comparable with those obtained using the whole
training data. Similar trends have been observed
for several learning curves created with different
test sets. This shows that, besides avoiding the
use of human labelled data, our approach allows
to drastically reduce the amount of training in-
stances. Considering the high costs of collecting
post-editions, and the fact that reference transla-
tions can be taken from parallel corpora, our solu-
tion represents a viable way to overcome the lack
of training data for binary QE geared towards in-
tegration in a CAT environment.
248
0 0.2 0.4 0.6 0.8 10.36
0.38
0.4
0.42
0.44
0.46
0.48
0.5
Training Set Size
Wei
ghte
d FP
R?F
DR
Figure 3: Learning curve for WMT-13 HTER 0.60.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.32
0.37
0.42
0.47
0.52
Training Set Size
Wei
ghte
d FP
R?F
DR
Figure 4: Learning curve for WMT-13 HTER 0.75.
6 Conclusion
We presented a task-oriented analysis of the use-
fulness of human-labelled data for binary qual-
ity estimation. Our target scenario is computer-
assisted translation, which calls for solutions to
present human translators with useful MT sugges-
tions (i.e. easier to correct than to rewrite from
scratch). Within this framework, the integration
of binary classifiers capable to distinguish ?good?
(useful) from ?bad? (useless) suggestions would
make possible to significantly increase translators?
productivity. Such binary classifiers, however,
need labelled training data (possibly of good qual-
ity) that are currently not available.
An intuitive solution to fill this gap is to take
advantage of an existing dataset, adapting its man-
ual annotations to our task. Exploring this solu-
tion (the first contribution of this paper) has to
face problems related to the subjectivity of human
judgements about translation quality, and the re-
sulting variability in the annotation. In particular,
our experiments with the WMT-12 dataset show
that any adaptation (either based on human judge-
ments or arbitrarily-set HTER thresholds) collides
with the problem of setting reasonable partition
criteria. Our results suggest that the subtle dif-
ferences between useful and useless translations
make subjective human judgements inadequate to
learn effective models.
Instead of relying on manually-assigned qual-
ity labels, an alternative solution to the problem
is to re-annotate an existing dataset. Proposing
an automatic way to do that (the second contri-
bution of this paper), we argue that reliable data
separations into positive and negative examples
can be obtained by measuring the similarities be-
tween: i) automatic translations and post-editings,
and ii) automatic translations and their references.
Our results demonstrate that binary classifiers built
from training data produced with our supervised
method are less prone to the misclassification of
bad suggestions.
As in any supervised learning framework, the
amount of data needed to obtain good results is of
crucial importance. By analysing the demand of
our automatic annotation method in terms of train-
ing data (the third contribution of this paper), we
show that competitive results can be obtained with
a fraction of the data needed by methods based on
human labels. Our results indicate that a good-
quality training set for binary classification can
be obtained with 40% less instances of [training,
post edited sentence, reference sentence], totally
avoiding manually-assigned quality judgements.
Our future works will address the improvement
of the automatic annotation procedure using super-
vised methods suitable to learn from unbalanced
training sets (e.g. one-class SVM, weighted ran-
dom forests), and the integration of new features
(e.g. GTM, meteor) to refine our classification of a
correct sentence into rewritten/post-edited. Then,
to boost binary QE results on the resulting corpora,
the ?baseline? features used for experiments in this
paper will be extended with new features explored
in recent works (Mehdad et al, 2012a; de Souza
et al, 2013; Turchi and Negri, 2013).
Acknowledgments
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
249
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: a Method for Measuring Ma-
chine Translation Confidence. In The 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, Proceed-
ings of the Conference, 19-24 June, 2011, Portland,
Oregon, USA, pages 211?219. The Association for
Computer Linguistics.
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the False Discovery Rate: a Practical and Pow-
erful Approach to Multiple Testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289?300.
Fre?de?ric Blain, Holger Schwenk, and Jean Senellart.
2012. Incremental Adaptation Using Translation In-
formation and Post-Editing Analysis. In Interna-
tional Workshop on Spoken Language Translation,
pages 234?241, Hong-Kong (China).
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Summer work-
shop final report, JHU/CLSP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), pages 10?51, Montre?al, Canada.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a Library for Support Vector Machines. ACM
Trans. Intell. Syst. Technol., 2(3):27:1?27:27, May.
Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.
2010. Plagiarism Detection using ROUGE and
WordNet. Journal of Computing, 2(3).
Jose? G. C. de Souza, Miquel Espla`-Gomis, Marco
Turchi, and Matteo Negri. 2013. Exploiting Quali-
tative Information from Automatic Word Alignment
for Cross-lingual NLP Tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013).
Michael Denkowski and Alon Lavie. 2012. Chal-
lenges in Predicting Machine Translation Utility
for Human Post-Editors. In Proceedings of AMTA
2012.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with Translation
Recommendation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philip Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X, pages 79?86,
Phuket, Thailand.
Maarit Koponen. 2012. Comparing Human Percep-
tions of Post-editing Effort with Post-editing Oper-
ations. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, pages 181?190. As-
sociation for Computational Linguistics.
Michael Lesk. 1986. Automated Sense Disambigua-
tion Using Machine-readable Dictionaries: How to
Tell a Pine Cone from an Ice Cream Cone. In Pro-
ceedings of the 5th annual international conference
on Systems documentation (SIGDOC86).
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
the ACL workshop on Text Summarization Branches
Out., pages 74?81, Barcelona, Spain.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent Translation using Discrim-
inative Learning: a Translation Memory-inspired
Approach. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1239?
1248.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012a. Detecting semantic equivalence and infor-
mation disparity in cross?lingual documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?12),
pages 120?124, Jeju Island, Korea.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012b. Match without a Referee: Evaluat-
ing MT Adequacy without Reference Translations.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, WMT ?12, pages 171?
180, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Martin Potthast, Alberto Barro?n-Ceden?o, Andreas
Eiselt, Benno Stein, and Paolo Rosso. 2010.
Overview of the 2nd International Competition on
Plagiarism Detection. Notebook Papers of CLEF,
10.
250
Christopher B. Quirk. 2004. Training a Sentence-
Level Machine Translation Confidence Measure. In
In Proceedings of LREC.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy,
or HTER?: Exploring Different Human Judgments
with a Tunable MT Metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 259?268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010.
TrustRank: Inducing Trust in Automatic Transla-
tions via Ranking. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 612?621, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL language weaver systems in the WMT12
quality estimation shared task. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion (WMT?12), pages 145?151, Montre?al, Canada.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Es-
timating the Sentence-Level Quality of Machine
Translation Systems. In Proceedings of the 13th
Annual Conference of the European Association
for Machine Translation (EAMT?09), pages 28?35,
Barcelona, Spain.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine translation, 24(1):39?50.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine transla-
tion adequacy. In Proceedings of the 13th Ma-
chine Translation Summit, pages 513?520, Xiamen,
China, September.
Lucia Specia. 2011. Exploiting Objective Annota-
tions for Measuring Translation Post-editing Effort.
pages 73?80.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual En-
tailment. Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
251
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352?358,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
FBK-UEdin participation to the WMT13 Quality Estimation shared-task
Jose? G. C. de Souza
FBK-irst
University of Trento
Trento, Italy
desouza@fbk.eu
Christian Buck
School of Informatics
University of Edinburgh
Edinburgh, UK
christian.buck@ed.ac.uk
Marco Turchi, Matteo Negri
FBK-irst
Trento, Italy
{turchi,negri}@fbk.eu
Abstract
In this paper we present the approach and
system setup of the joint participation of
Fondazione Bruno Kessler and University
of Edinburgh in the WMT 2013 Quality
Estimation shared-task. Our submissions
were focused on tasks whose aim was pre-
dicting sentence-level Human-mediated
Translation Edit Rate and sentence-level
post-editing time (Task 1.1 and 1.3, re-
spectively). We designed features that
are built on resources such as automatic
word alignment, n-best candidate transla-
tion lists, back-translations and word pos-
terior probabilities. Our models consis-
tently overcome the baselines for both
tasks and performed particularly well for
Task 1.3, ranking first among seven parti-
cipants.
1 Introduction
Quality Estimation (QE) for Machine Transla-
tion (MT) is the task of evaluating the quality
of the output of an MT system without relying
on reference translations. The WMT 2013 QE
Shared Task defined four different tasks covering
both word and sentence level QE. In this work
we describe the Fondazione Bruno Kessler (FBK)
and University of Edinburgh approach and system
setup of our participation to the shared task. We
developed models for two sentence-level tasks:
Task 1.1: Scoring and ranking for post-editing ef-
fort, and Task 1.3: Predicting post-editing time.
The first task aims at predicting the Human-
mediated Translation Edit Rate (HTER) (Snover
et al, 2006) between a suggestion generated by
a machine translation system and its manually
post-edited version. The data set contains 2,754
English-Spanish sentence pairs post-edited by one
translator (2,254 for training and 500 for test). We
participated only in the scoring mode of this task.
The second task requires to predict the time, in
seconds, that was required to post edit a transla-
tion given by a machine translation system. Par-
ticipants are provided with 1,087 English-Spanish
sentence pairs, source and suggestion, along with
their respective post-edited sentence and post-
editing time in seconds (803 data points for train-
ing and 284 for test).
For both tasks we applied supervised learning
methods and made use of information about word
alignments, n-best diversity scores, word posterior
probabilities, pseudo-references, and back trans-
lation to train our models. In the remainder of
this paper we describe the features designed for
our participation (Section 2), the learning methods
used to build our models (Section 3), the experi-
ments that led to our submitted systems (Section
4), and we briefly conclude our experience in this
evaluation task (Section 5).
2 Features
2.1 Word Alignment
Information about word alignments is used to ex-
tract quantitative (amount and distribution of the
alignments) and qualitative features (importance
of the aligned terms) under the assumption that
features that explore what is aligned can bring im-
provements to tasks where sentence-level seman-
tic relations need to be identified. Among the pos-
sible applications, Souza et al (2013) recently in-
vestigated with success their application in Cross-
lingual Textual Entailment for content synchro-
nization (Mehdad et al, 2012; Negri et al, 2013).
For our experiments in both tasks we built word
alignment models using the resources made avail-
able for the evaluation campaign. To train the
word alignment models we used the MGIZA++
implementation (Gao and Vogel, 2008) of the IBM
models (Brown et al, 1993) and the concatenation
of Europarl, News Commentary, MultiUN, paral-
352
lel corpora made available for task 1.3. The train-
ing data comprises about 12.8 million sentence
pairs.
The word alignment features are divided into
three main groups: AL, POS and IDF. The
AL group regards quantitative information about
aligned and unaligned words between source
sentence (src) and machine translation output
(tgt). The features of this group are computed
for both src and tgt:
? proportion of aligned words;
? number of contiguous unaligned words nor-
malized by the length of the sentence;
? length of the longest sequence of
aligned/unaligned words normalized by
the length of the sentence;
? average length of aligned/unaligned se-
quences of words;
? position of the first/last unaligned word nor-
malized by the length of the sentence;
? proportion of aligned n-grams in the sen-
tence.
To compute the features of the POS group
we use part-of-speech (PoS) information for each
word in src and tgt. Training and test data for
both tasks were preprocessed with the TreeTag-
ger (Schmid, 1995) and mapped to a more coarse-
grained set of part-of-speech tags (P ) based on the
universal PoS tag set by Petrov et al (2012). In
this group there are two different types of features:
one is computed for the alignments (the mapping
between a word in src and a word in tgt) and
the other is computed for aligned words (words in
src that are aligned to one or more words in tgt
and vice-versa). The features computed over the
alignments are:
? proportion of alignments connecting words
with the same PoS tag;
? proportion of alignments connecting words
with the same PoS tag for each tag p ? P .
The features implemented for aligned words
are:
? proportion of aligned words tagged with p in
the sentence (p ? P ). This feature is pro-
cessed for both src and tgt;
? proportion of words in src aligned with
words in tgt that share the same PoS tag
(and vice-versa);
? proportion of words tagged with p in src and
that are aligned to words with the same tag
p in tgt (and vice-versa). This is done for
every p ? P .
The last group, IDF, is composed by one fea-
ture that explores the notion of inverse document
frequency as another source of qualitative infor-
mation. The idea is that rare words (with higher
IDF) are more informative than frequent words.
The IDF scores for each word are calculated for
English and Spanish on each side of the parallel
corpora used to build the alignment models. This
feature is calculated for both src and tgt (at test
stage, the average IDF value of each language is
assigned to unseen terms):
? summation of the IDF scores of aligned
words in src divided by the sum of IDF
scores of the aligned words in tgt (and vice-
versa).
Preliminary experiments have been executed to
find the best word alignment algorithm for each
task. We explored three different word alignment
algorithms: the hidden Markov model (HMM)
(Vogel et al, 1996) and IBM models 3 and 4
(Brown et al, 1993). We also tried three sym-
metrization models (Koehn et al, 2005): union,
intersection, and grow-diag-final-and, a more
complex symmetrization method which combines
intersection with some alignments from the union.
The best alignment and symmetrization combina-
tion found for Task 1.1 was IBM4 with intersec-
tion and for task 1.3 was HMM with intersec-
tion. These experiments were carried out in 10-
fold cross-validation on the training set and used
only the alignment features.
2.2 N-best Diversity scores
Our n-best diversity features are based on the intu-
ition that a large number of possible choices gen-
erally leads to more errors. While a similar notion
can be expressed locally by counting the transla-
tion options for each word or phrase, we consider
n-best lists as a good approximation of the search
space. This allows us to circumvent problems as-
sociated with the local measures, such as ambigu-
ous alignment and segmentation, and limitations
353
of using the search graph directly such as the in-
ability compute edit distance between hypotheses.
Thus, to quantify the coherence of translation
options we compute a (symmetrical) matrix of
pairwise Levenshtein distances, either on token or
character level, for n-best lists of size up to 100k1
using the baseline system and the systems we de-
scribe in Section 2.4. For this matrix the following
features are produced:
1. The index of the central hypothesis, i.e. the
translation with the minimum average dis-
tance to all other entries.
2. The average edit distance between the cen-
tral hypothesis and all other entries normal-
ized by the length of top scoring hypothesis.
3. Edit distance between top scoring and central
hypothesis
4. Number of hypotheses with an edit distance
to the top-scoring hypothesis below a set
threshold.
2.3 Word Posterior Probabilities
Following previous work on word posterior prob-
abilities (WPPs) (Ueffing et al, 2003) we com-
puted the sequence of edit operations needed to
transform the MT suggestion into all entries of an
n-best list in which we normalized the logarithmic
model scores to resemble probabilities. Tokens are
considered incorrect is the operation is either in-
sert or substitute, otherwise the probability of the
hypothesis counts towards the correctness of the
word. These word-level features were then nor-
malized by taking the geometric mean of the in-
dividual probabilities. We did this for all systems
described in Section 2.4 and varying sizes of n be-
tween 10 and 100k.
2.4 Pseudo-references and back-translation
Motivated by the success of pseudo-reference fea-
tures (Soricut et al, 2012) we employed three ad-
ditional MT systems: one similar to the original
system but trained on more data, a hierarchical
phrase-based system, and a Spanish-English sys-
tem to translate back into English. All models
1Computing the pair-wise edit-distances between all 100k
entries is computationally expensive. However, we found the
n-best lists to be highly repetitive, so that on average only
3.7% of the values had to be computed. The computation is
also trivially parallel.
have been estimated using publicly available soft-
ware (SRILM (Stolcke, 2002), Moses (Koehn et
al., 2007)), and corpora (Europarl, News Com-
mentary, MultiUN, Gigaword). Using the predic-
tions of the English-Spanish systems as pseudo-
references and likewise the original source as ref-
erence for the back-translation system we com-
puted a number of automatic metrics including
BLEU (Papineni et al, 2002), GTM (Turian et al,
2003), PER (Tillmann et al, 1997), TER (Snover
et al, 2006) and Meteor (Denkowski and Lavie,
2011).
3 Learning algorithms
To build our models using the features presented
in Section 2 we tried different learning algorithms.
After some preliminary experiments for both tasks
we decided to use mainly two: support vector
machines (SVM) and extremely randomized trees
(Geurts et al, 2006). For all experiments pre-
sented in this paper we use the Scikit-learn (Pe-
dregosa et al, 2011) implementations of the above
algorithms.
In preliminary experiments we noticed that the
number of features that we were using for both
tasks was leading to poor results when using the
SVM regression (SVR) models. In order to cope
with this problem we performed feature selection
prior to the SVM regression training. For that
we used Randomized Lasso, or stability selec-
tion (Meinshausen and Bu?hlmann, 2010). It re-
samples the training data several times and fits a
Lasso regression model on each sample. Features
that appear in a given number of samples are re-
tained. Both the fraction of the data to be sam-
pled and the threshold to select the features can be
configured. In our experiments we set the sam-
pling fraction to 75%, the selection threshold to
25% and the number of re-samples to 200.
To optimize the SVR with radial basis function
(RBF) kernel hyper-parameters we used random
search (Bergstra and Bengio, 2012) instead of the
traditional grid search procedure. We found ran-
dom search to be as efficient or better than grid
search and it drastically reduced the time required
to compute the best parameter combination.
Finally, we trained an extremely randomized
forest, i.e. an ensemble of extremely randomized
trees. Each tree can be parameterized differently.
The results of the individual trees are combined by
averaging their predictions. When a tree is built,
354
System Features MAE RMSE Predict. Interval Parameters
SVR Base 0.127 0.163 [0.046, 0.671] 347.5918, 0.001, 0.0001
SVR Base + All 0.121 0.155 [0.090, 0.714] 0.4052, 0.0753, 0.0010
RL + SVR Sel(Base + All) 0.119 0.1534 [0.084, 0.745] 40.5873, 0.0484, 0.0002
ET Base + All 0.123 0.156 [0.142, 0.708] 100
ET Base + All 0.122 0.155 [0.164, 0.712] 1000
Table 1: Experiments results for Task 1.1 on 10-fold cross-validation. ?Base? are the 17 baseline features.
?All? corresponds to all the features described in Section 2 in a total of 141 features. ?SVR? is support
vector regression, ?RL? is randomized Lasso and ?ET? is extremely randomized trees. MAE stands for
the average mean absolute error and RMSE is the root mean squared error. Parameters for SVR are C, ,
? and for ET is the number of estimators.
the node splitting step is done at random by pick-
ing the best split among a random subset of the
input features.
4 Experiments
For both tasks we set up a baseline system that
uses the same 17 black box ?baseline? features
provided for the WMT 2012 QE shared task
(Callison-Burch et al, 2012). The baseline model
is trained with an SVM regression with RBF ker-
nel and optimized parameters. Parameter opti-
mization for SVM regression models was per-
formed with 1000 iterations of random search for
which the process was set to minimize the mean
absolute error (MAE)2. The parameters of SVR
with RBF kernel (the penalty parameter C, the
width of the insensitivity zone , and the RBF pa-
rameter ?) are sampled from an exponential distri-
bution.
Experiments for both tasks were run using
10-fold cross-validation on the training set. In
Task 1.3 some data points were annotated by
2 or more post-editors and, in a normal cross-
validation scheme, the same data point might ap-
pear in the training and test set but annotated by
different post-editors. To address this characteris-
tic we implemented a cross-validation that divides
along source sentences, so that all translations of a
source segment end up in either the training or test
portion of a split. The number of features available
for both tasks is not the same (112 for Task 1.1
and 141 for Task 1.3) because there were fewer n-
best diversity, pseudo-references and word poste-
rior probability based features developed with dif-
ferent parameters due to time constraints.
2Given by MAE =
?N
i=1
|H(si)?V (si)|
N , where H(si)is the hypothesis score for the entry si and V (si) is the gold
standard value for si in a dataset with N entries.
During our experiments with the training set,
the best model for Task 1.1 was the combination
of randomized Lasso feature selection with SVR
(0.119 MAE). The extremely randomized trees
presented results around 0.12 MAE worse than the
figures obtained by the SVR models. Results ob-
tained for Task 1.1 are summarized in Table 1.
As for Task 1.3, training results are presented in
Table 2. The best model combines feature selec-
tion (randomized Lasso) with SVR. During train-
ing it obtained the lowest average MAE (38.6).
Compared to the models built with extremely ran-
domized trees, the prediction interval of this sys-
tem is narrower. This indicates that the tree-based
models cover a wider range of data points than the
SVR-based models.
In the official results released by the organiz-
ers our submissions had close performances for
Task 1.1. The difference between the SVR and the
extremely randomized tree models is very small
(around 0.0012 MAE points). For Task 1.3 our
best submission is the one based on ensembles of
trees, a trend that was not observed during train-
ing. Our hypothesis is that the tree-based ensem-
ble model was capable of generalizing the train-
ing data better than the SVR-based ones and that
despite the low number of employed features the
latter was prone to overfitting.
Table 3 presents the official evaluation numbers
for both tasks.
4.1 Feature analysis
To gain some insight about the relevance of the
features we explored in our submissions, we com-
pared the output of the randomized Lasso with
the most important features computed by the ex-
tremely randomized tree algorithm. Below we
present the features that appear in the intersection
355
System Features MAE RMSE Predict. Interval Parameters
SVR Base 41.3 69.2 [5.6, 315.7] 138.7359, 2.3331, 0.0185
SVR Base + All 40.2 70.6 [8.6, 335.6] 308.3817, 0.2194, 0.0009
RL + SVR Sel(Base + All) 38.6 69.1 [11.5, 332.0] 161.5705, 7.3370, 0.0460
ET Base + All 44.1 72.2 [11.9, 446.2] 100
ET Base + All 43.7 72.0 [12.6, 446.2] 1000
Table 2: Experiments results for Task 1.3 on 10-fold cross-validation. ?Base? are the 17 baseline features.
?All? corresponds to all the features described in Section 2 in a total of 141 features. ?SVR? is support
vector regression, ?RL? is randomized Lasso and ?ET? is extremely randomized trees. MAE stands for
the average mean absolute error and RMSE is the root mean squared error. Parameters for SVR are C, ,
? and for ET is the number of estimators.
System MAE RMSE
Task 1.1
Official Baseline 0.1491 0.1822
RL + SVR 0.1450 0.1773
ET 0.1438 0.1768
Task 1.3
Official Baseline 51.93 93.35
RL + SVR 47.92 86.66
ET 47.52 82.60
Table 3: Official results for tasks 1.1 and 1.3 on
the test set.
of these two sets for each task.
In Task 1.1, the feature selection algorithm re-
tained 29 out of 112 features. We take the intersec-
tion of this set with the 29 most relevant features
computed by the ensemble tree-based method.
This selection comes from features based on dif-
ferent resources:
? proportion of words in src aligned with
words in tgt that share the same PoS tag;
? average number of translations per source
word according to IBM Model 1 thresholded
P (t|s) > 0.01;
? average number of translations per source
word according to IBM Model 1 thresholded
P (t|s) > 0.2;
? average source sentence token length;
? number of times the top-scoring hypothesis is
repeated in an 10k-best list;
? position of the first unaligned word normal-
ized by the length of the sentence for src
and tgt;
? position of the last unaligned word normal-
ized by the length of the sentence for src
and tgt;
? summation of the IDF scores of aligned
words in tgt divided by the summation of
IDF scores of the aligned words in src;
? length of the longest sequence of unaligned
words normalized by the length of the src;
? percentage of bigrams in the 4th quartile of
frequency of the source language corpus;
? percentage of trigrams in the 4th quartile of
frequency of the source language corpus;
? proportion of alignments connecting words
with the same PoS tag;
? proportion of aligned words in src.
For Task 1.3, the randomized Lasso selection
reduced the input feature vector from 141 fea-
tures to 19. We compared these features with the
19 most important features computed by the ex-
tremely randomized tree algorithm. As above the
intersection of both sets utilizes many resources:
? proportion of aligned words in src with the
adjective PoS tag.
? rank of central hypothesis (see Section 2.2)
and average edit distance to all other entries
in 10k-best list of Spanish-English backtrans-
lation system;
? language model probability for tgt;
? length of the longest sequence of aligned
words in tgt;
356
? number of occurrences of the target word
within the target hypothesis averaged for all
words in the hypothesis;
? percentage of bigrams in the 4th quartile of
frequency of the source language corpus;
? percentage of trigrams in the 4th quartile of
frequency of the source language corpus;
? number of contiguous unaligned words in
tgt normalized by the length of tgt.
5 Conclusion
This paper presented the participation of FBK
and University of Edinburgh to the WMT 2013
Quality Estimation shared task. Our approach
explored features based on word alignment, n-
best diversity scores, pseudo-references and back-
translations, and word posterior probabilities. We
experimented with two different learning methods,
SVR and extremely randomized trees for predict-
ing sentence-level post-editing time and HTER.
Our submitted systems were particularly suc-
cessful for predicting sentence-level post-editing
time, ranking 1st among seven participants. The
submitted models for predicting HTER consis-
tently overcome the baseline for the task. In addi-
tion to the description of our approach and system
setup, we presented a first analysis of the features
used in our models with the objective of assess-
ing the importance of the features used either for
predicting time or HTER.
6 Acknowledgments
This work was partially funded by the European
Commission under the project MateCat, Grant
287688. The authors want to thank Philipp Koehn
for training two of the models used in Section 2.2.
References
James Bergstra and Yoshua Bengio. 2012. Random
Search for Hyper-Parameter Optimization. Journal
of Machine Learning Research, 13:281?305, March.
Peter F. E Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation :
Parameter Estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation, pages 10?
51, Montreal, Canada, June. Association for Com-
putational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance, SETQA-
NLP ?08, pages 49?57, Stroudsburg, PA, USA.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42, March.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zenz, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007 Demo and Poster Sessions, pages 177?
180, June.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Infor-
mation Disparity in Cross?lingual Documents. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?12),
pages 120?124, Jeju Island, Korea.
Nicolai Meinshausen and Peter Bu?hlmann. 2010.
Stability selection. Journal of the Royal Statis-
tical Society: Series B (Statistical Methodology),
72(4):417?473, July.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2013.
Semeval-2013 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
July.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
357
E. Duchesnay. 2011. Scikit-learn : Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, May. European Language Resources
Association (ELRA).
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. In
Proceedings of the ACL SIGDAT-Workshop.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Association for Machine Translation in
the Americas, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the 7th Workshop on Statistical Machine Transla-
tion, pages 145?151.
Jose? G. C. de Souza, Miquel Espla`-Gomis, Marco
Turchi, and Matteo Negri. 2013. Exploiting qualita-
tive information from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual Meeting
of the Association for Computational Linguistics -
Short Papers (ACL Short Papers 2013).
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search for Statistical Translation.
In Proceedings of the Fifth European Conference
on Speech Communication and Technology, pages
2667?2670, Rhodos, Greece.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003. Evaluation of machine translation and its
evaluation. In In Proceedings of MT Summit IX,
pages 386?393, New Orleans, LA, USA.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence measures for statistical machine
translation. In In Procedings of Machine Transla-
tion Summit IX.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
358
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 322?328,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
FBK-UPV-UEdin participation in the WMT14 Quality Estimation
shared-task
Jos
?
e G. C. de Souza
?
University of Trento
Fondazione Bruno Kessler
Trento, Italy
desouza@fbk.eu
Jes?us Gonz
?
alez-Rubio
?
PRHLT Group
U. Polit`ecnica de Val`encia
Valencia, Spain
jegonzalez@prhlt.upv.es
Christian Buck
?
University of Edinburgh
School of Informatics
Edinburgh, Scotland, UK
cbuck@lantis.de
Marco Turchi, Matteo Negri
Fondazione Bruno Kessler
turchi,negri@fbk.eu
Abstract
This paper describes the joint submission
of Fondazione Bruno Kessler, Universitat
Polit`ecnica de Val`encia and University of
Edinburgh to the Quality Estimation tasks
of the Workshop on Statistical Machine
Translation 2014. We present our submis-
sions for Task 1.2, 1.3 and 2. Our systems
ranked first for Task 1.2 and for the Binary
and Level1 settings in Task 2.
1 Introduction
Quality Estimation (QE) for Machine Translation
(MT) is the task of evaluating the quality of the
output of an MT system without reference transla-
tions. Within the WMT 2014 QE Shared Task four
evaluation tasks were proposed, covering both
word and sentence level QE. In this work we de-
scribe the Fondazione Bruno Kessler (FBK), Uni-
versitat Polit`ecnica de Val`encia (UPV) and Uni-
versity of Edinburgh (UEdin) approach and sys-
tem setup for the shared task.
We developed models for two sentence-level
tasks: Task 1.2, scoring for post-editing effort,
and Task 1.3, predicting post-editing time, and
for all word-level variants of Task 2, binary and
multiclass classification. As opposed to previous
editions of the shared task, this year the partici-
pants were not supplied with the MT system that
was used to produce the translation. Furthermore
no system-internal features were provided. Thus,
while the trained models are tuned to detect the
errors of a specific system the features have to be
generated independently (black-box).
2 Sentence Level QE
We submitted runs to two sentence-level tasks:
Task 1.2 and Task 1.3. The first task aims at
?
Contributed equally to this work.
predicting the Human mediated Translation Edit
Rate (HTER) (Snover et al., 2006) between a sug-
gestion generated by a machine translation sys-
tem and its manually post-edited version. The
data set contains 1,104 English-Spanish sentence
pairs post-edited by one translator (896 for train-
ing and 208 for test). The second task requires
to predict the time, in milliseconds, that was re-
quired to post edit a translation given by a ma-
chine translation system. Participants are provided
with 858 English-Spanish sentence pairs, source
and suggestion, along with their respective post-
edited sentence and post-editing time in seconds
(650 data points for training and 208 for test). We
participated in the scoring mode of both tasks.
2.1 Features
For our sentence-level submissions we compute
features using different resources that do not use
the MT system internals. We use the same set of
features for both Task 1.2 and 1.3.
QuEst Black-box features (quest79). We ex-
tract 79 black-box features that capture the com-
plexity, fluency and adequacy aspects of the QE
problem. These features are extracted using the
implementation provided by the QuEst framework
(Specia et al., 2013). Among them are the 17 base-
line features provided by the task organizers.
The complexity features are computed on the
source sentence and indicate the complexity of
translating the segment. Examples of these fea-
tures are the language model (LM) probabilities
of the source sentence computed in a corpus of the
source language, different surface counts like the
number of punctuation marks and the number of
tokens in the source sentence, among others.
The fluency features are computed over the
translation generated by the MT system and in-
dicate how fluent the translation is in the target
322
language. One example would again be the LM
probability of the translation given by a LM model
trained on a corpus of the target language. Another
example is the average number of occurrences of
the target word within the target segment.
The third aspect covered by the QuEst features
is the adequacy of the translation with respect to
the source sentence, i.e., how the meaning of the
source is preserved in the translation. Examples of
features are the ratio of nouns, verbs and adjectives
in the source and in the translation. For a more
detailed description of the features in this group
please refer to (Specia et al., 2013).
Word alignment (wla). Following our last
year?s submission (de Souza et al., 2013a) we ex-
plore information about word alignments to ex-
tract quantitative (amount and distribution of the
alignments) and qualitative features (importance
of the aligned terms). Our assumption is that
features that explore what is aligned can bring
improvements to tasks where sentence-level se-
mantic relations need to be identified. We train
the word alignment models with the MGIZA++
toolkit (Gao and Vogel, 2008) implementation of
the IBM models (Brown et al., 1993). The models
are built on the concatenation of Europarl, News
Commentary, and MultiUN parallel corpora made
available in the QE shared task of 2013, compris-
ing about 12.8 million sentence pairs. A more de-
tailed description of the 89 features extracted can
be found in (de Souza et al., 2013a; de Souza et
al., 2013b).
Word Posterior Probabilities (wpp). Using an
external SMT system we produce 100k-best lists
from which we derive Word Posterior Probabili-
ties as detailed in Subsection 3.1.
We use the geometric mean of these probabili-
ties to derive a sentence-level score.
Because the system that we use to produce the
N-best list is not the same that generated the sug-
gestions some suggested words never appear in the
N-best list and thus receive zero probability. To
overcome this issue we first clip the WPPs to a
minimum probability. Using a small sample of the
data to estimate this number we arrive at:
log(p)
min
= ?2.
N-best diversity (div). Using the same 100k-
best list as above we extract a number of measures
that grasp the spatial distribution of hypotheses in
the search space as described in (de Souza et al.,
2013a).
Word Prediction (wpred). We introduce the
use of the predictions provided by the word-level
QE system described in Section 3 to leverage in-
formation for the sentence-level tasks. We com-
bine the binary word-level predictions in different
ways, with the objective of measuring the fluency
of the translation in a more fine-grained way. We
target a quantitative aspect of the words by com-
puting ratios of OK or BAD predictions. Further-
more, we also explore a qualitative aspect by cal-
culating ratios of different classes of words given
by their part-of-speech tags, indicating the qual-
ity of distinct meaningful regions that compose the
translation sentence. In total, we compute 18 fea-
tures:
? number of OK predictions divided by the no.
of words in the translation sentence (1 fea-
ture);
? number of OK function/content words predic-
tions divided by the no. of function/content
words in the translation (2 features);
? number of OK nouns, verbs, proper-nouns,
adjective, pronouns predictions divided by
the total nouns, verbs, proper-nouns, adjec-
tive, pronouns (5 features);
? size of the longest sequence of OK/BAD word
predictions divided by the total number of
OK/BAD predictions in the translation (2 fea-
tures);
? number of OK predicted n-grams divided by
the total number of n-grams in the transla-
tion. We vary n from 2 to 5 (4 features);
? number of words predicted as OK in the
first/second half of the translation divided by
the total number of words in the first/second
half of the translation (2 features).
? number of words predicted as OK in the
first/second quarter of the translation di-
vided by the total number of words in the
first/second quarter of the translation (2 fea-
tures).
For some instances of the sentence-level tasks
we were not able to produce word-level predic-
tions due to an incomplete overlap between the
word-level and sentence-level tasks datasets. For
such data points we use the median of the feature
column for Task 1.2 and the mean for Task 1.3.
323
Method Features Train T1.2 Train T1.3 Test T1.2 Test T1.3
SVR baseline 16.90 16864 15.23 21490
ET baseline 16.25 17888 17.73 19400
ET quest79 + wla + wpp 15.62 17474 14.44 18658
ET quest79 + wla + wpp + div
2
15.57 17471 14.38 18693
ET quest79 + wla + wpp + div + wpred
1
15.05 16392 12.89 17477
Table 1: Training and test results for Task 1.2 and 1.3. Scores are the MAE on a development set
randomly sampled from the training data (20%). Baseline features were provided by the shared task
organizers. We used Support Vector Machines (SVM) regression to train the baseline models (first row).
Submissions are marked with
1
and
2
for primary and secondary, respectively.
2.2 Experimental Setup
We build the sentence-level models for both tasks
(T1.2 and T1.3) with the features described in Sec-
tion 2.1 using one learning algorithm: extremely
randomized trees (ET) (Geurts et al., 2006). ET is
an ensemble of randomized trees in which each
decision tree can be parameterized differently.
When a tree is built, the node splitting step is done
at random by picking the best split among a ran-
dom subset of the input features. All the trees
are grown on the whole training set and the re-
sults of the individual trees are combined by aver-
aging their predictions. The models produced by
this method demonstrated to be robust to a large
number of input features. For our experiments and
submissions we used the ET implementation in-
cluded in the Scikit-learn library (Pedregosa et al.,
2011).
During training we evaluate the models on a
development set. The development set was ob-
tained by randomly sampling 20% of the training
data. The remaining 80% were used for training.
The training process was carried out by optimiz-
ing the ET hyper-parameters with 100 iterations
of random search optimization (Bergstra and Ben-
gio, 2012) set to minimize the mean absolute er-
ror (MAE)
1
on 10-fold cross-validation over the
training data. The ET hyper-parameters optimized
are: the number of decision trees in the ensemble,
the maximum number of features to consider when
looking for the best split, the maximum depth of
the trees used in the ensembles, the minimal num-
ber of samples required to split a node of the tree,
and the minimum number of samples in newly cre-
ated leaves. For the final submissions we run the
random search with 1000 iterations over the whole
training dataset.
1
Given by MAE =
?
N
i=1
|H(s
i
)?V (s
i
)|
N
, where H(s
i
) is
the hypothesis score for the entry s
i
and V (s
i
) is the gold
standard value for s
i
in a dataset with N entries.
2.3 Results
We train models on different combinations of fea-
ture groups (described in Section 2.1). Experi-
ments results are summarized in Table 1. We have
results with baseline features for both SVR and the
ET models. For Task 1.2, adding features from dif-
ferent groups leads to increasing improvements.
The combination of the quest79, wla and wpp
groups outperforms the SVR baseline for Task 1.2
but not for Task 1.3. However, when compared
to the ET model trained with the baseline fea-
tures, it is possible to observe improvements with
this group of features. In addition, adding the
div group on top of the previous three leads to
marginal improvements for both tasks. The best
feature combination is given when adding the fea-
tures based on the word-level predictions, config-
uring the combination of all the feature groups to-
gether (a total of 221 features). For both tasks
this is our primary submission. The contrastive
run for both tasks is the best feature group com-
bination without the word-prediction-based fea-
tures, quest79, wla, wpp and div for Task 1.2 and
quest79, wla, wpp for Task 1.3.
Results on the test set can be found in the two
last columns of Table 1 and are in line with what
we found in the training phase. The rows that do
not correspond to the official submissions and that
are reported on the test set are experiments done
after the evaluation phase. For both tasks the im-
provements increase as we add features on top of
the baseline feature set and the best performance
is reached when using the word prediction fea-
tures with all the other features. The SVR base-
lines performance are the official numbers pro-
vided by the organizers. For Task 1.2 our primary
submission achieves a MAE score lower than the
score achieved during the training phase, show-
ing that the model is robust. For Task 1.3, how-
ever, we do not observe such trend. Even though
324
the primary submission for this task consistently
improves over the other feature combinations, it
does not outperform the score obtained during the
training phase. This might be explained due to
the difference in the distribution between train-
ing and test labels. In Task 1.2 the two distri-
butions are more similar than in Task 1.3, which
presents slightly different distributions between
training and test data.
3 Word-Level QE
Task 2 is the word-level quality estimation of auto-
matically translated news sentences without given
reference translations. Participants are required to
produce a label for each word in one or more of
the following settings:
Binary classification: a OK/BAD label, where
BAD indicates the need for editing the word.
Level1 classification: OK, Accuracy, or
Fluency label specifying a coarser level of
errors for each word, or OK for words with
no error.
Multi-Class classification: one of the 20 error la-
bels described in the shared-task description
or OK for words with no error.
We submit word-level quality estimations for
the English-Spanish translation direction. The cor-
pus contains 1957 training sentences for a total of
47411 Spanish words, and 382 test sentences for a
total of 9613 words.
3.1 Features
Word Posterior Probabilities (WPP) In order
to generate an approximation of the decoder?s
search space as well as an N-best list of possi-
ble translations we re-translate the source using
the system that is available for the 2013 WMT QE
Shared Task (Bojar et al., 2013).
Certainly, there is a mismatch between the orig-
inal system and the one that we used but, since our
system was trained using the same news domain
as the QE data, we assume that both face similar
ambiguous words or possible reorderings. Using
this system we generate a 100k-best list which is
the foundation of several features.
We extract a set of word-level features based on
posterior probabilities computed over N-best lists
as proposed by previous works (Blatz et al., 2004;
Ueffing and Ney, 2007; Sanchis et al., 2007).
Consider a target word e
i
belonging to a transla-
tion e = e
1
. . . e
i
. . . e
|e|
generated from a source
sentence f . Let N (f) be the list of N-best trans-
lations for f . We compute features as the nor-
malized sum of probabilities of those translations
S(e
i
) ? N (f) that ?contain? word e
i
:
1
?
e
??
?N (f)
P(e
??
| f)
?
e
?
?S(e
i
)
P(e
?
| f) (1)
where P(e | f) is the probability translation e given
source sentence f according to the SMT model.
We follow (Zens and Ney, 2006) and extract
three different WPP features depending on the
criteria chosen to compute S(e
i
):
S(e
i
) = {e
?
? N (f) | a=Le(e
?
, e)?e
?
a
i
= e
i
}
S(e
i
) contain those translations e
?
for which the
word Levenshtein-aligned (Levenshtein, 1966) to
position i in e is equal to e
i
.
S(e
i
) = {e
?
? N (f) | e
?
i
= e
i
}
A second option is to select those translations
e
?
that contain the word e
i
at position i.
S(e
i
) = {e
?
? N (f) | ?i
?
: e
?
i
?
= e
i
}
As third option, we select those translations e
?
that contain the word e
i
, disregarding its position.
Confusion Networks (CN) We use the same N-
best list used to compute the WPP features in the
previous section to compute features based on the
graph topology of confusion networks (Luong et
al., 2014). First, we Levenshtein-align all trans-
lations in the N-best list using e as skeleton, and
merge all of them into a confusion network. In this
network, each word-edge is labelled with the pos-
terior probability of the word. The output edges of
each node define different confusion sets of words,
each word belonging to one single confusion set.
Each complete path passing through all nodes in
the network represents one sentence in the N-best
list, and must contain exactly one link from each
confusion set. Looking to the confusion set which
the hypothesis word belongs to, we extract four
different features: maximum and minimum proba-
bility in the set (2 features), number of alternatives
in the set (1 feature) and entropy of the alternatives
in the set (1 feature).
Language Models (LM) As language model
features we produced n-gram length/backoff be-
haviour and conditional probabilities for every
word in the sentence. We employed both an inter-
polated LM taken from the MT system discussed
325
in Section 3 as well as a very large LM which we
built on 62 billion tokens of monolingual data ex-
tracted from Common Crawl, a public web crawl.
While generally following the procedure of Buck
et al. (2014) we apply an additional lowercasing
step before training the model.
Word Lexicons (WL) We compute two dif-
ferent features based on statistical word lexi-
cons (Blatz et al., 2004):
Avg. probability:
1
|f |+1
?
|f |
j=0
P(e
i
| f
j
)
Max. probability: max
0?j?|f |
P(e
i
| f
j
)
where P(e | f) is a probabilistic lexicon, and f
0
is
the source ?NULL? word (Brown et al., 1993).
POS tags (POS) We extract the part-of-speech
(POS) tags for both source and translation sen-
tences using TreeTagger (Schmid, 1994). We use
the actual POS tag of the target word as a feature.
Specifically, we represent it as a one-hot indicator
vector where all values are equal to zero except
the one representing the current tag of the word,
which is set to one. Regarding the source POS
tags, we first compute the lexical probability of
each target word given each source word. Then,
we compute two different feature vectors for each
target word. On the one hand, we use an indica-
tor vector to represent the POS tag of the maxi-
mum probability source word. On the other hand,
we sum up the indicator vectors for all the source
words each one weighted by the lexical probability
of the corresponding word. As a result, we obtain
a vector that represents the probability distribution
of source POS tags for each target word. Addi-
tionally, we extract a binary feature that indicates
whether the word is a stop word or not.
2
Stacking (S) Finally, we also exploit the diverse
granularity of the word labels. The word classes
for the Level1 and Multi-class conditions are fine
grained versions of the Binary annotation, i.e. the
OK examples are the same for all cases.
We re-use our binary predictions as an addi-
tional feature for the finer-grained classes. How-
ever, due to time constrains, we were not able to
run the proper nested cross-validation but used a
model trained on all available data, which there-
fore over-fits on the training data. Cross-validation
results using the stacking approach are thus very
optimistic.
2
https://code.google.com/p/stop-words/
3.2 Classifiers
We use bidirectional long short-term memory
recurrent neural networks (BLSTM-RNNs) as
implemented in the RNNLib package (Graves,
2008). Recurrent neural networks are a connec-
tionist model containing a self-connected hidden
layer. The recurrent connection provides informa-
tion of previous inputs, hence, the network can
benefit from past contextual information. Long
short-term memory is an advanced RNN archi-
tecture that allows context information over long
periods of time. Finally, BLSTM-RNNs com-
bine bidirectional recurrent neural networks and
the long short-term memory architecture allowing
forward and backward context information. Us-
ing such context modelling classifier we can avoid
the use of context-based features that have been
shown to lead to only slight improvements in QE
accuracy (Gonz?alez-Rubio et al., 2013).
As a secondary binary model we train a CRF.
Our choice of implementation is Pocket CRF
3
which, while currently unmaintained, implements
continuous valued features. We use a history of
size 2 for all features and perform 10-fold cross-
validation, training on 9 folds each time.
3.3 Experimental Setup
The free parameters of the BLSTM-RNNs are op-
timized by 10-fold cross-validation on the train-
ing set. Each cross-validation experiment con-
sider eight folds for training, one held-out fold
for development, and a final held-out fold for test-
ing. We estimate the neural network with the eight
training folds using the prediction performance in
the validation fold as stopping criterion. The re-
sult of each complete cross-validation experiment
is the average of the results for the predictions of
the ten held-out test folds. Additionally, to avoid
noise due to the random initialization of the net-
work, we repeat each cross-validation experiment
ten times and average the results. Once the opti-
mal values of the free parameters are established,
we estimate a new BLSTM-RNN using the full
training corpus and we use it as the final model
to predict the class labels of the test words.
Since our objective is to detect words that need
to be edited, we use the weighted averaged F
1
score over the different class labels that denote an
error as our main performance metric (wF1
err
).
We also report the weighted averaged F
1
scores
3
http://pocket-crf-1.sourceforge.net/
326
Binary Level1 MultiClass
Method Features wF1
err
wF1
all
wF1
err
wF1
all
wF1
err
wF1
all
BLSTM-RNNs LM+WPP+CN+WL 35.9 63.0 23.7 59.4 10.7 55.5
+POS 38.5
1
62.7 26.7
1
59.5 12.7
1
55.5
+Stacking ? ? 82.9
2
93.9 64.7
2
88.0
CRF LM+WPP+CN+WL+POS 39.5
2
62.4 0 ? ? ? ?
Table 2: Cross-validation results for the different setups tested for Task 2. Our two submissions are
marked as (
1
) and (
2
) respectively.
over all the classes (wF1
all
).
3.4 Results
Table 2 presents the wF1
err
and wF1
all
scores
for different sets of features. Our initial experi-
ment includes language model (LM), word poste-
rior probability (WPP), confusion network (CN),
and word lexicon (WL) features for a total of 11
features. We extend this basic feature set with the
indicator features based on POS tags for a total of
163 features. We further extend the feature vectors
by adding the stacking feature in a total of 164 fea-
tures.
Analyzing the results we observe that prediction
accuracy is quite low. Our hypothesis is that this is
due to the skewed class distribution. Even for the
binary classification scenario (the most balanced
of the three conditions), OK labels account for two
thirds of the samples. This effect worsens with in-
creasing number of error classes and the resulting
sparsity of observations. As a result, the system
tends to classify all samples as OK which leads to
the low F
1
scores presented in Table 2.
We can observe that the use of POS tags indica-
tor features clearly improved the prediction accu-
racy of the systems in the three conditions. This
setup is our primary submission for the three con-
ditions of task 2.
In addition, we observe that the use of the stack-
ing feature provides a considerable improvement
in prediction accuracy for Level1 and MultiClass.
As discussed above the cross-validation results for
the stacking features are very optimistic. Test pre-
dictions using this setup are our contrastive sub-
mission for Level1 and MultiClass conditions.
Results achieved on the official test set can be
found in Table 3. Much in line with our cross-
validation results the stacking-features prove help-
ful, albeit by a much lower margin. For the bi-
nary task the RNN model strongly outperforms the
CRF.
Setup Binary Level1 MultiClass
BLSTM-RNN 48.7 37.2 17.1
+ Stacking ? 38.5 23.1
CRF 42.6 ? ?
Table 3: Test results for Task 2. Numbers are
weighted averaged F
1
scores (%) for all but the
OK class.
4 Conclusion
This paper describes the approaches and system
setups of FBK, UPV and UEdin in the WMT14
Quality Estimation shared-task. In the sentence-
level QE tasks 1.2 (predicting post-edition effort)
and 1.3 (predicting post-editing time, in ms) we
explored different features and predicted with a
supervised tree-based ensemble learning method.
We were able to improve our results by explor-
ing features based on the word-level predictions
made by the system developed for Task 2. Our best
system for Task 1.2 ranked first among all partici-
pants.
In the word-level QE task (Task 2), we explored
different sets of features using a BLSTM-RNN as
our classification model. Cross-validation results
show that POS indicator features, despite sparse,
were able to improve the results of the baseline
features. Also, the use of the stacking feature pro-
vided a big leap in prediction accuracy. With this
model, we ranked first in the Binary and Level1
settings of Task 2 in the evaluation campaign.
Acknowledgments
This work was supported by the MateCat and Cas-
macat projects, which are funded by the EC un-
der the 7
th
Framework Programme. The authors
would like to thank Francisco
?
Alvaro Mu?noz for
providing the RNN classification software.
327
References
James Bergstra and Yoshua Bengio. 2012. Random
Search for Hyper-Parameter Optimization. Journal
of Machine Learning Research, 13:281?305.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the international conference on Computational Lin-
guistics, pages 315?321.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram Counts and Language Models from
the Common Crawl. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
Jos?e G. C. de Souza, Christian Buck, Marco Turchi,
and Matteo Negri. 2013a. FBK-UEdin participation
to the WMT13 Quality Estimation shared-task. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 352?358.
Jos?e G. C. de Souza, Miquel Espl?a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting qual-
itative information from automatic word alignment
for cross-lingual nlp tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 771?776.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Jes?us Gonz?alez-Rubio, Jos?e R. Navarro-Cerdan, and
Francisco Casacuberta. 2013. Partial least squares
for word confidence estimation in machine transla-
tion. In 6th Iberian Conference on Pattern Recog-
nition and Image Analysis, (IbPRIA) LNCS 7887,
pages 500?508. Springer.
Alex Graves. 2008. Rnnlib: A recurrent neural
network library for sequence learning problems.
http://sourceforge.net/projects/
rnnl/.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014. Word confidence estimation and
its integration in sentence quality estimation for ma-
chine translation. In Knowledge and Systems Engi-
neering, volume 244, pages 85?98. Springer.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn : Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Alberto Sanchis, Alfons Juan, and Enrique Vidal.
2007. Estimation of confidence measures for ma-
chine translation. In Proceedings of the Machine
Translation Summit XI, pages 407?412.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Association for Machine Translation in
the Americas.
Lucia Specia, Kashif Shah, Jos?e G. C. de Souza, and
Trevor Cohn. 2013. QuEst?a translation quality es-
timation framework. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, pages 79?84.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33:9?40.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 72?77.
328
Proceedings of the 4th International Workshop on Computational Terminology, pages 22?31,
Dublin, Ireland, August 23 2014.
Identification of Bilingual Terms from Monolingual Documents for
Statistical Machine Translation
Mihael Arcan1 Claudio Giuliano2 Marco Turchi2 Paul Buitelaar1
1 Unit for Natural Language Processing, Insight @ NUI Galway, Ireland
{mihael.arcan , paul.buitelaar}@insight-centre.org
2 FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
{giuliano, turchi}@fbk.eu
Abstract
The automatic translation of domain-specific documents is often a hard task for generic Sta-
tistical Machine Translation (SMT) systems, which are not able to correctly translate the large
number of terms encountered in the text. In this paper, we address the problems of automatic
identification of bilingual terminology using Wikipedia as a lexical resource, and its integration
into an SMT system. The correct translation equivalent of the disambiguated term identified in
the monolingual text is obtained by taking advantage of the multilingual versions of Wikipedia.
This approach is compared to the bilingual terminology provided by the Terminology as a Ser-
vice (TaaS) platform. The small amount of high quality domain-specific terms is passed to the
SMT system using the XML markup and the Fill-Up model methods, which produced a relative
translation improvement up to 13% BLEU score points
1 Introduction
Translation tasks often need to deal with domain-specific terms in technical documents, which require
specific lexical knowledge of the domain. Nowadays, SMT systems are suitable to translate very frequent
expressions but fail in translating domain-specific terms. This mostly depends on a lack of domain-
specific parallel data from which the SMT systems can learn. Translation tools such as Google Translate
or open source phrase-based SMT systems, trained on generic data, are the most common solutions and
they are often used to translate manuals or very specific texts, resulting in unsatisfactory translations.
This problem is particular relevant for professional translators that work with documents coming from
different domains and are supported by generic SMT systems. A valuable solution to help them in han-
dling domain-specific terms is represented by online terminology resources, e.g. IATE - Inter-Active
Terminology for Europe,1 which are continuously updated and can be easily queried. However, the man-
ual use of these services can be very time demanding. For this reason, the identification and embedding
of domain-specific terms in an SMT system is a crucial step towards increasing translator productivity
and translation quality in highly specific domains.
In this paper, we propose an approach to automatically detect monolingual domain-specific terms from
a source language document and identify their equivalents using Wikipedia cross-lingual links. For this
purpose we extend The Wiki Machine API,2 a tool for linking terms in text to Wikipedia pages, adding
two more components able to first identify domain-specific terms, and to find their translations in a target
language. The identified bilingual terms are then compared with those obtained by TaaS (Skadins? et al.,
2013). The embedding of the domain-specific terms into an SMT system is performed by use of the
XML markup approach, which uses the terms as preferred translation candidates at run time, and the
Fill-Up model (Bisazza et al., 2011), which emphasizes phrase pairs extracted from the bilingual terms.
Our results show that the performance of our technique and TaaS are comparable in the identification
of monolingual and bilingual domain-specific terms. From the machine translation point of view, our
experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative
improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1 http://iate.europa.eu/ 2 https://bitbucket.org/fbk/thewikimachine/
Terminology questions in texts authored by patients
Noemie Elhadad
Department of Biomedical Informatics
Columbia University, USA
noemie@dbmi.columbia.edu
This work is c er a Creative Com ons Attribution 4.0 Internatio l License. Page numb rs and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
22
2 Methodology
Given a source document, it is processed by our pipeline that: (i) with the help of The Wiki Machine, it
identifies, disambiguates and links all terms in the document to the Wikipedia pages; (ii) the terms and
their links are used to identify the domain of the document and filter out the terms that are not domain-
specific; (iii) the translation of such terms is obtained following the Wikipedia cross-lingual links; (iv)
the bilingual domain-specific terms are embedded into the SMT system using different strategies. In the
rest of this section, each step is described in detail.
2.1 Bilingual Term Identification
Term Detection and Linking The Wiki Machine is a tool for linking terms in text to Wikipedia pages
and enriching them with information extracted from Wikipedia and Linked Open Data (LOD) resources
such as DBPedia or Freebase. The Wiki Machine has been preferred among other approaches because it
achieves the best performance in term disambiguation and linking (Mendes et al., 2011), and facilitates
the extraction of structured information from Wikipedia.
The annotation process consists of a three-step pipeline based on statistical and machine learning
methods that exclusively uses Wikipedia to train the models. No linguistic processing, such as stemming,
morphology analysis, POS tagging, or parsing, is performed. This choice facilitates the portability of the
system as the only requirement is the existence of a Wikipedia version with a sufficient coverage for the
specific language and domain. The first step identifies and ranks the terms by relevance using a simple
statistical approach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated
and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages.
The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must
be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense,
a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses
an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano
et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means
of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The
third step enriches the linked terms using information extracted from Wikipedia and LOD resources.
The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e.,
orthographical and morphological variants, synonyms, and related terms), images, topic, type, cross
language links, etc. For example, in the text ?click right mouse key to pop up menu and Gnome panel?,
The Wiki Machine identifies the terms mouse, key, pop up menu and Gnome panel. For the ambiguous
term mouse, the linking algorithm returns the Wikipedia page ?Mouse (computing)?, and the other terms
used to link that page in Wikipedia with their frequency, i.e., computer mouse, mice, and Mouse.
In the context of the experiments reported here, we were specifically interested in the identification of
domain-specific bilingual terminology to be embedded into the SMT system. For this reason, we extend
The Wiki Machine adding the functionality of filtering out terms that do not belong to the document
domain, and of automatically retrieving term translations.
Domain Detection To identify specific terms, we assign a domain to each linked term in a text, after
that we obtain the most frequent domain and filter out the terms that are out of scope. In the example
above, the term mouse is accepted because it belongs to the domain computer science, as the majority of
terms (mouse, pop up menu and Gnome panel), while the term key in the domain music is rejected.
The large number of languages and domains to cover prevents us from using standard text classification
techniques to categorize the document. For this reason, we implemented an approach based on the
mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia
categories are created and assigned by different human editors, and are therefore less rigorous, coherent
and consistent than usual ontologies. In addition, the Wikipedia?s category hierarchy forms a cyclic graph
(Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a
hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural
Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity,
allows us reducing the number of domains to few tens instead of some hundred thousands (800,000
23
categories in the English Wikipedia) and does not require any language-specific training data. Wikipedia
categories that contain more pages (?1,000) have been manually mapped to WordNet domains. The
domain for a term is obtained as follows. First, for each term, we extract its set of categories, C, from
the Wikipedia page linked to it. Second, by means of a recursive procedure, all possible outgoing paths
(usually in a large number) from each category in C are followed in the graph of Wikipedia categories.
When one of the mapped categories to a WordNet domain is found, the approach stops and associates the
relative WordNet domain to the term. In this way, more and more domains are assigned to a single term.
Third, to isolate the most relevant one, these domains are ranked according the number of times they have
been found following all the paths. The most frequent domain is assigned to the terms. Although this
process needs the human intervention for the manual mapping, it is done once and it is less demanding
than annotating large amounts of training documents for text classification, because it does not require
the reading of the document for topic identification.
Bilingual Term Extraction The last phase consists in finding the translation of the domain terminol-
ogy. We exploit the Wikipedia cross-language links, which, however, provide an alignment at page level
not at term level. To deal with this issue we introduced the following procedure. If the term is equal to
the source page title (ignoring case) we return the target page; otherwise, we return the most frequent al-
ternative form of the term in the target language. From the previous example, the system is able to return
the Italian page Mouse and all terms used in the Italian Wikipedia to express this concept of Mouse in
computer science. Using this information, the term mouse is paired with its translation into Italian.
2.2 Integration of Bilingual Terms into SMT
A straightforward approach for adding bilingual terms to the SMT system consists of concatenating the
training data and the terms. Although it has been shown to perform better than more complex techniques
(Bouamor et al., 2012), it is still affected by major disadvantages that limits its use in real applications.
In particular, when small amounts of bilingual terms are concatenated with a large training dataset, terms
with ambiguous translations are penalised, because the most frequent and general translations often
receive the highest probability, which drives the SMT system to ignore specific translations.
In this paper, we focus on two techniques that give more priority to specific translations than generic
ones: the Fill-Up model and the XML markup approach. The Fill-Up model has been developed to
address a common scenario where a large generic background model exists, and only a small quantity
of in-domain data can be used to build an in-domain model. Its goal is to leverage the large coverage
of the background model, while preserving the domain-specific knowledge coming from the in-domain
data. Given the generic and the in-domain phrase tables, they are merged. For those phrase pairs that
appear in both tables, only one instance is reported in the Fill-Up model with the largest probabilities
according to the tables. To keep track of a phrase pair?s provenance, a binary feature that penalises if
the phrase pair comes from the background table is added. The same strategy is used for reordering
tables. In our experiments, we use the bilingual terms identified from the source data as in-domain
data. Word alignments are computed on the concatenation of the data. Phrase extraction and scoring
are carried out separately on each corpus. The XML markup approach makes it possible to directly pass
external knowledge to the decoder, specifying translations for particular spans of the source sentence. In
our scenario, the source term is used to identify a span in the source sentence, while the target term is
directly passed to the decoder. With the setting exclusive, the decoder uses only the specified translations
ignoring other possible translations in the translation model.
3 Experimental Setting
In our experiments, we used different English-Italian and Italian-English test sets from two domains: (i)
a small subset of the GNOME project data3 (4,3K tokens) and KDE4 Data4 (9,5K) for the IT domain
and (ii) a subset of the EMEA corpus (11K) for the medical domain.
In order to assess the quality of the monolingual and bilingual terms, we create a terminological gold
standard. Two annotators with a linguistic background and English and Italian proficiency were asked
3 https://l10n.gnome.org/ 4 http://i18n.kde.org/
24
to mark all domain-specific terms in a set of 66 English and Italian documents of the GNOME corpus,
and a set of 100 paragraphs (4,3K tokens) from the KDE4 corpus.5 Domain-specificity was defined as
all (multi-)words that are typically used in the IT domain and that may have different Italian translations
in other domains. The average Cohen?s Kappa of GNOME and KDE anno computed at token level was
0.66 for English and 0.53 for Italian. Following Landis and Koch (1977), this corresponds to a substantial
and moderate agreement between the annotators.
Finally the gold standard dataset was generated by the intersection of the annotations of the two an-
notators. In detail, for the GNOME dataset the annotators marked 93 single-word and 134 multi-word
expressions (MWEs), resulting 227 terms in overall. For the KDE anno dataset, 321 monolingual terms
for the GNOME dataset were annotated, whereby 192 of them were multi-word expressions. This results
in 190 unique bilingual terms for the GNOME corpus and 355 for the KDE anno dataset.
We compare the monolingual and bilingual terms identified by our approach to the terms obtained
by the online service TaaS,6 which is a cloud-based platform for terminology services based on the
state-of-the-art terminology extraction and bilingual terminology alignment methods. TaaS provides
several options in term identification, of which we selected TWSC, Tilde wrapper system for CollTerm,
(Pinnis et al., 2012). TWSC is based on linguistic analysis, i.e. part of speech tagging and morpho-
syntactic patterns, enriched with statistical features. TaaS allows for lookup in several manually and
automatically built monolingual and bilingual terminological resources and for our experiment we use
EuroTermBank (ETB), Taus Data and Web Data. Accessing several resources, TaaS may provide several
translations for a unique source term, but not an indicator of their translation quality. To avoid assigning
the same probability to all the translations of the same source term, we prioritise a translation by the
resource it was provided. In our case, we favour first the translation provided by ETB. If no translation
is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting
the term extraction approach, TaaS requires manual specification of the source and target languages, the
domain, and the source document. Since we focused on the IT and medical domains we set the options
to ?Information and communication technology? and ?Medicine and pharmacy?, respectively.
For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where
the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit
(Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage,
we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl
(Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of
?37M tokens and a development set of ?10K tokens.
In our experiments, an instance of Moses trained on the generic parallel dataset was used in three
different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup
approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the
Fill-Up method as background translation model.
4 Evaluation
In this Section, we report the performance of the different term identification tools and term embedding
methods for the two domains: IT and the medical domain. For evaluating the extracted monolingual
and bilingual terms, we calculate precision, recall and f-measure using the manually labelled KDE anno
and GNOME datasets. In addition, we perform a manual inspection of a subset of the bilingual identi-
fied terms. The BLEU metric (Papineni et al., 2002) was used to automatically evaluate the quality of
the translations. The metric calculates the overlap of n-grams between the SMT system output and a
reference translation, provided by a professional translator.
4.1 Monolingual Term Identification
In Table 1, the column ?Ident.? represents the number of identified terms for each tool, whereby we
observed TaaS always extracts more terms than The Wiki Machine. While extracting Italian terms,
TaaS extracts twice as more terms as The Wiki Machine, which can be explained by the overall lower
5 In the rest of the paper, we refer to the annotated part of KDE4 as KDE anno
6 https://demo.taas-project.eu/
25
English Italian
KDE anno Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 431 144 287 0.442 0.594 0.507 518 147 371 0.326 0.511 0.398
The Wiki Machine 327 247 80 0.400 0.406 0.403 207 184 23 0.429 0.268 0.330
GNOME Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 311 119 192 0.260 0.355 0.301 359 110 249 0.272 0.415 0.329
The Wiki Machine 275 199 76 0.303 0.364 0.330 196 167 29 0.331 0.275 0.301
Table 1: Evaluation of monolingual term identification for the KDE anno and GNOME dataset.
amount of Italian pages in Wikipedia compared to the English version. Focusing on the amount of
identified single-word and multi-word expressions, it is interesting to notice that TaaS, independently of
the language, extracts around twice as more MWEs than single words. Differently, The Wiki Machine
identifies mostly single-word terms, whereby they represent around three-fourth of all identified terms
for English and around 12% for Italian.
For the KDE anno dataset, TaaS in most cases (except in precision for the Italian KDE anno dataset)
outperforms The Wiki Machine approach in all metrics. Especially we observed a higher recall produced
by the TaaS approach, which can be deduced from the higher number of extracted MWEs compared to
The Wiki Machine approach. On the English GNOME dataset, The Wiki Machine performs comparable
results to TaaS, with a slightly higher recall and F1. On the Italian side, The Wiki Machine identifies less
MWEs than TaaS, which results in a low recall and F1.
In summary, we observe that TaaS performs best on the KDE anno dataset, whereas The Wiki Machine
and TaaS perform comparable results on the GNOME dataset. Analysing the overall results, we notice
that precision, recall and F1 are generally better in English than in Italian. This is due to the fact that
Italian tends to use more words to express the same concept compared to English.
4.2 Bilingual Term Identification
Table 2 reports the performance of The Wiki Machine and TaaS in the identification of bilingual terms
evaluated against the manually produced list of terms. In both language pairs and datasets, TaaS and The
Wiki Machine mostly identify similar amounts of bilingual terms (column ?Ident.?) and match with the
gold standard (column ?Mat.?). Only for KDE anno, It?En, TaaS identifies almost 50% more bilingual
terms than The Wiki Machine.
It is worth noticing that, although TaaS is accessing high quality manually-produced termbases, e.g.
ETB in our results, there is no evidence that it works significantly better than The Wiki Machine access-
ing Wikipedia. In fact, in terms of F1, The Wiki Machine performs best on the GNOME annotated test
set, while it is outperformed by TaaS on KDE anno. In both cases, differences in performance are mini-
mal. According to the precision measure, The Wiki Machine seems to be able to produce more accurate
bilingual terms.
The automatic evaluation shows difficulties (low F1 scores) for The Wiki Machine and TaaS in iden-
tifying bilingual terms that perfectly match the gold standard. To better understand the quality of term
translations, we asked one of the annotators involved in the creation of the gold standard to perform a
manual evaluation of a subset of fifty bilingual terms randomly selected from each list. We used the
four error categories proposed in (Aker et al., 2013): 1) The terms are exact translations of each other
in the domain; 2) Inclusion: Not an exact translation, but an exact translation of one term is entirely
contained within the term in the other language; 3) Overlap: Not category 1 or 2, but the terms share at
least one translated word; 4) Unrelated: No word in either term is a translation of a word in the other.
The percentages of bilingual terms assigned to each class are shown in Table 3.
In terms of comparison between the two tools, the manual evaluation confirms that there is no evidence
that a tool produces better term translations than the other in all the test sets. In fact, except for KDE anno
En?It where TaaS outperforms The Wiki Machine, the percentage of bilingual terms assigned to class
1 for both the tools is almost similar. In terms of absolute scores, the manual evaluation shows that
the quality of the identified bilingual terms is relatively high (merging the terms assigned to classes 1
26
GNOME En?It Ident. Mat. Precision Recall F1
TaaS 145 20 0.138 0.105 0.119
The Wiki Machine 156 25 0.160 0.130 0.144
GNOME It?En Ident. Mat. Precision Recall F1
TaaS 139 21 0.151 0.110 0.127
The Wiki Machine 140 23 0.164 0.121 0.139
KDE anno En?It Ident. Mat. Precision Recall F1
TaaS 249 65 0.261 0.183 0.215
The Wiki Machine 229 49 0.202 0.138 0.164
KDE anno It?En Ident. Mat. Precision Recall F1
TaaS 228 58 0.254 0.163 0.199
The Wiki Machine 155 48 0.292 0.135 0.185
Table 2: Automatic evaluation of bilingual terms ex-
tracted from GNOME and KDE anno.
GNOME En?It 1 2 3 4
TaaS 0.66 0.08 0.00 0.26
The Wiki Machine 0.70 0.08 0.06 0.16
GNOME It?En 1 2 3 4
TaaS 0.78 0.08 0.02 0.12
The Wiki Machine 0.68 0.12 0.04 0.16
KDE anno En?It 1 2 3 4
TaaS 0.90 0.00 0.06 0.04
The Wiki Machine 0.70 0.10 0.06 0.14
KDE anno It?En 1 2 3 4
TaaS 0.70 0.10 0.10 0.10
The Wiki Machine 0.64 0.22 0.08 0.06
Table 3: Manual evaluation of bilingual terms
based on four error categories (1-4).
and 2, we reach a score, in most of the cases, larger than 80%). This is in contrast with the automatic
evaluation, which reports limited performances (F1 ? 0.2) for both methods. The main reason is that
the automatic evaluation requires a perfect match between the identified and the gold standard bilingual
terms to measure an improvement in F1, while the manual evaluation can reward bilingual terms that do
not perfectly match any gold standard terms but are correct translations of each other. An example is
the multi-word bilingual term ?settings of the network connection? impostazioni della connessione di
rete? that is present in the gold standard as a single multi-word term, while it is identified by The Wiki
Machine as two distinct bilingual terms, i.e. ?network connection? connessione di rete? and ?settings
? impostazioni?. From the translation point of view, both the distinct terms are correct and they are
assigned to class 1 during the manual evaluation, but they are ignored by the automatic evaluation.
The analysis of terms assigned to error class four shows that both methods are affected by similar
problems. The main source of error is the correct detection of the source term domain, which results in
a translated term that does not belong to the correct domain. For instance, in the bilingual term ?stringhe
? shoe and boot laces?, the term ?stringhe? (?strings? in the IT domain) is translated into ?laces?. Simi-
larly, the English term ?launchers? (?lanciatori? in Italian in the IT domain) is translated into ?lanciarazzi
multiplo? (?multiple rocket launchers? in English), which is clearly not an IT term. Furthermore, The
Wiki Machine seems to have more problems in identifying the right morphological variation, e.g. ?in-
dirizzi ip? ip address?, where ?indirizzi? is a plural noun and needs to be translated into ?addresses?.
This is expected because page titles in Wikipedia are not always inflected. An interesting example high-
lighted by the annotator in the TaaS translations is: ?percorso di ricerca? ? ?how do i access refresh
grid texture??, where the Italian term (?search path? in English) is translated with a completely wrong
translation. In the next Section we evaluate whether the automatic identified bilingual terms can improve
the performance of an SMT system and if it is robust to the aforementioned errors.
4.3 Embedding Terminology into SMT
Our further experiments focused on the automatic evaluation of the translation quality of the EMEA,
GNOME and KDE test sets (Table 4). The obtained bilingual terminology from TaaS and The Wiki Ma-
chine was embedded through the Fill-Up and XML markup approaches. The approximate randomization
approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances
are statistically significant with a p-value < 0.05. The parameters of the baseline method and the Fill-Up
models were optimized on the development set.
Injecting the obtained TaaS bilingual terms improves the BLEU score in several cases. XML markup
outperforms the general baseline approach in three (out of eight) datasets, whereby three of them are
statistically significant (GNOME En?It, KDE anno En?It). Embedding the same bilingual terminol-
ogy into the Fill-Up model helped to outperform the baseline approach for all test sets, whereby only the
result for EMEA En?It is not statistically significant.
27
GNOME KDE anno EMEA KDE4
En?It It?En En?It It?En En?It It?En En?It It?En
general baseline 15.39 21.62 15.58 22.64 25.88 25.75 19.22 23.54
XML Mark-up (TaaS) 15.87 22.45* 17.62* 23.88* 25.84 25.74 18.97 24.27*
Fill-Up Model (TaaS) 16.22* 22.73* 17.61* 23.45* 25.95 26.02* 19.69* 24.56*
XML Mark-up (The Wiki Machine) 15.49 20.57 17.19* 23.44* 25.59 24.97 17.74 22.16
Fill-Up Model (The Wiki Machine) 15.82 21.70 16.48* 23.28* 26.35* 26.44* 19.61* 24.14*
Table 4: Automatic BLEU Evaluation on GNOME, KDE and EMEA datasets with different term em-
bedding strategies (bold results = best performance ; * statistically significant compared to baseline).
Finally, we investigate the impact of embedding the identified terms provided by The Wiki Machine.
When we suggest translation candidates with the XML markup, it only slightly outperforms the baseline
approach for GNOME En?It, but statistically significant improves the translations for the KDE anno
test set for both language directions. Similarly to previous observations, the Fill-Up model improves
further the translations, i.e. the translations are statistically significant better than the baseline for both
language pairs of both KDE test sets as well as for EMEA.
To better understand our translation results, we manually inspected the EMEA En?It sentences, which
have the best translation performance. For each of the source sentence and the translation method,
we analyse the translated sentences and the bilingual terms that match at least one word in the source
sentence. Both translation strategies tried to encapsulate the bilingual terms, but there is clear evidence
that the Fill-Up model better embeds the target terms in the context of the translation. For instance in
the following example, the target sentence produced by the XML markup (XML trg) does not contain
the article ?la?, uses a wrong conjunction (?di? instead of ?per?) and wrongly orders the adjective with
the noun (?adulti pazienti? instead of ?pazienti adulti?). All these issues are correctly addressed by the
Fill-Up model (Fill-Up trg) which produces a smoother translation.
source sentence: adult patients receive therapy for tumours
reference sentence: pazienti adulti ricevono la terapia per i tumori
bilingual terms: therapy? terapia, patients? pazienti, adult? adulti
XML trg: adulti pazienti ricevono terapia di tumori
Fill-Up trg: pazienti adulti ricevono la terapia per i tumori
Analysing the number of suggested bilingual terms per sentence, we notice that The Wiki Machine
tends to propose more terms than TaaS (on average, The Wiki Machine 3.1, TaaS 2.5 per sentence).
Of these terms, TaaS provides on average more translations for each unique source term than The Wiki
Machine (on average, TaaS 1.51, The Wiki Machine 1).
In addition to evaluating the performance of TaaS and The Wiki Machine separately, for the EMEA
dataset we concatenate the terminological lists provided by the tools and supply it to the XML markup
and the Fill-Up approach. Embedding the combined terminology with the XML markup produces a
BLEU score of 25.59 for En?It and 24.92 for It?En. This performance is similar to the scores obtained
using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole
terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En?It and 27.02 for It?En,
which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of
the two term identification methods and suggests a novel research direction.
5 Related Work
The main focus of our research is on bilingual term identification and the embedding of this knowledge
into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that
an SMT system built by using a large general resource cannot be used to translate domain-specific terms,
we have to provide the system domain-specific lexical knowledge.
Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term
identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual
dictionary entries from Wikipedia to support the machine translation system. Based on exact string
28
matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual
dictionary. Besides the interwiki link system, Erdmann et al. (2009) enhances their bilingual dictionary
by using redirection page titles and anchor text within Wikipedia. To filter out incorrect term translation
pairs, the authors use the backward link information to prove if a redirect page title or an anchor text
represents a synonymous expression. Niehues and Waibel (2011) analyse different methods to integrate
the extracted Wikipedia titles into their system, whereby they explore methods to disambiguate between
different translations by using the text in the articles. In addition, the authors use morphological forms
of terms to enhance the extracted bilingual dictionary. The results show that the number of out-of-
vocabulary words could be reduced by 50% on computer science lectures, which improved the translation
quality by more than 1 BLEU point. Arcan et al. (2013) restrict term identification to the observed
domain by using the frequency information of Wikipedia categories. Different from these approaches
we focus on domain-specific dictionary generation, ignoring identified terms which do not belong to the
domain to be observed. Furthermore, we take advantage of the Wikipedia category graph representation
and its linking to WordNet domain, which allowed us to identify the domain we were interested in.
Furthermore, research has been done on the integration of domain-specific parallel data into SMT,
either by retraining small domain-specific and large general resources as one concatenated parallel data
(Koehn and Schroeder, 2007), adding new phrase pairs directly into the phrase table (Langlais, 2002;
Ren et al., 2009; Haddow and Koehn, 2012) or assigning adequate weights to the in- and out-of-domain
translation models (Foster and Kuhn (2007); La?ubli et al. (2013)). Bouamor et al. (2012) address the
problem of finding the best approach to integrate new obtained knowledge in an SMT system, and show
that they should be used as additional parallel sentences to train the translation model. In our approach,
we use the XML markup and the Fill-Up approach, which handles the in-domain parallel data equally
to the out-domain data. Furthermore, Okita and Way (2010) investigate the effect of integrating bilin-
gual terminology in the training step of an SMT system, and analyse in particular the performance and
sensitivity of the word aligner. As opposed to their approach, we do not have prior knowledge about the
bilingual terminology, since we extract it from the document to be translated.
6 Conclusion
In this paper we presented an approach to identify bilingual domain-specific terms starting from a mono-
lingual text and to integrate these into an SMT system. With the help of terminological and lexical
resources, we are able to discover a small amount (?200) of high-quality domain-specific terms and
enhanced the performance of an SMT system trained on large amounts (1.8M) of parallel sentences.
Monolingual and bilingual term evaluation showed no evidence that one of the tested tools (The Wiki
Machine or TaaS) produces better terms than the other in all the test sets. Depending on the manual map-
ping between the Wikipedia categories and WordNet domains and the existence of a Wikipedia version,
our approach is language and domain independent, does not need training data and is able to overcome
the sparseness and coherence problems of the Wikipedia categories. Evaluation of the two systems on
different language directions and domains shows significant improvements over the baseline in terms
of two BLEU scores (up to 13%) and confirms the applicability of such techniques in a real scenario.
It is interesting to notice that the Fill-Up technique regularly outperforms the XML markup approach,
taking advantage of all terms and not only the overlapping terms in the text to be translated. Our contri-
bution shows a different context of using Fill-Up and extends the usability of it in terms of embedding
terminological knowledge into SMT. In future work, we plan to focus on exploiting morphological term
variations taking advantage of the alternative terms (i.e., orthographical and morphological variants,
synonyms, and related terms) provided by The Wiki Machine. This will make it possible to increase the
coverage adding new terms and the accuracy of the proposed method for bilingual term identification.
Acknowledgments
This publication has emanated from research supported in part by a research grant from Science Founda-
tion Ireland (SFI) under Grant Number SFI/12/RC/2289 and by the European Union supported projects
EuroSentiment (Grant No. 296277), LIDER (Grant No. 610782) and MateCat (ICT-2011.4.2-287688).
29
References
Ahmet Aker, Monica Paramita, and Robert Gaizauskas. 2013. Extracting bilingual terminologies from comparable
corpora. In Proceedings of ACL, Sofia, Bulgaria.
Mihael Arcan, Susan Marie Thomas, Derek De Brandt, and Paul Buitelaar. 2013. Translating the FINREP taxon-
omy using a domain-specific corpus. In Machine Translation Summit XIV, pages 199?206.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the wordnet domains
hierarchy: semantics, coverage and balancing. In Proceedings of the Workshop on Multilingual Linguistic
Ressources, pages 101?108. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based
SMT Adaptation. In Proceedings of IWSLT.
Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2012. Identifying bilingual multi-word expres-
sions for statistical machine translation. In Proceedings of the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instability . In Proceedings of the Association for Computational
Lingustics.
Maike Erdmann, Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio. 2009. Improving the extraction of bilin-
gual terminology from wikipedia. ACM Trans. Multimedia Comput. Commun. Appl., 5(4):31:1?31:17, Novem-
ber.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages 1618?1621. ISCA.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo Strapparava. 2009. Kernel methods for minimally
supervised wsd. Computational Linguistics, 35(4):513?528.
Barry Haddow and Philipp Koehn. 2012. Analysing the Effect of Out-of-Domain Data on SMT Systems. In
Proceedings of the Seventh Workshop on Statistical Machine Translation, Montre?al, Canada. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In
Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual
Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages 79?86. AAMT.
J. Richard Landis and Gary G. Koch. 1977. Measurement of Observer Agreement for Categorical Data. In
Biometrics, volume 33, pages 159?174.
Philippe Langlais. 2002. Improving a general-purpose statistical translation engine by terminological lexicons. In
Proceedings of the 2nd International Workshop on Computational Terminology (COMPUTERM) ?2002, Taipei,
Taiwan, pages 1?7.
Samuel La?ubli, Mark Fishel, Martin Volk, and Manuela Weibel. 2013. Combining statistical machine translation
and translation memories with domain adaptation. In Stephan Oepen, Kristin Hagen, and Janne Bondi Johan-
nesse, editors, Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013),
May 22?24, 2013, Oslo University, Norway, Linko?ping Electronic Conference Proceedings, pages 331?341,
Oslo, May. Linko?pings universitet Electronic Press.
30
Pablo N Mendes, Max Jakob, Andre?s Garc??a-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shedding light
on the web of documents. In Proceedings of the 7th International Conference on Semantic Systems, pages 1?8.
ACM.
Rada Mihalcea. 2007. Using Wikipedia for Automatic Word Sense Disambiguation. In Proceedings of NAACL-
HLT, pages 196?203.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia to Translate Domain-specific Terms in SMT. In nterna-
tional Workshop on Spoken Language Translation, San Francisco, CA, USA.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29.
Tsuyoshi Okita and Andy Way. 2010. Statistical Machine Translation with Terminology. In Proceedings of the
First Symposium on Patent Information Processing (SPIP), Tokyo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 311?318.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, Inguna Skadin?a, Marko Tadic?, and Tatiana Gornostay. 2012.
Term extraction, tagging, and mapping tools for under-resourced languages. In Proceedings of the Terminology
and Knowledge Engineering (TKE2012) Conference.
Zhixiang Ren, Yajuan Lu?, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation
using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions:
Identification, Interpretation, Disambiguation and Applications, MWE ?09, pages 47?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Raivis Skadins?, Marcis Pinnis, Tatiana Gornostay, and Andrejs Vasiljevs. 2013. Application of online terminology
services in statistical machine translation. In Proceedings of the XIV Machine Translation Summit, Nice, France.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel Varga.
2006. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th
International Conference on Language Resources and Evaluation (LREC?2006).
Jo?rg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mariani, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Francis M. Tyers and Jacques A. Pieanaar. 2008. Extracting bilingual word pairs from wikipedia. In Collabo-
ration: interoperability between people in the creation of language resources for less-resourced languages (A
SALTMIL workshop).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08, pages 993?1000.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In
Proceedings of the TextGraphs-2 Workshop (NAACL-HLT), pages 1?8, Rochester, April. Association for Com-
putational Linguistics.
31
