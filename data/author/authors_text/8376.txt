Bilingual Terminology Acquisition from Comparable Corpora and Phrasal
Translation to Cross-Language Information Retrieval
Fatiha Sadat
Nara Institute of Science and Technology
Nara, 630-0101, Japan
{fatia-s, yosikawa, uemura}@is.aist-nara.ac.jp
Masatoshi Yoshikawa
Nagoya University
Nagoya, 464-8601, Japan
Shunsuke Uemura
Nara Institute of Science and Technology
Nara, 630-0101, Japan
Abstract
The present paper will seek to present
an approach to bilingual lexicon extrac-
tion from non-aligned comparable cor-
pora, phrasal translation as well as evalua-
tions on Cross-Language Information Re-
trieval. A two-stages translation model
is proposed for the acquisition of bilin-
gual terminology from comparable cor-
pora, disambiguation and selection of best
translation alternatives according to their
linguistics-based knowledge. Different re-
scoring techniques are proposed and eval-
uated in order to select best phrasal trans-
lation alternatives. Results demonstrate
that the proposed translation model yields
better translations and retrieval effective-
ness could be achieved across Japanese-
English language pair.
1 Introduction
Although, corpora have been an object of study of
some decades, recent years saw an increased inter-
est in their use and construction. With this increased
interest and awareness has come an expansion in the
application to knowledge acquisition, such as bilin-
gual terminology. In addition, non-aligned com-
parable corpora have been given a special inter-
est in bilingual terminology acquisition and lexical
resources enrichment (Dejean et al, 2002; Fung,
2000; Koehn and Knight, 2002; Rapp, 1999).
This paper presents a novel approach to bilin-
gual terminology acquisition and disambiguation
from scarce resources such as comparable corpora,
phrasal translation through re-scoring techniques as
well as evaluations on Cross-Language Information
Retrieval (CLIR). CLIR consists of retrieving docu-
ments written in one language using queries written
in another language. An application is completed on
a large-scale test collection, NTCIR for Japanese-
English language pair.
2 The Proposed Translation Model in
CLIR
Figure 1 shows the overall design of the proposed
translation model in CLIR consisting of three main
parts as follows:
- Bilingual terminology acquisition from
bi-directional comparable corpora, completed
through a two-stages term-by-term translation
model.
- Linguistic-based pruning, which is applied on
the extracted translation alternatives in order to filter
and detect terms and their translations that are mor-
phologically close enough, i.e., with close or similar
part-of-speech tags.
- Phrasal translation, completed on the source
query after re-scoring the translation alternatives re-
lated to each source query term. The proposed re-
scoring techniques are based on the World Wide
Web (WWW), a large-scale test collection such as
NTCIR, the comparable corpora or a possible inter-
action with the user, among others.
Finally, a linear combination to bilingual dictio-
naries, bilingual thesauri and transliteration for the
special phonetic alphabet of foreign words and loan-
words, would be possible depending on the cost and
availability of linguistic resources.
2.1 Two-stages Comparable Corpora-based
Approach
The proposed two-stages approach on bilingual ter-
minology acquisition and disambiguation from com-
parable corpora (Sadat et al, 2003) is described as
follows:
- Bilingual terminology acquisition from source
language to target language to yield a first translation
model, represented by similarity vectors SIMS?T .
- Bilingual terminology acquisition from target
language to source language to yield a second
translation model, represented by similarity vectors
SIMT?S .
- Merge the first and second models to yield a two-
stages translation model, based on bi-directional
comparable corpora and represented by similarity
vectors SIM(S?T .
We follow strategies of previous researches (De-
jean et al, 2002; Fung, 2000; Rapp, 1999) for the
first and second models and propose a merging and
disambiguation process for the two-stages transla-
tion model. Therefore, context vectors of each term
in source and target languages are constructed fol-
lowing a statistics-based metric. Next, context vec-
tors related to source words are translated using a
preliminary bilingual seed lexicon. Similarity vec-
tors SIMS?T and SIMT?S related to the first and
second models respectively, are constructed for each
pair of source term and target translation using the
cosine metric.
The merging process will keep common pairs of
source term and target translation (s,t) which appear
in SIMS?T as (s,t) but also in SIMT?S as (t,s),
to result in combined similarity vectors SIMS?T
for each pair (s,t).The product of similarity values in
vectors SIMS?T and SIM(T?S will yield similar-
ity values in SIMS?T for each pair (s,t) of source
term and target translation.
2.2 Linguistics-based Pruning
Morphological knowledge such as Part-of-Speech
(POS), context of terms extracted from thesauri
could be valuable to filter and prune the extracted
translation candidates. POS tags are assigned to
each source term (Japanese) via morphological anal-
ysis.
Bilingual Seed 
Lexicon
Linguistic-based Pruning
(Filtering based on Morphological knowledge of source 
terms and translation alternatives)
Phrasal Translation
(Re-scoring the translation alternatives)
Bilingual Terminology
Extraction
Japanese ? English
Merging & Disambiguation
Japanese ? English
Translation  Candidates
Disambiguation
Phrasal Translation Candidates
Biling
u
al
 T
erm
in
ology
 A
cq
uisitio
n
 
(T
w
o
-stag
es
 C
o
m
p
a
rable
 C
o
rp
o
ra
-b
a
sed
 M
od
el)
Ling
uistic
-
b
a
sed
 P
ru
ning
Ph
ra
sal
 T
ra
n
slatio
n
 
 
/
 S
electio
n
Japanese
Doc.
Content words (nouns, verbs, adjectives, adverbs, foreign words)
Bilingual Terminology
Extraction
English ? JapaneseJapanese
Morphological
Analyzer
English
Morphological
Analyzer
Filtered Translation Candidates
Comparable Corpora
(Japanese-English) English
Doc.
Linguistic Preprocessing
WWW
NTCIR
Test 
Collection
Comparable 
Corpora
Morphological
Analysis
Interactive 
Mode
Figure 1: The Overall Design of the Proposed Model
for Bilingual Terminology Acquisition and Phrasal
Translation in CLIR
As well, a target language morphological anal-
ysis will assign POS tags to the translation candi-
dates. We restricted the pruning technique to nouns,
verbs, adjectives and adverbs, although other POS
tags could be treated in a similar way. For Japanese-
English pair of languages, Japanese nouns and verbs
are compared to English nouns and verbs, respec-
tively. Japanese adverbs and adjectives are com-
pared to English adverbs and adjectives, because of
the close relationship between adverbs and adjec-
tives in Japanese (Sadat et al, 2003).
Finally, the generated translation alternatives are
sorted in decreasing order by similarity values and
rank counts are assigned in increasing order. A fixed
number of top-ranked translation alternatives are se-
lected and misleading candidates are discarded.
2.3 Phrasal Translation
Query translation ambiguity can be drastically mit-
igated by considering the query as a phrase and re-
stricting the single term translation to those candi-
dates that were selected by the proposed combined
statistics-based and linguistics-based approach (Sa-
dat et al, 2003). Therefore, after generating a
ranked list of translation candidates for each source
term, re-scoring techniques are proposed to estimate
the coherence of the translated query and decide the
best phrasal translation.
Assume a source query Q having n terms {s1
. . .sn}. Phrasal translation of the source query Q
is completed according to the selected top-ranked
translation alternatives for each source term si and
a re-scoring factor RFk, as follows:
Qphras =
?
k=1..thres
[Qk(s1..sn)?RFk(t1..tn; s1..sn)]
Where, Qk(s1..sn) represents the phrasal translation
candidate associated to rank k. The re-scoring factor
RFk(t1..tn; s1..sn) is estimated using one of the re-
scoring techniques, described below.
Re-scoring through the WWW
The WWW can be considered as an exemplar lin-
guistic resource for decision-making (Grefenstette,
1999). In the present study, the WWW is exploited
in order to re-score the set of translation candidates
related to the source terms.
Sequences of all possible combinations are con-
structed between elements of sets of highly ranked
translation alternatives. Each sequence is sent to a
popular Web portal (here, Google) to discover how
often the combination of translation alternatives ap-
pears. Number of retrieved WWW pages in which
the translated sequence occurred is used to represent
the re-scoring factor RF for each sequence of trans-
lation candidates. Phrasal translation candidates are
sorted in decreasing order by re-scoring factors RF .
Finally, a number (thres) of highly ranked phrasal
translation sequences is selected and collated into
the final phrasal translation.
Re-scoring through a Test Collection
Large-scale test collections could be used to re-
score the translation alternatives and complete a
phrasal translation. We follow the same steps as the
WWW-based technique, replacing the WWW by a
test collection and a retrieval system to index docu-
ments of the test collection.
NTCIR test collection (Kando, 2001) could be a a
good alternative for Japanese-English language pair,
especially if involving the comparable corpora.
Re-scoring through the Comparable Corpora
Comparable corpora could be considered for the
disambiguation of translation alternatives and thus
selection of best phrasal translations (Sadat et al,
2002). Our proposed algorithm to estimate the re-
scoring factor RF , relies on the source and tar-
get language parts of the comparable corpora us-
ing statistics-based measures. Co-occurrence ten-
dencies are estimated for each pair of source terms
using the source language text and each pair of trans-
lation alternatives using the target language text.
Re-scoring through an Interactive Mode
An interactive mode (Ogden and Davis, 2000)
could help solve the problem of phrasal translation.
The interactive environment setting should optimize
the phrasal translation, select best phrasal transla-
tion alternatives and facilitate the information access
across languages. For instance, the user can access a
list of all possible phrases ranked in a form of hier-
archy on the basis of word ranks associated to each
translation alternative. Selection of a phrase will
modify the ranked list of phrases and will provide
an access to documents related to the phrase.
3 Experiments and Evaluations in CLIR
Experiments have been carried out to measure the
improvement of our proposal on bilingual Japanese-
English tasks in CLIR, i.e. Japanese queries to re-
trieve English documents. Collections of news ar-
ticles from Mainichi Newspapers (1998-1999) for
Japanese and Mainichi Daily News (1998-1999) for
English were considered as comparable corpora. We
have also considered documents of NTCIR-2 test
collection as comparable corpora in order to cope
with special features of the test collection during
evaluations. NTCIR-2 (Kando, 2001) test collec-
tion was used to evaluate the proposed strategies in
CLIR. SMART information retrieval system (Salton,
1971), which is based on vector space model, was
used to retrieve English documents.
Thus, Content words (nouns, verbs, adjectives,
adverbs) were extracted from English and Japanese
texts. Morphological analyzers, ChaSen version
2.2.9 (Matsumoto and al., 1997) for texts in
Japanese and OAK2 (Sekine, 2001) for texts in En-
glish were used in linguistic pre-processing. EDR
(EDR, 1996) was used to translate context vectors
of source and target languages.
First experiments were conducted on the several
combinations of weighting parameters and schemes
of SMART retrieval system for documents terms and
query terms. The best performance was realized by
ATN.NTC combined weighting scheme.
The proposed two-stages model using comparable
corpora showed a better improvement in terms of av-
erage precision compared to the simple model (one-
stage comparable corpora-based translation) with
+27.1% and a difference of -32.87% in terms of av-
erage precision of the monolingual retrieval. Com-
bination to linguistics-based pruning showed a bet-
ter performance in terms of average precision with
+41.7% and +11.5% compared to the simple compa-
rable corpora-based model and the two-stages com-
parable corpora-based model, respectively.
Applying re-scoring techniques to phrasal transla-
tion yields significantly better results with 10.35%,
8.27% and 3.08% for the WWW-based, the NTCIR-
based and comparable corpora-based techniques, re-
spectively compared to the hybrid two-stages com-
parable corpora and linguistics-based pruning.
The proposed approach based on bi-directional
comparable corpora largely affected the translation
because related words could be added as translation
alternatives or expansion terms. Effects of extracting
bilingual terminology from bi-directional compara-
ble corpora, pruning using linguistics-based knowl-
edge and re-scoring using different phrasal trans-
lation techniques were positive on query transla-
tion/expansion and thus document retrieval.
4 Conclusion
We investigated the approach of extracting bilin-
gual terminology from comparable corpora in or-
der to enrich existing bilingual lexicons and en-
hance CLIR. We proposed a two-stages translation
model involving extraction and disambiguation of
the translation alternatives. Linguistics-based prun-
ing was highly effective in CLIR. Most of the se-
lected terms can be considered as translation can-
didates or expansion terms. Exploiting different
phrasal translation techniques revealed to be effec-
tive in CLIR. Although we conducted experiments
and evaluations on Japanese-English language pair,
the proposed translation model is common across
different languages.
Ongoing research is focused on the integration of
other linguistics-based techniques and combination
to transliteration for katakana, the special phonetic
alphabet to Japanese language.
References
H. Dejean, E. Gaussier and F. Sadat. 2002. An Approach
based on Multilingual Thesauri and Model Combina-
tion for Bilingual Lexicon Extraction. In Proc. COL-
ING 2002, Taipei, Taiwan.
EDR. 1996. Japan Electronic Dictionary Research Insti-
tute, Ltd. EDR electronic dictionary version 1.5 EDR.
Technical guide. Technical report TR2-007.
P. Fung. 2000. A Statistical View of Bilingual Lexi-
con Extraction: From Parallel Corpora to Non-Parallel
Corpora. In Jean Veronis, Ed. Parallel Text Process-
ing.
G. Grefenstette. 1999. The WWW as a Resource for
Example-based MT Tasks. In ASLIB?99 Translating
and the Computer 21.
N. Kando. 2001. Overview of the Second NTCIR Work-
shop. In Proc. Second NTCIR Workshop on Research
in Chinese and Japanese Text Retrieval and Text Sum-
marization.
P. Koehn and K. Knight. 2002. Learning a Translation
Lexicon from Monolingual Corpora. In Proc. ACL-02
Workshop on Unsupervised Lexical Acquisition.
Y. Matsumoto, A. Kitauchi, T. Yamashita, O. Imaichi and
T. Imamura. 1997. Japanese morphological analysis
system ChaSen manual. Technical Report NAIST-IS-
TR97007.
W. C. Ogden and M. W. Davis. 2000. Improving Cross-
Language Text Retrieval with Human Interactions. In
Proc. 33rd Hawaii International Conference on Sys-
tem Sciences.
R. Rapp. 1999. Automatic Identification of Word Trans-
lations from Unrelated English and German Corpora.
In Proc. European Association for Computational Lin-
guistics EACL?99.
F. Sadat, A. Maeda, M. Yoshikawa and S. Uemura.
2002. Exploiting and Combining Multiple Resources
for Query Expansion in Cross-Language Information
Retrieval. IPSJ Transactions of Databases, TOD 15,
43(SIG 9):39?54.
F. Sadat, M. Yoshikawa and S. Uemura. 2003. Learn-
ing Bilingual Translations from Comparable Corpora
to Cross-Language Information Retrieval: Hybrid
Statistics-based and Linguistics-based Approach. In
Proc. IRAL 2003, Sapporo, Japan.
G. Salton. 1971. The SMART Retrieval System, Experi-
ments in Automatic Documents Processing. Prentice-
Hall, Inc., Englewood Cliffs, NJ.
G. Salton and J. McGill. 1983. Introduction to Modern
Information Retrieval. New York, Mc Graw-Hill.
S. Sekine. 2001. OAK System-Manual. New York Uni-
versity.
Learning Bilingual Translations from Comparable Corpora to
Cross-Language Information Retrieval: Hybrid Statistics-based and
Linguistics-based Approach
Fatiha Sadat
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi
Nara, 630-0101, Japan
{fatia-s, uemura}@is.aist-nara.ac.jp, yosikawa@itc.nagoya-u.ac.jp
Masatoshi Yoshikawa
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya, 464-8601, Japan
Shunsuke Uemura
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi,
Nara, 630-0101, Japan
Abstract
Recent years saw an increased interest
in the use and the construction of large
corpora. With this increased interest
and awareness has come an expansion
in the application to knowledge acqui-
sition and bilingual terminology extrac-
tion. The present paper will seek to
present an approach to bilingual lexi-
con extraction from non-aligned compa-
rable corpora, combination to linguistics-
based pruning and evaluations on Cross-
Language Information Retrieval. We pro-
pose and explore a two-stages translation
model for the acquisition of bilingual ter-
minology from comparable corpora, dis-
ambiguation and selection of best transla-
tion alternatives on the basis of their mor-
phological knowledge. Evaluations using
a large-scale test collection on Japanese-
English and different weighting schemes
of SMART retrieval system confirmed the
effectiveness of the proposed combina-
tion of two-stages comparable corpora
and linguistics-based pruning on Cross-
Language Information Retrieval.
Keywords: Cross-Language Information
Retrieval, Comparable corpora, Transla-
tion, Disambiguation, Part-of-Speech.
1 Introduction
Researches on corpus-based approaches to machine
translation (MT) have been on the rise, particularly
because of their promise to provide bilingual termi-
nology and enrich lexical resources such as bilingual
dictionaries and thesauri. These approaches gener-
ally rely on large text corpora, which play an impor-
tant role in Natural Language Processing (NLP) and
Information Retrieval (IR). Moreover, non-aligned
comparable corpora have been given a special in-
terest in bilingual terminology acquisition and lex-
ical resources enrichment (Dagan and Itai, 1994;
Dejean et al, 2002; Diab and Finch, 2000; Fung,
2000; Koehn and Knight, 2002; Nakagawa, 2000;
Peters and Picchi, 1995; Rapp, 1999; Shahzad and
al., 1999; Tanaka and Iwasaki, 1996).
Unlike parallel corpora, comparable corpora are
collections of texts from pairs or multiples of lan-
guages, which can be contrasted because of their
common features, in the topic, the domain, the au-
thors or the time period. This property made com-
parable corpora more abundant, less expensive and
more accessible through the World Wide Web.
In the present paper, we are concerned by exploit-
ing scarce resources for bilingual terminology ac-
quisition, then evaluations on Cross-Language In-
formation Retrieval (CLIR). CLIR consists of re-
trieving documents written in one language using
queries written in another language. An application
is conducted on NTCIR, a large-scale data collection
for (Japanese, English) language pair.
The remainder of the present paper is organized
as follows: Section 2 presents the proposed two-
stages approach for bilingual terminology acquisi-
tion from comparable corpora. Section 3 describes
the integration of linguistic knowledge for pruning
the translation candidates. Experiments and evalua-
tions in CLIR are discussed in Sections 4. Section 5
concludes the present paper.
2 Two-stages Comparable Corpora-based
Approach
Our proposed approach to bilingual terminology ac-
quisition from comparable corpora (Sadat et al,
2003; Sadat et al, 2003) is based on the assump-
tion of similar collocation, i.e., If two words are mu-
tual translations, then their most frequent collocates
are likely to be mutual translations as well. More-
over, we apply this assumption in both directions of
the corpora, i.e., find translations of the source term
in the target language corpus but also translations
of the target terms in the source language corpus.
The proposed two-stages approach for the acquisi-
tion, disambiguation and selection of bilingual ter-
minology is described as follows:
? Bilingual terminology acquisition from source
language to target language to yield a first
translation model, represented by similarity
SIMS?T .
? Bilingual terminology acquisition from target
language to source language to yield a sec-
ond translation model, represented by similar-
ity SIMT?S .
? Merge the first and second models to yield
a two-stages translation model, based on bi-
directional comparable corpora and repre-
sented by similarity SIMS?T .
We follow strategies of previous researches (De-
jean et al, 2002; Fung, 2000; Rapp, 1999) for
the first and second translation models and propose
a merging strategy for the two-stages translation
model (Sadat et al, 2003).
First, word frequencies, context word frequencies
in surrounding positions (here three-words window)
are computed following a statistics-based metrics,
the log-likelihood ratio (Dunning, 1993). Context
vectors for each source term and each target term
are constructed. Next, context vectors of the tar-
get words are translated using a preliminary bilin-
gual dictionary. We consider all translation candi-
dates, keeping the same context frequency value as
the source term. This step requires a seed lexicon, to
expand using the proposed bootstrapping approach
of this paper. Similarity vectors are constructed for
each pair of source term and target term using the
cosine metric (Salton and McGill, 1983).
Therefore, similarity vectors SIMS?T and
SIMT?S for the first and second models are con-
structed and merged for a bi-directional acquisition
of bilingual terminology from source language to
target language. The merging process will keep
common pairs of source term and target transla-
tion (s,t) which appear in SIMS?T as pairs of (s,t)
but also in SIMT?S as pairs of (t,s), to result in
combined similarity vectors SIMS?T for each pair
(s,t).The product of similarity values of both simi-
larity vectors SIMS?T for pairs (s,t) and SIMT?S
for pairs (t,s) will result in similarity values in vec-
tors SIMS?T .
Therefore, similarity vectors of the two-stages
translation model are expressed as follows:
SIMS?T = {(s, t, simS?T (t|s)) | (s, t, simS?T (t|s))
? SIMS?T ? (t, s, simT?S(s|t)) ? SIMT?S
? simS?T (t|s) = simS?T (t|s) ? simT?S(s|t)}
3 Linguistics-based Pruning
Combining linguistic and statistical methods is be-
coming increasingly common in computational lin-
guistics, especially as more corpora become avail-
able (Klanvans and Tzoukermann, 1996; Sadat et
al., 2003). We propose to integrate linguistic con-
cepts into the corpora-based translation model. Mor-
phological knowledge such as Part-of-Speech (POS)
tags, context of terms, etc., could be valuable to filter
and prune the extracted translation candidates. The
objective of the linguistics-based pruning technique
is the detection of terms and their translations that
are morphologically close enough, i.e., close or sim-
ilar POS tags. This proposed approach will select a
fixed number of equivalents from the set of extracted
target translation alternatives that match the Part-of-
Speech of the source term.
Therefore, POS tags are assigned to each source
term (Japanese) via morphological analysis. As
well, a target language morphological analysis will
assign POS tags to the translation candidates. We
restricted the pruning technique to nouns, verbs, ad-
jectives and adverbs, although other POS tags could
be treated in a similar way. For Japanese-English1
pair of languages, Japanese nouns (   ) are com-
pared to English nouns (NN) and Japanese verbs ( 
 ) to English verbs (VB). Japanese adverbs (   )
are compared to English adverbs (RB) and adjec-
tives (JJ); while, Japanese adjectives (   ) are
compared to English adverbs (RB) and adjectives
(JJ). This is because most adverbs in Japanese are
formed from adjectives. Thus. We select pairs or
source term and target translation (s,t) such as:
POS(s) = ?NN? and POS(t) = ?   ?
POS(s) = ?VB? and POS(t) = ?   ?
POS(s) = ?RB? and [POS(t) = ?   ? or ? 	  ?]
POS(s) = ?JJ? and [POS(t) = ?   ? or ?   ?]
Japanese foreign words (tagged FW) were consid-
ered as loanwords, i.e., technical terms and proper
nouns imported from foreign languages; and there-
fore were not pruned with the proposed linguistics-
based technique but could be treated via translitera-
tion.
The generated translation alternatives are sorted
in decreasing order by similarity values. Rank
counts are assigned in increasing order, starting at
1 for the first sorted list item. A fixed number of
top-ranked translation alternatives are selected and
misleading candidates are discarded.
In order to demonstrate the procedure of our
translation model, we give an example in Japanese
and explain how the English translations are ex-
tracted, disambiguated and selected and how the
phrasal translation is constructed.
Given a simple Japanese query ? 


	

	
	Cross-Language Information Retrieval Based on
Category Matching Between Language Versions of a Web Directory
Fuminori Kimura
Graduate School of
Information Science,
Nara Institute of
Science and Technology
8916-5 Takayama,
Ikoma, Nara, Japan
Akira Maeda
Department of
Computer Science,
Ritsumeikan University
1-1-1 Noji-Higashi,
Kusatsu, Shiga, Japan
Masatoshi Yoshikawa
Information Technology
Center, Nagoya University
Furo-cho, Chigusa-ku,
Nagoya, Aichi, Japan
Shunsuke Uemura
Graduate School of
Information Science,
Nara Institute of
Science and Technology
8916-5 Takayama,
Ikoma, Nara, Japan
Abstract
Since the Web consists of documents in
various domains or genres, the method
for Cross-Language Information Retrieval
(CLIR) of Web documents should be in-
dependent of a particular domain. In this
paper, we propose a CLIR method which
employs a Web directory provided in mul-
tiple language versions (such as Yahoo!).
In the proposed method, feature terms are
first extracted from Web documents for
each category in the source and the tar-
get languages. Then, one or more corre-
sponding categories in another language
are determined beforehand by comparing
similarities between categories across lan-
guages. Using these category pairs, we in-
tend to resolve ambiguities of simple dic-
tionary translation by narrowing the cat-
egories to be retrieved in the target lan-
guage.
1 Introduction
With the popularity of the Internet, more and more
languages are becoming to be used for Web docu-
ments, and it is now much easier to access docu-
ments written in foreign languages. However, exist-
ing Web search engines only support the retrieval of
documents which are written in the same language
as the query, so the monolingual users are not able to
retrieve documents written in non-native languages
efficiently. Also, there might be cases, depending
on the user?s demand, where information written in
a language other than the user?s native language is
rich. Needs for retrieving such information must
be large. In order to satisfy such needs on a usual
monolingual retrieval system, the user him-/herself
has to manually translate the query by using a dic-
tionary, etc. This process not only imposes a burden
to the user but also might choose incorrect transla-
tions for the query, especially for languages that are
unfamiliar to the user.
To fulfill such needs, researches on Cross-
Language Information Retrieval (CLIR), a tech-
nique to retrieve documents written in a certain lan-
guage using a query written in another language,
have been active in recent years. A variety of meth-
ods, including employing corpus statistics for the
translation of terms and the disambiguation of trans-
lated terms, are studied and a certain results has
been obtained. However, corpus-based disambigua-
tion methods are heavily affected by the domain of
the training corpus, so the retrieval effectiveness for
other domains might drop significantly. Besides,
since the Web consists of documents in various do-
mains or genres, the method for CLIR of Web docu-
ments should be independent of a particular domain.
In this paper, we propose a CLIR method which
employs Web directories provided in multiple lan-
guage versions (such as Yahoo!). Our system uses
two or more language versions of a Web directory.
One version is the query language, and others are
the target languages. From these language versions,
category correspondences between languages are es-
timated in advance. First, feature terms are extracted
from Web documents for each category in the source
and the target languages. Then, one or more cor-
responding categories in another language are de-
termined beforehand by comparing similarities be-
tween categories across languages. Using these cat-
egory pairs, we intend to resolve ambiguities of
simple dictionary translation by narrowing the cat-
egories to be retrieved in the target language.
2 Related Work
Approaches to CLIR can be classified into three cat-
egories; document translation, query translation, and
the use of inter-lingual representation. The approach
based on translation of target documents has the ad-
vantage of utilizing existing machine translation sys-
tems, in which more content information can be used
for disambiguation. Thus, in general, it achieves
a better retrieval effectiveness than those based on
query translation(Sakai, 2000). However, since it is
impractical to translate a huge document collection
beforehand and it is difficult to extend this method to
new languages, this approach is not suitable for mul-
tilingual, large-scale, and frequently-updated collec-
tion of the Web. The second approach transfers both
documents and queries into an inter-lingual repre-
sentation, such as bilingual thesaurus classes or a
language-independent vector space. The latter ap-
proach requires a training phase using a bilingual
(parallel or comparable) corpus as a training data.
The major problem in the approach based on the
translation and disambiguation of queries is that the
queries submitted from ordinary users of Web search
engines tend to be very short (approximately two
words on average (Jansen et al, 2000)) and usually
consist of just an enumeration of keywords (i.e. no
context). However, this approach has an advantage
that the translated queries can simply be fed into ex-
isting monolingual search engines. In this approach,
a source language query is first translated into target
language using a bilingual dictionary, and translated
query is disambiguated. Our method falls into this
category.
It is pointed out that corpus-based disambiguation
methods are heavily affected by the difference in do-
main between query and corpus. Hull suggests that
the difference between query and corpus may cause
bad influence on retrieval effectiveness in the meth-
ods that use parallel or comparable corpora (Hull,
1997). Lin et al conducted comparative experi-
ments among three monolingual corpora that have
different domains and sizes, and has concluded that
large-scale and domain-consistent corpus is needed
for obtaining useful co-occurrence data (Lin et al,
1999).
On the Web retrieval, which is the target of our re-
search, the system has to cope with queries in many
different kinds of topics. However, it is impracti-
cal to prepare corpora that cover any possible do-
mains. In our previous paper(Kimura et al, 2003),
we proposed a CLIR method which uses documents
in a Web directory that has several language versions
(such as Yahoo!), instead of using existing corpora,
in order to improve the retrieval effectiveness. In
this paper, we propose an extension of our method
which takes account of the hierarchical structure of
Web directories. Dumais et al(Dumais and Chen,
2000) suggests that the precision of Web document
classification could be improved to a certain extent
by limiting the target categories to compare by us-
ing the hierarchical structure of a Web directory. In
this paper, we try to improve our proposed method
by incorporating the hierarchical structure of a Web
directory for merging categories.
3 Proposed System
3.1 Outline of the System
Our system uses two or more language versions of
a Web directory. One version is the query language
(language A in Figure 1), others are the target lan-
guages to be retrieved (language B in Figure 1).
From these language versions, category correspon-
dences between languages are estimated in advance.
The preprocessing consists of the following four
steps: 1) term extraction from Web documents in
each category, 2) feature term extraction, 3) transla-
tion of feature terms, and 4) estimation of category
correspondences between different languages. Fig-
ure 1 illustrates the flow of the preprocessing. This
example shows a case that category a in language A
corresponds to a category in language B. First, the
system extracts terms from Web documents which
belong to category a (1). Secondly, the system cal-
culates the weights of the extracted terms. Then
higher-weighted terms are extracted as the feature
term set fa of category a (2). Thirdly, the system
translates the feature term set fa into language B
(3). Lastly, the system compares the translated fea-
ture term set of category a with feature term sets of
all categories in language B, and estimates the cor-
responding category of category a from language B
(4).
These category pairs are used on retrieval. First,
the system estimates appropriate category for the
query in the query language. Next, the system se-
lects the corresponding category in the target lan-
guage using the pre-estimated category pairs. Fi-
nally, the system retrieves Web documents in the se-
lected corresponding category.
language A
category a
feature term
    set f
language B
category a
feature term
    set f
...
compare among languages
(1)
(2)
(3)
(4)
feature
term DB
a b
Figure 1: Preprocessing.
3.2 Preprocessing
3.2.1 Feature Term Extraction
The feature of each category is represented by
its feature term set. Feature term set is a set of
terms that seem to distinguish the category. The
feature term set of each category is extracted in the
following steps: First, the system extracts terms
from Web documents that belong to a given cate-
gory. In this time, system also collect term fre-
quency of each word in each category and normal-
ize these frequency for each category. Second, the
system calculates the weights of the extracted terms
using TF?ICF (term frequency ? inverse category fre-
quency). Lastly, top n ranked terms are extracted as
the feature term set of the category.
Weights of feature terms are calculated by
TF?ICF. TF?ICF is a variation of TF?IDF (term fre-
quency ? inverse document frequency). Instead of
using a document as the unit, TF?ICF calculates
weights by category. TF?ICF is calculated as fol-
lows:
tf ? icf(ti, c) = f(ti)Nc ? log
N
ni + 1
where ti is the term appearing in the category c,
f(ti) is the term frequency of term ti, Nc is the total
number of terms in the category c, ni is number of
the categories that contain the term ti?and N is the
number of all categories in the directory.
3.2.2 Category Matching Between Languages
For estimating category correspondences between
languages, we compare each feature term set of a
category which is extracted in section 3.2.1, and cal-
culates similarities between categories across lan-
guages.
In order to compare two categories between lan-
guages, feature term set must be translated into the
target language. First, for each feature term, the
system looks up the term in a bilingual dictionary
and extracts all translation candidates for the feature
term. Next, the system checks whether each trans-
lation candidate exists in the feature term set of the
target category. If the translation candidate exists,
the system checks the candidate?s weight in the tar-
get category. Lastly, the highest-weighted transla-
tion candidate in the feature term set of the target
category is selected as the translation of the feature
term. Thus, translation candidates are determined
for each category, and translation ambiguity is re-
solved.
If no translation candidate for a feature term ex-
ists in the feature term set of the target category, that
term is ignored in the comparison. However, there
are some cases that the source language term itself
is useful as a feature term in the target language. For
example, some English terms (mostly abbreviations)
are commonly used in documents written in other
languages (e.g. ?WWW?, ?HTM?, etc.). Therefore,
in case that no translation candidate for a feature
term exists in the feature term set of the target cat-
egory, the feature term itself is checked whether it
exists in the feature term set of the target category.
If it exists, the feature term itself is treated as the
translation of the feature term in the target category.
As an example, we consider that an English term
?system? is translated into Japanese for the cate-
gory ???????????????>?????
? >?????? (Computers and Internet >Soft-
ware >Security)? (hereafter called ????????
for short). The English term ?system? has the fol-
lowing translation candidates in a dictionary; ???
(universe/space)????? (method)????? (orga-
nization)????? (organ)??????? (system)??
etc. We check each of these translation candidates in
the feature term set of the category ???????.?
Then the highest-weighted term of these translation
candidates in the category ???????? is deter-
mined as the translation of the English term ?sys-
tem? in this category. If no translation candidate ex-
ists in the feature term set of the category ?????
??,? the English term ?system? itself is treated as
the translation.
Once all the feature terms are translated, the sys-
tem calculates the similarities between categories
across languages. The similarity between the source
category a and the target category b is calculated as
the total of multiplying the weights of each feature
term in the category a by the weight of its transla-
tion in the category b. The similarity of the category
a for the category b is calculated as follows:
sim(a, b) =
?
f?fa
w(f, a) ? w(t, b)
where f is a feature term, fa is the feature term set
of category a, t is the translation of f in the category
feature term set
 of category a
feature term set
 of category bfeature term f
translation
candidates
t1
t2
t3
.
.
.
dictionary
compare
translation term
in category b
Figure 2: Feature term translation.
b, and w(f, a) is the weight of f in a.
The system calculates the similarities of category
a for each category in the target language using the
above-mentioned method. Then, a category with the
highest similarity in the target language is selected
as the correspondent of category a.
As an example, we consider an example of
calculating the similarity of an English category
?Computers and Internet >Security and Encryption?
(hereafter called ?Encryption? for short) for the cat-
egory ???????? which is mentioned above.
Suppose that the feature term set of the category
?Encryption? has the following feature terms; ?pri-
vacy?, ?system?, etc., and the weights of these terms
are 0.007110, 0.006327, ? ? ?. Also suppose that the
Japanese translations of these terms are ?????
?? (privacy)?, ????? (system)?, etc., and the
weights of these terms are 0.023999, 0.047117, ? ? ?.
In this case, the similarity of the category ?Encryp-
tion? (s1) for the category ???????? (s2) is
calculated as follows:
sim(s1, s2) = 0.007110? 0.023999
+0.006327? 0.047117
+ ? ? ?
3.2.3 Retrieval
Figure 3 illustrates the processing flow of a re-
trieval. When the user submits a query, the follow-
ing steps are processed.
In our system, a query consists of some keywords,
not of a sentence. We define the query vector ~q as
follows:
~q = (q1, q2, . . . , qn)
where qk is the weight of the k-th keyword in the
query. We define the values of all qk are 1.
First, the system calculates the relevance between
the query and each category in the source language,
and determines the relevant category of the query in
the source language (1). The relevance between the
query and each category is calculated by multiply-
ing the inner product between query terms and the
feature term set of the target category by the angle
of these two vectors. The relevance between query
q and category c is calculated as follows:
rel(q, c) = ~q ? ~c ? ~q ? ~c|~q| ? |~c|
where ~c is a vector of category c defined as follows:
~c = (w1, w2, . . . , wn)
where wk is the weight of the k-th keyword in the
feature term set of c.
If there is more than one category whose rele-
vance for the query exceeds a certain threshold, all
of them are selected as the relevant categories of the
query. It is because there might be some cases that,
for example, documents in the same domain belong
to different categories, or a query concept belongs to
multiple domains.
Second, the corresponding category in the tar-
get language is selected by using category corre-
spondences between languages mentioned in section
3.2.2 (2). Third, the query is translated into the tar-
get language by using a dictionary and the feature
term set of the corresponding category (3). Finally,
the system retrieves documents in the corresponding
category (4).
4 Category Merging
4.1 Previous Experiments
In our previous paper(Kimura et al, 2003), we con-
ducted experiments of category matching using the
subsets of English and Japanese versions of Yahoo!.
language A language B
(4)
feature
term DB
query
(language A)
query
(language B)
(1)
(2)
(3)
(4)
Figure 3: Processing in retrieval.
The English subset is 559 categories under the cat-
egory ?Computers and Internet? and the Japanese
subset is 654 categories under the corresponding
category ??????????????? (Com-
puters and Internet).? Total size of English web
pages in each category after eliminating HTML tags
are 45,905 bytes on average, ranging from 476 to
1,084,676 bytes. Total size of Japanese web pages
are 22,770 bytes on average, ranging from 467 to
409,576 bytes.
In our previous experiments, we could not match
categories across languages with adequate accuracy.
It may have been caused by the following reasons;
one possible reason is that the size of Web docu-
ments was not enough for statistics in some cate-
gories, and another is that some categories are ex-
cessively divided as a distinct domain.
For the former observation, we eliminated the cat-
egories whose total bytes of Web documents are less
than 30KB, but the results were not improved.
4.2 Method of Category Merging
Considering the result of the above experiments, we
need to solve the problem of excessive division of
categories in order to accurately match categories
between languages.
The problem might be caused by the following
reasons; one possible reason is that there are some
categories which are too close in topic, and it might
cause poor accuracy. Another possible reason is that
some categories have insufficient amount of text in
order to obtain statistically significant values for fea-
ture term extraction. Considering the above observa-
tions, we might expect that the accuracy will be im-
proved by merging child categories at some level in
the category hierarchy in order to merge some cate-
gories similar in topic and to increase the amount of
text in a category.
Accordingly, we solve the problem by merging
child categories into the parent category at some
level using the directory hierarchy. As child cate-
gories are specialized ones of the parent category,
we can assume that these categories have similar
topic. Besides, even if two categories have no direct
link from each other, we can assume that categories
that have same parent category might also have sim-
ilar topic.
However, we still need further investigation on at
which level categories should be merged.
Figure 4: Category merging.
5 Experiments
We are conducting experiments of the proposed
method to detect relevance category of a query. In
this experiment, we used the same subsets men-
tioned in section 4.1. We merged the categories three
levels below the category ?Computers and Internet?
into the parent. The number of categories after cate-
gory merging is 342 in English and 265 in Japanese.
At first, we have done the experiment using the
following formula that uses only inner product,
before using the calculation mentioned in section
3.2.3.
relinner(q, c) = ~q ? ~c
In this experiment, the query has three
terms: ?encryption?(=q1), ?security?(=q2), and
?system?(=q3).
Table 1 is the list of top 10 relevant categories in
first experiment. Almost all the categories in the Ta-
ble 1 are relevant to the query. Thus, the relevance
calculation method by only inner product is regarded
as an effective method. However, this method has
the following problem. The category that has few
query terms might be given high relevance when the
category has the only one query term whose weight
in the category is extremely high.
In order to reduce this effect, we propose the im-
proved method mentioned in section 3.2.3. The
method is revised to take account of the angle be-
tween ~q and ~c. Ultimately, the most relevant cate-
gory has the vector whose length is long and whose
factors are flat. The length is considered by inner
product, on the other hand, flatness is considered by
the angle between ~q and ~c.
Table 2 is the list of top 10 relevant categories in
the second experiment using revised method. Al-
though noticeable improvement does not appear, the
relevance of the categories which matches few query
terms are ranked lower than the first experiment.
6 Conclusions
In this paper, we proposed a method using a Web
directory for CLIR. The proposed method is inde-
pendent of a particular domain because it uses docu-
ments in a Web directory as the corpus. Our method
is particularly effective for the case that the docu-
ment collection covers wide range of domains such
as the Web. Besides, our method does not require
expensive linguistic resources except for a dictio-
nary. Therefore, our method can easily be extended
to other languages as long as the language versions
of a Web directory exist and the dictionary can be
obtained.
Future work includes improving the category
matching method and the evaluation of retrieval ef-
fectiveness.
References
Susan Dumais and Hao Chen. 2000. Hierarchical clas-
sification of Web content. Proceedings of the 23rd
ACM International Conference on Research and De-
velopment in Information Retrieval(SIGIR2000).
David A. Hull. 1997. Using structured queries for dis-
ambiguation in cross-language information retrieval.
Electronic Working Notes of the AAAI Symposium on
Cross-Language Text and Speech Retrieval.
Bernard J. Jansen, Amanda Spink, and Tefko Saracevic.
2000. Real life, real user queries on the Web. Infor-
mation Processing & Management, 36(2).
Fuminori Kimura, Akira Maeda, Masatoshi Yoshikawa,
and Shunsuke Uemura. 2003. Cross-Language Infor-
mation Retrieval using Web Directory Structure. The
14th Data Engineering Workshop, (in Japanese).
Chuan-Jie Lin, Wen-Cheng Lin, Guo-Wei Bian, and
Hsin-Hsi Chen. 1999. Description of the NTU
Japanese-English cross-lingual information retrieval
system used for NTCIR workshop. First NTCIR Work-
shop on Research in Japanese Text Retrieval and Term
Recognition.
Tetsuya Sakai. 2000. MT-based Japanese-English cross-
language IR experiments using the TREC test collec-
tions. Proceedings of The Fifth International Work-
shop on Information Retrieval with Asian Languages
(IRAL2000).
Table 1: The list of top 10 relevant category calculated by inner product.
category name relevance weight(q1/q2/q3)
Computers and Internet/Security and Encryp-
tion/Challenges/
0.166845 0.112607/0.054238/0.000000
Computers and Internet/Security and Encryp-
tion/Conferences/
0.126984 0.000000/0.126984/0.000000
Computers and Internet/Security and Encryp-
tion/Web Directories/
0.106283 0.012577/0.093706/0.000000
Computers and Internet/Security and Encryp-
tion/Organizations/
0.089169 0.006647/0.076520/0.006002
Business and Economy/Business to Busi-
ness/Computers/Security and Encryption/
0.087314 0.006391/0.074656/0.006267
Computers and Internet/Security and Encryp-
tion/Encryption Policy/
0.086271 0.075185/0.011086/0.000000
Computers and Internet/Security and Encryp-
tion/Mailing Lists/
0.075399 0.017247/0.058152/0.000000
Computers and Internet/Software/Operating
Systems/File Systems/
0.075088 0.027648/0.024968/0.022472
Computers and Internet/Internet/World Wide
Web/Security and Encryption/
0.073100 0.005671/0.05612/0.011309
Computers and Internet/Software/Operating
Systems/Inferno/
0.070922 0.000000/0.000000/0.070922
Table 2: The list of the top 10 relevance category calculated by proposed method in section 3.2.3.
category name relevance weight(q1/q2/q3)
Computers and Internet/Security and Encryp-
tion/Challenges/
0.128587 0.112607/0.054238/0.000000
Computers and Internet/Software/Operating
Systems/File Systems/
0.074822 0.027648/0.024968/0.022472
Computers and Internet/Security and Encryp-
tion/Conferences/
0.073314 0.00000/0.126984/0.000000
Computers and Internet/Security and Encryp-
tion/Web Directories/
0.068980 0.012577/0.093706/0.000000
Computers and Internet/Security and Encryp-
tion/Organizations/
0.059585 0.006647/0.07652/0.006002
Business and Economy/Business to Busi-
ness/Computers/Security and Encryption/
0.058539 0.006391/0.074656/0.006267
Computers and Internet/Security and Encryp-
tion/Encryption Policy/
0.056542 0.075185/0.011086/0.000000
Computers and Internet/Security and Encryp-
tion/Mailing Lists/
0.054113 0.017247/0.058152/0.000000
Computers and Internet/Internet/World Wide
Web/Security and Encryption/
0.053628 0.005671/0.05612/0.011309
Computers and Internet/Programming and De-
velopment/Languages/Java/Security/
0.046474 0.000000/0.054276/0.01271
