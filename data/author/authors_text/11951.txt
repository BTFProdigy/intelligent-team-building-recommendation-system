Proceedings of the Workshop on BioNLP: Shared Task, pages 77?85,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Annotation with CRFs and Precision Grammars
Andrew MacKinlay, David Martinez and Timothy Baldwin
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
{amack,davidm,tim}@csse.unimelb.edu.au
Abstract
This work describes a system for the tasks
of identifying events in biomedical text and
marking those that are speculative or negated.
The architecture of the system relies on
both Machine Learning (ML) approaches and
hand-coded precision grammars. We submit-
ted the output of our approach to the event ex-
traction shared task at BioNLP 2009, where
our methods suffered from low recall, al-
though we were one of the few teams to pro-
vide answers for task 3.
1 Introduction
We present in this paper our techniques for the tasks
1 and 3 of the event extraction shared task at BioNLP
2009. We make use of both Machine Learning (ML)
approaches and hand-coded precision grammars in
an architecture that combines multiple dedicated
modules. In the third task on negation/speculation,
we extract extract rich linguistic features resulting
from our HPSG high-precision grammar to train an
ML classifier.
2 Methodology
2.1 Task 1: Shallow Features and CRFs
Our system consists of two main modules, the first
of which is devoted to the detection of event trigger
words, and the second to event?theme analysis.
2.1.1 Trigger-word detection
We developed two separate systems to perform
trigger word detection, and also a hybrid system
which combines their outputs. The first system is
a simple dictionary-based look-up tagger; the sec-
ond system learns a structured model from the train-
ing data using conditional random fields (CRFs).
For pre-processing, we relied on the domain-specific
token and sentence splitter from the JULIE Lab
(Tomanek et al, 2007) and the GENIA tagger for
lemmatisation, POS tagging, chunking, and protein
detection (Tsuruoka et al, 2005).
The look-up tagger operates by counting the oc-
currences in the training data of different event tags
for a given term. Over the development and test data,
each occurrence of a given term is assigned the event
class with the highest prior in the training data. We
experimented with a frequency cut-off that allows us
to explore the precision/recall trade-off.
Our second system relies on CRFs, as imple-
mented in the CRF++ toolkit (Lafferty et al, 2001).
CRFs provide a discriminative framework for build-
ing structured models to segment and label sequence
data. CRFs have the well-known advantage that they
both model sequential effects and support the use of
large numbers of features. In our experiments we
used the following feature types: word-forms, lem-
mas, POS, chunk tags, protein annotation, and gram-
matical dependencies. For dependency annotation,
we used the Bikel parser and GDep as provided by
the organisers. This information was provided as a
feature that expresses the grammatical function of
the token. We explored window sizes of?3 and?4.
Finally, we tested combining the outputs of the
look-up tagger and CRF, by selecting all trigger
words from both outputs.
77
2.1.2 Event-theme construction
We constructed the output for task 1 by differenti-
ating among three types of events, according to their
expected themes: basic events, binding events, and
regulation events. We applied a simple strategy, as-
signing the closest events or proteins within a given
sentence as themes.
For the basic events, we simply assigned the clos-
est protein, an approach that we found to perform
well over the training and development data. For
binding events, we estimated the maximum dis-
tance away from the event word(s) for themes, and
the maximum number of themes. For regulation
events, we had to choose between proteins or events
as themes, and the CAUSE field was also required.
Again, we relied on a maximum distance threshold,
and gave priority to events over proteins as themes.
We removed regulation events as theme candidates,
since our basic approach could not indicate the di-
rection of the regulation. We also tested predicting
the CAUSE by relying on the protein closest to the
regulation event.
2.2 Task 3: Deep Parsing and Maximum
Entropy classification
For task 3 we ran a syntactic parser over the abstracts
and used the outputs to construct feature vectors for
a machine learning algorithm. We built two classi-
fiers (possibly with overlapping sets of feature vec-
tors) for each training run: one to identify specula-
tion and one for negation. We deliberately built a
separate binary classifier for each task instead of a
single four-class classifier, since the problem natu-
rally decomposes this way. Speculation and nega-
tion are independent of one another (informally, not
statistically) and it enables us to focus on feature en-
gineering for each subtask.
2.2.1 Deep Parsing with the ERG
It seemed likely that syntactico-semantic analysis
would be useful for task 3. To identify negation or
speculation with relatively high precision, it is prob-
able that knowledge of the relationships of possibly
distant elements (such as the negation particle not)
to a particular target word would provide valuable
information for classification.
Further to this, it was our intention to evaluate
the utility of deep parsing in such an approach,
rather than a shallower annotation such as the out-
put of a dependency parser. With this in mind,
we selected the English Resource Grammar1 (ERG:
Copestake and Flickinger (2000)), an open-source,
broad-coverage high-precision grammar of English
in the HPSG framework.
While the ERG is relatively robust across dif-
ferent domains, it is a general-purpose resource,
and there are some aspects of the language used in
the biomedical abstracts that cause difficulties; un-
known word handling is especially important given
the nature of terms in the domain. Fortunately we
can make some optimisations to mitigate this. The
GENIA tagger mentioned in Section 2.1.1 provides
both POS and named entity annotations, which we
used to constrain the input to the ERG in two ways:
? Biological named entities identified by the GE-
NIA tagger are flagged as such, and the parser
does not attempt to decompose them.
? POS tags are appended to each input token to
constrain the token to an appropriate category
if it is absent from the ERG lexicon.
With these modifications to the parser, as well as
preprocessing to handle differences in the tokenisa-
tion expected by the ERG to the output of the tagger,
we were able to obtain a spanning parse for 72% of
the training sentences. This still leaves 28% of the
sentences inaccessible ? the need for a fallback strat-
egy is discussed further in Section 4.2.
2.2.2 Feature Extraction from RMRSs
Rather than outputting syntactic parse trees, the
ERG can also produce output in particular semantic
formalisms: Minimal Recursion Semantics (MRS:
Copestake et al (2005)) and the closely related Ro-
bust Minimal Recursion Semantics (RMRS: Copes-
take (2004)). For our feature generation here we
make use of the latter.
Figure 1 shows an example RMRS obtained from
one of the training documents. While there is in-
sufficient space to give a complete treatment here,
we highlight several aspects for expository purposes.
1Specifically the July 2008 version, downloadable from
http://lingo.stanford.edu/ftp/test/
78
l1,
{ l3: thus a 1?62:67?(e5, ARG1: h4),
l16: generic unk nom rel?68:78?(x11, CARG: ?nf- kappa b?),
l6: udef q rel?68:89?(x9, RSTR: h8, BODY: h7),
l10: compound rel?68:89?(e12, ARG1: x9, ARG2: x11),
l13: udef q rel?68:89?(x11, RSTR: h15, BODY: h14),
l101: activation n 1?79:89?(x9),
l17: neg rel?94:97?(e19, ARG1: h18),
l20: require v 1?98:106?(e2, ARG1: u21, ARG2: x9),
l102: parg d rel?98:106?(e22, ARG1: e2, ARG2: x9),
l103: for p?107:110?(e24, ARG1: e2, ARG2: x23),
l34: generic unk nom rel?111:129?(x29,
CARG: ?neuroblastoma cell?),
l25: udef q rel?111:146?(x23, RSTR: h27, BODY: h26),
l28: compound rel?111:146?(e30, ARG1: x23, ARG2: x29),
l31: udef q rel?111:146?(x29, RSTR: h33, BODY: h32),
l104: differentiation n of?130:146?(x23, ARG1: u35) },
{ h4 qeq l17, h8 qeq l10, h15 qeq l16, h18 qeq l20, h27 qeq l28,
h33 qeq l34 },
{ l10 in-g l101, l20 in-g l102, l20 in-g l103, l28 in-g l104 }
Figure 1: RMRS representation of the sentence Thus NF-
kappa B activation requires neuroblastoma cell differ-
entiation showing, in order, elementary predicates, qeq-
constraints, and in-g constraints
The primary component of an RMRS is bag of ele-
mentary predicates, or EPs. Each EP shown has: (a)
a label, such as ?l104?; (b) a predicate name, such as
? differentiation n 1? (where ?n? indicates the part-
of-speech); (c) character indices to the source sen-
tence; and (d) a set of arguments. The first argu-
ment is always ARG0 and is afforded special sta-
tus, generally referring to the variable introduced by
the predicate. Subsequent arguments are labelled ac-
cording to the relation of the argument to the pred-
icate. Arguments can be variables such as ?e30? or
?x23? (where the first letter indicates the nature of
the variable ? ?e? referring to events and ?x? to enti-
ties), or handles such as ?h33?.
These handles are generally used in the qeq con-
straints, which relate a handle to a label, indicating
a particular kind of outscoping relationship between
the handle and the label ? either that the handle and
label are equal or that the handle is equal to the label
except that one or more quantifiers occur between
the two (the name is derived from ?equality mod-
ule quantifiers?). Finally there are in-g constraints
which indicate that labels can be treated as equal.
For our purposes this simply affects which qeq con-
straints they participate in ? for example from the
in-g constraint ?l28 in-g l104? and the qeq constraint
?h27 qeq l28?, we can also infer that ?h27 qeq l104?.
In constructing features, we make use of:
? The outscopes relationship (specifically qeq-
outscopes) ? if EP A has a handle argument
which qeq-outscopes the label of EP B, A is
said to immediately outscope B ; outscopes is
the transitive closure of this.
? The shared-argument relationship, where EPs
C and D refer to the same variable in one
or more of their argument positions. We also
in some cases make further restrictions on the
types of arguments (ARG0 , RSTR , etc) that
may be shared on either end of the relationship.
2.2.3 Feature Sets and Classification
Feature vectors for a given event are constructed
on the basis of the trigger word for the particular
event, which we assume has already been identified;
a natural consequence is that all events with the same
trigger words have identical feature vectors. We use
the term trigger EPs to describe the EP(s) which cor-
respond to that trigger word ? i.e. those whose char-
acter span encompasses the trigger word. We have
a potentially large set of related EPs (with the kinds
of relationships described above), which we filter to
create the various feature sets, as outlined below.
We have several feature sets targeted at identify-
ing negation:
? NEGOUTSCOPE2: If any EPs in the RMRS
have predicate names in { no q, but+not c,
nor c, only a, never a, not+as+yet a,
not+as+yet a, unable a, neg rel}, and that
EP outscopes a trigger EP, set a general feature
as well as a specific one for the particle.
? NEGCONJINDEX: If any EPs in the RMRS
have predicate names in { not c, but+not c,
nor c}, the R-INDEX (RHS of a conjunction)
of that EP is the ARG0 a trigger EP, set a gen-
eral feature as well as a specific one for the par-
ticle ? capturing the notion that these conjunc-
tions are semantically negative for the particle
on the right. This also had a corresponding fea-
ture for the L-INDEX of nor c, corresponding
to the LHS of the neither...nor construction.
79
? ARG0NEGOUTSCOPEESA: For any EPs
which have an argument that matches the
ARG0 of a trigger EP, if they are outscoped
by an EP whose predicate name is in
the list { only a, never a, not+as+yet a,
not+as+yet a, unable a, neg rel}, set a gen-
eral feature to true, as well as features for the
name of the outscoping and outscoped EPs.
This is designed to catch trigger EP which are
nouns, where the verb of which they are subject
or object (or indeed an adjective/preposition to
which they are linked) is semantically negated.
And several targeted at identifying speculation:
? SPECVOBJ2: if a verb is a member of
the set { investigate, study, examine, test,
evaluate, observe} and its ARG2 (which cor-
responds to the verb object) is the ARG0 of a
trigger EP. This has a general feature for if any
of the verbs match, and a feature which is spe-
cific to each verb in the target list.
? SPECVOBJ2+WN: as above, but augment the
list of seed verbs with a list of WordNet sisters
(i.e. any lemmas from any synsets for the verb),
and add a feature which is set for the seed verbs
which gave rise to other sister verbs.
? MODALOUTSCOPE: modal verbs (can, should,
etc) may be strong indicators of specula-
tion; this sets a value when the trigger EP is
outscoped by any predicate corresponding to a
modal, both as a general feature and a specific
feature for the particular modal.
? ANALYSISSA: the ARG0 of the trigger EP is
also an argument of an EP with the predicate
name analysis n. Such constructions involv-
ing the word analysis are relatively frequent in
speculative events in the data.
And some general features, aiming to see if the
learning algorithm could pick up other patterns we
had missed:
? TRIGPREDPROPS: Set a feature value for the
predicate name of each trigger EP, as well as
the POS of each trigger EP.
? TRIGOUTSCOPES: Set a feature value for the
predicate name and POS of each EP that is
outscoped by the trigger EP.
? MODADJ: Set a feature value for any EPs
which have an ARG1 which matches the ARG0
of the trigger EP if their POS is marked as ad-
jective or adverb.
? +CONJ: This is actually a variant on the feature
extraction method, which attempts to abstract
away the effect of conjunctions. If the trigger
EP is a member of a conjunction (i.e. shares
an ARG0 with the L-INDEX or R-INDEX of a
conjunction), also treat the EPs which are con-
junction parents (and their conjunctive parents
if they exist) as trigger EPs in the feature con-
struction.
2.2.4 Implementation
To produce training data to feed into a classifier,
we parsed as many sentences as possible using the
ERG, and used the output RMRSs to create train-
ing data using various combinations of the feature
sets described above. The construction of features,
however, presupposes annotations for the events and
trigger words. For producing training data, we used
the provided trigger annotations. For the test phase,
we simply use the outputs of the classifier we built
in phase 1, selecting the combination with the best
performance over the development set. This pipeline
architecture places limits on annotation performance
? in particular, the recall in task 1 is an upper bound
on task 3 recall. We used a maximum entropy clas-
sification algorithm for the ML component here ? it
has a low level of parameterization and is a solid per-
former in NLP tasks. The implementation we used
was Zhang Le?s Maxent Toolkit.2
3 Development experiments
3.1 Task 1
We devised a set of experiments over the trial, train-
ing, and development data in order to estimate the
parameters for our final submission. Using the trial
data, we performed manual error analysis on the
rules used to construct events. With the training
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
80
data, we performed our own evaluation based on
cross-validation to detect trigger words and con-
struct events. For the experiments over the devel-
opment data, we relied on the evaluation interface
provided by the organisation. We focused on testing
the following modules: look-up tagger, CRF, com-
bined system, and event construction.
First, we tuned the parameters of our look-up tag-
ger over the training data. We used a threshold on
the minimum number of term occurrences required
to use the class information for that term from the
training data. We evaluated thresholding on raw fre-
quencies, and also on the percentage of occurrences
of the term that were linked to the majority event. In
cross-validation over the training data, we found that
the raw-frequency threshold worked best, achiev-
ing a maximum F-score of 38.86%, as compared to
30.81% for the percentage approach (the results are
shown in the bottom part of Table 1). We also es-
timated the frequency threshold as ? 25, and ob-
served that most of the terms identified consisted of
a single word, due to data sparseness in the training
set.
Our next experiments are devoted to the CRF sys-
tem, focusing on feature engineering. The results
over the training data for: (a) the full feature set, and
(b) removing one feature type at a time, are shown
in Table 1, for windows of size ?3 and ?4. We can
see that the best F-score is achieved by the?3 word-
window system when removing the syntactic depen-
dencies from Bikel?s parser. These results improved
over the look-up system.
As a final experiment on feature combinations
and window size, we used the development evalu-
ation interface. We submitted the best combinations
shown in the above experiment, and also syntactic
dependencies extracted with GDep. We observed
the same behaviour as in training data, with the ?3
word window obtaining the best F-score, and syn-
tactic dependencies harming performance. These re-
sults are shown in the upper part of Table 2. Our
final CRF system used this configuration (?3 word
window and all feature types except syntactic depen-
dencies).
Our next step was to test the integration of the
look-up tagger and CRF into a single system. We
observed that by combining the outputs directly we
W. size Feats. Rec. Prec. FSc.
?3 All 30.28 64.44 41.20
?3 ?synt. dep. 30.20 65.01 41.24
?3 ?protein NER 28.04 65.73 39.31
?3 ?chunking 30.13 65.16 41.20
?3 ?POS 29.68 65.25 40.80
?3 ?lemma 27.96 62.60 38.66
?3 ?word form 29.98 63.81 40.79
?4 All 28.86 66.15 40.19
?4 ?synt. dep. 29.75 67.06 41.22
?4 ?protein NER 28.11 66.73 39.56
?4 ?chunking 28.56 66.61 39.98
?4 ?POS 28.19 66.67 39.62
?4 ?lemma 26.55 65.20 37.73
?4 ?word form 28.19 65.28 39.38
Look-up (freq.) 52.14 30.97 38.86
Look-up (perc.) 38.20 25.82 30.81
Table 1: Trigger-word detection performance over train-
ing data. Results for the look-up tagger and CRFs with
the full feature set and when removing one feature type
at a time, for 3 and 4 word windows. The best results per
column are shown in bold.
W. size Feats. Rec. Prec. FSc.
?3 All - synt. 17.55 56.17 26.75
?4 All - synt. 17.38 56.75 26.62
?3 All (GDep) 15.48 58.69 24.50
Combined (All) 26.94 27.83 27.38
Combined (Best) 21.24 39.92 27.73
Table 2: Performance of selected feature and window-
size combinations over development data. Best results
per column are given in bold.
could improve over the recall of CRF, and achieve
higher F-score. This approach is referred to as
?Combined (All)? in Table 2. We also tested the
results when choosing either the look-up tagger or
CRF depending on their performance over each
event in the training data. The results of this sys-
tem (?Combined (Best)?) show a slight improve-
ment over the basic combination.
Finally, we analysed the results of the event con-
struction step. We used the gold-standard trigger an-
notation over the trial data and analysed the errors of
our rules. We found out that there were three main
types of error: (1) incorrect assignation of regulation
themes; (2) trigger words having multiple themes;
and (3) themes crossing sentence boundaries. We
plan to address these problems in future work. We
also observed that predicting CAUSE for the regula-
tory events caused the F-score to drop, resulting in
us removing this functionality from the system.
81
N1: NEGOUTSCOPE2+CONJ, NEGCONJINDEX
N2: N1, TRIGPREDPROPS
N3: N1, ARG0NEGOUTSCOPEESA
N4: N3, TRIGPREDPROPS, NEGVOUTSCOPE
N5: N3, NEGVOUTSCOPE
S1: SPECVOBJ2+WN+CONJ, ANALYSISSA
S2: S1, TRIGPREDPROPS
S3: S1, MODADJ, MODALOUTSCOPE
S4: S3, TRIGOUTSCOPES
S5: SPECVOBJ2+WN+CONJ, MODADJ,
MODALOUTSCOPE,TRIGOUTSCOPES
B+y?x: Context window of lemmatized tokens: x preceding and yfollowing.
Table 3: Task 3 feature sets
3.2 Task 3
We evaluated the classification performance of vari-
ous feature sets (including some not described here)
using 10-fold cross-validation over the training data
in the initial stages. We ran various combinations of
the most promising features over the development
data and evaluated their relative performance in an
attempt to avoid overfitting.
To evaluate the performance boost we got in task
3 relative to more naive methods, we also experi-
mented with feature sets based on a bag-of-words
approach with a sliding context window of lemma-
tised tokens on either side. We evaluated all com-
binations of preceding and following context win-
dow sizes from 0 to 3. There are features for tokens
that precede the trigger, follow the trigger, or lie any-
where within the context window, as well as for the
trigger itself. A ?token? here may also be a named
biological entity (protein etc) produced by GENIA
tagger in our preprocessing phase, which would not
be lemmatised. For comparability we only evaluate
these features for sentences which we were able to
parse. For the best performing baseline and RMRS-
based feature sets, we also tested them in combina-
tion to see whether the features produced were com-
plementary.
In Table 4 we present the results over the develop-
ment data, using the provided gold-standard annota-
tions of trigger words, as well as some selected re-
sults for our other task 1 outputs. The gold-standard
figures are unrealistically high compared to what
we would expect to achieve against the test data,
but they are indicative at least of what we could
achieve with a perfect event classifier. Similar to
Task 1 Mod Feats. Rec. Prec. FSc.
Gold Spec B+2?2 23.2 40.0 29.3
Gold Spec B+3?3 22.1 47.7 30.2
Gold Spec S2 15.8 83.3 26.5
Gold Spec S3 18.9 78.3 30.5
Gold Spec S3,B+2?2 21.1 58.8 31.0
Gold Spec S3,B+3?3 23.2 57.9 33.1
Comb(best) Spec S3 4.2 21.0 7.0
Gold Spec S4 17.9 94.4 30.1
Gold Spec S5 17.9 100.0 30.4
Gold Neg B+0?2 14.0 33.3 19.7
Gold Neg B+1?3 15.0 30.2 20.0
Gold Neg N2 19.6 61.8 29.8
Comb(best) Neg N2 0.9 7.7 1.7
Gold Neg N3 15.9 68.0 25.8
Gold Neg N4 19.6 67.7 30.4
Gold Neg N4,B+1?3 22.4 52.2 31.4
Gold Neg N4,B+0?2 24.3 68.4 35.9
Gold Neg N5 16.8 69.2 30.1
Table 4: Results (exact match) over development data
for task 3 using gold-standard event/trigger annotations
and selected other annotations for task 1. Feature sets
described in Table 3
task 1, our system shows reasonable precision but
suffers badly in recall. The substantially poorer per-
formance when using our own annotations for the in-
put events is discussed in more detail in Section 4.2
One area where we could improve is to go after
the 30% of sentences for which we do not have a
spanning parse and resultant RMRS. To reuse ex-
isting infrastructure, we could produce RMRS out-
put from an alternative processing component with
broader coverage but less precision. Several meth-
ods exist to do this ? e.g. producing RMRS out-
put from RASP (Briscoe et al, 2006) is described in
Frank (2004). However there is clearly room for im-
provement in the remaining 70% of sentences which
we can parse ? our results in Table 4 are still well
below the limit of roughly 70% recall.3
Additional lexical resources beyond WordNet,
particularly domain-specific ones, are likely to be
useful in boosting performance since they will help
maximally utilise the training data. Additionally,
we have not yet made use of other event annota-
tions apart from the trigger words ? features based
on characteristics such as the event class or proper-
ties of the event arguments could also be useful.
3We have not performed any analysis to verify whether the
number of events per sentence differs between parseable and
unparseable sentences.
82
System Rec. Prec. FSc.
Combined (Best) 17.44 39.99 24.29
Combined (All) 24.36 30.87 27.23
CRF 12.23 62.24 20.44
CRF (+ synt feats) 12.01 61.91 20.11
Look-Up 22.88 29.67 25.84
Look-Up (freq >= 20) 23.26 26.74 24.88
Look-Up (freq >= 30) 21.37 30.50 25.13
Table 5: Task 1 results with approximate span matching,
recursive evaluation (our final submission is in bold)
4 Results
4.1 Task 1
Our experiments on the training and development set
showed that our CRF++ was biased towards preci-
sion at the cost of recall, and for the look-up system
the best F-score was obtained when aiming for high
recall at the cost of lower precision. The best results
were obtained when combining both approaches,
and this was the composition of the system we sub-
mitted.
For our final submission, the CRF++ approach
had a ?3 word window, and all the features ex-
cept for syntactic dependencies, which were found
to harm performance. Our final look-up system re-
lied on raw frequencies to choose candidate terms,
and those above 24 occurrences in training data were
included in the dictionary. For the combination, we
observed that for most events the look-up system
performed better (although the overall F-score was
lower), and we decided to use the CRF++ output
only for the events that showed better performance
than the look-up system (TRANSCRIPTION, GENE
EXPRESSION, and POSITIVE REGULATION).
The results over the test data for our final submis-
sion and the main variants we explored are shown in
Table 5. We can see that the CRF performed poorly,
with very low recall over the test set, in contrast with
the development results, where the higher recall re-
sulted in a higher F-score than the look-up approach.
The best of our systems was the full combination of
CRF and the look-up tagger, with a 27.23% F-score.
The results for each event separately are given in
Table 6. The system performs much worse on regu-
lation events, due to the difficulty of having to cor-
Event Class Rec. Prec. FSc.
Localization 25.86 65.22 37.04
Binding 17.00 28.92 21.42
Gene-expression 45.71 69.18 55.05
Transcription 34.31 26.26 29.75
Protein-catabolism 42.86 85.71 57.14
Phosphorylation 45.19 64.21 53.04
EVT-TOTAL 35.84 53.15 42.81
Regulation 15.46 13.24 14.26
Positive-regulation 13.84 14.82 14.31
Negative-regulation 12.14 20.44 15.23
REG-TOTAL 13.73 15.31 14.48
ALL-TOTAL 24.36 30.87 27.23
Table 6: Results for the different events from our com-
bined system. Averaged scores for single events, regula-
tions, and all.
rectly identify other events in the near context.
4.2 Task 3
For testing, we repurposed all of the development
data as training data and retrained our classifiers.
The results in Table 7 were somewhat disappointing,
but a drop in recall versus the equivalent run over
the development data using oracle task 1 annotations
was unsurprising and the ratio of this drop is within
the bounds of what we would expect. The substan-
tial drop in precision can similarly be explained by
flow-on effects from our task 1 classification, a nat-
ural consequence of our pipeline architecture. It is
quite possible for our system to identify false pos-
itive events as being modified; in the online eval-
uation system, these classifications of non-existent
events reduce our precision in task 3.
In the feature engineering stage, we primarily
used the oracle data for task 1 to maximise the
amount of training data available. We felt that if we
were to use our task 1 classifications for events and
trigger words, the effectively lower number of train-
ing instances would only hurt performance. How-
ever this possibly led to bias towards features which
were more useful for classifying events that we
couldn?t successfully classify in task 1. The devel-
opment set shows similar performance drops under
these conditions in Table 4.
It is also possible that our features work reason-
ably but that our classification engine trained over
the oracle data simply learnt the wrong parameters
83
Task 1 Mod Fts. Rec. Prec. FSc.
Comb(Best) Spc B+3?3 2.88 12.24 4.67
Comb(Best) Spc S2 4.33 37.50 7.76
Comb(Best) Spc S3 4.81 30.30 8.30
Comb(All) Spc S3 5.29 26.19 8.80
Comb(Best) Spc S3,B+3?3 4.81 14.08 7.17
Comb(Best) Spc S4 3.85 27.59 6.75
Comb(Best) Spe S5 3.85 27.59 6.75
Comb(Best) Neg B+3?1 3.96 25.00 6.84Comb(Best) Neg N2 5.29 34.48 9.17
Comb(All) Neg N2 5.73 30.00 9.62
Comb(Best) Neg N3 5.29 27.78 8.88
Comb(Best) Neg N4 5.29 34.48 9.17
Comb(Best) Neg N4,B+0?2 4.85 28.12 8.27
Comb(All) Neg N4 5.73 27.27 9.47
Comb(Best) Neg N5 5.29 29.41 8.96
Table 7: Results over test data for task 3 using gold-
standard event annotations (approx recursive matching),
showing which set of trigger word classifications from
task 1 was used as input (submitted results in bold). Fea-
ture sets described in table 3
for the events we had identified correctly in task 1.
We could check this by training a classifier using
our task 1 event classifications combined with the
gold-standard trigger annotations. However com-
bining the gold-standard annotations for task 3 with
the classifier outputs of task 1 is non-trivial and was
not attempted due to time constraints. It also would
have been instructive to calculate a ceiling on our
task 3 performance given our performance in task 1
? i.e. how many modifications we could have cor-
rectly identified with a perfect task 3 classifier, but
we were not able to show this for similar reasons.
5 Conclusions
Our analysis of task 1 seemed to indicate that the
scarcity of training instances was the main reason
for the low recall of CRFs. The look-up system con-
tributed to increase the recall, but at the cost of lower
precision. In order to improve this module we plan
to find ways to extend the training data automatically
in a bootstrapping process.
Another limitation of our system is the event-
construction module, which follows simple rules
and performs poorly on regulation events. For this
subtask we plan to extend the rule set and apply op-
timisation techniques, following the lessons learned
in error analysis.
In task 3 we investigated the application of a pre-
cise, general-purpose grammar over this domain,
and were relatively successful. However, while the
parse coverage for task 3 is very respectable for a
precision grammar on comparatively difficult mate-
rial, it is clearly unwise to throw away 30% of sen-
tences, so a method to extract features from these is
desirable. Further sources of data would also be use-
ful, such as data from the event annotations them-
selves, and additional lexical resources tailored to
the biomedical domain.
We have also shown the syntactico-semantic out-
put of a deep parser, in the form of an RMRS, can
be beneficial in such a task compared with a more
naive approach based on bags of words within a
sliding context window. From Table 4, for nega-
tion, the syntactic features provided substantial per-
formance gains over the best set of baseline param-
eters we could find. For speculation the evidence
here is less compelling, with similar scores from
both approaches. Over test data in Table 7, the the
deep methods showed superior performance, albeit
over a smaller number of instances. Regardless, the
RMRS still has some advantages, giving (unsurpris-
ingly) higher precision than the baseline methods.
Combining naive and deep features does tend to give
slightly higher performance than either of the inputs
over the development data (although not over the test
data, perhaps due to the poorer performance of naive
methods), suggesting that the two approaches iden-
tify slightly different kinds of modification.
Our system suffered from the pipeline approach ?
there was no way to recover from an incorrect classi-
fication in task 1, resulting in greatly reduced preci-
sion and recall in task 3. It is possible that a carefully
constructed integrated system could annotate events
for trigger words and argument at the same time as
modification, with features shared between the two,
which may avoid some of these issues.
Acknowledgements
We wish to thank Rebecca Dridan, Dan Flickinger
and Lawrence Cavedon for their advice. NICTA
is funded by the Australian Government as repre-
sented by the Department of Broadband, Communi-
cations and the Digital Economy and the Australian
Research Council through the ICT Centre of Excel-
lence program.
84
References
Edward Briscoe, John Carroll, and Rebecca Watson.
2006. The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 Interactive Poster
System, pages 77?80, Sydney, Australia.
Ann Copestake and Dan Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Interna-
tional Conference on Language Resources and Evalu-
ation.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
pages 281?332.
Ann Copestake. 2004. Report on the design of RMRS.
Technical Report D1.1a, University of Cambridge,
Cambridge, UK.
Anette Frank. 2004. Constraint-based RMRS construc-
tion from shallow grammars. In COLING ?04: Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, page 1269, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning, pages 282?289.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on condi-
tional random fields. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 49?57, Melbourne, Australia.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics - 10th Panhellenic Conference on Infor-
matics, pages 382?392.
85
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228?236,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Effects of Semantic Annotations on Precision Parse Ranking
Andrew MacKinlay??, Rebecca Dridan??, Diana McCarthy?? and Timothy Baldwin??
? Dept. of Computing and Information Systems, University of Melbourne, Australia
? NICTA Victoria Research Laboratories, University of Melbourne, Australia
? Department of Informatics, University of Oslo, Norway
? Computational Linguistics and Phonetics, Saarland University, Germany
amack@csse.unimelb.edu.au, rdridan@ifi.uio.no,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
We investigate the effects of adding semantic
annotations including word sense hypernyms
to the source text for use as an extra source
of information in HPSG parse ranking for the
English Resource Grammar. The semantic an-
notations are coarse semantic categories or en-
tries from a distributional thesaurus, assigned
either heuristically or by a pre-trained tagger.
We test this using two test corpora in different
domains with various sources of training data.
The best reduces error rate in dependency F-
score by 1% on average, while some methods
produce substantial decreases in performance.
1 Introduction
Most start-of-the-art natural language parsers (Char-
niak, 2000; Clark and Curran, 2004; Collins, 1997)
use lexicalised features for parse ranking. These are
important to achieve optimal parsing accuracy, and
yet these are also the features which by their nature
suffer from data-sparseness problems in the training
data. In the absence of reliable fine-grained statis-
tics for a given token, various strategies are possible.
There will often be statistics available for coarser
categories, such as the POS of the particular token.
However, it is possible that these coarser represen-
tations discard too much, missing out information
which could be valuable to the parse ranking. An
intermediate level of representation could provide
valuable additional information here. For example,
?This research was conducted while the second author was
a postdoctoral researcher within NICTA VRL.
?The third author is a visiting scholar on the Erasmus
Mundus Masters Program in ?Language and Communication
Technologies? (LCT, 2007?0060).
assume we wish to correctly attach the prepositional
phrases in the following examples:
(1) I saw a tree with my telescope
(2) I saw a tree with no leaves
The most obvious interpretation in each case has the
prepositional phrase headed by with attaching in dif-
ferent places: to the verb phrase in the first example,
and to the noun tree in the second. Such distinctions
are difficult for a parser to make when the training
data is sparse, but imagine we had seen examples
such as the following in the training corpus:
(3) Kim saw a eucalypt with his binoculars
(4) Sandy observed a willow with plentiful foliage
There are few lexical items in common, but in each
case the prepositional phrase attachment follows the
same pattern: in (3) it attaches to the verb, and in
(4) to the noun. A conventional lexicalised parser
would have no knowledge of the semantic similarity
between eucalypt and tree, willow and tree, binoc-
ulars and telescope, or foliage and leaves, so would
not be able to make any conclusions about the earlier
examples on the basis of this training data. However
if the parse ranker has also been supplied with in-
formation about synonyms or hypernyms of the lex-
emes in the training data, it could possibly have gen-
eralised, to learn that PPs containing nouns related
to seeing instruments often modify verbs relating to
observation (in preference to nouns denoting inani-
mate objects), while plant flora can often be modi-
fied by PPs relating to appendages of plants such as
leaves. This is not necessarily applicable only to PP
attachment, but may help in a range of other syntac-
tic phenomena, such as distinguishing between com-
plements and modifiers of verbs.
228
The synonyms or hypernyms could take the form
of any grouping which relates word forms with se-
mantic or syntactic commonality ? such as a label
from the WordNet (Miller, 1995) hierarchy, a sub-
categorisation frame (for verbs) or closely related
terms from a distributional thesaurus (Lin, 1998).
We present work here on using various levels
of semantic generalisation as an attempt to im-
prove parse selection accuracy with the English Re-
source Grammar (ERG: Flickinger (2000)), a preci-
sion HPSG-based grammar of English.
2 Related Work
2.1 Parse Selection for Precision Grammars
The focus of this work is on parsing using hand-
crafted precision HPSG-based grammars, and in
particular the ERG. While these grammars are care-
fully crafted to avoid overgeneration, the ambiguity
of natural languages means that there will unavoid-
ably be multiple candidate parses licensed by the
grammar for any non-trivial sentence. For the ERG,
the number of parses postulated for a given sentence
can be anywhere from zero to tens of thousands. It
is the job of the parse selection model to select the
best parse from all of these candidates as accurately
as possible, for some definition of ?best?, as we dis-
cuss in Section 3.2.
Parse selection is usually performed by training
discriminative parse selection models, which ?dis-
criminate? between the set of all candidate parses.
A widely-used method to achieve this is outlined
in Velldal (2007). We feed both correct and incor-
rect parses licensed by the grammar to the TADM
toolkit (Malouf, 2002), and learn a maximum en-
tropy model. This method is used by Zhang et al
(2007) and MacKinlay et al (2011) inter alia. One
important implementation detail is that rather than
exhaustively ranking all candidates out of possibly
many thousands of trees, Zhang et al (2007) showed
that it was possible to use ?selective unpacking?,
which means that the exhaustive parse forest can be
represented compactly as a ?packed forest?, and the
top-ranked trees can be successively reconstructed,
enabling faster parsing using less memory.
2.2 Semantic Generalisation for parse ranking
Above, we outlined a number of reasons why
semantic generalisation of lexemes could enable
parsers to make more efficient use of training data,
and indeed, there has been some prior work investi-
gating this possibility. Agirre et al (2008) applied
two state-of-the-art treebank parsers to the sense-
tagged subset of the Brown corpus version of the
Penn Treebank (Marcus et al, 1993), and added
sense annotation to the training data to evaluate their
impact on parse selection and specifically on PP-
attachment. The annotations they used were oracle
sense annotations, automatic sense recognition and
the first sense heuristic, and it was this last method
which was the best performer in general. The sense
annotations were either the WordNet synset ID or
the coarse semantic file, which we explain in more
detail below, and replaced the original tokens in
the training data. The largest improvement in pars-
ing F-score was a 6.9% reduction in error rate for
the Bikel parser (Bikel, 2002), boosting the F-score
from 0.841 to 0.852, using the noun supersense only.
More recently, Agirre et al (2011) largely repro-
duced these results with a dependency parser.
Fujita et al (2007) add sense information to im-
prove parse ranking with JaCy (Siegel and Bender,
2002), an HPSG-based grammar which uses simi-
lar machinery to the ERG. They use baseline syn-
tactic features, and also add semantic features based
on dependency triples extracted from the semantic
representations of the sentence trees output by the
parser. The dataset they use has human-assigned
sense tags from a Japanese lexical hierarchy, which
they use as a source of annotations. The dependency
triples are modified in each feature set by replacing
elements of the semantic triples with corresponding
senses or hypernyms. In the best-performing con-
figuration, they use both syntactic and semantic fea-
tures with multiple levels of the the semantic hier-
archy from combined feature sets. They achieve a
5.6% improvement in exact match parsing accuracy.
3 Methodology
We performed experiments in HPSG parse rank-
ing using the ERG, evaluating the impact on parse
selection of semantic annotations such as coarse
sense labels or synonyms from a distributional the-
229
WESCIENCE LOGON
Total Sentences 9632 9410
Parseable Sentences 9249 8799
Validated Sentences 7631 8550
Train/Test Sentences 6149/1482 6823/1727
Tokens/sentence 15.0 13.6
Training Tokens 92.5k 92.8k
Table 1: Corpora used in our experiments, with total sen-
tences, how many of those can be parsed, how many of
the parseable sentences have a single gold parse (and are
used in these experiments), and average sentence length
saurus. Our work here differs from the aforemen-
tioned work of Fujita et al (2007) in a number of
ways. Firstly, we use purely syntactic parse selec-
tion features based on the derivation tree of the sen-
tence (see Section 3.4.3), rather than ranking using
dependency triples, meaning that our method is in
principle able to be integrated into a parser more eas-
ily, where the final set of dependencies would not be
known in advance. Secondly, we do not use human-
created sense annotations, instead relying on heuris-
tics or trained sense-taggers, which is closer to the
reality of real-world parsing tasks.
3.1 Corpora
Following MacKinlay et al (2011), we use two pri-
mary training corpora. First, we use the LOGON
corpus (Oepen et al, 2004), a collection of En-
glish translations of Norwegian hiking texts. The
LOGON corpus contains 8550 sentences with ex-
actly one gold parse, which we partitioned ran-
domly by sentence into 10 approximately equal sec-
tions, reserving two sections as test data, and us-
ing the remainder as our training corpus. These
sentences were randomly divided into training and
development data. Secondly, we use the We-
Science (Ytrest?l et al, 2009) corpus, a collection
of Wikipedia articles related to computational lin-
guistics. The corpus contains 11558 sentences, from
which we randomly chose 9632, preserving the re-
mainder for future work. This left 7631 sentences
with a single gold tree, which we divided into a
training set and a development set in the same way.
The corpora are summarised in Table 1.
With these corpora, we are able to investigate in-
domain and cross-domain effects, by testing on a
different corpus to the training corpus, so we can
examine whether sense-tagging alleviates the cross-
domain performance penalty noted in MacKinlay et
al. (2011). We can also use a subset of each training
corpus to simulate the common situation of sparse
training data, so we can investigate whether sense-
tagging enables the learner to make better use of a
limited quantity of training data.
3.2 Evaluation
Our primary evaluation metric is Elementary De-
pendency Match (Dridan and Oepen, 2011). This
converts the semantic output of the ERG into a set
of dependency-like triples, and scores these triples
using precision, recall and F-score as is conven-
tional for other dependency evaluation. Following
MacKinlay et al (2011), we use the EDMNA mode
of evaluation, which provides a good level of com-
parability while still reflecting most the semantically
salient information from the grammar.
Other work on the ERG and related grammars has
tended to focus on exact tree match, but the granu-
lar EDM metric is a better fit for our needs here ?
among other reasons, it is more sensitive in terms
of error rate reduction to changes in parse selection
models (MacKinlay et al, 2011). Additionally, it is
desirable to be able to choose between two different
parses which do not match the gold standard exactly
but when one of the parses is a closer match than the
other; this is not possible with exact match accuracy.
3.3 Reranking for parse selection
The features we are adding to the parse selection
procedure could all in principle be applied by the
parser during the selective unpacking stage, since
they all depend on information which can be pre-
computed. However, we wish to avoid the need for
multiple expensive parsing runs, and more impor-
tantly the need to modify the relatively complex in-
ternals of the parse ranking machinery in the PET
parser (Callmeier, 2000). So instead of performing
the parse ranking in conjunction with parsing, as is
the usual practice, we use a pre-parsed forest of the
top-500 trees for each corpus, and rerank the forest
afterwards for each configuration shown.
The pre-parsed forests use the same models which
were used in treebanking. Using reranking means
that the set of candidate trees is held constant, which
230
means that parse selection models never get the
chance to introduce a new tree which was not in
the original parse forest from which the gold tree
was annotated, which may provide a very small per-
formance boost (although when the parse selection
models are similar as is the case for most of the mod-
els here, this effect is likely to be very small).
3.4 Word Sense Annotations
3.4.1 Using the WordNet Hierarchy
Most experiments we report on here make some
use of the WordNet sense inventory. Obviously we
need to determine the best sense and corresponding
WordNet synset for a given token. We return to this
in Section 3.4.2, but for now assume that the sense
disambiguation is done.
As we are concerned primarily with making
commonalities between lemmas with different base
forms apparent to the parse selection model, the fine-
grained synset ID will do relatively little to provide
a coarser identifier for the token ? indeed, if two
tokens with identical forms were assigned different
synset IDs, we would be obscuring the similarity.1
We can of course make use of the WordNet hier-
archy, and use hypernyms from the hierarchy to tag
each candidate token, but there are a large number
of ways this can be achieved, particularly when it
is possibly to assign multiple labels per token as is
the case here (which we discuss in Section 3.4.3).
We apply two relatively simple strategies. We noted
in Section 2.2 that Agirre et al (2008) found that
the semantic file was useful. This is the coarse lex-
icographic category label, elsewhere denoted super-
sense (Ciaramita and Altun, 2006), which is the
terminology we use. Nouns are divided into 26
coarse categories such as ?animal?, ?quantity? or
?phenomenon?, and verbs into 15 categories such as
?perception? or ?consumption?. In some configura-
tions, denoted SS, we tag each open-class token with
one of the supersense labels.
Another configuration attempts to avoid making
assumptions about which level of the hierarchy will
be most useful for parse disambiguation, instead
leaving it the MaxEnt parse ranker to pick those la-
bels from the hierarchy which are most useful. Each
1This could be useful for verbs since senses interact strongly
subcategorisation frames, but that is not our focus here.
open class token is labelled with multiple synsets,
starting with the assigned leaf synset and travelling
as high as possible up the hierarchy, with no distinc-
tion made between the different levels in the hier-
archy. Configurations using this are designated HP,
for ?hypernym path?.
3.4.2 Disambiguating senses
We return now to the question of determination
of the synset for a given token. One frequently-
used and robust strategy is to lemmatise and POS-
tag each token, and assign it the first-listed sense
from WordNet (which may or may not be based on
actual frequency counts). We POS-tag using TnT
(Brants, 2000) and lemmatise using WordNet?s na-
tive lemmatiser. This yields a leaf-level synset, mak-
ing it suitable as a source of annotations for both SS
and HP. We denote this ?WNF? for ?WordNet First?
(shown in parentheses after SS or HP).
Secondly, to evaluate whether a more informed
approach to sense-tagging helps beyond the naive
WNF method, in the ?SST? method, we use the out-
puts of SuperSense Tagger (Ciaramita and Altun,
2006), which is optimised for assigning the super-
senses described above, and can outperform a WNF-
style baseline on at least some datasets. Since this
only gives us coarse supersense labels, it can only
provide SS annotations, as we do not get the leaf
synsets needed for HP. The input we feed in is POS-
tagged with TnT as above, for comparability with
the WNF method, and to ensure that it is compati-
ble with the configuration in which the corpora were
parsed ? specifically, the unknown-word handling
uses a version of the sentences tagged with TnT. We
ignore multi-token named entity outputs from Su-
perSense Tagger, as these would introduce a con-
founding factor in our experiments and also reduce
comparability of the results with the WNF method.
3.4.3 A distributional thesaurus method
A final configuration attempts to avoid the need
for curated resources such as WordNet, instead us-
ing an automatically-constructed distributional the-
saurus (Lin, 1998). We use the thesaurus from
McCarthy et al (2004), constructed along these
lines using the grammatical relations from RASP
(Briscoe and Carroll, 2002) applied to 90 millions
words of text from the British National Corpus.
231
root_frag
np_frg_c
hdn_bnp_c
aj-hdn_norm_c
legal_a1
"legal"
n_pl_olr
issue_n1
"issues"
Figure 1: ERG derivation tree for the phrase Legal issues
[n_-_c_le "issues"]
[n_pl_olr n_-_c_le "issues"]
[aj-hdn_norm_c n_pl_olr n_-_c_le "issues"]
(a) Original features
[n_-_c_le noun.cognition]
[n_pl_olr n_-_c_le noun.cognition]
[aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition]
(b) Additional features in leaf mode, which augment the original
features
[noun.cognition "issues"]
[n_pl_olr noun.cognition "issues"]
[aj-hdn_norm_c n_pl_olr noun.cognition "issues"]
(c) Additional features in leaf-parent (?P?) mode, which augment
the original features
Figure 2: Examples of features extracted from for
"issues" node in Figure 1 with grandparenting level
of 2 or less
To apply the mapping, we POS-tag the text with
TnT as usual, and for each noun, verb and adjec-
tive we lemmatise the token (with WordNet again,
falling back to the surface form if this fails), and
look up the corresponding entry in the thesaurus. If
there is a match, we select the top five most simi-
lar entries (or fewer if there are less than five), and
use these new entries to create additional features,
as well as adding a feature for the lemma itself in all
cases. This method is denoted LDT for ?Lin Distri-
butional Thesaurus?. We note that many other meth-
ods could be used to select these, such as different
numbers of synonyms, or dynamically changing the
number of synonyms based on a threshold against
the top similarity score, but this is not something we
evaluate in this preliminary investigation.
Adding Word Sense to Parse Selection Models
We noted above that parse selection using the
methodology established by Velldal (2007) uses
human-annotated incorrect and correct derivation
trees to train a maximum entropy parse selection
model. More specifically, the model is trained using
features extracted from the candidate HPSG deriva-
tion trees, using the labels of each node (which are
the rule names from the grammar) and those of a
limited number of ancestor nodes.
As an example, we examine the noun phrase Le-
gal issues from the WESCIENCE corpus, for which
the correct ERG derivation tree is shown in Figure 1.
Features are created by examining each node in the
tree and at least its parent, with the feature name set
to the concatenation of the node labels. We also gen-
erally make used of grandparenting features, where
we examine earlier ancestors in the derivation tree.
A grandparenting level of one means we would also
use the label of the grandparent (i.e. the parent?s par-
ent) of the node, a level of two means we would add
in the great-grandparent label, and so on. Our exper-
iments here use a maximum grandparenting level of
three. There is also an additional transformation ap-
plied to the tree ? the immediate parent of each leaf
is, which is usually a lexeme, is replaced with the
corresponding lexical type, which is a broader par-
ent category from the type hierarchy of the grammar,
although the details of this are not relevant here.
For the node labelled "issues" in Figure 1 with
grandparenting levels from zero to two, we would
extract the features as shown in Figure 2(a) (where
the parent node issue_n1 has already been re-
placed with its lexical type n_-c_le).
In this work here, we create variants of these fea-
tures. A preprocessing script runs over the training
or test data, and for each sentence lists variants of
each token using standoff markup indexed by char-
acter span, which are created from the set of addi-
tional semantic tags assigned to each token by the
word sense configuration (from those described in
Section 3.4) which is currently in use. These sets of
semantic tags for a given word could be a single su-
persense tag, as in SS, a set of synset IDs as in HP
or a set of replacement lemmas in LDT. In all cases,
the set of semantic tags could also be empty ? if ei-
ther the word has a part of speech which we are not
232
Test Train SS (WNF) SSp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 85.09/82.33/83.69 +0.09 84.81/82.20/83.48 ?0.11
WESC (92k) 86.56/83.58/85.05 86.83/84.04/85.41 +0.36 87.03/83.96/85.47 +0.42
LOG (23k) 88.60/87.23/87.91 88.72/87.20/87.95 +0.04 88.43/87.00/87.71 ?0.21
LOG (92k) 91.74/90.15/90.94 91.82/90.07/90.94 ?0.00 91.90/90.13/91.01 +0.07
WESC
WESC (23k) 86.80/84.43/85.60 87.12/84.44/85.76 +0.16 87.18/84.50/85.82 +0.22
WESC (92k) 89.34/86.81/88.06 89.54/86.76/88.13 +0.07 89.43/87.23/88.32 +0.26
LOG (23k) 83.74/81.41/82.56 84.02/81.43/82.71 +0.15 84.10/81.67/82.86 +0.31
LOG (92k) 85.98/82.93/84.43 86.02/82.69/84.32 ?0.11 85.89/82.76/84.30 ?0.13
Table 2: Results for SS (WNF) (supersense from first WordNet sense), evaluated on 23k tokens (approx 1500
sentences) of either WESCIENCE or LOGON, and trained on various sizes of in-domain and cross-domain training
data. Subscript ?p? indicates mappings were applied to leaf parents rather than leaves.
Test Train SS (SST) SSp(SST)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.97/82.38/83.65 +0.06 85.32/82.66/83.97 +0.37
WESC (92k) 86.56/83.58/85.05 87.05/84.47/85.74 +0.70 86.98/83.87/85.40 +0.35
LOG (23k) 88.60/87.23/87.91 88.93/87.50/88.21 +0.29 88.84/87.40/88.11 +0.20
LOG (92k) 91.74/90.15/90.94 91.67/90.02/90.83 ?0.10 91.47/89.96/90.71 ?0.23
WESC
WESC (23k) 86.80/84.43/85.60 86.88/84.29/85.56 ?0.04 87.32/84.48/85.88 +0.27
WESC (92k) 89.34/86.81/88.06 89.53/86.54/88.01 ?0.05 89.50/86.56/88.00 ?0.05
LOG (23k) 83.74/81.41/82.56 84.06/81.30/82.66 +0.10 83.96/81.64/82.78 +0.23
LOG (92k) 85.98/82.93/84.43 86.13/82.96/84.51 +0.08 85.76/82.84/84.28 ?0.16
Table 3: Results for SS (SST) (supersense from SuperSense Tagger)
Test Train HPWNF HPp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.56/82.03/83.28 ?0.32 84.74/82.20/83.45 ?0.15
WESC (92k) 86.56/83.58/85.05 86.65/84.22/85.42 +0.37 86.41/83.65/85.01 ?0.04
LOG (23k) 88.60/87.23/87.91 88.58/87.26/87.92 +0.00 88.58/87.35/87.96 +0.05
LOG (92k) 91.74/90.15/90.94 91.68/90.19/90.93 ?0.01 91.66/89.85/90.75 ?0.19
WESC
WESC (23k) 86.80/84.43/85.60 86.89/84.19/85.52 ?0.08 87.18/84.43/85.78 +0.18
WESC (92k) 89.34/86.81/88.06 89.74/86.96/88.33 +0.27 89.23/86.88/88.04 ?0.01
LOG (23k) 83.74/81.41/82.56 83.87/81.20/82.51 ?0.04 83.47/81.00/82.22 ?0.33
LOG (92k) 85.98/82.93/84.43 85.89/82.38/84.10 ?0.33 85.75/83.03/84.37 ?0.06
Table 4: Results for HPWNF (hypernym path from first WordNet sense)
Test Train LDTp(5)
P/ R/ F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.48/82.18/83.31 ?0.28
WESC (92k) 86.56/83.58/85.05 86.36/84.14/85.23 +0.19
LOG (23k) 88.60/87.23/87.91 88.28/86.99/87.63 ?0.28
LOG (92k) 91.74/90.15/90.94 91.01/89.25/90.12 ?0.82
WESC
WESC (23k) 86.80/84.43/85.60 86.17/83.51/84.82 ?0.78
WESC (92k) 89.34/86.81/88.06 88.31/85.61/86.94 ?1.12
LOG (23k) 83.74/81.41/82.56 83.60/81.18/82.37 ?0.19
LOG (92k) 85.98/82.93/84.43 85.74/82.96/84.33 ?0.11
Table 5: Results for LDT (5) (Lin-style distributional thesaurus, expanding each term with the top-5 most similar)
233
attempting to tag semantically, or if our method has
no knowledge of the particular word.
The mapping is applied at the point of feature ex-
traction from the set of derivation trees ? at model
construction time for the training set and at rerank-
ing time for the development set. If a given leaf to-
ken has some set of corresponding semantic tags, we
add a set of variant features for each semantic tag,
duplicated and modified from the matching ?core?
features described above. There are two ways these
mappings can be applied, since it is not immedi-
ately apparent where the extra lexical generalisation
would be most useful. The ?leaf? variant applies to
the leaf node itself, so that in each feature involving
the leaf node, add a variant where the leaf node sur-
face string has been replaced with the new seman-
tic tag. The ?parent? variant, which has a subscript
?P? (e.g. SSp(WNF) ) applies the mapping to the
immediate parent of the leaf node, leaving the leaf
itself unchanged, but creating variant features with
the parent nodes replaced with the tag.
For our example here, we assume that we have
an SS mapping for Figure 2(a), and that this has
mapped the token for "issues" to the WordNet
supersense noun.cognition. For the leaf vari-
ant, the extra features that would be added (either for
considering inclusion in the model, or for scoring a
sentence when reranking) are shown in Figure 2(b),
while those for the parent variant are in Figure 2(c).
3.4.4 Evaluating the contribution of sense
annotations
Wewish to evaluate whether adding sense annota-
tions improve parser accuracy against the baseline of
training a model in the conventional way using only
syntactic features. As noted above, we suspect that
this semantic generalisation may help in cases where
appropriate training data is sparse ? that is, where
the training data is from a different domain or only
a small amount exists. So to evaluate the various
methods in these conditions, we train models from
small (23k token) training sets and large (96k token)
training sets created from subsets of each corpus
(WESCIENCE and LOGON). For the baseline, we
train these models without modification. For each
of the various methods of adding semantic tags, we
then re-use each of these training sets to create new
models after adding the appropriate additional fea-
tures as described above, to evaluate whether these
additional features improve parsing accuracy
4 Results
We present an extensive summary of the results ob-
tained using the various methods in Tables 2, 3, 4
and 5. In each case we show results for applying
to the leaf and to the parent. Aggregating the re-
sults for each method, the differences range between
substantially negative and modestly positive, with a
large number of fluctuations due to statistical noise.
LDT is the least promising performer, with only
one very modest improvement, and the largest de-
creases in performance, of around 1%. The HP-
WNF and HPp(WNF) methods make changes in
either direction ? on average, over all four train-
ing/test combinations, there are very small drops
in F-score of 0.02% for HPWNF, and 0.06% for
HPp(WNF), which indicates that neither of the
methods is likely to be useful in reliably improving
parser performance.
The SS methods are more promising. SS (WNF)
and SSp(WNF) methods yield an average im-
provement of 0.10% each, while SS (SST) and
SSp(SST) give average improvements of 0.12%
and 0.13% respectively (representing an error rate
reduction of around 1%). Interestingly, the increase
in tagging accuracy we might expect using Super-
Sense Tagger only translates to a modest (and prob-
ably not significant) increase in parser performance,
possibly because the tagger is not optimised for the
domains in question. Amongst the statistical noise
it is hard to discern overall trends; surprisingly, it
seems that the size of the training corpus has rela-
tively little to do with the success of adding these su-
persense annotations, and that the corpus being from
an unmatched domain doesn?t necessarily mean that
sense-tagging will improve accuracy either. There
may be a slight trend for sense annotations to be
more useful when WESCIENCE is the training cor-
pus (either in the small or the large size).
To gain a better insight into how the effects
change as the size of the training corpus changes for
the different domains, we created learning curves for
the best-performing method, SSp(SST) (although
as noted above, all SS methods give similar levels
of improvement), shown in Figure 3. Overall, these
234
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on LOGON Corpus
Test Corpus
LOGON*
LOGON* +SS
WeSc
WeSc +SS
(a) LOGON
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on WeScience Corpus
Test Corpus
LOGON
LOGON +SS
WeSc*
WeSc* +SS
(b) WESCIENCE
Figure 3: EDMNA learning curves for SS (SST) (supersense from SuperSense Tagger). ?*? denotes in-domain
training corpus.
graphs support the same conclusions as the tables
? the gains we see are very modest and there is a
slight tendency for WESCIENCE models to benefit
more from the semantic generalisation, but no strong
tendencies for this to work better for cross-domain
training data or small training sets.
5 Conclusion
We have presented an initial study evaluat-
ing whether a fairly simple approach to using
automatically-created coarse semantic annotations
can improve HPSG parse selection accuracy using
the English Resource Grammar. We have provided
some weak evidence that adding features based on
semantic annotations, and in particular word super-
sense, can provide modest improvements in parse
selection performance in terms of dependency F-
score, with the best-performing method SSp(SST)
providing an average reduction in error rate over 4
training/test corpus combinations of 1%. Other ap-
proaches were less promising. In all configurations,
there were instances of F-score decreases, some-
times substantial.
It is somewhat surprising that we did not achieve
reliable performance gains which were seen in the
related work described above. One possible expla-
nation is that the model training parameters were
suboptimal for this data set since the characteris-
tics of the data are somewhat different than with-
out sense annotations. The failure to improve some-
what mirrors the results of Clark (2001), who was at-
tempting to improve the parse ranking performance
of the unification-based based probabilistic parser of
Carroll and Briscoe (1996). Clark (2001) used de-
pendencies to rank parses, and WordNet-based tech-
niques to generalise this model and learn selectional
preferences, but failed to improve performance over
the structural (i.e. non-dependency) ranking in the
original parser. Additionally, perhaps the changes
we applied in this work to the parse ranking could
possibly have been more effective with features
based on semantic dependences as used by Fujita
et al (2007), although we outlined reasons why we
wished to avoid this approach.
This work is preliminary and there is room for
more exploration in this space. There is scope for
much more feature engineering on the semantic an-
notations, such as using different levels of the se-
mantic hierarchy, or replacing the purely lexical fea-
tures instead of augmenting them. Additionally,
more error analysis would reveal whether this ap-
proach was more useful for avoiding certain kinds
of parser errors (such as PP-attachment).
Acknowledgements
NICTA is funded by the Australian Government as
represented by the Department of Broadband, Com-
munications and the Digital Economy and the Aus-
tralian Research Council through the ICT Centre of
Excellence program.
235
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of ACL-08: HLT, pages
317?325, Columbus, Ohio, June.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency parsing
with semantic classes. In Proceedings of the 49th An-
nual Meeting of the Association of Computational Lin-
guistics, ACL-HLT 2011 Short Paper, Portland, Ore-
gon?.
D. M. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of the second international conference on Human
Language Technology Research, pages 178?182, San
Francisco, CA, USA.
T. Brants. 2000. Tnt ? a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Ap-
plied Natural Language Processing, pages 224?231,
Seattle, Washington, USA, April.
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd International Conference on Language Resources
and Evaluation, pages 1499?1504.
U. Callmeier. 2000. Pet ? a platform for experimenta-
tion with efficient HPSG processing techniques. Nat.
Lang. Eng., 6(1):99?107.
J. Carroll and E. Briscoe. 1996. Apportioning devel-
opment effort in a probabilistic lr pars- ing system
through evaluation. In Proceedings of the SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 92?100, Philadelphia, PA.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 132?139.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594?602, Sydney, Australia,
July.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104?111.
S. Clark. 2001. Class-based Statistical Models for Lex-
ical Knowledge Acquisition. Ph.D. thesis, University
of Sussex.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23, Madrid, Spain, July.
R. Dridan and S. Oepen. 2011. Parser evaluation us-
ing elementary dependency matching. In Proceedings
of the 12th International Conference on Parsing Tech-
nologies, pages 225?230, Dublin, Ireland, October.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Nat. Lang. Eng., 6
(1):15?28.
S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Ex-
ploiting semantic information for HPSG parse selec-
tion. In ACL 2007 Workshop on Deep Linguistic Pro-
cessing, pages 25?32, Prague, Czech Republic, June.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2,
pages 768?774.
A. MacKinlay, R. Dridan, D. Flickinger, and T. Baldwin.
2011. Cross-domain effects on parse selection for pre-
cision grammars. Research on Language & Computa-
tion, 8(4):299?340.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
the Sixth Conference on Natural Language Learning
(CoNLL-2002), pages 49?55.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
the penn treebank. Comput. Linguist., 19(2):313?330.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 279?es.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39?41.
S. Oepen, D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods: A rich and dynamic
treebank for HPSG. Research on Language & Com-
putation, 2(4):575?596.
M. Siegel and E.M. Bender. 2002. Efficient deep pro-
cessing of japanese. In Proceedings of the 3rd work-
shop on Asian language resources and international
standardization-Volume 12, pages 1?8.
E. Velldal. 2007. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo Department of Informatics.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and annotating Wikipedia sub-domains ? to-
wards a new eScience community resourc. In Pro-
ceedings of the Seventh International Workshop on
Treebanks and Linguistic Theories, Groeningen, The
Netherlands, January.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency in
unification-based n-best parsing. In IWPT ?07: Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 48?59, Morristown, NJ, USA.
236
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 15?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Intelligent Linux Information Access by Data Mining: the ILIAD Project
Timothy Baldwin,? David Martinez,? Richard B. Penman,? Su Nam Kim,?
Marco Lui,? Li Wang? and Andrew MacKinlay?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
Abstract
We propose an alternative to conventional in-
formation retrieval over Linux forum data,
based on thread-, post- and user-level analysis,
interfaced with an information retrieval engine
via reranking.
1 Introduction
Due to the sheer scale of web data, simple keyword
matching is an effective means of information ac-
cess for many informational web queries. There
still remain significant clusters of information access
needs, however, where keyword matching is less
successful. One such instance is technical web fo-
rums and mailing lists (collectively termed ?forums?
for the purposes of this paper): technical forums
are a rich source of information when troubleshoot-
ing, and it is often possible to resolve technical
queries/problems via web-archived data. The search
facilities provided by forums and web search en-
gines tend to be over-simplistic, however, and there
is a desperate need for more sophisticated search (Xi
et al, 2004; Seo et al, 2009), including: favour-
ing threads which have led to a successful resolu-
tion; reflecting the degree of clarity/reproducibility
of the proposed solution in a given thread; repre-
senting threads via their threaded rather than sim-
ple chronological structure; the ability to highlight
key aspects of the thread, in terms of the problem
description and solution which led to a successful
resolution; and ideally, the ability to represent the
problem and solution in normalised form via infor-
mation extraction.
This paper provides a brief outline of an attempt
to achieve these and other goals in the context of
Linux web user forum data, in the form of the IL-
IAD (Intelligent Linux Information Access by Data
Mining) project. Linux users and developers rely
particularly heavily on web user forums and mail-
ing lists, due to the nature of the community, which
is highly decentralised ? with massive proliferation
of packages and distributions? and notoriously bad
at maintaining up-to-date documentation at a level
suitable for newbie and even intermediate users.
2 Project Outline
Our proposed solution is as follows: (1) crawl data
from a variety of web user forums; (2) analyse each
thread, to identify named entities and generate meta-
data; (3) analyse post-level linkages; (4) predict
user-level features which are expected to impinge on
the quality of search results; and finally (5) draw to-
gether the features from (1) to (4) to enhance the
quality of a traditional ranked IR approach. We
briefly review each step below. Given space limi-
tations, we focus on outlining our interpretation of
the task in this paper. For further details and results,
the reader is referred to the key papers cited herein.
2.1 Crawling
The first step is to crawl data from a variety of fo-
rums and mailing lists, for which we have developed
open-source scraping software in the form of SITE-
SCRAPER.1 SITESCRAPER is designed such that the
user simply copies relevant content from a browser-
rendered version of a given set of pages, which it
interprets as a structured record, and translates into
a generalised XPATH query.
2.2 Thread-level analysis
Next, we perform named entity recognition (NER)
over each thread to identify entities such as package
and distribution names, version numbers and snip-
pets of code; as part of this, we perform version
1http://sitescraper.googlecode.com/
15
anchoring, in identifying what entity each version
number relates to.
To generate thread-level metadata, we classify
each thread for the following three features, based
on an ordinal scale of 1?5 (Baldwin et al, 2007):
Complete: Is the problem description complete?
Solved: Is a solution provided in the thread?
Task Oriented: Is the thread about a specific
problem?
We additionally automatically classify the nature
of the thread content, in terms of, e.g., whether it
contains documentation or installation details, or re-
lates to software, hardware or programming.
Our experiments on thread-level classification are
based on a set of 250 annotated threads from Lin-
uxQuestions and other forums, as well as a dataset
from CNET.
2.3 Post-level analysis
We automatically analyse the post-to-post discourse
structure of each thread, in terms of which (preced-
ing) post(s) each post relates to, and how, building
off the work of Rose? et al (1995) and Wolf and Gib-
son (2005). For example, a given post may refute
the solution proposed in an earlier post, and also
propose a novel solution in response to the initiat-
ing post.
Separately, we are developing techniques for
identifying whether a new post to a given forum
is sufficiently similar to other (ideally resolved)
threads that the author should be prompted to first
check the existing threads for redundancy before a
new thread is initiated.
Our experiments on post-level analysis are, once
again, based on data from LinuxQuestions and
CNET.
2.4 User-level analysis
We are also experimenting with profiling users vari-
ously, based on a 5-point ordinal scale across a range
of user characteristics. Our experiments are based
on data from LinuxQuestions (Lui, 2009).
2.5 IR ranking
The various features are interfaced with an ad hoc
information retrieval (IR) system via a learning-to-
rank approach (Cao et al, 2007). In order to carry
out IR evaluation, we have developed a set of queries
and relevance judgements over a large-scale set of
forum data.
Our experiments to date have been based on com-
bination over three IR engines (LUCENE, ZETTAIR
and LEMUR), and involved thread-level metadata
only, but we have achieved encouraging results, sug-
gesting that thread-level metadata can enhance IR
effectiveness.
3 Conclusions
This paper provides an outline of the ILIAD project,
focusing on the tasks of crawling, thread-level anal-
ysis, post-level analysis, user-level analysis and IR
reranking. We have designed a series of class sets
for the component tasks, and carried out experimen-
tation over a range of data sources, achieving en-
couraging results.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
References
T Baldwin, D Martinez, and RB Penman. 2007. Auto-
matic thread classification for Linux user forum infor-
mation access. In Proc of ADCS 2007.
Z Cao, T Qin, TY Liu, MF Tsai, and H Li. 2007. Learn-
ing to rank: from pairwise approach to listwise ap-
proach. In Proc of ICML 2007.
M Lui. 2009. Impact of user characteristics on online fo-
rum classification tasks. Honours thesis, University of
Melbourne. http://repository.unimelb.edu.
au/10187/5745.
CP Rose?, B Di Eugenio, LS Levin, and C Van Ess-
Dykema. 1995. Discourse processing of dialogues
with multiple threads. In Proc of ACL 1995.
J Seo, WB Croft, and DA Smith. 2009. Online commu-
nity search using thread structure. In Proc of CIKM
2009.
F Wolf and E Gibson. 2005. Representing discourse co-
herence: A corpus-based study. Comp Ling, 31(2).
W Xi, J Lind, and E Brill. 2004. Learning effective rank-
ing functions for newsgroup search. In Proc of SIGIR
2004.
16
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35?44,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Extracting Biomedical Events and Modifications Using Subgraph
Matching with Noisy Training Data
Andrew MacKinlay?, David Martinez?, Antonio Jimeno Yepes?,
Haibin Liu?, W. John Wilbur? and Karin Verspoor?
? NICTA Victoria Research Laboratory, University of Melbourne, Australia
{andrew.mackinlay, david.martinez}@nicta.com.au
{antonio.jimeno, karin.verspoor}@nicta.com.au
? National Center for Biotechnology Information, Bethesda, MD, USA
haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov
Abstract
The Genia Event (GE) extraction task of
the BioNLP Shared Task addresses the ex-
traction of biomedical events from the nat-
ural language text of the published litera-
ture. In our submission, we modified an
existing system for learning of event pat-
terns via dependency parse subgraphs to
utilise a more accurate parser and signifi-
cantly more, but noisier, training data. We
explore the impact of these two aspects of
the system and conclude that the change in
parser limits recall to an extent that cannot
be offset by the large quantities of training
data. However, our extensions of the sys-
tem to extract modification events shows
promise.
1 Introduction
In this paper, we describe our submission to the
Genia Event (GE) information extraction subtask
of the BioNLP Shared Task. This task requires the
development of systems that are capable of iden-
tifying bio-molecular events as those events are
expressed in full-text publications. The task rep-
resents an important contribution to the broader
problem of converting unstructured information
captured in the biomedical literature into struc-
tured information that can be used to index and
analyse bio-molecular relationships.
This year?s task builds on previous instantia-
tions of this task (Kim et al, 2009; Kim et al,
2012), with only minor changes in the task defini-
tion introduced for 2011. The task organisers pro-
vided full text publications annotated with men-
tions of biological entities including proteins and
genes, and asked participants to provide annota-
tions of simple events including gene expression,
binding, localization, and protein modification, as
well as higher-order regulation events (e.g., pos-
itive regulation of gene expression). In our sub-
mission, we built on a system originally developed
for the BioNLP-ST 2011 (Liu et al, 2011) and ex-
tended in more recent work (Liu et al, 2013a; Liu
et al, 2013b). This system learns to recognise sub-
graphs of syntactic dependency parse graphs that
express a given bio-molecular event, and matches
those subgraphs to new text using an algorithm
called Approximate Subgraph Matching.
Due to the method?s fundamental dependency
on the syntactic dependency parse of the text, in
this work we set out to explore the impact of
substituting the previously employed dependency
parsers with a different parser which has been
demonstrated to achieve higher performance than
other commonly used parsers for full-text biomed-
ical literature (Verspoor et al, 2012).
In addition, we aimed to address the relatively
lower recall of the method through incorporation
of large quantities of external training data, ac-
quired through integration of previously automat-
ically extracted bio-molecular events available in
a web repository of such extracted events, EVEX
(Van Landeghem et al, 2011; Van Landeghem
et al, 2012), and additional bio-molecular events
generated from a large sample of full text pub-
lications using one of the state-of-the-art event
extraction systems, TEES (Bjo?rne and Salakoski,
2011). Since the performance of the subgraph
matching method, as an instance-based learning
strategy (Alpaydin, 2004), is dependent on having
good training examples that express the events in a
range of syntactic structures, the motivation under-
lying this was to increase the amount of training
data available to the system, even if that data was
derived from a less-than-perfect source. The aug-
mentation of training corpora with external unla-
belled data that is automatically processed to gen-
erate additional labels has been explored for re-
training the same system, in an approach known as
self-training. This approach has been shown to be
35
very effective for improving parsing performance
(McClosky et al, 2006; McClosky and Charniak,
2008). Self-training of the TEES system has been
previously explored (Bjorne et al, 2012), with
somewhat mixed results, but with evidence sug-
gesting it could be useful with an appropriate strat-
egy for selecting training examples. Here, rather
than training our system with its own output over
external data, we explore a semi-supervised learn-
ing approach in which we train our system with the
outputs of a different system (TEES) over external
data.
2 Methodology
2.1 Base Event Extraction System
The event extraction algorithm is essentially the
same as the one used in Liu et al (2013b). A fuller
description can be found there, but we summarise
the most important aspects of it here.
2.1.1 Event Extraction with ASM
The principal method used in event extraction is
Approximate Subgraph Matching, or ASM (Liu et
al., 2013a). Broadly, we learn subgraph patterns
from the event structures in the training data, and
then apply them by looking for matches with the
patterns of the learned rules, using ASM to allow
for non-exact matches of the patterns.
The first stage in this is learning the rules which
link subgraphs to associated patterns. The input
is a set of dependency-parsed articles (the setup
is described in ?2.1.2), and a set of gold-standard
annotations of proteins and events in the shared
task format. Using the standoff annotations in the
training data, every protein and trigger is mapped
to one or more nodes in the corresponding depen-
dency graphs. In addition, the textual content of
every protein is replaced with a generic string en-
abling abstraction over individual protein names.
Then, for each event annotation in the training
data, we retrieve the nodes from the graph corre-
sponding to the associated trigger and protein en-
tities. We determine the shortest path (or paths, in
case of a tie) connecting the graph trigger to each
of the event argument nodes. For arguments which
are themselves events (e.g., for regulatory events),
the node corresponding to the trigger of the event
argument is used instead of a protein node. Where
there are multiple arguments, we take the union of
the shortest paths to each individual argument.
This path is then used as the pattern compo-
nent of an event rule. The rule also consists of an
event type, and a mapping from event arguments
to nodes from the pattern graph, or to an event
type/node pair for nested event arguments. Af-
ter processing all training documents, we get on
the order of a few thousand rules; this can be de-
creased slightly by removing rules with subgraphs
that are isomorphic to those of other rules.
In principle, this set of rules could then be di-
rectly applied to the test documents, by searching
for any matching subgraphs. However, in practice
doing so leads to very low recall, since the pat-
terns are not general enough to get a broad range of
matches on new data. We can alleviate this by re-
laxing the strictness of the subgraph matching pro-
cess. Most basically, we relax node matching. In-
stead of requiring an exact match between both the
token and the part-of-speech of the nodes of the
sentence graph and those from the rule subgraph,
we also allow a match on the basis of the lemma
(according to BioLemmatizer (Liu et al, 2012)),
and a coarse-grained POS-tag (where there is only
one POS-tag for nouns, verbs and adjectives).
More importantly, we also relax the require-
ments on how closely the graphs must match, by
using ASM. ASM defines distances measures be-
tween subgraphs, based on structure, edge labels
and edge directions, and uses a set of specified
weights to combine them into an overall subgraph
distance. We have a pre-configured set of distance
thresholds for each event type, and for each sen-
tence/rule pairing, we extract events for any rules
with subgraphs under the given threshold.
The problem with this approximate matching is
that some rules now match too broadly, and pre-
cision is reduced. This is mitigated by adding
an iterative optimisation phase. In each iteration,
we run the event extraction using the current rule
set over some dataset ? usually the training set,
or a subset of it. We check the contribution of
each rule in terms of postulated events and actual
events which match the gold standard. If the ra-
tio of matched to postulated events is too low (for
the work reported here, the threshold is 0.25), the
rule is discarded. This process is repeated until no
more rules are discarded. This can take multiple
iterations since the rules are interdependent due to
the presence of nested event arguments.
The optimisation step is by far the most time-
consuming step of our process, especially for the
large rule sets produced in some configurations.
36
We were able to improve optimisation times some-
what by parallelising the event extraction, and
temporarily removing documents with long ex-
traction times from the optimisation process un-
til as late as possible, but it remained the primary
bottleneck in our experimentation.
2.1.2 Parsing Pipeline
In our parsing pipeline, we first split sentences
using the JULIE Sentence Boundary Detector, or
JSBD (Tomanek et al, 2007). We then parse
using a version of clearnlp1 (Choi and McCal-
lum, 2013), a successor to ClearParser (Choi and
Palmer, 2011), which was shown to have state-
of-the-art performance over the CRAFT corpus
of full-text biomedical articles (Verspoor et al,
2012). We use dependency and POS-tagging mod-
els trained on the CRAFT corpus (except where
noted); these pre-trained models are provided with
clearnlp. Our fork of clearnlp integrates to-
ken span marking into the parsing process, so the
dependency nodes can easily be matched to the
standoff annotations provided with the shared task
data. This pipeline is not dependent on any pre-
annotated data, so can thus be trivially applied to
extra data not provided as part of the shared task.
In addition the parsing is fast, requiring roughly 46
wall-clock seconds (processing serially) to parse
the 5059 sentences from the training and develop-
ment sets of the 2013 GE task ? an average of 9 ms
per sentence. The ability to apply the same pars-
ing configuration to new text was useful for adding
extra training data, as discussed in ?2.2.
The usage of clearnlp as the parser is the pri-
mary point of difference between our system and
that of Liu et al (2013b), who use the Charniak-
Johnson parser with the McClosky biomedical
model (CJM; McClosky and Charniak (2008)), al-
though there are other minor differences in tokeni-
sation and sentence splitting. We expected that the
higher accuracy of clearnlp over biomedical text
would translate into increased accuracy of event
detection in the shared task; we consider this ques-
tion in some detail below.
2.2 Adding Noisy Training Data
One of the limitations of the ASM approach is that
the high precision comes at the cost of lower re-
call. Our hypothesis is that adding extra training
instances, even if some are errors, will raise re-
call and improve overall performance. We utilised
1https://code.google.com/p/clearnlp/
two sources of automatically-annotated data: the
EVEX database, and running an automatic event
annotator over documents from PubMed Central
(PMC) and MEDLINE.
To test our hypothesis, we utilise one of the
best performing automatic event extractors in pre-
vious BioNLP tasks: TEES (Turku Event Extrac-
tion System)2 (Bjo?rne et al, 2011). We expand our
pool of training examples by adding the highest-
confidence events TEES identifies in unlabelled
text. We explored different approaches to ranking
events based on classifier confidence empirically.
TEES relies on multi-class SVMs both for trig-
ger and event classification, and produces confi-
dence scores for each prediction. We explored
ranking events according to: (i) score of the trig-
ger prediction, (ii) score of the event-type predic-
tion, and (iii) sum of trigger and event type predic-
tions. We also compared the performance when
selecting the top-k events overall, versus choos-
ing the top-k events for each event type. We also
tested adding as many instances per event-type as
there were in the manually-annotated dataset, with
different multiplying factors. Finally, we evalu-
ated the effect of using different splits of the data
for the evaluation and optimisation steps of ASM.
This is the full list of parameters that we tested
over held-out data:
? Original confidence scores: we ranked events
according to the three SVM scores mentioned
above: trigger prediction, event-type predic-
tion, and combined.
? Overall top-k: we selected the top 1,000,
5,000, 10,000, 20,000, 30,000, 40,000, and
50,000 for the different experimental runs.
? Top-k per type: for each event type, we se-
lected the top 400, 1,000, and 2,000.
? Training bias per type: we add as many in-
stances from EVEX per type as there are in
the manually annotated data. We experiment
with adding up to 6 times as many as in man-
ually annotated data.
? Training/optimisation split: we combine
manually and automatically annotated data
for training. For optimisation we tested
different options: manually annotated only,
manual + automatic, manual + top-100
events, and manual + top-1000 events.
2http://jbjorne.github.com/TEES/
37
We did not explore all these settings exhaus-
tively due to time constraints, and we report here
the most promising settings. It is worth mention-
ing that most of the configurations contributed to
improve the baseline performance. We only ob-
served drops when using automatically-annotated
data in the optimisation step.
2.2.1 Data from EVEX
Conveniently, the developers of TEES have re-
leased the output of their tool over the full 2009
collection of MEDLINE, consisting of abstracts of
biomedical articles, in a collection known as the
EVEX dataset. We used the full EVEX dataset as
provided by the University of Turku, and explored
different ways of ranking the full list of events as
described above.
2.2.2 Data from TEES
To augment the training data, we annotated two
data sets with TEES based on MEDLINE and
PubMed Central (PMC). The developers of TEES
released a trained model for the GE 2013 training
data that we utilised.
Due to the long pre-processing time of TEES,
which includes gene named entity recognition,
part-of-speech tagging and parsing, we used the
EVEX pre-processed MEDLINE, which required
some adaptation of the EVEX XML to the XML
format accepted by TEES. Once this adaptation
was finished, the files were processed by TEES.
Then, we have selected articles from PMC us-
ing a query containing specific MeSH headings
related to the GE task and limiting the result to
only the Open Access part of PMC. From the al-
most 600k articles from the PMC Open Access set,
we reduced the total number of articles to around
155k. The PMC query is the following:
(Genetic Phenomena[MH] OR Metabolic
Phenomena[MH] OR Cell Physiological
Phenomena[MH] OR Biochemical
Processes[MH]) AND open access[filter]
Furthermore, the articles were split into sections
and specific sections from the full text like Intro-
duction, Background and Methods were removed
to reduce the quantity of text to be annotated by
TEES. The PMC files produced by this filtering
were processed by TEES on the NICTA cluster.
2.3 Modification Detection
To evaluate the utility of ASM for a diverse range
of tasks, we also applied it to the task of detect-
ing modification (SPECULATION or NEGATION)
NEGATION cues
? Basic: not, no, never, nor, only, neither, fail, cease,
stop, terminate, end, lacking, missing, absent, absence,
failure, negative, unlikely, without, lack, unable
? Data-derived: any, prevention, prevent, disrupt, dis-
ruption
SPECULATION cues:
? Basic: analysis, whether, may, should, can, could, un-
certain, questionable, possible, likely, probable, prob-
ably, possibly, conceivable, conceivably, perhaps, ad-
dress, analyze, analyse, assess, ask, compare, consider,
enquire, evaluate, examine, experiment, explore, inves-
tigate, test, research, study, speculate
? Data-derived: measure, measurement, suggest, sug-
gestion, value, quantify, quantification, determine, de-
termination, detect, detection, calculate, calculation
Table 1: Modification cues
of events. In event detection, triggers are explic-
itly annotated, so the linguistic cue which indi-
cates that an event is occurring is easy to identify.
As described in Section 3.2, these triggers are im-
portant for learning event patterns.
The event extraction method is based on paths
between dependency graph nodes, so it is neces-
sary to have at least two relevant graph nodes be-
fore we can determine a path between them. For
learning modification rules, one graph node is the
trigger of the event which is subjec to modifica-
tion. However here we needed a method to deter-
mine another node in the sentence which provided
evidence that NEGATION or SPECULATION was
occurring, and could thus form an endpoint for a
semantically relevant graph pattern. To achieve
this, we specified a set cue lemmas for NEGATION
and SPECULATION. The basic set of cue lemmas
came from a variety of sources. Some were man-
ually specified and some were derived from previ-
ous work on modification detection (Cohen et al,
2011; MacKinlay et al, 2012). We manually ex-
panded this cue list to include obvious derivational
variants. This gave us a basic set of 34 SPECULA-
TION and 21 NEGATION cues.
We also used a data-driven strategy to find ad-
ditional lemmas indicative of modification. We
adapted the method of Rayson and Garside (2000)
which uses log-likelihood for finding words that
characterise differences between corpora. Here,
the ?corpora? are sentences attached to all events
in the training set, and sentences attached to events
which are subject to NEGATION or SPECULATION
(treated separately). We build a frequency distri-
bution over lemmas in each set of sentences, and
calculate the log-likelihood for all lemmas, us-
38
ing the observed frequency from the modification
events and the expected frequency over all events.
Sorting by decreasing log-likelihood, we get a
list of lemmas which are most strongly associated
with NEGATION or SPECULATION. We manually
examined the highest-ranked lemmas from these
two lists and noted lemmas which may occur,
according to human judgment, in phrases which
would denote the relevant modification type. We
found seven extra SPECULATION cues and three
extra NEGATION cues. Expanding with morpho-
logical variants as described above yielded 47
SPECULATION cues and 26 NEGATION cues to-
tal. These cues are shown, divided into basic and
data-derived, in Table 1.
For every node N with a lemma in the appro-
priate set of cue lemmas, we create a rule based
on the shortest path between the cue lemma node
N and the event trigger node. The trigger lem-
mas are replaced with generic lemmas which only
reflect the POS-tag of the trigger, to broaden the
range of possible matches. Each rule thus consists
of the POS-tag of an event trigger, and a subgraph
pattern including the abstracted event trigger node.
At modification detection time, the rules are ap-
plied in a similar way to the event rules. After
detecting events, we look for matches of each ex-
tracted event with every modification rule. A rule
R is considered to match if the event trigger node
POS tag matches the POS tag of the rule, and the
subgraph pattern of the rule matches the graph of
the sentence, including a node corresponding to
the event trigger node. If R is found to match
for a given event and sentence, any events which
have the trigger defined in the rule are marked as
SPECULATION or NEGATION as appropriate. As
in event extraction, we use ASM to allow a looser
match between graphs, but initial experimentation
showed that increasing the match thresholds be-
yond a relatively small distance was detrimental.
We have not yet added an optimisation phase for
modification, which might allow larger ASM dis-
tance threshold to have more benefit.
3 Results
We present our results over development data,
and the official test. We report the Approximate
Span/Approximate Recursive metric in all our ta-
bles, for easy comparison of scores. We describe
the data split used for development, explain our
event extraction results, and finally describe our
performance in modification detection.
3.1 Data division for development
In the data provided by the task organisers, the
split of data between training and development
sets, with 249 and 222 article sections respec-
tively, was fairly even. If we had used such a split,
we would have had an unfeasibly small amount
of data to train from during development, and
possible unexpected effects when we sharply in-
creased the amount of training data for running
over the held-out test set. We instead used our
own data set split during development, pooling
the provided training and development sets, and
randomly selecting six PMC articles (PMC IDs
2626671, 2674207, 3062687, 3148254, 3333881
and 3359311) for the development set, with the
remainder available for training. We respected ar-
ticle boundaries in the new split to avoid training
and testing on sentences taken from different sec-
tions of the same article. Results over the devel-
opment set reported in this section are over this
data split. We will refer to our training subset as
GE13tr, and to the testing subset as GE13dev.
For our runs over the official test of this chal-
lenge, we merged all the manually annotated data
from 2013 to be used as training. We also per-
formed some experiments with adding the exam-
ples from the 2011 GE task to our training data.
3.2 Event Extraction
For our first experiment, we evaluated the contri-
bution of the automatically annotated data over us-
ing GE13tr data only. We performed a set of ex-
periments to explore the parameters described in
Section 2.2 over two sources of extra examples:
EVEX and TEES.
Using EVEX data in training resulted in clear
improvements in performance when only manu-
ally annotated data was consulted for optimisa-
tion. The increase was mainly due to the better
recall, with small variations in precision over the
baseline for the majority of experiments. Our best
run over the GE13dev data followed this setting:
rank events according to trigger scores, include all
top-30000 events (without considering the types of
the events), and use only manually annotated data
for the optimisation step. Other settings also per-
formed well, as we will see below.
For TEES, we selected noisy examples from
MEDLINE and PMC to be used as additional
39
System Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+TEES 59.27 29.89 39.74
+TEES +EVEX (top5k) 46.93 30.78 37.18
+TEES +EVEX (top20k) 56.32 31.90 40.73
+TEES +EVEX (top30k) 55.34 32.48 40.93
+TEES +EVEX (pt1k) 58.54 30.96 40.50
+TEES +EVEX (trx4) 57.83 31.23 40.56
Table 2: Impact of adding extra training data to the
ASM method. top5k,20k,30k: using the top 5,000,
20,000, and 30,000 events. pt1k: using the top
1,000 events per event-type. trx4: following the
training bias of events, with a multiplying factor
of four. For TEES we always use the top 10,000
events. Evaluated over GE13dev.
training data. Initial results showed that when us-
ing only MEDLINE annotated data in the train-
ing step, the performance decreased compared to
not using any additional data. This might have
been due to differences between the EVEX pre-
processed data that we used and what TEES was
expecting, so the MEDLINE set was not consid-
ered for further experimentation. Using PMC ar-
ticles annotated with TEES in the training step se-
lected by the evidence score of TEES shows an in-
crease of recall while slightly decreasing the pre-
cision, which was expected. We selected the top
10000 events from the PMC set based on the evi-
dence score as additional training data.
Table 2 summarises the results of combin-
ing different settings of EVEX with TEES. We
achieve a considerable boost in recall, at the cost
of precision for most configurations. The only set-
ting where there is a slight drop in F-score is the
experiment with only 5000 events from EVEX; in
the remaining runs we are able to alleviate the drop
in precision, and improve the F-score. Consider-
ing the addition of top-events according to their
type, the increment in recall is slightly lower, but
these runs are able to reach similar F-score to the
best ones, using less training data. Results with
TEES might be slightly overoptimistic since the
PMC annotation is based on a TEES model trained
on the 2013 GE data and our configurations are
evaluated on a subset of this data.
For our next experiment, we tested the contribu-
tion of adding the dataset from the 2011 GE task
to the training dataset. We use this data both in
the training and optimisation steps. The results are
Train Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
Table 3: Adding GE11 data to the training and op-
timisation steps. Evaluated over GE13dev.
Parser Train Prec. Rec. F-sc.
clearnlp
GE13 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
CJM
GE13 60.96 33.11 42.91
+GE11 64.11 38.93 48.44
Table 4: Performance depending on the applied
parsing pipeline (clearnlp for this work against
the CJM pipeline of Liu et al (2013b)) over
GE13dev. For each run, the available data was
used both in training and optimisation.
given in Table 3, where we can observe a boost in
recall at the cost of precision. Overall, the im-
proved F-score suggests that this dataset would
make a useful contribution to the system.
We also compared our system to that of Liu
et al (2013b), where the primary difference
(although not the only difference, as noted in
?2.1.2) is the use of clearnlp instead of the CJM
(Charniak-Johnson/McClosky) pipeline. It is thus
somewhat surprising to see in Table 4 that the
CJM pipeline outperforms our clearnlp pipeline
by 5.5?8% in F-score, depending on the train-
ing data. For the smaller GE13-only training set,
the gap is smaller, and the precision figures are
in fact comparable. However, the recall is uni-
formly lower, suggesting that the rules learned
from clearnlp parses are for some reason less gen-
erally applicable. Another interesting difference
is that our clearnlp pipeline gets a smaller benefit
from the addition of the GE11 training data. We
consider possible reasons for this in ?4.1.
Table 5 contains the evaluation of different ex-
periments on the official test data. We tested the
baseline system using the training and develop-
ment data from 2011 and 2013 GE tasks and the
addition of TEES and EVEX data. The additional
data improves the recall slightly compared to not
using it, while, as expected, it decreases the pre-
cision. Table 5 also shows the results for our of-
ficial submission (+TEES+EVEX sub), which due
to time constraints was a combination of the opti-
mised rules of different data splits and has a lower
40
Train Prec. Rec. F-sc.
GE11, GE13 65.71 32.57 43.55
+TEES+EVEX 63.67 33.50 43.91
+TEES+EVEX * 50.68 36.99 42.77
Table 5: Test set results, always optimised over
gold data only. * denotes the official submission.
performance compared to the other results.
3.3 Modification Detection
We show results for selected modification detec-
tion experiments in Table 6. In all cases we used
all of the available gold training data from the
GE11 and GE13 datasets. To assess the impact of
modification cues, we show results using the basic
set as well as with the addition of the data-derived
cues. It has often been noted (MacKinlay et al,
2012; Cohen et al, 2011) that modification detec-
tion accuracy is strongly dependent on the quality
of the upstream event annotation, so we provide an
oracle evaluation, using gold-standard event anno-
tations rather than automatic output.
The performance over the automatically-
annotated runs is respectable, given that the recall
is fundamentally limited by the recall of the input
event annotations, which is only around 30% for
the configurations shown. With the oracle event
annotations, the results improve substantially,
with considerable gains in precision, and recall
increasing by a factor of 4?6. This boost in recall
in particular is more than we would naively expect
from the roughly threefold increase in recall over
the events. It seems that many of the modification
rules we learned were even more effective over
events which our pipeline was unable to detect.
The modification rules were learned from oracle
event data, but this does not fully explain the
discrepancy. Regardless, our algorithm for mod-
ification detection showed excellent performance
over the oracle annotations. Over the 2009 version
of the BioNLP shared task data, MacKinlay et al
(2012) report F-scores of 54.6% for NEGATION
and 51.7% for SPECULATION. These are not
directly comparable with those in Table 6, but
running our newer algorithm over the same 2009
data gives F-scores of 84.2% for NEGATION and
69.1% for SPECULATION.
For the official run, which conflates event
extraction and modification detection accuracy,
our system was ranked third for NEGATION and
SPECULATION out of the three competing teams,
although the other teams had event extraction F-
scores of roughly 8% higher than our system. For
SPECULATION, our system had the highest preci-
sion of 34.15%, while the F-score of 20.22% was
close to the best result of 23.92%. Our NEGA-
TION detection was less competitive, with an F-
score of 20.94% ? roughly 6% lower than the other
teams. We cannot extrapolate directly from the or-
acle evaluation in Table 6, but it seems to indicate
that an increase in event extraction accuracy would
have flow-on benefits in modification detection.
4 Discussion
4.1 Detrimental Effects of Parser Choice
The biggest surprise here was that clearnlp, a
more accurate dependency parser for the biomed-
ical domain, as evaluated on the CRAFT tree-
bank, gave a substantially lower event extrac-
tion F-score than the CJM parser. To determine
whether preprocessing caused the differences, we
replaced the existing modules (sentence-splitting
from JSBD and tokenisation/POS-tagging from
clearnlp) with the BioC-derived versions from the
CJM pipeline, but this yielded only an insignifi-
cant decrease in accuracy.
Over the same training data, the optimised rules
from CJM have an average of 2.6 nodes per sub-
graph path, compared to 3.9 nodes per path using
clearnlp. A longer path is less likely to match
than a shorter path, so this may help to explain
the lower generalisability of the clearnlp-derived
rules. While it is possible for a longer subgraph
to match just as generally, if the test sentences
are parsed consistently, in general there are more
nodes and edges which can fail to match due to mi-
nor surface variations. One way to mitigate this is
to raise the ASM distance thresholds to compen-
sate for this; preliminary experiments suggest it
would provide a small (? 1%) boost in F-score but
this would not close the gap between the parsers.
Both parsers produce outputs with Stanford
Dependency labels (de Marneffe and Manning,
2008), so we might naively expect similar graph
topology and subgraph pattern lengths. However,
the CJM pipeline produces graphs in the ?CCpro-
cessed? SD format, which are simpler and denser.
If a node N has a link to a node O with a conjunc-
tion link to another node P (from e.g. and), an ex-
tra link with the same label is added directly from
N to P in the CCprocessed format. This means
41
NEGATION SPECULATION
Eval Events (F-sc) Cues P / R / F P / R / F
Dev
GE13+TEES+EVEX (40.93) Basic 32.69 / 13.71 / 19.32 37.04 / 14.49 / 20.83
GE13+TEES+EVEX (40.93) B + Data 32.69 / 12.88 / 18.48 39.71 / 17.20 / 24.00
Oracle (100.0) B + Data 82.48 / 71.07 / 76.35 78.79 / 67.71 / 72.83
Test
GE11+GE13 (43.55) B + Data 39.53 / 13.99 / 20.66 50.00 / 13.85 / 21.69
GE11+GE13+TEES+EVEX * (42.77) B + Data 32.76 / 15.38 / 20.94 34.15 / 14.36 / 20.22
Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing the
F-score of the configuration), as well as using oracle event annotations from the gold standard, over our
development set and the official test set. Rules are learned from GE13+GE11 gold data (excluding any
test data). Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or the
augmented set with data-driven additions (47 SPEC/26 NEG). * denotes the official submission.
there are more direct links in the graph, match-
ing the semantics more closely. The shortest path
fromN to P is now direct, instead of viaO, which
could enable the CJM pipeline to produce more
general rules.
To evaluate how much this detrimentally af-
fects the clearnlp pipeline, as a post hoc in-
vestigation, we implemented a conversion mod-
ule. Using Stanford Dependency parser code,
we replicated the CCprocessed conversion on the
clearnlp graphs, reducing the average subgraph
pattern length to 2.8, and slightly improving ac-
curacy. Over our development set, compared to
the results in Table 3 it gave a 0.7% absolute F-
score boost over using GE13 training-data only,
and 1.1% over using GE11 and GE13 training data
(in both cases improving recall). Over the test
set, the improvement was greater, with a P/R/F
of 35.66/64.99/46.05, a 2.5% increase in F-score
compared to the results in Table 5 and only 2.9%
less than the official Liu et al (2012) submission.
Clearly some of the inter-parser discrepancies
are due to surface features and post-processing,
and as noted above, we can also achieve small im-
provements by relaxing ASM thresholds, so some
problems may be caused by the default parameters
being suboptimal for the parser. However, the ac-
curacy is still lower where we would expect it to
be higher, and this remaining discrepancy is diffi-
cult to explain without performing a detailed error
analysis, which we leave for future work.
4.2 Effect of additional data
Our initial intuition that using additional noisy
training data during the training of the system
would improve the performance is supported by
the results in Table 2. Table 3 shows that us-
ing a larger set of manually annotated data based
on 2011 GE task data also improves performance.
However, these tables also indicate that adding
manually annotated data produces an increase in
performance comparable to adding the noisy data,
despite its smaller size, and when using this man-
ually annotated set together with the noisy data,
the improvement resulting from the noisy data is
smaller (Table 5). Noisy data was only used dur-
ing training, which limits its effectiveness?any
rule extracted from automatically acquired anno-
tations that are not seen during optimisation of the
rule set will have a lower weight. On the other
hand, we found that using noisy data for optimi-
sation seemed to decrease performance. Together,
these results suggest that studying strategies, pos-
sibly self-training, for selection of events from the
noisy data to be used during rule set optimisation
in the ASM method are warranted.
5 Conclusion
Using additional training data, whether manually
annotated or noisy, improves the performance of
our baseline event extraction system. The gains
that we achieved by adding training data, however,
were outweighed by a loss of performance due to
our parser substitution, with longer dependency
subgraphs limiting rule generalisability the most
likely explanation. Our experiments demonstrate
that while a given parser might be ?better? in one
evaluation context, that advantage may not trans-
late to improved performance in a downstream
task that depends strongly on the parser output.
We presented an extension of the subgraph match-
ing methodology to extract modification events
which, when based on a good core event extrac-
tion system, shows very promising results.
42
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program. This research was
supported in part by the Intramural Research Pro-
gram of the NIH, NLM.
References
Ethem Alpaydin. 2004. Introduction to Machine
Learning. MIT Press.
Jari Bjo?rne and T. Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Ex-
tracting contextualized complex biological events
with rich graph-based features sets. Computational
Intelligence, 27(4):541?557.
Jari Bjorne, Filip Ginter, and Tapio Salakoski. 2012.
University of turku in the bionlp?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 687?692, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
K.B. Cohen, K. Verspoor, H.L. Johnson, C. Roeder,
P.V. Ogren, W.A. Baumgartner, E. White, H. Tip-
ney, and L. Hunter. 2011. High-precision biological
event extraction: Effects of system and data. Com-
putational Intelligence, 27(4):681701, November.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of bionlp?09 shared task on
event extraction. Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 1?9.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The genia event and protein coreference tasks of
the bionlp shared task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
H. Liu, R. Komandur, and K. Verspoor. 2011. From
graphs to events: A subgraph matching approach for
information eextraction from biomedical text. ACL
HLT 2011, page 164.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLoS ONE, 8(4):e60954, 04.
Haibin Liu, Karin Verspoor, Don Comeau, Andrew
MacKinlay, and W. John Wilbur. 2013b. General-
izing an approximate subgraph matching-based sys-
tem to extract events in molecular biology and can-
cer genetics. In Proceedings of the 2013 BioNLP
Workshop Companion Volume for the Shared Task.
Andrew MacKinlay, David Martinez, and Timo-
thy Baldwin. 2012. Detecting modification of
biomedical events using a deep parsing approach.
BMC Medical Informatics and Decision Making,
12(Suppl 1):S4.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the Association for Computational Linguistics (ACL
2008, short papers).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
conference of the North American chapter of the
ACL, pages 152?159.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In The Workshop
on Comparing Corpora, pages 1?6, Hong Kong,
China, October. Association for Computational Lin-
guistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on con-
ditional random fields. In Proceedings of the 10th
Conference of the Pacific Association for Compu-
tational Linguistics, pages 49?57, Melbourne, Aus-
tralia.
S. Van Landeghem, F. Ginter, Y. Van de Peer, and
T. Salakoski. 2011. EVEX: A pubmed-scale re-
source for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
43
S. Van Landeghem, K. Hakala, S. Ro?nnqvist,
T. Salakoski, Y. Van de Peer, and F. Ginter. 2012.
Exploring biomolecular literature with EVEX: Con-
necting genes through events, homology and indirect
associations. Advances in Bioinformatics, Special
issue Literature-Mining Solutions for Life Science
Research:ID 582765.
Karin Verspoor, K. Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer, ,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natural
language processing tools. BMC Bioinformatics.
44
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 76?85,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Generalizing an Approximate Subgraph Matching-based System to Extract
Events in Molecular Biology and Cancer Genetics
Haibin Liu
haibin.liu@nih.gov
NCBI, Bethesda, MD, USA
Karin Verspoor
karin.verspoor@nicta.com.au
NICTA, Melbourne, VIC, Australia
Donald C. Comeau
comeau@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Andrew MacKinlay
andrew.mackinlay@nicta.com.au
NICTA, Melbourne, VIC, Australia
W. John Wilbur
wilbur@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Abstract
We participated in the BioNLP 2013 shared
tasks, addressing the GENIA (GE) and the Can-
cer Genetics (CG) event extraction tasks. Our
event extraction is based on the system we re-
cently proposed for mining relations and events
involving genes or proteins in the biomedical
literature using a novel, approximate subgraph
matching-based approach. In addition to han-
dling the GE task involving 13 event types uni-
formly related to molecular biology, we gener-
alized our system to address the CG task tar-
geting a challenging set of 40 event types re-
lated to cancer biology with various arguments
involving 18 kinds of biological entities. More-
over, we attempted to integrate a distributional
similarity model into our system to extend the
graph matching scheme for more events. In ad-
dition, we evaluated the impact of using paths of
all possible lengths among event participants as
key contextual dependencies to extract potential
events as compared to using only the shortest
paths within the framework of our system.
We achieved a 46.38% F-score in the CG task
and a 48.93% F-score in the GE task, ranking
3rd and 4th respectively. The consistent perfor-
mance confirms that our system generalizes well
to various event extraction tasks and scales to
handle a large number of event and entity types.
1 Introduction
Understanding the sophisticated interactions between
various components of biological systems and conse-
quences of these biological processes on the function
and behavior of the systems provides profound im-
pacts on translational biomedical research, leading to
more rapid development of new therapeutics and vac-
cines for combating diseases. For the past five years,
the BioNLP shared task series has served as an in-
strumental platform to promote the development of
text mining methodologies and resources for the au-
tomatic extraction of semantic events involving genes
or proteins such as gene expression, binding, or reg-
ulatory events from the biomedical literature (Kim et
al., 2009; Kim et al, 2011). An event typically cap-
tures the association of multiple participants of vary-
ing numbers and with diverse semantic roles (Anani-
adou et al, 2010). Since events often serve as partic-
ipants in other events, the extraction of such nested
event structures provides an integrated, network view
of these biological processes.
Previous shared tasks focused exclusively on
events at the molecular and sub-cellular level. How-
ever, biological processes at higher levels of organi-
zation are equally important, such as cell prolifer-
ation, organ growth and blood vessel development.
While preserving the classic event extraction tasks
such as the GE task, the BioNLP-ST 2013 broad-
ens the scope of application domains by introducing
many new issues in biology such as cancer genetics
and pathway curation. On behalf of NCBI (National
Center for Biotechnology Information), our team par-
ticipated in the GENIA (GE) task and the Cancer Ge-
netics (CG) task. Compared to the GE task that aims
for 13 types of events concerning the protein NF-?B,
the CG task targets a challenging set of 40 types of
biological processes related to the development and
progression of cancer involving 18 entity types. This
additionally requires that event extraction systems be
able to associate entities and events at the molecular
level with anatomy level effects and organism level
outcomes of cancer biology.
Our event extraction is based on the system we re-
cently proposed for mining relations and events in-
volving genes or proteins in the biomedical litera-
ture using a novel, Approximate Subgraph Matching-
based (ASM) approach (Liu et al, 2013a). When
evaluated on the GE task of the BioNLP-ST 2011, its
performance is comparable to the top systems in ex-
tracting 9 types of biological events. In the BioNLP-
76
ST 2013, we generalized our system to investigate
both CG and GE tasks. Moreover, we attempted to in-
tegrate a distributional similarity model into the sys-
tem to extend the graph matching scheme for more
events. The graph representation that considers paths
of all possible lengths (all-paths) between any two
nodes has been encoded in graph kernels used in
conjunction with Support Vector Machines (SVM),
and led to state-of-the-art performance in extracting
protein-protein (Airola et al, 2008) and drug-drug in-
teractions (Zhang et al, 2012). Borrowing from the
idea of the all-paths representation, in addition, we
evaluated the impact of using all-paths among event
participants as key contextual dependencies to extract
potential events as compared to using only the short-
est paths within the framework of our system.
The rest of the paper is organized as follows: In
Section 2, we briefly introduce our ASM-based event
extraction system. Section 3 describes our experi-
ments aiming to extend our system. Section 4 elab-
orates some implementation details and Section 5
presents our results and discussion. Finally, Section
6 summarizes the paper and introduces future work.
2 ASM-based Event Extraction
The underlying assumption of our event extraction
approach is that the contextual dependencies of each
stated biological event represent a typical context for
such events in the biomedical literature. Our ap-
proach falls into the machine learning category of
instance-based reasoning (Alpaydin, 2004). Specif-
ically, the key contextual structures are learned from
each labeled positive instance in a set of train-
ing data and maintained as event rules in the form
of subgraphs. Extraction of events is performed
by searching for an approximate subgraph isomor-
phism between key dependencies and input sen-
tence graphs using an approximate subgraph match-
ing (ASM) algorithm designed for literature-based
relational knowledge extraction (Liu et al, 2013a).
By introducing error tolerance into the graph match-
ing process, our approach is capable of retrieving
events encoded within complex dependency contexts
while maintaining the extraction precision at a high
level. The ASM algorithm has been released as open
source software1. See (Liu et al, 2013a) for more de-
tails on the ASM algorithm, its complexity and the
comparison with existing graph distance metrics.
Figure 1 illustrates the overall architecture of our
ASM-based system with three core components high-
1http://asmalgorithm.sourceforge.net
lighted: rule induction, sentence matching and rule
set optimization. Our approach focuses on extract-
ing events expressed within the boundaries of a single
sentence. It is also assumed that entities involved in
the target event have been annotated. Next, we briefly
describe the core components of the system.
Rule Induction
Preprocessing
Sentence Matching
Postprocessing
Training data Testing data
Rule Set
Optimization
Figure 1: ASM-based Event Extraction Framework
2.1 Rule Induction
Event rules are learned automatically using the fol-
lowing method. Starting with the dependency graph
of each training sentence, for each annotated event,
the shortest dependency path connecting the event
trigger to each event argument in the undirected ver-
sion of the graph is selected. While additional in-
formation such as individual words in each sentence
(bag-of-words), sequences of words (n-grams) and
semantic concepts is typically used in the state-of-
the-art supervised learning-based systems to cover a
broader context (Airola et al, 2008; Buyko et al,
2009; Bjo?rne et al, 2012), the shortest path be-
tween two tokens in the dependency graph is par-
ticularly likely to carry the most valuable informa-
tion about their mutual relationship (Bunescu and
Mooney, 2005a; Thomas et al, 2011b; Rinaldi et
al., 2010). In case there exists more than one short-
est path, all of them are considered. For multi-token
event triggers, the shortest path connecting every trig-
ger token to each event argument is extracted, and the
union of the paths is then computed for each trigger.
For regulatory events that take a sub-event as an ar-
gument, the shortest path is extracted so as to connect
the trigger of the main event to that of the sub-event.
For complex events that involve multiple argu-
ments, we computed the dependency path union of
all shortest paths from trigger to each event argument,
resulting in a graph in which all event participants are
jointly depicted. Individual dependency paths con-
necting triggers to each argument are also considered
to determine event arguments independently. If the
77
resulting arguments share the same event trigger, they
are grouped together to form a potential event. In our
approach, the individual paths aim to retrieve more
potential events while the path unions retain the pre-
cision advantage of joint inference.
While the dependencies of such paths are used as
the graph representation of the event, a detailed de-
scription records the participants of the event, their
semantic role labels and the associated nodes in the
graph. All participating biological entities are re-
placed with a tag denoting their entity type, e.g. ?Pro-
tein? or ?Organism?, to ensure generalization of the
learned rules. As a result, each annotated event is
generalized and transformed into a generic graph-
based rule. The resulting event rules are categorized
into different target event types.
2.2 Sentence Matching
Event extraction is achieved by matching the induced
rules to each testing sentence and applying the de-
scriptions of rule tokens (e.g. role labels) to the cor-
responding sentence tokens. Since rules and sentence
parses all possess a graph representation, event recog-
nition becomes a subgraph matching problem. We
introduced a novel approximate subgraph matching
(ASM) algorithm (Liu et al, 2013a) to identify a sub-
graph isomorphic to a rule graph within the graph of
a testing sentence. The ASM problem is defined as
follows.
Definition 1. An event rule graph Gr =
(Vr, Er) is approximately isomorphic to a subgraph
Ss of a sentence graph Gs = (Vs, Es), denoted
by Gr ?=t Ss ? Gs, if there is an injective
mapping f : Vr ? Vs such that, for a given
threshold t, t ? 0, the subgraph distance be-
tween Gr and Gs satisfies 0 ? subgraphDistf (Gr,
Gs) ? t, where subgraphDistf (Gr, Gs) = ws ?
structDistf (Gr, Gs) + wl ? labelDistf (Gr, Gs) +
wd ? directionalityDistf (Gr, Gs).
The subgraph distance is proposed to be the
weighted summation of three penalty-based measures
for a candidate match between the two graphs. The
measure structDist compares the distance between
each pair of matched nodes in one graph to the
distance between corresponding nodes in the other
graph, and accumulates the structural differences.
The distance in rule graphs is defined as the length
of the shortest path between two nodes. The distance
in sentence graphs is defined as the length of the path
between corresponding nodes that leads to minimum
structural difference with the distance in rule graphs.
Because dependency graphs are edge-labeled, ori-
ented graphs, the measures labelDist and direction-
alityDist evaluate respectively the overall differences
in edge labels and directionalities on the compared
path between each pair of matched nodes in the two
graphs. The real numbers ws, wl and wd are non-
negative weights associated with the measures.
The weights ws, wl and wd are defaulted to be
equal but can be tuned to change the emphasis of the
overall distance function. The distance threshold t
controls the isomorphism quality of the retrieved sub-
graphs from sentences. A smaller t allows only lim-
ited variations and always looks for a sentence sub-
graph as closely isomorphic to the rule graph as pos-
sible. A larger t enables the extraction of events de-
scribed in complicated dependency contexts, thus in-
creasing the chance of retrieving more events. How-
ever, it can incur a bigger search cost due to the eval-
uation of more potential solutions.
An iterative, bottom-up matching process is used
to ensure the extraction of complex and nested events.
Starting with the extraction of simple events, simple
event rules are first matched with a testing sentence.
Next, as potential arguments of higher level events,
obtained simple events continue to participate in the
subsequent matching process between complex event
rules and the sentence to initiate the iterative process
for detecting complex events with nested structures.
The process terminates when no new candidate event
is generated for the testing sentence.
During the matching phase we relax the event
rules that contain sub-event arguments such that any
matched event can substitute for the sub-event. We
believe that the contextual structures linking anno-
tated sub-events of a certain type are generalizable
to other event types. This relaxation increases the
chance of extracting complex events with nested
structures but still takes advantage of the contextual
constraints encoded in the rule graphs.
2.3 Rule Set Optimization
Typical of instance-based reasoners, the accuracy of
rules with which to compare an unseen sentence is
crucial to the success of our approach. For instance, a
Transcription rule encoding a noun compound mod-
ification dependency between ?TNF? and ?mRNA?
derived from an event context ?expression of TNF
mRNA? should not produce a Transcription event
for the general phrase ?level of TNF mRNA? even
though they share a matchable dependency. Such
matches result in false positive events.
78
Therefore, we measured the accuracy of each rule
ri in terms of its prediction result via Eq.(1). For rules
that produce at least one prediction, we ranked them
byAcc(ri) and excluded the ones with aAcc(ri) ratio
lower than an empirical threshold, e.g. 1:4.
Acc(ri) =
#correct predictions by ri
#total predictions by ri
(1)
Because of nested event structures, the removal
of some rules might incur a propagating effect on
rules relying on them to produce arguments for the
extraction of higher order events. Therefore, an it-
erative rule set optimization process, in which each
iteration performs sentence matching, rule ranking
and rule removal sequentially, is conducted, lead-
ing to a converged, optimized rule set. While the
ASM algorithm aims to extract more potential events,
this performance-based evaluation component en-
sures the precision of our event extraction framework.
3 Extensions to Event Extraction System
In the BioNLP-ST 2013, we attempted two different
ways to extend the current event extraction system:
(1) integrate a distributional similarity model into the
system to extend the graph matching scheme for more
events; (2) use paths of all possible lengths (all-paths)
among event participants as key contextual depen-
dencies to extract events. We next elaborate these
system extensions in detail.
3.1 Integrating Distributional Similarity Model
The proposed subgraph distance measure of the ASM
algorithm focuses on capturing differences in the
overall graph structure, edge labels and directional-
ities. However, when determining the injective node
mapping between graphs, the matching remains at the
surface word level.
In the current setting, various node features can be
considered when comparing two graph nodes, result-
ing in different matching criteria. The features in-
clude POS tags (P), event trigger (T), token lemmas
(L) and tokens themselves (A). For instance, a match-
ing criterion, ?P*+L?, requires that the relaxed POS
tags (P*) and the lemmatized form (L) of tokens be
identical for each rule node to match with a sentence
node. The relaxed POS allows the plural form of
nouns to match with the singular form, and the con-
jugations of verbs to match with each other. How-
ever, the inability to go beyond surface level match-
ing prevents node tokens that share similar meaning
but possess distinct orthography from matching with
each other. For instance, a mismatch between rule
token ?crucial? and a sentence token ?critical? could
lead to an undiscovered Positive regulation event.
We attempted to use only POS information in the
node matching scheme and observed a nearly 14%
increase in recall (Liu et al, 2013b). However, the
precision drops sharply, resulting in an undesirable
F-score. This indicates that the lexical information
is a critical supplement to the contextual dependency
constraints in accurately capturing events within the
framework of our system. Moreover, we attempted to
extend the node matching using the synsets of Word-
Net (Fellbaum, 1998) to allow tokens to match with
their synonyms (Liu et al, 2011). However, since
WordNet is developed for the general English lan-
guage, it relates biomedical terms e.g., ?expression?
with general words such as ?aspect? and ?face?, thus
leading to incorrect events.
In this work, we integrated a distributional simi-
larity model (DSM) into our node matching scheme
to further improve the generalization of event rules.
A distributional similarity model is constructed
based on the distributional hypothesis (Harris, 1954):
words that occur in the same contexts tend to share
similar meanings. We expect that the incorporation
of DSM will enable our system to capture matching
tokens in testing sentences that do not appear in the
training data while maintaining the extraction pre-
cision at a high level. There have been many ap-
proaches to compute the similarity between words
based on their distribution in a corpus (Landauer and
Dumais, 1997; Pantel and Lin, 2002). The output is a
ranked list of similar words to each word. We reim-
plemented the model proposed by (Pantel and Lin,
2002) in which each word is represented by a fea-
ture vector and each feature corresponds to a context
where the word appears. The value of the feature
is the pointwise mutual information (Manning and
Schu?tze, 1999) between the feature and the word. Let
c be a context and Fc(w) be the frequency count of a
word w occurring in context c. The pointwise mutual
information, miw,c between c and w is defined as:
miw,c =
Fc(w)
N?
i
Fi(w)
N ?
?
j
Fc(j)
N
(2)
where N =
?
i
?
j
Fi(j) is the total frequency count
of all words and their contexts.
Since mutual information is known to be biased
towards infrequent words/features, the above mutual
79
information value is multiplied by a discounting fac-
tor as described in (Pantel and Lin, 2002). The simi-
larity between two words is then computed using the
cosine coefficient (Salton and McGill, 1986) of their
mutual information vectors.
We experimented with two different approaches to
integrate the DSM into our event extraction system.
First, the model is directly embedded into the node
matching scheme. Once a match cannot be deter-
mined by surface tokens, the DSM is invoked to allow
a match if the sentence token appears in the list of the
top M most similar words to the rule token. Sec-
ond, additional event rules are generated by replac-
ing corresponding rule tokens with their top M most
similar words, rather than allow DSM to participate
in the node matching. While the first method mea-
sures the consolidated extraction ability of an event
rule by combining its DSM-generalized performance,
the second approach provides a chance to evaluate the
impact of each DSM-introduced similar word indi-
vidually on event extraction.
3.2 Adopting All-paths for Event Rules
Airola et al proposed an all-paths graph (APG) ker-
nel for extracting protein-protein interactions (PPI),
in which the kernel function counts weighted shared
dependency paths of all possible lengths (Airola et
al., 2008). Thomas et al adopted this kernel as
one of the three models used in the ensemble learn-
ing for extracting drug-drug interactions (Thomas et
al., 2011a) and won the recent DDIExtraction 2011
challenge (Segura-Bedmar et al, 2011). The JULIE
lab adapted the APG kernel to event extraction us-
ing syntactically pruned and semantically enriched
dependency graphs (Buyko et al, 2009).
The graph representation of the kernel consists of
two sub-representations: the full dependency parse
and the surface word sequence of the sentence where
a pair of interacting entities occurs. At the expense
of computational complexity, this representation en-
ables the kernel to explore broader contexts of an
interaction, thus taking advantage of the entire de-
pendency graph of the sentence. When comparing
two interaction instances, instead of using only the
shortest path that might not always provide suffi-
cient syntactic information about relations, the ker-
nel considers paths of all possible lengths between
any two nodes. More recently, a hash subgraph pair-
wise (HSP) kernel-based approach was also proposed
for drug-drug interactions and adopts the same graph
representation as the APG kernel (Zhang et al, 2012).
In contrast, the graph representation that our ASM
algorithm searches in a sentence is inherently re-
stricted to the shortest path among target entities in
event rules, as described in Section 2.2. Borrowing
from the idea of the all-path graph representation, in
this work we attempted to explore contexts beyond
the shortest paths to enrich our rule set. We evalu-
ated within the framework of our system the impact
of using acyclic paths of all possible lengths among
event participants as key contextual dependencies to
populate the event rule set as compared to using only
the shortest paths in the current system setting.
4 Implementation
4.1 Preprocessing
We employed the preprocessed data in the
BioC (Comeau et al, 2013) compliant XML format
provided by the shared task organizers as supporting
resources. The BioC project attempts to address
the interoperability among existing natural language
processing tools by providing a unified BioC XML
format. The supporting analyses include tokeniza-
tion, sentence segmentation, POS tagging and
lemmatization. Different syntactic parsers analyze
text based on different underlying methodologies, for
instances, the Stanford parser (Klein and Manning,
2003) performs joint inference over the product of an
unlexicalized Probabilistic Context-Free Grammar
(PCFG) parser and a lexicalized dependency parser
while the McClosky-Charniak-Johnson (Charniak)
parser (McClosky and Charniak, 2008) is based on
N -best parse reranking over a lexicalized PCFG
model. In order to take advantage of multiple aspects
of structural analysis of sentences, both Stanford
parser and Charniak parser, which are among the best
performing parsers trained on the GENIA Treebank
corpus, are used to parse the training sentences and
produce dependency graphs for learning event rules.
Only the Charniak parser is used on the testing
sentences in the event extraction phase.
4.2 ASM Parameter Setting
The GE task includes 13 different event types. Since
each type possesses its own event contexts, an indi-
vidual threshold te is assigned to each type. Together
with the 3 distance function weights ws, wl and wd,
the ASM requires 16 parameters for the GE event ex-
traction task. Similarly, the ASM requires 43 param-
eters to cater to the 40 diverse event types of the CG
task. As reported in (Liu et al, 2013a), we used a
genetic algorithm (GA) (Cormen et al, 2001) to au-
80
tomatically determine values of the 12 ASM param-
eters for the 2011 GE task using the training data.
We inherited these previously determined parameters
and adapted them into the 2013 tasks according to
the event type and its argument configuration. For in-
stance, ?Pathway? events in the CG task is assigned
the same te as the ?Binding? events in the GE task as
they possess similar argument configurations.
Table 1 shows the parameter setting for the 2013
GE task with the equal weights ws = wl = wd con-
straint. The graph node matching criterion ?P*+L?
that requires the relaxed POS tags and the token lem-
mas to be identical is used in the ASM.
Parameter Value Parameter Value
tGene expression 8 tUbiquitination 3
tTranscription 7 tBinding 7
tProtein catabolism 10 tRegulation 3
tPhosphorylation 8 tPositive regulation 3
tLocalization 8 tNegative regulation 3
tAcetylation 3 ws 10
tDeacetylation 3 wl 10
tProteinmodification 3 wd 10
Table 1: ASM parameter setting for the 2013 GE task
4.3 Distributional Similarity Model
In our implementation, we made following improve-
ments to the original Pantel model (Pantel and Lin,
2002): (1) lemmas of words generated by the Bi-
oLemmatizer (Liu et al, 2012) are used to achieve
generalization. The POS information is combined
with each lemmatized word to disambiguate its cat-
egory. (2) instead of the linear context where a
word occurs, we take advantage of dependency con-
texts inferred from dependency graphs. For instance,
?toxicity?amod? is extracted as a feature of the to-
ken ?nonhematopoietic JJ?. It captures the dependent
token, the type and the directionality of the depen-
dency. (3) the resulting miw,c is scaled into the [0, 1]
range by
? ?miw,c
1 + ? ?miw,c
to avoid greater miw,c values
dominating the similarity calculation between words.
An empirical ? = 0.01 is used. (4) while only the
immediate dependency contexts of a word are used
in our model, our implementation is flexible so that
contexts of various dependency depths could be taken
into consideration.
In order to cover a wide range of words and capture
the diverse usages of them in biomedical texts, in-
stead of resorting to an existing corpus, our distribu-
tional similarity model is built based on a random se-
lection of 5 million abstracts from the entire PubMed.
When computing miw,c, we filtered out contexts of
each word where the word occurs less than 5 times.
Eventually, the model contains 2.8 million distinct to-
kens and 0.4 million features. When it is queried with
an amino acid, e.g, ?lysine?, the top 15 tokens in the
resulting ranked list are all correct amino acid names.
5 Results and Discussion
This section reports our results on the GE and the CG
tasks respectively, including the attempted extensions
to our ASM-based event extraction system.
5.1 GE task
5.1.1 Datasets
The 2013 GE task dataset is composed of full-text
articles from PubMed Central, which are divided into
smaller segments by the task organizers according to
various sections of the articles. Table 2 presents some
statistics of the GE dataset.
Attributes Counted Training Development Testing
Full article segments 222 249 305
Proteins 3,571 4,138 4,359
Annotated events 2,817 3,199 3,301
Table 2: Statistics of BioNLP-ST 2013 GE dataset
As distributed, the development set is bigger than
the training set. For better system generalization, we
randomly reshuffled the data and created a 353/118
training/development division, a roughly 3:1 ratio
consistent with the settings in previous GE tasks.
The results reported on the training/development data
thereafter are based on our new data partition.
5.1.2 GE Results on Development Set
Table 3 shows the event extraction results on the 118
development documents based on event rules derived
from different parsers. Only the numbers of unique,
optimized rules are reported and those that possess
isomorphic graph representations determined by an
Exact Subgraph Matching (ESM) algorithm (Liu et
al., 2013b) are removed. The ensemble rule set com-
bines rules derived from both parsers and achieves
a better performance than that of using individual
parsers. It makes sense that the Charniak parser is
favored and leads to a performance close to the en-
semble performance because sentences from which
events are extracted are parsed by the Charniak parser
as well. However, we retained the additional rules
from the Stanford parser in the hope that they may
contribute to the testing data.
When embedding the distributional similarity
model (DSM) directly into the graph node matching
81
Parser Type Event Rule Recall Precision F-score
Charniak 2,923 47.01% 66.01% 54.91%
Stanford 3,305 43.66% 67.67% 53.08%
Ensemble 4,617 47.45% 65.65% 55.09%
Table 3: Performance of using different parsers
scheme, we performed the DSM on all rule tokens ex-
cept biological entities, meaning that for each rule to-
ken, if a match will be granted if a rule token appears
in the top M most similar word list of a sentence to-
ken, e.g., ?DSM 3? denotes the top 3 similar words
determined by the DSM. We further performed DSM
only on trigger tokens for comparison, as presented
in Table 4.
All Tokens Recall Precision F-score
DSM 1 47.98% 52.56% 50.17%
DSM 3 48.68% 35.07% 40.77%
DSM 10 53.43% 19.38% 28.44%
Trigger Tokens Recall Precision F-score
DSM 1 48.06% 54.22% 50.95%
DSM 3 48.59% 37.00% 42.01%
DSM 10 53.35% 24.65% 33.72%
Table 4: Performance of integrated DSM
Even though the DSM helps to substantially in-
crease the recall to 53.43%, we observed a significant
precision drop which leads to an inferior F-score to
the ensemble baseline in Table 3. A close evaluation
of the generated graph matches reveals that antonyms
produced by the DSM contributes to most of the false
positive events. For instance, the most similar words
for the verb ?increase? and the adjective ?high? re-
turned by the model are ?decrease? and ?low? be-
cause they tend to occur in the same contexts. Fur-
ther investigation is needed to automatically filter out
the antonyms. When generating additional rules us-
ing the top M most similar words from the DSM,
since all the rules undergo the optimization process,
the event extraction precision is ensured. However,
the recall increase from simple events is diluted by
the counter effect of the introduced false positives in
detecting regulation-related complex events, result-
ing in a comparable performance to the baseline.
Table 5 gives the performance comparison of us-
ing all-paths and the shortest paths in our event ex-
traction system. Using all-paths does not bring in a
significant improvement in F-score but takes 27 it-
erations to optimize as compared to the 5-iteration
optimization on shortest paths. Most of the rules in-
duced from all-paths are eventually discarded by the
optimization process. The all-paths graph represen-
tation was motivated by the observation that short-
est paths between candidate entities often exclude
relation-signaling words when detecting binary re-
lationships (Airola et al, 2008). Exploring broader
contexts ensures such words to be considered. In the
event extraction task, however, since triggers have
been annotated, they are naturally incorporated into
the shortest paths connecting trigger to each event ar-
gument. This in part explains why contexts beyond
shortest paths did not bring in an appreciable benefit.
All Tokens Recall Precision F-score
All-paths 48.77% 64.64% 55.59%
Shortest paths 47.45% 65.65% 55.09%
Table 5: Performance of using all-paths
5.1.3 GE Results on Testing Set
Since integrating the DSM and all-paths do not pro-
vide significant performance improvements to our
system, we decided to retain the original settings in
the ASM when extracting events from the testing
data. While most of the 2011 shared task datasets are
composed of PubMed abstracts compared to full-text
articles in the 2013 GE task, our system focuses on
extracting events expressed within the boundaries of
a single sentence. Therefore, in order to take advan-
tage of existing annotated resources, we incorporated
the annotated data of 2011 GE task and EPI (Epi-
genetics and Post-translational Modifications) task to
enrich the training instances of corresponding event
types of the 2013 GE task. Eventually, we obtained a
total of 14,448 rules of different event types from our
training data. In practice, it takes the ASM less than a
second to match the entire rule set with one document
and return results.
Our submitted system achieves a 48.93% F-score
on the 305 testing documents of the GE task, ranking
4th among 12 participating teams. Table 6 presents
the performance of the top eight systems.
System Recall Precision F-score
EVEX 45.44% 58.03% 50.97%
TEES 2.1 46.17% 56.32% 50.74%
BioSEM 42.47% 62.83% 50.68%
NCBI 40.53% 61.72% 48.93%
DlutNLP 40.81% 57.00% 47.56%
HDS4NLP 37.11% 51.19% 43.03%
NICTANLM 36.99% 50.68% 42.77%
USheff 31.69% 63.28% 42.23%
Table 6: Performance of top 8 systems in GE task
Our performance is within a reasonable mar-
gin from the best-performing system ?EVEX?, and
shows an overall superior precision over most partic-
ipating teams; only two of the top 5 systems obtained
82
a precision in the 60% range. Particularly for the
regulation-related complex events, we are the only
team that achieved a precision over 55% among all
12 participating systems. This indicates that event
rules automatically learned and optimized over train-
ing data generalize well to the unseen text, and have
the ability to identify precisely corresponding events.
We further evaluated the impact of the additonal
training instances from 2011 tasks and the ensemble
rule set derived from different parsers as presented
in Table 7. With the help from the 2011 data, our
F-score is increased by 3% and we became the only
team that detected ?Ubiquitination? events from test-
ing data. In addition, rules derived from the Stanford
parser do not provide additional benefits on the test-
ing data compared to using the Charniak parser alone.
System Attribute Recall Precision F-score
Ensemble 2013 + 2011 data 40.53% 61.72% 48.93%
Ensemble 2013 data 35.63% 63.91% 45.75%
Charniak 2013 data 35.29% 65.71% 45.92%
Table 7: Impact of 2011 data and ensemble rule set
5.2 CG task
5.2.1 Datasets
The CG task dataset is prepared based on a previ-
ously released corpus of angiogenesis domain ab-
stracts (Wang et al, 2011). It targets a challenging
set of 40 types of biological processes related to the
development and progression of cancer involving 18
entity types (Pyysalo et al, 2012). Table 8 presents
some statistics of the CG dataset.
Attributes Counted Training Development Testing
Abstracts 300 100 200
Entities 10,935 3,634 6,955
Annotated events 8,803 2,915 5,972
Table 8: Statistics of BioNLP-ST 2013 CG dataset
5.2.2 CG Results on Testing Set
We generalized our event extraction system to the CG
task and the corresponding annotated data of the 2011
tasks is also incorporated in the training phase to ob-
tain the optimized event rule set. Due to time con-
straints, the impact of integrating the DSM and all-
paths is not evaluated on the CG task. We achieved
a 46.38% F-score on the 200 testing documents of
the CG task, ranking 3rd among the 6 participating
teams. Table 9 gives the primary evaluation results of
the 6 participating teams; only ?TEES-2.1? and we
participated in both GE and CG tasks. The detailed
results of each of the targeted 40 event types is avail-
able from the official CG task website.
Team Recall Precision F-score
TEES-2.1 48.76% 64.17% 55.41%
NaCTeM 48.83% 55.82% 52.09%
NCBI 38.28% 58.84% 46.38%
RelAgent 41.73% 49.58% 45.32%
UET-NII 19.66% 62.73% 29.94%
ISI 16.44% 47.83% 24.47%
Table 9: Performance of all systems in 2013 CG task
Inconsistent with other biological entities, the en-
tity annotation for the optional ?Site? argument in-
volved in events such as ?Binding?, ?Mutation? and
?Phosphorylation? are not provided by the task orga-
nizers. We consider that detecting ?Site? entities is
related to entity detection and we would like to focus
our system on the event extraction itself. Thus, we
decided to ignore the ?Site? argument in our system.
However, a problem will arise that even though the
other arguments are correctly identified for an event,
it might still be evaluated as false positive if a ?Site?
argument is not detected. This results in both false
positive and false negative events. In addition, since
we did not perform the secondary task which requires
us to detect modifications of the predicted events, in-
cluding negation and speculation, about 7.5% anno-
tated instances in the testing data are thus missed,
causing damage to our recall in the overall evalua-
tion. The organizers have agreed to issue an additonal
evaluation that will focus on core event extraction tar-
gets excluding optional arguments such as ?Site? and
the secondary task. We will conduct more detailed
analysis on the results once they are made available.
6 Conclusion and Future Work
In the BioNLP-ST 2013, we generalized our ASM-
based system to address both GE and CG tasks.
We attempted to integrate a distributional similarity
model into our system to extend the graph match-
ing scheme. We also evaluated the impact of using
paths of all possible lengths among event participants
as key contextual dependencies to extract potential
events as compared to using only the shortest paths
within the framework of our system.
We achieved a 46.38% F-score in the CG task and
a 48.93% F-score in the GE task, ranking 3rd and
4th respectively. While the distributional similarity
model did not improve the overall performance of our
system in the tasks, we would like to further investi-
gate the antonym problem introduced by the model in
our future work.
83
Acknowledgments
This research was supported by the Intramural Re-
search Program of the NIH, NLM.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski1. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9 Suppl 11:s2.
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press.
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for systems
biology by text mining the literature. Trends in Biotech-
nology, 28(7):381?390.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012. Uni-
versity of turku in the BioNLP?11 shared task. BMC
Bioinformatics, 13 Suppl 11:S4.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, pages 724?731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Information
Processing Systems (NIPS). Vancouver, BC, December.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In BioNLP ?09: Proceedings of the
Workshop on BioNLP, pages 19?27, Morristown, NJ,
USA. Association for Computational Linguistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Verspoor,
Thomas C. Wiegers, Cathy H. Wu, and W. John Wilbur.
2013. BioC: A minimalist approach to interoperability
for biomedical text processing. submitted.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. The MIT Press.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. Bradford Books.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP Shared Task 2009 Workshop, pages
1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6. As-
sociation for Computational Linguistics, June.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 423?430. Association for Computa-
tional Linguistics.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211?240.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching ap-
proach for information extraction from biomedical text.
In Proceedings of BioNLP Shared Task 2011 Work-
shop, pages 164?172. Association for Computational
Linguistics, June.
Haibin Liu, Tom Christiansen, William A Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a lemmati-
zation tool for morphological processing of biomedical
text. Journal of Biomedical Semantics, 3:3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLOS ONE, 8:4 e60954.
Haibin Liu, Vlado Keselj, and Christian Blouin. 2013b.
Exploring a subgraph matching approach for extracting
biological events from literature. Computational Intel-
ligence.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language processing.
MIT Press, Cambridge, MA, USA.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
Association for Computational Linguistics, pages 101?
104, Columbus, Ohio. The Association for Computer
Linguistics.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?02, pages 613?619,
New York, NY, USA. ACM.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol
Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012.
Event extraction across multiple levels of biological or-
ganization. Bioinformatics, 28:i575?i581.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon
Clematide, Thrse Vachon, and Martin Romacker. 2010.
Ontogene in BioCreative II.5. IEEE/ACM Trans. Com-
put. Biology Bioinform., 7(3):472?480.
84
Gerard Salton and Michael J. McGill. 1986. Introduction
to Modern Information Retrieval. McGraw-Hill, Inc.,
New York, NY, USA.
Isabel Segura-Bedmar, Paloma Martinez, and Daniel
Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011
Challenge Task: Extraction of Drug-Drug Interactions
from Biomedical Texts. In Proceedings of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction 2011,
pages 1?9.
Philippe Thomas, Mariana Neves, Illes Solt, Domonkos
Tikk, and Ulf Leser. 2011a. Relation extraction for
drug-drug interactions using ensemble learning. In Pro-
ceedings of DDIExtraction-2011 challenge task, pages
11?18.
Philippe Thomas, Stefan Pietschmann, Ille?s Solt,
Domonkos Tikk, and Ulf Leser. 2011b. Not all
links are equal: Exploiting dependency types for the
extraction of protein-protein interactions from text. In
Proceedings of BioNLP 2011 Workshop, pages 1?9.
Association for Computational Linguistics, June.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehensive
benchmark of kernel methods to extract protein?protein
interactions from literature. PLoS Computational Biol-
ogy, 6:e1000837, July.
Xinglong Wang, Iain McKendrick, Ian Barrett, Ian Dix,
Tim French, Jun?ichi Tsujii, and Sophia Ananiadou.
2011. Automatic extraction of angiogenesis bioprocess
from text. Bioinformatics, 27(19):2730?2737.
Yijia Zhang, Hongfei Lin, Zhihao Yang, Jian Wang, and
Yanpeng Li. 2012. A single kernel-based approach to
extract drug-drug interactions from biomedical litera-
ture. PLOS ONE, 7(11): e48901.
85
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 12?22,
Dublin, Ireland, August 23rd 2014.
Integrating UIMA with Alveo, a human communication science virtual 
laboratory 
 
Dominique Estival 
 U. of Western Sydney 
d.estival@uws.edu.au  
Steve Cassidy 
Macquarie University 
steve.cassidy@mq.edu.au 
Karin Verspoor 
University of Melbourne 
karin.verspoor@unimelb.edu.au 
                           Andrew MacKinlay                            Denis Burnham 
       RMIT                                      U. of Western Sydney  
                 andrew.mackinlay@rmit.edu.au                d.burnham@uws.edu.au 
Abstract 
This paper describes two aspects of Alveo, a new virtual laboratory for human communication science 
(HCS). As a platform for HCS researchers, the integration of the Unstructured Information Management 
Architecture (UIMA) with Alveo was one of the aims during the development phase and we report on 
the choices that were made for the implementation. User acceptance testing (UAT) constituted an inte-
gral part of the development and evolution of Alveo and we present the distributed testing organisation, 
the test development process and the evolution of the tests. We conclude with some lessons learned re-
garding multi-site collaborative work on the development and deployment of HLT research infrastruc-
ture. 
 
1 Introduction 
The Alveo Virtual Laboratory provides a new platform for collaborative research in human communi-
cation science (HCS). 1 Funded by the Australian Government National eResearch Collaboration Tools 
and Resources (NeCTAR) program, it involves partners from 16 institutions in a range of disciplines: 
linguistics, natural language processing, speech science, psychology, as well as music and acoustic 
processing. The goal of the platform is to provide easy access to a variety of databases and a range of 
analysis tools, in order to foster inter-disciplinary research and facilitate the discovery of new methods 
for solving old problems or the application of known methods to new datasets (Estival, Cassidy, 
Sefton, & Burnham, 2013). The platform integrates a number of tools and enables non-technical users 
to process communication resources (including not only text and speech corpora but also music re-
cordings and videos) using these tools in a straightforward manner. In this paper, we report on the re-
cent integration of the Unstructured Information Management Architecture (UIMA) with Alveo. This 
integration is bi-directional, in that existing resources and annotations captured over those resources in 
Alveo can flow to a UIMA process, and new annotations produced by a UIMA process can be con-
sumed and persisted by Alveo. We also introduce the general approach to user acceptance testing 
(UAT) of Alveo, focussing on the organisation and process acceptance adopted to meet the acceptance 
criteria required for the project and to ensure user uptake within the research community. Finally, we 
demonstrate the application of the testing process for acceptance of the UIMA integration. 
Section 2 briefly describes Alveo and its components, in particular the tools and corpora already 
available on the platform and the workflow engine, then Section 3 describes the Alveo-UIMA integra-
tion. Section 4 describes the UAT requirement and the organisation of the testing among the Alveo 
project partners, outlines the actual testing process and gives examples of the tests, among them the 
UIMA tests, which were developed for the project. Section 5 discusses alternative strategies and we 
conclude with some lessons learned regarding multi-site collaborative work on the development and 
deployment of HLT research infrastructure. 
                                                 
1  This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0. 12
2 The Alveo Virtual Laboratory 
Alveo provides easy access to a range of databases relevant to human communication science disci-
plines, including speech, text, audio and video, some of which would previously have been difficult 
for researchers to access or even know about. The system implements a uniform and secure license 
management system for the diverse licensing and user agreement conditions required. Browsing, 
searching and dataset manipulation are also functionalities which are available in a consistent manner 
across the data collections through the web-based Discovery Interface. The first phase of the project 
(December 2012 ? June 2014) saw the inclusion of the collections shown in Table 1. 
 
1. PARADISEC (Pacific and Regional Archive for Digital Sources in Endangered Cultures): audio, video, 
text and image resources for Australian and Pacific Island languages (Thieberger, Barwick, Billington, & 
Vaughan, 2011) 
2. AusTalk, audio-visual speech corpus  of Australian English (Burnham et al., 2011) 
3. The Australian National Corpus  (S. Cassidy, Haugh, Peters, & Fallu, 2012) comprising: Australian Cor-
pus of English (ACE); Australian Radio Talkback (ART); AustLit; Braided Channels; Corpus of Oz Early 
English (COOEE); Griffith Corpus of Spoken English (GCSAusE); International Corpus of English (ICE-
AUS); Mitchell & Delbridge corpus; Monash Corpus of Spoken English (Musgrave & Haugh, 2009). 
4. AVOZES, a visual speech corpus (Goecke & Millar, 2004) 
5. UNSW Pixar Emotional Music Excerpts: Pixar movie theme music expressing different emotions 
6. Sydney University Room Impulse Responses: environmental audio samples which, through convolution 
with speech or music, can create the effect of that speech or music in that acoustic environment 
7. Macquarie University Battery of Emotional Prosody: sung sentences with different prosodic patterns 
8. Colloquial Jakartan Indonesian corpus: audio and text, recorded in Jakarta in the early 1990?s (ANU) 
9. The ClueWeb dataset (http://lemurproject.org/clueweb12/). 
  Table 1: Alveo Data Collections 
 
Through the web-based Discovery interface, the user can select items based on the results of faceted 
search across the collections and can organise selected data in Items Lists. Beyond browsing and 
searching, Alveo offers the possibility of analysing and processing the data with a range of tools. In 
the first phase of the project, the tools listed in Table 2 were integrated within Alveo.  
 
1. EOPAS (PARADISEC tool) for text interlinear text and media analysis 
2. NLTK (Natural Language Toolkit) for text analytics with linguistic data (Bird, Klein, & Loper, 2009) 
3. EMU, for search, speech analysis and interactive labelling of spectrograms and waveforms (Steve Cassidy 
& Harrington, 2000) 
4. AusNC Tools: KWIC, Concordance, Word Count, statistical summary and statistical analysis 
5. Johnson-Charniak parser, to generate full parse trees for text sentences (Charniak & Johnson, 2005) 
6. ParseEval, to evaluate the syllabic parse of consonant clusters (Shaw & Gafos, 2010) 
7. HTK-modifications, a patch to HTK (Hidden Markov Model Toolkit, http://htk.eng.cam.ac.uk/) to enable 
missing data recognition 
8. DeMoLib, for video analysis (http://staff.estem-uc.edu.au/roland/research/demolib-home/) 
9. PsySound3, for physical and psycho-acoustical analysis of complex visual and auditory scenes (Cabrera, 
Ferguson, & Schubert, 2007) 
10. ParGram, grammar for Indonesian (Arka, 2012) 
11. INDRI, for information retrieval with large data sets (http://www.lemurproject.org/indri/) 
Table 2: Alveo Tools 
 
Most of these tools require significant expertise to set up and one of the Alveo project goals is to make 
this easier for non-technical researchers. The Alveo Workflow Engine is built around the Galaxy open 
source workflow management system (Goecks, Nekrutenko, Taylor, & Team, 2010), which was origi-
nally designed for use in the life sciences to support researchers in running pipelines of tools to ma-
nipulate data. Workflows in Galaxy can be stored, shared and published, and we hope this will also 
become a way for human communication science researchers to codify and exchange common anal-
yses.  
A number of the tools listed in Table 2 have been packaged as Python scripts, for instance NLTK 
based scripts to carry out part-of-speech tagging, stemming and parsing. Other tools are implemented 13
in R, e.g. EMU/R and ParseEval. An API is provided to mediate access to data, ensuring that permis-
sions are respected, and providing a way to access individual items, and 'mount' datasets for fast ac-
cess (Steve Cassidy, Estival, Jones, Burnham, & Burghold, 2014). An instance of the Galaxy Work-
flow engine is run on a virtual machine in the NeCTAR Research Cloud, a secure platform for Aus-
tralian research, funded by the same government program (https://www.nectar.org.au/research-cloud).  
Finally, a UIMA interface has been developed to enable the conversion of Alveo items, as well as their 
associated annotations, into UIMA CAS documents, for analysis in a conventional UIMA pipeline. 
Conversely annotations from a UIMA pipeline can be associated with a document in Alveo. Figure 1 
gives an overview of the architecture. 
 
 
Figure 1: The architecture of the Alveo Virtual Laboratory 
 
2.1 Annotations in Alveo 
Annotations in Alveo are stored in a standoff format based on the model described in the ISO Linguis-
tic Annotation Framework (ISO-LAF).  Internally, annotations are represented as RDF using the DA-
DA (Steve Cassidy, 2010). Each annotation is identified by a distinct URI and references into the 
source documents are stored as offsets either using character positions, times or frame counts for au-
dio/video data.  Annotations have an associated type attribute that denotes the kind of annotation 
(speaker turn, part of speech, phonetic segment) and a label that defines a simple string value associat-
ed with the annotation.  Annotations may also have other properties defined as standard RDF proper-
ties and values.   
The API exposes a direct interface to the annotation store in RDF via a SPARQL endpoint, but the 
normal mode of access is via the REST API where each item (document) has a corresponding URI 
that returns the collection of annotations on that item in a JSON-LD format.  JSON-LD allows us to 
represent the full RDF namespaces of properties and values in a concise format that is easily processed 
using standard JSON tools.  An example annotation delivered in this format is shown in Figure 2.  The 
same JSON-LD format can be used to upload new annotations to be stored in Alveo. 
 
{ 
 "@context": "https://app.alveo.edu.au/schema/json-ld", 
 "commonProperties": { 
    "alveo:annotates":"https://app?AusE08/document/GCSAusE07.mp3" 
 }, 
 "alveo:annotations": [ 
 { 
  "@id": "http://ns.ausnc.org.au/corpora/gcsause/annotation/535958", 
  "type": "http://ns.ausnc.org.au/schemas/annotation/conversation/micropause", 
  "@type": "dada:TextAnnotation", 
  "end": "34", 
  "start": "33" 
 }, 
?} 
Figure 2: An example of the Alveo JSON-LD annotation format 
 
 
 14
 3 UIMA 
3.1 Background 
The Unstructured Information Management Architecture (UIMA) is an Apache open source project 
(http://uima.apache.org) (D. Ferrucci & Lally, 2004) that provides an architecture for developing ap-
plications involving analysis of large volumes of unstructured information. This framework allows 
development of modular pipelines for analysing the sorts of data available in Alveo, including speech, 
text and video. A number of groups around the world have adopted UIMA to enable easier interopera-
bility and sharing of language technology components. This is true particularly in the biomedical natu-
ral language processing community; several groups have made tools available as UIMA modules. 
Most OpenNLP modules have been wrapped for use within UIMA, and the UIMA community more 
broadly has a range of language technology tools available in UIMA-compliant modules (David 
Ferrucci et al., 2010). 
Each component in a UIMA application implements interfaces defined by the framework and pro-
vides self-describing metadata via XML descriptor files. The framework manages these components 
and the data flow between them. Since UIMA applications are defined in terms of descriptors that 
clearly specify both the component modules of the application and the configuration parameter set-
tings for executing the application, they are ?re-runnable, re-usable procedures? of the kind that Alveo 
aims to capture.  
Given the objective of Alveo to facilitate access to analysis tools, and the UIMA objective of mak-
ing such analysis tools interoperable, bringing the two frameworks together made sense. Thus the ob-
jective of the integration was to build a bidirectional translation layer between Alveo and any standard 
UIMA pipeline. In other words, the translation component was required to: 1) read corpus data includ-
ing associated annotations stored in Alveo into a UIMA pipeline and 2) store annotations produced by 
a UIMA pipeline in Alveo. 
3.2 Overview of the Conversion Layer Architecture 
We opted for the most straightforward connection between the two frameworks, i.e. communicating 
directly with the Alveo REST API. The approach involves allowing annotations and documents to 
flow from Alveo, be processed externally to Alveo in a specially-configured UIMA pipeline, and then 
providing a mechanism for new annotations over the documents to be returned to Alveo for storage. 
The Alveo REST API provides access to item metadata, documents and annotations using a JSON-LD 
based interchange format.  The API supports most actions that are available via the web interface in-
cluding meta-data queries and retrieval of documents either individually or in batches.  
We built the UIMA-Alveo conversion layer, denoted Alveo-UIMA. It allows reading a group of 
documents from Alveo, and converting the documents along with their associated annotations into 
UIMA Common Annotation Structure (CAS) instances. It also allows annotations produced by a UI-
MA Collection Processing Engine (CPE) pipeline on a set of CASes to be uploaded to an Alveo serv-
er. Alveo-UIMA is built on top of a native Java wrapper for the Alveo REST API. It is implemented in 
Java and is distributed as an open-source package.2 The conversion layer exposes the Alveo data as 
native Java data structures and is also available as a standalone package,3 providing, as a side effect, a 
method to access the Alveo REST API without needing to invoke the UIMA machinery. Similar pack-
ages are also available for Python and R in the Alveo repository. 
The first component of the UIMA interface, for reading existing items, is implemented as UIMA 
Collection Reader. This takes as parameters an Alveo item list ID, corresponding to a user-created list 
of documents, and server credentials. It converts the Alveo items from that item list, as well as their 
associated annotations, into UIMA CAS documents, which can then be used as part of a conventional 
UIMA pipeline. The UIMA processing pipeline can then take advantage of the annotations download-
ed from Alveo (e.g. by using a speaker turn annotation to demarcate a sentence). 
                                                 
2 https://github.com/Alveo; https://github.com/Alveo/alveo-uima 
3 https://github.com/Alveo/alveo-java-rest-api 
15
The second component is for the opposite direction, i.e. taking annotations from a UIMA pipeline 
and associating them with the document in Alveo. There is no capability yet to add new documents 
that derive from outside Alveo, as this is not currently possible using Alveo?s REST API. This means 
that documents for which we are uploading UIMA annotations must have originated in Alveo and 
have come from the Collection Reader, ensuring that each document has appropriate identifying 
metadata for Alveo.  
Annotations need to be converted from Alveo to UIMA and vice versa, since the annotation formats 
are not identical. Some attributes, such as textual character offsets, are directly convertible, while oth-
ers require more work. Every document retrieved from the Alveo server has metadata (e.g. data 
source, recording date and language) and annotations (e.g. POS tags) associated with it. Converting 
metadata from Alveo to UIMA is straightforward, as the expected metadata fields can be directly 
mapped to a customised UIMA type. Converting annotations from between the frameworks requires 
more work, due to the required use of a type system in UIMA. This is discussed further below. 
3.3 Conversion from Alveo to UIMA 
An annotation in Alveo consists of a beginning and ending offset, which can correspond to a character 
span for textual data or a time span for audio-visual data, a human-readable plain-text label indicating 
additional attributes of the data, and a type (a URI indicating the kind of entity to which the annotation 
corresponds, e.g. ?speaker-turn?, ?intonation? or ?part-of-speech?). Since UIMA also encodes text 
annotations as character spans, these can be straightforwardly converted into the UIMA CAS (audio-
visual data can be similarly treated, but we have focused on text in the current work).  
UIMA allows the definition of custom data types with specific fields for storing salient values. We 
add a generic Alveo annotation type (inheriting from the standard UIMA Annotation, which means it 
still has spans attached). The label of the annotation in Alveo is a non-decomposable string, so this 
top-level type has a field to store the label as a string.  
Handling annotation types requires more care. Types in Alveo are encoded as fully-qualified URIs, 
while types in UIMA are more strictly defined. In particular, UIMA has a notion of a fully-specified 
type system associated with each pipeline specification, including a full inheritance hierarchy up to a 
root type and features corresponding to attributes of each type.  There are also minor differences be-
tween the encoding of the type names  (instead of a URI, UIMA uses a ?big-endian? qualified type 
name similar to a Java package, such as com.example.nlp.Sentence). 
In addition, UIMA component specification requires specifying in advance the type system to 
which all annotations represented within UIMA must conform. All these requirements are handled in a 
type system generation phase triggered when the Alveo-UIMA collection reader is created.4 During 
this phase, the reader requests an enumeration of all known type URIs from the Alveo server. Since 
we have no explicit additional information about type inheritance from the URIs, we make as few as-
sumptions as possible by having all types inherit from a generic Alveo annotation parent type. The 
Alveo URIs are automatically converted to UIMA types names, essentially by reversing the compo-
nents of the domain name, and replacing the ?/? character in the path component of the URI with ?.?, 
with some extra handling of non-alphanumeric characters, giving conversions such as the following: 
 http://example.com/nlp/sentence  ? com.example.nlp.Sentence  
In addition, the type URI is stored as an attribute of the UIMA annotation, providing an explicit record 
of the original Alveo type. Because it is far more natural to work with UIMA types than comparing 
string values when manipulating and filtering annotations in a UIMA pipeline, the automatically gen-
erated type system is very beneficial. 
We note that there have been proposals to simplify the internal UIMA type system through the use 
of a generic Referent type which refers to an external source of domain semantics (D. Ferrucci, 
Lally, Verspoor, & Nyberg, 2009) This would be a good strategy to pursue here, so that the UIMA 
annotations could refer explicitly to the Alveo URIs as an external type system. However, it has been 
noted previously that this representational choice has consequences for the indexing and reasoning 
over the semantics of annotations in UIMA analysis engines (Verspoor, Baumgartner Jr, Roeder, & 
                                                 
4 If the framework users are using UIMA canonically, where the type systems are described by pre-existing XML descriptors, 
they can explicitly request generation of the appropriate XML. The wrapper was primarily developed using UIMAFit 
(https://uimafit.apache.org/uimafit), which allows a more dynamic approach. 
16
Hunter, 2009). The current version of UIMA does not provide direct support for this model and hence 
our strategy is in line with current technical practice. These proposals aim to not replicate the full ex-
ternal type structure within the UIMA type system definition, and this is the practice we follow here, 
although since Alveo does not currently have a strong notion of type hierarchy this was not a signifi-
cant consideration. 
3.4 Conversion from UIMA to Alveo 
The annotation upload component is implemented as a UIMA CAS Consumer, i.e. a component which 
accepts CASes and performs some action with them. To upload annotations, as noted above, we ex-
pect the supplied CAS to derive originally from the Alveo server, with annotations added to that CAS 
by UIMA processing components. The original metadata from Alveo is used to determine the URL of 
the original item, and otherwise ignored.  
The first step in annotation upload is to retrieve the original source document and remove from the 
set of annotations to be uploaded those annotations which already appear in the version found on the 
server. In addition, since processing pipelines may produce a wide variety of annotations which may 
not all be appropriate or relevant for uploading to Alveo, the annotation type must occur in a precon-
figured whitelist. Converting each annotation from UIMA to Alveo is in some ways the inverse of the 
operation described in the previous section, although there are some intricacies to the process.  
The character spans can be directly converted as before; again the type and label require more work. 
For type conversion, some sensible default behaviours are used. A configurable list of UIMA features 
are inspected on the UIMA annotations, and the first match found is used as the Alveo annotation 
type. This list of features naturally includes the default feature for storing the type URI, ensuring that 
annotations which derive from Alveo originally can be matched back to the original annotation. If no 
matches are found, a type URI is inferred from the fully-qualified UIMA annotation type name, using 
the inverse operation to that described above.  
Alveo annotations also have labels, as noted in the previous section. As with the annotation types, 
there is a similar list of UIMA feature names which can be used to populate the label attribute on the 
Alveo annotation, defaulting to the empty string if no feature name is found. If these strategies do not 
produce the desired behaviour when uploading, it is possible to customise them by implementing a 
Java interface. Alternatively, it is also possible to insert a custom UIMA component into the pipeline 
to convert the added UIMA annotations so that the Alveo conversion works as desired. 
4 Alveo User Acceptance Testing 
As it was a requirement of the funding agency for the project to provide evidence of User Acceptance 
Testing (UAT) and acceptance of the results of these tests by the project governing body (the Steering 
Committee), the project was organised from the start around these requirements, with all the partners 
bidding for participation in tests of specific components or versions of the system. A testing schedule 
was developed to accompany the system development, with the aim of gathering feedback from the 
project partners during development to provide targeted input for improvement. A sub-committee of 
the Steering Committee was designated to oversee the tests distributed to the testers, examine the re-
ports summarising the results of those tests and recommend acceptance. 
Alveo was designed and implemented in partnership with Intersect, a commercial software devel-
opment company specialised in the support of academic eResearch. This partnership afforded exten-
sive professional support during development, using the Agile process (Beck & al, 2001) as well as 
thorough regression testing and debugging. In other projects of this type, Intersect provided UAT or 
managed the UAT process in-house. For the Alveo project, since user testing was the main way in 
which the academic partners were involved in the project, UAT was organised by the academic part-
ners with technical support from Intersect. The central team at the lead institution oversaw the creation 
of the tests (see section 4.2), distributed the tests and monitored the results.    
4.1 The Alveo UAT process 
During development, Alveo was deployed incrementally on separate servers. While the Production 
Server remained stable between versions, the Staging 1 server was reserved for UAT and only updated 
17
when new functionalities were added; the Staging 2 Server was used for development and frequently 
updated. This rarely caused problems, even with the distributed nature of the testing process. 
 Each partner site engaged High Degree Researchers (HDRs), generally Masters and Doctoral 
students but also Post-Doctoral Fellows or project members, who had an interest in a particular do-
main or tool, or who could provide critical comments about the functionalities. Some Testers were 
Linguistics students with no computing background, some were Computer Science students with lim-
ited linguistic knowledge. At some sites, the Testers were Research Assistants who had worked on the 
tools or corpora contributed by their institutions, while others were the tool developers themselves. 
This variety of backgrounds and skills ensured coverage of the main domains and functionalities ex-
pected of the Alveo Virtual Lab. Some sites had undertaken to conduct large amounts of testing 
throughout the development, while other partners only chose to perform limited or more targeted test-
ing, with commitments varying from 10 to 200 hours. Over 30 Testers participated at various times 
during of the project and a total of more than 300 hours has been spent on testing during Phase I. 
4.2 Evolution of the tests 
For each version of the system during development (Prototype, Version 1, 2, and 3) a series of tests 
were developed and posted on a Google Form. To record the results, the Testers filled out a Google 
form which was monitored by the central team. The first tests developed were very directive, giving 
very specific instructions as to what actions the user was asked to perform and what results were ex-
pected for each action, as shown in Figure 3, one of the tests for Version 1. 
 
 
Figure 3: Test for Alveo Version 1 
 
Gradually the tests became more open-ended, giving less guidance and gathering more informative 
feedback. The latest round of testing asked Testers to log in and to carry out a small research task, as 
shown in Figure 4, the instructions for the open form testing of Version 3. 
 
 
Figure 4: Open form test for Alveo Version 3 
 
Some of the early tests, such as the one shown in Figure 3, have become tutorials provided on the Al-
veo web page and are now available as help from within the Virtual Lab. 
Test 2 - Browsing COOEE 
1. Login to the main website. 
2. In the list of facets on the left, click Corpus, this should show a list of corpus names. 
3. From the list of corpus names click cooee, the page should update to show 1354 results, listing the first 10 
matching items from the COOEE corpus. 
4. In the list of facets on the left click Created, this should show a list of decades. 
5. From the list of decades click 1870-1879, the page should update to show 61 results which are COOEE items 
from the 1870s. 
6. From the list of matching items, click on the first item, the page should update to show the details of this 
item. Verify that the Created date is within the 1870s and that the isPartOf field shows cooee. 
7. Scroll down to the bottom of the page where you should see links to the documents in this item.  Click on 
the document marked Text(Original), you should see the text of the document including some markup at 
the start and end of the file. 
8. Use the Back button in your browser to return to the item display page, click on the document marked 
Text(Plain), you should see the text of the document with no markup. 
9. Use the Back button in your browser to return to the item display page. 
10. When you are finished, click on the HCSvLab logo on the top left of the page to return to the home page and 
reset your search. 
Based on your own research interests and based on what you've seen of the HCS vLab platform, please try to make 
use of the virtual lab to carry out a small research task. Use the form below to tell us about what you tried to do: the 
collections and tools that you used, a description of your task, the outcomes and any problems that you faced. 
18
4.3 UIMA Testing 
In order to test the Alveo-UIMA implementation and provide an example of how it can be used, we 
created a tutorial application5 available from the Alveo github repository. This tutorial shows an ex-
ample of instantiating a UIMA CPE pipeline which reads documents from an Alveo item list, aug-
ments it with part-of-speech annotations and uploads them to the Alveo server. A UIMA pipeline con-
sists of a collection reader, and one or more CAS annotators. The UIMA tutorial pipeline includes the 
standard Collection Reader from Alveo-UIMA, a basic POS-tagging CAS annotator from DKPro-
Core,6 and the annotation uploading CAS annotator from Alveo-UIMA. An advanced version also 
demonstrates implementing an interface which remaps the POS tag types from those automatically-
derived from DKPro.outputs. 
5 Discussion 
5.1 Related Work 
There are several frameworks that have been developed to enable development and evaluation of text 
processing workflows, and UIMA has been used as the backbone for a few such frameworks due to its 
support for processing module interoperability. The Argo web service (Rak, Rowley, & Ananiadou, 
2012; Rak, Rowley, Carter, & Ananiadou, 2013) is a recent web application that enables development 
of UIMA-based text processing workflows through an on-line graphical interface. In contrast to Al-
veo, documents are uploaded to the system within an individual user space, and resulting annotations 
are not persisted outside of the UIMA data structures; although they can be serialised and stored for 
subsequent re-use in processing pipelines, or exported as RDF, they are not directly accessible within 
the framework itself. The repository contains a wide range of NLP components, e.g., modules to per-
form sentence splitting, POS tagging, parsing, and a number of information extraction tasks targeted to 
biomedical text. 
The U-Compare system (Kano et al., 2009; Kano, Dorado, McCrohon, Ananiadou, & Tsujii, 2010) 
also supports evaluation and performance comparison of UIMA-based automated annotation tools. It 
was designed with UIMA in mind from the ground up, enabling UIMA workflow creation and execu-
tion through a GUI. Therefore it assumes that all analysis of collections is performed with a set of 
UIMA components, and indeed provides a substantial number of such components in their repository, 
although other components can be added. The system is launched locally via Java Web Start; given 
recent changes to how browsers interact with Java, this no longer works reliably and off-line use (after 
downloading and installing) is likely necessary, although interaction with web service-based pro-
cessing components is possible (Kontonasios, Korkontzelos, Kolluru, & Ananiadou, 2011). 
A competing framework based on the GATE architecture (Cunningham, Maynard, Bontcheva, & 
Tablan, 2002) is the cloud-based AnnoMarket platform. This framework provides access to natural 
language processing (NLP) components, and a limited number of existing resources (one at the time of 
writing7, with the facility to upload user-specific data) on a fee-for-service basis (passing along costs 
of using the Amazon cloud services). Results of NLP studies of this data can be downloaded, or in-
dexed and made available for search. There are a wide array of annotation services and pre-configured 
pipelines available within the AnnoMarket that can be applied to a user?s document collection, either 
directly through the on-line application or via a web service API. 
5.2 Alternative strategies for UIMA integration with Alveo 
There were several possible places where the UIMA-Alveo translation layer could have been inserted, 
and indeed several possible architectures were considered for integrating UIMA with Alveo.  
Since Alveo was already working with the workflow engine Galaxy, one option was to create a 
compatibility layer to bridge UIMA with Galaxy, for instance to enable a pre-configured UIMA pipe-
line to be instantiated via a Galaxy wrapper. The technical details for accomplishing this were not im-
mediately obvious, and it was decided that this approach would add substantial complexity to the con-
version of annotations in the conversion layer. 
                                                 
5 http://github.com/Alveo/alveo-uima-tutorial 
6 https://code.google.com/p/dkpro-core-asl/ 
7 https://annomarket.com/dataSources, accessed 29 May 2014 
19
Another option that was considered was to allow for dynamic construction of UIMA workflows 
from UIMA components directly through the Alveo web interface. UIMA is a workflow engine analo-
gous to Galaxy, in that it enables dynamic configuration of pipelines from the available set of UIMA 
components set up in a given environment. In principle, therefore, it would be possible to enable spec-
ification, instantiation, and execution of UIMA pipelines from a set of UIMA components made avail-
able via Alveo. However, this would have required a substantial development effort specifically tar-
geted towards hosting UIMA components and manipulating UIMA pipelines; it was decided that a 
more general approach to integrating a broader range of tools was more appropriate for Alveo. Given 
the recent availability of the Argo web application, an Alveo/Argo integration could be considered that 
would enable users to create UIMA workflows with Argo but execute them from Alveo, and on doc-
uments or corpora stored in Alveo (Rak et al., 2012; Rak et al., 2013). The current web service-based 
architecture of the Alveo-UIMA integration lends itself well to this possibility. This could be explored 
in future work. 
The current implementation assumes that an Alveo user will have the knowledge to create and run 
UIMA pipelines externally to Alveo. A complementary strategy, possible now that the conversion lay-
er is in place, would be to make complete, pre-configured UIMA pipelines available as tools that can 
be applied to Alveo corpora/data. A number of such services, e.g. services aimed at annotation of text 
with one of a set of biomedically-relevant entity types (diseases, genes, chemicals) have been built 
(MacKinlay & Verspoor, 2013). Each such service is run as a separate UIMA instance that is accessed 
via a web service.  Text is passed in via the REST interface, handed over to the UIMA instance, pro-
cessed, and annotations are returned. This basic model could be replicated for a number of UIMA 
pipelines that do standard text-related processing (e.g. split sentences, perform part of speech tagging 
and parsing, etc.) such that text extracted from Alveo could be processed by the UIMA-based service 
and annotations returned. This approach has been criticised for its inability to be extended or adapted 
(Tablan, Bontcheva, Roberts, Cunningham, & Dimitrov, 2013) although it is suitable where pre-
packaged pipelines can be applied to accomplish tasks of broad interest. 
6 Conclusions 
The development of Alveo presented a number of challenges, some technical, such as the integration 
of UIMA with the platform, and others more logistic, such as the distributed nature of testing during 
development. In this paper, we described the solution and the choices we made for the implementation 
of UIMA pipelines, given the constraints regarding the organisation of items, documents and their as-
sociated annotations in Alveo. One of the conditions of success of such a project is that the platform 
be used by researchers for their own projects and on their own data. The organisation of the User Ac-
ceptance Testing, requiring partners to contribute during the development, and providing exposure to 
the tools and the datasets to a large group of diverse researchers is expected to lead to a much wider 
uptake of Alveo as a platform for HCS research in Australia. We plan to open it to users outside the 
original project partners during Phase II (2014-2016). We will also continue to explore further interac-
tions with complementary frameworks, such that the data and annotation storage available in Alveo 
can be enhanced via processing and tools from external services to supplement the functionality that is 
currently directly integrated. 
 
Acknowledgements 
We gratefully acknowledge funding from the Australian Government National eResearch Collabora-
tion Tools and Resources (NeCTAR) and thank all our collaborating partners in the Alveo Virtual La-
boratory project: University of Western Sydney, Macquarie University, RMIT, University of Mel-
bourne, Australian National University, University of Western Australia, University of Sydney, Uni-
versity of New England, University of Canberra, Flinders University, University of New South Wales, 
University of La Trobe, University of Tasmania, ASSTA, AusNC Inc., NICTA, and Intersect. 
 
 
 
20
References 
Arka, I. W. (2012). Developing a Deep Grammar of Indonesian within the ParGram Framework: Theoretical 
and Implementational Challenges Paper presented at the 26th Pacific Asia Conference on 
Language,Information and Computation.  
Beck, K., et al. (2001). Manifesto for Agile Software Development. http://agilemanifesto.org/ 
Bird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python - Analyzing Text with the 
Natural Language Toolkit: O'Reilly Media. 
Burnham, D., Estival, D., Fazio, S., Cox, F., Dale, R., Viethen, J., . . . Wagner, M. (2011). Building an audio-
visual corpus of Australian English: large corpus collection with an economical portable and 
replicable Black Box. Paper presented at the Interspeech 2011, Florence, Italy.  
Cabrera, D., Ferguson, S., & Schubert, E. (2007). 'Psysound3': Software for Acoustical and Psychoacoustical 
Analysis of Sound Recordings. Paper presented at the International Community on Auditory Display.  
Cassidy, S. (2010). An RDF Realisation of LAF in the DADA Annotation Server. Paper presented at the ISA-5, 
Hong Kong.  
Cassidy, S., Estival, D., Jones, T., Burnham, D., & Burghold, J. (2014). The Alveo Virtual Laboratory: A Web 
Based Repository API. Paper presented at the 9th Language Resources and Evaluation Conference 
(LREC 2014), Reykjavik, Iceland.  
Cassidy, S., & Harrington, J. (2000). Multi-level Annotation in the Emu Speech Database Management System. 
Speech Communication, 33, 61?77.  
Cassidy, S., Haugh, M., Peters, P., & Fallu, M. (2012). The Australian National Corpus : national infrastructure 
for language resources. Paper presented at the LREC.  
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. Paper 
presented at the 43rd Annual Meeting on Association for Computational Linguistics.  
Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V. (2002). GATE: A Framework and Graphical 
Development Environment for Robust NLP Tools and Applications. Paper presented at the 40th 
Anniversary Meeting of the Association for Computational Linguistics (ACL'02), Philadelphia, USA.  
Estival, D., Cassidy, S., Sefton, P., & Burnham, D. (2013). The Human Communication Science Virtual Lab. 
Paper presented at the 7th eResearch Australasia Conference, Brisbane, Australia.  
Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A. A., . . . Welty, C. (2010). Building 
Watson: An Overview of the DeepQA Project. AI Magazine, 31(3), 59-79. doi: 
http://dx.doi.org/10.1609/aimag.v31i3.2303  
Ferrucci, D., & Lally, A. (2004). UIMA: an architectural approach to unstructured information processing in the 
corporate research environme. Natural Language Engineering, 10(3-4), 327-348.  
Ferrucci, D., Lally, A., Verspoor, K., & Nyberg, A. (2009). Unstructured Information Management Architecture 
(UIMA) Version 1.0 Oasis Standard. 
Goecke, R., & Millar, J. B. (2004). The Audio-Video Australian English Speech Data Corpus AVOZES. Paper 
presented at the 8th International Conference on Spoken Language Processing (INTERSPEECH 2004 - 
ICSLP), Jeju, Korea.  
Goecks, J., Nekrutenko, A., Taylor, J., & Team, T. G. (2010). Galaxy: a comprehensive approach for supporting 
accessible, reproducible, and transparent computational research in the life sciences. Genome Biology, 
11(8), R86.  
Kano, Y., Baumgartner, W. A., McCrohon, L., Ananiadou, S., Cohen, K. B., Hunter, L., & Tsujii, J. I. (2009). U-
Compare: share and compare text mining tools with UIMA. Bioinformatics, 25(15), 1997-1998.  
Kano, Y., Dorado, R., McCrohon, L., Ananiadou, S., & Tsujii, J. (2010). U-Compare: An Integrated Language 
Resource Evaluation Platform Including a Comprehensive UIMA Resource Library. Paper presented at 
the LREC. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.8878&rep=rep1&type=pdf 
Kontonasios, G., Korkontzelos, I., Kolluru, B., & Ananiadou, S. (2011). Adding text mining workflows as web 
services to the BioCatalogue. Paper presented at the Proceedings of the 4th International Workshop on 
Semantic Web Applications and Tools for the Life Sciences (SWAT4LS '11 ).  
MacKinlay, A., & Verspoor, K. (2013). A Web Service Annotation Framework for CTD Using the UIMA 
Concept Mapper. Paper presented at the Fourth BioCreative Challenge Evaluation Workshop. 
http://www.biocreative.org/media/store/files/2013/bc4_v1_14.pdf 
Musgrave, S., & Haugh, M. (2009). The AusNC Project: Plans, Progress and Implications for Language 
Technology. Paper presented at the ALTA 2009, Sydney.  
Rak, R., Rowley, A., & Ananiadou, S. (2012). Collaborative Development and Evaluation of Text-processing 
Workflows in a UIMA-supported Web-based Workbench. Paper presented at the LREC. 
http://www.lrec-conf.org/proceedings/lrec2012/pdf/960_Paper.pdf 
Rak, R., Rowley, A., Carter, J., & Ananiadou, S. (2013). Development and Analysis of NLP Pipelines in Argo. 
Paper presented at the ACL. http://aclweb.org/anthology//P/P13/P13-4020.pdf 21
Shaw, J. A., & Gafos, A. I. (2010). Quantitative evaluation of competing syllable parses. Paper presented at the 
11th Meeting of the Association for Computational Linguistics. Special Interest Group on 
Computational Morphology and Phonology, Uppsala, Sweden.  
Tablan, V., Bontcheva, K., Roberts, I., Cunningham, H., & Dimitrov, M. (2013). AnnoMarket: An Open Cloud 
Platform for NLP. Paper presented at the 51st Annual Meeting of the Association for Computational 
Linguistics (ACL 2013), Sofia, Bulgaria.  
Thieberger, N., Barwick, L., Billington, R., & Vaughan, J. (Eds.). (2011). Sustainable data from digital 
research: Humanities perspectives on digital scholarship. A PARDISEC Conference: Custom Book 
Centre. http://ses.library.usyd.edu.au/handle/2123/7890. 
Verspoor, K., Baumgartner Jr, W., Roeder, C., & Hunter, L. (2009). Abstracting the types away from a UIMA 
type system From Form to Meaning: Processing Texts Automatically (pp. 249-256). 
 
22
