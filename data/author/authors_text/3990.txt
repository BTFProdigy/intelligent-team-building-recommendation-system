Detecting Article Errors Based on
the Mass Count Distinction
Ryo Nagata1, Takahiro Wakana2, Fumito Masui2,
Atsuo Kawai2, and Naoki Isu2
1 Hyogo University of Teacher Education,
942-1 Shimokume, Yashiro, 673-1494 Japan
rnagata@info.hyogo-u.ac.jp
2 Mie University, 1577, Kurimamachiya, Tsu, 514-8507, Japan
{wakana, masui, kawai, isu}@ai.info.mie-u.ac.jp
Abstract. This paper proposes a method for detecting errors concern-
ing article usage and singular/plural usage based on the mass count
distinction. Although the mass count distinction is particularly impor-
tant in detecting these errors, it has been pointed out that it is hard
to make heuristic rules for distinguishing mass and count nouns. To
solve the problem, first, instances of mass and count nouns are auto-
matically collected from a corpus exploiting surface information in the
proposed method. Then, words surrounding the mass (count) instances
are weighted based on their frequencies. Finally, the weighted words are
used for distinguishing mass and count nouns. After distinguishing mass
and count nouns, the above errors can be detected by some heuristic
rules. Experiments show that the proposed method distinguishes mass
and count nouns in the writing of Japanese learners of English with
an accuracy of 93% and that 65% of article errors are detected with a
precision of 70%.
1 Introduction
Although several researchers [1,2,3] have shown that heuristic rules are effective
to detecting grammatical errors in the English writing of second language learn-
ers, it has been pointed out that it is hard to write heuristic rules for detecting
article errors [1]. To be precise, it is hard to write heuristic rules for distinguish-
ing mass and count nouns which are particularly important in detecting article
errors. The major reason for this is that whether a noun is a mass noun or a
count noun greatly depends on its meaning or its surrounding context (Refer to
Pelletier and Schubert [4] for detailed discussion on the mass count distinction).
Article errors are very common among Japanese learners of English [1,5].
This is perhaps because the Japanese language does not have an article system
similar to that of English. Thus, it is favorable for error detecting systems aiming
at Japanese learners of English to be capable of detecting article errors. In other
words, such systems need to somehow distinguish mass and count nouns in the
writing of Japanese learners of English.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 815?826, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
816 R. Nagata et al
In view of this background, we propose a method for automatically dis-
tinguishing mass and count nouns in context to complement the conventional
heuristic rules for detecting grammatical errors. In this method, mass and count
nouns are distinguished by words surrounding the target noun. Words surround-
ing the target noun are collected from a corpus and weighted based on their
occurrences. The weighted words are used for distinguishing mass and count
nouns in detecting article errors.
Given the mass count distinction, errors concerning singular/plural usage,
which are also common in the writing of Japanese learners of English, can be
detected as well as article errors. For example, given that the noun information
is a mass noun, informations can be detected as an error. Considering this, we
include errors concerning singular/plural usage in the target errors of this paper.
Hereafter, to keep the notation simple, the target errors1 will be referred to as
article errors.
The next section describes related work on distinguishing mass and count
nouns. Section 3 proposes the method for automatically distinguishing mass and
count nouns. Section 4 describes heuristic rules for detecting article errors based
on the mass count distinction given by the proposed method. Section 5 discusses
results of experiments conducted to evaluate the proposed method.
2 Related Work
Several researchers have proposed methods for distinguishing mass and count
nouns in the past. Allan [6] has presented an approach to distinguishing nouns
that are used only as either mass or count based on countability environments.
This distinction is called countability preferences. Baldwin and Bond [7,8] have
proposed several methods for learning the countability preferences from cor-
pora2. Bond and Vatikitis-Bateson [9] have shown that nouns? countability can
be predicted using an ontology3. O?Hara et al [10] have proposed a method for
classifying mass and count nouns based on semantic information (Cyc ontological
types [11]).
Unfortunately, it is difficult to apply the above methods to complement
the conventional heuristic rules for detecting grammatical errors. The meth-
ods [6,7,8,9] are not enough for the purpose, because the majority of nouns can
be used as both mass and count depending on the surrounding context [12].
The methods [9,10] cannot be readily applicable to the purpose because they
work only when semantic information on nouns is given. It would be difficult to
extract semantic information from nouns in the writing of learners of English.
1 The details of the target errors are shown in Sect. 4.
2 They define four way countability preferences: fully countable, uncountable, bipar-
tite, and plural only.
3 They define five way countability preferences: fully countable, strongly countable,
weakly countable, uncountable, and plural only.
Detecting Article Errors Based on the Mass Count Distinction 817
3 Distinguishing Mass and Count Nouns
In the proposed method, decision lists [13] are used to distinguish mass and count
nouns. Generally, decision lists are learned from a set of manually tagged training
data. In the proposed method, however, training data can be automatically
generated from a raw corpus.
Section 3.1 describes how to generate training data. Section 3.2 describes how
to learn decision lists from the training data. Section 3.3 explains the method
for distinguishing mass and count nouns using the decision lists.
3.1 Generating Training Data
To generate training data, first, instances of the target noun that head their
noun phrase (NP) are collected from a corpus with their surrounding words.
This can be simply done by an existing chunker or parser.
Then, the collected instances are tagged with mass or count by tagging rules.
For example, the underlined chicken:
Example 1. ... are a lot of chickens in the roost ...
is tagged as
Example 2. ... are a lot of chickens/count in the roost ...
because it is in plural form.
We have made tagging rules based on linguistic knowledge [6,14,12]. Figure 1
and Table 1 represent the tagging rules. Figure 1 shows the framework of the tag-
ging rules. Eachnode in Fig. 1 represents a question applied to the instance in ques-
tion. For example, the root node reads ?Is the instance in question plural??. Each
leaf represents a result of the classification. For example, if the answer is ?yes? at the
root node, the instance in question is tagged with count. Otherwise, the question
at the lower node is applied and so on. The tagging rules do not classify instances
as mass or count in some cases. These unclassified instances are tagged with the
symbol ???. Unfortunately, they cannot readily be included in training data. For
simplicity of implementation, they are excluded from training data.
Table 1. Words used in the tagging rules
(a) (b) (c)
the indefinite article much the definite article
another less demonstrative adjectives
one enough possessive adjectives
each all interrogative adjectives
? sufficient quantifiers
? ? ?s genitives
818 R. Nagata et al




























yes
yes
yes
yes
no
no
no
no










yes no
COUNT
modified by a little?
?
COUNT
MASS
? MASS
plural?
modified by one of the words
in Table 1(a)?
modified by one of the words
in Table 1(b)?
modified by one of the words
in Table 1(c)?
Fig. 1. Framework of the tagging rules
Note that the tagging rules can be used only for distinguishing mass and
count nouns in texts containing no errors. They cannot be used in the writing
of Japanese learners of English that may contain errors including article errors;
they are based on article and the distinction between singular and plural.
Finally, the tagged instances are stored in a file with their surrounding words.
Each line in the file consists of one of the tagged instances and its surrounding
words as shown in Example 2. The file is used as training data for learning a
decision list.
3.2 Learning Decision Lists
A decision list consists of a set of rules that are learned from training data. Each
rule matches with the template as follows:
If a condition is true, then a decision . (1)
To define the template in the proposed method, let us have a look at the
following two examples:
Example 3. I read the paper .
Example 4. The paper is made of hemp pulp.
The underlined papers in both sentences cannot simply be classified as mass or
count by the tagging rules presented in Sect. 3.1 because both are singular and
modified by the definite article. Nevertheless, we can tell that the former is a
count noun and that the latter is a mass noun from the contexts. This suggests
that the mass count distinction is often determined by words surrounding the
target noun. In Example 3, we can tell that the paper refers to something that
can be read from read , and therefore it is a count noun. Likewise, in Example 4,
the paper refers to a certain substance from made and pulp, and therefore it is
a mass noun.
Detecting Article Errors Based on the Mass Count Distinction 819
Taking this observation into account, we define the template based on words
surrounding the target noun. To formalize the template, we will use a random
variable MC that takes either mass or count to denote that the target noun is
a mass noun or a count noun, respectively. We will also use w and C to denote
a word and a certain context around the target noun, respectively. We define
three types of C: np, ?k, and +k that denote the contexts consisting of the noun
phrase that the target noun heads, k words to the left of the noun phrase, and
k words to its right, respectively. Then the template is formalized by
If a word w appears in the context C of the target noun,
then the target noun is distinguished as MC.
Hereafter, to keep the notation simple, the template is abbreviated to
wC ? MC. (2)
Now rules that match with the template can be learned from the training
data generated in Sect. 3.1. All we need to do is to collect words in C from the
training data. Here, the words in Table 1 are excluded. Also, function words
such as pronouns and auxiliary verbs, cardinal and quasi-cardinal numerals, and
the target noun are excluded. All words are reduced to their morphological stem
and converted entirely to lower case when collected. For example, the following
tagged instance:
Example 5. She ate a piece of fried chicken/mass for dinner.
would give a set of rules that match with the template:
Example 6.
piece
?3 ? mass, frynp ? mass, dinner+3 ? mass
for the target noun chicken being mass when k = 3.
In addition to the above rules, a default rule is defined. It is based on the
target noun itself and used when no other confident rules4 are found in the
decision list for the target noun. It is defined by
t ? MCmajor (3)
where t and MCmajor denote the target noun and the major case of MC in the
training data, respectively. Equation (3) reads ?If the target noun appears, then
it is distinguished as the major case?.
The log-likelihood ratio [15] decides in which order rules in a decision list are
applied to the target noun in novel context. It is defined by
log
p(MC|wC)
p(MC|wC)
(4)
4 Confidence is given by the log-likelihood ratio, which will be defined by (4).
820 R. Nagata et al
where MC is the exclusive event of MC and p(MC|wC) is the probability that
the target noun is used as MC when w appears in the context C. For the default
rule, the log-likelihood ratio is defined by
log
p(MCmajor|t)
p(MCmajor|t)
(5)
It is important to exercise some care in estimating p(MC|wC). In principle,
we could simply count the number of times that w appears in the context C of
the target noun used as MC in the training data. However, this estimate can be
unreliable, when w does not appear often in the context. To solve this problem,
using a smoothing parameter ? [16], p(MC|wC) is estimated by
p(MC|wC) =
f(wC , MC) + ?
f(wC) + m?
(6)
where f(wC) and f(wC , MC) are occurrences of w appearing in C and those in
C of the target noun used as MC, respectively. The constant m is the number
of possible classes, that is, m = 2 (mass or count) in our case, and introduced
to satisfy p(MC|wC) + p(MC|wC) = 1. In this paper, ? is set to 0.5. Likewise,
p(MCmajor|t) is estimated by
p(MCmajor|t) =
f(t, MCmajor) + ?
f(t) + m?
(7)
Rules in a decision list are sorted in descending order by (4) and (5). They are
tested on the target noun in novel context in this order. Rules sorted below the
default rule are discarded because they are never used as we will see in Sect. 3.3.
Table 2 shows part of a decision list for the target noun chicken that was
learned from a subset of the BNC (British National Corpus) [17]. Note that the
rules are divided into two columns for the purpose of illustration in Table 2; in
practice, they are merged into one just as shown in Table 3.
On one hand, we associate the words in the left half with food or cooking.
On the other hand, we associate those in the right half with animals. From
this observation, we can say that chicken is a count noun in the sense of an
animal but a mass noun when referring to food or cooking, which agrees with
the knowledge presented in previous work [18].
Table 2. Rules in a decision list (target noun: chicken, k = 3)
Mass Count
wC Log-likelihood ratio wC Log-likelihood ratio
piece?3 1.49 count?3 1.49
fish?3 1.28 peck+3 1.32
dish?3 1.23 pignp 1.23
skin+3 1.23 run?3 1.23
serve+3 1.18 eggnp 1.18
Detecting Article Errors Based on the Mass Count Distinction 821
Table 3. An example of a decision list (target noun: chicken, k = 3)
Rules Log-likelihood ratio
piece?3 ? mass, count?3 ? count 1.49
peck+3 ? count 1.32
fish?3 ? mass 1.28
dish?3 ? mass, pignp ? count, ? ? ? 1.23
: :
3.3 Distinguishing the Target Noun in Novel Context
To distinguish the target noun in novel context, each rule in the decision list
is tested on it in the sorted order until the first applicable one is found. It is
distinguished by the first applicable one. If two or more applicable rules (e.g.,
?piece
?3 ? mass? and ?count?3 ? count? in Table 3) are found, it is distin-
guished by the major decisions of the two or more applicable rules. For example,
suppose there are three applicable rules and two of them are for mass nouns
(one of them is for count nouns). In this case, the target noun is distinguished as
mass. Ties are broken by rules sorted below the ties. If ties include the default
rule, it is distinguished by the default rule.
The following is an example of distinguishing the target noun chicken. Sup-
pose that the decision list shown in Table 3 and the following sentence are given:
Example 7. I ate a piece of chicken with salad.
It turns out that the first rule ?piece3 ? mass? in Table 3 is applicable to the
instance. Thus, it is distinguished as a mass noun.
It should be noted that rules sorted below the default rule are never used
because the default rule is always applicable to the target noun. This is the
reason why rules sorted below the default rule are discarded as mentioned in
Sect. 3.2.
4 Heuristic Rules for Detecting Article Errors
So far, a method for distinguishing mass and count nouns has been described.
This section describes heuristic rules for detecting article errors based on the
mass count distinction given by the method.
Article errors are detected by the following three steps. Rules in each step
are examined on each target noun in the target text.
In the first step, any mass noun in plural form is detected as an article error.
If an article error is detected in the first step, the rest of the steps are not applied.
In the second step, article errors are detected by the rules described in Ta-
ble 4. The symbol ?? in Table 4 denotes that the combination of the corre-
sponding row and column is erroneous. For example, the third row denotes that
822 R. Nagata et al
Table 4. Detection rules used in the second step
Count Mass
Pattern Singular Plural Singular Plural
{another, each, one} ?   
{a lot of, all, enough, lots of, sufficient}  ? ? 
{much}   ? 
{kind of, sort of, that, this} ?  ? 
{few, many, these,those}  ?  
{countless, numerous, several, various}  ?  
cardinal number except one  ?  
{any, some, no, ?s genitives} ? ? ? 
{interrogative adjectives, possessive adjectives} ? ? ? 
Table 5. Detection rules used in the third step
Singular Plural
a the ? a the ?
Mass  ? ?   
Count ? ?   ? ?
plural count nouns, singular mass nouns, and plural mass nouns that are mod-
ified by another , each, or one are erroneous. The symbol ??? denotes that no
error can be detected by the table. If one of the rules in Table 4 is applied to
the target noun, the third step is not applied.
In the third step, article errors are detected by the rules described in Table 5.
The symbols ?a?, ?the?, and ??? in Table 5 denote the indefinite article, the
definite article, and no article, respectively. The symbols ?? and ??? are the
same as in Table 4. For example, ?? in the third row and second column denotes
that the singular mass nouns modified by the indefinite article is erroneous.
In addition to the three steps, article errors are detected by exceptional rules.
The indefinite article that modifies other than the head noun is judged to be
erroneous (e.g., *an expensive). Likewise, the definite article that modifies other
than the head noun and adjectives is judged to be erroneous (e.g., *the them).
5 Experiments
5.1 Experimental Conditions
A subset of essays5 written by Japanese learners of English were used as the
target texts in the experiments. The subset contained 30 essays (1747 words). A
native speaker of English who was a professional rewriter of English recognized
62 article errors in the subset.
5 http://www.lb.u-tokai.ac.jp/lcorpus/index-j.html
Detecting Article Errors Based on the Mass Count Distinction 823
The British National Corpus (BNC) [17] was used to learn decision lists.
Spoken data were excluded from the corpus. Also, sentences the OAK system6,
which was used to extract NPs from the corpus, failed to analyze were excluded.
After these operations, the size of the corpus approximately amounted to 80
million words (the size of the original BNC is approximately 100 million words).
Hereafter, unless otherwise specified, the corpus will be referred to as the BNC.
Performance of the proposed method was evaluated by accuracy, recall, and
precision. Accuracy is defined by
No. of mass and count nouns distinguished correctly
No. of distinguished target nouns
. (8)
Namely, accuracy measures how accurately the proposed method distinguishes
mass and count nouns. Recall is defined by
No. of article errors detected correctly
No. of article errors in the target essays
. (9)
Recall measures how well the proposed method detects all the article errors in
the target essays. Precision is defined by
No. of article errors detected correctly
No. of detected article errors
. (10)
Precision measures how well the proposed method detects only the article errors
in the target essays.
5.2 Experimental Procedures
First, decision lists for each target noun in the target essays were learned from
the BNC. To extract noun phrases and their head nouns, the OAK system was
used7. An optimal value for k (window size of context) was estimated as follows.
For 23 nouns8 shown in [12] as examples of nouns used as both mass and count
nouns, accuracy was calculated using the BNC and ten-fold cross validation. As
a result of setting k = 3, 10, 50, it turned out that k = 3 maximized the average
accuracy. Following this result, k = 3 was selected in the experiments.
Second, the target nouns were distinguished whether they were mass or count
by the proposed method, and then article errors were detected by the mass
6 OAK System Homepage: http://nlp.cs.nyu.edu/oak/
7 We evaluated how accurately training data can be generated by the tagging rules
using the OAK system. It turned out that the accuracy was 0.997 against 2903
instances of 23 nouns shown in [12] which were randomly selected from the BNC;
1694 of those were tagged with mass or count by the tagging rules and 1689 were
tagged correctly. The five errors were due to the OAK system.
8 In [12], 25 nouns are shown. Of those, two nouns (hate and spelling) were excluded
because they only appeared 12.1 and 15.6 times on average in the ten-fold cross
validation, respectively.
824 R. Nagata et al
count distinction and the heuristic rules described in Sect. 4. As a preprocessing,
spelling errors in the target essays were corrected using a spell checker.
Finally, the results of the detection were compared to those done by the
native-speaker of English. From the comparison, accuracy, recall, and precision
were calculated.
Comparison of performance of the proposed method to that of other meth-
ods is difficult because there is no generally accepted test set or performance
baseline [19]. Given this limitation, we compared performance of the proposed
method to that of Grammarian9, a commercial grammar checker. We also com-
pared it to that of a method that used only the default rules in the decision lists.
We tested them on the same target essays to measure their performances.
5.3 Experimental Results and Discussion
In the experiments, the proposed method distinguished mass and count nouns in
the target essays with accuracy of 0.93. This means that the proposed method
is effective to distinguishing mass and count nouns in the writing of Japanese
learners of English. From this result, we can say that the proposed method can
complement the conventional heuristic rules for detecting grammatical errors.
Because of the high accuracy of the proposed method, it detected more than
half of the article errors in the target essays (Table 6). Of the undetected article
errors (22 out of 62), only four were due to the misclassification of mass and
count nouns by the proposed method. The rest were article errors that were not
detected even if the mass count distinction was given. For example, extra definite
articles such as ?I like *the gardening.? cannot be detected even if whether the
noun ?gardening? is a mass noun or a count noun is given. Therefore, it is
necessary to exploit other sources of information than the mass count distinction
to detect these kinds of article error. For instance, exploiting the relation between
sentences could be used to detect these kinds of article error.
The proposed method outperformed the method using only the default rules
in both recall and precision. This means that words surrounding the target nouns
are good indicators of the mass count distinction. For example, the proposed
method correctly distinguished the target noun place in the phrase beautiful
place as a count noun by ?beautifulnp ? count? and detected an article error
from it whereas the method using only the default rules did not.
Table 6. Experimental results
Method Recall Precision
Proposed 0.65 0.70
Default only 0.60 0.69
Grammarian 0.13 1.00
9 Grammarian Pro X ver. 1.5: http://www.mercury-soft.com/
Detecting Article Errors Based on the Mass Count Distinction 825
In precision, the proposed method was outperformed by Grammarian; since
Grammarian is a commercial grammar checker, it seems to be precision-oriented.
The proposed method made 17 false-positives. Of the 17 false-positives, 13
were due to the misclassification of mass and count nouns by the proposed
method. Especially, the proposed method often made false-positives in idiomatic
phrases (e.g., by plane). This result implies that some methods for handling
idiomatic phrases may improve the performance. Four were due to the chun-
ker used to analyze the target essays. Since the chunker is designed for ana-
lyzing texts that contain no errors, it is possible that a chunker designed for
analyzing texts written by Japanese learners of English reduces this kind of
false-positive.
6 Conclusions
This paper has proposed a method for distinguishing mass and count nouns to
complement the conventional heuristic rules for detecting grammatical errors.
The experiments have shown that the proposed method distinguishes mass and
count nouns with a high accuracy (0.93) and that the recall and precision are
0.65 and 0.70, respectively. From the results, it follows that the proposed method
can complement the conventional heuristic rules for detecting grammatical errors
in the writing of Japanese learners of English.
The experiments have also shown that approximately 35% of article errors
in the target essays are not detected by the mass count distinction. For future
work, we will study methods for detecting the undetected article errors.
Acknowledgments
The authors would like to thank Sekine Satoshi who has developed the OAK
System. The authors also would like to thank three anonymous reviewers for
their advice on this paper.
References
1. Kawai, A., Sugihara, K., Sugie, N.: ASPEC-I: An error detection system for English
composition. IPSJ Journal (in Japanese) 25 (1984) 1072?1079
2. McCoy, K., Pennington, C., Suri, L.: English error correction: A syntactic user
model based on principled ?mal-rule? scoring. In: Proc. 5th International Confer-
ence on User Modeling. (1996) 69?66
3. Schneider, D., McCoy, K.: Recognizing syntactic errors in the writing of second
language learners. In: Proc. 17th International Conference on Computational Lin-
guistics. (1998) 1198?1204
4. Pelletier, F., Schubert, L.: Two theories for computing the logical form of mass
expressions. In: Proc.10th International Conference on Computational Linguistics.
(1984) 108?111
826 R. Nagata et al
5. Izumi, E., Uchimoto, K., Saiga, T., Supnithi, T., Isahara, H.: Automatic error
detection in the Japanese learners? English spoken data. In: Proc. 41st Annual
Meeting of the Association for Computational Linguistics. (2003) 145?148
6. Allan, K.: Nouns and countability. J. Linguistic Society of America 56 (1980)
541?567
7. Baldwin, T., Bond, F.: A plethora of methods for learning English countability.
In: Proc. 2003 Conference on Empirical Methods in Natural Language Processing.
(2003) 73?80
8. Baldwin, T., Bond, F.: Learning the countability of English nouns from corpus
data. In: Proc. 41st Annual Meeting of the Association for Computational Lin-
guistics. (2003) 463?470
9. Bond, F., Vatikiotis-Bateson, C.: Using an ontology to determine English count-
ability. In: Proc. 19th International Conference on Computational Linguistics.
(2002) 99?105
10. O?Hara, T., Salay, N., Witbrock, M., Schneider, D., Aldag, B., Bertolo, S., Panton,
K., Lehmann, F., Curtis, J., Smith, M., Baxter, D., Wagner, P.: Inducing criteria
for mass noun lexical mappings using the Cyc KB, and its extension to WordNet.
In: Proc. 5th International Workshop on Computational Semantics. (2003) 425?441
11. Lenat, D.: CYC: A large-scale investment in knowledge infrastructure. Communi-
cations of the ACM 38 (1995) 33?38
12. Huddleston, R., Pullum, G.: The Cambridge Grammar of the English Language.
Cambridge University Press, Cambridge (2002)
13. Rivest, R.: Learning decision lists. Machine Learning 2 (1987) 229?246
14. Gillon, B.: The lexical semantics of English count and mass nouns. In: Proc. Special
Interest Group on the Lexicon of the Association for Computational Linguistics.
(1996) 51?61
15. Yarowsky, D.: Unsupervised word sense disambiguation rivaling supervised meth-
ods. In: Proc. 33rd Annual Meeting of the Association for Computational Linguis-
tics. (1995) 189?196
16. Yarowsky, D.: Homograph Disambiguation in Speech Synthesis. Springer-Verlag
(1996)
17. Burnard, L.: Users Reference Guide for the British National Corpus. version 1.0.
Oxford University Computing Services, Oxford (1995)
18. Ostler, N., Atkins, B.: Predictable meaning shift: Some linguistic properties of
lexical implication rules. In: Proc. of 1st SIGLEX Workshop on Lexical Semantics
and Knowledge Representation. (1991) 87?100
19. Chodorow, M., Leacock, C.: An unsupervised method for detecting grammatical
errors. In: Proc. 1st Meeting of the North America Chapter of the Association for
Computational Linguistics. (2000) 140?147
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 241?248,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Feedback-Augmented Method for Detecting Errors in the Writing of
Learners of English
Ryo Nagata
Hyogo University of Teacher Education
6731494, Japan
rnagata@hyogo-u.ac.jp
Atsuo Kawai
Mie University
5148507, Japan
kawai@ai.info.mie-u.ac.jp
Koichiro Morihiro
Hyogo University of Teacher Education
6731494, Japan
mori@hyogo-u.ac.jp
Naoki Isu
Mie University
5148507, Japan
isu@ai.info.mie-u.ac.jp
Abstract
This paper proposes a method for detect-
ing errors in article usage and singular plu-
ral usage based on the mass count distinc-
tion. First, it learns decision lists from
training data generated automatically to
distinguish mass and count nouns. Then,
in order to improve its performance, it is
augmented by feedback that is obtained
from the writing of learners. Finally, it de-
tects errors by applying rules to the mass
count distinction. Experiments show that
it achieves a recall of 0.71 and a preci-
sion of 0.72 and outperforms other meth-
ods used for comparison when augmented
by feedback.
1 Introduction
Although several researchers (Kawai et al, 1984;
McCoy et al, 1996; Schneider and McCoy, 1998;
Tschichold et al, 1997) have shown that rule-
based methods are effective to detecting gram-
matical errors in the writing of learners of En-
glish, it has been pointed out that it is hard to
write rules for detecting errors concerning the ar-
ticles and singular plural usage. To be precise, it
is hard to write rules for distinguishing mass and
count nouns which are particularly important in
detecting these errors (Kawai et al, 1984). The
major reason for this is that whether a noun is a
mass noun or a count noun greatly depends on its
meaning or its surrounding context (refer to Al-
lan (1980) and Bond (2005) for details of the mass
count distinction).
The above errors are very common among
Japanese learners of English (Kawai et al, 1984;
Izumi et al, 2003). This is perhaps because the
Japanese language does not have a mass count dis-
tinction system similar to that of English. Thus, it
is favorable for error detection systems aiming at
Japanese learners to be capable of detecting these
errors. In other words, such systems need to some-
how distinguish mass and count nouns.
This paper proposes a method for distinguishing
mass and count nouns in context to complement
the conventional rules for detecting grammatical
errors. In this method, first, training data, which
consist of instances of mass and count nouns, are
automatically generated from a corpus. Then,
decision lists for distinguishing mass and count
nouns are learned from the training data. Finally,
the decision lists are used with the conventional
rules to detect the target errors.
The proposed method requires a corpus to learn
decision lists for distinguishing mass and count
nouns. General corpora such as newspaper ar-
ticles can be used for the purpose. However,
a drawback to it is that there are differences in
character between general corpora and the writ-
ing of non-native learners of English (Granger,
1998; Chodorow and Leacock, 2000). For in-
stance, Chodorow and Leacock (2000) point out
that the word concentrate is usually used as a noun
in a general corpus whereas it is a verb 91% of
the time in essays written by non-native learners
of English. Consequently, the differences affect
the performance of the proposed method.
In order to reduce the drawback, the proposed
method is augmented by feedback; it takes as feed-
back learners? essays whose errors are corrected
by a teacher of English (hereafter, referred to as
the feedback corpus). In essence, the feedback
corpus could be added to a general corpus to gen-
erate training data. Or, ideally training data could
be generated only from the feedback corpus just as
241
from a general corpus. However, this causes a se-
rious problem in practice since the size of the feed-
back corpus is normally far smaller than that of a
general corpus. To make it practical, this paper
discusses the problem and explores its solution.
The rest of this paper is structured as follows.
Section 2 describes the method for detecting the
target errors based on the mass count distinction.
Section 3 explains how the method is augmented
by feedback. Section 4 discusses experiments con-
ducted to evaluate the proposed method.
2 Method for detecting the target errors
2.1 Generating training data
First, instances of the target noun that head their
noun phrase (NP) are collected from a corpus with
their surrounding words. This can be simply done
by an existing chunker or parser.
Then, the collected instances are tagged with
mass or count by the following tagging rules. For
example, the underlined chicken:
... are a lot of chickens in the roost ...
is tagged as
... are a lot of chickens/count in the roost ...
because it is in plural form.
We have made tagging rules based on linguistic
knowledge (Huddleston and Pullum, 2002). Fig-
ure 1 and Table 1 represent the tagging rules. Fig-
ure 1 shows the framework of the tagging rules.
Each node in Figure 1 represents a question ap-
plied to the instance in question. For example, the
root node reads ?Is the instance in question plu-
ral??. Each leaf represents a result of the classi-
fication. For example, if the answer is yes at the
root node, the instance in question is tagged with
count. Otherwise, the question at the lower node
is applied and so on. The tagging rules do not
classify instances as mass or count in some cases.
These unclassified instances are tagged with the
symbol ???. Unfortunately, they cannot readily be
included in training data. For simplicity of imple-
mentation, they are excluded from training data1.
Note that the tagging rules can be used only for
generating training data. They cannot be used to
distinguish mass and count nouns in the writing
of learners of English for the purpose of detecting
1According to experiments we have conducted, approxi-
mately 30% of instances are tagged with ??? on average. It is
highly possible that performance of the proposed method will
improve if these instances are included in the training data.
the target errors since they are based on the articles
and the distinction between singular and plural.
Finally, the tagged instances are stored in a file
with their surrounding words. Each line of it con-
sists of one of the tagged instances and its sur-
rounding words as in the above chicken example.
2.2 Learning Decision Lists
In the proposed method, decision lists are used for
distinguishing mass and count nouns. One of the
reasons for the use of decision lists is that they
have been shown to be effective to the word sense
disambiguation task and the mass count distinc-
tion is highly related to word sense as we will see
in this section. Another reason is that rules for dis-
tinguishing mass and count nouns are observable
in decision lists, which helps understand and im-
prove the proposed method.
A decision list consists of a set of rules. Each
rule matches the template as follows:
If a condition is true, then a decision   (1)
To define the template in the proposed method,
let us have a look at the following two examples:
1. I read the paper.
2. The paper is made of hemp pulp.
The underlined papers in both sentences cannot
simply be classified as mass or count by the tag-
ging rules presented in Section 2.1 because both
are singular and modified by the definite article.
Nevertheless, we can tell that the former is a count
noun and the latter is a mass noun from the con-
texts. This suggests that the mass count distinc-
tion is often determined by words surrounding the
target noun. In example 1, we can tell that the pa-
per refers to something that can be read such as
a newspaper or a scientific paper from read, and
therefore it is a count noun. Likewise, in exam-
ple 2, we can tell that the paper refers to a certain
substance from made and pulp, and therefore it is
a mass noun.
Taking this observation into account, we define
the template based on words surrounding the tar-
get noun. To formalize the template, we will use
a random variable  that takes either 	 or

	 to denote that the target noun is a mass noun
or a count noun, respectively. We will also use
 and  to denote a word and a certain context
around the target noun, respectively. We define
242



























yes
yes
yes
yes
no
no
no
no










yes no
COUNT
modified by a little?
?
COUNT
MASS
? MASS
plural?
modified by one of the wordsin Table 1(a)?
modified by one of the wordsin Table 1(b)?
modified by one of the wordsin Table 1(c)?
Figure 1: Framework of the tagging rules
Table 1: Words used in the tagging rules
(a) (b) (c)
the indenite article much the denite article
another less demonstrative adjectives
one enough possessive adjectives
each sufficient interrogative adjectives
? ? quantiers
? ? ?s genitives
three types of  :  ,  , and Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 595?602,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Reinforcing English Countability Prediction with One Countability per
Discourse Property
Ryo Nagata
Hyogo University of Teacher Education
6731494, Japan
rnagata@hyogo-u.ac.jp
Atsuo Kawai
Mie University
5148507, Japan
kawai@ai.info.mie-u.ac.jp
Koichiro Morihiro
Hyogo University of Teacher Education
6731494, Japan
mori@hyogo-u.ac.jp
Naoki Isu
Mie University
5148507, Japan
isu@ai.info.mie-u.ac.jp
Abstract
Countability of English nouns is impor-
tant in various natural language process-
ing tasks. It especially plays an important
role in machine translation since it deter-
mines the range of possible determiners.
This paper proposes a method for reinforc-
ing countability prediction by introducing
a novel concept called one countability per
discourse. It claims that when a noun
appears more than once in a discourse,
they will all share the same countability in
the discourse. The basic idea of the pro-
posed method is that mispredictions can
be correctly overridden using efficiently
the one countability per discourse prop-
erty. Experiments show that the proposed
method successfully reinforces countabil-
ity prediction and outperforms other meth-
ods used for comparison.
1 Introduction
Countability of English nouns is important in var-
ious natural language processing tasks. It is par-
ticularly important in machine translation from
a source language that does not have an article
system similar to that of English, such as Chi-
nese and Japanese, into English since it determines
the range of possible determiners including arti-
cles. It also plays an important role in determining
whether a noun can take singular and plural forms.
Another useful application is to detect errors in ar-
ticle usage and singular/plural usage in the writing
of second language learners. Given countability,
these errors can be detected in many cases. For
example, an error can be detected from ?We have
a furniture.? given that the noun furniture is un-
countable since uncountable nouns do not tolerate
the indefinite article.
Because of the wide range of applications, re-
searchers have done a lot of work related to
countability. Baldwin and Bond (2003a; 2003b)
have proposed a method for automatically learn-
ing countability from corpus data. Lapata and
Keller (2005) and Peng and Araki (2005) have
proposed web-based models for learning count-
ability. Others including Bond and Vatikiotis-
Bateson (2002) and O?Hara et al (2003) use on-
tology to determine countability.
In the application to error detection, re-
searchers have explored alternative approaches
since sources of evidence for determining count-
ability are limited compared to other applications.
Articles and the singular/plural distinction, which
are informative for countability, cannot be used in
countability prediction aiming at detecting errors
in article usage and singular/plural usage. Return-
ing to the previous example, the countability of the
noun furniture cannot be determined as uncount-
able by the indefinite article; first, its countabil-
ity has to be predicted without the indefinite arti-
cle, and only then whether or not it tolerates the
indefinite article is examined using the predicted
countability. Also, unlike in machine translation,
the source language is not given in the writing of
second language learners such as essays, which
means that information available is limited.
To overcome these limitations, Nagata
et al (2005a) have proposed a method for
predicting countability that relies solely on words
(except articles and other determiners) surround-
ing the target noun. Nagata et al (2005b) have
shown that the method is effective to detecting
errors in article usage and singular/plural usage in
the writing of Japanese learners of English. They
595
also have shown that it is likely that performance
of the error detection will improve as accuracy of
the countability prediction increases since most of
false positives are due to mispredictions.
In this paper, we propose a method for reinforc-
ing countability prediction by introducing a novel
concept called one countability per discourse that
is an extension of one sense per discourse pro-
posed by Gale et al (1992). It claims that when
a noun appears more than once in a discourse,
they will all share the same countability in the dis-
course. The basic idea of the proposed method
is that initially mispredicted countability can be
corrected using efficiently the one countability per
discourse property.
The next section introduces the one countability
per discourse concept and shows that it can be a
good source of evidence for predicting countabil-
ity. Section 3 discusses how it can be efficiently
exploited to predict countability. Section 4 de-
scribes the proposed method. Section 5 describes
experiments conducted to evaluate the proposed
method and discusses the results.
2 One Countability per Discourse
One countability per discourse is an extension
of one sense per discourse proposed by Gale
et al (1992). One sense per discourse claims that
when a polysemous word appears more than once
in a discourse it is likely that they will all share
the same sense. Yarowsky (1995) tested the claim
on about 37,000 examples and found that when a
polysemous word appeared more than once in a
discourse, they took on the majority sense for the
discourse 99.8% of the time on average.
Based on one sense per discourse, we hypothe-
size that when a noun appears more than once in a
discourse, they will all share the same countability
in the discourse, that is, one countability per dis-
course. The motivation for this hypothesis is that
if one sense per discourse is satisfied, so is one
countability per discourse because countability is
often determined by word sense. For example, if
the noun paper appears in a discourse and it has
the sense of newspaper, which is countable, the
rest of papers in the discourse also have the same
sense according to one sense per discourse, and
thus they are also countable.
We tested this hypothesis on a set of nouns1
1The conditions of this test are shown in Section 5. Note
that although the source of the data is the same as in Section 5,
as Yarowsky (1995) did. We calculated how ac-
curately the majority countability for each dis-
course predicted countability of the nouns in the
discourse when they appeared more than once. If
the one countability per discourse property is al-
ways satisfied, the majority countability for each
discourse should predict countability with the ac-
curacy of 100%. In other others, the obtained ac-
curacy represents how often the one countability
per discourse property is satisfied.
Table 1 shows the results. ?MCD? in Table 1
stands for Majority Countability for Discourse and
its corresponding column denotes accuracy where
countability of individual nouns was predicted
by the majority countability for the discourse in
which they appeared. Also, ?Baseline? denotes
accuracy where it was predicted by the majority
countability for the whole corpus used in this test.
Table 1: Accuracy obtained by Majority Count-
ability for Discourse
Target noun MCD Baseline
advantage 0.772 0.618
aid 0.943 0.671
authority 0.864 0.771
building 0.850 0.811
cover 0.926 0.537
detail 0.829 0.763
discipline 0.877 0.652
duty 0.839 0.714
football 0.938 0.930
gold 0.929 0.929
hair 0.914 0.902
improvement 0.735 0.685
necessity 0.769 0.590
paper 0.807 0.647
reason 0.858 0.822
sausage 0.821 0.750
sleep 0.901 0.765
stomach 0.778 0.778
study 0.824 0.781
truth 0.783 0.724
use 0.877 0.871
work 0.861 0.777
worry 0.871 0.843
Average 0.851 0.754
Table 1 reveals that the one countability per dis-
discourses in which the target noun appears only once are
excluded from this test unlike in Section 5.
596
course property is a good source of evidence for
predicting countability compared to the baseline
while it is not as strong as the one sense per dis-
course property is. It also reveals that the tendency
of one countability per discourse varies from noun
to noun. For instance, nouns such as aid and
cover show a strong tendency while others such
as advantage and improvement do not. On aver-
age, ?MCD? achieves an improvement of approx-
imately 10% in accuracy over the baseline.
Having observed the results, it is reasonable to
exploit the one countability per discourse prop-
erty for predicting countability. In order to do
it, however, the following two questions should
be addressed. First, how can the majority count-
ability be obtained from a novel discourse? Since
our intention is to predict values of countability of
instances in a novel discourse, none of them are
known. Second, even if the majority countability
is known, how can it be efficiently exploited for
predicting countability? Although we could sim-
ply predict countability of individual instances of
a target noun in a discourse by the majority count-
ability for the discourse, it is highly possible that
this simple method will cause side effects consid-
ering the results in Table 1. These two questions
are addressed in the next section.
3 Basic Idea
3.1 How Can the Majority Countability be
Obtained from a Novel Discourse?
Although we do not know the true value of the ma-
jority countability for a novel discourse, we can
at least estimate it because we have a method for
predicting countability to be reinforced by the pro-
posed method. That is, we can predict countability
of the target noun in a novel discourse using the
method. Simply counting the results would give
the majority countability for it.
Here, we should note that countability of each
instance is not the true value but a predicted one.
Considering this fact, it is sensible to set a cer-
tain criterion in order to filter out spurious predic-
tions. Fortunately, most methods based on ma-
chine learning algorithms give predictions with
their confidences. We use the confidences as the
criterion. Namely, we only take account of predic-
tions whose confidences are greater than a certain
threshold when we estimate the majority count-
ability for a novel discourse.
3.2 How Can the Majority Countability be
Efficiently Exploited?
In order to efficiently exploit the one countabil-
ity per discourse property, we treat the majority
countability for each discourse as a feature in ad-
dition to other features extracted from instances of
the target noun. Doing so, we let a machine learn-
ing algorithm decide which features are relevant to
the prediction. If the majority countability feature
is relevant, the machine learning algorithm should
give a high weight to it compared to others.
To see this, let us suppose that we have a set
of discourses in which instances of the target noun
are tagged with their countability (either countable
or uncountable2) for the moment; we will describe
how to obtain it in Subsection 4.1. For each dis-
course, we can know its majority countability by
counting the numbers of countables and uncount-
ables. We can also generate a model for predicting
countability from the set of discourses using a ma-
chine learning algorithm. All we have to do is to
extract a set of training data from the tagged in-
stances and to apply a machine learning algorithm
to it. This is where the majority countability fea-
ture comes in. The majority countability for each
instance is added to its corresponding training data
as a feature to create a new set of training data be-
fore applying a machine learning algorithm; then
a machine learning algorithm is applied to the new
set. The resulting model takes the majority count-
ability feature into account as well as the other fea-
tures when making predictions.
It is important to exercise some care in count-
ing the majority countability for each discourse.
Note that one countability per discourse is always
satisfied in discourses where the target noun ap-
pears only once. This suggests that it is highly
possible that the resulting model too strongly fa-
vors the majority countability feature. To avoid
this, we could split the discourses into two sets,
one for where the target noun appears only once
and one for where it appears more than once, and
train a model on each set. However, we do not
take this strategy because we want to use as much
data as possible for training. As a compromise,
we approximate the majority countability for dis-
courses where the target noun appears only once
to the value unknown.
2This paper concentrates solely on countable and un-
countable nouns, since they account for the vast majority of
nouns (Lapata and Keller, 2005).
597
  
 
 
























yes
yes
yes
yes
no
no
no
no
 









yes no
COUNTABLE
modified by a little?
?
COUNTABLE
UNCOUNTABLE
? UNCOUNTABLE
plural?
modified by one of the wordsin Table 2(a)?
modified by one of the wordsin Table 2(b)?
modified by one of the wordsin Table 2(c)?
Figure 1: Framework of the tagging rules
Table 2: Words used in the tagging rules
(a) (b) (c)
the indenite article much the denite article
another less demonstrative adjectives
one enough possessive adjectives
each sufficient interrogative adjectives
? ? quantiers
? ? ?s genitives
4 Proposed Method
4.1 Generating Training Data
As discussed in Subsection 3.2, training data are
needed to exploit the one countability per dis-
course property. In other words, the proposed
method requires a set of discourses in which in-
stances of the target noun are tagged with their
countability. Fortunately, Nagata et al (2005b)
have proposed a method for tagging nouns with
their countability. This paper follows it to gener-
ate training data.
To generate training data, first, instances of the
target noun used as a head noun are collected from
a corpus with their surrounding words. This can be
simply done by an existing chunker or parser.
Second, the collected instances are tagged with
either countable or uncountable by tagging rules.
For example, the underlined paper:
... read a paper in the morning ...
is tagged as
... read a paper/countable in the morning ...
because it is modified by the indefinite article.
Figure 1 and Table 2 represent the tagging rules
based on Nagata et al (2005b)?s method. Fig-
ure 1 shows the framework of the tagging rules.
Each node in Figure 1 represents a question ap-
plied to the instance in question. For instance, the
root node reads ?Is the instance in question plu-
ral??. Each leaf represents a result of the classifi-
cation. For instance, if the answer is ?yes? at the
root node, the instance in question is tagged with
countable. Otherwise, the question at the lower
node is applied and so on. The tagging rules do
not classify instances in some cases. These unclas-
sified instances are tagged with the symbol ???.
Unfortunately, they cannot readily be included in
training data. For simplicity of implementation,
they are excluded from training data (we will dis-
cuss the use of these excluded data in Section 6).
Note that the tagging rules cannot be used for
countability prediction aiming at detecting errors
in article usage and singular/plural usage. The
reason is that they are useless in error detection
where whether determiners and the singular/plural
distinction are correct or not is unknown. Obvi-
ously, the tagging rules assume that the target text
contains no error.
Third, features are extracted from each instance.
As the features, the following three types of con-
textual cues are used: (i) words in the noun phrase
that the instance heads, (ii) three words to the left
of the noun phrase, and (iii) three words to its
right. Here, the words in Table 2 are excluded.
Also, function words (except prepositions) such
as pronouns, cardinal and quasi-cardinal numer-
598
als, and the target noun are excluded. All words
are reduced to their morphological stem and con-
verted entirely to lower case when collected. In
addition to the features, the majority countability
is used as a feature. For each discourse, the num-
bers of countables and uncountables are counted
to obtain its majority countability. In case of ties,
it is set to unknown. Also, it is set to unknown
when only one instance appears in the discourse
as explained in Subsection 3.2.
To illustrate feature extraction, let us consider
the following discourse (target noun: paper):
... writing a new paper/countable in his room ...
... read papers/countable with ...
The discourse would give a set of features:
-3=write, NP=new, +3=in, +3=room, MC=c
-3=read, +3=with, MC=c
where ?MC=c? denotes that the majority count-
ability for the discourse is countable. In this exam-
ple (and in the following examples), the features
are represented in a somewhat simplified manner
for the purpose of illustration. In practice, features
are represented as a vector.
Finally, the features are stored in a file with their
corresponding countability as training data. Each
piece of training data would be as follows:
-3=read, +3=with, MC=c, LABEL=c
where ?LABEL=c? denotes that the countability
for the instance is countable.
4.2 Model Generation
The model used in the proposed method can be re-
garded as a function. It takes as its input a feature
vector extracted from the instance in question and
predicts countability (either countable or uncount-
able). Formally, 	 
 where  ,  , and 

denote the model, the feature vector, and 
 ,
respectively; here, 0 and 1 correspond to count-
able and uncountable, respectively.
Given the specification, almost any kind of ma-
chine learning algorithm cab be used to generate
the model used in the proposed method. In this
paper, the Maximum Entropy (ME) algorithm is
used which has been shown to be effective in a
wide variety of natural language processing tasks.
Model generation is done by applying the ME
algorithm to the training data. The resulting model
takes account of the features including the major-
ity countability feature and is used for reinforcing
countability prediction.
4.3 Reinforcing Countability Prediction
Before explaining the reinforcement procedure, let
us introduce the following discourse for illustra-
tion (target noun: paper):
... writing paper in room ... wrote paper in ...
... submitted paper to ...
Note that articles and the singular/plural distinc-
tion are deliberately removed from the discourse.
This kind of situation can happen in machine
translation from a source language that does not
have articles and the singular/plural distinction3.
The situation is similar in the writing of second
language learners of English since they often omit
articles and the singular/plural distinction or use
improper ones. Here, suppose that the true values
of the countability for all instances are countable.
A method to be reinforced by the proposed
method would predict countability as follows:
... writing paper/countable (0.97) in room ...
... wrote paper/countable (0.98) in ...
... submitted paper/uncountable (0.57) to ...
where the numbers in brackets denote the confi-
dences given by the method. The third instance is
mistakenly predicted as uncountable4.
Now let us move on to the reinforcement pro-
cedure. It is divided into three steps. First, the
majority countability for the discourse in question
is estimated by counting the numbers of the pre-
dicted countables and uncountables whose confi-
dences are greater than a certain threshold. In case
of ties, the values of the majority countability is
set to unknown. In the above example, the major-
ity countability for the discourse is estimated to be
countable when the threshold is set to  (two
countables). Second, features explained in Sub-
section 4.1 are extracted from each instance. As
for the majority countability feature, the estimated
one is used. Returning to the above example, the
three instances would give a set of features:
-3=write, +3=in, +3=room, MC=c,
-3=write, +3=in, MC=c,
-3=submit, +3=to, MC=c.
Finally, the model generated in Subsection 4.2
is applied to the features to predict countability.
Because of the majority countability feature, it
3For instance, the Japanese language does not have an ar-
ticle system similar to that of English, neither does it mark
the singular/plural distinction.
4The reason would be that the contextual cues did not ap-
pear in the training data used in the method.
599
is likely that previous mispredictions are overrid-
den by correct ones. In the above example, the
third one would be correctly overridden by count-
able because of the majority countability feature
(MC=c) that is informative for the instance being
countable.
5 Experiments
5.1 Experimental Conditions
In the experiments, we chose Nagata
et al (2005a)?s method as the one to be re-
inforced by the proposed method. In this
method, the decision list (DL) learning algo-
rithm (Yarowsky, 1995) is used. However, we
used the ME algorithm because we found that the
method with the ME algorithm instead of the DL
learning algorithm performed better when trained
on the same training data.
As the target noun, we selected 23 nouns that
were also used in Nagata et al (2005a)?s experi-
ments. They are exemplified as nouns that are used
as both countable and uncountable by Huddleston
and Pullum (2002).
Training data were generated from the writ-
ten part of the British National Corpus (Burnard,
1995). A text tagged with the text tags was used
as a discourse unit. From the corpus, 314 texts,
which amounted to about 10% of all texts, were
randomly taken to obtain test data. The rest of
texts were used to generate training data.
We evaluated performance of prediction by ac-
curacy. We defined accuracy by the ratio of the
number of correct predictions to that of instances
of the target noun in the test data.
5.2 Experimental Procedures
First, we generated training data for each target
noun from the texts using the tagging rules ex-
plained in Subsection 4.1. We used the OAK sys-
tem5 to extract noun phrases and their heads. Of
the extracted instances, we excluded those that had
no contextual cues from the training data (and also
the test data). We also generated another set of
training data by removing the majority countabil-
ity features from them. This set of training data
was used for comparison.
Second, we obtained test data by applying the
tagging rules described in Subsection 4.1 to each
instance of the target noun in the 314 texts. Na-
gata et al (2005b) showed that the tagging rules
5http://www.cs.nyu.edu/ sekine/PROJECT/OAK/
achieved an accuracy of 0.997 in the texts that
contained no errors. Considering these results, we
used the tagging rules to obtain test data. Instances
tagged with ??? were excluded in the experiments.
Third, we applied the ME algorithm6 to the
training data without the majority countability fea-
ture. Using the resulting model, countability of
the target nouns in the test data was predicted.
Then, the predictions were reinforced by the pro-
posed method. The threshold to filter out spu-
rious predictions was set to  . For compar-
ison, the predictions obtained by the ME model
were simply replaced with the estimated majority
countability for each discourse. In this method, the
original predictions were used when the estimated
majority countability was unknown. Also, Nagata
et al (2005a)?s method that was based on the DL
learning algorithm was implemented for compari-
son.
Finally, we calculated accuracy of each method.
In addition to the results, we evaluated the baseline
on the same test data where all predictions were
done by the majority countability for the whole
corpus (training data).
5.3 Experimental Results and Discussion
Table 3 shows the accuracies7. ?ME? and ?Pro-
posed? in Table 3 refer to accuracies of the ME
model and the ME model reinforced by the pro-
posed method, respectively. ?ME+MCD? refers
to accuracy obtained by replacing predictions of
the ME model with the estimated majority count-
ability for each discourse. Also, ?DL? refers to
accuracy of the DL-based method.
Table 3 shows that the three ME-based meth-
ods (?Proposed?, ?ME?, and ?ME+MCD?) per-
form better than ?DL? and the baseline. Espe-
cially, ?Proposed? outperforms the other methods
in most of the target nouns.
Figure 2 summarizes the comparison between
the three ME-based methods. Each plot in Fig-
ure 2 represents each target noun. The horizon-
tal and vertical axises correspond to accuracy of
?ME? and that of ?Proposed? (or ?ME+MCD?),
respectively. The diagonal line corresponds to the
line ff