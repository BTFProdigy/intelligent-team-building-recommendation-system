Bayesian Network, a model for NLP?
Davy Weissenbacher
Laboratoire d?Informatique de Paris-Nord
Universite Paris-Nord
Villetaneuse, FRANCE
davy.weissenbacher@lipn.univ-paris13.fr
Abstract
The NLP systems often have low perfor-
mances because they rely on unreliable
and heterogeneous knowledge. We show
on the task of non-anaphoric it identifi-
cation how to overcome these handicaps
with the Bayesian Network (BN) formal-
ism. The first results are very encourag-
ing compared with the state-of-the-art sys-
tems.
1 Introduction
When a pronoun refers to a linguistic expression
previously introduced in the text, it is anaphoric.
In the sentence Nonexpression of the locus even
when it is present suggests that these chromo-
somes[...], the pronoun it refers to the referent
designated as ?the locus?. When it does not re-
fer to any referent, as in the sentence Thus, it is
not unexpected that this versatile cellular... the
pronoun is semantically empty or non-anaphoric.
Any anaphora resolution system starts by identi-
fying the pronoun occurrences and distinguishing
the anaphoric and non-anaphoric occurrences of it.
The first systems that tackled this classification
problem were based either on manually written
rules or on the automatic learning of relevant sur-
face clues. Whatever strategy is used, these sys-
tems see their performances limited by the quality
of knowledge they exploit, which is usually only
partially reliable and heterogeneous.
This article describes a new approach to go be-
yond the limits of traditional systems. This ap-
proach stands on the formalism, still little ex-
ploited for NLP, of Bayesian Network (BN). As
a probabilistic formalism, it offers a great expres-
sion capacity to integrate heterogeneous knowl-
edge in a single representation (Peshkin, 2003)
as well as an elegant mechanism to take into ac-
count an a priori estimation of their reliability in
the classification decision (Roth, 2002). In order
to validate our approach we carried out various ex-
periments on a corpus made up of abtsracts of ge-
nomic articles.
Section 2 presents the state of the art for the
automatic recognition of the non-anaphoric oc-
curences of it. Our BN-based approach is exposed
in section 3. The experiments are reported in sec-
tion 4, and results are discussed in section 5.
2 Identification of Non-anaphoric it
occurences
The decisions made by NLP systems depend on
the available knowledge. However this informa-
tion is often weakly reliable and leads to erroneous
or incomplete results.
One of first pronoun classifier system is pre-
sented by (Paice, 1987). It relies on a set of logical
first order rules to distinguish the non-anaphoric
occurences of the pronoun it. Non-anaphoric se-
quences share remarkable forms (they start with an
it and end with a delimiter like to, that, whether...).
The rules expresses some constraints which vary
according to the delimiter. They concern the left
context of the pronoun (it should not be immedi-
ately preceded by certain words like before, from
to), the distance between the pronoun and the de-
limiter (it must be shorter than 25 words long), and
finally the lexical items occurring between the pro-
noun and the delimiter (the sequence must or must
not contain certain words belonging to specific
sets, such as words expressing modality over the
sentence content, e.g. certain, known, unclear...).
Tests performed by Paice show good results with
195
91.4%Accuracy1 on a technical corpus. However
the performances are degraded if one applies them
to corpora of different natures: the number of false
positive increases.
In order to avoid this pitfall, (Lappin, 1994) pro-
poses some more constrained rules in the form of
finite state automata. Based on linguistic knowl-
edge the automata recognize specific sequences
like It is not/may be<Modaladj>; It is <Cogv-
ed> that <Subject> where <Modaladj> and
<Cogv> are modal adjective and cognitive verbs
classes known to introduce non-anaphoric it (e.g.
necessary, possible and recommend, think). This
system has a good precision (few false positive
cases), but has a low recall (many false negative
cases). Any sequence with a variation is ignored
by the automata and it is difficult to get exhaustive
adjective and verb semantic classes2. In the next
paragraphs we refer to Lappin rules? as Highly
Constraint rules (HC rules) and Paice rules? as
Lightly Constraint rules (LC rules).
(Evans, 2001) gives up the constraints brought
into play by these rules and proposes a machine
learning approach based on surface clues. The
training determines the relative weight of the vari-
ous corpus clues. Evans considers 35 syntactic and
contextual surface clues (e.g. pronoun position in
the sentence, lemma of the following verb) on a
manually annotated sample. The system classifies
the new it occurences by the k-nearest neighbor
method metric. The first tests achieve a satisfac-
tory score: 71.31%Acc on a general language cor-
pus. (Clement, 2004) carries out a similar test in
the genomic domain. He reduces the number of
Evans?s surface clues to the 21 most relevant ones
and classifies the new instances with a Support
Vector Machine(SVM). It obtains 92.71%Acc to
be compared with a 90.78%Acc score for the LC
rules on the same corpus. The difficulty, however,
comes from the fact that the information on which
1Accuracy(Acc) is a classifi cation measure:
Acc= P+NP+N+p+n where p is the number of anaphoric
pronoun occurences tagged as non-anaphoric, which we
call the false positive cases, n the number of non-anaphoric
pronoun ocurrences tagged as anaphoric, the false negative
cases. P and N are the numbers of correctly tagged
non-anaphoric and anaphoric pronoun occurences, the true
positive and negative cases respectively.
2For instance in the sentences It is well documented that
treatment of serum-grown... and It is generally accepted that
Bcl-2 exerts... the it occurences are not classifi ed as non-
anaphorics because documented does not belong to the origi-
nal verb class <Cogv> and generally does not appear in the
previous automaton.
the systems are built is often diverse and hetero-
geneous. This system is based on atomic surface
clues only and does not make use of the linguistic
knowledge or the relational information that the
constraints of the previous systems encode. We ar-
gue that these three types of knowledge that are the
HC rules, the LC rules, and the surfaces clues are
all relevant and complementary for the task and
that they must be unified in a single representation.
3 A Bayesian Network Based System
Contain
No?Contain
Contain?Known?Noun
Anaphoric?It
Non?anaphoric?It
Pronoun
Star
No?Start
Start?Proposition
Start
No?Start
Start?Sentence
Start
No?Start
Start?Abstract
No?match
Match
Left?Context?Constraints
Contain
No?Contain
Contain?Known?Adjective
Match
No?match
Superior?eleven
...
three
Inferior?three
Contain
No?Contain
More
Three
Two
One
Other
Preposition
Object
Subject
Grammatical?Role
Match
No?match
To
That
Whether?if
Which?Who
Other
Sequence?Length
LCR?Automata
Contain?Known?Verb
HCR?Automata
Unknown?Words
Delimitor
Figure 1: A Bayesian Network for identification
ofnon-anaphoric it occurrences
Neither the surface clues nor the surface clues
are reliable indicators of the pronoun status. They
encode heterogeneous pieces of information and
consequently produce different false negative and
positive cases. The HC rules have a good precision
but tag only few pronouns. On the opposite, the
LC rules, which have a good recall, are not precise
enough to be exploited as such and the additional
surface clues must be checked. Our model com-
bines these clues and take their respective reliabil-
ity in to account. It obtains better results than those
obtained from each clue exploited separately.
The BN is a model designed to deal with dubi-
ous pieces of information. It is based on a qualita-
tive description of their dependancy relationships,
a directed acyclic graph, and a set of condition-
nal probablities, each node being represented as
a Random Variable (RV). Parametrizing the BN
associates an a priori probability distribution to
196
the graph. Exploiting the BN (inference stage)
consists in propagating new pieces of informa-
tion through the network edges and updating them
according to observations (a posteriori probabili-
ties).
We integrated all the clues exploted by of the
previous methods within the same BN. We use de-
pendancy relationships to express the fact that two
clues are combined. The BN is manually designed
(choice of the RV values and graph structure). On
the Figure1, the nodes associated with the HC
rules method are marked in grey, white is for
the LC rules method and black for the Clement?s
method3. The Pronoun node estimates the de-
cision probability for a given it occurence to be
non-anaphoric.
The parameterising stage establishes the a pri-
ori probability values for all possible RV by sim-
ple frequency counts in a training corpus. They
express the weight of each piece of information in
the decision, its a priori reliability in the classifi-
cation decision4 . The inference stage exploits the
relationships for the propagation of the informa-
tion and the BN operates by information reinforce-
ment to label a pronoun. We applied all precedent
rules and checked surface clues on the sequence
containing the it occurrence and set observation
values to the correspondant RV probabilities. A
new probability is computed for the node?s vari-
able Pronoun: if it is superior or equal to 50%
the pronoun is labeled non-anaphoric, anaphoric
otherwise.
Let us consider the sentence extracted from our
corpus: It had previously been thought that ZE-
BRA?s capacity to disrupt EBV latency.... No HC
rule recognizes the sequence even by tolerating 3
unknown words 5, but a LC rule matches it with
4 words between the pronoun and the delimiter
that6. Among the surface clues, we checked that
the sequence is at the beginning of the sentence
3Only signifi cant surface clues for our modelisation have
been added to the BN.
4Among the 2000 it occurences of our training cor-
pus (see next section), the HC rules recognized 649
of the 727 non-anaphoric pronouns and they have er-
roneously recognized as non-anaphoric 17 pronouns, so
we set the HCR-rules node probabilities as P(HCR-
rules=Match|Pronoun=Non-Anaphoric)=89.2% and P(HCR-
rules=Match|Pronoun=Anaphoric)=1.3% which expresses
the expected value for the false negative cases and the false
positive cases produced by the HC rules respectively.
5So we set P(HC-rules = No-match)=1 and P(Unknown-
Words = More)=1.
6We set P(LC-rules = Match)=1, P(Sequence-Length =
four)=1 and P(Delimitor = That)=1.
Table 1: Prediction Results (Accuracy/False Posi-
tive Cases/False Negatives Cases)
Method Results
Highly Constraint Rules 88.11% / 12.8 / 169.1
Lightly Constraint Rules 88.88% / 123.6 / 24.2
Support Vector Machine 92.71% / - / -
Naive Bayesian Classifier 92.58% / 74.1 / 19.5
Bayesan Network 95.91% / 21.0 / 38.2
(1) but that the sentence is not the first of the ab-
stract (2). The sentence also contains the adverb
previously (3) and the verb think (4), which words
belong to our semantic classes7. The a priori
probability for the pronoun to be non-anaphoric is
36.2%. After modifying the probabilities of the
nodes of the BN according to the corpus obser-
vations, the a posteriori probability computed for
this occurence is 99.9% and the system classifies
it as non-anaphoric.
4 Experiments and Discussion
Medline is a database specialized in genomic re-
search articles. We extracted from it 11966 ab-
stracts with keywords bacillus subtilis, transcrip-
tion factors, Human, blood cells, gene and fu-
sion. Among these abstracts, we isolated 3347
occurences of the pronoun it and two human an-
notators tagged it occurences as either anaphoric
or non-anaphoric8 . After discussion, the two an-
notators achieved a total agreement.
We implemented the HC rules, LC rules and
surface clues using finite transducers and extracted
the pronoun syntactic role from the results of
the Link Parser analysis of the corpus (Aubin,
2005). As a working approximation, we automati-
caly generated the verb, adjective and noun classes
from the training corpus: among all it occurences
tagged as non-anaphoric, we selected the verbs,
adjectives and nouns occurring between the delim-
iter and the pronoun. We considered a third of the
corpus for training and the remaining for testing.
Our experiment was performed using 20-cross val-
idation.
Table1 summarizes the average results reached
7Others node values are set consequently.
8Corpus is available at http://www-lipn.univ-
paris13.fr/?weissenbacher/
197
by the state-of-the-art methods described above9.
The BN system achieved a better classification
than other methods.
In order to neutralize and comparatively quan-
tify the contribution in the decision of the depen-
dancy relationships between the factors, we have
implemented a Naive Bayesian Classifier (NBC)
which exploits the same pieces of knowledge and
the same parameters as the BN but it does not
profit from reinforcement mechanism, which leads
to a rise in the number of false positive cases.
Our BN, which has a good precision, never-
theless tags as non-anaphoric some occurrences
which are not. The most recurrent error corre-
sponds to the sequences ending with a delimiter
to recognized by some LC rules. Although none
HC rule matches the sequence, its minimal length
and the fact that it contains particular adjectives
or verbs like assumed or shown, makes this con-
figuration caracteristic enough to tag the pronoun
as non-anaphoric. When the delimiter is that, this
classification is correct 10 but it is always incorrect
when the delimiter is to11. For the delimiter to, the
rules must be more carefully designed.
Three different factors explain the false nega-
tive cases. Firstly, some sequences were ignored
because the delimiter remained implicit12. Sec-
ondly, the presence of apposition clauses increases
the sequence length and decreases the confidence.
Dedicated algorithms taking advantage of a deeper
syntactic analysis could resolve these cases. The
last cause is the non-exhaustiveness of the verb,
adjective and noun classes. It should be possible
to enrich them automatically. In our experiments
we have noticed that if a LC rule matches a se-
quence in the first clause of the first sentence in the
abstract then the pronoun is non-anaphoric. We
could automatically extract from Medline a large
number of such sentences and extend our classes
by selecting the verbs, adjectives and nouns occur-
ing between the pronoun and the delimiter in these
sentences.
5 Conclusion
Our system can of course be enhanced along the
previous axes. However, it is interesting to note
9We have completed the Clement?s SVM score for the
same biological corpus to compare its results with ours.
10Like in the sentence It is assumed that the SecY protein
of B. subtilis has multiple roles...
11Like in the sentence It is assumed to play a role in ...
12For example Thus, it appears T3SO4 has no intrinsic...
that it achieves better results than the comparable
state-of-the art systems, although it relies on the
same set of rules and surface clues. This com-
parison confirms the fact that the BN model pro-
poses an interesting way to combine the various
clues, some of then being only partially reliable.
We are continuing our work and expect to confirm
the contribution of BN to NLP problems on a task
which is more complex than the classification of it
occurences: the resolution of anaphora.
References
S. Aubin, A. Nazarenko and C. Nedellec. 2005.
Adapting a General Parser to a Sublanguage. Pro-
ceedings of the International Conference on Re-
cent Advances in Natural Language Processing
(RANLP?05), 1:89?93.
L. Clemente, K. Satou and K. Torisawa. 2004. Im-
proving the Identification of Non-anaphoric It Us-
ing Support Vector Machines. Actes d?International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications, 1:58?61.
I. Dagan and A. Itai. 1990. Automatic Processing
of Large Corpora for the Resolution of Anaphora
References. Proceedings of the 13th International
Conference on Computational Linguistics (COL-
ING?90), 3:1?3.
R. Evans. 2001. Applying Machine Learning Toward
an Automatic Classification of it. Literary and lin-
guistic computing, 16:45?57.
S. Lappin and H.J. Leass. 1994. An Algorithm for
Pronominal Anaphora Resolution. Computational
Linguistics, 20(4):535?561.
C.D. Paice and G.D. Husk. 1987. Towards the Auto-
matic Recognition of Anaphoric Features in English
Text: the Impersonal Pronoun It. Computer Speech
and Language, 2:109?132.
L. Peshkin and A. Pfeffer 2003. Bayesian Information
Extraction Network. In Proc.18th Int. Joint Conf.
Artifical Intelligence, 421?426.
D. Roth and Y. Wen-tau. 2002. Probalistic Reasoning
for Entity and Relation Recognition. Proceedings of
the 19th InternationalConference on Computational
Linguistics (COLING?02), 1:1?7.
198
Event-based Information Extraction for the biomedical domain: the Caderige project 
 
Erick Alphonse**, Sophie Aubin*, Philippe Bessi?res**, Gilles Bisson****, Thierry Hamon*, 
Sandrine Lagarrigue***, Adeline Nazarenko*, Alain-Pierre Manine**, Claire N?dellec**, 
Mohamed Ould Abdel Vetah**, Thierry Poibeau*, Davy Weissenbacher* 
 
*Laboratoire d?Informatique de Paris-Nord  
CNRS UMR 7030 
Av. J.B. Cl?ment 93430 F-Villetaneuse 
{firstname.lastname}@lipn.univ-paris13.fr 
**Laboratoire Math?matique, Informatique et G?nome (MIG), 
INRA,  
Domaine de Vilvert, 78352 F-Jouy-en-Josas 
{firstname.lastname}@jouy.inra.fr 
***Laboratoire de G?n?tique Animale,  
INRA-ENSAR 
Route de Saint Brieuc, 35042 Rennes Cedex 
lagarrig@roazhon.inra.fr 
****Laboratoire Leibniz ? UMR CNRS 5522  
46 Avenue F?lix Viallet - 38031 F-Grenoble Cedex 
Gilles.Bisson@imag.fr 
 
 Abstract  
This paper gives an overview of the 
Caderige project. This project involves 
teams from different areas (biology, 
machine learning, natural language 
processing) in order to develop high-
level analysis tools for extracting 
structured information from biological 
bibliographical databases, especially 
Medline. The paper gives an overview 
of the approach and compares it to the 
state of the art.  
1 Introduction 
Developments in biology and biomedicine are 
reported in large bibliographical databases 
either focused on a specific species (e.g. 
Flybase, specialized on Drosophilia 
Menogaster) or not (e.g. Medline). This type 
of  information sources is crucial for biologists 
but there is a lack of tools to explore them and 
extract relevant information. While recent 
named entity recognition tools have gained a 
certain success on these domains, event-based 
Information Extraction (IE) is still a challenge.  
The Caderige project aims at designing and 
integrating Natural Language Processing 
(NLP) and Machine Learning (ML) techniques 
to explore, analyze and extract targeted 
information in biological textual databases. We 
promote a corpus-based approach focusing on 
text pre-analysis and normalization: it is 
intended to drain out the linguistic variation 
dimension, as most as possible. Actually, the 
MUC (1995) conferences have demonstrated 
that extraction is more efficient when 
performed on normalized texts. The extraction 
patterns are thus easier to acquire or learn, 
more abstract and easier to maintain 
Beyond extraction patterns, it is also possible 
to acquire from the corpus, via ML methods, a 
part of the knowledge necessary for text 
normalization as shown here.  
This paper gives an overview of current 
research activities and achievements of the 
Caderige project. The paper first presents our 
approach and compares it with the one 
developed in the framework of a similar 
project called Genia (Collier et al 1999). We 
then propose an account of Caderige 
techniques on various filtering and 
normalization tasks, namely, sentence filtering, 
resolution of named entity synonymy, 
syntactic parsing, and ontology learning. 
Finally, we show how extraction patterns can 
be learned from normalized and annotated 
documents, all applied to biological texts.  
2 Description of our approach 
In this section, we give some details about the 
motivations and choices of implementation. 
We then briefly compare our approach with the 
one of the Genia project. 
43
2.1 Project organization 
The Caderige project is a multi disciplinary 
French research project on the automatic 
mining of textual data from the biomedical 
domain and is mainly exploratory orientated. It 
involved biology teams (INRA), computer 
science teams (LIPN, INRA and Leibniz-
IMAG) and NLP teams (LIPN) as major 
partners, plus LRI and INRIA from 2000 to 
2003. 
2.2 Project motivations 
Biologists can search bibliographic databases 
via the Internet, using keyword queries that 
retrieve a large superset of relevant papers. 
Alternatively, they can navigate through 
hyperlinks between genome databanks and 
referenced papers. To extract the requisite 
knowledge from the retrieved papers, they 
must identify the relevant abstracts or 
paragraphs. Such manual processing is time 
consuming and repetitive, because of the 
bibliography size, the relevant data sparseness, 
and the database continuous updating. From 
the Medline database, the focused query 
?Bacillus subtilis and transcription? which 
returned 2,209 abstracts in 2002, retrieves 
2,693 of them today. We chose this example 
because Bacillus subtilis is a model bacterium 
and transcription is a central phenomenon in 
functional genomics involved in genic 
interaction, a popular IE problem. 
GerE stimulates cotD transcription and 
inhibits cotA transcription in vitro by 
sigma K RNA polymerase, as expected from 
in vivo studies, and, unexpectedly, 
profoundly inhibits in vitro 
transcription of the gene (sigK) that 
encode sigma K. 
Figure 1: A sentence describing a genic interaction 
Once relevant abstracts have been retrieved, 
templates should be filled by hand since there 
is no available IE tool operational in genomics  
Type: positive 
Agent: GerE 
 
Interaction 
Target: transcription of the 
gene sigK 
Figure 2: A template describing a genic 
interaction. 
Still, applying IE ? la MUC to genomics and 
more generally to biology is not an easy task 
because IE systems require deep analysis 
methods to locate relevant fragments. As 
shown in the example in Figures 1 and 2, 
retrieving that GerE is the agent of the 
inhibition of the transcription of the gene sigK 
requires at least syntactic dependency analysis 
and coordination processing. In most of the 
genomics IE tasks (function, localization, 
homology) the methods should then combine 
the semantic-conceptual analysis of text 
understanding methods with IE through pattern 
matching. 
2.3 Comparison with the Genia project 
Our approach is very close to the one of the 
Genia project (Collier et al, 1999). Both 
projects rely on precise high-level linguistic 
analysis to be able to perform IE. The kind of 
information being searched is similar, 
concerning mainly gene and protein interaction 
as most of the research in this domain. The 
Genia corpus (Ohtae et al 2001) is not 
specialized on a specific species whereas ours 
is based on Bacillus Subtilis.  
Both projects develop annotation tools and 
Document Type Definition (DTD), which are, 
for the most part, compatible. The aim here is 
to build training corpus to which various 
techniques of NLP and ML are applied in 
order to acquire efficient event-based 
extraction patterns. The choice of ML and 
NLP methods differs but their aim is similar to 
our: normalizing text with predicate-arguments 
structures for learning better patterns. For 
example, Genia uses a combination of parsers 
to finally perform an HPSG-like analysis. The 
Caderige syntactic analysis is based on the 
specialization of the Link Parser (Sleator and 
Temperley, 1993 see section 4) to the 
biological domain.  
 In the following two sections, we detail our 
text filtering and normalization methods. 
Filtering aims at pruning the irrelevant part of 
the corpus while normalization aims at 
building an abstract representation of the 
relevant text. Section 4 is devoted to the 
acquisition of extraction patterns from the 
filtered and normalized text. 
3 Text filtering 
IR and text filtering are a prerequisite step to 
IE, as IE methods (including normalization and 
learning) cannot be applied to large and 
irrelevant corpora (they are not robust enough 
and they are computationally expensive). IR 
here is done through Medline interface by 
keyword queries for filtering the appropriate 
44
document subset. Then, text filtering, reduces 
the variability of textual data with the 
following assumptions: 
? desired information is local to sentences ; 
? relevant sentences contain at least two gene 
names. 
These hypotheses may lead to miss some genic 
interactions, but we assume that information 
redundancy is such that at least one instance of 
each interaction is contained into a single 
sentence in the corpus. The documents 
retrieved are thus segmented into sentences 
and the sentences with at least two gene names 
are selected. 
To identify the only relevant sentences among 
thoses,  classical supervised ML methods have 
been applied to a Bacillus Subtilis corpus in 
which relevant and irrelevant sentences had 
been annotated by a biological expert. Among 
SVMs, Na?ve Bayes (NB) methods, Neural 
Networks, decision trees (Marcotte et al, 
2001;  Nedellec et al, 2001), (Nedellec et al 
2001) demonstrates that  simple NB methods 
coupled with feature selection seem to perform 
well by yielding around 85 % precision and 
recall. Moreover, our first experiments show 
that the linguistic-based representation changes 
such as the use of lemmatization, terminology 
and named entities, do not lead to significant 
improvements. The relevant sentences filtered 
at this step are then used as input of the next 
tasks, normalization and IE. 
4 Normalization 
This section briefly presents three text 
normalization tasks: normalization of entity 
names, normalization of relations between text 
elements through syntactic dependency parsing 
and semantic labeling. The normalization 
process, by providing an abstract 
representation of the sentences, allows the 
identification of regularities that simplify the 
acquisition or learning of pattern rules. 
4.1 Entity names normalization 
Named Entity recognition is a critical point in 
biological text analysis, and a lot of work was 
previously done to detect gene names in text 
(Proux and al., 1998), (Fukuda and al., 1998). 
So, in Caderige, we do not develop any 
original NE extraction tool. We focus on a less 
studied problem that is synonyms recognition.  
Beyond typographical variations and 
abbreviations, biological entities often have 
several different names. Synonymy of gene 
names is a well-known problem, partly due to 
the huge amount of data manipulated (43.238 
references registered in Flybase for 
Drosophilia Melanogaster for example). Genes 
are often given a temporary name by a 
biologist. This name is then changed according 
to information on the concerned gene: for 
example SYGP-ORF50 is a gene name 
temporarily attributed by a sequencing project 
to the PMD1 yeast gene. We have shown that, 
in addition to available data in genomic 
database (GenBank, SwissProt,?), it is 
possible to acquire many synonymy relations 
with good precision through text analysis. By 
focusing on synonymy trigger phrases such as 
"also called" or "formerly", we can extract text 
fragments of that type :  gene trigger gene. 
However, the triggers themselves are subject to 
variation and the arguments of the synonymy 
relation must be precisely identified. We have 
shown that it is possible to define patterns to 
recognize synonymy expressions. These 
patterns have been trained on a representative 
set of sentences from Medline and then tested 
on a new corpus made of 106 sentences 
containing the keyword formerly. Results on 
the test corpus are the following: 97.5% 
precision, 75% recall. We chose to have a high 
precision since the acquired information must 
be valid for further acquisition steps 
(Weissenbacher, 2004).  
The approach that has been developed is very 
modular since abstract patterns like gene 
trigger gene (the trigger being a linguistic 
marker or a simple punctuation) can be 
instantiated by various linguistic items. A 
score can be computed for each instantiation of 
the pattern, during a learning phase on a large 
representative corpus. The use of a reduced 
tagged corpus and of a large untagged corpus 
justify the use of semi-supervised learning 
techniques.  
4.2  Sentence parsing 
The extraction of structured information from 
texts requires precise sentence parsing tools 
that exhibit relevant relation between domain 
entities. Contrary to (Akane et al 2001), we 
chose a partial parsing approach: the analysis 
is focused on relevant parts of texts and, from 
these chunks, on specific relations. Several 
reasons motivate this choice: among others, the 
fact that relevant information generally appears 
in predefined syntactic patterns and, moreover, 
45
the fact that we want to learn domain 
knowledge ontologies from specific syntactic 
relations (Faure and Nedellec, 2000 ; Bisson et 
al. 2000). 
First experiments have been done on several 
shallow parsers. It appeared that constituent 
based parsers are efficient to segment the text 
in syntactic phrases but fail to extract relevant 
functional relationships betweens phrases. 
Dependency grammars are more adequate 
since they try to establish links between heads 
of syntactic phrases. In addition, as described 
in Schneider (1998), dependency grammars are 
looser on word order, which is an advantage 
when working on  a domain specific language.  
Two dependency-based syntactic parsers have 
been tested (Aubin 2003): a hybrid commercial 
parser (henceforth HCP) that combines 
constituent and dependency analysis, and a 
pure dependency analyzer: the Link Parser.   
Prasad and Sarkar (2000) promote a twofold 
evaluation for parsers: on the one hand the use 
of a representative corpus and, on the other 
hand, the use of specific manually elaborated 
sentences. The idea is to evaluate analyzers on 
real data (corpus evaluation) and then to check 
the performance on specific syntactic 
phenomena. In this experiment, we chose to 
have only one corpus, made of sentences 
selected from the Medline corpus depending 
on their syntactic particularity. This strategy 
ensures representative results on real data. 
A set of syntactic relations was then selected 
and manually evaluated. This led to the results 
presented for major relations only in table 1. 
For each analyzer and relation, we compute a 
recall and precision score (recall = # relevant 
found relations / # relations to be found; 
precision = # relevant found relations / # 
relations found by the system).  
The Link Parser generally obtains better results 
than HCP. One reason is that a major 
particularity of our corpus (Medline abstracts) 
is that sentences are often (very) long (27 
words on average) and contain several clauses. 
The dependency analyzer is more accurate to 
identify relevant relationships between 
headwords whereas the constituent parser is 
lost in the sentence complexity. We finally 
opted for the Link Parser. Another advantage 
of the Link Parser is the possibility to modify 
its set of rules (see next subsection). The Link 
parser is currently used in INRA to extract 
syntactic relationships from texts in order to 
learn domain ontologies on the basis of a 
distributional analysis (Harris 1951, Faure and 
N?dellec, 1999).  
4.3 Recycling a general parser for biology 
During the evaluation tests, we noticed that 
some changes had to be applied either to the 
parser or to the text itself to improve the 
syntactic analysis of our biomedical corpus. 
The corpus needs to be preprocessed: sentence 
segmentation, named entities and terms 
recognition are thus performed using generic 
modules tuned for the biology domain
1
. Term 
recognition allows the removing of numerous 
structure ambiguities, which clearly benefits 
the parsing quality and execution time.  
                                                     
1
 A term analyser is currently being built at LIPN 
using existing term resources like Gene Ontology 
(see Hamon and Aubin, 2004). 
  Link Parser HCP 
Rel nbRel relOK R. RelTot P. RelOK R RelTot P. 
Subject 
18 13 0.72 19 0.68 14 0.78 20 0.65 
Object 
18 16 0.89 17 0.94 9 0.5 13 0.69 
Prep 
48 25 0.52 55 0.45 20 0.42 49 0.41 
V-GP1 
14 13 0.93 15 0.87 9 0.64 23 0.39 
O-GP 
16 7 0.43 12 0.58 12 0.75 28 0.43 
NofN 
16 13 0.81 15 0.87 14 0.87 26 0.54 
VtoV 
10 9 0.9 9 1 7 0.7 7 1 
VcooV 
10 8 0.8 9 0.89 6 0.6 6 1 
NcooN 
10 8 0.7 10 0.8 4 0.4 6 0.67 
nV-Adj 
10 8 0.8 9 0.89 0 0 0 1 
PaSim 
18 17 0.94 18 0.94 17 0.94 22 0.77 
PaRel 
12 11 0.92 11 1 8 0.67 11 0.73 
Table 1: Evaluation of two parsers on various syntactic relations 
Relations meaning: subject = subject-verb, Object = verb-object, Prep = prepositional phrase, V-GP = verb-prep. 
phrase, O-GP = Object- prep. phrase, NofN = Noun of noun, VtoV = Verb to Verb, VcooV = Verb coord. Verb, 
NcooN = Noun coord. Noun, nV-Adj = not + Verb or adjective, PaSim = passive form, PaRel = passive relative 
46
Concerning the Link Parser, we have manually 
introduced new rules and lexicon to allow the 
parsing of syntactic structures specific to the 
domain. For instance, the Latin-derived Noun 
Adjective phrase "Bacillus subtilis" has a 
structure inverse to the canonical English noun 
phrase (Adjective Noun). Another major task 
was to loosen the rules constraints because 
Medline abstracts are written by biologists 
who express themselves in sometimes broken 
English. A typical error is the omission of the 
determinant before some nouns that require 
one. We finally added words unknown to the 
original parser. 
4.4 Semantic labelling 
Asium software is used to semi-automatically 
acquire relevant semantic categories by 
distributional semantic analysis of parsed 
corpus. These categories contribute to text 
normalization at two levels, disambiguating 
syntactic parsing and typing entities and 
actions for IE. Asium is based on an original 
ascendant hierarchical clustering method that 
builds a hierarchy of semantic classes from the 
syntactic dependencies parsed in the training 
corpus. Manual validation is required in order 
to distinguish between different meanings 
expressed by identical syntactic structures. 
5 Extraction pattern learning 
Extraction pattern learning requires a training 
corpus from which the relevant and 
discriminant regularities can be automatically 
identified. This relies on two processes: text 
normalization that is domain-oriented but not 
task-oriented (as described in previous 
sections), and task-oriented annotation by the 
expert of the task.  
5.1 Annotation procedure 
The Caderige annotation language is based on 
XML and a specific DTD (Document Type 
Definition that can be used to annotate both 
prokaryote and eukaryote organisms by 50 
tags with up to 8 attributes. Such a precision is 
required for learning feasibility and extraction 
efficiency. Practically, each annotation aims at 
highlighting the set of words in the sentence 
describing: 
? Agents (A): the entities activating or 
controlling the interaction 
? Targets (T): the entities that are produced 
or controlled 
? Interaction (I): the kind of control 
performed during the interaction 
? Confidence (C): the confidence level in this 
interaction. 
The annotation of ?A low level of GerE 
activated transcription of CotD by GerE RNA 
polymerase in vitro ...? is given below. The 
attributes associated to the tag <GENIC-
INTERACTION> express the fact that the 
interaction is a transcriptional activation and 
that it is certain. The other tags (<IF>, 
<AF1>, ?) mark  the agent (AF1 and AF2), the 
target (TF1) and the interaction (IF). 
 
<GENIC-INTERACTION 
 id=?1?  
 type=?transcriptional?  
 assertion=?exist?  
 regulation=?activate?  
 uncertainty=?certain?  
 self-contained=?yes?  
 text-clarity=?good?> 
  <IF>A<I> low level </I>of</IF>     
  <AF1><A1  
     type=protein  
        role=modulate  
        direct=yes> GerE 
  </A1></AF1>,  
  <IF><I>activated</I> transcription  
      of</IF>    
     <TF1><T1 type=protein> CotD </T1>           
         </TF1> by   
     <AF2><A2  
           type=protein  
         role=required> 
       GerE RNA polymerase 
   </A2></AF2>,  
   <CF>but<C>in vitro</C></CF> 
</GENIC-INTERACTION> 
5.2 The annotation editor2 
Annotations cannot be processed in text form 
by biologists. The annotation framework 
developed by Caderige provide a general XML 
editor with a graphic interface for creating, 
checking and revising annotated documents. 
For instance, it displays the text with graphic 
attributes as defined in the editor XML style 
sheet, it allows to add the tags without strong 
constraint on the insertion order and it 
automatically performs some checking. 
The editor interface is composed of four main 
parts (see Figure 3). The editable text zone for 
annotation, the list of XML tags that can be 
used at a given time, the attributes zone to edit 
the values of the selected tag, and the XML 
                                                     
2
 Contact one of the authors if you are interested to 
use this annotation tool in a research project 
47
code currently generated. In the text zone, the 
above sentence is displayed as follows: 
A low level of GerE activated 
transcription of CotD by GerE RNA 
polymerase but in vitro 
This editor is currently used by some of the 
Caderige project partners and at SIB (Swiss 
Institute of BioInformatics) with another DTD, 
in the framework of the European BioMint 
project. Several corpora on various species 
have been annotated using this tool, mainly by 
biologists from INRA.  
5.3 Learning 
The vast majority of approaches relies on 
hand-written pattern rules that are based on 
shallow representations of the sentences (e.g. 
Ono et al, 2001). In Caderige, the deep 
analysis methods increase the complexity of 
the sentence representation, and thus of the IE 
patterns. ML techniques appear therefore very 
appealing to automate the process of rule 
acquisition (Freitag, 1998; Califf et al, 1998; 
Craven et al, 1999).  
Learning IE rules is seen as a discrimination  
task, where the concept to learn is a n-ary 
relation between arguments which correspond 
to the template fields. For example, the 
template in figure 2 can be filled by learning a 
ternary relation genic-interaction(X,Y,Z), 
where X,Y and Z are the type, the agent and 
the target of the interaction. The learning 
algorithm is provided with a set of positive and 
negative examples built from the sentences 
annotated and normalized. We use the 
relational learning algorithm, Propal (Alphonse 
et al, 2000). The appeal of using a relational 
method for this task is that it can naturally 
represent the relational structure of the 
syntactic dependencies in the normalized 
sentences and the background knowledge if 
needed, such as for instance semantic relations.  
For instance, the IE rules learned by Propal 
extract, from the following sentence :"In this 
mutant, expression of the spoIIG gene, whose 
transcription depends on both sigA and the 
phosphorylated Spo0A protein, Spo0AP, a 
major transcription factor during early stages 
of sporulation, was greatly reduced at 43 
degrees C.", successfully extract the two 
relations genic-interaction(positive, sigA, 
spoIIG) and genic-interaction(positive, 
Spo0AP, spoIIG). As preliminary experiments, 
we selected a subset of sentences as learning 
dataset, similar to this one. The performance of 
the learner evaluated by ten-fold cross-
validation is 69?6.5% of recall and 86?3.2% 
of precision. This result is encouraging, 
showing that the normalization process 
provides a good representation for learning IE 
rules with both high recall and high precision. 
6 Conclusion 
We have presented in this paper some results 
from the Caderige project. Two major issues 
are the development of a specific annotation 
editor for domain specialists and a set of 
machine learning and linguistic processing 
tools tuned for the biomedical domain.  
Current developments focus on the use of 
learning methods in the extraction process. 
These methods are introduced at different 
levels in the system architecture. A first use is 
Figure 3: the Caderige annotation editor 
48
the acquisition of domain knowledge to 
enhance the extraction phase. A second use 
concerns a dynamic adaptation of existing 
modules during the analysis according to 
specific features in a text or to specific text 
genres.  
7 References 
E. Agichtein and H. Yu (2003). Extracting 
synonymous gene and protein terms from 
biological literature. Bioinformatics, vol. 19 
Suppl.1, Oxford Press. 
E. Alphonse and C. Rouveirol (2000). Lazy 
propositionalisation for Relational  
Learning. In 14th European Conference on 
Artificial Intelligence (ECAI?00, W. Horn ed.), 
Berlin, pp. 256-260.  
S. Aubin (2003). ?valuation comparative de deux 
analyseurs produisant des relations syntaxiques. 
In workshop TALN and multilinguism. Batz-sur-
Mer. 
Y. Akane, Y. Tateisi, Y. Miyao and J. Tsujii. 
(2001). Event extraction from biomedical papers 
using a full parser. In Proceedings of the sixth 
Pacific Symposium on Biocomputing (PSB 2001). 
Hawaii, U.S.A.. pp. 408-419.  
G. Bisson, C. Nedellec, L. Ca?amero 2000. 
Designing clustering methods for ontology 
building: The Mo?K workbench. In Proceedings 
of Ontology Learning workshop (ECAI 2000), 
Berlin, 22 ao?t 2000.  
M. E. Califf, 1998. Relational Learning Techniques 
for Natural Language Extraction. Ph.D. 
Disseration, Computer Science Department, 
University of Texas, Austin, TX. AI Technical 
Report 98-276. 
N. Collier, Hyun Seok Park, Norihiro Ogata, Yuka 
Tateisi, Chikashi Nobata, Takeshi Sekimizu, 
Hisao Imai and Jun'ichi Tsujii. (1999). The 
GENIA project: corpus-based knowledge 
acquisition and information extraction from 
genome research papers. In Proceedings of the 
European Association for Computational 
Linguistics (EACL 1999). 
M. Craven et al, 1999. Constructing Biological 
Knowledge Bases by Extracting Information 
from Text Sources. ISMB 1999: 77-86 
D. Faure and C. Nedellec (1999). Knowledge 
acquisition of predicate argument structures from 
technical texts using Machine Learning: the 
system ASIUM. In EKAW'99, pp. 329-334, 
Springer-Verlag.  
D. Freitag, 1998, Multistrategy learning for 
information extraction. In Proceedings of the 
Fifteenth International Conference on Machine 
Learning, 161-169. Madison, WI: Morgan 
Kaufmann 
T. Hamon and S. Aubin (2004). Evaluating 
terminological resource coverage for relevant 
sentence selection and semantic class building. 
LIPN internal report. 
K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi 
(1998). Toward information extraction : 
identifying protein names from biological papers. 
Proceedings of the Pacific Symposium of 
Biocomputing, pp. 707-718. 
Z. Harris (1951). Methods in Structural Linguistics. 
Chicago. University of Chicago Press.  
E.M. Marcotte, I. Xenarios I., and D. Eisenberg 
(2001). Mining litterature for protein-protein 
interactions. In Bioinformatics, vo. 17 n? 4, 
pp. 359-363. 
MUC (1995) Proceeding of the 6
th
 Message 
understanding Conference. Morgan Kaufmann. 
Palo Alto.  
C. N?dellec, M. Ould Abdel Vetah and P. Bessi?res 
(2001). Sentence Filtering for Information 
Extraction in Genomics: A Classification 
Problem. In Proceedings of the International 
Conference on Practical Knowledge Discovery in 
Databases (PKDD?2001), pp. 326?338. Springer 
Verlag, LNAI 2167, Freiburg. 
T. Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima 
and Jun'ichi Tsujii. (2001). Ontology Based 
Corpus Annotation and Tools. In Proceedings of 
the 12th Genome Informatics 2001. pp. 469--470. 
T. Ono, H. Hishigaki, A. Tanigami and T. Takagi 
(2001). Automated extraction of information on 
protein-protein interactions from the biological  
literature. Bioinformatics. vol 17, n? 2, pp. 155-
161, Oxford Press. 
B. Prasad and A. Sarkar (2000) Comparing Test-
suite based evaluation and Corpus-based 
evaluation of a wide-coverage grammar for 
English. In Using Evaluation within Human 
Language Technology. LREC. Athens.  
D. Proux, F. Rechenmann, L. Julliard, V. Pillet, B. 
Jacq (1998). Detecting gene symbols and names 
in biological texts : a first step toward pertinent 
information extraction. In Genome Informatics, 
vol. 9, pp. 72-80. 
G. Schneider (1998). A Linguistic Comparison of 
Constituency, Dependency and Link Grammar. 
PhD thesis, Institut f?r Informatik der Universit?t 
Z?rich, Switzerland. 
D. Sleator and D. Temperley (1993). Parsing 
English with a Link Grammar. In Third 
International Workshop on Parsing 
Technologies. Tilburg. Netherlands. 
D. Weissenbacher (2004). La relation de 
synonymie en g?nomique. In Recital conference. 
Fes. 
49
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 1?9,
Baltimore, Maryland USA, June 26-27 2014. c?2014 Association for Computational Linguistics
Natural Language Processing Methods for Enhancing Geographic 
Metadata for Phylogeography of Zoonotic Viruses 
Tasnia Tahsin 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 ttahsin@asu.edu 
Rachel Beard 
Department of Biomedical  
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
rachel.beard@asu.edu 
Robert Rivera 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 rdriver1@asu.edu 
   
Rob Lauder 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 rlauder@asu.edu 
Davy Weissenbacher 
Department of Biomedical  
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
dweissen@asu.edu 
Garrick Wallstrom 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 gwallstrom@asu.edu 
 
Matthew Scotch 
Department of Biomedical Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 mscotch@asu.edu 
Graciela Gonzalez 
Department of Biomedical Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 Graciela.gonzalez@asu.edu 
  
Abstract 
Zoonotic viruses, viruses that are trans-
mittable between animals and humans, 
represent emerging or re-emerging patho-
gens that pose significant public health 
threats throughout the world. It is there-
fore crucial to advance current surveil-
lance mechanisms for these viruses 
through outlets such as phylogeography. 
Phylogeographic techniques may be ap-
plied to trace the origins and geographical 
distribution of these viruses using se-
quence and location data, which are often 
obtained from publicly available data-
bases such as GenBank. Despite the abun-
dance of zoonotic viral sequence data in 
GenBank records, phylogeographic anal-
ysis of these viruses is greatly limited by 
the lack of adequate geographic metadata. 
Although more detailed information may 
often be found in the related articles refer-
enced in these records, manual extraction 
of this information presents a severe bot-
tleneck. In this work, we propose an auto-
mated system for extracting this infor-
mation using Natural Language Pro-
cessing (NLP) methods. In order to vali-
date the need for such a system, we first 
determine the percentage of GenBank rec-
ords with ?insufficient? geographic 
metadata for seven well-studied zoonotic 
viruses. We then evaluate four different 
named entity recognition (NER) systems 
which may help in the automatic extrac-
tion of information from related articles 
that can be used to improve the GenBank 
geographic metadata. This includes a 
novel dictionary-based location tagging 
system that we introduce in this paper.  
1
1 Introduction 
Zoonotic viruses, viruses that are transmittable 
between animals and humans, have become in-
creasingly prevalent in the last century leading to 
the rise and re-emergence of a variety of diseases 
(Krauss, 2003). In order to enhance currently 
available surveillance systems for these viruses, a 
better understanding of their origins and transmis-
sion patterns is required. This need has led to a 
greater amount of research in the field of phylo-
geography, the study of geographical lineages of 
species (Avise, 2000). Population health agencies 
frequently apply phylogeographic techniques to 
trace the evolutionary changes within viral line-
ages that affect their diffusion and transmission 
among animal and human hosts (Ciccozzi et al., 
2013; Gray and Salemi, 2012; Weidmann et al., 
2013). Prediction of virus migration routes en-
hances the chances of isolating the viral strain for 
vaccine production. In addition, if the source of 
the strain is identified, intervention methods may 
be applied to block the virus at the source and 
limit outbreaks in other areas.  
Phylogeographic analysis depends on the utili-
zation of both the sequence data and the location 
of collection of specific viral sequences.  Re-
searchers often use publicly available databases 
such as GenBank for retrieving this information. 
For instance, Wallace and Fitch (2008) used data 
from GenBank records to study the migration of 
the H5N1 virus in various animal hosts over Eu-
rope, Asia and Africa, and were able to identify 
the Guangdong province in China as the source of 
the outbreak.  However, the extent of phylogeo-
graphic modeling is highly dependent on the spec-
ificity of available geospatial information and the 
lack of geographic data more specific than the 
state or province level may limit phylogeographic 
analysis and distort results. In the previous exam-
ple, Wallace and Fitch (2008) had to use town-
level information to identify the source of the 
H5N1 outbreak; without specific location data, 
they would not have been able to identify the 
Guangdong province as the source. Unfortu-
nately, while there is an abundance of sequence 
data in GenBank records, many of them lack suf-
ficient geographic metadata that would enable 
specific identification of the isolate?s location of 
collection. A prior study conducted by Scotch et 
al. (2011) showed that the geographic information 
of 80% of the GenBank records associated with 
single or double stranded RNA viruses within tet-
rapod hosts is less specific than 1st level adminis-
trative boundaries (ADM1) such as state or prov-
ince.  
Though many of the records lack specific geo-
graphic metadata, more detailed information is of-
ten available within the journal articles referenced 
in them. However, manual extraction of this infor-
mation is time-consuming and cumbersome and 
presents a severe bottleneck on phylogeographic 
analysis. In this work, we investigate the potential 
of NLP techniques to enhance the geographic data 
available for phylogeographic studies of zoonotic 
viruses using NER systems. In addition to geo-
graphic metadata and sequence information, Gen-
Bank records also contain several other forms of 
metadata such as host, collection date and gene for 
each isolate. Journal articles that are referenced in 
these records often mention the location of isola-
tion for the viral sample in conjunction with re-
lated metadata (Figure 1 provides an example of 
such a case). Therefore, by allowing identification 
of location mentions along with mentions of re-
lated GenBank metadata in these articles, we be-
lieve that NER systems may help to accurately 
link each GenBank record to its corresponding lo-
cation of isolation and distinguish it from other lo-
cation mentions.  
Previously Scotch et al. (2011) evaluated the 
performance of BANNER (Leaman and Gonza-
lez, 2008) and the Stanford NER tool (Finkel et 
al., 2005) for automated identification of gene and 
location mentions respectively, in 10 full-text 
PubMed articles, each related to a specific Gen-
Bank record. They were both found to achieve f-
scores of less than 0.45, thereby establishing the 
need for NER systems with better performance 
and/or a larger test corpus (Scotch et al, 2011). In 
this study, we start by evaluating the state of geo-
graphic insufficiency for zoonotic viruses in Gen-
Bank records using a new automated approach. 
Next, we further expand upon the work done by 
Scotch et al. (2011) by building our own diction-
ary-based location-tagging system and evaluating 
its performance on a larger corpus corresponding 
to over 8,500 GenBank records for zoonotic vi-
ruses. In addition, we also evaluate the perfor-
mance of three other state-of-the-art NER tools 
for tagging gene, date and species mentions in this 
corpus. We believe that identification of these en-
tities will be useful for the future development of 
a system for extracting the location of collection 
of viral isolates from articles related to their re-
spective GenBank records.   
 
 
2
 
                                GenBank Record 
 
                
 
 
Figure 1. Example of how the date, gene, and strain metadata within a GenBank record may be 
used to differentiate between two potential locations in a related article 
2 Methods 
The process undertaken to complete this study can 
be divided into three distinct stages: selection of 
the zoonotic viruses and extraction of relevant 
GenBank data related to each virus, computation 
of ?sufficiency? statistics on the extracted data, 
and development/evaluation of NER systems for 
tagging location, gene, date and species mentions 
in full-text PubMed Central articles. A detailed 
description of each phase is given below. 
2.1 Virus Selection and GenBank Data Ex-
traction 
The domain of this study has been limited to zo-
onotic viruses that are most consistently docu-
mented and tracked by public health, agriculture 
and wildlife state departments within the United 
States. These viruses include influenza, rabies, 
hantavirus, western equine encephalitis (WEE), 
eastern equine encephalitis (EEE), St. Louis en-
cephalitis (SLE), and West Nile virus (WNV). 
The Entrez Programming Utilities (E-Utilities) 
was used to download the following fields from 
59,595 GenBank records associated with these vi-
ruses: GenBank Accession ID, PubMed Central 
ID, Strain name, Collection date and Country. 
These records were the result of a query per-
formed to retrieve all accession numbers related 
to the selected viruses which had at least one ref-
erence to a PubMed Central article. The results 
                                                 
1 Iso.org. [Internet]. Gen?ve. c2013. Available from 
http://www.iso.org/iso/home/standards/country_codes.htm 
 
from the query was retrieved on August 22nd, 
2013.  
2.2 Sufficiency Analysis 
Database Integration: The data extracted from 
Genbank was used to compute the percentage of 
GenBank records that had insufficient geographic 
information for each of the selected viruses. In or-
der to perform this computation, we used data 
from the ISO 3166-1 alpha-2 1  table and the 
GeoNames database. The ISO 3166-1 alpha-2 is 
the International Standard for representing coun-
try names using two-letter codes. The GeoNames2 
database contains a variety of geospatial data for 
over 10 million locations on earth, including the 
ISO 3166-1 alpha-2 code for the country of each 
location and a feature code that can be used to de-
termine the administrative level of each location. 
To allow for efficient querying, we downloaded 
the main GeoNames table and the ISO alpha-2 
country codes table from their respective websites 
and stored them in a local SQL database. Prior to 
adding the ISO data to the database, some com-
monly used country names and their correspond-
ing country codes were added to the table since it 
only included a single title for each country. For 
example, the ISO table included the country name 
?United States? but not alternate names such as 
?USA?, ?United States of America?, or ?US?. Us-
ing the created database in conjunction with a par-
ser written in Java, we were able to retrieve most 
2 Geonames.org. [Internet]. Egypt. c2013. [updated 2013 
Apr 30] Available from http://www.geonamesorg/EG/ad-
ministrative-division-egypt.html 
Related PubMed Article 
3
of the geographic information present within the 
records and classify each of them as sufficient or 
insufficient.   
 
 
 
Figure 2. Sufficiency Criteria 
 
Sufficiency Criteria: For the purpose of this 
project, we considered any geographical bound-
ary more specific than ADM1 to be ?sufficient?. 
Based on this criterion, a feature code in 
GeoNames was categorized as sufficient only if it 
was absent from the following list of feature 
codes: ADM1, ADM1H, ADMD, ADMDH, PCL, 
PCLD, PCLF, PCLH, PCLI and PCLS. Evalua-
tion of the geographical sufficiency of a GenBank 
record was dependent upon whether the record in-
cluded a country name. A GenBank record with a 
country mention was called sufficient if the geo-
graphic information extracted from that record in-
cluded another place mention whose feature code 
fell within the class of sufficient feature codes and 
whose ISO country code matched that of the re-
trieved country. For instance, a GenBank record 
with the geographic metadata ?Orange County, 
United States? will be called sufficient since the 
place ?Orange County? has a sufficient feature 
code of ?ADM2? and a country code of ?US? 
which matches the country code of the retrieved 
country, ?United States?. Place mentions with 
matching country codes often had several differ-
ent feature codes in GeoNames. Such places were 
only called sufficient if all feature codes corre-
sponding to the given pair of place name and 
country code were classified as sufficient. In cases 
where the GenBank record had no country men-
tion, the record was called sufficient only if all 
matching GeoNames entries for any of the places 
mentioned in it had sufficient feature codes. The 
sufficiency criteria were designed to ensure that a 
geographic location is only called sufficient if its 
administrative level was found to be more specific 
than ADM1 without any form of ambiguity. Fig-
ure 3 illustrates the pathways of geographical suf-
ficiency for GenBank records in a diagram. 
Sufficiency Computation: In order to obtain 
the geographic information for each Genbank rec-
ord, we used a Java parser which automatically 
extracted data from the ?country? field of each 
record.  Since the ?country? field typically con-
tained multiple place mentions divided by a set of 
delimiters consisting of comma, colon and hy-
phen, we first split this field using these delimit-
ers.  We then checked each string obtained 
through this process against the ISO country code 
table to determine whether it was a potential coun-
try name for the record?s location.  If the query 
returned no results, then the locally stored 
GeoNames table was searched and for each match 
found, the corresponding ISO country code and 
feature code were extracted.  Figure 4 shows a di-
agram of this process. 
 
 
 
Figure 3. Sufficiency Calculation Example 
 
In cases where no sufficient location data was 
found from the ?country? field of a GenBank rec-
ord, the Java parser searched through its ?strain? 
field. This was done because some viral strains 
such as influenza include their location of origin 
integrated into their names. For example, the in-
fluenza strain ?A/duck/Alberta/35/76? indicates 
that the geographic origin of the strain is Alberta. 
The different sections of a strain field are sepa-
rated by either forward slash, parenthesis, comma, 
colon, hyphen or underscore and so we used a set 
of delimiters consisting of these characters to split 
this field. Each string thus retrieved was queried 
as before on the ISO country code table and the 
GeoNames table. GeoNames often returned 
matches for strings like ?raccoons? and ?chicken? 
which were actually meant to be names of host 
species within the ?strain? field, and so a list of 
4
 
 
Figure 4. Example of annotation including all four entities  
 
some of the most frequently seen host name men-
tions in these records was manually created and 
filtered out before querying GeoNames.  
Some of the place mentions contained very spe-
cific location information which resulted in 
GeoNames not finding a match for them. A list 
was created for strings like ?north?, ?south-east?, 
?governorate? etc. which when removed from a 
place mention may produce a match. In cases of 
potential place mentions which contained any one 
of these strings and for which GeoNames returned 
no matching result, a second query was performed 
after removal of the string. 
Evaluation of Sufficiency Computation: We 
manually annotated 10% of all influenza records 
in GenBank which reference at least one PubMed 
Central article as sufficient or insufficient based 
on our sufficiency criteria (5731 records). We 
then ran our program on these records and com-
pared system results with annotated results. 
2.3 Development/Evaluation of NER sys-
tems 
Creation of Gold Standard Corpus: We created 
a gold standard corpus consisting of twenty-seven 
manually-annotated full-text PubMed Central ar-
ticles in order to evaluate the performance of NER 
systems for tagging location, gene, species and 
date mentions in text. The articles corresponded to 
over 8,500 GenBank records and were randomly 
sampled using the subset of extracted GenBank 
records which contained a link to PubMed Central 
articles and had insufficient geographic metadata.  
Three annotators tagged the following four en-
tities in each article using the freely available an-
notation tool, BRAT (Stenetorp et al., 2012): gene 
names, locations, dates and species. Figure 4 pro-
vides an example of the manual annotation in 
BRAT. We annotated all mentions of each entity 
type, not only those relevant to zoonotic viruses, 
in order to evaluate system performance. A total 
of over 19,000 entities were annotated within this 
corpus. The number of tokens annotated was 
about 24,000. A set of annotation guidelines was 
created for this process (available upon request). 
Before creating the guidelines, each annotator in-
dividually annotated six common articles and 
compared and discussed their results to devise a 
reasonable set of rules for annotating each entity. 
After discussion, the annotators re-annotated the 
common articles based on the guidelines and di-
vided the remaining articles amongst themselves. 
The inter-annotator agreement was calculated for 
each pair of annotators. The annotated corpus will 
be made available at diego.asu.edu/downloads. 
Development of Automated Location Tag-
ger: We developed a dictionary-based NER sys-
tem using the GeoNames database for automated 
identification of location mentions in text. The 
dictionary used by this system, which we will 
hereby refer to as GeoNamer, was created by re-
trieving distinct place names from the GeoNames 
table and filtering out commonly used words from 
the retrieved set. Words filtered out include stop 
words such as ?are? and ?the?, generic place names 
such as ?cave? and ?hill?, numbers like ?one? and 
?two?, domain specific words such as ?biology? 
and ?DNA?, most commonly used surnames like 
?Garcia?, commonly used animal names such as 
?chicken? and ?fox? and other miscellaneous 
words such as ?central?. This was a crucial step 
since the GeoNames database contains a wide ar-
ray of commonly used English words which may 
cause a large volume of false positives if not re-
moved. The final dictionary consists of 5,396,503 
entries. In order to recognize place mentions in a 
5
given set of text files, GeoNamer first builds a Lu-
cene index on the contents of the files. It then con-
structs a phrase query for every entry in the 
Geonames dictionary and runs each query on the 
Lucene index. The document id, query text, start 
offset and end offset for every match found is 
written to an output file. We chose this approach 
because of its simplicity and efficiency.  
Evaluation of NER Systems: Four different 
NER systems for identifying species, gene, date 
and location mentions in text were evaluated us-
ing the created gold standard. The evaluated sys-
tems include LINNEAUS (Gerner et al., 2010), 
BANNER, Stanford SUTime (Chang and Man-
ning, 2012) and GeoNamer. LINNEAUS, BAN-
NER and Stanford SUTime are widely-used, 
state-of-the-art open source NER systems for 
recognition of species, gene and temporal expres-
sions respectively. GeoNamer is the system we 
developed in this work for the purpose of tagging 
locations, as described earlier.  
3 Results 
3.1 Sufficiency Analysis 
The system for classifying records as sufficient or 
insufficient was found to have an accuracy of 72% 
as compared to manual annotation.  98% of the 
errors was due to insufficient records being called 
sufficient. The results of the sufficiency analysis 
are given in Table 1. 64% of all GenBank records 
extracted for this project contained insufficient 
geographic information. Amongst the seven stud-
ied viruses, WEE had the highest and EEE had the 
lowest percentage of insufficient records.  
 
Virus 
Type  
Number of 
Entries  
% Insuffi-
cient  
WEE  67  90  
Rabies  4450  85  
WNV  1084  79  
SLE  141  74  
Hanta  1745  66  
Influenza  51734  62  
EEE  374  51  
All  59595  64  
 
Table 1. Percentage of GenBank records with in-
sufficient geographic information for each zoon-
otic virus studied in this project 
3.2 Gold Standard Corpus 
The results for the comparison of the annota-
tions performed by our three annotators on 6 com-
mon papers can be found in Table 2. We used the 
F-score between each pair of annotators as a 
measure of inter-rater agreement and had over 
90% agreement with overlap matching and over 
86% agreement with exact matching in all cases. 
The final gold standard corpus contained approx-
imately 19,000 entities corresponding to approxi-
mately 24,000 tokens. 
 
Entity F-score 
(A,B) 
(Exact; 
Overlap) 
F-score 
(?,?) 
(Exact; 
Overlap) 
F-score  
(?,?) 
(Exact; 
Overlap) 
Date .975; 
.978 
.979; 
.987 
.962; 
.973 
Gene .914; 
.926 
.913; 
.932 
.911; 
.954 
Location .945; 
.961 
.907; 
.931 
.914; 
.935 
Species .909; 
.956 
.874; 
.940 
.915; 
.959 
Virus .952; 
.958 
.947; 
.966 
.947; 
.955 
Mean .939; 
.956 
.924; 
.951 
.930; 
.955 
 
Table 2. Frequency of Annotated Entities for 6 
common annotated papers 
3.3 Performance Analysis of NER Systems 
The performance metrics for the NER systems 
at tagging the desired entities in the test set are 
listed in Table 3. The highest performance was 
achieved by Stanford SUTime for date tagging. 
Tagging of genes had the lowest performance. 
 
Entity  Precision  
(Exact; 
Overlap)  
Recall 
(Exact; 
Overlap)  
F-score  
(Exact; 
Overlap)  
BAN-
NER  
0.070; 
0.239  
0.114; 
0.395  
0.087; 
0.297  
Geo-
Namer 
0.452; 
0.626  
0.658; 
0.783  
0.536; 
0.696  
LIN-
NEAUS  
0.853; 
0.962  
0.563; 
0.658  
0.678; 
0.781  
Stanford 
SUTime 
0.800; 
0853  
0.681; 
0.727  
0.736; 
0.785  
 
Table 3. Performance Statistics of NER 
 
6
4 Discussion 
Based on our analysis, at least half of the Gen-
Bank records for each of the studied zoonotic vi-
ruses lack sufficient geographic information, and 
the proportion of insufficient records can be as 
high as 90%. Our automated system for classify-
ing records as insufficient or sufficient was found 
to have an accuracy of 72% with 98% of the errors 
being a result of insufficient records being called 
sufficient. Therefore, our computed estimate of 
insufficiency is very likely to be an underestima-
tion of the actual problem. The virus with the 
highest level of sufficiency, EEE, had a large 
number of records with county level information 
in the ?country? field. However, the insufficient 
records for this virus typically contained no place 
mention, not even at the country level. A key rea-
son for our calculated percentage of sufficient 
GenBank records being higher for these seven vi-
ruses than what has been previously computed by 
Scotch et al. (2011) was the inclusion of the 
?strain? field. The ?strain? field often contained 
specific location information which, when com-
bined with place mentions present within the 
?country? field, made the record geographically 
sufficient. The virus for which the inclusion of 
?strain? field had the greatest impact on boosting 
the sufficiency percentage was influenza. Most of 
the GenBank records associated with this virus 
had structured ?strain? fields from which the par-
ser could easily separate place mentions using 
GeoNames. 
Although the sufficiency classifications pro-
duced by our system were correct most of the 
time, there were a few cases where a record got 
incorrectly labeled as insufficient even when it 
contained detailed geographic information. This 
typically happened because GeoNames failed to 
return matching results for these places. For in-
stance, the country field ?India: Majiara,WB? was 
not found to be sufficient even though Majiara is 
a city in India because GeoNames has no entry for 
it. In some cases the lack of matching result was 
due to spelling variations of the place name. For 
instance the country field ?Indonesia: Yogjakarta? 
was called insufficient since ?Yogjakarta? is 
spelled as ?Yogyakarta? in GeoNames. Some-
times the database simply did not contain the ex-
act string present in the GenBank record. For in-
stance, it does not have any entry for the place 
?south Kalimantan? but it contains the place name 
?kalimantan?. The number of sufficient records 
which were called insufficient by our system due 
to inexact matching were greatly mitigated by re-
moving strings such as ?south? from the place 
mention, as described in the ?Methods? section. 
Most of the NER systems performed signifi-
cantly better with overlap measures than with ex-
act-match measures. This is because our annota-
tion guidelines typically involved tagging the 
longest possible match for each entity and the au-
tomated systems frequently missed portions of 
each annotation. Stanford SUTime had the best 
overlap f-measure of 0.785, closely followed by 
LINNEAUS with an overlap f-measure of 0.781. 
Although Stanford SUTime was fairly effective at 
finding date mentions in text, it tagged all four-
digit-numbers such as ?1012? and ?2339? as 
years, leading to a number of false positives. The 
poor recall of LINNEAUS was mostly caused be-
cause the dictionary used by LINNEAUS tagged 
only species mentions in text while we tagged ge-
nus and family mentions as well. It also missed a 
lot of commonly used animal names such as mon-
key, bat, badger and wolf. GeoNamer was the 
third best performer with the highest recall but 
second lowest precision. This is because the 
GeoNames dictionary contains an extensively 
large list of location names, many of which are 
commonly used words such as ?central?. Even 
though we filtered out a vast majority of these 
words, it still produced false positives such as 
?wizard?. However, its performance was consid-
erably better than that of the Stanford location tag-
ger used by Scotch et al. (2011) which was found 
to have a recall, precision and f-score of 0.26, 0.81 
and 0.39 respectively. The improved performance 
was achieved because of the higher recall of our 
system. The GeoNames dictionary provides an 
extensive coverage of all location mentions in the 
world and the Stanford NER system, which is a 
CRF classifier trained on a different dataset, was 
not able to recognize many of the place mentions 
present in full-text PMC articles related to Gen-
Bank records.  
BANNER showed the poorest performance 
amongst all the entity taggers evaluated in this pa-
per. In fact, the f-score we achieved for BANNER 
in this study was much lower that its past f-score 
of 0.42 within the domain of articles related to 
GenBank records for viral isolates (Scotch et al., 
2011).  As mentioned by Scotch et al. (2011), a 
key reason for BANNER?s poor performance in 
this domain is the difference between the data set 
used to train the BANNER model and the annota-
tion corpus used to test this system. The version 
of BANNER used in these two studies was trained 
on the training set for the BioCreative 2 Gene 
7
Mention task, which comprised of 15,000 sen-
tences from PubMed abstracts. These abstracts of-
ten contained the full names for gene and protein 
mentions while the full-text articles we used 
mostly contained the abbreviated forms of gene 
names, which BANNER tended to miss. The arti-
cles also contained abbreviated forms of several 
entities such as viral strain name (e.g. H1N1) and 
species name (e.g. VEEV) which look similar to 
abbreviated gene names. Therefore, BANNER of-
ten misclassified these entities as gene mentions. 
A possible reason for BANNER having a much 
lower performance in this study than in the previ-
ous study conducted by Scotch et al (2011) is the 
presence of a large number of tables in the journal 
articles we selected. BANNER is a machine learn-
ing system based on conditional random fields 
which uses orthographic, morphological and shal-
low syntax features extracted from sentences to 
identify gene mentions in text. Such features do 
not help greatly for extraction from tables. There-
fore, BANNER was often not able to identify the 
gene mentions in the tables present within our cor-
pus, thereby producing false negatives. Moreover, 
it tagged several entries within the table as a single 
gene name, thereby producing false positives as 
well. This reduced both the recall and precision of 
BANNER. 
Although this study explores the problem of in-
sufficient geographic information in GenBank 
more thoroughly than past studies, the number of 
papers annotated as the gold standard is still lim-
ited. Thus, the performance of the taggers re-
ported can be construed as a preliminary estimate 
at best. The set of taggers and their performance 
seem to be adequate for a large-scale application, 
with the exception of BANNER. However, we did 
not make any changes to the BANNER system 
(specifically, re-training) since changes to it are 
not possible until sufficient data is annotated for 
retraining. 
5 Conclusions and Future Work 
It can be concluded that the majority of Gen-
Bank records for zoonotic viruses do not contain 
sufficient geographic information concerning 
their origin. In order to enable phylogeographic 
analysis of these viruses and thereby monitor their 
spread, it is essential to develop an efficient mech-
anism for extracting this information from pub-
lished articles. Automated NER systems may help 
accelerate this process significantly. Our results 
indicate that the NER systems LINNEAUS, Stan-
ford SUTime and GeoNamer produce satisfactory 
performance in this domain and thus can be used 
in the future for linking GenBank records with 
their corresponding geographic information. 
However, the current version of BANNER is not 
well-suited for this task. We will need to train 
BANNER specifically for this purpose before in-
corporating it within our system. 
We are currently altering the component of our 
program which classifies records as sufficient or 
insufficient in order to reduce the number of errors 
due to insufficient records being called sufficient. 
We are also manually looking through GenBank 
records for zoonotic viruses with insufficient geo-
graphic metadata and linking them to the location 
mentions in related articles which we deem to be 
the most likely location of collection for the given 
viral isolate. The resulting annotated corpus will 
be used to train and evaluate an automated system 
for populating GenBank geographic metadata. 
We have already covered all GenBank records re-
lated to Encephalitis viruses and close to 10% of 
all records related to Influenza which are linked to 
PubMed Central articles. The annotation process 
has revealed that a large proportion of the infor-
mation allowing linkage of GenBank records to 
geographic metadata is often present in tables 
within the articles in addition to textual sentences. 
Therefore, we have developed a Python parser for 
automatically linking GenBank records to loca-
tion mentions using tables from the HTML ver-
sion of the PubMed Central articles.  Future work 
will include further expansion of this annotation 
corpus and the development of an integrated sys-
tem for enhancing GenBank geographic metadata 
for phylogeographic analysis of zoonotic viruses.  
Acknowledgement 
Research reported in this publication was sup-
ported by the NIAID of the NIH under Award 
Number R56AI102559 to MS and GG. The con-
tent is solely the responsibility of the authors and 
does not necessarily represent the official views 
of the National Institutes of Health 
 
 
 
 
 
 
 
 
8
 References 
Avise, John C. (2000). Phylogeography : the history 
and formation of species Cambridge, Mass.: Harvard 
University Press. 
Chang, Angel X., and Christopher Manning. "SUTime: 
A library for recognizing and normalizing time ex-
pressions." LREC. 2012. 
Ciccozzi M, et al. Epidemiological history and phylo-
geography of West Nile virus lineage 2. Infection, 
Genetics and Evolution. 2013:17;46-50. 
Finkel JR, Grenager T, Manning C. Incorporating non-
local information into information extraction sys-
tems by Gibbs sampling. In: Proceedings of the 43rd 
annual meeting of the association for computational 
linguistics (ACL 2005); 2005. p. 363?70. 
Gerner M, Nenadic G, and Bergman CM. LINNAEUS: 
A species name identification system for biomedical 
literature. BMC Bioinformatics. 2010;11(85). 
Gray RR, and Salemi M. Integrative molecular phylo-
geography in the context of infectious diseases on 
the human-animal interface. Parasitology-Cam-
bridge. 2012;139:1939-1951 
Krauss, H. (2003). Zoonoses: infectious diseases trans-
missible from animals to humans (3rd ed.). Wash-
ington, D.C.: ASM Press. 
Leaman R and Gonzalez G. BANNER: An executable 
survey of advances in biomedical named entity 
recognition. Pacific Symposium on Biocomputing. 
2008;13:652-663. 
Scotch, Matthew, et al. Enhancing phylogeography by 
improving geographical information from GenBank. 
Journal of biomedical informatics. 2011;44:S44-
S47. 
Stenetorp P, et al. BRAT: A Web-based Tool for NLP-
Assisted Text Annotation. EACL '12 Proceedings 
Wallace, R.G. and W.M. Fitch, Influenza A H5N1 im-
migration is filtered out at some international bor-
ders. PLoS One, 2008. 3(2): p. e1697. 
Weidmann M, et al. Molecular phylogeography of tick-
borne encephalitis virus in Central Europe. Journal 
of General Virology. 2013;94:2129-2139. 
9
