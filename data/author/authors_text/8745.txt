Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1170?1174,
Prague, June 2007. c?2007 Association for Computational Linguistics
Adapting the RASP System for the CoNLL07 Domain-Adaptation Task
Rebecca Watson and Ted Briscoe
Computer Laboratory
University of Cambridge
FirstName.LastName@cl.cam.ac.uk
Abstract
We describe our submission to the domain
adaptation track of the CoNLL07 shared
task in the open class for systems using ex-
ternal resources. Our main finding was that
it was very difficult to map from the annota-
tion scheme used to prepare training and de-
velopment data to one that could be used to
effectively train and adapt the RASP system
unlexicalized parse ranking model. Never-
theless, we were able to demonstrate a sig-
nificant improvement in performance utiliz-
ing bootstrapping over the PBIOTB data.
1 Introduction
The CoNLL07 domain adaptation task was created
to explore how a parser trained in one domain might
be adapted to a new one. The training data were
drawn from the PTB (Marcus et al, 1993) rean-
notated with dependency relations (Johansson and
Nugues, 2007, hereafter DRs). The test data were
taken from a corpus of biomedical articles (Kulick
et al, 2004) and the CHILDES database (Brown,
1973; MacWhinney, 2000) also reannotated with
DRs (see Nivre et al, 2007) for further details of
the task, annotation format, and evaluation scheme.
The development data consisted of a small amount
of annotated and unannotated biomedical and con-
versational data.
The RASP system (Briscoe et al, 2006) utilizes
a manually-developed grammar and outputs gram-
matical bilexical dependency relations (see Briscoe,
2006 for a detailed description, hereafter GRs). Wat-
son et al (2007) describe a semi-supervised, boot-
strapping approach to training the parser which uti-
lizes unlabelled partially-bracketed input with re-
spect to the system derivations. For the domain
adaptation task we retrained RASP by mapping our
GR scheme to the DR scheme and annotation for-
mat, and used this mapping to select a derivation
to train the unlexicalized parse ranking model from
the annotated PTB training data. We also performed
similar partially-supervised bootstrapping over the
200 annotated biomedical sentences in the develop-
ment data. We then tried unsupervised bootstrap-
ping from the unannotated development data based
on these initial models.
As the parser requires input to consist of a se-
quence of one of 150 CLAWS PoS tags, we also uti-
lize a first-order HMM PoS tagger which has been
trained on manually-annotated data from the LOB,
BNC and Susanne Corpora (see Briscoe, 2006 for
further details). Accordingly, we submitted our re-
sults in the open class.
2 Training and Adaptation
The RASP parser is a generalized LR parser which
builds a non-deterministic generalized LALR(1)
parse table from the grammar (Tomita, 1987). A
context-free ?backbone? is automatically derived
from a unification grammar. The residue of fea-
tures not incorporated into the backbone are unified
on each reduce action and if unification fails the as-
sociated derivation paths also fail. The parser cre-
ates a packed parse forest represented as a graph-
structured stack.
Inui et al (1997) describe the probability model
1170
utilized in the system where a transition is repre-
sented by the probability of moving from one stack
state, ?i?1, (an instance of the graph structured
stack) to another, ?i. They estimate this probability
using the stack-top state si?1, next input symbol li
and next action ai. This probability is conditioned
on the type of state si?1. Ss and Sr are mutually
exclusive sets of states which represent those states
reached after shift or reduce actions, respectively.
The probability of an action is estimated as:
P (li, ai, ?i|?i?1) ?
{
P (li, ai|si?1) si?1 ? Ss
P (ai|si?1, li) si?1 ? Sr
}
Therefore, normalization is performed over all
lookaheads for a state or over each lookahead for the
state depending on whether the state is a member of
Ss or Sr, respectively. In addition, Laplace estima-
tion can be used to ensure that all actions in the table
are assigned a non-zero probability.
These probabilities are estimated from counts
of actions which yield derivations compatible with
training data. We use a confidence-based self-
training approach to select derivations compatible
with the annotation of the training and development
data to train the model. In Watson et al (2007), we
utilized unlabelled partially-bracketed training data
as the starting point for this semi-supervised train-
ing process. Here we start from the DR-annotated
training data, map it to GRs, and then find the one
or more derivations in our grammar which yield GR
output consistent with the GRs recovered from the
DR scheme. Following Watson et al (2007), we
utilize the subset of sentences in the training data
for which there is a single derivation consistent with
this mapping to build an initial trained parse ranking
model. Then we use this model to rank the deriva-
tions consistent with the mapping in the portion of
the training data which remains ambiguous given
the mapping. We then train a new model based on
counts from these consistent derivations which are
weighted in somemanner by our confidence in them,
given both the degree of remaining ambiguity and
also the ranking and/or derivation probabilities pro-
vided by the initial model.
Thus, the first and hardest step was to map the
DR scheme to our GR scheme. Issues concerning
this mapping are discussed in section 4. Given this
mapping, we determined the subset of sentences in
the (PTB) training data for which there was a sin-
gle derivation in the grammar compatible with the
set of mapped GRs. These derivations were used
to create the initial trained model (B) from the uni-
form model (A). To evaluate the performance of
these and subsequent models, we tested them using
our own GR-based evaluation scheme over 560 sen-
tences from our reannotated version of DepBank, a
subset of section 23 of the WSJ PTB (see Briscoe
& Carroll, 2006). Table 1 gives the unlabelled pre-
cision, recall and microaveraged F1 score of these
models over this data. Model B was used to rerank
derivations compatible with the mapped GRs recov-
ered for the PTB training data. Model C was built
from the weighted counts of actions in the initial set
of unambiguous data and from the highest-ranked
derivations over the training data (i.e. we do not in-
clude duplicate counts from the unambiguous data).
Counts were weighted with scores ranging [0 ? 1]
corresponding to the overall probability of the rel-
evant derivation. The evaluation shows a steady
increase in performance for these successive mod-
els. We also explored other variants of this boot-
strapping approach involving use of weighted counts
from the top n ranked parses derived from the initial
model (see Watson et al, 2007, for details), but none
performed better than simply selecting the highest-
ranked derivation.
To adapt the trained parser, we used the same
technique for the 200 in-domain biomedical sen-
tences (PBIOTB), using Model C to find the highest-
ranked parse compatible with the annotation, and
derived Model D from the combined counts from
this and the previous training data. We then used
Model D to rank the parses for the unannotated
in-domain data (PBIOTB unsupervised), and de-
rived Model E from the combined counts from the
highest-ranked parses for all of the training and de-
velopment data. We then iterated this process two
more times over the unannotated datasets (each with
an increasing number of examples though increas-
ingly less relevant to the test data). The performance
over our out-of-domain PTB-derived test data re-
mains approximately the same for all these models.
Therefore, we chose to use Model G for the blind
test as it incorporates most information from the in-
1171
Mdl. Data Init. Prec. Rec. F1
A Uniform - 71.06 69.00 70.01
PTB
B Unambig. A 75.94 73.16 74.53
C Ambig. B 77.88 75.11 76.47
PBIOTB
D Supervised C 77.86 75.09 76.45
E Unsup. 1 D 77.98 75.25 76.59
F Unsup. 2 E 77.85 75.19 76.50
G Unsup. 3 F 77.76 75.09 76.41
CHILDES
H Unsup. 1 C 78.34 75.59 76.94
Table 1: Performance of Successive Bootstrapping
Models
Score Avg. Std
PCHEMTB - labelled 55.47 65.11 09.64
PCHEMTB - unlab.ed 62.79 70.24 08.14
CHILDES - unlab.ed 45.61 56.12 09.17
Table 2: Official Scores
domain data. For the CHILDES data we performed
one iteration of unsupervised adaptation in the same
manner starting from Model C.
3 Evaluation
For the blind test submission we used Models G and
H to parse the PCHEMTB and CHILDES data, re-
spectively. We then mapped our GR output from
the highest-ranked parses to the DR scheme and an-
notation format required by the CoNLL evaluation
script. Our reported results are given in Table 2.
We used the annotated versions of the blind test
data supplied after the official evaluation to assess
the degree of adaptation of the parser to the in-
domain data. We mapped from the DR scheme and
annotation format to our GR format and used our
evaluation script to calculate the precision, recall
and microaveraged F1 score for the unadapted mod-
els and their adapted counterparts on the blind test
data, given in Table 3. The results for CHILDES
show no evidence of adaptation to the domain. How-
ever, those for PCHEMTB show a statistically sig-
nificant (Wilcoxin Signed Ranks) improvement over
the initial model. The generally higher scores in
Model Test Data Prec. Rec. F1
C PCHEMTB 71.58 73.69 72.62
G PCHEMTB 72.32 74.56 73.42
C CHILDES 82.64 65.18 72.88
H CHILDES 81.71 64.58 72.14
Table 3: Performance of (Un)Adapted Models
Table 3, as compared to Table 2, reflect the differ-
ences between the task annotation scheme and our
GR representation as well as those of the evaluation
schemes, which we discuss in the next section.
4 Discussion
The biggest issue for us participating in the shared
task was the difficulty of reconciling the DR an-
notation scheme with our GR scheme, given the
often implicit and sometimes radical underlying
differences in linguistic assumptions between the
schemes.
Firstly, the PoS tagsets are different and ours con-
tains three times the number of tags. Given that the
grammar uses these tags as preterminal categories,
this puts us at a disadvantage in mapping the anno-
tated training and development data to optimal input
to train the (semi-)supervised models.
Secondly, there are 17 main types of GR rela-
tion and a total of 46 distinctions when GR sub-
types are taken into account ? for instance the GR
ncsubj has two subtypes depending on whether the
surface subject is the underlying object of a passive
clause. The DR scheme has far fewer distinctions
creating similar difficulties when creating optimal
(semi-)supervised training data.
Thirdly, the topology of the dependency graphs
is often significantly different because of reversed
head-dependent bilexical relations and their knock-
on effects ? for instance, the DR AUX relation treats
the (leftmost) auxiliary as head and modifiers of the
verb group attach to the leftmost auxiliary, while the
GR scheme treats the main verb as (semantic) head
and modifiers of the verb group attach to it.
Fourthly, the treatment of punctuation is very dif-
ferent. The DR scheme includes punctuation mark-
ers in DRs which attach to the root of the subgraph
over which they have scope. By contrast, the GR
scheme does not output punctuation marks directly
1172
but follows Nunberg?s (1990) linguistic analysis of
punctuation as delimiting and typing text units or
adjuncts (at constituent boundaries). Thus the GR
scheme includes text (adjunct) relations and treats
punctuation marks as indicators of such relations ?
for instance, for the example The subject GRs ? nc-
subj, xsubj and csubj ? all have subtypes., RASP
outputs the GR (ta dash GRs and) indicating that
the dash-delimited parenthetical is a text adjunct of
GRs with head and, while the DR scheme gives
(DEP GRs and), and two (P and ?) relations cor-
responding to each dash mark.
Although we attempted to derive an optimal and
error-free mapping between the schemes, this was
hampered by the lack of information concerning the
DR scheme, lack of time, and the very different ap-
proaches to punctuation. This undoubtedly limited
our ability to train effectively from the PTB data and
to adapt the trained parser using the in-domain data.
For instance, the mean average unlabelled F1 score
between the GRs mapped from the annotated PTB
training data and closest matching set of GRs output
by RASP for this data is 84.56 with a standard de-
viation of 12.41. This means that the closest match-
ing derivation which is used for training the initial
model is on average only around 85% similar even
by the unlabelled measure. Thus, the mapping pro-
cedure required to relate the annotated data to RASP
derivations is introducing considerable noise into the
training process.
Mapping difficulties also depressed our official
scores very significantly. In training and adapting
we found that bootstrapping based on unlabelled de-
pendencies worked better in all cases than utilizing
the labelled mapping we derived. For the official
submission, we removed all ta, quote and passive
GRs and mapped all punctuation marks to the P re-
lation with head 0. Furthermore, we do not generate
a root relation, though we assumed any word that
was not a dependent in other GRs to have the depen-
dent ROOT. In our own evaluations based on map-
ping the annotated training and development data to
our GR scheme, we remove all P relations and map
ROOT relations to the type root which we added
to our GR hierarchy. We determined the semantic
head of each parse during training so as to compare
against the root GR and better utilize this additional
information. In the results given in Table 1 over our
DepBank test set, the effect of removing the P de-
pendencies is to depress the F1 scores by over 20%.
For the CHILDES and PCHEMTB blind test data,
our F1 scores improve by over 7% and just under 9%
respectively when we factor out the effect of P rela-
tions. These figures give an indication of the scale
of the problem caused by these representional differ-
ences.
5 Conclusions
The main conclusion that we draw from this experi-
ence is that it is very difficult to effectively relate lin-
guistic annotations even when these are inspired by
a similar (dependency-based) theoretical tradition.
The scores we achieved were undoubtedly further
depressed by the need to use a partially-supervised
boostrapping approach to training because the DR
scheme is less informative than the GR one, and by
our decision to use an entirely unlexicalized parse
ranking model for these experiments. Despite these
difficulties, performance on the PCHEMTB dataset
using the adapted model improved significantly over
that of the unadapted model, suggesting that boot-
strapping using confidence-based self-training is a
viable technique.
Acknowledgements
This research has been partially supported by the
EPSRC via the RASP project (grants GR/N36462
and GR/N36493) and the ACLEX project (grant
GR/T19919). The first author is funded by the
Overseas Research Students Awards Scheme and the
Poynton Scholarship appointed by the Cambridge
Australia Trust in collaboration with the Cambridge
Commonwealth Trust.
References
E. Briscoe (2006) An introduction to tag sequence
grammars and the RASP system parser, Univer-
sity of Cambridge, Computer Laboratory Techni-
cal Report, UCAM-CL-TR-662.
E. Briscoe and J. Carroll (2006) ?Evaluating the Ac-
curacy of an Unlexicalized
Statistical Parser on the PARC DepBank?, Pro-
ceedings of the ACL-Coling?06, Sydney, Aus-
tralia.
1173
Briscoe, E.J., J. Carroll and R. Watson (2006) ?The
Second Release of the RASP System?, Proceed-
ings of the ACL-Coling?06, Sydney, Australia.
R. Brown (1973) A First Language: The Early
Stages, Harvard University Press.
Inui, K., V. Sornlertlamvanich, H. Tanaka and
T. Tokunaga (1997) ?A new formalization of prob-
abilistic GLR parsing?, Proceedings of the 5th
International Workshop on Parsing Technologies,
MIT, Cambridge, Massachusetts, pp. 123?134.
R. Johansson and P. Nugues (2007) Extended
Constituent-to-Dependency Conversion for En-
glish, NODALIDA16.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R.
McDonald, M. Palmer, A. Schein and L. Ungar
(2004) ?Integrated Annotation for Biomedical In-
formation Extraction?, Proceedings of the HLT-
NAACL2004, Boston, MA..
B. MacWhinney (2000) The CHILDES Project:
Tools for Analyzing Talk, Lawrence Erlbaum.
M. Marcus, B. Santorini and M. Marcinkiewicz
(1993) ?Building a Large Annotated Corpus of
English: the Penn Treebank?, Computational Lin-
guistics, vol.19.2, 313?330.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel and D. Yuret (2007) ?The CoNLL
2007 Shared Task on Dependency Parsing?, Pro-
ceedings of the EMNLP-CoNLL2007, Prague.
G. Nunberg (1990) The Linguistics of Punctuation,
CSLI Publications.
Tomita, M. (1987) ?An Efficient Augmented
Context-Free Parsing Algorithm?, Computational
Linguistics, vol.13(1?2), 31?46.
R. Watson, E. Briscoe and J. Carroll (2007) ?Semi-
supervised Training of a Statistical Parser from
Unlabeled Partially-bracketed Data?, Proceedings
of the IWPT07, Prague.
1174
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
           
The Second Release of the RASP System
Ted Briscoe? John Carroll? Rebecca Watson?
?Computer Laboratory, University of Cambridge, Cambridge CB3 OFD, UK
firstname.lastname@cl.cam.ac.uk
?Department of Informatics, University of Sussex, Brighton BN1 9QH, UK
J.A.Carroll@sussex.ac.uk
Abstract
We describe the new release of the RASP
(robust accurate statistical parsing) sys-
tem, designed for syntactic annotation of
free text. The new version includes a
revised and more semantically-motivated
output representation, an enhanced gram-
mar and part-of-speech tagger lexicon, and
a more flexible and semi-supervised train-
ing method for the structural parse ranking
model. We evaluate the released version
on the WSJ using a relational evaluation
scheme, and describe how the new release
allows users to enhance performance using
(in-domain) lexical information.
1 Introduction
The first public release of the RASP system
(Briscoe & Carroll, 2002) has been downloaded
by over 120 sites and used in diverse natural lan-
guage processing tasks, such as anaphora res-
olution, word sense disambiguation, identifying
rhetorical relations, resolving metonymy, detect-
ing compositionality in phrasal verbs, and diverse
applications, such as topic and sentiment classifi-
cation, text anonymisation, summarisation, infor-
mation extraction, and open domain question an-
swering. Briscoe & Carroll (2002) give further de-
tails about the first release. Briscoe (2006) pro-
vides references and more information about ex-
tant use of RASP and fully describes the modifi-
cations discussed more briefly here.
The new release, which is free for all non-
commercial use1, is designed to address several
weaknesses of the extant toolkit. Firstly, all mod-
ules have been incrementally improved to cover a
greater range of text types. Secondly, the part-of-
speech tagger lexicon has been semi-automatically
enhanced to better deal with rare or unseen be-
haviour of known words. Thirdly, better facil-
ities have been provided for user customisation.
1See http://www.informatics.susx.ac.uk/research/nlp/rasp/
for licence and download details.
?raw text
Tokeniser
?
PoS Tagger
?
Lemmatiser
?
Parser/Grammar
?
Parse Ranking Model
Figure 1: RASP Pipeline
Fourthly, the grammatical relations output has
been redesigned to better support further process-
ing. Finally, the training and tuning of the parse
ranking model has been made more flexible.
2 Components of the System
RASP is implemented as a series of modules writ-
ten in C and Common Lisp, which are pipelined,
working as a series of Unix-style filters. RASP
runs on Unix and is compatible with most C com-
pilers and Common Lisp implementations. The
public release includes Lisp and C executables for
common 32- and 64-bit architectures, shell scripts
for running and parameterising the system, docu-
mentation, and so forth. An overview of the sys-
tem is given in Figure 1.
2.1 Sentence Boundary Detection and
Tokenisation
The system is designed to take unannotated text or
transcribed (and punctuated) speech as input, and
not simply to run on pre-tokenised input such as
that typically found in corpora produced for NLP
purposes. Sentence boundary detection and to-
kenisation modules, implemented as a set of deter-
ministic finite-state rules in Flex (an open source
re-implementation of the original Unix Lex utility)
77
         
 PPPPP
QQQ
HHH
!!!!
HHH
QQQQ
!!!!!!!!      
(((((((

hhhhhhhhh
PPPPPSS

        
aaaa
XXXXXXX
hhhhhhhhhh
dependent
ta arg mod det aux conj
mod arg
ncmod xmod cmod pmod
subj or dobj
subj comp
ncsubj xsubj csubj obj pcomp clausal
dobj obj2 iobj xcomp ccomp
Figure 2: The GR hierarchy
and compiled into C, convert raw ASCII (or Uni-
code in UTF-8) data into a sequence of sentences
in which, for example punctuation tokens are sep-
arated from words by spaces, and so forth.
Since the first release this part of the system
has been incrementally improved to deal with a
greater variety of text types, and handle quo-
tation appropriately. Users are able to modify
the rules used and recompile the modules. All
RASP modules now accept XML mark up (with
certain hard-coded assumptions) so that data can
be pre-annotated?for example to identify named
entities?before being passed to the tokeniser, al-
lowing for more domain-dependent, potentially
multiword tokenisation and classification prior to
parsing if desired (e.g. Vlachos et al, 2006), as
well as, for example, handling of text with sen-
tence boundaries already determined.
2.2 PoS and Punctuation Tagging
The tokenised text is tagged with one of 150
part-of-speech (PoS) and punctuation labels (de-
rived from the CLAWS tagset). This is done
using a first-order (?bigram?) hidden markov
model (HMM) tagger implemented in C (Elwor-
thy, 1994) and trained on the manually-corrected
tagged versions of the Susanne, LOB and (sub-
set of) BNC corpora. The tagger has been aug-
mented with an unknown word model which per-
forms well under most circumstances. However,
known but rare words often caused problems as
tags for all realisations were rarely present. A se-
ries of manually developed rules has been semi-
automatically applied to the lexicon to amelio-
rate this problem by adding further tags with low
counts to rare words. The new tagger has an accu-
racy of just over 97% on the DepBank part of sec-
tion 23 of the Wall Street Journal, suggesting that
this modification has resulted in competitive per-
formance on out-of-domain newspaper text. The
tagger implements the Forward-Backward algo-
rithm as well as the Viterbi algorithm, so users can
opt for tag thresholding rather than forced-choice
tagging (giving >99% tag recall on DepBank, at
some cost to overall system speed). Recent exper-
iments suggest that this can lead to a small gain
in parse accuracy as well as coverage (Watson,
2006).
2.3 Morphological Analysis
The morphological analyser is also implemented
in Flex, with about 1400 finite-state rules in-
corporating a great deal of lexically exceptional
data. These rules are compiled into an efficient
C program encoding a deterministic finite state
transducer. The analyser takes a word form and
CLAWS tag and returns a lemma plus any inflec-
tional affixes. The type and token error rate of
the current system is less than 0.07% (Minnen,
Carroll and Pearce, 2001). The primary system-
internal value of morphological analysis is to en-
able later modules to use lexical information asso-
ciated with lemmas, and to facilitate further acqui-
sition of such information from lemmas in parses.
2.4 PoS and Punctuation Sequence Parsing
The manually-developed wide-coverage tag se-
quence grammar utilised in this version of the
parser consists of 689 unification-based phrase
structure rules (up from 400 in the first release).
The preterminals to this grammar are the PoS
and punctuation tags2. The terminals are featu-
ral descriptions of the preterminals, and the non-
terminals project information up the tree using
an X-bar scheme with 41 attributes with a maxi-
mum of 33 atomic values. Many of the original
2The relatively high level of detail in the tagset helps the
grammar writer to limit overgeneration and overacceptance.
78
     
rules have been replaced with multiple more spe-
cific variants to increase precision. In addition,
coverage has been extended in various ways, no-
tably to cover quotation and word order permuta-
tions associated with direct and indirect quotation,
as is common in newspaper text. All rules now
have a rule-to-rule declarative specification of the
grammatical relations they license (see ?2.6). Fi-
nally, around 20% of the rules have been manu-
ally identified as ?marked? in some way; this can
be exploited in customisation and in parse ranking.
Users can specify that certain rules should not be
used and so to some extent tune the parser to dif-
ferent genres without the need for retraining.
The current version of the grammar finds at least
one parse rooted in S for about 85% of the Susanne
corpus (used for grammar development), and most
of the remainder consists of phrasal fragments
marked as independent text sentences in passages
of dialogue. The coverage of our WSJ test data is
84%. In cases where there is no parse rooted in S,
the parser returns a connected sequence of partial
parses covering the input. The criteria are partial
parse probability and a preference for longer but
non-lexical combinations (Kiefer et al, 1999).
2.5 Generalised LR Parser
A non-deterministic LALR(1) table is constructed
automatically from a CF ?backbone? compiled
from the feature-based grammar. The parser
builds a packed parse forest using this table to
guide the actions it performs. Probabilities are as-
sociated with subanalyses in the forest via those
associated with specific actions in cells of the LR
table (Inui et al, 1997). The n-best (i.e. most
probable) parses can be efficiently extracted by
unpacking subanalyses, following pointers to con-
tained subanalyses and choosing alternatives in or-
der of probabilistic ranking. This process back-
tracks occasionally since unifications are required
during the unpacking process and they occasion-
ally fail (see Oepen and Carroll, 2000).
The probabilities of actions in the LR table
are computed using bootstrapping methods which
utilise an unlabelled bracketing of the Susanne
Treebank (Watson et al, 2006). This makes the
system more easily retrainable after changes in the
grammar and opens up the possibility of quicker
tuning to in-domain data. In addition, the struc-
tural ranking induced by the parser can be re-
ranked using (in-domain) lexical data which pro-
vides conditional probability distributions for the
SUBCATegorisation attributes of the major lexi-
cal categories. Some generic data is supplied for
common verbs, but this can be augmented by user
supplied, possibly domain specific files.
2.6 Grammatical Relations Output
The resulting set of ranked parses can be dis-
played, or passed on for further processing, in a
variety of formats which retain varying degrees of
information from the full derivations. We origi-
nally proposed transforming derivation trees into
a set of named grammatical relations (GRs), il-
lustrated as a subsumption hierarchy in Figure 2,
as a way of facilitating cross-system evaluation.
The revised GR scheme captures those aspects
of predicate-argument structure that the system is
able to recover and is the most stable and gram-
mar independent representation available. Revi-
sions include a treatment of coordination in which
the coordinator is the head in subsuming relations
to enable appropriate semantic inferences, and ad-
dition of a text adjunct (punctuation) relation to
the scheme.
Factoring rooted, directed graphs of GRs into a
set of bilexical dependencies makes it possible to
compute the transderivational support for a partic-
ular relation and thus compute a weighting which
takes account both of the probability of derivations
yielding a specific relation and of the proportion
of such derivations in the forest produced by the
parser. A weighted set of GRs from the parse for-
est is now computed efficiently using a variant of
the inside-outside algorithm (Watson et al, 2005).
3 Evaluation
The new system has been evaluated using our re-
annotation of the PARC dependency bank (Dep-
Bank; King et al, 2003)?consisting of 560 sen-
tences chosen randomly from section 23 of the
Wall Street Journal?with grammatical relations
compatible with our system. Briscoe and Carroll
(2006) discuss issues raised by this reannotation.
Relations take the following form: (relation
subtype head dependent initial) where relation
specifies the type of relationship between the head
and dependent. The remaining subtype and ini-
tial slots encode additional specifications of the re-
lation type for some relations and the initial or un-
derlying logical relation of the grammatical sub-
ject in constructions such as passive. We deter-
79
        
mine for each sentence the relations in the test set
which are correct at each level of the relational hi-
erarchy. A relation is correct if the head and de-
pendent slots are equal and if the other slots are
equal (if specified). If a relation is incorrect at
a given level in the hierarchy it may still match
for a subsuming relation (if the remaining slots all
match); for example, if a ncmod relation is mis-
labelled with xmod, it will be correct for all rela-
tions which subsume both ncmod and xmod, e.g.
mod. Similarly, the GR will be considered incor-
rect for xmod and all relations that subsume xmod
but not ncmod. Thus, the evaluation scheme cal-
culates unlabelled dependency accuracy at the de-
pendency (most general) level in the hierarchy.
The micro-averaged precision, recall and F1 score
are calculated from the counts for all relations in
the hierarchy. The macroaveraged scores are the
mean of the individual scores for each relation.
On the reannotated DepBank, the system
achieves a microaveraged F1 score of 76.3%
across all relations, using our new training method
(Watson et al, 2006). Briscoe and Carroll (2006)
show that the system has equivalent accuracy to
the PARC XLE parser when the morphosyntactic
features in the original DepBank gold standard are
taken into account. Figure 3 shows a breakdown
of the new system?s results by individual relation.
Acknowledgements
Development has been partially funded by
the EPSRC RASP project (GR/N36462 and
GR/N36493) and greatly facilitated by Anna Ko-
rhonen, Diana McCarthy, Judita Preiss and An-
dreas Vlachos. Much of the system rests on ear-
lier work on the ANLT or associated tools by Bran
Boguraev, David Elworthy, Claire Grover, Kevin
Humphries, Guido Minnen, and Larry Piano.
References
Briscoe, E.J. (2006) An Introduction to Tag Sequence Gram-
mars and the RASP System Parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll (2002) ?Robust accurate statisti-
cal annotation of general text?, Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC?02),
Las Palmas, Gran Canaria, pp. 1499?1504.
Briscoe, E.J. and J. Carroll (2006) ?Evaluating the Accu-
racy of an Unlexicalized Statistical Parser on the PARC
DepBank?, Proceedings of the COLING/ACL Conference,
Sydney, Australia.
Elworthy, D. (1994) ?Does Baum-Welch re-estimation help
taggers??, Proceedings of the 4th ACL Conference on Ap-
plied NLP, Stuttgart, Germany, pp. 53?58.
Relation Precision Recall F1 std GRs
dependent 79.76 77.49 78.61 10696
aux 93.33 91.00 92.15 400
conj 72.39 72.27 72.33 595
ta 42.61 51.37 46.58 292
det 87.73 90.48 89.09 1114
arg mod 79.18 75.47 77.28 8295
mod 74.43 67.78 70.95 3908
ncmod 75.72 69.94 72.72 3550
xmod 53.21 46.63 49.70 178
cmod 45.95 30.36 36.56 168
pmod 30.77 33.33 32.00 12
arg 77.42 76.45 76.94 4387
subj or dobj 82.36 74.51 78.24 3127
subj 78.55 66.91 72.27 1363
ncsubj 79.16 67.06 72.61 1354
xsubj 33.33 28.57 30.77 7
csubj 12.50 50.00 20.00 2
comp 75.89 79.53 77.67 3024
obj 79.49 79.42 79.46 2328
dobj 83.63 79.08 81.29 1764
obj2 23.08 30.00 26.09 20
iobj 70.77 76.10 73.34 544
clausal 60.98 74.40 67.02 672
xcomp 76.88 77.69 77.28 381
ccomp 46.44 69.42 55.55 291
pcomp 72.73 66.67 69.57 24
macroaverage 62.12 63.77 62.94
microaverage 77.66 74.98 76.29
Figure 3: Accuracy on DepBank
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga
(1997) ?A new formalization of probabilistic GLR pars-
ing?, Proceedings of the 5th International Workshop on
Parsing Technologies (IWPT?97), MIT, pp. 123?134.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf (1999) ?A
bag of useful techniques for efficient and robust parsing?,
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, University of Mary-
land, pp. 473?480.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan (2003) ?The PARC700 Dependency Bank?, Proceed-
ings of the 4th International Workshop on Linguistically
Interpreted Corpora (LINC-03), Budapest, Hungary.
Minnen, G., J. Carroll and D. Pearce (2001) ?Applied mor-
phological processing of English?, Natural Language En-
gineering, vol.7.3, 225?250.
Oepen, S. and J. Carroll (2000) ?Ambiguity packing in
constraint-based parsing ? practical results?, Proceedings
of the 1st Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle, WA,
pp. 162?169.
Watson, R. (2006) ?Part-of-speech tagging models for pars-
ing?, Proceedings of the 9th Annual CLUK Colloquium,
Open University, Milton Keynes, UK.
Watson, R., E.J. Briscoe and J. Carroll (2006) Semi-
supervised Training of a Statistical Parser from Unlabeled
Partially-bracketed Data, forthcoming.
Watson, R., J. Carroll and E.J. Briscoe (2005) ?Efficient ex-
traction of grammatical relations?, Proceedings of the 9th
Int. Workshop on Parsing Technologies (IWPT?05), Van-
couver, Canada.
80
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 160?170,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficient extraction of grammatical relations
Rebecca Watson, John Carroll
 
and Ted Briscoe
Computer Laboratory, University of Cambridge, Cambridge, CB3 OFD, UK
firstname.lastname@cl.cam.ac.uk
 
Department of Informatics, University of Sussex, Brighton BN1 9QH, UK
J.A.Carroll@sussex.ac.uk
Abstract
We present a novel approach for applying
the Inside-Outside Algorithm to a packed
parse forest produced by a unification-
based parser. The approach allows a node
in the forest to be assigned multiple inside
and outside probabilities, enabling a set of
?weighted GRs? to be computed directly
from the forest. The approach improves
on previous work which either loses effi-
ciency by unpacking the parse forest be-
fore extracting weighted GRs, or places
extra constraints on which nodes can be
packed, leading to less compact forests.
Our experiments demonstrate substantial
increases in parser accuracy and through-
put for weighted GR output.
1 Introduction
RASP is a robust statistical analysis system for
English developed by Briscoe and Carroll (2002).
It contains a syntactic parser which can output
analyses in a number of formats, including (n-
best) syntactic trees, robust minimal recursion se-
mantics (Copestake, 2003), grammatical relations
(GRs), and weighted GRs. The weighted GRs for
a sentence comprise the set of grammatical relations
in all parses licensed for that sentence, each GR is
weighted based on the probabilities of the parses
in which it occurs. This weight is normalised to
fall within the range  0,1  where  indicates that
all parses contain the GR. Therefore, high precision
GR sets can be determined by thresholding on the
GR weight (Carroll and Briscoe, 2002). Carroll and
Briscoe compute weighted GRs by first unpacking
all parses or the n-best subset from the parse forest.
Hence, this approach is either (a) inefficient (and for
some examples impracticable) if a large number of
parses are licensed by the grammar, or (b) inaccu-
rate if the number of parses unpacked is less than
the number licensed by the grammar.
In this paper, we show how to obviate the need
to trade off efficiency and accuracy by extracting
weighted GRs directly from the parse forest us-
ing a dynamic programming approach based on the
Inside-Outside algorithm (IOA) (Baker, 1979; Lari
and Young, 1990). This approach enables efficient
calculation of weighted GRs over all parses and sub-
stantially improves the throughput and memory us-
age of the parser. Since the parser is unification-
based, we also modify the parsing algorithm so that
local ambiguity packing is based on feature structure
equivalence rather than subsumption.
Similar dynamic programming techniques that
are variants of the IOA have been applied for re-
lated tasks, such as parse selection (Johnson, 2001;
Schmid and Rooth, 2001; Geman and Johnson,
2002; Miyao and Tsujii, 2002; Kaplan et al, 2004;
Taskar et al, 2004). The approach we take is similar
to Schmid and Rooth?s (2001) adaptation of the al-
gorithm, where ?expected governors? (similar to our
?GR specifications?) are determined for each tree,
and alternative nodes in the parse forest have the
same lexical head. Initially, they create a packed
parse forest and during a second pass the parse forest
nodes are split if multiple lexical heads occur. The
IOA is applied over this split data structure. Simi-
larly, Clark and Curran (2004) alter their packing al-
gorithm so that nodes in the packed chart have the
same semantic head and ?unfilled? GRs. Our ap-
160
proach is novel in that while calculating inside prob-
abilities we allow any node in the parse forest to
have multiple semantic heads.
Clark and Curran (2004) apply Miyao and Tsu-
jii?s (2002) dynamic programming approach to de-
termine weighted GRs. They outline an alterna-
tive parse selection method based on the resulting
weighted GRs: select the (consistent) GR set with
the highest average weighted GR score. We apply
this parse selection approach and achieve 3.01% rel-
ative reduction in error. Further, the GR set output
by this approach is a consistent set whereas the high
precision GR sets outlined in (Carroll and Briscoe,
2002) are neither consistent nor coherent.
The remainder of this paper is organised as fol-
lows: Section 2 gives details of the RASP sys-
tem that are relevant to this work. Section 3 de-
scribes our test suite and experimental environment.
Changes required to the current parse forest cre-
ation algorithm are discussed in Section 4, while
Section 5 outlines our dynamic programming ap-
proach for extracting weighted GRs (EWG). Sec-
tion 6 presents experimental results showing (a) im-
proved efficiency achieved by EWG, (b) increased
upper bounds of precision and recall achieved us-
ing EWG, and (c) increased accuracy achieved by
a parse selection algorithm that would otherwise be
too inefficient to consider. Finally, Section 7 out-
lines our conclusions and future lines of research.
2 The RASP System
RASP is based on a pipelined modular architec-
ture in which text is pre-processed by a series of
components including sentence boundary detection,
tokenisation, part of speech tagging, named entity
recognition and morphological analysis, before be-
ing passed to a statistical parser1 . A brief overview
of relevant aspects of syntactic processing in RASP
is given below; for full details of system compo-
nents, see Briscoe and Carroll (1995; 2002; 2005)2.
1Processing times given in this paper do not include these
pre-processing stages, since they take negligible time compared
with parsing.
2RASP is freely available for research use; visit
http://www.informatics.susx.ac.uk/research/nlp/rasp/
2.1 The Grammar
Briscoe and Carroll (2005) describe the (manually-
written) feature-based unification grammar and the
rule-to-rule mapping from local trees to GRs. The
mapping specifies for each grammar rule the seman-
tic head(s) of the rule (henceforth, head), and one or
more GRs that should be output (optionally depend-
ing on feature values instantiated at parse time). For
example, Figure 1 shows a grammar rule analysing a
verb phrase followed by a prepositional phrase mod-
ifier. The rule identifies the first daughter (1) as the
semantic head, and specifies that one of five possi-
ble GRs is to be output, depending on the value of
the PSUBCAT syntactic feature; so, for example, if the
feature has the value NP, then the relation is ncmod
(non-clausal modifier), with slots filled by the se-
mantic heads of the first and second daughters (the 1
and 2 arguments).
Before parsing, a context free backbone is derived
automatically from the grammar, and an LALR(1)
parse table is computed from this backbone (Carroll,
1993, describes the procedure in detail). Probabili-
ties are associated with actions in the parse table,
by training on around 4K sentences from the Su-
sanne corpus (Sampson, 1995), each sentence hav-
ing been semi-automatically converted from a tree-
bank bracketing to a tree conforming to the unifica-
tion grammar (Briscoe and Carroll, 1995).
2.2 The Parse Forest
When parsing, the LALR table action probabilities
are used to assign a score to each newly derived
(sub-)analysis. Additionally, on each reduce ac-
tion (i.e. complete application of a rule), the rule?s
daughters are unified with the sequence of sub-
analyses being consumed. If unification fails then
the reduce action is aborted. Local ambiguity pack-
ing (packing, henceforth) is performed on the ba-
sis of feature structure subsumption. Thus, the
parser builds and returns a compact structure that ef-
ficiently represents all parses licensed by the gram-
mar: the parse forest. Since unification often fails
it is not possible to apply beam or best first search
strategies during construction of the parse forest;
statistically high scoring paths often end up in unifi-
cation failure. Hence, the parse forest represents all
parses licensed by the grammar.
161
V1/vp_pp : V1[MOD +] --> H1 P2[ADJ -, WH -] :
1 :
2 = [PSUBCAT NP], (ncmod _ 1 2) :
2 = [PSUBCAT NONE], (ncmod prt 1 2) :
2 = [PSUBCAT (VP, VPINF, VPING, VPPRT, AP)], (xmod _ 1 2) :
2 = [PSUBCAT (SFIN, SINF, SING)], (cmod _ 1 2) :
2 = [PSUBCAT PP], (pmod 1 2).
Figure 1: Example grammar rule, showing the rule name and syntactic specification (on the first line),
identification of daughter 1 as the semantic head (second line), and possible GR outputs depending on the
parse-time value of the PSUBCAT feature of daughter 2 (subsequent lines).
Figure 2 shows a simplified parse forest contain-
ing three parses generated for the following pre-
processed text3:
I PPIS1 see+ed VVD the AT man NN1
in II the AT park NN1
The GR specifications shown are instantiated based
on the values of syntactic features at daughter nodes,
as discussed in Section 2.1 above. For example, the
V1/vp pp sub-analysis (towards the left hand side of
the Figure) contains the instantiated GR specifica-
tion   1, (ncmod 1 2)  since its second daughter has
the value NP for its PSUBCAT feature.
Henceforth, we will use the term ?node? to refer to
data structures in our parse forest corresponding to a
rule instantiation: a sub-analysis resulting from ap-
plication of a reduce action. Back pointers are stored
in nodes, indicating which daughters were used to
create the sub-analysis. These pointers provide a
means to traverse the parse forest during subsequent
processing stages. A ?packed node? is a node rep-
resenting a sub-analysis that is subsumed by, and
hence packed into, another node. Packing is consid-
ered for nodes only if they are produced in the same
LR state and represent sub-analyses with the same
word span. A parse forest can have a number of root
nodes, each one dominating analyses spanning the
whole sentence with the specified top category.
2.3 Parser Output
From the parse forest, RASP unpacks the ?n-best?4
syntactic trees using a depth-first beam search (Car-
roll, 1993). There are a number of types of analysis
3The part of speech tagger uses a subset of the Lancaster
CLAWS2 tagset ? http://www.comp.lancs.ac.uk/computing/research/
ucrel/claws2tags.html
4This number  is specified by the user, and represents the
maximal number of parses to be unpacked.
output available, including syntactic tree, grammati-
cal relations (GRs) and robust minimal recursion se-
mantics (RMRS). Each of these is computed from
the n-best trees.
Another output possibility is weighted GRs (Car-
roll and Briscoe, 2002); this is the unique set of GRs
from the n-best GRs, each GR weighted according
to the sum of the probabilities of the parses in which
it occurs. Therefore, a number of processing stages
determine this output: unpacking the n-best syntac-
tic trees, determining the corresponding n-best GR
sets and finding the unique set of GRs and corre-
sponding weights.
The GRs for each parse are computed from the
set of GR specifications at each node, passing the
(semantic) head of each sub-analysis up to the next
higher level in the parse tree (beginning from word
nodes). GR specifications for nodes (which, if re-
quired, have been instantiated based on the features
of daughter nodes) are referred to as ?unfilled? un-
til the slots containing numbers are ?filled? with the
corresponding heads of daughter nodes. For exam-
ple, the grammar rule named NP/det n has the un-
filled GR specification   2, (det 2 1)  . Therefore, if
an NP/det n local tree has two daughters with heads
the and cat respectively, the resulting filled GR spec-
ification will be   cat, (det cat the)  , i.e. the head of
the local tree is cat and the GR output is (det cat the).
Figure 3 illustrates the n-best GRs and the
corresponding (non-normalised and normalised)
weighted GRs for the sentence I saw the man in
the park. The corresponding parse forest for this
example is shown in Figure 2. Weights on the
GRs are normalised probabilities representing the
weighted proportion of parses in which the GR
occurs. This weighting is in practice calculated
as the sum of parse probabilities for parses con-
162
T/
tx
t-
sc
1/
-
S/
np
_v
p
I_
PP
IS
1
V1
/v
_n
p_
pp
V1
/v
p_
pp
V1
/v
_n
p
se
e+
ed
_V
VD
th
e_
AT
ma
n_
NN
1
in
_I
I
th
e_
AT
pa
rk
_N
N1
N1
/n
N1
/n
NP
/d
et
_n
NP
/d
et
_n
PP
/p
1
PP
/p
1
PP
/p
1
P1
/p
_n
p
NP
/d
et
_n
N1
/n
1_
pp
1
P1
/p
_n
p
P1
/p
_n
p
in
_I
I
V1
/v
_n
p
in
_I
I
-0.
02
63
28
93
2
-4.
05
27
59
6
-0.
47
56
08
94
-3.
19
57
16
-2.
78
88
75
-3.
23
82
97
2
-4.
61
76
63
2e
-4
-0.
00
67
16
03
06
-1.
35
22
34
5e
-4
-0.
13
60
40
73 -
0.0
01
26
28
43
3
-1.
35
22
34
5e
-4
-1.
08
08
25
7
-0.
39
11
12
9
-7.
49
44
67
7
-3.
77
18
31
3
-3.
83
19
09
2
-3.
59
08
41
8
-0.
00
64
18
27
42
-3.
83
19
09
2
-3.
59
08
41
8
-1.
59
90
18
3
-0.
32
94
57
7
-3.
83
19
09
2
-3.
59
08
41
8
-2.
75
51
12
4
-3.
77
18
31
3
<1
>
<2
,(d
et 
2 1
)>
<1
,(d
ob
j 1
 2)
>
<1
,(d
ob
j 1
 2)
>
<1
,(d
ob
j 1
 2)
>
<1
>
<2
,(d
et 
2 1
)>
<2
,(d
et 
2 1
)>
<1
>
<1
,(n
cm
od
 _ 
1 2
)>
<1
>
<1
>
<1
,(io
bj 
1 3
)>
 
<2
,(n
csu
bj 
2 1
)> 
<1
,(d
ob
j 1
 2)
>
<1
,(n
cm
od
 _ 
1 2
)>
<1
,(d
ob
j 1
 2)
>
*p
ac
ke
d*
*p
ac
ke
d*
Figure 2: Simplified parse forest for I saw the man in the park. Each element in the directed acyclic graph
represents a node in the parse forest and is shown with the sub-analysis? rule name, reduce probability
(or shift probability at word nodes) and (instantiated) GR specifications. Two nodes are packed into the
V1/v np pp node, so there will be three alternative parses for the sentence. Nodes with multiple in-going
pointers on their left are shared. All thin lines indicate pointers from left to right, i.e. from mother to daughter
nodes.
163
taining the specific GR, normalised by the sum
of all parse probabilities. For example, the GR
(iobj see+ed in) is in one parse with probability
 

 	 , the non-normalised score. The sum of
all parse probabilities is        
  . Therefore,
the normalised probability (and final weight) of the
GR is  
 Proceedings of the 10th Conference on Parsing Technologies, pages 23?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Semi-supervised Training of a Statistical Parser
from Unlabeled Partially-bracketed Data
Rebecca Watson and Ted Briscoe
Computer Laboratory
University of Cambridge, UK
FirstName.LastName@cl.cam.ac.uk
John Carroll
Department of Informatics
University of Sussex, UK
J.A.Carroll@sussex.ac.uk
Abstract
We compare the accuracy of a statisti-
cal parse ranking model trained from a
fully-annotated portion of the Susanne
treebank with one trained from unla-
beled partially-bracketed sentences de-
rived from this treebank and from the
Penn Treebank. We demonstrate that
confidence-based semi-supervised tech-
niques similar to self-training outperform
expectation maximization when both are
constrained by partial bracketing. Both
methods based on partially-bracketed
training data outperform the fully su-
pervised technique, and both can, in
principle, be applied to any statistical
parser whose output is consistent with
such partial-bracketing. We also explore
tuning the model to a different domain
and the effect of in-domain data in the
semi-supervised training processes.
1 Introduction
Extant statistical parsers require extensive and
detailed treebanks, as many of their lexical and
structural parameters are estimated in a fully-
supervised fashion from treebank derivations.
Collins (1999) is a detailed exposition of one
such ongoing line of research which utilizes the
Wall Street Journal (WSJ) sections of the Penn
Treebank (PTB). However, there are disadvan-
tages to this approach. Firstly, treebanks are ex-
pensive to create manually. Secondly, the richer
the annotation required, the harder it is to adapt
the treebank to train parsers which make differ-
ent assumptions about the structure of syntac-
tic analyses. For example, Hockenmeier (2003)
trains a statistical parser based on Combinatory
Categorial Grammar (CCG) on the WSJ PTB,
but first maps the treebank to CCG derivations
semi-automatically. Thirdly, many (lexical) pa-
rameter estimates do not generalize well be-
tween domains. For instance, Gildea (2001) re-
ports that WSJ-derived bilexical parameters in
Collins? (1999) Model 1 parser contribute about
1% to parse selection accuracy when test data
is in the same domain, but yield no improve-
ment for test data selected from the Brown Cor-
pus. Tadayoshi et al (2005) adapt a statistical
parser trained on the WSJ PTB to the biomed-
ical domain by retraining on the Genia Corpus,
augmented with manually corrected derivations
in the same format. To make statistical parsing
more viable for a range of applications, we need
to make more effective and flexible use of extant
training data and minimize the cost of annota-
tion for new data created to tune a system to a
new domain.
Unsupervised methods for training parsers
have been relatively unsuccessful to date, in-
cluding expectation maximization (EM) such as
the inside-outside algorithm (IOA) over PCFGs
(Baker, 1979; Prescher, 2001). However, Pereira
and Schabes (1992) adapted the IOA to apply
over semi-supervised data (unlabeled bracket-
ings) extracted from the PTB. They constrain
the training data (parses) considered within the
IOA to those consistent with the constituent
boundaries defined by the bracketing. One ad-
vantage of this approach is that, although less
information is derived from the treebank, it gen-
23
eralizes better to parsers which make different
representational assumptions, and it is easier,
as Pereira and Schabes did, to map unlabeled
bracketings to a format more consistent with
the target grammar. Another is that the cost
of annotation with unlabeled brackets should be
lower than that of developing a representation-
ally richer treebank. More recently, both Riezler
et al (2002) and Clark and Curran (2004) have
successfully trained maximum entropy parsing
models utilizing all derivations in the model con-
sistent with the annotation of the WSJ PTB,
weighting counts by the normalized probability
of the associated derivation. In this paper, we
extend this line of investigation by utilizing only
unlabeled and partial bracketing.
We compare the performance of a statisti-
cal parsing model trained from a detailed tree-
bank with that of the same model trained with
semi-supervised techniques that require only un-
labeled partially-bracketed data. We contrast
an IOA-based EM method for training a PGLR
parser (Inui et al, 1997), similar to the method
applied by Pereira and Schabes to PCFGs, to a
range of confidence-based semi-supervised meth-
ods described below. The IOA is a generaliza-
tion of the Baum-Welch or Forward-Backward
algorithm, another instance of EM, which can be
used to train Hidden Markov Models (HMMs).
Elworthy (1994) and Merialdo (1994) demon-
strated that Baum-Welch does not necessarily
improve the performance of an HMM part-of-
speech tagger when deployed in an unsuper-
vised or semi-supervised setting. These some-
what negative results, in contrast to those of
Pereira and Schabes (1992), suggest that EM
techniques require fairly determinate training
data to yield useful models. Another motiva-
tion to explore alternative non-iterative meth-
ods is that the derivation space over partially-
bracketed data can remain large (>1K) while
the confidence-based methods we explore have a
total processing overhead equivalent to one iter-
ation of an IOA-based EM algorithm.
As we utilize an initial model to annotate ad-
ditional training data, our methods are closely
related to self-training methods described in the
literature (e.g. McClosky et al 2006, Bacchi-
ani et al 2006). However these methods have
been applied to fully-annotated training data
to create the initial model, which is then used
to annotate further training data derived from
unannotated text. Instead, we train entirely
from partially-bracketed data, starting from the
small proportion of ?unambiguous? data whereby
a single parse is consistent with the annota-
tion. Therefore, our methods are better de-
scribed as semi-supervised and the main focus
of this work is the flexible re-use of existing
treebanks to train a wider variety of statistical
parsing models. While many statistical parsers
extract a context-free grammar in parallel with
a statistical parse selection model, we demon-
strate that existing treebanks can be utilized to
train parsers that deploy grammars that make
other representational assumptions. As a result,
our methods can be applied by a range of parsers
to minimize the manual effort required to train
a parser or adapt to a new domain.
?2 gives details of the parsing system that are
relevant to this work. ?3 and ?4 describe our
data and evaluation schemes, respectively. ?5
describes our semi-supervised training methods.
?6 explores the problem of tuning a parser to a
new domain. Finally, ?7 gives conclusions and
future work.
2 The Parsing System
Sentences are automatically preprocessed in a
series of modular pipelined steps, including to-
kenization, part of speech (POS) tagging, and
morphological analysis, before being passed to
the statistical parser. The parser utilizes a man-
ually written feature-based unification grammar
over POS tag sequences.
2.1 The Parse Selection Model
A context-free ?backbone? is automatically de-
rived from the unification grammar1 and a gen-
eralized or non-deterministic LALR(1) table is
1This backbone is determined by compiling out the
values of prespecified attributes. For example, if we com-
pile out the attribute PLURAL which has 2 possible val-
ues (plural or not) we will create 2 CFG rules for each
rule with categories that contain PLURAL. Therefore,
no information is lost during this process.
24
constructed from this backbone (Tomita, 1987).
The residue of features not incorporated into
the backbone are unified on each reduce action
and if unification fails the associated derivation
paths also fail. The parser creates a packed
parse forest represented as a graph-structured
stack.2 The parse selection model ranks com-
plete derivations in the parse forest by com-
puting the product of the probabilities of the
(shift/reduce) parse actions (given LR state and
lookahead item) which created each derivation
(Inui et al, 1997).
Estimating action probabilities, consists of
a) recording an action history for the correct
derivation in the parse forest (for each sen-
tence in a treebank), b) computing the fre-
quency of each action over all action histories
and c) normalizing these frequencies to deter-
mine probability distributions over conflicting
(i.e. shift/reduce or reduce/reduce) actions.
Inui et al (1997) describe the probability
model utilized in the system where a transition
is represented by the probability of moving from
one stack state, ?i?1, (an instance of the graph
structured stack) to another, ?i. They estimate
this probability using the stack-top state si?1,
next input symbol li and next action ai. This
probability is conditioned on the type of state
si?1. Ss and Sr are mutually exclusive sets
of states which represent those states reached
after shift or reduce actions, respectively. The
probability of an action is estimated as:
P (li, ai, ?i|?i?1) ?
{
P (li, ai|si?1) si?1 ? Ss
P (ai|si?1, li) si?1 ? Sr
}
Therefore, normalization is performed over all
lookaheads for a state or over each lookahead
for the state depending on whether the state is
a member of Ss or Sr, respectively (hereafter
the I function). In addition, Laplace estimation
can be used to ensure that all actions in the
2The parse forest is an instance of a feature forest as
defined by Miyao and Tsujii (2002). We will use the term
?node? herein to refer to an element in a derivation tree
or in the parse forest that corresponds to a (sub-)analysis
whose label is the mother?s label in the corresponding CF
?backbone? rule.
table are assigned a non-zero probability (the
IL function).
3 Training Data
The treebanks we use in this work are in one of
two possible formats. In either case, a treebank
T consists of a set of sentences. Each sentence
t is a pair (s,M), where s is the automatically
preprocessed set of POS tagged tokens (see ?2)
and M is either a fully annotated derivation, A,
or an unlabeled bracketing U . This bracketing
may be partial in the sense that it may be com-
patible with more than one derivation produced
by a given parser. Although occasionally the
bracketing is itself complete but alternative non-
terminal labeling causes indeterminacy, most of-
ten the ?flatter? bracketing available from ex-
tant treebanks is compatible with several alter-
native ?deeper? mostly binary-branching deriva-
tions output by a parser.
3.1 Derivation Consistency
Given t = (s,A), there will exist a single deriva-
tion in the parse forest that is compatible (cor-
rect). In this case, equality between the deriva-
tion tree and the treebank annotation A iden-
tifies the correct derivation. Following Pereira
and Schabes (1992) given t = (s, U), a node?s
span in the parse forest is valid if it does not
overlap with any span outlined in U , and hence,
a derivation is correct if the span of every node
in the derivation is valid in U . That is, if no
crossing brackets are present in the derivation.
Thus, given t = (s, U), there will often be more
than one derivation compatible with the partial
bracketing.
Given the correct nodes in the parse forest
or in derivations, we can then extract the cor-
responding action histories and estimate action
probabilities as described in ?2.1. In this way,
partial bracketing is used to constrain the set of
derivations considered in training to those that
are compatible with this bracketing.
3.2 The Susanne Treebank and
Baseline Training Data
The Susanne Treebank (Sampson, 1995) is uti-
lized to create fully annotated training data.
25
This treebank contains detailed syntactic deriva-
tions represented as trees, but the node label-
ing is incompatible with the system grammar.
We extracted sentences from Susanne and auto-
matically preprocessed them. A few multiwords
are retokenized, and the sentences are retagged
using the POS tagger, and the bracketing de-
terministically modified to more closely match
that of the grammar, resulting in a bracketed
corpus of 6674 sentences. We will refer to this
bracketed treebank as S, henceforth.
A fully-annotated and system compatible
treebank of 3543 sentences from S was also
created. We will refer to this annotated tree-
bank, used for fully supervised training, as B.
The system parser was applied to construct
a parse forest of analyses which are compati-
ble with the bracketing. For 1258 sentences,
the grammar writer interactively selected cor-
rect (sub)analyses within this set until a sin-
gle analysis remained. The remaining 2285 sen-
tences were automatically parsed and all consis-
tent derivations were returned. Since B contains
more than one possible derivation for roughly
two thirds of the data the 1258 sentences (paired
with a single tree) were repeated twice so that
counts from these trees were weighted more
highly. The level of reweighting was determined
experimentally using some held out data from
S. The baseline supervised model against which
we compare in this work is defined by the func-
tion IL(B) as described in ?2.1. The costs of
deriving the fully-annotated treebank are high
as interactive manual disambiguation takes an
average of ten minutes per sentence, even given
the partial bracketing derived from Susanne.
3.3 The WSJ PTB Training Data
The Wall Street Journal (WSJ) sections of the
Penn Treebank (PTB) are employed as both
training and test data by many researchers in
the field of statistical parsing. The annotated
corpus implicitly defines a grammar by provid-
ing a labeled bracketing over words annotated
with POS tags. We extracted the unlabeled
bracketing from the de facto standard training
sections (2-21 inclusive).3 We will refer to the
resulting corpus as W and the combination (con-
catenation) of the partially-bracketed corpora S
and W as SW .
3.4 The DepBank Test Data
King et al (2003) describe the development
of the PARC 700 Dependency Bank, a gold-
standard set of relational dependencies for 700
sentences (from the PTB) drawn at random
from section 23 of the WSJ (the de facto stan-
dard test set for statistical parsing). In all the
evaluations reported in this paper we test our
parser over a gold-standard set of relational de-
pendencies compatible with our parser output
derived (Briscoe and Carroll, 2006) from the
PARC 700 Dependency Bank (DepBank, hence-
forth).
The Susanne Corpus is a (balanced) subset of
the Brown Corpus which consists of 15 broad
categories of American English texts. All but
one category (reportage text) is drawn from dif-
ferent domains than the WSJ. We therefore, fol-
lowing Gildea (2001) and others, consider S, and
also the baseline training data, B, as out-of-
domain training data.
4 The Evaluation Scheme
The parser?s output is evaluated using a rela-
tional dependency evaluation scheme (Carroll,
et al, 1998; Lin, 1998) with standard measures:
precision, recall and F1. Relations are organized
into a hierarchy with the root node specifying an
unlabeled dependency. The microaveraged pre-
cision, recall and F1 scores are calculated from
the counts for all relations in the hierarchy which
subsume the parser output. The microaveraged
F1 score for the baseline system using this eval-
uation scheme is 75.61%, which ? over similar
sets of relational dependencies ? is broadly com-
parable to recent evaluation results published by
King and collaborators with their state-of-the-
art parsing system (Briscoe et al, 2006).
3The pipeline is the same as that used for creating S
though we do not automatically map the bracketing to
be more consistent with the system grammar, instead,
we simply removed unary brackets.
26
4.1 Wilcoxon Signed Ranks Test
The Wilcoxon Signed Ranks (Wilcoxon, hence-
forth) test is a non-parametric test for statistical
significance that is appropriate when there is one
data sample and several measures. For example,
to compare the accuracy of two parsers over the
same data set. As the number of samples (sen-
tences) is large we use the normal approximation
for z. Siegel and Castellan (1988) describe and
motivate this test. We use a 0.05 level of sig-
nificance, and provide z-value probabilities for
significant results reported below. These results
are computed over microaveraged F1 scores for
each sentence in DepBank.
5 Training from Unlabeled
Bracketings
We parsed all the bracketed training data us-
ing the baseline model to obtain up to 1K top-
ranked derivations and found that a significant
proportion of the sentences of the potential set
available for training had only a single deriva-
tion compatible with their unlabeled bracket-
ing. We refer to these sets as the unambiguous
training data (?) and will refer to the remaining
sentences (for which more than one derivation
was compatible with their unlabeled bracketing)
as the ambiguous training data (?). The avail-
ability of significant quantities of unambiguous
training data that can be found automatically
suggests that we may be able to dispense with
the costly reannotation step required to gener-
ate the fully supervised training corpus, B.
Table 1 illustrates the split of the corpora into
mutually exclusive sets ?, ?, ?no match? and
?timeout?. The latter two sets are not utilized
during training and refer to sentences for which
all parses were inconsistent with the bracketing
and those for which no parses were found due
to time and memory limitations (self-imposed)
on the system.4 As our grammar is different
from that implicit in the WSJ PTB there is a
high proportion of sentences where no parses
were consistent with the unmodified PTB brack-
4As there are time and memory restrictions during
parsing, the SW results are not equal to the sum of those
from S and W analysis.
Corpus | ? | | ? | No Match Timeout
S 1097 4138 1322 191
W 6334 15152 15749 1094
SW 7409 19248 16946 1475
Table 1: Corpus split for S, W and SW .
eting. However, a preliminary investigation of
no matches didn?t yield any clear patterns of
inconsistency that we could quickly ameliorate
by simple modifications of the PTB bracketing.
We leave for the future a more extensive investi-
gation of these cases which, in principle, would
allow us to make more use of this training data.
An alternative approach that we have also ex-
plored is to utilize a similar bootstrapping ap-
proach with data partially-annotated for gram-
matical relations (Watson and Briscoe, 2007).
5.1 Confidence-Based Approaches
We use ? to build an initial model. We then
utilize this initial model to derive derivations
(compatible with the unlabeled partial brack-
eting) for ? from which we select additional
training data. We employ two types of selection
methods. First, we select the top-ranked deriva-
tion only and weight actions which resulted in
this derivation equally with those of the initial
model (C1). This method is similar to ?Viterbi
training? of HMMs though we do not weight
the corresponding actions using the top parse?s
probability. Secondly, we select more than one
derivation, placing an appropriate weight on
the corresponding action histories based on the
initial model?s confidence in the derivation. We
consider three such models, in which we weight
transitions corresponding to each derivation
ranked r with probability p in the set of size n
either using 1n , 1r or p itself to weight counts.5
For example, given a treebank T with sentences
t = (s, U), function P to return the set of
parses consistent with U given t and function A
that returns the set of actions given a parse p,
then the frequency count of action ak in Cr is
5In ?2.1 we calculate action probabilities based on fre-
quency counts where we perform a weighted sum over
action histories and each history has a weight of 1. We
extend this scheme to include weights that differ between
action histories corresponding to each derivation.
27
determined as follows:
| ak |=
?|T |
i=1
?|P (ti)|
j=1,ak?A(pij)
1
j
These methods all perform normalization over
the resulting action histories using the training
function IL and will be referred to as Cn, Cr
and Cp, respectively. Cn is a ?uniform? model
which weights counts only by degree of ambi-
guity and makes no use of ranking information.
Cr weights counts by derivation rank, and Cp
is simpler than and different to one iteration of
EM as outside probabilities are not utilized. All
of the semi-supervised functions described here
take two arguments: an initial model and the
data to train over, respectively.
Models derived from unambiguous training
data, ?, alone are relatively accurate, achiev-
ing indistinguishable performance to that of the
baseline system given either W or SW as train-
ing data. We utilize these models as initial mod-
els and train over different corpora with each of
the confidence-based models. Table 2 gives re-
sults for all models. Results statistically signifi-
cant compared to the baseline system are shown
in bold print (better) or italic print (worse).
These methods show promise, often yielding sys-
tems whose performance is significantly better
than the baseline system. Method Cr achieved
the best performance in this experiment and re-
mained consistently better in those reported be-
low. Throughout the different approaches a do-
main effect can be seen, models utilizing just S
are worse, although the best performing models
benefit from the use of both S and W as training
data (i.e. SW ).
5.2 EM
Our EM model differs from that of Pereira and
Schabes as a PGLR parser adds context over
a PCFG so that a single rule can be applied
in several different states containing reduce ac-
tions. Therefore, the summation and normaliza-
tion performed for a CFG rule within IOA is in-
stead applied within such contexts. We can ap-
ply I (our PGLR normalization function with-
out Laplace smoothing) to perform the required
steps if we output the action history with the
Model Prec Rec F1 P (z)?
Baseline 77.05 74.22 75.61 -
IL(?(S)) 76.02 73.40 74.69 0.0294
C1(IL(?(S)), ?(S)) 77.05 74.22 75.61 0.4960
Cn(IL(?(S)), ?(S)) 77.51 74.80 76.13 0.0655
Cr(IL(?(S)), ?(S)) 77.73 74.98 76.33 0.0154
Cp(IL(?(S)), ?(S)) 76.45 73.91 75.16 0.2090
IL(?(W )) 77.01 74.31 75.64 0.1038
C1(IL(?(W )), ?(W )) 76.90 74.23 75.55 0.2546
Cn(IL(?(W )), ?(W )) 77.85 75.07 76.43 0.0017
Cr(IL(?(W )), ?(W )) 77.88 75.04 76.43 0.0011
Cp(IL(?(W )), ?(W )) 77.40 74.75 76.05 0.1335
IL(?(SW )) 77.09 74.35 75.70 0.1003
C1(IL(?(SW )), ?(SW )) 76.86 74.21 75.51 0.2483
Cn(IL(?(SW )), ?(SW )) 77.88 75.05 76.44 0.0048
Cr(IL(?(SW )), ?(SW )) 78.01 75.13 76.54 0.0007
Cp(IL(?(SW )), ?(SW )) 77.54 74.95 76.23 0.0618
Table 2: Performance of all models on DepBank.
?represents the statistical significance of the sys-
tem against the baseline model.
corresponding normalized inside-outside weight
for each node (Watson et al, 2005).
We perform EM starting from two initial mod-
els; either a uniform probability model, IL(), or
from models derived from unambiguous train-
ing data, ?. Figure 1 shows the cross entropy
decreasing monotonically from iteration 2 (as
guaranteed by the EM method) for different cor-
pora and initial models. Some models show an
initial increase in cross-entropy from iteration 1
to iteration 2, because the models are initial-
ized from a subset of the data which is used to
perform maximisation. Cross-entropy increases,
by definition, as we incorporate ambiguous data
with more than one consistent derivation.
Performance over DepBank can be seen in
Figures 2, 3, and 4 for each dataset S, W and
SW, respectively. Comparing the Cr and EM
lines in each of Figures 2, 3, and 4, it is evident
that Cr outperforms EM across all datasets, re-
gardless of the initial model applied. In most
cases, these results are significant, even when
we manually select the best model (iteration)
for EM.
The graphs of EM performance from itera-
tion 1 illustrate the same ?classical? and ?initial?
patterns observed by Elworthy (1994). When
EM is initialized from a relatively poor model,
such as that built from S (Figure 2), a ?classical?
28
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
0 2 4 6 8 10 12 14 16
H(C,G)
Iteration Number
EM(IL(), S)
r
r
r r r r r r r r r r r r r
r
EM(IL(?(S)), S)
c
c
c c c c c c c c c c c c c
c
EM(IL(), W )
4
4 4 4 4 4 4 4 4 4 4 4 4 4 4
4
EM(IL(?(W )),W )
?
? ? ? ? ? ? ? ? ? ? ? ? ? ?
?
EM(IL(), SW )
2
2 2 2 2 2 2 2 2 2 2 2 2 2 2
2
EM(IL(?(SW )), SW )
3
3 3 3 3 3 3 3 3 3 3 3 3 3 3
3
Figure 1: Cross Entropy Convergence for vari-
ous training data and models, with EM.
pattern emerges with relatively steady improve-
ment from iteration 1 until performance asymp-
totes. However, when the starting point is better
(Figures 3 and 4), the ?initial? pattern emerges
in which the best performance is reached after a
single iteration.
6 Tuning to a New Domain
When building NLP applications we would want
to be able to tune a parser to a new domain
with minimal manual effort. To obtain training
data in a new domain, annotating a corpus with
partial-bracketing information is much cheaper
than full annotation. To investigate whether
such data would be of value, we considered W
to be the corpus over which we were tuning and
applied the best performing model trained over
S, Cr(IL(?(S)), ?(S)), as our initial model. Fig-
ure 5 illustrates the performance of Cr compared
to EM.
Tuning using Cr was not significantly differ-
ent from the model built directly from the entire
data set with Cr, achieving 76.57% as opposed
to 76.54% F1 (see Table 2). By contrast, EM
performs better given all the data from the be-
ginning rather than tuning to the new domain.
74
74.5
75
75.5
76
76.5
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(S)), ?(S))
EM(IL(), S)
r
r r
r
r
r r r r r r r
r
r r
r
EM(IL(?(S)), S)
b
b b
b
b b
b
b
b b b b b b b
b
b
Figure 2: Performance over S for Cr and EM.
75
75.2
75.4
75.6
75.8
76
76.2
76.4
76.6
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(W )), ?(W ))
EM(IL(), W )
r
r
r
r r r r
r r r r
r
r r r
r
EM(IL(?(W )),W )
b
b
b
b
b b b b b b
b b b b
b b
b
Figure 3: Performance over W for Cr and EM.
29
75
75.2
75.4
75.6
75.8
76
76.2
76.4
76.6
76.8
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(SW )), ?(SW ))
EM(IL(), SW )
r r
r
r r r r r r r r r r
r r
r
EM(IL(?(SW )), SW )
b
b
b
b
b b b b b b b b b b b b
b
Figure 4: Performance over SW for Cr and EM.
Cr generally outperforms EM, though it is worth
noting the behavior of EM given only the tun-
ing data (W ) rather than the data from both do-
mains (SW ). In this case, the graph illustrates a
combination of Elworthy?s ?initial? and ?classical?
patterns. The steep drop in performance (down
to 69.93% F1) after the first iteration is proba-
bly due to loss of information from S. However,
this run also eventually converges to similar per-
formance, suggesting that the information in S
is effectively disregarded as it forms only a small
portion of SW , and that these runs effectively
converge to a local maximum over W .
Bacchiani et al (2006), working in a similar
framework, explore weighting the contribution
(frequency counts) of the in-domain and out-of-
domain training datasets and demonstrate that
this can have beneficial effects. Furthermore,
they also tried unsupervised tuning to the in-
domain corpus by weighting parses for it by
their normalized probability. This method is
similar to our Cp method. However, when we
tried unsupervised tuning using the WSJ and
an initial model built from S in conjunction with
our confidence-based methods, performance de-
graded significantly.
74
74.5
75
75.5
76
76.5
77
0 2 4 6 8 10 12 14 16
F1
Iteration Number
Baseline
Cr(IL(?(SW )), ?(SW ))
Cr(Cr(IL(?(S)), ?(S)), W )
EM(IL(?(SW )), SW )
b
b
b
b
b b b b b b b b b b b b
b
EM(Cr(IL(?(S)), ?(S)), W )
r
r
r
r
r r
r
r r r r r
r r r
r
EM(Cr(IL(?(S)), ?(S)), SW )
c
c
c
c c c c c
c c c c c c c c
c
Figure 5: Tuning over the WSJ PTB (W ) from
Susanne Corpus (S).
7 Conclusions
We have presented several semi-supervised
confidence-based training methods which have
significantly improved performance over an ex-
tant (more supervised) method, while also re-
ducing the manual effort required to create
training or tuning data. We have shown
that given a medium-sized unlabeled partially
bracketed corpus, the confidence-based models
achieve superior results to those achieved with
EM applied to the same PGLR parse selection
model. Indeed, a bracketed corpus provides flex-
ibility as existing treebanks can be utilized de-
spite the incompatibility between the system
grammar and the underlying grammar of the
treebank. Mapping an incompatible annotated
treebank to a compatible partially-bracketed
corpus is relatively easy compared to mapping
to a compatible fully-annotated corpus.
An immediate benefit of this work is that
(re)training parsers with incrementally-modified
grammars based on different linguistic frame-
works should be much more straightforward ?
see, for example Oepen et al (2002) for a good
discussion of the problem. Furthermore, it sug-
gests that it may be possible to usefully tune
30
a parser to a new domain with less annotation
effort.
Our findings support those of Elworthy (1994)
and Merialdo (1994) for POS tagging and sug-
gest that EM is not always the most suit-
able semi-supervised training method (espe-
cially when some in-domain training data is
available). The confidence-based methods were
successful because the level of noise introduced
did not outweigh the benefit of incorporating
all derivations compatible with the bracketing
in which the derivations contained a high pro-
portion of correct constituents. These findings
may not hold if the level of bracketing available
does not adequately constrain the parses consid-
ered ? see Hwa (1999) for a related investigation
with EM.
In future work we intend to further investigate
the problem of tuning to a new domain, given
that minimal manual effort is a major prior-
ity. We hope to develop methods which required
no manual annotation, for example, high preci-
sion automatic partial bracketing using phrase
chunking and/or named entity recognition tech-
niques might yield enough information to sup-
port the training methods developed here.
Finally, further experiments on weighting the
contribution of each dataset might be beneficial.
For instance, Bacchiani et al (2006) demon-
strate imrpovements in parsing accuracy with
unsupervised adaptation from unannotated data
and explore the effect of different weighting of
counts derived from the supervised and unsu-
pervised data.
Acknowledgements
The first author is funded by the Overseas Re-
search Students Awards Scheme, and the Poyn-
ton Scholarship awarded by the Cambridge Aus-
tralia Trust in collaboration with the Cam-
bridge Commonwealth Trust. Development of
the RASP system was and is supported by the
EPSRC (grants GR/N36462, GR/N36493 and
GR/T19919).
References
Bacchiani, M., Riley, M., Roark, B. and R.
Sproat (2006) ?MAP adaptation of stochas-
tic grammars?, Computer Speech and Lan-
guage, vol.20.1, pp.41?68.
Baker, J. K. (1979) ?Trainable grammars for
speech recognition? in Klatt, D. and Wolf,
J. (eds.), Speech Communications Papers for
the 97th Meeting of the Acoustical Society of
America, MIT, Cambridge, Massachusetts,
pp. 557?550.
Briscoe, E.J., J. Carroll and R. Watson (2006)
?The Second Release of the RASP System?,
Proceedings of ACL-Coling?06, Sydney, Aus-
tralia.
Carroll, J., Briscoe, T. and Sanfilippo, A. (1998)
?Parser evaluation: a survey and a new
proposal?, Proceedings of LREC, Granada,
pp. 447?454.
Clark, S. and J. Curran (2004) ?Parsing the WSJ
Using CCG and Log-Linear Models?, Pro-
ceedings of 42nd Meeting of the Association
for Computational Linguistics, Barcelona,
pp. 103?110.
Collins, M. (1999) Head-driven Statistical Mod-
els for Natural Language Parsing, PhD Dis-
sertation, University of Pennsylvania.
Elworthy, D. (1994) ?Does Baum-Welch Re-
estimation Help Taggers??, Proceedings of
ANLP, Stuttgart, Germany, pp. 53?58.
Gildea, D. (2001) ?Corpus variation and parser
performance?, Proceedings of EMNLP, Pitts-
burgh, PA.
Hockenmaier, J. (2003) Data and models for sta-
tistical parsing with Combinatory Categorial
Grammar, PhD Dissertation, The Univer-
sity of Edinburgh.
Hwa, R. (1999) ?Supervised grammar induction
using training data with limited constituent
information?, Proceedings of ACL, College
Park, Maryland, pp. 73?79.
Inui, K., V. Sornlertlamvanich, H. Tanaka and
T. Tokunaga (1997) ?A new formalization
of probabilistic GLR parsing?, Proceedings
31
of IWPT, MIT, Cambridge, Massachusetts,
pp. 123?134.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple
and R. Kaplan (2003) ?The PARC700 De-
pendency Bank?, Proceedings of LINC, Bu-
dapest.
Lin, D. (1998) ?Dependency-based evaluation
of MINIPAR?, Proceedings of Workshop at
LREC?98 on The Evaluation of Parsing Sys-
tems, Granada, Spain.
McClosky, D., Charniak, E. and M. Johnson
(2006) ?Effective self-training for parsing?,
Proceedings of HLT-NAACL, New York.
Merialdo, B. (1994) ?Tagging English Text with
a Probabilistic Model?, Computational Lin-
guistics, vol.20.2, pp.155?171.
Miyao, Y. and J. Tsujii (2002) ?Maximum En-
tropy Estimation for Feature Forests?, Pro-
ceedings of HLT, San Diego, California.
Oepen, S., K. Toutanova, S. Shieber, C. Man-
ning, D. Flickinger, and T. Brants (2002)
?The LinGO Redwoods Treebank: Motiva-
tion and preliminary applications?, Proceed-
ings of COLING, Taipei, Taiwan.
Pereira, F and Y. Schabes (1992) ?Inside-
Outside Reestimation From Partially
Bracketed Corpora?, Proceedings of ACL,
Delaware.
Prescher, D. (2001) ?Inside-outside estimation
meets dynamic EM?, Proceedings of 7th
Int. Workshop on Parsing Technologies
(IWPT01), Beijing, China.
Riezler, S., T. King, R. Kaplan, R. Crouch,
J. Maxwell III and M. Johnson (2002)
?Parsing the Wall Street Journal using a
Lexical-Functional Grammar and Discrimi-
native Estimation Techniques?, Proceedings
of 40th Annual Meeting of the Association
for Computational Linguistics, Philadelphia,
pp. 271?278.
Sampson, G. (1995) English for the Computer,
Oxford University Press, Oxford, UK.
Siegel S. and N. J. Castellan (1988) Nonpara-
metric Statistics for the Behavioural Sci-
ences, 2nd edition, McGraw-Hill.
Tadayoshi, H., Y. Miyao and J. Tsujii (2005)
?Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain?,
Proceedings of IJCNLP, Jeju Island, Korea.
Tomita, M. (1987) ?An Efficient Augmented
Context-Free Parsing Algorithm?, Computa-
tional Linguistics, vol.13(1?2), pp.31?46.
Watson, R. and E.J. Briscoe (2007) ?Adapting
the RASP system for the CoNLL07 domain-
adaptation task?, Proceedings of EMNLP-
CoNLL-07, Prague.
Watson, R., J. Carroll and E.J. Briscoe (2005)
?Efficient extraction of grammatical rela-
tions?, Proceedings of 9th Int. Workshop on
Parsing Technologies (IWPT?05), Vancou-
ver, Ca..
32
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 1?4,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Camtology: Intelligent Information Access for Science
Ted Briscoe1,2, Karl Harrison5, Andrew Naish-Guzman4, Andy Parker1,
Advaith Siddharthan3, David Sinclair4, Mark Slater5 and Rebecca Watson2
1University of Cambridge 2iLexIR Ltd 4Camtology Ltd 5University of Birmingham
ejb1@cl.cam.ac.uk,
parker@hep.phy.cam.ac.uk,
rfw@ilexir.co.uk
3University of Aberdeen
advaith@abdn.ac.uk
david.sinclair@imense.co.uk,
a.naish@gmail.com
kh@hep.ph.bham.ac.uk,
mws@hep.ph.bham.ac.uk
Abstract
We describe a novel semantic search engine
for scientific literature. The Camtology sys-
tem allows for sentence-level searches of PDF
files and combines text and image searches,
thus facilitating the retrieval of information
present in tables and figures. It allows the user
to generate complex queries for search terms
that are related through particular grammati-
cal/semantic relations in an intuitive manner.
The system uses Grid processing to parallelise
the analysis of large numbers of papers.
1 Introduction
Scientific, technological, engineering and medi-
cal (STEM) research is entering the so-called 4th
Paradigm of ?data-intensive scientific discovery?, in
which advanced data mining and pattern discovery
techniques need to be applied to vast datasets in or-
der to drive further discoveries. A key component
of this process is efficient search and exploitation of
the huge repository of information that only exists in
textual or visual form within the ?bibliome?, which
itself continues to grow exponentially.
Today?s computationally driven research methods
have outgrown traditional methods of searching for
scientific data, creating a widespread and unfulfilled
need for advanced search and information extrac-
tion. Camtology combines text and image process-
ing to create a unique solution to this problem.
2 Status
Camtology has developed a search and information
extraction system which is currently undergoing us-
ability testing with the curation team for FlyBase1,
a $1m/year NIH-funded curated database covering
the functional genomics of the fruit fly. To provide
a scalable solution capable of analysing the entire
STEM bibliome of over 20m electronic journal and
1http://flybase.org/
conference papers, we have developed a robust sys-
tem that can be used with a grid of computers run-
ning distributed job management software.
This system has been deployed and tested using
a subset of the resources provided by the UK Grid
for Particle Physics (Britton et al, 2009), part of the
worldwide Grid of around 200000 CPU cores as-
sembled to allow analysis of the petabyte-scale data
volumes to be recorded each year by experiments at
the Large Hadron Collider in Geneva. Processing
of the FlyBase archive of around 15000 papers re-
quired about 8000 hours of CPU time, and has been
successfully completed in about 3 days, with up to a
few hundred jobs run in parallel. A distributed spi-
der for collecting open-source PDF documents has
also been developed. This has been run concurrently
on over 2000 cores cores, and has been used to re-
trieve over 350000 subject-specific papers, but these
are not considered in the present demo.
3 Functionality
Camtology?s search and extraction engine is the first
to integrate a full structural analysis of a scientific
paper in PDF format (identifying headings, sections,
captions and associated figures, citations and ref-
erences) with a sentence-by-sentence grammatical
analysis of the text and direct visual search over
figures. Combining these capabilities allows us to
transform paper search from keyword based paper
retrieval, where the end result is a set of putatively
relevant PDF files which need to be read, to informa-
tion extraction based on the ability to interactively
specify a rich variety of linguistic patterns which
return sentences in specific document locales, and
which combine text with image-based constraints;
for instance:
?all sentences in figure captions which contain
any gene name as the theme of the action ?ex-
press? where the figure is a picture of an eye?
1
Camtology allows the user to build up such com-
plex queries quickly though an intuitive process of
query refinement.
Figures often convey information crucial to the
understanding of the content of a paper and are typ-
ically not available to search. Camtology?s search
engine integrates text search to the figure and cap-
tion level with the ability to re-rank search returns on
the basis of visual similarity to a chosen archetype
(ambiguities in textual relevance are often resolved
by visual appearance). Figure 1 provides a compact
overview of the search functionality supported by
our current demonstrator. Interactively, constructing
and running such complex queries takes a few sec-
onds in our intuitive user interface, and allows the
user to quickly browse and then aggregate informa-
tion across the entire collection of papers indexed by
the system. For instance, saving the search result
from the example above would yield a computer-
readable list of gene names involved in eye develop-
ment (in fruit flies in our demonstrator) in a second
or so. With existing web portals and keyword based
selection of PDF files (for example, Google Scholar,
ScienceDirect, DeepDyve or PubGet), a query like
this would typically take many hours to open and
read each one, using cut and paste to extract gene
names (and excludes the possibility of ordering re-
sults on a visual basis). The only other alterna-
tive would require expensive bespoke adaptation of
a text mining system by IT professionals using li-
censed software (such as Ariadne Genomics, Temis
or Linguamatics). This option is only available to a
tiny minority of researchers working for large well-
funded corporations.
4 Summary of Technology
4.1 PDF to SciXML
The PDF format represents a document in a
manner designed to facilitate printing. In short,
it provides information on font and position for
textual and graphical units. To enable informa-
tion retrieval and extraction, we need to convert
this typographic representation into a logical one
that reflects the structure of scientific documents.
We use an XML schema called SciXML (first
introduced in Teufel et al (1999)) that we extend
to include images. We linearise the textual ele-
ments in the PDF, representing these as <div>
elements in XML and classify these divisions as
{Title|Author|Affiliation|Abstract|Footnote|Caption|
Heading|Citation| References|Text} in a constraint
satisfaction framework.
In addition, we identify all graphics in the PDF,
including lines and images. We then identify ta-
bles by looking for specific patterns of text and
lines. A bounding box is identified for a table and
an image is generated that overlays the text on the
lines. Similarly we overlay text onto images that
have been identified and identify bounding boxes
for figures. This representation allows us to re-
trieve figures and tables that consist of text and
graphics. Once bounding boxes for tables or fig-
ures have been identified, we identify a one-to-one
association between captions and boxes that min-
imises the total distance between captions and their
associated figures or tables. The image is then ref-
erenced from the caption using a ?SRC? attribute;
for example, in (abbreviated for space constraints):
<CAPTION SRC=
?FBrf0174566 fig 6 o.png?>
<b>Fig. 6. </b> Phenotypic
analysis of denticle belt fusions
during embryogenesis. (A)
The denticle belt fusion phe-
notype resulted in folds around
the surrounding fused... ...(G)
...the only cuticle phenotype
of the DN-EGFR-expressing
embryos was strong denticle
belt fusions in alternating
parasegments (<i>paired
</i>domains).</CAPTION>
Note how informative the caption is, and the value
of being able to search this caption in conjunction
with the corresponding image (also shown above).
4.2 Natural Language Processing
Every sentence, including those in abstracts, titles
and captions, is run through our named-entity recog-
niser and syntactic parser. The output of these sys-
tems is then indexed, enabling semantic search.
Named Entity Recognition
NER in the biomedical domain was implemented
as described in Vlachos (2007). Gene Mention
tagging was performed using Conditional Random
Fields and syntactic parsing, using features derived
from grammatical relations to augment the tagging.
We also use a probabilistic model for resolution of
non-pronominal anaphora in biomedical texts. The
model focuses on biomedical entities and seeks to
find the antecedents of anaphora, both coreferent
and associative ones, and also to identify discourse-
new expressions (Gasperin and Briscoe, 2008).
2
Parsing
The RASP toolkit (Briscoe et al, 2006) is used
for sentence boundary detection, tokenisation, PoS
tagging and finding grammatical relations (GR) be-
tween words in the text. GRs are triplets consisting
of a relation-type and two arguments and also en-
code morphology, word position and part-of-speech;
for example, parsing ?John likes Mary.? gives us a
subject relation and a direct object relation:
(|ncsubj| |like+s:2 VVZ| |John:1 NP1|)
(|dobj| |like+s:2 VVZ| |Mary:3 NP1|)
Representing a parse as a set of flat triplets allows
us to index on grammatical relations, thus enabling
complex relational queries.
4.3 Image Processing
We build a low-dimensional feature vector to sum-
marise the content of each extracted image. Colour
and intensity histograms are encoded in a short bit
string which describes the image globally; this is
concatenated with a description of the image derived
from a wavelet decomposition (Jacobs et al, 1995)
that captures finer-scale edge information. Efficient
similar image search is achieved by projecting these
feature vectors onto a small number of randomly-
generated hyperplanes and using the signs of the
projections as a key for locality-sensitive hashing
(Gionis et al, 1999).
4.4 Indexing and Search
We use Lucene (Goetz, 2002) for indexing and re-
trieving sentences and images. Lucene is an open
source indexing and information retrieval library
that has been shown to scale up efficiently and han-
dle large numbers of queries. We index using fields
derived from word-lemmas, grammatical relations
and named entities. At the same time, these complex
representations are hidden from the user, who, as a
first step, performs a simple keyword search; for ex-
ample ?express Vnd?. This returns all sentences that
contain the words ?express? and ?Vnd? (search is
on lemmatised words, so morphological variants of
?express? will be retrieved). Different colours rep-
resent different types of biological entities and pro-
cesses (green for a gene), and blue shows the entered
search terms in the result sentences. An example
sentence retrieved for the above query follows:
It is possible that like ac , sc and l?sc ,
vnd is expressed initially in cell clusters and
then restricted to single cells .
Next, the user can select specific words in the
returned sentences to indirectly specify a relation.
Clicking on a word will select it, indicated by un-
derlining of the word. In the example above, the
words ?vnd? and ?expressed? have been selected by
the user. This creates a new query that returns sen-
tences where ?vnd? is the subject of ?express? and
the clause is in passive voice. This retrieval is based
on a sophisticated grammatical analysis of the text,
and can retrieve sentences where the words in the
relation are far apart. An example of a sentence re-
trieved for the refined query is shown below:
First , vnd might be spatially regulated in a
manner similar to ac and sc and selectively
expressed in these clusters .
Camtology offers two other functionalities. The
user can browse the MeSH (Medical Subject Head-
ings) ontology and retrieve papers relevant to a
MeSH term. Also, for both search and MeSH brows-
ing, retrieved papers are plotted on a world map; this
is done by converting the affiliations of the authors
into geospatial coordinates. The user can then di-
rectly access papers from a particular site.
5 Script Outline
I Quick overview of existing means of searching sci-
ence (PubMed, FlyBase, Google Scholar).
II Walk through the functionality of Camtology (these
are numbered in Figure 1:
? (1) Initial query through textual search box; (2)
Retrieval of relevant sentences; (3) Query re-
finement by clicking on words; (4) Using im-
plicit grammatical relations for new search;
? Alternative to search: (5) Browse MeSH On-
tology to retrieve papers with MeSH terms.
? (6) Specifically searching for tables/figures
? (7) Viewing the affiliation of the authors of re-
trieved papers on a world map.
? (8) Image search using similarity of image.
6 Acknowledgements
This work was supported in part by a STFC miniP-
IPSS grant to the University of Cambridge and
iLexIR Ltd.
References
T. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the RASP system. In Proc. ACL 2006.
D. Britton, AJ Cass, PEL Clarke, et al 2009. GridPP:
the UK grid for particle physics. Philosophical Trans-
actions A, 367(1897):2447.
3
C. Gasperin and T. Briscoe. 2008. Statistical anaphora
resolution in biomedical texts. In Proc. COLING?08.
A. Gionis, P. Indyk, and R. Motwani. 1999. Similarity
search in high dimensions via hashing. In Proc. 25th
ACM Internat. Conf. on Very Large Data Bases.
B. Goetz. 2002. The Lucene search engine: Powerful,
flexible, and free. Javaworld http://www. javaworld.
com/javaworld/jw-09-2000/jw-0915-lucene. html.
C.E. Jacobs, A. Finkelstein, and D.H. Salesin. 1995. Fast
multiresolution image querying. In Proc. 22nd ACM
annual conference on Computer graphics and interac-
tive techniques.
S. Teufel, J. Carletta, and M. Moens. 1999. An annota-
tion scheme for discourse-level argumentation in re-
search articles. In Proc. EACL?99.
A. Vlachos. 2007. Tackling the BioCreative2 gene men-
tion task with CRFs and syntactic parsing. In Proc.
2nd BioCreative Challenge Evaluation Workshop.
F
ig
ur
e
1:
S
cr
ee
ns
ho
ts
sh
ow
in
g
fu
nc
ti
on
al
it
y
of
th
e
C
am
to
lo
gy
se
ar
ch
en
gi
ne
.
4
