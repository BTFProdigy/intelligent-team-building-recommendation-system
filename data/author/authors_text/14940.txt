Proceedings of BioNLP Shared Task 2011 Workshop, pages 102?111,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP 2011 Task Bacteria Biotope ? The Alvis system 
Zorana Ratkovic1,2   Wiktoria Golik1    Pierre Warnier1   Philippe Veber1   Claire N?dellec1 
1 MIG INRA UR1077, Domaine de Vilvert F-850 Jouy-en-Josas, France forename.name@jouy.inra.fr   
2 LaTTiCe UMR 8094 CNRS Univ. Paris 3 1 rue Maurice Arnoux F-92120 MONTROUGE   Abstract 
This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope (BB) task of the Bi-oNLP 2011 shared tasks. Bacteria, geo-graphical locations and host entities were processed by a pattern-based approach and domain lexical resources. For the extraction of environment locations, we propose a framework based on semantic analysis sup-ported by an ontology of the biotope do-main. Domain-specific rules were devel-oped for dealing with Bacteria anaphora. Official results show that our Alvis system achieves the best performance of participat-ing systems. 
1 Introduction Given a set of Web pages, the information extrac-tion goal of the Bacteria Biotope (BB) task is to precisely identify bacteria and their locations and to relate them. The type of the predicted locations has to be selected among eight types. Among them the host and host-part locations have to be related by the part-of relation. Three teams participated in the challenge.  BB task example Ureaplasma parvum is a mycoplasma and a pathogenic 
ureolytic mollicute which colonises 
 the urogenital tracts of humans.  One of the specificities of the BB task is that the bacteria location vocabulary is very large and vari-ous as opposed to protein subcellular locations in 
biology challenges (Kim et al, 2010) and geo-graphical locations (Zhou et al, 2005). Locations include natural environments and hosts as well as food and medical locations. In order to deal with this heterogeneity, we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope (BB) termino-ontology. This mapping derives the type of location terms and filters out non-location terms. Large external dictionaries of host names (i.e. NCBI taxonomy) and geographical names (i.e. Agrovoc thesaurus) complete the lexical resources. The high frequency of bacteria anaphora and ambiguous antecedent candidates in the corpus was also a difficulty. Our Alvis system implements an anaphora resolution algorithm that takes into con-sideration the anaphoric distance and the position of the antecedent in the sentence. Alvis predicts the bacteria names and their relation to the locations with the help of hand-made patterns based on lin-guistic analysis and lexical resources.  The methods for predicting and typing locations (section 2) and bacteria (section 3) are first de-scribed. Section 4 details the method for relating them. Section 5 comments the experimental results. 2 Location  Our system handles separately the recognition of host and geographical names by dictionary map-pings, while the recognition of locations of the en-vironment and host part types is based on linguistic analysis and ontology inference.  Host names and geographical names appeared to be easier to predict by using a named-entity recog-nition strategy than the other types of location. They are less subject to variation than environ-mental locations, which can include any physical feature. For host name extraction, we used the NCBI taxonomy as the major source. Only the eu-karyote subtree was considered for host detection. 
Localization 
Part-of 
102
Our system filters out the ambiguous names such as Indicator (honeyguides) or Dialysis (xylophage insect) by comparing them to a list of common words in English. The host name list was enriched with additional common names including non-taxonomic host groups (e.g. herbivores), progeny names (e.g. calf) and human categories (e.g. pa-tient). The resulting host name list contains more than 1,800,000 scientific names and 60,000 com-mon names. The geographical name recognition component uses a small dictionary of all geo-graphic terms from the Agrovoc thesaurus sub-vocabularies. At first, we considered using the very rich resource GeoNames. However, it contains too many ambiguous names to be directly usable by short-term development. 2.1 Location of Environment type The identification of environment locations is done in two steps. First, the automatic extraction of all candidate terms from the test corpus, then the as-signment of a location type to these terms with the help of the Bacteria Biotope (BB) termino-ontology. The type assigned to a given term is the type of the closest concept label in the ontology. Since the BB termino-ontology was originally not structured according to the eight types, in order to be usable it first had to be enriched by the new concepts and then mapped to this topology.  Corpus term extraction. The corpus terms were automatically extracted by the AlvisNLP/ML pipe-line (Nedellec et al, 2008) with BioYatea (Nedel-lec et al, 2010). BioYatea is the version of Yatea (Hamon & Aubin, 2006) adapted to the biology domain. We modified BioYatea setting according to the training dataset study. We observed that most of the location terms in the training dataset are noun phrases with adjective modifiers (e.g. ro-dent nests) while prepositional phrases are rather rare (e.g. breaks in the skin). We set the term boundaries of BioYatea to include all prepositions except the of preposition. Considering other prepo-sitions such as with may yield syntactic attachment errors, thus we prefer the risk of incomplete terms to incorrect prepositional attachments. Bacteria Biotope ontology. We used the Bacte-ria Biotope (BB) termino-ontology for typing the extracted terms. It is under development for the study of bacteria phenotypes and habitats. The high level of the habitat part is structured in a manner similar to that proposed by the one level classifica-
tion by Floyd (Floyd et al, 2005). It has a fine-grained structure with the same goal as the general-ist EnvO habitat ontology (Field et al, 2008), but it focuses on bacteria phenotype and biotope model-ing. It includes a terminological level that records lexical forms of the concepts including terms, synonyms and variations. For the purpose of the challenge, the initial on-tology was manually completed using location concepts. The training corpus, as well as the habitat and isolation site fields of the GOLD database on sequenced prokaryotes (Liolios et al, 2009) are the main sources of location terms and synonyms. The analysis of the training corpus mainly led to the addition of adjectival forms of host parts (e.g. lym-phatic, intracellular) and human references (e.g. patient, infant, progeny).  The GOLD database isolation site field is a very rich source of bacteria location terms. It is filled by natural language descriptions of matters, natural habitats, hosts and geographical locations. For in-stance, the isolation site of Anoxybacillus flavi-thermus bacterium is waste water drain at the Wairakei geothermal power station in New Zea-land. The term analysis of GOLD isolation site en-tries yielded 3,415 location terms including 1,050 geographical names. Hundreds of these terms were manually added to the BB termino-ontology. The lack of time as well as the full sentence structure of the GOLD resource prevented us from correctly handling them in a fully automatic way. We are currently developing a method for the automatic alignment of the terms extracted from GOLD to the BB termino-ontology. Additionally, the GOLD habitat field provided around a hundred different terms that have been directly integrated into the BB termino-ontology. The current version of the habitat subpart of the BB termino-ontology contains 1,247 concepts and 266 synonyms.  Location types in Bacteria Biotope ontology. The BB termino-ontology has been developed pre-vious to the BB task and the structure of its habitat subpart does not reflect the eight location types of the task. In order to reuse the ontology for the BB task, we assigned types to each location concept. We manually associated the high level nodes of the location hierarchies to the eight BB task types. The types of the lower level concepts were then auto-matically inferred. For instance, the concept aquatic environment is tagged Water in the ontol-
103
ogy and all of its descendants lake, sea, ocean are of type Water as well. Local type exceptions were manually tagged. For instance, the waste tree in-cludes water-carried wastes of type Water and solid industrial residues of type Environment. This way all concepts in the resulting typed ontology were assigned a unique type. The concept types are then propagated to their associated term classes at the terminological level. For instance, underground water and its synonym subterranean water are both typed as Water. The resulting typed BB termino-ontology is then usable for deriving the types of the terms extracted from the test corpus. Derivation of location type. The BB termino-ontology scope is too limited for the correct predic-tion of all candidate term types by Boolean and exact comparison. From the 2,290 candidate terms of the test corpus, only 152 belong as such to the BB termino-ontology. We propose a method based on the head comparison of the candidate and BB terms for the derivation of the candidate term type.  The quality of the ontology-based annotation depends to a large extent on an accurate match be-tween the resource and the terms extracted from the corpus. Our method targets the syntactic structure of terms (candidate and BB terms) in order to gath-er the most of semantically similar terms. This approach differs from the ontology alignment and population methods that also use the information from the ontology structure in order to infer seman-tic relationships (e.g. hyponyms, meronyms) (Eu-zenat, 2007). It also differs from semantic annota-tion supported by context analysis such as distribu-tional semantics (Grefenstette, 1994) or Hearst pat-terns (Hearst, 1992). It belongs to the class of methods that focus on the morphology of the cor-pus terms, which use string-based (Levensthein, 1966, Jaro, 1989) or linguistic-based methods (Jac-quemin & Tzoukermann, 1999).  Even though the context-based approach should produce very good results, we chose a less time-consuming method that is easier and faster to set up, which is based on morphosyntactic analysis.  In our case, string similarity measures turn out to be irrelevant (laboratory rat does not mean rat labo-ratory). We observed that in candidate and BB terms, the head is very often the most informative element. Thus, the linguistic-based analysis of terms, in particular the head-similarity analysis (Hamon & Nazarenko, 2001), represents a promis-ing alternative. Our method is inspired by 
MetaMap (Aronson, 2001). MetaMap tags bio-medical corpora with the UMLS Metathesaurus by syntactic analysis that takes into account lexical heads of terms. The similarity scores computed by linguistically-based metrics are higher for terms whose heads have previously been analyzed.  The MetaMap method includes a variant compu-tation that maps acronyms, abbreviations, syno-nyms as well as derivational, inflectional and spell-ing variants. Our term typing method is less sophis-ticated and uses a few lexical variants due to the lack of a complete resource. Some ontology en-richment applications also use head-supported term matching, as in Desmontils (Desmontils et al, 2003). In Desmontils, new concepts belonging to WordNet (Fellbaum, 1998) are automatically added to the ontology in order to improve the indexing process. However, the analysis of the results shows that a great number of concepts found in the texts are not considered because they do not exist in WordNet. Our typing task uses a similar head-based method, but only for type derivation.  Our system derives the location type of candi-date terms in several steps. First, if there is a term in the BB termino-ontology that is strictly equal to the candidate term, it is assigned the same type. Then, the other candidate terms are assigned types according to the comparison of their heads to the BB term heads. We assume that in most of the cases the term head conveys the information about the type and is non-ambiguous. A given head H is non-ambiguous if all BB terms with head H are of the same type. The location term head set is the set of all habitat term heads found in the BB termino-ontology. The current version contains 693 differ-ent heads. Let Te denote the extracted term to be typed. If the head of Te does not belong to the BB term head set, then the type of Te is simply not Lo-cation (e.g. high metabolic diversity). If Te head does belong to the BB term head set and the head is non-ambiguous, then Te is assigned the associated type. For instance, the head of the extracted term stratified lake is lake. The type of all the BB terms with lake head is Water (e.g. meromictic lake). Stratified lake is therefore typed as Water.  Specific processing is applied to terms with am-biguous heads. The associative set of BB term heads and types exhibits some cases of ambiguous heads with multiple types that we analyzed in de-tail. There are two kinds of ambiguities that were 
104
processed in different ways. In the first, multiple types reflect different roles of the same object. In the second, the head is non-informative with re-spect to the type.  In the latter case the type is con-veyed by the subterm (term after head removal). We qualify non-informative BB term heads as neu-tral. They mainly denote habitats (habitat, envi-ronment, medium, zone) and extracts (sample, sur-face, isolate, material, content). In this case, the type is derived from the subterm. For instance, the head isolate of the extracted term marine isolate is neutral. After head removal, it is assigned the type Water since marine is of type Water. Freshwater has the same type as freshwater medium or fresh-water environment since medium and environment are neutral heads.  Some heads have more than one type although they denote specific locations. Their multiple types reflect different uses or states. For instance, the head bottle has two types: Food and Medical. The type Food is derived from the BB concept water bottle and the type Medical is derived from bedside water bottles in a hospital environment. The correct type for the extracted terms is then selected by a set of patterns based on the context of the term in the document. For instance, many vegetables and meats could be either of type Host or Food. The type is Host by default. One pattern states that if a term includes or is preceded by a food processing-related word (e.g. cooked, grilled, fermented), then the term is reassigned the type Food. Another pat-tern states that if a host is preceded by a death-related adjective (dead, decaying), then its type should be revised as Environment.  Our system currently includes nine disambigua-tion/retyping patterns. The first version of the type derivation method was automatically applied to the 1,263 GOLD terms after head analysis. Manual examination of the results yielded an extension of the two lists of neutral heads and heads with am-biguous types. There are 20 neutral heads and 21 ambiguous heads in the current version of the BB termino-ontology. The head-matching algorithm appears to be quite productive for the biotope terms. The procedure applied to the test corpus yielded the following figures: BioYatea extracted 2,290 terms. 416 terms matching the post-processing filters were discarded. This includes terms which are too general (i.e. approach, diver-sity), terms containing irrelevant or non desirable adjectives (i.e. numerous deficiencies, known spe-
cies) and terms containing forbidden words accord-ing to the annotation location rules (i.e. bacteria, pathogen, contaminated, parasite). Finally, 1,873 candidate terms were kept. Among these figures:  - 152  terms belong to the BB termino-ontology  - 90  terms were typed using the ontology heads - 6 terms with several types were handled by disambiguation patterns. We plan to extend the list of neutral heads and dis-criminate adjectives for type disambiguation by machine learning classification applied to the BB termino-ontology modifiers. Location entity boundary. The analysis of term extraction result from the training corpus shows that the predicted boundaries of locations were not fully consistent with the task annotation guidelines. Post-processing adjusts incorrect boundaries by filtering irrelevant words, packing and merging terms. Irrelevant words (e.g. contaminated, in-fected, host species, disease, inflammation) were removed from the location candidate terms inde-pendently of their types (e.g. contaminated Bach-man Road site vs. Bachman Road ; host plant vs. plant). Note that BioYatea extracts not only the maximum terms (e.g. contaminated Bachman Road site), but also their constituents (Bachman Road site, Bachman Road and site). Boundary adjust-ment often consists in selecting the relevant alter-native among the subterms.  Other boundary issues are handled by several patterns, which are applied after the typing stage. These patterns are type-dependent: each pattern only applies to one type or a subset of location types. When necessary, they shift the boundaries in order to include relevant modifiers.  They also split location terms or join adjacent location terms. BioYatea may have missed relevant modifiers be-cause of POS-tagging errors. For instance, if a na-tionality name precedes a location, then it is in-cluded (e.g. German oil field). Also, it frequently happens that hosts are modifiers of host parts (e.g. insect gut). BioYatea extracts the whole term and its constituents. The term is correctly typed as Host-part and the host modifier as Host. In order to avoid embedded locations, a specific pattern is de-voted to the splitting of these terms. In this way insect gut (Host-part) becomes insect (Host) and gut (Host-part). Most of these patterns involve several specific lexicons, including cardinal directions, relevant and 
105
irrelevant modifiers for each type of location, as well as types, which can be merged and split. The current resources were manually built by examin-ing the location terms of the training set and GOLD isolation fields. The acquisition of relevant and irrelevant modifiers could be automated by ma-chine learning. Some linguistic phenomena could be better handled by the customization of BioYa-tea. For instance BioYatea considers the preposi-tion with as a term boundary so it cannot extract terms containing with, like areas with high sulfur and salt concentrations.  3 Extraction of Bacteria names  We observed in the training corpus that not only were bacteria names tagged, but also higher level taxa (families) and lower level taxa (strains). We used the NCBI taxonomy as the main bacteria tax-on resource since it includes all organism levels and is kept up-to-date. This bacteria dictionary was enriched by taxa from the training corpus, in par-ticular by non standard abbreviations (e.g. Chl. = Chlorobium, ssp. = subsp) and plurals, (Vibrios as the plural for Vibrio) that were hopefully rather rare. Determining the boundaries of the bacteria names was one of the main issues because corpus strain names do not always follow conventional nomenclature rules.  Also, the recognition of bacte-ria name is evaluated using a strict exact match. Patterns were developed to account for such cases. They handle inversion (LB400 of Burkholderia xe-novorans instead of Burkholderia xenovorans LB400) and parenthesis (Tropheryma whipplei (the Twist strain) instead of Tropheryma whipplei strain Twist).  The corpus also mentions names of bacte-ria that contain modifiers not found in the NCBI dictionary, such as antimicrobial-resistant C. coli or L. pneumophila serogroup 1. Such cases, as well as abbreviations (e.g. GSB for green sulfur bacte-ria) and partial strain names (e.g. strain DSMZ 245 T for Chlorobium limicola strain DSMZ 245 T) were also specifically handled. The main source of error in bacteria name pre-diction is due to the mixture of family names and strain name abbreviations in the same text. It fre-quently happens that the strain name is abbreviated into the first word of the name. For instance Bar-tonella henselae is abbreviated as Bartonella. Un-fortunately, Bartonella is a genus mentioned in the 
same text, thus yielding ambiguities between the anaphora and the family name, which are identical. 3.1 Bacteria anaphora resolution Anaphors are frequent in the text, especially for bacteria reference and to a smaller extent for host reference. Our effort focused on bacteria anaphora resolution ignoring host anaphora. The extraction method of location relations (section 4) assumes that the relation arguments, location and bacterium (or anaphora of the bacterium) occur in the same sentence. From a total of 2,296 sentences in the training corpus, only 363 sentences contain both the location and the explicit bacterium, while 574 mention only the location. Two thirds of the loca-tions do not co-occur with bacteria. This demon-strates the importance of recovering the bacteria for these cases, which is potentially referred to by an explicit anaphora.  The manual examination of the training corpus showed that the most frequent anaphora of bacteria are not pronouns but higher level taxa, often pre-ceded by a demonstrative determinant, (i.e. This bacteria, This Clostridium) and sortal anaphora (i.e. genus, organism, species and strain), both of which are commonly found in biological texts (Torri & Vijay-Shanker, 2007). The style of some of the documents is rather relaxed and the antece-dent may be ambiguous even for a human reader. We observed three types of anaphora in the corpus. First, the standard anaphora which includes both pronouns and sortal anaphora, which requires a unique bacterial antecedent. Second, bi-anaphora or an anaphora that requires two bacteria antece-dents. This happens when the properties of two strains are compared in the document. Finally, the case of a higher taxon being used to refer to a lower taxon, which we named name taxon anaph-ora.  Anaphora with a unique antecedent C. coli is pathogenic in animals and humans. Peo-ple usually get infected by eating poultry that con-tained the bacteria, eating raw food, drinking raw milk, and drinking bottle water [?].  Anaphora with two antecedents C. coli is usually found hand in hand with its bac-teria relative, C. jejuni. These two organisms are recognized as the two most leading causes of acute inflammation of intestine in the United States and other nations. 
106
 Name taxon anaphora Ticks become infected with Borrelia duttonii while feeding on an infected rodent. Borrelia then multi-plies rapidly, causing a generalized infection throughout the tick.  For anaphora detection and resolution a pattern-based approach was preferred to machine learning because the constraints for relating anaphora to antecedent candidates of the same taxonomy level were mainly semantic and domain-dependent and the annotation of anaphora was not provided in the training corpus.  Anaphora detection consists of identifying po-tential anaphora in the corpus, given a list of pro-nouns, sortal anaphora and taxa and then filtering out irrelevant cases (Segura-Bedmar et al, 2010, Lin & Lian, 2004) before anaphora resolution. Not all the pronouns, sortal anaphora terms and higher taxon bacteria are anaphoric. For example, if a higher taxon is preceded or followed by the word genus, this signals that it is not anaphoric but that the text is actually about the higher taxon.   Non-anaphoric higher taxon Burkholderia cenocepacia HI2424[?] The genus Burkholderia consists of some 35 bacte-rial species, most of which are soil saprophytes and phytopathogens that occupy a wide range of environmental niches.  The anaphora resolution algorithm takes into ac-count two features: the distance to the antecedent candidate and its position in the sentence. The an-tecedent is usually found in proximity to the ana-phora, in order to maintain the coherence of the text. Therefore, our method ranks the antecedent candidates according to the anaphoric distance counted in sentences.  If more than one bacterium is found in a given sentence, their position is discriminate. Centering theory states that in a sentence the most prominent entities and therefore the most probable antecedent candidates are in the order: subject > object > other position (Grosz et al, 1995). In English, due to the SVO order of the language the subject is most of-ten found at the beginning of the sentence, fol-lowed by the object and the others. Therefore, the method retains the leftmost bacterium in the sen-tence when searching for the best antecedent can-didate. 
More precisely, the method selects the first ante-cedent that it finds according to the following pre-cedence list: - First bacterium in the current sentence (s) - First bacterium in the previous sentence    (s-1) - First bacterium in sentence s-2 - First bacterium in sentence s-3 - First bacterium in the current paragraph - Last bacterium in the previous paragraph - First bacterium in the first sentence of the document - The first bacterium ever mentioned. -  The method only relates anaphora to antecedents that are found before. It does not handle cataphors since they are rarely found in the corpus. For ana-phors that require two antecedents we use the same criteria but search for two bacteria in each sentence or paragraph, instead of one. For taxon anaphora we look for the presence of a lower taxon in the document found before the anaphora that is com-patible according to the species taxonomy. The counts of anaphora detected by the patterns are given in Table 1.   Corpus Single ante Bi ante Taxon ante Train 933 4 129 Dev 204 3 22 Test 240 0 18 Total 1,377 7 169  Table 1. The count of the types of anaphora per corpus.  The anaphora resolution algorithm allowed us to retrieve more sentences that contain both a bacte-rium and a location.  Out of the 574 sentences that contain only a location, 436 were found to contain an anaphora related to at least one bacterium. The remaining 138 sentences are cases where there is no bacterial anaphora or the bacterium name is im-plicit. It frequently happens that the bacterium is referred to through its action. For example in the sentence below, the bacterium name could be de-rived from the name of the disease that it causes.   In the 1600s anthrax was known as the "Black bane" and killed over 60,000 cows.  One of the questions we had about the resolution of anaphora is whether anaphora that are found in the same sentence together with a bacterium (there-fore potentially its antecedent) should be consid-
107
ered or not.  We tested this on the development set. We found that removing such anaphora from con-sideration improved the overall score. It yielded an F-score of 53.22% (precision: 46.17%, recall: 62.81%), compared to the original F-score of 50.15% (precision: 41.06%, recall: 64.44%). This improvement in F-score is solely due to an increase in precision, which shows that while resolving anaphora is important and required, the incorrect recognition of terms as anaphora and incorrect anaphora resolution can introduce noise. 4 Relation extraction In this work we concentrated most of our effort on the prediction of entities. For the prediction of events we used a strategy based on the co-occurrence of  arguments and trigger words within a sentence: - If a bacteria name, a location and a trigger word are present in a sentence, then the system pre-dicts a Localization event between the bacte-rium and the location. - If a bacteria anaphora, a location and a trigger word are present in a sentence, then the system predicts a Localization event between each ana-phora antecedent and the location. - If a host, a host part, a bacterium and at least one trigger word are present in a sentence, then the system predicts a PartOf event between the host and the host part.  The list of trigger words contains 20 verbs (e.g. inhabit, colonize, but also discover, isolate), 16 disease markers (e.g. chronic, pathogen) and 19 other relevant words (e.g. ingest, environment, niche). This list was designed by ranking words in the sentences of the training corpus containing both a bacteria name and a location. The ranking crite-rion used was the information gain with respect to whether the sentence contained an event or not. The ranked list was adjusted by removing spurious words and adding domain knowledge words. By removing the constraint of the occurrence of a trigger word in the sentence, we can determine that the maximum recall the method can achieve with this strategy is 47% (precision: 41%, F-score: 44%). The selected trigger word list yielded a re-call close to the maximum, thus it seems that the trigger words do not affect the recall and are suit-able for the task.  
5 Results Table 2 summarizes the official scores that the Bib-liome Alvis system achieved for the Bacteria Biotope Task. It ranked first among three partici-pants. The first column gives the recall of entity prediction. The prediction of hosts and bacteria named-entities achieved a good recall of 84 and 82, respectively.    Entity recall Event recall Event Precis. F-score Bacteria 84 - - - Host 82 61 48 53 Host part 72 53 42 47 Env. 53 29 24 26 Geo. 29 13 38 19 Food - - 29 41 Medical 100 50 33 40 Water 83 60 55 57 Soil 86 69 59 63 Total  45 45 45  Table 2. Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011.  However, geographical locations based on a similar strategy were poorly predicted (29%). Our system predicted only 15 countries. A more appropriate resource of geographical names than the Agrovoc thesaurus would certainly increase the recall of geographical locations.  The host parts, medical, water and soil locations predicted with the same ontology-based method were surprisingly good with a recall of 72, 100, 83 and 86, respectively. The small size of the ontology and the small number of different term heads (i.e. 51 different heads) initially appeared as a limitation factor for reuse on new corpora. The good recall shows that the location vocabulary of the test set has similarities with the training set compared to potential space of location names.  The potential space is reflected by the richness of the GOLD iso-lation site field. This demonstrates the robustness of the type derivation approach based on term heads. The correctness of the derivation type can-not be calculated without a corpus where all the locations and not only bacteria ones are annotated. The recall of the environment location prediction is a little bit lower, 53%. The environment type in-
108
cludes many different types that cannot all be an-ticipated. Therefore the coverage of the BB ter-mino-ontology environment part is limited except for water and soil, which are more focused topics.  The localization event recall (column 2) is on average 20% lower for all types than the location entity recall. The regularity of the difference may suggest that once the argument is identified, the localization relation is equally harder to find by our method independently of the type. The localization event precision (column 3) is more difficult to ana-lyze because many sources of error may be in-volved, such as an incorrect arguments, incorrect anaphora resolution, relation to the wrong bacte-rium among several or the absence of a relation.  The prediction precision of localization events involving soil, water and host is better than envi-ronment and food. The manual analysis of the test corpus shows that in some cases environmental locations were mentioned as potential sources of industrial applications without actually being bac-teria isolation places. For instance, in Other fields of application for thermostable enzymes are starch-processing, organic synthesis, diagnostics, waste treatment, pulp and paper manufacture, and ani-mal feed and human food, the Alvis system errone-ously predicted waste treatment, paper manufac-ture, animal feed and human food.  This is due to the fact that the system does not handle modalities. Such hypotheses are specific to the BB task text genre, i.e. Bacteria sequencing projects. Such pro-jects contain details for potential industrial applica-tions, which are absent from academic literature. Ambiguous types are also a source of error. De-spite the host dictionary cleaning, some ambigui-ties remained. For example, the head canal in tooth root canal is erroneously typed as water and should be disambiguated with its tooth host-part modifier.  After test publication we measured the gain of anaphora resolution by using the on-line service. The anaphora resolution algorithm was found to have a strong impact on the final result.  Running the test set using all of the modules except for the anaphora resolution algorithm yielded a decrease in the F-score by almost 13% (F-score: 32.5%, preci-sion: 48.5%, 24.4%).  This shows that the addition of an anaphora resolution algorithm significantly increases the precision and that a resolution algo-rithm adapted to the Bacteria domain is necessary for the Biotope corpus. 
The part-of event prediction relies on the strict co-occurrence of a bacterium, trigger word, host and host part within a sentence. An additional run with the more relaxed constraint where the bacte-rium can be denoted by an anaphora as well yielded a gain of 6 recall points, a loss of 5 preci-sion points with a net benefit of 1 F-measure point. 6 Discussion The use of trigger words for the selection of sen-tences for relation extraction does not take into ac-count the structure or syntax of the sentence for the prediction of relation arguments. The system pre-dicts all combinations of bacteria and locations as localization events and all combination of host and host parts as part-of event. This has a negative ef-fect on the precision measure since some pairs are irrelevant as in the sentence below.  Baumannia cicadellinicola. This newly discovered or-ganism is an obligate endosymbiont of the leafhopper insect Homalodisca coagulata (Say), also known as the Glassy-Winged Sharpshooter, which feeds on the xylem of plants.  It has been shown that the use of syntactic de-pendencies to extract biological events (such as protein-protein interactions) improves the results of such systems (Erkan et al, 2007, Manine et al, 2008, Airola et al 2008). The use of syntactic de-pendencies could offer a more in depth examina-tion of the syntax and the semantics and therefore allow for a more refined extraction of bacteria-localization and host-host part relations.   Term extraction appears to be a good method for predicting locations including unseen terms, but it is limited by the typing strategy that filters out all terms with unknown heads (with respect to the BB termino-ontology). In the future, we will study the effect of linguistic markers such as enumeration and exemplification structures for recovering addi-tional location terms. For instance, in heated or-ganic materials such as compost heaps, rotting hay, manure piles or mushroom growth medium, our system has correctly typed heated organic ma-terials as environment but not the other examples because of their unknown heads. The promising performance of the Alvis system on the BB task shows that a combination of semantic analysis and domain-adapted resources is a good strategy for information extraction in the biology domain. 
109
References  Agrovoc: http://aims.fao.org/website/AGROVOC-Thesaurus Antti Airola, Sampo Pyysalo, Jari Bj?rne, Tapio Pah-nikkala, Filip Ginter, and Tapio Salakoski. 2008. A Graph Kernel for Protein-Protein Interaction Extrac-tion. BioNLP2008: Current Trends in Biomedical Natural Language Processing, pages 1-9. Alan R. Aronson. 2001. Effective mapping of biomedi-cal  text to the UMLS Metathesaurus: The MetaMap program. Proceedings of AMIA Symposium 2001, pages 17-21. Emmanuel Desmontils, Christine Jacquin and  Laurent Simon. 2003. Ontology enrichment and indexing process. Research report RR-IRIN-03.05, Institut de Recherche en Informatique de Nantes, Nantes, Fran-ce. J?r?me Euzenat and Pavel Shvaiko. 2007. Ontology matching, Springer Verlag, Heidelberg (DE),page 333. Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metage-nomes: the Minimum Information about a Genome Sequence (MIGS) specification. Nature Biotechnol-ogy 26, pages 541-547. GeoNames: http://www.geonames.org/  Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery.  Natural Language Processing and Machine Translation. London: Kluwer Academic Publishers. Barbara J. Grosz, Araving K. Joshi and Scott Weinstein. 1995. Centering: A Framework for Modelling the Lo-cal Coherence of Discourse.  University of Pennsyl-vania Institute for Research in Cognitive Science Technical Reports Series. G?ne? Erkan, Arzucan ?zg?r and Dragomir R. Radev. 2007. Semi-Supervised Classification for Extracting Protein Interaction Sentences using Dependency Parsing. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pag-es 228-237. Thierry Hamon and Sophie Aubin. 2006. Improving term extraction with terminological resources. In Salakoski, T. et al, editors, Advances in Natural Lan-guage Processing 5th International Conference on NLP (Fin- TAL?06), pages 380?387. Springer. Thierry Hamon and Adeline Nazarenko. 2001. Detection of synonymy links between terms: experiment and re-
sults, Recent Advances in Computational Terminol-ogy. Pages 185-208. John Benjamins. Marti A. Hearst. 1992. Automatic acquisition of hypo-nyms from large text corpora. In Zampolli, A.(ed.), Proceedings of the 14 th COLING, pages 539?545, Nantes, France. Christian Jacquemin and Evelyne Tzoukermann. 1999. NLP for term variant extraction: A synergy of mor-phology, lexicon, and syntax. In Strzalkowski, T. (ed.), Natural language information retrieval, volume 7 of Text, speech and language technology, chapter 2, pages  25?74. Dordrecht & Boston: Kluwer Aca-demic Publishers. Matthew A. Jaro. 1989. Advances in record linkage me-thodology as applied to matching the 1985 census of Tampa, Florida. Journal of the American Statistical Association 84(406), pages 414-20. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano and Jun?ichi Tsujii. (to appear). Extract-ing bio-molecular events from literature - the Bi-oNLP?09 shared task. Special issue of the Interna-tional Journal of Computational Intelligence. Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Dok-lady akademii nauk SSSR, 163(4):845-848, 1965. In Russian. English translation in Soviet Physics Dok-lady, 10(8), pages 707-710. Yu-Hsiang Lin and Tyne Liang. 2004. Pronomial and Sortal Anaphora Resolution for Biomedical Litera-ture. In Proceedings of ROCLING XVI: Conference on Computational Linguistics and Speech Processing.  Konstantinos Liolios, I-Min A. Chen., Konstantinos Mavromatis, Nektarios Tavernarakis, Philip Hugen-holtz, Victor M. Markowitz and Nikos C. Kyrpides. 2009. The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. NAR Epub. Alain-Pierre Manine, Erick Alphonse and Philippe Bes-si?res. 2008. Information extraction as an ontology population task and its application to genic interac-tions, 20th IEEE Intl. Conf. Tools with Artificial In-telligence, ICTAI'08., vol. II, pp. 74-81. NCBI taxonomy: http://www.ncbi.nlm.nih.gov/Taxonomy/  Claire N?dellec, Wiktoria Golik, Sophie Aubin and Robert Bossy. 2010. Building Large Lexicalized On-tologies from Text: a Use Case in Indexing Biotech-nology Patents, International Conference on Knowl-edge Engineering and Knowledge Management (EKAW 2010), Lisbon, Portugal. 
110
Isabel Segura-Bedmar, Mario Crespo, C?sar de De Pa-blo-S?nchez and Paloma Mart?nez. 2010. Resolving anaphoras for the extraction of drug-drug interactions in pharmacological documents. BMC Bioinformatics 11(Supl 2):S1. Manabu Torii and K. Vijay-Shanker. 2007. Sortal Anaphora Resolution in Medline Abstracts. Computa-tional Intelligence 23, pages 15-27. Zhou GuoDong, Su Jian, Zhang Jie and Zhang Min. 2005. Exploring Various Knowledge in Relation Ex-traction. In Proceedings of the 43rd Annual Meeting of the ACL, pages 427-434, Ann Arbor. Association for Computational Linguistics.  
111
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99?103,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013: Supporting Resources
Pontus Stenetorp 1 Wiktoria Golik 2 Thierry Hamon 3
Donald C. Comeau 4 Rezarta Islamaj Dog?an 4 Haibin Liu 4 W. John Wilbur 4
1 National Institute of Informatics, Tokyo, Japan
2 French National Institute for Agricultural Research (INRA), Jouy-en-Josas, France
3 University Paris 13, Paris, France
4 National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD, USA
pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr
{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov
Abstract
This paper describes the technical con-
tribution of the supporting resources pro-
vided for the BioNLP Shared Task 2013.
Following the tradition of the previous
two BioNLP Shared Task events, the task
organisers and several external groups
sought to make system development easier
for the task participants by providing auto-
matically generated analyses using a vari-
ety of automated tools. Providing analy-
ses created by different tools that address
the same task also enables extrinsic evalu-
ation of the tools through the evaluation of
their contributions to the event extraction
task. Such evaluation can improve under-
standing of the applicability and benefits
of specific tools and representations. The
supporting resources described in this pa-
per will continue to be publicly available
from the shared task homepage
http://2013.bionlp-st.org/
1 Introduction
The BioNLP Shared Task (ST), first organised in
2009, is an ongoing series of events focusing on
novel challenges in biomedical domain informa-
tion extraction. In the first BioNLP ST, the or-
ganisers provided the participants with automat-
ically generated syntactic analyses from a variety
of Natural Language Processing (NLP) tools (Kim
et al, 2009) and similar syntactic analyses have
since then been a key component of the best per-
forming systems participating in the shared tasks.
This initial work was followed up by a similar ef-
fort in the second event in the series (Kim et al,
2011), extended by the inclusion of software tools
and contributions from the broader BioNLP com-
munity in addition to task organisers (Stenetorp et
al., 2011).
Although no formal study was carried out to es-
timate the extent to which the participants utilised
the supporting resources in these previous events,
we note that six participating groups mention us-
ing the supporting resources in published descrip-
tions of their methods (Emadzadeh et al, 2011;
McClosky et al, 2011; McGrath et al, 2011;
Nguyen and Tsuruoka, 2011; Bjo?rne et al, 2012;
Vlachos and Craven, 2012). These resources have
been available also after the original tasks, and
several subsequent studies have also built on the
resources. Van Landeghem et al (2012) applied a
visualisation tool that was made available as a part
of the supporting resources, Vlachos (2012) em-
ployed the syntactic parses in a follow-up study
on event extraction, Van Landeghem et al (2013)
used the parsing pipeline created to produce the
syntactic analyses, and Stenetorp et al (2012) pre-
sented a study of the compatibility of two different
representations for negation and speculation anno-
tation included in the data.
These research contributions and the overall
positive reception of the supporting resources
prompted us to continue to provide supporting re-
sources for the BioNLP Shared Task 2013. This
paper presents the details of this technical contri-
bution.
2 Organisation
Following the practice established in the
BioNLP ST 2011, the organisers issued an
open call for supporting resources, welcoming
contributions relevant to the task from all authors
of NLP tools. In the call it was mentioned that
points such as availability for research purposes,
support for well-established formats and access
99
Name Annotations Availability
BioC Lemmas and syntactic constituents Source
BioYaTeA Terms, lemmas, part-of-speech and syntactic constituencies Source
Cocoa Entities Web API
Table 1: Summary of tools/analyses provided by external groups.
to technical documentation would considered
favourable (but not required) and each supporting
resource provider was asked to write a brief
description of their tools and how they could
potentially be applied to aid other systems in the
event extraction task. This call was answered
by three research groups that offered to provide
a variety of semantic and syntactic analyses.
These analyses were provided to the shared
task participants along with additional syntactic
analyses created by the organisers.
However, some of the supporting resource
providers were also participants in the main event
extraction tasks, and giving them advance access
to the annotated texts for the purpose of creating
the contributed analyses could have given those
groups an advantage over others. To address this
issue, the texts were made publicly available one
week prior to the release of the annotations for
each set of texts. During this week, the supporting
analysis providers annotated the texts using their
automated tools and then handed the analyses to
the shared task organisers, who made them avail-
able to the task participants via the shared task
homepage.
3 Analyses by External Groups
This section describes the tools that were applied
to create supporting resources by the three exter-
nal groups. These contributions are summarised in
Table 1.
BioC Don Comeau, Rezarta Islamaj, Haibin
Liu and John Wilbur of the National Center for
Biotechnology Information provided the output of
the shallow parser MedPost (Smith et al, 2004)
and the BioLemmatizer tool (Liu et al, 2012),
supplied in the BioC XML format1 for annota-
tion interchange (Comeau et al, 2013). The BioC
format address the problem of interoperability be-
tween different tools and platforms by providing a
unified format for use by various tools. Both Med-
Post and BioLemmatizer are specifically designed
1http://bioc.sourceforge.net/
for biomedical texts. The former annotates parts-
of-speech and performs sentence splitting and to-
kenisation, while the latter performs lemmatisa-
tion. In order to make it easier for participants
to get started with the BioC XML format, the
providers also supplied example code for parsing
the format in both the Java and C++ programming
languages.
BioYaTeA Wiktoria Golik of the French Na-
tional Institute for Agricultural Research (INRA)
and Thierry Hamon of University Paris 13 pro-
vided analyses created by BioYaTeA2 (Golik et
al., 2013). BioYaTeA is a modified version of the
YaTeA term extraction tool (Aubin and Hamon,
2006) adapted to the biomedical domain. Working
on a noun-phrase level, BioYaTeA provides anno-
tations such as lemmas, parts-of-speech, and con-
stituent analysis. The output formats used were a
simple tabular format as well as BioYaTeA-XML,
an XML representation specific to the tool.
Cocoa S. V. Ramanan of RelAgent Private Ltd
provided the output of the Compact cover anno-
tator (Cocoa) for biological noun phrases.3 Co-
coa provides noun phrase-level entity annotations
for over 20 different semantic categories such as
macromolecules, chemicals, proteins and organ-
isms. These annotations were made available for
the annotated texts for the shared task along with
the opportunity for the participants to use the Co-
coa web API to annotate any text they may con-
sider beneficial for their system. The data format
used by Cocoa is a subset of the standoff format
used for the shared task entity annotations, and it
should thus be easy to integrate into existing event
extraction systems.
4 Analyses by Task Organisers
This section describes the syntactic parsers ap-
plied by the task organisers and the pre-processing
2http://search.cpan.org/?bibliome/
Lingua-BioYaTeA/
3http://npjoint.com/
100
Name Model Availability
Enju Biomedical Binary
Stanford Combination Binary, Source
McCCJ Biomedical Source
Table 2: Parsers used for the syntactic analyses.
and format conversions applied to their output.
The applied parsers are listed in Table 2.
4.1 Syntactic Parsers
Enju Enju (Miyao and Tsujii, 2008) is a deep
parser based on the Head-Driven Phrase Struc-
ture Grammar (HPSG) formalism. Enju analyses
its input in terms of phrase structure trees with
predicate-argument structure links, represented in
a specialised XML-format. To make the analyses
of the parser more accessible to participants, we
converted its output into the Penn Treebank (PTB)
format using tools included with the parser. The
use of the PTB format also allow for its output to
be exchanged freely for that of the other two syn-
tactic parsers and facilitates further conversions
into dependency representations.
McCCJ The BLLIP Parser (Charniak and John-
son, 2005), also variously known as the Charniak
parser, the Charniak-Johnson parser, or the Brown
reranking parser, has been applied in numerous
biomedical domain NLP efforts, frequently using
the self-trained biomedical model of McClosky
(2010) (i.e. the McClosky-Charniak-Johnson or
McCCJ parser). The BLLIP Parser is a con-
stituency (phrase structure) parser and the applied
model produces PTB analyses as its native out-
put. These analyses were made available to par-
ticipants without modification.
Stanford The Stanford Parser (Klein and Man-
ning, 2002) is a widely used publicly available
syntactic parser. As for the Enju and BLLIP
parsers, a model trained on a dataset incorporating
biomedical domain annotations is available also
for the Stanford parser. Like the BLLIP parser,
the Stanford parser is constituency-based and pro-
duces PTB analyses, which were provided to task
participants. The Stanford tools additionally in-
corporate methods for automatic conversion from
this format to other representations, discussed fur-
ther below.
4.2 Pre-processing and Conversions
To create the syntactic analyses from the Enju,
BLLIP and Stanford Parser systems, we first ap-
plied a uniform set of pre-processing steps in order
to normalise over differences in e.g. tokenisation
and thus ensure that the task participants can eas-
ily swap the output of one system for another. This
pre-processing was identical to that applied in the
BioNLP 2011 Shared Task, and included sentence
splitting of the annotated texts using the Genia
Sentence Splitter,4 the application of a set of post-
processing heuristics to correct frequently occur-
ring sentence splitting errors, and Genia Treebank-
like tokenisation (Tateisi et al, 2004) using a to-
kenisation script created by the shared task organ-
isers. 5
Since several studies have indicated that repre-
sentations of syntax and aspects of syntactic de-
pendency formalism differ in their applicability to
support information extraction tasks (Buyko and
Hahn, 2010; Miwa et al, 2010; Quirk et al, 2011),
we further converted the output of each of the
parsers from the PTB representation into three
other representations: CoNNL-X, Stanford De-
pendencies and Stanford Collapsed Dependencies.
For the CoNLL-X format we employed the con-
version tool of Johansson and Nugues (2007), and
for the two Stanford Dependency variants we used
the converter provided with the Stanford CoreNLP
tools (de Marneffe et al, 2006). These analyses
were provided to participants in the output for-
mats created by the respective tools, i.e. the TAB-
separated column-oriented format CoNLL and the
custom text-based format of the Stanford Depen-
dencies.
5 Results and Discussion
Just like in previous years the supporting resources
were well-received by the shared task participants
and as many as five participating teams mentioned
utilising the supporting resources in their initial
submissions (at the time of writing, the camera-
ready versions were not yet available). This level
of usage of the supporting resources by the partici-
pants is thus comparable to what was observed for
the 2011 shared task.
Following in the tradition of the 2011 support-
4https://github.com/ninjin/geniass
5https://github.com/ninjin/bionlp_
st_2013_supporting/blob/master/tls/
GTB-tokenize.pl
101
ing resources, to aim for reproducibility, the pro-
cessing pipeline containing pre/post-processing
and conversion scripts for all the syntactic parses
has been made publicly available under an open
licence.6 The repository containing the pipeline
also contains detailed instructions on how to re-
produce the output and how it can potentially be
applied to other texts.
Given the experience of the organisers in
analysing medium-sized corpora with a variety of
syntactic parsers, many applied repeatedly over
several years, we are also happy to report that the
robustness of several publicly available parsers has
recently improved noticeably. Random crashes,
corrupt outputs and similar failures appear to be
transitioning from being expected to rare occur-
rences.
In this paper, we have introduced the supporting
resources provided for the BioNLP 2013 Shared
Task by the task organisers and external groups.
These resources included both syntactic and se-
mantic annotations and were provided to allow the
participants to focus on the various novel chal-
lenges of constructing event extraction systems by
minimizing the need for each group to separately
perform standard processing steps such as syntac-
tic analysis.
Acknowledgements
We would like to give special thanks to Richard
Johansson for providing and allowing us to dis-
tribute an improved and updated version of his for-
mat conversion tool.7 We would also like to ex-
press our appreciation to the broader NLP com-
munity for their continued efforts to improve the
availability of both code and data, thus enabling
other researchers to stand on the shoulders of gi-
ants.
This work was partially supported by the
Quaero programme funded by OSEO (the French
agency for innovation). The research of Donald
C. Comeau, Rezarta Islamaj Dog?an, Haibin Liu
and W. John Wilbur was supported by the Intra-
mural Research Program of the National Institutes
of Health (NIH), National Library of Medicine
(NLM).
6https://github.com/ninjin/bionlp_st_
2013_supporting
7https://github.com/ninjin/
pennconverter
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, pages
380?387. Springer.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the Impact of Alternative Dependency Graph Encod-
ings on Solving Event Extraction Tasks. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 982?992,
Cambridge, MA, October.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Ver-
spoor, Thomas C. Wiegers, Cathy H. Wu, and
W. John Wilbur. 2013. BioC: A minimalist ap-
proach to interoperability for biomedical text pro-
cessing. submitted.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
153?154. Association for Computational Linguis-
tics.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Claire Ne?dellec. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
In Special Issue of the journal Research in Comput-
ing Science, Samos, Greece, March. 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of the 16th Nordic Conference
on Computational Linguistics (NODALIDA), pages
105?112.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
102
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, 15(2003):3?10.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event Extraction as Dependency
Parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
Association for Computational Linguistics.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.
Liam R McGrath, Kelly Domico, Courtney D Cor-
ley, and Bobbie-Jo Webb-Robertson. 2011. Com-
plex biological event extraction from full text us-
ing signatures of linguistic and semantic features.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 130?137. Association for Compu-
tational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating Dependency Rep-
resentations for Event Extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 779?787,
Beijing, China, August.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Nhung TH Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 94?101. Association for Compu-
tational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 155?
163, Portland, Oregon, USA, June.
Larry Smith, Thomas Rindflesch, and W. John Wilbur.
2004. MedPost: a part-of-speech tagger for bio
medical text. Bioinformatics, 20(14):2320?2321.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta,
Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Bridging the gap between scope-based and event-
based negation/speculation annotations: a bridge not
too far. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 47?56. Association for Computa-
tional Linguistics.
Y Tateisi, T Ohta, and J Tsujii. 2004. Annotation of
predicate-argument structure on molecular biology
text. Proceedings of the Workshop on the 1st In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-04).
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Gin-
ter. 2012. Exploring biomolecular literature with
EVEX: connecting genes through events, homology,
and indirect associations. Advances in Bioinformat-
ics, 2012.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, et al 2013. Large-scale event extrac-
tion from literature with multi-level gene normaliza-
tion. PloS one, 8(4):e55814.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction. In
Workshop on Reinforcement Learning, page 143.
103
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 161?169,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP shared Task 2013 ? An Overview of the  Bacteria Biotope Task 
Robert Bossy1, Wiktoria Golik1, Zorana Ratkovic1,2, Philippe Bessi?res1, Claire N?dellec1  1Unit? Math?matique, Informatique et G?nome MIG INRA UR1077 ? F-78352 Jouy-en-Josas ? France 2LaTTiCe UMR 8094 CNRS, 1 rue Maurice Arnoux, F-92120 Montrouge ? France forename.name@jouy.inra.fr  Abstract 
This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2013, which follows BioNLP-ST-11. The Bacteria Biotope task aims to extract the location of bacteria from scientific web pages and to characterize these locations with respect to the OntoBiotope ontology. Bacteria locations are crucial knowledge in biology for phenotype studies. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results.  1 Introduction The Bacteria Biotope (BB) task extends the BioNLP 2013 Shared Task molecular biology scope. It consists of extracting bacteria and their locations from web pages, and categorizing the locations with respect to the OntoBiotope1 ontology of microbe habitats. The locations denote the places where given species live. The bacteria habitat information is critical for the study of the interaction between the species and their environment, and for a better understanding of the underlying biological mechanisms at a molecular level. The information on bacteria biotopes and their properties is very abundant in scientific literature and in genomic databases and BRC (Biology Resource Center) catalogues. However, the information is highly diverse and expressed in natural language (Bossy et al, 2012). The two critical missing steps for population of biology databases and biotope knowledge modeling are (1) the automatic extraction of organism/location pairs and (2) the normalization of the habitat names with respect to biotope ontologies.                                                         1http://bibliome.jouy.inra.fr/MEM-OntoBiotope/OntoBiotope_BioNLP-ST13.obo 
The aim of the previous edition of the BB task (BioNLP-ST?11) was to solve the first information extraction step. The results obtained by the participant systems reached 45 percent F-measure. These results showed both the feasibility of the task, as well as a large room for improvement (Bossy et al, 2012).  The 2013 edition of the BB task maintains the primary objective of event extraction, and introduces the second issue of biotope normalization. It is handled through the categorization of the locations into a large set of types defined in the OntoBiotope ontology. Bacteria locations range from hosts, plant and animals, to natural environments (e.g. water, soil), including industrial environments.  BB?11 set of categories contained 7 types. This year, entity categorization has been enriched to better answer the biological needs, as well as to contribute to the general problem of automatic semantic annotation by ontologies. BB task is divided into three sub-tasks. Entity detection and event extraction are tackled by two distinct sub-tasks, so that the contribution of each method could be assessed. A third sub-task conjugates the two in order to measure the impact of the method interactions. 2 Context Biological motivation. Today, new sequencing methods allow biologists to study complex environments such as microbial ecosystems. Therefore, the sequence annotation process is facing radical changes with respect to the volume of data and the nature of the annotations to be considered. Not only do biochemical functions still need to be assigned to newly identified genes, but biologists have to take into account the conditions and the properties of the ecosystems in which microorganisms are living and are identified, as well as the interactions and relationships developed with their environment and other 
161
living organisms (Korbel et al, 2005). Metagenomic studies of ecosystems yield important information on the phylogenetic composition of the microbiota. The availability of bacteria biotope information represented in a formal language would then pave the way for many new environment-aware bioinformatic services. The development of methods that are able to extract and normalize natural language information at a large scale would allow us to rapidly obtain and summarize information that the bacterial species or genera are associated with in the literature. In turn, this will allow for the formulation of hypotheses regarding properties of the bacteria, the ecosystem, and the links between them.  The pioneering work on EnvDB (Pignatelli et al, 2009) aimed to link GenBank sequences of microbes to biotope mentions in scientific papers. However, EnvDB was affected by the incompleteness of the GenBank isolation source field, the low number of related bibliographic references, the bag-of-words extraction method and the small size of its habitat classification. Habitat categories. The most developed classifications of habitats are EnvO, the Metagenome classification supported by the Genomics Standards Consortium (GSC), and the OntoBiotope ontology developed by our group. EnvO (Environment Ontology project) targets a Minimum Information about a Genome Sequence (MIGS) specification (Field et al, 2008) of mainly Eukaryotes. This ambitious detailed environment ontology aims to support standard manual annotations  of all types of organism environments and biological samples. However, it suffers from some limitations for bacterial biotope descriptions. A large part of EnvO is devoted to environmental biotopes and extreme habitats, whilst it fails to finely account for the main trends in bacteria studies, such as their technological use for food transformation and bioremediation, and their pathogenic or symbiotic properties. Moreover, EnvO terms are often poorly suited for bacteria literature analysis (Ratkovic et al, 2012). The Metagenome Classification  from JGI of DOE (Joint Genome Institute, US Department Of Energy) is intended to classify metagenome projects and samples according to a mixed typology of habitats (e.g. environmental, host) and their physico-chemical properties (e.g. pH, salinity) (Ivanova et al, 2010). It is a valuable 
source of vocabulary for the analysis of bacteria literature, but its structure and scope are strongly biased by the indexing of metagenome projects. The OntoBiotope ontology is appropriate for the categorization of bacteria biotopes in the BB task because its scope and its organization reflect the scientific subject division and the microbial diversity. Its size (1,756 concepts) and its deep hierarchical structure are suitable for a fine-grained normalization of the habitats. Its vocabulary has been selected after a thorough terminological analysis of relevant scientific documents, papers, GOLD (Chen et al, 2010) and GenBank, which was partly automated by term extraction. Related terms are attached to the OntoBiotope concept labels (i.e. 383 synonyms), improving OntoBiotope coverage of natural language documents.  Its structure and a part of its vocabulary have been inspired by EnvO, the Metagenome classification and the small ATCC (American Type Collection Culture) classification for microbial collections (Floyd et al, 2005). Explicit references to 34 EnvO terms are given in the OntoBiotope file. Its main topics are: - ? Artificial ? environments (industrial and domestic), Agricultural habitats, Aquaculture habitats, Processed food; - Medical environments, Living organisms, Parts of living organisms, Bacteria-associated habitats; - ? Natural ? environment habitats, Habitats wrt physico-chemical property (including extreme ones); - Experimental medium (i.e. experimental biotopes designed for studying bacteria). The structure, the comprehensiveness and the detail of the habitat classification are critical factors for research in biology. Biological investigations involving the habitats of bacteria are very diverse and still unanticipated. Thus, shallow and light classifications are insufficient to tackle the full extent of the biological questions. Indexing genomic data with a hierarchical fine-grained ontology such as OntoBiotope allows us to obtain aggregated and adjusted information by selecting the right level or axis of abstraction. Bacteria Biotope Task.  The corpus is the same as BB?11. The documents are scientific web pages intended for a general audience in the form of encyclopedia notices. They focus on a single organism or a family. The habitat mentions are dense and more diverse than 
162
in PubMed abstracts. These features make the task both useful and feasible with a reduced investment in biology. Its linguistic characteristics, high frequency of anaphora, entities denoted by complex nominal expressions raised interesting question for BioNLP that have been treated for a long time in the general and the biomedical domains. 3 Task description The BB Task is split into two secondary goals: 1. The detection of entities and their categorization(s) (Sub-task 1). 2. The extraction of Localization relations given the entities (sub-task 2) Sub-task 1 involves the prediction of habitat entities and their position in the text. The participant also has to assign each entity to one or more concepts of the OntoBiotope ontology: the categorization task. For instance, in the excerpt Isolated from the water of abalone farm, the entity abalone farm should be assigned the OntoBiotope category fish farm. Sub-task 2 is a relation extraction task. The schema of this task contains three types of entities: - The Habitat type is the same as in sub-task 1. - Geographical entities represent location and organization named entities. - Bacteria entities are bacterial taxa. Additionally, there are two types of relations illustrated by Figure 1. - Localization relations link Bacteria to the place where they live (either a Habitat or a Geographical). - PartOf relations relate couples of Habitat entities, a living organism, which is a host (e.g. adult human), and a part of this living organism (e.g. gut).  Bifidobacterium longum. This organism is 
found in adult humans and formula fed infants 
as a normal component of gut flora. 
Figure 1. Example of a localization event in the BB Task. Sub-task 2 participants are provided with document texts and entities, and should predict the relations between the candidate entities. 
Sub-task 3 is the combination of these two sub-tasks. It consists of predicting both the entity positions and the relations between entities. Compared to sub-task 1, the systems have to predict Habitat entities, but also Geographical and Bacteria entities. It is similar to the BB task of BioNLP-ST?11, except that no categorization of the entities is required. 4 Corpus description The BB corpus document sources are web pages from bacteria sequencing projects, (EBI, NCBI, JGI, Genoscope) and encyclopedia pages from MicrobeWiki. The documents are publicly available. Table 1 gives the distribution of the entities and relations in the corpora per sub-task.   Training + Dev Test 1 & 3 Test 2 Document 78 27 26 Word 25,828 7,670 10,353     Bacteria 1,347 332 541 Geographical 168 38 82 Habitat 1,545 507 623 OntoBiotope cat. 1,575 522 NA Total entities 3,060 877 1,246     Localization 1,030 269 538 Part of Host 235 111 129 Total relations 1,265 328 667 Table 1. BB?13 corpus figures. The categorization of entities by a large ontology (sub-task 1) offers a novel task to the BioNLP-ST community; a close examination of the annotated corpus allowed us to anticipate the challenges for participating teams. A total of 2,052 entities have been manually annotated for sub-task 1 (training, development and test sets together). These entities have 1,036 distinct surface forms, which means that an entity surface form is repeated a little less than twice, on average. However, only a quarter of the surface forms are actually repeated; three quarters are unique in the corpus. Moreover, 60% of habitat entities have a surface form that does not match one of the synonyms of their ontology concept. This configuration suggests that methods that simply propagate surface forms and concept attributions from ontology synonyms and from training entities would be inefficient. We have developed a baseline prediction that projects the ontology synonyms and the training corpus 
Localization Localization 
Part of Part of 
163
habitat surface forms onto the test. This prediction scores a high Slot Error Rate of 0.74. We also note there are a few ambiguous forms (i.e. 112 forms) that are synonyms in several different concepts or that do not always denote a habitat, and a few entities are assigned more than one concept (i.e. 42 of them). These are difficult cases that require prediction methods capable of word sense disambiguation. The low number of ambiguous occurrences has a low impact on the participant scores, although their presence may motivate more sophisticated methods. 5 Annotation methodology The methodology of entity position and relations annotation is similar to BB Task?11. It involved seven scientists who participated in a double-blind annotation (each document was annotated twice), followed by a conflict resolution phase. They used the AlvisAE annotation editor (Papazian et al, 2012). The guidelines included some improvements that are detailed below. Boundaries. Habitat entities may be either names or adjective. In the case of adjectives, the head is included in the entity span if it denotes a location (e.g. intestinal sample) and is excluded otherwise (e.g. hospital epidemic). The entity spans may be discontinuous, which is relevant for overlapping entities like ground water and surface water in ground and surface water. The major change is the inclusion of all modifiers that describe the location in the habitat entity span. This makes the entity more informative and the entity boundaries easier to predict, and less subject to debate. For instance, in the example,  isolated from the water of an abalone farm,  the water entity extends from water to farm. Note that in sub-task 1, all entities have to be predicted, even when not involved in a relation. This led to the annotation of embedded entities as potential habitats for bacteria, such as abalone farm and abalone in the above example.  Equivalent sets of entities.  As in BB?11, there are many equivalent mentions of the same bacteria in the documents that play a similar role with respect to the Localization relation. Selecting only one of them as the gold reference would have been arbitrary. When this is the case, the reference annotation includes equivalent sets of entities that convey the same information (e.g. Borrelia garinii vs. B. garinii, but not Borrelia).  
Category assignment. The assignment of categories to habitat entities has been done in two steps: (i) an automatic pre-annotation by the method of Ratkovic et al, (2012) and (ii) a manual double-blind revision followed by a conflict resolution phase. In the manual annotation phase, the most frequent conflicts between annotators were the same as in the previous edition. They involved the assignment of entities to either the living organism category, organic matter or food. An example is the cane entity in cane cuttings. To handle these cases, the guidelines assert that a dead organism cannot be assigned to a living organism category. The high quality of the pre-annotation and its visualization and revision using the AlvisAE annotation editor notably sped-up the annotation process. Table 2 summarizes the figures of the pre-annotation. For sub-task 1, the pre-annotation consisted of assigning OntoBiotope categories to entities for the whole corpus (train+dev+test). The pre-annotation yielded very high results with an F-measure of almost 90%. The pre-annotation was also useful to assess the relevance of the OntoBiotope ontology for the BB task. For sub-task 2, the pre-annotation consisted of the detection of entities in the test set, where no categorization is needed. The second line in Table 2 shows that the recall of entity detection affects the F-score, but that it still made the prediction helpful for the annotators. Further data analysis revealed that the terminology-based approach of the pre-annotation poorly detected the correct boundaries of embedded entities, thereby decreasing the recall of the entity recognition.   Recall Precision F1 Corpus sub-task1 89.7% 90.1% 89.9% Test sub-task 2 47.3% 95.7% 63.3% Table 2. Pre-annotation scores. 6 Evaluation procedure The evaluation procedure was similar to the previous edition in terms of resources, schedule and metrics except that an original relevant metric was developed for the new problem of entity categorization in a hierarchy.  6.1 Campaign organization The training and development corpora with the reference annotations were made available to the participants eleven weeks before the release of 
164
the test sets. Participating teams then had ten days to submit their predictions. As with all BioNLP-ST tasks, each participant submitted a single final prediction for each BB sub-task. The detailed evaluation results were computed, provided to the participants and published on the BioNLP website two days after the submission deadline.  6.2 Evaluation metrics Sub-task 1. In this sub-task participants were given only the document texts. They had to predict habitat entities along with their categorization with the OntoBiotope ontology. The evaluation of sub-task 1 takes into account the accuracy of the boundaries of the predicted entities as well as of the ontology category. Entity pairing. The evaluation algorithm performs an optimal pairwise matching between the habitat entities in the reference and the predicted entities. We defined a similarity between two entities that takes into account the boundaries and the categorization. Each reference entity is paired with the predicted entity for which the similarity is the highest among non-zero similarities.  If the boundaries of a reference entity do not overlap with any predicted entity, then it is a false negative, or a deletion. Conversely, if the boundaries of a predicted entity do not overlap with any reference entity, then it is a false positive, or an insertion. If the similarity between the entities is 1, then it is a perfect match. But if the similarity is lower than 1, then it is a substitution.  Entity similarity. The similarity M between two entities is defined as: M = J . W J measures the accuracy of the boundaries between the reference and the predicted entities. It is defined as a Jaccard Index adapted to segments (Bossy et al, 2012). For a pair of entities with the exact same boundaries, J equals to 1. W measures the accuracy between the ontology concept assignment of the reference entity and the predicted concept assignment of the predicted entity. We used the semantic similarity proposed by Wang, et al (2007). This similarity compares the set of all ancestors of the concept assigned to the reference entity and the set of all ancestors of 
the concept assigned to the predicted entity. The similarity is the Jaccard Index between the two sets of ancestors; however, each ancestor is weighted with a factor equal to: dw where d is the number of steps between the attributed concept and the ancestor. w is a constant greater than zero and lower than or equal to 1. If both the reference and predicted entities are assigned the same concept, then the sets of ancestors are equal and W is equal to 1. If the pair of entities has different concept attributions, W is lower than 1 and depends on the relative depth of the lowest common ancestor. The lower the common ancestor is, the higher the value of W. The exponentiation by the w constant ensures that the weight of the ancestors decreases non-linearly. This similarity thus favors predictions in the vicinity of the reference concept. Note that since the ontology root is the ancestor of all concepts, W is always strictly greater than zero. (Wang et al, 2007) showed experimentally that a value of 0.8 for the w constant is optimal for clustering purposes. However we noticed that w high values tend to favor sibling predictions over ancestor/descendant predictions that are preferable here, whilst low w values do not penalize enough ontology root predictions. We settled w with a value of 0.65, which ensures that ancestor/descendant predictions always have a greater value than sibling predictions, while root predictions never yield a similarity greater than 0.5. As specified above, if the similarity M < 1, then the entity pair is a substitution. We define the importance of the substitution S as: S = 1 - M Prediction score. Most IE tasks measure the quality of a prediction with Precision and Recall, eventually merged into an F1. However the pairing detects false positives and false negatives, but also substitutions. In such cases, the Recall and Precision factor the substitutions twice, and thus underestimate false negatives and false positives. We therefore used the Slot Error Rate (SER) that has been devised to undertake this shortcoming (Makhoul et al, 1999): SER = (S + I + D) / N where: - S represents the number of substitutions. 
165
- I represents the total number of insertions. - D represents the total number of deletions. - N is the number of entities in the reference. The SER is a measure of errors, so the lower it is the better. A SER equal to zero means that the prediction is perfect. The SER is unbound, though a value greater than one means that there are more mistakes in the prediction than entities in the reference. We also computed the Recall, the Precision and F1 measures in order to facilitate the interpretation of results: Recall =M / N Precision = M / P where M is the sum of the similarity M for all pairs in the optimal pairing, N is the number of entities in the reference, and P the number of entities in the prediction. Sub-task 2. In sub-task 2, the participants had to predict relations between candidate arguments, which are Bacteria, Habitat and Geographical entities. This task can be viewed as a categorization task of all pairs of entities. Thus, we evaluate submissions with Recall, Precision and F1. Sub-task 3. Sub-task 3 is similar to sub-task 2, but it includes entity prediction. This is the same setting as the BB task in BioNLP-ST 2011, except for entity categorization. We used the same evaluation metrics based on Recall, Precision and F1 (Bossy et al, 2012). The highlights of this measure are: ? it is based on the pairing between reference and predicted relations that maximizes a similarity; ? the similarity of the boundaries of Habitat and Geographical entities is relaxed and defined as the Jaccard Index (in the same way as in sub-task 1); ? the boundaries of Bacteria is strict: the evaluation rejects all relations where the Bacteria has incorrect boundaries. 7 Results  7.1 Participating systems Five teams submitted ten predictions to the three BB sub-tasks. LIMSI (CNRS, France), see (Grouin, 2013) is the only team that submitted to the three sub-tasks. LIPN (U. Paris-Nord, France), (Bannour et al, 2013) only submitted to 
sub-task 1. TEES (TUCS, Finland), (Bj?rne and Salakoski, 2013) only submitted to sub-task 2. Finally, IRISA (INRIA, France), (Claveau, 2013))) and Boun (U. Bo?azi?i, Turkey), (Karadeniz and ?zg?r), submitted to sub-tasks 1 and 2. The scores of the submissions according to the official metrics are shown in decreasing rank order in Tables 3 to 6.  Participant Rank  SER  F1  IRISA 1  0.46 0.57  Boun 2  0.48 0.59  LIPN 3  0.49 0.61  LIMSI 4  0.66 0.44 Table 3. Scores for Sub-task 1 of the BB Task.   Participant Entity  detection Category  assignment  SER F1 SER  F1  IRISA  0.43 0.60  0.35 0.67  Boun  0.42 0.65  0.36 0.71  LIPN  0.46 0.64  0.38 0.72  LIMSI  0.45 0.71  0.66 0.50 Table 4. Detailed scores for Sub-task 1 of the BB Task. Participant systems to sub-task 1 obtained high scores despite the novelty of the task (0.46 SER for the 1st, IRISA). The results of the first three systems are very close despite the diversity of the methods. The decomposition of the scores of the predictions of entities with correct boundaries and their assignment to the right category are shown in Table 4. They are quite balanced with a slightly better rate for category assignment, with the exception of the LIMSI system, which is notably better in entity detection. This table also shows the dependency of the two entity detection and categorization steps. Errors in the entity boundaries affect the quality of categorization. Table 5 details the scores for sub-task 2. The prediction of location relations remains a difficult problem even with the entities being given. There are two reasons for this. First, there is high diversity of bacteria and locations. The many mentions of different bacteria and locations in the same paragraph make it a challenge to select the right pairing among candidate arguments. This is particularly true for the PartOf relation compared to the Localization relation (columns 5 and 6). All systems obtained 
166
a recall much lower than the precision, which may be interpreted training data overfitting.  Participant Rec. Prec.  F1  F1 PartOf F1 Loc.  TEES 2.1  0.28 0.82  0.42  0.22 0.49  IRISA  0.36 0.46  0.40  0.2 0.45  Boun  0.21 0.38  0.27  0.2 0.29  LIMSI  0.4 0.19  0.6  0.0 0.7 Table 5. Scores of Sub-task 2 for the BB Task. The second challenge is the high frequency of anaphora, especially with a bacteria antecedent. For BioNLP-ST 2011, we already pointed out that coreference resolution is critical in order to capture all relations that are not expressed inside a sentence. Participant Rec.  Prec.  F1   TEES 2.1 0.12 (0.41) 0.18 (0.61) 0.14  (0.49)  LIMSI 0.4 (0.9) 0.12 (0.82)   0.6  (0.15) Table 6. Scores of Sub-task 3 for the BB Task. (the relaxed scores are given in parentheses.) The results of sub-task 3 (Table 6) may appear disappointing compared to the first two sub-tasks and BB?11. Further analysis shows that the system scores were affected by their poor entity boundary detection and the PartOf relation predictions. In order to demonstrate this we computed a relaxed score that differs from the primary score by: - removing PartOf relations from the reference and the prediction; - accepting Localization relations even if the Bacteria entity boundaries  do not match; - removing the penalty for the incorrect boundaries of Habitat entities. This relaxed score is equivalent to ignoring PartOf relations and considering the boundaries of predicted entities as perfect. The result is exhibited in Table 6 between parentheses. The most determinant factor is the relaxation of Bacteria entity boundaries because errors are severely penalized. An error analysis of the submitted predictions revealed that more than half of the rejected Localization predictions had a Bacteria argument with incorrect boundaries.  7.2 Systems description and result analysis The participants deployed various assortments of methods ranging from linguistics and machine learning to hand-coded pattern-matching. Sub-
task 1 was handled in two successive steps, candidate entity detection and category assignment. Entity detection. The approaches combine  (1) the use of lexicons (IRISA and LIMSI), (2) then text analysis by chunking (IRISA), noun phrase analysis (Boun), term analysis by BioYaTeA (LIPN) and Cocoa entity detection (LIMSI),  (3) with additional rules (TextMarker by LIPN) or machine learning (CRF by LIMSI) for the adaptation to the corpus.  The LIMSI system combining Cocoa entity detection (BioNLP supporting resource) with CRF obtained the best result, 11 points over the less linguistics-based approach of IRISA as shown in Table 4.  Assignment of categories to entities. It was mainly realized using hand-coded rules (LIMSI, Boun), machine learning with Whisk (LIPN) or a similarity between ontology labels and the text entities (IRISA). It is interesting to note that although the approaches are very different, the three types of methods obtained close results ranging from 0.35 to 0.38 SER, apart one outlier. Prediction of relations. Sub-task 2 was completed by applying hand-coded rules (LIMSI, Boun), that were much less successful than the two machine-learning-based approaches, i.e. kNN by IRISA and multi-step SVM by TEES-2.1. In the case of TEES-2.1 attributes were generated by McCCJ parses, which may explain its success in the prediction of PartOf relations that is 20 point over the second method that did not use any parsing. Prediction of entities and relations. Sub-task 3 was completed by LIMSI using the successive application of its methods from sub-tasks 1 and 2. TEES-2.1 applied its multi-step SVM classification of sub-task 2 for relation prediction completed by additional SVM steps for candidate entity detection. These experiments allow for the comparison of very different state-of-the-art methods, resources and integration strategies. However the tight gap between the scores of the different systems prevents us from drawing a definitive conclusion. Additional criteria other than scores may also be taken into account: the simplicity of deployment, the ease of adaptation to new 
167
domains, the availability of relevant resources and the potential for improvement. 8 Conclusion After BioNLP-ST?11, the second edition of the Bacteria Biotope Task provides a wealth of new information on the generalization of the entity categorization methods to a large set of categories. The final submissions of the 5 teams show very promising results with a broad variety of methods. The introduction of new metrics appeared appropriate to reveal the quality of the results and to highlight relevant contrasts. The prediction of events still remains challenging in documents where the candidate arguments are very dense, and where most relations involve several sentences. A thorough analysis of the results indicates clear directions for improvement.  Acknowledgments This work has been partially supported by the Quaero program, funded by OSEO, the French state agency for innovation and the INRA OntoBiotope Network. References Sondes Bannour, Laurent Audibert, Henry Soldano. 2013. Ontology-based semantic annotation: an automatic hybrid rule-based method. Present volume. Jari Bj?rne, Tapio Salakoski. 2013. TEES 2.1: Automated Annotation Scheme Learning in the BioNLP 2013 Shared Task. Present volume. Robert Bossy, Julien Jourde, Alain-Pierre Manine A., Philippe Veber, Erick Alphonse, Maarten van de Guchte, Philippe Bessi?res, Claire N?dellec. 2012. BioNLP Shared Task - The Bacteria Track. BMC Bioinformatics 13(Suppl 11):S3, June .  Vincent Claveau. 2013. IRISA participation to BioNLP-ST 2013: lazy-learning and information retrieval for information extraction tasks. Present volume. Liolios K., Chen I.M., Mavromatis K., Tavernarakis N., Hugenholtz P., Markowitz V.M., Kyrpides N.C. (2010). The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. Nucleic Acids Res., 38(Database issue):D346-54. EnvDB database. http://metagenomics.uv.es/envDB/ EnvO Project. http://environmentontology.org 
Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metagenomes: the ?Minimum Information about a Genome Sequence? (MIGS) specification. Nature Biotechnology. 26: 541-547. Cyril Grouin. 2013. Building A Contrasting Taxa Extractor for Relation Identification from Assertions: BIOlogical Taxonomy & Ontology Phrase Extraction System. Present volume. ?lknur Karadeniz, Arzucan ?zg?r. 2013. Bacteria Biotope Detection, Ontology-based Normalization, and Relation Extraction using Syntactic Rules. Present volume. Korbel J.O., Doerks T., Jensen L.J., Perez-Iratxeta C., Kaczanowski S., Hooper S.D., Andrade M.A., Bork P. (2005). Systematic association of genes to phenotypes by genome and literature mining. PLoS Biol., 3(5):e134. Melissa M. Floyd, Jane Tang, Matthew Kane and David Emerson. 2005. Captured Diversity in a Culture Collection: Case Study of the Geographic and Habitat Distributions of Environmental Isolates Held at the American Type Culture Collection. Applied and Environmental Microbiology. 71(6):2813-23. GenBank. http://www.ncbi.nlm.nih.gov/  GOLD. http://www.genomesonline.org/cgi-bin/GOLD/bin/gold.cgi Ivanova N., Tringe S.G., Liolios K., Liu W.T., Morrison N., Hugenholtz P., Kyrpides N.C. (2010). A call for standardized classification of metagenome projects. Environ. Microbiol., 12(7):1803-5. John Makhoul, Francis Kubala, Richard Schwartz, and Ralph Weischedel. 1999. Performance measures for information extraction, in Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February.  von Mering C., Hugenholtz P., Raes J., Tringe S.G., Doerks T., Jensen L.J., Ward N., Bork P. (2007). Quantitative phylogenetic assessment of microbial communities in diverse environments. Science, 315(5815):1126-30. Metagenome Classification. /metagenomic_classification_tree.cgi MicrobeWiki. http://microbewiki.kenyon.edu/index.php/MicrobeWiki  Microbial Genomics Program at JGI. http://genome.jgi-psf.org/programs/bacteria-archaea/index.jsf Microorganisms sequenced at Genoscope. http://www.genoscope.cns.fr/spip/Microorganisms-sequenced-at.html 
168
Miguel Pignatelli, Andr?s Moya, Javier Tamames.  (2009). EnvDB, a database for describing the environmental distribution of prokaryotic taxa. Environmental Microbiology Reports. 1:198-207. Fr?d?ric Papazian, Robert Bossy and Claire N?dellec. 2012. AlvisAE: a collaborative Web text annotation editor for knowledge acquisition. The 6th Linguistic Annotation Workshop (The LAW VI), Jeju, Korea. Prokaryote Genome Projects at NCBI. http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi  Zorana Ratkovic, Wiktoria Golik, Pierre Warnier. 2012. Event extraction of bacteria biotopes: a knowledge-intensive NLP-based approach. BMC Bioinformatics 2012, 13(Suppl 11):S8, 26June. .  Javier Tamames and Victor de Lorenzo. 2010. EnvMine: A text-mining system for the automatic extraction of contextual information. BMC Bioinformatics. 11:294. James Z. Wang, Zhidian Du, Rapeeporn Payattakool, Philip S. Yu, and Chin-Fu Chen. 2007. A New Method to Measure the Semantic Similarity of GO Terms. Bioinformatics. 23: 1274-1281. 
169
