Dialogue Act Recognition with Bayesian Networks for Dutch Dialogues
Simon Keizer, Rieks op den Akker and Anton Nijholt
Department of Computer Science
University of Twente
P.O. Box 217, 7500 AE Enschede, The Netherlands
{skeizer,infrieks,anijholt}@cs.utwente.nl
Abstract
This paper presents work on using
Bayesian networks for the dialogue act
recognition module of a dialogue sys-
tem for Dutch dialogues. The Bayesian
networks can be constructed from the
data in an annotated dialogue corpus.
For two series of experiments - using
different corpora but the same anno-
tation scheme - recognition results are
presented and evaluated.
1 Introduction
In several papers (Nijholt, 2000; Luin et al, 2001;
Nijholt et al, 2001) we reported on our virtual
music centre - the VMC - a virtual environment
inhabited by (embodied) agents and on multi-
modal interaction between human users and these
agents. Of these agents Karin is an embodied
agent users can ask for information about theatre
performances (see Figure 1).
A second agent is the navigation agent. Nav-
igation is a) way finding - the user knows where
he wants to go but doesn?t know how to go there;
or b) exploring the environment - the user walks
through the environment to obtain an overview of
the building and the objects, locations, rooms that
are in it. Related to these navigation tasks the nav-
igation assistant has the task to assist the visitor
in a) explaining how to go from his current loca-
tion to a location he is looking for and b) to give
the agent information about objects, and locations
in the environment. The navigation agent is not
present as an avatar in the environment. The user
sees the environment from a first person perspec-
tive and interacts with the agents by means of a
Dutch dialogue. The user has two views of the
environment: a) a first person view of the visible
part of the 3D virtual theatre and b) an abstract
2D map of the floor of the building the user is
visiting. This map is shown in a separate win-
dow. In a multi-modal interaction the user can
point at locations or objects on the 2D map and
either ask information about that object or loca-
tion or he can ask the assistant to bring him to the
location pointed at.
Figure 1: Karin in the VMC.
An important part of our dialogue systems for
natural language interaction with agents is the
module for recognition of the dialogue acts per-
formed by the human user (visitor). This pa-
per discusses the construction of and experiments
with Bayesian networks as implementation of this
module.
Various other work has been presented on us-
ing statistical techniques for dialogue act classi-
fication (Andernach, 1996; Stolcke et al, 2000),
and even some first efforts on using Bayesian net-
works for this task (Pulman, 1996; Keizer, 2001).
Other work on using Bayesian networks in dia-
logue systems aims more at interaction and user
modelling (Paek and Horvitz, 2000) and does not
specifically involve linguistic aspects.
The paper is organised as follows. Section 2
       Philadelphia, July 2002, pp. 88-94.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
provides some necessary and general background
about the use of Bayesian networks for speech act
recognition. In Section 3 we discuss experiments
with a Bayesian network for dialogue act classi-
fication based on a dialogue corpus for the Karin
agent. In Section 4 we discuss our current ex-
periments with a network for the navigation dia-
logue system that was automatically created from
a small corpus. Section 5 reflects on our findings
and presents plans for the near future.
2 Bayesian Networks and Speech Act
Recognition
Since Austin and Searle deliberately producing a
linguistic utterance (?locutionary act?) is perform-
ing a speech act (?illocutionary act?). Many re-
searchers have contributed in distinguishing and
categorising types of speech acts we can perform.
See (Traum, 2000) for a valuable discussion on
dialogue act taxonomies and an extensive bibli-
ography.
A dialogue system needs a user model. The
better the user model the better the system is able
to understand the user?s intentions from the locu-
tionary act. We consider the human participant
in a dialogue as a source of communicative ac-
tions. Actions can be some verbal dialogue act
or some non-verbal pointing act (the act of point-
ing at some object). We assume the user is ra-
tional: there is a dependency between the action
performed and the intentional state of the user. If
we restrict to communicative acts that are realized
by uttering (speaking or typing) a sentence we
can model the user by a probability distribution
P (U = u|DA = da): the probability that the user
produces an utterance u (the stochastic variable U
has value u) given that he performs a dialogue act
da ( DA has the value da). Or - maybe better: the
confidence we can have in believing that the user
uses utterance u if we know that the dialogue act
he performs is da. Since there are many distinct
wordings u for performing a given dialogue act
da and on the other hand there are distinct dia-
logue acts that can be performed by the same ut-
terance, we need more than superficial linguistic
information to decide upon the intended dialogue
act given an utterance. The task of the dialogue
act recognition (DAR) module of a dialogue sys-
tem is to answer the question: what is the most
likely dialogue act da intended by the user given
the system has observed the utterance u in a dia-
logue context c. (Notice that we have equated the
utterance produced by the user with the utterance
recognised by the system: there is no information
loss between the module that records the utter-
ance and the input of the dialogue act recognition
module.)
To make this problem tractable we further re-
strict the model by assuming that a) the user en-
gaged in a dialogue can only have the intention
to perform one of a finite number of possible di-
alogue acts; b) each of the possible natural lan-
guage utterances u produced by the user and ob-
served by the system can be represented by a fi-
nite number of feature value pairs (fi = vi); and
c) the dialogue context can be represented by a
finite number of feature value pairs (gi = ci).
Given these restrictions the DAR problem be-
comes to find that value da of DA that maximises
P ( DA = da | f1 = v1, . . . , fn = vn,
g1 = c1, . . . , gm = cm ).
For the probabilistic model from which this can
be computed we use a Bayesian network (Pearl,
1988). A Bayesian network is a directed acyclic
graph in which the nodes represent the stochastic
variables considered, while the structure (given
by the arcs between the nodes) constitutes a set
of conditional independencies among these vari-
ables: a variable is conditionally independent of
its non-descendants in the network, given its par-
ents in the network. Consider the network in Fig-
ure 2: it contains one node representing the di-
alogue act (DA), 3 nodes representing utterance
features (NumWrds, CanYou and IWant) and
a node representing a context feature (PrevDA).
From the network structure follows that for ex-
ample variable DA is conditionally independent
of variable NumWrds, given variable CanYou.
The conditional independencies make the
model computationally more feasible: finding a
specification of the joint probability distribution
(jpd) for the model reduces to finding the condi-
tional probability distributions of each of the vari-
ables given their network parents. In our example
network, the following jpd specification holds:
P (DA, NumWrds,CanYou, IWant,PrevDA) =
P (IWant) ? P (PrevDA|DA) ? P (CanYou) ?
DA
PrevDA
IWant
CanYou
NumWrds
Figure 2: A Bayesian Network for Dialogue Act
Recognition.
? P (DA|CanYou, IWant) ?
? P (NumWrds|CanYou,PrevDA)
The construction of a Bayesian network hence
amounts to choosing a network structure (the con-
ditional independencies) and choosing the condi-
tional probability distributions. In practice, the
probabilities will have to be assessed from em-
pirical data by using statistical techniques. The
structure can generated from data too, but another
option is to choose it manually: the arcs in the
network can be chosen, based on the intuition that
they represent a causal or temporal relationship
between two variables. Strictly spoken however,
a Bayesian network only represents informational
relationships between variables.
Notice that the machine learning technique
known as Naive Bayes Classifier (see for instance
(Mitchell, 1997)) assumes that all variables are
conditionally independent of each other given
the variable that has to be classified. A Naive
Bayes classifier can be seen as a special case of
a Bayesian network classifier, where the network
structure consists of arcs from the class variable
to all variables representing the features: see Fig-
ure 3.
DA
IWant CanYou NumWrdsPrevDA
Figure 3: Naive Bayes classifier as Bayesian Net-
work.
Naive Bayes classifiers will perform as good
as the Bayesian network technique only if indeed
all feature variables are conditionally indepen-
dent, given the class variable. The problem is
of course how do we know that they are condi-
tionally independent? If we don?t have complete
analytical knowledge about the (in)dependencies,
only analysing the data can give an answer to this
question. The advantage of using Bayesian net-
works is that methods exist to construct the net-
work structure as well as the conditional proba-
bilities. Moreover Bayesian networks are more
flexible in their use: unlike Bayesian classifiers
we can retrieve the posterior probabilities of all
the network variables without re-computation of
the model. The same advantage do Bayesian net-
works have over Decision Tree learning methods
like C4.5 that output a decision tree for classi-
fying instances with respect to a given selected
class variable. Experiments have shown that
Naive Bayesian classifiers give results that are as
good as or even better than those obtained by
decision tree classification techniques. Hence,
there are theoretical as well as practical reasons
to use Bayesian networks. However, since there
is hardly any experience in using Bayesian net-
works for dialogue act classification we have to
do experiments to see whether this technique also
performs better than the alternatives mentioned
above for this particular application.
The next two sections describe experiments
with 1) the SCHISMA corpus - elaborating on pre-
vious work described in (Keizer, 2001) - and 2) a
preliminary small corpus of navigation dialogues.
We motivate our choice of dialogue acts and fea-
tures and present some first results in training a
Bayesian network and testing its performance.
3 Experiments with the Schisma corpus
3.1 Dialogue acts and features
The current dialogue system for interacting with
Karin is based on analyses of the SCHISMA cor-
pus. This is a corpus of 64 dialogues, obtained
through Wizard of Oz experiments. The interac-
tion between the wizard - a human simulating the
system to be developed - and the human user was
established through keyboard-entered utterances,
so the dialogues are textual. The task at hand
is information exchange and transaction: users
are enabled to make inquiries about theatre per-
formances scheduled and if desired, make ticket
reservations.
We have manually annotated 20 dialogues
from the SCHISMA corpus, using two layers
of the DAMSL multi-layer annotation scheme
(Allen and Core, 1997), a standard for annotat-
ing task-oriented dialogues in general. The layer
of Forward-looking Functions contains acts that
characterise the effect an utterance has on the
subsequent dialogue, while acts on the layer of
Backward-looking Functions indicate how an ut-
terance relates to the previous dialogue. Because
DAMSL does not provide a refined set of dia-
logue acts concerning information-exchange, we
have added some new dialogue acts. For example,
ref-question, if-question and alts-
question were added as acts that further spec-
ify the existing info-request.
For the experiments, we selected a subset of
forward- and backward-looking functions from
the hierarchy that we judged as the most impor-
tant ones to recognise: those are listed in Table
1. In Figure 4, a fragment of an example dia-
logue between S (the server) and C (the client) is
given, in which we have indicated what forward-
and backward-looking functions were performed
in each utterance.
Forward-looking
Functions
assert
open-option
request
ref-question
if-question
alts-question
action-directive
offer
commit
conventional
expressive
otherff
Backward-looking
Functions
accept
approve
reject
disapprove
hold
acknowledge
not-understood
positive answer
negative answer
feedback
otherbf
Table 1: Dialogue Acts for SCHISMA.
The user utterances have also been tagged man-
ually with linguistic features. We have distin-
guished the features in Table 2, assuming they can
be provided for by a linguistic parser.
The dialogue context features selected include
the backward-looking function of the last system
utterance and the forward-looking function of the
previous user utterance. In the experiment with
S: Hello, how can I help you?
conventional
C: When can I see Herman Finkers?
ref-question; otherbf
S: On Saturday the 12th at 20h.
assert; pos-answer
C: I would like 2 tickets please.
action-directive; otherbf
S: Do you have a discount card?
if-question; hold
. . .
Figure 4: Dialogue fragment with forward- and
backward-looking functions.
Sentence Type
declarative
yn-question
wh-question
imperative
noun phrase
adverbial
adjective
number
interjective
continuation
Subject Type
first person
second person
third person
Punctuation
period
question mark
exclam. mark
comma
none
Table 2: Utterance features for SCHISMA.
the SCHISMA dialogues we have constructed a
network structure (see Figure 5) by hand and then
used the data of the annotated dialogues to train
the required conditional probabilities.
PBFS
FFC
SeTp
SuTp
Punct
PFFC
Figure 5: Bayesian network for DAR to be trained
with the SCHISMA dialogues.
The choice of structure is based on the in-
tuition that the model reflects how a client de-
cides which communicative action to take; al-
though the arcs themselves have no explicit
meaning - they only contribute to the set of
conditional independencies - they can be seen
here as a kind of temporal or causal relation-
ships between the variables (as mentioned ear-
lier in Section 2): given the dialogue context -
defined by the previous forward-looking function
of the client (PFFC) and the previous backward-
looking function of the server (PBFS), the client
decides which forward-looking function to per-
form (FFC); from this decision he/she formulates
a natural language utterance with certain features
including the sentence type (SeTp) the subject
type (SuTp) and punctuation (Punct).
Recalling the notion of conditional indepen-
dence in Bayesian networks described in Section
2, it follows that by choosing the network struc-
ture of Figure 5, we have made the (admittedly,
disputable) assumption that, given the forward-
looking function of the client, the three utterance
features are conditionally independent of each
other.
3.2 Results and evaluation
For assessing the conditional probability distribu-
tions, we have used the Maximum A Posteriori
(MAP) learning technique - see e.g. (Heckerman,
1999). For training we have used 330 data sam-
ples which is 75% of the available data; the re-
maining samples have been used for testing. We
have measured the performance of the network
in terms of the accuracy of estimating the cor-
rect forward-looking function for different cases
of available evidence, varying from having no ev-
idence at all to having evidence on all features.
This resulted in an average accuracy of 43.5%.
Adding complete evidence to the network for ev-
ery test sample resulted in 38.7% accuracy.
As the amount of data from the SCHISMA cor-
pus currently available is rather small, the results
cannot expected to be very good and more data
have to be collected for further experiments. Still,
the testing results show that the accuracy is signif-
icantly better than an expected accuracy of 8.3%
in the case of guessing the dialogue act randomly.
A tighter baseline commonly used is the relative
frequency of the most frequent dialogue act. For
the data used here, this gives a baseline of 32.5%,
which is still less than our network?s accuracy.
4 Experiments with the navigation
corpus
4.1 Dialogue acts and features
A small corpus of dialogues was derived from the
first implementation of a dialogue system for in-
teraction with the navigation agent. For the ex-
periments with the navigation corpus we also use
the DAMSL layers of Forward- and Backward-
looking functions. On each of these two lay-
ers we only distinguish dialogue acts on the first
level of the hierarchies (see Table 3 for the di-
alogue acts used); a more refined subcategorisa-
tion should be performed by a second step in the
DAR module. The dialogue acts in Table 1 can
be found at the deeper levels of the DAMSL hi-
erarchy, e.g. a request is a special case of a
infl addr fut act and an acknowledge
is a special case of an understanding. The
dialogue act recogniser may also use more ap-
plication specific knowledge in further identifi-
cation of the user intention. Information that
may be used is dialogue information concerning
topic/focus.
Forward-looking
Functions
statement
infl addr fut act
info request
comm sp fut act
conventional
expl performative
exclamation
Backward-looking
Functions
agreement
understanding
answer
Table 3: Dialogue Acts for Navigation.
For the navigation dialogues, we have chosen
a set of surface features of what will eventually
be spoken utterances, in contrast to the typed di-
alogues in the SCHISMA corpus. Therefore, we
don?t use a textual feature like punctuation. For
each utterance, the feature values are found au-
tomatically using a tagger (the features in the
SCHISMA dialogues were tagged manually). In
Table 4 we have listed the features with their pos-
sible values we initially consider relevant.
The dialogue context features include the
backward- and forward-looking function of the
previous dialogue act. This is always a dialogue
act performed by the system. The possible dia-
logue acts performed by the system are the same
as those performed by the user.
The network is generated from data that were
obtained by manually annotating the user ut-
terances in the navigation corpus following the
DAMSL instructions as close as possible. As
with every categorisation there are problematic
Features Values
lenq one, few, many
iswh true, false
not in prev true, false
startsWithCanYou true, false
startsWithCanI true, false
startsWithIWant true, false
containsPositive true, false
containsNegative true, false
containsOkay true, false
containsLocativePrep true, false
containsLocativeAdverb true, false
containsTell true, false
containsDo true, false
Table 4: Surface features of user utterances and
their possible values.
border cases, e.g. when to annotate with in-
direct speech acts. We used the criterion that
such an act should be recognised without task-
specific considerations. Therefore the utterance
?I want to make a phone-call? is annotated as a
statement although eventually it should be in-
terpreted as an info request (?where can I
find a phone??) in the context of a navigation di-
alogue.
After the dialogue act has been recognised the
navigation agent will make a plan for further ac-
tions and perform the planned actions. We will
not discuss that here.
4.2 Results and evaluation
In this experiment the data are used for learning
both structure and conditional probabilities of a
Bayesian network. We have used an implementa-
tion of the K2 algorithm (Cooper and Herskovits,
1992) to generate the network structure and then
- like in the SCHISMA experiment - used MAP to
assess the conditional probability distributions.
Starting from the small corpus of navigation
dialogues, a procedure has been planned to iter-
atively enlarge the corpus: given the annotated
corpus, derive a network, use the network in a
dialogue system, test the network and add these
dialogues - with the corrected backward- and
forward-looking functions - to the corpus. This
results in a more extended set of annotated dia-
logues. And we start again. After each of the
cycles we compare the results (in terms of accu-
racies) with the results of the previous cycle. This
should give more insight in the usefulness of the
features and values chosen for the Bayesian net-
work. After deciding to adapt the set of features
we automatically annotate the corpus; we derive
a new network and we test again.
The current corpus is too small to expect good
results from a generated network, especially if the
data are used for learning both the structure and
the probability distributions. From the initial cor-
pus of 81 utterances 75% was used for generating
a Bayesian network. Testing on the remaining
25% resulted in accuracy of 57.1% for classify-
ing the forward-looking function and 81.0% for
classifying the backward-looking function. Af-
ter this first cycle, new data have been generated
interactively, following the procedure described
above. The Bayesian network trained from this
new data set resulted in the improved accuracies
of 76.5% and 88.2% for classifying the forward-
and backward-looking function respectively. Fol-
lowing this training and testing procedure, we
hope to develop Bayesian networks with increas-
ing performance.
5 Discussion and conclusions
In this paper we have discussed the use of
Bayesian networks for dialogue act recognition in
a natural language dialogue system. We have de-
scribed the construction of such networks from
data in two cases: 1) using annotated dialogues
from the SCHISMA corpus - information ex-
change and transaction - and 2) using a small cor-
pus of annotated navigation dialogues.
As the amount of data currently available is
rather small (especially the navigation corpus),
the network performances measured are not too
impressive. In order to get more data, we have de-
veloped a testing environment which at the same
time enables us to enlarge the corpus. With the
increasing amount of data we hope to construct
Bayesian networks with increasing performance.
As for the SCHISMA corpus, there are 44 dia-
logues that remain to be annotated, also resulting
in more data.
One of the first and most important questions
to be answered concerns the selection of a set of
features (and their values) that set up the model.
We started with a set of features selected on in-
tuition. Then the dialogue corpus was annotated.
As a result of experiments we may conclude that
some of the features have no selective value, so
we can leave them out of the model.
In the future we would like to compare the
approach of using Bayesian networks with other
classifiers that can also be constructed from data,
e.g. decision trees or Bayesian classifiers. Figure
5 shows the accuracies of three different classi-
fiers that were generated from the current set of
navigation data.
Class Bayesian Decision Naive
variable network tree Bayes
forw funct 76.5% 50.0% 55.9%
backw funct 88.2% 64.7% 61.8%
Table 5: Accuracies of three different classi-
fiers for classifying the forward-looking func-
tion (forw funct) and backward-looking function
(backw funct), where all classifiers have been
built from the same set of navigation data.
In our future experiments we will take into ac-
count more refined performance measures like
precision and recall and confusion matrices in
which classification results for individual dia-
logue act types are shown. Such results can help
us make decisions w.r.t. the selected dialogue act
types and features.
Furthermore, non-verbal communicative ac-
tions like pointing at objects in the virtual envi-
ronment could be relevant in recognising dialog
acts and should therefore be made available as
possible features in our Bayesian network clas-
sifiers.
Acknowledgement
We would like to thank the referees for their com-
ments on our paper; these have been very useful
to us in preparing this final version.
References
J. Allen and M. Core. 1997. Draft of
DAMSL: Dialog Act Markup in Several Lay-
ers. URL: http://www.cs.rochester.
edu/research/trains/annotation.
T. Andernach. 1996. A machine learning approach to
the classification and prediction of dialogue utter-
ances. In Proceedings of the Second International
Conference on New Methods in Language Process-
ing (NeMLaP-2), pages 98?109, Ankara, Turkey.
G. F. Cooper and E. Herskovits. 1992. A Bayesian
method for the induction of probabilistic networks
from data. Machine Learning, 9:309?347.
D. Heckerman. 1999. A tutorial on learning with
Bayesian networks. In M. Jordan, editor, Learning
in Graphical Models. MIT Press, Cambridge MA.
S. Keizer. 2001. A Bayesian approach to dialogue
act classification. In P. Ku?hnlein, H. Rieser, and
H. Zeevat, editors, BI-DIALOG 2001: Proc. of the
5th Workshop on Formal Semantics and Pragmatics
of Dialogue, pages 210?218.
J. van Luin, R. op den Akker, and A. Nijholt. 2001.
A dialogue agent for navigation support in virtual
reality. In J. Jacko and A. Sears, editors, ACM
SIGCHI Conf. CHI 2001: Anyone. Anywhere, pages
117?118, Seattle. Association for Computing Ma-
chinery.
T.M. Mitchell. 1997. Machine Learning. Computer
Science Series. McGraw-Hill.
A. Nijholt, J. Zwiers, and B. van Dijk. 2001. Maps,
agents and dialogue for exploring a virtual world.
In N. Callaos, S. Long, and M. Loutfi, editors, 5th
World Multiconference on Systemics, Cybernetics
and Informatics (SCI 2001), volume VII of Human
Information and Education Systems, pages 94?99,
Orlando, July.
A. Nijholt. 2000. Multimodal interactions with
agents in virtual worlds. In N. Kasabov, editor,
Future Directions for Intelligent Systems and Infor-
mation Science, Physica-Verlag: Studies in Fuzzi-
ness and Soft Computing, chapter 8, pages 148?
173. Springer.
T. Paek and E. Horvitz. 2000. Conversation as action
under uncertainty. In 16th Conference on Uncer-
tainty in Artificial Intelligence (UAI-2000), pages
455?464, San Francisco, CA, June. Morgan Kauf-
mann Publishers.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
S.G. Pulman. 1996. Conversational games, belief re-
vision and Bayesian networks. In J. Landsbergen,
J. Odijk, K. van Deemter, and G. Veldhuijzen van
Zanten, editors, Computational Linguistics in the
Netherlands.
A. Stolcke et al 2000. Dialogue act modelling for au-
tomatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?
374.
D.R. Traum. 2000. 20 questions on dialogue act tax-
onomies. Journal of Semantics, 17(1):7?30.
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 37?45,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multidimensional Dialogue Management
Simon Keizer and Harry Bunt
Department of Language and Information Science
Faculty of Arts, Tilburg University
P.O. Box 90153, 5000 LE Tilburg, The Netherlands
{s.keizer,harry.bunt}@uvt.nl
Abstract
In this paper we present an approach to
dialogue management that supports the
generation of multifunctional utterances.
It is based on the multidimensional dia-
logue act taxonomy and associated con-
text model as developed in Dynamic Inter-
pretation Theory (DIT). The multidimen-
sional organisation of the taxonomy re-
flects that there are various aspects that di-
alogue participants have to deal with si-
multaneously during a dialogue. Besides
performing some underlying task, a par-
ticipant also has to pay attention to vari-
ous aspects of the communication process
itself, including social conventions.
Therefore, a multi-agent approach is pro-
posed, in which for each of the dimensions
in the taxonomy a specialised dialogue act
agent is designed, dedicated to the gener-
ation of dialogue acts from that particular
dimension. These dialogue act agents op-
erate in parallel on the information state of
the system. For a simplified version of the
taxonomy, a dialogue manager has been
implemented and integrated into an inter-
active QA system.
1 Introduction
During (task-oriented) dialogues, the participants
have to deal with many different aspects of com-
munication simultaneously. Besides some under-
lying task that may be performed through the dia-
logue, there are also various aspects of managing
the communicative process itself, including deal-
ing with social obligations. Therefore, speakers
often use utterances that are multifunctional.
We will present an approach to dialogue man-
agement that accounts for the generation of multi-
functional utterances. The approach is based on a
dialogue theory involving a multidimensional dia-
logue act taxonomy and associated context model.
In this theory, called Dynamic Interpretation The-
ory (DIT) (Bunt, 1996; Bunt, 2000a), a dialogue is
modelled as a sequence of (sets of) dialogue acts
operating on the Information State of each of the
participants. The dialogue acts are organised in a
taxonomy that is multidimensional, i.e., each ut-
terance may involve dialogue acts of at most one
type from each dimension. The taxonomy has di-
mensions for aspects like feedback, interaction-
management, social obligations management and
managing the underlying task.
In a dialogue system developed according to
the principles of DIT, the information state is rep-
resented through a context model, containing all
information considered relevant for interpreting
user utterances an generating system utterances in
terms of dialogue acts. Hence, given the multidi-
mensionality of the taxonomy, the input interpre-
tation components of the system result in several
dialogue acts for each utterance, at most one from
each of the dimensions. Using these recognised
user dialogue acts, the context model is updated.
On the other hand, the ultimate task for a di-
alogue manager component of a dialogue system
is deciding which dialogue acts to generate. So,
again with the multidimensional organisation of
the taxonomy in mind, we argue for a multi-agent
approach, in which the dialogue act generation
task is divided over several agents that operate in
parallel on the context model, each agent being
dedicated to the generation of dialogue acts from
one particular dimension in the taxonomy. This
leads to the design of a number of so-called Di-
37
alogue Act Agents, including e.g. a task-oriented
agent, two feedback agents and an agent dealing
with social obligations management.
The multi-agent approach to dialogue manage-
ment itself is not new: JASPIS (Turunen and
Hakulinen, 2000; Salonen et al, 2004) is a multi-
agent framework for dialogue systems which al-
lows for implementations of several agents for the
same tasks, varying from input interpretation and
output presentation to dialogue management. De-
pending on the situation, the agent that is most
appropriate for a given task is selected in a pro-
cess involving several so-called ?evaluators?. In
JASPIS the multi-agent approach is aimed at flex-
ibility and adaptiveness, while our approach fo-
cuses more on supporting multidimensionality in
communication.
In a very general sense, our dialogue manage-
ment approach follows an information state update
approach similar to the dialogue managers that are
developed within the TRINDI framework (Lars-
son and Traum, 2000). For example, Matheson
et al (2000) describe the implementation of a di-
alogue management system focusing in the con-
cepts of grounding and discourse obligations.
An approach to dialogue management which
identifies several simultaneous processes in the
generation of system utterances, is described in
(Stent, 2002). In this approach, which is imple-
mented in the TRIPS dialogue system, dialogue
contributions are generated through three core
components operating independently and concur-
rently, using a system of conversation acts or-
ganised in several levels (Traum and Hinkelman,
1992).
Although there are apparent similarities be-
tween our approach and that of the TRINDI based
dialogue managers and the TRIPS system, there
are clear differences as well, which for an impor-
tant part stem from the system of dialogue acts
used and the way the information state is organ-
ised. More particularly, the way in which mech-
anisms for generating dialogue acts along multi-
ple dimensions are modelled and implemented by
means of multiple agents, differs from existing ap-
proaches.
This paper is organised as follows. First we ex-
plain the closely connected DIT notions of dia-
logue act and information state, and the multi-
dimensional dialogue act taxonomy and context
model (Sections 2 and 3). We then introduce
the multi-agent approach to dialogue management
(Section 4) and illustrate it by a description of
the current implementation (Section 4.1). This
implementation is carried out in the PARADIME
project (PARallel Agent-based DIalogue Manage-
ment Engine), which is part of the multiproject
IMIX (Interactive Multimodal Information Ex-
traction). The PARADIME dialogue manager is
integrated into an interactive question-answering
system that is developed in a collaboration be-
tween several projects participating in IMIX. The
paper ends with conclusions and directions for fu-
ture research (Section 5).
2 The DIT dialogue act taxonomy
Based on studies of a variety of dialogues from
several dialogue corpora, a dialogue act taxonomy
was developed consisting of a number of dimen-
sions, reflecting the idea that during a dialogue,
several aspects of the communication need to be
attended to by the dialogue participants (Bunt,
2006). Even within single utterances, several as-
pects are dealt with at the same time, i.e., in gen-
eral, utterances are multifunctional. The multidi-
mensional organisation of the taxonomy supports
this multifunctionality in that it allows several di-
alogue acts to be performed in each utterance, at
most one from each dimension. The 11 dimen-
sions of the taxonomy are listed below, with brief
descriptions and/or specific dialogue act types in
that dimension. For convenience, the dimensions
are further grouped into so-called layers. At the
top level are two layers: one for dialogue con-
trol acts and one coinciding with the task-domain
dimension. Dialogue control is further divided
into 3 layers: Feedback (2 dimensions), Interac-
tion Management (7 dimensions), and a layer co-
inciding with the Social Obligations Management
dimension.
? Dialogue Control
? Feedback
1. Auto-Feedback: acts dealing with the
speaker?s processing of the addressee?s
utterances; contains positive and nega-
tive feedback acts on the levels of per-
ception, interpretation, evaluation, and
execution;
2. Allo-Feedback: acts dealing with the
addressee?s processing of the speaker?s
previous utterances (as viewed by the
38
speaker); contains positive and negative
feedback-giving acts and feedback elic-
itation acts, both on the levels of per-
ception, interpretation, evaluation, and
execution;
? Interaction management
3. Turn Management: turn accepting,
giving, grabbing, keeping;
4. Time Management: stalling, pausing;
5. Dialogue Structuring: opening,
preclosing, closing, dialogue act an-
nouncement;
6. Partner Processing Management:
completion, correct-misspeaking;
7. Own Processing Management: error
signalling, retraction, self-correction;
8. Contact Management: contact check,
contact indication;
9. Topic Management: topic introduction,
closing, shift, shift announcement;
10. Social Obligations Management: saluta-
tion, self-introduction, gratitude, apology,
valediction;
11. Task/domain: acts that concern the specific
underlying task and/or domain.
Formally, a dialogue act in DIT consists of a
Semantic Content and a Communicative Function,
the latter specifying how the information state
of the addressee is to be updated with the for-
mer. A dialogue act in a particular dimension
may have either a dimension-specific communica-
tive function, or a General-Purpose communica-
tive function with a content type (type of semantic
content) in that dimension. The general-purpose
communicative functions are hierarchically or-
ganised into the branches of Information Trans-
fer and Action Discussion functions, Information
Transfer consisting of information-seeking (e.g.,
WH-QUESTION, YN-QUESTION, CHECK) and
information-providing functions (e.g., INFORM,
WH-ANSWER, YN-ANSWER, CONFIRM), and
Action Discussion consisting of commissives
(e.g., OFFER, PROMISE, ACCEPT-REQUEST) and
directives (e.g., INSTRUCT, REQUEST, DECLINE-
OFFER).
The taxonomy is currently being evaluated in
annotation experiments, involving several anno-
tators and several dialogue corpora. Measuring
inter-annotator agreement will give an indication
of the usability of the taxonomy and annotation
scheme. A first analysis has resulted in promising
scores (Geertzen and Bunt, 2006).
3 The DIT context model
The Information State according to DIT is repre-
sented by a Context Model, containing all infor-
mation considered relevant for interpreting user
utterances (in terms of dialogue acts) and gener-
ating system dialogue acts (leading to system ut-
terances). The contents of the context model are
therefore very closely related to the dialogue act
taxonomy; in (Bunt and Keizer, 2005) it is ar-
gued that the context model serves as a formal se-
mantics for dialogue annotation, such an annota-
tion being a kind of underspecified semantic rep-
resentation. In combination with additional gen-
eral conceptual considerations, the context model
has evolved into a five component structure:
1. Linguistic Context: linguistic information
about the utterances produced in the dialogue
so far (a kind of ?extended dialogue history?);
information about planned system dialogue
acts (a ?dialogue future?);
2. Semantic Context: contains current infor-
mation about the task/domain, including as-
sumptions about the dialogue partner?s infor-
mation;
3. Cognitive Context: the current processing
states of both participants (on the levels of
perception, interpretation, evaluation, and
task execution), as viewed by the speaker;
4. Physical and Perceptual Context: the percep-
tible aspects of the communication process
and the task/domain;
5. Social Context: current communicative pres-
sures.
In Figure 1, a feature structure representation of
the context model is given, in which the five com-
ponents have been specified in further detail. This
specification forms the basis for the dialogue man-
ager being implemented in the PARADIME project.
The Linguistic Context contains features for
storing dialogue acts performed in the dialogue so
far: user utts and system utts, having lists of di-
alogue act representations as values. It also has
features for information about topics and conver-
sational structure: topic struct and conv state re-
spectively. Finally, there are two features that
39
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
LingContext :
?
?
?
?
?
?
?
?
?
user utts : ?last user dial act = uda0 , uda?1 , uda?2 , . . .?
system utts : ?last system dial act = sda0 , sda?1 , sda?2 , . . .?
topic struct : ?referents?
conv state : opening |body |closing
candidate dial acts : . . .
dial acts pres : . . .
?
?
?
?
?
?
?
?
?
SemContext :
?
?
?
?
task progress : comp quest |quest qa|answ eval |user sat
user info needs : ?. . . ,
[
question : . . .
satisfied : +|?
]
, . . .?
qa answers : ?. . .?
?
?
?
?
CogContext :
[
own proc state : [proc problem : perc|int |eval |exec|none]
partner proc state : [proc problem : perc|int |eval |exec|none]
]
PhysPercContext :
[ ]
SocContext :
[
reactive pressures : none|grt |apo|thk |valed
interactive pressures : none|grt |apo|thk |valed
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Feature structure representation of the PARADIME context model.
are related to the actual generation of system di-
alogue acts: candidate dial acts stores the dia-
logue acts generated by the dialogue act agents,
and dial acts pres stores combined dialogue acts
for presentation as system output; in Section 4,
this will be discussed in more detail.
The specification of the Semantic Context is
determined by the character of the task-domain.
In Section 4.1, the task-domain of interactive
question-answering on encyclopedic medical in-
formation will be discussed and from that, the
specification of the Semantic Context for this pur-
pose.
The Cognitive Context is specified by means of
two features, representing the processing states of
the system (own proc state) and the user (part-
ner proc state). Both features indicate whether or
not a processing problem was encountered, and if
so, on which level of processing this happened.
The Physical and Perceptual Context is consid-
ered not to be relevant for the current system func-
tionality.
The Social Context is specified in terms of re-
active and interactive pressures; the correspond-
ing features indicate whether or not a pressure ex-
ists and if so, for which social obligations manage-
ment act it is a pressure (e.g., reactive pressures:
grt indicates a pressure for the system to respond
to a greeting).
4 Dialogue Act Agents
Having discussed the dialogue act taxonomy and
context model in DIT, we can now move on to the
dialogue management approach that is also closely
connected to these concepts. Having 11 dimen-
sions of dialogue acts that each attend to a dif-
ferent aspect of communication, the generation of
(system) dialogue acts should also happen along
those 11 dimensions. As a dialogue act in a di-
mension can be selected independent of the other
dimensions, we propose to divide the generation
process over 11 Dialogue Act Agents operating in
parallel on the information state of the system,
each agent dedicated to generating dialogue acts
from one particular dimension.
All of the dialogue act agents continuously
monitor the context model and, if appropriate, try
to generate candidate dialogue acts from their as-
sociated dimension. This process of monitoring
and act generation is modelled through a trigger-
ing mechanism: if the information state satisfies
the agent?s triggering conditions, i.e., if there is
a motivation for generating a dialogue act from a
particular dimension, the corresponding agent gets
triggered and tries to generate such a dialogue act.
For example, the Auto-Feedback Agent gets trig-
gered if a processing problem is recorded in the
Own Processing State of the Cognitive Context.
The agent then tries to generate a negative auto-
feedback act in order to solve the processing prob-
40
lem (e.g., ?Could you repeat that please?? or ?Did
you say ?five???). The Auto-Feedback Agent may
also be triggered if it has reason to believe that the
user is not certain that the system has understood
a previous utterance, or simply if it has not given
any explicit positive feedback for some time. In
these cases of triggering, the agent tries to gener-
ate a positive auto-feedback act.
Hence the dialogue management process in-
volves 11 dialogue act agents that operate in par-
allel on the context model. The dialogue acts gen-
erated by these agents are kept in the linguistic
context as candidates. The selection of dialogue
acts from different dimensions may happen inde-
pendently, but for their order of performance and
their combination, the relative importance of the
dimensions at the given point in the dialogue has
to be taken into account.
An additional Evaluation Agent monitors the
list of candidates and decides which of them can
be combined into a multifunctional system utter-
ance for generation, and when. Some of the dia-
logue act candidates may have higher priority and
should be generated at once, some may be stored
for possible generation in later system turns, and
some will already be implicitly performed through
the performance of other candidate acts.
4.1 A dialogue manager for interactive QA
The current implementation of the PARADIME
dialogue manager is integrated in an interactive
question-answering (QA) system, as developed
the IMIX multiproject. The task-domain at hand
concerns encyclopedic information in the medical
domain, in particular RSI (Repetitive Strain In-
jury). The system consists of several input anal-
ysis modules (ASR, syntactic analysis in terms
of dependency trees, and shallow semantic tag-
ging), three different QA modules that take self-
contained domain questions and return answers
retrieved from several electronic documents with
text data in the medical domain, and a presentation
module that takes the output from the dialogue
manager, possibly combining any QA-answers to
be presented, into a multimodal system utterance.
The dialogue management module provides
support for more interactive, coherent dialogues,
in which problems can be solved about both com-
munication and question-answering processes. In
interaction with the user, the system should play
the role of an Information Search Assistant (ISA).
This HCI metaphor posits that the dialogue system
is not an expert on the domain, but merely assists
the user in formulating questions about the domain
that will lead to QA answers from the QA mod-
ules satisfying the user?s information need (Akker
et al, 2005).
In the context model for this dialogue manager,
as represented by the feature structure in Figure 1,
the Semantic Context has been further specified
according to this underlying task. It contains a
state variable for keeping track of the question-
answering process (the feature task progress with
values to distinguish between the states of com-
posing a self-contained question to send to the QA
modules, waiting for the QA results in case a QA-
question has been sent, evaluating the QA results,
and discussing the results with the user). Also, the
Semantic Context keeps a record of user?s infor-
mation need, by means of a list user info needs
of ?information need? specifications in terms of
semantic descriptions of domain questions and
whether or not these info-needs have been satis-
fied.
For the first version of the dialogue manager
we have defined a limited system functionality,
and following from that a simplified version of
the dialogue act taxonomy. This simplification
means for example that Social Obligations Man-
agement (SOM) and the various dimensions in
the Interaction Management (IM) layer have been
merged into one dimension, following the obser-
vation that utterances with a SOM function very
often also have a function in the IM layer, es-
pecially in human-computer dialogue; see (Bunt,
2000b). Also several general-purpose commu-
nicative functions have been clustered into single
types. Table 1 lists the dialogue acts that the dia-
logue act recogniser is able to identify from user
utterances.
GP AUF IM-SOM
YN-Question PosAutoFb Init-Open
WH-Question NegAutoFb-Int Init-Close
H-Question NegAutoFb-Eval
Request
Instruct
Table 1: Dialogue act types for interpreting user
utterances.
Table 2 lists the dialogue acts that can be gen-
erated by the dialogue manager. Task-domain
acts, generally answers to questions about the do-
41
main, consist of a general-purpose function (either
a WH-ANSWER or UNC-WH-ANSWER; the latter
reflecting that the speaker is uncertain about the in-
formation provided) with a semantic content con-
taining the answers obtained from QA.
AUF ALF IM-SOM
NegAutoFb-Int Fb-Elicit React-Open
NegAutoFb-Exe React-Close
Table 2: Dialogue act types for generating system
responses.
The above considerations have resulted in a di-
alogue manager containing 4 dialogue act agents
that operate on a slightly simplified version of the
context model as specified in Figure 1: a Task-
Oriented (TO) Agent, an Auto-Feedback (AUF)
Agent, an Allo-Feedback (AUF) Agent, and an
Interaction Management and Social Obligations
Management (IMSOM) Agent. In addition, a (cur-
rently very simple) Evaluation Agent takes care of
merging candidate dialogue acts for output presen-
tation.
In Appendices A.1 and A.2, two example di-
alogues with the IMIX demonstrator system are
given, showing system responses based on can-
didate dialogue acts from several dialogue act
agents. The ISA metaphor is reflected in the sys-
tem behaviour especially in the way in which QA
results are presented to the user. In system utter-
ances S2 and S3 in Appendix A.1, for example,
the answer derived from the retrieved QA results is
isolated from the first part of the system utterance,
showing that the system has a neutral attitude con-
cerning that answer.
4.1.1 The Task-Oriented Agent
The TO-Agent is dedicated to the generation of
task-specific dialogue acts, which in practice in-
volves ANSWER dialogue acts intended to satisfy
the user?s information need about the (medical)
domain as indicated through his/her domain ques-
tions. The agent is triggered if a new information
need is recorded in the Semantic Context. Once it
has been triggered, the agent sends a request to the
QA modules to come up with answers to a ques-
tion asked, and evaluates the returned results. This
evaluation is based on the number of answers re-
ceived and the confidence scores of the answers;
the confidence scores are also part of the output of
the QA modules. If the QA did not find any an-
swers or if the answers produced had confidence
scores that were all below some lower threshold,
the TO-Agent will not generate a dialogue act, but
write an execution problem in the Own Process-
ing State of the Cognitive Context (which causes
the Auto-Feedback Agent to be triggered, see Sec-
tion 4.1.2; an example can be found in the dia-
logue in Appendix A.2). Otherwise, the TO-Agent
tries to make a selection from the QA answers
to be presented to the user. If this selection will
end up containing extremely many answers, again,
an execution problem is written in the Cognitive
Context (the question might have been too gen-
eral to be answerable). Otherwise, the selection
will be included in an answer dialogue act, either
a WHANSWER, or UNCWHANSWER (uncertain
wh-answer) in case the confidence scores are be-
low some upper threshold. System utterances S1
and S2 in the example dialogue in Appendix A.1
illustrate this variation. The selection is narrowed
down further if there is a subselection of answers
with confidences that are significantly higher than
those of the other answers in the selection.
4.1.2 The Auto-Feedback-Agent
The AUF-Agent is dedicated to the generation
of auto-feedback dialogue acts. It currently pro-
duces negative auto-feedback acts on the levels
of interpretation (?I didn?t understand what you
said?), evaluation (?I do not know what to do with
this?) and execution (?I could not find any answers
to your question?). It may also decide to occa-
sionally give positive feedback to the user. In the
future, we would also like this agent to be able
to generate articulate feedback acts, for example
with the purpose of resolving reference resolution
problems, as in:
U: what is RSI?
S: RSI (repetitive strain injury) is a pain or
discomfort caused by small repetitive move-
ments or tensions.
U: how can it be prevented?
S: do you mean ?RSI? or ?pain??
4.1.3 The Allo-Feedback Agent
The ALF-Agent is dedicated to the generation
of allo-feedback dialogue acts. For example, it
may generate a feedback-elicitation act if it has
reason to believe that the user might not be sat-
isfied with an answer (?Was this an answer to your
question??).
42
4.1.4 Interaction Management and Social
Obligations Management Agent
The IM-SOM Agent is dedicated to the gener-
ation of social obligations management acts, pos-
sibly also functioning as dialogue structuring acts
(opening resp. closing a dialogue through a greet-
ing resp. valediction act). It gets triggered if
communicative pressures are recorded in the So-
cial Context. Currently it only responds to re-
active pressures as caused by initiative greetings
and goodbyes. The example dialogues in Appen-
dices A.1 and A.2 illustrate this type of social be-
haviour.
4.1.5 Multi-agent Architecture of the
Dialogue Manager
In Figure 2, a schematic overview of the multi-
agent dialogue manager is given. It shows the
context model with four components (for now, the
Physical and Perceptual Context is considered to
be of minor importance and is therefore ignored),
a set of dialogue act agents, and an Evaluation
Agent. The dialogue act agents each monitor the
context model and may be triggered if certain con-
ditions are satisfied. The TO-agent may also write
to the Cognitive Context (particularly in case of
execution problems). All agents may construct
a dialogue act and write it in the candidates list
in the Linguistic Context. The Evaluation Agent
monitors this candidates list and selects one or
more dialogue acts from it for presentation as sys-
tem output. In this way, a control module may
decide to take this combination of dialogue act for
presentation anytime and send it to the presenta-
tion module to produce a system utterance.
With this initial design of a multi-agent dia-
logue manager, the system is able to support mul-
tifunctional output. The beginning of the example
dialogue in Appendix A.1 illustrates multifunc-
tionality, both in input interpretation and output
generation. The system has recognised two dia-
logue acts in processing U1 (a conventional open-
ing and a domain question), and S1 is generated
on the basis of two candidate dialogue acts gen-
erated by different dialogue act agents: the IM-
SOM-Agent (generated the react-greeting act) and
the TO-Agent (generated the answer act).
5 Conclusions and future work
We have presented a dialogue management ap-
proach supporting the generation of multifunc-
candidatedialogue acts
IM?SOM?Agent
TO?Agent
AUF?Agent
ALF?Agent
Semantic Context
Cognitive Context
Social Context
Linguistic Context
candidatedialogue acts
Eval?Agent
dialogue actsfor presentation
DIALOGUE ACT AGENTS
CONTEXT MODEL
Figure 2: Architecture of the PARADIME dialogue
manager.
tional utterances. The approach builds on a di-
alogue theory involving a multidimensional dia-
logue act taxonomy and an information state on
which the dialogue acts operate. Several dialogue
acts from different dimensions are generated by
dialogue act agents associated with these dimen-
sions, and can thus be combined into multifunc-
tional system utterances.
A first implementation of a dialogue manager
following this multi-agent approach has been in-
tegrated into an interactive QA system and sup-
ports a limited range of dialogue acts from the
DIT taxonomy, both for interpreting user utter-
ances and generating system utterances. The sys-
tem is able to attend to different aspects of the
communication simultaneously, involving reactive
social behaviour, answering domain questions and
giving feedback about utterance interpretation and
the question-answering process.
Future development will involve extending the
range of dialogue acts to be covered by the dia-
logue manager, for a part following from the def-
inition of an extended system functionality, and
consequently, extending the set of dialogue act
agents. This also has consequences for the Eval-
uation Agent: the process of combination and se-
lection will be more complex if more dialogue act
types can be expected and if the dialogue acts have
a semantic content that is more than just a collec-
tion of QA-answers.
In terms of system functionality we aim at sup-
43
port for generating articulate feedback, i.e., feed-
back acts that are not merely signalling processing
success or failure, but (in case of negative feed-
back) also contain a further specification of the
processing problem at hand. For example, the sys-
tem may have encountered problems in processing
certain parts of a user utterance, or in resolving an
anaphor; then it should be able to ask the user a
specific question in order to obtain the informa-
tion required to solve the processing problem (see
the example in Section 4.1.2). The articulate feed-
back acts may also involve dealing with problems
in the question answering process, where the sys-
tem should be able to give specific instructions to
the user to reformulate his question or give addi-
tional information about his information need.
In addition to supporting generation of articu-
late feedback acts, we also aim at dialogues be-
tween user and system that are more coherent and
natural, i.e., the system should be more aware of
the conversational structure, and display more re-
fined social behaviour. Not only should it gener-
ate simple reactions to greetings, apologies, and
goodbyes; it should also be able to generate initia-
tive social acts, for example, apologies after sev-
eral cases of negative auto-feedback.
The extended set of dialogue acts will also lead
to an extended context model. Related to the
context model and updating mechanism is on-
going work on belief dynamics and grounding
in DIT (Morante and Bunt, 2005). The defined
mechanisms for the creation, strengthening, adop-
tion, and cancelling of beliefs and goals in the
context model are currently being implemented
in a demonstrator tool and will also be integrated
in the information state update mechanism of the
PARADIME dialogue manager.
Acknowledgement
This work is part of PARADIME (Parallel Agent-
based Dialogue Management Engine), which is a
subproject of IMIX (Interactive Multimodal Infor-
mation eXtraction), a multiproject on Dutch lan-
guage and speech technology, funded by the Dutch
national science foundation (NWO).
We would like to thank the reviewers for their
valuable comments, which really helped us to im-
prove our paper.
References
R. op den Akker, H. Bunt, S. Keizer, and B. van
Schooten. 2005. From question answering to spo-
ken dialogue: Towards an information search assis-
tant for interactive multimodal information extrac-
tion. In Proceedings of the 9th European Confer-
ence on Speech Communication and Technology, In-
terspeech 2005, pages 2793?2796.
H. Bunt and S. Keizer. 2005. Dialogue semantics
links annotation to context representation. In Joint
TALK/AMI Workshop on Standards for Multimodal
Dialogue Context. http://homepages.inf.
ed.ac.uk/olemon/standcon-SOI.html.
H. Bunt. 1996. Dynamic interpretation and dia-
logue theory. In M.M. Taylor, F. Ne?el, and D.G.
Bouwhuis, editors, The Structure of Multimodal Di-
alogue, Volume 2, pages 139?166. John Benjamins.
H. Bunt. 2000a. Dialogue pragmatics and context
specification. In H. Bunt and W. Black, editors,
Abduction, Belief and Context in Dialogue, Studies
in Computational Pragmatics, pages 81?150. John
Benjamins.
H. Bunt. 2000b. Non-problems and social obliga-
tions in human-computer conversation. In Proceed-
ings of the 3rd International Workshop on Human-
Computer Conversation, pages 36?41.
H. Bunt. 2006. Dimensions in dialogue act anno-
tation. In Proceedings Fifth International Confer-
ence on Language Resources and Evaluation (LREC
2006).
J. Geertzen and H. Bunt. 2006. Measuring annotator
agreement in a complex hierarchical dialogue act an-
notation scheme. In 7th SIGdial Workshop on Dis-
course and Dialogue.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dialogue
move engine toolkit. Natural Language Engineer-
ing: Special Issue on Best Practice in Spoken Lan-
guage Dialogue Systems, 6(3?4):323?340.
C. Matheson, M. Poesio, and D. Traum. 2000. Mod-
eling grounding and discourse obligations using up-
date rules. In Proceedings of the First Annual Meet-
ing of the North American Chapter of the ACL.
R. Morante and H. Bunt. 2005. Micro-analysis of the
belief transfer in information dialogues. In C. Gar-
dent and B. Gaiffe, editors, DIALOR?05 Proceed-
ings of the Ninth Workshop on the Semantics and
Pragmatics of Dialogue, pages 135?138.
E.P. Salonen, M. Hartikainen, M. Turunen, J. Hakuli-
nen, and J.A. Funk. 2004. Flexible dialogue man-
agement using distributed and dynamic dialogue
control. In Proceedings of ICSLP 2004.
A.J. Stent. 2002. A conversation acts model for gen-
erating spoken dialogue contributions. Computer
44
Speech and Language, Special Issue on Spoken Lan-
guage Generation, 16(3?4):313?352.
D.R. Traum and E.R. Hinkelman. 1992. Conversa-
tion acts in task-oriented spoken dialogue. Compu-
tational Intelligence, 3(8):575?599.
M. Turunen and J. Hakulinen. 2000. Jaspis ? a frame-
work for multilingual adaptive speech applications.
In Sixth International Conference on Spoken Lan-
guage Processing (ICSLP 2000), volume 2, pages
719?722.
A Example dialogues
A.1 Example 1: greetings, answers and
unc-answers
U1: hallo, ik wilde graag weten wat rsi is
(hello, I wanted to know what is rsi)
[INIT-OPEN; INDWHQ, task-domain]
S1: Hallo. Ik heb de volgende informatie gevonden:
(repetitive strain injury) Algemene benaming voor
(blijvende) beschadigingen aan het lichaam ten gevolge
van langdurige, eentonige werkzaamheden.
(Hello. I found the following information: ...)
[REACT-OPEN; ANSWER, task-domain]
U2: wat moet ik doen als ik denk rsi te hebben
(what should I do if I think I have RSI)
[WHQUESTION, task-domain]
S2: De volgende informatie levert mogelijk een antwoord
op uw vraag:
Ontspanning betekent minder stress, minder aange-
spannen spieren en dus minder kans op RSI.
(The following information might answer your ques-
tion: ...)
[UNCANSWER, task-domain]
U3: helpt ontspanning ook als ik eenmaal rsi heb
(does relaxing help as well, once I have RSI)
[YNQUESTION, task-domain]
S3: Ik heb de volgende informatie gevonden:
Behandelmethoden bij RSI Er zijn vele wegen die naar
Rome leiden. Verbetering van de doorbloeding ...
(I found the following information: ...)
[ANSWER, task-domain]
U4: oke, bedankt
(okay, thanks)
[INIT-CLOSE]
S4: tot ziens. U kunt de buttons gebruiken om te stoppen
of om een nieuwe dialoog te beginnen.
(Goodbye. You may use the buttons to stop or to start a
new dialogue.)
[REACT-CLOSE]
A.2 Example 2: negative execution-feedback
U1: hallo!
(hello!)
[INIT-OPEN]
S1: Hallo. Stel uw vraag.
(Hello. Ask your question.)
[REACT-OPEN]
U2: wat kan ik als beelschermwerker tegen rugpijn doen
(what can I, a person working behind a computer
screen all the time, do against pain in the back)
[WHQUESTION, task-domain]
S2: Ik heb helaas geen antwoorden kunnen vinden; probeer
de vraag te herformuleren.
(Unfortunately, I could not find any answers; please try
to reformulate the question)
[NEGAUTOFB-EXEC]
U3: wat kan men tegen rugpijn doen
(what can be done against pain in the back)
[WHQUESTION, task-domain]
S3: ik heb de volgende informatie gevonden:
Deze is afhankelijk van de oorzaak. Rugpijn die door
instabiliteit of door tekortschietende spieren en ban-
den ontstaat, kan behandeld worden met houdingscor-
rigerende oefentherapie.
(I found the following information: ...)
[ANSWER, task-domain]
U4: bedankt!
(thanks!)
[INIT-CLOSE]
S4: Tot ziens. U kunt de buttons gebruiken om te stoppen
of om een nieuwe dialoog te beginnen.
(Goodbye. U may use the buttons to stop or to start a
new dialogue.)
[REACT-CLOSE]
45
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112?119,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Training and Evaluation of the HIS POMDP Dialogue System in Noise
M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, K. Yu, S. Young
Machine Intelligence Laboratory
Engineering Department
Cambridge University
United Kingdom
Abstract
This paper investigates the claim that a di-
alogue manager modelled as a Partially Ob-
servable Markov Decision Process (POMDP)
can achieve improved robustness to noise
compared to conventional state-based dia-
logue managers. Using the Hidden Infor-
mation State (HIS) POMDP dialogue man-
ager as an exemplar, and an MDP-based dia-
logue manager as a baseline, evaluation results
are presented for both simulated and real dia-
logues in a Tourist Information Domain. The
results on the simulated data show that the
inherent ability to model uncertainty, allows
the POMDP model to exploit alternative hy-
potheses from the speech understanding sys-
tem. The results obtained from a user trial
show that the HIS system with a trained policy
performed significantly better than the MDP
baseline.
1 Introduction
Conventional spoken dialogue systems operate by
finding the most likely interpretation of each user
input, updating some internal representation of the
dialogue state and then outputting an appropriate re-
sponse. Error tolerance depends on using confidence
thresholds and where they fail, the dialogue manager
must resort to quite complex recovery procedures.
Such a system has no explicit mechanisms for rep-
resenting the inevitable uncertainties associated with
speech understanding or the ambiguities which natu-
rally arise in interpreting a user?s intentions. The re-
sult is a system that is inherently fragile, especially
in noisy conditions or where the user is unsure of
how to use the system.
It has been suggested that Partially Observable
Markov Decision Processes (POMDPs) offer a nat-
ural framework for building spoken dialogue sys-
tems which can both model these uncertainties
and support policies which are robust to their ef-
fects (Young, 2002; Williams and Young, 2007a).
The key idea of the POMDP is that the underlying
dialogue state is hidden and dialogue management
policies must therefore be based not on a single state
estimate but on a distribution over all states.
Whilst POMDPs are attractive theoretically, in
practice, they are notoriously intractable for any-
thing other than small state/action spaces. Hence,
practical examples of their use were initially re-
stricted to very simple domains (Roy et al, 2000;
Zhang et al, 2001). More recently, however, a num-
ber of techniques have been suggested which do al-
low POMDPs to be scaled to handle real world tasks.
The two generic mechanisms which facilitate this
scaling are factoring the state space and perform-
ing policy optimisation in a reduced summary state
space (Williams and Young, 2007a; Williams and
Young, 2007b).
Based on these ideas, a number of real-world
POMDP-based systems have recently emerged. The
most complex entity which must be represented in
the state space is the user?s goal. In the Bayesian
Update of Dialogue State (BUDS) system, the user?s
goal is further factored into conditionally indepen-
dent slots. The resulting system is then modelled
as a dynamic Bayesian network (Thomson et al,
2008). A similar approach is also developed in
112
(Bui et al, 2007a; Bui et al, 2007b). An alterna-
tive approach taken in the Hidden Information State
(HIS) system is to retain a complete representation
of the user?s goal, but partition states into equiva-
lence classes and prune away very low probability
partitions (Young et al, 2007; Thomson et al, 2007;
Williams and Young, 2007b).
Whichever approach is taken, a key issue in a real
POMDP-based dialogue system is its ability to be
robust to noise and that is the issue that is addressed
in this paper. Using the HIS system as an exem-
plar, evaluation results are presented for a real-world
tourist information task using both simulated and
real users. The results show that a POMDP system
can learn noise robust policies and that N-best out-
puts from the speech understanding component can
be exploited to further improve robustness.
The paper is structured as follows. Firstly, in Sec-
tion 2 a brief overview of the HIS system is given.
Then in Section 3, various POMDP training regimes
are described and evaluated using a simulated user at
differing noise levels. Section 4 then presents results
from a trial in which users conducted various tasks
over a range of noise levels. Finally, in Section 5,
we discuss our results and present our conclusions.
2 The HIS System
2.1 Basic Principles
A POMDP-based dialogue system is shown in Fig-
ure 1 where sm denotes the (unobserved or hidden)
machine state which is factored into three compo-
nents: the last user act au, the user?s goal su and
the dialogue history sd. Since sm is unknown, at
each time-step the system computes a belief state
such that the probability of being in state sm given
belief state b is b(sm). Based on this current belief
state b, the machine selects an action am, receives
a reward r(sm, am), and transitions to a new (un-
observed) state s?m, where s?m depends only on sm
and am. The machine then receives an observation
o? consisting of an N-best list of hypothesised user
actions. Finally, the belief distribution b is updated
based on o? and am as follows:
b?(s?m) = kP (o?|s?m, am)
?
sm?Sm
P (s?m|am, sm)b(sm)
(1)
where k is a normalisation constant (Kaelbling et al,
1998). The first term on the RHS of (1) is called the
observation model and the term inside the summa-
tion is called the transition model. Maintaining this
belief state as the dialogue evolves is called belief
monitoring.
Speech
Understanding
Speech
Generation
User
au
am
~
su
am
Belief
Estimator
Dialog
Policy
sd
b(     )sm
sm = <au,s u,s d>
au
1
.
.

au
N
Figure 1: Abstract view of a POMDP-based spoken dia-
logue system
At each time step t, the machine receives a reward
r(bt, am,t) based on the current belief state bt and the
selected action am,t. Each action am,t is determined
by a policy ?(bt) and building a POMDP system in-
volves finding the policy ?? which maximises the
discounted sum R of the rewards
R =
??
t=0
?tr(bt, am,t) (2)
where ?t is a discount coefficient.
2.2 Probability Models
In the HIS system, user goals are partitioned and
initially, all states su ? Su are regarded as being
equally likely and they are placed in a single par-
tition p0. As the dialogue progresses, user inputs
result in changing beliefs and this root partition is
repeatedly split into smaller partitions. This split-
ting is binary, i.e. p ? {p?, p ? p?} with probability
P (p?|p). By replacing sm by its factors (su, au, sd)
and making reasonable independence assumptions,
it can be shown (Young et al, 2007) that in parti-
113
tioned form (1) becomes
b?(p?, a?u, s?d) = k ? P (o?|a?u)
? ?? ?
observation
model
P (a?u|p?, am)
? ?? ?
user action
model
?
?
sd
P (s?d|p?, a?u, sd, am)
? ?? ?
dialogue
model
P (p?|p)b(p, sd)
? ?? ?
partition
splitting
(3)
where p is the parent of p?.
In this equation, the observation model is approx-
imated by the normalised distribution of confidence
measures output by the speech recognition system.
The user action model allows the observation prob-
ability that is conditioned on a?u to be scaled by the
probability that the user would speak a?u given the
partition p? and the last system prompt am. In the
current implementation of the HIS system, user dia-
logue acts take the form act(a = v) where act is the
dialogue type, a is an attribute and v is its value [for
example, request(food=Chinese)]. The user action
model is then approximated by
P (a?u|p?, am) ? P (T (a?u)|T (am))P (M(a?u)|p?)
(4)
where T (?) denotes the type of the dialogue act
and M(?) denotes whether or not the dialogue act
matches the current partition p?. The dialogue
model is a deterministic encoding based on a simple
grounding model. It yields probability one when the
updated dialogue hypothesis (i.e., a specific combi-
nation of p?, a?u, sd and am) is consistent with the
history and zero otherwise.
2.3 Policy Representation
Policy representation in POMDP-systems is non-
trivial since each action depends on a complex prob-
ability distribution. One of the simplest approaches
to dealing with this problem is to discretise the state
space and then associate an action with each dis-
crete grid point. To reduce quantisation errors, the
HIS model first maps belief distributions into a re-
duced summary space before quantising. This sum-
mary space consists of the probability of the top
two hypotheses plus some status variables and the
user act type associated with the top distribution.
Quantisation is then performed using a simple dis-
tance metric to find the nearest grid point. Ac-
tions in summary space refer specifically to the top
two hypotheses, and unlike actions in master space,
they are limited to a small finite set: greet, ask, ex-
plicit confirm, implicit confirm, select confirm, of-
fer, inform, find alternative, query more, goodbye.
A simple heuristic is then used to map the selected
next system action back into the full master belief
space.
1
Observation
From
User
Ontology Rules
2
N
ua
ma
~
From
System
1
1
2
2
2
1
ds
2
ds
1
ds
2
ds
3
ds
1
up
2
up
3
up
POMDP
Policy
2h
3h
4h
5h
1h







~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
Belief
State
Application Database
Action
Refinement
(heuristic)
ma
^
Strategic
Action
Specific
Action
Map to
Summary
Space
ma
b
b^
Summary Space
Figure 2: Overview of the HIS system dialogue cycle
The dialogue manager is able to support nega-
tions, denials and requests for alternatives. When the
selected summary action is to offer the user a venue,
the summary-to-master space mapping heuristics
will normally offer a venue consistent with the most
likely user goal hypothesis. If this hypothesis is then
rejected its belief is substantially reduced and it will
no longer be the top-ranking hypothesis. If the next
system action is to make an alternative offer, then
the new top-ranking hypothesis may not be appro-
priate. For example, if an expensive French restau-
rant near the river had been offered and the user asks
for one nearer the centre of town, any alternative of-
fered should still include the user?s confirmed de-
sire for an expensive French restaurant. To ensure
this, all of the grounded features from the rejected
hypothesis are extracted and all user goal hypothe-
ses are scanned starting at the most likely until an
alternative is found that matches the grounded fea-
tures. For the current turn only, the summary-to-
master space heuristics then treat this hypothesis as
if it was the top-ranking one. If the system then of-
fers a venue based on this hypothesis, and the user
accepts it, then, since system outputs are appended
to user inputs for the purpose of belief updating, the
114
alternative hypothesis will move to the top, or near
the top, of the ranked hypothesis list. The dialogue
then typically continues with its focus on the newly
offered alternative venue.
2.4 Summary of Operation
To summarise, the overall processing performed by
the HIS system in a single dialogue turn (i.e. one cy-
cle of system output and user response) is as shown
in Figure 2. Each user utterance is decoded into an
N-best list of dialogue acts. Each incoming act plus
the previous system act are matched against the for-
est of user goals and partitions are split as needed.
Each user act au is then duplicated and bound to
each partition p. Each partition will also have a
set of dialogue histories sd associated with it. The
combination of each p, au and updated sd forms a
new dialogue hypothesis hk whose beliefs are eval-
uated using (3). Once all dialogue hypotheses have
been evaluated and any duplicates merged, the mas-
ter belief state b is mapped into summary space b?
and the nearest policy belief point is found. The as-
sociated summary space machine action a?m is then
heuristically mapped back to master space and the
machine?s actual response am is output. The cycle
then repeats until the user?s goal is satisfied.
3 Training and Evaluation with a
Simulated User
3.1 Policy optimisation
Policy optimisation is performed in the discrete
summary space described in the previous section us-
ing on-line batch ?-greedy policy iteration. Given
an existing policy ?, dialogs are executed and ma-
chine actions generated according to ? except that
with probability ? a random action is generated. The
system maintains a set of belief points {b?i}. At each
turn in training, the nearest stored belief point b?k to
b? is located using a distance measure. If the distance
is greater than some threshold, b? is added to the set
of stored belief points. The sequence of points b?k
traversed in each dialogue is stored in a list. As-
sociated with each b?i is a function Q(b?i, a?m) whose
value is the expected total reward obtained by choos-
ing summary action a?m from state b?i. At the end
of each dialogue, the total reward is calculated and
added to an accumulator for each point in the list,
discounted by ? at each step. On completion of a
batch of dialogs, the Q values are updated accord-
ing to the accumulated rewards, and the policy up-
dated by choosing the action which maximises each
Q value. The whole process is then repeated until
the policy stabilises.
In our experiments, ? was fixed at 0.1 and ? was
fixed at 0.95. The reward function used attempted
to encourage short successful dialogues by assign-
ing +20 for a successful dialogue and ?1 for each
dialogue turn.
3.2 User Simulation
To train a policy, a user simulator is used to gen-
erate responses to system actions. It has two main
components: a User Goal and a User Agenda. At
the start of each dialogue, the goal is randomly
initialised with requests such as ?name?, ?addr?,
?phone? and constraints such as ?type=restaurant?,
?food=Chinese?, etc. The agenda stores the di-
alogue acts needed to elicit this information in a
stack-like structure which enables it to temporarily
store actions when another action of higher priority
needs to be issued first. This enables the simulator
to refer to previous dialogue turns at a later point. To
generate a wide spread of realistic dialogs, the sim-
ulator reacts wherever possible with varying levels
of patience and arbitrariness. In addition, the sim-
ulator will relax its constraints when its initial goal
cannot be satisfied. This allows the dialogue man-
ager to learn negotiation-type dialogues where only
an approximate solution to the user?s goal exists.
Speech understanding errors are simulated at the di-
alogue act level using confusion matrices trained on
labelled dialogue data (Schatzmann et al, 2007).
3.3 Training and Evaluation
When training a system to operate robustly in noisy
conditions, a variety of strategies are possible. For
example, the system can be trained only on noise-
free interactions, it can be trained on increasing lev-
els of noise or it can be trained on a high noise level
from the outset. A related issue concerns the gener-
ation of grid points and the number of training itera-
tions to perform. For example, allowing a very large
number of points leads to poor performance due to
over-fitting of the training data. Conversely, having
too few point leads to poor performance due to a lack
115
of discrimination in its dialogue strategies.
After some experimentation, the following train-
ing schedule was adopted. Training starts in a
noise free environment using a small number of grid
points and it continues until the performance of the
policy levels off. The resulting policy is then taken
as an initial policy for the next stage where the noise
level is increased, the number of grid points is ex-
panded and the number of iterations is increased.
This process is repeated until the highest noise level
is reached. This approach was motivated by the ob-
servation that a key factor in effective reinforcement
learning is the balance between exploration and ex-
ploitation. In POMDP policy optimisation which
uses dynamically allocated grid points, maintaining
this balance is crucial. In our case, the noise intro-
duced by the simulator is used as an implicit mech-
anism for increasing the exploration. Each time ex-
ploration is increased, the areas of state-space that
will be visited will also increase and hence the num-
ber of available grid points must also be increased.
At the same time, the number of iterations must be
increased to ensure that all points are visited a suf-
ficient number of times. In practice we found that
around 750 to 1000 grid points was sufficient and
the total number of simulated dialogues needed for
training was around 100,000.
A second issue when training in noisy conditions
is whether to train on just the 1-best output from the
simulator or train on the N-best outputs. A limit-
ing factor here is that the computation required for
N-best training is significantly increased since the
rate of partition generation in the HIS model in-
creases exponentially with N. In preliminary tests,
it was found that when training with 1-best outputs,
there was little difference between policies trained
entirely in no noise and policies trained on increas-
ing noise as described above. However, policies
trained on 2-best using the incremental strategy did
exhibit increased robustness to noise. To illustrate
this, Figures 3 and 4 show the average dialogue suc-
cess rates and rewards for 3 different policies, all
trained on 2-best: a hand-crafted policy (hdc), a pol-
icy trained on noise-free conditions (noise free) and
a policy trained using the incremental scheme de-
scribed above (increm). Each policy was tested us-
ing 2-best output from the simulator across a range
of error rates. In addition, the noise-free policy was
also tested on 1-best output.
Figure 3: Average simulated dialogue success rate as a
function of error rate for a hand-crafted (hdc), noise-free
and incrementally trained (increm) policy.
Figure 4: Average simulated dialogue reward as a func-
tion of error rate for a hand-crafted (hdc), noise-free and
incrementally trained (increm) policy.
As can be seen, both the trained policies improve
significantly on the hand-crafted policies. Further-
more, although the average rewards are all broadly
similar, the success rate of the incrementally trained
policy is significantly better at higher error rates.
Hence, this latter policy was selected for the user
trial described next.
4 Evaluation via a User Trial
The HIS-POMDP policy (HIS-TRA) that was incre-
mentally trained on the simulated user using 2-best
lists was tested in a user trial together with a hand-
crafted HIS-POMDP policy (HIS-HDC). The strat-
egy used by the latter was to first check the most
likely hypothesis. If it contains sufficient grounded
116
keys to match 1 to 3 database entities, then offer is
selected. If any part of the hypothesis is inconsis-
tent or the user has explicitly asked for another sug-
gestion, then find alternative action is selected. If
the user has asked for information about an offered
entity then inform is selected. Otherwise, an un-
grounded component of the top hypothesis is identi-
fied and depending on the belief, one of the confirm
actions is selected.
In addition, an MDP-based dialogue manager de-
veloped for earlier trials (Schatzmann, 2008) was
also tested. Since considerable effort has been put in
optimising this system, it serves as a strong baseline
for comparison. Again, both a trained policy (MDP-
TRA) and a hand-crafted policy (MDP-HDC) were
tested.
4.1 System setup and confidence scoring
The dialogue system consisted of an ATK-based
speech recogniser, a Phoenix-based semantic parser,
the dialogue manager and a diphone based speech
synthesiser. The semantic parser uses simple phrasal
grammar rules to extract the dialogue act type and a
list of attribute/value pairs from each utterance.
In a POMDP-based dialogue system, accurate
belief-updating is very sensitive to the confidence
scores assigned to each user dialogue act. Ideally
these should provide a measure of the probability of
the decoded act given the true user act. In the evalu-
ation system, the recogniser generates a 10-best list
of hypotheses at each turn along with a compact con-
fusion network which is used to compute the infer-
ence evidence for each hypothesis. The latter is de-
fined as the sum of the log-likelihoods of each arc
in the confusion network and when exponentiated
and renormalised this gives a simple estimate of the
probability of each hypothesised utterance. Each ut-
terance in the 10-best list is passed to the semantic
parser. Equivalent dialogue acts output by the parser
are then grouped together and the dialogue act for
each group is then assigned the sum of the sentence-
level probabilities as its confidence score.
4.2 Trial setup
For the trial itself, 36 subjects were recruited (all
British native speakers, 18 male, 18 female). Each
subject was asked to imagine himself to be a tourist
in a fictitious town called Jasonville and try to find
particular hotels, bars, or restaurants in that town.
Each subject was asked to complete a set of pre-
defined tasks where each task involved finding the
name of a venue satisfying a set of constraints such
as food type is Chinese, price-range is cheap, etc.,
and getting the value of one or more additional at-
tributes of that venue such as the address or the
phone number.
For each task, subjects were given a scenario to
read and were then asked to solve the task via a di-
alogue with the system. The tasks set could either
have one solution, several solutions, or no solution
at all in the database. In cases where a subject found
that there was no matching venue for the given task,
he/she was allowed to try and find an alternative
venue by relaxing one or more of the constraints.
In addition, subjects had to perform each task at
one of three possible noise levels. These levels cor-
respond to signal/noise ratios (SNRs) of 35.3 dB
(low noise), 10.2 dB (medium noise), or 3.3 dB
(high noise). The noise was artificially generated
and mixed with the microphone signal, in addition
it was fed into the subject?s headphones so that they
were aware of the noisy conditions.
An instructor was present at all times to indicate
to the subject which task description to follow, and
to start the right system with the appropriate noise-
level. Each subject performed an equal number of
tasks for each system (3 tasks), noise level (6 tasks)
and solution type (6 tasks for each of the types 0, 1,
or multiple solutions). Also, each subject performed
one task for all combinations of system and noise
level. Overall, each combination of system, noise
level, and solution type was used in an equal number
of dialogues.
4.3 Results
In Table 1, some general statistics of the corpus re-
sulting from the trial are given. The semantic error
rate is based on substitutions, insertions and dele-
tions errors on semantic items. When tested after the
trial on the transcribed user utterances, the semantic
error rate was 4.1% whereas the semantic error rate
on the ASR input was 25.2%. This means that 84%
of the error rate was due to the ASR.
Tables 2 and 3 present success rates (Succ.) and
average performance scores (Perf.), comparing the
two HIS dialogue managers with the two MDP base-
117
Number of dialogues 432
Number of dialogue turns 3972
Number of words (transcriptions) 18239
Words per utterance 4.58
Word Error Rate 32.9
Semantic Error Rate 25.2
Semantic Error Rate transcriptions 4.1
Table 1: General corpus statistics.
line systems. For the success rates, also the stan-
dard deviation (std.dev) is given, assuming a bino-
mial distribution. The success rate is the percentage
of successfully completed dialogues. A task is con-
sidered to be fully completed when the user is able to
find the venue he is looking for and get al the addi-
tional information he asked for; if the task has no so-
lution and the system indicates to the user no venue
could be found, this also counts as full completion.
A task is considered to be partially completed when
only the correct venue has been given. The results on
partial completion are given in Table 2, and the re-
sults on full completion in Table 3. To mirror the re-
ward function used in training, the performance for
each dialogue is computed by assigning a reward of
20 points for full completion and subtracting 1 point
for the number of turns up until a successful recom-
mendation (i.e., partial completion).
Partial Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 68.52 (4.83) 4.80 8.91
MDP-TRA 70.37 (4.75) 4.75 9.32
HIS-HDC 74.07 (4.55) 7.04 7.78
HIS-TRA 84.26 (3.78) 4.63 12.22
Table 2: Success rates and performance results on partial
completion.
Full Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 64.81 (4.96) 5.86 7.10
MDP-TRA 65.74 (4.93) 6.18 6.97
HIS-HDC 63.89 (4.99) 8.57 4.20
HIS-TRA 78.70 (4.25) 6.36 9.38
Table 3: Success rates and performance results on full
completion.
The results show that the trained HIS dialogue
manager significantly outperforms both MDP based
dialogue managers. For success rate on partial com-
pletion, both HIS systems perform better than the
MDP systems.
4.3.1 Subjective Results
In the user trial, the subjects were also asked for
a subjective judgement of the systems. After com-
pleting each task, the subjects were asked whether
they had found the information they were looking
for (yes/no). They were also asked to give a score
on a scale from 1 to 5 (best) on how natural/intuitive
they thought the dialogue was. Table 4 shows the
results for the 4 systems used. The performance of
the HIS systems is similar to the MDP systems, with
a slightly higher success rate for the trained one and
a slightly lower score for the handcrafted one.
System Succ. Rate (std.dev) Score
MDP-HDC 78 (4.30) 3.52
MDP-TRA 78 (4.30) 3.42
HIS-HDC 71 (4.72) 3.05
HIS-TRA 83 (3.90) 3.41
Table 4: Subjective performance results from the user
trial.
5 Conclusions
This paper has described recent work in training a
POMDP-based dialogue manager to exploit the ad-
ditional information available from a speech under-
standing system which can generate ranked lists of
hypotheses. Following a brief overview of the Hid-
den Information State dialogue manager and pol-
icy optimisation using a user simulator, results have
been given for both simulated user and real user di-
alogues conducted at a variety of noise levels.
The user simulation results have shown that al-
though the rewards are similar, training with 2-best
rather than 1-best outputs from the user simulator
yields better success rates at high noise levels. In
view of this result, we would have liked to inves-
tigate training on longer N-best lists, but currently
computational constraints prevent this. We hope in
the future to address this issue by developing more
efficient state partitioning strategies for the HIS sys-
tem.
118
The overall results on real data collected from the
user trial clearly indicate increased robustness by the
HIS system. We would have liked to be able to
plot performance and success scores as a function
of noise level or speech understanding error rate,
but there is great variability in these kinds of com-
plex real-world dialogues and it transpired that the
trial data was insufficient to enable any statistically
meaningful presentation of this form. We estimate
that we need at least an order of magnitude more
trial data to properly investigate the behaviour of
such systems as a function of noise level. The trial
described here, including transcription and analysis
consumed about 30 man-days of effort. Increasing
this by a factor of 10 or more is not therefore an
option for us, and clearly an alternative approach is
needed.
We have also reported results of subjective suc-
cess rate and opinion scores based on data obtained
from subjects after each trial. The results were only
weakly correlated with the measured performance
and success rates. We believe that this is partly due
to confusion as to what constituted success in the
minds of the subjects. This suggests that for subjec-
tive results to be meaningful, measurements such as
these will only be really useful if made on live sys-
tems where users have a real rather than imagined
information need. The use of live systems would
also alleviate the data sparsity problem noted earlier.
Finally and in conclusion, we believe that despite
the difficulties noted above, the results reported in
this paper represent a first step towards establish-
ing the POMDP as a viable framework for develop-
ing spoken dialogue systems which are significantly
more robust to noisy operating conditions than con-
ventional state-based systems.
Acknowledgements
This research was partly funded by the UK EPSRC
under grant agreement EP/F013930/1 and by the
EU FP7 Programme under grant agreement 216594
(CLASSIC project: www.classic-project.org).
References
TH Bui, M Poel, A Nijholt, and J Zwiers. 2007a. A
tractable DDN-POMDP Approach to Affective Dia-
logue Modeling for General Probabilistic Frame-based
Dialogue Systems. In Proc 5th Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
pages 34?57.
TH Bui, B van Schooten, and D Hofs. 2007b. Practi-
cal dialogue manager development using POMDPs .
In 8th SIGdial Workshop on Discourse and Dialogue,
Antwerp.
LP Kaelbling, ML Littman, and AR Cassandra. 1998.
Planning and Acting in Partially Observable Stochastic
Domains. Artificial Intelligence, 101:99?134.
N Roy, J Pineau, and S Thrun. 2000. Spoken Dialogue
Management Using Probabilistic Reasoning. In Proc
ACL.
J Schatzmann, B Thomson, and SJ Young. 2007. Error
Simulation for Training Statistical Dialogue Systems.
In ASRU 07, Kyoto, Japan.
J Schatzmann. 2008. Statistical User and Error Mod-
elling for Spoken Dialogue Systems. Ph.D. thesis, Uni-
versity of Cambridge.
B Thomson, J Schatzmann, K Weilhammer, H Ye, and
SJ Young. 2007. Training a real-world POMDP-based
Dialog System. In HLT/NAACL Workshop ?Bridging
the Gap: Academic and Industrial Research in Dialog
Technologies?, Rochester.
B Thomson, J Schatzmann, and SJ Young. 2008.
Bayesian Update of Dialogue State for Robust Dia-
logue Systems. In Int Conf Acoustics Speech and Sig-
nal Processing ICASSP, Las Vegas.
JD Williams and SJ Young. 2007a. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
JD Williams and SJ Young. 2007b. Scaling POMDPs
for Spoken Dialog Management. IEEE Audio, Speech
and Language Processing, 15(7):2116?2129.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Dia-
log Management. In ICASSP 2007, Honolulu, Hawaii.
SJ Young. 2002. Talking to Machines (Statistically
Speaking). In Int Conf Spoken Language Processing,
Denver, Colorado.
B Zhang, Q Cai, J Mao, E Chang, and B Guo. 2001.
Spoken Dialogue Management as Planning and Acting
under Uncertainty. In Eurospeech, Aalborg, Denmark.
119
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 272?275,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
k-Nearest Neighbor Monte-Carlo Control Algorithm
for POMDP-based Dialogue Systems
F. Lefe`vre?, M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
Spoken Dialogue Systems Group
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{frfl2, mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
In real-world applications, modelling di-
alogue as a POMDP requires the use of
a summary space for the dialogue state
representation to ensure tractability. Sub-
optimal estimation of the value func-
tion governing the selection of system re-
sponses can then be obtained using a grid-
based approach on the belief space. In
this work, the Monte-Carlo control tech-
nique is extended so as to reduce training
over-fitting and to improve robustness to
semantic noise in the user input. This tech-
nique uses a database of belief vector pro-
totypes to choose the optimal system ac-
tion. A locally weighted k-nearest neigh-
bor scheme is introduced to smooth the de-
cision process by interpolating the value
function, resulting in higher user simula-
tion performance.
1 Introduction
In the last decade dialogue modelling as a Partially
Observable Markov Decision Process (POMDP)
has been proposed as a convenient way to improve
spoken dialogue systems (SDS) trainability, nat-
uralness and robustness to input errors (Young et
al., 2009). The POMDP framework models dia-
logue flow as a sequence of unobserved dialogue
states following stochastic moves, and provides a
principled way to model uncertainty.
However, to deal with uncertainty, POMDPs
maintain distributions over all possible states. But
then training an optimal policy is an NP hard
problem and thus not tractable for any non-trivial
application. In recent works this issue is ad-
dressed by mapping the dialog state representation
?Fabrice Lefe`vre is currently on leave from the Univer-
sity of Avignon, France.
space (the master space) into a smaller summary
space (Williams and Young, 2007). Even though
optimal policies remain out of reach, sub-optimal
solutions can be found by means of grid-based al-
gorithms.
Within the Hidden Information State (HIS)
framework (Young et al, 2009), policies are rep-
resented by a set of grid points in the summary be-
lief space. Beliefs in master space are first mapped
into summary space and then mapped into a sum-
mary action via the dialogue policy. The resulting
summary action is then mapped back into master
space and output to the user.
Methods which support interpolation between
points are generally required to scale well to large
state spaces (Pineau et al, 2003). In the current
version of the HIS framework, the policy chooses
the system action by associating each new belief
point with the single, closest, grid point. In the
present work, a k-nearest neighbour extension is
evaluated in which the policy decision is based on
a locally weighted regression over a subset of rep-
resentative grid points. This method thus lies be-
tween a strictly grid-based and a point-based value
iteration approach as it interpolates the value func-
tion around the queried belief point. It thus re-
duces the policy?s dependency on the belief grid
point selection and increases robustness to input
noise.
The next section gives an overview of the
CUED HIS POMDP dialogue system which we
extended for our experiments. In Section 3, the
grid-based approach to policy optimisation is in-
troduced followed by a presentation of the k-
nn Monte-Carlo policy optimization in Section 4,
along with an evaluation on a simulated user.
272
2 The CUED Spoken Dialogue System
2.1 System Architecture
The CUED HIS-based dialogue system pipelines
five modules: the ATK speech recogniser, an
SVM-based semantic tuple classifier, a POMDP
dialogue manager, a natural language generator,
and an HMM-based speech synthesiser. During
an interaction with the system, the user?s speech
is first decoded by the recogniser and an N-best
list of hypotheses is sent to the semantic classifier.
In turn the semantic classifier outputs an N-best
list of user dialogue acts. A dialogue act is a se-
mantic representation of the user action headed by
the user intention (such as inform, request,
etc) followed by a list of items (slot-value pairs
such as type=hotel, area=east etc). The
N-best list of dialogue acts is used by the dialogue
manager to update the dialogue state. Based on
the state hypotheses and the policy, a machine ac-
tion is determined, again in the form of a dialogue
act. The natural language generator translates the
machine action into a sentence, finally converted
into speech by the HMM synthesiser. The dia-
logue system is currently developed for a tourist
information domain (Towninfo). It is worth not-
ing that the dialogue manager does not contain any
domain-specific knowledge.
2.2 HIS Dialogue Manager
The unobserved dialogue state of the HIS dialogue
manager consists of the user goal, the dialogue his-
tory and the user action. The user goal is repre-
sented by a partition which is a tree structure built
according to the domain ontology. The nodes in
the partition consist mainly of slots and values.
When querying the venue database using the par-
tition, a set of matching entities can be produced.
The dialogue history consists of the grounding
states of the nodes in the partition, generated us-
ing a finite state automaton and the previous user
and system action. A hypothesis in the HIS ap-
proach is then a triple combining a partition, a user
action and the respective set of grounding states.
The distribution over all hypotheses is maintained
throughout the dialogue (belief state monitoring).
Considering the ontology size for any real-world
problem, the so-defined state space is too large for
any POMDP learning algorithm. Hence to obtain a
tractable policy, the state/action space needs to be
reduced to a smaller scale summary space. The set
of possible machine dialogue acts is also reduced
in summary space. This is mainly achieved by re-
Master Space
Masters Sppp c  uamyppp
eus Sers Sppp us SamypppMMM
Mast er S pr
cr S pr uSr pSm Mayt
Summary Space
Master Spcu rmymty
c
um
a
ycr
y rcsProceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552?1561,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Phrase-based Statistical Language Generation using
Graphical Models and Active Learning
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek,
Simon Keizer, Blaise Thomson, Kai Yu and Steve Young?
Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK
{f.mairesse, mg436, fj228, sk561, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
Most previous work on trainable language
generation has focused on two paradigms:
(a) using a statistical model to rank a
set of generated utterances, or (b) using
statistics to inform the generation deci-
sion process. Both approaches rely on
the existence of a handcrafted generator,
which limits their scalability to new do-
mains. This paper presents BAGEL, a sta-
tistical language generator which uses dy-
namic Bayesian networks to learn from
semantically-aligned data produced by 42
untrained annotators. A human evalua-
tion shows that BAGEL can generate nat-
ural and informative utterances from un-
seen inputs in the information presentation
domain. Additionally, generation perfor-
mance on sparse datasets is improved sig-
nificantly by using certainty-based active
learning, yielding ratings close to the hu-
man gold standard with a fraction of the
data.
1 Introduction
The field of natural language generation (NLG) is
one of the last areas of computational linguistics to
embrace statistical methods. Over the past decade,
statistical NLG has followed two lines of research.
The first one, pioneered by Langkilde and Knight
(1998), introduces statistics in the generation pro-
cess by training a model which reranks candi-
date outputs of a handcrafted generator. While
their HALOGEN system uses an n-gram language
model trained on news articles, other systems have
used hierarchical syntactic models (Bangalore and
Rambow, 2000), models trained on user ratings of
?This research was partly funded by the UK EPSRC un-
der grant agreement EP/F013930/1 and funded by the EU
FP7 Programme under grant agreement 216594 (CLASSiC
project: www.classic-project.org).
utterance quality (Walker et al, 2002), or align-
ment models trained on speaker-specific corpora
(Isard et al, 2006).
A second line of research has focused on intro-
ducing statistics at the generation decision level,
by training models that find the set of genera-
tion parameters maximising an objective function,
e.g. producing a target linguistic style (Paiva and
Evans, 2005; Mairesse and Walker, 2008), gener-
ating the most likely context-free derivations given
a corpus (Belz, 2008), or maximising the expected
reward using reinforcement learning (Rieser and
Lemon, 2009). While such methods do not suffer
from the computational cost of an overgeneration
phase, they still require a handcrafted generator to
define the generation decision space within which
statistics can be used to find an optimal solution.
This paper presents BAGEL (Bayesian networks
for generation using active learning), an NLG sys-
tem that can be fully trained from aligned data.
While the main requirement of the generator is to
produce natural utterances within a dialogue sys-
tem domain, a second objective is to minimise the
overall development effort. In this regard, a major
advantage of data-driven methods is the shift of
the effort from model design and implementation
to data annotation. In the case of NLG systems,
learning to produce paraphrases can be facilitated
by collecting data from a large sample of annota-
tors. Our meaning representation should therefore
(a) be intuitive enough to be understood by un-
trained annotators, and (b) provide useful gener-
alisation properties for generating unseen inputs.
Section 2 describes BAGEL?s meaning represen-
tation, which satisfies both requirements. Sec-
tion 3 then details how our meaning representation
is mapped to a phrase sequence, using a dynamic
Bayesian network with backoff smoothing.
Within a given domain, the same semantic
concept can occur in different utterances. Sec-
tion 4 details how BAGEL exploits this redundancy
1552
to improve generation performance on sparse
datasets, by guiding the data collection process
using certainty-based active learning (Lewis and
Catlett, 1994). We train BAGEL in the informa-
tion presentation domain, from a corpus of utter-
ances produced by 42 untrained annotators (see
Section 5.1). An automated evaluation metric is
used to compare preliminary model and training
configurations in Section 5.2, while Section 5.3
shows that the resulting system produces natural
and informative utterances, according to 18 hu-
man judges. Finally, our human evaluation shows
that training using active learning significantly im-
proves generation performance on sparse datasets,
yielding results close to the human gold standard
using a fraction of the data.
2 Phrase-based generation from
semantic stacks
BAGEL uses a stack-based semantic representa-
tion to constrain the sequence of semantic con-
cepts to be searched. This representation can be
seen as a linearised semantic tree similar to the
one previously used for natural language under-
standing in the Hidden Vector State model (He
and Young, 2005). A stack representation provides
useful generalisation properties (see Section 3.1),
while the resulting stack sequences are relatively
easy to align (see Section 5.1). In the context of
dialogue systems, Table 1 illustrates how the input
dialogue act is first mapped to a set of stacks of
semantic concepts, and then aligned with a word
sequence. The bottom concept in the stack will
typically be a dialogue act type, e.g. an utterance
providing information about the object under dis-
cussion (inform) or specifying that the request
of the user cannot be met (reject). Other con-
cepts include attributes of that object (e.g., food,
area), values for those attributes (e.g., Chinese,
riverside), as well as special symbols for negat-
ing underlying concepts (e.g., not) or specifying
that they are irrelevant (e.g., dontcare).
The generator?s goal is thus finding the
most likely realisation given an unordered
set of mandatory semantic stacks Sm derived
from the input dialogue act. For example,
s =inform(area(centre)) is a mandatory stack
associated with the dialogue act in Table 1 (frame
8). While mandatory stacks must all be conveyed
in the output realisation, Sm does not contain the
optional intermediary stacks Si that can refer to
(a) general attributes of the object under discus-
sion (e.g., inform(area) in Table 1), or (b) to
concepts that are not in the input at all, which are
associated with the singleton stack inform (e.g.,
phrases expressing the dialogue act type, or clause
aggregation operations). For example, the stack
sequence in Table 1 contains 3 intermediary stacks
for t = 2, 5 and 7.
BAGEL?s granularity is defined by the semantic
annotation in the training data, rather than external
linguistic knowledge about what constitutes a unit
of meaning, i.e. contiguous words belonging to
the same semantic stack are modelled as an atomic
observation unit or phrase.1 In contrast with word-
level models, a major advantage of phrase-based
generation models is that they can model long-
range dependencies and domain-specific idiomatic
phrases with fewer parameters.
3 Dynamic Bayesian networks for NLG
Dynamic Bayesian networks have been used suc-
cessfully for speech recognition, natural language
understanding, dialogue management and text-to-
speech synthesis (Rabiner, 1989; He and Young,
2005; Lefe`vre, 2006; Thomson and Young, 2010;
Tokuda et al, 2000). Such models provide a
principled framework for predicting elements in a
large structured space, such as required for non-
trivial NLG tasks. Additionally, their probabilistic
nature makes them suitable for modelling linguis-
tic variation, i.e. there can be multiple valid para-
phrases for a given input.
BAGEL models the generation task as finding
the most likely sequence of realisation phrases
R? = (r1...rL) given an unordered set of manda-
tory semantic stacks Sm, with |Sm| ? L. BAGEL
must thus derive the optimal sequence of semantic
stacks S? that will appear in the utterance given
Sm, i.e. by inserting intermediary stacks if needed
and by performing content ordering. Any num-
ber of intermediary stacks can be inserted between
two consecutive mandatory stacks, as long as all
their concepts are included in either the previous
or following mandatory stack, and as long as each
stack transition leads to a different stack (see ex-
ample in Table 1). Let us define the set of possi-
ble stack sequences matching these constraints as
Seq(Sm) ? {S = (s1...sL) s.t. st ? Sm ? Si}.
We propose a model which estimates the dis-
1The term phrase is thus defined here as any sequence of
one or more words.
1553
Charlie Chan is a Chinese restaurant near Cineworld in the centre of town
Charlie Chan Chinese restaurant Cineworld centre
name food type near near area area
inform inform inform inform inform inform inform inform
t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8
Table 1: Example semantic stacks aligned with an utterance for the dialogue act
inform(name(Charlie Chan) type(restaurant) area(centre) food(Chinese) near(Cineworld)). Mandatory
stacks are in bold.
tribution P (R|Sm) from a training set of real-
isation phrases aligned with semantic stack se-
quences, by marginalising over all stack sequences
in Seq(Sm):
P (R|Sm) =
?
S?Seq(Sm)
P (R,S|Sm)
=
?
S?Seq(Sm)
P (R|S,Sm)P (S|Sm)
=
?
S?Seq(Sm)
P (R|S)P (S|Sm) (1)
Inference over the model defined in (1) requires
the decoding algorithm to consider all possible or-
derings over Seq(Sm) together with all possible
realisations, which is intractable for non-trivial do-
mains. We thus make the additional assumption
that the most likely sequence of semantic stacks
S? given Sm is the one yielding the optimal reali-
sation phrase sequence:
P (R|Sm) ? P (R|S
?)P (S?|Sm) (2)
with S? = argmax
S?Seq(Sm)
P (S|Sm) (3)
The semantic stacks are therefore decoded first
using the model in Fig. 1 to solve the argmax
in (3). The decoded stack sequence S? is then
treated as observed in the realisation phase, in
which the model in Fig. 2 is used to find the real-
isation phrase sequence R? maximising P (R|S?)
over all phrase sequences of length L = |S?| in
our vocabulary:
R? = argmax
R=(r1...rL)
P (R|S?)P (S?|Sm) (4)
= argmax
R=(r1...rL)
P (R|S?) (5)
In order to reduce model complexity, we fac-
torise our model by conditioning the realisation
phrase at time t on the previous phrase rt?1,
and the previous, current, and following semantic
stacks. The semantic stack st at time t is assumed
last mandatory 
stack
stack set 
validator
first frame
semantic 
stack s
stack set tracker
repeated frame final frame
Figure 1: Graphical model for the semantic decod-
ing phase. Plain arrows indicate smoothed proba-
bility distributions, dashed arrows indicate deter-
ministic relations, and shaded nodes are observed.
The generation of the end semantic stack symbol
deterministically triggers the final frame.
to depend only on the previous two stacks and the
last mandatory stack su ? Sm with 1 ? u < t:
P (S|Sm) =
?
?
?
?T
t=1 P (st|st?1, st?2, su)
if S ? Seq(Sm)
0 otherwise
(6)
P (R|S?) =
T?
t=1
P (rt|rt?1, s
?
t?1, s
?
t , s
?
t+1) (7)
While dynamic Bayesian networks typically
take sequential inputs, mapping a set of seman-
tic stacks to a sequence of phrases is achieved
by keeping track of the mandatory stacks that
were visited in the current sequence (see stack set
tracker variable in Fig. 1), and pruning any se-
quence that has not included all mandatory input
stacks on reaching the final frame (see observed
stack set validator variable in Fig. 1). Since the
number of intermediary stacks is not known at de-
coding time, the network is unrolled for a fixed
number of frames T defining the maximum num-
ber of phrases that can be generated (e.g., T =
50). The end of the stack sequence is then deter-
mined by a special end symbol, which can only
be emitted within the T frames once all mandatory
stacks have been visited. The probability of the re-
sulting utterance is thus computed over all frames
up to the end symbol, which determines the length
1554
L of S? and R?. While the decoding constraints
enforce that L > |Sm|, the search for S? requires
comparing sequences of different lengths. A con-
sequence is that shorter sequences containing only
mandatory stacks are likely to be favoured. While
future work should investigate length normalisa-
tion strategies, we find that the learned transition
probabilities are skewed enough to favour stack
sequences including intermediary stacks.
Once the topology and the decoding constraints
of the network have been defined, any inference al-
gorithm can be used to search for S? and R?. We
use the junction tree algorithm implemented in the
Graphical Model ToolKit (GMTK) for our exper-
iments (Bilmes and Zweig, 2002), however both
problems can be solved using a standard Viterbi
search given the appropriate state representation.
In terms of computational complexity, it is impor-
tant to note that the number of stack sequences
Seq(Sm) to search over increases exponentially
with the number of input mandatory stacks. Nev-
ertheless, we find that real-time performance can
be achieved by pruning low probability sequences,
without affecting the quality of the solution.
3.1 Generalisation to unseen semantic stacks
In order to generalise to semantic stacks which
have not been observed during training, the re-
alisation phrase r is made dependent on under-
specified stack configurations, i.e. the tail l
and the head h of the stack. For example, the
last stack in Table 1 is associated with the head
centre and the tail inform(area). As a re-
sult, BAGEL assigns non-zero probabilities to re-
alisation phrases in unseen semantic contexts, by
backing off to the head and the tail of the stack.
A consequence is that BAGEL?s lexical realisa-
tion can generalise across contexts. For exam-
ple, if reject(area(centre)) was never ob-
served at training time, P (r = centre of town|s =
reject(area(centre))) will be estimated by
backing off to P (r = centre of town|h =
centre). BAGEL can thus generate ?there are
no venues in the centre of town? if the phrase
?centre of town? was associated with the con-
cept centre in a different context, such as
inform(area(centre)). The final realisation
model is illustrated in Fig. 2:
realisation phrase r
repeated frame final framefirst frame
stack head h
semantic 
stack s
stack tail l
Figure 2: Graphical model for the realisation
phase. Dashed arrows indicate deterministic re-
lations, and shaded node are observed.
!"#$%&& '(")*+
11111 ,,,,,,,| +?+?? ttttttttt sssllrlhr
ttttttt sllrlhr ,,,,,| 111 +??
111 ,,,,| +?? tttttt llrlhr
ttt lhr ,|21,| ?? ttt sss
uttt ssss ,,| 21 ??
tt hr |1| ?tt ss
trts
Figure 3: Backoff graphs for the semantic decod-
ing and realisation models.
P (R|S?) =
L?
t=1
P (rt|rt?1, ht, lt?1, lt, lt+1,
s?t?1, s
?
t , s
?
t+1) (8)
Conditional probability distributions are repre-
sented as factored language models smoothed us-
ing Witten-Bell interpolated backoff smoothing
(Bilmes and Kirchhoff, 2003), according to the
backoff graphs in Fig. 3. Variables which are the
furthest away in time are dropped first, and par-
tial stack variables are dropped last as they are ob-
served the most.
It is important to note that generating unseen se-
mantic stacks requires all possible mandatory se-
mantic stacks in the target domain to be prede-
fined, in order for all stack unigrams to be assigned
a smoothed non-zero probability.
3.2 High cardinality concept abstraction
While one should expect a trainable generator
to learn multiple lexical realisations for low-
cardinality semantic concepts, learning lexical
realisations for high-cardinality database entries
(e.g., proper names) would increase the number of
model parameters prohibitively. We thus divide
pre-terminal concepts in the semantic stacks into
two types: (a) enumerable attributes whose val-
ues are associated with distinct semantic stacks in
1555
our model (e.g., inform(pricerange(cheap))),
and (b) non-enumerable attributes whose values
are replaced by a generic symbol before train-
ing in both the utterance and the semantic stack
(e.g., inform(name(X)). These symbolic values
are then replaced in the surface realisation by the
corresponding value in the input specification. A
consequence is that our model can only learn syn-
onymous lexical realisations for enumerable at-
tributes.
4 Certainty-based active learning
A major issue with trainable NLG systems is the
lack of availability of domain-specific data. It is
therefore essential to produce NLG models that
minimise the data annotation cost.
BAGEL supports the optimisation of the data
collection process through active learning, in
which the next semantic input to annotate is de-
termined by the current model. The probabilis-
tic nature of BAGEL allows the use of certainty-
based active learning (Lewis and Catlett, 1994),
by querying the k semantic inputs for which the
model is the least certain about its output real-
isation. Given a finite semantic input space I
representing all possible dialogue acts in our do-
main (i.e., the set of all sets of mandatory seman-
tic stacks Sm), BAGEL?s active learning training
process iterates over the following steps:
1. Generate an utterance for each semantic input Sm ? I
using the current model.2
2. Annotate the k semantic inputs {S1m...S
k
m} yielding
the lowest realisation probability, i.e. for q ? (1..k)
Sqm = argmin
Sm?I\{S1m...S
q?1
m }
(max
R
P (R|Sm)) (9)
with P (R|Sm) defined in (2).
3. Retrain the model with the additional k data points.
The number of utterances to be queried k should
depend on the flexibility of the annotators and the
time required for generating all possible utterances
in the domain.
5 Experimental method
BAGEL?s factored language models are trained us-
ing the SRILM toolkit (Stolcke, 2002), and de-
coding is performed using GMTK?s junction tree
inference algorithm (Bilmes and Zweig, 2002).
2Sampling methods can be used if I is infinite or too
large.
Since each active learning iteration requires gen-
erating all training utterances in our domain, they
are generated using a larger clique pruning thresh-
old than the test utterances used for evaluation.
5.1 Corpus collection
We train BAGEL in the context of a dialogue
system providing information about restaurants
in Cambridge. The domain contains two dia-
logue act types: (a) inform: presenting infor-
mation about a restaurant (see Table 1), and (b)
reject: informing that the user?s constraints can-
not be met (e.g., ?There is no cheap restaurant
in the centre?). Our domain contains 8 restau-
rant attributes: name, food, near, pricerange,
postcode, phone, address, and area, out of
which food, pricerange, and area are treated
as enumerable.3 Our input semantic space is ap-
proximated by the set of information presentation
dialogue acts produced over 20,000 simulated di-
alogues between our statistical dialogue manager
(Young et al, 2010) and an agenda-based user
simulator (Schatzmann et al, 2007), which results
in 202 unique dialogue acts after replacing non-
enumerable values by a generic symbol. Each di-
alogue act contains an average of 4.48 mandatory
semantic stacks.
As one of our objectives is to test whether
BAGEL can learn from data provided by a large
sample of untrained annotators, we collected a
corpus of semantically-aligned utterances using
Amazon?s Mechanical Turk data collection ser-
vice. A crucial aspect of data collection for
NLG is to ensure that the annotators under-
stand the meaning of the semantics to be con-
veyed. Annotators were first asked to provide
an utterance matching an abstract description
of the dialogue act, regardless of the order in
which the constraints are presented (e.g., Offer
the venue Taj Mahal and provide the information
type(restaurant), area(riverside), food(Indian),
near(The Red Lion)). The order of the constraints
in the description was randomised to reduce the
effect of priming. The annotators were then asked
to align the attributes (e.g., Indicate the region of
the utterance related to the concept ?area?), and
the attribute values (e.g., Indicate only the words
related to the concept ?riverside?). Two para-
phrases were collected for each dialogue act in
our domain, resulting in a total of 404 aligned ut-
3With the exception of areas defined as proper nouns.
1556
rt st ht lt
<s> START START START
The Rice Boat inform(name(X)) X inform(name)
is a inform inform EMPTY
restaurant inform(type(restaurant)) restaurant inform(type)
in the inform(area) area inform
riverside inform(area(riverside)) riverside inform(area)
area inform(area) area inform
that inform inform EMPTY
serves inform(food) food inform
French inform(food(French)) French inform(food)
food inform(food) food inform
</s> END END END
Table 2: Example utterance annotation used to estimate the conditional probability distributions of the
models in Figs. 1 and 2 ( rt=realisation phrase, st=semantic stack, ht=stack head, lt=stack tail).
terances produced by 42 native speakers of En-
glish. After manually checking and normalising
the dataset,4 the layered annotations were auto-
matically mapped to phrase-level semantic stacks
by splitting the utterance into phrases at annotation
boundaries. Each annotated utterance is then con-
verted into a sequence of symbols such as in Ta-
ble 2, which are used to estimate the conditional
probability distributions defined in (6) and (8).
The resulting vocabulary consists of 52 distinct se-
mantic stacks and 109 distinct realisation phrases,
with an average of 8.35 phrases per utterance.
5.2 BLEU score evaluation
We first evaluate BAGEL using the BLEU auto-
mated metric (Papineni et al, 2002), which mea-
sures the word n-gram overlap between the gen-
erated utterances and the 2 reference paraphrases
over a test corpus (with n up to 4). While BLEU
suffers from known issues such as a bias towards
statistical NLG systems (Reiter and Belz, 2009), it
provides useful information when comparing sim-
ilar systems. We evaluate BAGEL for different
training set sizes, model dependencies, and active
learning parameters. Our results are averaged over
a 10-fold cross-validation over distinct dialogue
acts, i.e. dialogue acts used for testing are not seen
at training time,5 and all systems are tested on the
same folds. The training and test sets respectively
contain an average of 181 and 21 distinct dialogue
acts, and each dialogue act is associated with two
paraphrases, resulting in 362 training utterances.
4The normalisation process took around 4 person-hour for
404 utterances.
5We do not evaluate performance on dialogue acts used
for training, as the training examples can trivially be used as
generation templates.
!"#$
!"%
!"%$
!"#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!"$
!"$$
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'(()*+,-(
!".
!".$!"
#
$
%
&
'
(
)
%
*
+
,
-
"
/+)01234)5234+66/+)01234)5234+667)8+)6'1'9-)0-*281:30!";$ <! =! .! #! >! <!! <=! <$! =!! =$! ;!! ;#=
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
Figure 4: BLEU score averaged over a 10-fold
cross-validation for different training set sizes and
network topologies, using random sampling.
Results: Fig. 4 shows that adding a dependency
on the future semantic stack improves perfor-
mances for all training set sizes, despite the added
model complexity. Backing off to partial stacks
also improves performance, but only for sparse
training sets.
Fig. 5 compares the full model trained using
random sampling in Fig. 4 with the same model
trained using certainty-based active learning, for
different values of k. As our dataset only con-
tains two paraphrases per dialogue act, the same
dialogue act can only be queried twice during the
active learning procedure. A consequence is that
the training set used for active learning converges
towards the randomly sampled set as its size in-
creases. Results show that increasing the train-
ing set one utterance at a time using active learn-
ing (k = 1) significantly outperforms random
sampling when using 40, 80, and 100 utterances
(p < .05, two-tailed). Increasing the number of
utterances to be queried at each iteration to k = 10
results in a smaller performance increase. A possi-
1557
!"#
!"##
!"$
!"$#
!"%
!"%#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'()*+,-'+./0(1
!"2#
!"3
!"3#
4! 5! 3! $! 6! 4!! 45! 4#! 5!! 5#! 2!! 2$5
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
7890:;,/;'<(0(1,=>47890:;,/;'<(0(1,=>4!
Figure 5: BLEU score averaged over a 10-fold
cross-validation for different numbers of queries
per iteration, using the full model with the query
selection criterion (9).
!"#
!"##
!"$
!"$#
!"%
!"%#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'(()(*+,-*.
!"/#
!"0
!"0#
1! 2! 0! $! 3! 1!! 12! 1#! 2!! 2#! /!! /$2
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
4*+,-*.),5-)6-7854*9+5:;)<9,';)6<-:;
Figure 6: BLEU score averaged over a 10-fold
cross-validation for different query selection cri-
teria, using the full model with k = 1.
ble explanation is that the model is likely to assign
low probabilities to similar inputs, thus any value
above k = 1 might result in redundant queries
within an iteration.
As the length of the semantic stack sequence
is not known before decoding, the active learn-
ing selection criterion presented in (9) is biased
towards longer utterances, which tend to have a
lower probability. However, Fig. 6 shows that
normalising the log probability by the number of
semantic stacks does not improve overall learn-
ing performance. Although a possible explanation
is that longer inputs tend to contain more infor-
mation to learn from, Fig. 6 shows that a base-
line selecting the largest remaining semantic input
at each iteration performs worse than the active
learning scheme for training sets above 20 utter-
ances. The full log probability selection criterion
defined in (9) is therefore used throughout the rest
of the paper (with k = 1).
5.3 Human evaluation
While automated metrics provide useful informa-
tion for comparing different systems, human feed-
back is needed to assess (a) the quality of BAGEL?s
outputs, and (b) whether training models using ac-
tive learning has a significant impact on user per-
ceptions. We evaluate BAGEL through a large-
scale subjective rating experiment using Amazon?s
Mechanical Turk service.
For each dialogue act in our domain, partici-
pants are presented with a ?gold standard? human
utterance from our dataset, which they must com-
pare with utterances generated by models trained
with and without active learning on a set of 20, 40,
100, and 362 utterances (full training set), as well
as with the second human utterance in our dataset.
See example utterances in Table 3. The judges are
then asked to evaluate the informativeness and nat-
uralness of each of the 8 utterances on a 5 point
likert-scale. Naturalness is defined as whether the
utterance could have been produced by a human,
and informativeness is defined as whether it con-
tains all the information in the gold standard utter-
ance. Each utterance is taken from the test folds of
the cross-validation experiment presented in Sec-
tion 5.2, i.e. the models are trained on up to 90%
of the data and the training set does not contain the
dialogue act being tested.
Results: Figs. 7 and 8 compare the naturalness
and informativeness scores of each system aver-
aged over all 202 dialogue acts. A paired t-test
shows that models trained on 40 utterances or
less produce utterances that are rated significantly
lower than human utterances for both naturalness
and informativeness (p < .05, two-tailed). How-
ever, models trained on 100 utterances or more do
not perform significantly worse than human utter-
ances for both dimensions, with a mean difference
below .10 over 202 comparisons. Given the large
sample size, this result suggests that BAGEL can
successfully learn our domain using a fraction of
our initial dataset.
As far as the learning method is concerned, a
paired t-test shows that models trained on 20 and
40 utterances using active learning significantly
outperform models trained using random sam-
pling, for both dimensions (p < .05). The largest
increase is observed using 20 utterances, i.e. the
naturalness increases by .49 and the informative-
ness by .37. When training on 100 utterances, the
effect of active learning becomes insignificant. In-
1558
Input inform(name(the Fountain) near(the Arts Picture House) area(centre) pricerange(cheap))
Human There is an inexpensive restaurant called the Fountain in the centre of town near the Arts Picture House
Rand-20 The Fountain is a restaurant near the Arts Picture House located in the city centre cheap price range
Rand-40 The Fountain is a restaurant in the cheap city centre area near the Arts Picture House
AL-20 The Fountain is a restaurant near the Arts Picture House in the city centre cheap
AL-40 The Fountain is an affordable restaurant near the Arts Picture House in the city centre
Full set The Fountain is a cheap restaurant in the city centre near the Arts Picture House
Input reject(area(Barnwell) near(Saint Mary?s Church))
Human I am sorry but I know of no venues near Saint Mary?s Church in the Barnwell area
Full set I am sorry but there are no venues near Saint Mary?s Church in the Barnwell area
Input inform(name(the Swan)area(Castle Hill) pricerange(expensive))
Human The Swan is a restaurant in Castle Hill if you are seeking something expensive
Full set The Swan is an expensive restaurant in the Castle Hill area
Input inform(name(Browns) area(centre) near(the Crowne Plaza) near(El Shaddai) pricerange(cheap))
Human Browns is an affordable restaurant located near the Crowne Plaza and El Shaddai in the centre of the city
Full set Browns is a cheap restaurant in the city centre near the Crowne Plaza and El Shaddai
Table 3: Example utterances for different input dialogue acts and system configurations. AL-20 = active
learning with 20 utterances, Rand = random sampling.
!"## !"$%
!"&'!"(% !")* *"%% *"%#
*"%'
++"$
!!"$
**"$
$
!
"
#
$
%
$
#
&
'
(
#
)
$
"
*
*
%
*
+
,
(
"
,-./01
##"$ +% *% #%% !(+
!
"
#
$
%
$
#
&
'
(
#
)
$
"
*
*
%
*
+
,
(
"
-(#.$.$/%*"&%*.0"
234567897-:.5.;<=1-.8=447:-.378>8*"%'
Figure 7: Naturalness mean opinion scores for dif-
ferent training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p < .05).
terestingly, while models trained on 100 utterances
outperform models trained on 40 utterances using
random sampling (p < .05), they do not signifi-
cantly outperform models trained on 40 utterances
using active learning (p = .15 for naturalness and
p = .41 for informativeness). These results sug-
gest that certainty-based active learning is benefi-
cial for training a generator from a limited amount
of data given the domain size.
Looking back at the results presented in Sec-
tion 5.2, we find that the BLEU score correlates
with a Pearson correlation coefficient of .42 with
the mean naturalness score and .35 with the mean
informativeness score, over all folds of all systems
tested (n = 70, p < .01). This is lower than
previous correlations reported by Reiter and Belz
(2009) in the shipping forecast domain with non-
expert judges (r = .80), possibly because our do-
main is larger and more open to subjectivity.
!"## !"$$
#"%&!"'& !"()
#"%$ #"%#
#"&!
**"+
!!"+
##"+
+
!
"
#
$
%
&
$
'
(
)
*
#
+
&
,
"
$
"
-
-
%
-
.
(
)
"
,-./01
&&"+ *% #% &%% !)*
!
"
#
$
%
&
$
'
(
)
*
#
+
&
,
"
$
"
-
-
%
-
.
(
)
"
/)#&$&$0%-"+%-&1"
234567897-:.5.;<=1-.8=447:-.378>8#"&!
Figure 8: Informativeness mean opinion scores for
different training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p < .05).
6 Related work
While most previous work on trainable NLG re-
lies on a handcrafted component (see Section 1),
recent research has started exploring fully data-
driven NLG models.
Factored language models have recently been
used for surface realisation within the OpenCCG
framework (White et al, 2007; Espinosa et al,
2008). More generally, chart generators for
different grammatical formalisms have been
trained from syntactic treebanks (White et al,
2007; Nakanishi et al, 2005), as well as from
semantically-annotated treebanks (Varges and
Mellish, 2001). However, a major difference with
our approach is that BAGEL uses domain-specific
data to generate a surface form directly from se-
mantic concepts, without any syntactic annotation
(see Section 7 for further discussion).
1559
This work is strongly related to Wong and
Mooney?s WASP?1 generation system (2007),
which combines a language model with an in-
verted synchronous CFG parsing model, effec-
tively casting the generation task as a translation
problem from a meaning representation to natu-
ral language. WASP?1 relies on GIZA++ to align
utterances with derivations of the meaning repre-
sentation (Och and Ney, 2003). Although early
experiments showed that GIZA++ did not perform
well on our data?possibly because of the coarse
granularity of our semantic representation?future
work should evaluate the generalisation perfor-
mance of synchronous CFGs in a dialogue system
domain.
Although we do not know of any work on ac-
tive learning for NLG, previous work has used
active learning for semantic parsing and informa-
tion extraction (Thompson et al, 1999; Tang et al,
2002), spoken language understanding (Tur et al,
2003), speech recognition (Hakkani-Tu?r et al,
2002), word alignment (Sassano, 2002), and more
recently for statistical machine translation (Blood-
good and Callison-Burch, 2010). While certainty-
based methods have been widely used, future work
should investigate the performance of committee-
based active learning for NLG, in which examples
are selected based on the level of disagreement be-
tween models trained on subsets of the data (Fre-
und et al, 1997).
7 Discussion and conclusion
This paper presents and evaluates BAGEL, a sta-
tistical language generator that can be trained en-
tirely from data, with no handcrafting required be-
yond the semantic annotation. All the required
subtasks?i.e. content ordering, aggregation, lex-
ical selection and realisation?are learned from
data using a unified model. To train BAGEL in a di-
alogue system domain, we propose a stack-based
semantic representation at the phrase level, which
is expressive enough to generate natural utterances
from unseen inputs, yet simple enough for data to
be collected from 42 untrained annotators with a
minimal normalisation step. A human evaluation
over 202 dialogue acts does not show any differ-
ence in naturalness and informativeness between
BAGEL?s outputs and human utterances. Addition-
ally, we find that the data collection process can
be optimised using active learning, resulting in a
significant increase in performance when training
data is limited, according to ratings from 18 hu-
man judges.6 These results suggest that the pro-
posed framework can largely reduce the develop-
ment time of NLG systems.
While this paper only evaluates the most likely
realisation given a dialogue act, we believe that
BAGEL?s probabilistic nature and generalisation
capabilities are well suited to model the linguis-
tic variation resulting from the diversity of annota-
tors. Our first objective is thus to evaluate the qual-
ity of BAGEL?s n-best outputs, and test whether
sampling from the output distribution can improve
naturalness and user satisfaction within a dialogue.
Our results suggest that explicitly modelling
syntax is not necessary for our domain, possi-
bly because of the lack of syntactic complexity
compared with formal written language. Never-
theless, future work should investigate whether
syntactic information can improve performance in
more complex domains. For example, the reali-
sation phrase can easily be conditioned on syntac-
tic constructs governing that phrase, and the recur-
sive nature of syntax can be modelled by keeping
track of the depth of the current embedded clause.
While syntactic information can be included with
no human effort by using syntactic parsers, their
robustness to dialogue system utterances must first
be evaluated.
Finally, recent years have seen HMM-based
synthesis models become competitive with unit se-
lection methods (Tokuda et al, 2000). Our long
term objective is to take advantage of those ad-
vances to jointly optimise the language genera-
tion and the speech synthesis process, by combin-
ing both components into a unified probabilistic
concept-to-speech generation model.
References
S. Bangalore and O. Rambow. Exploiting a probabilistic hi-
erarchical model for generation. In Proceedings of the
18th International Conference on Computational Linguis-
tics (COLING), pages 42?48, 2000.
A. Belz. Automatic generation of weather forecast texts us-
ing comprehensive probabilistic generation-space models.
Natural Language Engineering, 14(4):431?455, 2008.
J. Bilmes and K. Kirchhoff. Factored language models and
generalized parallel backoff. In Proceedings of HLT-
NAACL, short papers, 2003.
J. Bilmes and G. Zweig. The Graphical Models ToolKit: An
open source software system for speech and time-series
processing. In Proceedings of ICASSP, 2002.
6The full training corpus and the generated
utterances used for evaluation are available at
http://mi.eng.cam.ac.uk/?farm2/bagel.
1560
M. Bloodgood and C. Callison-Burch. Bucking the trend:
Large-scale cost-focused active learning for statistical ma-
chine translation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics
(ACL), 2010.
D. Espinosa, M. White, and D. Mehay. Hypertagging: Su-
pertagging for surface realization with CCG. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), 2008.
Y. Freund, H. S. Seung, E.Shamir, and N. Tishby. Selective
sampling using the query by committee algorithm. Ma-
chine Learning, 28:133?168, 1997.
D. Hakkani-Tu?r, G. Riccardi, and A. Gorin. Active learn-
ing for automatic speech recognition. In Proceedings of
ICASSP, 2002.
Y. He and S. Young. Semantic processing using the Hidden
Vector State model. Computer Speech & Language, 19
(1):85?106, 2005.
A. Isard, C. Brockmann, and J. Oberlander. Individuality and
alignment in generated dialogues. In Proceedings of the
4th International Natural Language Generation Confer-
ence (INLG), pages 22?29, 2006.
I. Langkilde and K. Knight. Generation that exploits corpus-
based statistical knowledge. In Proceedings of the 36th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 704?710, 1998.
F. Lefe`vre. A DBN-based multi-level stochastic spoken lan-
guage understanding system. In Proceedings of the IEEE
Workshop on Spoken Language Technology (SLT), 2006.
D. D. Lewis and J. Catlett. Heterogeneous uncertainty am-
pling for supervised learning. In Proceedings of ICML,
1994.
F. Mairesse and M. A. Walker. Trainable generation of Big-
Five personality styles through data-driven parameter esti-
mation. In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL), 2008.
H. Nakanishi, Y. Miyao, , and J. Tsujii. Probabilistic methods
for disambiguation of an HPSG-based chart generator. In
Proceedings of the IWPT, 2005.
F. J. Och and H. Ney. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?51, 2003.
D. S. Paiva and R. Evans. Empirically-based control of nat-
ural language generation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 58?65, 2005.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
L. R. Rabiner. Tutorial on Hidden Markov Models and se-
lected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?285, 1989.
E. Reiter and A. Belz. An investigation into the validity
of some metrics for automatically evaluating natural lan-
guage generation systems. Computational Linguistics, 25:
529?558, 2009.
V. Rieser and O. Lemon. Natural language generation as
planning under uncertainty for spoken dialogue systems.
In Proceedings of the Annual Meeting of the European
Chapter of the ACL (EACL), 2009.
M. Sassano. An empirical study of active learning with sup-
port vector machines for japanese word segmentation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. Agenda-based user simulation for bootstrap-
ping a POMDP dialogue system. In Proceedings of HLT-
NAACL, short papers, pages 149?152, 2007.
A. Stolcke. SRILM ? an extensible language modeling
toolkit. In Proceedings of the International Conference
on Spoken Language Processing, 2002.
M. Tang, X. Luo, and S. Roukos. Active learning for statis-
tical natural language parsing. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), 2002.
C. Thompson, M. E. Califf, and R. J. Mooney. Active learn-
ing for natural language parsing and information extrac-
tion. In Proceedings of ICML, 1999.
B. Thomson and S. Young. Bayesian update of dialogue state:
A POMDP framework for spoken dialogue systems. Com-
puter Speech & Language, 24(4):562?588, 2010.
Y. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and
T. Kitamura. Speech parameter generation algorithms for
HMM-based speech synthesis. In Proceedings of ICASSP,
2000.
G. Tur, R. E. Schapire, and D. Hakkani-Tu?r. Active learn-
ing for spoken language understanding. In Proceedings of
ICASSP, 2003.
S. Varges and C. Mellish. Instance-based natural language
generation. In Proceedings of the Annual Meeting of the
North American Chapter of the ACL (NAACL), 2001.
M. A. Walker, O. Rambow, and M. Rogati. Training a sen-
tence planner for spoken dialogue using boosting. Com-
puter Speech and Language, 16(3-4), 2002.
M. White, R. Rajkumar, and S. Martin. Towards broad cov-
erage surface realization with CCG. In Proceedings of the
Workshop on Using Corpora for NLG: Language Genera-
tion and Machine Translation, 2007.
Y. W. Wong and R. Mooney. Generation by inverting a se-
mantic parser that uses statistical machine translation. In
Proceedings of HLT-NAACL, 2007.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and K. Yu. The Hidden Information State
model: a practical framework for POMDP-based spoken
dialogue management. Computer Speech and Language,
24(2):150?174, 2010.
1561
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 116?123,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Parameter estimation for agenda-based user simulation
Simon Keizer, Milica Gas?ic?, Filip Jurc???c?ek, Franc?ois Mairesse,
Blaise Thomson, Kai Yu, and Steve Young ?
University of Cambridge, Department of Engineering, Cambridge (UK)
{sk561,mg436,fj228,farm2,brmt2,ky219,sjy}@cam.ac.uk
Abstract
This paper presents an agenda-based user
simulator which has been extended to be
trainable on real data with the aim of more
closely modelling the complex rational be-
haviour exhibited by real users. The train-
able part is formed by a set of random de-
cision points that may be encountered dur-
ing the process of receiving a system act
and responding with a user act. A sample-
based method is presented for using real
user data to estimate the parameters that
control these decisions. Evaluation results
are given both in terms of statistics of gen-
erated user behaviour and the quality of
policies trained with different simulators.
Compared to a handcrafted simulator, the
trained system provides a much better fit
to corpus data and evaluations suggest that
this better fit should result in improved di-
alogue performance.
1 Introduction
In spoken dialogue systems research, modelling
dialogue as a (Partially Observable) Markov Deci-
sion Process ((PO)MDP) and using reinforcement
learning techniques for optimising dialogue poli-
cies has proven to be an effective method for de-
veloping robust systems (Singh et al, 2000; Levin
et al, 2000). However, since this kind of optimi-
sation requires a simulated user to generate a suffi-
ciently large number of interactions to learn from,
this effectiveness depends largely on the quality
of such a user simulator. An important require-
ment for a simulator is for it to be realistic, i.e., it
should generate behaviour that is similar to that of
real users. Trained policies are then more likely to
perform better on real users, and evaluation results
on simulated data are more likely to predict results
on real data more accurately.
?This research was partly funded by the UK EPSRC un-
der grant agreement EP/F013930/1 and by the EU FP7 Pro-
gramme under grant agreement 216594 (CLASSiC project:
www.classic-project.org).
This is one of the reasons why learning user
simulation models from data on real user be-
haviour has become an important direction of re-
search (Scheffler and Young, 2001; Cuaya?huitl et
al., 2005; Georgila et al, 2006). However, the data
driven user models developed so far lack the com-
plexity required for training high quality policies
in task domains where user behaviour is relatively
complex. Handcrafted models are still the most
effective in those cases.
This paper presents an agenda-based user simu-
lator which is handcrafted for a large part, but ad-
ditionally can be trained with data from real users
(Section 2). As a result, it generates behaviour that
better reflects the statistics of real user behaviour,
whilst preserving the complexity and rationality
required to effectively train dialogue management
policies. The trainable part is formed by a set of
random decision points, which, depending on the
context, may or may not be encountered during
the process of receiving a system act and decid-
ing on a response act. If such a point is encoun-
tered, the simulator makes a random decision be-
tween a number of options which may directly or
indirectly influence the resulting output. The op-
tions for each random decision point are reason-
able in the context in which it is encountered, but
a uniform distribution of outcomes might not re-
flect real user behaviour.
We will describe a sample-based method for es-
timating the parameters that define the probabili-
ties for each possible decision, using data on real
users from a corpus of human-machine dialogues
(Section 3). Evaluation results will be presented
both in terms of statistics on generated user be-
haviour and the quality of dialogue policies trained
with different user simulations (Section 4).
2 Agenda-based user simulation
In agenda-based user simulation, user acts are gen-
erated on the basis of a user goal and an agenda
(Schatzmann et al, 2007a). The simulator pre-
sented here is developed and used for a tourist in-
116
formation application, but is sufficiently generic to
accommodate slot-filling applications in any do-
main.1 The user goal consists of the type of venue,
for example hotel, bar or restaurant, a list
of constraints in the form of slot value pairs, such
as food=Italian or area=east, and a list
of slots the user wants to know the value of, such
as the address (addr), phone number (phone),
or price information (price) of the venue. The
user goals for the simulator are randomly gener-
ated from the domain ontology describing which
combinations of venue types and constraints are
allowed and what are the possible values for each
slot. The agenda is a stack-like structure contain-
ing planned user acts. When the simulator receives
a system act, the status of the user goal is updated
as well as the agenda, typically by pushing new
acts onto it. In a separate step, the response user
act is selected by popping one or more items off
the agenda.
Although the agenda-based user simulator in-
troduced by Schatzmann et al (2007a) was en-
tirely handcrafted, it was realistic enough to suc-
cessfully test a prototype POMDP dialogue man-
ager and train a dialogue policy that outperformed
a handcrafted baseline (Young et al, 2009). A
method to train an agenda-based user simula-
tor from data was proposed by Schatzmann et
al. (2007b). In this approach, operations on
the agenda are controlled by probabilities learned
from data using a variation of the EM algorithm.
However, this approach does not readily scale to
more complex interactions in which users can, for
example, change their goal midway through a dia-
logue.
2.1 Random decision parameters
Each time the user simulator receives a system act,
a complex, two-fold process takes place involving
several decisions, made on the basis of both the
nature of the incoming system act and the infor-
mation state of the user, i.e., the status of the user
goal and agenda. The first phase can be seen as
an information state update and involves actions
like filling requested slots or checking whether the
provided information is consistent with the user
goal constraints. In the second phase, the user de-
cides which response act to generate, based on the
updated agenda. Many of the decisions involved
are deterministic, allowing only one possible op-
tion given the context. Other decisions allow for
some degree of variation in the user behaviour and
are governed by probability distributions over the
1We have to date also implemented systems in appoint-
ment scheduling and bus timetable inquiries.
options allowed in that context. For example, if
the system has offered a venue that matches the
user?s goal, the user can randomly decide to either
change his goal or to accept the venue and ask for
additional information such as the phone number.
The non-deterministic part of the simulator is
formalised in terms of a set of random decision
points (RDPs) embedded in the decision process.
If an RDP is encountered (depending on the con-
text), a random choice between the options de-
fined for that point is made by sampling from a
probability distribution. Most of the RDPs are
controlled by a multinomial distribution, such as
deciding whether or not to change the goal after
a system offer. Some RDPs are controlled by a
geometric distribution, like in the case where the
user is planning to specify one of his constraints
(with an inform act popped from the agenda) and
then repeatedly adds an additional constraint to the
act (by combining it with an additional inform act
popped from the agenda) until it randomly decides
not to add any more constraints (or runs out of
constraints to specify). The parameter for this dis-
tribution thus controls how cautious the user is in
providing information to the system.
Hence, the user simulator can be viewed as
a ?decision network?, consisting of deterministic
and random decision points. This is illustrated in
Figure 1 for the simplified case of a network with
only four RDPs; the actual simulator has 23 RDPs,
with 27 associated parameters in total. Each time
the simulator receives a system act, it follows a
path through the network, which is partly deter-
mined by that system act and the user goal and
agenda, and partly by random decisions made ac-
cording to the probability distributions for each
random decision point i given by its parameters
?i.
3 Training the simulator from data
The parameterisation of the user simulator as de-
scribed in Section 2.1 forms the basis for a method
for training the simulator with real user data. The
parameters describing the probability distributions
for each RDP are estimated in order to generate
user behaviour that fits the user behaviour in the
corpus as closely as possible. In order to do so,
a sample based maximum likelihood approach is
taken, in which the simulator is run repeatedly
against the system acts in the corpus, and the ran-
dom decisions that lead to simulated acts matching
the true act in the corpus are recorded. The param-
eters are then estimated using the counts for each
of the random decision points.
117
incoming
system act
outgoing
user act
user goal + agenda
?2
?1
?3
?4
Figure 1: User simulator viewed as a ?decision network?: square nodes indicate deterministic decision
points; round nodes indicate random decision points, and have associated parameters ?i; the loop on one
of the nodes indicates it has a geometric distribution associated with it.
3.1 Parameter estimation
Before starting the process of matching simulated
acts with true acts and collecting counts for the
RDPs, the parameters are initialised to values cor-
responding to uniform distributions. Then, the
simulator is run against all dialogues in the cor-
pus in such a way that for each turn in a dialogue
(consisting of a system act and a user act), the user
simulator is provided with the system act and is
run repeatedly to generate several simulated user
response acts for that turn. For the first turn of a di-
alogue, the simulator is initialised with the correct
user state (see Section 3.2). For each response, the
simulator may make different random decisions,
generally leading to different user acts. The deci-
sions that lead to a simulated act that matches the
true act are recorded as successful. By generating
a sufficiently large number of simulated acts, all
possible combinations of decisions are explored to
find a matching act. Given the high complexity of
the simulator, this sampling approach is preferred
over directly enumerating all decision combina-
tions to identify the successful ones. If none of
the combinations are successful, then either a) the
processing of the dialogue is ended, or b) the cor-
rect context is set for the next turn and processing
is continued. Whereas the former approach aims at
matching sequences of turns, the latter only aims
at matching each user turn separately. In either
case, after all data is processed, the parameters are
estimated using the resulting counts of successful
decisions for each of the RDPs.
For each RDP i, let DPi represent the decision
taken, and dij the j?th possible decision. Then, for
each decision point i that is controlled by a multi-
nomial distribution, the corresponding parameter
estimates ?ij are obtained as follows from the de-
cision frequencies c(DPi = dij):
?ij =
c(DPi = dij)
?
j c(DPi = dij)
(1)
Random decision points that are controlled
by geometric distributions involve potentially
multiple random decisions between two options
(Bernoulli trials). The parameters for such RDPs
are estimated as follows:
?i =
(
1
n
n
?
k=1
bik
)?1
(2)
where bik is the number of Bernoulli trials re-
quired at the k?th time decision point i was en-
countered. In terms of the decision network, this
estimate is correlated with the average number of
times the loop of the node was taken.
3.2 User goal inference
In order to be able to set the correct user goal
state in any given turn, a set of update rules is
used to infer the user?s goals from a dialogue be-
forehand, on the basis of the entire sequence of
system acts and ?true? user acts (see Section 4.1)
in the corpus. These update rules are based on
the notion of dialogue act preconditions, which
specify conditions of the dialogue context that
must hold for a dialogue agent to perform that
act. For example, a precondition for the act
inform(area=central) is that the speaker
wants a venue in the centre. The user act model
118
of the HIS dialogue manager is designed accord-
ing to this same notion (Keizer et al, 2008). In this
model, the probability of a user act in a certain dia-
logue context (the last system act and a hypothesis
regarding the user goal) is determined by checking
the consistency of its preconditions with that con-
text. This contributes to updating the system?s be-
lief state on the basis of which it determines its re-
sponse action. For the user goal inference model,
the user act is given and therefore its precondi-
tions can be used to directly infer the user goal.
So, for example, in the case of observing the user
act inform(area=central), the constraint
(area=central) is added to the user goal.
In addition to using the inferred user goals, the
agenda is corrected in cases where there is a mis-
match between real and simulated user acts in the
previous turn.
In using this offline goal inference model, our
approach takes a position between (Schatzmann et
al., 2007b), in which the user?s goal is treated as
hidden, and (Georgila et al, 2006), in which the
user?s goal is obtained directly from the corpus an-
notation.
4 Evaluation
The parameter estimation technique for training
the user simulator was evaluated in two differ-
ent ways. The first evaluation involved compar-
ing the statistics of simulated and real user be-
haviour. The second evaluation involved compar-
ing dialogue manager policies trained with differ-
ent simulators.
4.1 Data
The task of the dialogue systems we are develop-
ing is to provide tourist information to users, in-
volving venues such as bars, restaurants and hotels
that the user can search for and ask about. These
venues are described in terms of features such as
price range, area, type of food, phone number,
address, and so on. The kind of dialogues with
these systems are commonly called slot-filling di-
alogues.
Within the range of slot-filling applications the
domain is relatively complex due to its hierarchi-
cal data structure and relatively large number of
slots and their possible values. Scalability is in-
deed one of the primary challenges to be addressed
in statistical approaches to dialogue system devel-
opment, including user simulation.
The dialogue corpus that was used for training
and evaluating the simulator was obtained from
the evaluation of a POMDP spoken dialogue sys-
tem with real users. All user utterances in the
resulting corpus were transcribed and semanti-
cally annotated in terms of dialogue acts. Dia-
logue acts consist of a series of semantic items,
including the type (describing the intention of
the speaker, e.g., inform or request) and a
list of slot value pairs (e.g., food=Chinese or
area=south). An extensive analysis of the an-
notations from three different people revealed a
high level of inter-annotator agreement (ranging
from 0.81 to 0.94, depending on which pair of an-
notations are compared), and a voting scheme for
selecting a single annotation for each turn ensured
the reliability of the ?true? user acts used for train-
ing the simulator.
4.2 Corpus statistics results
A first approach to evaluating user simulations is
to look at the statistics of the user behaviour that
is generated by a simulator and compare it with
that of real users as observed in a dialogue cor-
pus. Several metrics for such evaluations have
been considered in the literature, all of which have
both strong points and weaknesses. For the present
evaluation, a selection of metrics believed to give
a reasonable first indication of the quality of the
user simulations was considered2 .
4.2.1 Metrics
The first corpus-based evaluation metric is the Log
Likelihood (LL) of the data, given the user simu-
lation model. This is what is in fact maximised by
the parameter estimation algorithm. The log like-
lihood can be computed by summing the log prob-
abilities of each user turn du in the corpus data D:
ll(D|{?ij}, {?i}) =
?
u
log P (du|{?ij}, {?i})
(3)
The user turn probability is given by the prob-
ability of the decision paths (directed paths in the
decision network of maximal length, such as the
one indicated in Figure 1 in bold) leading to a sim-
ulated user act in that turn that matches the true
user act. The probability of a decision path is ob-
tained by multiplying the probabilities of the de-
cisions made at each decision point i that was en-
countered, which are given by the parameters ?ij
2Note that not all selected metrics are metrics in the strict
sense of the word; the term should therefore be interpreted as
a more general one.
119
and ?i:
logP (du|{?ij}, {?i}) =
?
i?Im(u)
log
(
?
j
?ij ? ?ij(u)
)
+ (4)
?
i?Ig(u)
log
(
?
k
(1 ? ?i)k?1 ? ?i ? ?ik(u)
)
where Im(u) = {i ? Im|?j ?ij(u) > 0} and
Ig(u) = {i ? Ig|
?
k ?ik(u) > 0} are the subsets
of the multinomial (Im) and geometric (Ig) de-
cision points respectively containing those points
that were encountered in any combination of deci-
sions resulting in the given user act:
?ij(u) =
?
?
?
?
?
1 if decision DPi = dij was
taken in any of the
matching combinations
0 otherwise
(5)
?ik(u) =
?
?
?
?
?
1 if any of the matching
combinations required
k > 0 trials
0 otherwise
(6)
It should be noted that the log likelihood only
represents those turns in the corpus for which the
simulated user can produce a matching simulated
act with some probability. Hence, it is impor-
tant to also take into account the corpus cover-
age when considering the log likelihood in cor-
pus based evaluation. Dividing by the number of
matched turns provides a useful normalisation in
this respect.
The expected Precision (PRE), Recall (RCL),
and F-Score (FS) are obtained by comparing the
simulated user acts with the true user acts in the
same context (Georgila et al, 2006). These scores
are obtained by pairwise comparison of the simu-
lated and true user act for each turn in the corpus
at the level of the semantic items:
PRE = #(matched items)#(items in simulated act) (7)
RCL = #(matched items)#(items in true act) (8)
FS = 2 ? PRE ? RCLPRE + RCL (9)
By sampling a sufficient number of simulated
acts for each turn in the corpus and comparing
them with the corresponding true acts, this results
in an accurate measure on average.
The problem with precision and recall is that
they are known to heavily penalise unseen data.
Any attempt to generalise and therefore increase
the variability of user behaviour results in lower
scores.
Another way of evaluating the user simulator
is to look at the global user act distributions it
generates and compare them to the distributions
found in the real user data. A common metric
for comparing such distributions is the Kullback-
Leibler (KL) distance. In (Cuaya?huitl et al,
2005) this metric was used to evaluate an HMM-
based user simulation approach. The KL dis-
tance is computed by taking the average of the
two KL divergences3 DKL(simulated||true) and
DKL(true||simulated), where:
DKL(p||q) =
?
i
pi ? log2(
pi
qi
) (10)
KL distances are computed for both full user act
distributions (taking into account both the dia-
logue act type and slot value pairs) and user act
type distributions (only regarding the dialogue act
type), denoted by KLF and KLT respectively.
4.2.2 Results
For the experiments, the corpus data was ran-
domly split into a training set, consisting of 4479
user turns in 541 dialogues, used for estimat-
ing the user simulator parameters, and a test set,
consisting of 1457 user turns in 175 dialogues,
used for evaluation only. In the evaluation, the
following parameter settings were compared: 1)
non-informative, uniform parameters (UNIF); 2)
handcrafted parameters (HDC); 3) parameters es-
timated from data (TRA); and 4) deterministic pa-
rameters (DET), in which for each RDP the prob-
ability of the most probable decision according to
the estimated parameters is set to 1, i.e., at all
times, the most likely decision according to the es-
timated parameters is chosen.
For both trained and deterministic parameters,
a distinction is made between the two approaches
to matching user acts during parameter estimation.
Recall that in the turn-based approach, in each
turn, the simulator is run with the corrected con-
text to find a matching simulated act, whereas in
the sequence-based approach, the matching pro-
cess for a dialogue is stopped in case a turn
is encountered which cannot be matched by the
simulator. This results in estimated parameters
TRA-T and deterministic parameters DET-T for
3Before computing the distances, add-one smoothing was
applied in order to avoid zero-probabilities.
120
PAR nLL-T nLL-S PRE RCL FS KLF KLT
UNIF ?3.78 ?3.37 16.95 (?0.75) 9.47 (?0.59) 12.15 3.057 2.318
HDC ?4.07 ?2.22 44.31 (?0.99) 34.74 (?0.95) 38.94 1.784 0.623
TRA-T ?2.97 - 37.60 (?0.97) 28.14 (?0.90) 32.19 1.362 0.336
DET-T ?? - 47.70 (?1.00) 40.90 (?0.98) 44.04 2.335 0.838
TRA-S - ?2.13 43.19 (?0.99) 35.68 (?0.96) 39.07 1.355 0.155
DET-S - ?? 49.39 (?1.00) 43.04 (?0.99) 46.00 2.310 0.825
Table 1: Results of the sample-based user simulator evaluation on the Mar?09 training
corpus (the corpus coverage was 59% for the turn-based and 33% for the sequence-based
matching approach).
PAR nLL-T nLL-S PRE RCL FS KLF KLT
UNIF ?3.61 ?3.28 16.59 (?1.29) 9.32 (?1.01) 11.93 2.951 2.180
HDC ?3.90 ?2.19 45.35 (?1.72) 36.04 (?1.66) 40.16 1.780 0.561
TRA-T ?2.84 - 38.22 (?1.68) 28.74 (?1.57) 32.81 1.405 0.310
DET-T ?? - 49.15 (?1.73) 42.17 (?1.71) 45.39 2.478 0.867
TRA-S - ?2.12 43.90 (?1.72) 36.52 (?1.67) 39.87 1.424 0.153
DET-S - ?? 50.73 (?1.73) 44.41 (?1.72) 47.36 2.407 0.841
Table 2: Results of the sample-based user simulator evaluation on the Mar?09 test corpus
(corpus coverage 59% for the turn-based, and 36% for sequence-based matching).
the turn-based approach and analogously TRA-S
and DET-S for the sequence-based approach. The
corresponding normalised (see Section 4.2.1) log-
likelihoods are indicated by nLL-T and nLL-S.
Tables 1 and 2 give the results on the training
and test data respectively. The results show that in
terms of log-likelihood and KL-distances, the es-
timated parameters outperform the other settings,
regardless of the matching method. In terms of
precision/recall (given in percentages with 95%
confidence intervals), the estimated parameters
are worse than the handcrafted parameters for
turn-based matching, but have similar scores for
sequence-based matching.
The results for the deterministic parameters il-
lustrate that much better precision/recall scores
can be obtained, but at the expense of variability as
well as the KL-distances. It will be easier to train
a dialogue policy on such a deterministic simula-
tor, but that policy is likely to perform significantly
worse on the more varied behaviour generated by
the trained simulator, as we will see in Section 4.3.
Out of the two matching approaches, the
sequence-based approach gives the best results:
TRA-S outperforms TRA-T on all scores, except
for the coverage which is much lower for the
sequence-based approach (33% vs. 59%).
4.3 Policy evaluation results
Although the corpus-based evaluation results give
a useful indication of how realistic the behaviour
generated by a simulator is, what really should be
evaluated is the dialogue management policy that
is trained using that simulator. Therefore, differ-
ent parameter sets for the simulator were used to
train and evaluate different policies for the Hidden
Information State (HIS) dialogue manager (Young
et al, 2009). Four different policies were trained:
one policy using handcrafted simulation param-
eters (POL-HDC); two policies using simulation
parameters estimated (using the sequence-based
matching approach) from two data sets that were
obtained by randomly splitting the data into two
parts of 358 dialogues each (POL-TRA1 and POL-
TRA2); and finally, a policy using a determin-
istic simulator (POL-DET) constructed from the
trained parameters as discussed in Section 4.2.2.
The policies were then each evaluated on the sim-
ulator using the four parameter settings at different
semantic error rates.
The performance of a policy is measured in
terms of a reward that is given for each dialogue,
i.e. a reward of 20 for a successful dialogue, mi-
nus the number of turns. A dialogue is consid-
ered successful if the system has offered a venue
matching the predefined user goal constraints and
has given the correct values of all requested slots
for this venue. During the policy optimisation, in
which a reinforcement learning algorithm tries to
optimise the expected long term reward, this dia-
logue scoring regime was also used.
In Figures 2, 3, and 4, evaluation results are
given resulting from running 3000 dialogues at
each of 11 different semantic error rates. The
curves show average rewards with 95% confidence
intervals. The error rate is controlled by a hand-
121
-2
 0
 2
 4
 6
 8
 10
 12
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 2: Average rewards for each policy when
evaluated on UM-HDC.
-4
-2
 0
 2
 4
 6
 8
 10
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 3: Average rewards for each policy when
evaluated on UM-TRA1.
 2
 4
 6
 8
 10
 12
 14
 16
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 4: Average rewards for each policy when
evaluated on UM-DET.
 0
 1
 2
 3
 4
 5
 6
 7
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
 lo
ss
Error rate
POL-HDC
POL-TRA2
POL-DET
Figure 5: Average loss in reward for each policy,
across three different simulators.
crafted error model that converts the user act gen-
erated by the simulator into an n-best list of dia-
logue act hypotheses.
The policy that was trained using the hand-
crafted simulator (POL-HDC) outperforms the
other policies when evaluated on that same sim-
ulator (see Figure 2), and both policies trained us-
ing the trained simulators (POL-TRA1 and POL-
TRA2) outperform the other policies when evalu-
ated on either trained simulator (see Figure 3 for
the evaluation on UM-TRA1; the evaluation on
UM-TRA2 is very similar and therefore omitted).
There is little difference in performance between
policies POL-TRA1 and POL-TRA2, which can
be explained by the fact that the two trained
parameter settings are quite similar, in contrast
to the handcrafted parameters. The policy that
was trained on the deterministic parameters (POL-
DET) is competitive with the other policies when
evaluated on UM-DET (see Figure 4), but per-
forms significantly worse on the other parameter
settings which generate the variation in behaviour
that the dialogue manager did not encounter dur-
ing training of POL-DET.
In addition to comparing the policies when eval-
uated on each simulator separately, another com-
parison was made in terms of the average perfor-
mance across all simulators. For each policy and
each simulator, we first computed the difference
between the policy?s performance and the ?maxi-
mum? performance on that simulator as achieved
by the policy that was also trained on that simu-
lator, and then averaged over all simulators. To
avoid biased results, only one of the trained simu-
lators was included. The results in Figure 5 show
that the POL-TRA2 policy is more robust than
POL-DET, and has similar robustness as POL-
HDC. Similar results are obtained when including
UM-TRA1 only.
Given that the results of Section 4.2 show that
the dialogues generated by the trained simulator
more closely match real corpus data, and given
that the above simulation results show that the
POL-TRA policies are at least as robust as the
122
other policies, it seems likely that policies trained
using the trained user simulator will show im-
proved performance when evaluated on real users.
However, this claim can only be properly
demonstrated in a real user evaluation of the di-
alogue system containing different dialogue man-
agement policies. Such a user trial would also be
able to confirm whether the results from evalua-
tions on the trained simulator can more accurately
predict the actual performance expected with real
users.
5 Conclusion
In this paper, we presented an agenda-based user
simulator extended to be trainable on real user
data whilst preserving the necessary rationality
and complexity for effective training and evalu-
ation of dialogue manager policies. The exten-
sion involved the incorporation of random deci-
sion points in the process of receiving and re-
sponding to a system act in each turn. The deci-
sions made at these points are controlled by prob-
ability distributions defined by a set of parameters.
A sample-based maximum likelihood approach
to estimating these parameters from real user data
in a corpus of human-machine dialogues was dis-
cussed, and two kinds of evaluations were pre-
sented. When comparing the statistics of real ver-
sus simulated user behaviour in terms of a selec-
tion of different metrics, overall, the estimated pa-
rameters were shown to give better results than
the handcrafted baselines. When evaluating dia-
logue management policies trained on the simula-
tor with different parameter settings, it was shown
that: 1) policies trained on a particular parame-
ter setting outperform other policies when evalu-
ated on the same parameters, and in particular, 2)
a policy trained on the trained simulator outper-
forms other policies on a trained simulator. With
the general goal of obtaining a dialogue manager
that performs better in practice, these results are
encouraging, but need to be confirmed by an eval-
uation of the policies on real users.
Additionally, there is still room for improving
the quality of the simulator itself. For example,
the variation in user behaviour can be improved by
adding more random decision points, in order to
achieve better corpus coverage. In addition, since
there is no clear consensus on what is the best met-
ric for evaluating user simulations, additional met-
rics will be explored in order to get a more bal-
anced indication of the quality of the user simu-
lator and how the various metrics are affected by
modifications to the simulator. Perplexity (related
to the log likelihood, see (Georgila et al, 2005)),
accuracy (related to precision/recall, see (Zuker-
man and Albrecht, 2001; Georgila et al, 2006)),
and Crame?r-von Mises divergence (comparing di-
alogue score distributions, see (Williams, 2008))
are some of the metrics worth considering.
References
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-computer dialogue sim-
ulation using hidden markov models. In Proc.
ASRU?05, pages 290?295.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning user simulations for information state up-
date dialogue systems. In Proc. Interspeech ?05.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
simulation for spoken dialogue systems: Learning
and evaluation. In Proc. Interspeech/ICSLP.
S. Keizer, M. Gas?ic?, F. Mairesse, B. Thomson, K. Yu,
and S. Young. 2008. Modelling user behaviour in
the HIS-POMDP dialogue manager. In Proc. SLT,
Goa, India.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialogue strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007a. Agenda-based user simula-
tion for bootstrapping a POMDP dialogue system.
In Proceedings HLT/NAACL, Rochester, NY.
J. Schatzmann, B. Thomson, and S. Young. 2007b.
Statistical user simulation with a hidden agenda. In
Proc. SIGDIAL?07, pages 273?282, Antwerp, Bel-
gium.
K. Scheffler and S. Young. 2001. Corpus-based dia-
logue simulation for automatic strategy learning and
evaluation. In Proceedings NAACL Workshop on
Adaptation in Dialogue.
S. Singh, M. Kearns, D. Litman, and M. Walker. 2000.
Reinforcement learning for spoken dialogue sys-
tems. In S. Solla, T. Leen, and K. Mu?ller, editors,
Advances in Neural Information Processing Systems
(NIPS). MIT Press.
J. Williams. 2008. Evaluating user simulations with
the Crame?r-von Mises divergence. Speech Commu-
nication, 50:829?846.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2009. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language, 24(2):150?174.
I. Zukerman and D. Albrecht. 2001. Predictive statis-
tical models for user modeling. User Modeling and
User-Adapted Interaction, 11:5?18.
123
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 201?204,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Gaussian Processes for Fast Policy Optimisation of POMDP-based
Dialogue Managers
M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
Modelling dialogue as a Partially Observ-
able Markov Decision Process (POMDP)
enables a dialogue policy robust to speech
understanding errors to be learnt. How-
ever, a major challenge in POMDP pol-
icy learning is to maintain tractability, so
the use of approximation is inevitable.
We propose applying Gaussian Processes
in Reinforcement learning of optimal
POMDP dialogue policies, in order (1) to
make the learning process faster and (2) to
obtain an estimate of the uncertainty of the
approximation. We first demonstrate the
idea on a simple voice mail dialogue task
and then apply this method to a real-world
tourist information dialogue task.
1 Introduction
One of the main challenges in dialogue manage-
ment is effective handling of speech understand-
ing errors. Instead of hand-crafting the error han-
dler for each dialogue step, statistical approaches
allow the optimal dialogue manager behaviour
to be learnt automatically. Reinforcement learn-
ing (RL), in particular, enables the notion of plan-
ning to be embedded in the dialogue management
criteria. The objective of the dialogue manager is
for each dialogue state to choose such an action
that leads to the highest expected long-term re-
ward, which is defined in this framework by the Q-
function. This is in contrast to Supervised learn-
ing, which estimates a dialogue strategy in such a
way as to make it resemble the behaviour from a
given corpus, but without directly optimising over-
all dialogue success.
Modelling dialogue as a Partially Observable
Markov Decision Process (POMDP) allows action
selection to be based on the differing levels of un-
certainty in each dialogue state as well as the over-
all reward. This approach requires that a distribu-
tion of states (belief state) is maintained at each
turn. This explicit representation of uncertainty in
the POMDP gives it the potential to produce more
robust dialogue policies (Young et al, 2010).
The main challenge in the POMDP approach is
the tractability of the learning process. A dis-
crete state space POMDP can be perceived as a
continuous space MDP where the state space con-
sists of the belief states of the original POMDP.
A grid-based approach to policy optimisation as-
sumes discretisation of this space, allowing for
discrete space MDP algorithms to be used for
learning (Brafman, 1997) and thus approximating
the optimal Q-function. Such an approach takes
the order of 100, 000 dialogues to train a real-
world dialogue manager. Therefore, the training
normally takes place in interaction with a simu-
lated user, rather than real users. This raises ques-
tions regarding the quality of the approximation
as well as the potential discrepancy between sim-
ulated and real user behaviour.
Gaussian Processes have been successfully used
in Reinforcement learning for continuous space
MDPs, for both model-free approaches (Engel et
al., 2005) and model-based approaches (Deisen-
roth et al, 2009). We propose using GP Rein-
forcement learning in a POMDP dialogue man-
ager to, firstly, speed up the learning process and,
secondly, obtain the uncertainty of the approxima-
tion. We opt for the model-free approach since it
has the potential to allow the policy obtained in
interaction with the simulated user to be further
refined in interaction with real users.
In the next section, the core idea of the method is
explained on a toy dialogue problem where differ-
ent aspects of GP learning are examined. Follow-
ing that, in Section 3, it is demonstrated how this
methodology can be effectively applied to a real
world dialogue. We conclude with Section 4.
2 Gaussian Process RL on a Toy Problem
2.1 Gaussian Process RL
A Gaussian Process is a generative model of
Bayesian inference that can be used for function
regression (Rasmussen and Williams, 2005). A
Gaussian Process is fully defined by a mean and a
kernel function. The kernel function defines prior
function correlations, which is crucial for obtain-
ing good posterior estimates with just a few ob-
servations. GP-Sarsa is an on-line reinforcement
learning algorithm for both continuous and dis-
crete MDPs that incorporates GP regression (En-
201
gel et al, 2005). Given the observation of rewards,
it estimates the Q-function utilising its correlations
in different parts of the state and the action space
defined by the kernel function. It also gives a vari-
ance of the estimate, thus modelling the uncer-
tainty of the approximation.
2.2 Voice Mail Dialogue Task
In order to demonstrate how this methodology
can be applied to a dialogue system, we first ex-
plain the idea on the voice mail dialogue prob-
lem (Williams, 2006).
The state space of this task consists of three states:
the user asked for the message either to be saved
or deleted, or the dialogue ended. The system
can take three actions: ask the user what to do,
save or delete the message. The observation of
what the user wants is corrupted with noise, there-
fore we model this as a three-state POMDP. This
POMDP can be viewed as a continuous MDP,
where the MDP state is the POMDP belief state,
a 3-dimensional vector of probabilities. For both
learning and evaluation, a simulated user is used
which makes an error with probability 0.3 and ter-
minates the dialogue after at most 10 turns. In the
final state, it gives a positive reward of 10 or a
penalty of ?100 depending on whether the system
performed a correct action or not. Each interme-
diate state receives the penalty of ?1. In order to
keep the problem simple, a model defining tran-
sition and observation probabilities is assumed so
that the belief can be easily updated, but the policy
optimisation is performed in an on-line fashion.
2.3 Kernel Choice for GP-Sarsa
The choice of kernel function is very important
since it defines the prior knowledge about the Q-
function correlations. They have to be defined on
both states and actions. In the voice mail dialogue
problem the action space is discrete, so we opt for
a simple ? kernel over actions:
k(a, a?) = 1 ? ?a(a?), (1)
where ?a is the Kronecker delta function. The
state space is a 3-dimensional continuous space
and the kernel functions over the state space that
we explore are given in Table 1. Each kernel func-
kernel function expression
polynomial k(x,x?) = ?x,x??
parametrised poly. k(x,x?) =
PD
i=1
xix
?
i
r2i
Gaussian k(x,x?) = p2 exp ? ?x ? x
??2
2?2k
scaled norm k(x,x?) = 1 ? ?x ? x
??2
?x?2?x??2
Table 1: Kernel functions
tion defines a different correlation. The polyno-
mial kernel views elements of the state vector as
features, the dot-product of which defines the cor-
relation. They can be given different relevance ri
in the parametrised version. The Gaussian ker-
nel accounts for smoothness, i.e., if two states are
close to each other the Q-function in these states
is correlated. The scaled norm kernel defines posi-
tive correlations in the points that are close to each
other and a negative correlation otherwise. This
is particularly useful for the voice mail problem,
where, if two belief states are very different, tak-
ing the same action in these states generates a neg-
atively correlated reward.
2.4 Optimisation of Kernel Parameters
Some kernel functions are in a parametrised
form, such as Gaussian or parametrised polyno-
mial kernel. These parameters, also called the
hyper-parameters, are estimated by maximising
the marginal likelihood1 on a given corpus (Ras-
mussen and Williams, 2005). We adapted the
available code (Rasmussen and Williams, 2005)
for the Reinforcement learning framework to ob-
tain the optimal hyper-parameters using a dialogue
corpus labelled with states, actions and rewards.
2.5 Grid-based RL Algorithms
To assess the performance of GP-Sarsa, it was
compared with a standard grid-based algorithm
used in (Young et al, 2010). The grid-based ap-
proach discretises the continuous space into re-
gions with their representative points. This then
allows discrete MDP algorithms to be used for pol-
icy optimisation, in this case the Monte Carlo Con-
trol (MCC) algorithm (Sutton and Barto, 1998).
2.6 Optimal POMDP Policy
The optimal POMDP policy was obtained us-
ing the POMDP solver toolkit (Cassandra, 2005),
which implements the Point Based Value Itera-
tion algorithm to solve the POMDP off-line using
the underlying transition and observation proba-
bilities. We used 300 sample dialogues between
the dialogue manager governed by this policy and
the simulated user as data for optimisation of the
kernel hyper-parameters (see Section 2.4).
2.7 Training set-up and Evaluation
The dialogue manager was trained in interaction
with the simulated user and the performance was
compared between the grid-based MCC algorithm
and GP-Sarsa across different kernel functions
from Table 1.
The intention was, not only to test which algo-
rithm yields the best policy performance, but also
to examine the speed of convergence to the opti-
mal policy. All the algorithms use an ?-greedy
approach where the exploration rate ? was fixed
at 0.1. The learning process greatly depends on
1Also called evidence maximisation in the literature.
202
the actions that are taken during exploration. If
early on during the training, the systems discovers
a path that generates high rewards due to a lucky
choice of actions, then the convergence is faster.
To alleviate this, we adopted the following proce-
dure. For every training set-up, exactly the same
training iterations were performed using 1000 dif-
ferent random generator seedings. After every 20
dialogues the resulting 1000 partially optimised
policies were evaluated. Each of them was tested
on 1000 dialogues. The average reward of these
1000 dialogues provides just one point in Fig. 1.
20 60 100 140 180 220 260 300 340 380 420 460 500 540 580 620
?50
?45
?40
?35
?30
?25
?20
?15
?10
?5
0
Training dialogues
Av
er
ag
e 
re
wa
rd
polynomial kernel ? 
? Gaussian kernel with learned hyper?parameters
? scaled norm kernel
polynomial kernel with learned hyper?parameters
?
 
 
Optimal POMDP Policy
GP?Sarsa
Grid?based Monte Carlo Control
Figure 1: Evaluation results on Voice Mail task
The grid-based MCC algorithm used a Euclid-
ian distance to generate the grid by adding every
point that was further than 0.01 from other points
as a representative of a new region. As can be
seen from Fig 1, the grid-Based MCC algorithm
has a relatively slow convergence rate. GP-Sarsa
with the polynomial kernel exhibited a learning
rate similar to MCC in the first 300 training di-
alogues, continuing with a more upward learning
trend. The parametrised polynomial kernel per-
forms slightly better. The Gaussian kernel, how-
ever, achieves a much faster learning rate. The
scaled norm kernel achieved close to optimal per-
formance in 400 dialogues, with a much higher
convergence rate then the other methods.
3 Gaussian Process RL on a Real-world
Task
3.1 HIS Dialogue Manager on CamInfo
Domain
We investigate the use of GP-Sarsa in a real-
world task by extending the Hidden Information
State (HIS) dialogue manager (Young et al, 2010).
The application domain is tourist information for
Cambridge, whereby the user can ask for informa-
tion about a restaurant, hotel, museum or another
tourist attraction in the local area. The database
consists of more than 400 entities each of which
has up to 10 attributes that the user can query.
The HIS dialogue manager is a POMDP-based di-
alogue manager that can tractably maintain belief
states for large domains. The key feature of this
approach is the grouping of possible user goals
into partitions, using relationships between differ-
ent attributes from possible user goals. Partitions
are combined with possible user dialogue actions
from the N-best user input as well as with the di-
alogue history. This combination forms the state
space ? the set of hypotheses, the probability dis-
tribution over which is maintained during the di-
alogue. Since the number of states for any real-
world problem is too large, for tractable policy
learning, both the state and the action space are
mapped into smaller scale summary spaces. Once
an adequate summary action is found in the sum-
mary space, it is mapped back to form an action in
the original master space.
3.2 Kernel Choice for GP-Sarsa
The summary state in the HIS system is a four-
dimensional space consisting of two elements that
are continuous (the probability of the top two hy-
potheses) and two discrete elements (one relating
the portion of the database entries that matches the
top partition and the other relating to the last user
action type). The summary action space is discrete
and consists of eleven elements.
In order to apply the GP-Sarsa algorithm, a kernel
function needs to be specified for both the sum-
mary state space and the summary action space.
The nature of this space is quite different from the
one described in the toy problem. Therefore, ap-
plying a kernel that has negative correlations, such
as the scaled norm kernel (Table 1) might give un-
expected results. More specifically, for a given
summary action, the mapping procedure finds the
most appropriate action to perform if such an ac-
tion exists. This can lead to a lower reward if
the summary action is not adequate but would
rarely lead to negatively correlated rewards. Also,
parametrised kernels could not be used for this
task, since there was no corpus available for hyper-
parameter optimisation. The polynomial kernel
(Table 1) assumes that the elements of the space
are features. Due to the way the probability is
maintained over this very large state space, the
continuous variables potentially encode more in-
formation than in the simple toy problem. There-
fore, we used the polynomial kernel for the con-
tinuous elements. For discrete elements, we utilise
the ?-kernel (Eq. 2.3).
3.3 Active Learning GP-Sarsa
The GP RL framework enables modelling the un-
certainty of the approximation. The uncertainty
estimate can be used to decide which actions
to take during the exploration (Deisenroth et al,
203
2009). In detail, instead of a random action, the
action in which the Q-function for the current state
has the highest variance is taken.
3.4 Training Set-up and Evaluation
Policy optimisation is performed by interacting
with a simulated user on the dialogue act level.
The simulated user gives a reward at the final state
of the dialogue, and that is 20 if the dialogue was
successful, 0 otherwise, less the number of turns
taken to fulfil the user goal. The simulated user
takes a maximum of 100 turns in each dialogue,
terminating it when all the necessary information
has been obtained or if it looses patience.
A grid-based MCC algorithm provides the base-
line method. The distance metric used ensures
that the number of regions in the grid is small
enough for the learning to be tractable (Young et
al., 2010).
In order to measure how fast each algorithm
learns, a similar training set-up to the one pre-
sented in Section 2.7 was adopted and the aver-
aged results are plotted on the graph, Fig. 2.
200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000
2
3
4
5
6
7
8
9
Training dialogues
Av
er
ag
e 
re
wa
rd
?  Grid?based Monte Carlo Control
?  GP?Sarsa with polynomial kernel
?  Active learning GP?Sarsa with polynomial kernel
Figure 2: Evaluation results on CamInfo task
The results show that in the very early stage of
learning, i.e., during the first 400 dialogues, the
GP-based method learns faster. Also, the learning
process can be accelerated by adopting the active
learning framework where the actions are selected
based on the estimated uncertainty.
After performing many iterations in an incremen-
tal noise learning set-up (Young et al, 2010) both
the GP-Sarsa and the grid-based MCC algorithms
converge to the same performance.
4 Conclusions
This paper has described how Gaussian Processes
in Reinforcement learning can be successfully ap-
plied to dialogue management. We implemented
a GP-Sarsa algorithm on a toy dialogue prob-
lem, showing that with an appropriate kernel func-
tion faster convergence can be achieved. We also
demonstrated how kernel parameters can be learnt
from a dialogue corpus, thus creating a bridge
between Supervised and Reinforcement learning
methods in dialogue management. We applied
GP-Sarsa to a real-world dialogue task showing
that, on average, this method can learn faster than
a grid-based algorithm. We also showed that the
variance that GP is estimating can be used in an
Active learning setting to further accelerate policy
optimisation.
Further research is needed in the area of kernel
function selection. The results here suggest that
the GP framework can facilitate faster learning,
which potentially allows the use of larger sum-
mary spaces. In addition, being able to learn ef-
ficiently from a small number of dialogues offers
the potential for learning from direct interaction
with real users.
Acknowledgements
The authors would like to thank Carl Rasmussen
for valuable discussions. This research was partly
funded by the UK EPSRC under grant agreement
EP/F013930/1 and by the EU FP7 Programme un-
der grant agreement 216594 (CLASSiC project).
References
RI Brafman. 1997. A Heuristic Variable Grid Solution
Method for POMDPs. In AAAI, Cambridge, MA.
AR Cassandra. 2005. POMDP solver.
http://www.cassandra.org/pomdp/
code/index.shtml.
MP Deisenroth, CE Rasmussen, and J Peters. 2009.
Gaussian Process Dynamic Programming. Neuro-
comput., 72(7-9):1508?1524.
Y Engel, S Mannor, and R Meir. 2005. Reinforcement
learning with Gaussian processes. In ICML ?05:
Proceedings of the 22nd international conference on
Machine learning, pages 201?208, New York, NY.
CE Rasmussen and CKI Williams. 2005. Gaussian
Processes for Machine Learning. MIT Press, Cam-
bridge, MA.
RS Sutton and AG Barto. 1998. Reinforcement Learn-
ing: An Introduction. Adaptive Computation and
Machine Learning. MIT Press, Cambridge, MA.
JD Williams. 2006. Partially Observable Markov De-
cision Processes for Spoken Dialogue Management.
Ph.D. thesis, University of Cambridge.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The Hid-
den Information State Model: a practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
204
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
Proceedings of the SIGDIAL 2013 Conference, pages 223?232,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Training and evaluation of an MDP model for social multi-user
human-robot interaction
Simon Keizer, Mary Ellen Foster,
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh (UK)
{s.keizer,m.e.foster,o.lemon}@hw.ac.uk
Andre Gaschler, Manuel Giuliani
fortiss GmbH
Munich (Germany)
{gaschler,giuliani}@fortiss.org
Abstract
This paper describes a new approach to
automatic learning of strategies for social
multi-user human-robot interaction. Us-
ing the example of a robot bartender that
tracks multiple customers, takes their or-
ders, and serves drinks, we propose a
model consisting of a Social State Recog-
niser (SSR) which processes audio-visual
input and maintains a model of the social
state, together with a Social Skills Execu-
tor (SSE) which takes social state updates
from the SSR as input and generates robot
responses as output. The SSE is modelled
as two connected Markov Decision Pro-
cesses (MDPs) with action selection poli-
cies that are jointly optimised in interaction
with a Multi-User Simulation Environment
(MUSE). The SSR and SSE have been in-
tegrated in the robot bartender system and
evaluated with human users in hand-coded
and trained SSE policy variants. The re-
sults indicate that the trained policy out-
performed the hand-coded policy in terms
of both subjective (+18%) and objective
(+10.5%) task success.
1 Introduction
As the use of robot technology in the home as well
as in public spaces is increasingly gaining attention,
the need for effective and robust models for natural
and social human robot interaction becomes more
important. Whether it involves robot companions
(Vardoulakis et al, 2012), game-playing robots
(Klotz et al, 2011; Brooks et al, 2012; Cuaya?huitl
and Kruijff-Korbayova?, 2012), or robots that help
people with exercising (Fasola and Mataric, 2013),
human users should be able to interact with such
service robots in an effective and natural way, us-
ing speech as well as other modalities of commu-
nication. Furthermore, with the emergence of new
application domains there is a particular need for
methods that enable rapid development of mod-
els for such new domains. In this respect, data-
driven approaches are appealing for their capability
to automatically exploit empirical data to arrive at
realistic and effective models for interpreting user
behaviour, as well as to learn strategies for effective
system behaviour.
In spoken dialogue systems research, statisti-
cal methods for spoken language understanding,
dialogue management, and natural language gen-
eration have proven to be feasible for effective
and robust interactive systems (Rieser and Lemon,
2011; Lemon and Pietquin, 2012; Young et al,
2010; Young et al, 2013). Although such methods
have recently also been applied to (multi-modal)
human-robot interaction (Stiefelhagen et al, 2007;
Cuaya?huitl et al, 2012), work on multi-user human-
robot interaction has been limited to non-statistical,
hand-coded models (Klotz et al, 2011).
On the other hand, substantial work has been
done in the field of situated multi-party interaction
in general, including data-driven approaches. In
particular, Bohus & Horvitz (2009) have addressed
the task of recognising engagement intentions using
online learning in the setting of a screen-based em-
bodied virtual receptionist, and have also worked
on multi-party turn-taking in this context (Bohus
and Horvitz, 2011).
In this paper we describe a statistical approach
to automatic learning of strategies for selecting ef-
fective as well as socially appropriate robot actions
in a multi-user context. The approach has been de-
veloped using the example of a robot bartender (see
Figure 1) that tracks multiple customers, takes their
orders, and serves drinks. We propose a model con-
sisting of a Social State Recogniser (SSR) which
processes audio-visual input and maintains a model
of the social state, and a Social Skills Executor
(SSE) which takes social state updates from the
SSR as input and generates robot responses as out-
223
put. The SSE is modelled as a hierarchy of two con-
nected Markov Decision Processes (MDPs) with
action selection policies that are jointly optimised
in interaction with a Multi-User Simulation Envi-
ronment (MUSE).
Figure 1: The robot bartender with two customers
In the remainder of this paper we will describe
the robot system in more detail (Section 2), fol-
lowed by descriptions of the SSR (Section 3), the
SSE (Section 4), and MUSE (Section 5). In Sec-
tion 6 we then discuss in more detail the MDP
model for the SSE and the process of jointly opti-
mising the policies, and present evaluation results
on simulated data. Next, we present results of the
first evaluation of the integrated SSE-MDP compo-
nent with human users (Section 7). The paper is
concluded in Section 8.
2 Robot bartender system
The robot system we used for evaluating the models
is equipped with vision and speech input processing
modules, as well as modules controlling two robot
arms and a talking head. Based on observations
about the users in the scene and their behaviour, the
system must maintain a model of the social context,
and decide on effective and socially appropriate
responses in that context. Such a system must be
able to engage in, maintain, and close interactions
with users, take a user?s order by means of a spoken
conversation, and serve their drinks. The overall
aim is to generate interactive behaviour that is both
task- effective and socially appropriate: in addition
to efficiently taking orders and serving drinks, the
system should, e.g., deal with customers on a first-
come, first-served basis, and should manage the
customers? patience by asking them politely to wait
until the robot is done serving another customer.
As shown in Figure 1, the robot hardware con-
sists of a pair of manipulator arms with grippers,
mounted to resemble human arms, along with
an animatronic talking head capable of produc-
ing facial expressions, rigid head motion, and lip-
synchronised synthesised speech. The input sen-
sors include a vision system which tracks the loca-
tion, facial expressions, gaze behaviour, and body
language of all people in the scene in real time
(Pateraki et al, 2013), along with a linguistic pro-
cessing system (Petrick et al, 2012) combining a
speech recogniser with a natural-language parser
to create symbolic representations of the speech
produced by all users. More details of the architec-
ture and components are provided in (Foster et al,
2012). An alternative embodiment of the system is
also available on the NAO platform.
3 Social State Recogniser
The primary role of the Social State Recogniser
(SSR) is to turn the continuous stream of messages
produced by the low-level input and output com-
ponents of the system into a discrete representa-
tion of the world, the robot, and all entities in the
scene, integrating social, interaction-based, and
task-based properties. The state is modelled as a
set of relations such as facePos(A)=(x,y,z) or
closeToBar(A); see (Petrick and Foster, 2013)
for details on the representation used.
In addition to storing all of the low-level sensor
information, the SSR also infers additional rela-
tions that are not directly reported by the sensors.
For example, it fuses information from vision and
speech to determine which user should be assigned
to a recognised spoken contribution. It also pro-
vides a constant estimate of whether each customer
is currently seeking attention from the bartender
(seeksAttention(A)): the initial version of this
estimator used a hand-coded rule based on the ob-
servation of human behaviour in real bars (Huth
et al, 2012), while a later version (Foster, 2013)
makes use of a supervised learning classifier trained
on labelled recordings of humans interacting with
the first version of the robot bartender.
The SSR provides a query interface to allow
other system components access to the relations
stored in the state, and also publishes an updated
state to the SSE every time there is a change which
might require a system action in response (e.g.,
a customer appears, begins seeking attention, or
makes a drink order).
224
4 Social Skills Executor
The Social Skills Executor (SSE) controls the be-
haviour of the robot system, based on the social
state updates it receives from the SSR. The out-
put of the SSE consists of a combination of non-
communicative robot actions and/or communica-
tive actions with descriptions of their multi-modal
realisations. In the bartender domain, the non-
communicative actions typically involve serving
a specific drink to a specific user, whereas the com-
municative actions have the form of dialogue acts
(Bunt et al, 2010), directed at a specific user, e.g.
setQuestion(drink) (?What would you like to
drink??) or initialGreeting() (?Hello?).
In our design of the SSE, the decision making
process resulting in such outputs (including the ?no
action? output) consists of three stages: 1) social
multi-user coordination: managing the system?s
engagement with the users present in the scene (e.g.,
accept a user?s bid for attention, or proceed with an
engaged user), 2) single-user interaction: if pro-
ceeding with an engaged user, generating a high-
level response to that user, in the form of a com-
municative act or physical action (e.g., greeting the
user or serving him a drink), and 3) multi-modal
fission: selecting a combination of modalities for
realising a chosen response (e.g., a greeting can be
realised through speech and/or a nodding gesture).
One advantage of such a hierarchical design is that
strategies for the different stages can be developed
independently. Another is that it makes automatic
policy optimisation more scalable.
5 Multi-User Simulated Environment
In order to test and evaluate the SSE, as well as to
train SSE action selection policies, we developed
a Multi-User Simulated Environment (MUSE).
MUSE allows for rapidly exploring the large space
of possible states in which the SSE must select
actions. A reward function that incorporates in-
dividual rewards from all simulated users in the
environment is used to encode preferred system
behaviour in a principled way. A simulated user
assigns a reward if they are served the correct drink,
and gives penalties associated with their waiting
time and various other forms of undesired system
responses (see Section 6.1 for more details about
the reward function). All of this provides a practi-
cal platform for evaluating different strategies for
effective and socially appropriate behaviour. It also
paves the way for automatic optimisation of poli-
cies, for example by using reinforcement learning
techniques, as we will discuss in Section 6.1.
The simulated environment replaces the vision
and speech processing modules in the actual robot
bartender system, which means that it generates 1)
vision signals in every time-frame, and 2) speech
processing results, corresponding to sequences of
time-frames where a user spoke. The vision obser-
vations contain information about users that have
been detected, where they are in the scene, whether
they are speaking, and where their attention is di-
rected to. Speech processing results are represented
semantically, in the form of dialogue acts (e.g.,
inform(drink=coke), ?I would like a coke?). As
described in Section 3, the SSR fuses the vision and
speech input, for example to associate an incoming
dialogue act with a particular user.
The simulated signals are the result of combin-
ing the output from the simulated users in the en-
vironment. Each simulated user is initialised with
a random goal (in our domain a type of drink they
want to order), enters the scene at some point, and
starts bidding for attention at some point. Each
simulated user also maintains a state and gener-
ates responses given that state. These responses
include communicative actions directed at the bar-
tender, which are translated into a multi-channel
vision input stream processed by the SSR, and, in
case the user realises the action through speech,
a speech processing event after the user has fin-
ished speaking. Additionally, the simulated users
start with a given patience level, which is reduced
in every frame that the user is bidding for atten-
tion or being served by the system. If a user?s pa-
tience has reduced to zero, s/he gives up and leaves
the bar. However, it is increased by a given fixed
amount when the system politely asks the user to
wait, encoded as a pausing dialogue act. The be-
haviour of the simulated users is partly controlled
by a set of probability distributions that allow for
a certain degree of variation. These distributions
have been informed by statistics derived from a
corpus of human-human customer-bartender inter-
actions (Huth et al, 2012).
In addition to information about the simulated
users, MUSE also provides feedback about the
execution of robot actions to the SSR, in partic-
ular the start and end of all robot speech and non-
communicative robot actions. This type of informa-
tion simulates the feedback that is also provided in
the actual bartender system by the components that
directly control the robot head and arms. Figure 2
225
Figure 2: Social state recognition and social skills execution in a multi-user simulated environment.
shows the architecture of the system interacting
with the simulated environment.
6 MDP model for multi-user interaction
To enable automatic optimisation of strategies for
multi-user social interaction, the SSE model as de-
scribed in Section 4 was cast as a hierarchy of two
Markov Decision Processes (MDPs), correspond-
ing to the social multi-user coordination and single-
user interaction stages of decision making. Both
MDPs have their own state spaces S1 and S2, each
defined by a set of state features, extracted from
the estimated social state made available by the
SSR?see Tables 1 and 3. They also have their own
action setsA1 andA2, corresponding to the range
of decisions that can be made at the two stages (Ta-
bles 2 and 4), and two policies pi1 : S1 ? A1 and
pi2 : S2 ? A2, mapping states to actions.
6.1 Policy optimisation
Using the MDP model as described above, we
jointly optimise the two policies, based on the re-
wards received through the SSR from the simulated
environment MUSE. Since MUSE gives rewards
on a frame-by-frame basis, they are accumulated
in the social state until the SSR publishes a state
update. The SSE stores the accumulated reward
together with the last state encountered and action
taken in that state, after which that reward is reset
in the social state. After each session (involving
interactions with two users in our case), the set
of encountered state-action pairs and associated
rewards is used to update the policies.
The reward provided by MUSE in each frame
is the sum of rewards Ri given by each individual
simulated user i, and a number of general penalties
arising from the environment as a whole. User
rewards consist of a fixed reward in case their goal
is satisfied (i.e., when they have been served the
drink they wanted and ordered), a penalty in case
they are still waiting to be served, a penalty in case
they are engaged with the system but have not been
served their drink yet, and additional penalties, for
example when the system turns his attention to
another user when the user is still talking to it, or
when the system serves a drink before the user has
ordered, or when the system serves another drink
when the user already has been served their drink.
General penalties are given for example when the
system is talking while no users are present.
The policies are encoded as functions that assign
a value to each state-action pair; these so-called
Q-values are estimates of the long-term discounted
cumulative reward. Given the current state, the
policy selects the action with the highest Q-value:
pi(s) = arg max
a
Q(s, a) (1)
Using a Monte-Carlo Control algorithm (Sutton
and Barto, 1998), the policies are optimised by
running the SSR and SSE against MUSE and using
the received reward signal to update the Q-values
after each interaction sequence. During training,
the SSE uses an -greedy policy, i.e., it takes a
random exploration action with probability  = 0.2.
226
Index Feature Values
4 ? i Interaction status for user i + 1 nonEngaged/seeksAttention/engaged
4 ? i + 1 Location of user i + 1 notPresent/!closeToBar/closeToBar
4 ? i + 2 User i + 1 was served a drink no/yes
4 ? i + 3 User i + 1 asked to wait no/yes
Table 1: State features for the social multi-user coordination policy. For each user, 4 features are included
in the state space, resulting in 32 ? 22 = 36 states for interactions with up to 1 user, increasing to 1296
states for interactions with up to 2 users and 46, 656 states for up to 3 users.
Index Action
0 No action
3 ? i + 1 Ask user i + 1 to wait
3 ? i + 2 Accept bid for attention from user i + 1
3 ? i + 3 Proceed interaction with (engaged) user i + 1
Table 2: Actions for the social multi-user coordination policy.
In the policy update step, a discount factor ? = 0.95
is used, which controls the impact that rewards
received later in a session have on the value of state-
action pairs encountered earlier in that session.
Figure 3 shows the learning curve of a joint
policy optimisation, showing average rewards ob-
tained after running the SSE with trained policies
for 500 runs, at several stages of the optimisation
process (after every 2500 sessions/runs/iterations,
the trained policy was saved for evaluation). In this
particular setup, simulated users gave a reward of
550 upon goal completion but in the total score this
is reduced considerably due to waiting time (-2 per
frame), task completion time (-1 per frame) and
various other potential penalties. Also indicated
are the performance levels of two hand-coded SSE
policies, one of which uses a strategy of asking a
user to wait when already engaged with another
user (labelled HDC), and one in which that second
user is ignored until it is done with the engaged user
(labelled HDCnp). The settings for user patience
as discussed in Section 5 determine which of these
policies works best; ideally these settings should be
derived from data if available. Nevertheless, even
with the hand-coded patience settings, the learning
curve indicates that both policies are outperformed
in simulation after 10k iterations, suggesting that
the best strategy for managing user patience can be
found automatically.
7 Human user evaluation
The SSE described above has been integrated in
the full robot bartender system and evaluated for
the first time with human users. In the experiment,
both a hand-coded version and a trained version
of the SSE component were tested; see Table 6 in
Appendix A for the trajectory of state-action pairs
of an example session. The hand-coded version
uses the policy labelled HDC, not HDCnp (see
Section 6.1). In each of the sessions carried out, one
recruited subject and one confederate (one of the
experimenters) approached the bartender together
as clients and both tried to order a drink (coke or
lemonade). After each interaction, the subject filled
out the short questionnaire shown in Figure 4.
Q1: Did you successfully order a drink from the bartender?
[Y/N]
Please state your opinion on the following statements:
[ 1:strongly disagree; 2:disagree; 3:slightly disagree;
4:slightly agree; 5:agree; 6:strongly agree ]
Q2: It was easy to attract the bartender?s attention [1?6]
Q3: The bartender understood me well [1?6]
Q4: The interaction with the bartender felt natural [1?6]
Q5: Overall, I was happy about the interaction [1?6]
Figure 4: Questionnaire from the user study.
37 subjects took part in this study, resulting in a
total of 58 recorded drink-ordering interactions:
29 that used the hand-coded SSE for interaction
management, and 29 that used the trained SSE.
The results from the experiment are summarised
in Table 5. We analysed the results using a linear
mixed model, treating the SSE policy as a fixed fac-
tor and the subject ID as a random factor. Overall,
the pattern of the subjective scores suggests a slight
preference for the trained SSE version, although
227
Index Feature Values
0 Reactive pressure none/thanking/greeting/goodbye/apology
1 Status of user goal unknown/usrInf/sysExpConf/sysImpConf/
grounded/drinkServed/sysAsked
2 Own proc. state none/badASR
Table 3: State features for the single-user interaction policy. In this case, there are 5 ? 7 ? 2 = 70 states.
Index Action Example
0 No action
1 returnGreeting() ?Hello?
2 autoPositive() ?Okay?
3 acceptThanking() ?You?re welcome?
4 autoNegative() ?What did you say??
5 setQuestion(drink) ?What drink would you like??
6 acceptRequest(drink=x) + serveDrink(x) ?Here?s your coke?
Table 4: Actions for the single-user interaction policy, which correspond to possible dialogue acts, except
for ?no action? and serving a drink. The specific drink types required for two of the actions are extracted
from the fully specified user goal in the social state maintained by the SSR.
only the difference in perceived success was statis-
tically significant at the p < 0.05 level. The actual
success rate of the trained policy was also some-
what higher, although not significantly so. Also,
the interactions with the trained SSE took slightly
longer than the ones with the hand-coded SSE in
terms of the number of system turns (i.e., the num-
ber of times the SSE receives a state update and
selects a response action, excluding the times when
it selects a non-action); however, this did not have
any overall effect on the users? subjective ratings.
The higher success rate for the trained SSE could
be partly explained by the fact that fewer ASR prob-
lems were encountered when using this version;
however, since the SSE was not triggered when a
turn was discarded due to low-confidence ASR, this
would not have had an effect on the number of sys-
tem turns. There was another difference between
the hand-coded and trained policies that could have
affected both the success rate and the number of
system turns: for interactions in which a user has
not ordered yet, nor been asked for their order, the
hand-coded strategy randomly chooses between
asking the user for their order and doing nothing,
letting the user take the initiative to place the order,
whereas the trained policy always asks the user for
their order (this action has the highest Q-value, al-
though in fact the value for doing nothing in such
cases is also relatively high).
We also carried out a stepwise multiple linear
regression on the data from the user experiment
to determine which of the objective measures had
the largest effect, as suggested by the PARADISE
evaluation framework (Walker et al, 2000). The re-
sulting regression functions are shown in Figure 5.
In summary, all of the subjective responses were
significantly affected by the objective task success
(i.e., the number of drinks served); the number of
low-ASR turns also affected most of the responses,
while various measures of dialogue efficiency (such
as the system response time and the time taken to
serve drinks) also had a significant impact. In gen-
eral, these regression functions explain between
15?25% of the variance in the subjective measures.
As an initial analysis of the validity of the sim-
ulated environment, we compared the state distri-
bution of the simulated data accumulated during
policy optimisation with that of the human user
evaluation data. In terms of coverage, we found
that only 46% of all states encountered in the real
data were also encountered during training. How-
ever, many of these states do not occur very often
and many of them do not require any action by
the robot (a trained policy can easily be set to take
no-action for unseen states). If we only include
states that have been encountered at least 20 times,
the coverage increases to over 70%. For states en-
countered at least 58 times, the coverage is 100%,
though admittedly this covers only the 10 most
frequently encountered states. The similarity of
the two distributions can be quantified by comput-
ing the KL-divergence, but since such a number is
228
Figure 3: Learning curve for joint optimisation of SSE-MDP policies.
System NS PSucc* PAtt PUnd PNat POv NDSrvd NST NBAsr
SSE-TRA 29 97% 4.10 4.21 3.00 3.83 1.97 (98.5%) 7.38 3.14
SSE-HDC 29 79% 4.14 3.83 2.93 3.83 1.76 (88.0%) 6.86 3.82
TOTAL 58 88% 4.12 4.02 2.97 3.83 1.86 (93.0%) 7.12 3.48
Table 5: Overview of system performance results from the experiment. In the leftmost column SSE-TRA
and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS indicates the number of
sessions; the columns PSucc (perceived success), PAtt (perceived attention recognition), PUnd (perceived
understanding), PNat (perceived naturalness), and POv (perceived overall performance) give average
scores resulting from the 5 respective questionnaire questions; NDSrvd indicates the average number of
drinks served per session (out of 2 maximum ? the percentage is given in brackets); NST indicates the
average number of system turns per session; while NBAsr indicates the average number of cases where
the user speech was ignored because the ASR confidence was below a predefined threshold. The marked
column indicates that the difference between the two SSE versions was significant at the p < 0.05 level.
hard to interpret in itself, this will only be useful
if there were a state distribution from an alterna-
tive simulator or an improved version of MUSE for
comparison.
8 Conclusion
In this paper we presented a new approach to au-
tomatic learning of strategies for social multi-user
human-robot interaction, demonstrated using the
example of a robot bartender that tracks multiple
customers, takes their orders, and serves drinks.
We presented a model consisting of a Social State
Recogniser (SSR) which processes audio-visual in-
put and maintains a model of the social state, and
a Social Skills Executor (SSE) which takes social
state updates from the SSR as input and generates
robot responses as output. The main contribution
of this work has been a new MDP-based model
for the SSE, incorporating two connected MDPs
with action selection policies that are jointly op-
timised in interaction with a Multi-User Simula-
tion Environment (MUSE). In addition to showing
promising evaluation results with simulated data,
we also presented results from a first evaluation of
the SSE component with human users. The experi-
ments showed that the integrated SSE component
worked quite well, and that the trained SSE-MDP
achieved higher subjective and objective success
rates (+18% and +10.5% respectively).
Our model currently only utilises two policies,
but in more complex scenarios the task could be
further modularised and extended by introducing
more MDPs, for example for multimodal fission
and natural language generation. The approach of
using a hierarchy of MDPs has some similarity with
the Hierarchical Reinforcement Learning (HRL)
approach which uses a hierarchy of Semi-Markov
Decision Processes (SMDPs). In (Cuaya?huitl et al,
229
PSucc = 0.88 + 0.14 ? N(NDSrvd) ? 0.07 ? N(NBAsr) (r2 = 0.21)
PAtt = 4.12 + 0.76 ? N(NDSrvd) ? 0.46 ? N(RTm) ? 0.38 ? N(FDTm) (r2 = 0.22)
PUnd = 4.02 + 0.41 ? N(NDSrvd) ? 0.36 ? N(NBAsr) ? 0.40 ? N(NST) ? 0.41 ? N(RTm) ? 0.39 ? N(STm) (r2 = 0.24)
PNat = 2.97 + 0.36 ? N(NDSrvd) ? 0.29 ? N(NBAsr) ? 0.31 ? N(NST) ? 0.44 ? N(RTm) (r2 = 0.16)
POv = 3.83 + 0.65 ? N(NDSrvd) ? 0.38 ? N(NBAsr) ? 0.52 ? N(RTm) (r2 = 0.24)
Figure 5: PARADISE regression functions from the user study. The labels are the same as those in Table 5,
with the following additions: RTm is the mean system response time per user, STm is the mean serving
time per user, and FDTm is the mean time to serve the first drink; all times are measured in milliseconds.
N represents a Z score normalisation function (Cohen, 1995).
2012) for example, this hierarchy is motivated by
the identification of multiple tasks that the robot
can carry out and for which multiple SMDP agents
are defined. In every step of the interaction, control
lies with a single SMDP agent somewhere in the
hierarchy; once it arrives at its final state it returns
control to its parent SMDP. An additional transi-
tion model is introduced to permit switching from
an incomplete SMDP to another SMDP at the same
level, making interactions more flexible. In our ap-
proach, control always starts at the top level MDP
and lower level MDPs are triggered depending on
the action taken by their parent MDP. For social
interaction with multiple users, flexible switching
between interactions with different users is impor-
tant, so an arguably more sophisticated HRL ap-
proach to multi-user interaction will rely heavily
on the transition model. Another approach to mod-
ularising the task domain through multiple policies
is described in (Lison, 2011), where ?meta-control?
of the policies relies on an activation vector. As in
the HRL SMDP approach, this approach has not
been applied in the context of multi-user interaction.
In any case, a more thorough and possibly experi-
mental analysis comparing our approach with these
other approaches would be worth investigating.
In the future, we plan to extend our MDP model
to a POMDP (Partially Observable MDP) model,
taking uncertainty about both speech and visual
input into account in the optimisation of SSE poli-
cies by incorporating alternative hypotheses and
confidence scores provided by the input modules
into the social state. Since hand-coding strategies
becomes more challenging in the face of increased
uncertainty due to noisy input, the appeal of auto-
matic strategy learning in a POMDP framework
becomes even stronger. In a previous offline ver-
sion of our combined SSR and SSE, we have shown
in preliminary simulation experiments that even in
an MDP setting, an automatically trained SSE pol-
icy outperforms a hand-coded policy when noise is
added to the speech channel (Keizer et al, 2013).
Another direction of research is to annotate the
data collected in the described experiment for fur-
ther analysis and use it to improve the features of
the simulated environment. The improved models
should lead to trained policies that perform better
when evaluated again with human users. We will
also make use of the findings of the PARADISE
regression to fine-tune the reward function used
for policy optimisation: note that two of the main
features indicated by the PARADISE procedure?
task success and dialogue efficiency?are already
those included in the current reward function, and
we will add a feature to account for the effects of
ASR performance. We are also considering using
collected data for direct supervised or off-policy
reinforcement learning of SSE strategies.
Finally, we aim to extend our domain both in
terms of interactive capabilities (e.g., handling com-
munication problems, social obligations manage-
ment, turn-taking) and task domain (e.g., handling
more than the current maximum of 2 users, group
orders, orders with multiple items). In order to
make the (PO)MDP model more scalable and thus
keeping the learning algorithms tractable, we also
aim to incorporate techniques such as value func-
tion approximation into our model.
Acknowledgments
The research leading to these results has received
funding from the European Union?s Seventh Frame-
work Programme (FP7/2007?2013) under grant
agreement no. 270435, JAMES: Joint Action for
Multimodal Embodied Social Systems, http://
james-project.eu/. Thanks to Ingmar Kessler
for help in running the user experiment.
230
References
Dan Bohus and Eric Horvitz. 2009. Learning to pre-
dict engagement with a spoken dialog system in
open-world settings. In Proceedings SIGdial, Lon-
don, UK.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn
taking in situated dialog: Study, lessons, and direc-
tions. In Proceedings SIGdial, Portland, OR.
A. Brooks, J. Gray, G. Hoffman, A. Lockerd, H. Lee,
and C. Breazeal. 2012. Robot?s play: Interactive
games with sociable machines. Computers in Enter-
tainment, 2(3).
H. Bunt, J. Alexandersson, J. Carletta, J.-W. Choe, A.C.
Fang, K. Hasida, K. Lee, V. Petukhova, A. Popescu-
Belis, L. Romary, C. Soria, and D. Traum. 2010.
Towards an ISO standard for dialogue act annotation.
In Proceedings LREC, Valletta, Malta.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Boston.
Heriberto Cuaya?huitl and Ivana Kruijff-Korbayova?.
2012. An interactive humanoid robot exhibiting flex-
ible sub-dialogues. In Proceedings NAACL HLT,
Montreal, Canada.
H. Cuaya?huitl, I. Kruijff-Korbayova?, and N. Dethlefs.
2012. Hierarchical dialogue policy learning using
flexible state transitions and linear function approxi-
mation. In Proceedings COLING, Mumbai, India.
Juan Fasola and Maja J. Mataric. 2013. A socially as-
sistive robot exercise coach for the elderly. Journal
of Human Robot Interaction, 2(3). To appear.
Mary Ellen Foster, Andre Gaschler, Manuel Giuliani,
Amy Isard, Maria Pateraki, and Ronald P. A. Pet-
rick. 2012. Two people walk into a bar: Dynamic
multi-party social interaction with a robot agent. In
Proceedings ICMI, Santa Monica, CA.
Mary Ellen Foster. 2013. How can I help you?
Comparing engagement classification strategies for
a robot bartender. Submitted.
K. Huth, S. Loth, and J.P. De Ruiter. 2012. Insights
from the bar: A model of interaction. In Proceedings
of Formal and Computational Approaches to Multi-
modal Communication.
Simon Keizer, Mary Ellen Foster, Zhuoran Wang, and
Oliver Lemon. 2013. Machine learning of social
states and skills for multi-party human-robot inter-
action. Submitted.
David Klotz, Johannes Wienke, Julia Peltason, Britta
Wrede, Sebastian Wrede, Vasil Khalidov, and Jean-
Marc Odobez. 2011. Engagement-based multi-
party dialog with a humanoid robot. In Proceedings
SIGdial, Portland, OR.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-driven Methods for Adaptive Spoken Dialogue
Systems: Computational Learning for Conversa-
tional Interfaces. Springer.
Pierre Lison. 2011. Multi-policy dialogue manage-
ment. In Proceedings SIGdial, Portland, OR.
Maria Pateraki, Markos Sigalas, Georgios Chliveros,
and Panos Trahanias. 2013. Visual human-robot
communication in social settings. In the Work-
shop on Semantics, Identification and Control of
Robot-Human-Environment Interaction, held within
the IEEE International Conference on Robotics and
Automation (ICRA).
Ronald P. A. Petrick and Mary Ellen Foster. 2013.
Planning for social interaction in a robot bartender
domain. In Proceedings ICAPS, Rome, Italy.
Ronald P. A. Petrick, Mary Ellen Foster, and Amy Isard.
2012. Social state recognition and knowledge-level
planning for human-robot interaction in a bartender
domain. In AAAI 2012 Workshop on Grounding Lan-
guage for Physical Systems, Toronto, ON, Canada,
July.
Verena Rieser and Oliver Lemon. 2011. Rein-
forcement Learning for Adaptive Dialogue Systems.
Springer.
R. Stiefelhagen, H. Ekenel, C. Fu?gen, P. Gieselmann,
H. Holzapfel, F. Kraft, K. Nickel, M. Voit, and
A. Waibel. 2007. Enabling multimodal human-
robot interaction for the Karlsruhe humanoid robot.
IEEE Transactions on Robotics, 23(5):840?851.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
L. Pfeifer Vardoulakis, L. Ring, B. Barry, C. Sidner,
and T. Bickmore. 2012. Designing relational agents
as long term social companions for older adults. In
Proceedings IVA, Santa Cruz, CA.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Blaise Thomson, and Kai Yu. 2010. The
Hidden Information State model: a practical frame-
work for POMDP based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
Steve Young, M. Gas?ic?, B. Thomson, and J. Williams.
2013. POMDP-based statistical spoken dialogue
systems: a review. Proceedings of the IEEE. To
appear.
231
Appendix A: Example session with two guests ordering a drink
Timestamp Level 1 MDP Level 2 MDP DescriptionState features Action State features Action
13:28:45:966 0 1 0 0 0 0 0 0 0 - - A1 visible, but not close to bar; no response
generated yet.
13:28:48:029 1 2 0 0 0 0 0 0 2 - - A1 not close to bar and seeking attention: BT
acknowledges this and engages with A1.
13:28:53:680 3 2 0 0 1 2 0 0 4 - - A2 visible, close to the bar, and seeking atten-
tion; BT is already engaged with A1 and there-
fore asks A2 to wait.
13:28:55:715 3 2 0 0 1 2 0 1 3 0 0 0 1 BT continues his interaction with A1 and asks
for their order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 BT continues with A1 and waits for them to
order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 Same as above: BT still waiting for A1?s order.
. . . Due to repeated ASR failures, this state action
pair is encountered several times.
13:29:52:066 3 2 0 0 1 2 0 1 3 0 1 0 2 A1?s has now been successfully recognised; BT
serves the ordered drink to A1.
13:30:12:013 3 2 1 0 1 2 0 1 5 - - A2 still seeking attention; BT can now acknowl-
edge this and engage with A1.
13:30:13:307 1 2 1 0 3 2 0 1 6 0 0 0 1 BT continues with A2 and asks for their order.
13:30:14:475 1 2 1 0 3 2 0 0 6 0 6 0 0 BT continues with A2 and waits for them to
order
13:30:17:737 1 2 1 0 3 2 0 0 6 0 1 0 2 A2?s recognised; BT serves ordered drink to A2.
13:30:37:623 1 2 1 0 3 2 1 0 0 - - Both A1 and A2 have been served; BT does
nothing
13:30:41:440 1 2 1 0 3 2 1 0 0 - - Same as above.
. . .
Table 6: SSE-MDP trajectory for one session from the evaluation data, showing the states and response
actions taken for both MDPs. The states are represented via their value indices, corresponding to Tables 1
and 3; the action indices similarly correspond to the actions in Tables 2 and 4. In the descriptions, A1 and
A2 refer to the first and second user detected; BT refers to the bartender.
232
