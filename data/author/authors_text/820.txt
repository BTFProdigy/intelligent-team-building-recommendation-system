 
Spoken Dialogue for Simulation Control and Conversational Tutoring 
 
Elizabeth Owen Bratt Karl Schultz Brady Clark 
CSLI,  
Stanford University, 
Stanford, CA 94305 
CSLI,  
Stanford University, 
Stanford, CA 94305 
CSLI,  
Stanford University, 
Stanford, CA 94305 
ebratt@csli.stanford.edu schultzk@csli.stanford.edu bzack@csli.stanford.edu 
 
  
 
 
Abstract 
 
 
This demonstration shows a flexible tutoring 
system for studying the effects of different 
tutoring strategies enhanced by a spoken 
language interface.  The hypothesis is that 
spoken language increases the effectiveness 
of automated tutoring.  The domain is Navy 
damage control. 
1 Technical Content 
 
This demonstration shows a flexible tutoring 
system for studying the effects of different tutoring 
strategies enhanced by a spoken language interface.  
The hypothesis is that spoken language increases the 
effectiveness of automated tutoring.  Our focus is on 
the SCoT-DC spoken language tutor for Navy 
damage control; however,  because SCoT-DC 
performs reflective tutoring on DC-Train simulator 
sessions, we have also developed a speech interface 
for the existing DC-Train damage control simulator, 
to promote ease of use as well as consistency of 
interface. 
Our tutor is developed within the Architecture for 
Conversational Intelligence (Lemon et al 2001).  We 
use the Open Agent Architecture (Martin et al 1999) 
for communication between agents based on the 
Nuance speech recognizer, the Gemini natural 
language system (Dowding et al 1993), and Festival 
speech synthesis. Our tutor adds its own dialogue 
manager agent, for general principles of 
conversational intelligence, and a tutor agent, which 
uses tutoring strategies and tactics to plan out an 
appropriate review and react to the student's answers 
to questions and desired topics. 
  
The SCoT-DC tutor, in Socratic style, asks 
questions rather than giving explanations.  The tutor 
has a repertoire of hinting tactics to deploy in 
response to student answers to questions, and 
identifies and iscusses repeated mistakes.  The 
student is able to ask "why" questions after certain 
tutor explanations, and  to alter the tutorial plan by 
requesting that the tutor skip discussion of certain 
topics. In DC-Train, the system uses several windows 
to provide information graphically, in addition to the 
spoken messages.  In SCoT-DC, the Ship Display 
from DC-Train is used for both multimodal input and 
output.  
 
Both DC-Train and SCoT-DC use the same 
overall Gemini grammar, with distinct top-level 
grammars producing appropriate subsets for each 
application. Our Gemini grammar currently has 166 
grammar rules and 811 distinct words.  In a Nuance 
language model compiled from the Gemini grammar 
(Moore 1998), different top-level grammars are used 
in SCoT-DC to enhance speech recognition based on 
expected answers. 
 
2 Performance Assessment 
 
Experiments to assess the effectiveness of SCoT-
DC tutoring are underway in March 2004, with 15 
subjects currently scheduled.  In July 2003, students 
in the Repair Locker Head class at the Navy Fleet 
Training Center in San Diego ran 12 sessions with 
DC-Train. Sessions ranged from 1 to 65 user 
utterances, with an average of  21.  The average 
utterance length was 7 words. In speech recognition, 
about 22% of utterances were rejected, and the
sentences with a recognition hypothesis had a word 
error rate of 27%.  The transcribed data, combined 
with developer test run data, gave us 327 unique out-
of-grammar sentences.  Of these, we found 79 
examples where the automatic Nuance endpointing 
cut off an utterance too early, and 20 examples of 
disfluent speech.  118 sentences were determined to 
be potentially useful phrasings to add to the grammar, 
while 73 sentences were found to lie outside the 
scope of the application. 
 
To address these issues, w have added new 
phrasings to the grammar.  We also intend to use 
Nuance?s Listen & Learn offline grammar adaptation 
tool, to give higher probabilities to likely sentences 
while retaining broad grammar-based coverage.  We 
may also adjust endpointing time, based on partial 
speech recognition hypothesis, to give extra time to 
the kinds of sentences typically occurring with more 
internal pauses. Disfluencies may decrease as users 
become more familiar with DC-Train and SCoT-DC 
during the comparatively longer use xpected from 
each user in a typical tutoring session
The graphical interface for the DC-Train 
simulator is shown in Figure 1. 
 
 
 
Figure 1: DC-Train simulator GUI 
 
Each window on the screen is modeled on a 
source of information available to a real-lifeDCA on 
a ship, including as a detailed drawing of the several 
hundred compartments on the ship, a record of all 
communications to and from the DCA, a hazard 
detection panel showing the locations of alarms 
which have occurred, and a panel showing the 
firemain, i.e. the pipes carrying water throughout the 
ship, and the valves and pumps controlling the flow 
of the water.  The window depicting heads represents 
the other personnel in the same room as the DCA, 
who are available to receive and transmit messages. 
 While in the original version of DC-Train, 
the DCA?s orders and communications to other 
personnel on the ship took place through a menu 
system, this demo presents the newer spoken 
dialogue interface.  Spoken commands take the form 
of actual Navy commands, thus enabling the Navy 
student to train in the same manner as they would 
perform these duties through radio communications 
on a ship. 
The user clicks a button to begin speaking, 
and the speech is recognized by Nuance, using a 
grammar-based language model automatically 
derived from the Gemini grammar used for parsing 
and interpretation of the commands.  A dialogue 
manager then maps the Gemini logical forms into 
DC-Train commands.  To allow the student to 
monitor the success of the speech recognizer, the text 
of the utterance is displayed. Responses from the 
simulated personnel are spoken by Festival speech 
synthesis, and also displayed as text on the screen. 
Most spoken interactions with DC-Train 
involve the student DCA giving single commands 
without any use of dialogue structure; however, the 
system will query the student for missing  required 
parameters of commmands, such as the repair team 
who is to perform the action, or the number of  the 
pump to start on the firemain.  If the student does not 
respond to these queries, the system will provide the 
context of the command missing the parameter as 
part of a more informative request.  The student 
retains the ability to issue other commands at this 
time, and need not respond to the system if there is a 
more pressing crisis elsewhere. 
At the end of a DC-Train session, the 
student can then receive customized feedback and 
tutoring from SCoT-DC, based on a record of the 
student?s actions compared to what an expert DCA 
would have done at each point, based on  rules 
accounting for the state of the simulation. The goal of 
the tutorial interaction is to identify and remediate 
any gaps in the student?s understanding of damage 
control doctrine, and to improve the student?s 
performance in issuing the correct commands without 
hesitation.   
The graphical interface to the SCoT-DC 
tutor is shown in Figure 2. 
  
 
 
Figure 2: ScoT-DC tutor GUI 
 
SCoT-DC uses two instances of the Ship Display 
from DC-Train, seen in Figure 3, one to give an 
overall view of the ship and one to zoom in on 
affected compartments, with color indicating the type 
of crisis in a compartment and the state of damage 
control there.   The student can click on a 
compartment in the Ship Display as a way of 
indicating that compartment to the system. The 
automated tutor and the student communicate 
through speech, while the lower window displays the 
text of both sides of the interaction, and permits the 
user to scroll back through the entire tutorial session.   
 
Figure 3: Highlighted Compartment  
 
The tutor can also display bulkheads used to set 
boundaries for firefighting, as in Figure 4. 
 
Figure 4:  Highlighted Bulkhead Walls 
 
A third kind of graphical information that the 
tutor may convey to the student involves regions of 
jurisdiction for repair  teams, shown in Figure 5.
 
Figure 5: Repair Team Jurisdiction Regions 
As in DC-Train, the student clicks to begin 
speaking, then Nuance speech recognition provides a 
string of words to be interpreted by a Gemini 
grammar.  Also as in DC-Train, responses from the 
tutor are  synthesized by Festival, although the tutor 
speaks with a more natural voice provided by 
FestVox limited domain synthesis, in which large 
units of the tutor?s utterances may be taken from 
prompts recorded for this application.   
Interpretation of the Gemini interpreted 
forms is handled by a more complex dialogue 
manager in SCoT-DC than in DC-Train, with a 
structured representation of the dialogue, which is 
used to guide the system?s use of discourse markers, 
among other things.  The dialogue is mainly driven 
by the tutor agent?s strategies, though the student can 
request to move on to future topics without 
completing the current discussion, and also ask a 
?Why? question after some explanations. 
 Tutorial strategies generally guide the 
overall path of the conversation, such as choosing 
which crises to discuss based on the errors made by 
the student. Tutorial tactics apply at a lower-level 
throughout the dialogue, for example, when a student 
gives an incorrect answer, the tutor will give a 
general hint and repos the question.  If the student 
answers incorrectly a second time the tutor will give 
a more specific hint and ask the question again.  If 
the student fails a third time the tutor will give the 
correct answer, and proceed. 
 Running a  full DC-Train scenario takes 20-
40 minutes, and has the flavor of the following 
excerpt: 
 
 [buzzing alarm goes off, it is a fire alarm] 
 
DCCO: Fire in compartment 2-78-01-L. 
Student: Net80 to repair locker 2, investigate 
compartment 2-78-01-L. 
Repair Locker 2: Reports, fire in compartment 2-78-
01-L. 
Repair Locker 2: Reports, smoke in compartment 2-
78-01-L. 
Student: Net80 to repair locker 2, fight the fire in 
compartment 2-78-01-L. 
Student: Net80 to repair locker 2, set fire and smoke 
boundaries on primary forward 78, primary aft 126, 
secondary forward 42, secondary aft 174, above 1, 
below 2. 
 
A reflective dialogue with the tutor will takes 
around 10 minutes.  The following gives a sample of 
the kind of tutorial interaction.  
 
Tutor: Hello, we are about to review your session 
from earlier today. 
Tutor: All right, are you ready? 
Student: Yes. 
Tutor: In this session there were 5 primary crises and 
1 secondary crisis. 
Tutor: 5 of the 6 crises were resolved. 
Tutor: Most of your actions were correct, however 
you made several mistakes. 
Tutor: OK, let?s discuss a few of those crises.
Tutor: The first crisis was a fire here in Crew Living 
space No. 1. [centers on this compartment] 
Tutor: This fire was not contained early enough.
Tutor: As a result, the fire spread to the adjacent 
Combat sytem equipment room No. 2. [highlights 
2nd compartment] 
Tutor: OK, let?s review how to handle this crisis. 
[removes highlight from 2nd compartment] 
Tutor: What?s the first thing you should do when the 
fire alarm sounds? 
Student: Send repair two to investigate. 
Tutor: Yes, that?s right. 
Tutor: However, you failed to do this during the 
session. 
Tutor: This is a mistake you made more than once. 
Tutor: Remember, alarms need to be verified first. 
Tutor: OK, suppose that there is a fire. 
Tutor: What?s the next step? 
Student: Set fire and smoke boundaries. 
Tutor: Yes. That?s what you did during the session.
Tutor: OK, what else needs to be done? 
Student: The repair team should fight the fire. 
Tutor: No, you missed a step. 
Tutor: Remember, the crisis might interfere with 
other ship systems. 
Tutor: Let?s try again. 
Tutor: What else needs to be done?
Student: I don?t know. 
Tutor: Well. 
Tutor: You should order repair five to mechanically 
and electrically isolate the compartment. 
 
 
A video clip of an older version of the ScoT-DC 
system is available at http://www-
csli.stanford.edu/semlab/muri/November2002Demo.h
tml 
 
3 Acknowledgements 
 
This work is supported by the Department of the 
Navy under research grant N000140010660, a 
multidisciplinary university research initiative on 
natural language interaction with intelligent tutoring 
systems. 
 
3.1 References  
 
A. Black and K. Lenzo, 1999. Building Voices in the 
Festival Speech Synthesis System (DRAFT) 
Available at 
http://www.cstr.ed.ac.uk/projects/festival/papers.h
tml. 
A. Black and P. Taylor. 1997. Festival speech 
synthesis system: system documentation (1.1.1). 
Technical Report Technical Report HCRC/TR-
83, University of Edinburgh Human 
Communication Research Centre. 
V. V. Bulitko and D. C. Wilkins. 1999. Automated 
instructor assistant for ship damage control. In  
Proceedings of AAAI-99. 
J. Dowding, M. Gawron, D. Appelt, L. Cherny, R. 
Moore, and D. Moran. 1993. Gemini: A natural 
language system for spoken language 
understanding.  In Procdgs of ACL 31. 
Oliver Lemon, Alexander Gruenstein, and Stanley 
Peters. 2002. Collaborative Activities and Multi-
tasking in Dialogue Systems , Traitement 
Automatique des Langues (TAL), 43(2):131- 54, 
special issue on dialogue. 
D. Martin, A. Cheyer, and D. Moran. 1999. ``The 
open agent architecture: A framework for 
building distributed software systems,'' Applied 
Artificial Intelligence, v.13:91-128. 
Robert C. Moore. 1998. Using Natural Language 
Knowledge Sources in Speech Recognition." 
Proceedings of the NATO Advanced Study 
Institute. 
Karl Schultz, Elizabeth Owen Bratt, Brady Clark, 
Stanley Peters, Heather Pon-Barry, and Pucktada 
Treeratpituk. 2003. A Scalable, Reusable Spoken 
Conversational Tutor: SCoT. In AIED 2003 
Supplementary Procdgs. (V. Aleven et aleds). 
Univ. of Sydney. 367-377.  
 
 
Compi l ing  Language Mode ls  f rom a L ingu is t i ca l ly  Mot ivated  
Un i f i ca t ion  Grammar  
Manny Rayner t>, Beth Ann Hockey t, Frankie James t 
Elizabeth Owen Bratt ++, Sharon Goldwater ++ and Jean Mark Gawron ~ 
tResea.rch Inst i tute for 
Advanced Computer  Science 
Mail Stop 19-39 
NASA Ames Research Center 
Moffett Field, CA 94035-1000 
Abstract 
Systems now exist which are able to con:pile 
unification gralmnars into language models that 
can be included in a speech recognizer, but it 
is so far unclear whether non-trivial linguisti- 
cally principled gralnlnars can be used for this 
purpose. We describe a series of experiments 
which investigate the question empirica.lly, by 
incrementally constructing a grammar and dis- 
covering what prot)lems emerge when succes- 
sively larger versions are compiled into finite 
state graph representations and used as lan- 
guage models for a medium-vocabulary recog- 
nition task. 
1 Introduction ~ 
Construction of speech recognizers for n:ediuln- 
vocabulary dialogue tasks has now becolne an 
important I)ractical problem. The central task 
is usually building a suitable language model, 
and a number of standard methodologies have 
become established. Broadly speaking, these 
fall into two main classes. One approach is 
to obtain or create a domain corpus, and froln 
it induce a statistical anguage model, usually 
some kind of N-gram grammar; the alternative 
is to manually design a grammar which specifies 
the utterances the recognizer will accept. There 
are many theoretical reasons to prefer the first 
course if it is feasible, but in practice there is of- 
ten no choice. Unless a substantial domain cor- 
pus is available, the only method that stands a 
chance of working is hand-construction f an ex- 
i The majority of the research reported was performed 
at I{IACS under NASA Cooperative Agreement~ Number 
NCC 2-1006. The research described in Section 3 was 
supported by the Defense Advanced Research Projects 
Agency under Con~racl~ N66001-94 C-6046 with the 
Naval Command, Control, and Ocean Surveillance Cen- 
ter. 
SRI International  
333 Ravenswood Ave 
Menlo Park, CA 94025 
*netdecisions 
Well ington House 
East Road 
Cambr idge CB1 1BH 
England 
plicit grammar based on the grammar-writer's 
intuitions. 
If the application is simple enough, experi- 
ence shows that good grammars of this kind 
can be constructed quickly and efficiently using 
commercially available products like ViaVoice 
SDK (IBM 1999) or the Nuance Toolkit (Nu- 
ance 1999). Systems of this kind typically al- 
low specification of some restricted subset of the 
class of context-free grammars, together with 
annotations that permit the grammar-writer to
associate selnantic values with lexical entries 
and rules. This kind of framework is fl:lly ad- 
equate for small grammars. As the gran:mars 
increase in size, however, the limited expres- 
sive power of context-free language notation be- 
conies increasingly burdensome. The grainn:a,r 
tends to beconie large and unwieldy, with many 
rules appearing in multiple versions that con- 
stantly need to be kept in step with each other. 
It represents a large developn:ent cost, is hard 
to maintain, and does not usually port well to 
new applications. 
It is tempting to consider the option of mov- 
ing towards a :::ore expressive grammar tbrmal- 
isln, like unification gramnm.r, writing the orig- 
inal grammar in unification grammar form and 
coml)iling it down to the context-free notation 
required by the underlying toolkit. At least 
one such system (Gemilfi; (Moore ct al 1997)) 
has been implemented and used to build suc- 
cessful and non-trivial applications, most no- 
tably ComnmndTalk (Stent ct al 1999). Gem- 
ini accepts a slightly constrained version of the 
unification grammar formalism originally used 
in the Core Language Engine (Alshawi 1992), 
and compiles it into context-free gran:nmrs in 
the GSL formalism supported by the Nuance 
Toolkit. The Nuance Toolkit con:piles GSL 
gran:mars into sets of probabilistic finite state 
670 
gra.phs (PFSGs), which form the final bmguage 
model. 
The relative success of the Gemilfi system 
suggests a new question. Ulfification grammars 
ha.re been used many times to build substantial 
general gramlnars tbr English and other na.tu- 
ra\[ languages, but the language model oriented 
gra.mln~rs o far developed fi)r Gemini (includ- 
ing the one for ColnmandTalk) have a.ll been 
domain-sl)ecific. One naturally wonders how 
feasible it is to take yet another step in the di- 
rection of increased genera.lity; roughly, what 
we want to do is start with a completely gen- 
eral, linguistically motivated gramma.r, combine 
it with a domain-specific lexicon, and compile 
the result down to a domain-specitic context- 
free grammar that can be used as a la.nguage 
model. If this 1)tetra.mine can be rea.lized, it is 
easy to believe that the result would 1)e a.n ex- 
tremely useful methodology tbr rapid construc- 
tion of la.nguage models. It is i lnportant o note 
tha.t there are no obvious theoretical obstacles 
in our way. The clailn that English is context- 
free has been respectable since a.t least the early 
8(Is (Pullum and Gazda.r 1982) 'e, and the idea. 
of using unification grammar as a. compact wa 5, 
of tel)resenting an ulMerlying context-fl'e~e, lan- 
guage is one of the main inotivations for GPSG 
(Gazdar et al1985) and other formalislns based 
on it. The real question is whether the goal is 
practically achievable, given the resource limi- 
tations of current technology. 
In this l)a.1)er, we describe work aimed at the 
target outlined above, in which we used the 
Gemini system (described in more detail in Sec- 
tion 2) to a.ttempt o compile a. va.riety of lin- 
guistically principled unification gralnlna.rs into 
la.ngua.ge lnodels. Our first experiments (Sec- 
tion 3) were pertbrmed on a. large pre-existing 
unification gramlna.r. These were unsuccessful, 
for reasons that were not entirely obvious; in 
order to investigate the prol)lem more system- 
atically, we then conducted a second series of 
experilnents (Section 4), in which we increlnen- 
tally 1)uilt up a smMler gra.lnlna.r. By monitor- 
ing; the behavior of the compilation process and 
the resulting langua.ge model as the gra.lmnar~s 
2~1e m'e aware l, hal, this claim is most~ 1)robably not 
l;rue for natural languages ill gelmraI (lh'csnall cl al 
1987), but furl~hcr discussion of t.his point is beyond I.he 
scope of t, llC paper. 
cover~ge was expanded, we were a.ble to iden- 
tit~ the point a,t which serious problems began 
to emerge (Section 5). In the fina.1 section, we 
summarize and suggest fltrther directions. 
2 Tile Genfini Language Model  
Compi le r  
To lnake the paper nlore self-contained, this sec- 
tion provides some background on the method 
used by Gemini to compile unifica.tion grain- 
mars into CFGs, and then into language mod- 
els. The ha.sic idea. is the obvious one: enu- 
mera.te all possible instantiations of the feal;ures 
in the grammar rules and lexicon entries, and 
thus tra.nsform esch rule and entry in the ()rig- 
inal unification grammar into a set of rules in 
the derived CFG. For this to be possible, the 
relevant fe~ttul'es Inust be constrained so that 
they can only take values in a finite predefined 
range. The finite range restriction is inconve- 
nient for fea.tures used to build semantic repre- 
sentations, and the tbrmalism consequently dis- 
tinguishes syntactic and semantic features; se- 
lmmtic features axe discarded a.t the start of the 
compilation process. 
A naive iml)lelnentation of the basic lnethod 
would be iml)raetical for any but the small- 
est a.nd simplest grammars, and considera.ble 
ingemfity has been expended on various opti- 
mizations. Most importantly, categories axe ex- 
panded in a demand-driven fa.shion, with infer  
lnatiotl being percolated 1)oth t)otton>up (from 
the lexicon) and top-down (fl'om the grammar's 
start symbol). This is done in such a. way 
that potentially valid colnl)inations of feature 
instantiations in rules are successively filtered 
out if they are not licensed by the top-down 
and bottom-ul) constra.ints. Ranges of feature 
values are also kept together when possible, so 
that sets of context-free rules produced by the 
mdve algorithm may in these cases be merged 
into single rules. 
By exploiting the structure of the gram- 
mar a.nd lexicon, the demand-driven expansion 
lnethod can often effect substa.ntial reductions 
in the size of the derived CFG. (For the type 
of grammar we consider in this paper, the re- 
duction is typically by ,~ fa.etor of over 102?). 
The downside is that even an app~trently slnall 
cha.nge in the syntactic t>atures associated with 
a. rule may have a large eIfect on the size of 
671 
the CFG, if it opens up or blocks an impor- 
tant percolation path. Adding or deleting lexi- 
con entries can also have a significant effect on 
the size of the CFG, especially when there are 
only a small number of entries in a given gram- 
matical category; as usual, entries of this type 
behave from a software ngineering standpoint 
like grammar ules. 
The language model compiler also performs 
a number of other non-trivial transformations. 
The most important of these is related to the 
fact that Nuance GSL grammars are not al- 
lowed to contain left-recursive rules, and left- 
recursive unification-grammar rules must con- 
sequently be converted into a non-left-recursive 
fort::. Rules of this type do not however occur 
in the gramlnars described below, and we conse- 
quently omit further description of the method. 
3 Initial Experiments 
Our initial experiments were performed on a 
recent unification grammar in the ATIS (Air 
Travel Information System) domain, developed 
as a linguistically principled grammar with a 
domain-specific lexicon. This grammar was 
cre~ted for an experiment COl::t)aring cover- 
age and recognition performance of a hand- 
written grammar with that of a.uto:::atically de- 
rived recognition language models, as increas- 
ing amounts of data from the ATIS corpus 
were made available for each n:ethod. Exam- 
ples of sentences covered by this gralnlnar are 
"yes", "on friday", "i want to fly from boston 
to denver on united airlines on friday septem- 
ber twenty third", "is the cheapest one way 
fare from boston to denver a morning flight", 
and "what flight leaves earliest from boston to 
san francisco with the longest layover in den- 
ver". Problems obtaining a working recognition 
grammar from the unification grammar ended 
our original experiment prematurely, and led 
us to investigate the factors responsible for the 
poor recognition performance. 
We explored several ikely causes of recogni- 
tion trouble: number of rules, ::umber of vocab- 
ulary items, size of node array, perplexity, and 
complexity of the grammar, measured by aver- 
age and highest number of transitions per graph 
in the PFSG form of the grammar. 
We were able to in:mediately rule out sim- 
ple size metrics as the cause of Nuance's diffi- 
culties with recognition. Our smallest air travel 
grammar had 141 Gemini rules and 1043 words, 
producing a Nuance grammar with 368 rules. 
This compares to the Con:mandTalk grammar, 
which had 1231 Gemini rules and 1771 words, 
producing a Nuance gran:n:ar with 4096 rules. 
To determine whether the number of the 
words in the grammar or the structure of 
the phrases was responsible for the recognition 
problems, we created extreme cases of a Word+ 
grammar (i.e. a grammar that constrains the 
input to be any sequence of the words in the 
vocabulary) and a one-word-per-category gram- 
mar. We found that both of these variants 
of our gralmnar produced reasonable recogni- 
tion, though the Word+ grammar was very in- 
accurate. However, a three-words-per-category 
grammar could not produce snccessflfl speech 
recognition. 
Many thature specifications can lnake a gram- 
mar ::tore accurate, but will also result in a 
larger recognition grammar due to multiplica- 
tion of feature w~lues to derive the categories 
of the eontext-fl'ee grammar. We experimented 
with various techniques of selecting features to 
be retained in the recognition grammar. As de- 
scribed in the previous ection, Gemini's default 
method is to select only syntactic features and 
not consider semantic features in the recogni- 
tion grammar. We experimented with selecting 
a subset of syntactic features to apply and with 
applying only se:nantic sortal features, and no 
syntactic features. None of these grammars pro- 
duced successful speech recognition. 
/.Fro::: these experiments, we were unable to 
isolate any simple set of factors to explain which 
grammars would be problematic for speech 
recognition. However, the numbers of transi- 
tions per graph in a PFSG did seem suggestive 
of a factor. The ATIS grammar had a high of 
1184 transitions per graph, while the semantic 
grammar of CommandTalk had a high of 428 
transitions per graph, and produced very rea- 
sonable speech recognition. 
Still, at; the end of these attempts, it beca.me 
clear that we did not yet know the precise char- 
acteristic that makes a linguistically motivated 
grammar intractable for speech recognition, nor 
the best way to retain the advantages of the 
hand-written grammar approach while provid- 
ing reasonable speech recognition. 
672 
4 Incrementa l  Grammar  
Deve lopment  
In our second series of experiments, we in- 
crelnenta.lly developed a. new grammar front 
s('ra.tch. The new gra.mma.r is basica.lly a s('a.led- 
down and a.dapted version of tile Core Lan- 
guage Engine gramme\ for English (Puhnan 
1!)92; Rayner 1993); concrete development work 
a.nd testing were organized a.round a. speech in- 
terfa c(; to a. set; of functionalities oflhred by a 
simple simula,tion of the Space Shuttle (Rather, 
Hockey gll(l James 2000). Rules and lexical 
entries were added in sma.ll groups, typically 
2-3 rules or 5 10 lexical entries in one incre- 
ment. After each round of exl)a.nsion , we tested 
to make sure that the gramlnar could still 1)e 
compiled into a. usa.bh; recognizer, a.nd a.t sev- 
ere.1 points this suggested changes in our iln- 
1)\]ementation strategy. The rest of this section 
describes tile new grmmnar in nlore detail. 
4.1 Overv iew of  Ru les  
The current versions of the grammar and lexi- 
con contain 58 rules a.nd 30J. Ulfinflectesl entries 
respectively. They (:over the tbllowing phenom- 
el i  :~IZ 
1. Top-level utl;er~tnces: declarative clauses, 
WH-qtlestions, Y-N questions, iml)erat;ives, 
etlil)tical NPs and I)Ps, int(;rject.ions. 
~.. / \ ]  9 \,~ H-lnovement of NPs and PPs. 
3. The fbllowing verb types: intr~nsi- 
tive, silnple transitive, PP con:plen-mnt, 
lnodaJ/a.uxiliary, -ing VP con-q)len:ent, par- 
ticleq-NP complement, sentential comple- 
lnent, embedded question complement. 
4. PPs: simple PP, PP with postposition 
("ago")~ PP lnodifica,tion of VP and NP. 
5. Relat;ive clauses with both relative NP pro- 
1101111 ("tit(; telnperature th,tt I measured )
and relative PP ("the (loci: where I am"). 
6. Numeric determiners, time expressions, 
and postmodification of NP 1)y nun:eric ex- 
pressions. 
7. Constituent conjunction of NPs and 
cl~ulses. 
Tilt following examl)le sentences illustrate 
current covera,ge: 3 '-. , ':how ~d)out scenario 
three.?", "wha, t is the temperature?", "mea- 
sure the pressure a,t flight deck", "go to tile 
crew ha.tch a.nd (:lose it", "what were ten:per- 
a.tttt'e a, nd pressure a.t iifteen oh five?", "is the 
telnpera.ture going ttp'. ~', "do the fi?ed sensors 
sa.y tha.t the pressure is decreasing. , "find out 
when the pressure rea.ched fifteen p s i . . . .  wh~t 1 
is the pressure that you mea.sured?", "wha.t is 
the tempera.lure where you a.re?", ?~(:a.n you find 
out when the fixed sensors ay the temperature 
at flight deck reached thirty degrees celsius?". 
4.2 Unusua l  Features  o f  the  Grammar  
Most of the gramn:~u', as already sta.ted, is 
closely based on the Core Language Eng!ne 
gra.nlnla.r. \?e briefly sllnllna.rize the main di- 
vergences between the two gramnlars. 
4.2.1 I nvers ion  
The new gramlna, r uses a. novel trea.tment of 
inversion, which is p~trtly designed to simplify 
the l)l'ocess of compiling a, fea,ture gl'anllna, r into 
context-free form. The CLE grammar's trea.t- 
l l tent of invers ion uses a, movement account, in 
which the fronted verb is lnoved to its notional 
pla.ce in the VP through a feature. So, tbr 
example, the sentence "is pressure low?" will 
in the origina.1 CLE gramma.r ha.re the phrase- 
structure 
::\[\[iS\]l" \ [p ressure \ ]N / ,  \[\[\]V \[IO\V\]AI),\]\]V'\]'\],'g" 
in whk:h the head of th(, VP is a V gap coin- 
dexed with tile fronted main verb 1,~ . 
Our new gra.mn:ar, in contrast, hal:dles in- 
version without movement, by making the con> 
bination of inverted ver\]) and subject into a. 
VBAR constituent. A binary fea.ture invsubj  
picks o:ll; these VBARs, a.nd there is a. question- 
forma,tion rule of tilt form 
S --> VP : E invsub j=y\ ]  
Continuing the example, the new gram- 
mar a.ssigns this sentence tilt simpler phrase- 
structure 
"\[\[\[is\] v \[press:ire\] N*'\] v .A .  \[\[low\] J\] V.\] S" 
4.2.2 Sorta l  Const ra in ts  
Sortal constra,ints are coded into most gr~un:nnr 
rules as synta.ctic features in a straight-forward 
lna.nner, so they are available to the compilation 
673 
process which constructs the context-free gram- 
mar, ~nd ultimately tile language model. The 
current lexicon allows 11 possible sortal values 
tbr nouns, and 5 for PPs. 
We have taken the rather non-standard step 
of organizing tile rules for PP modification so 
that a VP or NP cannot be modified by two 
PPs  of the same sortal type. The principal mo- 
tivation is to tighten the language model with 
regard to prepositions, which tend to be pho- 
netically reduced and often hard to distinguish 
from other function words. For example, with- 
out this extra constraint we discovered that an 
utterance like 
measure temperature at flight deck 
and lower deck 
would frequently be misrecognized as 
measure temperature at flight deck in 
lower deck 
5 Exper iments  with Incremental  
G r am 111 ar  S 
Our intention when developing the new gram- 
mar was to find out just when problems began 
to emerge with respect to compilation of tan- 
gm~ge models. Our initial hypothesis was that 
these would l)robably become serious if the rules 
for clausal structure were reasonably elaborate; 
we expected that the large number of possible 
ways of combining modal and auxiliary verbs, 
question forlnation, movement, and sentential 
complements would rapidly combine to produce 
an intractably loose language model. Interest- 
ingly, this did not prove to be the case. In- 
stead, the rules which appear to be the primary 
ca.use of difficulties are those relating to relative 
clauses. We describe the main results in Sec- 
tion 5.1; quantitative results on recognizer per- 
tbrmance are presented together in Section 5.2. 
5.1 Main Findings 
We discovered that addition of the single rule 
which allowed relative clause modification of an 
NP had a dr~stic effect on recognizer perfor- 
lnance. The most obvious symptoms were that 
recognition became much slower and the size of 
the recognition process much larger, sometimes 
causing it to exceed resource bounds. The false 
reject rate (the l)roportion of utterances which 
fell below the recognizer's mininmnl confidence 
theshold) also increased substantially, though 
we were surprised to discover no significant in- 
crea.se in the word error rate tbr sentences which 
did produce a recognition result. To investi- 
gate tile cause of these effects, we examined the 
results of perfornfing compilation to GSL and 
PFSG level. The compilation processes are such 
that symbols retain mnemonic names, so that it 
is relatively easy to find GSL rules and gral)hs 
used to recognize phrases of specified gralnmat- 
ical categories. 
At the GSL level, addition of the relative 
clause rule to the original unification grammar 
only increased the number of derived Nuance 
rules by about 15%, from 4317 to 4959. The av- 
erage size of the rules however increased much 
more a. It, is easiest o measure size at the level of 
PFSGs, by counting nodes and transitions; we 
found that the total size of all the graphs had in- 
creased from 48836 nodes and 57195 tra.nsitions 
to 113166 nodes and 140640 transitions, rather 
more than doubling. The increase was not dis- 
tributed evenly between graphs. We extracted 
figures for only the graphs relating to specific 
grammatical categories; this showed that, the 
number of gra.1)hs fbr NPs had increased from 
94 to 258, and lnoreover that the average size 
of each NP graph had increased fronl 21 nodes 
and 25.5 transitions to 127 nodes and 165 tra.nsi- 
tions, a more than sixfold increase. The graphs 
for clause (S) phrases had only increased in 
number froln 53 to 68. They ha.d however also 
greatly increased in average size, from 171 nodes 
and 212 transitions to 445 nodes and 572 tran- 
sitions, or slightly less than a threefold increase. 
Since NP and S are by far the most important 
categories in the grammar, it is not strange that 
these large changes m~tke a great difference to 
the quality of the language model, and indi- 
rectly to that of speech recognition. 
Colnparing the original unification gramlnar 
and the compiled CSL version, we were able to 
make a precise diagnosis. The problem with the 
relative clause rules are that they unify feature 
values in the critical S and NP subgralnlnars; 
this means that each constrains the other, lead- 
ing to the large observed increase in the size 
and complexity of the derived Nuance grammar. 
aGSL rules are written in all notat ion which allows 
disjunction and Klccne star. 
674 
Specifically, agreement ilffbrmation and sortal 
category are shared between the two daugh- 
ter NPs in the relative clause modification rule, 
which is schematically as follows: 
Igp: \[agr=A, sort=S\]  --+ 
NP: \[agr=A, sort=S\] 
REL:\[agr=A, sort=S\]  
These feature settings ~re needed in order to get 
tile right alternation in pairs like 
the robot that *measure/measures 
the teml)erature \[agr\] 
the *deck/teml)era.ture tha.t you 
measured \[sort\] 
We tested our hypothesis by colnlnenting ()lit 
the agr and sor t  features in the above rule. 
This completely solves the main 1)robh;in of ex- 
1)lesion in the size of the PFSG representation; 
tile new version is only very slightly larger than 
tile one with no relative clause rule (50647 nodes 
and 59322 transitions against 48836 nodes and 
57195 transitions) Most inL1)ortantty, there is 
no great increase in the number or average size 
of the NP and S graphs. NP graphs increase in 
number froin 94 to 130, and stay constant in a.v- 
era ge size.; S graphs increase in number f}om 53 
to 64, and actually decrease, in aa;erage size to 
13,5 nodes and 167 transitions. Tests on st)eech 
(l~t;a. show that recognition quality is nea~rly :lie 
sa.me as for the version of the recognizer which 
does not cover relative clauses. Although speed 
is still significantly degraded, the process size 
has been reduced sufficiently that the 1)roblen:s 
with resource bounds disappear. 
It would be rea.sonal)le 1:o expect tim: remov- 
ing the explosion in the PFSG ret)resentation 
would result in mL underconstrained language 
model for the relative clause paxt of the gram- 
mar, causing degraded 1)erformance on utter- 
ances containing a, relative clause. Interestingly, 
this does not appear to hapl)en , though recog- 
nition speed under the new grammar is signif- 
icaatly worse for these utterances COml)ared to 
utterances with no relative clause. 
5.2 Recogn i t ion  Resu l ts  
This section summarizes our empirical recog- 
nition results. With the help of the Nuance 
Toolkit batchrec  tool, we evah:ated three ver- 
sions of the recognizer, which differed only with 
respect to tile language model, no_re ls  used 
the version of the language model derived fl'onI a 
granLn:a.r with the relative clause rule removed; 
re l s  is the version derived from the fltll gram- 
lnar; and un l inked  is the colnl)romise version, 
which keeps the relative clause rule but removes 
the critical features. We constructed a corpus 
of 41 utterances, of mean length 12.1 words. 
The utterances were chosen so that the first, 31 
were within the coverage of all three versions 
of the grammar; the last 10 contained relative 
clauses, and were within the coverage of re :s  
and un: inked but :tot of no_rels .  Each utter- 
anee was recorded by eight different subjects, 
none of whom had participated in development 
of the gra.mmar or recognizers. Tests were run 
on a dual-processor SUN Ultra60 with 1.5 GB 
of RAM. 
The recognizer was set, to reject uttera.nces if 
their a.ssociated confidence measure fell under 
the default threshold. Figures 1 and 2 sum- 
marize the re.suits for the first 31 utterances 
(no relative clauses) and the last 10 uttera:Lces 
(relative clauses) respectively. Under '?RT', 
we give inean recognition speed (averaged over 
subjects) e?pressed as a multiple of real time; 
'PRe.j' gives the false reject rate, the :heart l)er - 
centage of utterances which were reiected ue to 
low confidence measures; 'Me:n' gives the lnean 
1)ercentage of uttera.nces which fhiled due to the. 
recognition process exceeding inemory resource 
bounds; and 'WER,' gives the mean word er- 
ror rate on the sentences that were neither re- 
jected nor failed due to resource bound prob- 
lems. Since the distribution was highly skewed, 
all mea.ns were calculated over the six subjects 
renm.i:fing after exclusion of the extreme high 
and low values. 
Looking first at Figure 1, we see that re l s  is 
clearly inferior to no_re ls  on tile subset of the 
corpus which is within the coverage of both ver- 
sions: nea.rly twice as many utterances are re- 
jected due to low confidence values or resource 
1)roblems, and recognition speed is about five 
times slower, un l inked  is in contrast :tot sig- 
nificantly worse than no_re ls  in terms of recog- 
nition performance, though it is still two and a 
half times slower. 
Figure 2 compares re l s  and un l inked on the 
utterances containing a relative clause. It seems 
reasona.ble to say that recognition performance 
675 
I C4ran"nar I I FR .i I IWER 1 
no_rels 1.04 9.0% - 6.0% 
re l s  4.76 16.1% 1.1% 5.7% 
un l inked  2.60 9.6% - 6.5% 
Figure 1: Evaluation results for 31 utterances 
not containing relative clauses, averaged across 
8 subjects excluding extreme values. 
Grammar xRT FRej Men: WER\ ]  
re l s  4.60 26.7% 1.6% 3.5%\] 
un l inked 5.29 20.0% - 5.4%J 
Figure 2: Evaluation results for i0 utter~mces 
containing relative clauses, averaged across 8 
subjects excluding extreme values. 
is comparable for the two versions: rels has 
lower word error rate, but also rqjects more 
utterances. Recognition speed is marginally 
lower for unl inked,  though it is not clear to us 
whether the difference is significant given the 
high variability of the data. 
6 Conc lus ions  and  Fur ther  
D i rec t ions  j 
We found the results presented above surpris- 
ing and interesting. When we 1)egal: our pro- 
gramme of attempting to compile increasingly 
larger linguistically based unification grammars 
into language models, we had expected to see a 
steady combinatorial increase, which we guessed 
would be most obviously related to complex 
clause structure. This did not turn out to be the 
case. Instead, the serious problems we encoun- 
tered were caused by a small number of crit- 
ical rules, of which the one for relative clause 
modification was by the far the worst. It was 
not immediately obvious how to deal with the 
problem, but a careful analysis revealed a rea- 
sonable con:promise solution, whose only draw- 
back was a significant but undisastrous degra- 
dation in recognition speed. 
It seems optimistic to hope that the rela- 
tive clause problem is the end of the story; the 
obvious way to investigate is by continuing to 
expand the gramlnar in the same incremental 
fashion, and find out what happens next. We 
intend to do this over the next few months, and 
expect in due course to be able to l)resent fur- 
ther results. 
References  
H. Alshawi. 1992. The Core Language Engine. 
Cambridge, Massachusetts: The MIT Press. 
J. Bresnan, R.M. Kapla.n, S. Peters and A. Za- 
enen. Cross-Serial Dependencies in Dutch. 
1987. In W. J. Savitch et al(eds.), The For- 
real Complexity of Natural Languagc, Reidel, 
Dordrecht, pages 286-319. 
G. Gazdar, E. Klein, G. Pullum and I. Sag. 
1985. Generalized Phrase Structure Gram- 
mar Basil Blackwell. 
IBM. 1999. ViaVoice SDK tbr Windows, ver- 
sion 1.5. 
R. Moore, J. Dowding, H. Bratt, J.M. Gawron, 
Y. Gorfl:, and A. Cheyer. 1997. Com- 
mandTalk: A Spoken-Language Interface 
tbr Battlefield Simulations. Proceedings 
of the Fifth Conference on Applied Nat- 
uraI Languagc Processing, pages 1-7, 
Washington, DC. Available online from 
http ://www. ai. sri. com/natural-language 
/project s/arpa-sl s / commandt alk. html. 
Nuance Communications. 1999. Nuance Speech 
Recognition System Developer's Manv, aI, Ver- 
sion 6.2 
G. Pullum and G. Gazdar. 1982. Natural Lan- 
guages and Context-Free Languages. Lin- 
guistics and Philosophy, 4, pages 471-504. 
S.G. Puhnan. 1992. Unification-Based Synta.c- 
tic Analysis. In (Alshawi 1992) 
M. Rayner. 1993. English Linguistic Coverage. 
In M.S. Agn~s et al 1993. Spoken Language 
Translator: First Year Report. SRI Techni- 
cal Report CRC-043. Available online from 
http ://www. sri. com. 
M. Rayner, B.A. Hockey and F. James. 2000. 
Turning Speech into Scripts. To appear in 
P~vceedings of the 2000 AAAI Spring Sym- 
posium on Natural Language Dialogues with 
Practical Robotic Devices 
A. Stent, J. Dowding, J.M. Gawron, E.O. 
Bratt, and R. Moore. 1999. The Coin- 
mandTalk Spoken Dialogue System. P'rv- 
cecdings of the 37th Annual Meeting of the 
ACL, pages 183-190. Available online from 
ht tp  ://www. a i .  s r i .  com/natura l - language 
/p ro jec t  s /a rpa-s  :s  / commandt a:k.  html. 
676 
Building a Robust Dialogue System with Limited Data * 
Sharon  J .  Go ldwater ,  E l i zabeth  Owen Brat t ,  Jean  Mark  Gawron ,  and  John  Dowdingt  
, .  SR I  In ternat iona l  
333 Ravenswood Avenue 
Men lo  Park ,  CA 94025 
{goldwater, owen, gawron, dowding} @ai.sri.cora 
Abst ract  
We describe robustness techniques used in the Com- 
mandTalk system at: the recognition level, the pars- 
ing level, and th~ dia16gue level, and how these were 
influenced by the lack of domain data. We used 
interviews with subject matter experts (SME's) to 
develop a single grammar for recognition, under- 
standing, and generation, thus eliminating the need 
for a robust parser. We broadened the coverage of 
the recognition grammar by allowing word insertions 
and deletions, and we implemented clarification and 
correction subdialogues to increase robustness at the 
dialogue level. We discuss the applicability of these 
techniques to other domains. 
1 I n t roduct ion  
Three types of robustness must be considered when 
designing a dialogue system. First, there is robust- 
ness at the recognition level. When plentiful data 
is available, a robust n-gram language model can be 
produced, but when data is limited, producing a ro- 
bust language model for recognition can be prob- 
lematic. Second, there is robustness at the level 
of the parser. Robust parsing is often achieved by 
combining a full parser with a partial parser and 
fragment-combining rules, but even then some utter- 
ances may be correctly recognized, only to be parsed 
incorrectly or not at all. Finally, there is robustness 
at the dialogue level. Utterances may be uninter- 
pretable within the context of the dialogue due to 
errors on the part of either the system or the user, 
and the dialogue manager should be able to handle 
such problems gracefully. 
Our CommandTalk dialogue system was designed 
for a highly specialized omain with little available 
data, so finding ways to build a robust system with 
* This research was supported by the Defense Advanced Re- 
search Projects Agency under Contract N66001-94-C-6046 
with the Space and Naval Warfare Systems Center. The views 
and conclusions contained in this document are those of the 
authors and should not be interpreted asnecessarily repre- 
senting the official policies, either express or implied, of the 
Defense Advanced Research Projects Agency of the U.S. Gov- 
ernment. 
? Currently affiliated with GO.corn 
limited data was a major concern. In this paper, 
we discuss our methods and their applicability to 
other domains. Section 2 gives a brief overview of 
the CommandTalk system. In Section 3, we discuss 
the approach we took to building recognition, under- 
standing, and generaffon models for CommandTalk, 
and how it relates to the first two types of robustness 
mentioned. Section 4 discusses additional robust- 
ness techniques at the recognizer level, and Section 5 
describes dialogue-level robustness techniques. Sec- 
tion 6 discusses the applicability of our methods to 
other domains. 
2 CommandTa lk  
CommandTalk is a spoken-language interface to the 
ModSAF (Modular Semi-Automated Forces) battle- 
field simulator, developed with the goal of allow- 
ing military commanders to interact with simulated 
forces in a manner as similar as possible to the way 
they would command actual forces. CommandTalk 
allows the use of ordinary English commands and 
mouse gestures to 
? Create forces and control measures (points and 
lines) 
? Assign missions to forces 
? Modify missions during execution 
? Control ModSAF system functions, such as the 
map display 
? Get information about the state of the simula- 
tion 
CommandTalk consists of a number of indepen- 
dent, cooperating agents interacting through SRI's 
Open Agent Architecture (OAA) (Martin et al, 
1998). OAA uses a facilitator agent that plans and 
coordinates interactions among agents during dis- 
tributed computation. An introduction to the basic 
CommandTalk agents can be found in Moore et al 
(1997). CommandTalk's dialogue component is de- 
scribed in detail in Stent et al (1999), and its use 
of linguistic and situational context is described in 
Dowding et al (1999). 
61 
3 The  One-Grammar  Approach  
In a domain with limited data, the inability to col- 
lect a sufficient corpus for training a statistical lan- 
guage model can be a significant problem. For 
CommandTalk, we did not create a statistical lan- 
guage model. Instead, with information gathered 
from interviews of subject matter experts (SME's), 
we developed a handwritten grammar using Gemini 
(Dowding et al, 1993), a unification-based gram- 
mar formalism. We used this unification grammar 
for both natural language understanding and gener- 
ation, and, using a grammar compiler we developed, 
compiled it into a context-free form suitable for the 
speech recognizer as well. 
The effe~s_ of this single-grammar pproach on 
the robustness of the CommandTalk system were 
twofold. On the negative side, we presumably ended 
up with a recognition language model with less cov- 
erage than a statistical model would have had. Our 
attempts to deal with this are discussed in the next 
section. On the positive side, we eliminated the 
usual discrepancy incoverage between the recognizer 
and the natural language parser. This was advanta- 
geous, since no fragment-combining or other parsing 
robustness techniques were needed. 
Our approach ad other advantages a well. Any 
changes we made to the understanding grammar 
were automatically reflected in the recognition and 
generation grammars, making additions and modifi- 
cations efficient. Also, anecdotal evidence suggests 
that the language used by the system often influ- 
ences the language used by speakers, o maintaining 
consistency between the input and output of the sys- 
tem is desirable. 
4 Ut terance-Leve l  Robustness  
It is difficult o write a grammar that is constrained 
enough to be useful without excluding some rea- 
sonable user utterances. To alleviate this prob- 
lem, we modified the speech recognition grammar 
and natural language parser to allow certain "close- 
to-grammar" utterances. Utterances with inserted 
words, such as Center on Checkpoint 1 now or zoom 
way out (where Center on Checkpoint 1 and zoom 
out are grammatical) were permitted by allowing 
the recognizer to skip unknown words. We also al- 
lowed utterances with deleted words, as long as those 
words did not contribute to the semantics of the ut- 
terance as determined by the Gemini semantic rules 
constraining logical forms. For example, a user could 
say, Set speed, 40 kph rather than Set speed to 40 kph. 
The idea behind these modifications was to allow ut- 
terances with a slightly broader ange of wordings 
than those in the grammar, but with essentially the 
same meanings: 
We began by testing the effects of these modi- 
fications on in-grammar utterances, to ensure that 
Time, CPURT 
SRR 
AWER 
SER 
Non-Robust Robust 
0.664 : 1.05 
2.56% 1.70% 
1.68% 2.94% 
10.00% ~ 12.07% 
Table 1: In-Grammar Recognition Results 
they did not significantly decr egse recognition per- 
formance. We used a small test corpus of approxi- 
mately 800 utterances read by SRI employees. We 
collected four measures of performance: 
? Recognition time, measured, in multiples of 
CPU real time (CPURT). A recognition time 
of lxCPURT means that on,our CPU (a Sun 
Ultra2), recognition took exactly as~ long as the 
duration of the utterance. : 
? Sentence reject rate (SRR).' The percentage of 
sentences that the recognizer rejects. 
? Adjusted word error rate (A:WER). The per- 
centage of words in non:rejected sentences that 
are misrecognized. 
? Sentence rror rate (SER). The percentage of 
sentences in which some sort of error occurred, 
either a complete rejection or misrecognized 
word. 
Several parameters affected the results, most no- 
tably the numerical penalties assigned for inserting 
or deleting words, and the pruning threshold of the 
recognizer. Raising the pruning threshold caused 
both reject and error rates to go down, but slowed 
recognition. Lowering the penalties caused rejection 
rates to go down, but word and Sentence rror rates 
to go up, since some sentences which had been re- 
jected were now recognized partially correctly, and 
some sentences which had been recognized correctly 
now included some errors. Lowering the penalties 
also led to slower recognition. 
Table 1 shows recognition results for the non- 
robust and robust versions 0f the recognition gram- 
mar on in-grammar utterances: Th e pruning thresh- 
old is the same for both versions and the insertion 
and deletion penalties are set to intermediate val- 
ues. Recognition times for the robust grammar are 
about 60% slower than those of the control gram- 
mar, but still at acceptable l vels. Reject and error 
rates are fairly close for the two grammars. Overall, 
adding robustness to the recognition grammar did 
not severely penalize in-grammar recognition per- 
formance. 
We had very little out-of-grammar data for Com- 
mandTalk, and finding subjects in this highly spe- 
cialized domain would have been difficult and ex- 
pensive. To test our robustness techniques on out- 
62 
of-grammar utterances, we decided to port them 
to another domain with easily accessible users and 
data; namely, the ATIS air travel domain. We wrote 
a small grammar covering part of the ATIS data 
and ,compiled it into a recognition grammar using 
the same techniques as in CommandTalk. Unfortu- 
nately, we were unable to carry out any experiments, 
because the recognition grammar we derived yielded 
recognition times that were so slow as to be imprac- 
tical. We discuss these results further in Section 6. 
5 Diaiogue-Level Robustness 
To be considered robust at the dialogue level, a sys- 
tem must be able to deal with situations where an 
utterance is recognized and parsed, but cannot be in- 
terpreted withi~4he current system state or dialogue 
context. In addition~it must be easy for the user to 
correct faulty interpretations on the part of the sys- 
tem. Contextual interpretation problems may occur 
for a variety of reasons, including misrecognitions, 
incorrect reference resolution, and confusion or in- 
completeness on the part of the user. 
The CommandTalk dialogue manager maintains 
a Stack to ~keep 'track of the current discourse con- 
text and uses small finite-state machines to represent 
different~ types of subdialogues. Below we illustrate 
some types of  subdialogues and other techniques 
which provide robustness at the dialogue level. Note 
that for each utterance, we write what the system 
recognizes, not what the user actually says. 
5.1 Correction Subdlalogues 
Sx? 1: 
U 1 :Create a CEV at 76 53 
S 2 ?. 
U 3 Put Objective Golf here <click> 
S 4 ? I will locate Objective Golf at FQ 
? 658 583 
U 5 I said Objective Alpha 
S 6 ? I will locate Objective Alpha at FQ 
658 853 
Allowing the user to correct full or partial utterances 
can remedy interpretation problems caused by mis- 
recognitions, incorrect reference resolution, or user 
error. 
In Example 1, the system responds to the user's 
first utterance by producing a rising tone, illustrated 
by the ? symbol, to indicate successful interpreta- 
tion and execution of the command, in this case cre- 
ation of a CEV, a type of vehicle. (Unsuccessful 
interpretation is indicated by a falling tone, illus- 
trated by the ? symbol.) In utterances 3 through 
6, a misrecognition causes the system to perform 
the wrong behavior. The user initiates a correction 
subdialogue, and the system goes on to correctly re- 
interpret he full utterance. 
5.2 Impl ic i t  Conf i rmat ion  
Ex. 2: 
U 7 CEV, advance to Checkpoint 1 
S 8 ? 100All will proceed to Checkpoint 
1 
Use of implicit confirmation in combination with 
correction subdialogues makes it easy to correct 
faulty interpretations as soon as possible by alert- 
ing the user to possible sources of error. 
In utterances 7 and 8, the system must resolve 
the user's reference, "CEV", to a particular unit. 
It therefore choes the user's command using the 
CEV's unique call sign. This makes explicit the sys- 
tem's interpretation of the user's utterance, giving 
the user a chance to correct he system if necessary. 
Note that utterance 4 also contains an implicit 
confirmation, since the system has resolved the 
user's gesture to a set of coordinates. 
5.3 C lar i f i ca t ion  Subd ia logues  
Ex.  3: 
U 9 
S 10 
U 11 
Create a CEV at 72 69 
? 
CEV, conduct crater breach facing- 
south 
S 12 ? There are two CEV's. Do you mean 
100All or 100A12? 
U 13 A12 
S 14 ? Which point should 100A12 conduct 
crater breach oriented south at? 
U 15 Checkpoint 2 
S 16 ? There is no Checkpoint 2. Which 
point should 100A12 conduct crater 
breach oriented south at? 
U 17 I meant Checkpoint 1 
S 18 ? 100A12 will conduct crater breach 
oriented south at Checkpoint 1 
Clarification subdialogues are generally initiated by 
the system as a result of errors or incomplete com- 
mands on the part of the user. 
Example 3 illustrates three different ypes of prob- 
lems that can be corrected by system questions. 
First, the user's reference to "CEV" in utterance 
11 is ambiguous, so the system asks a question to 
determine which CEV the user is referring to. Next, 
the system asks the user to supply a missing piece 
of information that is required to carry out the com- 
mand. Finally, when the user makes an error by 
referring to a point that doesn't exist, the system 
prompts for a correction. 
6 Discussion and Conclusions 
CommandTalk is an example of a successful and ro- 
bust dialogue system in a domain with limited ac- 
63 
cess to both data and subjects. The pre-dialogue 
version of CommandTalk was used in the STOW 
(Synthetic Theater of War) '97 ACTD (Advanced 
Concept Technology Demonstration) exercise, an in- 
tensive 48-hour continuous military simulation by 
all four U.S. military services, and received high 
praise. The dialogue portion of the system has in- 
creased CommandTalk's usefulness and robustness. 
Nevertheless, everal questions remain, not the least 
of which is whether the robustness techniques used 
for CommandTalk can be successfully transferred to 
other domains. 
We have no doubt that our methods for adding ro- 
bustness at the dialogue level can and should be im- 
plemented in other domains, but this is not as clear 
for our parsing a-nd recognition robustness methods. 
The one-grammar approach is key to our elimi- 
nating the necessity for robust parsing, renders a 
large corpus for generating a recognition model un- 
necessary, and has other advantages as well. Yet 
our experience in the ATIS domain suggests that 
further research into this approach is needed. Our 
ATIS grammar is based on a grammar of general 
English and has a very different structure from that 
of CommandTalk's semantic grammar, but we were 
unable to isolate the factor or factors responsible for 
its poor recognition performance. Recent research 
(Rayner et al, 2000) suggests that it may be pos- 
sible to compile a useful recognition model from a 
general English unification grammar if the gram- 
mar is constructed carefully and a few compromises 
are made. We also believe that using an appropri- 
ate grammar approximation algorithm to reduce the 
complexity of the recognition model may prove fruit- 
ful. This would reintroduce some discrepancy be- 
tween the recognition and understanding language 
models, but maintain the other advantages of the 
one-grammar pproach. 
In either case, the effectiveness of our recognition 
robustness techniques remains an open question. We 
know they have no significant negative impact on in- 
grammar ecognition, but whether they are helpful 
in recognizing and~ more importantly, interpreting 
out-of-grammar utterances is unknown. We have 
been unable to evaluate them so far in the Com- 
mandTalk or any other domain, although we hope 
to do so in the future. 
Another possible solution to the problem of 
producing a workable robust recognition grammar 
would return to a statistical approach rather than 
using word insertions and deletions. Stolcke and 
Segal (1994) describe a method for combining a 
context-free grammar with an n-gram model gen- 
erated from a small corpus of a few hundred utter- 
ances to create a more accurate n-gram model. This 
method would provide a robust recognition model 
based on the context-free grammar compiled from 
64 
our unification grammar. We would'still have to 
write only one grammar for the system, it would still 
influence the recognition model, and we could still 
be sure that the system would never say anything it 
couldn't recognize. This approach Would require us- 
ing robust parsing methods, but might be the best 
solution for other domains if compiling a practical 
recognition grammar proves too difficult. 
Despite the success of the CommandTalk system, 
it is clear that more investigation is called for to 
determine how best to develop dialogue systems in 
domains with limited data. Researchers must de- 
termine which types of unification grammars can be 
compiled into practical recognition grammars using 
existing technology, whether grammar approxima- 
tions or other techniques can produce good results 
for a broader range of grammars, whether allow- 
ing word insertions and deletions is an effective ro- 
bustness technique, orwhether we should use other 
methods altogether. 
Re ferences  
J. Dowding, J. Gawron, D. Appelt, L. Cherny, 
R. Moore, and D. Moran. 1993. Gemini: A Natu- 
ral Language System for Spoken Language Under- 
standing. In Proceedings of the Thirty-First An- 
nual Meeting of the ACL, Columbus, OH. Associ- 
ation for Computational Linguistics. 
J. Dowding, E. Owen Bratt, and S. Goldwater. 
1999. Interpreting Language in Context in Com- 
mandTalk. In Communicative Agents: The Use 
of Natural Language in Embodied Systems, pages 
63-67. 
D. Martin, A. Cheyer, and D. Moran. 1998. Build- 
ing Distributed Software Systems with the Open 
Agent Architecture. In Proceedings of the Third 
International Conference on the Practical Appli- 
cation of Intelligent Agents and Multi-Agent Tech- 
nology, Blackpool, Lancashire, UK. The Practical 
Application Company Ltd. 
R. Moore, J. Dowding, H. Bratt, J. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A Spoken-Language Interface for Battlefield Sim- 
ulations. In Proceedings of the Fifth Conference 
on Applied Natural Language Processing, pages 
1-7, Washington, DC. Association for Computa- 
tional Linguistics. 
M. Rayner, B. A. Hockey, F. James, E. Owen Bratt, 
S. Goldwater, and J. M. Gawron. 2000. Compil- 
ing Language Models from a Linquistically Moti- 
vated Unification Grammar. Submitted to COL- 
ING '00. 
A. Stent, J. Dowding, J. Gawron, E. Owen Bratt, 
and R. Moore. 1999. The CommandTalk Spoken 
Dialogu.e System. In Proceedings of the 37th An- 
nual Meeting of the A CL. Association of Compu- 
tational Linguistics. 
A. Stolcke and J. Segal. 1994. Precise N-Gram 
Probabilities from Stochastic Context-free Gram- 
mar.: In Proceedings of the 32nd Annual Meeting 
off :the ~Association for Computational Linguistics, 
pages 74~-79, 
65 
