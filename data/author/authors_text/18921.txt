Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1423?1434, Dublin, Ireland, August 23-29 2014.
Million-scale Derivation of Semantic Relations
from a Manually Constructed Predicate Taxonomy
Motoki Sano
?
Kentaro Torisawa
?
Julien Kloetzer
?
Chikara Hashimoto
?
Istv
?
an Varga
?
Jong-Hoon Oh
?
? ? ? ? ?
National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan
?
NEC Knowledge Discovery Research Laboratories, Kanagawa, 211-8666, Japan
{
?
msano,
?
torisawa,
?
julien,
?
ch,
?
rovellia}@nict.go.jp,
?
vistvan@az.jp.nec.com
Abstract
We manually created a semantic taxonomy called Phased Predicate Template Taxonomy (PPTT)
that covers 12,023 predicate templates (i.e., predicates with one argument slot like ?rescue X?)
and derived from it various semantic relations between these templates on a million-instance
scale (70%-80% precision level). The derived relations include entailment (e.g., rescue X?X is
alive), happens-before (e.g., buy X?drink X), and a novel relation type anomalous obstruction
(e.g., X is sold out;cannot buy X). Such derivation became possible thanks to PPTT?s design
and the use of statistical methods.
1 Introduction
Databases of various semantic relations between natural language expressions are indispensable knowl-
edge for many NLP applications. For instance, entailment relations are crucial in information extraction
and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014).
Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are im-
portant for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire
those relations with a broad coverage. Although many sophisticated machine learning techniques have
been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky,
2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al.,
2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness
in the input data. In this work we take a completely different approach: we manually construct a seman-
tic lexicon called Phased Predicate Template Taxonomy (PPTT), and derive various types of semantic
relations on a large-scale by using it. Our target language is Japanese, but examples are given in English
for simplicity throughout this paper.
PPTT is a taxonomy of predicate templates (predicates with one argument slot like rescue X, ?Tem-
plate? hereafter) that classifies templates according to phases of story concerning an entity denoted by
X. In the story, or the ?life? of the entity X, X can be anticipated, created, then execute its function and
finally it may collapse and become deficient. Anticipation, creation, execution, collapse, deficiency of X
can be seen as such phases of story concerning X, and PPTT classifies templates into 41 semantic classes
each of which corresponds to a phase. In other words, PPTT provides a way to describe the stories of var-
ious entities that constitute this world, and we believe that PPTT (partly) reflects how we understand the
world and its entities. Accordingly, PPTT can also provide a way to derive various semantic knowledge
about this world such as the happens-before relation between events involving an entity, e.g., since the
creation phase usually occurs before the execution phase, invent X (creation phase) is likely to happen-
before use X (execution phase). In addition, entailment relations can be derived: since the creation phase
of an object X must have occurred if X is in its execution phase, it implies that use X is likely to entail
invent X.
In addition, there are ups and downs in stories; some entities suffer setbacks in their stories. PPTT de-
scribes such ?ups and downs? by means of a recently proposed semantic polarity, excitation (Hashimoto
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1423
et al., 2012). Excitation classifies templates into excitatory, inhibitory, and neutral; an excitatory tem-
plate like install X and buy X indicates that the main function, effect, purpose or role of the entity referred
to by the X of the template is activated, enhanced, or prepared,
1
while an inhibitory template like unin-
stall X and X is canceled roughly indicates that it is deactivated or suppressed. Neutral templates are
neither excitatory nor inhibitory (e.g., consider X). Roughly speaking, an excitatory template expresses
the events that contribute to turn on the function of X, while an inhibitory template expresses the events
that contribute to turn off or not to turn on the function of X. Then, in PPTT, excitatory and inhibitory
respectively correspond to ?ups? and ?downs? in the story of X. The phases in PPTT are marked accord-
ing to these ups and downs. Accordingly, PPTT can derive many antonymous contradiction pairs like
install X?uninstall X, as Hashimoto et al. did, though we omit the detail for space limitation. Moreover,
PPTT can derive a huge volume of anomalous obstruction, a contradiction-like novel semantic relation
that we propose in this paper, like X is canceled;(cannot) buy X and X is sold out;(cannot) buy X,
which indicate that if X is canceled or sold out, you cannot buy X. Anomalous obstruction should be
used for Why-type QA (Oh et al., 2013), as well as a novel system that warns a user who wants to buy
a commercial product that the product is started to be sold out or canceled in various e-commerce sites
without any application-specific coding.
As suggested, a story has a temporal order between its phases, which we call the canonical temporal
order. In addition, some phases in a story would enable or necessitate another phase in the same story to
occur. In PPTT, these relations are embodied in various temporal-semantic links between phases. Note
that each link between two phases does not guarantee that every possible pair of templates taken from
the two phases has such semantic relations; it just indicates that there exists such tendencies. Despite the
absence of the guarantee, PPTT?s links enable a million-scale derivation of semantic relations with the
help of distributional similarity. In existing resources such as WordNet (Fellbaum, 1998), the links are
assumed to be 100% correct, but it would be hard to have such absolutely correct links in a million-scale.
Hence, we believe that our approximate links are more useful for a large-scale relation derivation.
Note that the goal of our PPTT project is to derive a wide range of semantic relations on a large scale,
rather than to complete a comprehensive template taxonomy. As such, PPTT lacks some templates as
described in later sections. Nevertheless, we believe that our design brings much more good than harm,
since we could generate various semantic relations on a million scale thanks to PPTT. Our experimental
results show that we can derive 4.4 million happens-before relation instances with 79.5% precision,
0.5 million entailment relation instances with 70.0% precision, and one million anomalous obstruction
relation instances with 73.5% precision. Constructing the PPTT taxonomy requires a manual labor cost,
which amounted to three man-months in our case; however, we believe that this cost is lower than the
cost for developing highly-precise automatic acquisition methods for all of happens-before, entailment,
contradiction, and anomalous obstruction relations.
We plan to release PPTT and the derived relation instances after the manual annotation of the derived
instances to the NLP community.
2 Related Works
PPTT might resemble other semantic lexicons created in the long history of NLP (Levin, 1993; Kipper
et al., 2006; Fellbaum, 1998; Bond et al., 2009; Fillmore, 1976; Baker et al., 1998; Halliday, 1985;
Pustejovsky et al., 2003; Puscasu and Mititelu, 2008; Bejar et al., 1991; Jurgens et al., 2012). PPTT
is different in that it primarily aims at deriving various types of semantic relations on a large scale ex-
ploiting the notion of the phase of story, rather than being a comprehensive taxonomy like those existing
semantic lexicons. As a result, PPTT can derive more varieties of semantic relations between templates
than any one of those existing lexicons. From WordNet (Fellbaum, 1998; Bond et al., 2009), we can de-
rive entailment and contradiction relations using synsets and synset-links that represent relations such as
?troponym?, ?antonym? and ?entailment?. However, happens-before and anomalous obstruction relations
1
The above definition is slightly different from the original one in Hashimoto et al. (2012). We inserted the verb ?prepared?
into the original definition. This clarifies that various preparation processes for X, such as buy X, can be regarded as excitatory
templates. We also assume that such templates as X exists and have X, which mean little more than just existence, are regarded
as excitatory templates in PPTT based on the assumption that existence can be regarded as preparation for the function of X.
1424
cannot be derived from it, since there is no information on temporal ordering except that on causality.
From VerbNet (Levin, 1993; Kipper et al., 2006), the hyponymy/synonymy type of entailment relations
may be derived using templates in the same verb classes constructed based on shared syntactic behavior,
possibly with the help of statistical methods. However, the other types of relations that can be derived
from PPTT cannot be derived from VerbNet, since there is no link representing relationships between the
verb classes. FrameNet (Fillmore, 1976; Baker et al., 1998) was used to derive hyponymy/synonymy
types of entailment (Coyne and Rambow, 2009; Aharon et al., 2010) using information such as a Frame-
to-frame relation ?Inheritance? (is-a relation). In addition, happens-before relations can be derived using
?Precedes? (Later-Earlier relations). However, since it does not contain semantic constraints like en-
ablement and necessity that PPTT contains, it is not trivial to derive presupposition type of entailment
or anomalous obstruction instances from it. TimeML (Pustejovsky et al., 2003; Puscasu and Mititelu,
2008) contains various temporal information and can be used to derive context-dependent happens-before
relations such as the relation between ?leaves? and ?will not hear? in the sentence ?If Graham leaves to-
day, he will not hear Sabine? through TLINK (Pustejovsky et al., 2003) annotated manually; thus, it is
difficult to derive context-independent relations from it, while they can be derived from PPTT. Besides,
since it covers only temporal information, it is difficult to derive other types of relations from it. From
Bejar et al.?s semantic relation taxonomy of lexical pairs (Bejar et al., 1991; Jurgens et al., 2012),
using semantic relation categories such as ?act: act attribute? (e.g., creep:slow), lexical entailment rela-
tions were extracted (Turney and Mohammad, 2014). However, it is not trivial to derive happens-before
or anomalous obstruction relations from it since it does not contain information on temporal sequences
between verbs.
Furthermore, our work differs from automatic methods for extracting temporal or causal relations
(Szpektor et al., 2007; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Talukdar et al.,
2012; Hashimoto et al., 2012; Hashimoto et al., 2014) in that our method does not require that target
pairs co-occur in a document, unlike the previous methods. Hence, our method is likely to be immune
to data sparseness. We could actually derive a wide range of relation instances that were rarely written
in documents because they were too commonsensical (e.g., X is constructed happens-before sew (some-
thing) at X). Needless to say, such commonsensical knowledge is often needed to develop intelligent
systems.
3 PPTT Design
In PPTT, templates are organized hierarchically into three levels. In each level, there are classes that
correspond to phases of stories, which we call Level-0 (L0), Level-1 (L1), and Level-2 (L2) classes.
Each template belongs to only one class at each level. In the following, we describe each level.
3.1 L0-Classes and L0-Links
First we divided the entire story concerning an entity X into five phases: non-existence, existence, func-
tioning, non-existence to existence transition and existence to non-existence transition. Then we created
the five L0-classes listed below, each of which corresponds to one of these five phases.
Non-existence Class The class of templates that do not entail the existence of X, e.g., plan
X.
2
Existence Class The class of templates that entail X?s existence but does not imply the execu-
tion of its main function or the achievement of its objectives, e.g., buy X, X exists.
Functioning Class The class of templates that imply the execution of X?s main function or
the achievement of its objectives, e.g., use X, eat X.
Non-existence to Existence Transition Class (NET Class) The class of templates that ex-
press the transition from a situation in which X does not exist to a situation in which it
exists, e.g., manufacture X.
2
One might think the definition of the Non-existence Class should be ?the templates that DO entail X?s NON-EXISTENCE?.
We did not use such a definition because it would overlook many templates that are consistent with X?s NON-EXISTENCE but
DO NOT entail X?s NON-EXISTENCE, like order X.
1425
Existence to Non-existence Transition Class (ENT Class) The class of templates that ex-
press the transition from a situation in which X exists to a situation in which it does not
exist, e.g., dismantle X.
!"#$%&'()%#*%+,-.((+!"#"$%&'()%*%
/&'()%#*%+,-.((+!"#"$%+,-%*% 01#*2"#'#3+,-.((+!"#"$%,.!%*%
/&'()%#*%+)"+!"#$%&'()%#*%++45.#('2"#+6/!47+,-.((++!"#"$%/0.1()2'!%*%
!"#$%&'()%#*%+)"+/&'()%#*%++45.#('2"#+6!/47+,-.((+!"#"$%1(),3(42,5!%*%
Figure 1: L0-links among L0-classes.
As mentioned in the introduction, we assume
a canonical temporal order among L0-classes.
For instance, templates in the NET class (e.g.,
manufacture X) should refer to events that usu-
ally happen before those events referred to by
templates in the Existence class (e.g., buy X),
Functioning class (e.g., use X) and ENT class
(e.g., dismantle X). We enumerated such tem-
poral restrictions, each of which is represented
by a link in Figure 1, which we call L0-links
and used them for deriving relations. Note that
we did not set any L0-link between the Exis-
tence class and the Functioning class because
the events described by them may happen in various orders or have temporal overlap. For example, X
exists should have temporal overlap with use X.
Of course, such metaphysical notions as the canonical temporal order and the phases must have many
complications and exceptions. First, many templates that have the neutral excitation polarity (Hashimoto
et al., 2012) did not seem to follow the canonical temporal order among L0-classes. For instance, since
the neutral template think about X does not entail the existence of X, it belongs to the Non-existence
class but one can consider X while X exists or while it is functioning or even after it is collapsed and
violate canonical temporal ordering. For this reason, we excluded neutral templates from PPTT and will
deal with them in a different framework as a future work. In addition, although we did not assume a
temporal order between the Existence class and the Functioning class, some templates in these classes
have a happens-before relation as special cases (e.g., buy X in the Existence class happens before eat
X in the Functioning class). The proposed L0-links also cause problems. For instance, order X (Non-
Existence class) may not always happen before create X (NET class) even though the L0-links indicate
a happens-before relation between their classes. We dealt as far as possible with such cases in level 2
with L2-classes, which are finer than L0-classes. Nonetheless, we stress that the overall plausibility of
the canonical temporal order among L0-classes was experimentally confirmed through the derivation of
happens-before relations only using L0-links. Note that the design of the L0-classes was inspired by the
Generative Lexicon (Pustejovsky, 1998) and Aristotle?s Entelecheia (Aristotle, 1987).
3.2 L1-Classes
Excitation
L0-class Excitatory Inhibitory
POTENTIAL class FORECLOSING class
Non-existence class e.g., plan X e.g., prevent X
ENABLING class INCOMMODE class
Existence class e.g., buy X e.g., weaken X
ACTUALIZING class DISORDERING class
Functioning class e.g., X functions e.g., X loses
GENERATING class
NET class e.g., X is born N/A
CORRUPTING class
ENT class N/A e.g., destroy X
Table 1: L1-classes.
Next, we divided some L0-classes into
L1-classes using the excitation polar-
ity (Hashimoto et al., 2012) to intro-
duce ?ups and downs? to PPTT, which
enables to capture semantic inconsis-
tencies between templates (e.g., in-
stall X?uninstall X) and negative in-
teraction between the events referred
to by the templates in PPTT (e.g., X
is canceled;(cannot) hold X). Excita-
tion was originally proposed for recog-
nizing contradictions and causal rela-
tions between templates and then was
successfully applied to other deep se-
mantic processing (Oh et al., 2013; Varga et al., 2013; Kloetzer et al., 2013; Hashimoto et al., 2014).
1426
As shown in Table 1, we divided each of three L0-classes (Non-existence class, Existence class and
Functioning class) into two L1-classes, each of which corresponds to excitatory and inhibitory. Since
the transition to an existence situation can be interpreted as an enhancement of an entity?s function, we
assumed that all the templates in the NET classes are excitatory because they express a transition of
entity X from a non-existence situation to an existence situation. Similarly, we assume all the templates
from the ENT class are inhibitory. Also, L1-classes do not have specific links between them beside the
L0-links from their parent classes.
3.3 L2-Classes and L2-Links
Finally, we divided L1-classes into 41 L2-classes. Specifically, we first roughly grouped together seman-
tically similar templates from the same L1-class and identified the common semantic properties among
them. Note that in the rough grouping, we classified templates so that the resulting groups fit into fine-
grained phases in the story concerning X.
After this initial grouping, we classified all the templates into the L2-classes that are listed in Table
3 alongside the classification criteria and the number of templates in each class. As the classification
criteria, we used the identified common semantic properties among members of each class. Note that
some L2-classes can be regarded as a subset of another L2-class. For instance, the PROHIBIT L2-class
can be seen as a subset of the PREVENTION L2-class. When a template meets the classification criteria
of both a subset class and its superset class, we classified it into the subset class.
We also made links called L2-links between the L2-classes. The motivation behind this is to capture
finer temporal-semantic constraints that could not be specified at Level-0 and Level-1 as well as to
capture the temporal-semantic constraints inside a single L0 or L1-class. For example, the temporal
order between buy X and eat X is encoded in a L2-link between the ACQUISITION and EXECUTION L2-
classes, while there is no L0-link between the Existence L0-class (class of buy X) and the Functioning
L0-class (class of eat X). This exemplifies that the L2- and L0-links complement each other.
Each L2-link has one of the six types of temporal-semantic links that are summarized in Table 2 with
the number of links of each type. The link types were designed to capture how the events referred to by
the templates in a class affect the occurrence or non-occurrence of the events referred to by the templates
in a class in the past, present, or future. C
1
and C
2
being two L2-classes, C
1
?s effect on the occurrence or
non-occurrence of C
2
is represented by Positive (
+
) and Negative (
?
) links, respectively, while C
1
?s effect
on the past, present, or future phase of X expressed by C
2
is represented by Past, Present, and Future
links, respectively. For instance, the Past
+
link from the ABANDONMENT class to the ACQUISITION
class indicates that a template from the ACQUISITION class (e.g., obtain X) must occur before a template
from the ABANDONMENT class (e.g., get rid of X), and the Future
?
link from the PROHIBIT class to
the EXECUTION class indicates that templates from the PROHIBIT class (e.g., ban X) disable templates
from the EXECUTION class (e.g., utilize X). Notice that L2-links represent such semantic constraints as
enablement and necessity in addition to temporal order, and they are useful for deriving various kinds of
semantic relations including entailment and anomalous obstruction, as shown in a later section. The first
author of this paper hand-labeled the links between every combination of L2-class pairs by considering
the name of the classes and a few example templates in each.
Positive Negative
Past If C
1
occurred, C
2
must have occurred.
e.g.,FORGETTING
Past
+
? RECOGNITION; X is forgotten
Past
+
? X is
recognized (55 links)
If C
1
occurred, C
2
COULD NOT have occurred.
e.g.,CREATION
Past
?
? PREVENTION; X is generated
Past
?
? X
is prevented (438 links)
Present While C
1
is taking place, C
2
must be taking place.
e.g.,INITIATION
Present
+
? BEING; X is started
Present
+
? X
exists (73 links)
While C
1
is taking place, C
2
CANNOT take place.
e.g.,ENHANCEMENT
Present
?
? DEGRADATION; X is enhanced
Present
?
? X is deteriorated (496 links)
Future C
1
enables C
2
to occur. e.g.,PREPARATION
Future
+
? EXECUTION;
X is customized
Future
+
? X is executed (90 links)
C
1
DISABLEs C
2
to occur. e.g.,
DEFICIENCY
Future
?
? PROVISION; X does not exist
Future
?
?
X is provided (210 links)
Table 2: Types and numbers of L2-links in PPTT. Link direction is C
1
? C
2
.
1427
Non-existence L0-class: Potential L1-class (578) / Foreclosing L1-class (178)
DESIRE entails that X is desired but unlike PLANNING or DEMAND, it does not entail that X is planned or requested, e.g.,
desire X, want X (48).
PLANNING entails that X is planned but does not entail that X is requested. Unlike DEMAND, it does not assume that a person
other than the Planner will carry out X, e.g., plan X, conspire X (72).
DEMAND entails that X is requested. Unlike PLANNING, it assumes that a person other than the Demander will carry out X,
e.g., order X (252).
APPROVAL entails that X is approved or permitted and that there was a plan or a demand before approving, e.g., permit X, accept
X (80).
FEAR entails that X is expected and that X is a source of anxiety or fear, e.g., fear X, worry about X (13).
ANTICIPATION entails that X is expected but unlike FEAR, does not entail that X is a source of anxiety or fear, e.g., forecast X, predict
X (24).
SEARCH entails that X is searched for but unlike DESIRE or DEMAND, does not entail that X is desired or requested, e.g.,
search for X (89).
PREVENTION entails that X is prevented. Unlike CANCELATION, it does not entail that there was a plan or a demand before
preventing, e.g., preclude X (54).
CANCELATION entails that X is canceled and that there was a plan or demand before canceling, e.g., cancel X, give up X (34).
PROHIBIT entails that X is prohibited. X?s right or ability to be generated or used is taken away. e.g., ban X, forbid X (39).
POSTPONE entails that X is postponed, e.g., postpone X, defer X (15).
DEFICIENCY entails that X does not exist but does not entail that it is prevented, canceled, prohibited, or postponed, as in other
L2-classes of Foreclosing L1-class. e.g., lack X, X is absent (36).
NET L0-class: Generating L1-class (596)
SYMBOLIZATION entails that X transits from non-existence to existence as a kind of (semiotic) representation, e.g., write X, compose
(music) X (13).
CREATION entails that X transits from non-existence to existence. Unlike SYMBOLIZATION, X is not limited to a semiotic
representation, and unlike TRANSFORMATION, it focuses less on transformation from another entity. generate X,
cause X (509).
TRANSFORMATION entails that X transits from non-existence to existence as a result of transformation. Unlike CREATION, it focuses on
the transformation from another entity, e.g., turn into X (74).
ENT L0-class: Corrupting L1-class (622)
COLLAPSE entails that X transits from existence to non-existence by dying, being eliminated, or being destroyed. Unlike CON-
VERSION , it focuses less on transformation, e.g., destroy X, kill X (588).
CONVERSION entails that X transits from existence to non-existence by transforming X into an another entity, e.g., turned from X,
changed from X (34).
Existence L0-class: Enabling L1-class (3,536) / Incommode L1-class (1,355)
RECOGNITION entails that X is recognized or sensed, e.g., find X, feel X (308).
SELECTION entails that X is selected, e.g., appoint X, choose X (139).
ENCOUNTER entails that X emerges as a result of transportation, e.g., send X, X arrives (407).
ACQUISITION entails that X is obtained and possessed, e.g., buy X, catch X (482).
PROVISION entails that X is handed to be possessed, e.g., sell X, render X (422).
ENHANCEMENT entails that X is extended, improved, or supported, e.g., increase X, help X (880).
PREPARATION entails that X is arranged, connected, or qualified in preparation to execute its function, e.g., cook X, install X (822).
BEING entails that X is existing or living but does not entail that X is recognized, selected, encountered, acquired, enhanced,
or prepared, as in other L2-classes of the Enabling L1-class, e.g., X exists, X lives (76).
UNRECOGNIZING entails that X is not recognized or sensed but unlike FORGETTING, does not entail that X was previously recognized,
e.g., overlook X (8).
FORGETTING entails that X is forgotten and that X was once recognized, e.g., forget X, lose memory of X (8).
UNSELECTING entails that X is not selected, e.g., alternate X, reject X (46).
SEPARATION entails that X is left or separated as a result of transportation, e.g., X leaves, send X away (114).
ABANDONMENT entails that X is not possessed as a result of being thrown away, e.g., throw X away, renounce X (58).
DEPRIVATION entails that X was taken away without the permission of a possessor, e.g., steal X, take X away (102).
DEGRADATION entails that X is reduced, deteriorated, or interrupted, e.g., X is weakened, attack X (908).
UNPREPARED entails that X is unprepared, disconnected, or unqualified, e.g., X is uninstalled, X is disconnected (111).
Functioning L0-class: Actualizing L1-class (4,460) / Disordering L1-class (698)
EXECUTION entails that the function of X is executed but unlike WORKING, does not entail that X successfully satisfies its function,
e.g., ignite X (966).
WORKING entails that the function of X is carried out and that X successfully satisfies its function, e.g., X functions, cleaned by
X (3,106).
INITIATION entails that X is started or continued, e.g., start X, open X (185).
SUCCESS entails that X accomplished its goal and the result of the execution of its function is evaluated positively, e.g., accom-
plish X, X wins (203).
SUSPENSION entails that the function of X is suspended but unlike FINISHING, does not entail that its function is terminated, e.g.,
suspend X (133).
DYSFUNCTION entails that the function of X is executed but X is performing poorly, e.g., X is sluggish, bored by X (196).
FINISHING entails that X is terminated, e.g., end X, finish X. (110).
FAILURE entails that X fails to accomplish its goal and the result of the execution of its function is evaluated negatively, e.g., X
is defeated (259).
Table 3: PPTT classes. The number in parentheses indicates the number of templates in PPTT.
1428
Note that the existence of an L2-link does not guarantee that the semantic properties specified by it
hold for all the possible template pairs taken from the class pair it connects. The cost of hand-labelling
the links with such guarantees is prohibitively high because we would have to check all of the template
combinations. We empirically evaluated the validity of the links in our experiments below although this
is not a direct evaluation since the relations we derived are different from the ones given to the links.
4 Construction of PPTT and Relation Derivation
Using the automatic acquisition method proposed by Hashimoto et al. (2012), we collected 10,825 can-
didates of excitatory/inhibitory templates from a 600-million-page web corpus (hereafter, WCorpus).
Hashimoto et al.?s method constructs a network of templates based on their co-occurrence in sentences
with a small number of seed templates of which excitation polarity are assigned manually, and infers the
polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et
al., 2005). Then, we added the 20,000 most frequent templates in the corpus that could not be extracted
automatically for a total of 30,825 templates.
Three human annotators (not the authors) judged the polarity of the templates, and we included the
excitatory and the inhibitory templates but excluded the neutral templates in PPTT due to the reason
discussed in Section 3.1. We also excluded templates whose variable X is the subject of a transitive verb.
This is because the subject position is often occupied by living things, and since the functions/objectives
of such subjects seem difficult to identify, it is often difficult to judge whether such templates should be
classified into the Functioning class or another. After applying these two restrictions, the first author
classified the remaining 12,023 templates in PPTT.
In this work, we derived happens-before, entailment and anomalous obstruction relations among tem-
plates from PPTT. The target data is the set of all the template pairs such that a noun exists with which
both templates of the pair co-occur at least 100 times in WCorpus. We denote this set of the template
pairs by TP100, and all the relation derivations pick up template pairs as relation instances from it. This
is because in our preliminary experiments, we found that the relation instance candidates taken from
outside of TP100 had much lower precision. The relation derivation itself is quite simple and consists
of the following two steps.
Step 1 Select L0-links or types of L2-links that are expected to represent a target semantic
relation (e.g., Present
+
links are expected to represent entailment, since they represent
the relations between classes where ?While C
1
is taking place, C
2
must be taking place?.)
and extract all the class pairs connected by the selected links (e.g., INITIATION L2-class
Present
+
? BEING L2-class). Enumerate all the template pairs from the intersection between
TP100 and the extracted class pairs (e.g., X is started
Present
+
? X exists).
Step 2 If necessary, rank the relation instance candidates that are extracted in Step1 by distri-
butional similarity scores between the templates that compose the candidates, computed
with WCorpus.
5 Experiments
This section reports our experiments on semantic relation derivation. Derived relation instances were
marked by three human annotators (not the authors) who voted to break ties. Unless stated otherwise,
we asked them to mark a template pair as negative if they found any noun that can be placed in both
templates? argument slots and makes the template pair a negative sample for the target relation, and
positive otherwise.
5.1 Happens-Before Relation
Following Regneri et al. (2010), we assumed template
1
(T
1
) has a happens-before relationwith template
2
(T
2
) iff one event expressed by T
1
normally happens before another expressed by T
2
, provided that both
events occur. Below are our four methods to derive happens-before relation instances, each of which
uses different links. Note that we did not use distributional similarity in this experiment.
1429
H1 uses the 55 pairs of L2-classes connected by L2-link Past
+
, meaning that a template in a
class must occur before another.
H2 uses the 90 pairs of L2-classes connected by L2-link Future
+
, i.e., a template in a class
often enables another to occur.
H3 uses the 474 pairs of L2-classes connected by one of the seven L0-links in Figure 1, i.e.,
the canonical temporal order links.
All is the union of H1-H3 results.
We prepared two baselines; HB-Ptn is a pattern-based method based on Chklovski and Pantel (2004a).
It extracts template pairs in TP100 that were connected in WCorpus by one of manually collected 73
conjunctives expressing temporal order, such as after and before, and which either shared the same
argument or the second template was filled by the pronouns it, this, or that. Random is a random
sampling from TP100.
Three annotators annotated 200 random samples from each method?s output. Fleiss? kappa was .56
(moderate agreement). The results of their majority vote are summarized in Table 4. The recall was
estimated against the number of positive samples in TP100 based on the precision of Random. The
precision of all of our four methods is reasonably high for such a difficult task, and the number of
relations derived by All reached about 4.4 million. The recall of All exceeds 65%, which we believe is
quite high. HB-Ptn suffered from low recall, probably due to the data sparseness in WCorpus. Table 5
shows examples of the derived happens-before relations alongside L2-classes of the templates, the L2-
links between the classes and the original Japanese templates. The acquired relations included many
unexpected but correct happens-before relations, like compose (a piece of music) X?relax by X.
Actually, it is difficult to fairly compare our work and previous works on temporal relation acqui-
sition, due to differences in language, the data used, and the methodologies. Nonetheless, our result
with 79.5% precision is at least five times larger than the English data released by Chambers et al.
(cs.stanford.edu/people/nc/schemas), which contains around 870,000 ?before? relation candidates and
happens-before database in the VerbOcean (Chklovski and Pantel, 2004a) that covers 4,205 relations.
Considering our method is completely different from theirs, we believe that our contribution is valuable.
Setting/Method Precision (%) # of Pairs Recall (%)
H1 83.5 1,113,280 18.0
H2 70.5 1,524,557 20.8
H3 67.0 3,837,116 49.7
All 79.5 4,387,781 67.5
HB-Ptn 53.0 32,288 0.3
Random 18.0 28,717,454 100.0
Table 4: Happens-before derivation performance.
boil X?eat X
PREPARATION Class
Future
+
? EXECUTION Class
X wo niru? X wo taberu
compose (a piece of music) X?relax by X
SYMBOLIZATION Class
Past
+
? WORKING Class
X wo sakkyoku-suru? X de rirakkusu-suru
Table 5: Examples of happens-before relation.
5.2 Entailment Relation
Below are our proposed methods to derive entailment relations.
Present+.DIFF extracts the 32 class pairs that are composed of DIFFERENT L2-classes and
are connected by the Present
+
links, meaning that a template in a class must occur simul-
taneously with another template in another class, and ranks all the possible template pairs
taken from each class pair using Hashimoto et al.?s (2009) conditional probability based
similarity measure for entailment recognition.
Present+.SAME extracts the 41 class pairs that are composed of the SAME L2-classes and
are connected with the Present
+
links, and ranks all the template pairs from each class
pair using Hashimoto et al.?s similarity.
Past+ extracts the 55 pairs of L2-classes that are connected with the Past
+
links, meaning that
a template in a class must occur before another, and ranks all the template pairs from each
class pair using Hashimoto et al.?s similarity.
1430
Baseline-HAS is our baseline which is our implementation of Hashimoto et al. (2009) for entailment
recognition; it ranks all the template pairs in TP100 by Hashimoto et al.?s score. Our methods can be
seen as the restrictions of the output of the baseline method using the extracted PPTT?s class pairs.
0e+00 2e+04 4e+04 6e+04 8e+04 1e+05
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Number of template pairs (sorted by score)
Pr
ec
isi
on
Present+.DIFFPresent+.SAMEPast+Baseline-HAS
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
Figure 2: Entailment derivation performance.
Three annotators hand-labeled 500
random samples from the top 100,000
template pairs for each method. The
kappa was .59 (moderate agreement),
and the results of their majority vote
are presented in Figure 2. Table 6
shows examples of Proposed methods?
outputs. The restriction of the class
pairs in our method contributed to much
higher precision than using the state-of-
the-art method alone.
Since the precision of Past+ is quite
high for the top 100,000 pairs, we an-
notated an additional 500 random sam-
ples from the top 500,000 pairs. Accord-
ing to this annotation, the top 408,610
pairs had 70% precision, implying that
after merging all the top pairs extracted
by Present+.DIFF, Present+.SAME and Past+ whose precisions exceeded 70%, we had 0.49 million
entailment pairs with 70% precision. With Baseline-HAS, we derived only 24,000 with the same preci-
sion. Also, the JapaneseWordNet (v.1.1) covers only 2.4% of the pairs in the manually annotated positive
samples from our proposed methods through the ?synsets? or any ?synlinks?. We analyzed 200 samples
from the positive samples not covered by WordNet and found that 49.5% are the hyponymy type (e.g.,
boil X?heat X), 39.0% are the backward presupposition type (e.g., complete X?start X), and 11.5% are
the synonymy type (e.g., X passes away?X dies). This seems to imply that our methods are better at
deriving all types of entailment, while WordNet might be effective for only the synonymy type. In addi-
tion, by analyzing all the positive samples, we confirmed that the different types of entailment pairs were
derived with different L2-links; 88.1% of the positive samples from Present+.DIFF and Present+.SAME
require that two events referred to by the two templates occur with temporal overlap (e.g., equip X?X
exists, i.e. X is equipped while X exists), while 96.7% of those from Past+ were the backward presuppo-
sition type, in which an event entails another event that happened before it. This shows that the L2-links
were useful for deriving various fine-grained types of entailment.
get X?X exists (X wo nyuushu-suru ? X ga sonzai-suru ) ACQUISITION Class
Present
+
? BEING Class
evolve into X?change into X (X ni shinka-suru ? X ni kawaru ) TRANSFORMATION Class
Present
+
? TRANSFORMATION Class
close (a shop) X?make X (X wo heiten-suru ? X wo tsukuru ) FINISHING Class
Past
+
? CREATION Class
Table 6: Examples of entailment.
5.3 Anomalous Obstruction Relation
We assumed that template
1
(T
1
) like X is sold out has an anomalous obstruction relation with template
2
(T
2
) like buy X (denoted as X is sold out;(cannot) buy X) iff: (A) the event expressed by T
1
prevents
the event expressed by T
2
from occurring; (B) T
1
expresses an event that should not happen if everything
about the variable X goes as expected; and (C) T
2
expresses another event in which the function of X is
executed, enhanced, or prepared. We derived anomalous obstructions, by generating all of the possible
template pairs from the 88 L2-class pairs connected by Future
?
L2-links. These indicate that the events
expressed by the templates in the first class of a pair disable the events expressed by the templates in the
second class. Also, to confirm that the templates of the first class in a pair express an unexpected event,
1431
we required the disabler class to have the inhibitory polarity and the disabled class to be excitatory.
Otherwise, we would obtain such pairs as INITIATION;PLANNING (e.g., start X;schedule X), which
indeed express the prevention relation (Barker and Szpakowicz, 1995), i.e., ?scheduling X would not
occur after starting X,? which is different from anomalous obstruction.
Three annotators annotated 200 random samples for each method, and the results of their majority
vote are summarized in Table 7, where Random refers to a random baseline using TP100. The recall
was estimated using the number of positive samples provided by Random. The kappa was .60 (moderate
agreement). 73.5% precision, 26.4% recall against the positive samples in TP100, and more than one
million outputs of our proposed method are reasonably high/large results for this difficult task. Table 8
shows examples of Proposed?s outputs. ?(cannot)? was attached to disabled templates for readability.
Setting/Method Precision # of Pairs Recall
Proposed 73.5 1,081,405 26.4
Random 10.5 28,717,454 100.0
Table 7: Performance of anomalous obstruc-
tion derivation.
prohibit X;(cannot) exhibit X PROHIBIT Class
Future
?
? EXECUTION Class
X wo kinshi-suru;X wo kookai-suru
break X;(cannot) utilize X COLLAPSE CLASS
Future
?
? EXECUTION CLASS
X wo kowasu;X wo riyo-suru
Table 8: Examples of anomalous obstruction.
6 Conclusion
In this work, we manually constructed a Phased Predicate Template Taxonomy (PPTT), which is a net-
work of semantically coherent classes of templates and derived semantic relations including entailment
from it in a million-instance scale. Future work will extend PPTT to cover non-excitatory/non-inhibitory
templates and generate richer structural knowledge similar to full-fledged scripts (Schank and Abelson,
1977) and narrative schemas (Chambers and Jurafsky, 2011).
Acknowledgements
We would like to thank three anonymous reviewers for many useful comments and advices on the
manuscript of this paper.
References
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010. Generating entailment rules from framenet. In Pro-
ceedings of the ACL 2010 Conference Short Papers, ACLShort ?10, pages 241?246, Stroudsburg, PA, USA.
ACL.
Aristotle. 1987. De Anima (Translated by Hugh Lawson-Tancred). Penguin Classics, London.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of
the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference
on Computational Linguistics - Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA, USA. ACL.
Ken Barker and Stan Szpakowicz. 1995. Interactive semantic analysis of clause level relationships. In Proceedings
of PACLING ?95, Brisbane.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cognitive and Psychometric Analysis of Analogical Problem
Solving. Springer-Verlag, New York.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure
optimization. Comput. Linguist., 38(1):73?111, March.
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka Uchimoto, Takayuki Kuribayashi, and Kyoko Kanzaki.
2009. Enhancing the japanese wordnet. In Proceedings of the 7th Workshop on Asian Language Resources,
ALR7, pages 1?8, Stroudsburg, PA, USA. ACL.
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of
ACL-08: HLT, pages 789?797, Columbus, Ohio, June. ACL.
1432
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ?09, pages 602?610,
Stroudsburg, PA, USA. ACL.
Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 976?986, Portland, Oregon, USA, June. ACL.
Timothy Chklovski and Patrick Pantel. 2004a. Path analysis for refining verb relations. In In Proceedings of KDD
Workshop on Link Analysis and Group Detection (LinkKDD-04), Seattle, WA.
Timothy Chklovski and Patrick Pantel. 2004b. Verbocean: Mining the web for fine-grained semantic verb rela-
tions. In Dekang Lin and Dekai Wu, editors, Proceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?04, pages 33?40, Barcelona, Spain, July. ACL.
Robert Coyne and Owen Rambow. 2009. Lexpar: A freely available english paraphrase lexicon automatically
extracted from framenet. In Proceedings of the Third IEEE International Conference on Seman- tic Computing.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational,
evaluation and approaches. Natural Language Engineering, 15(4):i?xvii.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.
Charles J. Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of
Sciences: Conference on the Origin and Development of Language and Speech, 280(1):20?32.
Michael A.K. Halliday. 1985. An Introduction to Functional Grammar. Arnold, 1st edition.
Chikara Hashimoto, Kentaro Torisawa, KowKuroda, Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama. 2009.
Large-scale verb entailment acquisition from the Web. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1172?1181, Singapore, August. ACL.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012. Excitatory
or inhibitory: a new semantic orientation extracts contradiction and causality from the web. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 619?630, Stroudsburg, PA, USA. ACL.
Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer, Motoki Sano, Istv?an Varga, Jong-Hoon Oh, and Yutaka
Kidawara? 2014. Toward future scenario generation: Extracting event causality exploiting semantic relation,
context, and association features. In Proceedings of the 52nd Annual Meeting of the Association for Computa-
tional Linguistics, Baltimore, USA, June. Association for Computational Linguistics.
David A. Jurgens, Peter D. Turney, Saif M. Mohammad, and Keith J. Holyoak. 2012. Semeval-2012 task 2: Mea-
suring degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computa-
tional Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Semantic Evaluation, SemEval ?12, pages 356?364, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending verbnet with novel verb
classes. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC
2006), pages 731?738, Genoa, Italy, June.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa, Chikara Hashimoto, Jong-Hoon Oh, Motoki Sano, and Kiy-
onori Ohtake. 2013. Two-stage method for large-scale acquisition of contradiction pattern pairs using entail-
ment. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages
693?703, Seattle, Washington, USA, October. ACL.
Beth Levin. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago and London.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake. 2013.
Why-question answering using intra- and inter-sentential causal relations. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1733?1743, Sofia,
Bulgaria, August. ACL.
1433
Georgiana Puscasu and Verginica Barbu Mititelu. 2008. Annotation of wordnet verbs with timeml event classes.
In Bente Maegaard Joseph Mariani Jan Odijk Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, editor, Proceedings of the Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
James Pustejovsky, Jos Castao, Robert Ingria, Roser Saur, Robert Gaizauskas, Andrea Setzer, and Graham Katz.
2003. Timeml: Robust specification of event and temporal expressions in text. In in Fifth International Work-
shop on Computational Semantics (IWCS-5.
James Pustejovsky. 1998. The Generative Lexicon. MIT Press, Cambridge.
Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experi-
ments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
979?988, Uppsala, Sweden, July. ACL.
Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding: an Inquiry into Human
Knowledge Structures. L. Erlbaum, Hillsdale, NJ.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456?463,
Prague, Czech Republic, June. ACL.
Hiroya Takamura, Takashi Inui, andManabu Okumura. 2005. Extracting semantic orientations of words using spin
model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),
pages 133?140, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell. 2012. Acquiring temporal constraints between relations.
In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM
?12, pages 992?1001, New York, NY, USA. ACM.
P. D. Turney and S. M. Mohammad. 2014. Experiments with three approaches to recognizing lexical entailment.
Natural Language Engineering, FirstView:1?40, 5.
Istv?an Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon
Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1619?1629, Sofia, Bulgaria, August. ACL.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Language Learning, pages 194?204, Jeju Island, Korea,
July. ACL.
1434
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 987?997,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Toward Future Scenario Generation: Extracting Event Causality
Exploiting Semantic Relation, Context, and Association Features
Chikara Hashimoto? Kentaro Torisawa? Julien Kloetzer? Motoki Sano?
Istva?n Varga? Jong-Hoon Oh? Yutaka Kidawara??
? ? ? ? ? ??National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan
?NEC Knowledge Discovery Research Laboratories, Nara, 630-0101, Japan
{
? ch, ? torisawa, ? julien, ? msano, ? rovellia, ??kidawara}@nict.go.jp
Abstract
We propose a supervised method of
extracting event causalities like conduct
slash-and-burn agriculture?exacerbate
desertification from the web using se-
mantic relation (between nouns), context,
and association features. Experiments
show that our method outperforms base-
lines that are based on state-of-the-art
methods. We also propose methods of
generating future scenarios like conduct
slash-and-burn agriculture?exacerbate
desertification?increase Asian dust (from
China)?asthma gets worse. Experi-
ments show that we can generate 50,000
scenarios with 68% precision. We also
generated a scenario deforestation con-
tinues?global warming worsens?sea
temperatures rise?vibrio parahaemolyti-
cus fouls (water), which is written in no
document in our input web corpus crawled
in 2007. But the vibrio risk due to global
warming was observed in Baker-Austin
et al (2013). Thus, we ?predicted? the
future event sequence in a sense.
1 Introduction
The world can be seen as a network of causal-
ity where people, organizations, and other kinds
of entities causally depend on each other. This
network is so huge and complex that it is almost
impossible for humans to exhaustively predict the
consequences of a given event. Indeed, after the
Great East Japan Earthquake in 2011, few ex-
pected that it would lead to an enormous trade
deficit in Japan due to a sharp increase in en-
ergy imports. For effective decision making that
carefully considers any form of future risks and
chances, we need a system that helps humans do
scenario planning (Schwartz, 1991), which is a
decision-making scheme that examines possible
future events and assesses their potential chances
and risks. Our ultimate goal is to develop a system
that supports scenario planning through generat-
ing possible future events using big data, which
would contain what Donald Rumsfeld called ?un-
known unknowns?1 (Torisawa et al, 2010).
To this end, we propose a supervised method
of extracting such event causality as conduct
slash-and-burn agriculture?exacerbate desertifi-
cation and use its output to generate future sce-
narios (scenarios), which are chains of causal-
ity that have been or might be observed in
this world like conduct slash-and-burn agricul-
ture?exacerbate desertification?increase Asian
dust (from China)?asthma gets worse. Note that,
in this paper, A?B denotes that A causes B, which
means that ?if A happens, the probability of B in-
creases.? Our notion of causality should be inter-
preted probabilistically rather than logically.
Our method extracts event causality based on
three assumptions that are embodied as features
of our classifier. First, we assume that two nouns
(e.g. slash-and-burn agriculture and desertifica-
tion) that take some specific binary semantic rela-
tions (e.g. A CAUSES B) tend to constitute event
causality if combined with two predicates (e.g.
conduct and exacerbate). Note that semantic re-
lations are not restricted to those directly relevant
to causality like A CAUSES B but can be those that
might seem irrelevant to causality like A IS AN
INGREDIENT FOR B (e.g. plutonium and atomic
bomb as in plutonium is stolen?atomic bomb is
made). Our underlying intuition is the observation
that event causality tends to hold between two en-
tities linked by semantic relations which roughly
entail that one entity strongly affects the other.
Such semantic relations can be expressed by (oth-
erwise unintuitive) patterns like A IS AN INGRE-
DIENT FOR B. As such, semantic relations like the
MATERIAL relation can also be useful. (See Sec-
1http://youtu.be/GiPe1OiKQuk
987
tion 3.2.1 for a more intuitive explanation.)
Our second assumption is that there are gram-
matical contexts in which event causality is more
likely to appear. We implement what we con-
sider likely contexts for event causality as con-
text features. For example, a likely context of
event causality (underlined) would be: CO2 levels
rose, so climatic anomalies were observed, while
an unlikely context would be: It remains uncertain
whether if the recession is bottomed the declining
birth rate is halted. Useful context information in-
cludes the mood of the sentences (e.g., the uncer-
tainty mood expressed by uncertain above), which
is represented by lexical features (Section 3.2.2).
The last assumption embodied in our associa-
tion features is that each word of the cause phrase
must have a strong association (i.e., PMI, for ex-
ample) with that of the effect phrase as slash-and-
burn agriculture and desertification in the above
example, as in Do et al (2011).
Our method exploits these features on top of our
base features such as nouns and predicates. Exper-
iments using 600 million web pages (Akamine et
al., 2010) show that our method outperforms base-
lines based on state-of-the-art methods (Do et al,
2011; Hashimoto et al, 2012) by more than 19%
of average precision.
We require that event causality be self-
contained, i.e., intelligible as causality without the
sentences from which it was extracted. For ex-
ample, omit toothbrushing?get a cavity is self-
contained, but omit toothbrushing?get a girl-
friend is not since this is not intelligible without a
context: He omitted toothbrushing every day and
got a girlfriend who was a dental assistant of den-
tal clinic he went to for his cavity. This is im-
portant since future scenarios, which are gener-
ated by chaining event causality as described be-
low, must be self-contained, unlike Hashimoto et
al. (2012). To make event causality self-contained,
we wrote guidelines for manually annotating train-
ing/development/test data. Annotators regarded
as event causality only phrase pairs that were
interpretable as event causality without contexts
(i.e., self-contained). From the training data, our
method seemed to successfully learn what self-
contained event causality is.
Our scenario generation method generates sce-
narios by chaining extracted event causality; gen-
erating A?B?C from A?B and B?C. The chal-
lenge is that many acceptable scenarios are over-
looked if we require the joint part of the chain (B
above) to be an exact match. To increase the num-
ber of acceptable scenarios, our method identifies
compatibility w.r.t causality between two phrases
by a recently proposed semantic polarity, exci-
tation (Hashimoto et al, 2012), which properly
relaxes the chaining condition (Section 3.1 de-
scribes it). For example, our method can iden-
tify the compatibility between sea temperatures
are high and sea temperatures rise to chain global
warming worsens?sea temperatures are high
and sea temperatures rise?vibrio parahaemolyti-
cus2 fouls (water). Accordingly, we generated
a scenario deforestation continues?global warm-
ing worsens?sea temperatures rise?vibrio para-
haemolyticus fouls (water), which is written in
no document in our input web corpus that was
crawled in 2007, but the vibrio risk due to global
warming has actually been observed in the Baltic
sea and reported in Baker-Austin et al (2013). In
a sense, we ?predicted? the event sequence re-
ported in 2013 by documents written in 2007. Our
experiments also show that we generated 50,000
scenarios with 68% precision, which include con-
duct terrorist operations?terrorist bombing oc-
curs?cause fatalities and injuries?cause eco-
nomic losses and the above ?slash-and-burn agri-
culture? scenario (Section 5.2). Neither is written
in any document in our input corpus.
In this paper, our target language is Japanese.
However, we believe that our ideas and methods
are applicable to many languages. Examples are
translated into English for ease of explanation.
Supplementary notes of this paper are available
at http://khn.nict.go.jp/analysis/
member/ch/acl2014-sup.pdf.
2 Related Work
For event causality extraction, clues used by
previous methods can roughly be categorized
as lexico-syntactic patterns (Abe et al, 2008;
Radinsky et al, 2012), words in context (Oh et
al., 2013), associations among words (Torisawa,
2006; Riaz and Girju, 2010; Do et al, 2011), and
predicate semantics (Hashimoto et al, 2012). Be-
sides features similar to those described above, we
propose semantic relation features3 that include
those that are not obviously related to causality.
We show that such thorough exploitation of new
and existing features leads to high performance.
2A bacterium in the sea causing food-poisoning.
3Radinsky et al (2012) and Tanaka et al (2012) used se-
mantic relations to generalize acquired causality instances.
988
Other clues include shared arguments (Torisawa,
2006; Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009), which we ignore since we tar-
get event causality about two distinct entities.
To the best of our knowledge, future scenario
generation is a new task, although previous works
have addressed similar tasks (Radinsky et al,
2012; Radinsky and Horvitz, 2013). Neither in-
volves chaining and restricts themselves to only
one event causality step. Besides, the events they
predict must be those for which similar events
have previously been observed, and their method
only applies to news domain.
Some of the scenarios we generated are written
on no page in our input web corpus. Similarly,
Tsuchida et al (2011) generated semantic knowl-
edge like causality that is written in no sentence.
However, their method cannot combine more than
two pieces of knowledge unlike ours, and their tar-
get knowledge consists of nouns, but ours consists
of verb phrases, which are more informative.
Tanaka et al (2013)?s web information analy-
sis system provides a what-happens-if QA service,
which is based on our scenario generation method.
3 Event Causality Extraction Method
This section describes our event causality extrac-
tion method. Section 3.1 describes how to extract
event causality candidates, and Section 3.2 details
our features. Section 3.3 shows how to rank event
causality candidates.
3.1 Event Causality Candidate Extraction
We extract the event causality between two events
represented by two phrases from single sentences
that are dependency parsed.4 We obtained sen-
tences from 600 million web pages. Each phrase
in the event causality must consist of a predicate
with an argument position (template, hereafter)
like conduct X and a noun like slash-and-burn
agriculture that completes X. We also require the
predicate of the cause phrase to syntactically de-
pend on the effect phrase in the sentence from
which the event causality was extracted; we guar-
antee this by verifying the dependencies of the
original sentence. In Japanese, since the tempo-
ral order between events is usually determined by
precedence in a sentence, we require the cause
phrase to precede the effect phrase. For context
4We used a Japanese dependency parser called J.DepP
(Yoshinaga and Kitsuregawa, 2009), available at http://
www.tkl.iis.u-tokyo.ac.jp/?ynaga/jdepp/.
feature extraction, the event causality candidates
are accompanied by the original sentences from
which they were extracted.
Excitation We only keep the event causality
candidates each phrase of which consists of exci-
tation templates, which have been shown to be ef-
fective for causality extraction (Hashimoto et al,
2012) and other semantic NLP tasks (Oh et al,
2013; Varga et al, 2013; Kloetzer et al, 2013a).
Excitation is a semantic property of templates that
classifies them into excitatory, inhibitory, and neu-
tral. Excitatory templates such as cause X entail
that the function, effect, purpose or role of their ar-
gument?s referent is activated, enhanced, or man-
ifested, while inhibitory templates such as lower
X entail that it is deactivated or suppressed. Neu-
tral ones like proportional to X belong to neither
of them. We collectively call both excitatory and
inhibitory templates excitation templates. We ac-
quired 43,697 excitation templates by Hashimoto
et al?s method and the manual annotation of exci-
tation template candidates.5 We applied the exci-
tation filter to all 272,025,401 event causality can-
didates from the web and 132,528,706 remained.
After applying additional filters (see Section A
in the supplementary notes) including those based
on a stop-word list and a causal connective list
to remove unlikely event causality candidates that
are not removed by the above filter, we finally ac-
quired 2,451,254 event causality candidates.
3.2 Features for Event Causality Classifier
3.2.1 Semantic Relation Features
We hypothesize that two nouns with some particu-
lar semantic relations are more likely to constitute
event causality. Below we describe the semantic
relations that we believe are likely to constitute
event causality.
CAUSATION is the causal relation between two
entities and is expressed by binary patterns like
A CAUSES B. Deforestation and global warming
might complete the A and B slots. We manually
collected 748 binary patterns for this relation. (See
Section B in the supplementary notes for examples
of our binary patterns.)
MATERIAL is the relation between a material
and a product made of it (e.g. plutonium and
5Hashimoto et al?s method constructs a network of tem-
plates based on their co-occurrence in web sentences with a
small number of polarity-assigned seed templates and infers
the polarity of all the templates in the network by a constraint
solver based on the spin model (Takamura et al, 2005).
989
atomic bomb) and can be expressed by A IS MADE
OF B. Its relation to event causality might seem
unclear, but a material can be seen as a ?cause?
of a product. Indeed materials can participate
in event causality with the help of such template
pairs as A is stolen?B is made as in plutonium is
stolen?atomic bomb is made. We manually col-
lected 187 binary patterns for this relation.
NECESSITY?s patterns include A IS NECES-
SARY FOR B, which can be filled with verbal apti-
tude and ability to think. Noun pairs with this rela-
tion can constitute event causality when combined
with template pairs like improve A?cultivate B.
We collected 257 patterns for this relation.
USE is the relation between means (or instru-
ments) and the purpose for using them. A IS USED
FOR B is a pattern of the relation, which can be
filled with e-mailer and exchanges of e-mail mes-
sages. Note that means can be seen as ?causing?
or ?realizing? the purpose of using the means in
this relation, and actually event causality can be
obtained by incorporating noun pairs of this rela-
tion into template pairs like activate A?conduct
B. 2,178 patterns were collected for this relation.
PREVENTION is the relation expressed by pat-
terns like A PREVENTS B, which can be filled with
toothbrushing and periodontal disease. This rela-
tion is, so to speak, ?negative CAUSATION? since
the entity denoted by the noun completing the A
slot makes the entity denoted by the B noun NOT
realized. Such noun pairs mean event causality
by substituting them into template pairs like omit
A?get B. The number of patterns is 490.
The experiments in Section 5.1.1 show that not
only CAUSATION and PREVENTION (?negative
CAUSATION?) but the other relations are also ef-
fective for event causality extraction.
In addition, we invented the EXCITATION rela-
tion that is expressed by binary patterns made of
excitatory and inhibitory templates (Section 3.1).
For instance, we make binary patterns A RISES B
and A LOWERS B from excitatory template rise X
and inhibitory template lower X respectively. The
EXCITATION relation roughly means that A acti-
vates B (excitatory) or suppresses it (inhibitory).
We simply add an additional argument position to
each template in the 43,697 excitation templates to
make binary patterns. We restricted the argument
positions (represented by Japanese postpositions)
of the A slot to either ha (topic marker), ga (nomi-
native), or de (instrumental) and those of the B slot
to either ha, ga, de, wo (accusative), or ni (dative),
SR1: Binary pattern of our semantic relations that co-
occurs with two nouns of an event causality candi-
date in our web corpus.
SR2: Semantic relation types (e.g CAUSATION and EN-
TAILMENT) of the binary pattern of SR1. EXCITA-
TION is divided into six sub types based on the ex-
citation polarity of the binary patterns, the argument
positions, and the existence of causative markers. A
CAUSATION pattern, B BY A, constitutes an indepen-
dent relation called the BY relation.
Table 1: Semantic relation features.
and obtained 55,881 patterns.
Moreover, for broader coverage, we acquired
binary patterns that entail or are entailed by one
of the patterns of the above six semantic relations.
Those patterns were acquired from our web cor-
pus by Kloetzer et al (2013b)?s method, which ac-
quired 185 million entailment pairs with 80% pre-
cision from our web corpus and was used for con-
tradiction acquisition (Kloetzer et al, 2013a). We
acquired 335,837 patterns by this method. They
are class-dependent patterns, which have seman-
tic class restrictions on arguments. The semantic
classes were obtained from our web corpus based
on Kazama and Torisawa (2008). See De Saeger
et al (2009), De Saeger et al (2011) and Kloet-
zer et al (2013a) for more on our patterns. They
collectively constitute the ENTAILMENT relation.
Table 1 shows our semantic relation features. To
use them, we first make a database that records
which noun pairs co-occur with each binary pat-
tern. Then we check a noun pair (the nouns of the
cause and effect phrases) for each event causality
candidate, and give the candidate all the patterns
in the database that co-occur with the noun pair.
3.2.2 Context Features
We believe that contexts exist where event causal-
ity candidates are more likely to appear, as de-
scribed in Section 1. We developed features that
capture the characteristics of likely contexts for
Japanese event causality (See Section C in the sup-
plementary notes). In a nutshell, they represent a
connective (C1 and C2 in Section C), the distance
between the elements of event causality candidate
(C3 and C4), words in context (C5 to C8), the ex-
istence of adnominal modifier (9 to C10), and the
existence of additional arguments of cause and ef-
fect predicates (C13 to C20), among others.
3.2.3 Association Features
These features measure the association strength
between slash-and-burn agriculture and deser-
990
AC1: The CEA value, the sum of AC2, AC3, and AC4.
AC2: Do et al?s S
pp
. This is the association measure
between predicates, which is the product of AC5,
AC6 and AC7 below. They are calculated from the
132,528,706 event causality candidates in Section
3.1. We omit Do et al?s Dist, which is a constant
since we set our window size to one.
AC3: Do et al?s S
pa
. This is the association measure be-
tween arguments and predicates, which is the sum
of AC8 and AC9. They are calculated from the
132,528,706 event causality candidates.
AC4: Do et al?s S
aa
, which is PMI between arguments.
We obtained it in the same way as Filter 5 in the sup-
plementary notes.
AC5: PMI between predicates.
AC6 / AC7: Do et al?s max / IDF .
AC8: PMI between a cause noun and an effect predicate.
AC9: PMI between a cause predicate and an effect noun.
Table 2: CEA-based association features.
tification in conduct slash-and-burn agricul-
ture?exacerbate desertification for instance and
consist of CEA-, Wikipedia-, definition-, and web-
based features. CEA-based features are based
on the Cause Effect Association (CEA) measure
of Do et al (2011). It consists of association
measures like PMI between arguments (nouns),
between arguments and predicates, and between
predicates (Table 2). Do et al used it (along
with discourse relations) to extract event causality.
Wikipedia-based features are the co-occurrence
counts and the PMI values between cause and ef-
fect nouns calculated using Wikipedia (as of 2013-
Sep-19). We also checked whether an Wikipedia
article whose title is a cause (effect) noun con-
tains its effect (cause) noun, as detailed in Section
D.1 in the supplementary notes. Definition-based
features, as detailed in Section D.2 in the sup-
plementary notes, resemble the Wikipedia-based
features except that the information source is the
definition sentences automatically acquired from
our 600 million web pages using the method of
Hashimoto et al (2011). Web-based features
provide association measures between nouns us-
ing various window sizes in the 600 million web
pages. See Section D.3 for detail. Web-based as-
sociation measures were obtained from the same
database as AC4 in Table 2.
3.2.4 Base Features
Base features represent the basic properties of
event causality like nouns, templates, and their ex-
citation polarities (See Section E in the supple-
mentary notes). For B3 and B4, 500 semantic
classes were obtained from our web corpus using
the method of Kazama and Torisawa (2008).
3.3 Event Causality Scoring
Using the above features, a classifier6 classifies
each event causality candidate into causality and
non-causality. An event causality candidate is
given a causality score CScore, which is the SVM
score (distance from the hyperplane) that is nor-
malized to [0, 1] by the sigmoid function 1
1+e
?x
.
Each event causality candidate may be given mul-
tiple original sentences, since a phrase pair can ap-
pear in multiple sentences, in which case it is given
more than one SVM score. For such candidates,
we give the largest score and keep only one origi-
nal sentence that corresponds to the largest score.7
Original sentences are also used for scenario gen-
eration, as described below.
4 Future Scenario Generation Method
Our future scenario generation method creates
scenarios by chaining event causalities. A naive
approach chains two phrase pairs by exact match-
ing. However, this approach would overlook many
acceptable scenarios as discussed in Section 1. For
example, global warming worsens?sea tempera-
tures are high and sea temperatures rise?vibrio
parahaemolyticus fouls (water) can be chained to
constitute an acceptable scenario, but the joint part
is not the same string. Note that the two phrases
are not simply paraphrases; temperatures may be
rising but remain cold, or they may be decreasing
even though they remain high.
What characterizes two phrases that can be the
joint part of acceptable scenarios? Although we
have no definite answer yet, we name it the causal-
compatibility of two phrases and provide its pre-
liminary characterization based on the excitation
polarity. Remember that excitatory templates like
cause X entail that X?s function or effect is acti-
vated, but inhibitory templates like lower X entail
that it is suppressed (Section 3.1). Two phrases
are causally-compatible if they mention the same
entity (typically described by a noun) that is pred-
icated by the templates of the same excitation po-
larity. Indeed, both X rise and X are high are ex-
citatory and hence sea temperatures are high and
sea temperatures rise are causally-compatible.8
6We used SVMlight with the polynominal kernel (d = 2),
available at http://svmlight.joachims.org.
7Future work will exploit other original sentences, as sug-
gested by an anonymous reviewer.
8Using other knowledge like verb entailment (Hashimoto
et al, 2009) can be helpful too, which is further future work.
991
Scenarios (scs) generated by chaining causally-
compatible phrase pairs are scored by Score(sc),
which embodies our assumption that an acceptable
scenario consists of plausible event causality pairs:
Score(sc) =
?
cs?CAUS(sc)
CScore(cs)
where CAUS(sc) is a set of event causality
pairs that constitutes sc and cs is a member of
CAUS(sc). CScore(cs), which is cs?s score,
was described in Section 3.3.
Our method optionally applies the following
two filters to scenarios for better precision: An
original sentence filter removes a scenario if two
event causality pairs that are chained in it are ex-
tracted from original sentences between which no
word overlap exists other than words constituting
causality pairs. In this case, the two event causal-
ity pairs tend to be about different topics and con-
stitute an incoherent scenario. A common argu-
ment filter removes a scenario if a joint part con-
sists of two templates that share no argument in
our ?argument, template? database, which is com-
piled from the syntactic dependency data between
arguments and templates extracted from our web
corpus. Such a scenario tends to be incoherent too.
5 Experiments
5.1 Event Causality Extraction
Next we describe our experiments on event causal-
ity extraction and show (a) that most of our fea-
tures are effective and (b) that our method outper-
forms the baselines based on state-of-the-art meth-
ods (Do et al, 2011; Hashimoto et al, 2012). Our
method achieved 70% precision at 13% recall; we
can extract about 69,700 event causality pairs with
70% precision, as described below.
For the test data, we randomly sampled 23,650
examples of ?event causality candidate, origi-
nal sentence? among which 3,645 were positive
from 2,451,254 event causality candidates ex-
tracted from our web corpus (Section 3.1). For
the development data, we identically collected
11,711 examples among which 1,898 were posi-
tive. These datasets were annotated by three anno-
tators (not the authors), who annotated the event
causality candidates without looking at the origi-
nal sentences. The final label was determined by
majority vote. The training data were created
by the annotators through our preliminary experi-
ments and consists of 112,110 among which 9,657
Method Ave. prec. (%)
Proposed 46.27
w/o Context features 45.68
w/o Association features 45.66
w/o Semantic relation features 44.44
Base features only 41.29
Table 3: Ablation tests.
Semantic relations Ave. prec. (%)
All semantic relations (Proposed) 46.27
CAUSATION 45.86
CAUSATION and PREVENTION 45.78
None (w/o Semantic relation features) 44.44
Table 4: Ablation tests on semantic relations.
were positive. The Kappa (Fleiss, 1971) of their
judgments was 0.67 (substantial agreement (Lan-
dis and Koch, 1977)). These three datasets have
no overlap in terms of phrase pairs. About nine
man-months were required to prepare the data.
Our evaluation is based on average precision;9
we believe that it is important to rank the plausible
event causality candidates higher.
5.1.1 Ablation Tests
We evaluated the features of our method by ab-
lation tests. Table 3 shows the results of remov-
ing the semantic relation, the context, and the as-
sociation features from our method. All the fea-
ture types are effective and contribute to the per-
formance gain that was about 5% higher than the
Base features only. Proposed achieved 70% pre-
cision at 13% recall. We then estimated that, with
the precision rate, we can extract 69,700 event
causality pairs from the 2,451,254 event causality
candidates, among which the estimated number of
positive examples is 377,794.
Next we examined whether the semantic rela-
tions that do not seem directly relevant to causality
like MATERIAL are effective. Table 4 shows that
the performance degraded (46.27 ? 45.86) when
we only used the CAUSATION binary patterns and
their entailing and entailed patterns compared to
Proposed. Even when adding the PREVENTION
(?negative CAUSATION?) patterns and their entail-
ing and entailed patterns, the performance was still
slightly worse than Proposed. The performance
was even worse when using no semantic relation
(?None? in Table 4). Consequently we conclude
that not only semantic relations directly relevant
9It is obtained by computing the precision for each point
in the ranked list where we find a positive sample and aver-
aging all the precision figures (Manning and Schu?tze, 1999).
992
Method Ave. prec. (%)
w/o Wikipedia-based features 46.52
Proposed 46.27
w/o definition-based features 46.21
w/o Web-based features 46.15
w/o CEA-based features 45.80
Table 5: Ablation tests on association features.
Method Ave. prec. (%)
Proposed 46.27
Proposed-CEA 45.80
CEA
sup
21.77
CEA
uns
16.57
Table 6: Average precision of our proposed meth-
ods and baselines using CEA.
to causality like CAUSATION but also those that
seem to lack direct relevance to causality like MA-
TERIAL are somewhat effective.
Finally, Table 5 shows the performance drop
by removing the Wikipedia-, definition-, web-,
and CEA-based features. The CEA-based fea-
tures were the most effective, while the Wikipedia-
based ones slightly degraded the performance.
5.1.2 Comparison to Baseline Methods
We compared our method and two baselines based
on Do et al (2011): CEA
uns
is an unsupervised
method that uses CEA to rank event causality can-
didates, and CEA
sup
is a supervised method us-
ing SVM and the CEA features, whose ranking is
based on the SVM scores. The baselines are not
complete implementations of Do et al?s method
which uses discourse relations identified based on
Lin et al (2010) and exploits them with CEA
within an ILP framework. Nonetheless, we believe
that this comparison is informative since CEA can
be seen as the main component; they achieved a
F1 of 41.7% for extracting causal event relations,
but with only CEA they still achieved 38.6%.
Table 6 shows the average precision of the com-
pared methods. Proposed is our proposed method.
Proposed-CEA is Proposed without the CEA-
features and shows their contribution. Proposed
is the best and the CEA features slightly contribute
to the performance, as Proposed-CEA indicates.
We observed that CEA
sup
and CEA
uns
performed
poorly and tended to favor event causality candi-
dates whose phrase pairs were highly relevant to
each other but described the contrasts of events
rather than event causality (e.g. build a slow mus-
cle and build a fast muscle) probably because their
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
re
ci
si
on
Recall
	
	



Figure 1: Precision-recall curves of proposed
methods and baselines using CEA.
Method Ave. prec. (%)
Proposed 49.64
Cs
uns
30.38
Cs
sup
27.49
Table 7: Average precision of our proposed
method and baselines using Cs.
main components are PMI values. Figure 1 shows
their precision-recall curves.
Next we compared our method with the base-
lines based on Hashimoto et al (2012). They de-
veloped an automatic excitation template acqui-
sition method that assigns each template an ex-
citation value in range [?1, 1] that is positive if
the template is excitatory and negative if it is in-
hibitory. They ranked event causality candidates
by Cs(p
1
, p
2
) = |s
1
| ? |s
2
|, where p
1
and p
2
are
the two phrases of event causality candidates, and
|s
1
| and |s
2
| are the absolute excitation values of
p
1
?s and p
2
?s templates. The baselines are as fol-
lows: Cs
uns
is an unsupervised method that uses
Cs for ranking, and Cs
sup
is a supervised method
using SVM with Cs as the only feature that uses
SVM scores for ranking. Note that some event
causality candidates were not given excitation val-
ues for their templates, since some templates were
acquired by manual annotation without Hashimoto
et al?s method. To favor the baselines for fairness,
the event causality candidates of the development
and test data were restricted to those with excita-
tion values. Since Cs
sup
performed slightly better
when using all of the training data in our prelimi-
nary experiments, we used all of it.
Table 7 shows the average precision of the com-
pared methods. Proposed is our method. Its av-
erage precision is different from that in Table 6
due to the difference in test data described above.
Cs
uns
and Cs
sup
did not perform well. Many
993
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
re
ci
si
on
Recall

	

	
Figure 2: Precision-recall curves of proposed
methods and baselines using Cs.
phrase pairs described two events that often hap-
pen in parallel but are not event causality (e.g. re-
duce the intake of energy and increase the energy
consumption) in the highly ranked event causality
candidates of Cs
uns
and Cs
sup
. Figure 2 shows
their precision-recall curves.
Hashimoto et al (2012) extracted 500,000 event
causalities with about 70% precision. However, as
described in Section 1, our event causality crite-
ria are different; since they regarded phrase pairs
that were not self-contained as event causality
(their annotators checked the original sentences of
phrase pairs to see if they were event causality),
their judgments tended to be more lenient than
ours, which explains the performance difference.
In preliminary experiments, since our proposed
method?s performance degraded when Cs was in-
corporated, we did not use it in our method.
5.2 Future Scenario Generation
To show that our future scenario generation meth-
ods can generate many acceptable scenarios with
reasonable precision, we experimentally com-
pared four methods: Proposed, our scenario
generation method without the two filters, Pro-
posed+Orig, our method with the original sen-
tence filter, Proposed+Orig+Comm, our method
with the original sentence and common argument
filters, and Exact, a method that chains event
causality by exact matching.
Beginning events As the beginning event of a
scenario, we extracted nouns that describe social
problems (social problem nouns, e.g. deforesta-
tion) from Wikipedia to focus our evaluation on
the ability to generate scenarios about them, which
is a realistic use-case of scenario generation. We
extracted 557 social problem nouns and used the
cause phrases of the event causality candidates that
Two-step Three-step
Exact 1,000 (44.10) 1,000 (23.50)
Proposed 2,000 (32.25) 2,000 (12.55)
Proposed+Orig 995 (36.28) 602 (17.28)
Proposed+Orig+Comm 708 (38.70) 339 (17.99)
Table 8: Number of scenario samples and their
precision (%) in parentheses.
consisted of one of the social problem nouns as the
scenario?s beginning event.
Event causality We applied our event causality
extraction method to 2,451,254 candidates (Sec-
tion 3.1) and culled the top 1,200,000 phrase pairs
from them (See Section F in the supplementary
notes for examples). Some phrase pairs have the
same noun pairs and the same template polar-
ity pairs (e.g. omit toothbrushing?get a cavity
and neglect toothbrushing?have a cavity, where
omit X and neglect X are inhibitory and get X and
have X are excitatory). We removed such phrase
pairs except those with the highest CScore, and
960,561 phrase pairs remained, from which we
generated two- or three-step scenarios that con-
sisted of two or three phrase pairs.
Evaluation samples The numbers of two- and
three-step scenarios generated by Proposed were
217,836 and 5,288,352, while those of Exact were
22,910 and 72,746. We sampled 2,000 from Pro-
posed?s two- and three-step scenarios and 1,000
from those of Exact. We applied the filters to the
sampled scenarios of Proposed, and the results
were regarded as the sample scenarios of Pro-
posed+Orig and Proposed+Orig+Comm. Table
8 shows the number and precision of the samples.
Note that, for the diversity of the sampled scenar-
ios, our sampling proceeded as follows: (i) Ran-
domly sample a beginning event phrase from the
generated scenarios. (ii) Randomly sample an ef-
fect phrase for the beginning event phrase from the
scenarios. (iii) Regarding the effect phrase as a
cause phrase, randomly sample an effect phrase
for it, and repeat (iii) up to the specified number
of steps (2 or 3). The samples were annotated by
three annotators (not the authors), who were in-
structed to regard a sample as acceptable if each
event causality that constitutes it is plausible and
the sample as a whole constitutes a single coherent
story. Final judgment was made by majority vote.
Fleiss? kappa of their judgments was 0.53 (moder-
ate agreement), which is lower than the kappa for
the causality judgment. This is probably because
994
Two-step Three-step
Exact 2,085 1,237
Proposed 5,773 0
Proposed+Orig 4,107 0
Proposed+Orig+Comm 3,293 21,153
Table 9: Estimated number of acceptable scenar-
ios with a 70% precision rate.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10000 20000 30000 40000 50000 60000 70000
P
re
ci
si
on
Estimated number of acceptable scenarios

	

	

	

Figure 3: Precision-scenario curves (2-step).
scenario judgment requires careful consideration
about various possible futures for which individ-
ual annotators tend to draw different conclusions.
Result 1 Table 9 shows the estimated number
of acceptable scenarios generated with 70% pre-
cision. The estimated number is calculated as the
product of the recall at 70% precision and the
number of acceptable scenarios in all the gener-
ated scenarios, which is estimated by the anno-
tated samples. Figures 3 and 4 show the precision-
scenario curves for the two- and three-step sce-
narios, which illustrate how many acceptable sce-
narios can be generated with what precision. The
curve is drawn in the same way as the precision-
recall curve except that the X-axis indicates the
estimated number of acceptable scenarios. At
70% precision, all of the proposed methods out-
performed Exact in the two-step setting, and Pro-
posed+Orig+Comm outperformed Exact in the
three-step setting.
Result 2 To evaluate the top-ranked scenarios
of Proposed+Orig+Comm in the three-step set-
ting with more samples, the annotators labeled 500
samples from the top 50,000 of its output. 341
(68.20%) were acceptable, and the estimated num-
ber of acceptable scenarios at a precision rate of
70% and 80% are 26,700 and 5,200 (See Section H
in the supplementary notes). The ?terrorist oper-
ations? scenario and the ?slash-and-burn agricul-
ture? scenario in Section 1 were ranked 16,386th
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  100000 200000 300000 400000 500000 600000 700000
P
re
ci
si
on
Estimated number of acceptable scenarios

	

	

	

Figure 4: Precision-scenario curves (3-step).
and 21,968th. Next we examined how many of
the top 50,000 scenarios were acceptable and non-
trivial, i.e., found in no page in our input web cor-
pus, using the 341 acceptable samples. A scenario
was regarded as non-trivial if its nouns co-occur in
no page of the corpus. 22 among the 341 samples
were non-trivial. Accordingly, we estimate that
we can generate 2,200 (50,000?22
500
) acceptable and
non-trivial scenarios from the top 50,000. (See
Section G in the supplementary notes for exam-
ples of the generated scenarios.)
Discussion Scenario deforestation contin-
ues?global warming worsens?sea temperatures
rise?vibrio parahaemolyticus fouls (water)
was generated by Proposed+Orig+Comm. It
is written in no page in our input web corpus,
which was crawled in 2007.10 But we did find
a paper Baker-Austin et al (2013) that observed
the emerging vibrio risk in the Baltic sea due to
global warming. In a sense, we ?predicted? an
event observed in 2013 from documents written
in 2007, although the scenario was ranked as low
as 240,738th.
6 Conclusion
We proposed a supervised method for event
causality extraction that exploits semantic rela-
tion, context, and association features. We also
proposed methods for our new task, future sce-
nario generation. The methods chain event causal-
ity by causal-compatibility. We generated non-
trivial scenarios with reasonable precision, and
?predicted? future events from web documents.
Increasing their rank is future work.
10The corpus has pages where global warming, sea tem-
peratures, and vibrio parahaemolyticus happen to co-occur.
But they are either diaries where the three words appear sep-
arately in different topics or lists of arbitrary words.
995
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phrased event relation acquisition: Coupling
the relation-oriented and argument-oriented ap-
proaches. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING
2008), pages 1?8.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu,
Takuya Kawada, Kentaro Inui, Sadao Kurohashi,
and Yutaka Kidawara. 2010. Organizing informa-
tion on the web to support user judgments on in-
formation credibility. In Proceedings of 2010 4th
International Universal Communication Symposium
Proceedings (IUCS 2010), pages 122?129.
Craig Baker-Austin, Joaquin A. Trinanes, Nick G. H.
Taylor, Rachel Hartnell, Anja Siitonen, and Jaime
Martinez-Urtaza. 2013. Emerging vibrio risk at
high latitudes in response to ocean warming. Nature
Climate Change, 3:73?77.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation of Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 789?
797.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meet-
ing of the ACL and the 4th IJCNLP of the AFNLP
(ACL-IJCNLP 2009), pages 602?610.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of the IEEE International
Conference on Data Mining (ICDM 2009), pages
764?769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 825?835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), pages 294?303.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Masaki Murata, and Jun?ichi Kazama. 2009. Large-
scale verb entailment acquisition from the web. In
Proceedings of EMNLP 2009: Conference on Em-
pirical Methods in Natural Language Processing,
pages 1172?1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jun?ichi Kazama, and Sadao Kuro-
hashi. 2011. Extracting paraphrases from definition
sentences on the web. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1087?1097.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orienta-
tion extracts contradiction and causality from the
web. In Proceedings of EMNLP-CoNLL 2012: Con-
ference on Empirical Methods in Natural Language
Processing and Natural Language Learning, pages
619?630.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407?
415.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa,
Chikara Hashimoto, Jong-Hoon Oh, and Kiyonori
Ohtake. 2013a. Two-stage method for large-scale
acquisition of contradiction pattern pairs using en-
tailment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2013), pages 693?703.
Julien Kloetzer, Kentaro Torisawa, Stijn De Saeger,
Motoki Sano, Chikara Hashimoto, and Jun Gotoh.
2013b. Large-scale acquisition of entailment pattern
pairs. In Information Processing Society of Japan
(IPSJ) Kansai-Branch Convention 2013.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
pdtb-styled end-to-end discourse parser. Technical
report, School of Computing, National University of
Singapore.
Chris Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pages 1733?
1743.
996
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of Sixth
ACM International Conference on Web Search and
Data Mining (WSDM 2013), pages 255?264.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for news
events prediction. In Proceedings of International
World Wide Web Conference 2012 (WWW 2012),
pages 909?918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In 2010
IEEE Fourth International Conference on Semantic
Computing, pages 361?368.
Peter Schwartz. 1991. The Art of the Long View. Dou-
bleday.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pages 133?140.
Shohei Tanaka, Naoaki Okazaki, and Mitsuru Ishizuka.
2012. Acquiring and generalizing causal inference
rules from deverbal noun constructions. In Proceed-
ings of 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1209?
1218.
Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake,
Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii,
and Kentaro Torisawa. 2013. WISDOM2013: A
large-scale web information analysis system. In
Companion Volume of the Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2013) (Demo Track), pages
45?48.
Kentaro Torisawa, Stijn de Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kak-
izawa, Masaki Murata, Kow Kuroda, and Ichiro Ya-
mada. 2010. Organizing the web?s information ex-
plosion to discover unknown unknowns. New Gen-
eration Computing (Special Issue on Information
Explosion), 28(3):217?236.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT-NAACL2006), pages 57?64.
Masaaki Tsuchida, Kentaro Torisawa, Stijn De
Saeger, Jong Hoon Oh, Jun?ichi Kazama, Chikara
Hashimoto, and Hayato Ohwada. 2011. Toward
finding semantic relations not written in a single sen-
tence: An inference method using auto-discovered
rules. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP 2011), pages 902?910.
Istva?n Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 1619?1629.
Naoki Yoshinaga and Masaru Kitsuregawa. 2009.
Polynomial to linear: Efficient classification with
conjunctive features. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2009), pages 542?1551.
997
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 610?614,
Dublin, Ireland, August 23-24, 2014.
SZTE-NLP: Aspect Level Opinion Mining Exploiting Syntactic Cues
Viktor Hangya
1
, G
?
abor Berend
1
, Istv
?
an Varga
2?
, Rich
?
ard Farkas
1
1
University of Szeged
Department of Informatics
{hangyav,berendg,rfarkas}@inf.u-szeged.hu
2
NEC Corporation, Japan
Knowledge Discovery Research Laboratories
vistvan@az.jp.nec.com
Abstract
In this paper, we introduce our contribu-
tions to the SemEval-2014 Task 4 ? As-
pect Based Sentiment Analysis (Pontiki et
al., 2014) challenge. We participated in
the aspect term polarity subtask where
the goal was to classify opinions related
to a given aspect into positive, negative,
neutral or conflict classes. To solve this
problem, we employed supervised ma-
chine learning techniques exploiting a rich
feature set. Our feature templates ex-
ploited both phrase structure and depen-
dency parses.
1 Introduction
The booming volume of user-generated content
and the consequent popularity growth of online re-
view sites has led to vast amount of user reviews
that are becoming increasingly difficult to grasp.
There is desperate need for tools that can automat-
ically process and organize information that might
be useful for both users and commercial agents.
Such early approaches have focused on deter-
mining the overall polarity (e.g., positive, nega-
tive, neutral, conflict) or sentiment rating (e.g.,
star rating) of various entities (e.g., restaurants,
movies, etc.) cf. (Ganu et al., 2009). While the
overall polarity rating regarding a certain entity
is, without question, extremely valuable, it fails
to distinguish between various crucial dimensions
based on which an entity can be evaluated. Evalu-
ations targeting distinct key aspects (i.e., function-
ality, price, design, etc) provide important clues
that may be targeted by users with different priori-
ties concerning the entity in question, thus holding
?
The work was done while this author was working as a
guest researcher at the University of Szeged
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
much greater value in one?s decision making pro-
cess.
In this paper, we introduce our contribution to
the SemEval-2014 Task 4 ? Aspect Based Sen-
timent Analysis (Pontiki et al., 2014) challenge.
We participated in the aspect term polarity sub-
task where the goal was to classify opinions which
are related to a given aspect into positive, nega-
tive, neutral or conflict classes. We employed su-
pervised machine learning techniques exploiting a
rich feature set for target polarity detection, with
a special emphasis on features that deal with the
detection of aspect scopes. Our system achieved
an accuracy of 0.752 and 0.669 for the restaurant
and laptop domains, respectively.
2 Approach
We employed a four-class supervised (positive,
negative, neutral and conflict) classifier here. As
a normalization step, we converted the given texts
into their lowercased forms. Bag-of-words fea-
tures comprised the basic feature set for our max-
imum entropy classifier, which was shown to be
helpful in polarity detection (Hangya and Farkas,
2013).
In the case of aspect-oriented sentiment detec-
tion, we found it important to locate text parts
that refer to particular aspects. For this, we used
several syntactic parsing methods and introduced
parse tree based features.
2.1 Distance-weighted Bag-of-words Features
Initially, we used n-gram token features (unigrams
and bigrams). It could be helpful to take into con-
sideration the distance between the token in ques-
tion and the mention of the target aspect. The
closer a token is to an entity the more plausible
that the given token is related to the aspect.
610
<ROOT> The food was great but the service was awful .
DT NN VBD JJ CC DT NN VBD JJ .
ROOT
SBJNMOD PRD
COORD
P
CONJ
NMOD SBJ PRD
Figure 1: Dependency parse tree (MATE parser).
For this we used weighted feature vectors, and
weighted each n-gram feature by its distance in to-
kens from the mention of the given aspect:
1
e
1
n
|i?j|
,
where n is the length of the review and the values
i, j are the positions of the actual word and the
mentioned aspect.
2.2 Polarity Lexicon
To examine the polarity of the words comprising
a review, we incorporated the SentiWordNet sen-
timent lexicon (Baccianella et al., 2010) into our
feature set.
In this resource, synsets ? i.e. sets of word
forms sharing some common meaning ? are as-
signed positivity, negativity and objectivity scores.
These scores can be interpreted as the probabilities
of seeing some representatives of the synsets in
a positive, negative and neutral meaning, respec-
tively. However, it is not unequivocal to deter-
mine automatically which particular synset a given
word belongs to with respect its context. Consider
the word form great for instance, which might
have multiple, fundamentally different sentiment
connotations in different contexts, e.g. in expres-
sions such as ?great food? and ?great crisis?.
We determined the most likely synset a particu-
lar word form belonged to based on its contexts by
selecting the synset, the members of which were
the most appropriate for the lexical substitution
of the target word. The extent of the appropri-
ateness of a word being a substitute for another
word was measured relying on Google?s N-Gram
Corpus, using the indexing framework described
in (Ceylan and Mihalcea, 2011).
We look up the frequencies of the n-grams that
we derive from the context by replacing the tar-
get words with its synonyms(great) from various
synsets, e.g. good versus big. We count down the
frequency of the phrases food is good and food is
big in a huge set of in-domain documents (Cey-
lan and Mihalcea, 2011). Than we choose the
meaning which has the highest probability, good
in this case. This way we assign a polarity value
for each word in a text and created three new fea-
tures for the machine learning algorithm, which
are the number of positive, negative and objective
words in the given document.
2.3 Negation Scope Detection
Since negations are quite frequent in user reviews
and have the tendency to flip polarities, we took
special care of negation expressions. We collected
a set of negation expressions, like not, don?t, etc.
and a set of delimiters and, or, etc. It is reasonable
to think that the scope of a negation starts when
we detect a negation word in the sentence and it
lasts until the next delimiter. If an n-gram was in
a negation scope we added a NOT prefix to that
feature.
2.4 Syntax-based Features
It is very important to discriminate between text
fragments that are referring to the given aspect and
the fragments that do not, within the same sen-
tence. To detect the relevant text fragments, we
used dependency and constituency parsers. Since
adjectives are good indicators of opinion polarity,
we add the ones to our feature set which are in
close proximity with the given aspect term. We
define proximity between an adjective and an as-
pect term as the length of the non-directional path
between them in the dependency tree. We gather
adjectives in proximity less than 6.
Another feature, which is not aspect specific but
can indicate the polarity of an opinion, is the polar-
ity of words? modifiers. We defined a feature tem-
plate for tokens whose syntactic head is present in
611
ROOT
S
.
.
S
VP
ADJP
JJ
awful
VBD
was
NP
NN
service
DT
the
CC
but
S
VP
ADJP
JJ
great
VBD
was
NP
NN
food
DT
The
Figure 2: Constituency parse tree (Stanford parser).
our positive or negative lexicon. For dependency
parsing we used the MATE parser (Bohnet, 2010)
trained on the Penn Treebank (penn2malt conver-
sion), an example can be seen on Figure 1.
Besides using words that refer to a given aspect,
we tried to identify subsentences which refers to
the aspect mention. In a sentence we can express
our opinions about more than one aspect, so it is
important not to use subsentences containing opin-
ions about other aspects. We developed a sim-
ple rule based method for selecting the appropri-
ate subtree from the constituent parse of the sen-
tence in question (see Figure 2). In this method,
the root of this subtree is the leaf which contains
the given aspect initially. In subsequent steps the
subtree containing the aspect in its yield gets ex-
panded until the following conditions are met:
? The yield of the subtree consists of at least
five tokens.
? The yield of the subtree does not contain any
other aspect besides the five-token window
frame relative to the aspect in question.
? The current root node of the subtree is either
the non-terminal symbol PP or S.
Relying on these identified subtrees, we intro-
duced a few more features. First, we created
new n-gram features from the yield of the sub-
tree. Next, we determined the polarity of this sub-
tree with a method proposed by Socher et al. ()
and used it as a feature. We also detected those
words which tend to take part in sentences con-
veying subjectivity, using the ?
2
statistics calcu-
lated from the training data. With the help of these
words, we counted the number of opinion indica-
tor words in the subtree as additional features. We
used the Stanford constituency parser (Klein and
Manning, 2003) trained on the Penn Treebank for
these experiments.
2.5 Clustering
Aspect mentions can be classified into a few dis-
tinct topical categories, such as aspects regarding
the price, service or ambiance of some product or
service. Our hypothesis was that the distribution
of the sentiment categories can differ significantly
depending on the aspect categories. For instance,
people might tend to share positive ideas on the
price of some product rather than expressing neg-
ative, neutral or conflicting ideas towards it. In
order to make use of this assumption, we automat-
ically grouped aspect mentions based on their con-
texts as different aspect target words can still refer
to the very same aspect category (e.g. ?delicious
food? and ?nice dishes?).
Clustering of aspect mentions was performed
by determining a vector for each aspect term based
on the words co-occurring with them. 6, 485 dis-
tinct lemmas were found to co-occur with any of
the aspect phrases in the two databases, thus con-
text vectors originally consisted of that many el-
ements. Singular value decomposition was then
used to project these aspect vectors into a lower di-
mensional ?semantic? space, where k-means clus-
tering (with k = 10) was performed over the data
points. For each classification instance, we re-
garded the cluster ID of the particular aspect term
as a nominal feature.
612
3 Results
In this section, we will report our results on the
shared task database which consists of English
product reviews. There are 3, 000 laptop and
restaurant related sentences, respectively. Aspects
were annotated in these sentences, resulting in a
total of 6, 051 annotated aspects. In our experi-
ments, we used maximum entropy classifier with
the default parameter settings of the Java-based
machine learning framework MALLET (McCal-
lum, ).
w
e
i
g
h
t
i
n
g
c
l
u
s
t
e
r
-
p
o
l
a
r
i
t
y
p
a
r
s
e
r
s
s
e
n
t
i
m
e
n
t
0.7
0.72
0.74
0.76
0.78
systems
full-system
baseline
Figure 3: Accuracy on the restaurant test data.
w
e
i
g
h
t
i
n
g
c
l
u
s
t
e
r
-
p
o
l
a
r
i
t
y
p
a
r
s
e
r
s
s
e
n
t
i
m
e
n
t
0.62
0.64
0.66
0.68
0.7
systems
full-system
baseline
Figure 4: Accuracy on the laptop test data.
Our accuracy measured on the restaurant and
laptop test databases can be seen on figures 3 and
4. On the x-axis the accuracy loss can be seen
comparing to our baseline (n-gram features only)
and full-system, while turning off various sets of
features. Firstly, the weighting of n-gram features
are absent, then features based on aspect clustering
and words which indicate polarity in texts. After-
wards, features that are created using dependency
and constituency parsing are turned off and lastly
sentiment features based on the SentiWordNet lex-
icon are ignored. It can be seen that omitting the
features based on parsing results in the most seri-
ous drop in performance. We achieved 1.1 and 2.6
error reduction on the restaurant and laptop test
data using these features, respectively.
In Table 1 the results of several other participat-
ing teams can be seen on the restaurant and laptop
test data. There were more than 30 submissions,
from which we achieved the sixth and third best
results on the restaurants and laptop domains, re-
spectively. At the bottom of the table the official
baselines for each domain can be seen.
Team restaurant laptop
DCU 0.809 0.704
NRC-Canada 0.801 0.704
SZTE-NLP 0.752 0.669
UBham 0.746 0.666
USF 0.731 0.645
ECNU 0.707 0.611
baseline 0.642 0.510
Table 1: Accuracy results of several other partici-
pants. Our system is named SZTE-NLP.
4 Conclusions
In this paper, we presented our contribution to the
aspect term polarity subtask of the SemEval-2014
Task 4 ? Aspect Based Sentiment Analysis chal-
lenge. We proposed a supervised machine learn-
ing technique that employs a rich feature set tar-
geting aspect term polarity detection. Among the
features designed here, the syntax-based feature
group for the determination of the scopes of the as-
pect terms showed the highest contribution. In the
end, our system was ranked as 6
th
and 3
rd
, achiev-
ing an 0.752 and 0.669 accuracies for the restau-
rant and laptop domains, respectively.
613
Acknowledgments
Viktor Hangya and Istv?an Varga were funded in
part by the European Union and the European
Social Fund through the project FuturICT.hu
(T
?
AMOP-4.2.2.C-11/1/KONV-2012-0013).
G?abor Berend and Rich?ard Farkas was partially
funded by the ?Hungarian National Excellence
Program? (T
?
AMOP 4.2.4.A/2-11-1-2012-0001),
co-financed by the European Social Fund.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
Hakan Ceylan and Rada Mihalcea. 2011. An efficient
indexer for large n-gram corpora. In ACL (System
Demonstrations), pages 103?108. The Association
for Computer Linguistics.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In WebDB.
Viktor Hangya and Richard Farkas. 2013. Target-
oriented opinion mining from tweets. In Cognitive
Infocommunications (CogInfoCom), 2013 IEEE 4th
International Conference on, pages 251?254. IEEE.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st ACL, pages 423?430.
Andrew Kachites McCallum. Mallet: A machine
learning for language toolkit.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval ?14.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, October.
614
