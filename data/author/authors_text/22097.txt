Proceedings of the 7th Workshop on Statistical Machine Translation, pages 253?260,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Probes in a Taxonomy of Factored Phrase-Based Models ?
Ondr?ej Bojar, Bushra Jawaid, Amir Kamran
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,jawaid,kamran}@ufal.mff.cuni.cz
Abstract
We introduce a taxonomy of factored phrase-
based translation scenarios and conduct a
range of experiments in this taxonomy. We
point out several common pitfalls when de-
signing factored setups. The paper also de-
scribes our WMT12 submissions CU-BOJAR
and CU-POOR-COMB.
1 Introduction
Koehn and Hoang (2007) introduced ?factors? to
phrase-based MT to explicitly capture arbitrary fea-
tures in the phrase-based model. In essence, input
and output tokens are no longer atomic units but
rather vectors of atomic values encoding e.g. the lex-
ical and morphological information separately. Fac-
tored translation has been successfully applied to
many language pairs and with diverse types of infor-
mation encoded in the additional factors, i.a. (Bojar,
2007; Avramidis and Koehn, 2008; Stymne, 2008;
Badr et al, 2008; Ramanathan et al, 2009; Koehn et
al., 2010; Yeniterzi and Oflazer, 2010). On the other
hand, it happens quite frequently, that the factored
setup causes a loss compared to the phrase-based
baseline. The underlying reason is the complexity of
the search space which gets boosted when the model
explicitly includes detailed information, see e.g. Bo-
jar and Kos (2010) or Toutanova et al (2008).
? This work was supported by the project EuroMatrixPlus
(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of
the Czech Republic) and the Czech Science Foundation grants
P406/11/1499 and P406/10/P259. We are grateful for review-
ers? comments but we have to obey the 6 page limit. Thanks
also to Ales? Tamchyna for supplementary material on MERT.
Number of Number of
Translation Independent Structure
Steps Searches of Searches Nickname
One One ? Direct
Several
One ? Single-Step
Several
Serial Two-Step
Complex Complex
Figure 1: A taxonomy of factored phrase-based models.
In this paper, we first provide a taxonomy of
(phrase-based) translation setups and then we exam-
ine a range of sample configurations in this taxon-
omy. We don?t state universal rules, because the ap-
plicability of each of the setups depends very much
on the particular language pair, text domain and
amount of data available, but we hope to draw at-
tention to relevant design decisions.
The paper also serves as the description of our
WMT12 submissions CU-BOJAR and CU-POOR-
COMB between English and Czech.
2 A Taxonomy of Factored P-B Models
Figure 1 suggests a taxonomy of various Moses se-
tups. Following the definitions of Koehn and Hoang
(2007), a search consists of several translation and
generation steps: translation steps map source fac-
tors to target factors and generation steps produce
target factors from other target factors.
The taxonomy is vaguely linked to the types of
problems that can be expected with a given config-
uration. Direct translation is likely to suffer from
out-of-vocabulary issues (due to insufficient gener-
alization) on either side. Single-step scenarios have
253
a very high risk of combinatorial explosion of trans-
lation options (think cartesian product of all target
side factors) and/or of spurious ambiguity (several
derivations leading to the same output). Such added
ambiguity can lead to n-best lists with way fewer
unique items than the given n, which in turn ren-
ders MERT unstable, see also Bojar and Tamchyna
(2011). Serially connected setups (two as our Two-
Step or more) can lose relevant candidates between
the searches, unless some ambiguous representation
like lattices is passed between the steps.
An independent axis on which Moses setups can
be organized consists of the number and function of
factors on the source and the target side.
We use a very succint notation for the setups ex-
cept the ?complex? one: tX-Y denotes a translation
step between the factors X in the source language
and Y in the target language. Generation steps are
denoted with gY-Z, where both Y and Z are target-
side factors. Individual mapping steps are combined
with a plus, while individual source or target factors
are combined with an ?a?.
As a simple example, tF-F denotes the direct
translation from source form (F ) to the target form.
A linguistically motivated scenario with one search
can be written as tL-L+tT-T+gLaT-F : translate (1)
the lemma (L) to lemma, (2) the morphological tag
(T) to tag independently and (3) finally generate the
target form from the lemma and the tag.
We use two more operators: ?:? delimits al-
ternative decoding paths (Birch et al, 2007) used
within one search and ?=? delimits two independent
searches. A plausible setup is e.g. tF-LaT=tLaT-
F:tL-F motivated as follows: the source word form
is translated to the lemma and tag in the target lan-
guage. Then a second search (whose translation ta-
bles can be trained on larger monolingual data) con-
sists of two alternative decoding paths: either the
pair of L and T is translated into the target form, or
as a fallback, the tag is disregarded and the target
form is guessed only from the lemma (and the con-
text as scored by the language model). The example
also illustrated the priorities of the operators.
3 Common Settings
Throughout the experiments, we use the Moses
toolkit (Koehn et al, 2007) and GIZA++ (Och
Dataset Sents (cs/en) Toks (cs/en) Source
Small 197k parallel 4.2M/4.8M CzEng 1.0 news
Large 14.8M parallel 205M/236M CzEng 1.0 all
Mono 18M/50M 317M/1.265G WMT12 mono
Table 1: Summary of training data.
Decoding Path Language Models BLEU
tF-FaLaT form + lemma + tag 13.05?0.44
tF-FaT form + tag 13.01?0.44
tF-FaLaT form + tag 12.99?0.44
tF-F (baseline) form 12.42?0.44
tF-FaT form 12.19?0.44
tF-FaLaT form 12.08?0.45
Table 2: Direct en?cs translation (a single search with
one translation step only).
and Ney, 2000). The texts were processed us-
ing the Treex platform (Popel and Z?abokrtsky?,
2010)1, which included lemmatization and tagging
by Morce (Spoustova? et al, 2007). After the tag-
ging, we tokenized further so words like ?23-year?
or ?Aktualne.cz? became three tokens.
Our training data is summarized in Table 1.2
In most experiments reported here, we use the
Small dataset only. The language model (LM) for
these experiments is a 5-gram one based on the
target-side of Small only.
Our WMT12 submissions are based on the Large
and Mono data. The language model for the large
experiments uses 6-grams of forms and optionally
8-grams of morphological tags. As in previous
years, the language models are interpolated (to-
wards the best cross entropy on WMT08 dataset)
from domain-specific LMs, e.g. czeng-news, czeng-
techdoc, wmtmono-2011, wmtmono-2012.
Except where stated otherwise, we tune on the of-
ficial WMT10 test set and report BLEU (Papineni et
al., 2002) scores on the WMT11 test set.
4 Direct Setups
Table 2 lists our experiments with direct translation,
various factors and language models in our notation.
1http://ufal.mff.cuni.cz/treex/
2We did not include the parallel en-cs data made available
by the WMT12 organizers. This probably explains our loss
compared to UEDIN but allows a direct comparison with CU
TECTOMT, a deep syntactic MT based on the same data.
254
Decoding Paths LMs Avg. BLEU Eff. Nbl. Size
tL-L+tT-T+gLaT-F:tF-FaLaT F + L + T 13.31?0.06 12.24?1.33
tL-L+tT-T+gLaT-F F + L + T 13.30?0.05 40.33?3.82
tL-L+tT-T+gLaT-F F + T 13.17?0.01 39.91?2.58
tL-L+tT-T+gLaT-F:tF-FaLaT, 200-best-list F + L + T 13.15?0.24 20.47?5.63
tF-FaLaT F + L + T 13.13?0.06 34.28?3.08
tL-L+tT-T+gLaT-F:tF-FaLaT L + T 13.09?0.06 16.65?1.07
tF-FaT F + T 13.08?0.05 39.67?2.21
tL-L+tT-T+gLaT-F:tF-FaT F + T 13.01?0.43 14.87?5.04
tF-F (baseline) F 12.38?0.03 43.13?0.48
tL-L+tT-T+gLaT-F:tF-F F 12.30?0.03 17.83?3.27
Table 3: Results of three MERT runs of several single-step configurations.
Explicit modelling of target-side morphology im-
proves translation quality, compare tF-FaLaT with
the baseline tF-F. However, two results document
that if some detailed information is distinguished in
the output, it introduces target ambiguity and leads
to a loss in BLEU, unless the detailed information is
actually used in the language model: (1) tF-FaLaT
with LM on forms is worse than the baseline tF-F
but tF-FaLaT with all the three language models is
better, (2) tF-FaLaT with two LMs (forms and tags)
is negligibly worse than tF-FaT with the same lan-
guage models.
5 Single-Step Experiments
Single-step scenarios consist of more than one trans-
lation steps within a single search. We do not distin-
guish whether all the translation steps belong to the
same decoding path or to alternative decoding paths.
Table 3 lists several single-step configurations
(and three direct translations for a compari-
son). The single-step configurations always include
the linguistically-motivated tL-L+tT-T+gLaT-F with
varying language models and optionally with an al-
ternative decoding path to serve as the fallback.
Aware of the low stability of MERT (Clark et al,
2011), we run MERT three times and report the av-
erage BLEU score including the standard deviation.
The last column in Table 3 lists the average num-
ber of distinct candidates per sentence in the n-
best lists during MERT, dubbed ?effective n-best list
size?. Unless stated otherwise, we used 100-best
lists. We see that due to spurious ambiguity, e.g.
various segmentations of the input into phrases, the
effective size does not reach even a half of the limit.
We make three observations here:
(1) In this small data setting with a very morpho-
logically rich language, the complex setup tL-L+tT-
T+gLaT-F does not even need the alternative decod-
ing path tF-F. Ramanathan et al (2009) report gains
in English-to-Hindi translation and also probably do
not use alternative decoding paths.
(2) Reducing the range of language models used
leads to worse scores, which is in line with the ob-
servation made with direct setups. We are surprised
by the relative importance of the lemma-based LM.
(3) Alternative decoding paths significantly re-
duce effective n-best list size to just 12?18 unique
candidates per sentence. However, we don?t see
an obvious relation to the stability of MERT: the
standard deviations of BLEU average are very
similar except for two outliers: 13.15?0.24 and
13.01?0.43. One of the outliers, 13.15, is actually
a repeated run of the 13.31 with n-best-list size set
to 200. Here we see a slight increase in the effec-
tive size (20 instead of 12) but also a slight loss
in BLEU. We repeated the 13.31 experiment also
with n ? {300, 400, 500, 600}, three MERT runs for
each n. All the runs reached BLEU of about 13.30
except for one (n = 600) where the score dropped
to 11.50. The low result was obtained when MERT
ended at 25 iterations, the standard limit. On the
other hand, several successful runs also exhausted
the limit.
Figure 2 plots the BLEU scores in the 25 itera-
tions of the underperforming run with n = 600. The
MERT implementation in the Moses toolkit reports
at each iteration what we call ?predicted BLEU?,
i.e. the BLEU of translations selected by the current
255
 
0
 
0.0
2
 
0.0
4
 
0.0
6
 
0.0
8
 
0.1
 
0.1
2
 
0.1
4  0
 
5
 
10
 
15
 
20
 
25
 
0.1
285
 
0.1
29
 
0.1
295
 
0.1
3
 
0.1
305
 
0.1
31
 
0.1
315
BLEU
Iter
atio
ny2: 
Pre
dict
ed
y: P
red
icte
d
y: R
eal
Figure 2: Predicted and real devset BLEU scores.
weight settings from the (accumulated) n-best list.
We plot this predicted BLEU twice: once on the y2
axis alone and for the second time on the primary
y axis together with the real BLEU, i.e. the BLEU
of the dev set when Moses is actually run with the
weight settings. The real BLEU drops several times,
indicating that the prediction was misleading. Sim-
ilar drops were observed in all runs. With bad luck
as here, the iteration limit is reached when the opti-
mization is still recovering from such a drop.
To avoid such a pitfall, one should check the real
BLEU and continue or simply rerun the optimization
if the iteration limit was reached.
6 Two-Step Experiments
The linguistically motivated setups used in the pre-
vious sections are prohibitively expensive for large
data, see also Bojar et al (2009). A number of
researchers have thus tried diving the complexity
of search into two independent phases: (1) transla-
tion and reordering, and (2) conjugation and declina-
tion. The most promising results were obtained with
the second step predicting individual morphological
features using a specialized tool (Toutanova et al,
2008; Fraser et al, 2012). Here, we simply use one
more Moses search as Bojar and Kos (2010).
In the first step, source English gets translated to
a simplified Czech and in the second step, the sim-
plified Czech gets fully inflected.
6.1 Factors in Two-Step Setups
Two-step setups can use factors in the source, middle
or the target language. We experiment with factors
only in the middle language (affecting both the first
and the second search) and use only the form in both
source and target sides.
In the middle language, we experiment with one
or two factors. For presentation purposes, we always
speak about two factors: ?LOF? (?lemma or form?,
i.e. a representation of the lexical information) and
?MOT? (?modified tag?, i.e. representing the mor-
phological properties). In the single-factor experi-
ments the LOF and MOT are simply concatenated
into a token in the shape LOF+MOT.
Figure 3 illustrates the range of LOFs and MOTs
we experimented with. LOF0 and MOT0 are identi-
cal to the standard Czech lemma and morphological
tag as used e.g. in the Prague Dependency Treebank
(Hajic? et al, 2006).
LOF1 and MOT1 together make what Bojar and
Kos (2010) call ?pluslemma?. MOT1 is less com-
plex than the full tag by disregarding morphological
attributes not generally overt in the English source
side. For most words, LOF1 is simply the lemma,
but for frequent words, the full form is used. This
includes punctuation, pronouns and the verbs ?by?t?
(to be) and ?m??t? (to have).
MOT2 uses a more coarse grained part of speech
(POS) than MOT1. Depending on the POS, dif-
ferent attributes are included: gender and number
for nouns, pronouns, adjectives and verbs; case for
nouns, pronouns, adjectives and prepositions; nega-
tion for nouns and adjectives; tense and voice for
verbs and finally grade for adjectives. The remain-
ing grammatical categories are encoded using POS,
number, grade and negation.
6.2 Decoding Paths in Two-Step Setups
Each of the searches in the two-step setup can be
as complex as the various single-step configurations.
We test just one decoding path for the one or two
factors in the middle language.
All experiments with one middle factor (i.e. ?+?)
follow this config: tF-LOF+MOT = tLOF+MOT-F,
i.e. two direct translations where the first one pro-
duces the concatenated LOF and MOT tokens and
the second one consumes them. The first step uses a
5-gram LOF+MOT language model and the second
step uses a 5-gram LM based on forms.
This setup has the capacity to improve transla-
tion quality by producing forms of words never seen
aligned with a given source form. For example the
English word green would be needed in the parallel
256
Word Form LOF0 LOF1 MOT0 MOT1 MOT2 Gloss
lide? c?love?k c?love?k NNMP1-----A---1 NPA- NMP1-A people
by by?t by Vc------------- c--- V----- would
neoc?eka?vali oc?eka?vat oc?eka?vat VpMP---XR-NA--- pPN- VMP-RA expect
Figure 3: Examples of LOFs and MOTs used in our experiments.
Middle Factors 1 2
+ |
LOF0 +/|MOT0 11.11?0.48 12.42?0.48
LOF1 +/|MOT1 12.10?0.48 11.85?0.42
LOF1 +/|MOT2 11.87?0.51 12.47?0.51
Table 4: Two-step experiments.
data with all the morphological variants of the Czech
word zeleny?. Adding the middle step with appro-
priately reduced morphological information so that
only features overt in the source are represented in
the middle tokens (e.g. negation and number but not
the case) allows the model to find the necessary form
anywhere in the target-side data only:
green? zeleny?+NSA-?
{ zelene?ho (genitive)
zelene?mu (dative)
. . .
The experiments with two middle factors (i.e. ?|?)
use this path: tF-LOFaMOT = tLOFaMOT-F:LOF-
F. The first step is identical, except that now we use
two separate LMs, one for LOFs and one for MOTs.
The second step has two alternative decoding paths:
(1) as before, producing the form from both the LOF
and the MOT, and (2) ignoring the morphological
features from the source altogether and using just
target-side context to choose an appropriate form of
the word. This setup is capable of sacrificing ade-
quacy for a more fluent output.
6.3 Experiments with Two-Step Setups
Table 4 reports the BLEU scores when changing the
number of factors (?+? vs. ?|?) in the middle lan-
guage and the type of the LOF and MOT.
We see an interesting difference between MOT1
and MOT0 or 2. The more fine-grained MOT0 or 2
work better in the two-factor ?|? setup that allows
to disregard the MOT, while MOT1 works better in
the direct translation ?+?.
Overall, we see no improvement over the tF-F
baseline (BLEU of 12.42) and this is mainly due to
to the fact that we used Small data in both steps.
7 A Complex Moses Setup
Obviously, many setups fall under the ?complex?
category of our taxonomy, including also some sys-
tem combination approaches. We tried to combine
three Moses systems: (1) CU-BOJAR as described
below, (2) same setup like CU-BOJAR but optimized
towards 1-TER (Snover et al, 2006), and (3) a large-
data two-step setup.3 The system combination is
performed using a fourth Moses search that gets a
lattice (Dyer et al, 2008) of individual systems? out-
puts, performs an identity translation and scores the
candidates by language models and other features.
The lattice is created from the individual system out-
puts in the ROVER style (Matusov et al, 2008) uti-
lizing the source-to-hypothesis word alignments as
produced by the individual systems. We use our sim-
ple implementation for constructing the confusion
networks and converting them to the lattices. The
?combination Moses? was tuned on the WMT11 test
set towards BLEU. The resulting system is called
CU-POOR-COMB, because we felt it underperformed
the individual systems not only in BLEU but also in
an informal subjective evaluation.
Surprisingly, CU-POOR-COMB won the WMT12
automatic evaluation in TER. In the retrospect, this
is caused by TER overemphasizing word-level pre-
cision. CU-POOR-COMB skipped words not con-
firmed by several systems and its hypotheses are
shorter (18.1 toks/sent) than those by CU-BOJAR
(20.1 toks/sents) or the reference (21.9 toks/sent).
A quick manual inspection of 32 sentences suggests
that about one third or quarter of CU-POOR-COMB
suffer from some information loss whereas the rest
are acceptable or even better paraphrases. Prelim-
3The large two-step setup is identical to the one by (Bojar
and Kos, 2010), except that we use only the current Large and
Mono datasets as described in Section 3.
257
Our Scoring matrix.statmt.org
Test Set newstest-2011 newstest-2012
Metric BLEU TER*100 BLEU TER*100 BLEU TER
?cs
CU-POOR-COMB ?used?for? ?tuning? 14.17?0.53 64.07?0.53 14.0 0.741
CU-BOJAR (tFaT-FaT, lex. r.) 18.10?0.55 62.84?0.71 16.07?0.55 65.52?0.59 15.9 0.759
As ? but towards 1-TER 16.10?0.54 61.64?0.59 14.13?0.54 64.28?0.55 ? ?
Large Two-Step 17.34?0.57 63.47?0.66 15.37?0.54 65.85?0.57 ? ?
Unused (tFaT-FaT, dist. reord.) 18.07?0.56 62.74?0.70 15.92?0.57 65.50?0.60 ? ?
Unused (tF-FaT, dist. reord.) 17.85?0.58 63.13?0.68 15.73?0.55 65.85?0.58 ? ?
Unused (tF-F, lex. reord.) 17.73?0.58 63.04?0.68 15.61?0.57 65.76?0.58 ? ?
Unused (tFaT-F, dist. reord.) 17.62?0.56 62.97?0.70 15.33?0.58 65.70?0.59 ? ?
Unused (tF-F, dist. reord.) 17.51?0.57 63.32?0.69 15.48?0.56 65.79?0.58 ? ?
?en
CU-BOJAR (tF-F:tL-F, dist. reord.) 24.65?0.60 58.54?0.66 23.09?0.59 61.24?0.68 21.5 0.726
Unused (tF-F, dist. reord.) 24.62?0.59 58.66?0.66 22.90?0.56 61.63?0.67 ? ?
Table 5: Summary of large data runs and systems submitted to WMT12 manual evaluation. The upper part lists the
two submissions in en?cs translation and two more systems used in CU-POOR-COMB. The lower part of the table
shows the scores for CU-BOJAR when translating to English. All systems reported here use the Large and Mono data.
inary results of WMT 12 manual ranking indicate
that overall, our system combination performs poor.
8 Overview of Systems Submitted
Table 5 summarizes the scores for our two sys-
tem submissions. We report the scores in our to-
kenization on the official test sets of WMT11 and
WMT12 and also the scores as measured by http:
//matrix.statmt.org. Note that for the lat-
ter, we use the detokenized outputs processed by the
recommended normalization script.4
8.1 Details of CU-BOJAR for en?cs
We deliberately used only direct setups for the large
data and due to time constraints, we ran just a few
configurations, see Table 5.
We knew from previous years that including En-
glish (source) POS tag improves overall target sen-
tence structure: English words are often ambiguous
between noun and verb, so without the POS infor-
mation, verbs got often translated as nouns, render-
ing the sentence incomprehensible. Tagging and in-
cluding the source tag helps, as confirmed by the
tFaT-F setup being somewhat better than tF-F.
We also knew that target-side tag LM is helpful
(esp. if we can afford up to 8-grams in the LM).
This was confirmed by tF-FaT being better than tF-
F. Ultimately, we use tags on both sides: tFaT-FaT
4http://www.statmt.org/wmt11/
normalize-punctuation.perl
and get the best scores. This confirms that our par-
allel data is sufficiently large so that even the added
sparsity due to tags does not cause any trouble.
A little gain comes from a lexicalized reorder-
ing model (or-bi-fe) based on word forms, see CU-
BOJAR reaching 18.10 BLEU on WMT11 test set.
8.2 Details of CU-BOJAR for cs?en
For the translation into English, we tested just two
setups: tF-F and tF-F:tL-T. The latter setup falls
back to the Czech lemma, if the exact form is not
available. The gain is only small, because our paral-
lel data is already quite large.
9 Conclusion
We introduced a simple taxonomy of factored
phrase-based setups and conducted several probes
for English?Czech translation. We gained small
improvements in both small and large data settings.
We also warned about some common pitfalls: (1)
all target-side factors should be accompanied with a
language model to compensate for the added sparse-
ness, (2) alternative decoding paths significantly re-
duce the effective n-best list size, and (3) the infa-
mous instability of MERT can be caused by bad luck
at exhausted iteration limit.
On a general note, we learnt that a breadth-first
search for best configurations should be automated
as much as possible so that more human effort can
be invested into analysis.
258
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
pages 763?770, Columbus, Ohio, June. Association
for Computational Linguistics.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for english-to-arabic statistical machine
translation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics on Human Language Technologies: Short Pa-
pers, HLT-Short ?08, pages 153?156, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG Supertags in Factored Statistical Ma-
chine Translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 9?16,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 330?336, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin
Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k Z?abokrtsky?.
2009. English-Czech MT in 2008. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Compu-
tational Linguistics.
Ondr?ej Bojar. 2007. English-to-Czech Factored Machine
Translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 232?239,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL (Short Papers), pages 176?181. The
Association for Computer Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, pages 1012?1020, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proc. of EACL 2012. Associa-
tion for Computational Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, Zdene?k Z?abokrtsky?, and Magda S?evc???kova?
Raz??mova?. 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu
Hoang. 2010. More linguistic annotation for statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 115?120, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose B. Marino,
Matthias Paulik, Salim Roukos, Holger Schwenk, and
Hermann Ney. 2008. System Combination for Ma-
chine Translation of Spoken and Written Language.
IEEE Transactions on Audio, Speech and Language
Processing, 16(7):1222?1237, September.
Franz Josef Och and Hermann Ney. 2000. A Comparison
of Alignment Models for Statistical Machine Transla-
tion. In Proceedings of the 17th conference on Com-
putational linguistics, pages 1086?1090. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
Modular NLP Framework. In Hrafn Loftsson, Eirikur
Ro?gnvaldsson, and Sigrun Helgadottir, editors, Lec-
ture Notes in Artificial Intelligence, Proceedings of the
7th International Conference on Advances in Natu-
ral Language Processing (IceTAL 2010), volume 6233
of Lecture Notes in Computer Science, pages 293?
259
304, Berlin / Heidelberg. Iceland Centre for Language
Technology (ICLT), Springer.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in english-hindi smt. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ?09, pages
800?808, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings AMTA, pages 223?231, August.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Sara Stymne. 2008. German Compounds in Factored
Statistical Machine Translation. In Bengt Nordstrm
and Aarne Ranta, editors, Advances in Natural Lan-
guage Processing, volume 5221 of Lecture Notes in
Computer Science, pages 464?475. Springer Berlin /
Heidelberg.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
pages 514?522, Columbus, Ohio, June. Association
for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from english to turkish. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 454?464,
Uppsala, Sweden, July. Association for Computational
Linguistics.
260
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 37?42,
Dublin, Ireland, August 23-29 2014.
English to Urdu Statistical Machine Translation: Establishing a
Baseline
Bushra Jawaid, Amir Kamran and Ond?ej Bojar
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?m. 25, Praha 1, CZ-118 00, Czech Republic
jawaid,kamran,bojar@ufal.mff.cuni.cz
Abstract
The aim of this paper is to categorize and present the existence of resources for English-
to-Urdu machine translation (MT) and to establish an empirical baseline for this task.
By doing so, we hope to set up a common ground for MT research with Urdu to allow
for a congruent progress in this field. We build baseline phrase-based MT (PBMT) and
hierarchical MT systems and report the results on 3 official independent test sets. On all
test sets, hierarchial MT significantly outperformed PBMT. The highest single-reference
BLEU score is achieved by the hierarchical system and reaches 21.58% but this figure
depends on the randomly selected test set. Our manual evaluation of 175 sentences
suggests that in 45% of sentences, the hierarchical MT is ranked better than the PBMT
output compared to 21% of sentences where PBMT wins, the rest being equal.
1 Introduction
Statistical Machine Translation (SMT) has always been a challenging task for language pairs with
significant word ordering differences and rich inflectional morphology. The language pair such
as English and Urdu, despite of descending from the same family of Indo-European languages,
differs heavily in syntactic strucure and morphological characteristics. English is relatively
fixed word order language and follows subject-verb-object (SVO) structure whereas Urdu uses
restricted free word order language and most commonly follows the SOV pattern. Urdu word
order is restricted for only few parts of speeches such as adjectives always precede nouns and
postpositions follow nouns. Unlike English, Urdu is a pro-drop language. The morphology of
Urdu is similar to other Indo-European languages, e.g. by having inflectional morphological
system.
To the best of our knowledge, the research on English-to-Urdu machine translation has been
very much fragmented, preventing the authors to build upon the works of others. Our underlying
motivation for this paper is to establish a common ground and provide a concise summary of
available data resources and set up reproducible baseline results of several available test sets.
With this basis, future Urdu MT research should be able to stepwise improve the state of the
art, in contrast with the scattered experiments done so far (Khan et al., 2013; Ali et al., 2013;
Ali and Malik, 2010).
In Section 2, the experimental setup and data processing tools are described. Existing corpora
are introduced in Section 3, automatic results are reported in Section 4 and manual evaluation
is discussed in Section 5.
2 Experimental Setup
This section briefly introduces the selection of SMT models that are used to build the baseline
English-Urdu SMT system and also explains the processing of parallel data before passing it to
the MT system.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and
proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.
0/
37
2.1 Two Models of SMT
The state-of-the-art MT toolkit Moses1 (Koehn et al., 2007), offers two mainstream models of
SMT: phrase-based (PBMT) and syntax-based (SBMT) that includes the hierarchical model.
The PBMT model operates only on mapping of source phrases (short sequences of words) to
target phrases. For dealing with word order differences, two rather weak models are available:
lexicalized and distance-based. The lexicalized reordering models (Tillmann, 2004) are consid-
ered more advanced as they condition reordering on the actual phrases, whereas the latter model
makes the reordering cost (paid when picking source phrases out of sequence) dependent only
on the length of the jump. The distance-based model is suited well for local reordering but it is
fairly weak in capturing any long distance reorderings.
The syntax-based model (SBMT) builds upon Synchronous Context-Free Grammar (SCFG)
that synchronously generates source and target sentences. The grammar rules can either consist
of linguistically motivated non-terminals such as NP, VP etc. or the generic non-terminal ?X?
in which case the model is called ?hierarchical phrase-based? (Chiang, 2005; Chiang, 2007). In
either case, the model is capable of capturing long-distance reordering much better than the
lexicalized reordering of PBMT.
2.2 Data Processing and MT Training
For the training of our en-ur translation systems, the standard training pipeline of Moses is used
along with the GIZA++ (Och and Ney, 2000) alignment toolkit and a 5-gram SRILM language
model (Stolcke, 2002). The source texts were processed using the Treex platform (Popel and
?abokrtsk?, 2010)2, which included tokenization and lemmatization.
The target side of the corpus is tokenized using a simple tokenization script3 by Dan Zeman and
it is lemmatized using the Urdu Shallow Parser4 developed by Language Technologies Research
Center of IIIT Hyderabad.
The alignments are learnt from the lemmatized version of the corpus. In all other cases,
word forms (i.e. no morphological decomposition) in their true case (i.e. names capitalized but
sentence starts lowercased) are used. The lexicalized reordering model uses the feature set called
?msd-bidirectional-fe?.
3 Dataset
Parallel and monolingual data resources are very scarce for low-resource language pairs such as
English-Urdu. This section highlights the existing parallel and monolingual data resources that
can be utilized for training SMT models. The number of official test sets are also exhibited.
3.1 Parallel Corpus
Our parallel corpus consists of around 79K sentences collected from five different sources. The
collection comes from several domains such as News, Religion, Technology, Language and Culture
etc. 95% of the data is used for training, whereas the rest is evenly split into dev and test sets.
? Emille: EMILLE (Enabling Minority Language Engineering) (Baker et al., 2002) is a col-
lection of monolingual (written and spoken), parallel and annotated corpora of fourteen
South Asian languages which is distributed by the European Language Resources Associa-
tion (ELRA). The Urdu-English part are documents produced by the British Departments
of Health, Social Services, Education and Skills, and Transport, Local Government and the
Regions of British government translated into Urdu.
In this work, the manually sentence aligned version of English-Urdu Emille corpus Jawaid
and Zeman (2011) is used.
1http://statmt.org/moses/
2http://ufal.mff.cuni.cz/treex/
3The tokenization script can be downloaded from: http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5
4http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php
38
? IPC: The Indic Parallel Corpus (Post et al., 2012)5 is a collection of Wikipedia documents
of six Indian sub-continent languages translated into English through crowdsourcing in the
Amazon Mechanical Turk (MTurk) platform.
The English-Urdu part generally contains four (in some cases three) English translations
for each Urdu sentence. In a separate MTurk task, the Turkers voted which of the English
translations is the best one. The official training, dev and devtest sets is first merged and
afterwards the voting list is used to retrieve only the winning English sentence ignoring
sentences with no votes altogether. The official testset is left unaltered to report our final
results on this data.
? Quran: The publicly available parallel English and Urdu translation of Quranic data6 is
used, which is collected by Jawaid and Zeman (2011) in their work. The data consists of
6K aligned parallel sentences.
? Penn Treebank: Penn Treebank (Marcus et al., 1993) is an annotated corpus of around
4.5 million words originating from Wall Street Journal (WSJ), Brown corpus, Switchboard
and ATIS. The entire treebank in English is released by the Linguistic Data Consortium
(LDC). A subset of the WSJ section whose Urdu translations are provided by Center for
Language Engineering (CLE)7 is used. Out of 2,499 WSJ stories in the Treebank, only 317
are available in Urdu.
? Afrl: Afrl, the largest of the parallel resources we were able to get, is not publicly available.
The corpus originally consists of 87K sentences coming from mix of several domains mainly
news articles. The sentence alignments are manually checked of almost two thirds of the
corpus, around 4K misaligned and 30K duplicate sentences are discarded.
The statistics shown in Table 1 are reported after removing duplicated sentences from each
source. Almost all parallel corpora contained at least tens or hundreds of duplicate sentences.
Afrl on the other hand contained larger chunks of Emille and also smaller subset of Penn Tree-
bank. Around 3K sentences from Afrl that were seen in Emille are discarded but the Penn
Treebank subset of Afrl is left intact because it provides different Urdu translations.
Each parallel corpus is randomly split into train, dev and test sets according to its relative
size.
Corpus Sentences Tokens % of Data Train Dev Test
EN UR
AFRL 50,313 960,683 1,022,563 63.6% 47,769 1,272 1,272
EMILLE 8,629 152,273 199,320 10.9% 8,193 218 218
IPC 7,478 118,644 132,968 9.46% 7,098 190 190
QURAN 6,364 251,387 269,947 8.05% 6,040 162 162
PENN 6,204 158,727 179,457 7.86% 5,888 158 158
TOTAL 78,988 - - 100% 74,988 2,000 2,000
Table 1: Statistics of English-Urdu parallel corpora.
3.2 Monolingual Corpus
Jawaid et al. (2014) release8 a plain and annotated Urdu monolingual corpus of around 95.4
million tokens distributed in around 5.4 million sentences. The monolingual corpus is a mix
5http://joshua-decoder.org/data/indian-parallel-corpora/
6http://ufal.mff.cuni.cz/legacy/umc/005-en-ur/
7http://www.cle.org.pk/software/ling_resources/UrduNepaliEnglishParallelCorpus.htm
8http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5
39
of domains such as News, Religion, Blogs, Literature, Science, Education etc. Only plain text
monolingual data is used to build our language model.
3.3 Official Testsets
In addition to the testset that is created from the parallel corpora resources, results are reported
on three official testsets.
NIST 2008 Open Machine Translation (OpenMT) Evaluation9 has distributed test data from
2 domains: Newswire and Web. The Web data is collected from user forums, discussion groups
and blogs, whereas Newswire data is a mix of newswire stories and data from web. The test data
contain 4 English translations for each Urdu sentence, the first English translation is picked in
all cases. Because the majority of test sets are created in order to faciliatate Urdu-to-English
MT, most of them contain multiple English references against each Urdu source.
Another testset is released with the IPC. Only those sentences are used whose ids are present
in the voting list. The domain of the IPC test set is discussed in Section 3.1.
CLE10 has published small test set from News domain specifically for MT evaluation. The
test data contains 3 Urdu references against each source. All reference translations are used for
the evaluation.
Table 2 shows the number of sentences in each test set that are used for the final evaluation.
We also report the coverage of each test set (calculated on vocabulary size) i.e. how many source
words in a test set were seen in the training data. The notions used in Table 2 to introduce
coverage are explained in Section 4.
NIST 2008 IPC CLE
NewsWire Web Test
Sentences 400 600 544 400
Coverage ALL 84% 91% 90% 87%Except-Afrl 80% 87% 88% 84%
Table 2: Statistics of official English-Urdu test sets.
4 Results
The BLEU metric (Papineni et al., 2002) has been used to evaluate the performance of the
systems. Models are trained on two different datasets: all parallel corpora (referred as ?ALL?)
and parallel data excluding Afrl corpus (referred as ?Except-Afrl?). The latter model is trained
due to the fact that Afrl corpus is publicly not available. The community working on English-
Urdu machine translation can thus have one common baseline that could be used to evaluate
their improved systems in the future. Including Afrl allows us to see the gains in performance
thanks to the additional data.
Table 3 shows the baseline results of phrase-based and hierarchical systems when trained on
both datasets. The results are reported on two test sets: the test set of 2,000 sentences (called
Large in Table 3) as shown in Table 1 and its subset of 728 sentences which excludes 1,272 test
sentences from Afrl (called Small in Table 3).
PBMT performs better when integrated with lexicalized reordering model but Hierarchical
MT outperforms both PBMT setups on both smaller and larger test sets. The absolute BLEU
scores drop by up to 6 points when Afrl is removed from the training data, however they return
back to ?20 when Afrl is also removed from the test set. This highlights the importance of data
overall and the match in domain in particular, as supported by the differences in vocabulary
coverage (see the column ?Coverage? in Table 3).
Table 4 shows the results of the best performing setups (i.e. phrase-based with lexicalized
reordering model and hierarchical model) trained on both training datasets and evaluated on the
9http://catalog.ldc.upenn.edu/LDC2010T21
10http://www.cle.org.pk/software/ling_resources/testingcorpusmt.htm
40
Parallel Corpora Test Set Phrase-based Phrase-based-LexReo Hierarchical Coverage
ALL Large 18.30?0.74 19.19?0.72 21.35?0.84 92%
Except-Afrl Large 12.85?0.74 13.78?0.73 15.11?0.82 78%
Except-Afrl Small 18.41?1.25 19.67?1.27 21.21?1.55 91%
Table 3: Results of Phrase-based, Phrase-based with Lexical Reordering and Hierarchical MT
systems.
official test sets. The BLEU score for CLE test set is reported using all 3 reference translations
at once as well as the average of single-reference BLEUs, taking each reference translation
separately. IPC and NIST2008 results are evaluated on a single reference.
The hierarchical MT performs significantly better than the phrase-based MT on all test sets.
The lowest scores were achieved on the NIST2008 test set but it is difficult to pinpoint any
specific reason (other than some domain difference) because the coverage is comparable to other
test sets (see Table 2). Across all the test sets, Afrl corpus brings about 2 points BLEU absolute.
CLE IPC NIST2008
3 refs 1 ref (avg.) 1 ref 1 ref
ALL Phrase-based-LexReo 18.19?1.19 11.12?1.02 15.82?1.36 15.13?0.95Hierarchical 19.29?1.31 11.81?1.09 18.70?1.64 16.69?1.06
Except-Afrl Phrase-based-LexReo 16.53?1.13 9.92?0.96 13.82?1.20 11.65?0.87Hierarchical 18.48?1.28 11.30?1.03 16.91?1.54 13.01?0.84
Table 4: Results of Phrase-based and Hierarchical systems on official test sets.
5 Manual Evaluation
To manually analyze the output of best performing models sample of 175 sentences is randomly
selected from the large test set translated using both PBMT with lexical reordering and hier-
archical models trained on ?ALL? data sets. QuickJudge11 is used to rank the outputs. The
annotator is shown the source, reference and output from both machine translation systems, the
identity of the MT systems is not known. There are four permitted outcomes of the ranking:
both systems marked as equally good; both systems are equally bad or the output of one of the
systems is better than the other one. Here is the summary of annotation by a single annotator:
? Out of 175 sentences, 41 sentences received equally bad translations from both systems.
? 17 items are marked as equally good.
? In 79 cases, the hierarchical MT is ranked better than the phrase-based MT.
? In the remaining 38 cases, the phrase-based MT is ranked better than the hierarchical MT.
The results from the manual ranking show that the hierarchical systems wins twice more often
than PBMT. The two systems tie in about one third of input sentences, of which about 70 %
are cases where the translations are bad.
6 Conclusion
In this work, a collection of sizeable English-Urdu corpora is summarized for statistical machine
translation. These resources are used to build baseline phrase-based and hierarchical MT systems
for translation into Urdu and the results are reported on 3 independent official test sets. This
11http://ufal.mff.cuni.cz/project/euromatrix/quickjudge/
41
can hopefully serve as a baseline for a wider community of researchers. The output of both
translation models is manually analyzed and it confirms that the hierarchical model is preferred
over phrase-based MT for English-to-Urdu translation.
Acknowledgments
This work has been using language resources developed and/or stored and/or distributed
by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project
LM2010013). This work was also supported by the grant FP7-ICT-2011-7-288487 (MosesCore)
of the European Union.
References
Aasim Ali and Muhmmad Kamran Malik. 2010. Development of parallel corpus and english to urdu
statistical machine translation. Int. J. of Engineering & Technology IJET-IJENS, 10:31?33.
Aasim Ali, Arshad Hussain, and Muhammad Kamran Malik. 2013. Model for english-urdu statistical
machine translation. World Applied Sciences, 24:1362?1367.
Paul Baker, Andrew Hardie, Tony McEnery, Hamish Cunningham, and Robert J. Gaizauskas. 2002.
Emille, a 67-million word corpus of indic languages: Data collection, mark-up and harmonisation. In
LREC. European Language Resources Association.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of
ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201?228, June.
Bushra Jawaid and Daniel Zeman. 2011. Word-order issues in english-to-urdu statistical machine trans-
lation. Number 95, pages 87?106, Praha, Czechia.
Bushra Jawaid, Amir Kamran, and Ond?ej Bojar. 2014. A Tagged Corpus and a Tagger for Urdu (to
appear). Reykjav?k, Iceland. European Language Resources Association. In print.
Nadeem Khan, Waqas Anwar, Usama Ijaz Bajwa, and Nadir Durrani. 2013. English to urdu hierarchical
phrase-based statistical machine translation. In The 4th Workshop on South and Southeast Asian NLP
(WSSANLP), IJCNLP, pages 72?76, Nagoya, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In
Proc. of ACL Companion Volume, Demo and Poster Sessions, pages 177?180, Prague, Czech Republic.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Comput. Linguist., 19(2):313?330, June.
Franz Josef Och and Hermann Ney. 2000. A Comparison of Alignment Models for Statistical Machine
Translation. In Proc. of COLING, pages 1086?1090. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of ACL, pages 311?318.
Martin Popel and Zden?k ?abokrtsk?. 2010. TectoMT: Modular NLP Framework. In Lecture Notes in
Artificial Intelligence, Proceedings of the 7th International Conference on Advances in Natural Language
Processing (IceTAL 2010), volume 6233 of Lecture Notes in Computer Science, pages 293?304. Springer.
Matt Post, Chris Callison-Burch, and Miles Osborne. 2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proc. of WMT, ACL, pages 401?409, Montr?al, Canada.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In In Proceedings of the 7th
International Conference on Spoken Language Processing (ICSLP) 2002, pages 901?904.
Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proc. of
HLT-NAACL Short Papers, pages 101?104.
42
