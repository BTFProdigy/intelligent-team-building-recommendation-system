Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 740?747, Vancouver, October 2005. c?2005 Association for Computational Linguistics
BLANC1: Learning Evaluation Metrics for MT
Lucian Vlad Lita and Monica Rogati and Alon Lavie
Carnegie Mellon University
{llita,mrogati,alavie}@cs.cmu.edu
Abstract
We introduce BLANC, a family of dy-
namic, trainable evaluation metrics for ma-
chine translation. Flexible, parametrized
models can be learned from past data and
automatically optimized to correlate well
with human judgments for different cri-
teria (e.g. adequacy, fluency) using dif-
ferent correlation measures. Towards this
end, we discuss ACS (all common skip-
ngrams), a practical algorithm with train-
able parameters that estimates reference-
candidate translation overlap by comput-
ing a weighted sum of all common skip-
ngrams in polynomial time. We show that
the BLEU and ROUGE metric families are
special cases of BLANC, and we compare
correlations with human judgments across
these three metric families. We analyze the
algorithmic complexity of ACS and argue
that it is more powerful in modeling both
local meaning and sentence-level structure,
while offering the same practicality as the
established algorithms it generalizes.
1 Introduction
Although recent MT evaluation methods show
promising correlations to human judgments in terms
of adequacy and fluency, there is still considerable
room for improvement (Culy and Riehemann, 2003).
Most of these studies have been performed at a sys-
tem level and have not investigated metric robust-
ness at a lower granularity. Moreover, even though
the emphasis on adequacy vs. fluency is application-
dependent, automatic evaluation metrics do not dis-
tinguish between the need to optimize correlation
with regard to one or the other.
Machine translation automatic evaluation metrics
face two important challenges: the lack of powerful
features to capture both sentence level structure and
local meaning, and the difficulty of designing good
functions for combining these features into meaning-
ful quality estimation algorithms.
In this paper, we introduce BLANC1, an automatic
MT evaluation metric family that is a generaliza-
tion of popular and successful metric families cur-
rently used in the MT community (BLEU, ROUGE, F-
measure etc.). We describe an efficient, polynomial-
time algorithm for BLANC, and show how it can be
optimized to target adequacy, fluency or any other
criterion. We compare our metric?s performance
with traditional and recent automatic evaluation met-
rics. We also describe the parameter conditions under
which BLANC can emulate them.
Throughout the remainder of this paper, we dis-
tinguish between two components of automatic MT
evaluation: the statistics computed on candidate
and reference translations and the function used in
defining evaluation metrics and generating transla-
tion scores. Commonly used statistics include bag-
of-words overlap, edit distance, longest common sub-
sequence, ngram overlap, and skip-bigram overlap.
Preferred functions are various combinations of pre-
cision and recall (Soricut and Brill, 2004), including
1Since existing evaluation metrics (e.g. BLEU, ROUGE) are
special cases of our metric family, it is only natural to name it
Broad Learning and Adaptation for Numeric Criteria (BLANC) ?
white light contains light of all frequencies
740
weighted precision and F-measures (Van-Rijsbergen,
1979).
BLANC implements a practical algorithm with
learnable parameters for automatic MT evaluation
which estimates the reference-candidate translation
overlap by computing a weighted sum of common
subsequences (also known as skip-ngrams). Com-
mon skip-ngrams are sequences of words in their
sentence order that are found both in the reference
and candidate translations. By generalizing and sep-
arating the overlap statistics from the function used
to combine them, and by identifying the latter as a
learnable component, BLANC subsumes the ngram
based evaluation metrics as special cases and can
better reflect the need of end applications for ade-
quacy/fluency tradeoffs .
1.1 Related Work
Initial work in evaluating translation quality focused
on edit distance-based metrics (Su et al, 1992; Akiba
et al, 2001). In the MT context, edit distance (Lev-
enshtein, 1965) represents the amount of word inser-
tions, deletions and substitutions necessary to trans-
form a candidate translation into a reference trans-
lation. Another evaluation metric based on edit dis-
tance is the Word Error Rate (Niessen et al, 2000)
which computes the normalized edit distance. BLEU
is a weighted precision evaluation metric introduced
by IBM (Papineni et al, 2001). BLEU and its exten-
sions/variants (e.g. NIST (Doddington, 2002)) have
become de-facto standards in the MT community and
are consistently being used for system optimization
and tuning. These methods rely on local features
and do not explicitly capture sentence-level features,
although implicitly longer n-gram matches are re-
warded in BLEU. The General Text Matcher (GTM)
(Turian et al, 2003) is another MT evaluation method
that rewards longer ngrams instead of assigning them
equal weight.
(Lin and Och, 2004) recently proposed a set of
metrics (ROUGE) for MT evaluation. ROUGE-L is a
longest common subsequence (LCS) based automatic
evaluation metric for MT. The intuition behind it is
that long common subsequences reflect a large over-
lap between a candidate translation and a reference
translation. ROUGE-W is also based on LCS, but
assigns higher weights to sequences that have fewer
gaps. However, these metrics still do not distinguish
among translations with the same LCS but different
number of shorter sized subsequences, also indica-
tive of overlap. ROUGE-S attempts to correct this
problem by combining the precision/recall of skip-
bigrams of the reference and candidate translations.
However, by using skip-ngrams with n?=2, we might
be able to capture more information encoded in the
higher level sentence structure. With BLANC, we
propose a way to exploit local contiguity in a man-
ner similar to BLEU and also higher level structure
similar to ROUGE type metrics.
2 Approach
We have designed an algorithm that can perform a
full overlap search over variable-size, non-contiguous
word sequences (skip-ngrams) efficiently. At first
glance, in order to perform this search, one has to
first exhaustively generate all skip-ngrams in the can-
didate and reference segments and then assess the
overlap. This approach is highly prohibitive since the
number of possible sequences is exponential in the
number of words in the sentence. Our algorithm ?
ACS (all common skip-ngrams) ? directly constructs
the set of overlapping skip-ngrams through incremen-
tal composition of word-level matches. With ACS,
we can reduce computation complexity to a fifth de-
gree polynomial in the number of words.
Through the ACS algorithm, BLANC is not limited
only to counting skip-ngram overlap: the contribu-
tion of different skip-ngrams to the overall score is
based on a set of features. ACS computes the over-
lap between two segments of text and also allows
local and global features to be computed during the
overlap search. These local and global features are
subsequently used to train evaluation models within
the BLANC family. We introduce below several sim-
ple skip-ngram-based features and show that special-
case parameter settings for these features emulate the
computation of existing ngram-based metrics. In or-
der to define the relative significance of a particular
skip-ngram found by the ACS algorithm, we employ
an exponential model for feature integration.
2.1 Weighted Skip-Ngrams
We define skip-ngrams as sequences of n words taken
in sentence order allowing for arbitrary gaps. In algo-
rithms literature skip-ngrams are equivalent to subse-
quences. As special cases, skip-ngrams with n=2 are
741
referred to as skip-bigrams and skip-ngrams with no
gaps between the words are simply ngrams. A sen-
tence S of size |S| has C(|S|, n) = |S|!(|S|?n)!n! skip-
ngrams.
For example, the sentence ?To be or not to be? has
C(6, 2) = 15 corresponding skip-bigrams including
?be or?, ?to to?, and three occurrences of ?to be?.
It also has C(6, 4) = 15 corresponding skip-4grams
(n = 4) including ?to be to be? and ?to or not to?.
Consider the following sample reference and can-
didate translations:
R0: machine translated text is evaluated automatically
K1: machine translated stories are chosen automatically
K2: machine and human together can forge a friendship that
cannot be translated into words automatically
K3: machine code is being translated automatically
The skip-ngram ?machine translated automati-
cally? appears in both the reference R0 and all candi-
date translations. Arguably, a skip-bigram that con-
tains few gaps is likely to capture local structure
or meaning. At the same time, skip-ngrams spread
across a sentence are also very useful since they may
capture part of the high level sentence structure.
We define a weighting feature function for skip-
ngrams that estimates how likely they are to capture
local meaning and sentence structure. The weighting
function ? for a skip-ngram w1 ..wn is defined as:
?(w1..wn) = e???G(w1..wn) (1)
where ? ? 0 is a decay parameter and G(w1..wn)
measures the overall gap of the skip-ngram w1..wn in
a specific sentence. This overall skip-ngram weight
can be decomposed into the weights of its constituent
skip-bigrams:
?(w1..wn) = e???G(w1,..,wn) (2)
= e???
Pn?1
i=1 G(wi,wi+1)
=
n?1
?
i=1
?(wi wi+1) (3)
In equation 3, ?(wi wi+1) is the number of words
between wi and wi+1 in the sentence. In the example
above, the skip-ngram ?machine translated automat-
ically? has weight e?3? for sentence K1 and weight
e?12? = 1 for sentence K2.
In our initial experiments the gap G has been ex-
pressed as a linear function, but different families of
functions can be explored and their corresponding pa-
rameters learned. The parameter ? dictates the be-
havior of the weighting function. When ? = 0 ?
equals e0 = 1, rendering gap sizes irrelevant. In this
case, skip-ngrams are given the same weight as con-
tiguous ngrams. When ? is very large, ? approaches
0 if there are any gaps in the skip-ngram and is 1 if
there are no gaps. This setting has the effect of con-
sidering only contiguous ngrams and discarding all
skip-ngrams with gaps.
In the above example, although the skip-ngram
?machine translated automatically? has the same cu-
mulative gap in both in K1 and K3, the occurrence in
K1 has is a gap distribution that more closely reflects
that of the reference skip-ngram in R0. To model gap
distribution differences between two occurrences of a
skip-ngram, we define a piece-wise distance function
?XY between two sentences x and y. For two succes-
sive words in the skip-ngram, the distance function is
defined as:
?XY (w1w2) = e???|GX(w1,w2)?GY (w1,w2)| (4)
where ? ? 0 is a decay parameter. Intuitively, the
? parameter is used to reward better aligned skip-
ngrams. Similar to the ? function, the overall ?XY
distance between two occurrences of a skip-ngram
with n > 1 is:
?XY (w1..wn) =
n?1
?
i=1
?XY (wiwi+1) (5)
Note that equation 5 takes into account pairs of skip-
ngrams skip in different places by summing over
piecewise differences. Finally, using an exponen-
tial model, we assign an overall score to the matched
skip-ngram. The skip-ngram scoring function Sxy al-
lows independent features to be incorporated into the
overall score:
Sxy(wi..wk) = ?(wi..wk) ? ?xy(wi..wk)
?e?1f1(wi..wk) ? ... ? e?hfh(wi..wk) (6)
where features f1..fh can be functions based on the
syntax, semantics, lexical or morphological aspects
of the skip-ngram. Note that different models for
combining skip-ngram features can be used in con-
junction with ACS.
742
2.2 Multiple References
In BLANC we incorporate multiple references in a
manner similar to the ROUGE metric family. We
compute the precision and recall of each size skip-
ngrams for individual references. Based on these we
combine the maximum precision and maximum re-
call of the candidate translation obtained using all
reference translations and use them to compute an ag-
gregate F-measure.
The F-measure parameter ?F is modeled by
BLANC. In our experiments we optimized ?F indi-
vidually for fluency and adequacy.
2.3 The ACS Algorithm
We present a practical algorithm for extracting All
Common Skip-ngrams (ACS) of any size that appear
in the candidate and reference translations. For clar-
ity purposes, we present the ACS algorithm as it
relates to the MT problem: find all common skip-
ngrams (ACS) of any size in two sentences X and Y :
wSKIP ? Acs(?, ?,X, Y ) (7)
= {wSKIP1..wSKIPmin(|X|,|Y |)} (8)
where wSkipn is the set of all skip-ngrams of size n
and is defined as:
wSKIPn = {?w1..wn? | wi ? X,wi ? Y,?i ? [1..n]
and wi ? wj ,?i < j ? [1..n]}
Given two sentences X and Y we observe a match
(w, x, y) if word w is found in sentence X at index x
and in sentence Y at index y:
(w, x, y) ? {0 ? x ? |X|, 0 ? y ? |Y |,
w ? V, and X[x] = Y [y] = w} (9)
where V is the vocabulary with a finite set of words.
In the following subsections, we present the fol-
lowing steps in the ACS algorithm:
1. identify all matches ? find matches and generate
corresponding nodes in the dependency graph
2. generate dependencies ? construct edges ac-
cording to pairwise match dependencies
3. propagate common subsequences ? count
all common skip-ngrams using corresponding
weights and distances
In the following sections we use the following exam-
ple to illustrate the intermediate steps of ACS.
X. ?to be or not to be?
Y. ?to exist or not be?
2.3.1 Step 1: Identify All Matches
In this step we identify all word matches (w, x, y)
in sentences X and Y . Using the example above, the
intermediate inputs and outputs of this step are:
Input: X. ?to be or not to be?
Y. ?to exist or not be?
Output: (to,1,1); (to,5,1); (or,3,3); (be,2,5); . . .
For each match we create a corresponding node N
in a dependency graph. With each node we associate
the actual word matched and its corresponding index
positions in both sentences.
2.3.2 Step 2: Generate Dependencies
A dependency N1 ? N2 occurs when the two
corresponding matches (w1, x1, y1) and (w2, x2, y2)
can form a valid common skip-bigram: i.e. when
x1 < x2 and y1 < y2. Note that the matches can
cover identical words, but their indices cannot be the
same (x1 6= x2 and y1 6= y2) since a skip-bigram
requires two different word matches.
In order to facilitate the generation of all common
subsequences, the graph is populated with the
appropriate dependency edges:
for each node N in DAG
for each node M 6=N in DAG
if N(x)?M(x) and N(y)?M(y)
create edge E: N?M
compute ?XY (E)
compute ?(E)
This step incorporates the concepts of skip-ngram
weight and distance into the graph. With each edge
E : N1 ? N2 we associate step-wise weight and dis-
tance information for the corresponding skip-bigram
formed by matches (w1, x1, y1) and (w2, x2, y2).
Note that rather than counting all skip-ngrams,
which would be exponential in the worst case sce-
nario, we only construct a structure of match depen-
dencies (i.e. skip-bigrams). As in dynamic program-
ming, in order to avoid exponential complexity, we
compute individual skip-ngram scores only once.
2.3.3 Step 3: Propagate Common Subsequences
In this last step, the ACS algorithm counts all com-
mon skip-ngrams using corresponding weights and
distances. In the general case, this step is equiva-
lent measuring the overlap of the two sentences X
and Y . As a special case, if no features are used, the
743
ACS algorithm is equivalent to counting the number
of common skip-ngrams regardless of gap sizes.
// depth first search (DFS)
for each node N in DAG
compute node N?s depth
// initialize skip-ngram counts
for each node N in DAG
vN [1]? 1
for i=2 to LCS(X,Y)
vN [i] = 0
// compute ngram counts
for d=1 to MAXDEPTH
for each node N of depth d in DAG
for each edge E: N?M
for i=2 to d
vM [i] += Sxy(?(E), ?(E), vN [i-1])
After algorithm ACS is run, the number of skip-
ngrams (weighted skip-ngram score) of size k is sim-
ply the sum of the number of skip-ngrams of size k
ending in each node N ?s corresponding match:
wSKIPk =
?
Ni?DAG
vNi [k] (10)
2.3.4 ACS Complexity and Feasibility
In the worst case scenario, both sentences X and Y
are composed of exactly the same repeated word: X
= ?w w w w .. ? and Y = ?w w w w ..?. We let m = |X|
and n = |Y |. In this case, the number of matches is
M = n ? m. Therefore, Step 1 has worst case time
and space complexity of O(m ? n). However, em-
pirical data suggest that there are far fewer matches
than in the worst-case scenario and the actual space
requirements are drastically reduced. Even in the
worst-case scenario, if we assume the average sen-
tences is fewer than 100 words, the number of nodes
in the DAG would only be 10, 000. Step 2 of the al-
gorithm consists of creating edges in the dependency
graph. In the worst case scenario, the number of di-
rected edges is O(M2) and furthermore if the sen-
tences are uniformly composed of the same repeated
word as seen above, the worst-case time and space
complexity is m(m+1)/2 ?n(n+1)/2 = O(m2n2).
In Step 3 of the algorithm, the DFS complexity for
computing of node depths is O(M) and the complex-
ity of LCS(X,Y ) is O(m ? n). The dominant step
is the propagation of common subsequences (skip-
ngram counts). Let l be the size of the LCS. The up-
per bound on the size of the longest common subse-
quence is min(|X|, |Y |) = min(m,n). In the worst
case scenario, for each node we propagate l count val-
ues (the size of vector v) to all other nodes in the
DAG. Therefore, the time complexity for Step 3 is
O(M2 ? l) = O(m2n2l) (fifth degree polynomial).
3 BLANC as a Generalization of BLEU and
ROUGE
Due to its parametric nature, the All Common Sub-
sequences algorithm can emulate the ngram compu-
tation of several popular MT evaluation metrics. The
weighting function ? allows skip-ngrams with differ-
ent gap sizes to be assigned different weights. Param-
eter ? controls the shape of the weighting function.
In one extreme scenario, if we allow ? to take
very large values, the net effect is that all contiguous
ngrams of any size will have corresponding weights
of e0 = 1 while all other skip-ngrams will have
weights that are zero. In this case, the distance
function will only apply to contiguous ngrams which
have the same size and no gaps. Therefore, the dis-
tance function will also be 1. The overall result is
that the ACS algorithm collects contiguous common
ngram counts for all ngram sizes. This is equivalent
to computing the ngram overlap between two sen-
tences, which is equivalent to the ngram computa-
tion performed BLEU metric. In addition to comput-
ing ngram overlap, BLEU incorporates a thresholding
(clipping) on ngram counts based on reference trans-
lations, as well as a brevity penalty which makes sure
the machine-produced translations are not too short.
In BLANC, this is replaced by standard F-measure,
which research (Turian et al, 2003) has shown it can
be used successfully in MT evaluation.
Another scenario consists of setting the ? and ?
parameters to 0. In this case, all skip-ngrams are as-
signed the same weight value of 1 and skip-ngram
matches are also assigned the same distance value of
1 regardless of gap sizes and differences in gap sizes.
This renders all skip-ngrams equivalent and the ACS
algorithm is reduced to counting the skip-ngram over-
lap between two sentences. Using these counts, pre-
cision and recall-based metrics such as the F-measure
can be computed. If we let the ? and ? parameters to
be zero, disregard redundant matches, and compute
744
0 50 100
0
50
100
150
200
Arabic 2003
Sentence Length
#s
en
te
nc
es
0 50 100
0
50
100
150
200
250
300
350
Chinese 2003
Sentence Length
#s
en
te
nc
es
0 50 100
100
102
104
ACS #Matches
Sentence Length
Av
g 
#M
at
ch
es
0 50 100
100
105
ACS #Edges
Sentence Length
Av
g 
#E
dg
es
0 50 100
100
105
1010
ACS #Feature Calls
Sentence Length
Av
g 
#T
ot
al
Arabic
Chinese
Worst Case
Figure 1: Empirical and theoretical behavior of ACS on 2003 machine translation evaluation data (semilog scale).
the ACS only for skip-ngrams of size 2, the ACS algo-
rithm is equivalent to the ROUGE-S metric (Lin and
Och, 2004). This case represents a specific parameter
setting in the ACS skip-ngram computation.
The longest common subsequence statistic has also
been successfully used for automatic machine trans-
lation evaluation in the ROUGE-L (Lin and Och,
2004) algorithm. In BLANC, if we set both ? and
? parameters to zero, the net result is a set of skip-
bigram (common subsequence) overlap counts for all
skip-bigram sizes. Although dynamic programming
or suffix trees can be used to compute the LCS much
faster, under this parameter setting the ACS algorithm
can also produce the longest common subsequence:
LCS(X,Y )? argmax
k
ACS(wSKIPk) > 0
where Acs(wSKIPk) is the number of common
skip-ngrams (common subsequences) produced by
the ACS algorithm.
ROUGE-W (Lin and Och, 2004) relies on a
weighted version of the longest common subse-
quence, under which longer contiguous subsequences
are assigned a higher weight than subsequences that
incorporate gaps. ROUGE-W uses the polynomial
function xa in the weighted LCS computation. This
setting can also be simulated by BLANC by adjusting
the parameters ? to reward tighter skip-ngrams and ?
to assign a very high score to similar size gaps. In-
tuitively, ? is used to reward skip-ngrams that have
smaller gaps, while ? is used to reward better aligned
skip-ngram overlap.
4 Scalability & Data Exploration
In Figure 1 we show theoretical and empirical prac-
tical behavior for the ACS algorithm on the 2003
TIDES machine translation evaluation data for Ara-
bic and Chinese. Sentence length distribution is
somewhat similar for the two languages ? only a very
small amount of text segments have more than 50
tokens. We show the ACS graph size in the worst
case scenario, and the empirical average number of
matches for both languages as a function of sentence
length. We also show (on a log scale) the upper bound
on time/space complexity in terms of total number
of feature computations. Even though the worst-
case scenario is tractable (polynomial), the empirical
amount of computation is considerably smaller in the
form of polynomials of lower degree. In Figure 1,
sentence length is the average between reference and
candidate lengths.
Finally, we also show the total number of fea-
ture computations involved in performing a full over-
lap search and computing a numeric score for the
745
reference-candidate translation pair. We have exper-
imented with the ACS algorithm using a worst-case
scenario where all words are exactly the same for a
fifty words reference translation and candidate trans-
lation. In practice when considering real sentences
the number of matches is very small. In this setting,
the algorithm takes less than two seconds on a low-
end desktop system when working on the worst case
scenario, and less then a second for all candidate-
reference pairs in the TIDES 2003 dataset. This re-
sult renders the ACS algorithm very practical for au-
tomatic MT evaluation.
5 Experiments & Results
In the dynamic metric BLANC, we have implemented
the ACS algorithm using several parameters includ-
ing the aggregate gap size ?, the displacement feature
?, a parameter for regulating skip-ngram size contri-
bution, and the F-measure ?F parameter.
Until recently, most experiments that evaluate au-
tomatic metrics correlation to human judgments have
been performed at a system level. In such experi-
ments, human judgments are aggregated across sen-
tences for each MT system and compared to aggre-
gate scores for automatic metrics. While high scor-
ing metrics in this setting are useful for understand-
ing relative system performance, not all of them are
robust enough for evaluating the quality of machine
translation output at a lower granularity. Sentence-
level translation quality estimation is very useful
when MT is used as a component in a pipeline of text-
processing applications (e.g. question answering).
The fact that current automatic MT evaluation met-
rics including BLANC do not correlate well with hu-
man judgments at the sentence level, does not mean
we should ignore this need and focus only on system
level evaluation. On the contrary, further research is
required to improve these metrics. Due to its train-
able nature, and by allowing additional features to be
incorporated into its model, BLANC has the potential
to address this issue.
For comparison purposes with previous literature,
we have also performed experiments at system level
for Arabic. The datasets used consist of the MT trans-
lation outputs from all systems available through the
Tides 2003 evaluation (663 sentences) for training
and Tides 2004 evaluation (1353 sentences) for test-
ing.
We compare (Table 1) the performance of BLANC
on Arabic translation output with the performance
of more established evaluation metrics: BLEU and
NIST, and also with more recent metrics: ROUGE-
L and ROUGE-S (using an unlimited size skip win-
dow), which have been shown to correlate well with
human judgments at system level ? as confirmed by
our results. We have performed experiments in which
case information is preserved as well as experiments
that ignore case information. Since the results are
very similar, we only show here experiments under
the former condition. In order to maintain consis-
tency, when using any metric we apply the same pre-
processing provided by the MTEval script. When
computing the correlation between metrics and hu-
man judgments, we only keep strictly positive scores.
While this is not fully equivalent to BLEU smooth-
ing, it partially mitigates the same problem of zero
count ngrams for short sentences. In future work we
plan to implement smoothing for all metrics, includ-
ing BLANC.
We train BLANC separately for adequacy and flu-
ency, as well as for system level and segment level
correlation with human judgments. The BLANC pa-
rameters are currently trained using a simple hill-
climbing procedure and using several starting points
in order to decrease the chance of reaching a local
maximum.
BLANC proves to be robust across criteria and
granularity levels. As expected, different parameter
values of BLANC optimize different criteria (e.g. ad-
equacy and fluency). We have observed that train-
ing BLANC for adequacy results in more bias to-
wards recall (?F =3) compared to training it for flu-
ency (?F =2). This confirms our intuition that a dy-
namic, parametric metric is justified for automatic
evaluation.
6 Conclusions & Future Work
In previous sections we have defined simple distance
functions. More complex functions can also be incor-
porated in ACS. Skip-ngrams in the candidate sen-
tence might be rewarded if they contain fewer gaps in
the candidate sentence and penalized if they contain
more. Different distance functions could also be used
in ACS, including functions based on surface-form
features and part-of-speech features.
Most of the established MT evaluation methods are
746
Tides 2003 Arabic
System Level Segment Level
Method Adequacy Fluency Adequacy Fluency
BLEU 0.950 0.934 0.382 0.286
NIST 0.962 0.939 0.439 0.304
ROUGE-L 0.974 0.926 0.440 0.328
ROUGE-S 0.949 0.935 0.360 0.328
BLANC 0.988 0.979 0.492 0.391
Tides 2004 Arabic
System Level Segment Level
Method Adequacy Fluency Adequacy Fluency
BLEU 0.978 0.994 0.446 0.337
NIST 0.987 0.952 0.529 0.358
ROUGE-L 0.981 0.985 0.538 0.412
ROUGE-S 0.937 0.980 0.367 0.408
BLANC 0.982 0.994 0.565 0.438
Table 1: Pearson correlation of several metrics with human judgments at system level and segment level for fluency and adequacy.
static functions according to which automatic evalu-
ation scores are computed. In this paper, we have
laid the foundation for a more flexible, parametric ap-
proach that can be trained using existing MT data and
that can be optimized for highest agreement with hu-
man assessors, for different criteria.
We have introduced ACS, a practical algorithm
with learnable parameters for automatic MT evalu-
ation and showed that ngram computation of popu-
lar evaluation methods can be emulated through dif-
ferent parameters by ACS. We have computed time
and space bounds for the ACS algorithm and argued
that while it is more powerful in modeling local and
sentence structure, it offers the same practicality as
established algorithms.
In our experiments, we trained and tested BLANC
on data from consecutive years, and therefore tai-
lored the metric for two different operating points
in MT system performance. In this paper we show
that BLANC correlates well with human performance
when trained on previous year data for both sentence
and system level.
In the future, we plan to investigate the stability
and performance of BLANC and also apply it to auto-
matic summarization evaluation. We plan to optimize
the BLANC parameters for different criteria in addi-
tion to incorporating syntactic and semantic features
(e.g. ngrams, word classes, part-of-speech).
In previous sections we have defined simple dis-
tance functions. More complex functions can also
be incorporated in ACS. Skip-ngrams in the candi-
date sentence might be rewarded if they contain fewer
gaps in the candidate sentence and penalized if they
contain more. Different distance functions could also
be used in ACS, including functions based on surface-
form features and part-of-speech features.
Looking beyond the BLANC metric, this paper
makes the case for the need to shift to trained, dy-
namic evaluation metrics which can adapt to individ-
ual optimization criteria and correlation functions.
We plan to make available an implementation of
BLANC at http://www.cs.cmu.edu/ llita/blanc.
References
Y. Akiba, K. Iamamurfa, and E. Sumita. 2001. Using
multiple edit distances to automatically rank machine
translation output. MT Summit VIII.
C. Culy and S.Z. Riehemann. 2003. The limits of n-
gram translation evaluation metrics. Machine Transla-
tion Summit IX.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Human Language Technology Conference
(HLT).
V.I. Levenshtein. 1965. Binary codes capable of cor-
recting deletions, insertions, and reversals. Doklady
Akademii Nauk SSSR.
C.Y. Lin and F.J. Och. 2004. Automatic evaluation of
machine translation quality using longest common sub-
sequence and skip bigram statistics. ACL.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evaluation
for mt research. LREC.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report.
R. Soricut and E. Brill. 2004. A unified framework for
automatic evaluation using n-gram co-occurence statis-
tics. ACL.
K.Y. Su, M.W. Wu, and J.S. Chang. 1992. A new quanti-
tative quality measure for machine translation systems.
COLING.
J.P. Turian, L. Shen, and I.D. Melamed. 2003. Evaluation
of machine translation and its evaluation. MT Summit
IX.
C.J. Van-Rijsbergen. 1979. Information retrieval.
747
SPoT: A Trainable Sentence Planner
Marilyn A. Walker
AT&T Labs ? Research
Florham Park, NJ, USA
walker@research.att.com
Owen Rambow
AT&T Labs ? Research
Florham Park, NJ, USA
rambow@research.att.com
Monica Rogati
Carnegie Mellon University
Pittsburgh, PA, USA
mrogati+@cs.cmu.edu
Abstract
Sentence planning is a set of inter-related but distinct
tasks, one of which is sentence scoping, i.e. the choice
of syntactic structure for elementary speech acts and
the decision of how to combine them into one or more
sentences. In this paper, we present SPoT, a sentence
planner, and a new methodology for automatically train-
ing SPoT on the basis of feedback provided by human
judges. We reconceptualize the task into two distinct
phases. First, a very simple, randomized sentence-plan-
generator (SPG) generates a potentially large list of pos-
sible sentence plans for a given text-plan input. Second,
the sentence-plan-ranker (SPR) ranks the list of output
sentence plans, and then selects the top-ranked plan. The
SPR uses ranking rules automatically learned from train-
ing data. We show that the trained SPR learns to select a
sentence plan whose rating on average is only 5% worse
than the top human-ranked sentence plan.
1 Introduction
Sentence planning is a set of inter-related but distinct
tasks, one of which is sentence scoping, i.e. the choice
of syntactic structure for elementary speech acts and the
decision of how to combine them into sentences.1 For
example, consider the required capabilities of a sentence
planner for a mixed-initiative spoken dialog system for
travel planning:
(D1) System1: Welcome.... What airport would you like to
fly out of?
User2: I need to go to Dallas.
System3: Flying to Dallas. What departure airport was
that?
User4: from Newark on September the 1st.
System5: What time would you like to travel on Septem-
ber the 1st to Dallas from Newark?
Utterance System1 requests information about the
caller?s departure airport, but in User2, the caller takes
the initiative to provide information about her destina-
tion. In System3, the system?s goal is to implicitly con-
firm the destination (because of the possibility of error
1We would like to thank Michael Collins and Rob Schapire for their
help, comments, and encouragement, and Noemie Elhadad and three
anonymous reviewers for very useful feedback. This work was partially
funded by DARPA under contract MDA972-99-3-0003.
in the speech recognition component), and request in-
formation (for the second time) of the caller?s departure
airport. In User4, the caller provides this information
but also provides the month and day of travel. Given the
system?s dialog strategy, the communicative goals for its
next turn are to implicitly confirm all the information that
the user has provided so far, i.e. the departure and desti-
nation cities and the month and day information, as well
as to request information about the time of travel. The
system?s representation of its communicative goals for
utterance System5 is in Figure 1. The job of the sentence
planner is to decide among the large number of potential
realizations of these communicative goals. Some exam-
ple alternative realizations are in Figure 2.2
implicit-confirm(orig-city:NEWARK)
implicit-confirm(dest-city:DALLAS)
implicit-confirm(month:9)
implicit-confirm(day-number:1)
request(depart-time)
Figure 1: The text plan (communicative goals) for utter-
ance System5 in dialog D1
Alt Realization H RB
0 What time would you like to travel on
September the 1st to Dallas from Newark?
5 .85
5 Leaving on September the 1st. What time
would you like to travel from Newark to Dal-
las?
4.5 .82
8 Leaving in September. Leaving on the 1st.
What time would you, traveling from Newark
to Dallas, like to leave?
2 .39
Figure 2: Alternative sentence plan realizations for the
text plan for utterance System5 in dialog D1. H = human
rating, RB = RankBoost score.
In this paper, we present SPoT, for ?Sentence Plan-
ner, Trainable?. We also present a new methodology
for automatically training SPoT on the basis of feed-
back provided by human judges. In order to train SPoT,
we reconceptualize its task as consisting of two distinct
phases. In the first phase, the sentence-plan-generator
2The meaning of the human ratings and RankBoost scores in Fig-
ure 2 are discussed below.
(SPG) generates a potentially large sample of possible
sentence plans for a given text-plan input. In the second
phase, the sentence-plan-ranker (SPR) ranks the sample
sentence plans, and then selects the top-ranked output to
input to the surface realizer. Our primary contribution is
a method for training the SPR. The SPR uses rules au-
tomatically learned from training data, using techniques
similar to (Collins, 2000; Freund et al, 1998).
Our method for training a sentence planner is unique
in neither depending on hand-crafted rules, nor on the
existence of a text or speech corpus in the domain of the
sentence planner obtained from the interaction of a hu-
man with a system or another human. We show that the
trained SPR learns to select a sentence plan whose rating
on average is only 5% worse than the top human-ranked
sentence plan. In the remainder of the paper, section 2
describes the sentence planning task in more detail. We
then describe the sentence plan generator (SPG) in sec-
tion 3, the sentence plan ranker (SPR) in section 4, and
the results in section 5.
2 The Sentence Planning Task
The term ?sentence planning? comprises many distinct
tasks and many ways of organizing these tasks have been
proposed in the literature. In general, the role of the sen-
tence planner is to choose abstract linguistic resources
(meaning-bearing lexemes, syntactic constructions) for a
text plan. In our case, the output of the dialog manager of
a spoken dialog system provides the input to our sentence
planner in the form of a single spoken dialog text plan
for each of the turns. (In contrast, the dialog managers
of most dialog systems today simply output completely
formed utterances which are passed on to the TTS mod-
ule.) Each text plan is an unordered set of elementary
speech acts encoding all of the system?s communicative
goals for the current turn, as illustrated in Figure 1. Each
elementary speech act is represented as a type (request,
implicit confirm, explicit confirm), with type-specific pa-
rameters. The sentence planner must decide among al-
ternative abstract linguistic resources for this text plan;
surface realizations of some such alternatives are in Fig-
ure 2.
As already mentioned, we divide the sentence plan-
ning task into two phases. In the first phase, the sentence-
plan-generator (SPG) generates 12-20 possible sentence
plans for a given input text plan. Each speech act
is assigned a canonical lexico-structural representation
(called a DSyntS ? Deep Syntactic Structure (Mel?c?uk,
1988)). The sentence plan is a tree recording how these
elementary DSyntS are combined into larger DSyntSs;
the DSyntS for the entire input text plan is associated
with the root node of the tree. In the second phase, the
sentence plan ranker (SPR) ranks sentence plans gener-
ated by the SPG, and then selects the top-ranked out-
put as input to the surface realizer, RealPro (Lavoie and
Rambow, 1997). The architecture is summarized in Fig-
ure 3.
-
--
-
S
P
R
.
.
Sentence Planner
S
P
G
H
RealPro
Realizer
Text Plan Chosen sp?tree with associated DSyntS
-
H
Sp?trees with associated DSyntSs
aDialog
System .
Figure 3: Architecture of SPoT
3 The Sentence Plan Generator
The research presented here is primarily concerned
with creating a trainable SPR. A strength of our ap-
proach is the ability to use a very simple SPG, as we
explain below. The basis of our SPG is a set of clause-
combining operations that incrementally transform a list
of elementary predicate-argument representations (the
DSyntSs corresponding to elementary speech acts, in
our case) into a single lexico-structural representation,
by combining these representations using the following
combining operations. Examples can be found in Fig-
ure 4.
  MERGE. Two identical main matrix verbs can be iden-
tified if they have the same arguments; the adjuncts are
combined.
  MERGE-GENERAL. Same as MERGE, except that one of
the two verbs may be embedded.
  SOFT-MERGE. Same as MERGE, except that the verbs
need only to be in a relation of synonymy or hyperonymy
(rather than being identical).
  SOFT-MERGE-GENERAL. Same as MERGE-GENERAL,
except that the verbs need only to be in a relation of syn-
onymy or hyperonymy.
  CONJUNCTION. This is standard conjunction with con-
junction reduction.
  RELATIVE-CLAUSE. This includes participial adjuncts to
nouns.
  ADJECTIVE. This transforms a predicative use of an ad-
jective into an adnominal construction.
  PERIOD. Joins two complete clauses with a period.
These operations are not domain-specific and are sim-
ilar to those of previous aggregation components (Ram-
bow and Korelsky, 1992; Shaw, 1998; Danlos, 2000), al-
though the various MERGE operations are, to our knowl-
edge, novel in this form.
The result of applying the operations is a sentence
plan tree (or sp-tree for short), which is a binary tree
with leaves labeled by all the elementary speech acts
Rule Sample first argument Sample second argument Result
MERGE You are leaving from Newark. You are leaving at 5 You are leaving at 5 from Newark
MERGE-GENERAL What time would you like to
leave?
You are leaving from Newark. What time would you like to leave
from Newark?
SOFT-MERGE You are leaving from Newark You are going to Dallas You are traveling from Newark to
Dallas
SOFT-MERGE-
GENERAL
What time would you like to
leave?
You are going to Dallas. What time would you like to fly to
Dallas?
CONJUNCTION You are leaving from Newark. You are going to Dallas. You are leaving from Newark and
you are going to Dallas.
RELATIVE-
CLAUSE
Your flight leaves at 5. Your flight arrives at 9. Your flight, which leaves at 5, ar-
rives at 9.
ADJECTIVE Your flight leaves at 5. Your flight is nonstop. Your nonstop flight leaves at 5.
PERIOD You are leaving from Newark. You are going to Dallas. You are leaving from Newark.
You are going to Dallas
Figure 4: List of clause combining operations with examples from our domain; an explanation of the operations is
given in Section 3.
from the input text plan, and with its interior nodes la-
beled with clause-combining operations3. Each node is
also associated with a DSyntS: the leaves (which corre-
spond to elementary speech acts from the input text plan)
are linked to a canonical DSyntS for that speech act (by
lookup in a hand-crafted dictionary). The interior nodes
are associated with DSyntSs by executing their clause-
combing operation on their two daughter nodes. (A PE-
RIOD node results in a DSyntS headed by a period and
whose daughters are the two daughter DSyntSs.) If a
clause combination fails, the sp-tree is discarded (for ex-
ample, if we try to create a relative clause of a struc-
ture which already contains a period). As a result, the
DSyntS for the entire turn is associated with the root
node. This DSyntS can be sent to RealPro, which returns
a sentence (or several sentences, if the DSyntS contains
period nodes). The SPG is designed in such a way that
if a DSyntS is associated with the root node, it is a valid
structure which can be realized.
2
1
3imp?confirm(month) request(time) soft?merge?general
imp?confirm(dest?city) imp?confirm(orig?city)
imp?confirm(day)
soft?merge
soft?merge?general
soft?merge?general
Figure 5: Alternative 0 Sentence Plan Tree
Figure 2 shows some of the realizations of alternative
sentence plans generated by our SPG for utterance Sys-
3The sp-tree is inspired by (Lavoie and Rambow, 1998). The rep-
resentations used by Danlos (2000), Gardent and Webber (1998), or
Stone and Doran (1997) are similar, but do not (always) explicitly rep-
resent the clause combining operations as labeled nodes.
21
3
soft?merge?general
imp?confirm(day)
period
soft?merge?general
imp?confirm(month)
imp?confirm(orig?city) imp?confirm(dest?city)
request(time)soft?merge?general
Figure 6: Alternative 5 Sentence Plan Tree
2
1
soft?merge?general
imp?confirm(orig?city)
request(time)
imp?confirm(dest?city)
relative?clause
imp?confirm(day)
period
period
imp?confirm(month)
Figure 7: Alternative 8 Sentence Plan Tree
tem5 in Dialog D1. Sp-trees for alternatives 0, 5 and
8 are in Figures 5, 6 and 7. For example, consider the
sp-tree in Figure 7. Node soft-merge-general merges
an implicit-confirmations of the destination city and the
origin city. The row labelled SOFT-MERGE in Figure 4
shows the result of applying the soft-merge operation
when Args 1 and 2 are implicit confirmations of the ori-
gin and destination cities. Figure 8 illustrates the rela-
tionship between the sp-tree and the DSyntS for alter-
native 8. The labels and arrows show the DSyntSs as-
sociated with each node in the sp-tree (in Figure 7), and
the diagram also shows how structures are composed into
larger structures by the clause combining operations.
number: sg
mood: question
mood: inf?to
AT1
time
travel
IN1
September
PRONOUN ON1
1 WHAT2
person: 2
Dallasimp?confirm(moth)
imp?confirm(day)
soft?merge?general
relative?clause request(time)
imp?confirm(dest?city)imp?confirm(orig?city)
TO1
mood:prespartmood:prespart
period
travel
mood:prespart
Newark
FROM1PRONOUN
leave
period
leave
like
PRONOUN
PRONOUN
Figure 8: Alternative 8 DSyntS (not all linguistic features are shown)
The complexity of most sentence planners arises from
the attempt to encode constraints on the application of,
and ordering of, the operations, in order to generate a sin-
gle high quality sentence plan. In our approach, we do
not need to encode such constraints. Rather, we gener-
ate a random sample of possible sentence plans for each
text plan, up to a pre-specified maximum number of sen-
tence plans, by randomly selecting among the operations
according to some probability distribution.4
4 The Sentence-Plan-Ranker
The sentence-plan-ranker SPR takes as input a set of sen-
tence plans generated by the SPG and ranks them. In
order to train the SPR we applied the machine learning
program RankBoost (Freund et al, 1998), to learn from
a labelled set of sentence-plan training examples a set of
rules for scoring sentence plans.
4.1 RankBoost
RankBoost is a member of a family of boosting algo-
rithms (Schapire, 1999). Freund et al (1998) describe
the boosting algorithms for ranking in detail: for com-
pleteness, we give a brief description in this section.
Each example  is represented by a set of  indicator
functions

	 for 
 . The indicator functions
are calculated by thresholding the feature values (counts)
described in section 4.2. For example, one such indicator
function might be


Evaluating a Trainable Sentence Planner for a Spoken Dialogue System
Owen Rambow
AT&T Labs ? Research
Florham Park, NJ, USA
rambow@research.att.com
Monica Rogati
Carnegie Mellon University
Pittsburgh, PA, USA
mrogati+@cs.cmu.edu
Marilyn A. Walker
AT&T Labs ? Research
Florham Park, NJ, USA
walker@research.att.com
Abstract
Techniques for automatically training
modules of a natural language gener-
ator have recently been proposed, but
a fundamental concern is whether the
quality of utterances produced with
trainable components can compete with
hand-crafted template-based or rule-
based approaches. In this paper We ex-
perimentally evaluate a trainable sen-
tence planner for a spoken dialogue sys-
tem by eliciting subjective human judg-
ments. In order to perform an ex-
haustive comparison, we also evaluate
a hand-crafted template-based genera-
tion component, two rule-based sen-
tence planners, and two baseline sen-
tence planners. We show that the train-
able sentence planner performs better
than the rule-based systems and the
baselines, and as well as the hand-
crafted system.
1 Introduction
The past several years have seen a large increase
in commercial dialog systems. These systems
typically use system-initiative dialog strategies,
with system utterances highly scripted for style
and register and recorded by voice talent. How-
ever several factors argue against the continued
use of these simple techniques for producing the
system side of the conversation. First, text-to-
speech has improved to the point of being a vi-
able alternative to pre-recorded prompts. Second,
there is a perceived need for spoken dialog sys-
tems to be more flexible and support user initia-
tive, but this requires greater flexibility in utter-
ance generation. Finally, systems to support com-
plex planning are being developed, which will re-
quire more sophisticated output.
As we move away from systems with pre-
recorded prompts, there are two possible ap-
proaches to producing system utterances. The
first is template-based generation, where ut-
terances are produced from hand-crafted string
templates. Most current research systems use
template-based generation because it is concep-
tually straightforward. However, while little or
no linguistic training is needed to write templates,
it is a tedious and time-consuming task: one or
more templates must be written for each combi-
nation of goals and discourse contexts, and lin-
guistic issues such as subject-verb agreement and
determiner-noun agreement must be repeatedly
encoded for each template. Furthermore, main-
tenance of the collection of templates becomes a
software engineering problem as the complexity
of the dialog system increases.1
The second approach is natural language gen-
eration (NLG), which customarily divides the
generation process into three modules (Rambow
and Korelsky, 1992): (1) Text Planning, (2) Sen-
tence Planning, and (3) Surface Realization. In
this paper, we discuss only sentence planning; the
role of the sentence planner is to choose abstract
lexico-structural resources for a text plan, where
a text plan encodes the communicative goals for
an utterance (and, sometimes, their rhetorical
structure). In general, NLG promises portability
across application domains and dialog situations
by focusing on the development of rules for each
generation module that are general and domain-
1Although we are not aware of any software engineering
studies of template development and maintenance, this claim
is supported by abundant anecdotal evidence.
independent. However, the quality of the output
for a particular domain, or a particular situation
in a dialog, may be inferior to that of a template-
based system without considerable investment in
domain-specific rules or domain-tuning of gen-
eral rules. Furthermore, since rule-based systems
use sophisticated linguistic representations, this
handcrafting requires linguistic knowledge.
Recently, several approaches for automatically
training modules of an NLG system have been
proposed (Langkilde and Knight, 1998; Mel-
lish et al, 1998; Walker, 2000). These hold
the promise that the complex step of customiz-
ing NLG systems by hand can be automated,
while avoiding the need for tedious hand-crafting
of templates. While the engineering benefits of
trainable approaches appear obvious, it is unclear
whether the utterance quality is high enough.
In (Walker et al, 2001) we propose a new
model of sentence planning called SPOT. In
SPOT, the sentence planner is automatically
trained, using feedback from two human judges,
to choose the best from among different options
for realizing a set of communicative goals. In
(Walker et al, 2001), we evaluate the perfor-
mance of the learning component of SPOT, and
show that SPOT learns to select sentence plans
that are highly rated by the two human judges.
While this evaluation shows that SPOT has in-
deed learned from the human judges, it does not
show that using only two human judgments is
sufficient to produce more broadly acceptable re-
sults, nor does it show that SPOT performs as well
as optimized hand-crafted template or rule-based
systems. In this paper we address these questions.
Because SPOT is trained on data from a work-
ing system, we can directly compare SPOT to the
hand-crafted, template-based generation compo-
nent of the current system. In order to perform an
exhaustive comparison, we also implemented two
rule-based and two baseline sentence-planners.
One baseline simply produces a single sentence
for each communicative goal. Another baseline
randomly makes decisions about how to combine
communicative goals into sentences. We directly
compare these different approaches in an evalua-
tion experiment in which 60 human subjects rate
each system?s output on a scale of 1 to 5.
The experimental design is described in section
System1: Welcome.... What airport would you like to fly
out of?
User2: I need to go to Dallas.
System3: Flying to Dallas. What departure airport was
that?
User4: from Newark on September the 1st.
System5: Flying from Newark to Dallas, Leaving on the
1st of September, And what time did you want
to leave?
Figure 1: A dialog with AMELIA
2. The sentence planners used in the evaluation
are described in section 3. In section 4, we present
our results. We show that the trainable sentence
planner performs better than both rule-based sys-
tems and as well as the hand-crafted template-
based system. These four systems outperform the
baseline sentence planners. Section 5 summarizes
our results and discusses related and future work.
2 Experimental Context and Design
Our research concerns developing and evaluat-
ing a portable generation component for a mixed-
initiative travel planning system, AMELIA, de-
veloped at AT&T Labs as part of DARPA Com-
municator. Consider the required generation ca-
pabilities of AMELIA, as illustrated in Figure 1.
Utterance System1 requests information about
the caller?s departure airport, but in User2, the
caller takes the initiative to provide information
about her destination. In System3, the system?s
goal is to implicitly confirm the destination (be-
cause of the possibility of error in the speech
recognition component), and request information
(for the second time) of the caller?s departure air-
port. This combination of communicative goals
arises dynamically in the dialog because the sys-
tem supports user initiative, and requires differ-
ent capabilities for generation than if the system
could only understand the direct answer to the
question that it asked in System1.
In User4, the caller provides this information
but takes the initiative to provide the month and
day of travel. Given the system?s dialog strategy,
the communicative goals for its next turn are to
implicitly confirm all the information that the user
has provided so far, i.e. the departure and desti-
nation cities and the month and day information,
as well as to request information about the time
of travel. The system?s representation of its com-
municative goals for System5 is in Figure 2. As
before, this combination of communicative goals
arises in response to the user?s initiative.
implicit-confirm(orig-city:NEWARK)
implicit-confirm(dest-city:DALLAS)
implicit-confirm(month:9)
implicit-confirm(day-number:1)
request(depart-time:whatever)
Figure 2: The text plan (communicative goals) for
System5 in Figure 1
Like most working research spoken dialog
systems, AMELIA uses hand-crafted, template-
based generation. Its output is created by choos-
ing string templates for each elementary speech
act, using a large choice function which depends
on the type of speech act and various context con-
ditions. Values of template variables (such as ori-
gin and destination cities) are instantiated by the
dialog manager. The string templates for all the
speech acts of a turn are heuristically ordered and
then appended to produce the output. In order to
produce output that is not highly redundant, string
templates must be written for every possible com-
bination of speech acts in a text plan. We refer to
the output generated by AMELIA using this ap-
proach as the TEMPLATE output.
System Realization
TEMPLATE Flying from Newark to Dallas, Leaving on
the 1st of September, And what time did
you want to leave?
SPoT What time would you like to travel on
September the 1st to Dallas from Newark?
RBS (Rule-
Based)
What time would you like to travel on
September the 1st to Dallas from Newark?
ICF (Rule-
Based)
What time would you like to fly on
September the 1st to Dallas from Newark?
RANDOM Leaving in September. Leaving on the
1st. What time would you, traveling from
Newark to Dallas, like to leave?
NOAGG Leaving on the 1. Leaving in September.
Going to Dallas. Leaving from Newark.
What time would you like to leave?
Figure 3: Sample outputs for System5 of Figure 1
for each type of generation system used in the
evaluation experiment.
We perform an evaluation using human sub-
jects who judged the TEMPLATE output of
AMELIA against five NLG-based approaches:
SPOT, two rule-based approaches, and two base-
lines. We describe them in Section 3. An exam-
ple output for the text plan in Figure 2 for each
system is in Figure 3. The experiment required
human subjects to read 5 dialogs of real inter-
actions with AMELIA. At 20 points over the 5
dialogs, AMELIA?s actual utterance (TEMPLATE)
is augmented with a set of variants; each set of
variants included a representative generated by
SPOT, and representatives of the four compari-
son sentence planners. At times two or more of
these variants coincided, in which case sentences
were not repeated and fewer than six sentences
were presented to the subjects. The subjects rated
each variation on a 5-point Likert scale, by stating
the degree to which they agreed with the state-
ment The system?s utterance is easy to under-
stand, well-formed, and appropriate to the dialog
context. Sixty colleagues not involved in this re-
search completed the experiment.
3 Sentence Planning Systems
This section describes the five sentence planners
that we compare. SPOT, the two rule-based
systems, and the two baseline sentence planners
are all NLG based sentence planners. In Sec-
tion 3.1, we describe the shared representations
of the NLG based sentence planners. Section 3.2
describes the baselines, RANDOM and NOAGG.
Section 3.3 describes SPOT. Section 3.4 de-
scribes the rule-based sentence planners, RBS
and ICF.
3.1 Aggregation in Sentence Planning
In all of the NLG sentence planners, each speech
act is assigned a canonical lexico-structural rep-
resentation (called a DSyntS ? Deep Syntactic
Structure (Melc?uk, 1988)). We exclude issues of
lexical choice from this study, and restrict our at-
tention to the question of how elementary struc-
tures for separate elementary speech acts are as-
sembled into extended discourse. The basis of all
the NLG systems is a set of clause-combining op-
erations that incrementally transform a list of el-
ementary predicate-argument representations (the
DSyntSs corresponding to the elementary speech
acts of a single text plan) into a list of lexico-
structural representations of one or more sen-
tences, that are sent to a surface realizer. We uti-
lize the RealPro Surface realizer with all of the
Rule Arg 1 Arg 2 Result
MERGE You are leaving from Newark. You are leaving at 5 You are leaving at 5 from Newark
SOFT-MERGE You are leaving from Newark You are going to Dallas You are traveling from Newark to
Dallas
CONJUNCTION You are leaving from Newark. You are going to Dallas. You are leaving from Newark and
you are going to Dallas.
RELATIVE-
CLAUSE
Your flight leaves at 5. Your flight arrives at 9. Your flight, which leaves at 5, ar-
rives at 9.
ADJECTIVE Your flight leaves at 5. Your flight is nonstop. Your nonstop flight leaves at 5.
PERIOD You are leaving from Newark. You are going to Dallas. You are leaving from Newark.
You are going to Dallas
RANDOM CUE-
WORD
What time would yo like to
leave?
n/a Now, what time would you like to
leave?
Figure 4: List of clause combining operations with examples
sentence planners (Lavoie and Rambow, 1997).
DSyntSs are combined using the operations ex-
emplified in Figure 4. The result of applying the
operations is a sentence plan tree (or sp-tree
for short), which is a binary tree with leaves la-
beled by all the elementary speech acts from the
input text plan, and with its interior nodes la-
beled with clause-combining operations. As an
example, Figure 5 shows the sp-tree for utterance
System5 in Figure 1. Node soft-merge-general  
merges an implicit-confirmation of the destina-
tion city and the origin city. The row labelled
SOFT-MERGE in Figure 4 shows the result when
Args 1 and 2 are implicit confirmations of the ori-
gin and destination. See (Walker et al, 2001) for
more detail on the sp-tree. The experimental sen-
tence planners described below vary how the sp-
tree is constructed.
2
1
3imp?confirm(month) request(time) soft?merge?general
imp?confirm(dest?city) imp?confirm(orig?city)
imp?confirm(day)
soft?merge
soft?merge?general
soft?merge?general
Figure 5: A Sentence Plan Tree for Utterance Sys-
tem 5 in Dialog D1
3.2 Baseline Sentence Planners
In one obvious baseline system the sp-tree is con-
structed by applying only the PERIOD operation:
each elementary speech act is realized as its own
sentence. This baseline, NOAGG, was suggested
by Hovy and Wanner (1996). For NOAGG, we
order the communicative acts from the text plan
as follows: implicit confirms precede explicit
confirms precede requests. Figure 3 includes a
NOAGG output for the text plan in Figure 2.
A second possible baseline sentence planner
simply applies combination rules randomly ac-
cording to a hand-crafted probability distribution
based on preferences for operations such as the
MERGE family over CONJUNCTION and PERIOD.
In order to be able to generate the resulting sen-
tence plan tree, we exclude certain combinations,
such as generating anything other than a PERIOD
above a node labeled PERIOD in a sentence plan.
The resulting sentence planner we refer to as
RANDOM. Figure 3 includes a RANDOM output
for the text plan in Figure 2.
In order to construct a more complex, and
hopefully better, sentence planner, we need to en-
code constraints on the application of, and order-
ing of, the operations. It is here that the remaining
approaches differ. In the first approach, SPOT,
we learn constraints from training material; in the
second approach, rule-based, we construct con-
straints by hand.
3.3 SPoT: A Trainable Sentence Planner
For the sentence planner SPOT, we reconceptu-
alize sentence planning as consisting of two dis-
tinct phases as in Figure 6. In the first phase, the
sentence-plan-generator (SPG) randomly gener-
ates up to twenty possible sentence plans for a
given text-plan input. For this phase we use the
RANDOM sentence-planner. In the second phase,
the sentence-plan-ranker (SPR) ranks the sample
-
--
-
S
P
R
.
.
Sentence Planner
S
P
G
H
RealPro
Realizer
Text Plan Chosen sp?tree with associated DSyntS
-
H
Sp?trees with associated DSyntSs
aDialog
System .
Figure 6: Architecture of SPoT
sentence plans, and then selects the top-ranked
output to input to the surface realizer. The SPR
is automatically trained by applying RankBoost
(Freund et al, 1998) to learn ranking rules from
training data. The training data was assembled
by using RANDOM to randomly generate up to
20 realizations for 100 turns; two human judges
then ranked each of these realizations (using the
setup described in Section 2). Over 3,000 fea-
tures were discovered from the generated trees
by routines that encode structural and lexical as-
pects of the sp-trees and the DSyntS. RankBoost
identified the features that contribute most to a
realization?s ranking. The SPR uses these rules
to rank alternative sp-trees, and then selects the
top-ranked output as input to the surface realizer.
Walker et al (2001) describe SPOT in detail.
3.4 Two Rule-Based Sentence Planners
It has not been the object of our research to con-
struct a rule-based sentence planner by hand, be
it domain-independent or optimized for our do-
main. Our goal was to compare the SPOT sen-
tence planner with a representative rule-based
system. We decided against using an existing off-
the-shelf rule-based system, since it would be too
complex a task to port it to our application. In-
stead, we constructed two reasonably representa-
tive rule-based sentence planners. This task was
made easier by the fact that we could reuse much
of the work done for SPOT, in particular the data
structure of the sp-tree and the implementation of
the clause-combining operations. We developed
the two systems by applying heuristics for pro-
ducing good output, such as preferences for ag-
gregation. They differ only in the initial ordering
of the communicative acts in the input text plan.
In the first rule-based system, RBS (for ?Rule-
Based System?), we order the speech acts with
explicit confirms first, then requests, then implicit
confirms. Note that explicit confirms and requests
do not co-occur in our data set. The second rule-
based system is identical, except that implicit con-
firms come first rather than last. This system we
call ICF (for ?Rule-based System with Implicit
Confirms First?).
In the initial step of both RBS and ICF,
we take the two leftmost members of the text
plan and try to combine them using the follow-
ing preference ranking of the combination op-
erations: ADJECTIVE, the MERGEs, CONJUNC-
TION, RELATIVE-CLAUSE, PERIOD. The first
operation to succeed is chosen. This yields a bi-
nary sp-tree with three nodes, which becomes the
current sp-tree. As long as the root node of
the current sp-tree is not a PERIOD, we iterate
through the list of remaining speech acts on the
ordered text plan, combining each one with the
current sp-tree using the preference-ranked opera-
tions as just described. The result of each iteration
step is a binary, left-branching sp-tree. However,
if the root node of the current sp-tree is a PERIOD,
we start a new current sp-tree, as in the initial step
described above. When the text plan has been ex-
hausted, all partial sp-trees (all of which except
for the last one are rooted in PERIOD) are com-
bined in a left-branching tree using PERIOD. Cue
words are added as follows: (1) The cue word
now is attached to utterances beginning a new
subtask; (2) The cue word and is attached to ut-
terances continuing a subtask; (3) The cue words
alright or okay are attached to utterances contain-
ing implicit confirmations. Figure 3 includes an
RBS and an ICF output for the text plan in Fig-
ure 2. In this case ICF and RBS differ only in
the verb chosen as a more general verb during the
SOFT-MERGE operation.
We illustrate the RBS procedure with an ex-
ample for which ICF works similarly. For RBS,
the text plan in Figure 2 is ordered so that the re-
quest is first. For the request, a DSyntS is cho-
sen that can be paraphrased as What time would
you like to leave?. Then, the first implicit-confirm
is translated by lookup into a DSyntS which on
its own could generate Leaving in September.
We first try the ADJECTIVE aggregation opera-
tion, but since neither tree is a predicative ad-
jective, this fails. We then try the MERGE fam-
ily. MERGE-GENERAL succeeds, since the tree
for the request has an embedded node labeled
leave. The resulting DSyntS can be paraphrased
as What time would you like to leave in Septem-
ber?, and is attached to the new root node of
the resulting sp-tree. The root node is labeled
MERGE-GENERAL, and its two daughters are the
two speech acts. The implicit-confirm of the
day is added in a similar manner (adding an-
other left-branching node to the sp-tree), yielding
a DSyntS that can be paraphrased as What time
would you like to leave on September the 1st? (us-
ing some special-case attachment for dates within
MERGE). We now try and add the DSyntS for
the implicit-confirm, whose DSyntS might gener-
ate Going to Dallas. Here, we again cannot use
ADJECTIVE, nor can we use MERGE or MERGE-
GENERAL, since the verbs are not identical. In-
stead, we use SOFT-MERGE-GENERAL, which
identifies the leave node with the go root node of
the DSyntS of the implicit-confirm. When soft-
merging leave with go, fly is chosen as a general-
ization, resulting in a DSyntS that can be gener-
ated as What time would you like to fly on Septem-
ber the 1st to Dallas?. The sp-tree has added a
layer but is still left-branching. Finally, the last
implicit-confirm is added to yield a DSyntS that
is realized as What time would you like to fly on
September the 1st to Dallas from Newark?.
4 Experimental Results
All 60 subjects completed the experiment in a half
hour or less. The experiment resulted in a total
of 1200 judgements for each of the systems be-
ing compared, since each subject judged 20 ut-
terances by each system. We first discuss overall
differences among the different systems and then
make comparisons among the four different types
of systems: (1) TEMPLATE, (2) SPOT, (3) two
rule-based systems, and (4) two baseline systems.
All statistically significant results discussed here
had p values of less than .01.
We first examined whether differences in hu-
man ratings (score) were predictable from the
System Min Max Mean S.D.
TEMPLATE 1 5 3.9 1.1
SPoT 1 5 3.9 1.3
RBS 1 5 3.4 1.4
ICF 1 5 3.5 1.4
No Aggregation 1 5 3.0 1.2
Random 1 5 2.7 1.4
Figure 7: Summary of Overall Results for all Sys-
tems Evaluated
type of system that produced the utterance be-
ing rated. A one-way ANOVA with system as the
independent variable and score as the dependent
variable showed that there were significant differ-
ences in score as a function of system. The overall
differences are summarized in Figure 7.
As Figure 7 indicates, some system outputs re-
ceived more consistent scores than others, e.g.
the standard deviation for TEMPLATE was much
smaller than RANDOM. The ranking of the sys-
tems by average score is TEMPLATE, SPOT, ICF,
RBS, NOAGG, and RANDOM. Posthoc compar-
isons of the scores of individual pairs of systems
using the adjusted Bonferroni statistic revealed
several different groupings.2
The highest ranking systems were TEMPLATE
and SPOT, whose ratings were not statistically
significantly different from one another. This
shows that it is possible to match the quality of a
hand-crafted system with a trainable one, which
should be more portable, more general and re-
quire less overall engineering effort.
The next group of systems were the two rule-
based systems, ICF and RBS, which were not
statistically different from one another. However
SPOT was statistically better than both of these
systems (p   .01). Figure 8 shows that SPOT
got more high rankings than either of the rule-
based systems. In a sense this may not be that
surprising, because as Hovy and Wanner (1996)
point out, it is difficult to construct a rule-based
sentence planner that handles all the rule interac-
tions in a reasonable way. Features that SPoT?s
SPR uses allow SPOT to be sensitive to particular
discourse configurations or lexical collocations.
In order to encode these in a rule-based sentence
2The adjusted Bonferroni statistic guards against acci-
dentally finding differences between systems when making
multiple comparisons among systems.
planner, one would first have to discover these
constraints and then determine a way of enforc-
ing them. However the SPR simply learns that
a particular configuration is less preferred, result-
ing in a small decrement in ranking for the cor-
responding sp-tree. This flexibility of increment-
ing or decrementing a particular sp-tree by a small
amount may in the end allow it to be more sensi-
tive to small distinctions than a rule-based system.
Along with the TEMPLATE and RULE-BASED
systems, SPOT also scored better than the base-
line systems NOAGG and RANDOM. This is also
somewhat to be expected, since the baseline sys-
tems were intended to be the simplest systems
constructable. However it would have been a pos-
sible outcome for SPOT to not be different than
either system, e.g. if the sp-trees produced by
RANDOM were all equally good, or if the ag-
gregation rules that SPOT learned produced out-
put less readable than NOAGG. Figure 8 shows
that the distributions of scores for SPOT vs. the
baseline systems are very different, with SPOT
skewed towards higher scores.
Interestingly NOAGG also scored better than
RANDOM (p   .01), and the standard deviation
of its scores was smaller (see Figure 7). Remem-
ber that RANDOM?s sp-trees often resulted in ar-
bitrarily ordering the speech acts in the output.
While NOAGG produced redundant utterances, it
placed the initiative taking speech act at the end of
the utterance in its most natural position, possibly
resulting in a preference for NOAGG over RAN-
DOM. Another reason to prefer NOAGG could be
its predictability.
5 Discussion and Future Work
Other work has also explored automatically train-
ing modules of a generator (Langkilde and
Knight, 1998; Mellish et al, 1998; Walker, 2000).
However, to our knowledge, this is the first re-
ported experimental comparison of a trainable
technique that shows that the quality of system
utterances produced with trainable components
can compete with hand-crafted or rule-based tech-
niques. The results validate our methodology;
SPOT outperforms two representative rule-based
sentence planners, and performs as well as the
hand-crafted TEMPLATE system, but is more eas-
ily and quickly tuned to a new domain: the train-
ing materials for the SPOT sentence planner can
be collected from subjective judgements from a
small number of judges with little or no linguistic
knowledge.
Previous work on evaluation of natural lan-
guage generation has utilized three different ap-
proaches to evaluation (Mellish and Dale, 1998).
The first approach is a subjective evaluation
methodology such as we use here, where human
subjects rate NLG outputs produced by different
sources (Lester and Porter, 1997). Other work has
evaluated template-based spoken dialog genera-
tion with a task-based approach, i.e. the genera-
tor is evaluated with a metric such as task com-
pletion or user satisfaction after dialog comple-
tion (Walker, 2000). This approach can work
well when the task only involves one or two ex-
changes, when the choices have large effects over
the whole dialog, or the choices vary the con-
tent of the utterance. Because sentence plan-
ning choices realize the same content and only
affect the current utterance, we believed it impor-
tant to get local feedback. A final approach fo-
cuses on subproblems of natural language gener-
ation such as the generation of referring expres-
sions. For this type of problem it is possible to
evaluate the generator by the degree to which it
matches human performance (Yeh and Mellish,
1997). When evaluating sentence planning, this
approach doesn?t make sense because many dif-
ferent realizations may be equally good.
However, this experiment did not show that
trainable sentence planners produce, in general,
better-quality output than template-based or rule-
based sentence planners. That would be im-
possible: given the nature of template and rule-
based systems, any quality standard for the output
can be met given sufficient person-hours, elapsed
time, and software engineering acumen. Our prin-
cipal goal, rather, is to show that the quality of the
TEMPLATE output, for a currently operational dia-
log system whose template-based output compo-
nent was developed, expanded, and refined over
about 18 months, can be achieved using a train-
able system, for which the necessary training data
was collected in three person-days. Furthermore,
we wished to show that a representative rule-
based system based on current literature, without
massive domain-tuning, cannot achieve the same
1 1.5 2 2.5 3 3.5 4 4.5 5
0
200
400
600
800
1000
1200
Score
Nu
mb
er 
of 
pla
ns
 w
ith
 th
at 
sc
ore
 or
 m
ore
AMELIA
SPOT  
IC    
RB    
NOAGG 
RAN   
Figure 8: Chart comparing distribution of human ratings for SPOT, RBS, ICF, NOAGG and RANDOM.
level of quality. In future work, we hope to extend
SPoT and integrate it into AMELIA.
6 Acknowledments
This work was partially funded by DARPA under
contract MDA972-99-3-0003.
References
Y. Freund, R. Iyer, R. E. Schapire, and Y.Singer. 1998.
An efficient boosting algorithm for combining pref-
erences. In Machine Learning: Proc. of the Fif-
teenth International Conference.
E.H. Hovy and L. Wanner. 1996. Managing sentence
planning requirements. In Proc. of the ECAI?96
Workshop Gaps and Bridges: New Directions in
Planning and Natural Language Generation.
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proc.
of COLING-ACL.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In
Proc. of the Third Conference on Applied Natural
Language Processing, ANLP97, pages 265?268.
J. Lester and B. Porter. 1997. Developing and em-
pirically evaluating robust explanation generators:
The knight experiments. Computational Linguis-
tics, 23-1:65?103.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12(3).
C. Mellish, A. Knott, J. Oberlander, and M.
O?Donnell. 1998. Experiments using stochas-
tic search for text planning. In Proc. of Interna-
tional Conference on Natural Language Genera-
tion, pages 97?108.
I. A. Melc?uk. 1988. Dependency Syntax: Theory and
Practice. SUNY, Albany, New York.
O. Rambow and T. Korelsky. 1992. Applied text
generation. In Proc. of the Third Conference on
Applied Natural Language Processing, ANLP92,
pages 40?47.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In Proc. of the North
American Meeting of the Association for Computa-
tional Linguistics.
M. A. Walker. 2000. An application of reinforcement
learning to dialogue strategy selection in a spoken
dialogue system for email. Journal of Artificial In-
telligence Research, 12:387?416.
C.L. Yeh and C. Mellish. 1997. An empirical study
on the generation of anaphora in chinese. Compu-
tational Linguistics, 23-1:169?190.
Unsupervised Learning of Arabic Stemming using a Parallel Corpus
Monica Rogati
?
Computer Science Department,
Carnegie Mellon University
mrogati@cs.cmu.edu
Scott McCarley
IBM TJ Watson
Research Center
jsmc@watson.ibm.com
Yiming Yang
Language Technologies Institute,
Carnegie Mellon University
yiming@cs.cmu.edu
Abstract
This paper presents an unsupervised learn-
ing approach to building a non-English
(Arabic) stemmer. The stemming model
is based on statistical machine translation
and it uses an English stemmer and a small
(10K sentences) parallel corpus as its sole
training resources. No parallel text is
needed after the training phase. Mono-
lingual, unannotated text can be used to
further improve the stemmer by allow-
ing it to adapt to a desired domain or
genre. Examples and results will be given
for Arabic , but the approach is applica-
ble to any language that needs affix re-
moval. Our resource-frugal approach re-
sults in 87.5% agreement with a state of
the art, proprietary Arabic stemmer built
using rules, affix lists, and human anno-
tated text, in addition to an unsupervised
component. Task-based evaluation using
Arabic information retrieval indicates an
improvement of 22-38% in average pre-
cision over unstemmed text, and 96% of
the performance of the proprietary stem-
mer above.
1 Introduction
Stemming is the process of normalizing word vari-
ations by removing prefixes and suffixes. From an
?
Work done while a summer intern at IBM TJ Watson Re-
search Center
information retrieval point of view, prefixes and suf-
fixes add little or no additional meaning; in most
cases, both the efficiency and effectiveness of text
processing applications such as information retrieval
and machine translation are improved.
Building a rule-based stemmer for a new, arbitrary
language is time consuming and requires experts
with linguistic knowledge in that particular lan-
guage. Supervised learning also requires large quan-
tities of labeled data in the target language, and qual-
ity declines when using completely unsupervised
methods. We would like to reach a compromise
by using a few inexpensive and readily available re-
sources in conjunction with unsupervised learning.
Our goal is to develop a stemmer generator that
is relatively language independent (to the extent that
the language accepts stemming) and is trainable us-
ing little, inexpensive data. This paper presents
an unsupervised learning approach to non-English
stemming. The stemming model is based on statisti-
cal machine translation and it uses an English stem-
mer and a small (10K sentences) parallel corpus as
its sole training resources.
A parallel corpus is a collection of sentence pairs
with the same meaning but in different languages
(i.e. United Nations proceedings, bilingual newspa-
pers, the Bible). Table 1 shows an example that uses
the Buckwalter transliteration (Buckwalter, 1999).
Usually, entire documents are translated by humans,
and the sentence pairs are subsequently aligned by
automatic means. A small parallel corpus can be
available when native speakers and translators are
not, which makes building a stemmer out of such
corpus a preferable direction.
Arabic English
m$rwE Altqryr Draft report
wAkdt mmvlp zAm-
byA End ErDhA
lltqryr An bldhA
y$hd tgyyrAt xTyrp
wbEydp Almdy fy
AlmydAnyn Al-
syAsy wAlAqtSAdy
In introducing the report,
the representative of Zam-
bia emphasised that her
country was undergoing
serious and far-reaching
changes in the political
and economic field.
Table 1: A Tiny Arabic-English Parallel Corpus
We describe our approach towards reaching this
goal in section 2. Although we are using resources
other than monolingual data, the unsupervised na-
ture of our approach is preserved by the fact that
no direct information about non-English stemming
is present in the training data.
Monolingual, unannotated text in the target lan-
guage is readily available and can be used to further
improve the stemmer by allowing it to adapt to a de-
sired domain or genre. This optional step is closer to
the traditional unsupervised learning paradigm and
is described in section 2.4, and its impact on stem-
mer quality is described in 3.1.4.
Our approach (denoted by UNSUP in the rest of
the paper) is evaluated in section 3.1 by compar-
ing it to a proprietary Arabic stemmer (denoted by
GOLD). The latter is a state of the art Arabic stem-
mer, and was built using rules, suffix and prefix lists,
and human annotated text. GOLD is an earlier ver-
sion of the stemmer described in (Lee et al, ).
The task-based evaluation section 3.2 compares
the two stemmers by using them as a preprocessing
step in the TREC Arabic retrieval task. This section
also presents the improvement obtained over using
unstemmed text.
1.1 Arabic details
In this paper, Arabic was the target language but the
approach is applicable to any language that needs
affix removal. In Arabic, unlike English, both pre-
fixes and suffixes need to be removed for effective
stemming. Although Arabic provides the additional
challenge of infixes, we did not tackle them because
they often substantially change the meaning. Irregu-
lar morphology is also beyond the scope of this pa-
per. As a side note for readers with linguistic back-
ground (Arabic in particular), we do not claim that
the resulting stems are units representing the entire
paradigm of a lexical item. The main purpose of
stemming as seen in this paper is to conflate the to-
ken space used in statistical methods in order to im-
prove their effectiveness. The quality of the result-
ing tokens as perceived by humans is not as impor-
tant, since the stemmed output is intended for com-
puter consumption.
1.2 Related Work
The problem of unsupervised stemming or morphol-
ogy has been studied using several different ap-
proaches. For Arabic, good results have been ob-
tained for plural detection (Clark, 2001). (Gold-
smith, 2001) used a minimum description length
paradigm to build Linguistica, a system for which
the reported accuracy for European languages is cca.
83%. Note that the results in this section are not di-
rectly comparable to ours, since we are focusing on
Arabic.
A notable contribution was published by Snover
(Snover, 2002), who defines an objective function to
be optimized and performs a search for the stemmed
configuration that optimizes the function over all
stemming possibilities of a given text.
Rule-based stemming for Arabic is a problem
studied by many researchers; an excellent overview
is provided by (Larkey et al, ).
Morphology is not limited to prefix and suffix re-
moval; it can also be seen as mapping from a word to
an arbitrary meaning carrying token. Using an LSI
approach, (Schone and Jurafsky, ) obtained 88% ac-
curacy for English. This approach also deals with
irregular morphology, which we have not addressed.
A parallel corpus has been successfully used be-
fore by (Yarowsky et al, 2000) to project part of
speech tags, named entity tags, and morphology in-
formation from one language to the other. For a par-
allel corpus of comparable size with the one used
in our results, the reported accuracy was 93% for
French (when the English portion was also avail-
able); however, this result only covers 90% of the
tokens. Accuracy was later improved using suffix
trees.
(Diab and Resnik, 2002) used a parallel corpus
for word sense disambiguation, exploiting the fact
that different meanings of the same word tend to be
translated into distinct words.
2 Approach
Figure 1: Approach Overview
Our approach is based on the availability of the
following three resources:
? a small parallel corpus
? an English stemmer
? an optional unannotated Arabic corpus
Our goal is to train an Arabic stemmer using these
resources. The resulting stemmer will simply stem
Arabic without needing its English equivalent.
We divide the training into two logical steps:
? Step 1: Use the small parallel corpus
? Step 2: (optional) Use the monolingual corpus
The two steps are described in detail in the fol-
lowing subsections.
2.1 Step 1: Using the Small Parallel Corpus
Figure 2: Step 1 Iteration
In Step 1, we are trying to exploit the English
stemmer by stemming the English half of the paral-
lel corpus and building a translation model that will
establish a correspondence between meaning carry-
ing substrings (the stem) in Arabic and the English
stems.
For our purposes, a translation model is a matrix
of translation probabilities p(Arabic stem| English
stem) that can be constructed based on the small
parallel corpus (see subsection 2.2 for more details).
The Arabic portion is stemmed with an initial guess
(discussed in subsection 2.1.1)
Conceptually, once the translation model is built,
we can stem the Arabic portion of the parallel corpus
by scoring all possible stems that an Arabic word
can have, and choosing the best one. Once the Ara-
bic portion of the parallel corpus is stemmed, we can
build a more accurate translation model and repeat
the process (see figure 2). However, in practice, in-
stead of using a harsh cutoff and only keeping the
best stem, we impose a probability distribution over
the candidate stems. The distribution starts out uni-
form and then converges towards concentrating most
of the probability mass in one stem candidate.
2.1.1 The Starting Point
The starting point is an inherent problem for un-
supervised learning. We would like our stemmer to
give good results starting from a very general initial
guess (i.e. random). In our case, the starting point
is the initial choice of the stem for each individual
word. We distinguish several solutions:
? No stemming.
This is not a desirable starting point, since affix
probabilities used by our model would be zero.
? Random stemming
As mentioned above, this is equivalent to im-
posing a uniform prior distribution over the
candidate stems. This is the most general start-
ing point.
? A simple language specific rule - if available
If a simple rule is available, it would provide a
better than random starting point, at the cost of
reduced generality. For Arabic, this simple rule
was to use Al as a prefix and p as a suffix. This
rule (or at least the first half) is obvious even
to non-native speakers looking at transliterated
text. It also constitutes a surprisingly high base-
line.
2.2 The Translation Model ?
We adapted Model 1 (Brown et al, 1993) to our
purposes. Model 1 uses the concept of alignment
between two sentences e and f in a parallel corpus;
the alignment is defined as an object indicating for
each word ei which word fj generated it. To ob-
tain the probability of an foreign sentence f given the
English sentence e, Model 1 sums the products of
the translation probabilities over all possible align-
ments:
Pr(f |e) ?
?
{a}
m
?
j=1
t(fj|eaj )
The alignment variable ai controls which English
word the foreign word fi is aligned with. t(f |e) is
simply the translation probability which is refined
iteratively using EM. For our purposes, the transla-
tion probabilities (in a translation matrix) are the fi-
nal product of using the parallel corpus to train the
translation model.
To take into account the weight contributed by
each stem, the model?s iterative phase was adapted
to use the sum of the weights of a word in a sentence
instead of the count.
2.3 Candidate Stem Scoring
As previously mentioned, each word has a list of
substrings that are possible stems. We reduced the
problem to that of placing two separators inside each
Arabic word; the ?candidate stems? are simply the
substrings inside the separators. While this may
seem inefficient, in practice words tend to be short,
and one or two letter stems can be disallowed.
An initial, naive approach when scoring the stem
would be to simply look up its translation probabil-
ity, given the English stem that is most likely to be
its translation in the parallel sentence (i.e. the En-
glish stem aligned with the Arabic stem candidate).
Figure 3 presents scoring examples before normal-
ization.
?Note that the algorithm to build the translation model is
not a ?resource? per se, since it is a language-independent algo-
rithm.
English Phrase: the advisory committee
Arabic Phrase: Alljnp AlAst$Aryp
Task: stem AlAst$Aryp
Choices Score
AlAst$Aryp 0.2
AlAst$Aryp 0.7
AlAst$Aryp 0.8
AlAst$Aryp 0.1
.
.
.
.
.
.
Figure 3: Scoring the Stem
However, this approach has several drawbacks
that prevent us from using it on a corpus other than
the training corpus. Both of the drawbacks below
are brought about by the small size of the parallel
corpus:
? Out-of-vocabulary words: many Arabic stems
will not be seen in the small corpus
? Unreliable translation probabilities for low-
frequency stems.
We can avoid these issues if we adopt an alternate
view of stemming a word, by looking at the prefix
and the suffix instead. Given the word, the choice
of prefix and suffix uniquely determines the stem.
Since the number of unique affixes is much smaller
by definition, they will not have the two problems
above, even when using a small corpus. These prob-
abilities will be considerably more reliable and are
a very important part of the information extracted
from the parallel corpus. Therefore, the score of a
candidate stem should be based on the score of the
corresponding prefix and the suffix, in addition to
the score of the stem string itself:
score(?pas??) = f(p) ? f(a) ? f(s)
where a = Arabic stem, p = prefix, s=suffix
When scoring the prefix and the suffix, we could
simply use their probabilities from the previous
stemming iteration. However, there is additional in-
formation available that can be successfully used to
condition and refine these probabilities (such as the
length of the word, the part of speech tag if given
etc.).
English Phrase: the advisory committee
Arabic Phrase: Alljnp AlAst$Aryp
Task: stem AlAst$Aryp
Choices Score
AlAst$Aryp 0.8
AlAst$Aryp 0.7
AlAst$Ary 0.6
AlAst$Aryp 0.1
.
.
.
.
.
.
Figure 4: Alternate View: Scoring the Prefix and
Suffix
2.3.1 Scoring Models
We explored several stem scoring models, using
different levels of available information. Examples
include:
? Use the stem translation probability alone
score = t(a|e)
where a = Arabic stem, e = corresponding word
in the English sentence
? Also use prefix (p) and suffix (s) conditional
probabilities; several examples are given in ta-
ble 2.
Probability con-
ditioned on
Scoring Formula
the candidate stem t(a|e) ? p(p,s|a)+p(s|a)?p(p|a)2
the length of the
unstemmed Arabic
word (len)
t(a|e) ?
p(p,s|len)+p(s|len)?p(p|len)
2
the possible pre-
fixes and/or suf-
fixes
t(a|e) ? p(s|Spossible) ?
p(p|Ppossible)
the first and last
letter
t(a|e)?p(s|last)?p(p|first)
Table 2: Example Scoring Models
The first two examples use the joint probability
of the prefix and suffix, with a smoothing back-off
(the product of the individual probabilities). Scor-
ing models of this form proved to be poor perform-
ers from the beginning, and they were abandoned in
favor of the last model, which is a fast, good approx-
imation to the third model in Table 2. The last two
models successfully solve the problem of the empty
prefix and suffix accumulating excessive probability,
which would yield to a stemmer that never removed
any affixes. The results presented in the rest of the
paper use the last scoring model.
2.4 Step 2: Using the Unlabeled Monolingual
Data
This optional second step can adapt the trained stem-
mer to the problem at hand. Here, we are moving
away from providing the English equivalent, and we
are relying on learned prefix, suffix and (to a lesser
degree) stem probabilities. In a new domain or cor-
pus, the second step allows the stemmer to learn new
stems and update its statistical profile of the previ-
ously seen stems.
This step can be performed using monolingual
Arabic data, with no annotation needed. Even
though it is optional, this step is recommended since
its sole resource can be the data we would need to
stem anyway (see Figure 5).
Arabic Arabic
StemmedStemmerUnstemmed
Figure 5: Step 2 Detail
Step 1 produced a functional stemming model.
We can use the corpus statistics gathered in Step 1
to stem the new, monolingual corpus. However, the
scoring model needs to be modified, since t(a|e) is
no longer available. By removing the conditioning,
the first/last letter scoring model we used becomes
score = p(a) ? p(s|last) ? p(p|first)
The model can be updated if the stem candidate
score/probability distribution is sufficiently skewed,
and the monolingual text can be stemmed iteratively
using the new model. The model is thus adapted to
the particular needs of the new corpus; in practice,
convergence is quick (less than 10 iterations).
3 Results
3.1 Unsupervised Training and Testing
For unsupervised training in Step 1, we used a small
parallel corpus: 10,000 Arabic-English sentences
from the United Nations(UN) corpus, where the En-
glish part has been stemmed and the Arabic translit-
erated.
For unsupervised training in Step 2, we used a
larger, Arabic only corpus: 80,000 different sen-
tences in the same dataset.
The test set consisted of 10,000 different sen-
tences in the UN dataset; this is the testing set used
below unless specified.
We also used a larger corpus ( a year of Agence
France Press (AFP) data, 237K sentences) for Step 2
training and testing, in order to gauge the robustness
and adaptation capability of the stemmer. Since the
UN corpus contains legal proceedings, and the AFP
corpus contains news stories, the two can be seen as
coming from different domains.
3.1.1 Measuring Stemmer Performance
In this subsection the accuracy is defined as agree-
ment with GOLD. GOLD is a state of the art, pro-
prietary Arabic stemmer built using rules, suffix and
prefix lists, and human annotated text, in addition
to an unsupervised component. GOLD is an ear-
lier version of the stemmer described in (Lee et al,
). Freely available (but less accurate) Arabic light
stemmers are also used in practice.
When measuring accuracy, all tokens are consid-
ered, including those that cannot be stemmed by
simple affix removal (irregulars, infixes). Note that
our baseline (removing Al and p, leaving everything
unchanged) is higher that simply leaving all tokens
unchanged.
For a more relevant task-based evaluation, please
refer to Subsection 3.2.
3.1.2 The Effect of the Corpus Size: How little
parallel data can we use?
We begin by examining the effect that the size of
the parallel corpus has on the results after the first
step. Here, we trained our stemmer on three dif-
ferent corpus sizes: 50K, 10K, and 2K sentences.
The high baseline is obtained by treating Al and p
as affixes. The 2K corpus had acceptable results (if
this is all the data available). Using 10K was sig-
nificantly better; however the improvement obtained
when five times as much data (50K) was used was
insignificant. Note that different languages might
have different corpus size needs. All other results
Figure 6: Results after Step 1 : Corpus Size Effect
in this paper use 10K sentences.
3.1.3 The Knowledge-Free Starting Point after
Step 1
Figure 7: Results after Step 1 : Effect of Knowing
the Al+p rule
Although severely handicapped at the beginning,
the knowledge-free starting point manages to narrow
the performance gap after a few iterations. Knowing
the Al+p rule still helps at this stage. However, the
performance gap is narrowed further in Step 2 (see
figure 8), where the knowledge free starting point
benefitted from the monolingual training.
3.1.4 Results after Step 2: Different Corpora
Used for Adaptation
Figure 8 shows the results obtained when aug-
menting the stemmer trained in Step 1. Two dif-
ferent monolingual corpora are used: one from the
same domain as the test set (80K UN), and one from
a different domain/corpus, but three times larger
(237K AFP). The larger dataset seems to be more
useful in improving the stemmer, even though the
domain was different.
Figure 8: Results after Step 2 (Monolingual Corpus)
The baseline and the accuracy after Step 1 are pre-
sented for reference.
3.1.5 Cross-Domain Robustness
Figure 9: Results after Step 2 : Using a Different
Test Set
We used an additional test set that consisted of
10K sentences taken from AFP, instead of UN as in
previous experiments shown in figure 8 . Its pur-
pose was to test the cross-domain robustness of the
stemmer and to further examine the importance of
applying the second step to the data needing to be
stemmed.
Figure 9 shows that, even though in Step 1 the
stemmer was trained on UN proceedings, the re-
sults on the cross-domain (AFP) test set are compa-
rable to those from the same domain (UN, figure 8).
However, for this particular test set the baseline was
much higher; thus the relative improvement with re-
spect to the baseline is not as high as when the unsu-
pervised training and testing set came from the same
collection.
3.2 Task-Based Evaluation : Arabic
Information Retrieval
Task Description:
Given a set of Arabic documents and an Arabic
query, find a list of documents relevant to the query,
and rank them by probability of relevance.
We used the TREC 2002 documents (several
years of AFP data), queries and relevance judg-
ments. The 50 queries have a shorter, ?title? compo-
nent as wel as a longer ?description?. We stemmed
both the queries and the documents using UNSUP
and GOLD respectively. For comparison purposes,
we also left the documents and queries unstemmed.
The UNSUP stemmer was trained with 10K UN
sentences in Step 1, and with one year?s worth of
monolingual AFP data (1995) in Step 2.
Evaluation metric: The evaluation metric used
below is mean average precision (the standard IR
metric), which is the mean of average precision
scores for each query. The average precision of a
single query is the mean of the precision scores after
each relevant document retrieved. Note that aver-
age precision implicitly includes recall information.
Precision is defined as the ratio of relevant docu-
ments to total documents retrieved up to that point
in the ranking.
Results
Figure 10: Arabic Information Retrieval Results
We looked at the effect of different testing con-
ditions on the mean average precision for the 50
queries. In Figure 10, the first set of bars uses the
query titles only, the second set adds the descrip-
tion, and the last set restricts the results to one year
(1995), using both the title and description. We
tested this last condition because the unsupervised
stemmer was refined in Step 2 using 1995 docu-
ments. The last group of bars shows a higher relative
improvement over the unstemmed baseline; how-
ever, this last condition is based on a smaller sample
of relevance judgements (restricted to one year) and
is therefore not as representative of the IR task as the
first two testing conditions.
4 Conclusions and Future Work
This paper presents an unsupervised learning ap-
proach to building a non-English (Arabic) stemmer
using a small sentence-aligned parallel corpus in
which the English part has been stemmed. No paral-
lel text is needed to use the stemmer. Monolingual,
unannotated text can be used to further improve the
stemmer by allowing it to adapt to a desired domain
or genre. The approach is applicable to any language
that needs affix removal; for Arabic, our approach
results in 87.5% agreement with a proprietary Ara-
bic stemmer built using rules, affix lists, and hu-
man annotated text, in addition to an unsupervised
component. Task-based evaluation using Arabic in-
formation retrieval indicates an improvement of 22-
38% in average precision over unstemmed text, and
93-96% of the performance of the state of the art,
language specific stemmer above.
We can speculate that, because of the statistical
nature of the unsupervised stemmer, it tends to fo-
cus on the same kind of meaning units that are sig-
nificant for IR, whether or not they are linguistically
correct. This could explain why the gap betheen
GOLD and UNSUP is narrowed with task-based
evaluation and is a desirable effect when the stem-
mer is to be used for IR tasks.
We are planning to experiment with different lan-
guages, translation model alternatives, and to extend
task-based evaluation to different tasks such as ma-
chine translation and cross-lingual topic detection
and tracking.
5 Acknowledgements
We would like to thank the reviewers for their help-
ful observations and for identifying Arabic mis-
spellings. This work was partially supported by
the Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. This research is also spon-
sored in part by the National Science Foundation
(NSF) under grants EIA-9873009 and IIS-9982226,
and in part by the DoD under award 114008-
N66001992891808. However, any opinions, views,
conclusions and findings in this paper are those of
the authors and do not necessarily reflect the posi-
tion of policy of the Government and no official en-
dorsement should be inferred.
References
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of machine translation:
Parameter estimation. In Computational Linguistics,
pages 263?311.
Tim Buckwalter. 1999. Buckwalter transliteration.
http://www.cis.upenn.edu/?cis639/arabic/info/translit-
chart.html.
Alexander Clark. 2001. Learning morphology with pair
hidden markov models. In ACL (Companion Volume),
pages 55?60.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
255?262, July.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. In Computational
Linguistics.
Leah Larkey, Lisa Ballesteros, and Margaret Connell.
Improving stemming for arabic information retrieval:
Light stemming and co-occurrence analysis. In SIGIR
2002, pages 275?282.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. Language model
based arabic word segmentation. In To appear in ACL
2003.
Patrick Schone and Daniel Jurafsky. Knowledge-free in-
duction of morphology using latent semantic analy-
sis. In 4th Conference on Computational Natural Lan-
guage Learning, Lisbon, 2000.
Matthew Snover. 2002. An unsupervised knowledge
free algorithm for the learning of morphology in natu-
ral languages. Master?s thesis, Washington University,
May.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora.
Customizing Parallel Corpora at the Document Level  
Monica ROGATI and Yiming YANG 
Computer Science Department, Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213 
mrogati@cs.cmu.edu, yiming@cs.cmu.edu 
 
 
Abstract 
Recent research in cross-lingual 
information retrieval (CLIR) established the 
need for properly matching the parallel corpus 
used for query translation to the target corpus. 
We propose a document-level approach to 
solving this problem: building a custom-made 
parallel corpus by automatically assembling it 
from documents taken from other parallel 
corpora. Although the general idea can be 
applied to any application that uses parallel 
corpora, we present results for CLIR in the 
medical domain. In order to extract the best-
matched documents from several parallel 
corpora, we propose ranking individual 
documents by using a length-normalized 
Okapi-based similarity score between them and 
the target corpus. This ranking allows us to 
discard 50-90% of the training data, while 
avoiding the performance drop caused by a 
good but mismatched resource, and even 
improving CLIR effectiveness by 4-7% when 
compared to using all available training data. 
1 Introduction 
Our recent research in cross-lingual information 
retrieval (CLIR) established the need for properly 
matching the parallel corpus used for query 
translation to the target corpus (Rogati and Yang, 
2004). In particular, we showed that using a 
general purpose machine translation (MT) system 
such as SYSTRAN, or a general purpose parallel 
corpus - both of which perform very well for news 
stories (Peters, 2003) - dramatically fails in the 
medical domain. To explore solutions to this 
problem, we used cosine similarity between 
training and target corpora as respective weights 
when building a translation model. This approach 
treats a parallel corpus as a homogeneous entity, an 
entity that is self-consistent in its domain and 
document quality. In this paper, we propose that 
instead of weighting entire resources, we can select 
individual documents from these corpora in order 
to build a parallel corpus that is tailor-made to fit a 
specific target collection. To avoid confusion, it is 
helpful to remember that in IR settings the true test 
data are the queries, not the target documents. The 
documents are available off-line and can be (and 
usually are) used for training and system 
development. In other words, by matching the 
training corpora and the target documents we are 
not using test data for training.  
(Rogati and Yang, 2004) also discusses 
indirectly related work, such as query translation 
disambiguation and building domain-specific 
language models for speech recognition. We are 
not aware of any additional related work. 
In addition to proposing individual documents 
as the unit for building custom-made parallel 
corpora, in this paper we start exploring the criteria 
used for individual document selection by 
examining the effect of ranking documents using 
the length-normalized Okapi-based similarity score 
between them and the target corpus. 
2 Evaluation Data 
2.1 Medical Domain Corpus: Springer 
The Springer corpus consists of 9640 documents 
(titles plus abstracts of medical journal articles) 
each in English and in German, with 25 queries in 
both languages, and relevance judgments made by 
native German speakers who are medical experts 
and are fluent in English. We split this parallel 
corpus into two subsets, and used the first subset 
(4,688 documents) for training, and the remaining 
subset (4,952 documents) as the test set in all our 
experiments. This configuration allows us to 
experiment with CLIR in both directions (EN-DE 
and DE-EN). We applied an alignment algorithm 
to the training documents, and obtained a sentence-
aligned parallel corpus with about 30K sentences 
in each language.  
2.2 Training Corpora 
In addition to Springer, we have used four other 
English-German parallel corpora for training: 
? NEWS is a collection of 59K sentence 
aligned news stories, downloaded from the 
web (1996-2000), and available at 
http://www.isi.edu/~koehn/publications/de-
news/ 
? WAC is a small parallel corpus obtained by 
mining the web (Nie et al, 2000), in no 
particular domain 
? EUROPARL is a parallel corpus provided 
by (Koehn). Its documents are sentence 
aligned European Parliament proceedings. 
This is a large collection that has been 
successfully used for CLEF, when the target 
corpora were collections of news stories 
(Rogati and Yang, 2003). 
? MEDTITLE is an English-German parallel 
corpus consisting of 549K paired titles of 
medical journal articles. These titles were 
gathered from the PubMed online database 
(http://www.ncbi.nlm.nih.gov/PubMed/).  
Table 1 presents a summary of the five training 
corpora characteristics. 
 
Name Size (sent) Domain 
NEWS 59K news 
WAC 60K mixed 
EUROPAR
L 665K politics 
SPRINGE
R 30K medical 
MEDTITL
E 550K medical 
 
Table 1. Characteristics of Parallel Training 
Corpora 
 
3 Selecting Documents from Parallel Corpora   
While selecting and weighing entire training 
corpora is a problem already explored by (Rogati 
and Yang, 2004), in this paper we focus on a lower 
granularity level: individual documents in the 
parallel corpora. We seek to construct a custom 
parallel corpus, by choosing individual documents 
which best match the testing collection. We 
compute the similarity between the test collection 
(in German or English) and each individual 
document in the parallel corpora for that respective 
language.  We have a choice of similarity metrics, 
but since this computation is simply retrieval with 
a long query, we start with the Okapi model  
(Robertson, 1993), as implemented by the Lemur 
system (Olgivie and Callan, 2001). Although the 
Okapi model takes into account average document 
length, we compare it with its length-normalized 
version, measuring per-word similarity. The two 
measures are identified in the results section by 
?Okapi? and ?Normalized?. 
Once the similarity is computed for each 
document in the parallel corpora, only the top N 
most similar documents are kept for training. They 
are an approximation of the domain(s) of the test 
collection. Selecting N has not been an issue for 
this corpus   (values between 10-75% were safe). 
However, more generally, this parameter can be 
tuned to a different test corpus as any other 
parameter. Alternatively, the document score can 
also be incorporated into the translation model, 
eliminating the need for thresholding. 
4 CLIR Method 
We used a corpus-based approach, similar to that 
in (Rogati and Yang, 2003). Let L1 be the source 
language and L2 be the target language. The cross-
lingual retrieval consists of the following steps: 
1. Expanding a query in L1 using blind 
feedback 
2. Translating the query by taking the dot 
product between the query vector (with 
weights from step 1) and a translation 
matrix obtained by calculating translation 
probabilities or term-term similarity using 
the parallel corpus. 
3. Expanding the query in L2 using blind 
feedback 
4. Retrieving documents in L2 
Here, blind feedback is the process of retrieving 
documents and adding the terms of the top-ranking 
documents to the query for expansion. We used 
simplified Rocchio positive feedback as 
implemented by Lemur (Olgivie and Callan, 2001). 
For the results in this paper, we have used 
Pointwise Mutual Information (PMI) instead of 
IBM Model 1 (Brown et al, 1993), since (Rogati 
and Yang, 2004) found it to be as effective on 
Springer, but faster to compute.  
 
5 Results and Discussion 
5.1 Empirical Settings 
For the retrieval part of our system, we adapted 
Lemur (Ogilvie and Callan, 2001)  to allow the use 
of weighted queries. Several parameters were 
tuned, none of them on the test set.  In our corpus-
based approach, the main parameters are those 
used in query expansion based on pseudo-
relevance, i.e., the maximum number of documents 
and the maximum number of words to be used, and 
the relative weight of the expanded portion with 
respect to the initial query. Since the Springer 
training set is fairly small, setting aside a subset of 
the data for parameter tuning was not desirable. 
We instead chose parameter values that were stable 
on the CLEF collection (Peters, 2003): 5 and 20 as 
the maximum numbers of documents and words, 
respectively. The relative weight of the expanded 
portion with respect to the initial query was set to 
0.5. The results were evaluated using mean 
average precision (AvgP), a standard performance 
measure for IR evaluations. 
In the following sections, DE-EN refers to 
retrieval where the query is in German and the 
documents in English, while EN-DE refers to 
retrieval in the opposite direction. 
5.2 Using the Parallel Corpora Separately 
Can we simply choose a parallel corpus that 
performed very well on news stories, hoping it is 
robust across domains? Natural approaches also 
include choosing the largest corpus available, or 
using all corpora together. Figure 1 shows the 
effect of these strategies. 
 
 
Figure 1. CLIR results on the Springer test set by 
using PMI with different training corpora. 
 
 
We notice that choosing the largest collection 
(EUROPARL), using all resources available 
without weights (ALL), and even choosing a large 
collection in the medical domain (MEDTITLE) are 
all sub-optimal strategies.   
Given these results, we believe that resource 
selection and weighting is necessary. Thoroughly 
exploring weighting strategies is beyond the scope 
of this paper and it would involve collection size, 
genre, and translation quality in addition to a 
measure of domain match. Here, we start by 
selecting individual documents that match the 
domain of the test collection. We examine the 
effect this choice has on domain-specific CLIR.  
5.3 Using Okapi weights to build a custom 
parallel corpus 
Figures 2 and 3 compare the two document 
selection strategies discussed in Section 3 to using 
all available documents, and to the ideal (but not 
truly optimal) situation where there exists a ?best? 
resource to choose and this collection is known. By 
?best?, we mean one that can produce optimal 
results on the test corpus, with respect to the given 
metric In reality, the true ?best? resource is 
unknown: as seen above, many intuitive choices 
for the best collection are not optimal. 
 
40
45
50
55
60
1 10 100
Percent Used (log)
A
v
er
ag
e 
Pr
ec
is
io
n
Okapi Normalized
All Corpora Best Corpus
 
 
Figure 2. CLIR  DE-EN performance vs. Percent 
of Parallel Documents  Used. ?Best Corpus? is 
given by an oracle and is usually unknown. 
 
 
50
55
60
65
70
1 10 100
Percent Used (log)
A
v
er
ag
e 
Pr
ec
is
io
n
Okapi Normalized
All Corpora Best Corpus
 
 
Figure 3. CLIR  EN-DE performance vs. Percent 
of Parallel Documents Used. ?Best Corpus? is 
given by an oracle and is usually unknown 
 
0
10
20
30
40
50
60
70
EN-DE DE-ENAvgP.
SPRINGER MEDTITLE WAC
NEWS EUROPARL ALL
 Notice that the normalized version performs better 
and is more stable. Per-word similarity is, in this 
case, important when the documents are used to 
train translation scores: shorter parallel documents 
are better when building the translation matrix. Our 
strategy accounts for a 4-7% improvement over 
using all resources with no weights, for both 
retrieval directions. It is also very close to the 
?oracle? condition, which chooses the best 
collection in advance. More importantly, by using 
this strategy we are avoiding the sharp 
performance drop when using a mismatched, 
although very good, resource (such as 
EUROPARL). 
 
6 Future Work 
We are currently exploring weighting strategies 
involving collection size, genre, and estimating 
translation quality in addition to a measure of 
domain match.  Another question we are 
examining is the granularity level used when 
selecting resources, such as selection at the 
document or cluster level.  
Similarity and overlap between resources 
themselves is also worth considering while 
exploring tradeoffs between redundancy and noise.  
We are also interested in how these approaches 
would apply to other domains.  
 
7 Conclusions 
We have examined the issue of selecting 
appropriate training resources for cross-lingual 
information retrieval. We have proposed and 
evaluated a simple method for creating a 
customized parallel corpus from other available 
parallel corpora by matching the domain of the test 
documents with that of individual parallel 
documents. We noticed that choosing the largest 
collection, using all resources available without 
weights, and even choosing a large collection in 
the medical domain are all sub-optimal strategies. 
The techniques we have presented here are not 
restricted to CLIR and can be applied to other 
areas where parallel corpora are necessary, such as 
statistical machine translation. The trained 
translation matrix can also be reused and can be 
converted to any of the formats required by such 
applications. 
 
8 Acknowledgements 
We would like to thank Ralf Brown for collecting 
the MEDTITLE and SPRINGER data.  
This research is sponsored in part by the National 
Science Foundation (NSF) under grant IIS-
9982226, and in part by the DOD under award 
114008-N66001992891808. Any opinions and 
conclusions in this paper are the authors? and do 
not necessarily reflect those of the sponsors. 
References 
Brown, P.F, Pietra, D., Pietra, D, Mercer, R.L. 1993.The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. In Computational Linguistics, 
19:263-312 
Koehn, P. Europarl: A Multilingual Corpus for 
Evaluation of Machine Translation. Draft, 
Unpublished. 
Nie, J. Y., Simard, M. and Foster, G.. 2000. Using 
parallel web pages for multi-lingual IR. In C. 
Peters(Ed.),  Proceedings of the CLEF 2000 forum  
Ogilvie, P. and Callan, J. 2001.  Experiments using the 
Lemur toolkit. In Proceedings of the Tenth Text 
Retrieval Conference (TREC-10).  
Peters, C. 2003. Results of the CLEF 2003 Cross-Language 
System Evaluation Campaign. Working Notes for the 
CLEF 2003 Workshop, 21-22 August, Trondheim, 
Norway 
Robertson, S.E. and all. 1993. Okapi at TREC. In The 
First TREC Retrieval Conference, Gaithersburg, MD.  
pp. 21-30 
Rogati, M and Yang, Y. 2003. Multilingual Information 
Retrieval using Open, Transparent Resources in 
CLEF 2003 . In C. Peters (Ed.), Results of the 
CLEF2003 cross-language evaluation forum 
Rogati, M and Yang, Y. 2004. Resource Selection for 
Domain Specific Cross-Lingual IR. In Proceedings of 
ACM SIGIR Conference on Research and 
Development in Information Retrieval (SIGIR'04). 
