Proceedings of the 12th Conference of the European Chapter of the ACL, pages 345?353,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
End-to-End Evaluation in Simultaneous Translation
Olivier Hamon1,2, Christian F?gen3, Djamel Mostefa1, Victoria Arranz1,
Muntsin Kolss3, Alex Waibel3,4 and Khalid Choukri1
1Evaluations and Language Resources Distribution Agency (ELDA), Paris, France
2 LIPN (UMR 7030) ? Universit? Paris 13 & CNRS, Villetaneuse, France
3 Univerit?t Karlsruhe (TH), Germany
4 Carnegie Mellon University, Pittsburgh, USA
{hamon|mostefa|arranz|choukri}@elda.org,
{fuegen|kolss|waibel}@ira.uka.de
Abstract
This paper presents the end-to-end evalu-
ation of an automatic simultaneous trans-
lation system, built with state-of-the-art
components. It shows whether, and for
which situations, such a system might be
advantageous when compared to a human
interpreter. Using speeches in English
translated into Spanish, we present the
evaluation procedure and we discuss the
results both for the recognition and trans-
lation components as well as for the over-
all system. Even if the translation process
remains the Achilles? heel of the system,
the results show that the system can keep
at least half of the information, becoming
potentially useful for final users.
1 Introduction
Anyone speaking at least two different languages
knows that translation and especially simultaneous
interpretation are very challenging tasks. A human
translator has to cope with the special nature of
different languages, comprising phenomena like
terminology, compound words, idioms, dialect
terms or neologisms, unexplained acronyms or ab-
breviations, proper names, as well as stylistic and
punctuation differences. Further, translation or in-
terpretation are not a word-by-word rendition of
what was said or written in a source language. In-
stead, the meaning and intention of a given sen-
tence have to be reexpressed in a natural and fluent
way in another language.
Most professional full-time conference inter-
preters work for international organizations like
the United Nations, the European Union, or the
African Union, whereas the world?s largest em-
ployer of translators and interpreters is currently
the European Commission. In 2006, the European
Parliament spent about 300 million Euros, 30% of
its budget, on the interpretation and translation of
the parliament speeches and EU documents. Gen-
erally, about 1.1 billion Euros are spent per year
on the translating and interpreting services within
the European Union, which is around 1% of the
total EU-Budget (Volker Steinbiss, 2006).
This paper presents the end-to-end evaluation
of an automatic simultaneous translation system,
built with state-of-the-art components. It shows
whether, and in which cases, such a system might
be advantageous compared to human interpreters.
2 Challenges in Human Interpretation
According to Al-Khanji et al (2000), researchers
in the field of psychology, linguistics and interpre-
tation seem to agree that simultaneous interpre-
tation (SI) is a highly demanding cognitive task
involving a basic psycholinguistic process. This
process requires the interpreter to monitor, store
and retrieve the input of the source language in
a continuous manner in order to produce the oral
rendition of this input in the target language. It is
clear that this type of difficult linguistic and cog-
nitive operation will force even professional in-
terpreters to elaborate lexical or synthetic search
strategies.
Fatigue and stress have a negative effect on the
interpreter, leading to a decrease in simultaneous
interpretation quality. In a study by Moser-Mercer
et al (1998), in which professional speakers were
asked to work until they could no longer provide
acceptable quality, it was shown that (1) during
the first 20 minutes the frequency of errors rose
steadily, (2) the interpreters, however, seemed to
be unaware of this decline in quality, (3) after 60
minutes, all subjects made a total of 32.5 mean-
ing errors, and (4) in the category of nonsense the
number of errors almost doubled after 30 minutes
on the task.
Since the audience is only able to evaluate the
simultaneously interpreted discourse by its form,
345
the fluency of an interpretation is of utmost im-
portance. According to a study by Kopczynski
(1994), fluency and style were third on a list of
priorities (after content and terminology) of el-
ements rated by speakers and attendees as con-
tributing to quality. Following the overview in
(Yagi, 2000), an interpretation should be as natu-
ral and as authentic as possible, which means that
artificial pauses in the middle of a sentence, hes-
itations, and false-starts should be avoided, and
tempo and intensity of the speaker?s voice should
be imitated.
Another point to mention is the time span be-
tween a source language chunk and its target lan-
guage chunk, which is often referred to as ear-
voice-span. Following the summary in (Yagi,
2000), the ear-voice-span is variable in duration
depending on some source and target language
variables, like speech delivery rate, information
density, redundancy, word order, syntactic charac-
teristics, etc. Short delays are usually preferred for
several reasons. For example, the audience is irri-
tated when the delay is too large and is soon asking
whether there is a problem with the interpretation.
3 Automatic Simultaneous Translation
Given the explanations above on human interpre-
tation, one has to weigh two factors when consid-
ering the use of simultaneous translation systems:
translation quality and cost.
The major disadvantage of an automatic system
compared to human interpretation is its translation
quality, as we will see in the following sections.
Current state-of-the-art systems may reach satis-
factory quality for people not understanding the
lecturer at all, but are still worse than human inter-
pretation. Nevertheless, an automatic system may
have considerable advantages.
One such advantage is its considerable short-
term memory: storing long sequences of words is
not a problem for a computer system. Therefore,
compensatory strategies are not necessary, regard-
less of the speaking rate of the speaker. However,
depending on the system?s translation speed, la-
tency may increase. While it is possible for hu-
mans to compress the length of an utterance with-
out changing its meaning (summarization), it is
still a challenging task for automatic systems.
Human simultaneous interpretation is quite ex-
pensive, especially due to the fact that usually two
interpreters are necessary. In addition, human in-
terpreters require preparation time to become fa-
miliar with the topic. Moreover, simultaneous in-
terpretation requires a soundproof booth with au-
dio equipment, which adds an overall cost that is
unacceptable for all but the most elaborate multi-
lingual events. On the other hand, a simultaneous
translation system also needs time and effort for
preparation and adaptation towards the target ap-
plication, language and domain. However, once
adapted, it can be easily re-used in the same do-
main, language, etc. Another advantage is that the
transcript of a speech or lecture is produced for
free by using an automatic system in the source
and target languages.
3.1 The Simultaneous Translation System
Figure 1 shows a schematic overview of the si-
multaneous translation system developed at Uni-
versit?t Karlsruhe (TH) (F?gen et al, 2006b). The
speech of the lecturer is recorded with the help
of a close-talk microphone and processed by the
speech recognition component (ASR). The par-
tial hypotheses produced by the ASR module are
collected in the resegmentation component, for
merging and re-splitting at appropriate ?seman-
tic? boundaries. The resegmented hypotheses are
then transferred to one or more machine transla-
tion components (MT), at least one per language
pair. Different output technologies may be used
for presenting the translations to the audience. For
a detailed description of the components as well
as the client-server framework used for connect-
ing the components please refer to (F?gen et al,
2006b; F?gen et al, 2006a; Kolss et al, 2006; F?-
gen and Kolss, 2007; F?gen et al, 2001).
3.2 End-to-End Evaluation
The evaluation in speech-to-speech translation
jeopardises many concepts and implies a lot of
subjectivity. Three components are involved and
an overall system may grow the difficulty of esti-
mating the output quality. However, two criteria
are mainly accepted in the community: measuring
the information preservation and determining how
much of the translation is understandable.
Several end-to-end evaluations in speech-to-
speech translation have been carried out in the last
few years, in projects such as JANUS (Gates et
al., 1996), Verbmobil (N?bel, 1997) or TC-STAR
(Hamon et al, 2007). Those projects use the
main criteria depicted above, and protocols differ
in terms of data preparation, rating, procedure, etc.
346
Dictionary
Source
Hypothesis Translatable
Segment
Model
Source Boundary
Resegmen?
tationRecognition
Speech
Translation
Model Model
Target Language
Machine
Translation
Model
Source Acoustic
Model
Source Language
Output
Translated
Translation
Vocabulary
Audio Stream
Text
Output
(Subtitles)(Synthesis)
Spoken
Figure 1: Schematic overview and information flow of the simultaneous translation system. The main
components of the system are represented by cornered boxes and the models used for theses components
by ellipses. The different output forms are represented by rounded boxes.
To our opinion, to evaluate the performance of a
complete speech-to-speech translation system, we
need to compare the source speech used as input to
the translated output speech in the target language.
To that aim, we reused a large part of the evalua-
tion protocol from the TC-STAR project(Hamon
et al, 2007).
4 Evaluation Tasks
The evaluation is carried out on the simultaneously
translated speech of a single speaker?s talks and
lectures in the field of speech processing, given in
English, and translated into Spanish.
4.1 Data used
Two data sets were selected from the talks and
lectures. Each set contained three excerpts, no
longer than 6 minutes each and focusing on dif-
ferent topics. The former set deals with speech
recognition and the latter with the descriptions of
European speech research projects, both from the
same speaker. This represents around 7,200 En-
glish words. The excerpts were manually tran-
scribed to produce the reference for the ASR eval-
uation. Then, these transcriptions were manually
translated into Spanish by two different transla-
tors. Two reference translations were thus avail-
able for the spoken language translation (SLT)
evaluation. Finally, one human interpretation was
produced from the excerpts as reference for the
end-to-end evaluation. It should be noted that for
the translation system, speech synthesis was used
to produce the spoken output.
4.2 Evaluation Protocol
The system is evaluated as a whole (black box
evaluation) and component by component (glass
box evaluation):
ASR evaluation. The ASR module is evaluated
by computing the Word Error Rate (WER) in case
insensitive mode.
SLT evaluation. For the SLT evaluation, the au-
tomatically translated text from the ASR output is
compared with two manual reference translations
by means of automatic and human metrics. Two
automatic metrics are used: BLEU (Papineni et
al., 2001) and mWER (Niessen et al, 2000).
For the human evaluation, each segment is eval-
uated in relation to adequacy and fluency (White
and O?Connell, 1994). For the evaluation of ad-
equacy, the target segment is compared to a ref-
erence segment. For the evaluation of fluency,
the quality of the language is evaluated. The two
types of evaluation are done independently, but
each evaluator did both evaluations (first that of
fluency, then that of adequacy) for a certain num-
ber of segments. For the evaluation of fluency,
evaluators had to answer the question: ?Is the text
written in good Spanish??. For the evaluation of
adequacy, evaluators had to answer the question:
?How much of the meaning expressed in the ref-
erence translation is also expressed in the target
translation??.
For both evaluations, a five-point scale is pro-
posed to the evaluators, where only extreme val-
ues are explicitly defined. Three evaluations are
carried out per segment, done by three different
evaluators, and segments are divided randomly,
because evaluators must not recreate a ?story?
347
and thus be influenced by the context. The total
number of judges was 10, with around 100 seg-
ments per judge. Furthermore, the same number
of judges was recruited for both categories: ex-
perts, from the domain with a knowledge of the
technology, and non-experts, without that knowl-
edge.
End-to-End evaluation. The End-to-End eval-
uation consists in comparing the speech in the
source language to the output speech in the tar-
get language. Two important aspects should be
taken into account when assessing the quality of
a speech-to-speech system.
First, the information preservation is measured
by using ?comprehension questionnaires?. Ques-
tions are created from the source texts (the En-
glish excerpts), then questions and answers are
translated into Spanish by professional translators.
These questions are asked to human judges after
they have listened to the output speech in the tar-
get language (Spanish). At a second stage, the an-
swers are analysed: for each answer a Spanish val-
idator gives a score according to a binary scale (the
information is either correct or incorrect). This al-
lows us to measure the information preservation.
Three types of questions are used in order to di-
versify the difficulty of the questions and test the
system at different levels: simple Factual (70%),
yes/no (20%) and list (10%) questions. For in-
stance, questions were: What is the larynx respon-
sible for?, Have all sites participating in CHIL
built a CHIL room?, Which types of knowledge
sources are used by the decoder?, respectively.
The second important aspect of a speech-to-
speech system is the quality of the speech output
(hereafter quality evaluation). For assessing the
quality of the speech output one question is asked
to the judges at the end of each comprehension
questionnaire: ?Rate the overall quality of this au-
dio sample?, and values go from 1 (?1: Very bad,
unusable?) to 5 (?It is very useful?). Both auto-
matic system and interpreter outputs were evalu-
ated with the same methodology.
Human judges are real users and native Span-
ish speakers, experts and non-experts, but different
from those of the SLT evaluation. Twenty judges
were involved (12 excerpts, 10 evaluations per ex-
cerpt and 6 evaluations per judge) and each judge
evaluated both automatic and human excerpts on a
50/50 percent basis.
5 Components Results
5.1 Automatic Speech Recognition
The ASR output has been evaluated using the
manual transcriptions of the excerpts. The overall
Word Error Rate (WER) is 11.9%. Table 1 shows
the WER level for each excerpt.
Excerpts WER [%]
L043-1 14.5
L043-2 14.5
L043-3 9.6
T036-1 11.3
T036-2 11.7
T036-3 9.2
Overall 11.9
Table 1: Evaluation results for ASR.
T036 excerpts seem to be easier to recognize au-
tomatically than L043 ones, probably due to the
more general language of the former.
5.2 Machine Translation
5.2.1 Human Evaluation
Each segment within the human evaluation is eval-
uated 4 times, each by a different judge. This aims
at having a significant number of judgments and
measuring the consistency of the human evalua-
tions. The consistency is measured by computing
the Cohen?s Kappa coefficient (Cohen, 1960).
Results show a substantial agreement for flu-
ency (kappa of 0.64) and a moderate agreement
for adequacy (0.52).The overall results of the hu-
man evaluation are presented in Table 2. Regard-
ing both experts? and non-experts? details, agree-
ment is very similar (0.30 and 0.28, respectively).
All judges Experts Non experts
Fluency 3.13 2.84 3.42
Adequacy 3.26 3.21 3.31
Table 2: Average rating of human evalua-
tions [1<5].
Both fluency and adequacy results are over the
mean. They are lower for experts than for non-
experts. This may be due to the fact that experts
are more familiar with the domain and therefore
more demanding than non experts. Regarding the
detailed evaluation per judge, scores are generally
lower for non-experts than for experts.
348
5.2.2 Automatic Evaluation
Scores are computed using case-sensitive metrics.
Table 3 shows the detailed results per excerpt.
Excerpts BLEU [%] mWER [%]
L043-1 25.62 58.46
L043-2 22.60 62.47
L043-3 28.73 62.64
T036-1 34.46 55.13
T036-2 29.41 59.91
T036-3 35.17 50.77
Overall 28.94 58.66
Table 3: Automatic Evaluation results for SLT.
Scores are rather low, with a mWER of 58.66%,
meaning that more than half of the translation is
correct. According to the scoring, the T036 ex-
cerpts seem to be easier to translate than the L043
ones, the latter being of a more technical nature.
6 End-to-End Results
6.1 Evaluators Agreement
In this study, ten judges carried out the evaluation
for each excerpt. In order to observe the inter-
judges agreement, the global Fleiss?s Kappa co-
efficient was computed, which allows to measure
the agreement between m judges with r criteria of
judgment. This coefficient shows a global agree-
ment between all the judges, which goes beyond
Cohen?s Kappa coefficient. However, a low co-
efficient requires a more detailed analysis, for in-
stance, by using Kappa for each pair of judges.
Indeed, this allows to see how deviant judges are
from the typical judge behaviour. For m judges,
n evaluations and r criteria, the global Kappa is
defined as follows:
? = 1 ?
nm2 ?
?n
i=1
?r
j=1 X
2
ij
nm(m? 1) ?rj=1 Pj(1 ? Pj)
where:
Pj =
?n
i=1 Xij
nm
and: Xij is the number of judgments for the ith
evaluation and the jth criteria.
Regarding quality evaluation (n = 6, m = 10,
r = 5), Kappa values are low for both human in-
terpreters (? = 0.07) and the automatic system
(? = 0.01), meaning that judges agree poorly
(Landis and Koch, 1977). This is explained by
the extreme subjectivity of the evaluation and the
small number of evaluated excerpts. Looking at
each pair of judges and the Kappa coefficients
themselves, there is no real agreement, since most
of the Kappa values are around zero. However,
some judge pairs show fair agreement, and some
others show moderate or substantial agreement. It
is observed, though, that some criteria are not fre-
quently selected by the judges, which limits the
statistical significance of the Kappa coefficient.
The limitations are not the same for the com-
prehension evaluation (n = 60, m = 10, r = 2),
since the criteria are binary (i.e. true or false). Re-
garding the evaluated excerpts, Kappa values are
0.28 for the automatic system and 0.30 for the in-
terpreter. According to Landis and Koch (1977),
those values mean that judges agree fairly. In
order to go further, the Kappa coefficients were
computed for each pair of judges. Results were
slightly better for the interpreter than for the au-
tomatic system. Most of them were between 0.20
and 0.40, implying a fair agreement. Some judges
agreed moderately.
Furthermore, it was also observed that for the
120 available questions, 20 had been answered
correctly by all the judges (16 for the interpreter
evaluation and 4 for the automatic system one)
and 6 had been answered wrongly by all judges (1
for the former and 5 for the latter). That shows a
trend where the interpreter comprehension would
be easier than that of the automatic system, or at
least where the judgements are less questionable.
6.2 Quality Evaluation
Table 4 compares the quality evaluation results of
the interpreter to those of the automatic system.
Samples Interpreter Automatic system
L043-1 3.1 1.6
L043-2 2.9 2.3
L043-3 2.4 2.1
T036-1 3.6 3.1
T036-2 2.7 2.5
T036-3 3.5 2.5
Mean 3.03 2.35
Table 4: Quality evaluation results for the inter-
preter and the automatic system [1<5].
As can be seen, with a mean score of 3.03 even
for the interpreter, the excerpts were difficult to
interpret and translate. This is particularly so for
349
L043, which is more technical than T036. The
L043-3 excerpt is particularly technical, with for-
mulae and algorithm descriptions, and even a com-
plex description of the human articulatory system.
In fact, L043 provides a typical presentation with
an introduction, followed by a deeper description
of the topic. This increasing complexity is re-
flected on the quality scores of the three excerpts,
going from 3.1 to 2.4.
T036 is more fluent due to the less technical na-
ture of the speech and the more general vocabu-
lary used. However, the T036-2 and T036-3 ex-
cerpts get a lower quality score, due to the descrip-
tion of data collections or institutions, and thus the
use of named entities. The interpreter does not
seem to be at ease with them and is mispronounc-
ing some of them, such as ?Grenoble? pronounced
like in English instead of in Spanish. The inter-
preter seems to be influenced by the speaker, as
can also be seen in his use of the neologism ?el ce-
nario? (?the scenario?) instead of ?el escenario?.
Likewise, ?Karlsruhe? is pronounced three times
differently, showing some inconsistency of the in-
terpreter.
The general trend in quality errors is similar to
those of previous evaluations: lengthening words
(?seeee?ales?), hesitations, pauses between syl-
lables and catching breath (?caracter?s...ticas?),
careless mistakes (?probibilidad? instead of ?prob-
abilidad?), self-correction of wrong interpreting
(?reconocien-/reconocimiento?), etc.
An important issue concerns gender and num-
ber agreement. Those errors are explained by
the presence of morphological gender in Spanish,
like in ?estos se?ales? instead of ?estas se?ales?
(?these signals?) together with the speaker?s speed
of speech. The speaker seems to start by default
with a masculine determiner (which has no gen-
der in English), adjusting the gender afterward de-
pending on the noun following. A quick transla-
tion may also be the cause for this kind of errors,
like ?del se?al acustico? (?of the acoustic signal?)
with a masculine determiner, a feminine substan-
tive and ending in a masculine adjective. Some
translation errors are also present, for instance
?computerizar? instead of ?calcular? (?compute?).
The errors made by the interpreter help to un-
derstand how difficult oral translation is. This
should be taken into account for the evaluation of
the automatic system.
The automatic system results, like those of
the interpreter, are higher for T036 than for L043.
However, scores are lower, especially for the
L043-1 excerpt. This seems to be due to the
type of lexicon used by the speaker for this ex-
cerpt, more medical, since the speaker describes
the articulatory system. Moreover, his description
is sometimes metaphorical and uses a rather col-
loquial register. Therefore, while the interpreter
finds it easier to deal with these excerpts (known
vocabulary among others) and L043-3 seems to be
more complicated (domain-specific, technical as-
pect), the automatic system finds it more compli-
cated with the former and less with the latter. In
other words, the interpreter has to ?understand?
what is said in L043-3, contrary to the automatic
system, in order to translate.
Scores are higher for the T036 excerpts. In-
deed, there is a high lexical repetition, a large
number of named entities, and the quality of the
excerpt is very training-dependant. However, the
system runs into trouble to process foreign names,
which are very often not understandable. Differ-
ences between T036-1 and the other T036 excerpts
are mainly due to the change in topic. While the
former deals with a general vocabulary (i.e. de-
scription of projects), the other two excerpts de-
scribe the data collection, the evaluation metrics,
etc., thus increasing the complexity of translation.
Generally speaking, quality scores of the au-
tomatic system are mainly due to the transla-
tion component, and to a lesser extent to the
recognition component. Many English words are
not translated (?bush?, ?keyboards?, ?squeaking?,
etc.), and word ordering is not always correct.
This is the case for the sentence ?how we solve
it?, translated into ?c?mo nos resolvers lo? instead
of ?c?mo lo resolvemos?. Funnily enough, the
problems of gender (?maravillosos aplicaciones?
- masc. vs fem.) and number (?pueden real-
mente ser aplicado? - plu. vs sing.) the in-
terpreter has, are also found for the automatic
system. Moreover, the translation of compound
nouns often shows wrong word ordering, in partic-
ular when they are long, i.e. up to three words (e.g.
?reconocimiento de habla sistemas? for ?speech
recognition system? instead of ?sistemas de re-
conocimiento de habla?).
Finally, some error combinations result in fully
non-understandable sentences, such as:
?usted tramo se en emacs es squeaking
ruido y dries todos demencial?
350
where the following errors take place:
? tramo: this translation of ?stretch? results
from the choice of a substantive instead of a
verb, giving rise to two choices due to the lex-
ical ambiguity: ?estiramiento? and ?tramo?,
which is more a linear distance than a stretch
in that context;
? se: the pronoun ?it? becomes the reflexive
?se? instead of the personal pronoun ?lo?;
? emacs: the recognition module transcribed
the couple of words ?it makes? into ?emacs?,
not translated by the translation module;
? squeaking: the word is not translated by the
translation module;
? dries: again, two successive errors are made:
the word ?drives? is transcribed into ?dries?
by the recognition module, which is then left
untranslated.
The TTS component also contributes to decreas-
ing the output quality. The prosody module finds it
hard to make the sentences sound natural. Pauses
between words are not very frequent, but they do
not sound natural (i.e. like catching breath) and
they are not placed at specific points, as it would
be done by a human. For instance, the prosody
module does not link the noun and its determiner
(e.g. ?otros aplicaciones?). Finally, a not user-
friendly aspect of the TTS component is the rep-
etition of the same words always pronounced in
the same manner, what is quite disturbing for the
listener.
6.3 Comprehension Evaluation
Tables 5 and 6 present the results of the compre-
hension evaluation, for the interpreter and for the
automatic system, respectively. They provide the
following information:
identifiers of the excerpt: Source data are the
same for the interpreter and the automatic
system, namely the English speech;
subj. E2E: The subjective results of the end-to-
end evaluation are done by the same assessors
who did the quality evaluation. This shows
the percentage of good answers;
fair E2E: The objective verification of the an-
swers. The audio files are validated to check
whether they contain the answers to the ques-
tions or not (as the questions were created
from the English source). This shows the
maximum percentage of answers an evalua-
tor managed to find from either the interpreter
(speaker audio) or the automatic system out-
put (TTS) in Spanish. For instance, informa-
tion in English could have been missed by
the interpreter because he/she felt that this in-
formation was meaningless and could be dis-
carded. We consider those results as an ob-
jective evaluation.
SLT, ASR: Verification of the answers in each
component of the end-to-end process. In or-
der to determine where the information for
the automatic system is lost, files from each
component (recognised files for ASR, trans-
lated files for SLT, and synthesised files for
TTS in the ?fair E2E? column) are checked.
Excerpts subj. E2E fair E2E
L043-1 69 90
L043-2 75 80
L043-3 72 60
T036-1 80 100
T036-2 73 80
T036-3 76 100
Mean 74 85
Table 5: Comprehension evaluation results for the
interpreter [%].
Regarding Table 5, the interpreter loses 15%
of the information (i.e. 15% of the answers were
incorrect or not present in the interpreter?s trans-
lation) and judges correctly answered 74% of the
questions. Five documents get above 80% of cor-
rect results, while judges find almost above 70%
of the answers for the six documents.
Regarding the automatic system results (Table
6), the information rate found by judges is just
above 50% since, by extension, more than half the
questions were correctly answered. The lowest
excerpt, L043-1, gets a rate of 25%, the highest,
T036-1, a rate of 76%, which is in agreement with
the observation for the quality evaluation. Infor-
mation loss can be found in each component, es-
pecially for the SLT module (35% of the informa-
tion is lost here). It should be noticed that the TTS
module made also errors which prevented judges
351
Excerpts subj. E2E fair E2E SLT ASR
L043-1 25 30 30 70
L043-2 62 70 80 70
L043-3 43 40 60 100
T036-1 76 80 90 100
T036-2 61 70 60 80
T036-3 47 60 70 80
Mean 52 58 65 83
Table 6: Comprehension evaluation results for the
automatic system [%].
from answering related questions. Moreover, the
ASR module loses 17% of the information. Those
results are certainly due to the specific vocabulary
used in this experiment.
So as to objectively compare the interpreter with
the automatic system, we selected the questions
for which the answers were included in the inter-
preter files (i.e. those in the ?fair E2E? column
of Table 5). The goal was to compare the overall
quality of the speech-to-speech translation to in-
terpreters? quality, without the noise factor of the
information missing. The assumption is that the
interpreter translates the ?important information?
and skips the useless parts of the original speech.
This experiment is to measure the level of this in-
formation that is preserved by the automatic sys-
tem. So a new subset of results was obtained, on
the information kept by the interpreter. The same
study was repeated for the three components and
the results are shown in Tables 7 and 8.
Excerpts subj. E2E fair E2E SLT ASR
L043-1 27 33 33 78
L043-2 65 75 88 75
L043-3 37 67 83 100
T036-1 76 80 90 100
T036-2 69 88 75 100
T036-3 47 60 70 80
Mean 53 60 70 80
Table 7: Evaluation results for the automatic sys-
tem restricted to the questions for which answers
can be found in the interpreter speech [%].
Comparing the automatic system to the inter-
preter, the automatic system keeps 40% of the in-
formation where the interpreter translates the doc-
uments correctly. Those results confirm that ASR
loses a lot of information (20%), while SLT loses
10% further, and so does the TTS. Judges are quite
close to the objective validation and found most of
the answers they could possibly do.
Excerpts subj. E2E
L043-1 66
L043-2 90
L043-3 88
T036-1 80
T036-2 81
T036-3 76
Mean 80
Table 8: Evaluation results for interpreter, re-
stricted to the questions for which answers can be
found in the interpreter speech [%].
Subjective results for the restricted evaluation
are similar to the previous results, on the full data
(80% vs 74% of the information found by the
judges). Performance is good for the interpreter:
98% of the information correctly translated by the
automatic system is also correctly interpreted by
the human. Although we can not compare the
performance of the restricted automatic system to
that of the restricted interpreter (since data sets of
questions are different), it seems that of the inter-
preter is better. However, the loss due to subjective
evaluation seems to be higher for the interpreter
than for the automatic system.
7 Conclusions
Regarding the SLT evaluation, the results achieved
with the simultaneous translation system are still
rather low compared to the results achieved with
offline systems for translating European parlia-
ment speeches in TC-STAR. However, the offline
systems had almost no latency constraints, and
parliament speeches are much easier to recognize
and translate when compared to the more spon-
taneous talks and lectures focused in this paper.
This clearly shows the difficulty of the whole task.
However, the human end-to-end evaluation of the
system in which the system is compared with hu-
man interpretation shows that the current transla-
tion quality allows for understanding of at least
half of the content, and therefore, may be already
quite helpful for people not understanding the lan-
guage of the lecturer at all.
352
References
Rajai Al-Khanji, Said El-Shiyab, and Riyadh Hussein.
2000. On the Use of Compensatory Strategies in Si-
multaneous Interpretation. Meta : Journal des tra-
ducteurs, 45(3):544?557.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. In Educational and Psychological
Measurement, volume 20, pages 37?46.
Christian F?gen and Muntsin Kolss. 2007. The influ-
ence of utterance chunking on machine translation
performance. In Proc. of the European Conference
on Speech Communication and Technology (INTER-
SPEECH), Antwerp, Belgium, August. ISCA.
Christian F?gen, Martin Westphal, Mike Schneider,
Tanja Schultz, and Alex Waibel. 2001. LingWear:
A Mobile Tourist Information System. In Proc. of
the Human Language Technology Conf. (HLT), San
Diego, California, March. NIST.
Christian F?gen, Shajith Ikbal, Florian Kraft, Kenichi
Kumatani, Kornel Laskowski, John W. McDonough,
Mari Ostendorf, Sebastian St?ker, and Matthias
W?lfel. 2006a. The isl rt-06s speech-to-text system.
In Steve Renals, Samy Bengio, and Jonathan Fiskus,
editors, Machine Learning for Multimodal Interac-
tion: Third International Workshop, MLMI 2006,
Bethesda, MD, USA, volume 4299 of Lecture Notes
in Computer Science, pages 407?418. Springer Ver-
lag Berlin/ Heidelberg.
Christian F?gen, Muntsin Kolss, Matthias Paulik, and
Alex Waibel. 2006b. Open Domain Speech Trans-
lation: From Seminars and Speeches to Lectures.
In TC-Star Speech to Speech Translation Workshop,
Barcelona, Spain, June.
Donna Gates, Alon Lavie, Lori Levin, Alex. Waibel,
Marsal Gavalda, Laura Mayfield, and Monika Wosz-
cyna. 1996. End-to-end evaluation in janus: A
speech-to-speech translation system. In Proceed-
ings of the 6th ECAI, Budapest.
Olivier Hamon, Djamel Mostefa, and Khalid Choukri.
2007. End-to-end evaluation of a speech-to-speech
translation system in tc-star. In Proceedings of the
MT Summit XI, Copenhagen, Denmark, September.
Muntsin Kolss, Bing Zhao, Stephan Vogel, Ashish
Venugopal, and Ying Zhang. 2006. The ISL Statis-
tical Machine Translation System for the TC-STAR
Spring 2006 Evaluations. In TC-Star Workshop
on Speech-to-Speech Translation, Barcelona, Spain,
December.
Andrzej Kopczynski, 1994. Bridging the Gap: Empiri-
cal Research in Simultaneous Interpretation, chapter
Quality in Conference Interpreting: Some Pragmatic
Problems, pages 87?100. John Benjamins, Amster-
dam/ Philadelphia.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
In Biometrics, Vol. 33, No. 1 (Mar., 1977), pp. 159-
174.
Barbara Moser-Mercer, Alexander Kunzli, and Ma-
rina Korac. 1998. Prolonged turns in interpreting:
Effects on quality, physiological and psychological
stress (pilot study). Interpreting: International jour-
nal of research and practice in interpreting, 3(1):47?
64.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation, Athens,
Greece.
Rita N?bel. 1997. End-to-end Evaluation in Verb-
mobil I. In Proceedings of the MT Summit VI, San
Diego.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), Research Report, Com-
puter Science IBM Research Division, T.J.Watson
Research Center.
Accipio Consulting Volker Steinbiss. 2006.
Sprachtechnologien f?r Europa. www.tc-star.
org/pubblicazioni/D17_HLT_DE.pdf.
John S. White and Theresa A. O?Connell. 1994.
Evaluation in the arpa machine translation program:
1993 methodology. In HLT ?94: Proceedings of the
workshop on Human Language Technology, pages
135?140, Morristown, NJ, USA. Association for
Computational Linguistics.
Sane M. Yagi. 2000. Studying Style in Simultane-
ous Interpretation. Meta : Journal des traducteurs,
45(3):520?547.
353
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80?84,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The Universita?t Karlsruhe Translation System for the EACL-WMT 2009
Jan Niehues, Teresa Herrmann, Muntsin Kolss and Alex Waibel
Universita?t Karlsruhe (TH)
Karlsruhe, Germany
{jniehues,therrman,kolss,waibel}@ira.uka.de
Abstract
In this paper we describe the statistical
machine translation system of the Univer-
sita?t Karlsruhe developed for the transla-
tion task of the Fourth Workshop on Sta-
tistical Machine Translation. The state-of-
the-art phrase-based SMT system is aug-
mented with alternative word reordering
and alignment mechanisms as well as op-
tional phrase table modifications. We par-
ticipate in the constrained condition of
German-English and English-German as
well as in the constrained condition of
French-English and English-French.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT?09 Shared
Translation Task and the particular language-pair-
dependent variations of the system. We use stan-
dard alignment and training tools and a phrase-
based SMT decoder for creating state-of-the-art
MT systems for our contribution in the transla-
tion directions English-German, German-English,
English-French and French-English.
Depending on the language pair, the baseline
system is augmented with part-of-speech (POS)-
based short-range and long-range word reordering
models, discriminative word alignment (DWA)
and several modifications of the phrase table. Ex-
periments with different system variants were con-
ducted including some of those additional system
components. Significantly better translation re-
sults could be achieved compared to the baseline
results.
An overview of the system will follow in Sec-
tion 2, which describes the baseline architecture,
followed by descriptions of the additional system
components. Translation results for the different
languages and system variants are presented in
Section 5.
2 Baseline System
The core of our system is the STTK decoder (Vo-
gel, 2003), a phrase-based SMT decoder with a
local reordering window of 2 words. The de-
coder generates a translation for the input text
or word lattice by searching translation model
and language model for the hypothesis that max-
imizes phrase translation probabilities and target
language probabilities. The translation model, i.e.
the SMT phrase table is created during the training
phase by a modified version of the Moses Toolkit
(Koehn et al, 2007) applying GIZA++ for word
alignment. Language models are built using the
SRILM Toolkit. The POS-tags for the reorder-
ing models were generated with the TreeTagger
(Schmid, 1994) for all languages.
2.1 Training, Development and Test Data
We submitted translations for the English-
German, German-English, English-French and
French-English tasks. All systems were trained
on the Europarl and News Commentary corpora
using the Moses Toolkit and apply 4-gram lan-
guage models created from the respective mono-
lingual News corpora. All feature weights are au-
tomatically determined and optimized with respect
to BLEU via MERT (Venugopal et al, 2005).
For development and testing we used data pro-
vided by the WMT?09, news-dev2009a and news-
dev2009b, consisting of 1026 sentences each.
3 Word Reordering Model
One part of our system that differs from the base-
line system is the reordering model. To account
for the different word orders in the languages, we
used the POS-based reordering model presented in
Rottmann and Vogel (2007). This model learns
rules from a parallel text to reorder the source side.
The aim is to generate a reordered source side that
can be translated in a more monotone way.
80
In this framework, first, reordering rules are
extracted from an aligned parallel corpus and
POS information is added to the source side.
These rules are of the form VVIMP VMFIN PPER
? PPER VMFIN VVIMP and describe how the
source side has to be reordered to match the tar-
get side. Then the rules are scored according to
their relative frequencies.
In a preprocessing step to the actual decoding
different reorderings of the source sentences are
encoded in a word lattice. Therefore, for all re-
ordering rules that can be applied to a sentence the
resulting reorderings are added to the lattice if the
score is better than a given threshold. The decod-
ing is then performed on the resulting word lattice.
This approach does model the reordering well
if only short-range reorderings occur. But espe-
cially when translating from and to German, there
are also long-range reorderings that require the
verb to be shifted nearly across the whole sen-
tence. During this shift of the verb, the rest of
the sentence remains mainly unchanged. It does
not matter which words are in between, since they
are moved as a whole. Furthermore, rules in-
cluding an explicit sequence of POS-tags spanning
the whole sentence would be too specific. A lot
more rules would be needed to cover long-range
reorderings with each rule being applicable only
very sparsely. Therefore, we model long-range re-
ordering by generalizing over the unaffected se-
quences and introduce rules with gaps. (For more
details see Niehues and Kolss (2009)). These are
learned in a way similar to the other type of re-
ordering rules described above, but contain a gap
representing one or several arbitrary words. It is,
for example, possible to have the following rule
VAFIN * VVPP ? VAFIN VVPP *, which puts
both parts of the German verb next to each other.
4 Translation Model
The translation models of all systems we submit-
ted differ in some parts from the baseline system.
The main changes done will be described in this
section.
4.1 Word Alignment
The baseline method for creating the word align-
ment is to create the GIZA++ alignments in both
directions and then to combine both alignments
using a heuristic, e.g. grow-diag-final-and heuris-
tic, as provided by the Moses Toolkit. In some
of the submitted systems we used a discrimina-
tive word alignment model (DWA) to generate
the alignments as described in Niehues and Vogel
(2008) instead. This model is trained on a small
amount of hand-aligned data and uses the lexical
probability as well as the fertilities generated by
the GIZA++ Toolkit and POS information. We
used all local features, the GIZA and indicator fer-
tility features as well as first order features for 6
directions. The model was trained in three steps,
first using the maximum likelihood optimization
and afterwards it was optimized towards the align-
ment error rate. For more details see Niehues and
Vogel (2008).
4.2 Phrase Table Smoothing
The relative frequencies of the phrase pairs are a
very important feature of the translation model,
but they often overestimate rare phrase pairs.
Therefore, the raw relative frequency estimates
found in the phrase translation tables are smoothed
by applying modified Kneser-Ney discounting as
described in Foster et al (2006).
4.3 Lattice Phrase Extraction
For the test sentences the POS-based reordering
allows us to change the word order in the source
sentence, so that the sentence can be translated
more easily. But this approach does not reorder
the training sentences. This may cause problems
for phrase extraction, especially for long-range re-
orderings. For example, if the English verb is
aligned to both parts of the German verb, this
phrase can not be extracted, since it is not contin-
uous on the German side. In the case of German
as source language, the phrase could be extracted
if we also reorder the training corpus.
Therefore, we build lattices that encode the
different reorderings for every training sentence.
Then we can not only extract phrase pairs from the
monotone source path, but also from the reordered
paths. So it would be possible to extract the ex-
ample mentioned before, if both parts of the verb
were put together by a reordering rule. To limit
the number of extracted phrase pairs, we extract
a source phrase only once per sentence even if it
may be found on different paths. Furthermore, we
do not use the weights in the lattice.
If we use the same rules as for the test sets,
the lattice would be so big that the number of ex-
tracted phrase pairs would be still too high. As
mentioned before, the word reordering is mainly
81
a problem at the phrase extraction stage if one
word is aligned to two words which are far away
from each other in the sentence. Therefore, the
short-range reordering rules do not help much in
this case. So, only the long-range reordering rules
were used to generate the lattice for the training
corpus. This already leads to an increase of the
number of source phrases in the filtered phrase ta-
ble from 724K to 971K. The number of phrase
pairs grows from 5.1M to 6.7M.
4.4 Phrase Table Adaption
For most of the different tasks there was a huge
amount of parallel out-of-domain training data
available, but only a much smaller amount of in-
domain training data. Therefore, we tried to adapt
our system to the in-domain data. We want to
make use of the big out-of-domain data, but do
not want to lose the information encoded in the in-
domain data.
To achieve this, we built an additional phrase
table trained only on the in-domain data. Since
the word alignment does not depend heavily on the
domain we used the same word alignment. Then
we combined both phrase tables in the following
way. A phrase pair with features ? from the first
phrase table is added to the combined one with
features < ?, 1 >, where 1 is a vector of ones with
length equal to the number of features in the other
phrase table. The phrase pairs of the other phrase
table were added with the features < 1, ? >.
5 Results
We submitted system translations for the English-
German, German-English, English-French and
French-English task. Their performance is mea-
sured applying the BLEU metric. All BLEU
scores are computed on the lower-cased transla-
tions.
5.1 English-German
The system translating from English to German
was trained on the data described in Section 2.1.
The first system already uses the POS-based re-
ordering model for short-range reorderings. The
results of the different systems are shown in Ta-
ble 1.
We could improve the translation quality on the
test set by using the smoothed relative frequen-
cies in the phrase table as described before and
by adapting the phrase table. Then we used the
discriminative word alignment to generate a new
word alignment. For the training of the model
we used 500 hand-aligned sentences from the Eu-
roparl corpus. By training a translation model
based on this word alignment we could improve
the translation quality further. At last we added
the model for long-range reorderings, which per-
forms best on the test set.
The improvement achieved by smoothing is sig-
nificant at a level of 5%, the remaining changes are
not significant on their own. In all language pairs,
the problem occurs that some features do not lead
to an improvement on the development set, but on
the test set. One reason for this may be that the
development set is quite small.
Table 1: Translation results for English-German
(BLEU Score)
System Dev Test
Short-range 13.96 14.99
+ Smoothing 14.36 15.38
+ Adaptation 13.96 15.44
+ Discrim. WA 14.45 15.61
+ Long-range reordering 14.58 15.70
5.2 German-English
The German-English system was trained on the
same data as the English-German except that we
perform compound splitting as an additional pre-
processing step. The compound splitting was
done with the frequency-based method described
in Koehn et al (2003). For this language di-
rection, the initial system already uses phrase ta-
ble smoothing, adaptation and discriminative word
alignment, in addition to the techniques of the
English-German baseline system. The results are
shown in Table 2.
For this language pair, we could improve the
translation quality, first, by adding the long-range
reordering model. Further improvements could be
achieved by using lattice phrase extraction as de-
scribed before.
5.3 English-French
For creating the English-French translations, first,
the baseline system as described in Section 2
was used. This baseline was then augmented
with phrase table smoothing, short-range word re-
ordering and phrase table adaptation as described
above. In addition, the adapted phrase table was
82
Table 2: Translation results for German-English
(BLEU Score)
System Dev Test
Initial System 20.52 22.01
+ Long-range reordering 21.04 22.36
+ Lattice phrase extraction 20.69 22.64
postprocessed such that phrase table entries in-
clude the same amount of punctuation marks, es-
pecially quotation marks, in both source and tar-
get phrase. In contrast to the English?German
language pairs, the word reordering required
in English?French translations are restricted to
rather local word shifts which can be covered by
the short-range reordering feature. Applying addi-
tional long-range reordering is scarcely expected
to yield further improvements for these language
pairs and was not applied specifically in this task.
Table 3 shows the results of the system variants.
Table 3: Translation results for English-French
(BLEU Score)
System Dev Test
Baseline 20.97 20.87
+ Smoothing 21.42 21.32
+ Short-range reordering 20.79 22.26
+ Adaptation 21.05 21.97
+ cleanPT 21.50 21.98
Both on development and test set, smoothing
the probabilities in the phrase table resulted in an
increase of nearly 0.5 BLEU points. Applying
short-range word reordering did not lead to an im-
provement on the development set. However, the
increase in BLEU on the test set is substantial. The
opposite is the case when adapting the phrase ta-
ble: While phrase table adaptation improves the
translation quality on the development set, adapta-
tion leads to lower scores on the test set.
Thus, the system configuration that performed
best on the test set applies phrase table smoothing
and short-range word reordering. For creating the
translations for our submission, this configuration
was used.
5.4 French-English
For the French-English task, similar experiments
have been conducted. With respect to the base-
line system, improvements in translation quality
could be measured when applying phrase table
smoothing. An increase of 0.43 BLEU points was
achieved using short-range word reordering. Ad-
ditional experiments with adapting the phrase ta-
ble to the domain of the test set led to further im-
provement. Submissions for the shared task were
created using the system including all mentioned
features.
Table 4: Translation results for French-English
(BLEU Score)
System Dev Test
Baseline 21.29 22.41
+ Smoothing 21.55 22.59
+ Short-range reordering 22.55 23.02
+ Adaptation 21.72 23.20
+ cleanPT 22.60 23.21
6 Conclusions
We have presented our system for the WMT?09
Shared Translation Task. The submissions for the
language pairs English-German, German-English,
English-French and French-English have been
created by the STTK decoder applying different
additional methods for each individual language
pair to enhance translation quality.
Word reordering models covering short-
range reordering for the English?French and
English?German and long-range reordering for
English?German respectively proved to result in
better translations.
Smoothing the phrase probabilities in the phrase
table also increased the scores in all cases, while
adapting the phrase table to the test domain only
showed a positive influence on translation quality
in some of our experiments. Further tuning of the
adaptation procedure could help to clarify the ben-
efit of this method.
Using discriminative word alignment as an
alternative to performing word alignment with
GIZA++ did also improve the systems translating
between English and German. Future experiments
will be conducted applying discriminative word
alignment also in the English?French systems.
Acknowledgments
This work was partly supported by Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
83
References
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proc. of Empirical Methods in
Natural Language Processing. Sydney, Australia.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
HLT/NAACL 2003. Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of Second ACL Workshop on Statistical Ma-
chine Translation. Prague, Czech Republic.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation. Columbus, OH, USA.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proc. of Forth ACL Workshop on Statistical Machine
Translation. Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI. Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing. Manchester, UK.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimiza-
tion Rules for Statistical Machine Translation. In
Proc. of ACL 2005, Workshop on Data-drive Ma-
chine Translation and Beyond (WPT-05). Ann Ar-
bor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In NLP-KE?03. Beijing, China.
84
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 206?214,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A POS-Based Model for Long-Range Reorderings in SMT
Jan Niehues and Muntsin Kolss
Universita?t Karlsruhe
Karlsruhe, Germany
{jniehues,kolss}@ira.uka.de
Abstract
In this paper we describe a new approach
to model long-range word reorderings in
statistical machine translation (SMT). Un-
til now, most SMT approaches are only
able to model local reorderings. But even
the word order of related languages like
German and English can be very different.
In recent years approaches that reorder the
source sentence in a preprocessing step
to better match target sentences according
to POS(Part-of-Speech)-based rules have
been applied successfully. We enhance
this approach to model long-range reorder-
ings by introducing discontinuous rules.
We tested this new approach on a German-
English translation task and could signifi-
cantly improve the translation quality, by
up to 0.8 BLEU points, compared to a sys-
tem which already uses continuous POS-
based rules to model short-range reorder-
ings.
1 Introduction
Statistical machine translation (SMT) is currently
the most promising approach to machine transla-
tion of large vocabulary tasks. The approach was
first presented by Brown et al (1993) and has since
been used in many translation systems (Wang and
Waibel, 1998), (Och and Ney, 2000), (Yamada
and Knight, 2000), (Vogel et al, 2003). State-
of-the-art SMT systems often use translation mod-
els based on phrases to describe translation corre-
spondences and word reordering between two lan-
guages. The reordering of words is one of the main
difficulties in machine translation.
Phrase-based translation models by themselves
have only limited capability to model different
word orders in the source and target language, by
capturing local reorderings within phrase pairs. In
addition, the decoder can reorder phrases, subject
to constraints such as confining reorderings to a
relatively small window. In combination with a
distance-based distortion model, some short-range
reorderings can be handled. But for many lan-
guage pairs this is not sufficient, and several au-
thors have proposed additional reordering mod-
els as described in Section 2. In this work we
present a new method that explicitly handles long-
range word reorderings by applying discontinu-
ous, POS-based reordering rules.
The paper is structured as follows: In the next
section we present related work that was carried
out in this area. Afterwards, we describe the prob-
lem of long-range reordering. In Section 4 the
existing framework for reordering will be intro-
duced. Section 5 describes the extraction of rules
modeling long-range reorderings, and in the fol-
lowing section the integration into the framework
will be explained. Finally, the model will be eval-
uated in Section 7, and a conclusion is given in
Section 8.
2 Related Work
Several approaches have been proposed to ad-
dress the problem of word reordering in SMT. Wu
(1996) and Berger et al (1996), for example, re-
strict the possible reorderings either during decod-
ing time or during the alignment, but do not use
any additional linguistic knowledge. A compari-
son of both methods can be found in Zens and Ney
(2003).
Furthermore, techniques to use additional lin-
guistic knowledge to improve the word order have
been developed. Shen et al (2004) and Och et al
(2004) presented approaches to re-rank the output
of the decoder using syntactic information. Fur-
thermore, lexical block-oriented reordering mod-
els have been developed in Tillmann and Zhang
(2005) and Koehn et al (2005). These models de-
cide during decoding time for a given phrase, if
206
the next phrase should be aligned to the left or to
the right.
In recent years several approaches using re-
ordering rules on the source side have been applied
successfully in different systems. These rules can
be used in rescoring as in Chen et al (2006) or can
be used in a preprocessing step. The aim of this
step is to monotonize the source and target sen-
tence. In Collins et al (2005) and Popovic? and
Ney (2006) hand-made rules were used to reorder
the source side depending on information from a
syntax tree or based on POS information. These
rules had to be created manually, but only a few
rules were needed and they were able to model
long-range reorderings. Consequently, for every
language pair these rules have to be created anew.
In contrast, other authors propose data-driven
methods. In Costa-jussa` and Fonollosa (2006)
the source sentence is first translated into an aux-
iliary sentence, whose word order is similar to
the one of the target sentences. Thereby statisti-
cal word classes were used. Rottmann and Vogel
(2007),Zhang et al (2007) and Crego and Habash
(2008) used rules to reorder the source side and
store different possible reorderings in a word lat-
tice. They use POS tags and in the latter two cases
also chunk tags to generalize the rules. The dif-
ferent reorderings are assigned weights depending
on their relative frequencies (Rottmann and Vo-
gel, 2007) or depending on a source side language
model (Zhang et al, 2007).
In the presented work we will use discontinuous
rules in addition to the rules used in Rottmann and
Vogel (2007). This enables us to model long-range
reorderings although we only need POS informa-
tion and no chunk tags.
3 Long-Range Reorderings
One of the main problems when translating from
German to English is the different word order
in both languages. Although both languages are
closely related, the word order is very different
in some cases. Especially when translating the
verb long-range reorderings have to be performed,
since the position of the German verb is differ-
ent from the one in the English sentence in many
cases.
The finite verbs in the English language are al-
ways located at the second position, in the main
clauses as well as in subordinate clauses. In Ger-
man this is only true for the main clause. In con-
trast to that, in German subordinate clauses the
verb (glauben) is at the final position as shown in
Example 1.
Example 1: ..., die an den Markt und an die
Gleichbehandlung aller glauben.
... who believe in markets and equal treatment
for all.
Example 2: Das wird mit derart unter-
schiedlichen Mitgliedern unmo?glich sein .
That will be impossible with such disparate
members.
A second difference in both languages is the po-
sition of the infinitive verb (sein/be) as shown in
Example 2. In contrast to the English language,
where it directly follows the finite verb, it is at the
final position of the sentence in the German lan-
guage.
The two examples show that in order to be able
to handle the reorderings between German and
English, the model has to allow some words to
be shifted across the whole sentence. If this is
not handled correctly, phrase-based systems some-
times generate translations that omit words, as will
be shown in Section 7. This is especially problem-
atic in the German-English case because the verb
may be omitted, which carries the most important
information of the sentence.
4 POS-Based Reordering
We will first briefly introduce the framework pre-
sented in Rottmann and Vogel (2007) since we ex-
tended it to also use discontinuous rules.
In this framework, the first step is to extract re-
ordering rules. Therefore, an aligned parallel cor-
pus and the POS tags of the source side are needed.
For every sequence of source words where the tar-
get words are in a different order, a rule is ex-
tracted that describes how the source side has to be
reordered to match the target side. A rule may for
example look like this: VVIMP VMFIN PPER ?
PPER VMFIN VVIMP. The framework can handle
rules that only depend on POS tags as well as rules
that depend on POS tags and words. We will refer
to these rules as short-range reordering rules.
The next step is to calculate the relative frequen-
cies which are used as a score in the word lattice.
The relative frequencies are calculated as the num-
ber of times the source side is reordered this way
divided by the number of times the source side oc-
curred in the corpus.
In a preprocessing step to the actual decoding,
207
different reorderings of the source sentences are
encoded in a word lattice. For all reordering rules
that can be applied to the sentence, the resulting
edge is added to the lattice if the score is better
than a given threshold. If a reordering is generated
by different rules, only the path of the reordering
with the highest score is added to the lattice. Then,
decoding is performed on the resulting word lat-
tice.
5 Rule Extraction
To be able to handle long-range reorderings, we
extract discontinuous reordering rules in addition
to the continuous ones. The extracted rules should
look, for example, like this: VAFIN * VVPP ?
VAFIN VVPP *, where the placeholder ?*? repre-
sents one or more arbitrary POS tags.
Compared to the continuous, short-range re-
ordering rules described in the previous section,
extracting such discontinuous rules presents an ad-
ditional difficulty. Not only do we need to find
reorderings and extract the corresponding rules,
but we also have to decide which parts of the rule
should be replaced by the placeholder. Since it
is not always clear what is the best part to be re-
placed, we extract four different types of discon-
tinuous rules. Then we decide during decoding
which type of rules to use.
In a first step the reordering rule has to be found.
Since this is done in a different way than for the
continuous one, we will first describe it in detail.
Like the continuous rules, the discontinuous ones
are extracted from a word aligned corpus, whose
source side is annotated with POS tags. Then the
source side is scanned for reorderings. This is
done by comparing the alignment points ai and
ai+1 of two consecutive words. We found a re-
ordering if the target words aligned to fi and fi+1
are in a different order than the source words. In
our case the target word eai+1 has to precede the
target word eai . More formally said, we check the
following condition:
ai > ai+1 (1)
In Figure 1 an example with an automatically
generated alignment is given. There, for example,
a reordering can be found at the position of the
word ?Kenntnis?.
Since we only check the links of consecutive
words, we may miss some reorderings where there
is an unaligned word between the words with a
Figure 1: Example training sentence used to ex-
tract reordering rules
crossing link. However, in this case it is not clear
where to place the unaligned word, so we do not
extract rules from such a reordering.
So now we have found a reordering and also
the border between the left and right part of the
reordering. To be able to extract a rule for this
reordering we need to find the beginning of the
left and the end of the right part. This is done
by searching for the last word before and the first
word after the reordering. In the given example,
the left part is ?ihre Bereitschaft zur Kenntnis? and
the right part would be ?genommen?. As shown in
the figure, the words of the first part have to be
aligned to target words that follow the target word
aligned to the first word of the right part. Oth-
erwise, they would not be part of the reordering.
Consequently, to find the first word that is not part
of the reordering, we search for the first word be-
fore the word fi+1 that is aligned to the word eai+1
or to a target word before this word. More for-
mally, we search for the word fj that satisfies the
following condition:
j = argmaxl<i al ? ai+1 (2)
The first word after the reordering is found in the
same way. Formally, we search for the word fk
satisfying the condition:
k = argmaxl>i+1 al ? ai (3)
In our example, we now can extract the fol-
lowing reordering rule: ihre Bereitschaft zur
Kenntnis genommen ? genommen ihre Bere-
itschaft zur Kenntnis. In general, we will
extract the rule: fj+1 . . . fifi+1 . . . fk?1 ?
fi+1 . . . fk?1fj+1 . . . fi
208
An additional problem are unaligned words af-
ter fj and before fk. For these words it is not clear
if they are part of the reordering or not. There-
fore, we will include or exclude them depending
on the type of rule we extract. To be able to write
the rules in a easier way let fj? be the first word
following fj that is aligned and fk? the last word
before fk.
After extracting the reordering rule, we need to
replace some parts of the rule by a placeholder to
obtain more general rules. As described before, it
is not directly clear which part of the rule should
be replaced and therefore, we extract four different
types of rules.
In the reordering, there is always a left part, in
our example ihre Bereitschaft zur Kenntnis, and
a right part (genommen). So we can either re-
place the left or the right part of the reordering by
a placeholder. One could argue that always the
longer sequence should be replaced, since that is
more intuitive, but to lose no information we just
extract both types of rules. Later we will see that
depending on the language pair, one or the other
type will generalize better. In the evaluation part
the different types will be referred to as Left and
Right rules.
Furthermore, not the whole part has to be re-
placed. It can be argued that the first or last word
of the part is important to characterize the reorder-
ing and should therefore not be replaced. For each
of the types described before, we extract two dif-
ferent sub-types of rules, which leads altogether to
four different types of rules.
Let us first have a look at the types where we
replace the left part. If we replace the whole part,
in the example we would get the following rule: *
VVPP ? VVPP *. This would lead to problems
during rule application. Since the rule begins with
a placeholder, it is not clear where the matching
should start. Therefore, we also include the last
word before the reordering into the rule and can
now extract the following rule from the sentence:
VAFIN * VVPP ? VAFIN VVPP *. In general, we
extract the following rule to which we will refer as
Left All:
fj ? fi+1 . . . fk? ? fjfi+1 . . . fk??
As mentioned in the beginning, we extracted a
second sub-type of rule. This time, the first word
of the left part is not replaced. The reason can be
seen by looking at the reordered sequence. There,
the second part of the reordering is moved between
the last word before the reordering (fj) and the
first word of the first part (fj+1). In our example
this results in the following rule: VAFIN PPOSAT
* VVPP ? VAFIN VVPP PPOSAT * and in gen-
eral, we extract the rule (Left Part):
fjfj+1 ? fi+1 . . . fk? ? fjfi+1 . . . fk?fj+1 ?
If we replace the right part by a star, we sim-
ilarly get the following rule (Right All): PPOSAT
NN APPART NN * ? * PPOSAT NN APPART NN.
The other rule (Right Part) can not be extracted
from this example, since the right part has length
one. But in general we get the two rules:
fj? . . . fi ? fk?1fk ? ?fk?1fj+1 . . . fifk
fj? . . . fi ? fk ? ?fj? . . . fifk
Here we already see that the rules where the
first part is replaced result in typical reordering be-
tween the German and English language. The sec-
ond part of the verb is at the end of the sentence
in German, but in an English sentence it directly
follows the first part.
6 Rule Application
During the training of the system all reordering
rules are extracted from the parallel corpus in the
way described in the last section. The rules are
only used if they occur more often than a given
threshold value. In the experiments a threshold of
5 is used.
The rules are scored in the same way as the con-
tinuous rules were. The relative frequencies are
calculated as the number of times the rule was ex-
tracted divided by the number of times both parts
occur in one sentence.
Then, in the preprocessing step, continuous
rules as described in Section 4 and discontinuous
rules are applied to the source sentence. As in the
framework presented before, the rules are applied
only to the source sentence and not to the lattice.
Thus the rules cannot be applied recursively. For
the discontinuous rules the ?*? could match any
sequence of POS tags, but it has to consist of at
least one tag. If more than one rule can be ap-
plied to a sequence of POS tags and they generate
different output, all edges are added to the lattice.
If they generate the same sequence, only the rule
with the highest probability is applied.
209
In initial experiments we observed that some
rules can be applied very often to a sentence and
therefore the lattice gets quite big. Therefore, we
first check how often a rule can be applied to a
sentence. If this exceeds a given threshold, we do
not use this rule for this sentence. In these cases,
the rule will most likely not find a good reorder-
ing, but randomly shuffle the words. In the experi-
ments we use 5 as threshold, since this reduces the
lattices to a decent size.
These restrictions limit the number of reorder-
ings that have to be tested during decoding. But
if all reorderings that can be generated by the re-
maining rules would be inserted into the lattice,
the size of the lattice would still be too big to
be able to do efficient decoding. Therefore, only
rules with a probability greater than a given thresh-
old are used to reorder the source sentence. Since
the probabilities of the long-range reorderings are
quite small compared to those of the short-range
reorderings, we used two different thresholds.
7 Evaluation
We performed the experiments on the translation
task of the WMT?08 evaluation. Most of the ex-
periments were done on the German-English task,
but in the end also some results on German-French
and English-German are shown. The systems
were trained on the European Parliament Proceed-
ings (EPPS) and the News Commentary corpus.
For the German-French task we used the inter-
section of the parallel corpora from the German-
English and English-French task. The data was
preprocessed and we applied compound splitting
to the German corpus for the tasks translating from
German. Afterwards, the word alignment was
generated with the GIZA++-Toolkit and the align-
ments of the two directions were combined us-
ing the grow-diag-final-and heuristic. Then the
phrase tables were created where we performed
additional smoothing of the relative frequencies
(Foster et al, 2006). Furthermore, the phrase ta-
ble applied in the news task was adapted to this
domain. In addition, a 4-gram language model
was trained on both corpora. The rules were ex-
tracted using the POS tags generated by the Tree-
Tagger (Schmid, 1994). In the end a beam-search
decoder as described in Vogel (2003) was used
to optimize the weights using the MER-training
on the development sets provided for the different
task by the workshop. The systems were tested
Table 1: Evaluation of different Lattice sizes
generated by changing the short-range threshold
?short and long-range threshold ?long
?short ?long #Edges Dev Test
0.2 1 112K 24.57 27.25
0.1 1 203K 24.71 27.48
0.2 0.2 113K 24.70 27.51
0.2 0.1 121K 24.97 27.56
0.2 0.05 152K 25.28 27.80
0.1 0.1 212K 24.97 27.49
0.1 0.05 243K 25.12 27.81
on the test2007 set for the EPPS task and on the
nc-test2007 testset for the news task. For test set
translations the statistical significance of the re-
sults was tested using the bootstrap technique as
described in Zhang and Vogel (2004).
7.1 Lattice Creation
In a first group of experiments we analyzed the in-
fluence of the two thresholds that determine the
minimal probability of a rule that is used to insert
the reordering into the lattice. The experiments
were performed on the news task and used only the
long-range rules generated by the Part All rules.
The results are shown in Table 1 where ?short
is the threshold for the short-range reorderings
and ?long for the long-range reorderings. Con-
sequently, only paths were added that are gener-
ated by a short-range reordering rule that has a
probability greater than ?short or paths generated
by a long-range reordering rule with a minimum
probability of ?long. We used different thresholds
for both groups of rules since the probabilities of
long-range reorderings are in general lower.
The first two systems use no long-range reorder-
ings. Adding the long-range reorderings does im-
prove the translation quality and it makes sense to
add even all edges generated by rules with a prob-
ability of at least 0.05. Using this system, less
short-range reorderings are needed. The system
using the thresholds of 0.2 and 0.05 has a perfor-
mance nearly as good as the one using the thresh-
olds 0.1 and 0.05, but it needs fewer edges. If
long-range reordering is applied, fewer edges are
needed than in the case of using only short-range
reordering even though the translation quality is
better. Therefore, we used the thresholds 0.2 and
0.05 in the following experiments.
210
Figure 2: Most common long-range reordering rules of type Left Part
NN ADV * VAFIN ? NN VAFIN ADV *
VAFIN ART * VVPP ? VAFIN VVPP ART *
? ADV * PPER ? ? PPER ADV *
$, ART * VVINF PTKZU ? $, VVINF PTKZU ART *
PRELS ART * VVFIN ? PRELS VVFIN ART *
Figure 3: Most common long-range reordering rules of type Left All
PRELS * VAFIN ? PRELS VAFIN *
PRELS * VAFIN VVPP ? PRELS VAFIN VVPP *
PPER * VMFIN ? PPER VMFIN *
PRELS * VMFIN ? PRELS VMFIN *
VMFIN * VAINF ? VMFIN VAINF *
Table 2: Number of long-range reordering rules of
different types used to create the lattices
Type Left Right
Part 8079 1127
All 2470 509
Both 9223 1405
7.2 Rule Usage
We analyzed which long-range reordering rules
were used to build the lattices. First, we compared
the usage of the different types of rules. There-
fore, we counted the number of rules that were ap-
plied to the development set of 2000 sentences if
the thresholds 0.2 and 0.05 were used. The result-
ing numbers are shown in Table 2.
As it can be seen, the Left rules are more of-
ten used than the Right ones. This is what we
expected, since when translating from German to
English, the most important rules move the verb
to the left. And these rules should be more gen-
eral and therefore have a higher probability than
the rules that move the words preceding the verb
to the end of the sentence.
Next we analyzed which rules of the Left Part
ones are used most frequently. The five most fre-
quent rules are shown in Figure 2. The first, fourth
and fifth rule moves the verb more to the front,
as is often needed in English subordinate clauses.
The second one moves both parts of the verb to-
gether.
The third most frequent rule moves personal pro-
nouns to the front. In the English language the
Table 3: Translation results for the German-
English task using different rule types (BLEU)
Type EPPS NEWS
Dev Test Dev Test
Left Part 26.99 29.16 25.12 27.88
Right Part 26.69 28.73 24.76 27.28
Right/Left Part 26.99 28.96 25.06 27.69
Left All 26.77 28.76 24.37 26.56
Left Part/All 26.99 29.32 25.38 27.86
All 27.02 29.14 25.20 27.63
subject has to be always at the front. In contrast,
in German the word order is not that strict and the
subject can appear later.
We have done the same for the Left All rules.
The rules are shown in Figure 3. In this type of
rule the five most frequent rules all try to move the
verb more to the front of the sentence. In the last
case both parts of the verb are put together.
7.3 Rule Types
In a next group of experiments we evaluated the
performance of the different rule types. In Table 3
the translation performance of systems using dif-
ferent rule types is shown. The experiments were
carried out on the EPPS task as well as on the
NEWS task.
First it can be seen that the Left rules perform
better than the Right rules. This is not surpris-
ing, since they better describe how to reorder from
German to English and because they are more of-
ten used in the lattice. If both types are used this
211
Table 4: Summary of translation results for the
German-English tasks (BLEU)
System EPPS NEWS
Dev Test Dev Test
Baseline 25.47 27.24 23.40 25.90
Short 26.77 28.54 24.73 27.48
Long 26.99 29.32 25.38 27.86
lowers the performance a little. So if it is clear
which type explains the reordering better, only this
type should be used, but if that is not possible us-
ing both types can still help.
If both types of rules are compared, it can be
seen that Part rules seem to have a more positive
influence than All ones. The reason for this may be
that the Part rules can also be applied more often
than the rules of the other type. Using the com-
bination of both types of rules, the performance is
better on one task and equally good on the other
task. Consequently, we used the combination of
both types in the remaining experiments.
7.4 German-English
The results on the German-English task are sum-
marized in Table 4. The long-range reorderings
could improve the performance by 0.8 and 0.4
BLEU points on the different tasks compared to
a system applying only short-range reorderings.
These improvements are significant at a level of
5%.
We also analyzed the influence of tagging er-
rors. Therefore, we tagged every word of the test
sentence with the tag that this word is mostly as-
signed to in the training corpus. If the word does
not occur in the training corpus, it was tagged as a
noun. This results in different tags for 5% of the
words and a BLEU score of 27.68 on the NEWS
test set using long-range reorderings. So the trans-
lation quality drops by about 0.2 BLEU points, but
it is still better than the system using only short-
range reorderings.
In Figure 4 example translations of the baseline
system, the system modeling only short-range re-
orderings and the system using also long-range re-
orderings rules are shown. The part of the sen-
tences that needs long-range reorderings is always
underlined.
In the first two examples the verbal phrase con-
sists of two parts and the German one is splitted.
In these cases, it was impossible for the short-
Table 5: Translation results for the German-
French translation task (BLEU)
System EPPS NEWS
Dev Test Dev Test
Baseline 25.86 27.05 17.90 18.52
Short 27.02 28.06 18.59 19.99
Long 27.27 28.61 19.10 20.11
range reordering model to move the second part of
the verb to the front so that it could be translated
correctly. In one case this leads to a selection of a
phrase pair that removes the verb from the transla-
tion. Thus it is hard to understand the meaning of
the sentence.
In the other two examples the verb of the subor-
dinate clause has to be moved from the last posi-
tion in the German sentence to the second position
in the English one. This is again only possible us-
ing the long-range reordering rules. Furthermore,
if these rules are not used, it is possible that the
verb will be not translated at all as in the last ex-
ample.
7.5 German-French
We also performed similar experiments on the
German-French task. Since the type of reordering
needed for this language pair is similar to the one
used in the German-English task, we used also the
Left rules in the long-range reorderings. As it can
be seen in Table 5, the long-range reordering rules
could also help to improve the translation perfor-
mance for this language pair. The improvement on
the EPPS task is significant at a level of 5%.
7.6 English-German
In a last group of experiments we applied the same
approach also to the English-German translation
task. In this case the verb has to be moved to
the right, so that we used the Right rules for the
long-range reorderings. Looking at the rule us-
age of the different type of rules, the picture was
quite promising. This time the Right rules could
be applied more often and the Left ones only a few
times. But if we look at the results as shown in Ta-
ble 6, the long-range reorderings do not improve
the performance. We will investigate the reasons
for this in future work.
212
Figure 4: Example translation from German to English using different type of rules
Source: Diese Ma?nahmen werden als eine Art Wiedergutmachung fu?r fru?her begangenes
Unrecht angesehen .
Baseline: these measures will as a kind of compensation for once injustice done .
Short: these measures will as a kind of compensation for once injustice done .
Long: these measures will be seen as a kind of compensation for once injustice done .
Source: Das wird mit derart unterschiedlichen Mitgliedern unmo?glich sein .
Baseline: this will with such different impossible .
Short: this will with such different impossible .
Long: this will be impossible to such different members .
Source: Er braucht die Unterstu?tzung derer , die an den Markt und an die Gleichbehandlung
aller glauben .
Baseline: he needs the support of those who market and the equal treatment of all believe .
Short: it needs the support of those who in the market and the equal treatment of all believe .
Long: it needs the support of those who believe in the market and the equal treatment of all .
Source: .., da? sie das Einwanderungsproblem als politischen Hebel benutzen .
Baseline: .. that they the immigration problem as a political lever .
Short: .. that the problem of immigration as a political lever .
Long: .. that they use the immigration problem as a political lever .
Table 6: Translation results for the English-
German translation task (BLEU)
System EPPS NEWS
Dev Test Dev Test
Baseline 18.93 2072 16.31 17.91
Short 19.49 21.56 17.13 18.31
Long 19.56 21.33 16.93 18.15
8 Conclusion
We have presented a new method to model long-
range reorderings in statistical machine transla-
tion. This method extends a framework based
on extracting POS-based reordering rules from an
aligned parallel corpus by adding discontinuous
reordering rules. Allowing rules with gaps cap-
tures very long-range reorderings while avoiding
the data sparseness problem of very long continu-
ous reordering rules.
The extracted rules are used to generate a word
lattice with different possible reorderings of the
source sentence in a preprocessing step prior to de-
coding. Placing various restrictions on the appli-
cation of the rules keeps the lattice small enough
for efficient decoding. Compared to a baseline
system that only uses continuous reordering rules,
applying additional discontinuous rules improved
the translation performance on a German-English
translation task significantly by up to 0.8 BLEU
points.
In contrast to approaches like Collins et al
(2005) and Popovic? and Ney (2006), the rules are
created in a data-driven way and not manually. It
was therefore easily possible to transfer this ap-
proach to the German-French translation task, and
we showed that we could improve the translation
quality for this language pair as well. Further-
more, this approach needs only the POS informa-
tion and no syntax tree. Thus, if we use the ap-
proximation for the tags as described before, the
approach could also easily be integrated into a
real-time translation system.
An unsolved problem is still why this ap-
proach does not improve the results of the English-
German translation task. An explanation might be
that here the reordering problem is even more dif-
ficult, since the German word order is very free.
Acknowledgments
This work was partly supported by Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
213
proach to Natural Language Processing. Compua-
tional Linguistics, 22(1):39?71.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Boxing Chen, Mauro Cettolo, and Marcello Federico.
2006. Reordering Rules for Phrase-based Statisti-
cal Machine Translation. In International Workshop
on Spoken Language Translation (IWSLT 2006), Ky-
oto, Japan.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause Restructuring for Statistical Machine
Translation. In Proc. of the 43rd Annual Meeting on
Association for Computational Linguistics (ACL),
pages 531?540.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP 2006), Sydney, Australia.
Nizar Crego and Nizar Habash. 2008. Using Shal-
low Syntax Information to Improve Word Align-
ment and Reordering for SMT. In 46th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-08:
HLT), Columbus, Ohio, USA.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP
2006), Sydney, Australia.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
IWSLT, Pittsburgh, PA, USA.
Franz Josef Och and Herman Ney. 2000. Improved
Statistical Alignment Models. In 38th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2000), Hong Kong.
Franz J. Och, Daniel Gildea, Sanjeev P. Khudan-
pur, Anoop Sarkar, Kenji Yamada, Alexander
Fraser, Shankar Kumar, Libin Shen, David A.
Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir R. Radev. 2004. A Smorgasboard of Fea-
tures for Statistical Machine Translation. In Human
Language Technology Conference and the 5th Meet-
ing of the North American Association for Com-
putational Linguistics (HLT-NAACL 2004), Boston,
USA.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation (LREC 2006), Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Libin Shen, Anoop Sarkar, and Franz Och. 2004.
Discriminative Reranking for Machine Translation.
In Human Language Technology Conference and
the 5th Meeting of the North American Association
for Computational Linguistics (HLT-NAACL 2004),
Boston, USA.
Christoph Tillmann and Tong Zhang. 2005. A Local-
ized Prediction Model for Statistical Machine Trans-
lation. In 43rd Annual Meeting of the Association
for Computational Linguistics (ACL 2005), Ann Ar-
bor, Michigan, USA.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venogupal, Bing Zhao, and Alex Waibel.
2003. The CMU Statistical Translation System. In
MT Summit IX, New Orleans, LA, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Yeyi Wang and Alex Waibel. 1998. Fast Decoding
for Statistical Machine Translation. In ICSLP?98,
Sydney, Australia.
Dekai Wu. 1996. A Polynomial-time Algorithm for
Statistical Machine Translation. In ACL-96: 34th
Annual Meeting of the Assoc. for Computational
Linguistics, Santa Cruz, CA, USA, June.
Kenji Yamada and Kevin Knight. 2000. A Syntax-
based Statistical Translation Model. In 38th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2000), Hong Kong.
Richard Zens and Hermann Ney. 2003. A Compar-
ative Study on Reordering Constraints in Statistical
Machine Translation. In 41st Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 192?202, Sapporo, Japan.
Ying Zhang and Stephan Vogel. 2004. Measuring
Confidence Intervals for mt Evaluation Metrics. In
TMI 2004, Baltimore, MD, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-Level Reordering of Source Language Sen-
tences with Automatically Learned Rules for Sta-
tistical Machine Translation. In HLT-NAACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, Rochester, NY, USA.
214
