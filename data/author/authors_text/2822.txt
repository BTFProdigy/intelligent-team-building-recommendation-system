                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 4
                                                              Proceedings of HLT-NAACL
Exponential Priors for Maximum Entropy Models
Joshua Goodman
One Microsoft Way
Redmond, WA 98052
joshuago@microsoft.com
Abstract
Maximum entropy models are a common mod-
eling technique, but prone to overfitting. We
show that using an exponential distribution as
a prior leads to bounded absolute discounting
by a constant. We show that this prior is better
motivated by the data than previous techniques
such as a Gaussian prior, and often produces
lower error rates. Exponential priors also lead
to a simpler learning algorithm and to easier to
understand behavior. Furthermore, exponential
priors help explain the success of some previ-
ous smoothing techniques, and suggest simple
variations that work better.
1 Introduction
Conditional Maximum Entropy (maxent) models have
been widely used for a variety of tasks, including lan-
guage modeling (Rosenfeld, 1994), part-of-speech tag-
ging, prepositional phrase attachment, and parsing (Rat-
naparkhi, 1998), word selection for machine translation
(Berger et al, 1996), and finding sentence boundaries
(Reynar and Ratnaparkhi, 1997). They are also some-
times called logistic regression models, maximum likeli-
hood exponential models, log-linear models, and are even
equivalent to a form of perceptrons/single layer neural
networks. In particular, perceptrons that use the standard
sigmoid function, and optimize for log-loss are equivalent
to maxent. Multi-layer neural networks that optimize log-
loss are closely related, and much of the discussion will
apply to them implicitly.
Conditional maxent models have traditionally either
been unregularized or regularized by using a Gaussian
prior on the parameters. We will show that by using an
exponential distribution as the prior, several advantages
can be gained. We will show that in at least one case, an
exponential prior experimentally better matches the ac-
tual distribution of the parameters. We will also show that
it can lead to improved accuracy, and a simpler learning
algorithm. In addition, the exponential prior inspires an
improved version of Good Turing discounting with lower
perplexity.
Conditional maxent models are of the form
P?(y|x) =
exp
?F
i=1 ?ifi(x, y)
?
y? exp
?
i ?ifi(x, y
?)
where x is an input vector, y is an output, the f i are the so-
called indicator functions or feature values that are true
if a particular property of x, y is true, F is the number
of such features, ? represents the parameter set ?1...?n,
and ?i is a weight for the indicator fi. For instance,
if trying to do word sense disambiguation for the word
?bank?, x would be the context around an occurrence of
the word; y would be a particular sense, e.g. financial or
river; fi(x, y) could be 1 if the context includes the word
?money? and y is the financial sense; and ?i would be a
large positive number.
Maxent models have several valuable properties
(Della Pietra et al (1997) give a good overview.) The
most important is constraint satisfaction. For a given f i,
we can count how many times fi was observed in the
training data with value y, observed[i] =
?
j fi(xj , yj).
For a model P? with parameters ?, we can see how many
times the model predicts that fi would be expected to
occur: expected[i] =
?
j,y P?(y|xj)fi(xj , y). Maxent
models have the property that expected[i] = observed[i]
for all i and y. These equalities are called constraints.
The next important property is that the likelihood of the
training data is maximized (thus, the name maximum
likelihood exponential model.) Third, the model is as
similar as possible to the uniform distribution (minimizes
the Kullback-Leibler divergence), given the constraints,
which is why these models are called maximum entropy
models.
This last property ? similarity to the uniform distribu-
tion ? is a form of regularization. However, it turns out to
be an extremely weak one ? it is not uncommon for mod-
els, especially those that use all or most possible features,
to assign near-zero probabilities (or, if ?s may be infi-
nite, even actual zero probabilities), and to exhibit other
symptoms of severe overfitting. There have been a num-
ber of approaches to this problem, which we will discuss
in more detail in Section 3. The most relevant approach,
however, is the work of Chen and Rosenfeld (2000), who
implemented a Gaussian prior for maxent models. They
compared this technique to most of the previously im-
plemented techniques, on a language modeling task, and
concluded that it was consistently the best. We thus use it
as a baseline for our comparisons, and similar considera-
tions motivate our own technique, an exponential prior.
Chen et al place a Gaussian prior with 0 mean and ? 2i
variance on the model parameters (the ? is), and then find
a model that maximizes the posterior probability of the
data and the model. In particular, maxent models without
priors use the parameters ? that maximize
argmax
?
n
?
j=1
P?(yj |xj)
where xj , yj are training data instances. With a Gaussian
prior we find
arg max
?
n
?
j=1
P?(yj|xj) ?
F
?
i=1
1
?
2??2i
exp
(
?
?2i
2?2i
)
In this case, a trained model does not satisfy the con-
straints expected[i] = observed[i], but, as Chen and
Rosenfeld show, instead satisfies constraints
expected[i] = observed[i]? ?i
?2i
(1)
That is, instead of a model that matches the observed
count, we get a model matching the observed count minus
?
i
?2
i
: in language modeling terms, this is ?discounting.?
We do not believe that all models are generated by
the same process, and therefore we do not believe that
a single prior will work best for all problem types.
In particular, as we will describe in our experimen-
tal results section, when looking at one particular set
of parameters, we noticed that it was not at all Gaus-
sian, but much more similar to a 0 mean Laplacian,
f(?i) =
1
2?
i
exp
(
?
|?
i
|
?
i
)
, or to an exponential distri-
bution f(?i) = ?i exp (??i?i), which is non-zero only
for non-negative ?i. In some cases, learned parameter
distributions will not match the prior distribution, but in
some cases they will, so it seemed worth exploring one
of these alternate forms. Later, when we try to optimize
our models, the parameter seach will turn out to be much
simpler with an exponential prior, so we focus on that
distribution.
With an exponential prior, we maximize
arg max
??0
n
?
j=1
P?(yj |xj) ?
F
?
i=1
?i exp (??i?i) (2)
As we will describe, it is significantly simpler to perform
this maximization than the Gaussian maximization. Fur-
thermore, as we will describe, models satisfying Equa-
tion 2 will have the property that, for each ? i, either
a) ?i = 0 and expected[i] ? observed[i] ? ?i or b)
expected[i] = observed[i]??i. In other words, we essen-
tially just discount the observed counts by the constant
?i (which is the reciprocal of the standard deviation),
subject to the constraint that ?i is non-negative. This is
much simpler and more intuitive than the constraints with
the Gaussian prior (Equation 1), since those constraints
change as the values of ?i change. Furthermore, as we
will describe in Section 3, discounting by a constant is a
common technique for language model smoothing (Ney
et al, 1994; Chen and Goodman, 1999), but one that has
not previously been well justified; the exponential prior
gives some Bayesian justification.
In Section 5 we will show that on two very different
tasks ? grammar checking and a collaborative filtering
task ? the exponential prior yields lower error rates than
the Gaussian.
2 Learning algorithms and discounting
In this section we derive a learning algorithm for expo-
nential priors, with provable convergence properties, and
show that it leads to a simple discounting formula. Note
that the simple learning algorithm is an important con-
tribution: the algorithm for a Gaussian prior is quite a
bit more complicated, and previous related work with the
Laplacian prior (two-sided exponential) has had a diffi-
cult time finding learning algorithms; because the Lapla-
cian does not have a continuous first derivative, and be-
cause the exponential prior is bounded at 0, standard gra-
dient descent type algorithms may exhibit poor behav-
ior. Williams (1995) devotes a full ten pages to describ-
ing a somewhat heuristic approach for solving this prob-
lem, and even this discussion concludes ?In summary it
is left to the reader to supply the algorithms for deter-
mining successive search directions and the initially pre-
ferred value of? the step size (page 130).1 We show that
a very simple variation on a standard algorithm, General-
ized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972),
solves this problem. In particular, as we will show, while
GIS uses an update rule of the form
?i := ?i +
1
f#
log
observed[i]
expected[i]
1Williams? algorithm is for the much more complex case of
a multi-layer network, while ours is for the one layer case, but
there are no obvious simplifications to his approach for the one
layer case.
our modified algorithm uses a rule of the form
?i := max
(
0,
(
?i +
1
f#
log
observed[i]? ?i
expected[i]
))
(3)
Note that there are two different styles of model that
one can use, especially in the common case that there are
two outputs (values for y.) Consider a word sense disam-
biguation problem, trying to determine whether a word
like ?bank? means the river or financial sense, with ques-
tions like whether or not the word ?water? occurs nearby.
One could have a single indicator function f1(x, y) =
1 if water occurs nearby and values in the range ?? <
?1 < ?. We call this style ?double sided? indicators.
Alternatively, one could have two indicator functions,
f1(x, y) = 1 if water occurs nearby and y=river and
f2(x, y) = 1 if water occurs nearby and y=financial. In
this case, one could allow either ?? < ?1, ?2 < ? or
0 ? ?1, ?2 < ? . We call the style with two indicators
?single sided.? With a Gaussian prior, it does not mat-
ter which style one uses ? one can show that by chang-
ing ?2, exactly the same results will be achieved. With a
Laplacian (double sided exponential), one could also use
either style. With an exponential prior, only positive val-
ues are allowed, so one must use the double sided style,
so that one can learn that some indicators push towards
one sense, and some push towards the other ? that is,
rather than having one weight which is positive or neg-
ative, we have two weights which are positive or zero,
one of which pushes towards one answer, and the other
pushing towards the other.
We derive our constraints and learning algorithm by
very closely following the derivation of the algorithm by
Chen and Rosenfeld. It will be convenient to maximize
the log of the expression in 2 rather than the expression
itself. This leads to an objective function:
L(?)=
?
j
log P?(yj |xj) ?
F
?
i=1
?i?i + log ?i
=
?
j
F
?
i=1
?ifi(xj , yj) (4)
?
?
j
log
?
y
exp
(
F
?
i=1
?ifi(xj , y)
)
?
F
?
i=1
?i?i+log?i
Note that this objective function is convex (since it is the
sum of two convex functions.) Thus, there is a global
maximum value for this objective function.
Now, we wish to find the maximum. Normally, we
would do this by setting the derivative to 0, but the bound
of ?k ? 0 changes things a bit. The maximum can then
occur at the discontinuity in the derivative (?k = 0) or
when ?k > 0. We can explicitly check the value of the
objective function at the point ?k = 0. When there is a
maximum with ?k > 0 we know that the partial deriva-
tive with respect to ?k will be 0.
?
??
k
?
j
?F
i=1 ?ifi(xj , yj)
?
?
j log
?
y exp
(
?F
i=1 ?ifi(xj , y)
)
?
?F
i=1 ?i?i + const(?)
=
?
j fk(xj , yj)
?
?
j
?
y
f
k
(x
j
,y) exp(
?
F
i=1
?
i
f
i
(x
j
,y))
?
y
exp
(
?
F
i=1
?
i
f
i
(x
j
,y)
)
? ?k
=
?
j fk(xj , yj)?
?
j
?
y fk(xj , y)P?(y|xj)??k
This implies that at the optimum, when ?k > 0,
?
j
fk(xj , yj) ?
?
j
?
y
fk(xj , y)P?(y|xj)? ?k = 0
observed[k]? expected[k] ? ?k = 0
observed[k] ? ?k = expected[k] (5)
In other words, we discount the observed count by ? k ?
the absolute discounting equation. Sometimes it is better
for ?k to be set to 0 ? another possible optimal point is
when ?k = 0 and observed[k] ? ?k < expected[k]. One
of these two cases must hold at the optimum.
Notice an important property of exponential priors
(analogous to a similiar property for Laplace priors
(Williams, 1995; Tibshirani, 1994)): they often favor pa-
rameters that are exactly 0. This leads to a kind of natu-
ral pruning for exponential priors, not found in Gaussian
priors, which are only very rarely 0. (Note, however, that
one should not increase the ??s in the exponential prior
to control pruning, as this may lead to oversmoothing. If
additional pruning is needed for speed or memory sav-
ings, feature selection techniques should be used, such
as pruning small or infrequent parameters, instead of a
strengthened prior.)
Now, we can derive the update equations. The deriva-
tion is exactly the same as Chen and Rosenfeld?s (2000)
with the minor change of an exponential prior instead of
a Gaussian prior (we include it in the appendix.) Let
f#(x, y) =
?
i fi(x, y). Then, in the end, we get an
update equation of the form
?k := max
(
0, ?k +
1
f#
log
observed[k]? ?k
expected[k]
)
Compare this equation to the corresponding equation
with a Gaussian prior (Chen and Rosenfeld., 2000). With
a Gaussian prior, one can derive an equation of the form
observed[k]? ?k
?2k
= expected[k] exp
(
f#?k
)
and then solve for ?k. There is no closed form solution:
it must be solved using numerical methods, such as New-
ton?s method, making this update much more complex
and time consuming than the exponential prior.
One can also derive variations on this update. For in-
stance, in the Appendix, we derive an update for Im-
proved Iterative Scaling (Della Pietra et al, 1997) with
an exponential prior. Perhaps more importantly, one can
also derive updates for Sequential Conditional General-
ized Iterative Scaling (SCGIS) (Goodman, 2002), which
is several times faster than GIS. The SCGIS update for
binary features with an exponential prior is simply
?k := max
(
0, ?k + log
observed[k]? ?k
expected[k]
)
One might wonder why we simply don?t use conju-
gate gradient (CG) methods, which have been shown to
converge quickly for maxent. There has not been a for-
mal comparison of SCGIS to conjugate gradient methods.
In our own pilot studies, SCGIS is sometimes faster and
sometimes slower. Also, some versions of CG use heuris-
tics for the step size, and lose convergence guarantees.
Finally, SCGIS is simpler to implement than conjugate
gradient, and even for those with a conjugate gradient li-
brary, because of the parameter constraints (for an expo-
nential prior) or discontinuous derivatives (for a Lapla-
cian) standard conjugate gradient techniques need to be
at least somewhat modified.
Good-Turing discounting has been used or suggested
for language modeling several times (Rosenfeld, 1994,
page 38), (Lau, 1994). In particular, it has been suggested
to use an update of the form
?k := ?k +
1
f#
log
observed[k]?
expected[k]
where observed[k]? is the Good-Turing discounted value
of observed[k]. This update has a problem, as noted by its
proponents: the constraints are probably now inconsistent
? there is no model that can simultaneously satisfy them.
We note that a simple variation on this update, inspired
by the exponential prior, does not have these problems:
?k := max 0,
(
?k +
1
f#
log
observed[k]?
expected[k]
)
In particular, this can be thought of as picking an
?observed[k] for each observed[k]. This does not con-
stitute a Bayesian prior, since the value is picked after the
counts are observed, but it does lead to a convex objective
function with a global maximum, and the update function
will converge towards this maximum. Variations on the
constraints of Equation 5 will apply for this modified ob-
jective function. Furthermore, in the experimental results
section, we will see that on a language modeling task,
this modified update function outperforms the traditional
update. By using a well motivated approach inspired by
exponential priors, we can find a simple variation that has
better performance both theoretically and empirically.
3 Previous Work
There has been a fair amount of previous work on regu-
larization of maxent models. Early approaches focused
on feature selection (Della Pietra et al, 1997) or, simi-
larly, count cutoffs (Rosenfeld, 1994). By not using all
features, there is typically extra probability left-over for
unobserved events, which is distributed in a maximum
entropy fashion. The problem with this approach is that
it ignores useful information ? although low count or low
discrimination features may cause overfitting, they do
contain valuable information. Because of this, more re-
cent approaches (Rosenfeld, 1994, page 38), (Lau, 1994)
have tried techniques such as Good-Turing discounts
(Good, 1953).
There are a number of other approaches (Khudanpur,
1995; Newman, 1997) based on the fuzzy maxent frame-
work (Della Pietra and Della Pietra, 1993). Chen and
Rosenfeld (Chen and Rosenfeld., 2000) give a more com-
plete discussion of those approaches.
Chen and Rosenfeld (2000), following a suggestion of
Lafferty, implemented a Gaussian prior for maxent mod-
els. They compared this technique to most of the previ-
ously discussed techniques, on a language modeling task,
and concluded that it was consistently the best technique.
Tibshirani (1994) introduced Laplacian priors for lin-
ear models (linear regressions) and showed that an ob-
jective function that minimizes the absolute values of the
parameters corresponds to a Laplacian prior. He called
this the Least Absolute Shrinkage and Selection Operator
(LASSO) and showed that it leads to a type of feature se-
lection. Exponential priors are sometimes called single-
sided Laplacians, so obviously, the two techniques are
very closely related, so closely that we would not want to
claim that either was better than the other.
Williams (1995) introduced a Laplacian prior for neu-
ral networks. Single layer neural networks with cer-
tain loss functions are equivalent to logistic regres-
sion/maximum entropy models. Williams? algorithm was
for a more general case, and much more complex than the
one we describe.
More recently, Figueiredo et al (2003) in unpublished
independent work also examined Laplacian priors for lo-
gistic regression, deriving a somewhat more complex al-
gorithm than ours, but one that they extended to partially
supervised learning. He has shown that the results are
comparable to transductive SVMs.
Perkins and Theiler (2003) used logisitic regression
with a Laplacian prior as well. Their learning algorithm
was conjugate gradient descent. Normally, conjugate gra-
dient methods are only used on data that has a continuous
first derivative, so the code was modified to prune weights
that go exactly to zero.
Our main contribution then is not the use of Laplacian
priors with logistic regression, nor even the first good
learning algorithm for the model type. Our contributions
are performing an explicit comparison to a Gaussian prior
and showing improved performance on real data; noticing
that the fixed point of the models leads to absolute dis-
counting; showing that iterative-scaling style algorithms
including GIS, IIS, and SCGIS can be trivially modified
to use this prior; and explicitly showing that in at least one
case, parameters are actually consistent with this prior.
4 Kneser-Ney smoothing
In this section, we help explain the excellent performance
of Kneser-Ney smoothing, the best performing language
model smoothing technique. This justification not only
answers an important question ? why do discounting
methods work well ? but also gives guidance for extend-
ing Kneser-Ney smoothing to problems with fractional
counts, where solutions were not previously known.
Chen and Goodman (1999) performed an extensive
comparison of different smoothing (regularization) tech-
niques for language modeling. They found that a ver-
sion of Kneser-Ney smoothing (Kneser and Ney, 1995)
consistently was the best performing technique. Unfortu-
nately, while there are partial theoretical justifications for
Kneser-Ney smoothing, in terms of preserving marginals,
one important part has previously had no justification:
Kneser-Ney smoothing discounts counts, while most con-
ventional regularization techniques, justified by Dirichlet
priors, add to counts. Given that discounting was pre-
viously unjustified, it is exciting that we have found a
way to explain it. In fact, we can show that the Back-
off version of Kneser-Ney smoothing is an excellent ap-
proximation to a maximum entropy model with an ex-
ponential prior. In particular, Kneser-Ney smoothing is
derived by assuming absolute discounting, and then find-
ing the distribution that preserves marginals, i.e. mak-
ing expected = observed - discount. The differences be-
tween Backoff Kneser-Ney smoothing and maxent mod-
els with exponential priors are twofold. First, the backoff
version does not exactly preserve marginals ? an approx-
imation is made. Second, Backoff Kneser-Ney always
performs discounting, even when this effectively results
in lowering the probability, e.g. the equivalent of a neg-
ative value for ?. There is also an interpolated version
of Kneser-Ney smoothing. The interpolated version of
Kneser Ney smoothing works even better. It exactly pre-
serves marginals (except for discounting.) Also, the sec-
ondary distribution is combined with the primary distri-
bution; this has several effects, including that it is un-
likely to get the equivalent of a large negative ? value.
5 Experimental Results
In this section, we detail our experimental results, show-
ing that exponential priors outperform Gaussian priors on
two different data sets, and inspire improvements for a
third. For all experiments, except the language model ex-
periments, we used a single variance for both the Gaus-
sian and the exponential prior, rather than one per param-
eter, with the variance optimized on held out data. For the
language modeling experiments, we used three variances,
one each for the unigram, bigram, and trigram models,
again optimized on held out data.
We were inspired to use an exponential prior by an
actual examination of a data set. In particular, we used
the grammar-checking data of Banko and Brill (2001).
We chose this set because there are commonly used ver-
sions both with small amounts of data (which is when
we expect the prior to matter) and with large amounts of
data (which is required to easily see what the distribu-
tion over ?correct? parameter values is.) For one exper-
iment, we trained a model using a Gaussian prior, using
a large amount of data. We then found those parameters
(??s) that had at least 35 training instances ? enough to
typically overcome the prior and train the parameter re-
liably. We then graphed the distribution of these param-
eters. While it is common to look at the distribution of
data, the NLP and machine learning communities rarely
examine distributions of model parameters, and yet this
seems like a good way to get inspiration for priors to try,
using those parameters with enough data to help guess
the priors for those with less, or at least to determine the
correct form for the prior, if not the exact values. 2 The
results are shown in Figure 1, which is a histogram of ??s
with a given value. If the distribution were Gaussian, we
would expect this to look like an upside-down parabola.
If the distribution were Laplacian, we would expect it to
appear as a triangle (the bottom formed from the X-axis.)
Indeed, it does appear to be roughly triangular, and to the
extent that it diverges from this shape, it is convex, while
a Gaussian would be concave. We don?t believe that the
exponential prior is right for every problem ? our argu-
ment here is that based on both better accuracy (our next
experiment) and a better fit to at least some of the param-
eters, that the exponential prior is better for some models.
We then tried actually using exponential priors with
this application, and were able to demonstrate improve-
2Of course, those parameters with lots of data might be gen-
erated from a different prior than those with little data. This
technique is meant as a form of inspiration and evidence, but
not of proof. Similarly, all parameters may be generated by
some other process, e.g. a mixture of Gaussians. Finally, a prior
might be accurate but still behave poorly, because it might in-
teract poorly with other approximations. For instance, it might
interact poorly with the fact that we use argmax rather than the
true Bayesian posterior over models.
 1
 10
 100
 1000
 10000
-3 -2 -1  0  1  2  3
Nu
m
be
r o
f P
ar
am
et
er
s 
in
 B
uc
ke
t
Value of Lambda
Figure 1: Histogram of ? values
ments in error rate. We used a small data set, 100,000
sentences of training data and ten different confusable
word pairs. (Most training sentences did not contain ex-
amples of the confusable word pairs of interest, so the
actual number of training examples for each word-pair
was less than 100,000). We tried different priors for
the Gaussian and exponential prior, and found the best
single prior on average across all ten pairs. With this
best setting, we achieved a 14.51% geometric average
error rate with the exponential prior, and 15.45% with
the Gaussian. To avoid any form of cheating, we then
tried 10 different word pairs (the same as those used by
Banko and Brill (2001)) with this best parameter setting.
The results were 18.07% and 19.47% for the exponen-
tial and Gaussian priors respectively. (The overall higher
rate is due to the test set words being slightly more dif-
ficult.) We also tried experiments with 1 million and 10
million words, but there were not consistent differences;
improved smoothing mostly matters with small amounts
of training data.
We also tried experiments with a collaborative-filtering
style task, television show recommendation, based on
Nielsen data. The dataset used, and the definition of a col-
laborative filtering (CF) score is the same as was used by
Kadie et al (2002), although our random train/test split
is not the same, so the results are not strictly comparable.
We first ran experiments with different priors on a held-
out section of the training data, and then using the single
best value for the prior (the same one across all features),
we ran on the test data. With a Gaussian prior, the CF
score was 42.11, while with an exponential prior, it was
45.86, a large improvement.
Finally, we ran experiments with language modeling,
with mixed success. We used 1,000,000 words of train-
ing data (a small model, but one where smoothing mat-
ters) from the WSJ corpus and a trigram model with a
cluster-based speedup (Goodman, 2001). We evaluated
on test data using the standard language modeling mea-
sure, perplexity, where lower scores are better. We tried
five experiments: using Katz smoothing (a widely used
version of Good-Turing smoothing) (perplexity 238.0);
using Good Turing discounting to smooth maxent (per-
plexity 224.8); using our variation on Good-Turing, in-
spired by exponential priors, where ??s are bounded at 0
(perplexity 204.5); using an exponential prior (perplex-
ity 190.8); using a Gaussian prior (perplexity 183.7); and
using interpolated modified Kneser-Ney smoothing (per-
plexity 180.2). On the one hand, an exponential prior is
worse than a Gaussian prior in this case, and modified in-
terpolated Kneser-Ney smoothing is still the best known
smoothing technique (Chen and Goodman, 1999), within
noise of a Gaussian prior. On the other hand, searching
for parameters is extremely time consuming, and Good-
Turing is one of the few parameter-free smoothing meth-
ods. Of the three Good-Turing smoothing methods, the
one inspired by exponentials priors was the best.
Note that perplexity is 2entropy and in general, we
have found that exponential priors work slightly worse
on entropy measures than the Gaussian prior, even when
they are better on accuracy. This may be due to the fact
that an exponential prior ?throws away? some informa-
tion, whenever the ? would be negative. (In a pilot exper-
iment with a variation that does not throw away informa-
tion, the entropies are closer to the Gaussian.)
6 Conclusion
We have shown that an exponential prior for maxent mod-
els leads to a simple update formula that is easy to im-
plement, and to models that are easy to understand: ob-
servations are discounted, subject to the constraint that
? ? 0. We have also shown that in at least one case,
this prior better matches the underlying model, and that
for two applications, it leads to improved accuracy. The
prior also inspired an improved version of Good-Turing
smoothing with lower perplexity. Finally, an exponential
prior explains why models that discount by a constant
can be Bayesian, giving an alternative to Dirichlet pri-
ors which add a constant. This helps justify Kneser-Ney
smoothing, the best performing smoothing technique in
language modeling. In the future, we would like to use
our technique of examining the distribution of model pa-
rameters to see if other problems exhibit other priors be-
sides Gaussian and Laplacian/exponential, and if perfor-
mance on those problems can be improved through this
observation.
Acknowledgments
Thanks to John Platt, who suggested looking at Lapla-
cian priors, and to Chris Meek for helpful discussions,
and Jeff Bilmes for reading an earlier version of this pa-
per. Finally, thanks to Stan Chen and Roni Rosenfeld:
our derivation for exponential priors closely follows the
text of their derivation for Gaussian priors.
References
M. Banko and E. Brill. 2001. Mitigating the paucity of
data problem. In HLT.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech and Language, 13:359?394,
October.
Stanley Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for ME models. IEEE Trans. on
Speech and Audio Processing, 8(2):37?50, January.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathe-
matical Statistics, 43:1470?1480.
Stephen Della Pietra and Vincent Della Pietra. 1993. Sta-
tistical modeling by maximum entropy. Unpublished
Manuscript.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(4):380?393, April.
Mario A. T. Figueiredo, Balaji Krishnapuram, Lawrence
Carin, and Alexander J. Hartemink. 2003. Supervised
and semi-supervised sparse Bayesian classification.
I.J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3 and 4):237?264.
Joshua Goodman. 2001. Classes for fast maximum en-
tropy training. In ICASSP 2001.
Joshua Goodman. 2002. Sequential conditional general-
ized iterative scaling. In ACL ?02.
Carl M. Kadie, Christopher Meek, and David Hecker-
man. 2002. CFW: A collaborative filtering system
using posteriors over weights of evidence. In Proceed-
ings of UAI, pages 242?250.
S. Khudanpur. 1995. A method of maximum entropy es-
timation with relaxed constraints. In 1995 Johns Hop-
kins University Language Modeling Workshop.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling. In
ICASSP, volume 1, pages 181?184.
Raymond Lau. 1994. Adaptive statistical language mod-
elling. Master?s thesis, MIT.
W. Newman. 1997. Extension to the maximum en-
tropy method. IEEE Trans. on Information Theory,
IT-23(1):89?93, January.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochastic
language modeling. Computer, Speech, and Language,
8:1?38.
Simon Perkins and James Theiler. 2003. Online feature
selection using grafting. August.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
J. Reynar and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
ANLP.
Ronald Rosenfeld. 1994. Adaptive Statistical Language
Modeling: A Maximum Entropy Approach. Ph.D. the-
sis, Carnegie Mellon University, April.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Technical report.
Peter M. Williams. 1995. Bayesian regularization and
pruning using a Laplace prior. Neural Computation,
7:117?143.
A Derivation of Update Equation
In each iteration, we try to find ? = {?i} that maximizes
the increase in the objective function (subject to the con-
straint that ?i + ?i ? 0). From Equation 4,
L(? + ?) ? L(?) =
?
j
?
i
?ifi(xj , yj)
?
?
j
log
?
y
P?(y|xj) exp
(
?
i
?ifi(xj , y)
)
?
?
i
?i?i
As with the Gaussian prior, it is not clear how to maxi-
mize this function directly, so instead we use an auxiliary
function, B(?), with three important properties: first, we
can maximize it; second, it bounds this function from be-
low; third, it is larger than zero whenever ? is not at a lo-
cal optimum, i.e. does not satisfy the constraints in Equa-
tion 5. Using the well-known inequality log x ? x ? 1,
which implies ?logx ? 1 ? x, we get
LX(? + ?) ? LX(?) ?
?j
?
i
?ifi(xj , yj)
+
?
j
1?
?
y
P?(y|xj) exp
(
?
i
?ifi(xj , y)
)
?
?
i
?i?i (6)
Let f#(x, y) =
?
i fi(x, y). Modify Equation 6 to:
LX(? + ?) ? LX(?) ?
?
j
?
i
?ifi(xj , yj)+
?
j
1 ?
?
y
P?(y|xj) exp
(
f#(xj , y)
?
i
?i
fi(xj , y)
f#(xj , y)
)
?
?
i
?i?i (7)
Now, recall Jensen?s inequality, which states that for a
convex function g,
?
y
p(x)g(x) ? g
(
?
x
p(x)x
)
Notice that fi(x,y)
f#(x,y)
is a probability distribution. Thus, we
get
LX(? + ?) ? LX(?) ?
?
j
?
i
?ifi(xj , yj)+
?
j
1?
?
y
P?(y|xj)
?
i
fi(xj , y)
f#(xj , y)
exp
(
f#(xj , y)?i
)
?
?
i
?i?i (8)
Now, we would like to find ? that maximizes Equation
8. Thus, we take partial derivatives and set them to zero,
remembering to also check whether a maximum occurs
when ?k = 0.
?
??k
?
?
?
j
?
i
?ifi(xj , yj) +
?
j
1
?
?
y
P?(y|xj)
?
i
fi(xj , y)
f#(xj , y)
exp
(
f#(xj , y)?i
)
?
?
i
?i?i
)
=
?
j
fk(xj , yj)
+
?
j
?
?
y
P?(y|xj)
fk(xj , y)
f#(xj , y)
?
??k
exp
(
f#(xj , y)?k
)
??k
=
?
j
fk(xj , yj)
?
?
j
?
y
P?(y|xj)fk(xj , y) exp
(
f#(xj , y)?k
)
? ?k
= 0
This gives us a version of Improved Iterative Scaling with
an exponential Prior. In general, however, we prefer vari-
ations of Generalized Iterative Scaling, which may not
converge as quickly, but lead to simpler algorithms. In
particular, we set f# = maxx,y f#(x, y). Then, instead
of Equation 7, we get
LX(? + ?)? LX(?)
?
?
j
?
i
?ifi(xj , yj) +
?
j
1 ?
?
y
P?(y|xj) exp
(
f#
?
i
?i
fi(xj , y)
f#
)
?
?
i
?i?i (9)
We can follow essentially the same derivation from there.
(Technically, we need to add in a slack parameter; the
slack parameter can then be given a near-zero variance
prior so that its value stays at 0, and thus in practice it can
be ignored.) We thus get:
?
??k
?
?
?
j
?
i
?ifi(xj , yj)+
?
j
1?
?
y
P?(y|xj)
?
i
fi(xj , y)
f#
exp
(
f#?i ?
?
i
?i?i
)
?
?
=
?
j
fk(xj , yj)
?
?
j
?
y
P?(y|xj)fk(xj , y) exp
(
f#?k
)
? ?k
= observed[k]? expected[k] exp
(
f#?k
)
? ?k
= 0 (10)
From Equation 10 we get
?k =
1
f#
log
observed[k]? ?k
expected[k]
Now, ?k + ?k may be less than 0; in this case, an illegal
new value for ?k would result. We know, however, from
the monotonicity of all the equations with respect to ?k
that the lowest legal value of ?k will be the best, and we
thus arrive at
?k = max
(
??k,
1
f#
log
observed[k] ? ?k
expected[k]
)
or equivalently
?k := max
(
0, ?k +
1
f#
log
observed[k]? ?k
expected[k]
)
Sequential Conditional Generalized Iterative Scaling
Joshua Goodman
Microsoft Research
One Microsoft Way
Redmond, WA 98052
joshuago@microsoft.com
Abstract
We describe a speedup for training conditional maxi-
mum entropy models. The algorithm is a simple vari-
ation on Generalized Iterative Scaling, but converges
roughly an order of magnitude faster, depending on
the number of constraints, and the way speed is mea-
sured. Rather than attempting to train all model pa-
rameters simultaneously, the algorithm trains them
sequentially. The algorithm is easy to implement,
typically uses only slightly more memory, and will
lead to improvements for most maximum entropy
problems.
1 Introduction
Conditional Maximum Entropy models have been
used for a variety of natural language tasks, includ-
ing Language Modeling (Rosenfeld, 1994), part-
of-speech tagging, prepositional phrase attachment,
and parsing (Ratnaparkhi, 1998), word selection for
machine translation (Berger et al, 1996), and find-
ing sentence boundaries (Reynar and Ratnaparkhi,
1997). Unfortunately, although maximum entropy
(maxent) models can be applied very generally, the
typical training algorithm for maxent, Generalized
Iterative Scaling (GIS) (Darroch and Ratcliff, 1972),
can be extremely slow. We have personally used up
to a month of computer time to train a single model.
There have been several attempts to speed up max-
ent training (Della Pietra et al, 1997; Wu and Khu-
danpur, 2000; Goodman, 2001). However, as we
describe later, each of these has suffered from appli-
cability to a limited number of applications. Darroch
and Ratcliff (1972) describe GIS for joint probabil-
ities, and mention a fast variation, which appears to
have been missed by the conditional maxent com-
munity. We show that this fast variation can also
be used for conditional probabilities, and that it is
useful for a larger range of problems than traditional
speedup techniques. It achieves good speedups for
all but the simplest models, and speedups of an order
of magnitude or more for typical problems. It has
only one disadvantage: when there are many possi-
ble output values, the memory needed is prohibitive.
By combining this technique with another speedup
technique (Goodman, 2001), this disadvantage can
be eliminated.
Conditional maxent models are of the form
P (y|x) =
exp
?
i
?
i
f
i
(x, y)
?
y
?
exp
?
i
?
i
f
i
(x, y
?
)
(1)
where x is an input vector, y is an output, the f
i
are
the so-called indicator functions or feature values
that are true if a particular property of x, y is true,
and ?
i
is a weight for the indicator f
i
. For instance,
if trying to do word sense disambiguation for the
word ?bank?, x would be the context around an oc-
currence of the word; y would be a particular sense,
e.g. financial or river; f
i
(x, y) could be 1 if the con-
text includes the word ?money? and y is the financial
sense; and ?
i
would be a large positive number.
Maxent models have several valuable proper-
ties. The most important is constraint satisfaction.
For a given f
i
, we can count how many times f
i
was observed in the training data, observed[i] =
?
j
f
i
(x
j
, y
j
). For a model P
?
with parameters
?, we can see how many times the model pre-
dicts that f
i
would be expected: expected[i] =
?
j,y
P
?
(y|x
j
)f
i
(x
j
, y). Maxent models have the
property that expected[i] = observed[i] for all i.
These equalities are called constraints. An addi-
tional property is that, of models in the form of Equa-
tion 1, the maxent model maximizes the probability
of the training data. Yet another property is that max-
ent models are as close as possible to the uniform
distribution, subject to constraint satisfaction.
Maximum entropy models are most commonly
learned using GIS, which is actually a very simple
algorithm. At each iteration, a step is taken in a di-
rection that increases the likelihood of the training
                   Computational Linguistics (ACL), Philadelphia, July 2002, pp. 9-16.
                         Proceedings of the 40th Annual Meeting of the Association for
data. The step size is guaranteed to be not too large
and not too small: the likelihood of the training data
increases at each iteration and eventually converges
to the global optimum. Unfortunately, this guaran-
tee comes at a price: GIS takes a step size inversely
proportional to the maximum number of active con-
straints. Maxent models are interesting precisely be-
cause of their ability to combine many different kinds
of information, so this weakness of GIS means that
maxent models are slow to learn precisely when they
are most useful.
We will describe a variation on GIS that works
much faster. Rather than learning all parameters of
the model simultaneously, we learn them sequen-
tially: one, then the next, etc., and then back to the
beginning. The new algorithm converges to the same
point as the original one. This sequential learning
would not lead to much, if any, improvement, ex-
cept that we also show how to cache subcomputa-
tions. The combination leads to improvements of an
order of magnitude or more.
2 Algorithms
We begin by describing the classic GIS algorithm.
Recall that GIS converges towards a model in
which, for each f
i
, expected[i] = observed[i].
Whenever they are not equal, we can move
them closer. One simple idea is to just add
log observed[i]/expected[i] to ?
i
. The problem
with this is that it ignores the interaction with other
?s. If updates to other ?s made on the same iteration
of GIS have a similar effect, we could easily go too
far, and even make things worse. GIS introduces a
slowing factor, f#, equal to the largest total value of
f
i
: f# = max
j,y
?
i
f
i
(x
j
, y). Next, GIS computes
an update:
?
i
=
log observed[i]/expected[i]
f
#
(2)
We then add ?
i
to?
i
. This update provably converges
to the global optimum. GIS for joint models was
given by Darroch and Ratcliff (1972); the conditional
version is due to Brown et al (Unpublished), as
described by Rosenfeld (1994).
In practice, we use the pseudocode of Figure 1.1
We will write I for the number of training instances,
1Many published versions of the GIS algorithm require in-
clusion of a ?slack? indicator function so that the same number
of constraints always applies. In practice it is only necessary
that the total of the indicator functions be bounded by f#, not
necessarily equal to it. Alternatively, one can see this as includ-
ing the slack indicator, but fixing the corresponding ? to 0, and
expected[0..F ] = 0
for each training instance j
for each output y
s[j, y] := 0
for each i such that f
i
(x
j
, y) = 0
s[j, y] += ?
i
? f
i
(x
j
, y)
z :=
?
y
e
s[j,y]
for each output y
for each i such that f
i
(x
j
, y) = 0
expected[i] += f
i
(x
j
, y)? e
s[j,y]
/z
for each i
?
i
=
1
f
#
log
observed[i]
expected[i]
?
i
+= ?
i
Figure 1: One Iteration of Generalized Iterative Scal-
ing (GIS)
and F for number of indicator functions; we use Y
for the number of output classes (values for y). We
assume that we keep a data structure listing, for each
training instance x
j
and each value y, the i such that
f
i
(x
j
, y) = 0.
Now we can describe our variation on GIS. Basi-
cally, instead of updating all ??s simultaneously, we
will loop over each indicator function, and compute
an update for that indicator function, in turn. In par-
ticular, the first change we make is that we exchange
the outer loops over training instances and indicator
functions. Notice that in order to do this efficiently,
we also need to rearrange our data structures: while
we previously assumed that the training data was
stored as a sparse matrix of indicator functions with
non-zero values for each instance, we now assume
that the data is stored as a sparse matrix of instances
with non-zero values for each indicator. The size of
the two matrices is obviously the same.
The next change we make is to update each ?
i
near the inner loop, immediately after expected[i] is
computed, rather than after expected values for all
features have been computed. If we update the fea-
tures one at a time, then the meaning of f# changes.
In the original version of GIS, f# is the largest total
of all features. However, f# only needs to be the
largest total of all the features being updated, and in
not updating it, so that it can be ommitted from any equations;
the proofs that GIS improves at each iteration and that there is
a global optimum still hold.
z[1..I] = Y
s[1..I, 1..Y ] = 0
for each feature f
i
expected = 0
for each output y
for each instance j such that f
i
(x
j
, y) = 0
expected += f
i
(x
j
, y)? e
s[j,y]
/z[j]
?
i
=
1
max
j,y
f
i
(x
j
,y)
log
observed[i]
expected[i]
?
i
+= ?
i
for each output y
for each instance j such that f
i
(x
j
, y) = 0
z[j]?= e
s[j,y]
s[j, y] += ?
i
z[j] += e
s[j,y]
Figure 2: One Iteration of Sequential Conditional
Generalized Iterative Scaling (SCGIS)
this case, there is only one such feature. Thus, in-
stead of f#, we use max
j,y
f
i
(x
j
, y). In many max-
ent applications, the f
i
take on only the values 0 or
1, and thus, typically, max
j,y
f
i
(x
j
, y) = 1. There-
fore, instead of slowing by a factor of f#, there may
be no slowing at all!
We make one last change in order to get a speedup.
Rather than recompute for each instance j and each
output y, s[j, y] =
?
i
?
i
? f
i
(x
j
, y), and the corre-
sponding normalizing factors z =
?
y
e
s[j,y] we in-
stead keep these arrays computed as invariants, and
incrementally update them whenever a ?
i
changes.
With this important change, we now get a substantial
speedup. The code for this transformed algorithm is
given in Figure 2.
The space of models in the form of Equation 1 is
convex, with a single global optimum. Thus, GIS
and SCGIS are guaranteed to converge towards the
same point. For convergence proofs, see Darroch
and Ratcliff (1972), who prove convergence of the
algorithm for joint models.
2.1 Time and Space
In this section, we analyze the time and space re-
quirements for SCGIS compared to GIS. The space
results depend on Y, the number of output classes.
When Y is small, SCGIS requires only a small
amount more space than GIS. Note that in Section 3,
we describe a technique that, when there are many
output classes, uses clustering to get both a speedup
and to reduce the number of outputs, thus alleviating
the space issues.
Typically for GIS, the training data is stored as
a sparse matrix of size T of all non-zero indicator
functions for each instance j and output y. The trans-
posed matrix used by SCGIS is the same size T .
In order to make the relationship between GIS
and SCGIS clearer, the algorithms in Figures 1 and
2 are given with some wasted space. For instance,
the matrix s[j, y] of sums of ?s only needs to be
a simple array s[y] for GIS, but we wrote it as a
matrix so that it would have the same meaning in
both algorithms. In the space and time analyses, we
will assume that such space-wasting techniques are
optimized out before coding.
Now we can analyze the space and time for GIS.
GIS requires the training matrix, of size T , the ?s, of
size F , as well as the expected and observed arrays,
which are also size F . Thus, GIS requires space
O(T + F ). Since T must be at least as large as F
(we can eliminate any indicator functions that don?t
appear in the training data), this is O(T ).
SCGIS is potentially somewhat larger. SCGIS
also needs to store the training data, albeit in a differ-
ent form, but one that is also of size T . In particular,
the matrix is interchanged so that its outermost index
is over indicator functions, instead of training data.
SCGIS also needs the observed and ? arrays, both
of size F , and the array z[j] of size I , and, more im-
portantly, the full array s[j, y], which is of size IY .
In many problems, Y is small ? often 2 ? and IY is
negligible, but in problems like language modeling,
Y can be very large (60,000 or more). The overall
space for SCGIS, O(T +IY ), is essentially the same
as for GIS when Y is small, but much larger when
Y is large ? but see the optimization described in
Section 3.
Now, consider the time for each algorithm to ex-
ecute one iteration. Assume that for every instance
and output there is at least one non-zero indicator
function, which is true in practice. Notice that for
GIS, the top loops end up iterating over all non-zero
indicator functions, for each output, for each training
instance. In other words, they examine every entry
in the training matrix T once, and thus require time
T . The bottom loops simply require time F , which
is smaller than T . Thus, GIS requires time O(T ).
For SCGIS, the top loops are also over each non-
zero entry in the training data, which takes time
O(T ). The bottom loops also require time O(T ).
Thus, one iteration of SCGIS takes about as long
as one iteration of GIS, and in practice in our im-
plementation, each SCGIS iteration takes about 1.3
times as long as each GIS iteration. The speedup
in SCGIS comes from the step size: the update in
GIS is slowed by f#, while the update in SCGIS is
not. Thus, we expect SCGIS to converge by up to a
factor of f# faster. For many applications, f# can
be large.
The speedup from the larger step size is difficult
to analyze rigorously, and it may not be obvious
whether the speedup we in fact observe is actually
due to the f# improvement or to the caching. Note
that without the caching, each iteration of SCGIS
would be O(f#) times slower than an iteration of
GIS; the caching is certainly a key component. But
with the caching, each iteration of SCGIS is still
marginally slower than GIS (by a small constant fac-
tor). In Section 4, we in fact empirically observe that
fewer iterations are required to achieve a given level
of convergence, and this reduction is very roughly
proportional to f#. Thus, the speedup does appear
to be because of the larger step size. However, the
exact speedup from the step size depends on many
factors, including how correlated features are, and
the order in which they are trained.
Although we are not aware of any problems where
maxent training data does not fit in main memory,
and yet the model can be learned in reasonable time,
it is comforting that SCGIS, like GIS, requires se-
quential, not random, access to the training data. So,
if one wanted to train a model using a large amount
of data on disk or tape, this could still be done with
reasonable efficiency, as long as the s and z arrays,
for which we need random access, fit in main mem-
ory.
All of these analyses have assumed that the train-
ing data is stored as a precomputed sparse matrix of
the non-zero values for f
i
for each training instance
for each output. In some applications, such as lan-
guage modeling, this is not the case; instead, the
f
i
are computed on the fly. However, with a bit of
thought, those data structures also can be rearranged.
Chen and Rosenfeld (1999) describe a technique
for smoothing maximum entropy that is the best cur-
rently known. Maximum entropy models are natu-
rally maximally smooth, in the sense that they are
as close as possible to uniform, subject to satisfy-
ing the constraints. However, in practice, there may
be enough constraints that the models are not nearly
smooth enough ? they overfit the training data. Chen
and Rosenfeld describe a technique whereby a Gaus-
sian prior on the parameters is assumed. The models
no longer satisfy the constraints exactly, but work
much better on test data. In particular, instead of
attempting to maximize the probability of the train-
ing data, they maximize a slightly different objective
function, the probability of the training data times the
prior probability of the model:
argmax
?
J
?
j=1
P
?
(y
j
|x
j
)P (?) (3)
where P (?) =
?
I
i=1
1
?
2??
e
?
?
2
i
2?
2
. In other words,
the probability of the ?s is a simple normal distribu-
tion with 0 mean, and a standard deviation of ?.
Chen and Rosenfeld describe a modified update
rule in which to find the updates, one solves for ?
i
in
observed[i] = expected[i]? e?if# + ?i + ?i
?
2
SCGIS can be modified in a similar way to use an
update rule in which one solves for ?
i
in
observed[i] = expected[i]?e?i maxj,y fi(xj ,y)+?i + ?i
?
2
3 Previous Work
Although sequential updating was described for joint
probabilities in the original paper on GIS by Darroch
and Ratcliff (1972), GIS with sequential updating
for conditional models appears previously unknown.
Note that in the NLP community, almost all max-
ent models have used conditional models (which are
typically far more efficient to learn), and none to our
knowledge has used this speedup.2
There appear to be two main reasons this speedup
has not been used before for conditional models.
One issue is that for joint models, it turns out to be
more natural to compute the sumss[x], while for con-
ditional models, it is more natural to compute the ?s
and not store the sums s. Storing s is essential for our
speedup. Also, one of the first and best known uses
of conditional maxent models is for language mod-
eling (Rosenfeld, 1994), where the number of output
classes is the vocabulary size, typically 5,000-60,000
words. For such applications, the array s[j, y] would
be of a size at least 5000 times the number of train-
ing instances: clearly impractical (but see below for
2Berger et al (1996) use an algorithm that might appear
sequential, but an examination of the definition off# and related
work shows that it is not.
a recently discovered trick). Thus, it is unsurprising
that this speedup was forgotten.
There have been several previous attempts to
speed up maxent modeling. Best known is the work
of Della Pietra et al (1997), the Improved Iterative
Scaling (IIS) algorithm. Instead of treating f# as a
constant, we can treat it as a function of x
j
and y. In
particular, let f#(x, y) =
?
i
f
i
(x, y) Then, solve
numerically for ?
i
in the equation
observed[i] = (4)
?
j,y
P
?
(y|x
j
)? f
i
(x
j
, y)? exp(?
i
f
#
(x
j
, y))
Notice that in the special case where f#(x, y) is
a constant f#, Equation 4 reduces to Equation 2.
However, for training instances where f#(x
j
, y) <
f
#
, the IIS update can take a proportionately larger
step. Thus, IIS can lead to speedups when f#(x
j
, y)
is substantially less than f#. It is, however, hard to
think of applications where this difference is typi-
cally large. We only know of one limited experiment
comparing IIS to GIS (Lafferty, 1995). That experi-
ment showed roughly a factor of 2 speedup. It should
be noted that compared to GIS, IIS is much harder
to implement efficiently. When solving Equation 4,
one uses an algorithm such as Newton?s method that
repeatedly evaluates the function. Either one must
repeatedly cycle through the training data to compute
the right hand side of this equation, or one must use
tricks such as bucketing by the values of f#(x
j
, y).
The first option is inefficient and the second adds
considerably to the complexity of the algorithm.
Note that IIS and SCGIS can be combined by us-
ing an update rule where one solves for
observed[i] = (5)
?
j,y
P
?
(x
j
, y)? f
i
(x
j
, y)? exp(?
i
f
i
(x
j
, y))
For many model types, the f
i
take only the values 1
or 0. In this case, Equation 5 reduces to the normal
SCGIS update.
Brown (1959) describes Iterative Scaling (IS), ap-
plied to joint probabilities, and Jelinek (1997, page
235) shows how to apply IS to conditional probabili-
ties. For binary-valued features, without the caching
trick, SCGIS is the same as the algorithm described
by Jelinek. The advantage of SCGIS over IS is the
caching ? without which there is no speedup ? and
because it is a variation on GIS, it can be applied to
non-binary valued features. Also, with SCGIS, it is
clear how to apply other improvements such as the
smoothing technique of Chen and Rosenfeld (1999).
Several techniques have been developed specif-
ically for speeding up conditional maxent models,
especially when Y is large, such as language mod-
els, and space precludes a full discussion here. These
techniques include unigram caching, cluster expan-
sion (Lafferty et al, 2001; Wu and Khudanpur,
2000), and word clustering (Goodman, 2001). Of
these, the best appears to be word clustering, which
leads to up to a factor of 35 speedup, and which
has an additional advantage: it allows the SCGIS
speedup to be used when there are a large number of
outputs.
The word clustering speedup (which can be ap-
plied to almost any problem with many outputs, not
just words) works as follows. Notice that in both GIS
and in SCGIS, there are key loops over all outputs, y.
Even with certain optimizations that can be applied,
the length of these loops will still be bounded by, and
often be proportional to, the number of outputs. We
therefore change from a model of the form P (y|x)
to modeling P (cluster(y)|x) ? P (y|x, cluster(y)).
Consider a language model in which y is a word, x
represents the words preceding y, and the vocabulary
size is 10,000 words. Then for a model P (y|x), there
are 10,000 outputs. On the other hand, if we create
100 word clusters, each with 100 words per clus-
ter, then for a model P (cluster(y)|x), there are 100
outputs, and for a model P (y|x, cluster(y)) there are
also 100 outputs. Thus, instead of training one model
with a time proportional to 10,000, we train two mod-
els, each with time proportional to 100. Thus, in this
example, there is a 50 times speedup. In practice, the
speedups are not quite so large, but we do achieve
speedups of up to a factor of 35. Although the model
form learned is not exactly the same as the original
model, the perplexity of the form using two models is
actually marginally lower (better) than the perplex-
ity of the form using a single model, so there does
not seem to be any disadvantage to using it.
The word clustering technique can be extended to
use multiple levels. For instance, by putting words
into superclusters, such as their part of speech, and
clusters, such as semantically similar words of a
given part of speech, one could use a three level
model. In fact, the technique can be extended to
up to log
2
Y levels with two outputs per level, mean-
ing that the space requirements are proportional to 2
instead of to the original Y . Since SCGIS works
by increasing the step size, and the cluster-based
speedup works by increasing the speed of the in-
ner loop (whchi SCGIS shares), we expect that the
two techniques would complement each other well,
and that the speedups would be nearly multiplica-
tive. Very preliminary language modeling experi-
ments are consistent with this analysis.
There has been interesting recent unpublished
work by Minka (2001). While this work is very
preliminary, and the experimental setting somewhat
unrealistic (dense features artificially generated), es-
pecially for many natural language tasks, the results
are dramatic enough to be worth noting. In particu-
lar, Minka found that a version of conjugate gradient
descent worked extremely well ? much faster than
GIS. If the problem domain resembles Minka?s, then
conjugate gradient descent and related techniques
are well worth trying, and it would be interesting to
try these techniques for more realistic tasks.
SCGIS turns out to be related to boosting. As
shown by Collins et al (2002), boosting is in
some ways a sequential version of maxent. The
single largest difference between our algorithm and
Collins?is that we update each feature in order, while
Collins? algorithms select a (possibly new) feature
to update. That algorithm also require more storage
than our algorithm when data is sparse: fast imple-
mentations require storage of both the training data
matrix (to compute which feature to update) and the
transpose of the training data matrix (to perform the
update efficiently.)
4 Experimental Results
In this section, we give experimental results, show-
ing that SCGIS converges up to an order of magni-
tude faster than GIS, or more, depending on the num-
ber of non-zero indicator functions, and the method
of measuring performance.
There are at least three ways in which one could
measure performance of a maxent model: the ob-
jective function optimized by GIS/SCGIS; the en-
tropy on test data; and the percent correct on test
data. The objective function for both SCGIS and
GIS when smoothing is Equation 3: the probabil-
ity of the training data times the probability of the
model. The most interesting measure, the percent
correct on test data, tends to be noisy.
For a test corpus, we chose to use exactly the same
training, test, problems, and feature sets used by
Banko and Brill (2001). These problems consisted of
trying to guess which of two confusable words, e.g.
?their? or ?there?, a user intended. Banko and Brill
chose this data to be representative of typical ma-
chine learning problems, and, by trying it across data
sizes and different pairs of words, it exhibits a good
deal of different behaviors. Banko and Brill used
a standard set of features, including words within a
window of 2, part-of-speech tags within a window of
2, pairs of word or tag features, and whether or not
a given word occurred within a window of 9. Alto-
gether, they had 55 feature types. That is, there were
many thousands of features in the model (depending
on the exact model), but at most 55 could be ?true?
for a given training or test instance.
We examine the performance of SCGIS versus
GIS across three different axes. The most important
variable is the number of features. In addition to try-
ing Banko and Brill?s 55 feature types, we tried using
feature sets with 5 feature types (words within a win-
dow of 2, plus the ?unigram? feature) and 15 feature
types (words within a window of 2, tags within a
window of 2, the unigram, and pairs of words within
a window of 2). We also tried not using smoothing,
and we tried varying the training data size.
In Table 1, we present a ?typical? configuration,
using 55 feature types, and 10 million words of train-
ing, and smoothing with a Gaussian prior. The first
two columns show the different confusable words.
Each column shows the ratio of how much longer
(in terms of elapsed time) it takes GIS to achieve the
same results as 10 iterations of SCGIS. An ?XXX?
denotes a case in which GIS did not achieve the
performance level of SCGIS within 1000 iterations.
(XXXs were not included in averages.)3 The ?ob-
jec? column shows the ratio of time to achieve the
same value of the objective function (Equation 3);
the ?ent? column show the ratio of time to achieve
the same test entropy; and the ?cor? column shows
the ratio of time to achieve the same test error rate.
For all three measurements, the ratio can be up to a
factor of 30, though the average is somewhat lower,
and in two cases, GIS converged faster.
In Table 2 we repeat the experiment, but with-
out smoothing. On the objective function ? which
with no smoothing is just the training entropy ? the
increase from SCGIS is even larger. On the other
3On a 1.7 GHz Pentium IV with 10,000,000 words train-
ing, and 5 feature types it took between .006 and .24 seconds
per iteration of SCGIS, and between .004 and .18 seconds for
GIS. With 55 feature types, it took between .05 and 1.7 sec-
onds for SCGIS and between .03 and 1.2 seconds for GIS. Note
that many experiments use much larger datasets or many more
feature types; run time scales linearly with training data size.
objec ent cor
accept except 31.3 38.9 32.3
affect effect 27.8 10.7 6.4
among between 30.9 1.9 XXX
its it?s 26.8 18.5 11.1
peace piece 33.4 0.3 XXX
principal principle 24.1 XXX 0.2
then than 23.4 37.4 24.4
their there 17.3 31.3 6.1
weather whether 21.3 XXX 8.7
your you?re 36.8 9.7 19.1
Average 27.3 18.6 13.5
Table 1: Baseline: standard feature types (55), 10
million words, smoothed
objec ent cor
accept except 39.3 4.8 7.5
affect effect 46.4 5.2 5.1
among between 48.7 4.5 2.5
its it?s 47.0 3.2 1.4
peace piece 46.0 0.6 XXX
principal principle 43.9 5.7 0.7
then than 48.7 5.6 1.0
their there 46.8 8.7 0.6
weather whether 44.7 6.7 2.1
your you?re 49.0 2.0 29.6
Average 46.1 4.7 5.6
Table 2: Same as baseline, except no smoothing
criteria ? test entropy and percentage correct ? the
increase from SCGIS is smaller than it was with
smoothing, but still consistently large.
In Tables 3 and 4, we show results with small and
medium feature sets. As can be seen, the speedups
with smaller features sets (5 feature types) are less
than the speedups with the medium sized feature set
(15 feature types), which are smaller than the base-
line speedup with 55 features.
Notice that across all experiments, there were no
cases where GIS converged faster than SCGIS on
the objective function; two cases where it coverged
faster on test data entropy; and 5 cases where it con-
verged faster on test data correctness. The objective
function measure is less noisy than test data entropy,
and test data entropy is less noisy than test data er-
ror rate: the noisier the data, the more chance of
an unexpected result. Thus, one possibility is that
these cases are simply due to noise. Similarly, the
four cases in which GIS never reached the test data
objec ent cor
accept except 6.0 4.8 3.7
affect effect 3.6 3.6 1.0
among between 5.8 1.0 0.7
its it?s 8.7 5.6 3.3
peace piece 25.2 2.9 XXX
principal principle 6.7 18.6 1.0
then than 6.9 6.7 9.6
their there 4.7 4.2 3.6
weather whether 2.2 6.5 7.5
your you?re 7.6 3.4 16.8
Average 7.7 5.7 5.2
Table 3: Small feature set (5 feature types)
objec ent cor
accept except 10.8 10.7 8.3
affect effect 12.4 18.3 6.8
among between 7.7 14.3 9.0
its it?s 7.4 XXX 5.4
peace piece 14.6 4.5 9.4
principal principle 7.3 XXX 0.0
then than 6.5 13.7 11.0
their there 5.9 11.3 2.8
weather whether 10.5 29.3 13.9
your you?re 13.1 8.1 9.8
Average 9.6 13.8 7.6
Table 4: Medium feature set (15 feature types)
entropy of SCGIS and the four cases in which GIS
never reached the test data error rate of SCGIS might
also be attributable to noise. There is an alternative
explanation that might be worth exploring. On a dif-
ferent data set, 20 newsgroups, we found that early
stopping techniques were helpful, and that GIS and
SCGIS benefited differently depending on the ex-
act settings. It is possible that effects similar to the
smoothing effect of early stopping played a role in
both the XXX cases (in which SCGIS presumably
benefited more from the effects) and in the cases
where GIS beat SCGIS (in which cases GIS pre-
sumably benefited more.) Additional research would
be required to determine which explanation ? early
stopping or noise ? is correct, although we suspect
both explanations apply in some cases.
We also ran experiments that were the same as the
baseline experiment, except changing the training
data size to 50 million words and to 1 million words.
We found that the individual speedups were often
different at the different sizes, but did not appear to
be overall higher or lower or qualitatively different.
5 Discussion
There are many reasons that maxent speedups are
useful. First, in applications with active learning
or parameter optimization or feature set selection,
it may be necessary to run many rounds of maxent,
making speed essential. There are other fast algo-
rithms, such as Winnow, available, but in our ex-
perience, there are some problems where smoothed
maxent models are better classifiers than Winnow.
Furthermore, many other fast classification algo-
rithms, including Winnow, do not output probabil-
ities, which are useful for precision/recall curves,
or when there is a non-equal tradeoff between false
positives and false negatives, or when the output of
the classifier is used as input to other models. Fi-
nally, there are many applications of maxent where
huge amounts of data are available, such as for lan-
guage modeling. Unfortunately, it has previously
been very difficult to use maxent models for these
types of experiments. For instance, in one language
modeling experiment we performed, it took a month
to learn a single model. Clearly, for models of this
type, any speedup will be very helpful.
Overall, we expect this technique to be widely
used. It leads to very significant speedups ? up to an
order of magnitude or more. It is very easy to imple-
ment ? other than the need to transpose the training
data matrix, and store an extra array, it is no more
complex than standard GIS. It can be easily applied
to any model type, although it leads to the largest
speedups on models with more feature types. Since
models with many interacting features are the type
for which maxent models are most interesting, this
is typical. It requires very few additional resources:
unless there are a large number of output classes, it
uses about as much space as standard GIS, and when
there are a large number of output classes, it can
be combined with our clustering speedup technique
(Goodman, 2001) to get both additional speedups,
and to reduce the space requirements. Thus, there
appear to be no real impediments to its use, and it
leads to large, broadly applicable gains.
Acknowledgements
Thanks to Ciprian Chelba, Stan Chen, Chris Meek,
and the anonymous reviewers for useful comments.
References
M. Banko and E. Brill. 2001. Mitigating the paucity
of data problem. In HLT.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
P. Brown, S. DellaPietra, V. DellaPietra, R. Mercer,
A. Nadas, and S. Roukos. Unpublished. Transla-
tion models using learned features and a general-
ized Csiszar algorithm. IBM research report.
D. Brown. 1959. A note on approximations to prob-
ability distributions. Information and Control,
2:386?392.
S.F. Chen and R. Rosenfeld. 1999. A gaussian prior
for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108, Computer Science
Department, Carnegie Mellon University.
Michael Collins, Robert E. Schapire, and Yoram
Singer. 2002. Logistic regression, adaboost and
bregman distances. Machine Learning, 48.
J.N. Darroch and D. Ratcliff. 1972. Generalized it-
erative scaling for log-linear models. The Annals
of Mathematical Statistics, 43:1470?1480.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random
fields. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(4):380?393, April.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In ICASSP 2001.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition. MIT Press.
J. Lafferty, F. Pereira, andA. McCallum. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. In ICML.
John Lafferty. 1995. Gibbs-markov models. In
Computing Science and Statistics: Proceedings
of the 27th Symposium on the Interface.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Available from
http://www-white.media.mit.edu/
?tpminka/papers/learning.html.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
J. Reynar and A. Ratnaparkhi. 1997. A maximum
entropy approach to identifying sentence bound-
aries. In ANLP.
Ronald Rosenfeld. 1994. Adaptive Statistical Lan-
guage Modeling: A Maximum Entropy Approach.
Ph.D. thesis, Carnegie Mellon University, April.
J. Wu and S. Khudanpur. 2000. Efficient training
methods for maximum entropy language model-
ing. In ICSLP, volume 3, pages 114?117.
Exploring Asymmetric Clustering for Statistical Language Modeling 
Jianfeng Gao  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
jfgao@microsoft.com 
Joshua T. Goodman  
Microsoft Research, Redmond 
Washington 98052, USA  
joshuago@microsoft.com 
Guihong Cao1  
Department of Computer 
Science and Engineering of 
Tianjin University, China  
Hang Li  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
hangli@microsoft.com 
                                                     
1 This work was done while Cao was visiting Microsoft Research Asia. 
Abstract 
The n-gram model is a stochastic model, 
which predicts the next word (predicted 
word) given the previous words 
(conditional words) in a word sequence.  
The cluster n-gram model is a variant of 
the n-gram model in which similar words 
are classified in the same cluster. It has 
been demonstrated that using different 
clusters for predicted and conditional 
words leads to cluster models that are 
superior to classical cluster models which 
use the same clusters for both words.  This 
is the basis of the asymmetric cluster 
model (ACM) discussed in our study.  In 
this paper, we first present a formal 
definition of the ACM. We then describe 
in detail the methodology of constructing 
the ACM. The effectiveness of the ACM 
is evaluated on a realistic application, 
namely Japanese Kana-Kanji conversion.  
Experimental results show substantial 
improvements of the ACM in comparison 
with classical cluster models and word 
n-gram models at the same model size.  
Our analysis shows that the 
high-performance of the ACM lies in the 
asymmetry of the model. 
1 Introduction 
The n-gram model has been widely applied in many 
applications such as speech recognition, machine 
translation, and Asian language text input [Jelinek, 
1990; Brown et al, 1990; Gao et al, 2002].  It is a 
stochastic model, which predicts the next word 
(predicted word) given the previous n-1 words 
(conditional words) in a word sequence. 
The cluster n-gram model is a variant of the word 
n-gram model in which similar words are classified 
in the same cluster.  This has been demonstrated as 
an effective way to deal with the data sparseness 
problem and to reduce the memory sizes for realistic 
applications. Recent research [Yamamoto et al, 
2001] shows that using different clusters for 
predicted and conditional words can lead to cluster 
models that are superior to classical cluster models, 
which use the same clusters for both words [Brown 
et al, 1992].  This is the basis of the asymmetric 
cluster model (ACM), which will be formally 
defined and empirically studied in this paper.  
Although similar models have been used in previous 
studies [Goodman and Gao, 2000; Yamamoto et al, 
2001], several issues have not been completely 
investigated. These include: (1) an effective 
methodology for constructing the ACM, (2) a 
thorough comparative study of the ACM with 
classical cluster models and word models when they 
are applied to a realistic application, and (3) an 
analysis of the reason why the ACM is superior. 
The goal of this study is to address the above 
three issues. We first present a formal definition of 
the ACM; then we describe in detail the 
methodology of constructing the ACM including (1) 
an asymmetric clustering algorithm in which 
different metrics are used for clustering the 
predicted and conditional words respectively; and 
(2) a method for model parameter optimization in 
which the optimal cluster numbers are found for 
different clusters.  We evaluate the ACM on a real 
application, Japanese Kana-Kanji conversion, which 
converts phonetic Kana strings into proper Japanese 
orthography. The performance is measured in terms 
of character error rate (CER).  Our results show 
substantial improvements of the ACM in 
comparison with classical cluster models and word 
n-gram models at the same model size.  Our analysis 
shows that the high-performance of the ACM comes 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 183-190.
                         Proceedings of the 40th Annual Meeting of the Association for
from better structure and better smoothing, both of 
which lie in the asymmetry of the model. 
This paper is organized as follows: Section 1 
introduces our research topic, and then Section 2 
reviews related work. Section 3 defines the ACM 
and describes in detail the method of model 
construction. Section 4 first introduces the Japanese 
Kana-Kanji conversion task; it then presents our 
main experiments and a discussion of our findings.  
Finally, conclusions are presented in Section 5. 
2 Related Work 
A large amount of previous research on clustering 
has been focused on how to find the best clusters 
[Brown et al, 1992; Kneser and Ney, 1993; 
Yamamoto and Sagisaka, 1999; Ueberla, 1996; 
Pereira et al, 1993; Bellegarda et al, 1996; Bai et 
al., 1998]. Only small differences have been 
observed, however, in the performance of the 
different techniques for constructing clusters. In this 
study, we focused our research on novel techniques 
for using clusters ? the ACM, in which different 
clusters are used for predicted and conditional words 
respectively. 
The discussion of the ACM in this paper is an 
extension of several studies below. The first similar 
cluster model was presented by Goodman and Gao 
[2000] in which the clustering techniques were 
combined with Stolcke?s [1998] pruning to reduce 
the language model (LM) size effectively. Goodman 
[2001] and Gao et al [2001] give detailed 
descriptions of the asymmetric clustering algorithm.  
However, the impact of the asymmetric clustering 
on the performance of the resulting cluster model 
was not empirically studied there.  Gao et al, [2001] 
presented a fairly thorough empirical study of 
clustering techniques for Asian language modeling. 
Unfortunately, all of the above work studied the 
ACM without applying it to an application; thus 
only perplexity results were presented.  The first real 
application of the ACM was a simplified bigram 
ACM used in a Chinese text input system [Gao et al  
2002]. However, quite a few techniques (including 
clustering) were integrated to construct a Chinese 
language modeling system, and the contribution of 
using the ACM alone was by no means completely 
investigated. 
Finally, there is one more point worth 
mentioning. Most language modeling improvements 
reported previously required significantly more 
space than word trigram models [Rosenfeld, 2000]. 
Their practical value is questionable since all 
realistic applications have memory constraints. In 
this paper, our goal is to achieve a better tradeoff 
between LM performance (perplexity and CER) and 
model size. Thus, whenever we compare the 
performance of different models (i.e. ACM vs. word 
trigram model), Stolcke?s pruning is employed to 
bring the models compared to similar sizes. 
3 Asymmetric Cluster Model 
3.1 Model  
The LM predicts the next word wi given its history h 
by estimating the conditional probability P(wi|h). 
Using the trigram approximation, we have 
P(wi|h)?P(wi|wi-2wi-1), assuming that the next word 
depends only on the two preceding words.  
In the ACM, we will use different clusters for 
words in different positions.  For the predicted word, 
wi, we will denote the cluster of the word by PWi, 
and we will refer to this as the predictive cluster. .For 
the words wi-2 and wi-1 that we are conditioning on, 
we will denote their clusters by CWi-2 and CWi-1 
which we call conditional clusters.  When we which 
to refer to a cluster of a word w in general we will 
use the notation W.  The ACM estimates the 
probability of wi given the two preceeding words wi-2 
and wi-1 as the product of the following two 
probabilities: 
(1) The probability of the predicted cluster PWi 
given the preceding conditional clusters CWi-2 
and CWi-1, P(PWi|CWi-2CWi-1), and 
(2) The probability of the word given its cluster PWi 
and the preceding conditional clusters CWi-2 and 
CWi-1, P(wi|CWi-2CWi-1PWi). 
Thus, the ACM can be parameterized by 
)|()|()|( 1212 iiiiiiii PWCWCWwPCWCWPWPhwP ???? ?? (1) 
The ACM consists of two sub-models: (1) the 
cluster sub-model P(PWi|CWi-2CWi-1), and (2) the 
word sub-model P(wi|CWi-2CWi-1PWi). To deal with 
the data sparseness problem, we used a backoff 
scheme (Katz, 1987) for the parameter estimation of 
each sub-model. The backoff scheme recursively 
estimates the probability of an unseen n-gram by 
utilizing (n-1)-gram estimates. 
The basic idea underlying the ACM is the use of 
different clusters for predicted and conditional 
words respectively.  Classical cluster models are 
symmetric in that the same clusters are employed for 
both predicted and conditional words.  However, the 
symmetric cluster model is suboptimal in practice. 
For example, consider a pair of words like ?a? and 
?an?.  In general, ?a? and ?an? can follow the same 
words, and thus, as predicted words, belong in the 
same cluster. But, there are very few words that can 
follow both ?a? and ?an?. So as conditional words, 
they belong in different clusters. 
In generating clusters, two factors need to be 
considered: (1) clustering metrics, and (2) cluster 
numbers.  In what follows, we will investigate the 
impact of each of the factors. 
3.2 Asymmetric clustering  
The basic criterion for statistical clustering is to 
maximize the resulting probability (or minimize the 
resulting perplexity) of the training data. Many 
traditional clustering techniques [Brown et al, 
1992] attempt to maximize the average mutual 
information of adjacent clusters 
?=
21 , 2
12
2121 )(
)|(log)(),(
WW WP
WWPWWPWWI , (2) 
where the same clusters are used for both predicted 
and conditional words. We will call these clustering 
techniques symmetric clustering, and the resulting 
clusters both clusters.  In constructing the ACM, we 
used asymmetric clustering, in which different 
clusters are used for predicted and conditional 
words. In particular, for clustering conditional 
words, we try to minimize the perplexity of training 
data for a bigram of the form P(wi|Wi-1), which is 
equivalent to maximizing 
?
=
?
N
i
ii WwP
1
1)|( . (3) 
where N is the total number of words in the training 
data.  We will call the resulting clusters conditional 
clusters denoted by CW. For clustering predicted 
words, we try to minimize the perplexity of training 
data of P(Wi|wi-1)?P(wi|Wi). We will call the 
resulting clusters predicted clusters denoted by PW. 
We have2 
??
= ?
?
=
? ?=?
N
i i
ii
i
iiN
i
iiii WP
wWP
wP
WwPWwPwWP
1 1
1
1
1 )(
)(
)(
)()|()|(  
  ?
=
?
?
?= N
i i
ii
i
ii
WP
WwP
wP
wWP
1
1
1 )(
)(
)(
)(  
  ?
= ??
?= N
i
ii
i
i WwPwP
wP
1
1
1
)|()(
)( . 
Now, 
)(
)(
1?i
i
wP
wP is independent of the clustering used. 
Therefore, for the selection of the best clusters, it is 
sufficient to try to maximize 
?
=
?
N
i
ii WwP
1
1 )|( . (4) 
This is very convenient since it is exactly the op-
posite of what was done for conditional clustering. It 
                                                     
2 Thanks to Lillian Lee for suggesting this justification of 
predictive clusters. 
means that we can use the same clustering tool for 
both, and simply switch the order used by the 
program used to get the raw counts for clustering.  
The clustering technique we used creates a binary 
branching tree with words at the leaves.  The ACM 
in this study is a hard cluster model, meaning that 
each word belongs to only one cluster.  So in the 
clustering tree, each word occurs in a single leaf.   In 
the ACM, we actually use two different clustering 
trees. One is optimized for predicted words, and the 
other for conditional words. 
The basic approach to clustering we used is a 
top-down, splitting clustering algorithm. In each 
iteration, a cluster is split into two clusters in the 
way that the splitting achieves the maximal entropy 
decrease (estimated by Equations (3) or (4)). Finally, 
we can also perform iterations of swapping all words 
between all clusters until convergence i.e. no more 
entropy decrease can be found3. We find that our 
algorithm is much more efficient than agglomerative 
clustering algorithms ? those which merge words 
bottom up.  
3.3 Parameter optimization 
Asymmetric clustering results in two binary 
clustering trees. By cutting the trees at a certain 
level, it is possible to achieve a wide variety of 
different numbers of clusters.  For instance, if the 
tree is cut after the 8th level, there will be roughly 
28=256 clusters.  Since the tree is not balanced, the 
actual number of clusters may be somewhat smaller. 
We use Wl to represent the cluster of a word w using 
a tree cut at level l.  In particular, if we set l to the 
value ?all?, it means that the tree is cut at infinite 
depth, i.e. each cluster contains a single word. The 
ACM model of Equation (1) can be rewritten as 
 P(PWil|CWi-2jCWi-1j)?P(wi|PWi-2kCWi-1kCWil). (5) 
To optimally apply the ACM to realistic applications 
with memory constraints, we are always seeking the 
correct balance between model size and 
performance. We used Stolcke?s pruning method to 
produce many ACMs with different model sizes. In 
our experiments, whenever we compare techniques, 
we do so by comparing the performance (perplexity 
and CER) of the LM techniques at the same model 
sizes. Stolcke?s pruning is an entropy-based cutoff 
                                                     
3 Notice that for experiments reported in this paper, we 
used the basic top-down algorithm without swapping. 
Although the resulting clusters without swapping are not 
even locally optimal, our experiments show that the 
quality of clusters (in terms of the perplexity of the 
resulting ACM) is not inferior to that of clusters with 
swapping. 
method, which can be described as follows: all 
n-grams that change perplexity by less than a 
threshold are removed from the model. For pruning 
the ACM, we have two thresholds: one for the 
cluster sub-model P(PWil|CWi-2jCWi-1j) and one for 
the word sub-model P(wi|CWi-2kCWi-1kPWil) 
respectively, denoted by tc and  tw below. 
In this way, we have 5 different parameters that 
need to be simultaneously optimized: l, j, k, tc, and 
tw, where j, k, and l are the numbers of clusters, and tc 
and tw are the pruning thresholds.  
A brute-force approach to optimizing such a large 
number of parameters is prohibitively expensive. 
Rather than trying a large number of combinations 
of all 5 parameters, we give an alternative technique 
that is significantly more efficient. Simple math 
shows that the perplexity of the overall model 
P(PWil|CWi-2jCWi-1j)? P(wi|CWi-2kCWi-1kPWil) is 
equal to the perplexity of the cluster sub-model 
P(PWil|CWi-2jCWi-1j) times the perplexity of the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  The size of 
the overall model is clearly the sum of the sizes of 
the two sub-models.  Thus, we try a large number of 
values of j, l, and a pruning threshold tc for 
P(PWil|CWi-2jCWi-1j), computing sizes and 
perplexities of each, and a similarly large number of 
values of l,  k, and a separate threshold tw for 
P(wi|CWi-2kCWi-1kPWil).  We can then look at all 
compatible pairs of these models (those with the 
same value of l) and quickly compute the perplexity 
and size of the overall models.  This allows us to 
relatively quickly search through what would 
otherwise be an overwhelmingly large search space. 
4 Experimental Results and Discussion 
4.1 Japanese Kana-Kanji Conversion Task 
Japanese Kana-Kanji conversion is the standard 
method of inputting Japanese text by converting a 
syllabary-based Kana string into the appropriate 
combination of ideographic Kanji and Kana. This is 
a similar problem to speech recognition, except that 
it does not include acoustic ambiguity. The 
performance is generally measured in terms of 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. The role of the language model is, 
for all possible word strings that match the typed 
phonetic symbol string, to select the word string 
with the highest language model probability. 
Current products make about 5-10% errors in con-
version of real data in a wide variety of domains. 
4.2 Settings 
In the experiments, we used two Japanese 
newspaper corpora: the Nikkei Newspaper corpus, 
and the Yomiuri Newspaper corpus. Both text 
corpora have been word-segmented using a lexicon 
containing 167,107 entries.  
We performed two sets of experiments: (1) pilot 
experiments, in which model performance is 
measured in terms of perplexity and (2) Japanese 
Kana-Kanji conversion experiments, in which the 
performance of which is measured in terms of CER. 
In the pilot experiments, we used a subset of the 
Nikkei newspaper corpus: ten million words of the 
Nikkei corpus for language model training, 10,000 
words for held-out data, and 20,000 words for 
testing data. None of the three data sets overlapped.  
In the Japanese Kana-Kanji conversion experiments, 
we built language models on a subset of the Nikkei 
Newspaper corpus, which contains 36 million 
words. We performed parameter optimization on a 
subset of held-out data from the Yomiuri Newspaper 
corpus, which contains 100,000 words. We 
performed testing on another subset of the Yomiuri 
Newspaper corpus, which contains 100,000 words. 
In both sets of experiments, word clusters were 
derived from bigram counts generated from the 
training corpora. Out-of-vocabulary words were not 
included in perplexity and error rate computations. 
4.3 Impact of asymmetric clustering 
As described in Section 3.2, depending on the 
clustering metrics we chose for generating clusters, 
we obtained three types of clusters: both clusters 
(the metric of Equation (2)), conditional clusters 
(the metric of Equation (3)), and predicted clusters 
(the metric of Equation (4)). We then performed a 
series of experiments to investigate the impact of 
different types of clusters on the ACM. We used 
three variants of the trigram ACM: (1) the predictive 
cluster model P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) where 
only predicted words are clustered, (2) the 
conditional cluster model P(wi|Wi-2Wi-1) where only 
conditional words are clustered, and (3) the IBM 
model P(wi|Wi)? P(Wi|Wi-2Wi-1) which can be treated 
as a special case of the ACM of Equation (5) by 
using the same type of cluster for both predicted and 
conditional words, and setting k = 0, and l = j. For 
each cluster trigram model, we compared their 
perplexities and CER results on Japanese Kana- 
Kanji conversion using different types of clusters. 
For each cluster type, the number of clusters were 
fixed to the same value 2^6 just for comparison.  The 
results are shown in Table 1. It turns out that the 
benefit of using different clusters in different 
positions is obvious.  For each cluster trigram 
model, the best results were achieved by using the 
?matched? clusters, e.g. the predictive cluster model 
P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) has the best 
performance when the cluster Wi is the predictive 
cluster PWi generated by using the metric of 
Equation (4). In particular, the IBM model achieved 
the best results when predicted and conditional 
clusters were used for predicted and conditional 
words respectively. That is, the IBM model is of the 
form P(wi|PWi)? P(PWi|CWi-2CWi-1). 
 Con Pre Both Con + Pre 
Perplexity 287.7 414.5 377.6 --- Con 
model CER (%) 4.58 11.78 12.56 --- 
Perplexity 103.4 102.4 103.3 --- Pre 
model CER (%) 3.92 3.63 3.82 --- 
Perplexity 548.2 514.4 385.2 382.2 IBM 
model CER (%) 6.61 6.49 5.82 5.36 
Table 1: Comparison of different cluster types 
with cluster-based models 
4.4 Impact of parameter optimization 
In this section, we first present our pilot experiments 
of finding the optimal parameter set of the ACM (l, j, 
k, tc, tw) described in Section 2.3. Then, we compare 
the ACM to the IBM model, showing that the 
superiority of the ACM results from its better 
structure. 
In this section, the performance of LMs was 
measured in terms of perplexity, and the size was 
measured as the total number of parameters of the 
LM: one parameter for each bigram and trigram, one 
parameter for each normalization parameter ? that 
was needed, and one parameter for each unigram.  
We first used the conditional cluster model of the 
form P(wi|CWi-2jCWi-1j). Some sample settings of 
parameters (j, tw) are shown in Figure 1. The 
performance was consistently improved by 
increasing the number of clusters j, except at the 
smallest sizes.  The word trigram model was 
consistently the best model, except at the smallest 
sizes, and even then was only marginally worse than 
the conditional cluster models.  This is not surprising 
because the conditional cluster model always 
discards information for predicting words. 
We then used the predictive cluster model of the 
form P(PWil|wi-2wi-1)?P(wi|wi-2wi-1PWil), where only 
predicted words are clustered. Some sample settings 
of the parameters (l, tc, tw) are shown in Figure 2. For 
simplicity, we assumed tc=tw, meaning that the same 
pruning threshold values were used for both 
sub-models. It turns out that predictive cluster 
models achieve the best perplexity results at about 
2^6 or 2^8 clusters. The models consistently 
outperform the baseline word trigram models.  
We finally returned to the ACM of Equation (5), 
where both conditional words and the predicted 
word are clustered (with different numbers of 
clusters), and which is referred to as the combined 
cluster model below.  In addition, we allow different 
values of the threshold for different sub-models. 
Therefore, we need to optimize the model parameter 
set l, j, k, tc, tw.  
Based on the pilot experiment results using 
conditional and predictive cluster models, we tried 
combined cluster models for values l? [4, 10], j, 
k? [8, 16]. We also allow j, k=all. Rather than plot 
all points of all models together, we show only the 
outer envelope of the points.  That is, if for a given 
model type and a given point there is some other 
point of the same type with both lower perplexity 
and smaller size than the first point, then we do not 
plot the first, worse point.  
The results are shown in Figure 3, where the 
cluster number of IBM models is 2^14 which 
achieves the best performance for IBM models in 
our experiments.  It turns out that when l? [6, 8] and 
j, k>12, combined cluster models yield the best 
results. We also found that the predictive cluster 
models give as good performance as the best 
combined ones while combined models 
outperformed very slightly only when model sizes 
are small. This is not difficult to explain. Recall that 
the predictive cluster model is a special case of the 
combined model where words are used in 
conditional positions, i.e. j=k=all. Our experiments 
show that combined models achieved good 
performance when large numbers of clusters are 
used for conditional words, i.e. large j, k>12, which 
are similar to words. 
The most interesting analysis is to look at some 
sample settings of the parameters of the combined 
cluster models in Figure 3. In Table 2, we show the 
best parameter settings at several levels of model 
size. Notice that in larger model sizes, predictive 
cluster models (i.e. j=k=all) perform the best in 
some cases. The ?prune? columns (i.e. columns 6 and 
7) indicate the Stolcke pruning parameter we used.  
First, notice that the two pruning parameters (in 
columns 6 and 7) tend to be very similar.  This is 
desirable since applying the theory of relative 
entropy pruning predicts that the two pruning 
parameters should actually have the same value.   
Next, let us compare the ACM 
P(PWil|CWi-2jCWi-1j)?P(wi|CWi-2kCWi-1kPWil) to 
traditional IBM clustering of the form 
P(Wil|Wi-2lWi-1l)?P(wi|Wil), which is equal to 
P(Wil|Wi-2lWi-1l)?P(wi|Wi-20Wi-10Wil) (assuming the   
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06
size
pe
rp
lex
ity
2^12 clusters
2^14 clusters
2^16 clusters
word trigram
 
Figure 1. Comparison of conditional models 
applied with different numbers of clusters 
100
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
2^4 clusters
2^6 clusters
2^8 clusters
2^10 clusters
word trigram
 
Figure 2. Comparison of predictive models 
applied with different numbers of clusters 
100
110
120
130
140
150
160
170
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
ACM
IBM
word trigram
predictive model
 
Figure 3. Comparison of ACMs, predictive 
cluster model, IBM model, and word trigram 
model 
same type of cluster is used for both predictive and 
conditional words). Our results in Figure 3 show that 
the performance of IBM models is roughly an order 
of magnitude worse than that of ACMs. This is 
because in addition to the use of the symmetric 
cluster model, the traditional IBM model makes two 
more assumptions that we consider suboptimal.  
First, it assumes that j=l.  We see that the best results 
come from unequal settings of j and l.  Second, more 
importantly, IBM clustering assumes that k=0.  We 
see that not only is the optimal setting for k not 0, but 
also typically the exact opposite is the optimal: k=all 
in which case P(wi|CWi-2kCWi-1kPWil)= 
P(wi|wi-2wi-1PWil), or k=14, 16, which is very 
similar. That is, we see that words depend on the 
previous words and that an independence 
assumption is a poor one.  Of course, many of these 
word dependencies are pruned away ? but when a 
word does depend on something, the previous words 
are better predictors than the previous clusters. 
Another important finding here is that for most of 
these settings, the unpruned model is actually larger 
than a normal trigram model ? whenever k=all or 14, 
16, the unpruned model P(PWil|CWi-2jCWi-1j) ? 
P(wi|CWi-2kCWi-1kPWil) is actually larger than an 
unpruned model P(wi|wi-2wi-1). 
This analysis of the data is very interesting ? it 
implies that the gains from clustering are not from 
compression, but rather from capturing structure.  
Factoring the model into two models, in which the 
cluster is predicted first, and then the word is 
predicted given the cluster, allows the structure and 
regularities of the model to be found. This larger, 
better structured model can be pruned more 
effectively, and it achieved better performance than 
a word trigram model at the same model size. 
Model size Perplexity l j k tc tw 
2.0E+05 141.1 8 12 14 24 24 
2.5E+05 135.7 8 12 14 12 24 
5.0E+05 118.8 6 14 16 6 12 
7.5E+05 112.8 6 16 16 3 6 
1.0E+06 109.0 6 16 16 3 3 
1.3E+06 107.4 6 16 16 2 3 
1.5E+06 106.0 6 All all 2 2 
1.9E+06 104.9 6 All all 1 2 
Table 2: Sample parameter settings for the ACM 
4.5 CER results 
Before we present CER results of the Japanese 
Kana-Kanji conversion system, we briefly describe 
our method for storing the ACM in practice.  
One of the most common methods for storing 
backoff n-gram models is to store n-gram 
probabilities (and backoff weights) in a tree 
structure, which begins with a hypothetical root 
node that branches out into unigram nodes at the first 
level of the tree, and each of those unigram nodes in 
turn branches out into bigram nodes at the second 
level and so on. To save storage, n-gram 
probabilities such as P(wi|wi-1) and backoff weights 
such as ?(wi-2wi-1) are stored in a single (bigram) 
node array (Clarkson and Rosenfeld, 1997). 
Applying the above tree structure to storing the 
ACM is a bit complicated ? there are some 
representation issues. For example, consider the 
cluster sub-model P(PWil|CWi-2jCWi-1j). N-gram 
probabilities such as P(PWil|CWi-1j) and backoff 
weights such as ?(CWi-2jCWi-1j) cannot be stored in a 
single (bigram) node array, because l ? j and 
PW?CW. Therefore, we used two separate trees to 
store probabilities and backoff weights, 
respectively. As a result, we used four tree structures 
to store ACMs in practice: two for the cluster 
sub-model P(PWil|CWi-2jCWi-1j), and two for the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  We found 
that the effect of the storage structure cannot be 
ignored in a real application. 
In addition, we used several techniques to 
compress model parameters (i.e. word id, n-gram 
probability, and backoff weight, etc.) and reduce the 
storage space of models significantly. For example, 
rather than store 4-byte floating point values for all 
n-gram probabilities and backoff weights, the values 
are quantized to a small number of quantization 
levels. Quantization is performed separately on each 
of the n-gram probability and backoff weight lists, 
and separate quantization level look-up tables are 
generated for each of these sets of parameters.  We 
used 8-bit quantization, which shows no 
performance decline in our experiments. 
Our goal is to achieve the best tradeoff between 
performance and model size. Therefore, we would 
like to compare the ACM with the word trigram 
model at the same model size. Unfortunately, the 
ACM contains four sub-models and this makes it 
difficult to be pruned to a specific size. Thus for 
comparison, we always choose the ACM with 
smaller size than its competing word trigram model 
to guarantee that our evaluation is under-estimated. 
Experiments show that the ACMs achieve 
statistically significant improvements over word 
trigram models at even smaller model sizes (p-value 
=8.0E-9). Some results are shown in Table 3.  
Word trigram model ACM 
Size 
(MB) 
CER Size 
(MB) 
CER  CER 
Reduction 
1.8 4.56% 1.7 4.25% 6.8% 
5.8 4.08% 4.5 3.83% 6.1% 
11.7 4.04% 10.7 3.73% 7.7% 
23.5 4.00% 21.7 3.63% 9.3% 
42.4 3.98% 40.4 3.63% 8.8% 
Table 3:  CER results of ACMs and word 
trigram models at different model sizes 
Now we discuss why the ACM is superior to 
simple word trigrams.  In addition to the better 
structure as shown in Section 3.3, we assume here 
that the benefit of our model also comes from its 
better smoothing. Consider a probability such as 
P(Tuesday| party on). If we put the word ?Tuesday? 
into the cluster WEEKDAY, we decompose the 
probability 
When each word belongs to one class, simple math 
shows that this decomposition is a strict equality. 
However, when smoothing is taken into 
consideration, using the clustered probability will be 
more accurate than using the non-clustered 
probability. For instance, even if we have never seen 
an example of ?party on Tuesday?, perhaps we have 
seen examples of other phrases, such as ?party on 
Wednesday?; thus, the probability P(WEEKDAY | 
party on) will be relatively high. Furthermore, 
although we may never have seen an example of 
?party on WEEKDAY Tuesday?, after we backoff or 
interpolate with a lower order model, we may able to 
accurately estimate P(Tuesday | on WEEKDAY). 
Thus, our smoothed clustered estimate may be a 
good one. 
Our assumption can be tested empirically by 
following experiments.  We first constructed several 
test sets with different backoff rates4. The backoff 
rate of a test set, when presented to a trigram model, 
is defined as the number of words whose trigram 
probabilities are estimated by backoff bigram 
probabilities divided by the number of words in the 
test set.  Then for each test set, we obtained a pair of 
CER results using the ACM and the word trigram 
model respectively.  As shown in Figure 4, in both 
cases, CER increases as the backoff rate increases 
from 28% to 40%. But the curve of the word trigram 
model has a steeper upward trend.  The difference of 
the upward trends of the two curves can be shown 
more clearly by plotting the CER difference between 
them, as shown in Figure 5.  The results indicate that 
because of its better smoothing, when the backoff 
rate increases, the CER using the ACM does not 
increase as fast as that using the word trigram model.  
Therefore, we are reasonably confident that some 
portion of the benefit of the ACM comes from its 
better smoothing. 
2.1
2.3
2.5
2.7
2.9
3.1
3.3
3.5
3.7
3.9
0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4 0.41backoff rate
er
ro
r r
ate
word trigram model
ACM
 
Figure 4: CER vs. backoff rate. 
                                                     
4  The backoff rates are estimated using the baseline 
trigram model, so the choice could be biased against the 
word trigram model. 
P(Tuesday | party on) = P(WEEKDAY | party on)? 
P(Tuesday | party on WEEKDAY). 
0.25
0.27
0.29
0.31
0.33
0.35
0.37
0.39
0.41
0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42
backoff rate
er
ro
r r
ate
 di
ffe
re
nc
e
 
Figure 5: CER difference vs. backoff rate. 
5 Conclusion 
There are three main contributions of this paper. 
First, after presenting a formal definition of the 
ACM, we described in detail the methodology of 
constructing the ACM effectively. We showed 
empirically that both the asymmetric clustering and 
the parameter optimization (i.e. optimal cluster 
numbers) have positive impacts on the performance 
of the resulting ACM.  The finding demonstrates 
partially the effectiveness of our research focus: 
techniques for using clusters (i.e. the ACM) rather 
than techniques for finding clusters (i.e. clustering 
algorithms).  Second, we explored the actual 
representation of the ACM and evaluate it on a 
realistic application ? Japanese Kana-Kanji 
conversion.  Results show approximately 6-10% 
CER reduction of the ACMs in comparison with the 
word trigram models, even when the ACMs are 
slightly smaller.  Third, the reasons underlying the 
superiority of the ACM are analyzed. For instance, 
our analysis suggests the benefit of the ACM comes 
partially from its better structure and its better 
smoothing. 
All cluster models discussed in this paper are 
based on hard clustering, meaning that each word 
belongs to only one cluster. One area we have not 
explored is the use of soft clustering, where a word w 
can be assigned to multiple clusters W with a 
probability P(W|w) [Pereira et al, 1993]. Saul and 
Pereira [1997] demonstrated the utility of soft 
clustering and concluded that any method that 
assigns each word to a single cluster would lose 
information. It is an interesting question whether our 
techniques for hard clustering can be extended to 
soft clustering.  On the other hand, soft clustering 
models tend to be larger than hard clustering models 
because a given word can belong to multiple 
clusters, and thus a training instance P(wi|wi-2wi-1) 
can lead to multiple counts instead of just 1.  
References 
Bai, S., Li, H., Lin, Z., and Yuan, B. (1998). Building 
class-based language models with contextual statistics. In 
ICASSP-98, pp. 173-176. 
Bellegarda, J. R., Butzberger, J. W., Chow, Y. L., Coccaro, N. 
B., and Naik, D. (1996). A novel word clustering algorithm 
based on latent semantic analysis. In ICASSP-96.  
Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J., 
Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. 
(1990). A statistical approach to machine translation. 
Computational Linguistics, 16(2), pp. 79-85. 
Brown, P. F., DellaPietra V. J., deSouza, P. V., Lai, J. C., and 
Mercer, R. L. (1992). Class-based n-gram models of natural 
language. Computational Linguistics, 18(4), pp. 467-479. 
Clarkson, P. R., and Rosenfeld, R. (1997). Statistical language 
modeling using the CMU-Cambridge toolkit. In Eurospeech 
1997, Rhodes, Greece. 
Gao, J. Goodman, J. and Miao, J. (2001). The use of clustering 
techniques for language model ? application to Asian 
language. Computational Linguistics and Chinese Language 
Processing. Vol. 6, No. 1, pp 27-60. 
Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002). Toward a 
unified approach to statistical language modeling for Chinese. 
ACM Transactions on Asian Language Information 
Processing. Vol. 1, No. 1, pp 3-33. 
Goodman, J. (2001). A bit of progress in language modeling.  In 
Computer Speech and Language, October 2001, pp 403-434. 
Goodman, J., and Gao, J. (2000). Language model size 
reduction by predictive clustering. ICSLP-2000, Beijing. 
Jelinek, F. (1990). Self-organized language modeling for speech 
recognition. In Readings in Speech Recognition, A. Waibel 
and K. F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, pp. 
450-506. 
Katz, S. M. (1987). Estimation of probabilities from sparse data 
for the language model component of a speech recognizer. 
IEEE Transactions on Acoustics, Speech and Signal 
Processing, ASSP-35(3):400-401, March. 
Kneser, R. and Ney, H. (1993).  Improved clustering techniques 
for class-based statistical language modeling. In Eurospeech, 
Vol. 2, pp. 973-976, Berlin, Germany. 
Ney, H., Essen, U., and Kneser, R. (1994). On structuring 
probabilistic dependences in stochastic language modeling. 
Computer, Speech, and Language, 8:1-38. 
Pereira, F., Tishby, N., and Lee L. (1993). Distributional 
clustering of English words. In Proceedings of the 31st Annual 
Meeting of the ACL. 
Rosenfeld, R. (2000). Two decades of statistical language 
modeling: where do we go from here. In Proceeding of the 
IEEE, 88:1270-1278, August. 
Saul, L., and Pereira, F.C.N. (1997). Aggregate and mixed-order 
Markov models for statistical language processing. In 
EMNLP-1997. 
Stolcke, A. (1998). Entropy-based Pruning of Backoff 
Language Models. Proc. DARPA News Transcription and 
Understanding Workshop, 1998, pp. 270-274. 
Ueberla, J. P. (1996). An extended clustering algorithm for 
statistical language models. IEEE Transactions on Speech 
and Audio Processing, 4(4): 313-316. 
Yamamoto, H., Isogai, S., and Sagisaka, Y. (2001). Multi-Class 
Composite N-gram Language Model for Spoken Language 
Processing Using Multiple Word Clusters. 39th Annual 
meetings of the Association for Computational Linguistics 
(ACL?01), Toulouse, 6-11 July 2001. 
Yamamoto, H., and Sagisaka, Y. (1999). Multi-class Composite 
N-gram based on Connection Direction, In Proceedings of the 
IEEE International Conference on Acoustics, Speech and 
Signal Processing, May, Phoenix, Arizona. 
An Incremental Decision List Learner
Joshua Goodman
Microsoft Research
One Microsoft Way
Redmond, WA 98052
joshuago@microsoft.com
Abstract
We demonstrate a problem with the stan-
dard technique for learning probabilistic
decision lists. We describe a simple, in-
cremental algorithm that avoids this prob-
lem, and show how to implement it effi-
ciently. We also show a variation that adds
thresholding to the standard sorting algo-
rithm for decision lists, leading to similar
improvements. Experimental results show
that the new algorithm produces substan-
tially lower error rates and entropy, while
simultaneously learning lists that are over
an order of magnitude smaller than those
produced by the standard algorithm.
1 Introduction
Decision lists (Rivest, 1987) have been used for a
variety of natural language tasks, including accent
restoration (Yarowsky, 1994), word sense disam-
biguation (Yarowsky, 2000), finding the past tense of
English verbs (Mooney and Califf, 1995), and sev-
eral other problems. We show a problem with the
standard algorithm for learning probabilistic deci-
sion lists, and we introduce an incremental algorithm
that consistently works better. While the obvious im-
plementation for this algorithm would be very slow,
we also show how to efficiently implement it. The
new algorithm produces smaller lists, while simul-
taneously substantially reducing entropy (by about
40%), and error rates (by about 25% relative.)
Decision lists are a very simple, easy to understand
formalism. Consider a word sense disambiguation
task, such as distinguishing the financial sense of the
word ?bank? from the river sense. We might want the
decision list to be probabilistic (Kearns and Schapire,
1994) so that, for instance, the probabilities can be
propagated to an understanding algorithm. The de-
cision list for this task might be:
IF ?water? occurs nearby, output ?river: .95?, ?fi-
nancial: .05?
ELSE IF ?money? occurs nearby, output ?river: .1?,
?financial: .9?
ELSE IF word before is ?left?, output ?river: .8?,
?financial: .2?
ELSE IF ?Charles? occcurs nearby, output ?river:
.6?, ?financial: .4?
ELSE output ?river: .5?, ?financial: .5?
The conditions of the list are checked in order, and
as soon as a matching rule is found, the algorithm
outputs the appropriate probability and terminates.
If no other rule is used, the last rule always triggers,
ensuring that some probability is always returned.
The standard algorithm for learning decision lists
(Yarowsky, 1994) is very simple. The goal is to min-
imize the entropy of the decision list, where entropy
represents how uncertain we are about a particular de-
cision. For each rule, we find the expected entropy
using that rule, then sort all rules by their entropy,
and output the rules in order, lowest entropy first.
Decision lists are fairly widely used for many rea-
sons. Most importantly, the rule outputs they produce
are easily understood by humans. This can make de-
cision lists useful as a data analysis tool: the decision
list can be examined to determine which factors are
most important. It can also make them useful when
the rules must be used by humans, such as when pro-
ducing guidelines to help doctors determine whether
a particular drug should be administered. Decision
lists also tend to be relatively small and fast and easy
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 17-24.
                         Proceedings of the Conference on Empirical Methods in Natural
to apply in practice.
Unfortunately, as we will describe, the standard al-
gorithm for learning decision lists has an important
flaw: it often chooses a rule order that is suboptimal
in important ways. In particular, sometimes the al-
gorithm will use a rule that appears good ? has lower
average entropy ? in place of one that is good ? low-
ers the expected entropy given its location in the list.
We will describe a simple incremental algorithm that
consistently works better than the basic sorting al-
gorithm. Essentially, the algorithm builds the list in
reverse order, and, before adding a rule to the list,
computes how much the rule will reduce entropy at
that position. This computation is potentially very
expensive, but we show how to compute it efficiently
so that the algorithm can still run quickly.
2 The Algorithms
In this section, we describe the traditional algorithm
for decision list learning in more detail, and then mo-
tivate our new algorithm, and finally, describe our
new algorithm and variations on it in detail. For sim-
plicity only, we will state all algorithms for the binary
output case; it should be clear how to extend all of
the algorithms to the general case.
2.1 Traditional Algorithm
Decision list learners attempt to find models that
work well on test data. The test data consists of a se-
ries of inputs x
1
, ..., x
n
, and we are trying to predict
the corresponding results y
1
, ..., y
n
. For instance, in
a word sense disambiguation task, a given x
i
could
represent the set of words near the word, and y
i
could represent the correct sense of the word. Given
a model D which predicts probabilities P
D
(y|x),
the standard way of defining how well D works is
the entropy of the model on the test data, defined
as
?
n
i=1
?log
2
P
D
(y
i
|x
i
). Lower entropy is better.
There are many justifications for minimizing entropy.
Among others, the ?true? probability distribution has
the lowest possible entropy. Also, minimizing train-
ing entropy corresponds to maximizing the probabil-
ity of the training data.
Now, consider trying to learn a decision list. As-
sume we are given a list of possible questions,
q
1
, ..., q
n
. In our word sense disambiguation ex-
ample, the questions might include ?Does the word
?water? occur nearby,? or more complex ones, such
as ?does the word ?Charles? occur nearby and is the
word before ?river.?? Let us assume that we have
some training data, and that the system has two out-
puts (values for y), 0 and 1. Let C(q
i
, 0) be the
number of times that, when q
i
was true in the train-
ing data, the output was 0, and similarly for C(q
i
, 1).
Let C(q
i
) be the total number of times that q
i
was
true. Now, given a test instance, x, y for which q
i
(x)
is true, what probability would we assign to y = 1?
The simplest answer is to just use the probability in
the training data,C(q
i
, 1)/C(q
i
). Unfortunately, this
tends to overfit the training data. For instance, if q
i
was true only once in the training data, then, depend-
ing on the value for y that time, we would assign a
probability of 1 or 0. The former is clearly an over-
estimate, and the latter is clearly an underestimate.
Therefore, we smooth our estimates (Chen and Good-
man, 1999). In particular, we used the interpolated
absolute discounting method. Since both the tradi-
tional algorithm and the new algorithm use the same
smoothing method, the exact smoothing technique
will not typically affect the relative performance of
the algorithms. Let C(0) be the total number of ys
that were zero in the training, and let C(1) be the to-
tal number of ys that were one. Then, the ?unigram?
probability y is P (y) = C(y)
C(0)+C(1)
. Let N(q
i
) be the
number of non-zero ys for a given question. In par-
ticular, in the two class case, N(q
i
) will be 0 if there
were no occurences of the question q
i
, 1 if training
samples for q
i
always had the same value, and 2 if
both 1 and 0 values occurred. Now, we pick some
value d (using heldout data) and discount all counts
by d. Then, our probability distribution is
P (y|q
i
) =
?
?
?
(C(q
i
,y)?d)
C(q
i
)
+
dN(q
i
)
C(q
i
)
P (y) if C(q
i
, y) > 0
dN(q
i
)
C(q
i
)
P (y) otherwise
Now, the predicted entropy for a question q
i
is just
entropy(q
i
) = ?P (0|q
i
)log
2
P (0|q
i
)?P (1|q
i
)log
2
P (1|q
i
)
The typical training algorithm for decision lists is
very simple. Given the training data, compute the
predicted entropy for each question. Then, sort the
questions by their predicted entropy, and output a
decision list with the questions in order. One of the
questions should be the special question that is al-
ways TRUE, which returns the unigram probability.
Any question with worse entropy than TRUE will
show up later in the list than TRUE, and we will
never get to it, so it can be pruned away.
2.2 New Algorithm
Consider two weathermen in Seattle in the winter.
Assume the following (overly optimistic) model of
Seattle weather. If today there is no wind, then to-
morrow it rains. On one in 50 days, it is windy, and,
the day after that, the clouds might have been swept
away, leading to only a 50% chance of rain. So,
overall, we get rain on 99 out of 100 days. The lazy
weatherman simply predicts that 99 out of 100 days,
it will rain, while the smart weatherman gives the true
probabilities (i.e. 100% chance of rain tomorrow if
no wind today, 50% chance of rain tomorrow if wind
today.)
Consider the entropy of the two weathermen.
The lazy weatherman always says ?There is a 99%
chance of rain tomorrow; my average entropy is
?.99? log
2
.99 ? .01 ? log
2
.01 = .081 bits.? The
smart weatherman, if there is no wind, says ?100%
chance of rain tomorrow; my entropy is 0 bits.? If
there is wind, however, the smart weatherman says,
?50% chance of rain tomorrow; my entropy is 1 bit.?
Now, if today is windy, who should we trust? The
smart weatherman, whose expected entropy is 1 bit,
or the lazy weatherman, whose expected entropy is
.08 bits, which is obviously much better.
The decision list equivalent of this is as follows.
Using the classic learner, we learn as follows. We
have three questions: if TRUE then predict rain with
probability .99 (expected entropy = .081). If NO
WIND then predict rain with probability 1 (expected
entropy = 0). If WIND then predict rain with proba-
bility 1/2 (expected entropy = 1). When we sort these
by expected entropy, we get:
IF NO WIND, output ?rain: 100%? (entropy 0)
ELSE IF TRUE, output ?rain: 99%? (entropy .081)
ELSE IF WIND, output ?rain: 50%? (entropy 1)
Of course, we never reach the third rule, and on windy
days, we predict rain with probabiliy .99!
The two weathermen show what goes wrong with
a naive algorithm; we can easily do much better. For
the new algorithm, we start with a baseline ques-
tion, the question which is always TRUE and pre-
list = { TRUE }
do
for each question q
i
entropyReduce(i) =
entropy(list)? entropy(prepend(q
i
, list))
l = i such that entropyReduce(i) is largest
if entropyReduce(l) <  then
return list
else
list = prepend(q
l
, list)
Figure 1: New Algorithm, Simple Version
dicts the unigram probabilities. Then, we find the
question which if asked before all other questions
would decrease entropy the most. This is repeated
until some minimum improvement, , is reached.1
Figure 1 shows the new algorithm; the notation
entropy(list) denotes the training entropy of a poten-
tial decision list, and entropy(prepend(q
i
, list)) indi-
cates the training entropy of list with the question ?If
q
i
then output p(y|q
i
)? prepended.
Consider the Parable of the Two Weathermen. The
new learning algorithm starts with the baseline: If
TRUE then predict rain with probability 99% (en-
tropy .081). Then it prepends the rule that reduces
the entropy the most. The entropy reduction from
the question ?NO WIND? is .081? .99 = .08, while
the entropy for the question ?WIND? is 1 bit for
the new question, versus .5 ? 1 + .5 ? ?log
2
.01 =
.5 + .5 ? 6.64 = 3.82, for the old, for a reduction
of 2.82 bits, so we prepend the ?WIND? question.
Finally, we learn (at the top of the list), that if ?NO
WIND?, then rain 100%, yielding the following de-
cision list:
IF NO WIND, output ?rain: 100%? (entropy 0)
ELSE IF WIND, output ?rain: 50%? (entropy 1)
ELSE IF TRUE, output ?rain: 99%? (entropy .081)
Of course, we never reach the third rule.
Clearly, this decision list is better. Why did our
entropy sorter fail us? Because sometimes a smart
learner knows when it doesn?t know, while a dumb
rule, like our lazy weatherman who ignores the wind,
doesn?t know enough to know that in the current sit-
1This means we are building the tree bottom up; it would be
interesting to explore building the tree top-down, similar to a
decision tree, which would probably also work well.
list = {TRUE}
for each training instance x
j
, y
j
instanceEnt(j) = ?log
2
p(y
j
)
for each question q
i
// Now we compute entropyReduce(i) =
// entropy(TRUE)? entropy(q
i
,TRUE)
entropyReduce(i) = 0
for each x
j
, y
j
such that q
i
(x
j
)
entropyReduce(i) += log
2
p(y
j
)? log
2
p(y
j
|q
i
)
do
l = argmax
i
entropyReduce(i)
if entropyReduce(l) <  then
return list
else
list = prepend(q
l
, list)
for each x
j
, y
j
such that q
l
(x
j
)
for each k such that q
k
(x
j
)
entropyReduce(k) += instanceEnt(j)
instanceEnt(j) = ?log
2
p(y
j
|q
l
)
for each k such that q
k
(x
j
)
entropyReduce(k) ?= instanceEnt(j)
Figure 2: New Algorithm, Efficient Version
uation, the problem is harder than usual.
2.2.1 Efficiency
Unfortunately, the algorithm of Figure 1, if imple-
mented in a straight-forward way, will be extremely
inefficient. The problem is the inner loop, which
requires computing entropy(prepend(q
i
, list)). The
naive way of doing this is to run all of the training
data through each possible decision list. In practice,
the actual questions tend to be pairs or triples of sim-
ple questions. For instance, an actual question might
be ?Is word before ?left? and word after ?of??? Thus,
the total number of questions can be very large, and
running all the data through the possible new decision
lists for each question would be extremely slow.
Fortunately, we can precompute entropyReduce(i)
and incrementally update it. In order to do so, we also
need to compute, for each training instance x
j
, y
j
the
entropy with the current value of list. Furthermore,
we store for each question q
i
the list of instances
x
j
, y
j
such that q
i
(x
j
) is true. With these changes,
the algorithm runs very quickly. Figure 2 gives the
efficient version of the new algorithm.
for each question q
i
compute entropy(i)
list = questions sorted by entropy(i)
remove questions worse than TRUE
for each training instance x
j
, y
j
instanceEnt(j) = ?log
2
p(y
j
)
for each question q
i
in list in reverse order
entropyReduce = 0
for each x
j
, y
j
such that q
i
(x
j
)
entropyReduce +=
instanceEnt(j)? log
2
p(y
j
|q
i
)
if entropyReduce < 
remove q
i
from list
else
for each x
j
, y
j
such that q
i
(x
j
)
instanceEnt(j) = log
2
p(y
j
|q
i
)
Figure 3: Compromise: Delete Bad Questions
Note that this efficient version of the algorithm
may consume a large amount of space, because of the
need to store, for each question q
i
, the list of training
instances for which the question is true. There are a
number of speed-space tradeoffs one can make. For
instance, one could change the update loop from
for each x
j
, y
j
such that q
i
(x
j
)
to
for each x
j
, y
j
if q
i
(x
j
) then ...
There are other possible tradeoffs. For instance, typ-
ically, each question q
i
is actually written as a con-
junction of simple questions, which we will denote
Q
i
j
. Assume that we store the list of instances that
are true for each simple question Q
i
j
, and that q
i
is of
the form Q
i
1
&Q
i
2
&...&Q
i
I
. Then we can write an
update loop in which we first find the simple question
with the smallest number of true instances, and loop
over only these instances when finding the instances
for which q
i
is true:
k = argmin
j
number instances such that Q
i
j
for each x
j
, y
j
such that Q
i
k
(x
j
)
if q
i
(x
j
) then ...
2.3 Compromise Algorithm
Notice the original algorithm can actually allow rules
which make things worse. For instance, in our lazy
weatherman example, we built this decision list:
IF NO WIND, output ?rain: 100%? (entropy 0)
ELSE IF TRUE, output ?rain: 99%? (entropy .081)
ELSE IF WIND, output ?rain: 50%? (entropy 1)
Now, the second rule could simply be deleted, and the
decision list would actually be much better (although
in practice we never want to delete the ?TRUE? ques-
tion to ensure that we always output some probabil-
ity.) Since the main reason to use decision lists is be-
cause of their understandability and small size, this
optimization will be worth doing even if the full im-
plementation of the new algorithm is too complex.
The compromise algorithm is displayed in Figure 3.
When the value of  is 0, only those rules that improve
entropy on the training data are included. When the
value of  is ??, all rules are included (the stan-
dard algorithm). Even when a benefit is predicted,
this may be due to overfitting; we can get further
improvements by setting the threshold to a higher
value, such as 3, which means that only rules that
save at least three bits ? and thus are unlikely to lead
to overfitting ? are added.
3 Previous Work
There has been a modest amount of previous work
on improving probabilistic decision lists, as well as
a fair amount of work in related fields, especially in
transformation-based learning (Brill, 1995).
First, we note that non-probabilistic decision lists
and transformation-based learning (TBL) are actu-
ally very similar formalisms. In particular, as ob-
served by Roth (1998), in the two-class case, they
are identical. Non-probabilistic decision lists learn
rules of the form ?If q
i
then output y? while TBLs
output rules of the form ?If q
i
and current-class is
y
?
, change class to y?. Now, in the two class case, a
rule of the form ?If q
i
and current-class is y?, change
class to y? is identical to one of the form ?If q
i
change
class to y?, since either way, all instances for which
q
i
is TRUE end up with value y. The other difference
between decision lists and TBLs is the list ordering.
With a two-class TBL, one goes through the rules
from last-to-first, and finds the last one that applies.
With a decision list, one goes through the list in or-
der, and finds the first one that applies. Thus in the
two-class case, simply by changing rules of the form
?If q
i
and current-class is y?, change class to y? to
?If q
i
output y?, and reversing the rule order, we can
change any TBL to an equivalent non-probabilistic
decision list, and vice-versa. Notice that our incre-
mental algorithm is analogous to the algorithm used
by TBLs: in TBLs, at each step, a rule is added that
minimizes the training data error rate. In our prob-
abilistic decision list learner, at each step, a rule is
added that minimizes the training data entropy.
Roth notes that this equivalence does not hold in
an important case: when the answers to questions
are not static. For instance, in part-of-speech tagging
(Brill, 1995), when the tag of one word is changed, it
changes the answers to questions for nearby words.
We call such problems ?dynamic.?
The near equivalence of TBLs and decision lists is
important for two reasons. First, it shows the connec-
tion between our work and previous work. In partic-
ular, our new algorithm can be thought of as a prob-
abilistic version of the Ramshaw and Marcus (1994)
algorithm, for speeding up TBLs. Just as that al-
gorithm stores the expected error rate improvement
of each question, our algorithm stores the expected
entropy improvement. (Actually, the Ramshaw and
Marcus algorithm is somewhat more complex, be-
cause it is able to deal with dynamic problems such
as part-of-speech tagging.) Similarly, the space-
efficient algorithm using compound questions at
the end of Section 2.2.1 can be thought of as a
static probabilistic version of the efficient TBL of
Ngai and Florian (2001).
The second reason that the connection to TBLs is
important is that it shows us that probabilistic de-
cision lists are a natural way to probabilize TBLs.
Florian et al (2000) showed one way to make prob-
abilistic versions of TBLs, but the technique is some-
what complicated. It involved conversion to a deci-
sion tree, and then further growing of the tree. Their
technique does have the advantage that it correctly
handles the multi-class case. That is, by using a
decision tree, it is relatively easy to incorporate the
current state, while the decision list learner ignores
that state. However, this is not clearly an advantage
? adding extra dependencies introduces data sparse-
ness, and it is an empirical question whether depen-
dencies on the current state are actually helpful. Our
probabilistic decision lists can thus be thought of as
a competitive way to probabilize TBLs, with the ad-
vantage of preserving the list-structure and simplicity
of TBL, and the possible disadvantage of losing the
dependency on the current state.
Yarowsky (1994) suggests two improvements to
the standard algorithm. First, he suggests an op-
tional, more complex smoothing algorithm than the
one we applied. His technique involves estimating
both a probability based on the global probability
distribution for a question, and a local probability,
given that no questions higher in the list were TRUE,
and then interpolating between the two probabilities.
He also suggests a pruning technique that eliminates
90% of the questions while losing 3% accuracy; as
we will show in Section 4, our technique or varia-
tions eliminate an even larger percentage of ques-
tions while increasing accuracy. Yarowsky (2000)
also considered changing the structure of decision
lists to include a few splits at the top, thus combining
the advantages of decision trees and decision lists.
The combination of this hybrid decision list and the
improved smoothing was the best performer for par-
ticipating systems in the 1998 senseval evaluation.
Our technique could easily be combined with these
techniques, presumably leading to even better results.
However, since we build our decision lists from last
to first, rather than first to last, the local probability is
not available as the list is being built. But there is no
reason we could not interpolate the local probability
into a final list. Similarly, in Yarowsky?s technique,
the local probability is also not available at the time
the questions are sorted.
Our algorithm can be thought of as a natural prob-
abilistic version of a non-probabilistic decision list
learner which prepends rules (Webb, 1994). One
difficulty that that approach has is ranking rules. In
the probabilistic framework, using entropy reduction
and smoothing seems like a natural solution.
4 Experimental Results and Discussion
In this section, we give experimental results, showing
that our new algorithm substantially outperforms the
standard algorithm. We also show that while accu-
racy is competitive with TBLs, two linear classifiers
are more accurate than the decision list algorithms.
Many of the problems that probabilistic decision
list algorithms have been used for are very similar: in
a given text context, determine which of two choices
is most appropriate. Accent restoration (Yarowsky,
1994), word sense disambiguation (Yarowsky, 2000),
and other problems all fall into this framework, and
typically use similar feature types. We thus chose
one problem of this type, grammar checking, and
believe that our results should carry over at least
to these other, closely related problems. In partic-
ular, we chose to use exactly the same training, test,
problems, and feature sets used by Banko and Brill
(2001a; 2001b). These problems consisted of try-
ing to guess which of two confusable words, e.g.
?their? or ?there?, a user intended. Banko and Brill
chose this data to be representative of typical machine
learning problems, and, by trying it across data sizes
and different pairs of words, it exhibits a good deal of
different behaviors. Banko and Brill used a standard
set of features, including words within a window of
2, part-of-speech tags within a window of 2, pairs of
word or tag features, and whether or not a given word
occurred within a window of 9. Altogether, they had
55 feature types. They used all features of each type
that occurred at least twice in the training data.
We ran our comparisons using 7 different algo-
rithms. The first three were variations on the stan-
dard probabilistic decision list learner. In particular,
first we ran the standard sorted decision list learner,
equivalent to the algorithm of Figure 3, with a thresh-
old of negative infinity. That is, we included all rules
that had a predicted entropy at least as good as the
unigram distribution, whether or not they would ac-
tually improve entropy on the training data. We call
this ?Sorted: ??.? Next, we ran the same learner
with a threshold of 0 (?Sorted: 0?): that is, we in-
cluded all rules that had a predicted entropy at least
as good as the unigram distribution, and that would
at least improve entropy on the training data. Then
we ran the algorithm with a threshold of 3 (?Sorted:
3?), in an attempt to avoid overfitting. Next, we ran
our incremental algorithm, again with a threshold of
reducing training entropy by at least 3 bits.
In addition to comparing the various decision list
algorithms, we also tried several other algorithms.
First, since probabilistic decision lists are probabilis-
tic analogs of TBLs, we compared to TBL (Brill,
1995). Furthermore, after doing our research on de-
cision lists, we had several successes using simple
linear models, such as a perceptron model and a max-
imum entropy (maxent) model (Chen and Rosenfeld,
1999). For the perceptron algorithm, we used a varia-
tion that includes a margin requirement, ? (Zaragoza
wj
= 0
for 100 iterations or until no change
for each training instance x
j
, y
j
if q(x
j
) ? w
j
? y
j
< ?
w
j
+= q(x
j
)? y
j
Figure 4: Perceptron Algorithm with Margin
1M 10M 50M
Sorted: ?? 14.27% 8.88% 6.23%
Sorted: 0 13.16% 8.43% 5.84%
Sorted: 3 10.23% 6.30% 3.94%
Incremental: 3 10.80% 6.33% 4.09%
Transformation 10.36% 5.14% 4.00%
Maxent 8.60% 4.42% 2.62%
Perceptron 8.22% 3.96% 2.65%
Figure 5: Geometric Mean of Error Rate across
Training Sizes
and Herbrich, 2000). Figure 4 shows this incredibly
simple algorithm. We use q(x
j
) to represent the vec-
tor of answers to questions about input x
j
; w
j
is a
weight vector; we assume that the output, y
j
is -1 or
+1; and ? is a margin. We assume that one of the
questions is TRUE, eliminating the need for a sepa-
rate threshold variable. When ? = 0, the algorithm
reduces to the standard perceptron algorithm. The
inclusion of a non-zero margin and running to con-
vergence guarantees convergence for separable data
to a solution that works nearly as well as a linear
support vector machine (Krauth and Mezard, 1987).
Given the extreme simplicity of the algorithm and the
fact that it works so well (not just compared to the
algorithms in this paper, but compared to several oth-
ers we have tried), the perceptron with margin is our
favorite algorithm when we don?t need probabilities,
and model size is not an issue.
Most of our algorithms have one or more parame-
ters that need to be tuned. We chose 5 additional con-
fusable word pairs for parameter tuning and chose
parameter values that worked well on entropy and
error rate across data sizes, as measured on these 5
additional word pairs. For the smoothing discount
value we used 0.7. For thresholds for both the sorted
and the incremental learner, we used 3 bits. For the
perceptron algorithm, we set ? to 20. For TBL?s min-
imum number of errors to fix, the traditional value of
1M 10M 50M
Sorted: ?? 1065 10388 38893
Sorted: 0 831 8293 31459
Sorted: 3 45 462 1999
Incremental: 3 21 126 426
Transformation 15 77 244
Maxent 1363 12872 46798
Perceptron 1363 12872 46798
Figure 6: Geometric Mean of Model Sizes across
Training Sizes 1M 10M 50M
Sorted: ?? 0.91 0.70 0.55
Sorted: 0 0.81 0.64 0.47
Sorted: 3 0.47 0.43 0.29
Incremental: 3 0.49 0.36 0.25
Maxent 0.44 0.27 0.18
Figure 7: Arithmetic Mean of Entropy across Train-
ing Sizes
2 worked well. For the maxent model, for smooth-
ing, we used a Gaussian prior with 0 mean and 0.3
variance. Since sometimes one learning algorithm
is better at one size, and worse at another, we tried
three training sizes: 1, 10 and 50 million words.
In Figure 5, we show the error rates of each algo-
rithm at different training sizes, averaged across the
10 words in the test set. We computed the geomet-
ric mean of error rate, across the ten word pairs. We
chose the geometric mean, because otherwise, words
with the largest error rates would disproportionately
dominate the results. Figure 6, shows the geometric
mean of the model sizes, where the model size is the
number of rules. For maxent and perceptron mod-
els, we counted size as the total number of features,
since these models store a value for every feature.
For Sorted: ?? and Sorted: 0, the size is similar to
a maxent or perceptron model ? almost every rule is
used. Sorted: 3 drastically reduces the model size ?
by a factor of roughly 20 ? while improving perfor-
mance. Incremental: 3 is smaller still, by about an
additional factor of 2 to 5, although its accuracy is
slightly worse than Sorted: 3. Figure 7 shows the en-
tropy of each algorithm. Since entropy is logarthmic,
we use the arithmetic mean.
Notice that the traditional probabilistic decision
list learning algorithm ? equivalent to Sorted: ??
? always has a higher error rate, higher entropy, and
larger size than Sorted: 0. Similarly, Sorted: 3 has
lower entropy, higher accuracy, and smaller models
than Sorted: 0. Finally, Incremental: 3 has slightly
higher error rates, but slightly lower entropies, and
1/2 to 1/5 as many rules. If one wants a probabilistic
decision list learner, this is clearly the algorithm to
use. However, if probabilities are not needed, then
TBL can produce lower error rates, with still fewer
rules. On the other hand, if one wants either the low-
est entropies or highest accuracies, then it appears
that linear models, such as maxent or the perceptron
algorithm with margin work even better, at the ex-
pense of producing much larger models.
Clearly, the new algorithm works very well when
small size and probabilities are needed. It would
be interesting to try combining this algorithm with
decision trees in some way. Both Yarowsky (2000)
and Florian et al (2000) were able to get improve-
ments on the simple decision list structure by adding
additional splits ? Yarowsky by adding them at the
root, and Florian et al by adding them at the leaves.
Notice however that the chief advantage of decision
lists over linear models is their compact size and un-
derstandability, and our techniques simultaneously
improve those aspects; adding additional splits will
almost certainly lead to larger models, not smaller.
It would also be interesting to try more sophisticated
smoothing techniques, such as those of Yarowsky.
We have shown that a simple, incremental algo-
rithm for learning probabilistic decision lists can pro-
duce models that are significantly more accurate,
have significantly lower entropy, and are significantly
smaller than those produced by the standard sorted
learning algorithm. The new algorithm comes at the
cost of some increased time, space, and complexity,
but variations on it, such as the sorted algorithm with
thresholding, or the techniques of Section 2.2.1, can
be used to trade off space, time, and list size. Over-
all, given the substantial improvements from this al-
gorithm, it should be widely used whenever the ad-
vantages ? compactness and understandability ? of
probabilistic decision lists are needed.
References
M. Banko and E. Brill. 2001a. Mitigating the paucity of
data problem. In HLT.
M. Banko and E. Brill. 2001b. Scaling to very very large
corpora for natural language disambiguation. In ACL.
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing: A case study in
part-of-speech tagging. Comp. Ling., 21(4):543?565.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech and Language, 13:359?394.
S.F. Chen and R. Rosenfeld. 1999. A gaussian prior for
smoothing maximum entropy models. Technical Re-
port CMU-CS-99-108, Computer Science Department,
Carnegie Mellon University.
R. Florian, J. C. Henderson, and G. Ngai. 2000. Coaxing
confidences out of an old friend: Probabilistic classifi-
cations from transformation rule lists. In EMNLP.
M. Kearns and R. Schapire. 1994. Efficient distribution-
free learning of probabilistic concepts. Computer and
System Sciences, 48(3):464?497.
W. Krauth and M. Mezard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Physics A, 20:745?752.
R.J. Mooney and M.E. Califf. 1995. Induction of first-
order decision lists: Results on learning the past tense
of English verbs. In International Workshop on Induc-
tive Logic Programming, pages 145?146.
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In NA-ACL, pages 40?47.
L. Ramshaw and M. Marcus. 1994. Exploring the statis-
tical derivation of transformational rule sequences for
part-of-speech tagging. In Proceedings of the Balanc-
ing Act Workshop on Combining Symbolic and Statis-
tical Approaches to Language, pages 86?95. ACL.
R. Rivest. 1987. Learning decision lists. Machine Learn-
ing, 2(3):229?246.
Dan Roth. 1998. Learning to resolve natural language
ambiguities: A unified approach. In AAAI-98.
G. Webb. 1994. Learning decision lists by prepending in-
ferred rules, vol. b. In Second Singapore International
Conference on Intelligent Systems, pages 280?285.
David Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in
spanish and french. In ACL, pages 88?95.
David Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the Hu-
manities, 34(2):179?186.
Hugo Zaragoza and Ralf Herbrich. 2000. The perceptron
meets reuters. In Workshop on Machine Learning for
Text and Images at NIPS 2001.
