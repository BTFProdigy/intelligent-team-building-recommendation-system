Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 279?287,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Identifying fake Amazon reviews as learning from crowds
Tommaso Fornaciari
Ministero dell?Interno
Dipartimento della Pubblica Sicurezza
Segreteria del Dipartimento
ComISSIT
tommaso.fornaciari@interno.it
Massimo Poesio
University of Essex
CSEE
University of Trento
CIMeC
poesio@essex.ac.uk
Abstract
Customers who buy products such as
books online often rely on other customers
reviews more than on reviews found on
specialist magazines. Unfortunately the
confidence in such reviews is often mis-
placed due to the explosion of so-called
sock puppetry?authors writing glowing
reviews of their own books. Identifying
such deceptive reviews is not easy. The
first contribution of our work is the cre-
ation of a collection including a number
of genuinely deceptive Amazon book re-
views in collaboration with crime writer
Jeremy Duns, who has devoted a great
deal of effort in unmasking sock puppet-
ing among his colleagues. But there can
be no certainty concerning the other re-
views in the collection: all we have is a
number of cues, also developed in collab-
oration with Duns, suggesting that a re-
view may be genuine or deceptive. Thus
this corpus is an example of a collection
where it is not possible to acquire the
actual label for all instances, and where
clues of deception were treated as anno-
tators who assign them heuristic labels. A
number of approaches have been proposed
for such cases; we adopt here the ?learn-
ing from crowds? approach proposed by
Raykar et al. (2010). Thanks to Duns? cer-
tainly fake reviews, the second contribu-
tion of this work consists in the evaluation
of the effectiveness of different methods of
annotation, according to the performance
of models trained to detect deceptive re-
views.
1 Introduction
Customer reviews of books, hotels and other prod-
ucts are widely perceived as an important rea-
son for the success of e-commerce sites such as
amazon.com or tripadvisor.com. How-
ever, customer confidence in such reviews is often
misplaced, due to the growth of the so-called sock
puppetry phenomenon: authors / hoteliers writing
glowing reviews of their own works / hotels (and
occasionally also negative reviews of the competi-
tors).
1
The prevalence of this phenomenon has
been revealed by campaigners such as crime writer
Jeremy Duns, who exposed a number of fellow au-
thors involved in such practices.
2
A number of
sites have also emerged offering Amazon reviews
to authors for a fee.
3
Several automatic techniques for exposing such
deceptive reviews have been proposed in recent
years (Feng et al., 2012; Ott et al., 2001). But like
all work on deceptive language (computational or
otherwise) (Newman et al., 2003; Strapparava and
Mihalcea, 2009), such works suffer from a seri-
ous problem: the lack of a gold standard contain-
ing ?real life? examples of deceptive uses of lan-
guage. This is because it is very difficult to find
definite proof that an Amazon review is either de-
ceptive or genuine. Thus most researchers recre-
ate deceptive behavior in the lab, as done by New-
man et al. (2003). For instance, Ott et al. (2001),
Feng et al. (2012) and Strapparava and Mihalcea
(2009) used crowdsourcing, asking turkers to pro-
duce instances of deceptive behavior. Finally, Li
et al. (2011) classify reviews as deceptive or truth-
ful by hand on the basis of a series of heuristics:
they start by excluding anonymous reviews, then
use their helpfulness and other criteria to decide
1
The phenomenon predates Internet - see e.g., Amy Har-
mon, ?Amazon Glitch Unmasks War Of Reviewers?, New
York Times, February 14, 2004.
2
See Andrew Hough, ?RJ Ellory: fake book reviews
are rife on internet, authors warn?, telegraph.co.uk,
September 3, 2012
3
See Alison Flood, ?Sock puppetry and fake reviews:
publish and be damned?, guardian.co.uk, September 4,
2012 and David Streitfeld, ?Buy Reviews on Yelp, Get Black
Mark?, nytimes.com, October 18, 2012.
279
whether they are deceptive or not. Clearly a more
rigorous approach to establishing the truth or oth-
erwise of reviews on the basis of such heuristic
criteria would be useful.
In this work we develop a system for identify-
ing deceptive reviews in Amazon. Our proposal
makes two main contributions:
1. we identified in collaboration with Jeremy
Duns a series of criteria used by Duns and
other ?sock puppet hunters? to find suspicious
reviews / reviewers, and collected a dataset of
reviews some of which are certainly false as
the authors admitted so, whereas others may
be genuine or deceptive.
2. we developed an approach to the truthful-
ness of reviews based on the notion that the
truthfulness of a review is a latent variable
whose value cannot be known, but can be es-
timated using some criteria as potential indi-
cators of such value?as annotators?and then
we used the learning from crowds algorithm
proposed by Raykar et al. (2010) to assign a
class to each review in the dataset.
The structure of the paper is as follows. In Sec-
tion 2 we describe how we collected our dataset;
in Section 3 we show the experiments we carried
out and in Section 4 we discuss the results.
2 Deception clues and dataset
2.1 Examples of Unmasked Sock Puppetry
After reading an article by Alison Flood on The
Guardian of September 4th, 2012
4
, discussing
how crime writer Jeremy Duns had unmasked a
number of ?sock puppeteers,? we contacted him.
Duns was extremely helpful; he pointed us to the
other articles on the topic, mostly on The New York
Times, and helped us create a set of deception
clues and the dataset used in this work.
On July 25
th
, 2011, an article appeared on
www.moneytalksnews.com, entitled ?3 Tips
for Spotting Fake Product Reviews - From Some-
one Who Wrote Them?.
5
Sandra Parker, author
of the text, in that page described her experience
as ?professional review writer?. According to her
4
Sock puppetry and fake reviews: publish and be damned,
http://www.guardian.co.uk/books/2012/
sep/04/sock-puppetry-publish-be-damned
5
http://www.moneytalksnews.com/2011/07/25
/3-tips-for-spotting-fake-product-reviews--
-from-someone-who-wrote-them/
statements, advertising agencies were used to pay
her $10-20 for writing reviews on sites like Ama-
zon.com. She was not asked to lie, but ?if the re-
view wasn?t five star, they didn?t pay?. In an arti-
cle of August 19
th
, written by David Streitfeld on
www.nytimes.com,
6
she actually denied that
point: ?We were not asked to provide a five-star
review, but would be asked to turn down an as-
signment if we could not give one?.
In any case, in her article Sandra Parker gave
the readers some common sense-based advices, in
order to help them to recognize possible fake re-
views. One of these suggestions were also useful
for this study, as discussed in Section 2.3. From
our point of view, however, the most interesting
aspect of the article relied in the fact that, letting
know the name of an author of fake reviews, it
made possible to identify them in Amazon.com,
with an high degree of confidence.
A further article written on August 25
th
by
David Streitfeld gave us another similar opportu-
nity.
7
In fact, thanks to his survey, it was possible
to come to know the titles of four books, whose the
authors paid an agency in order to receive reviews.
2.2 The corpus
Using the suggestions of Jeremy Duns and the in-
formation in these articles we built a corpus we
called DEREV (DEception in REViews), consist-
ing of clearly fake, possibly fake, and possibly
genuine book reviews posted on www.amazon.
com. The corpus, which will be freely available
on demand, consists of 6819 reviews downloaded
from www.amazon.com, concerning 68 books
and written by 4811 different reviewers. The 68
books were chosen trying to balance the number
of reviews (our units of analysis) related to sus-
pect books which probably or surely received fake
reviews, with the number of reviews hypothesized
to be genuine in that we expected the authors of
the books not to have bought reviews. In partic-
ular, we put into the group of the suspect books -
henceforth SB - the reviews of the four books in-
dicated by David Streitfeld. To this first nucleus,
we also added other four books, written by three
of the authors of the previous group. We also in-
6
http://www.nytimes.com/2011/08/20/
technology/finding-fake-reviews-online.
html?_r=1&
7
http://www.nytimes.com/2012/08/26/busin
ess/book-reviewers-for-hire-meet-a-demand-
for-online-raves.html?pagewanted=all
280
cluded in the SB group the 22 books for which
Sandra Parker wrote a review. Lastly, we noticed
that some reviewers of the books pointed out by
David Streitfeld tended to write reviews of the
same books: we identified 16 of them, and consid-
ered suspect as well. In total, on November 17
th
,
2011 we downloaded the reviews of 46 books con-
sidered as suspect, which received 2707 reviews.
8
We also collected the reviews of 22 so called ?in-
nocent books?, for a total of 4112 reviews. These
books were mainly chosen among classic authors,
such as Conan Doyle or Kipling, or among liv-
ing writers who are so renowned that any reviews?
purchase would be pointless: this is the case, for
example, of Ken Follett and Stephen King. As
shown by the number of the reviews, the books
of these authors are so famous that they receive a
great amount of readers? opinions.
The size of DEREV is 1175410 tokens, con-
sidering punctuation blocks as single token. The
mean size of the reviews is 172.37 tokens. The ti-
tles of the reviews were neither included in these
statistics nor in the following analyses.
2.3 Deception clues
Once created the corpus, we identified a set of
clues, whose presence suggested the deceptiveness
of the reviews. These clues are:
Suspect Book - SB The first clue of deceptive-
ness was the reference of the reviews to a sus-
pect book, identified as described above. This
is the only clue which is constant for all the
reviews of the same book.
Cluster - Cl The second clue comes from the
suggestions given by Sandra Parker in her
mentioned article. As she pointed out, the
agencies she worked for were used to give her
48 hours to write a review. Being likely that
the same deadline was given to other review-
ers, Sandra Parker warns to pay attention if
the books receive many reviews in a short pe-
riod of time. Following her advice, we con-
sidered as positive this clue of deceptiveness
if the review belonged to a group of at least
two reviews posted within 3 days.
Nickname - NN A service provided by Amazon
is the possibility for the reviewers to register
8
We specify the date of the download because, obviously,
if the data collection would be repeated today, the overall
number of reviews would be greater.
in the website and to post comments using
their real name. Since the real identity of the
reviewers involves issues related to their rep-
utation, we supposed it is less probable that
the writers of fake reviews post their texts us-
ing their true name. Moreover, a similar as-
sumption was probably accepted by Li et al.
(2011), who considered the profile features of
the reviewers, and among them the use or not
of their real name.
Unknown Purchase - UP Lastly, the probably
most interesting information provided by
Amazon is whether the reviewer bought the
reviewed book through Amazon itself. It
is reasonable to think that, if the reviewer
bought the book, he also read it. Therefore,
the absence of information about the certified
purchase was considered a clue of deceptive-
ness.
2.4 Gold and silver standard
The clues of deception discussed above give us
a heuristic estimate of the truthfulness of the re-
views. Such estimation represents a silver stan-
dard of our classes, as these are not determined
through certain knowledge of the ground truth, but
simply thanks to hints of deceptiveness. The meth-
ods we used in order to assign the heuristic classes
to the reviews are described in the next Section;
however for our purposes we needed a gold stan-
dard, that is at least a subset of reviews whose
ground truth was known with a high degree of con-
fidence. This subset was identified as follows.
First, we considered as false the 22 reviews
published by Sandra Parker, even though not all
her reviews are characterized by the presence of
all the deception clues. Even though we cannot
really say whether her reviews reflect her opin-
ion of the books in question or not, she explic-
itly claimed to have been paid for writing them;
and she only bought on Amazon three of these
22 books. This is the most accurate knowledge
about fake reviews not artificially produced we
have found in literature. Then we focused on the
four books whose authors admitted to have bought
the reviews.
9
Three of them received many re-
views, which made it difficult to understand if
they were truthful or not. However, one of these
9
http://www.nytimes.com/2012/08/26/busin
ess/book-reviewers-for-hire-meet-a-demand-
for-online-raves.html?pagewanted=all
281
Table 1: The distribution of deception clues in the
reviews
Nr. clues Reviews Tot. %
False 4 903
rev. 3 1913 2816 41.30%
True 2 2528
rev. 1 1210
0 265 4003 58.70%
books (?Write your first book?, by Peter Biadasz)
received only 20 reviews, which therefore could
be considered as fake with high degree of proba-
bility. Even though we have no clear evidence that
a small number of reviews correlates with a greater
likelihood of deception, since we know this book
received fake reviews, and there are only few re-
views for it, we felt it is pretty likely that those
are fake. Therefore we examined the reviews writ-
ten by these twenty authors, and considered as
false only those showing the presence of all the
deception clues described above. In this way, we
found 96 reviews published by 14 reviewers, and
we added them to the 22 of Sandra Parker, for a
total of 118 reviews written by 15 authors.
Once identified this subset of fake reviews, we
selected other 118 reviews which did not show
the presence of any deception clue, that is chosen
from books above any suspicion, written by au-
thors who published the review having made use
of their real name and having bought the book
through Amazon and so on.
In the end, we identified a subset of DEREV
constituted by 236 reviews, whose class was
known with high degree of confidence and con-
sidered them as our gold standard.
3 Experiments
We carried out two experiments, in which the
classes assigned to the reviews of DEREV were
found adopting two different strategies. In the first
experiment the classes of the reviews were de-
termined using majority voting of our deception
clues. This experiment is thus conceptually simi-
lar to those of Li et al. (2011), who trained models
using supervised methods with the aim of identi-
fying fake reviews. We discuss this experiment in
the next Section. In the second experiment, learn-
ing from crowds was used (Raykar et al., 2010).
This approach is discussed in Section 3.2.1.
In both experiments we carried out a 10-fold
cross-validation where in each iteration feature se-
lection and training were carried out using 90% of
the part of the corpus with only silver standard an-
notation and 90% of the subset with gold. The test
set used in each iteration consisted of the remain-
ing tenth of reviews with gold standard classes,
which were employed in order to evaluate the pre-
dictions of the models. This allowed to estimate
the efficiency of the strategies we used to deter-
mine our silver standard classes.
3.1 Majority Voting
3.1.1 Determining the class of reviews by
majority voting
The deception clues discussed in Section 2.3 were
used in our first experiment to identify the class of
each review using majority voting. In other words,
those clues were considered as independent pre-
dictors of the class; the class predicted by the ma-
jority of the annotators/clues was assigned to the
review. Specifically, if 0, 1 or 2 deception clues
were found, the review was classified as true; if
there were 3 or 4, the review was considered false.
Table 1 shows the distribution of the number of
deception clues in the reviews in DEREV.
3.1.2 Feature selection
In both experiments each review was represented
as feature vector. The features were just of uni-
grams, bigrams and trigrams of lemmas and part-
of-speech (POS), as collected from the reviews
through TreeTagger
10
(Schmid, 1994).
Since in each experiment we applied a 10-fold
cross-validation, in every fold the features were
extracted from the nine-tenths of DEREV em-
ployed as training set. Once identified the train-
ing set, we computed the frequency lists of the
n-grams of lemmas and POS. The lists were col-
lected separately from the reviews belonging to
the class ?true? and to the class ?false?. Such sep-
aration was aimed to take into consideration the
most highly frequent n-grams of both genuine and
fake reviews. However, for the following steps of
the feature selection, only the n-grams which ap-
peared more than 300 times in every frequency list
were considered: a threshold empirically chosen
for ease of calculations. In fact, among the most
10
http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
282
Table 2: The most frequent n-grams collected
N-grams Lemmas POS Total
Unigrams 34 21
Bigrams 21 13
Trigrams 13 8
Total 68 42 110
frequents, in order to identify the features most
effective in discriminating the two classes of re-
views, the Information Gain (IG) of the selected n-
grams was computed (Kullback and Leibler, 1951;
Yang and Pedersen, 1997).
Then, after having found the Information Gain
of the n-grams of lemmas and part-of-speech, a
further reduction of the features was realized. In
fact, we selected a relatively small amount of fea-
tures, in order to facilitate the computation of the
Raykar et al.?s algorithm (discussed in Sub-section
3.2.1), and only the n-grams with the highest IG
values were selected to be taken as features of the
vectors which represented the reviews. In par-
ticular, the n-grams were collected according to
the scheme shown in Table 2. By the way, 8,
13, 21 and 34 are numbers belonging to the Fi-
bonacci series (Sigler, 2003). They were chosen
because they grow exponentially and are used, in
our case, to give wider representation to the short-
est n-grams.
Lastly, two more features were added to the fea-
ture set, that is the length of the review, considered
with and without punctuation. Therefore, in each
fold of the experiment, the vectors of the reviews
were constituted by 112 values: 2 corresponding
to the length of the review, and 110 representing
the (not normalized) frequency, into the review it-
self, of the selected n-grams of lemmas and POS.
3.1.3 Baselines
The best way to assess the improvement coming
from the algorithm would have been with respect
to a supervised baseline. However this was not
possible as we could only be certain regarding the
classification of a fraction of the reviews (our gold
standard: 236 reviews, for a total of about 23,000
tokens). We felt such a small dataset could not be
used for training, but only for evaluation; therefore
we used instead two simple heuristic baselines.
Majority baseline. The simplest metric for per-
formance evaluation is the majority baseline: al-
ways assign to a review the class most represented
in the dataset. Since in the subset of DEREV with
gold standard we had 50% of true and false re-
views, simply 50% is our majority baseline.
Random baseline. Furthermore, we estimated a
random baseline through a Monte Carlo simula-
tion. This kind of simulation allows to estimate the
performance of a classifier which performs several
times a task over random outputs whose distribu-
tion reflects that of real data.
In particular, for this experiment, since we had
236 reviews whose 50% were labeled as false,
100000 times we produced 236 random binomial
predictions, having p = .5. In each simulation,
the random prediction was compared with our real
data. It turned out that in less than .01% of tri-
als the level of 62.29% of correct predictions was
exceeded. The thresholds for precision and recall
in detecting deceptive reviews were 62.26% and
66.95% respectively.
3.1.4 Models
We tested a number of supervised learning meth-
ods to learn a classifier using the classes deter-
mined by majority voting, but the best results
were obtained using Support Vector Machines
(SVMs) (Cortes and Vapnik, 1995), already em-
ployed in many applications involving text classi-
fication (Yang and Liu, 1999).
3.1.5 Results
The results obtained by training a supervised clas-
sifier over the dataset with classes identified with
majority voting are shown in the Table 3. The
highest results are in bold. The methodological
approach and performance achieved in this exper-
iment seems to be comparable to that of Strappar-
ava and Mihalcea (2009) and, more recently, of Li
et al. (2011). However Li et al. (2011) evaluate the
effectiveness of different kind of features with the
aim of annotating unlabeled data, while we try to
evaluate the reliability of heuristic classes in train-
ing.
3.2 Learning from Crowds
3.2.1 The Learning from Crowds algorithm
As pointed out by Raykar et al. (2010), major-
ity voting is not necessarily the most effective
way to determine the real classes in problems like
283
Table 3: The experiment with the majority voting classes
Correctly Incorrectly Precision Recall F-measure
classified reviews classified reviews
False reviews 75 43 83.33% 63.56% 72.12%
True reviews 103 15
Total 178 58
Total accuracy 75.42%
Random baseline 62.29% 62.26% 66.95%
those of reviews where there is no gold standard.
This is because annotators are not equally reli-
able, and the reviews are not equally challenging.
Hence the output of the majority voting may be af-
fected by unevaluated biases. To address this prob-
lem, Raykar et al. (2010) presented a maximum-
likelihood estimator that jointly learns the classi-
fier/regressor, the annotator accuracy, and the ac-
tual true label.
For ease of exposition, Raykar et al. (2010) use
as classifier the logistic regression, even though
they specify their algorithm would work with any
classifier. In case of logistic regression, the prob-
ability for an entity x ? X of belonging to a class
y ? Y with Y = {1, 0} is a sigmoid function
of the weight vector w of the features of each in-
stance x
i
, that is p[y = 1|x,w] = ?(w
>
x), where,
given a threshold ?, the class y = 1 if w
>
x ? ?.
Annotators? performance, then, is evaluated ?in
terms of the sensitivity and specificity with respect
to the unknown gold standard?: in particular, in a
binary classification problem, for the annotator j
the sensitivity ?
j
is the rate of positive cases iden-
tified by the annotator ?i.e., the recall of positive
cases? while the specificity ?
j
is the annotator?s
recall of negative cases.
Given a dataset D constituted of indepen-
dently sampled entities, a number of annotators
R, and the relative parameters ? = {w,?, ?},
the likelihood function which needs to be maxi-
mized, according to Raykar et al. (2010), would
be p[D|?] =
?
N
i=1
p[y
1
i
, ...y
R
i
|x
i
, ?], and the
maximum-likelihood estimator is obtained by
maximizing the log-likelihood, that is
?
?
ML
= {??,
?
?, w?} = argmax
?
{ln p[D|?]}. (1)
Raykar et al. (2010) propose to solve this max-
imization problem (Bickel and Doksum, 2000)
through the technique of Expectation Maximiza-
tion (EM) (Dempster et al., 1977). The EM al-
gorithm can be used to recover the parameters of
the hidden distributions accounting for the distri-
bution of data. It consists of two steps, an Expecta-
tion step (E-step) followed by a Maximization step
(M-step), which are iterated until convergence.
During the E-step the expectation of the term y
i
is
computed starting from the current estimate of the
parameters. In the M-step the parameters ? are up-
dated by maximizing the conditional expectation.
Regarding the third parameter, w, Raykar et al.
(2010) admit there is not a closed form solution
and suggest to use the Newton-Raphson method.
3.2.2 Determining the class of reviews using
Learning from Crowds
In order to apply Raykar?s algorithm, we pro-
ceeded as follows. First, we applied the procedure
for feature selection described in Subsection 3.1.2
to create a single dataset: that is, the corpus was
not divided in folds, but the feature selection in-
volved all of DEREV. This dataset was built using
the classes resulting from the majority voting ap-
proach and included these columns:
? The class assignments of the four clues dis-
cussed in Sub-section 2.3 ? SB, Cl, NN, UP;
? The majority voting class;
? The 112 features identified according to the
procedure presented in Sub-section 3.1.2.
Then, we implemented the algorithm proposed
by Raykar et al. (2010) in R.
11
We computed a Lo-
gistic Regression (Gelman and Hill, 2007) on the
dataset to compute the weight vectorw, used to es-
timate for each instance the probability p
i
for the
review of belonging to the class ?true?. For the lo-
gistic regression we used the 112 surface features
11
http://www.r-project.org/
284
Table 4: The experiment with Raykar et al.?s algorithm classes
Correctly Incorrectly Precision Recall F-measure
classified reviews classified reviews
False reviews 85 33 78.70% 72.03% 75.22%
True reviews 95 23
Total 180 56
Total accuracy 76.27%
Random baseline 62.29% 62.26% 66.95%
mentioned above, adopting as class the majority
voting, as suggested by Raykar et al. (2010).
The parameters ? and ? were estimated regard-
ing the three clues Cl - Cluster, NN - Nickname
and UP - Unknown Purchase. The attribute SB -
Suspect Book was not used, in order to carry out
the EM algorithm exclusively on heuristic data, re-
moving the information obtained through sources
external to the dataset. The parameters ? and ?
of the three clues were obtained not from ran-
dom classes, as the EM algorithm would allow, but
again comparing the clues? labels with the major-
ity voting class. In fact, aware of the local maxi-
mum problem of EM, in this way we tried to en-
hance the reliability of the results posing a config-
uration which could be, at least theoretically, bet-
ter than a completely random one.
Knowing these values for each instance of the
dataset, we computed the E-step and we updated
our parameters in M-step.
The E-step and the M-step were iterated 100
times, in which the log-likelihood increases mono-
tonically, indicating a convergence to a local max-
imum.
The final value of p
i
determined the new class of
each instance: if p
i
> .5 the review was labeled as
true, otherwise as false. In the end, the EM clus-
terization allowed to label 3267 reviews as false
and 3552 as true, that is 47.91% and 52.09% of
DEREV respectively.
3.2.3 Feature selection
The feature selection for this experiment was ex-
actly the same presented for the previous one in
Sub-section 3.1.2; the only, fundamental differ-
ence was that in the first experiment the classes
derived from the majority voting rule, while in
the second experiment the classes were identified
through the Raykar et al.?s strategy.
3.2.4 Baselines
As in the first experiment, we compared the per-
formance of the models with the same majority
and random baselines discussed in Sub-section
3.1.3.
3.2.5 Models
We used the classes determined through the Learn-
ing by Crowds algorithm to train SVMs models,
with the same settings employed in the first exper-
iments.
3.2.6 Results
Table 4 shows the results of the classifier trained
over the dataset whose the classes were identified
through the Raykar et al.?s algorithm.
4 Discussion
4.1 Deceptive language in reviews
Of the 4811 reviewers who wrote reviews included
in our corpus, about 900 were anonymous, and
only 16 wrote 10 or more reviews. If, in one hand,
this prevented us from verifying the performance
of the models with respect to particular reviewers,
on the other hand we had the opportunity of evalu-
ating the style in writing reviews across many sub-
jects.
In our experiments, we extracted simple surface
features constituted by short n-grams of lemmas
and part-of-speech. In literature there is evidence
that also other kinds of features are effective in de-
tecting deception in reviews: for example, infor-
mation about the syntactic structures of the texts
(Feng et al., 2012). In our pilot studies we did not
obtain improvements using syntactic features. But
even the frequency of n-grams can provide some
insight regarding deceptive language in reviews;
and with this aim we focused on the unigrams ap-
pearing more than 50 times in the 236 reviews
285
constituting the gold standard of DEREV, whose
un/truthfulness is known. The use of self-referred
pronouns and adjectives is remarkably different in
true and fake reviews: in the genuine ones, the pro-
nouns ?I?, ?my? and ?me? are found 371, 74 and 51
times respectively, while in the fake ones the pro-
noun ?I? is present only 149 and ?me? and ?my?
less than 50 times. This reduced number of self-
references is coherent with the findings of other
well-known studies regarding deception detection
(Newman et al., 2003); however, while in truthful
reviews the pronoun ?you? appears only 84 times,
in the fake ones the frequency of ?you? and ?your?
is 151 and 75. It seems that while the truth-tellers
simple state their opinions, the deceivers address
directly the reader. Probably they tend to give ad-
vice: after all, this is what they are paid for. The
frequency of the word ?read? - that is the activ-
ity simulated in fake reviews - is also quite imbal-
anced: 137 in true reviews and 97 in the fake ones.
Lastly, it is maybe surprising that in the false re-
views terms related to positive feelings/judgments
do not have the highest frequency; instead in truth-
ful reviews we found 52 times the term ?good?
(and 56 times the ambiguous term ?like?): also this
outcome is similar to that of the mentioned study
of Newman et al. (2003).
4.2 Estimating the gold standard
The estimation of the gold standard is a recur-
rent problem in many tasks of text classification
and in particular with deceptive review identifica-
tion, that is an application where the deceptiveness
of the reviews cannot be properly determined but
only heuristically assessed.
In this paper we introduced a new dataset for
studying deceptive reviews, constituted by 6819
instances whose 236 (that is about 3.5% of the cor-
pus) were labeled with the highest degree of confi-
dence ever seen before. We used this subset to test
the models that we trained on the other reviews of
DEREV, whose the class was heuristically deter-
mined.
With this purpose, we adopted two techniques.
First, we simply considered the value of our clues
of deception as outputs of just as many annotators,
and we assigned the classes to each review accord-
ing to majority voting. Then we clustered our in-
stances using the Learning from Crowd algorithm
proposed by Raykar et al. (2010). Lastly we car-
ried out the two experiments of text classification
described above.
The results suggest that both methods achieve
accuracy well above the baseline. However, the
models trained using Learning from Crowd classes
not only achieved the highest accuracy, but also
outperformed the thresholds for precision and re-
call in detecting deceptive reviews (Table 4), while
the models trained with the majority voting classes
showed a very high precision, but at the expense of
the recall, which was lower than the baseline (Ta-
ble 3).
Since the results even with simple majority vot-
ing classes were positive, we carried out two more
experiments, identical to those described above
except that we included in the feature set the three
deception clues Cluster - Cl, Nickname - NN and
Unknown Purchase - UP. Both with majority vot-
ing and with learning from Crowds classes, the ac-
curacy of the models exceeded 97%. This might
seem to suggest that those clues are very effective;
but given that the deception clues were used to de-
rive the silver standard, their use as features could
be considered to some extent circular (Subsection
2.4). Moreover, not all of our non-linguistic cues
may be found in all review scenarios, and therefore
the applicability of our methods to all review sce-
narios will have to be investigated. Specifically,
Cluster is likely to be applicable to most review
domains, Nickname and Unknown Purchase are
Amazon features that may or may not be adopted
by other services allowing users to provide re-
views. However, our main concern was not to
evaluate the effectiveness of these specific clues of
deception, but to investigate whether better strate-
gies for labeling instances than simple majority
voting could be found.
In this perspective, the performance of our
second experiment, in which the Learning from
Crowds algorithm was employed, stands out. In
fact in that case we tried to identify the classes of
the instances abstaining from making use of any
external information regarding the reviews: in par-
ticular, we ignored the Suspect Book - SB clue of
deception which, by contrast, took part in the cre-
ation of the majority voting classes.
This outcome suggests that, even in scenarios
where the gold standard is unknown, the Learning
from Crowds algorithm is a reliable tool for label-
ing the reviews, so that effective models can be
trained in order to classify them as truthful or not.
286
References
Bickel, P. and Doksum, K. (2000). Mathemati-
cal statistics: basic ideas and selected topics.
Number v. 1 in Mathematical Statistics: Basic
Ideas and Selected Topics. Prentice Hall.
Cortes, C. and Vapnik, V. (1995). Support-vector
networks. Machine Learning, 20.
Dempster, A. P., Laird, N. M., and Rubin, D. B.
(1977). Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the
Royal Statistical Society. Series B (Methodolog-
ical), 39(1):1?38.
Feng, S., Banerjee, R., and Choi, Y. (2012). Syn-
tactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 171?175, Jeju
Island, Korea. Association for Computational
Linguistics.
Gelman, A. and Hill, J. (2007). Data Analysis
Using Regression and Multilevel/Hierarchical
Models. Analytical Methods for Social Re-
search. Cambridge University Press.
Kullback, S. and Leibler, R. A. (1951). On in-
formation and sufficiency. Ann. Math. Statist.,
22(1):79?86.
Li, F., Huang, M., Yang, Y., and Zhu, X. (2011).
Learning to identify review spam. In Proceed-
ings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume
Volume Three, pages 2488?2493. AAAI Press.
Newman, M. L., Pennebaker, J. W., Berry, D. S.,
and Richards, J. M. (2003). Lying Words:
Predicting Deception From Linguistic Styles.
Personality and Social Psychology Bulletin,
29(5):665?675.
Ott, M., Choi, Y., Cardie, C., and Hancock, J.
(2001). Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 309?319, Portland, Ore-
gon, USA. Association for Computational Lin-
guistics.
Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H.,
Florin, C., Bogoni, L., and Moy, L. (2010).
Learning from crowds. Journal of Machine
Learning Research, 11:1297?1322.
Schmid, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings
of International Conference on New Methods in
Language Processing.
Sigler, L., editor (2003). Fibonacci?s Liber Abaci:
A Translation Into Modern English of Leonardo
Pisano?s Book of Calculation. Sources and
Studies in the History of Mathematics and Phys-
ical Sciences. Springer Verlag.
Strapparava, C. and Mihalcea, R. (2009). The Lie
Detector: Explorations in the Automatic Recog-
nition of Deceptive Language. In Proceed-
ing ACLShort ?09 - Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers.
Yang, Y. and Liu, X. (1999). A re-examination of
text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR con-
ference on Research and development in infor-
mation retrieval, SIGIR ?99, pages 42?49, New
York, NY, USA. ACM.
Yang, Y. and Pedersen, J. O. (1997). A
comparative study on feature selection in
text categorization. CiteSeerX - Scientific
Literature Digital Library and Search En-
gine [http://citeseerx.ist.psu.edu/oai2] (United
States).
287
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 39?47,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
On the Use of Homogenous Sets of Subjects in Deceptive Language
Analysis
Tommaso Fornaciari
Center for Mind/Brain Sciences
University of Trento
tommaso.fornaciari@unitn.it
Massimo Poesio
Language and Computation Group
University of Essex
Center for Mind/Brain Sciences
University of Trento
massimo.poesio@unitn.it
Abstract
Recent studies on deceptive language sug-
gest that machine learning algorithms can
be employed with good results for classi-
fication of texts as truthful or untruthful.
However, the models presented so far do
not attempt to take advantage of the dif-
ferences between subjects. In this paper,
models have been trained in order to clas-
sify statements issued in Court as false or
not-false, not only taking into considera-
tion the whole corpus, but also by identify-
ing more homogenous subsets of producers
of deceptive language. The results suggest
that the models are effective in recogniz-
ing false statements, and their performance
can be improved if subsets of homogeneous
data are provided.
1 Introduction
Detecting deceptive communication is a challeng-
ing task, but one that could have a number of use-
ful applications. A wide variety of approaches to
the discovery of deceptive statements have been
attempted, ranging from using physiological sen-
sors such as lie detectors to using neuroscience
methods (Davatzikos et al, 2005; Ganis et al,
2003). More recently, a number of techniques
have been developed for recognizing deception
on the basis of the communicative behavior of
subjects. Given the difficulty of the task, many
such methods rely on both verbal and non-verbal
behavior, to increase accuracy. So for instance
De Paulo et al (2003) considered more than 150
cues, verbal and non-verbal, directly observed
through experimental subjects. But finding clues
indicating deception through manual inspection is
not easy. De Paulo et al asserted that ?behaviors
that are indicative of deception can be indicative
of other states and processes as well?.
The same point is made in more recent liter-
ature: thus Frank et al (2008) write ?We find
that there is no clue or clue pattern that is spe-
cific to deception, although there are clues spe-
cific to emotion and cognition?, and they wish
for ?real-world databases, identifying base rates
for malfeasant behavior in security settings, opti-
mizing training, and identifying preexisting excel-
lence within security organizations?. Jensen et al
(2010) exploited cues coming from audio, video
and textual data.
One solution is to let statistical and machine
learning methods discover the clues. Work such
as Fornaciari and Poesio (2011a,b); Newman et al
(2003); Strapparava and Mihalcea (2009) sug-
gests that these techniques can perform reason-
ably well at the task of discovering deception
even just from linguistic data, provided that cor-
pora containing examples of deceptive and truth-
ful texts are available. The availability of such
corpora is not a trivial problem, and indeed, the
creation of a realistic such corpus is one of the
problems in which we invested substantial effort
in our own previous work, as discussed in Section
3.
In the work discussed in this paper, we tackle
an issue which to our knowledge has not been
addressed before, due to the limitations of the
datasets previously available: this is whether the
individual difference between experimental sub-
jects affect deception detection. In previous work,
lexical (Fornaciari and Poesio, 2011a) and surface
(Fornaciari and Poesio, 2011b) features were em-
ployed to classify deceptive statements issued in
Italian Courts. In this study, we report the results
39
of experiments in which our methods were trained
either over the whole corpus or over smaller sub-
sets consisting of the utterances produced by more
homogenous subsets of subjects. These subsets
were identified either automatically, by cluster-
ing subjects according to their language profile,
or by using meta-information about the subjects
included in the corpus, such as their gender.
The structure of the paper is as follows. In Sec-
tion 2 some background knowledge is introduced.
In Section 3 the data set is described. In Section 4
we discuss our machine learning and experimen-
tal methods. Finally, the results are presented in
Section 5 and discussed in Section 6.
2 Background
2.1 Deceptive language analysis
From a methodological point of view, to investi-
gate deceptive language gives rise to some tricky
issues: first of all, the strategy chosen to collect
data. The literature can be divided in two main
families of studies:
? Field studies;
? Laboratory studies.
The first ones are usually interesting in forensic
applications but in such studies verifying the sin-
cerity of the statements is often complicated (Vrij,
2005). Laboratory studies, instead, are character-
ized by the artificiality of participants? psycholog-
ical conditions: therefore their findings may not
be generalized to deception encountered in real
life.
Due to practical difficulties in collection and
annotation of suitable data, in literature finding
papers in which real life linguistic data are em-
ployed, where truthfulness is surely known, is
less common and Zhou et al (2008) complain
about the lack of ?data set for evaluating decep-
tion detection models?. Just recently some studies
tried to fill this gap, concerning both the English
(Bachenko et al, 2008; Fitzpatrick and Bachenko,
2009) and Italian language (Fornaciari and Poe-
sio, 2011a,b). Just the studies on Italian language
come from data which have constituted the first
nucleus of the corpus analysed here.
2.2 Stylometry
Our own work and that of other authors that re-
cently employed machine learning techniques to
detect deception in text employs techniques very
similar to that of stylometry. Stylometry is a dis-
cipline which studies texts on the basis of their
stylistic features, usually in order to attribute them
to an author - giving rise to the branch of author
attribution - or to get information about the author
himself - this is the field of author profiling.
Stylometric analyses, which relies mainly on
machine learning algorithms, turned out to be ef-
fective in several forensic tasks: not only the clas-
sical field of author profiling (Coulthard, 2004;
Koppel et al, 2006; Peersman et al, 2011; Solan
and Tiersma, 2004) and author attribution (Luy-
ckx and Daelemans, 2008; Mosteller and Wallace,
1964), but also emotion detection (Vaassen and
Daelemans, 2011) and plagiarism analysis (Stein
et al, 2007). Therefore, from a methodological
point of view, Deceptive Language Analysis is a
particular application of stylometry, exactly like
other branches of Forensic Linguistics.
3 Data set
3.1 False testimonies in Court
In order to study deceptive language, we created
the DECOUR - DEception in COURt - corpus,
better described in Fornaciari and Poesio (2012).
DECOUR is a corpus constituted by the tran-
scripts of 35 hearings held in four Italian Courts:
Bologna, Bolzano, Prato and Trento. These tran-
scripts report verbatim the statements issued by a
total of 31 different subjects - four of which have
been heard twice. All the hearings come from
criminal proceedings for calumny and false tes-
timony (artt. 368 and 372 of the Italian Criminal
Code).
In particular, the hearings of DECOUR come
mainly from two situations:
? the defendant for any criminal proceeding
tries to use calumny against someone;
? a witness in any criminal proceeding lies for
some reason.
In both cases, a new criminal proceeding arises,
in which the subjects can issue new statements or
not, and having as a body of evidence the tran-
script of the hearing held in the previous proceed-
ing.
The crucial point is that DECOUR only in-
cludes text from individuals who in the end have
been found guilty. Hence the proceeding ends
40
with a judgment of the Court which summarize
the facts, pointing out precisely the lies told by
the speaker in order to establish his punishment.
Thanks to the transcripts of the hearing and to the
final judgment of the Court, it is possible to anno-
tate the statements of the speakers on the basis of
their truthfulness or untruthfulness, as follows.
3.2 Annotation and agreement
The hearings are dialogs, in which the judge, the
public prosecutor and the lawyer pose questions
to the witness/defendant who in turn has to give
them answers. These answers are the object of
investigation of this study. Each answer is con-
sidered a turn, delimited by the end of the pre-
vious and the beginning of the following inter-
vention of another individual. Each turn is con-
stituted by one or more utterances, delimited by
punctuation marks: period, triple-dots, question
and exclamation marks. Utterances are the anal-
ysis unit of DECOUR and have been annotated as
false, true or uncertain. In order to verify the
agreement in the judgments about truthfulness or
untruthfulness of the utterances, three annotators
separately annotated about 600 utterances. The
agreement study concerning the three classes of
utterances, described in detail in (Fornaciari and
Poesio, 2012), showed that the agreement value
was k=.57. Instead, if the problem is reduced to
a binary task - that is, if true and uncertain utter-
ances are collapsed into a single category of not-
false utterances, opposed to the category of false
ones - the agreement value is k=.64.
3.3 Corpus statistics
The whole corpus has been tokenized and sensi-
tive data have been made anonymous, according
to the previous agreement with the Courts. Then
DECOUR has been lemmatized and POS-tagged
using a version of TreeTagger1 (Schmid, 1994)
trained for Italian.
DECOUR is made up of 3015 utterances, which
come from 2094 turns. 945 utterances have been
annotated as false, 1202 as true and 868 as un-
certain. The size of DECOUR is 41819 tokens,
including punctuation blocks.
1http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
4 Methods
In this Section we first summarize our classifica-
tion methods from previous work, then discuss the
three experiments we carried out.
4.1 Classification methods
Each utterance is described by a feature vector.
As in our previous studies (Fornaciari and Poesio,
2011a,b) three kinds of features were used.
First of all, the feature vectors include very ba-
sic linguistic information such as the length of ut-
terances (with and without punctuation) and the
number of words longer than six letters.
The second type of information are lexical fea-
tures. These features have been collected mak-
ing use of LIWC - Linguistic Inquiry and Word
Count, a linguistic tool realized by Pennebaker
et al (2001) and widely employed in deception
detection (Newman et al, 2003; Strapparava and
Mihalcea, 2009). LIWC is based on a dictionary
in which each term is associated with an appro-
priate set of syntactical, semantical and/or psy-
chological categories. When a text is analysed
with LIWC, the tokens of the text are compared
with the LIWC dictionary. Every time a word
present in the dictionary is found, the count of
the corresponding categories grows. The output
is a profile of the text which relies on the rate of
incidence of the different categories in the text it-
self. LIWC also includes different dictionaries for
several languages, amongst which Italian (Agosti
and Rellini, 2007). Therefore it has been possi-
ble to apply LIWC to Italian deceptive texts, and
the approximate 80 linguistic dimensions which
constitute the Italian LIWC dictionary have been
included as features of the vectors.
Lastly, frequencies of lemmas and part-of-
speech n-grams were used. Five kinds of n-
grams of lemmas and part-of-speech were taken
into consideration: from unigrams to pentagrams.
These frequency lists come from the part of DE-
COUR employed as training set. More precisely,
they come from the utterances held as true or false
of the training set, while the uncertain utterances
have not been considered. In order to empha-
size the collection of features effective in clas-
sifying true and false statements, frequency lists
of n-grams have been built considering true and
false utterances separately. This means that, in
the training set, homologous frequency lists of n-
41
Table 1: The most frequent n-grams collected
N-grams Lemmas POS Total
Unigrams 50 15
Bigrams 40 12
Trigrams 30 9
Tetragrams 20 6
Pentagrams 10 3
Total 150 45 195
grams - unigrams, bigrams and so on - have been
collected from the subset of true utterances and
form the subset of false ones. From these lists,
the most frequent n-grams have been collected, in
a decreasing amount according to the length of the
n-grams. Table 1 shows in detail the number of
the most frequent lemmas and part-of-speech col-
lected for the different n-grams. Then the couples
of frequency lists were merged into one.
This procedure implies that the number of sur-
face features is not determined a priori. In fact
the 195 features indicated in Table 1, which are
collected from true and false utterances, are uni-
fied in a list where each feature has to appear
only once. Therefore, theoretically in the case of
perfect identity of features in true and false ut-
terances, a final list with the same 195 features
would be obtained. In the opposite case, if the
n-grams from true and false utterances would be
completely different, a list of 195 + 195, then 390
n-grams would result. The aim of this procedure
is to get a list of n-grams which could be as much
as possible representative of the features of true
and false utterances. Obviously, the smaller the
overlap of the features of the two subsets, the
greater the difference in the appearance of true
and false utterances, and greater the hope to reach
a good performance in the classification task.
We used the Support Vector Machine imple-
mentation in R (Dimitriadou et al, 2011). As
specified above, the classes of the utterances are
false vs. not-false, where the category of not-false
utterances results from the union of the true and
uncertain ones.
4.2 Corpus division
With the aim of training models able to classify
the utterances of DECOUR as false or not-false,
the corpus has been divided as follows:
Training set The 20 hearings coming from the
Courts of Bologna and Bolzano have been
employed as training set. In terms of anal-
ysis units, this means 2279 utterances, that
is 75.59% of DECOUR. The features of the
vectors come from this set of data.
Test set The 9 hearings of the Court of Trento
have been employed as test set, in order to
evaluate the effectiveness of the trained mod-
els. This test set was made up by 426 utter-
ances, which are 14.13% of DECOUR.
Development set The 6 hearings of the Court of
Prato have been employed as development
set during the phase of choice and calibration
of vector features, therefore this set of utter-
ances is not directly involved in the results of
the following experiments. The develpment
set was constituted by 310 utterances, that is
10.28% of DECOUR.
In the various experimental conditions, some sub-
sets of DECOUR have been taken into consider-
ation. Hence, different hearings have been re-
moved from the test and/or training set in order
to carry out different experiments. Since the test
sets vary in the different experiments, in relation
to each of them different chance levels have been
determined, in order to evaluate the effectiveness
of the models? performance.
4.3 Experiments
Three experiments were carried out. In the first
experiment, the entire corpus was used to train
and test our algorithms. In the second and third
experiment, sub-corpora were identified.
4.3.1 Experiment 1: whole test set
In the first experiment, the classification task
has been carried out simply employing the train-
ing set and the test set as described above, in order
to have a control as reference point in relation to
the following experiments.
4.3.2 Experiment 2: no outliers
In the second experiment, a more homogeneous
subset of DECOUR was obtained by automati-
cally identifying and removing outliers. This was
done in an unsupervised way by building vector
descriptions of the hearings and clustering them.
The features of these vectors were the same n-
grams described above, collected from the whole
42
Figure 1: Multi-Dimensional Scaling of DE-
COURE?ach entity corresponds to a hearing; the letters
represent the sex of the speakers.
corpus (not from the only test set); their values
were the mean values of the frequencies of the ut-
terances belonging to the hearing.
This data set has been transformed into a ma-
trix of between-hearing distances and a Multi-
Dimensional Scaling - MDS function has been
applied to this matrix (Baayen, 2008). Figure 1
shows the plot of MDS function. Each entity cor-
responds to a hearing, and is represented by a let-
ter indicating the sex of the speaker. Getting a
glimpse at Figure 1, it is possible to notice that,
in general, almost all the hearings are quite close
- that is, similar - to each other. Only three hear-
ings seem to be clearly more peripheral than all
the others, particularly the three most to the left in
Figure 1. These hearings have been considered as
outliers and shut out from the experiment. They
are two hearings from Trento and one from Prato.
In practice, it means that the training set, com-
ing from the hearings of Bologna and Bolzano,
remained the same as the previous experiment,
while two hearings have been removed from the
test set, which was constituted only by the hear-
ings of Trento.
4.3.3 Experiment 3: only male speakers
Different from the previous one, the third ex-
periment does not rely on a subset of data au-
tomatically identified. Instead, the subset comes
from personal information concerning the sub-
jects involved in the hearings. In fact, their sex,
place of birth and age at the moment of the hear-
ing are known. In this paper, places of birth
and age have not been taken into consideration,
since grouping them together in reliable cate-
gories raises issues that do not have a straightfor-
ward solution, and the size of the subsets of cor-
pus which would be obtained must be taken into
account.
Therefore this experiment has been carried out
taking into consideration only the sex of the sub-
jects, and in particular it concerned only the hear-
ings involving men. This meant reducing the
training set consistently, where seven hearings of
women were present and thence removed. Instead
from the test set just three hearings have been
taken off, one involving a woman and two involv-
ing a transsexual.
4.4 Baselines
The chance levels for the various test sets have
been calculated through Monte Carlo simula-
tions, each one specific to every experiment. In
each simulation, 100000 times a number of ran-
dom predictions has been produced, in the same
amount and with the same rate of false utterances
of the test set employed in the single experiment.
Then this random output was compared to the real
sequence of false and not-false utterances of the
test set, in order to count the amount of correct
predictions. The rate of correct answers reached
by less than 0.01% of the random predictions has
been accepted as chance threshold for every ex-
periment.
As a baseline, a simple majority baseline was
computed: to classify each utterance as belonging
to the most numerous class in the test set (not-
false).
5 Results
The test set of the first experiemnt, carried out
on the whole test set, was made up of 426 utter-
ances, of which 190 were false, that is 44.60%.
While the majority baseline is 55.40% of accu-
racy, a Monte Carlo simulation applied to the test
set showed that the chance level was 59.60% of
correct predictions. The results are shown in Ta-
ble 2. The overall accuracy - almost 66% - is
clearly above the chance level, being more than
six points greater than the baseline.
43
Table 2: Whole training and test set
Correctly Incorrectly
classified entities classified entities Precision Recall F-measure
False utterances 59 131 80.82% 31.05% 44.86%
True utterances 222 14 62.89% 94.07% 75.38%
Total 281 145
Total percent 65.96% 34.04%
Monte Carlo simulation 59.60%
Majority baseline 55.40%
Table 3: Test set without outliers
Correctly Incorrectly
classified entities classified entities Precision Recall F-measure
False utterances 51 90 80.95% 36.17% 50.00%
True utterances 180 12 66.67% 93.75% 77.92%
Total 231 102
Total percent 69.37% 30.63%
Monte Carlo simulation 61.26%
Majority baseline 57.66%
Table 4: Training and test set with only male speakers
Correctly Incorrectly
classified entities classified entities Precision Recall F-measure
False utterances 32 85 74.42% 27.35% 40.00%
True utterances 179 11 67.80% 94.21% 78.85%
Total 211 96
Total percent 68.73% 31.27%
Monte Carlo simulation 63.19%
Majority baseline 61.89%
In the second experiment, the test set without
outliers was made up of 333 utterances; 141 were
false, which means 42.34% of the test set. The
majority baseline was then at 57.66%, while the
chance threshold determined with a Monte Carlo
simulation had an accuracy rate of 61.26%. Ta-
ble 3 shows the results of the analyses. Taking the
outliers out of the test set alows tthe best perfor-
mance of the three experiments to be reached. In
fact the accuracy is more than 69%, which is more
than eight points above the highest chance level of
61.26%.
In the third experimental condition, where only
male speakers were considered, the training set
was made up of 13 hearings and the test set of
6 hearings. The utterances in the test set were
307, of which 117 were false, meaning 38.11%
of the test set. In this last case, the majority base-
line is at 61.89% of accuracy, while according to
a Monte Carlo simulation the chance level was
63.19%. The overall accuracy reached in this ex-
periment, shown in Table 4, was more than 68%:
higher than the first experiment, but in this case
the lower amount of false utterances in the test
set led to higher chance thresholds. Therefore the
difference between performance and the chance
44
level of 63.19% is now the smallest of all the ex-
periments: just five points and half.
From the point of view of detection of false
utterances, although with internal differences, all
the experiments are placed in the same reference
frame. In particular, the weak point in perfor-
mance is always the recall of false utterances,
which remains more or less at 30%. Instead the
good news comes from the precision in recogniz-
ing them, which is close to 80%. Regarding true
utterances, the recall is always good, being never
lower than 93%, while the precision is close to
65%.
6 Discussion
The goal of this paper was to verify if restricting
the analysis to more homogeneous subsets could
improve the accuracy of our models. The results
are mixed. On the one end, taking the outliers out
of the corpus results in a remarkable improvement
of accuracy in the classification task, in relation
to the performance of the models tested on the
whole test set. On the other end, in other cases
- most clearly, considering only speakers of the
male gender - we find no difference; our hypoth-
esis is that any potential advantage derived from
the increased homogeneity is offset by the reduc-
tion in training material (seven hearings are re-
moved in this case). So the conclusion may be
that increasing homogeneity is effective provided
that the remaining set is still sufficiently large.
Regarding the models? capacity to detect false
rather than true utterances, the difference between
the respective recalls is noteworthy. In fact, while
the recall of not-false utterances is very high, that
of false ones is poor. In other words, the results
indicate that an amount of false utterances is ef-
fectively so similar to the not-false ones, that the
models are not able to detect them. One challenge
for future studies is surely to find a way to detect
some aspect currently neglected of deceptive lan-
guage, which could be employed to widen the size
of false utterances which can be recognized.
On the other hand, in the two more reliable ex-
periments the precision in detecting false utter-
ances was about 80%. This could suggest that an
amount of false utterances exists, whose features
are in some way peculiar and different from not-
false ones. The data seem to show that this subset
could be more or less one third of all the false ut-
terances.
However, this study was not aimed to estimate
the possible performance of the models in an hy-
pothetic practical application. The experimental
conditions taken into consideration, in fact, are
considerably different from those that would be
present in a real life analysis.
The main reason of this difference is that in a
real case to classify every utterance of a hearing
would not be requested. A lot of statements are ir-
relevant or perfectly known as true. Furthermore
it would not make sense to classify all the utter-
ances which have not propositional value, such as
questions or meta-communicative acts. In the per-
spective of deception detection in a real life sce-
nario, to classify this last kind of utterances is use-
less. Only a subset of the propositional statements
should be classified. In a previous study, carried
out on a selection of utterances with propositional
value of a part of DECOUR, machine learning
models reached an accuracy of 75% in classifica-
tion task (Fornaciari and Poesio, 2011b). In that
study, precision and recall of false utterances are
also quite similar to those of this study, the first
being about 90% and the second about 50%.
From a theoretical point of view, the present
study suggests that it is possible to be relatively
confident in the effectiveness of the models in the
analysis of any kind of utterance. This means
that deceptive language is at least in part differ-
ent from the truthful one and stylometric analyses
can detect it. If this is true, the rate of precision
with which false statements are correctly classi-
fied should clearly exceed the chance level.
Also in this case, Monte Carlo simulation is
taken as reference point. Out of the 100000 ran-
dom trials carried out to determine the baseline for
the first experiment, less than 0.01% had a preci-
sion greater than 57.90% in classifying false ut-
terances, in front of a precision of the models at
80.82%. Regarding the second experiment, the
threshold for precision related to false utterances
was 58.15% against a precision of the models at
80.95%. In the third experiment, the baseline
for precision was 55.55% and the performance of
models was 74.42%. In every experiment the gap
is about twenty points per cent. The same cannot
be said about the recall of false utterances: the
baselines of Monte Carlo simulations in the three
experiments were about 51-54%, while the best
models? performance (of the second experiment)
did not exceed 36%.
45
The precision reached in recognizing false
statements shows that the models were reliable
in detection of deceptive language. On the other
hand a remarkable amount of false utterances was
not identified. The challenge for the future is to
understand to which extent it will be possible to
improve the recall in detecting false utterances,
not losing and hopefully improving the relative
precision. At that point, although in specific con-
texts, a computational linguistics? approach could
be really employed to detect deception in real life
scenarios.
7 Acknowledgements
To create DECOUR has been very complex, and
it would not have been possible without the kind
collaboration of a lot of people. Many thanks to
Dr. Francesco Scutellari, President of the Court
of Bologna, to Dr. Heinrich Zanon, President of
the Court of Bolzano, to Dr. Francesco Antonio
Genovese, President of the Court of Prato and to
Dr. Sabino Giarrusso, President of the Court of
Trento.
References
Agosti, A. and Rellini, A. (2007). The Italian
LIWC Dictionary. Technical report, LIWC.net,
Austin, TX.
Baayen, R. (2008). Analyzing linguistic data:
a practical introduction to statistics using R.
Cambridge University Press.
Bachenko, J., Fitzpatrick, E., and Schonwetter,
M. (2008). Verification and implementation
of language-based deception indicators in civil
and criminal narratives. In Proceedings of the
22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08,
pages 41?48, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Coulthard, M. (2004). Author identification, idi-
olect, and linguistic uniqueness. Applied Lin-
guistics, 25(4):431?447.
Davatzikos, C., Ruparel, K., Fan, Y., Shen, D.,
Acharyya, M., Loughead, J., Gur, R., and Lan-
gleben, D. (2005). Classifying spatial patterns
of brain activity with machine learning meth-
ods: Application to lie detection. NeuroImage,
28(3):663 ? 668.
De Paulo, B. M., Lindsay, J. J., Malone, B. E.,
Muhlenbruck, L., Charlton, K., and Cooper, H.
(2003). Cues to deception. Psychological Bul-
letin, 129(1):74?118.
Dimitriadou, E., Hornik, K., Leisch, F., Meyer,
D., and Weingessel, A. (2011). r-cran-
e1071. http://mloss.org/software/
view/94/.
Fitzpatrick, E. and Bachenko, J. (2009). Building
a forensic corpus to test language-based indi-
cators of deception. Language and Computers,
71(1):183?196.
Fornaciari, T. and Poesio, M. (2011a). Lexical
vs. surface features in deceptive language anal-
ysis. In Proceedings of the ICAIL 2011 Work-
shop Applying Human Language Technology to
the Law, AHLTL 2011, pages 2?8, Pittsburgh,
USA.
Fornaciari, T. and Poesio, M. (2011b). Sin-
cere and deceptive statements in italian crimi-
nal proceedings. In Proceedings of the Interna-
tional Association of Forensic Linguists Tenth
Biennial Conference, IAFL 2011, Cardiff,
Wales, UK.
Fornaciari, T. and Poesio, M. (2012). Decour: a
corpus of deceptive statements in italian courts.
In Proceedings of the eighth International Con-
ference on Language Resources and Evalua-
tion, LREC 2012. In press.
Frank, M. G., Menasco, M. A., and O?Sullivan,
M. (2008). Human behavior and deception de-
tection. In Voeller, J. G., editor, Wiley Hand-
book of Science and Technology for Homeland
Security. John Wiley & Sons, Inc.
Ganis, G., Kosslyn, S., Stose, S., Thompson, W.,
and Yurgelun-Todd, D. (2003). Neural corre-
lates of different types of deception: An fmri
investigation. Cerebral Cortex, 13(8):830?836.
Jensen, M. L., Meservy, T. O., Burgoon, J. K., and
Nunamaker, J. F. (2010). Automatic, Multi-
modal Evaluation of Human Interaction. Group
Decision and Negotiation, 19(4):367?389.
Koppel, M., Schler, J., Argamon, S., and Pen-
nebaker, J. (2006). Effects of age and gender on
blogging. In AAAI 2006 Spring Symposium on
Computational Approaches to Analysing We-
blogs.
46
Luyckx, K. and Daelemans, W. (2008). Author-
ship attribution and verification with many au-
thors and limited data. In Proceedings of the
22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08,
pages 513?520, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Mosteller, F. and Wallace, D. (1964). Infer-
ence and Disputed Authorship: The Federalist.
Addison-Wesley.
Newman, M. L., Pennebaker, J. W., Berry, D. S.,
and Richards, J. M. (2003). Lying Words:
Predicting Deception From Linguistic Styles.
Personality and Social Psychology Bulletin,
29(5):665?675.
Peersman, C., Daelemans, W., and Van Vaeren-
bergh, L. (2011). Age and gender prediction on
netlog data. Presented at the 21st Meeting of
Computational Linguistics in the Netherlands
(CLIN21), Ghent, Belgium.
Pennebaker, J. W., Francis, M. E., and Booth, R. J.
(2001). Linguistic Inquiry and Word Count
(LIWC): LIWC2001. Lawrence Erlbaum As-
sociates, Mahwah.
Schmid, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in
Language Processing.
Solan, L. M. and Tiersma, P. M. (2004). Author
identification in american courts. Applied Lin-
guistics, 25(4):448?465.
Stein, B., Koppel, M., and Stamatatos, E. (2007).
Plagiarism analysis, authorship identification,
and near-duplicate detection pan?07. SIGIR Fo-
rum, 41:68?71.
Strapparava, C. and Mihalcea, R. (2009). The
Lie Detector: Explorations in the Automatic
Recognition of Deceptive Language. In Pro-
ceeding ACLShort ?09 - Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers.
Vaassen, F. and Daelemans, W. (2011). Auto-
matic emotion classification for interpersonal
communication. In 2nd Workshop on Compu-
tational Approaches to Subjectivity and Senti-
ment Analysis (WASSA 2.011).
Vrij, A. (2005). Criteria-based content analysis
- A Qualitative Review of the First 37 Studies.
Psychology, Public Policy, and Law, 11(1):3?
41.
Zhou, L., Shi, Y., and Zhang, D. (2008). A
Statistical Language Modeling Approach to
Online Deception Detection. IEEE Transac-
tions on Knowledge and Data Engineering,
20(8):1077?1081.
47
