Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650?659,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Joint Unsupervised Coreference Resolution with Markov Logic
Hoifung Poon Pedro Domingos
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350, U.S.A.
{hoifung,pedrod}@cs.washington.edu
Abstract
Machine learning approaches to coreference
resolution are typically supervised, and re-
quire expensive labeled data. Some unsuper-
vised approaches have been proposed (e.g.,
Haghighi and Klein (2007)), but they are less
accurate. In this paper, we present the first un-
supervised approach that is competitive with
supervised ones. This is made possible by
performing joint inference across mentions,
in contrast to the pairwise classification typ-
ically used in supervised methods, and by us-
ingMarkov logic as a representation language,
which enables us to easily express relations
like apposition and predicate nominals. On
MUC and ACE datasets, our model outper-
forms Haghigi and Klein?s one using only a
fraction of the training data, and often matches
or exceeds the accuracy of state-of-the-art su-
pervised models.
1 Introduction
The goal of coreference resolution is to identify
mentions (typically noun phrases) that refer to the
same entities. This is a key subtask in many NLP
applications, including information extraction, ques-
tion answering, machine translation, and others. Su-
pervised learning approaches treat the problem as
one of classification: for each pair of mentions,
predict whether they corefer or not (e.g., McCal-
lum & Wellner (2005)). While successful, these
approaches require labeled training data, consisting
of mention pairs and the correct decisions for them.
This limits their applicability.
Unsupervised approaches are attractive due to the
availability of large quantities of unlabeled text.
However, unsupervised coreference resolution is
much more difficult. Haghighi and Klein?s (2007)
model, the most sophisticated to date, still lags su-
pervised ones by a substantial margin. Extending it
appears difficult, due to the limitations of its Dirich-
let process-based representation.
The lack of label information in unsupervised
coreference resolution can potentially be overcome
by performing joint inference, which leverages the
?easy? decisions to help make related ?hard? ones.
Relations that have been exploited in supervised
coreference resolution include transitivity (McCal-
lum & Wellner, 2005) and anaphoricity (Denis &
Baldridge, 2007). However, there is little work to
date on joint inference for unsupervised resolution.
We address this problem using Markov logic,
a powerful and flexible language that combines
probabilistic graphical models and first-order logic
(Richardson & Domingos, 2006). Markov logic
allows us to easily build models involving rela-
tions among mentions, like apposition and predi-
cate nominals. By extending the state-of-the-art al-
gorithms for inference and learning, we developed
the first general-purpose unsupervised learning al-
gorithm for Markov logic, and applied it to unsuper-
vised coreference resolution.
We test our approach on standard MUC and ACE
datasets. Our basic model, trained on a minimum
of data, suffices to outperform Haghighi and Klein?s
(2007) one. Our full model, using apposition and
other relations for joint inference, is often as accu-
rate as the best supervised models, or more.
650
We begin by reviewing the necessary background
on Markov logic. We then describe our Markov
logic network for joint unsupervised coreference
resolution, and the learning and inference algorithms
we used. Finally, we present our experiments and re-
sults.
2 Related Work
Most existing supervised learning approaches for
coreference resolution are suboptimal since they re-
solve each mention pair independently, only impos-
ing transitivity in postprocessing (Ng, 2005). More-
over, many of them break up the resolution step into
subtasks (e.g., first determine whether a mention is
anaphoric, then classify whether it is coreferent with
an antecedent), which further forsakes opportunities
for joint inference that have been shown to be help-
ful (Poon & Domingos, 2007). Using graph parti-
tioning, McCallum & Wellner (2005) incorporated
transitivity into pairwise classification and achieved
the state-of-the-art result on the MUC-6 dataset, but
their approach can only leverage one binary relation
at a time, not arbitrary relations among mentions.
Denis & Baldridge (2007) determined anaphoricity
and pairwise classification jointly using integer pro-
gramming, but they did not incorporate transitivity
or other relations.
While potentially more appealing, unsupervised
learning is very challenging, and unsupervised
coreference resolution systems are still rare to this
date. Prior to our work, the best performance in
unsupervised coreference resolution was achieved
by Haghighi & Klein (2007), using a nonparamet-
ric Bayesian model based on hierarchical Dirichlet
processes. At the heart of their system is a mixture
model with a few linguistically motivated features
such as head words, entity properties and salience.
Their approach is a major step forward in unsuper-
vised coreference resolution, but extending it is chal-
lenging. The main advantage of Dirichlet processes
is that they are exchangeable, allowing parameters
to be integrated out, but Haghighi and Klein forgo
this when they introduce salience. Their model thus
requires Gibbs sampling over both assignments and
parameters, which can be very expensive. Haghighi
and Klein circumvent this by making approxima-
tions that potentially hurt accuracy. At the same
time, the Dirichlet process prior favors skewed clus-
ter sizes and a number of clusters that grows loga-
rithmically with the number of data points, neither of
which seems generally appropriate for coreference
resolution.
Further, deterministic or strong non-deterministic
dependencies cause Gibbs sampling to break down
(Poon & Domingos, 2006), making it difficult to
leverage many linguistic regularities. For exam-
ple, apposition (as in ?Bill Gates, the chairman of
Microsoft?) suggests coreference, and thus the two
mentions it relates should always be placed in the
same cluster. However, Gibbs sampling can only
move one mention at a time from one cluster to
another, and this is unlikely to happen, because it
would require breaking the apposition rule. Blocked
sampling can alleviate this problem by sampling
multiple mentions together, but it requires that the
block size be predetermined to a small fixed number.
When we incorporate apposition and other regular-
ities the blocks can become arbitrarily large, mak-
ing this infeasible. For example, suppose we also
want to leverage predicate nominals (i.e., the sub-
ject and the predicating noun of a copular verb are
likely coreferent). Then a sentence like ?He is Bill
Gates, the chairman of Microsoft? requires a block
of four mentions: ?He?, ?Bill Gates?, ?the chair-
man of Microsoft?, and ?Bill Gates, the chairman
of Microsoft?. Similar difficulties occur with other
inference methods. Thus, extending Haghighi and
Klein?s model to include richer linguistic features is
a challenging problem.
Our approach is instead based on Markov logic,
a powerful representation for joint inference with
uncertainty (Richardson & Domingos, 2006). Like
Haghighi and Klein?s, our model is cluster-based
rather than pairwise, and implicitly imposes tran-
sitivity. We do not predetermine anaphoricity of a
mention, but rather fuse it into the integrated reso-
lution process. As a result, our model is inherently
joint among mentions and subtasks. It shares sev-
eral features with Haghighi & Klein?s model, but re-
moves or refines features where we believe it is ap-
propriate to. Most importantly, our model leverages
apposition and predicate nominals, which Haghighi
& Klein did not use. We show that this can be done
very easily in our framework, and yet results in very
substantial accuracy gains.
651
It is worth noticing that Markov logic is also well
suited for joint inference in supervised systems (e.g.,
transitivity, which tookMcCallum&Wellner (2005)
nontrivial effort to incorporate, can be handled in
Markov logic with the addition of a single formula
(Poon & Domingos, 2008)).
3 Markov Logic
In many NLP applications, there exist rich relations
among objects, and recent work in statistical rela-
tional learning (Getoor & Taskar, 2007) and struc-
tured prediction (Bakir et al, 2007) has shown that
leveraging these can greatly improve accuracy. One
of the most powerful representations for joint infer-
ence is Markov logic, a probabilistic extension of
first-order logic (Richardson & Domingos, 2006). A
Markov logic network (MLN) is a set of weighted
first-order clauses. Together with a set of con-
stants, it defines a Markov network with one node
per ground atom and one feature per ground clause.
The weight of a feature is the weight of the first-
order clause that originated it. The probability of
a state x in such a network is given by P (x) =
(1/Z) exp (
?
i wifi(x)), where Z is a normaliza-
tion constant, wi is the weight of the ith clause,
fi = 1 if the ith clause is true, and fi = 0 other-
wise.
Markov logic makes it possible to compactly
specify probability distributions over complex re-
lational domains. Efficient inference can be per-
formed using MC-SAT (Poon & Domingos, 2006).
MC-SAT is a ?slice sampling? Markov chain Monte
Carlo algorithm. Slice sampling introduces auxil-
iary variables u that decouple the original ones x,
and alternately samples u conditioned on x and vice-
versa. To sample from the slice (the set of states x
consistent with the current u), MC-SAT calls Sam-
pleSAT (Wei et al, 2004), which uses a combina-
tion of satisfiability testing and simulated annealing.
The advantage of using a satisfiability solver (Walk-
SAT) is that it efficiently finds isolated modes in the
distribution, and as a result the Markov chain mixes
very rapidly. The slice sampling scheme ensures
that detailed balance is (approximately) preserved.
MC-SAT is orders of magnitude faster than previous
MCMC algorithms like Gibbs sampling, making ef-
ficient sampling possible on a scale that was previ-
Algorithm 1 MC-SAT(clauses, weights,
num samples)
x(0) ? Satisfy(hard clauses)
for i? 1 to num samples do
M ? ?
for all ck ? clauses satisfied by x(i?1) do
With probability 1? e?wk add ck to M
end for
Sample x(i) ? USAT (M)
end for
ously out of reach.
Algorithm 1 gives pseudo-code for MC-SAT. At
iteration i ? 1, the factor ?k for clause ck is ei-
ther ewk if ck is satisfied in x(i?1), or 1 otherwise.
MC-SAT first samples the auxiliary variable uk uni-
formly from (0, ?k), then samples a new state uni-
formly from the set of states that satisfy ??k ? uk
for all k (the slice). Equivalently, for each k, with
probability 1 ? e?wk the next state must satisfy ck.
In general, we can factorize the probability distribu-
tion in any way that facilitates inference, sample the
uk?s, and make sure that the next state is drawn uni-
formly from solutions that satisfy ??k ? uk for all
factors.
MC-SAT, like most existing relational inference
algorithms, grounds all predicates and clauses, thus
requiring memory and time exponential in the pred-
icate and clause arities. We developed a general
method for producing a ?lazy? version of relational
inference algorithms (Poon & Domingos, 2008),
which carries exactly the same inference steps as the
original algorithm, but only maintains a small sub-
set of ?active? predicates/clauses, grounding more
as needed. We showed that Lazy-MC-SAT, the lazy
version of MC-SAT, reduced memory and time by
orders of magnitude in several domains. We use
Lazy-MC-SAT in this paper.
Supervised learning for Markov logic maximizes
the conditional log-likelihoodL(x, y) = logP (Y =
y|X = x), where Y represents the non-evidence
predicates, X the evidence predicates, and x, y their
values in the training data. For simplicity, from now
on we omit X , whose values are fixed and always
conditioned on. The optimization problem is convex
and a global optimum can be found using gradient
652
descent, with the gradient being
?
?wi
L(y) = ni(y)?
?
y? P (Y = y
?)ni(y?)
= ni(y)? EY [ni].
where ni is the number of true groundings of clause
i. The expected count can be approximated as
EY [ni] ?
1
N
N?
k=1
ni(yk)
where yk are samples generated by MC-SAT. To
combat overfitting, a Gaussian prior is imposed on
all weights.
In practice, it is difficult to tune the learning rate
for gradient descent, especially when the number
of groundings varies widely among clauses. Lowd
& Domingos (2007) used a preconditioned scaled
conjugate gradient algorithm (PSCG) to address this
problem. This estimates the optimal step size in each
step as
? =
?dT g
dTHd + ?dTd
.
where g is the gradient, d the conjugate update direc-
tion, and ? a parameter that is automatically tuned
to trade off second-order information with gradient
descent. H is the Hessian matrix, with the (i, j)th
entry being
?2
?wi?wj
L(y) = EY [ni] ? EY [nj ]? EY [ni ? nj ]
= ?CovY [ni, nj ].
The Hessian can be approximated with the same
samples used for the gradient. Its negative inverse
diagonal is used as the preconditioner.1
The open-source Alchemy package (Kok et al,
2007) provides implementations of existing algo-
rithms for Markov logic. In Section 5, we develop
the first general-purpose unsupervised learning al-
gorithm for Markov logic by extending the existing
algorithms to handle hidden predicates.2
1Lowd & Domingos showed that ? can be computed more
efficiently, without explicitly approximating or storing the Hes-
sian. Readers are referred to their paper for details.
2Alchemy includes a discriminative EM algorithm, but it as-
sumes that only a few values are missing, and cannot handle
completely hidden predicates. Kok & Domingos (2007) applied
Markov logic to relational clustering, but they used hard EM.
4 An MLN for Joint Unsupervised
Coreference Resolution
In this section, we present our MLN for joint unsu-
pervised coreference resolution. Our model deviates
from Haghighi & Klein?s (2007) in several impor-
tant ways. First, our MLN does not model saliences
for proper nouns or nominals, as their influence is
marginal compared to other features; for pronoun
salience, it uses a more intuitive and simpler def-
inition based on distance, and incorporated it as a
prior. Another difference is in identifying heads. For
the ACE datasets, Haghighi and Klein used the gold
heads; for the MUC-6 dataset, where labels are not
available, they crudely picked the rightmost token in
a mention. We show that a better way is to determine
the heads using head rules in a parser. This improves
resolution accuracy and is always applicable. Cru-
cially, our MLN leverages syntactic relations such
as apposition and predicate nominals, which are not
used by Haghighi and Klein. In our approach, what
it takes is just adding two formulas to the MLN.
As common in previous work, we assume that
true mention boundaries are given. We do not as-
sume any other labeled information. In particu-
lar, we do not assume gold name entity recogni-
tion (NER) labels, and unlike Haghighi & Klein
(2007), we do not assume gold mention types (for
ACE datasets, they also used gold head words). We
determined the head of a mention either by taking
its rightmost token, or by using the head rules in a
parser. We detected pronouns using a list.
4.1 Base MLN
The main query predicate is InClust(m, c!), which
is true iff mention m is in cluster c. The ?t!? notation
signifies that for each m, this predicate is true for a
unique value of c. The main evidence predicate is
Head(m, t!), where m is a mention and t a token, and
which is true iff t is the head of m. A key component
in our MLN is a simple head mixture model, where
the mixture component priors are represented by the
unit clause
InClust(+m,+c)
and the head distribution is represented by the head
prediction rule
InClust(m,+c) ? Head(m,+t).
653
All free variables are implicitly universally quanti-
fied. The ?+? notation signifies that the MLN con-
tains an instance of the rule, with a separate weight,
for each value combination of the variables with a
plus sign.
By convention, at each inference step we name
each non-empty cluster after the earliest mention it
contains. This helps break the symmetry among
mentions, which otherwise produces multiple op-
tima and makes learning unnecessarily harder. To
encourage clustering, we impose an exponential
prior on the number of non-empty clusters with
weight ?1.
The above model only clusters mentions with the
same head, and does not work well for pronouns. To
address this, we introduce the predicate IsPrn(m),
which is true iff the mention m is a pronoun, and
adapt the head prediction rule as follows:
?IsPrn(m) ? InClust(m,+c) ? Head(m,+t)
This is always false when m is a pronoun, and thus
applies only to non-pronouns.
Pronouns tend to resolve with men-
tions that are semantically compatible with
them. Thus we introduce predicates that
represent entity type, number, and gender:
Type(x, e!), Number(x, n!), Gender(x, g!),
where x can be either a cluster or mention,
e ? {Person, Organization, Location, Other},
n ? {Singular, Plural} and g ?
{Male, Female, Neuter}. Many of these are
known for pronouns, and some can be inferred
from simple linguistic cues (e.g., ?Ms. Galen?
is a singular female person, while ?XYZ Corp.?
is an organization).3 Entity type assignment is
represented by the unit clause
Type(+x,+e)
and similarly for number and gender. A mention
should agree with its cluster in entity type. This is
ensured by the hard rule (which has infinite weight
and must be satisfied)
InClust(m, c)? (Type(m, e)? Type(c, e))
3We used the following cues: Mr., Ms., Jr., Inc., Corp., cor-
poration, company. The proportions of known properties range
from 14% to 26%.
There are similar hard rules for number and gender.
Different pronouns prefer different entity types,
as represented by
IsPrn(m) ? InClust(m, c)
?Head(m,+t) ? Type(c,+e)
which only applies to pronouns, and whose weight is
positive if pronoun t is likely to assume entity type
e and negative otherwise. There are similar rules for
number and gender.
Aside from semantic compatibility, pronouns tend
to resolve with nearby mentions. To model this, we
impose an exponential prior on the distance (number
of mentions) between a pronoun and its antecedent,
with weight ?1.4 This is similar to Haghighi and
Klein?s treatment of salience, but simpler.
4.2 Full MLN
Syntactic relations among mentions often suggest
coreference. Incorporating such relations into our
MLN is straightforward. We illustrate this with
two examples: apposition and predicate nominals.
We introduce a predicate for apposition, Appo(x, y),
where x, y are mentions, and which is true iff y is an
appositive of x. We then add the rule
Appo(x, y)? (InClust(x, c)? InClust(y, c))
which ensures that x, y are in the same cluster if y is
an appositive of x. Similarly, we introduce a predi-
cate for predicate nominals, PredNom(x, y), and the
corresponding rule.5 The weights of both rules can
be learned from data with a positive prior mean. For
simplicity, in this paper we treat them as hard con-
straints.
4.3 Rule-Based MLN
We also consider a rule-based system that clusters
non-pronouns by their heads, and attaches a pro-
noun to the cluster which has no known conflicting
4For simplicity, if a pronoun has no antecedent, we define
the distance to be?. So a pronoun must have an antecedent in
our model, unless it is the first mention in the document or it can
not resolve with previous mentions without violating hard con-
straints. It is straightforward to soften this with a finite penalty.
5We detected apposition and predicate nominatives using
simple heuristics based on parses, e.g., if (NP, comma, NP) are
the first three children of an NP, then any two of the three noun
phrases are apposition.
654
type, number, or gender, and contains the closest an-
tecedent for the pronoun. This system can be en-
coded in an MLN with just four rules. Three of them
are the ones for enforcing agreement in type, num-
ber, and gender between a cluster and its members,
as defined in the base MLN. The fourth rule is
?IsPrn(m1) ? ?IsPrn(m2)
?Head(m1, h1) ? Head(m2, h2)
?InClust(m1, c1) ? InClust(m2, c2)
? (c1 = c2? h1 = h2).
With a large but not infinite weight (e.g., 100),
this rule has the effect of clustering non-pronouns
by their heads, except when it violates the hard
rules. The MLN can also include the apposition and
predicate-nominal rules. As in the base MLN, we
impose the same exponential prior on the number of
non-empty clusters and that on the distance between
a pronoun and its antecedent. This simple MLN is
remarkably competitive, as we will see in the exper-
iment section.
5 Learning and Inference
Unsupervised learning in Markov logic maximizes
the conditional log-likelihood
L(x, y) = logP (Y = y|X = x)
= log
?
z P (Y = y, Z = z|X = x)
where Z are unknown predicates. In our coref-
erence resolution MLN, Y includes Head and
known groundings of Type, Number and Gender,
Z includes InClust and unknown groundings of
Type, Number, Gender, and X includes IsPrn,
Appo and PredNom. (For simplicity, from now on
we drop X from the formula.) With Z, the opti-
mization problem is no longer convex. However, we
can still find a local optimum using gradient descent,
with the gradient being
?
?wi
L(y) = EZ|y[ni]? EY,Z [ni]
where ni is the number of true groundings of the ith
clause. We extended PSCG for unsupervised learn-
ing. The gradient is the difference of two expec-
tations, each of which can be approximated using
samples generated by MC-SAT. The (i, j)th entry of
the Hessian is now
?2
?wi?wj
L(y) = CovZ|y[ni, nj ]? CovY,Z [ni, nj ]
and the step size can be computed accordingly.
Since our problem is no longer convex, the nega-
tive diagonal Hessian may contain zero or negative
entries, so we first took the absolute values of the
diagonal and added 1, then used the inverse as the
preconditioner. We also adjusted ? more conserva-
tively than Lowd & Domingos (2007).
Notice that when the objects form independent
subsets (in our cases, mentions in each document),
we can process them in parallel and then gather suf-
ficient statistics for learning. We developed an ef-
ficient parallelized implementation of our unsuper-
vised learning algorithm using the message-passing
interface (MPI). Learning in MUC-6 took only one
hour, and in ACE-2004 two and a half.
To reduce burn-in time, we initialized MC-SAT
with the state returned by MaxWalkSAT (Kautz et
al., 1997), rather than a random solution to the hard
clauses. In the existing implementation in Alchemy
(Kok et al, 2007), SampleSAT flips only one atom
in each step, which is inefficient for predicates with
unique-value constraints (e.g., Head(m, c!)). Such
predicates can be viewed as multi-valued predi-
cates (e.g., Head(m) with value ranging over all
c?s) and are prevalent in NLP applications. We
adapted SampleSAT to flip two or more atoms in
each step so that the unique-value constraints are
automatically satisfied. By default, MC-SAT treats
each ground clause as a separate factor while de-
termining the slice. This can be very inefficient
for highly correlated clauses. For example, given
a non-pronoun mention m currently in cluster c and
with head t, among the mixture prior rules involv-
ing m InClust(m, c) is the only one that is satisfied,
and among those head-prediction rules involving m,
?IsPrn(m)?InClust(m, c)?Head(m, t) is the only
one that is satisfied; the factors for these rules mul-
tiply to ? = exp(wm,c + wm,c,t), where wm,c is the
weight for InClust(m, c), and wm,c,t is the weight
for ?IsPrn(m)?InClust(m, c)?Head(m, t), since
an unsatisfied rule contributes a factor of e0 = 1. We
extended MC-SAT to treat each set of mutually ex-
clusive and exhaustive rules as a single factor. E.g.,
for the above m, MC-SAT now samples u uniformly
655
from (0, ?), and requires that in the next state ?? be
no less than u. Equivalently, the new cluster and
head for m should satisfy wm,c? + wm,c?,t? ? log(u).
We extended SampleSAT so that when it consid-
ers flipping any variable involved in such constraints
(e.g., c or t above), it ensures that their new values
still satisfy these constraints.
The final clustering is found using the MaxWalk-
SAT weighted satisfiability solver (Kautz et al,
1997), with the appropriate extensions. We first ran
a MaxWalkSAT pass with only finite-weight formu-
las, then ran another pass with all formulas. We
found that this significantly improved the quality of
the results that MaxWalkSAT returned.
6 Experiments
6.1 System
We implemented our method as an extension to the
Alchemy system (Kok et al, 2007). Since our learn-
ing uses sampling, all results are the average of five
runs using different random seeds. Our optimiza-
tion problem is not convex, so initialization is im-
portant. The core of our model (head mixture) tends
to cluster non-pronouns with the same head. There-
fore, we initialized by setting all weights to zero,
and running the same learning algorithm on the base
MLN, while assuming that in the ground truth, non-
pronouns are clustered by their heads. (Effectively,
the corresponding InClust atoms are assigned to
appropriate values and are included in Y rather than
Z during learning.) We used 30 iterations of PSCG
for learning. (In preliminary experiments, additional
iterations had little effect on coreference accuracy.)
We generated 100 samples using MC-SAT for each
expectation approximation.6
6.2 Methodology
We conducted experiments on MUC-6, ACE-2004,
and ACE Phrase-2 (ACE-2). We evaluated our sys-
tems using two commonly-used scoring programs:
MUC (Vilain et al, 1995) and B3 (Amit & Bald-
win, 1998). To gain more insight, we also report
pairwise resolution scores and mean absolute error
in the number of clusters.
6Each sample actually contains a large number of ground-
ings, so 100 samples yield sufficiently accurate statistics for
learning.
The MUC-6 dataset consists of 30 documents for
testing and 221 for training. To evaluate the contri-
bution of the major components in our model, we
conducted five experiments, each differing from the
previous one in a single aspect. We emphasize that
our approach is unsupervised, and thus the data only
contains raw text plus true mention boundaries.
MLN-1 In this experiment, the base MLN was
used, and the head was chosen crudely as the
rightmost token in a mention. Our system was
run on each test document separately, using a
minimum of training data (the document itself).
MLN-30 Our system was trained on all 30 test doc-
uments together. This tests how much can be
gained by pooling information.
MLN-H The heads were determined using the head
rules in the Stanford parser (Klein & Manning,
2003), plus simple heuristics to handle suffixes
such as ?Corp.? and ?Inc.?
MLN-HA The apposition rule was added.
MLN-HAN The predicate-nominal rule was added.
This is our full model.
We also compared with two rule-based MLNs:
RULE chose the head crudely as the rightmost token
in a mention, and did not include the apposition rule
and predicate-nominal rule; RULE-HAN chose the
head using the head rules in the Stanford parser, and
included the apposition rule and predicate-nominal
rule.
Past results on ACE were obtained on different
releases of the datasets, e.g., Haghighi and Klein
(2007) used the ACE-2004 training corpus, Ng
(2005) and Denis and Baldridge (2007) used ACE
Phrase-2, and Culotta et al (2007) used the ACE-
2004 formal test set. In this paper, we used the
ACE-2004 training corpus and ACE Phrase-2 (ACE-
2) to enable direct comparisons with Haghighi &
Klein (2007), Ng (2005), and Denis and Baldridge
(2007). Due to license restrictions, we were not able
to obtain the ACE-2004 formal test set and so cannot
compare directly to Culotta et al (2007). The En-
glish version of the ACE-2004 training corpus con-
tains two sections, BNEWS and NWIRE, with 220
and 128 documents, respectively. ACE-2 contains a
656
Table 1: Comparison of coreference results in MUC
scores on the MUC-6 dataset.
# Doc. Prec. Rec. F1
H&K 60 80.8 52.8 63.9
H&K 381 80.4 62.4 70.3
M&W 221 - - 73.4
RULE - 76.0 65.9 70.5
RULE-HAN - 81.3 72.7 76.7
MLN-1 1 76.5 66.4 71.1
MLN-30 30 77.5 67.3 72.0
MLN-H 30 81.8 70.1 75.5
MLN-HA 30 82.7 75.1 78.7
MLN-HAN 30 83.0 75.8 79.2
Table 2: Comparison of coreference results in MUC
scores on the ACE-2004 (English) datasets.
EN-BNEWS Prec. Rec. F1
H&K 63.2 61.3 62.3
MLN-HAN 66.8 67.8 67.3
EN-NWIRE Prec. Rec. F1
H&K 66.7 62.3 64.2
MLN-HAN 71.3 70.5 70.9
training set and a test set. In our experiments, we
only used the test set, which contains three sections,
BNEWS, NWIRE, and NPAPER, with 51, 29, and
17 documents, respectively.
6.3 Results
Table 1 compares our system with previous ap-
proaches on the MUC-6 dataset, in MUC scores.
Our approach greatly outperformed Haghighi &
Klein (2007), the state-of-the-art unsupervised sys-
tem. Our system, trained on individual documents,
achieved an F1 score more than 7% higher than
theirs trained on 60 documents, and still outper-
formed it trained on 381 documents. Training on
the 30 test documents together resulted in a signif-
icant gain. (We also ran experiments using more
documents, and the results were similar.) Better
head identification (MLN-H) led to a large improve-
ment in accuracy, which is expected since for men-
tions with a right modifier, the rightmost tokens con-
fuse rather than help coreference (e.g., ?the chair-
man of Microsoft?). Notice that with this improve-
ment our system already outperforms a state-of-the-
Table 3: Comparison of coreference results in MUC
scores on the ACE-2 datasets.
BNEWS Prec. Rec. F1
Ng 67.9 62.2 64.9
D&B 78.0 62.1 69.2
MLN-HAN 68.3 66.6 67.4
NWIRE Prec. Rec. F1
Ng 60.3 50.1 54.7
D&B 75.8 60.8 67.5
MLN-HAN 67.7 67.3 67.4
NPAPER Prec. Rec. F1
Ng 71.4 67.4 69.3
D&B 77.6 68.0 72.5
MLN-HAN 69.2 71.7 70.4
Table 4: Comparison of coreference results in B3 scores
on the ACE-2 datasets.
BNEWS Prec. Rec. F1
Ng 77.1 57.0 65.6
MLN-HAN 70.3 65.3 67.7
NWIRE Prec. Rec. F1
Ng 75.4 59.3 66.4
MLN-HAN 74.7 68.8 71.6
NPAPER Prec. Rec. F1
Ng 75.4 59.3 66.4
MLN-HAN 70.0 66.5 68.2
art supervised system (McCallum & Wellner, 2005).
Leveraging apposition resulted in another large im-
provement, and predicate nominals also helped. Our
full model scores about 9% higher than Haghighi &
Klein (2007), and about 6% higher than McCallum
& Wellner (2005). To our knowledge, this is the best
coreference accuracy reported on MUC-6 to date.7
The B3 scores of MLN-HAN on the MUC-6 dataset
are 77.4 (precision), 67.6 (recall) and 72.2 (F1).
(The other systems did not report B3.) Interest-
ingly, the rule-based MLN (RULE) sufficed to out-
perform Haghighi & Klein (2007), and by using bet-
ter heads and the apposition and predicate-nominal
rules (RULE-HAN), it outperformed McCallum &
Wellner (2005), the supervised system. The MLNs
with learning (MLN-30 and MLN-HAN), on the
7As pointed out by Haghighi & Klein (2007), Luo et al
(2004) obtained a very high accuracy on MUC-6, but their sys-
tem used gold NER features and is not directly comparable.
657
Table 5: Our coreference results in precision, recall, and
F1 for pairwise resolution.
Pairwise Prec. Rec. F1
MUC-6 63.0 57.0 59.9
EN-BNEWS 51.2 36.4 42.5
EN-NWIRE 62.6 38.9 48.0
BNEWS 44.6 32.3 37.5
NWIRE 59.7 42.1 49.4
NPAPER 64.3 43.6 52.0
Table 6: Average gold number of clusters per document
vs. the mean absolute error of our system.
# Clusters MUC-6 EN-BN EN-NW
Gold 15.4 22.3 37.2
Mean Error 4.7 3.0 4.8
# Clusters BNEWS NWIRE NPAPER
Gold 20.4 39.2 55.2
Mean Error 2.5 5.6 6.6
other hand, substantially outperformed the corre-
sponding rule-based ones.
Table 2 compares our system to Haghighi & Klein
(2007) on the ACE-2004 training set in MUC scores.
Again, our system outperformed theirs by a large
margin. The B3 scores of MLN-HAN on the ACE-
2004 dataset are 71.6 (precision), 68.4 (recall) and
70.0 (F1) for BNEWS, and 75.7 (precision), 69.2
(recall) and 72.3 (F1) for NWIRE. (Haghighi &
Klein (2007) did not report B3.) Due to license re-
strictions, we could not compare directly to Culotta
et al (2007), who reported overall B3-F1 of 79.3 on
the formal test set.
Tables 3 and 4 compare our system to two re-
cent supervised systems, Ng (2005) and Denis
& Baldridge (2007). Our approach significantly
outperformed Ng (2005). It tied with Denis &
Baldridge (2007) on NWIRE, and was somewhat
less accurate on BNEWS and NPAPER.
Luo et al (2004) pointed out that one can ob-
tain a very high MUC score simply by lumping all
mentions together. B3 suffers less from this prob-
lem but is not perfect. Thus we also report pairwise
resolution scores (Table 5), the gold number of clus-
ters, and our mean absolute error in the number of
clusters (Table 6). Systems that simply merge all
mentions will have exceedingly low pairwise preci-
sion (far below 50%), and very large errors in the
number of clusters. Our system has fairly good pair-
wise precisions and small mean error in the number
of clusters, which verifies that our results are sound.
6.4 Error Analysis
Many of our system?s remaining errors involve nom-
inals. Additional features should be considered to
distinguish mentions that have the same head but are
different entities. For pronouns, many remaining er-
rors can be corrected using linguistic knowledge like
binding theory and salience hierarchy. Our heuris-
tics for identifying appositives and predicate nomi-
nals also make many errors, which often can be fixed
with additional name entity recognition capabilities
(e.g., given ?Mike Sullivan, VOA News?, it helps to
know that the former is a person and the latter an
organization). The most challenging case involves
phrases with different heads that are both proper
nouns (e.g., ?Mr. Bush? and ?the White House?).
Handling these cases requires domain knowledge
and/or more powerful joint inference.
7 Conclusion
This paper introduces the first unsupervised coref-
erence resolution system that is as accurate as su-
pervised systems. It performs joint inference among
mentions, using relations like apposition and predi-
cate nominals. It uses Markov logic as a representa-
tion language, which allows it to be easily extended
to incorporate additional linguistic and world knowl-
edge. Future directions include incorporating addi-
tional knowledge, conducting joint entity detection
and coreference resolution, and combining corefer-
ence resolution with other NLP tasks.
8 Acknowledgements
We thank the anonymous reviewers for their comments.
This research was funded by DARPA contracts NBCH-
D030010/02-000225, FA8750-07-D-0185, and HR0011-
07-C-0060, DARPA grant FA8750-05-2-0283, NSF grant
IIS-0534881, and ONR grant N-00014-05-1-0313 and
N00014-08-1-0670. The views and conclusions con-
tained in this document are those of the authors and
should not be interpreted as necessarily representing the
official policies, either expressed or implied, of DARPA,
NSF, ONR, or the United States Government.
658
References
Amit, B. & Baldwin, B. 1998. Algorithms for scoring
coreference chains. In Proc. MUC-7.
Bakir, G.; Hofmann, T.; Scho?lkopf, B.; Smola, A.;
Taskar, B. and Vishwanathan, S. (eds.) 2007. Pre-
dicting Structured Data. MIT Press.
Culotta, A.; Wick, M.; Hall, R. and McCallum, A. 2007.
First-order probabilistic models for coreference reso-
lution. In Proc. NAACL-07.
Denis, P. & Baldridge, J. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proc. NAACL-07.
Getoor, L. & Taskar, B. (eds.) 2007. Introduction to
Statistical Relational Learning. MIT Press.
Haghighi, A. & Klein, D. 2007. Unsupervised corefer-
ence resolution in a nonparametric Bayesian model. In
Proc. ACL-07.
Kautz, H.; Selman, B.; and Jiang, Y. 1997. A general
stochastic approach to solving problems with hard and
soft constraints. In The Satisfiability Problem: Theory
and Applications. AMS.
Klein, D. & Manning, C. 2003. Accurate unlexicalized
parsing. In Proc. ACL-03.
Kok, S.; Singla, P.; Richardson, M.; Domingos,
P.; Sumner, M.; Poon, H. & Lowd, D. 2007.
The Alchemy system for statistical relational AI.
http://alchemy.cs.washington.edu/.
Lowd, D. & Domingos, D. 2007. Efficient weight learn-
ing for Markov logic networks. In Proc. PKDD-07.
Luo, X.; Ittycheriah, A.; Jing, H.; Kambhatla, N. and
Roukos, S. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proc. ACL-04.
McCallum, A. & Wellner, B. 2005. Conditional models
of identity uncertainty with application to noun coref-
erence. In Proc. NIPS-04.
Ng, V. 2005. Machine Learning for Coreference Resolu-
tion: From Local Classification to Global Ranking. In
Proc. ACL-05.
Poon, H. & Domingos, P. 2006. Sound and efficient
inference with probabilistic and deterministic depen-
dencies. In Proc. AAAI-06.
Poon, H. & Domingos, P. 2007. Joint inference in infor-
mation extraction. In Proc. AAAI-07.
Poon, H. & Domingos, P. 2008. A general method for
reducing the complexity of relational inference and its
application to MCMC. In Proc. AAAI-08.
Richardson, M. & Domingos, P. 2006. Markov logic
networks. Machine Learning 62:107?136.
Vilain, M.; Burger, J.; Aberdeen, J.; Connolly, D. &
Hirschman, L. 1995. A model-theoretic coreference
scoring scheme. In Proc. MUC-6.
Wei, W.; Erenrich, J. and Selman, B. 2004. Towards
efficient sampling: Exploiting random walk strategies.
In Proc. AAAI-04.
659
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1?10,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Semantic Parsing
Hoifung Poon Pedro Domingos
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350, U.S.A.
{hoifung,pedrod}@cs.washington.edu
Abstract
We present the first unsupervised approach
to the problem of learning a semantic
parser, using Markov logic. Our USP
system transforms dependency trees into
quasi-logical forms, recursively induces
lambda forms from these, and clusters
them to abstract away syntactic variations
of the same meaning. The MAP semantic
parse of a sentence is obtained by recur-
sively assigning its parts to lambda-form
clusters and composing them. We evalu-
ate our approach by using it to extract a
knowledge base from biomedical abstracts
and answer questions. USP substantially
outperforms TextRunner, DIRT and an in-
formed baseline on both precision and re-
call on this task.
1 Introduction
Semantic parsing maps text to formal meaning
representations. This contrasts with semantic role
labeling (Carreras and Marquez, 2004) and other
forms of shallow semantic processing, which do
not aim to produce complete formal meanings.
Traditionally, semantic parsers were constructed
manually, but this is too costly and brittle. Re-
cently, a number of machine learning approaches
have been proposed (Zettlemoyer and Collins,
2005; Mooney, 2007). However, they are super-
vised, and providing the target logical form for
each sentence is costly and difficult to do consis-
tently and with high quality. Unsupervised ap-
proaches have been applied to shallow semantic
tasks (e.g., paraphrasing (Lin and Pantel, 2001),
information extraction (Banko et al, 2007)), but
not to semantic parsing.
In this paper we develop the first unsupervised
approach to semantic parsing, using Markov logic
(Richardson and Domingos, 2006). Our USP sys-
tem starts by clustering tokens of the same type,
and then recursively clusters expressions whose
subexpressions belong to the same clusters. Ex-
periments on a biomedical corpus show that this
approach is able to successfully translate syntac-
tic variations into a logical representation of their
common meaning (e.g., USP learns to map active
and passive voice to the same logical form, etc.).
This in turn allows it to correctly answer many
more questions than systems based on TextRun-
ner (Banko et al, 2007) and DIRT (Lin and Pantel,
2001).
We begin by reviewing the necessary back-
ground on semantic parsing and Markov logic. We
then describe our Markov logic network for un-
supervised semantic parsing, and the learning and
inference algorithms we used. Finally, we present
our experiments and results.
2 Background
2.1 Semantic Parsing
The standard language for formal meaning repre-
sentation is first-order logic. A term is any ex-
pression representing an object in the domain. An
atomic formula or atom is a predicate symbol ap-
plied to a tuple of terms. Formulas are recursively
constructed from atomic formulas using logical
connectives and quantifiers. A lexical entry de-
fines the logical form for a lexical item (e.g., a
word). The semantic parse of a sentence is de-
rived by starting with logical forms in the lexi-
cal entries and recursively composing the mean-
ing of larger fragments from their parts. In tradi-
tional approaches, the lexical entries and meaning-
1
composition rules are both manually constructed.
Below are sample rules in a definite clause gram-
mar (DCG) for parsing the sentence: ?Utah bor-
ders Idaho?.
V erb[?y?x.borders(x, y)]? borders
NP [Utah]? Utah
NP [Idaho]? Idaho
V P [rel(obj)]? V erb[rel] NP [obj]
S[rel(obj)]? NP [obj] V P [rel]
The first three lines are lexical entries. They are
fired upon seeing the individual words. For exam-
ple, the first rule applies to the word ?borders? and
generates syntactic category Verb with the mean-
ing ?y?x.borders(x, y) that represents the next-
to relation. Here, we use the standard lambda-
calculus notation, where ?y?x.borders(x, y)
represents a function that is true for any (x, y)-
pair such that borders(x, y) holds. The last two
rules compose the meanings of sub-parts into that
of the larger part. For example, after the first
and third rules are fired, the fourth rule fires and
generates V P [?y?x.borders(x, y)(Idaho)]; this
meaning simplifies to ?x.borders(x, Idaho) by
the ?-reduction rule, which substitutes the argu-
ment for a variable in a functional application.
A major challenge to semantic parsing is syn-
tactic variations of the same meaning, which
abound in natural languages. For example, the
aforementioned sentence can be rephrased as
?Utah is next to Idaho,??Utah shares a border with
Idaho,? etc. Manually encoding all these varia-
tions into the grammar is tedious and error-prone.
Supervised semantic parsing addresses this issue
by learning to construct the grammar automati-
cally from sample meaning annotations (Mooney,
2007). Existing approaches differ in the meaning
representation languages they use and the amount
of annotation required. In the approach of Zettle-
moyer and Collins (2005), the training data con-
sists of sentences paired with their meanings in
lambda form. A probabilistic combinatory cate-
gorial grammar (PCCG) is learned using a log-
linear model, where the probability of the final
logical form L and meaning-derivation tree T
conditioned on the sentence S is P (L, T |S) =
1
Z
exp (
?
i
w
i
f
i
(L, T, S)). Here Z is the normal-
ization constant and f
i
are the feature functions
with weights w
i
. Candidate lexical entries are gen-
erated by a domain-specific procedure based on
the target logical forms.
The major limitation of supervised approaches
is that they require meaning annotations for ex-
ample sentences. Even in a restricted domain,
doing this consistently and with high quality re-
quires nontrivial effort. For unrestricted text, the
complexity and subjectivity of annotation render it
essentially infeasible; even pre-specifying the tar-
get predicates and objects is very difficult. There-
fore, to apply semantic parsing beyond limited do-
mains, it is crucial to develop unsupervised meth-
ods that do not rely on labeled meanings.
In the past, unsupervised approaches have been
applied to some semantic tasks, but not to seman-
tic parsing. For example, DIRT (Lin and Pan-
tel, 2001) learns paraphrases of binary relations
based on distributional similarity of their argu-
ments; TextRunner (Banko et al, 2007) automati-
cally extracts relational triples in open domains us-
ing a self-trained extractor; SNE applies relational
clustering to generate a semantic network from
TextRunner triples (Kok and Domingos, 2008).
While these systems illustrate the promise of un-
supervised methods, the semantic content they ex-
tract is nonetheless shallow and does not constitute
the complete formal meaning that can be obtained
by a semantic parser.
Another issue is that existing approaches to se-
mantic parsing learn to parse syntax and semantics
together.1 The drawback is that the complexity
in syntactic processing is coupled with semantic
parsing and makes the latter even harder. For ex-
ample, when applying their approach to a different
domain with somewhat less rigid syntax, Zettle-
moyer and Collins (2007) need to introduce new
combinators and new forms of candidate lexical
entries. Ideally, we should leverage the enormous
progress made in syntactic parsing and generate
semantic parses directly from syntactic analysis.
2.2 Markov Logic
In many NLP applications, there exist rich rela-
tions among objects, and recent work in statisti-
cal relational learning (Getoor and Taskar, 2007)
and structured prediction (Bakir et al, 2007) has
shown that leveraging these can greatly improve
accuracy. One of the most powerful representa-
tions for this is Markov logic, which is a proba-
bilistic extension of first-order logic (Richardson
and Domingos, 2006). Markov logic makes it
1The only exception that we are aware of is Ge and
Mooney (2009).
2
possible to compactly specify probability distri-
butions over complex relational domains, and has
been successfully applied to unsupervised corefer-
ence resolution (Poon and Domingos, 2008) and
other tasks. A Markov logic network (MLN) is
a set of weighted first-order clauses. Together
with a set of constants, it defines a Markov net-
work with one node per ground atom and one fea-
ture per ground clause. The weight of a feature
is the weight of the first-order clause that origi-
nated it. The probability of a state x in such a
network is given by the log-linear model P (x) =
1
Z
exp (
?
i
w
i
n
i
(x)), where Z is a normalization
constant, w
i
is the weight of the ith formula, and
n
i
is the number of satisfied groundings.
3 Unsupervised Semantic Parsing with
Markov Logic
Unsupervised semantic parsing (USP) rests on
three key ideas. First, the target predicate and ob-
ject constants, which are pre-specified in super-
vised semantic parsing, can be viewed as clusters
of syntactic variations of the same meaning, and
can be learned from data. For example, borders
represents the next-to relation, and can be viewed
as the cluster of different forms for expressing this
relation, such as ?borders?, ?is next to?, ?share the
border with?; Utah represents the state of Utah,
and can be viewed as the cluster of ?Utah?, ?the
beehive state?, etc.
Second, the identification and clustering of can-
didate forms are integrated with the learning for
meaning composition, where forms that are used
in composition with the same forms are encour-
aged to cluster together, and so are forms that are
composed of the same sub-forms. This amounts to
a novel form of relational clustering, where clus-
tering is done not just on fixed elements in rela-
tional tuples, but on arbitrary forms that are built
up recursively.
Third, while most existing approaches (manual
or supervised learning) learn to parse both syn-
tax and semantics, unsupervised semantic pars-
ing starts directly from syntactic analyses and fo-
cuses solely on translating them to semantic con-
tent. This enables us to leverage advanced syn-
tactic parsers and (indirectly) the available rich re-
sources for them. More importantly, it separates
the complexity in syntactic analysis from the se-
mantic one, and makes the latter much easier to
perform. In particular, meaning composition does
not require domain-specific procedures for gener-
ating candidate lexicons, as is often needed by su-
pervised methods.
The input to our USP system consists of de-
pendency trees of training sentences. Compared
to phrase-structure syntax, dependency trees are
the more appropriate starting point for semantic
processing, as they already exhibit much of the
relation-argument structure at the lexical level.
USP first uses a deterministic procedure to con-
vert dependency trees into quasi-logical forms
(QLFs). The QLFs and their sub-formulas have
natural lambda forms, as will be described later.
Starting with clusters of lambda forms at the atom
level, USP recursively builds up clusters of larger
lambda forms. The final output is a probability
distribution over lambda-form clusters and their
compositions, as well as the MAP semantic parses
of training sentences.
In the remainder of the section, we describe
the details of USP. We first present the procedure
for generating QLFs from dependency trees. We
then introduce their lambda forms and clusters,
and show how semantic parsing works in this set-
ting. Finally, we present the Markov logic net-
work (MLN) used by USP. In the next sections, we
present efficient algorithms for learning and infer-
ence with this MLN.
3.1 Derivation of Quasi-Logical Forms
A dependency tree is a tree where nodes are words
and edges are dependency labels. To derive the
QLF, we convert each node to an unary atom with
the predicate being the lemma plus POS tag (be-
low, we still use the word for simplicity), and each
edge to a binary atom with the predicate being
the dependency label. For example, the node for
Utah becomes Utah(n
1
) and the subject depen-
dency becomes nsubj(n1, n2). Here, the n
i
are
Skolem constants indexed by the nodes. The QLF
for a sentence is the conjunction of the atoms for
the nodes and edges, e.g., the sentence above will
become borders(n
1
) ? Utah(n
2
) ? Idaho(n
3
) ?
nsubj(n
1
, n
2
) ? dobj(n
1
, n
3
).
3.2 Lambda-Form Clusters and Semantic
Parsing in USP
Given a QLF, a relation or an object is repre-
sented by the conjunction of a subset of the atoms.
For example, the next-to relation is represented
by borders(n
1
)? nsubj(n
1
, n
2
)? dobj(n
1
, n
3
),
and the states of Utah and Idaho are represented
3
by Utah(n
2
) and Idaho(n
3
). The meaning com-
position of two sub-formulas is simply their con-
junction. This allows the maximum flexibility in
learning. In particular, lexical entries are no longer
limited to be adjacent words as in Zettlemoyer and
Collins (2005), but can be arbitrary fragments in a
dependency tree.
For every sub-formula F , we define a corre-
sponding lambda form that can be derived by re-
placing every Skolem constant n
i
that does not
appear in any unary atom in F with a unique
lambda variable x
i
. Intuitively, such constants
represent objects introduced somewhere else (by
the unary atoms containing them), and corre-
spond to the arguments of the relation repre-
sented by F . For example, the lambda form
for borders(n
1
) ? nsubj(n
1
, n
2
) ? dobj(n
1
, n
3
)
is ?x
2
?x
3
. borders(n
1
) ? nsubj(n
1
, x
2
) ?
dobj(n
1
, x
3
).
Conceptually, a lambda-form cluster is a set of
semantically interchangeable lambda forms. For
example, to express the meaning that Utah bor-
ders Idaho, we can use any form in the cluster
representing the next-to relation (e.g., ?borders?,
?shares a border with?), any form in the cluster
representing the state of Utah (e.g., ?the beehive
state?), and any form in the cluster representing
the state of Idaho (e.g., ?Idaho?). Conditioned
on the clusters, the choices of individual lambda
forms are independent of each other.
To handle variable number of arguments, we
follow Davidsonian semantics and further de-
compose a lambda form into the core form,
which does not contain any lambda variable
(e.g., borders(n
1
)), and the argument forms,
which contain a single lambda variable (e.g.,
?x
2
.nsubj(n
1
, x
2
) and ?x
3
.dobj(n
1
, x
3
)). Each
lambda-form cluster may contain some number of
argument types, which cluster distinct forms of the
same argument in a relation. For example, in Stan-
ford dependencies, the object of a verb uses the de-
pendency dobj in the active voice, but nsubjpass
in passive.
Lambda-form clusters abstract away syntactic
variations of the same meaning. Given an in-
stance of cluster T with arguments of argument
types A
1
, ? ? ? , A
k
, its abstract lambda form is given
by ?x
1
? ? ??x
k
.T(n) ?
?
k
i=1
A
i
(n, x
i
).
Given a sentence and its QLF, semantic pars-
ing amounts to partitioning the atoms in the QLF,
dividing each part into core form and argument
forms, and then assigning each form to a cluster
or an argument type. The final logical form is de-
rived by composing the abstract lambda forms of
the parts using the ?-reduction rule.2
3.3 The USP MLN
Formally, for a QLF Q, a semantic parse L par-
titions Q into parts p
1
, p
2
, ? ? ? , p
n
; each part p is
assigned to some lambda-form cluster c, and is
further partitioned into core form f and argument
forms f
1
, ? ? ? , f
k
; each argument form is assigned
to an argument type a in c. The USP MLN de-
fines a joint probability distribution over Q and L
by modeling the distributions over forms and ar-
guments given the cluster or argument type.
Before presenting the predicates and formu-
las in our MLN, we should emphasize that they
should not be confused with the atoms and formu-
las in the QLFs, which are represented by reified
constants and variables.
To model distributions over lambda forms,
we introduce the predicates Form(p, f!) and
ArgForm(p, i, f!), where p is a part, i is the in-
dex of an argument, and f is a QLF subformula.
Form(p, f) is true iff part p has core form f, and
ArgForm(p, i, f) is true iff the ith argument in p
has form f.3 The ?f!? notation signifies that each
part or argument can have only one form.
To model distributions over arguments, we in-
troduce three more predicates: ArgType(p, i, a!)
signifies that the ith argument of p is assigned to
argument type a; Arg(p, i, p?) signifies that the
ith argument of p is p?; Number(p, a, n) signifies
that there are n arguments of p that are assigned
to type a. The truth value of Number(p, a, n) is
determined by the ArgType atoms.
Unsupervised semantic parsing can be captured
by four formulas:
p ? +c ? Form(p,+f)
ArgType(p, i,+a) ? ArgForm(p, i,+f)
Arg(p, i, p
?
) ? ArgType(p, i,+a) ? p
?
? +c
?
Number(p,+a,+n)
All free variables are implicitly universally quan-
tified. The ?+? notation signifies that the MLN
contains an instance of the formula, with a sep-
arate weight, for each value combination of the
2Currently, we do not handle quantifier scoping or se-
mantics for specific closed-class words such as determiners.
These will be pursued in future work.
3There are hard constraints to guarantee that these assign-
ments form a legal partition. We omit them for simplicity.
4
variables with a plus sign. The first formula mod-
els the mixture of core forms given the cluster, and
the others model the mixtures of argument forms,
argument types, and argument numbers, respec-
tively, given the argument type.
To encourage clustering and avoid overfitting,
we impose an exponential prior with weight ? on
the number of parameters.4
The MLN above has one problem: it often
clusters expressions that are semantically oppo-
site. For example, it clusters antonyms like ?el-
derly/young?, ?mature/immature?. This issue also
occurs in other semantic-processing systems (e.g.,
DIRT). In general, this is a difficult open problem
that only recently has started to receive some at-
tention (Mohammad et al, 2008). Resolving this
is not the focus of this paper, but we describe a
general heuristic for fixing this problem. We ob-
serve that the problem stems from the lack of nega-
tive features for discovering meanings in contrast.
In natural languages, parallel structures like con-
junctions are one such feature.5 We thus introduce
an exponential prior with weight ? on the number
of conjunctions where the two conjunctive parts
are assigned to the same cluster. To detect con-
junction, we simply used the Stanford dependen-
cies that begin with ?conj?. This proves very ef-
fective, fixing the majority of the errors in our ex-
periments.
4 Inference
Given a sentence and the quasi-logical form Q
derived from its dependency tree, the conditional
probability for a semantic parse L is given by
Pr(L|Q) ? exp (
?
i
w
i
n
i
(L,Q)). The MAP se-
mantic parse is simply argmax
L
?
i
w
i
n
i
(L,Q).
Enumerating all L?s is intractable. It is also un-
necessary, since most partitions will result in parts
whose lambda forms have no cluster they can be
assigned to. Instead, USP uses a greedy algorithm
to search for the MAP parse. First we introduce
some definitions: a partition is called ?-reducible
from p if it can be obtained from the current parti-
tion by recursively ?-reducing the part containing
p with one of its arguments; such a partition is
4Excluding weights of? or??, which signify hard con-
straints.
5For example, in the sentence ?IL-2 inhibits X in A and
induces Y in B?, the conjunction between ?inhibits? and ?in-
duces? suggests that they are different. If ?inhibits? and ?in-
duces? are indeed synonyms, such a sentence will sound awk-
ward and would probably be rephrased as ?IL-2 inhibits X in
A and Y in B?.
Algorithm 1 USP-Parse(MLN, QLF)
Form parts for individual atoms in QLF and as-
sign each to its most probable cluster
repeat
for all parts p in the current partition do
for all partitions that are ?-reducible from
p and feasible do
Find the most probable cluster and argu-
ment type assignments for the new part
and its arguments
end for
end for
Change to the new partition and assignments
with the highest gain in probability
until none of these improve the probability
return current partition and assignments
called feasible if the core form of the new part is
contained in some cluster. For example, consider
the QLF of ?Utah borders Idaho? and assume
that the current partition is ?x
2
x
3
.borders(n
1
) ?
nsubj(n
1
, x
2
) ? dobj(n
1
, x
3
), Utah(n
2
),
Idaho(n
3
). Then the following partition is
?-reducible from the first part in the above
partition: ?x
3
.borders(n
1
) ? nsubj(n
1
, n
2
) ?
Utah(n
2
) ? dobj(n
1
, x
3
), Idaho(n
3
). Whether
this new partition is feasible depends on whether
the core form of the new part ?x
3
.borders(n
1
) ?
nsubj(n
1
, n
2
) ? Utah(n
2
) ? dobj(n
1
, x
3
) (i.e.
borders(n
1
) ? nsubj(n
1
, n
2
) ? Utah(n
2
)) is
contained in some lambda-form cluster.
Algorithm 1 gives pseudo-code for our algo-
rithm. Given part p, finding partitions that are ?-
reducible from p and feasible can be done in time
O(ST ), where S is the size of the clustering in
the number of core forms and T is the maximum
number of atoms in a core form. We omit the proof
here but point out that it is related to the unordered
subtree matching problem which can be solved in
linear time (Kilpelainen, 1992). Inverted indexes
(e.g., from p to eligible core forms) are used to fur-
ther improve the efficiency. For a new part p and
a cluster that contains p?s core form, there are km
ways of assigning p?s m arguments to the k argu-
ment types of the cluster. For larger k and m, this
is very expensive. We therefore approximate it by
assigning each argument to the best type, indepen-
dent of other arguments.
This algorithm is very efficient, and is used re-
peatedly in learning.
5
5 Learning
The learning problem in USP is to maximize the
log-likelihood of observing the QLFs obtained
from the dependency trees, denoted by Q, sum-
ming out the unobserved semantic parses:
L
?
(Q) = logP
?
(Q)
= log
?
L
P
?
(Q,L)
Here, L are the semantic parses, ? are the MLN pa-
rameters, and P
?
(Q,L) are the completion likeli-
hoods. A serious challenge in unsupervised learn-
ing is the identifiability problem (i.e., the opti-
mal parameters are not unique) (Liang and Klein,
2008). This problem is particularly severe for
log-linear models with hard constraints, which are
common in MLNs. For example, in our USP
MLN, conditioned on the fact that p ? c, there is
exactly one value of f that can satisfy the formula
p ? c ? Form(p, f), and if we add some constant
number to the weights of p ? c ? Form(p, f) for
all f, the probability distribution stays the same.6
The learner can be easily confused by the infinitely
many optima, especially in the early stages. To
address this problem, we impose local normaliza-
tion constraints on specific groups of formulas that
are mutually exclusive and exhaustive, i.e., in each
group, we require that
?
k
i=1
e
w
i
= 1, where w
i
are the weights of formulas in the group. Group-
ing is done in such a way as to encourage the
intended mixture behaviors. Specifically, for the
rule p ? +c ? Form(p,+f), all instances given
a fixed c form a group; for each of the remain-
ing three rules, all instances given a fixed a form a
group. Notice that with these constraints the com-
pletion likelihood P (Q,L) can be computed in
closed form for any L. In particular, each formula
group contributes a term equal to the weight of the
currently satisfied formula. In addition, the opti-
mal weights that maximize the completion likeli-
hood P (Q,L) can be derived in closed form us-
ing empirical relative frequencies. E.g., the opti-
mal weight of p ? c? Form(p, f) is log(n
c,f
/n
c
),
where n
c,f
is the number of parts p that satisfy
both p ? c and Form(p, f), and n
c
is the number
of parts p that satisfy p ? c.7 We leverage this fact
for efficient learning in USP.
6Regularizations, e.g., Gaussian priors on weights, allevi-
ate this problem by penalizing large weights, but it remains
true that weights within a short range are roughly equivalent.
7To see this, notice that for a given c, the total contribu-
tion to the completion likelihood from all groundings in its
formula group is
?
f
w
c,f
n
c,f
. In addition,
?
f
n
c,f
= n
c
Algorithm 2 USP-Learn(MLN, QLFs)
Create initial clusters and semantic parses
Merge clusters with the same core form
Agenda? ?
repeat
for all candidate operations O do
Score O by log-likelihood improvement
if score is above a threshold then
Add O to agenda
end if
end for
Execute the highest scoring operation O? in
the agenda
Regenerate MAP parses for affected QLFs
and update agenda and candidate operations
until agenda is empty
return the MLN with learned weights and the
semantic parses
Another major challenge in USP learning is the
summation in the likelihood, which is over all pos-
sible semantic parses for a given dependency tree.
Even an efficient sampler like MC-SAT (Poon and
Domingos, 2006), as used in Poon & Domingos
(2008), would have a hard time generating accu-
rate estimates within a reasonable amount of time.
On the other hand, as already noted in the previous
section, the lambda-form distribution is generally
sparse. Large lambda-forms are rare, as they cor-
respond to complex expressions that are often de-
composable into smaller ones. Moreover, while
ambiguities are present at the lexical level, they
quickly diminish when more words are present.
Therefore, a lambda form can usually only belong
to a small number of clusters, if not a unique one.
We thus simplify the problem by approximating
the sum with the mode, and search instead for the
L and ? that maximize logP
?
(Q,L). Since the op-
timal weights and log-likelihood can be derived in
closed form given the semantic parses L, we sim-
ply search over semantic parses, evaluating them
using log-likelihood.
Algorithm 2 gives pseudo-code for our algo-
rithm. The input consists of an MLN without
weights and the QLFs for the training sentences.
Two operators are used for updating semantic
parses. The first is to merge two clusters, denoted
by MERGE(C
1
, C
2
) for clusters C
1
, C
2
, which does
the following:
and there is the local normalization constraint
?
f
e
w
c,f
= 1.
The optimal weights w
c,f
are easily derived by solving this
constrained optimization problem.
6
1. Create a new cluster C and add all core forms
in C
1
, C
2
to C;
2. Create new argument types for C by merg-
ing those in C
1
, C
2
so as to maximize the log-
likelihood;
3. Remove C
1
, C
2
.
Here, merging two argument types refers to pool-
ing their argument forms to create a new argument
type. Enumerating all possible ways of creating
new argument types is intractable. USP approxi-
mates it by considering one type at a time and ei-
ther creating a new type for it or merging it to types
already considered, whichever maximizes the log-
likelihood. The types are considered in decreasing
order of their numbers of occurrences so that more
information is available for each decision. MERGE
clusters syntactically different expressions whose
meanings appear to be the same according to the
model.
The second operator is to create a new clus-
ter by composing two existing ones, denoted by
COMPOSE(C
R
, C
A
), which does the following:
1. Create a new cluster C;
2. Find all parts r ? C
R
, a ? C
A
such that a is
an argument of r, compose them to r(a) by
?-reduction and add the new part to C;
3. Create new argument types for C from the ar-
gument forms of r(a) so as to maximize the
log-likelihood.
COMPOSE creates clusters of large lambda-forms
if they tend to be composed of the same sub-
forms (e.g., the lambda form for ?is next to?).
These lambda-forms may later be merged with
other clusters (e.g., borders).
At learning time, USP maintains an agenda that
contains operations that have been evaluated and
are pending execution. During initialization, USP
forms a part and creates a new cluster for each
unary atom u(n). It also assigns binary atoms of
the form b(n, n?) to the part as argument forms
and creates a new argument type for each. This
forms the initial clustering and semantic parses.
USP then merges clusters with the same core form
(i.e., the same unary predicate) using MERGE.8 At
each step, USP evaluates the candidate operations
and adds them to the agenda if the improvement is
8Word-sense disambiguation can be handled by including
a new kind of operator that splits a cluster into subclusters.
We leave this to future work.
above a threshold.9 The operation with the highest
score is executed, and the parameters are updated
with the new optimal values. The QLFs which
contain an affected part are reparsed, and opera-
tions in the agenda whose score might be affected
are re-evaluated. These changes are done very ef-
ficiently using inverted indexes. We omit the de-
tails here due to space limitations. USP terminates
when the agenda is empty, and outputs the current
MLN parameters and semantic parses.
USP learning uses the same optimization objec-
tive as hard EM, and is also guaranteed to find a
local optimum since at each step it improves the
log-likelihood. It differs from EM in directly opti-
mizing the likelihood instead of a lower bound.
6 Experiments
6.1 Task
Evaluating unsupervised semantic parsers is dif-
ficult, because there is no predefined formal lan-
guage or gold logical forms for the input sen-
tences. Thus the best way to test them is by using
them for the ultimate goal: answering questions
based on the input corpus. In this paper, we ap-
plied USP to extracting knowledge from biomedi-
cal abstracts and evaluated its performance in an-
swering a set of questions that simulate the in-
formation needs of biomedical researchers. We
used the GENIA dataset (Kim et al, 2003) as
the source for knowledge extraction. It contains
1999 PubMed abstracts and marks all mentions
of biomedical entities according to the GENIA
ontology, such as cell, protein, and DNA. As a
first approximation to the questions a biomedi-
cal researcher might ask, we generated a set of
two thousand questions on relations between enti-
ties. Sample questions are: ?What regulates MIP-
1alpha??, ?What does anti-STAT 1 inhibit??. To
simulate the real information need, we sample the
relations from the 100 most frequently used verbs
(excluding the auxiliary verbs be, have, and do),
and sample the entities from those annotated in
GENIA, both according to their numbers of occur-
rences. We evaluated USP by the number of an-
swers it provided and the accuracy as determined
by manual labeling.10
9We currently set it to 10 to favor precision and guard
against errors due to inexact estimates.
10The labels and questions are available at
http://alchemy.cs.washington.edu/papers/poon09.
7
6.2 Systems
Since USP is the first unsupervised semantic
parser, conducting a meaningful comparison of it
with other systems is not straightforward. Stan-
dard question-answering (QA) benchmarks do not
provide the most appropriate comparison, be-
cause they tend to simultaneously emphasize other
aspects not directly related to semantic pars-
ing. Moreover, most state-of-the-art QA sys-
tems use supervised learning in their key compo-
nents and/or require domain-specific engineering
efforts. The closest available system to USP in
aims and capabilities is TextRunner (Banko et al,
2007), and we compare with it. TextRunner is the
state-of-the-art system for open-domain informa-
tion extraction; its goal is to extract knowledge
from text without using supervised labels. Given
that a central challenge to semantic parsing is re-
solving syntactic variations of the same meaning,
we also compare with RESOLVER (Yates and Et-
zioni, 2009), a state-of-the-art unsupervised sys-
tem based on TextRunner for jointly resolving en-
tities and relations, and DIRT (Lin and Pantel,
2001), which resolves paraphrases of binary rela-
tions. Finally, we also compared to an informed
baseline based on keyword matching.
Keyword: We consider a baseline system based
on keyword matching. The question substring
containing the verb and the available argument is
directly matched with the input text, ignoring case
and morphology. We consider two ways to derive
the answer given a match. The first one (KW) sim-
ply returns the rest of sentence on the other side of
the verb. The second one (KW-SYN) is informed
by syntax: the answer is extracted from the subject
or object of the verb, depending on the question. If
the verb does not contain the expected argument,
the sentence is ignored.
TextRunner: TextRunner inputs text and outputs
relational triples in the form (R,A
1
, A
2
), where
R is the relation string, and A
1
, A
2
the argument
strings. Given a triple and a question, we first
match their relation strings, and then match the
strings for the argument that is present in the ques-
tion. If both match, we return the other argument
string in the triple as an answer. We report results
when exact match is used (TR-EXACT), or when
the triple string can contain the question one as a
substring (TR-SUB).
RESOLVER: RESOLVER (Yates and Etzioni,
2009) inputs TextRunner triples and collectively
resolves coreferent relation and argument strings.
On the GENIA data, using the default parameters,
RESOLVER produces only a few trivial relation
clusters and no argument clusters. This is not sur-
prising, since RESOLVER assumes high redun-
dancy in the data, and will discard any strings with
fewer than 25 extractions. For a fair compari-
son, we also ran RESOLVER using all extractions,
and manually tuned the parameters based on eye-
balling of clustering quality. The best result was
obtained with 25 rounds of execution and with the
entity multiple set to 200 (the default is 30). To an-
swer questions, the only difference from TextRun-
ner is that a question string can match any string
in its cluster. As in TextRunner, we report results
for both exact match (RS-EXACT) and substring
(RS-SUB).
DIRT: The DIRT system inputs a path and returns
a set of similar paths. To use DIRT in question
answering, we queried it to obtain similar paths
for the relation of the question, and used these
paths while matching sentences. We first used
MINIPAR (Lin, 1998) to parse input text using
the same dependencies as DIRT. To determine a
match, we first check if the sentence contains the
question path or one of its DIRT paths. If so, and if
the available argument slot in the question is con-
tained in the one in the sentence, it is a match, and
we return the other argument slot from the sen-
tence if it is present. Ideally, a fair comparison will
require running DIRT on the GENIA text, but we
were not able to obtain the source code. We thus
resorted to using the latest DIRT database released
by the author, which contains paths extracted from
a large corpus with more than 1GB of text. This
puts DIRT in a very advantageous position com-
pared with other systems. In our experiments, we
used the top three similar paths, as including more
results in very low precision.
USP: We built a system for knowledge extrac-
tion and question answering on top of USP. It
generated Stanford dependencies (de Marneffe et
al., 2006) from the input text using the Stan-
ford parser, and then fed these to USP-Learn11,
which produced an MLN with learned weights
and the MAP semantic parses of the input sen-
tences. These MAP parses formed our knowledge
base (KB). To answer questions, the system first
parses the questions12 using USP-Parse with the
11
? and ? are set to ?5 and ?10.
12The question slot is replaced by a dummy word.
8
Table 1: Comparison of question answering re-
sults on the GENIA dataset.
# Total # Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP 334 295 88%
learned MLN, and then matches the question parse
to parses in the KB by testing subsumption (i.e., a
question parse matches a KB one iff the former is
subsumed by the latter). When a match occurs, our
system then looks for arguments of type in accor-
dance with the question. For example, if the ques-
tion is ?What regulates MIP-1alpha??, it searches
for the argument type of the relation that contains
the argument form ?nsubj? for subject. If such an
argument exists for the relation part, it will be re-
turned as the answer.
6.3 Results
Table 1 shows the results for all systems. USP
extracted the highest number of answers, almost
doubling that of the second highest (RS-SUB).
It obtained the highest accuracy at 88%, and
the number of correct answers it extracted is
three times that of the second highest system.
The informed baseline (KW-SYN) did surpris-
ingly well compared to systems other than USP, in
terms of accuracy and number of correct answers.
TextRunner achieved good accuracy when exact
match is used (TR-EXACT), but only obtained a
fraction of the answers compared to USP. With
substring match, its recall substantially improved,
but precision dropped more than 20 points. RE-
SOLVER improved the number of extracted an-
swers by sanctioning more matches based on the
clusters it generated. However, most of those ad-
ditional answers are incorrect due to wrong clus-
tering. DIRT obtained the second highest number
of correct answers, but its precision is quite low
because the similar paths contain many errors.
6.4 Qualitative Analysis
Manual inspection shows that USP is able to re-
solve many nontrivial syntactic variations with-
out user supervision. It consistently resolves the
syntactic difference between active and passive
voices. It successfully identifies many distinct ar-
gument forms that mean the same (e.g., ?X stimu-
lates Y? ? ?Y is stimulated with X?, ?expression
of X? ? ?X expression?). It also resolves many
nouns correctly and forms meaningful groups of
relations. Here are some sample clusters in core
forms:
{investigate, examine, evaluate, analyze, study,
assay}
{diminish, reduce, decrease, attenuate}
{synthesis, production, secretion, release}
{dramatically, substantially, significantly}
An example question-answer pair, together with
the source sentence, is shown below:
Q: What does IL-13 enhance?
A: The 12-lipoxygenase activity of murine
macrophages.
Sentence: The data presented here indicate
that (1) the 12-lipoxygenase activity of murine
macrophages is upregulated in vitro and in vivo
by IL-4 and/or IL-13, . . .
7 Conclusion
This paper introduces the first unsupervised ap-
proach to learning semantic parsers. Our USP
system is based on Markov logic, and recursively
clusters expressions to abstract away syntactic
variations of the same meaning. We have suc-
cessfully applied USP to extracting a knowledge
base from biomedical text and answering ques-
tions based on it.
Directions for future work include: better han-
dling of antonyms, subsumption relations among
expressions, quantifier scoping, more complex
lambda forms, etc.; use of context and discourse
to aid expression clustering and semantic parsing;
more efficient learning and inference; application
to larger corpora; etc.
8 Acknowledgements
We thank the anonymous reviewers for their comments. This
research was partly funded by ARO grant W911NF-08-1-
0242, DARPA contracts FA8750-05-2-0283, FA8750-07-D-
0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-
D030010, NSF grants IIS-0534881 and IIS-0803481, and
ONR grant N00014-08-1-0670. The views and conclusions
contained in this document are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies, either expressed or implied, of ARO, DARPA,
NSF, ONR, or the United States Government.
9
References
G. Bakir, T. Hofmann, B. B. Scho?lkopf, A. Smola,
B. Taskar, S. Vishwanathan, and (eds.). 2007. Pre-
dicting Structured Data. MIT Press, Cambridge,
MA.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the Twentieth International Joint Con-
ference on Artificial Intelligence, pages 2670?2676,
Hyderabad, India. AAAI Press.
Xavier Carreras and Luis Marquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role la-
beling. In Proceedings of the Eighth Conference on
Computational Natural Language Learning, pages
89?97, Boston, MA. ACL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation, pages 449?
454, Genoa, Italy. ELRA.
Ruifang Ge and Raymond J. Mooney. 2009. Learning
a compositional semantic parser using an existing
syntactic parser. In Proceedings of the Forty Sev-
enth Annual Meeting of the Association for Compu-
tational Linguistics, Singapore. ACL.
Lise Getoor and Ben Taskar, editors. 2007. Introduc-
tion to Statistical Relational Learning. MIT Press,
Cambridge, MA.
Pekka Kilpelainen. 1992. Tree Matching Prob-
lems with Applications to Structured Text databases.
Ph.D. Thesis, Department of Computer Science,
University of Helsinki.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:180?82.
Stanley Kok and Pedro Domingos. 2008. Extract-
ing semantic networks from text via relational clus-
tering. In Proceedings of the Nineteenth European
Conference on Machine Learning, pages 624?639,
Antwerp, Belgium. Springer.
Percy Liang and Dan Klein. 2008. Analyzing the er-
rors of unsupervised learning. In Proceedings of the
Forty Sixth Annual Meeting of the Association for
Computational Linguistics, pages 879?887, Colum-
bus, OH. ACL.
Dekang Lin and Patrick Pantel. 2001. DIRT - dis-
covery of inference rules from text. In Proceedings
of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 323?328, San Francisco, CA. ACM Press.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, Granada, Spain.
ELRA.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 982?991,
Honolulu, HI. ACL.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Proceedings of the Eighth International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 311?324, Mexico City,
Mexico. Springer.
Hoifung Poon and Pedro Domingos. 2006. Sound and
efficient inference with probabilistic and determin-
istic dependencies. In Proceedings of the Twenty
First National Conference on Artificial Intelligence,
pages 458?463, Boston, MA. AAAI Press.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
649?658, Honolulu, HI. ACL.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62:107?136.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34:255?296.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammers. In Proceedings of the Twenty First
Conference on Uncertainty in Artificial Intelligence,
pages 658?666, Edinburgh, Scotland. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 878?887, Prague, Czech. ACL.
10
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 870?878,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Language ID in the Context of Harvesting Language Data off the Web
Fei Xia
University of Washington
Seattle, WA 98195, USA
fxia@u.washington.edu
William D. Lewis
Microsoft Research
Redmond, WA 98052, USA
wilewis@microsoft.com
Hoifung Poon
University of Washington
Seattle, WA 98195, USA
hoifung@cs.washington.edu
Abstract
As the arm of NLP technologies extends
beyond a small core of languages, tech-
niques for working with instances of lan-
guage data across hundreds to thousands
of languages may require revisiting and re-
calibrating the tried and true methods that
are used. Of the NLP techniques that has
been treated as ?solved? is language iden-
tification (language ID) of written text.
However, we argue that language ID is
far from solved when one considers in-
put spanning not dozens of languages, but
rather hundreds to thousands, a number
that one approaches when harvesting lan-
guage data found on the Web. We formu-
late language ID as a coreference resolu-
tion problem and apply it to a Web harvest-
ing task for a specific linguistic data type
and achieve a much higher accuracy than
long accepted language ID approaches.
1 Introduction
A large number of the world?s languages have
been documented by linguists; it is now increas-
ingly common to post current research and data
to the Web, often in the form of language snip-
pets embedded in scholarly papers. A particu-
larly common format for linguistic data posted to
the Web is ?interlinearized text?, a format used
to present language data and analysis relevant to
a particular argument or investigation. Since in-
terlinear examples consist of orthographically or
phonetically encoded language data aligned with
an English translation, the ?corpus? of interlinear
examples found on the Web, when taken together,
constitute a significant multilingual, parallel cor-
pus covering hundreds to thousands of the world?s
languages. Previous work has discussed methods
for harvesting interlinear text off the Web (Lewis,
2006), enriching it via structural projections (Xia
and Lewis, 2007), and even making it available to
typological analyses (Lewis and Xia, 2008) and
search (Xia and Lewis, 2008).
One challenge with harvesting interlinear data
off the Web is language identification of the har-
vested data. There have been extensive studies
on language identification (language ID) of writ-
ten text, and a review of previous research on this
topic can be found in (Hughes et al, 2006). In gen-
eral, a language ID method requires a collection
of text for training, something on the order of a
thousand or more characters. These methods work
well for languages with rich language resources;
for instance, Cavnar and Trenkle?s N-gram-based
algorithm achieved an accuracy as high as 99.8%
when tested on newsgroup articles across eight
languages (Cavnar and Trenkle, 1994). However,
the performance is much worse (with accuracy
dropping to as low as 1.66%) if there is very lit-
tle language data for training and the number of
languages being evaluated reaches a few hundred.
In this paper, we treat the language ID of har-
vested linguistic data as a coreference resolution
problem. Our method, although narrowly focused
on this very specific data type, makes it possible to
collect small snippets of language data across hun-
dreds of languages and use the data for linguistic
search and bootstrapping NLP tools.
2 Background
2.1 Interlinear glossed text (IGT)
In linguistics, the practice of presenting language
data in interlinear form has a long history, go-
ing back at least to the time of the structural-
ists. Interlinear Glossed Text, or IGT, is often
used to present data and analysis on a language
that the reader may not know much about, and
is frequently included in scholarly linguistic doc-
uments. The canonical form of an IGT consists
870
of three lines: a line for the language in question
(i.e., the language line), an English gloss line, and
an English translation. Table 1 shows the begin-
ning of a linguistic document (Baker and Stewart,
1996) which contains two IGTs: one in lines 30-
32, and the other in lines 34-36. The line numbers
are added for the sake of convenience.
1: THE ADJ/VERB DISTINCTION: EDO EVIDENCE
2:
3: Mark C. Baker and Osamuyimen Thompson Stewart
4: McGill University
....
27: The following shows a similar minimal pair from Edo,
28: a Kwa language spoken in Nigeria (Agheyisi 1990).
29:
30: (2) a. E`me`ri? mo`se?.
31: Mary be.beautiful(V)
32: ?Mary is beautiful.?
33:
34: b. E`me`ri? *(ye?) mo`se?.
35: Mary be.beautiful(A)
36: ?Mary is beautiful (A).?
...
Table 1: A linguistic document that contains IGT:
words in boldface are potential language names
2.2 The Online Database of Interlinear text
(ODIN)
ODIN, the Online Database of INterlinear text, is
a resource built from data harvested from schol-
arly documents (Lewis, 2006). It was built in
three steps: (1) crawling the Web to retrieve doc-
uments that may contain IGT, (2) extracting IGT
from the retrieved documents, and (3) identifying
the language codes of the extracted IGTs. The
identified IGTs are then extracted and stored in a
database (the ODIN database), which can be easily
searched with a GUI interface.1
ODIN currently consists about 189,000 IGT in-
stances extracted from three thousand documents,
with close to a thousand languages represented.
In addition, there are another 130,000 additional
IGT-bearing documents that have been crawled
and are waiting for further process. Once these
additional documents are processed, the database
is expected to expand significantly.
ODIN is a valuable resource for linguists, as it
can be searched for IGTs that belong to a partic-
ular language or a language family, or those that
contain a particular linguistic construction (e.g.,
passive, wh-movement). In addition, there have
1http://odin.linguistlist.org
been some preliminary studies that show the bene-
fits of using the resource for NLP. For instance, our
previous work shows that automatically enriched
IGT data can be used to answer typological ques-
tions (e.g., the canonical word order of a language)
with a high accuracy (Lewis and Xia, 2008), and
the information could serve as prototypes for pro-
totype learning (Haghighi and Klein, 2006).
3 The language ID task for ODIN
As the size of ODIN increases dramatically, it is
crucial to have a reliable module that automati-
cally identifies the correct language code for each
new extracted IGT to be added to ODIN. The cur-
rent ODIN system uses two language identifiers:
one is based on simple heuristics, and the other
on Cavnar and Trenkle?s algorithm (1994). How-
ever, because the task here is very different from
a typical language ID task (see below), both algo-
rithms work poorly, with accuracy falling below
55%. The focus of this paper is on building new
language identifiers with a much higher accuracy.
3.1 The data set
A small portion of the IGTs in ODIN have
been assigned the correct language code semi-
automatically. Table 2 shows the size of the data
set. We use it for training and testing, and all re-
sults reported in the paper are the average of run-
ning 10-fold cross validation on the data set unless
specified otherwise.
Table 2: The data set for the language ID task
# of IGT-bearing documents 1160
# of IGT instances 15,239
# of words on the language lines 77,063
# of languages 638
3.2 The special properties of the task
The task in hand is very different from a typical
language ID task in several respects:
? Large number of languages: The number of
languages in our data set is 638 and that of the
current ODIN database is close to a thousand.
As more data is added to ODIN, the number
of languages may reach several thousand as
newly added linguistic documents could refer
to any of approximately eight thousand living
or dead languages.
871
? The use of language code: When dealing
with only a few dozen languages, language
names might be sufficient to identify lan-
guages. This is not true when dealing with
a large number of languages, because some
languages have multiple names, and some
language names refer to multiple languages
(see Section 4.2). To address this problem,
we use language codes, since we can (mostly)
ensure that each language code maps to ex-
actly one language, and each language maps
to exactly one code.
? Unseen languages: In this data set, about
10% of IGT instances in the test data belong
to some languages that have never appeared
in the training data. We call it the unseen
language problem. This problem turns out to
be the major obstacle to existing language ID
methods.
? Extremely limited amount of training data
per language: On average, each language in
the training data has only 23 IGTs (116 word
tokens in the language lines) available, and
45.3% of the languages have no more than
10 word tokens in the training data.
? The length of test instances: The language
lines in IGT are often very short. The aver-
age length in this data set is 5.1 words. About
0.26% of the language lines in the data set are
totally empty due to the errors introduced in
the crawling or IGT extraction steps.
? Encoding issues: For languages that do not
use Roman scripts in their writing system,
the authors of documents often choose to use
Romanized scripts (e.g., pinyin for Chinese),
making the encoding less informative.
? Multilingual documents: About 40% of doc-
uments in the data set contain IGTs from
multiple languages. Therefore, the language
ID prediction should be made for each indi-
vidual IGT, not for the whole document.
? Context information: In this task, IGTs are
part of a document and there are often various
cues in the document (e.g., language names)
that could help predict the language ID of
specific IGT instances.
Hughes and his colleagues (2006) identified
eleven open questions in the domain of language
ID that they believed were not adequately ad-
dressed in published research to date. Interest-
ingly, our task encounters eight out of the eleven
open questions. Because of these properties, ex-
isting language ID algorithms do not perform well
when applied to the task (see Section 6).
4 Using context information
Various cues in the document can help predict the
language ID of IGTs, and they are represented as
features in our systems.
4.1 Feature templates
The following feature templates are used in our ex-
periments.
(F1): The nearest language that precedes the cur-
rent IGT.
(F2): The languages that appear in the neighbor-
hood of the IGT or at the beginning or the
end of a document.2 Another feature checks
the most frequent language occurring in the
document.
(F3): For each language in the training data, we
build three token lists: one for word uni-
grams, one for morph unigrams and the third
for character ngrams (n ? 4). These word
lists are compared with the token lists built
from the language line of the current IGT.
(F4): Similar to (F3), but the comparison is be-
tween the token lists built from the current
IGT with the ones built from other IGTs in
the same document. If some IGTs in the
same document share the same tokens, they
are likely to belong to the same language.
Here, all the features are binary: for features in
F3 and F4, we use thresholds to turn real-valued
features into binary ones. F1-F3 features can
be calculated by looking at the documents only,
whereas F4 features require knowing the language
codes of other IGTs in the same document.
4.2 Language table
To identify language names in a document and
map language names to language codes, we need
a language table that lists all the (language code,
2For the experiments reported here, we use any line within
50 lines of the IGT or the first 50 or the last 50 lines of the
document.
872
language name) pairs. There are three existing lan-
guage tables: (1) ISO 639-3 maintained by SIL
International,3 (2) the 15th edition of the Ethno-
logue,4 and (3) the list of ancient and dead lan-
guages maintained by LinguistList.5 6 We merged
the three tables, as shown in Table 3.
Table 3: Various language name tables
Language table # of lang # of lang
codes (code, name) pairs
(1) ISO 639-3 7702 9312
(2) Ethnologue v15 7299 42789
(3) LinguistList table 231 232
Merged table 7816 47728
The mapping between language names and lan-
guage codes is many-to-many. A language code
often has several alternate names in addition to the
primary name. For instance, the language code
aaa maps to names such as Alumu, Tesu, Arum,
Alumu-Tesu, Alumu, Arum-Cesu, Arum-Chessu,
and Arum-Tesu. While most language names map
to only one language code, there are exceptions.
For instance, the name Edo can map to either bin
or lew. Out of 44,071 unique language names in
the merged language table, 2625 of them (5.95%)
are ambiguous.7
To identify language names in a document, we
implemented a simple language name detector that
scans the document from left to right and finds the
longest string that is a language name according
to the language table. The language name is then
mapped to language codes. If a language name is
ambiguous, all the corresponding language codes
are considered by later stages. In Table 1, the
language names identified by the detector are in
boldface. The detector can produce false positive
(e.g., Thompson) because a language name can
have other meanings. Also, the language table is
by no means complete and the detector is not able
to recognize any language names that are missing
from the table.
3http://www.sil.org/iso639-3/download.asp
4http://www.ethnologue.com/codes/default.asp#using
5http://linguistlist.org/forms/langs/GetListOfAncientLgs.html
6While ISO 639-3 is supposed to include all the language
codes appearing in the other two lists, there is a lag in the
adoption of new codes, which means the ISO 639-3 list con-
tinues to be somewhat out-of-date with the lists from which
it is compiled since these other lists change periodically.
7Among the ambiguous names, 1996 names each map to
two language codes, 407 map to three codes, 130 map to four
codes, and so on. The most ambiguous name is Miao, which
maps to fourteen language codes.
5 Formulating the language ID task
The language ID task here can be treated as two
different learning problems.
5.1 As a classification problem
The language ID task can be treated as a classifica-
tion problem. A classifier is a function that maps
a training/test instance x to a class label y, and y
is a member of a pre-defined label set C . For lan-
guage ID, the training/test instance corresponds to
a document (or an IGT in our case), and C is the
set of language codes. We call this approach the
classification (CL) approach.
Most, if not all, of previous language ID meth-
ods, fall into this category. They differ with re-
spect to the underlying learning algorithms and the
choice of features or similarity functions. When
applying a feature-based algorithm (e.g., Maxi-
mum entropy) and using the features in Section
4.1, the feature vectors for the two IGTs in Ta-
ble 1 are shown in Table 4. Each line has the for-
mat ?instance name true lang code feat name1
feat name2 ...?, where feat names are the names
of features that are present in the instance. Take
the first IGT as an example, its true language code
is bin; the nearest language name (nearLC) is Edo
whose language code is bin or lew; the languages
that appear before the IGT includes Edo (bin or
lew), Thompson (thp), and so on. The presence of
LMw1 bin and LMm1 bin means that the overlap
between the word/morph lists for bin and the ones
built from the current IGT is higher than some
threshold. The feature vector for the second IGT
looks similar, except that it includes a F4 feature
IIw1 bin, which says that the overlap between the
word list built from the other IGTs in the same
document with language code bin and the word
list built from the current IGT is above a thresh-
old. Note that language codes are part of feature
names; therefore, a simple feature template such
as nearest language (nearLC) corresponds to hun-
dreds or even thousands of features (nearLC xxx).
The CL approach has several major limitations.
First, it cannot handle the unseen language prob-
lem: if an IGT in the test data belongs to a lan-
guage that does not appear in the training data, this
approach cannot classify it correctly. Second, the
lack of parameter tying in this approach makes it
unable to generalize between different languages.
For instance, if the word German appears right be-
fore an IGT, the IGT is likely to be German. The
873
igt1 bin nearLC bin nearLC lew prev50 bin prev50 lew prev50 thp ... LMw1 bin LMm1 bin ...
igt2 bin nearLC bin nearLC lew prev50 bin prev50 lew prev50 thp ... LMw1 bin LMm1 bin ... IIw1 bin ...
Table 4: Feature vectors for the IGTs in Table 1 when using the CL approach (Edo: bin/lew, Thompson:
thp, Kwa: etu/fip/kwb)
same is true if the word German is replaced by an-
other language name. But this property cannot be
leveraged easily by the CL approach without mod-
ifying the learning algorithm. This results in a pro-
liferation of parameters, making learning harder
and more prone to overfitting.
5.2 As a coreference resolution problem
A different way of handling the language ID task
is to treat it as a coreference resolution problem: a
mention is an IGT or a language name appearing
in a document, an entity is a language code, and
finding the language code for an IGT is the same as
linking a mention (i.e., an IGT) to an entity (i.e., a
language code).8 We call this approach the CoRef
approach. The major difference between the CL
approach and the CoRef approach is the role of
language code: in the former, language code is a
class label to be used to tag an IGT; and in the lat-
ter, language code is an entity which an IGT can
be linked to.
The language ID task shares many similarities
with a typical coreference resolution task. For
instance, language names are similar to proper
nouns in that they are often unambiguous. IGT
instances are like pronouns in that they often refer
to language names appearing in the neighborhood.
Once the language ID task is framed as a CoRef
problem, all the existing algorithms on CoRef can
be applied to the task, as discussed below.
5.2.1 Sequence labeling using traditional
classifiers
One common approach to the CoRef problem pro-
cesses the mentions sequentially and determine for
each mention whether it should start a new entity
or be linked to an existing mention (e.g., (Soon
et al, 2001; Ng and Cardie, 2002; Luo, 2007));
that is, the approach makes a series of decisions,
8There are minor differences between the language ID and
coreference resolution tasks. For instance, each entity in the
language ID task must be assigned a language code. This
means that ambiguous language names will evoke multiple
entities, each with a different language code. These differ-
ences are reflected in our algorithms.
one decision per (mention, entity) pair. Apply-
ing this to the language ID task, the (mention, en-
tity) pair would correspond to an (IGT, lang code)
pair, and each decision would have two possibili-
ties: Same when the IGT belongs to the language
or Diff when the IGT does not. Once the decisions
are made for all the pairs, a post-processing proce-
dure would check all the pairs for an IGT and link
the IGT to the language code with which the pair
has the highest confidence score.
Using the same kinds of features in Section 4.1,
the feature vectors for the two IGTs in Table 1 are
shown in Table 5. Comparing Table 4 and 5 re-
veals the differences between the CL approach and
the CoRef approach: the CoRef approach has only
two class labels (Same and Diff) where the CL ap-
proach has hundreds of labels (one for each lan-
guage code); the CoRef approach has much fewer
number of features because language code is not
part of feature names; the CoRef approach has
more training instances as each training instance
corresponds to an (IGT, lang code) pair.
igt1-bin same nearLC prev50 LMw1 LMm1 ...
igt1-lew diff nearLC prev50 ...
igt1-thp diff prev50 ...
...
igt2-bin same nearLC prev50 LMw1 LMm1 IIw1 ...
igt2-lew diff nearLC prev50 ...
igt2-thp diff prev50 ...
...
Table 5: Feature vectors for the IGTs in Table 1
when using the CoRef approach with sequence la-
beling methods
5.2.2 Joint Inference Using Markov Logic
Recently, joint inference has become a topic of
keen interests in both the machine learning and
NLP communities (e.g., (Bakir et al, 2007; Sut-
ton et al, 2006; Poon and Domingos, 2007)).
There have been increasing interests in formulat-
ing coreference resolution in a joint model and
conducting joint inference to leverage dependen-
874
cies among the mentions and entities (e.g., (Well-
ner et al, 2004; Denis and Baldridge, 2007; Poon
and Domingos, 2008)). We have built a joint
model for language ID in Markov logic (Richard-
son and Domingos, 2006).
Markov logic is a probabilistic extension of
first-order logic that makes it possible to com-
pactly specify probability distributions over com-
plex relational domains. A Markov logic net-
work (MLN) is a set of weighted first-order
clauses. Together with a set of constants, it de-
fines a Markov network with one node per ground
atom and one feature per ground clause. The
weight of a feature is the weight of the first-order
clause that originated it. The probability of a
state x in such a network is given by P (x) =
(1/Z) exp (?i wifi(x)), where Z is a normaliza-
tion constant, wi is the weight of the ith clause,
fi = 1 if the ith clause is true, and fi = 0
otherwise. Conditional probabilities can be com-
puted using Markov chain Monte Carlo (e.g., MC-
SAT (Poon and Domingos, 2006)). The weights
can be learned using pseudo-likelihood training
with L-BFGS (Richardson and Domingos, 2006).
Markov logic is one of the most powerful rep-
resentations for joint inference with uncertainty,
and an implementation of its existing learning and
inference algorithms is publicly available in the
Alchemy package (Kok et al, 2007).
To use the features defined in Section 4.1, our
MLN includes two evidence predicates: the first
one is HasFeature(i, l, f) where f is a feature in
F1-F3. The predicate is true iff the IGT-language
pair (i, l) has feature f . The second predicate is
HasRelation(i1, i2, r) where r is a relation that
corresponds to a feature in F4; this predicate is
true iff relation r holds between two IGTs i1, i2.
The query predicate is IsSame(i, l), which is true
iff IGT i is in language l. Table 6 shows the pred-
icates instantiated from the two IGTs in Table 1.
The language ID task can be captured in our
MLN with just three formulas:
IsSame(i, l)
HasFeature(i, l,+f) ? IsSame(i, l)
HasRelation(i1, i2,+r)? IsSame(i1, l)
? IsSame(i2, l)
The first formula captures the default probabil-
ity that an IGT belongs to a particular language.
IsSame(igt1, bin)
HasFeature(igt1, bin, nearLC)
HasFeature(igt1, bin, prev50)
HasFeature(igt1, bin, LMw1)
...
HasFeature(igt1, lew, nearLC)
HasFeature(igt1, lew, prev50)
...
IsSame(igt2, bin)
HasFeature(igt2, bin, nearLC)
HasFeature(igt2, bin, prev50)
HasFeature(igt2, bin, LMw1)
...
HasRelation(igt1, igt2, IIw1)
...
Table 6: The predicates instantiated from the IGTs
in Table 1
The second one captures the conditional likeli-
hoods of an IGT being in a language given the fea-
tures. The third formula says that two IGTs prob-
ably belong to the same language if they have a
certain relation r.
The plus sign before f and r in the formulas
signifies that the MLN will learn a separate weight
for each individual feature f and relation r. Note
that there is no plus sign before i and l, allowing
the MLN to achieve parameter tying by sharing the
same weights for different instances or languages.
5.2.3 The advantage of the Coref approach
Both methods of the CoRef approach address the
limitations of the CL approach: both can handle
the unseen language problem, and both do param-
eter tying in a natural way. Not only does parame-
ter tying reduce the number of parameters, it also
makes it possible to accumulate evidence among
different languages and different IGTs.
6 Experiments
In this section, we compare the two approaches
to the language ID task: the CL approach and the
CoRef approach. In our experiments, we run 10-
fold cross validation (90% for training and 10%
for testing) on the data set in Table 2 and report
the average of language ID accuracy.
The two approaches have different upper
bounds. The upper bound of the CL approach is
the percentage of IGTs in the test data that be-
long to a seen language. The upper bound of the
CoRef approach is the percentage of IGTs in the
test data that belong to a language whose language
name appears in the same document. For the data
set in Table 2, the upper bounds are 90.33% and
875
Table 7: The performance of the CL approach (# of classes: about 600, # of training instances=13,723)
Upper bound of TextCat MaxEnt classifier using context information
CL approach F1 F1-F2 F1-F3 F1-F4 (cheating)
# of features N/A N/A 769 5492 8226 8793
w/o the language filter 90.33 51.38 49.74 61.55 64.19 66.47
w/ the language filter 88.95 60.72 56.69 64.95 67.03 69.20
97.31% respectively. When the training data is
much smaller, the upper bound of the CL approach
would decrease tremendously, whereas the upper
bound of the CoRef approach remains the same.
6.1 The CL approach
As mentioned before, most existing language ID
algorithm falls into this category. We chose
TextCat,9 an implementation of Cavnar-Trenkle?s
algorithm (1994), as an example of these algo-
rithms. In order to take advantage of the con-
text information, we trained several classifiers
(e.g., decision tree, Naive Bayes, and maximum
entropy) using the Mallet package (McCallum,
2002) and a SVM classifier using the libSVM
package (Chang and Lin, 2001).
The result is in Table 7. The first column shows
the upper bound of the CL approach; the second
column is the result of running TextCat;10 the rest
of the table lists the result of running a MaxEnt
classifier with different feature sets.11 F4 features
require knowing the language code of other IGTs
in the document. In the F1-F4 cheating exper-
iments, the language codes of other IGTs come
from the gold standard. We did not implement
beam search for this because the difference be-
tween the cheating results and the results without
F4 features is relatively small and both are much
worse than the results in the CoRef approach.
In Table 7, the first row shows the number of
features; the second row shows the accuracy of the
two classifiers; the last row is the accuracy when
a post-processing filter is added: the filter takes
the ranked language list produced by a classifier,
throws away all the languages in the list that do
not appear in the document, and then outputs the
highest ranked language in the remaining list.
There are several observations. First, applying
the post-processing filter improves performance,
9http://odur.let.rug.nl/ vannoord/TextCat/
10We varied the lexicon size (m) ? an important tuned pa-
rameter for the algorithm ? from 100 and 800 and observed
a minor change to accuracy. The numbers reported here are
with lexicon size set to 800.
11The MaxEnt classifier slightly outperforms other classi-
fiers with the same feature set.
albeit it also lowers the upper bound of algorithms
as the correct language names might not appear
in the document. Second, the MaxEnt classifier
has hundreds of classes, thousands of features, and
millions of model parameters. This will cause se-
vere sparse data and overfitting problems.
6.2 The CoRef approach
For the CoRef approach, we built two systems as
described in Section 5: the first system is a Max-
Ent classifier with beam search, and the second
one is a MLN for joint inference.12 The results
are in Table 8.13
In the first system, the values of F4 features
for the test data come from the gold standard
in the F1-F4 cheating experiments, and come
from beam search in the non-cheating experi-
ments.14 In the second system, the predicate
HasRelation(i1, i2, r) instantiated from the test
data is treated as evidence in the F1-F4 cheat-
ing experiments, and as query in the F1-F4 non-
cheating experiments.
The results for the two systems are very similar
since they use same kinds of features. However,
with Markov logic, it is easy to add predicates and
formulas to allow joint inference. Therefore, we
believe that Markov logic offers more potential to
incorporate arbitrary prior knowledge and lever-
age further opportunities in joint inference.
Tables 7-8 show that, with the same kind of fea-
tures and the same amount of training data, the
CoRef approach has higher upper bound, fewer
model parameters, more training instances, and
much higher accuracy than the CL approach. This
study shows that properly formulating a task into
a learning problem is very important.
12For learning and inference, we used the existing im-
plementations of pseudo-likelihood training and MC-SAT in
Alchemy with default parameters.
13No language filter is needed since the approach links an
IGT to only the language names appearing in the document.
14It turns out that for this task the size of beam does not
matter much and simply using the top choice by the Max-
Ent classifier for each IGT almost always produces the best
results, so that is the setting used for this table and Table 9.
876
Table 8: The performance of the CoRef approach (# of classes=2, # of training instances=511,039)
Upper bound of F1 F1-F2 F1-F3 F1-F4 F1-F4
CoRef approach (cheating) (Non-cheating)
# of features N/A 2 12 17 22 22
Sequence labeling 97.31 54.37 66.32 83.49 90.26 85.10
Markov logic model 97.31 54.98 65.94 83.44 90.37 84.70
Table 9: The performance of the CoRef approach with less training data (the upper bound of the Coref
approach remains 97.31%)
% of training F1 F1-F2 F1-F3 F1-F4 F1-F4 Upper bound of
data used (cheating) (non-cheating) the CL approach
0.1% 54.37 54.84 65.28 81.21 70.15 1.66
0.5% 54.37 62.78 76.74 87.17 80.24 21.15
1.0% 54.37 60.58 76.09 87.24 81.20 28.92
10% 54.37 62.13 77.07 87.20 83.08 54.45
6.3 Experiments with much less data
Table 8 shows that the CoRef approach has very
few features and a much larger number of training
instances; therefore, it is likely that the approach
would work well even with much less training
data. To test the idea, we trained the model with
only a small fraction of the original training data
and tested on the same test data. The results with
the first system are in Table 9. Notice that the up-
per bound of the CoRef approach remains the same
as before. In contrast, the upper bound for the CL
model is much lower, as shown in the last column
of the table. The table shows when there is very
little training data, the CoRef approach still per-
forms decently, whereas the CL approach would
totally fail due to the extremely low upper bounds.
6.4 Error analysis
Several factors contribute to the gap between the
best CoRef system and its upper bound. First,
when several language names appear in close
range, the surface positions of the language names
are often insufficient to determine the prominence
of the languages. For instance, in pattern ?Similar
to L1, L2 ...?, L2 is the more prominent than L1;
whereas in pattern ?L1, a L2 language, ...?, L1 is.
The system sometimes chooses a wrong language
in this case.
Second, the language name detector described
in Section 4.2 produces many false negative (due
to the incompleteness of the language table) and
false positive (due to the fact that language names
often have other meanings).
Third, when a language name is ambiguous,
choosing the correct language code often requires
knowledge that might not even be present in the
document. For instance, a language name could
refer to a list of related languages spoken in the
same region, and assigning a correct language
code would require knowledge about the subtle
differences among those languages.
7 Conclusion and future work
In this paper we describe a language identification
methodology that achieves high accuracy with a
very small amount of training data for hundreds
of languages, significantly outperforming existing
language ID algorithms applied to the task. The
gain comes from two sources: by taking advan-
tage of context information in the document, and
by formulating the task as a coreference resolution
problem.
Our method can be adapted to harvest other
kinds of linguistic data from the Web (e.g., lexicon
entries, word lists, transcriptions, etc.) and build
other ODIN-like resources. Providing a means for
rapidly increasing the amount of data in ODIN,
while at the same time automatically increasing
the number of languages, can have a significant
positive impact on the linguistic community, a
community that already benefits from the existing
search facility in ODIN. Likewise, the increased
size of the resulting ODIN database could pro-
vide sufficient data to bootstrap NLP tools (e.g.,
POS taggers and parsers) for a large number of
low-density languages, greatly benefitting both the
fields of linguistics and NLP.
Acknowledgements This work has been sup-
ported, in part, by the NSF grants BCS-0748919
and BCS-0720670 and ONR grant N00014-08-1-
0670. We would also like to thank three anony-
mous reviewers for their valuable comments.
877
References
Mark C. Baker and Osamuyimen Thompson Stewart.
1996. Unaccusativity and the adjective/verb distinc-
tion: Edo evidence. In Proceedings of the Fifth An-
nual Conference on Document Analysis and Infor-
mation Retrieval (SDAIR), Amherst, Mass.
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola,
B. Taskar, and S. Vishwanathan (eds). 2007. Pre-
dicting Structured Data. MIT Press.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?175,
Las Vegas, US.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines. Available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference reso-
lution using integer programming. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 236?243, Rochester,
New York, April.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven grammar induction. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL
2006), pages 881?888, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC2006), pages 485?488, Genoa, Italy.
S. Kok, P. Singla, M. Richardson, P. Domingos,
M. Sumner, H Poon, and D. Lowd. 2007. The
Alchemy system for statistical relational AI. Tech-
nical report, Dept. of CSE, Univ. of Washington.
William Lewis and Fei Xia. 2008. Automatically Iden-
tifying Computationally Relevant Typological Fea-
tures. In Proc. of the Third International Joint Con-
ference on Natural Language Processing (IJCNLP-
2008), Hyderabad, India.
William Lewis. 2006. ODIN: A Model for Adapting
and Enriching Legacy Infrastructure. In Proc. of the
e-Humanities Workshop, held in cooperation with e-
Science 2006: 2nd IEEE International Conference
on e-Science and Grid Computing, Amsterdam.
Xiaoqiang Luo. 2007. Coreference or not: A
twin model for coreference resolution. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 73?80, Rochester, New
York.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approaches to Coreference Reso-
lution. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2002), pages 104?111, Philadelphia.
H. Poon and P. Domingos. 2006. Sound and effi-
cient inference with probabilistic and deterministic
dependencies. In Proc. of AAAI-06.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings
of the Twenty-Second National Conference on Artifi-
cial Intelligence (AAAI), pages 913?918, Vancouver,
Canada. AAAI Press.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with markov logic. In Proc.
of the 13th Conf. on Empirical Methods in Natural
Language Processing (EMNLP-2008).
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, pages 107?136.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4).
Charles Sutton, Andrew McCallum, and Jeff Bilmes
(eds.). 2006. Proc. of the HLT/NAACL-06 Work-
shop on Joint Inference for Natural Language Pro-
cessing.
B. Wellner, A. McCallum, F. Peng, and M. Hay. 2004.
An integrated, conditional model of information ex-
traction and coreference with application to citation
matching. In Proc. of the 20th Conference on Un-
certainty in AI (UAI 2004).
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 452?459, Rochester,
New York.
Fei Xia and William Lewis. 2008. Repurposing
Theoretical Linguistic Data for Tool Development
and Search. In Proc. of the Third International
Joint Conference on Natural Language Processing
(IJCNLP-2008), Hyderabad, India.
878
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209?217,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Morphological Segmentation with Log-Linear Models
Hoifung Poon?
Dept. of Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
hoifung@cs.washington.edu
Colin Cherry
Microsoft Research
Redmond, WA 98052
colinc@microsoft.com
Kristina Toutanova
Microsoft Research
Redmond, WA 98052
kristout@microsoft.com
Abstract
Morphological segmentation breaks words
into morphemes (the basic semantic units). It
is a key component for natural language pro-
cessing systems. Unsupervised morphologi-
cal segmentation is attractive, because in ev-
ery language there are virtually unlimited sup-
plies of text, but very few labeled resources.
However, most existing model-based systems
for unsupervised morphological segmentation
use directed generative models, making it dif-
ficult to leverage arbitrary overlapping fea-
tures that are potentially helpful to learning.
In this paper, we present the first log-linear
model for unsupervised morphological seg-
mentation. Our model uses overlapping fea-
tures such as morphemes and their contexts,
and incorporates exponential priors inspired
by the minimum description length (MDL)
principle. We present efficient algorithms
for learning and inference by combining con-
trastive estimation with sampling. Our sys-
tem, based on monolingual features only, out-
performs a state-of-the-art system by a large
margin, even when the latter uses bilingual in-
formation such as phrasal alignment and pho-
netic correspondence. On the Arabic Penn
Treebank, our system reduces F1 error by 11%
compared to Morfessor.
1 Introduction
The goal of morphological segmentation is to seg-
ment words into morphemes, the basic syntac-
tic/semantic units. This is a key subtask in many
? This research was conducted during the author?s intern-
ship at Microsoft Research.
NLP applications, including machine translation,
speech recognition and question answering. Past
approaches include rule-based morphological an-
alyzers (Buckwalter, 2004) and supervised learn-
ing (Habash and Rambow, 2005). While successful,
these require deep language expertise and a long and
laborious process in system building or labeling.
Unsupervised approaches are attractive due to the
the availability of large quantities of unlabeled text,
and unsupervised morphological segmentation has
been extensively studied for a number of languages
(Brent et al, 1995; Goldsmith, 2001; Dasgupta and
Ng, 2007; Creutz and Lagus, 2007). The lack
of supervised labels makes it even more important
to leverage rich features and global dependencies.
However, existing systems use directed generative
models (Creutz and Lagus, 2007; Snyder and Barzi-
lay, 2008b), making it difficult to extend them with
arbitrary overlapping dependencies that are poten-
tially helpful to segmentation.
In this paper, we present the first log-linear model
for unsupervised morphological segmentation. Our
model incorporates simple priors inspired by the
minimum description length (MDL) principle, as
well as overlapping features such as morphemes and
their contexts (e.g., in Arabic, the string Al is likely
a morpheme, as is any string between Al and a word
boundary). We develop efficient learning and infer-
ence algorithms using a novel combination of two
ideas from previous work on unsupervised learn-
ing with log-linear models: contrastive estimation
(Smith and Eisner, 2005) and sampling (Poon and
Domingos, 2008).
We focus on inflectional morphology and test our
209
approach on datasets in Arabic and Hebrew. Our
system, using monolingual features only, outper-
forms Snyder & Barzilay (2008b) by a large mar-
gin, even when their system uses bilingual informa-
tion such as phrasal alignment and phonetic corre-
spondence. On the Arabic Penn Treebank, our sys-
tem reduces F1 error by 11% compared to Mor-
fessor Categories-MAP (Creutz and Lagus, 2007).
Our system can be readily applied to supervised
and semi-supervised learning. Using a fraction of
the labeled data, it already outperforms Snyder &
Barzilay?s supervised results (2008a), which further
demonstrates the benefit of using a log-linear model.
2 Related Work
There is a large body of work on the unsupervised
learning of morphology. In addition to morpholog-
ical segmentation, there has been work on unsuper-
vised morpheme analysis, where one needs to deter-
mine features of word forms (Kurimo et al, 2007)
or identify words with the same lemma by model-
ing stem changes (Schone and Jurafsky, 2001; Gold-
smith, 2001). However, we focus our review specif-
ically on morphological segmentation.
In the absence of labels, unsupervised learning
must incorporate a strong learning bias that reflects
prior knowledge about the task. In morphological
segmentation, an often-used bias is the minimum
description length (MDL) principle, which favors
compact representations of the lexicon and corpus
(Brent et al, 1995; Goldsmith, 2001; Creutz and La-
gus, 2007). Other approaches use statistics on mor-
pheme context, such as conditional entropy between
adjacent n-grams, to identify morpheme candidates
(Harris, 1955; Keshava and Pitler, 2006). In this pa-
per, we incorporate both intuitions into a simple yet
powerful model, and show that each contributes sig-
nificantly to performance.
Unsupervised morphological segmentation sys-
tems also differ from the engineering perspective.
Some adopt a pipeline approach (Schone and Ju-
rafsky, 2001; Dasgupta and Ng, 2007; Demberg,
2007), which works by first extracting candidate
affixes and stems, and then segmenting the words
based on the candidates. Others model segmenta-
tion using a joint probabilistic distribution (Goldwa-
ter et al, 2006; Creutz and Lagus, 2007; Snyder and
Barzilay, 2008b); they learn the model parameters
from unlabeled data and produce the most proba-
ble segmentation as the final output. The latter ap-
proach is arguably more appealing from the mod-
eling standpoint and avoids error propagation along
the pipeline. However, most existing systems use
directed generative models; Creutz & Lagus (2007)
used an HMM, while Goldwater et al (2006) and
Snyder & Barzilay (2008b) used Bayesian models
based on Pitman-Yor or Dirichlet processes. These
models are difficult to extend with arbitrary overlap-
ping features that can help improve accuracy.
In this work we incorporate novel overlapping
contextual features and show that they greatly im-
prove performance. Non-overlapping contextual
features previously have been used in directed gen-
erative models (in the form of Markov models) for
unsupervised morphological segmentation (Creutz
and Lagus, 2007) or word segmentation (Goldwater
et al, 2007). In terms of feature sets, our model is
most closely related to the constituent-context model
proposed by Klein and Manning (2001) for grammar
induction. If we exclude the priors, our model can
also be seen as a semi-Markov conditional random
field (CRF) model (Sarawagi and Cohen, 2004).
Semi-Markov CRFs previously have been used for
supervised word segmentation (Andrew, 2006), but
not for unsupervised morphological segmentation.
Unsupervised learning with log-linear models has
received little attention in the past. Two notable ex-
ceptions are Smith & Eisner (2005) for POS tagging,
and Poon & Domingos (2008) for coreference res-
olution. Learning with log-linear models requires
computing the normalization constant (a.k.a. the
partition function) Z . This is already challenging in
supervised learning. In unsupervised learning, the
difficulty is further compounded by the absence of
supervised labels. Smith & Eisner (2005) proposed
contrastive estimation, which uses a small neighbor-
hood to compute Z . The neighborhood is carefully
designed so that it not only makes computation eas-
ier but also offers sufficient contrastive information
to aid unsupervised learning. Poon & Domingos
(2008), on the other hand, used sampling to approx-
imate Z .1 In this work, we benefit from both tech-
niques: contrastive estimation creates a manageable,
1Rosenfeld (1997) also did this for language modeling.
210
wvlAvwn
(##__##)
w
(##__vl)
vlAv
(#w__wn)
wn
(Av__##)
Figure 1: The morpheme and context (in parentheses)
features for the segmented word w-vlAv-wn.
informative Z , while sampling enables the use of
powerful global features.
3 Log-Linear Model for Unsupervised
Morphological Segmentation
Central to our approach is a log-linear model that
defines the joint probability distribution for a cor-
pus (i.e., the words) and a segmentation on the cor-
pus. The core of this model is a morpheme-context
model, with one feature for each morpheme,2 and
one feature for each morpheme context. We rep-
resent contexts using the n-grams before and after
the morpheme, for some constant n. To illustrate
this, a segmented Arabic corpus is shown below
along with its features, assuming we are tracking bi-
gram contexts. The segmentation is indicated with
hyphens, while the hash symbol (#) represents the
word boundary.
Segmented Corpus hnAk w-vlAv-wn bn-w
Al-ywm Al-jmAEp
Morpheme Feature:Value hnAk:1 w:2 vlAv:1
wn:1 bn:1 Al:2 ywm:1 jmAEp:1
hnAk:1 wvlAvwn:1 bnw:1 Alywm:1 Alj-
mAEp:1
Bigram Context Feature:Value ## vl:1
#w wn:1 Av ##:1 ## w#:1 bn ##:1
## yw:1 Al ##:2 ## jm:1 ## ##:5
Furthermore, the corresponding features for the seg-
mented word w-vlAv-wn are shown in Figure 1.
Each feature is associated with a weight, which
correlates with the likelihood that the correspond-
ing morpheme or context marks a valid morpholog-
ical segment. Such overlapping features allow us to
capture rich segmentation regularities. For example,
given the Arabic word Alywm, to derive its correct
segmentation Al-ywm, it helps to know that Al and
ywm are likely morphemes whereas Aly or lyw are
2The word as a whole is also treated as a morpheme in itself.
not; it also helps to know that Al ## or ## yw are
likely morpheme contexts whereas ly ## or ## wm
are not. Ablation tests verify the importance of these
overlapping features (see Section 7.2).
Our morpheme-context model is inspired by
the constituent-context model (CCM) proposed by
Klein and Manning (2001) for grammar induction.
The morphological segmentation of a word can be
viewed as a flat tree, where the root node corre-
sponds to the word and the leaves correspond to
morphemes (see Figure 1). The CCM uses uni-
grams for context features. For this task, however,
we found that bigrams and trigrams lead to much
better accuracy. We use trigrams in our full model.
For learning, one can either view the corpus as
a collection of word types (unique words) or tokens
(word occurrences). Some systems (e.g., Morfessor)
use token frequency for parameter estimation. Our
system, however, performs much better using word
types. This has also been observed for other mor-
phological learners (Goldwater et al, 2006). Thus
we use types in learning and inference, and effec-
tively enforce the constraint that words can have
only one segmentation per type. Evaluation is still
based on tokens to reflect the performance in real
applications.
In addition to the features of the morpheme-
context model, we incorporate two priors which cap-
ture additional intuitions about morphological seg-
mentations. First, we observe that the number of
distinct morphemes used to segment a corpus should
be small. This is achieved when the same mor-
phemes are re-used across many different words.
Our model incorporates this intuition by imposing
a lexicon prior: an exponential prior with nega-
tive weight on the length of the morpheme lexi-
con. We define the lexicon to be the set of unique
morphemes identified by a complete segmentation
of the corpus, and the lexicon length to be the to-
tal number of characters in the lexicon. In this
way, we can simultaneously emphasize that a lexi-
con should contain few unique morphemes, and that
those morphemes should be short. However, the lex-
icon prior alone incorrectly favors the trivial seg-
mentation that shatters each word into characters,
which results in the smallest lexicon possible (sin-
gle characters). Therefore, we also impose a corpus
prior: an exponential prior on the number of mor-
211
phemes used to segment each word in the corpus,
which penalizes over-segmentation. We notice that
longer words tend to have more morphemes. There-
fore, each word?s contribution to this prior is nor-
malized by the word?s length in characters (e.g., the
segmented word w-vlAv-wn contributes 3/7 to the to-
tal corpus size). Notice that it is straightforward to
incorporate such a prior in a log-linear model, but
much more challenging to do so in a directed gen-
erative model. These two priors are inspired by the
minimum description length (MDL) length princi-
ple; the lexicon prior favors fewer morpheme types,
whereas the corpus prior favors fewer morpheme to-
kens. They are vital to the success of our model,
providing it with the initial inductive bias.
We also notice that often a word is decomposed
into a stem and some prefixes and suffixes. This is
particularly true for languages with predominantly
inflectional morphology, such as Arabic, Hebrew,
and English. Thus our model uses separate lexicons
for prefixes, stems, and suffixes. This results in a
small but non-negligible accuracy gain in our exper-
iments. We require that a stem contain at least two
characters and no fewer characters than any affixes
in the same word.3 In a given word, when a mor-
pheme is identified as the stem, any preceding mor-
pheme is identified as a prefix, whereas any follow-
ing morpheme as a suffix. The sample segmented
corpus mentioned earlier induces the following lex-
icons:
Prefix w Al
Stem hnAk vlAv bn ywm jmAEp
Suffix wn w
Before presenting our formal model, we first in-
troduce some notation. Let W be a corpus (i.e., a set
of words), and S be a segmentation that breaks each
word in W into prefixes, a stem, and suffixes. Let ?
be a string (character sequence). Each occurrence of
? will be in the form of ?1??2, where ?1, ?2 are the
adjacent character n-grams, and c = (?1, ?2) is the
context of ? in this occurrence. Thus a segmentation
can be viewed as a set of morpheme strings and their
contexts. For a string x, L(x) denotes the number of
characters in x; for a word w, MS(w) denotes the
3In a segmentation where several morphemes have the max-
imum length, any of them can be identified as the stem, each
resulting in a distinct segmentation.
number of morphemes in w given the segmentation
S; Pref(W,S), Stem(W,S), Suff(W,S) denote
the lexicons of prefixes, stems, and suffixes induced
by S for W . Then, our model defines a joint proba-
bility distribution over a restricted set of W and S:
P?(W,S) = 1Z ? u?(W,S)
where
u?(W,S) = exp(
?
?
??f?(S) +
?
c
?cfc(S)
+ ? ? ?
??Pref(W,S)
L(?)
+ ? ? ?
??Stem(W,S)
L(?)
+ ? ? ?
??Suff(W,S)
L(?)
+ ? ? ?
w?W
MS(w)/L(w) )
Here, f?(S) and fc(S) are respectively the occur-
rence counts of morphemes and contexts under S,
and ? = (??, ?c : ?, c) are their feature weights.
?, ? are the weights for the priors. Z is the nor-
malization constant, which sums over a set of cor-
pora and segmentations. In the next section, we will
define this set for our model and show how to effi-
ciently perform learning and inference.
4 Unsupervised Learning
As mentioned in Smith & Eisner (2005), learning
with probabilistic models can be viewed as moving
probability mass to the observed data. The question
is from where to take this mass. For log-linear mod-
els, the answer amounts to defining the set that Z
sums over. We use contrastive estimation and define
the set to be a neighborhood of the observed data.
The instances in the neighborhood can be viewed
as pseudo-negative examples, and learning seeks to
discriminate them from the observed instances.
Formally, let W ? be the observed corpus, and let
N(?) be a function that maps a string to a set of
strings; let N(W ?) denote the set of all corpora that
can be derived from W ? by replacing every word
w ?W ? with one in N(w). Then,
Z = ?
W?N(W ?)
?
S
u(W,S).
212
Unsupervised learning maximizes the log-likelihood
of observing W ?
L?(W ?) = log
?
S
P (W ?, S)
We use gradient descent for this optimization; the
partial derivatives for feature weights are
?
??i
L?(W ?) = ES|W ?[fi]? ES,W [fi]
where i is either a string ? or a context c. The first
expected count ranges over all possible segmenta-
tions while the words are fixed to those observed in
W ?. For the second expected count, the words also
range over the neighborhood.
Smith & Eisner (2005) considered various neigh-
borhoods for unsupervised POS tagging, and
showed that the best neighborhoods are TRANS1
(transposing any pair of adjacent words) and
DELORTRANS1 (deleting any word or transposing
any pair of adjacent words). We can obtain their
counterparts for morphological segmentation by
simply replacing ?words? with ?characters?. As
mentioned earlier, the instances in the neighbor-
hood serve as pseudo-negative examples from which
probability mass can be taken away. In this regard,
DELORTRANS1 is suitable for POS tagging since
deleting a word often results in an ungrammatical
sentence. However, in morphology, a word less a
character is often a legitimate word too. For exam-
ple, deleting l from the Hebrew word lyhwh (to the
lord) results in yhwh (the lord). Thus DELORTRANS1
forces legal words to compete against each other for
probability mass, which seems like a misguided ob-
jective. Therefore, in our model we use TRANS1. It
is suited for our task because transposing a pair of
adjacent characters usually results in a non-word.
To combat overfitting in learning, we impose a
Gaussian prior (L2 regularization) on all weights.
5 Supervised Learning
Our learning algorithm can be readily applied to su-
pervised or semi-supervised learning. Suppose that
gold segmentation is available for some words, de-
noted as S?. If S? contains gold segmentations
for all words in W , we are doing supervised learn-
ing; otherwise, learning is semi-supervised. Train-
ing now maximizes L?(W ?, S?); the partial deriva-
tives become
?
??i
L?(W ?, S?) = ES|W ?,S?[fi] ? ES,W [fi]
The only difference in comparison with unsuper-
vised learning is that we fix the known segmenta-
tion when computing the first expected counts. In
Section 7.3, we show that when labels are available,
our model also learns much more effectively than a
directed graphical model.
6 Inference
In Smith & Eisner (2005), the objects (sentences) are
independent from each other, and exact inference is
tractable. In our model, however, the lexicon prior
renders all objects (words) interdependent in terms
of segmentation decisions. Consider the simple cor-
pus with just two words: Alrb, lAlrb. If lAlrb is seg-
mented into l-Al-rb, Alrb can be segmented into Al-
rb without paying the penalty imposed by the lexi-
con prior. If, however, lAlrb remains a single mor-
pheme, and we still segment Alrb into Al-rb, then
we introduce two new morphemes into the lexicons,
and we will be penalized by the lexicon prior ac-
cordingly. As a result, we must segment the whole
corpus jointly, making exact inference intractable.
Therefore, we resort to approximate inference. To
compute ES|W ?[fi], we use Gibbs sampling. To de-
rive a sample, the procedure goes through each word
and samples the next segmentation conditioned on
the segmentation of all other words. With m sam-
ples S1, ? ? ? , Sm, the expected count can be approx-
imated as
ES|W ?[fi] ? 1m
?
j
fi(Sj)
There are 2n?1 ways to segment a word of n char-
acters. To sample a new segmentation for a partic-
ular word, we need to compute conditional proba-
bility for each of these segmentations. We currently
do this by explicit enumeration.4 When n is large,
4These segmentations could be enumerated implicitly us-
ing the dynamic programming framework employed by semi-
Markov CRFs (Sarawagi and Cohen, 2004). However, in such a
setting, our lexicon prior would likely need to be approximated.
We intend to investigate this in future work.
213
this is very expensive. However, we observe that
the maximum number of morphemes that a word
contains is usually a small constant for many lan-
guages; in the Arabic Penn Treebank, the longest
word contains 14 characters, but the maximum num-
ber of morphemes in a word is only 5. Therefore,
we impose the constraint that a word can be seg-
mented into no more than k morphemes, where k
is a language-specific constant. We can determine
k from prior knowledge or use a development set.
This constraint substantially reduces the number of
segmentation candidates to consider; with k = 5, it
reduces the number of segmentations to consider by
almost 90% for a word of 14 characters.
ES,W [fi] can be computed by Gibbs sampling in
the same way, except that in each step we also sam-
ple the next word from the neighborhood, in addition
to the next segmentation.
To compute the most probable segmentation, we
use deterministic annealing. It works just like a sam-
pling algorithm except that the weights are divided
by a temperature, which starts with a large value and
gradually drops to a value close to zero. To make
burn-in faster, when computing the expected counts,
we initialize the sampler with the most probable seg-
mentation output by annealing.
7 Experiments
We evaluated our system on two datasets. Our main
evaluation is on a multi-lingual dataset constructed
by Snyder & Barzilay (2008a; 2008b). It consists of
6192 short parallel phrases in Hebrew, Arabic, Ara-
maic (a dialect of Arabic), and English. The paral-
lel phrases were extracted from the Hebrew Bible
and its translations via word alignment and post-
processing. For Arabic, the gold segmentation was
obtained using a highly accurate Arabic morpholog-
ical analyzer (Habash and Rambow, 2005); for He-
brew, from a Bible edition distributed by Westmin-
ster Hebrew Institute (Groves and Lowery, 2006).
There is no gold segmentation for English and Ara-
maic. Like Snyder & Barzilay, we evaluate on the
Arabic and Hebrew portions only; unlike their ap-
proach, our system does not use any bilingual in-
formation. We refer to this dataset as S&B . We
also report our results on the Arabic Penn Treebank
(ATB), which provides gold segmentations for an
Arabic corpus with about 120,000 Arabic words.
As in previous work, we report recall, precision,
and F1 over segmentation points. We used 500
phrases from the S&B dataset for feature develop-
ment, and also tuned our model hyperparameters
there. The weights for the lexicon and corpus pri-
ors were set to ? = ?1, ? = ?20. The feature
weights were initialized to zero and were penalized
by a Gaussian prior with ?2 = 100. The learning
rate was set to 0.02 for all experiments, except the
full Arabic Penn Treebank, for which it was set to
0.005.5 We used 30 iterations for learning. In each
iteration, 200 samples were collected to compute
each of the two expected counts. The sampler was
initialized by running annealing for 2000 samples,
with the temperature dropping from 10 to 0.1 at 0.1
decrements. The most probable segmentation was
obtained by running annealing for 10000 samples,
using the same temperature schedule. We restricted
the segmentation candidates to those with no greater
than five segments in all experiments.
7.1 Unsupervised Segmentation on S&B
We followed the experimental set-up of Snyder &
Barzilay (2008b) to enable a direct comparison. The
dataset is split into a training set with 4/5 of the
phrases, and a test set with the remaining 1/5. First,
we carried out unsupervised learning on the training
data, and computed the most probable segmentation
for it. Then we fixed the learned weights and the seg-
mentation for training, and computed the most prob-
able segmentation for the test set, on which we eval-
uated.6 Snyder & Barzilay (2008b) compared sev-
eral versions of their systems, differing in how much
bilingual information was used. Using monolingual
information only, their system (S&B-MONO) trails
the state-of-the-art system Morfessor; however, their
best system (S&B-BEST), which uses bilingual in-
formation that includes phrasal alignment and pho-
netic correspondence between Arabic and Hebrew,
outperforms Morfessor and achieves the state-of-
the-art results on this dataset.
5The ATB set is more than an order of magnitude larger and
requires a smaller rate.
6With unsupervised learning, we can use the entire dataset
for training since no labels are provided. However, this set-
up is necessary for S&B?s system because they used bilingual
information in training, which is not available at test time.
214
ARABIC Prec. Rec. F1
S&B-MONO 53.0 78.5 63.2
S&B-BEST 67.8 77.3 72.2
FULL 76.0 80.2 78.1
HEBREW Prec. Rec. F1
S&B-MONO 55.8 64.4 59.8
S&B-BEST 64.9 62.9 63.9
FULL 67.6 66.1 66.9
Table 1: Comparison of segmentation results on the S&B
dataset.
Table 1 compares our system with theirs. Our sys-
tem outperforms both S&B-MONO and S&B-BEST
by a large margin. For example, on Arabic, our sys-
tem reduces F1 error by 21% compared to S&B-
BEST, and by 40% compared to S&B-MONO. This
suggests that the use of monolingual morpheme con-
text, enabled by our log-linear model, is more help-
ful than their bilingual cues.
7.2 Ablation Tests
To evaluate the contributions of the major compo-
nents in our model, we conducted seven ablation
tests on the S&B dataset, each using a model that
differed from our full model in one aspect. The first
three tests evaluate the effect of priors, whereas the
next three test the effect of context features. The
last evaluates the impact of using separate lexicons
for affixes and stems.
NO-PRIOR The priors are not used.
NO-COR-PR The corpus prior is not used.
NO-LEX-PR The lexicon prior is not used.
NO-CONTEXT Context features are not used.
UNIGRAM Unigrams are used in context.
BIGRAM Bigrams are used in context.
SG-LEXICON A single lexicon is used, rather than
three distinct ones for the affixes and stems.
Table 2 presents the ablation results in compari-
son with the results of the full model. When some or
all priors are excluded, the F1 score drops substan-
tially (over 10 points in all cases, and over 40 points
in some). In particular, excluding the corpus prior,
as in NO-PRIOR and NO-COR-PR, results in over-
segmentation, as is evident from the high recalls and
low precisions. When the corpus prior is enacted
but not the lexicon priors (NO-LEX-PR), precision
ARABIC Prec. Rec. F1
FULL 76.0 80.2 78.1
NO-PRIOR 24.6 89.3 38.6
NO-COR-PR 23.7 87.4 37.2
NO-LEX-PR 79.1 51.3 62.3
NO-CONTEXT 71.2 62.1 66.3
UNIGRAM 71.3 76.5 73.8
BIGRAM 73.1 78.4 75.7
SG-LEXICON 72.8 82.0 77.1
HEBREW Prec. Rec. F1
FULL 67.6 66.1 66.9
NO-PRIOR 34.0 89.9 49.4
NO-COR-PR 35.6 90.6 51.1
NO-LEX-PR 65.9 49.2 56.4
NO-CONTEXT 63.0 47.6 54.3
UNIGRAM 63.0 63.7 63.3
BIGRAM 69.5 66.1 67.8
SG-LEXICON 67.4 65.7 66.6
Table 2: Ablation test results on the S&B dataset.
is much higher, but recall is low; the system now errs
on under-segmentation because recurring strings are
often not identified as morphemes.
A large accuracy drop (over 10 points in F1
score) also occurs when the context features are
excluded (NO-CONTEXT), which underscores the
importance of these overlapping features. We also
notice that the NO-CONTEXT model is compara-
ble to the S&B-MONO model; they use the same
feature types, but different priors. The accuracies of
the two systems are comparable, which suggests that
we did not sacrifice accuracy by trading the more
complex and restrictive Dirichlet process prior for
exponential priors. A priori, it is unclear whether us-
ing contexts larger than unigrams would help. While
potentially beneficial, they also risk aggravating the
data sparsity and making our model more prone to
overfitting. For this problem, however, enlarging the
context (using higher n-grams up to trigrams) helps
substantially. For Arabic, the highest accuracy is at-
tained by using trigrams, which reduces F1 error by
16% compared to unigrams; for Hebrew, by using
bigrams, which reduces F1 error by 17%. Finally, it
helps to use separate lexicons for affixes and stems,
although the difference is small.
215
ARABIC %Lbl. Prec. Rec. F1
S&B-MONO-S 100 73.2 92.4 81.7
S&B-BEST-S 200 77.8 92.3 84.4
FULL-S 25 84.9 85.5 85.2
50 88.2 86.8 87.5
75 89.6 86.4 87.9
100 91.7 88.5 90.0
HEBREW %Lbl. Prec. Rec. F1
S&B-MONO-S 100 71.4 79.1 75.1
S&B-BEST-S 200 76.8 79.2 78.0
FULL-S 25 78.7 73.3 75.9
50 82.8 74.6 78.4
75 83.1 77.3 80.1
100 83.0 78.9 80.9
Table 3: Comparison of segmentation results with super-
vised and semi-supervised learning on the S&B dataset.
7.3 Supervised and Semi-Supervised Learning
To evaluate our system in the supervised and semi-
supervised learning settings, we report the perfor-
mance when various amounts of labeled data are
made available during learning, and compare them
to the results of Snyder & Barzilay (2008a). They
reported results for supervised learning using mono-
lingual features only (S&B-MONO-S), and for su-
pervised bilingual learning with labels for both lan-
guages (S&B-BEST-S). On both languages, our sys-
tem substantially outperforms both S&B-MONO-S
and S&B-BEST-S. E.g., on Arabic, our system re-
duces F1 errors by 46% compared to S&B-MONO-
S, and by 36% compared to S&B-BEST-S. More-
over, with only one-fourth of the labeled data, our
system already outperforms S&B-MONO-S. This
demonstrates that our log-linear model is better
suited to take advantage of supervised labels.
7.4 Arabic Penn Treebank
We also evaluated our system on the Arabic Penn
Treebank (ATB). As is common in unsupervised
learning, we trained and evaluated on the entire set.
We compare our system with Morfessor (Creutz and
Lagus, 2007).7 In addition, we compare with Mor-
fessor Categories-MAP, which builds on Morfessor
and conducts an additional greedy search specifi-
cally tailored to segmentation. We found that it per-
7We cannot compare with Snyder & Barzilay?s system as its
strongest results require bilingual data, which is not available.
ATB-7000 Prec. Rec. F1
MORFESSOR-1.0 70.6 34.3 46.1
MORFESSOR-MAP 86.9 46.4 60.5
FULL 83.4 77.3 80.2
ATB Prec. Rec. F1
MORFESSOR-1.0 80.7 20.4 32.6
MORFESSOR-MAP 77.4 72.6 74.9
FULL 88.5 69.2 77.7
Table 4: Comparison of segmentation results on the Ara-
bic Penn Treebank.
forms much better than Morfessor on Arabic but
worse on Hebrew. To test each system in a low-
data setting, we also ran experiments on the set con-
taining the first 7,000 words in ATB with at least
two characters (ATB-7000). Table 4 shows the re-
sults. Morfessor performs rather poorly on ATB-
7000. Morfessor Categories-MAP does much bet-
ter, but its performance is dwarfed by our system,
which further cuts F1 error by half. On the full ATB
dataset, Morfessor performs even worse, whereas
Morfessor Categories-MAP benefits from the larger
dataset and achieves an F1 of 74.9. Still, our system
substantially outperforms it, further reducing F1 er-
ror by 11%.8
8 Conclusion
This paper introduces the first log-linear model for
unsupervised morphological segmentation. It lever-
ages overlapping features such as morphemes and
their contexts, and enables easy extension to incor-
porate additional features and linguistic knowledge.
For Arabic and Hebrew, it outperforms the state-
of-the-art systems by a large margin. It can also
be readily applied to supervised or semi-supervised
learning when labeled data is available. Future di-
rections include applying our model to other in-
flectional and agglutinative languages, modeling in-
ternal variations of morphemes, leveraging parallel
data in multiple languages, and combining morpho-
logical segmentation with other NLP tasks, such as
machine translation.
8Note that the ATB and ATB-7000 experiments each mea-
sure accuracy on their entire training set. This difference in
testing conditions explains why some full ATB results are lower
than ATB-7000.
216
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in minimum description length induction.
In Proceedings of the 15th Annual Conference of the
Cognitive Science Society.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In Proceedings of Human Language
Technology (NAACL).
Vera Demberg. 2007. A language-independent unsuper-
vised model for morphological segmentation. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Repub-
lic.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In Advances in Neu-
ral Information Processing Systems 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word segmenta-
tion: Context is important. In Proceedings of the 31st
Boston University Conference on Language Develop-
ment.
Alan Groves and Kirk Lowery, editors. 2006. The West-
minster Hebrew Bible Morphology Database. West-
minster Hebrew Institute, Philadelphia, PA, USA.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Samarth Keshava and Emily Pitler. 2006. A simple, intu-
itive approach to morpheme induction. In Proceedings
of 2nd Pascal Challenges Workshop, Venice, Italy.
Dan Klein and Christopher D. Manning. 2001. Natu-
ral language grammar induction using a constituent-
context model. In Advances in Neural Information
Processing Systems 14.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 649?
658, Honolulu, HI. ACL.
Ronald Rosenfeld. 1997. A whole sentence maximum
entropy language model. In IEEE workshop on Auto-
matic Speech Recognition and Understanding.
Sunita Sarawagi and William Cohen. 2004. Semimarkov
conditional random fields for information extraction.
In Proceedings of the Twenty First International Con-
ference on Machine Learning.
Patrick Schone and Daniel Jurafsky. 2001. Knowlege-
free induction of inflectional morphologies. In Pro-
ceedings of Human Language Technology (NAACL).
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
Proceedings of the Twenty Third National Conference
on Artificial Intelligence.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics.
217
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 813?821,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Joint Inference for Knowledge Extraction from Biomedical Literature
Hoifung Poon?
Dept. of Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
hoifung@cs.washington.edu
Lucy Vanderwende
Microsoft Research
Redmond, WA 98052
Lucy.Vanderwende@microsoft.com
Abstract
Knowledge extraction from online reposito-
ries such as PubMed holds the promise of
dramatically speeding up biomedical research
and drug design. After initially focusing on
recognizing proteins and binary interactions,
the community has recently shifted their at-
tention to the more ambitious task of recogniz-
ing complex, nested event structures. State-of-
the-art systems use a pipeline architecture in
which the candidate events are identified first,
and subsequently the arguments. This fails
to leverage joint inference among events and
arguments for mutual disambiguation. Some
joint approaches have been proposed, but they
still lag much behind in accuracy. In this pa-
per, we present the first joint approach for bio-
event extraction that obtains state-of-the-art
results. Our system is based on Markov logic
and adopts a novel formulation by jointly pre-
dicting events and arguments, as well as indi-
vidual dependency edges that compose the ar-
gument paths. On the BioNLP?09 Shared Task
dataset, it reduced F1 errors by more than 10%
compared to the previous best joint approach.
1 Introduction
Extracting knowledge from unstructured text has
been a long-standing goal of NLP and AI. The ad-
vent of the World Wide Web further increases its
importance and urgency by making available an as-
tronomical number of online documents containing
virtually unlimited amount of knowledge (Craven et
? This research was conducted during the author?s intern-
ship at Microsoft Research.
al., 1999). A salient example domain is biomedical
literature: the PubMed1 online repository contains
over 18 million abstracts on biomedical research,
with more than two thousand new abstracts added
each day; the abstracts are written in grammatical
English, which enables the use of advanced NLP
tools such as syntactic and semantic parsers.
Traditionally, research on knowledge extraction
from text is primarily pursued in the field of in-
formation extraction with a rather confined goal of
extracting instances for flat relational schemas with
no nested structures (e.g, recognizing protein names
and protein-protein interaction (PPI)). This restric-
tion mainly stems from limitations in available re-
sources and algorithms. The BioNLP?09 Shared
Task (Kim et al, 2009) is one of the first that
faced squarely information needs that are complex
and highly structured. It aims to extract nested
bio-molecular events from research abstracts, where
an event may have variable number of arguments
and may contain other events as arguments. Such
nested events are ubiquitous in biomedical literature
and can effectively represent complex biomedical
knowledge and subsequently support reasoning and
automated discovery. The task has generated much
interest, with twenty-four teams having submitted
their results. The top system by UTurku (Bjorne et
al., 2009) attained the state-of-the-art F1 of 52.0%.
The nested event structures make this task partic-
ularly attractive for applying joint inference. By al-
lowing information to propagate among events and
arguments, joint inference can facilitate mutual dis-
ambiguation and potentially lead to substantial gain
1http://www.ncbi.nlm.nih.gov/pubmed
813
in predictive accuracy. However, joint inference is
underexplored for this task. Most participants ei-
ther reduced the task to classification (e.g., by using
SVM), or used heuristics to combine manual rules
and statistics. The previous best joint approach was
Riedel et al (2009). While competitive, it still lags
UTurku by more than 7 points in F1.
In this paper, we present the first joint approach
that achieves state-of-the-art results for bio-event ex-
traction. Like Riedel et al (2009), our system
is based on Markov logic, but we adopted a novel
formulation that models dependency edges in ar-
gument paths and jointly predicts them along with
events and arguments. By expanding the scope of
joint inference to include individual argument edges,
our system can leverage fine-grained correlations to
make learning more effective. On the development
set, by merely adding a few joint inference formu-
las to a simple logistic regression model, our system
raised F1 from 28% to 54%, already tying UTurku.
We also presented a heuristic method to fix errors
in syntactic parsing by leveraging available semantic
information from task input, and showed that this in
turn led to substantial performance gain in the task.
Overall, our final system reduced F1 error by more
than 10% compared to Riedel et al (2009).
We begin by describing the shared task and re-
lated work. We then introduce Markov logic and our
Markov Logic Network (MLN) for joint bio-event
extraction. Finally, we present our experimental re-
sults and conclude.
2 Bio-Event Extraction
We follow the BioNLP?09 Shared Task (Kim et
al., 2009) on problem setup for bio-event extrac-
tion. A bio-molecular event (bio-event) refers to
the change of state for bio-molecules such as DNAs
and proteins. The goal is to extract these events
from unstructured text such as biomedical abstracts.
For each event, one needs to identify the trigger
words that signifies the event and the theme argu-
ments that undergo the change. In addition, for
regulation events, the cause argument also needs to
be identified if it is present. The task considers
nine event types: Expression, Transcription,
Localization, Phosphorylation, Catabolism,
Binding, Regulation, Positive regulation,
and Negative regulation. Only Binding can
take multiple themes. Regulation events may take
events as arguments. To facilitate evaluation, the
task fixes the type of non-event arguments to pro-
tein and provides ground truth of protein mentions
as input. 2
Like any NLP task, ambiguity is a central prob-
lem. The same event can be expressed in many
variations. For example, a Negative regulation
event may be signified by ?inhibition?, ?down-
regulation?, ?is abrogated by?, to name a few. On
the other hand, depending on the context, the same
expression may represent different events. For ex-
ample, ?level? may signify any one of five event
types in the training set, or signify none.
In addition, the nested event structures present
new challenges to knowledge extraction systems. To
recognize a complex event, besides from identifying
the event type and trigger words, one also needs to
identify its arguments and recursively identify their
event structures. A mistake in any part will render a
failure in this extraction.
The interdependencies among events and argu-
ments naturally argue for joint predictions. For
example, given the snippet ?the level of VCAM-
1 mRNA?, knowing that ?level? might signify an
event helps to recognize the prepositional phrase
(PP) as its theme. Conversely, the presence of the
PP suggests that ?level? is likely an event. More-
over, the word ?mRNA? in the PP indicates that the
event type is probably Transcription.
Most existing systems adopt a pipeline architec-
ture and reduce the task to independent classifica-
tions of events and arguments. For example, the best
system UTurku (Bjorne et al, 2009) first extracts a
list of candidate triggers with types, and then deter-
mines for each pair of candidate triggers or proteins
whether one is a theme or cause of the other. The
triggers missed in the first stage can never be recov-
ered in the second one. Moreover, since the second
stage is trained with gold triggers as input, any trig-
ger identified in the first stage tends to get at least
2The Shared Task also defines two other tasks (Tasks 2 and
3), which aim either to extract additional arguments (e.g., sites),
or to determine if an event is a negation or speculation. In this
paper, we focus on the core task (Task 1) as it is what most sys-
tems participate in, but our approach can be extended straight-
forwardly to handle the other tasks.
814
one argument, even though it may not be an event at
all. As a result, the authors had to use an ad hoc pro-
cedure to trade off precision and recall for the final
prediction task while training the first-stage extrac-
tor. In addition, each trigger or argument is classified
independently using a multi-class SVM.
While joint inference can potentially improve ac-
curacy, in practice, it is often very challenging to
make it work (Poon and Domingos, 2007). The pre-
vious best joint approach for this task was proposed
by Riedel et al (2009) (labeled UT+DBLS in Kim
et al (2009)). Their system is also based on Markov
logic (Domingos and Lowd, 2009). While compet-
itive (ranked fourth in the evaluation), their system
still lags UTurku by more than 7 points in F1.
Most systems, Riedel et al?s included, classify
each candidate argument path as a whole. A notable
exception is the UTokyo system (Saetre et al, 2009),
which incorporated sequential modeling by adapt-
ing a state-of-the-art PPI system based on MEMM.
But they considered adjacent words in the sentence,
which offered little help in this task, and their system
trailed UTurku by 15 points in F1.
All top systems for event extraction relied heav-
ily on syntactic features. We went one step further
by formulating joint predictions directly on depen-
dency edges. While this leverages sequential corre-
lation along argument paths, it also makes our sys-
tem more prone to the adverse effect of syntactic
errors. Joint syntactic and semantic processing has
received much attention lately (Hajic et al, 2009).
In this paper, we explore using a heuristic method
to correct syntactic errors based on semantic infor-
mation, and show that it leads to significant perfor-
mance gain for event extraction.
3 Markov Logic
In many NLP applications, there exist rich relation
structures among objects, and recent work in statisti-
cal relational learning (Getoor and Taskar, 2007) and
structured prediction (Bakir et al, 2007) has shown
that leveraging these can greatly improve accuracy.
One of the leading frameworks for joint inference
is Markov logic, a probabilistic extension of first-
order logic (Domingos and Lowd, 2009). A Markov
logic network (MLN) is a set of weighted first-order
clauses. Together with a set of constants, it defines a
Markov network with one node per ground atom and
one feature per ground clause. The weight of a fea-
ture is the weight of the first-order clause that gener-
ated it. The probability of a state x in such a network
is given by P (x) = (1/Z) exp (?i wifi(x)), where
Z is a normalization constant, wi is the weight of the
ith clause, fi = 1 if the ith clause is true, and fi = 0
otherwise.
Markov logic makes it possible to compactly
specify probability distributions over complex re-
lational domains. Efficient inference can be per-
formed using MC-SAT (Poon and Domingos, 2006).
MC-SAT is a ?slice sampling? Markov chain Monte
Carlo algorithm that uses an efficient satisfiability
solver to propose the next sample. It is orders of
magnitude faster than previous MCMC algorithms
like Gibbs sampling, making efficient sampling pos-
sible on a scale that was previously out of reach.
Supervised learning for Markov logic maximizes
the conditional log-likelihood of query predicates
given the evidence in the train data. This learning
objective is convex and can be optimized using gra-
dient descent, where the gradient is estimated using
MC-SAT.
In practice, it is often difficult to tune the learn-
ing rate, especially when the number of ground-
ings varies widely among clauses (known as ill-
conditioning in numerical optimization). This prob-
lem is particularly severe in relational domains. One
remedy is to apply preconditioning to the gradient.
For example, Poon & Domingos (2007) divided the
global learning rate by the number of true ground-
ings of the corresponding clause in the training data,
whereas Lowd & Domingos (2007) divided it by the
variance of the clause (also estimated using MC-
SAT). The latter can be viewed as approximating
the Hessian with its diagonal, and is guaranteed op-
timal when the weights are not correlated (e.g., in
logistic regression). Lowd & Domingos (2007) also
used a scaled conjugate gradient algorithm to incor-
porate second-order information and further adapt
the search direction.
The open-source Alchemy package (Kok et al,
2009) provides implementations of existing algo-
rithms for Markov logic.
815
4 An MLN for Joint Bio-Event Extraction
In this section, we present our MLN for joint bio-
event extraction. As standard for this task, we as-
sume that Stanford dependency parses are available
in the input. Our MLN jointly makes the following
predictions: for each token, whether it is a trigger
word (and if so, what is the event type), and for each
dependency edge, whether it is in an argument path
leading to a theme or cause.
To the best of our knowledge, the latter part makes
this formulation a novel one. By breaking the pre-
diction of an argument path into that on individual
dependency edges, it can leverage the correlation
among adjacent edges and make learning more ef-
fective. Indeed, compared to other top systems, our
MLN uses a much simpler set of features, but is still
capable of obtaining state-of-the-art results.3 Com-
putationally, this formulation is also attractive. The
number of predictions is bounded by the number of
tokens and edges, and is linear in sentence length,
rather than quadratic.
Our MLN also handles the regulation events
differently. We notice that events of the three
regulation types often occur in similar contexts, and
sometimes share trigger words (e.g., ?involve?).
Therefore, our MLN merges them into a single
event type Regulation, and additionally predicts
the regulation direction (Positive or Negative).
This allows it to pool information shared by the
three types.
Base MLN: The following are the main query pred-
icates we used, along with descriptions:
Event(i): token i signifies an event;
EvtType(i, e): i is of event type e;
RegType(i, r): i is of regulation type r;
InArgPath(i, j, a): the dependency edge from i
to j is in an argument path of type a, with a
being either Theme or Cause.
If event i has type Positive regulation,
both EvtType(i, Regulation) and
RegType(i, Positive) are true. Similarly
for Negative regulation. If the type is
3In future work, we plan to incorporate a much richer set of
features; Markov logic makes such extensions straightforward.
Table 1: Formulas in the base MLN.
Token(i,+t) ? EvtType(i,+e)
Token(i,+t) ? RegType(i,+r)
Token(j,+t) ? Dep(i, j, d) ? EvtType(i,+e)
Dep(i, j,+d) ? InArgPath(i, j,+a)
Dep(i, j,+d) ? Prot(i) ? InArgPath(i, j,+a)
Dep(i, j,+d) ? Prot(j) ? InArgPath(i, j,+a)
Token(i,+t) ? Dep(i, j,+d) ? InArgPath(i, j,+a)
Token(j,+t) ? Dep(i, j,+d) ? InArgPath(i, j,+a)
Regulation, only EvtType(i, Regulation) is
true.
The main evidence predicates are:
Token(i, w): token i has word w;
Dep(i, j, d): there is a dependency edge from i to
j with label d; 4
Prot(i): i is a protein.
Our base MLN is a logistic regression model, and
can be succintly captured by eight formulas in Ta-
ble 1. All free variables are implicitly universally
quantified. The ?+? notation signifies that the MLN
contains an instance of the formula, with a separate
weight, for each value combination of the variables
with a plus sign. The first three formulas predict
the event type and regulation direction based on the
token word or its neighbor in the dependency tree.
The next five formulas predict whether a depen-
dency edge is in an argument path, based on some
combinations of token word, dependency label, and
whether the nodes are proteins.
By default, we also added the unit formulas:
Theme(x, y), Cause(x, y), EventType(x,+e),
RegType(x,+r), which capture default regularities.
Joint Inference: Like any classification system, the
formulas in the base MLN make independent predic-
tions at inference time. This is suboptimal, because
query atoms are interdependent due to either hard
constraints (e.g., an event must have a type) or soft
correlation (e.g., ?increase? signifies an event and
the dobj edge from it leads to a theme). We thus
4For convenience, we include the reverse dependency edges
in the evidence. For example, if Dep(i, j, nn) is true, then so is
Dep(j, i,?nn).
816
augment the base MLN with two groups of joint-
inference formulas. First we incorporate the follow-
ing hard constraints.
Event(i) ? ?t. EvtType(i, t)
EvtType(i, t) ? Event(i)
RegType(i, r) ? EvtType(i, Regulation)
InArgPath(i, j, Theme) ? Event(i)
? ? k 6= j. InArgPath(k, i, Theme)
InArgPath(i, j, Cause)
? EvtType(i, Regulation)
? ? k 6= j. InArgPath(k, i, Cause)
InArgPath(i, j, Theme) ? Prot(j)
? ? k 6= i. InArgPath(j, k, Theme)
InArgPath(i, j, Cause) ? Event(j) ? Prot(j)
? ? k 6= i. InArgPath(j, k, Cause)
The first three formulas enforce that events must
have a type, that a token assigned an event (regula-
tion) type must be an (regulation) event. The next
four formulas enforce the consistency of argument
path assignments: an argument path must start with
an event, in particular, a cause path must start with a
regulation event; a theme path must eventually trace
to a protein, whereas a cause path may also stop at
an event (which does not have a cause itself). To
avoid looping, we forbid reverse edges in a path.5
Notice that with these constraints, adjacent edges
in the dependency tree correlate with each other
in their InArgPath assignments, much like in an
HMM for linear sequences. Moreover, these assign-
ments correlate with the event and event-type ones;
knowing that i probably signifies an event makes it
easier to detect an argument path, and vice versa.
In addition, events that share partial argument paths
can inform each other through the predictions on
edges. In the experiments section, we will see that
merely adding these hard constraints leads to 26-
point gain in F1.
We also notice that different trigger words may
use different dependencies to start an argument path
of a particular type. For example, for many verbs,
nsubj tends to start a cause path and dobj a theme
5This is violated in some cases, and can be relaxed. We
enforced it for simplicity in this paper.
path. However, for ?bind? that signifies a Binding
event, both lead to themes, as in ?A binds B?.
Such soft regularities can be captured by a single
joint formula: Token(i,+w) ? Dep(i, j,+d) ?
Event(i)? InArgPath(i, j,+a), which correlates
event and argument type with token and dependency.
Linguistically-Motivated Formulas: Natural lan-
guages often possess systematic syntactic alterna-
tions. For example, for the word ?increase?, if both
subject and object are present, as in ?A increases
the level of B?, the subject is the cause whereas
the object is the theme. However, if only sub-
ject is present, as in ?The level of B increases?,
the subject is the theme. We thus augment the
MLN with a number of context-specific formulas
such as: Token(i, increase)? Dep(i, j, nsubj)?
Dep(i, k, dobj) ? Event(i) ? Cause(i, j).6
5 Learning And Inference
When training data comprises of many independent
subsets (e.g., individual abstracts), stochastic gradi-
ent descent (SGD) is often a favorable method for
parameter learning. By adopting small and frequent
updates, it can dramatically speed up learning and
sometimes even improve accuracy. Moreover, it eas-
ily scales to large datasets since each time it only
needs to bring a few subsets into the memory.
In this paper, we used SGD to learn weights for
our MLN. During this process, we discovered some
general challenges for applying SGD to relational
domains. For example, the ill-conditioning problem
is particularly severe, and using a single learning
rate either makes learning extremely slow or leads
to divergence. Like Lowd & Domingos (2007),
we combat this by dividing the learning rate by the
variance. However, this still leads to divergence as
learning progresses. The reason is that some weights
are strongly correlated due to the joint formulas, es-
pecially the hard constraints. Therefore, the diag-
onal approximates the Hessian poorly. Inspired by
Poon & Domingos (2007), for each formula, we
count the numbers of true and false groundings in
the train data, and add the smaller of the two plus one
to the variance, before dividing the global rate by it.
6Available at http://research.microsoft.com/-
en-us/people/lucyv/naacl10.
817
We found that this is effective for making learning
stable in our experiments.
To compute the most probable state, we used MC-
SAT to estimate the marginal probability of each
query atom, and returned the ones with probability
above a threshold. This allows us to easily trade off
precision and recall by varying the threshold. To
speed up burn-in, we followed Poon et al (2009)
and first ran MC-SAT with deterministic annealing
for initialization.
6 Correcting Syntactic Errors With
Semantic Information
Two typical types of syntactic errors are PP-
attachment and coordination. For semantic tasks
such as bio-event extraction, these errors also have
the most adverse impact to performance. For ex-
ample, for the snippet ?involvement of p70 acti-
vation in IL-10 up-regulation by gp41?, the Stan-
ford parser makes two errors by attaching ?up-
regulation? to ?activation? instead of ?involvement?,
and attaching ?gp41? to ?involvement? instead of
?up-regulation?. This makes it very difficult to pre-
dict that ?gp41? is the cause of ?up-regulation?,
and that ?up-regulation? is the theme of ?involve-
ment?. For conjucts such as ?IL-2 and IL-4 ex-
pressions?, the parser will align ?IL-2? with ?ex-
pressions?, which makes it difficult to recognize the
expression event on ?IL-2?. For nested events like
?gp41 regulates IL-2 and IL-4 expressions?, this re-
sults in three extraction errors: IL-2 expression and
the regulation event on it are missing, whereas an
erroneous regulation event on IL-2 is predicted.
Syntactic errors are often incurred due to lack
of semantic information during parsing (e.g., the
knowledge that IL-2 and IL-4 are both proteins). In
this paper, we used a heuristic method to fix such
errors by incorporating two sources of semantic in-
formation: argument paths in training data and in-
put protein labels. For conjuncts (signified by prefix
conj in Stanford dependencies) between a protein
and a non-protein, we check whether the non-protein
has a protein child, if so, we remove the conjunct and
reattach the first protein to the non-protein. For PP-
attachments, we notice that often the errors can be
fixed by reattaching the child to the closest node that
fits a known attachment pattern (e.g., ?up-regulation
by PROTEIN?). We used the following heuristics to
gather attachment patterns. For each argument path
in the training data, if it consists of a single PP edge,
then we add the combination of governor, depen-
dency label, and dependent to the pattern. (Protein
names are replaced with a special string.) If a path
contains multiple edges, but a PP edge attaches to a
word to the left of the event trigger (e.g., ?gp41? at-
tached to ?involvement?), our system concludes that
the dependent should instead be attached to the trig-
ger and adds the corresponding pattern. In addition,
we added a few default patterns like ?involvement
in? and ?effect on?. For each PP edge, the candi-
dates for reattachment include the current governor,
and the governor?s parent and all rightmost descen-
dants (i.e., its rightmost child, the rightmost child of
that child, etc.) that are to the left of the dependent.
We reattach the dependent to the closest candidate
that fits an attachment pattern. If there is none, the
attachment remains unchanged. In total, the fraction
of reattachments is about 4%.
7 Experiments
We evaluated our system on the dataset for Task 1
in the BioNLP?09 Shared Task (Kim et al, 2009).
It consists of 800 abstracts for training, 150 for de-
velopment and 260 for test. We conducted feature
development and tuned hyperparameters using the
development set, and evaluated our final system on
test using the online tool provided by the organizers.
(The test annotations are not released to the public.)
All results reported were obtained using the main
evaluation criteria for the shared task.7
7.1 System
Our system first carries out lemmatization and
breaks up hyphenated words.8 It then uses the Stan-
ford parser (de Marneffe et al, 2006) to generate de-
pendencies. For simplicity, if an event contains mul-
tiple trigger words, only the head word is labeled.9
7Namely, ?Approximate Span/Approximate Recursive
Matching?. See Kim et al (2009) for details.
8E.g., ?gp41-induced? becomes ?gp41? and ?induced?, with
a new dependency edge labeled hyphen from ?induced? to
?gp41?. To avoid breaking up protein names with hyphens, we
only dehyphenate words with suffix in a small hand-picked list.
9Most events have only one trigger, and the chosen words
only need to lie within an approximate span in evaluation.
818
Table 2: Comparison of our full system with its variants
and with UTurku on the development set.
Rec. Prc. F1
BASE 17.4 67.2 27.7
BASE+HARD 49.4 58.5 53.6
FULL 51.5 60.0 55.5
?LING 50.5 59.6 54.7
?SYN-FIX 48.2 54.6 51.2
UTurku 51.5 55.6 53.5
We implemented our system as an extension to the
Alchemy system (Kok et al, 2009). In particular, we
developed an efficient parallelized implementation
of our stochastic gradient descent algorithm using
the message-passing interface (MPI). For learning,
we used a mini-batch of 20 abstracts and iterated
through the training files twice. For each mini-batch,
we estimated the gradient by running MC-SAT for
300 samples; the initialization was done by running
annealed MC-SAT for 200 samples, with tempera-
ture dropping from 10 to 0.1 at 0.05 decrements.
For inference, we initialized MC-SAT with 1000 an-
nealed samples, with temperature dropping from 10
to 0.1 at 0.01 decrements, we then ran MC-SAT for
5000 samples to compute the marginal probabilities.
This implementation is very efficient: learning took
about 20 minutes in a 32-core cluster with 800 train-
ing files; inference took a few minutes in average.
To obtain the final assignment, we set the query
atoms with probability no less than 0.4 to true and
the rest to false. The threshold is chosen to max-
imize F1 in the development set. To generate the
events, we first found arguments for each trigger i
by gathering all proteins and event triggers that were
accessible from i along an argument path without
first encountering another trigger. For triggers of
base event types, we dropped other triggers from
its argument list. For nested triggers, we generated
events recursively by first processing argument trig-
gers and generating their events, and then generating
events for the parent trigger by including all combi-
nations of argument events. For Binding triggers,
we group its arguments by the first dependency la-
bels in the argument paths, and generate events by a
cross-product of the group members.
Table 3: Per-type recall/precision/F1 for our full system
on the development set.
Rec. Prc. F1
Expression 75.6 79.1 77.3
Transcription 69.5 73.1 71.3
Phosphorylation 87.2 87.2 87.2
Catabolism 85.7 100 92.3
Localization 66.0 85.4 74.5
Binding 39.1 61.8 47.9
Positive regulation 41.8 51.0 46.0
Negative regulation 39.3 56.2 46.3
Regulation 41.4 33.2 36.8
7.2 Results
We first conducted experiments on the develop-
ment set to evaluate the contributions of individual
components. Table 2 compares their performances
along with that of UTurku. The base MLN (BASE)
alone performed rather poorly. Surprisingly, by just
adding the hard constraints to leverage joint infer-
ence (BASE+HARD), our system almost doubled
the F1, and tied UTurku. In addition, adding the
soft joint-inference formula results in further gain,
and our full system (FULL) attained an F1 of 55.5.
This is two points higher than UTurku and the best
reported result on this dataset. The linguistically-
motivated formulas are beneficial, as can seen by
comparing with the system without them (?LING),
although the difference is small. Fixing the syntactic
errors with semantic information, on the other hand,
leads to substantial performance gain. Without do-
ing it (?SYN-FIX), our system suffers an F1 loss of
more than four points. This verifies that the quality
of syntactic analysis is important for event extrac-
tion. The differences between FULL and other vari-
ants (except -LING) are all statistically significant at
1% level using McNemar?s test.
To understand the performance bottlenecks, we
show the per-type results in Table 3 and the re-
sults at the predicate level in Table 4.10 Both trig-
ger and argument-edge detections leave much room
for improvement. In particular, the system pro-
posed many incorrect regulation triggers, partly be-
cause regulation triggers have the most variations
10Numbers in Table 3 refer to events, whereas in Table 4 to
triggers. A trigger may signify multiple events, so numbers in
Table 4 can be smaller than that in Table 3.
819
Table 4: Predicate recall/precision/F1 for our full system
on the development set.
Rec. Prc. F1
Expression 80.1 82.0 81.0
Transcription 68.8 71.0 69.8
Phosphorylation 87.5 92.1 89.7
Catabolism 84.2 100 91.4
Localization 62.5 86.2 72.5
Binding 62.4 82.4 71.1
Positive regulation 65.8 70.7 68.2
Negative regulation 58.3 71.7 64.3
Regulation 61.7 43.4 50.9
All triggers 68.1 71.7 69.9
Argument edge 69.0 71.8 70.4
Table 5: Comparison of our full system with top systems
on the test set.
Rec. Prc. F1
UTurku 46.7 58.5 52.0
JULIELab 45.8 47.5 46.7
ConcordU 35.0 61.6 44.6
Riedel et al 36.9 55.6 44.4
FULL MLN 43.7 58.6 50.0
among all types. Our system did well in recognizing
Binding triggers, but performed much poorer at the
event level. This indicates that the bottleneck lies in
correctly identifying all arguments for multi-theme
events. Indeed, if we evaluate on individual event-
theme pairs for Binding, the F1 jumps 15 points to
62.8%, with precision 82.7% and recall 50.6%.
Finally, Table 5 compares our system with the top
systems on the test set. Our system trailed UTurku
due to a somewhat lower recall, but substantially
outperformed all other systems. In particular, it re-
duced F1 error by more than 10% compared to the
previous best joint approach by Riedel et al (2009).
7.3 Error Analysis
Through manual inspection, we found that many re-
maining errors were related to syntactic parses. The
problem is particularly severe when there are nested
or co-occuring PP-attachments and conjuncts (e.g.,
?increased levels of IL-2 and IL-4 mRNA and pro-
tein in the cell?). Our rule-based procedure in Sec-
tion 6 has high precision in fixing some of these er-
rors, but the coverage is limited. It also makes hard
decisions in a preprocessing step, which cannot be
reverted. A principled solution is to resolve syntactic
and semantic ambiguities in a joint model that inte-
grates reattachment decisions and extractions. This
can potentially resolve more syntactic errors, as ex-
traction makes more semantic information available,
and is more robust to reattachment uncertainty.
In some challenging cases, we found further op-
portunities for joint inference. For example, in the
sentence ?These cells are deficient in FasL expres-
sion, although their cytokine IL-2 production is nor-
mal?, ?normal? signifies a Positive regulation
event over ?IL-2 production? because of its contrast
with ?deficient?. Such events can be detected by in-
troducing additional joint inference rules that lever-
age syntactic structures such as subclauses.
We also found many cases where the annota-
tions differ for the same expressions. For ex-
ample, ?cotransfection with PROTEIN? is some-
times labeled as both an Expression event and a
Positive regulation event, and sometimes not
labeled at all. This occurs more often for regulation
events, which partly explains the low precision for
them.
8 Conclusion
This paper presents the first joint approach for bio-
event extraction that achieves state-of-the-art results.
This is made possible by adopting a novel formula-
tion that jointly predicts events, arguments, as well
as individual dependency edges in argument paths.
Our system is based on Markov logic and can be
easily extended to incorporate additional knowledge
and linguistic features to further improve accuracy.
Directions for future work include: leveraging ad-
ditional joint-inference opportunities, better integra-
tion of syntactic parsing and event extraction, and
applying this approach to other extraction tasks and
domains.
9 Acknowledgements
We give warm thanks to Sebastian Riedel and three
anonymous reviewers for helpful comments and
suggestions.
820
References
G. Bakir, T. Hofmann, B. B. Scho?lkopf, A. Smola,
B. Taskar, S. Vishwanathan, and (eds.). 2007. Pre-
dicting Structured Data. MIT Press, Cambridge, MA.
Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP Workshop
2009.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew
McCallum, Tom Mitchell, Kamal Nigam, and Sean
Slattery. 1999. Learning to construct knowledge bases
from the world wide web. Artificial Intelligence.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation, pages 449?454,
Genoa, Italy. ELRA.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan & Claypool, San Rafael, CA.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Jan Hajic, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Antonia Martii, Lluis Mar-
quez, Adam Meyers, Joakim Nivre, Sebastian Pado,
Jan Stepanek, Pavel Stranak, Mihai Surdeanu, Nian-
wen Xue, and Yi Zhang. 2009. The CoNLL-2009
Shared Task: syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP-09 Shared Task on event extraction. In Pro-
ceedings of the BioNLP Workshop 2009.
Stanley Kok, Parag Singla, Matt Richardson, Pedro
Domingos, Marc Sumner, Hoifung Poon, and Daniel
Lowd. 2009. The alchemy system for statistical re-
lational ai. Technical report, Dept. of CSE, Univ. of
Washington, http://alchemy.cs.washington.edu/.
Daniel Lowd and Pedro Domingos. 2007. Efficient
weight learning for markov logic networks. In Pro-
ceedings of the Eleventh European Conference on
Principles and Practice of Knowledge Discovery in
Databases, pages 200?211, Warsaw. Springer.
Hoifung Poon and Pedro Domingos. 2006. Sound and
efficient inference with probabilistic and determinis-
tic dependencies. In Proceedings of the Twenty First
National Conference on Artificial Intelligence, pages
458?463, Boston, MA. AAAI Press.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
Twenty Second National Conference on Artificial In-
telligence, pages 913?918, Vancouver, Canada. AAAI
Press.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of NAACL-HLT,
Boulder, Colorado. ACL.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Junichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP Workshop 2009.
Rune Saetre, Makoto Miwa, Kazuhiro Yoshida, and Ju-
nichi Tsujii. 2009. From protein-protein interaction
to molecular event extraction. In Proceedings of the
BioNLP Workshop 2009.
821
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 3?4,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Markov Logic in Natural Language Processing: Theory, Algorithms, and
Applications
Hoifung Poon, University of Washington
Natural languages are characterized by rich relational structures and
tight integration with world knowledge. As the field of NLP/CL moves
towards more complex and challenging tasks, there has been increasing
interest in applying joint inference to leverage such relations and
prior knowledge. Recent work in statistical relational learning
(a.k.a. structured prediction) has shown that joint inference can not
only substantially improve predictive accuracy, but also enable
effective learning with little or no labeled information. Markov logic
is the unifying framework for statistical relational learning, and has
spawned a series of successful NLP applications, ranging from
information extraction to unsupervised semantic parsing. In this
tutorial, I will introduce Markov logic to the NLP community and
survey existing NLP applications. The target audience of the tutorial
is all NLP researchers, students and practitioners. The audience will
gain the ability to efficiently develop state-of-the-art solutions to
NLP problems using Markov logic and the Alchemy open-source software.
1. Structure
The tutorial will be structured as follows:
Part One: Markov Logic
       In the first part I will motivate statistical relational learning
(SRL) for NLP problems, and introduce Markov logic as the unifying
framework. I will present state-of-the-art learning and inference
algorithms in Markov logic, and give an overview of the Alchemy
open-source software package. The duration of this part will be
approximately one hour and half.
Part Two: NLP Applications: Supervised Learning
       In the second part I will describe how to use Markov logic and
Alchemy to develop state-of-the-art solutions very efficiently for a
variety of NLP problems, including: logistic regression, text and
hypertext classification, vector-space and link-based information
retrieval, entity resolution, information integration, hidden Markov
models, Bayesian networks, information extraction, semantic role
labeling, and biomedical text mining. This part will also cover
practical tips on using Markov logic and Alchemy ? the kind of
information that is rarely found in research papers, but is key to
3
developing successful applications. This part will focus on supervised
learning and the duration will be approximately an hour.
Part Three: NLP Applications: Unsupervised Learning
       In the third and final part I will introduce the emerging direction
for statistical relation learning that leverages prior knowledge and
relational structures to enable effective learning with little or no
labeled data. As examples I will present recent work in applying
Markov logic to unsupervised coreference resolution and unsupervised
semantic parsing. I will also briefly touch on the exciting prospect
of machine reading from the Web. The duration will be about half an
hour.
2. Instructor
Hoifung Poon
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195, USA
hoifung@cs.washington.edu
Hoifung Poon is a fifth-year Ph.D. student at the University of
Washington, working with Pedro Domingos. His main research interest
lies in advancing machine learning methods to handle both complexity
and uncertainty, and in applying them to solving challenging NLP
problems with little or no labeled data. His most recent work
developed unsupervised learning methods for a number of NLP problems
ranging from morphological segmentation to semantic parsing, and
received the Best Paper Awards in NAACL-09 and EMNLP-09. At
Washington, he helped design course materials for the first offering
of the undergraduate machine learning course, and gave guest lectures
in both undergraduate and graduate machine learning classes. His prior
experience includes teaching undergraduate math classes in West
Virginia University, for which he was awarded the Outstanding Graduate
Teaching Assistant by the University.
4
Proceedings of NAACL-HLT 2013, pages 837?846,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Probabilistic Frame Induction
Jackie Chi Kit Cheung?
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
jcheung@cs.toronto.edu
Hoifung Poon
One Microsoft Way
Microsoft Research
Redmond, WA 98052, USA
hoifung@microsoft.com
Lucy Vanderwende
One Microsoft Way
Microsoft Research
Redmond, WA 98052, USA
lucyv@microsoft.com
Abstract
In natural-language discourse, related events
tend to appear near each other to describe a
larger scenario. Such structures can be formal-
ized by the notion of a frame (a.k.a. template),
which comprises a set of related events and
prototypical participants and event transitions.
Identifying frames is a prerequisite for infor-
mation extraction and natural language gen-
eration, and is usually done manually. Meth-
ods for inducing frames have been proposed
recently, but they typically use ad hoc proce-
dures and are difficult to diagnose or extend.
In this paper, we propose the first probabilistic
approach to frame induction, which incorpo-
rates frames, events, and participants as latent
topics and learns those frame and event transi-
tions that best explain the text. The number
of frame components is inferred by a novel
application of a split-merge method from syn-
tactic parsing. In end-to-end evaluations from
text to induced frames and extracted facts, our
method produces state-of-the-art results while
substantially reducing engineering effort.
1 Introduction
Events with causal or temporal relations tend to oc-
cur near each other in text. For example, a BOMB-
ING scenario in an article on terrorism might be-
gin with a DETONATION event, in which terrorists
set off a bomb. Then, a DAMAGE event might en-
sue to describe the resulting destruction and any
casualties, followed by an INVESTIGATION event
?This research was undertaken during the author?s internship
at Microsoft Research.
covering subsequent police investigations. After-
wards, the BOMBING scenario may transition into
a CRIMINAL-PROCESSING scenario, which begins
with police catching the terrorists, and proceeds to
a trial, sentencing, etc. A common set of partici-
pants serves as the event arguments; e.g., the agent
(or subject) of DETONATION is often the same as
the theme (or object) of INVESTIGATION and corre-
sponds to a PERPETRATOR.
Such structures can be formally captured by the
notion of a frame (a.k.a. template, scenario), which
consists of a set of events with prototypical transi-
tions, as well as a set of slots representing the com-
mon participants. Identifying frames is an explicit
or implicit prerequisite for many NLP tasks. Infor-
mation extraction, for example, stipulates the types
of events and slots that are extracted for a frame or
template. Online applications such as dialogue sys-
tems and personal-assistant applications also model
users? goals and subgoals using frame-like represen-
tations. In natural-language generation, frames are
often used to represent contents to be expressed as
well as to support surface realization.
Until recently, frames and related representations
have been manually constructed, which has limited
their applicability to a relatively small number of do-
mains and a few slots within a domain. Furthermore,
additional manual effort is needed after the frames
are defined in order to extract frame components
from text (e.g., in annotating examples and design-
ing features to train a supervised learning model).
This paradigm makes generalizing across tasks dif-
ficult, and might suffer from annotator bias.
Recently, there has been increasing interest in au-
837
tomatically inducing frames from text. A notable
example is Chambers and Jurafsky (2011), which
first clusters related verbs to form frames, and then
clusters the verbs? syntactic arguments to identify
slots. While Chambers and Jurafsky (2011) repre-
sents a major step forward in frame induction, it is
also limited in several aspects. The clustering used
ad hoc steps and customized similarity metrics, as
well as an additional retrieval step from a large ex-
ternal text corpus for slot generation. This makes it
hard to replicate their approach or adapt it to new
domains. Lacking a coherent model, it is also diffi-
cult to incorporate additional linguistic insights and
prior knowledge.
In this paper, we present PROFINDER (PROba-
bilistic Frame INDucER), the first probabilistic ap-
proach to frame induction. PROFINDER defines
a joint distribution over the words in a document
and their frame assignments by modeling frame
and event transitions, correlations among events and
slots, and their surface realizations. Given a set of
documents, PROFINDER outputs a set of induced
frames with learned parameters, as well as the most
probable frame assignments that can be used for
event and entity extraction. The numbers of events
and slots are dynamically determined by a novel
application of the split-merge approach from syn-
tactic parsing (Petrov et al, 2006). In end-to-end
evaluations from text to entity extraction using stan-
dard MUC and TAC datasets, PROFINDER achieved
state-of-the-art results while significantly reducing
engineering effort and requiring no external data.
2 Related Work
In information extraction and other semantic pro-
cessing tasks, the dominant paradigm requires two
stages of manual effort. First, the target representa-
tion is defined manually by domain experts. Then,
manual effort is required to construct an extractor
or to annotate examples to train a machine-learning
system. Recently, there has been a burgeoning body
of work in reducing such manual effort. For exam-
ple, a popular approach to reduce annotation effort is
bootstrapping from seed examples (Patwardhan and
Riloff, 2007; Huang and Riloff, 2012). However,
this still requires prespecified frames or templates,
and selecting seed words is often a challenging task
(Curran et al, 2007). Filatova et al (2006) construct
simple domain templates by mining verbs and the
named entity type of verbal arguments that are topi-
cal, whereas Shinyama and Sekine (2006) identify
query-focused slots by clustering common named
entities and their syntactic contexts. Open IE (Banko
and Etzioni, 2008) limits the manual effort to de-
signing a few domain-independent relation patterns,
which can then be applied to extract relational triples
from text. While extremely scalable, this approach
can only extract atomic factoids within a sentence,
and the resulting triples are noisy, non-canonicalized
text fragments.
More relevant to our approach is the recent work
in unsupervised semantic induction, such as un-
supervised semantic parsing (Poon and Domingos,
2009), unsupervised semantical role labeling (Swier
and Stevenson, 2004) and induction (Lang and Lap-
ata, 2011, e.g.), and slot induction from web search
logs (Cheung and Li, 2012). As in PROFINDER,
they model distributional contexts for slots and
roles. However, these approaches focus on the se-
mantics of independent sentences or queries, and do
not capture discourse-level dependencies.
The modeling of frame and event transitions in
PROFINDER is similar to a sequential topic model
(Gruber et al, 2007), and is inspired by the suc-
cessful applications of such topic models in sum-
marization (Barzilay and Lee, 2004; Daume? III and
Marcu, 2006; Haghighi and Vanderwende, 2009, in-
ter alia). There are, however, two main differences.
First, PROFINDER contains not a single sequential
topic model, but two (for frames and events, respec-
tively). In addition, it also models the interdepen-
dencies among events, slots, and surface text, which
is analogous to the USP model (Poon and Domin-
gos, 2009). PROFINDER can thus be viewed as a
novel combination of state-of-the-art models in un-
supervised semantics and discourse modeling.
In terms of aim and capability, PROFINDER is
most similar to Chambers and Jurafsky (2011),
which culminated from a series of work for iden-
tifying correlated events and arguments in narratives
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). By adopting a probabilistic approach,
PROFINDER has a sound theoretical underpinning,
and is easy to modify or extend. For example, in
Section 3, we show how PROFINDER can easily be
838
augmented with additional linguistically-motivated
features. Likewise, PROFINDER can easily be used
as a semi-supervised system if some slot designa-
tions and labeled examples are available.
The idea of representing and capturing stereotyp-
ical knowledge has a long history in artificial in-
telligence and psychology, and has assumed vari-
ous names such as frames (Minsky, 1974), schemata
(Rumelhart, 1975), and scripts (Schank and Abel-
son, 1977). In the linguistics and computational
linguistics communities, frame semantics (Fillmore,
1982) uses frames as the central representation of
word meaning, culminating in the development of
FrameNet (Baker et al, 1998), which contains over
1000 manually annotated frames. A similarly rich
lexical resource is the MindNet project (Richard-
son et al, 1998). Our notion of frame is related to
these representations, but there are also subtle differ-
ences. For example, Minsky?s frame emphasizes in-
heritance, which we do not model in this paper1. As
in semantic role labeling, FrameNet focuses on se-
mantic roles and does not model event or frame tran-
sitions, so the scope of its frames is often no more
than an event in our model. Perhaps the most sim-
ilar to our frame is Roger Schank?s scripts, which
capture prototypical events and participants in a sce-
nario such as restaurant dining. In their approach,
however, scripts are manually defined, making it
hard to generalize. In this regard, our work may be
viewed as an attempt to revive a long tradition in AI
and linguistics, by leveraging the recent advances in
computational power, NLP, and machine learning.
3 Probabilistic Frame Induction
In this section, we present PROFINDER, a proba-
bilistic model for frame induction. Let F be a set of
frames, where each frame F = (EF , SF ) comprises
a unique set of events EF and slots SF . Given a
document D and a word w in D, Zw = (f, e) repre-
sents an assignment of w to frame f ? F and frame
element e ? Ef ? Sf . At the heart of PROFINDER
is a generative model P?(D,Z) that defines a joint
distribution over document D and the frame assign-
ment to its words Z. Given a set of documents D,
1This should be a straightforward extension ? using the
split-and-merge approach, PROFINDER already produces a hi-
erarchy of events and slots in learning, although currently it
makes no use of the intermediate levels.
frame induction in PROFINDER amounts to deter-
mining the number of events and slots in each frame,
as well as learning the parameters ? by summing out
the latent assignments Z to maximize the likelihood
of the document set
?
D?D
P?(D).
The induced frames identify the key event structures
in the document set. Additionally, PROFINDER can
conduct event and entity extraction by computing
the most probable frame assignment Z. In the re-
mainder of the section, we first present the base
model for PROFINDER. We then introduce sev-
eral linguistically motivated refinements, as well as
efficient algorithms for learning and inference in
PROFINDER.
3.1 Base Model
The probabilistic formulation of PROFINDER makes
it extremely flexible for incorporating linguistic in-
tuition and prior knowledge. In this paper, we design
our PROFINDER model to capture three types of de-
pendencies.
Frame transitions between clauses A sentence
contains one or more clauses, each of which is a
minimal unit expressing a proposition. A clause is
unlikely to straddle different frames, so we stipu-
late that the words in a clause be assigned to the
same frame. On the other hand, frame transitions
can happen between clauses, and we adopt the com-
mon Markov assumption that the frame of a clause
only depends on the previous clause in the docu-
ment. Clauses are automatically extracted from the
dependency parse and further decomposed into an
event head and its syntactic arguments.
Event transitions within a frame Events tend to
transition into related events in the same frame, as
determined by their causal or temporal relations.
Each clause is assigned an event compatible with
its frame assignment (i.e., the event is in the given
frame). Like frame transitions, we assume that the
event assignment of a clause depends only on the
event of the previous clause.
Emission of event heads and slot words Simi-
lar to topics in topic models, each event determines
839
a multinomial from which the event head is gener-
ated; e.g., a DETONATION event might use verbs
such as detonate, set off or nouns such as denota-
tion, bombing as its event head. Additionally, as
in USP (Poon and Domingos, 2009), an event also
contains a multinomial of slots for each of its argu-
ment types2; e.g., the agent argument of a DETONA-
TION event is generally the PERPETRATOR slot of
the BOMBING frame. Finally, each slot has its own
multinomials for generating the argument head and
dependency label, regardless of the event.
Formally, let D be a document and C1, ? ? ? , Cl be
its clauses, the PROFINDER model is defined by
P?(D,Z) = PF?INIT(F1)?
?
i
PF?TRAN(Fi+1|Fi)
? PE?INIT(E1|F1)
?
?
i
PE?TRAN(Ei+1|Ei, Fi+1, Fi)
?
?
i
PE?HEAD(ei|Ei)
?
?
i,j
PSLOT(Si,j |Ei,j , Ai,j)
?
?
i,j
PA?HEAD(ai,j |Si,j)
?
?
i,j
PA?DEP(depi,j |Si,j)
Here, Fi, Ei denote the frame and event assign-
ment to clause Ci, respectively, and ei denotes the
event head. For the j-th argument of clause i,
Si,j denotes the slot assignment, Ai,j the argument
type, ai,j the head word, and depi,j the dependency
from the event head. PE?TRAN(Ei+1|Ei, Fi+1, Fi) =
PE?INIT(Ei+1|Fi+1) if Fi+1 6= Fi.
Essentially, PROFINDER combines a frame HMM
with an event HMM, where the first models frame
transition and emits events, and the second models
event transition within a frame and emits argument
slots.
3.2 Model refinements
The base model captures the main dependencies in
event narrative, but it can be easily extended to lever-
2USP generates the argument types along with events from
clustering. For simplicity, in PROFINDER we simply classify
a syntactic argument into subject, object, and prepositional ob-
ject, according to its Stanford dependency to the event head.
age additional linguistic intuition. PROFINDER in-
corporates three such refinements.
Background frame Event narratives often con-
tain interjections of general content common to all
frames. For example, in newswire articles, ATTRI-
BUTION is commonplace to describe who said or
reported a particular quote or fact. To avoid con-
taminating frames with generic content, we intro-
duce a background frame with its own events, slots,
and emission distributions, and a binary switch vari-
able Bi ? {BKG,CNT} that determines whether
clause i is generated from the actual content frame
Fi (CNT ) or background (BKG). We also stipu-
late that if BKG is chosen, the nominal frame stays
the same as the previous clause.
Stickiness in frame and event transitions Prior
work has demonstrated that promoting topic coher-
ence in natural-language discourse helps discourse
modeling (Barzilay and Lee, 2004). We extend
PROFINDER to leverage this intuition by incorporat-
ing a ?stickiness? prior (Haghighi and Vanderwende,
2009) to encourage neighboring clauses to stay in
the same frame. Specifically, along with introducing
the background frame, the frame transition compo-
nent now becomes
PF?TRAN(Fi+1|Fi, Bi+1) = (1)
?
??
??
1(Fi+1 = Fi), if Bi+1 = BKG
?1(Fi+1 = Fi)+
(1? ?)PF?TRAN(Fi+1|Fi),
if Bi+1 = CNT
where ? is the stickiness parameter, and the event
transition component correspondingly becomes
PE?TRAN(Ei+1|Ei, Fi+1, Fi, Bi+1) = (2)
?
??
??
1(Ei+1 = Ei), if Bi+1 = BKG
PE?TRAN(Ei+1|Ei), if Bi+1 = CNT,Fi = Fi+1
PE?INIT(Ei+1), if Bi+1 = CNT,Fi 6= Fi+1
Argument dependencies as caseframes As no-
ticed in previous work such as Chambers and Juraf-
sky (2011), the combination of an event head and a
dependency relation often gives a strong signal of
the slot that is indicated. For example, bomb >
nsubj (subject argument of bomb) often indicates
a PERPETRATOR. Thus, rather than simply emitting
840
Frame 
Event 
Background 
Event 
head 
?1 
?1 
?1 
?1 
?1 ?1 
???1 ?1 
? ?? 
?? 
?? 
?? 
?? ?? 
???? ?? 
. . .  
. . .  
|?| |?| |?| 
???????  
Arguments 
???????  ??????  
Figure 1: Graphical representation of our model. Hyper-
parameters, the stickiness factor, and the frame and event
initial and transition distributions are not shown for clar-
ity.
the dependency from the event head to an event ar-
gument depi,j , our model instead emits the pair of
event head and dependency relation, which we call
a caseframe following Bean and Riloff (2004).
3.3 Full generative story
To summarize, the distributions that are learned by
our model are the default distributions PBKG(B),
PF?INIT(F ), PE?INIT(E); the transition distri-
butions PF?TRAN(Fi+1|Fi), PE?TRAN(Ei+1|Ei);
and the emission distributions PSLOT(S|E,A,B),
PE?HEAD(e|E,B), PA?HEAD(a|S), PA?DEP(dep|S).
We used additive smoothing with uniform Dirich-
let priors for all the multinomials. The overall
generative story of our model is as follows:
1. Draw a Bernoulli distribution for PBKG(B)
2. Draw the frame, event, and slot distributions
3. Draw an event head emission distribution
PE?HEAD(e|E,B) for each frame including the
background frame
4. Draw event argument lemma and caseframe
emission distributions for each slot in each
frame including the background frame
5. For each clause in each document, generate the
clause-internal structure.
The clause-internal structure at clause i is gener-
ated by the following steps:
1. Generate whether this clause is background
(Bi ? {CNT,BKG} ? PBKG(B))
2. Generate the frame Fi and event Ei from
PF?INIT(F ), PE?INIT(E), or according to
equations 1 and 2
3. Generate the observed event head ei from
PE?HEAD(ei|Ei).
4. For each event argument:
(a) Generate the slot Si,j from
PSLOT(S|E,A,B).
(b) Generate the dependency/caseframe emis-
sion depi,j ? PA?DEP(dep|S) and the
lemma of the head word of the event ar-
gument ai,j ? PA?HEAD(a|S).
3.4 Learning and Inference
Our generative model admits efficient inference by
dynamic programming. In particular, after collaps-
ing the latent assignment of frame, event, and back-
ground into a single hidden variable for each clause,
the expectation and most probable assignment can
be computed using standard forward-backward and
Viterbi algorithms on fixed tree structures.
Parameter learning can be done using EM by al-
ternating the computation of expected counts and the
maximization of multinomial parameters. In par-
ticular, PROFINDER uses incremental EM, which
has been shown to have better and faster con-
vergence properties than standard EM (Liang and
Klein, 2009).
Determining the optimal number of events and
slots is challenging. One solution is to adopt a non-
parametric Bayesian method by incorporating a hi-
erarchical prior over the parameters (e.g., a Dirich-
let process). However, this approach can impose
unrealistic restrictions on the model choice and re-
sult in intractability which requires sampling or ap-
proximate inference to overcome. Additionally, EM
learning can suffer from local optima due to its non-
convex learning objective, especially when dealing
with a large number hidden states without a good
initialization.
To address these issues, we adopt a novel appli-
cation of the split-merge method previously used in
syntactic parsing for inferring refined latent syntac-
tic categories (Petrov et al, 2006). First, the model
is initialized with a number of frames, which is a
hyperparameter, and each frame is associated with
841
one event and two slots. Starting from this mini-
mal structure, EM training begins. After a number
of iterations, each event and slot state is ?split? in
two; that is, each original state now becomes two
new states. Each of the new states is generated with
half of the probability of the original, and contains
a duplicate of the associated emission distributions.
Some perturbation is then added to the probabilities
to break symmetry. After splitting, we merge back
a portion of the newly split events and slots that re-
sult in the least improvement in the likelihood of the
training data. For more details on split-merge, see
Petrov et al (2006)
By adjusting the number of split-merge cycles and
the merge parameters, our model learns the number
of events and slots in a dynamical fashion that is tai-
lored to the data. Moreover, our model starts with a
small number of frame elements, which reduces the
number of local optima and facilitates initial learn-
ing. After each split, the subsequent learning starts
with (a perturbed version of) the previously learned
parameters, which makes a good initialization that
is crucial for EM. Finally, it is also compatible with
the hierarchical nature of events and slots. For ex-
ample, slots can first be coarsely split into persons
versus locations, and later refined into subcategories
such as perpetrators and victims.
4 MUC-4 Entity Extraction Experiments
We first evaluate our model on a standard entity
extraction task, using the evaluation settings from
Chambers and Jurafsky (2011) (henceforth, C&J)
to enable a head-to-head comparison. Specifically,
we use the MUC-4 data set (1992) , which contains
1300 training and development documents on ter-
rorism in South America, with 200 additional doc-
uments for testing. MUC-4 contains four templates:
ATTACK, KIDNAPPING, BOMBING, and ARSON.3
All templates share the same set of predefined slots,
with the evaluation focusing on the following four:
PERPETRATOR, PHYSICAL TARGET, HUMAN TAR-
GET, and INSTRUMENT.
For each slot in a MUC template, the system
first identifies an induced slot that best maps to it
by F1 on the development set. As in C&J, tem-
3Two other templates have negligible counts and are ignored
as in C&J.
plate is ignored in final evaluation, so all the clusters
that belong to the same slot are then merged across
the templates; e.g., the PERPETRATOR clusters for
KIDNAPPING and BOMBING are merged. The fi-
nal precision, recall, and F1 are computed based on
these merged clusters. Correctness is determined by
matching head words, and slots marked as optional
in MUC are ignored when computing recall. All hy-
perparameters are tuned on the development set (see
Appendix A for their values).
Named entity type Named entity type is a useful
feature to filter out entities for particular slots; e.g. a
location cannot be an INSTRUMENT. We thus divide
each induced cluster into four clusters by named
entity type before performing the mapping, follow-
ing C&J?s heuristic and using a named entity recog-
nizer and word lists derived from WordNet: PER-
SON/ORGANIZATION, PHYSICAL OBJECT, LOCA-
TION, and OTHER.
Document classification The MUC-4 dataset
contains many documents that have words related
to MUC slots (e.g., plane and aviation), but are not
about terrorism. To reduce precision errors, C&J
first filtered irrelevant documents based on the speci-
ficity of event heads to learned frames. To estimate
the specificity, they used additional data retrieved
from a large external corpus. In PROFINDER, how-
ever, specificity can be easily estimated using the
probability distributions learned during training. In
particular, we define the probability of an event head
in a frame j as:
PF (w) =
?
EF?F
PE?HEAD(w|E)/|F |, (3)
and the probability of a frame given an event head
as:
P (F |w) = PF (w)/
?
F ??F
PF ?(w). (4)
We then follow the rest of C&J?s procedure to
score each learned frame with each MUC document.
Specifically, a document is mapped to a frame if the
average PF (w) in the document is above a threshold
and the document contains at least one trigger word
w? with P (F |w?) > 0.2. The threshold and the in-
duced frame were determined on the development
set, and were used to filter irrelevant documents in
the test set.
842
Unsupervised methods P R F1
PROFINDER (This work) 32 37 34
Chambers and Jurafsky (2011) 48 25 33
With additional information
PROFINDER +doc. classification 41 44 43
C&J 2011 +granularity 44 36 40
Table 1: Results on MUC-4 entity extraction. C&J 2011
+granularity refers to their experiment in which they
mapped one of their templates to five learned clusters
rather than one.
Results Compared to C&J, PROFINDER is con-
ceptually much simpler, using a single probabilis-
tic model and standard learning and inference algo-
rithms, and not requiring multiple processing steps
or customized similarity metrics. It only used the
data in MUC-4, whereas C&J required additional
text to be retrieved from a large external corpus (Gi-
gaword (Graff et al, 2005)) for each event cluster.
It currently does not make use of coreference infor-
mation, whereas C&J did. Remarkably, despite all
these, PROFINDER was still able to outperform C&J
on entity extraction, as shown in Table 1. We also
evaluated PROFINDER?s performance assuming per-
fect document classification (+doc. classification).
This led to a substantially higher precision, suggest-
ing that further improvement is possible from better
document classification.
Figure 2 shows part of a frame learned by
PROFINDER, which includes some slots and events
annotated in MUC. PROFINDER is also able to iden-
tify events and slots not annotated in MUC, a de-
sirable characteristic of unsupervised methods. For
example, it found a DISCUSSION event, an AR-
REST event (call, arrest, express, meet, charge), a
PEACE AGREEMENT slot (agreement, rights, law,
proposal), and an AUTHORITIES slot (police, gov-
ernment, force, command). The background frame
was able to capture many verbs related to attribu-
tion, such as say, continue, add, believe, although it
missed report.
5 Evaluating Frame Induction Using
Guided Summarization Templates
The MUC-4 dataset was originally designed for
information extraction and focuses on a limited
number of template and slot types. To evalu-
Event: Attack Event: Discussion
report, participate, kid-
nap, kill, release
hold, meeting, talk, dis-
cuss, investigate
Slot: Perpetrator Slot: Victim
PERSON/ORG PERSON/ORG
Words: guerrilla, po-
lice, source, person,
group
Words: people, priest,
leader, member, judge
Caseframes:
report>nsubj,
kidnap>nsubj,
kill>nsubj,
participate>nsubj,
release>nsubj
Caseframes:
kill>dobj,
murder>dobj,
release>dobj,
report>dobj,
kidnap>dobj
Figure 2: A partial frame learned by PROFINDER from
the MUC-4 data set, with the most probable emissions for
each event and slot. Labels are assigned by the authors
for readability.
ate PROFINDER?s capabilities in generalizing to
a greater variety of text, we designed and con-
ducted a novel evaluation based on the TAC guided-
summarization dataset. This evaluation was inspired
by the connection between summarization and infor-
mation extraction (White et al, 2001), and reflects a
conceptualization of summarization as inducing and
extracting structured information from source text.
Essentially, we adapted the TAC summarization an-
notation to create gold-standard slots, and used them
to evaluate entity extraction as in MUC-4.
Dataset We used the TAC 2010 guided-
summarization dataset in our experiments
(Owczarzak and Dang, 2010). This data set con-
sists of text from five domains (termed categories
in TAC), each with a template defined by TAC
organizers. In total, there are 46 document clusters
(termed topics in TAC), each of which contains 20
documents and has eight human-written summaries.
Each summary was manually segmented using
the Pyramid method (Nenkova and Passonneau,
2004) and each segment was annotated with a slot
(termed aspect in TAC) from the corresponding
template. Figure 3 shows an example and the full
set of templates is available at http://www.
nist.gov/tac/2010/Summarization/
Guided-Summ.2010.guidelines.html. In
843
(a) Accidents and Natural Disasters:
WHAT: what happened
WHEN: date, time, other temporal markers
WHERE: physical location
WHY: reasons for accident/disaster
WHO AFFECTED: casualties...
DAMAGES: ... caused by the disaster
COUNTERMEASURES: rescue efforts...
(b) (WHEN During the night of July 17,)
(WHAT a 23-foot <WHAT tsunami) hit the
north coast of Papua New Guinea (PNG)>,
(WHY triggered by a 7.0 undersea earth-
quake in the area).
(c) WHEN: night WHAT: tsunami, coast
WHY: earthquake
Figure 3: (a) A frame from the TAC Guided Summariza-
tion task with abbreviated slot descriptions. (b) A TAC
text span, segmented into several contributors with slot
labels. Note that the two WHAT contributors overlap, and
are demarcated by different bracket types. (c) The entities
that are extracted for evaluation.
TAC, each annotated segment (Figure 3b) is called
a contributor.
Evaluation Method We converted the contribu-
tors into a form that is more similar to the previ-
ous MUC evaluation, so that we can fairly compare
against previous work such as C&J that were de-
signed to extract information into that form. Specif-
ically, we extracted the head lemma from all the
maximal noun phrases found in the contributor (Fig-
ure 3c) and treated them as gold-standard entity slots
to extract. While this conversion may not be ideal in
some cases, it simplifies the TAC slots and enables
automatic evaluation. We leave the refinement of
this conversion to future work, and believe it could
be done by crowdsourcing.
For each TAC slot in a TAC category, we extract
entities from the summaries that belong to the given
TAC category. A system-induced entity is consid-
ered a match to a TAC-derived entity from the same
document if the head lemma in the former matches
one in the latter. Based on this matching criterion,
the system-induced slots are mapped to the TAC
slots in a way that achieves the best F1 for each
TAC slot. We allow a system slot to map to mul-
tiple TAC slots, due to potential overlaps in entities
1-best 5-best
Systems P R F1 P R F1
PROFINDER 24 25 24 21 38 27
C&J 58 6.1 11 50 12 20
Table 2: Results on TAC 2010 entity extraction with N -
best mapping for N = 1 and N = 5. Intermediate values
of N produce intermediate results, and are not shown for
brevity.
among TAC slots. For example, in a document about
a tsunami, earthquake may appear both in the WHAT
slot as a disaster itself, and in the CAUSE slot as a
cause for the tsunami.
One salient difference between TAC and MUC
slots is that TAC slots are often more general than
MUC slots. For example, TAC slots such as WHY
and COUNTERMEASURES likely correspond to mul-
tiple slots at the granularity of MUC. As a result, we
also consider mapping the N -best system-induced
slots to each TAC slot, for N up to 5.
Experiments We trained PROFINDER and a reim-
plementation of C&J on the 920 full source texts of
TAC 2010, and tested them on the 368 model sum-
maries. We did not provide C&J?s model with access
to external data, in order to enable fair comparison
with our model. Since all of the summary sentences
are expected to be relevant, we did not conduct doc-
ument or sentence relevance classification in C&J or
PROFINDER. We tuned all parameters by two-fold
cross validation on the summaries. We computed the
overall precision, recall, and F1 by taking a micro-
average over the results for each TAC slot.
Results The results are shown in Table 2.
PROFINDER substantially outperformed C&J in F1,
in both 1-best and N -best cases. As in MUC-4, the
precision of C&J is higher, partly because C&J often
did not do much in clustering and produced many
small clusters. For example, in the 1-best setting, the
average number of entities mapped to each TAC slot
by C&J is 21, whereas it is 208 for PROFINDER. For
both systems, the results are generally lower com-
pared to that in MUC-4, which is expected since this
task is harder given the greater diversity in frames
and slots to be induced.
844
6 Conclusion
We have presented PROFINDER, the first probabilis-
tic approach to frame induction and shown that it
achieves state-of-the-art results on end-to-end entity
extraction in standard MUC and TAC data sets. Our
model is inspired by recent advances in unsuper-
vised semantic induction and content modeling in
summarization. Our probabilistic approach makes
it easy to extend the model with additional linguistic
insights and prior knowledge. While we have made
a case for unsupervised methods and the importance
of robustness across domains, our method is also
amenable to semi-supervised or supervised learn-
ing if annotated data is available. In future work,
we would like to further investigate frame induction
evaluation, particularly in evaluating event cluster-
ing.
Acknowledgments
We would like to thank Nate Chambers for answer-
ing questions about his system. We would also like
to thank Chris Quirk for help with preprocessing the
MUC corpus, and the members of the NLP group at
Microsoft Research for useful discussions.
Appendix A. Hyperparameter Settings
We document below the hyperparameter settings for
PROFINDER that were used to generate the results
in the paper.
Hyperparameter MUC TAC
Number of frames, |F| 9 8
Frame stickiness, ? 0.125 0.5
Smoothing (frames, events, slots) 0.5 2
Smoothing (emissions) 0.05 0.2
Number of split-merge cycles 4 2
Iterations per cycle 10 10
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 17th International Conference on Compu-
tational linguistics.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. Pro-
ceedings of ACL-08: HLT, pages 28?36.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789?797, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 976?986, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Jackie C. K. Cheung and Xiao Li. 2012. Sequence clus-
tering and labeling for unsupervised query intent dis-
covery. In Proceedings of the 5th ACM International
Conference on Web Search and Data Mining, pages
383?392.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
Query-Focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 305?312, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 207?
214, Sydney, Australia, July. Association for Compu-
tational Linguistics.
845
Charles J. Fillmore. 1982. Frame semantics. Linguistics
in the Morning Calm, pages 111?137.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2005. English gigaword second edition. Linguistic
Data Consortium, Philadelphia.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. Artificial Intelligence
and Statistics (AISTATS).
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado, June. Association
for Computational Linguistics.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 286?295, Avignon, France, April. Association
for Computational Linguistics.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1117?1126, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 611?619, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Marvin Minsky. 1974. A framework for representing
knowledge. Technical report, Cambridge, MA, USA.
1992. Proceedings of the Fourth Message Understanding
Conference (MUC-4). Morgan Kaufmann.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004, volume 2004, pages 145?152.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC 2010
guided summarization task guidelines.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 717?
727, Prague, Czech Republic, June. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Stephen D. Richardson, William B. Dolan, and Lucy Van-
derwende. 1998. MindNet: Acquiring and structuring
semantic information from text. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, Volume 2, pages 1098?
1102, Montreal, Quebec, Canada, August. Association
for Computational Linguistics.
David Rumelhart, 1975. Notes on a schema for stories,
pages 211?236. Academic Press, Inc.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry Into Hu-
man Knowledge Structures. Lawrence Erlbaum, July.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
New York City, USA, June. Association for Computa-
tional Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labelling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 95?102, Barcelona, Spain, July. Association for
Computational Linguistics.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Multidoc-
ument summarization via information extraction. In
Proceedings of the First International Conference on
Human Language Technology Research. Association
for Computational Linguistics.
846
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 296?305,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Unsupervised Ontology Induction from Text
Hoifung Poon and Pedro Domingos
Department of Computer Science & Engineering
University of Washington
hoifung,pedrod@cs.washington.edu
Abstract
Extracting knowledge from unstructured
text is a long-standing goal of NLP. Al-
though learning approaches to many of its
subtasks have been developed (e.g., pars-
ing, taxonomy induction, information ex-
traction), all end-to-end solutions to date
require heavy supervision and/or manual
engineering, limiting their scope and scal-
ability. We present OntoUSP, a system that
induces and populates a probabilistic on-
tology using only dependency-parsed text
as input. OntoUSP builds on the USP
unsupervised semantic parser by jointly
forming ISA and IS-PART hierarchies of
lambda-form clusters. The ISA hierar-
chy allows more general knowledge to
be learned, and the use of smoothing for
parameter estimation. We evaluate On-
toUSP by using it to extract a knowledge
base from biomedical abstracts and an-
swer questions. OntoUSP improves on
the recall of USP by 47% and greatly
outperforms previous state-of-the-art ap-
proaches.
1 Introduction
Knowledge acquisition has been a major goal of
NLP since its early days. We would like comput-
ers to be able to read text and express the knowl-
edge it contains in a formal representation, suit-
able for answering questions and solving prob-
lems. However, progress has been difficult. The
earliest approaches were manual, but the sheer
amount of coding and knowledge engineering
needed makes them very costly and limits them to
well-circumscribed domains. More recently, ma-
chine learning approaches to a number of key sub-
problems have been developed (e.g., Snow et al
(2006)), but to date there is no sufficiently auto-
matic end-to-end solution. Most saliently, super-
vised learning requires labeled data, which itself is
costly and infeasible for large-scale, open-domain
knowledge acquisition.
Ideally, we would like to have an end-to-end un-
supervised (or lightly supervised) solution to the
problem of knowledge acquisition from text. The
TextRunner system (Banko et al, 2007) can ex-
tract a large number of ground atoms from the
Web using only a small number of seed patterns
as guidance, but it is unable to extract non-atomic
formulas, and the mass of facts it extracts is un-
structured and very noisy. The USP system (Poon
and Domingos, 2009) can extract formulas and ap-
pears to be fairly robust to noise. However, it is
still limited to extractions for which there is sub-
stantial evidence in the corpus, and in most cor-
pora most pieces of knowledge are stated only
once or a few times, making them very difficult to
extract without supervision. Also, the knowledge
extracted is simply a large set of formulas with-
out ontological structure, and the latter is essential
for compact representation and efficient reasoning
(Staab and Studer, 2004).
We propose OntoUSP (Ontological USP), a sys-
tem that learns an ISA hierarchy over clusters of
logical expressions, and populates it by translat-
ing sentences to logical form. OntoUSP is en-
coded in a few formulas of higher-order Markov
logic (Domingos and Lowd, 2009), and can be
viewed as extending USP with the capability to
perform hierarchical (as opposed to flat) cluster-
ing. This clustering is then used to perform hier-
archical smoothing (a.k.a. shrinkage), greatly in-
creasing the system?s capability to generalize from
296
sparse data.
We begin by reviewing the necessary back-
ground. We then present the OntoUSP Markov
logic network and the inference and learning al-
gorithms used with it. Finally, experiments on
a biomedical knowledge acquisition and question
answering task show that OntoUSP can greatly
outperform USP and previous systems.
2 Background
2.1 Ontology Learning
In general, ontology induction (constructing an
ontology) and ontology population (mapping tex-
tual expressions to concepts and relations in the
ontology) remain difficult open problems (Staab
and Studer, 2004). Recently, ontology learn-
ing has attracted increasing interest in both NLP
and semantic Web communities (Cimiano, 2006;
Maedche, 2002), and a number of machine learn-
ing approaches have been developed (e.g., Snow
et al (2006), Cimiano (2006), Suchanek et al
(2008,2009), Wu & Weld (2008)). However, they
are still limited in several aspects. Most ap-
proaches induce and populate a deterministic on-
tology, which does not capture the inherent un-
certainty among the entities and relations. Be-
sides, many of them either bootstrap from heuris-
tic patterns (e.g., Hearst patterns (Hearst, 1992))
or build on existing structured or semi-structured
knowledge bases (e.g., WordNet (Fellbaum, 1998)
and Wikipedia1), thus are limited in coverage.
Moreover, they often focus on inducing ontology
over individual words rather than arbitrarily large
meaning units (e.g., idioms, phrasal verbs, etc.).
Most importantly, existing approaches typically
separate ontology induction from population and
knowledge extraction, and pursue each task in a
standalone fashion. While computationally effi-
cient, this is suboptimal. The resulted ontology
is disconnected from text and requires additional
effort to map between the two (Tsujii, 2004). In
addition, this fails to leverage the intimate connec-
tions between the three tasks for joint inference
and mutual disambiguiation.
Our approach differs from existing ones in two
main aspects: we induce a probabilistic ontology
from text, and we do so by jointly conducting on-
tology induction, population, and knowledge ex-
traction. Probabilistic modeling handles uncer-
tainty and noise. A joint approach propagates in-
1http : //www.wikipedia.org
formation among the three tasks, uncovers more
implicit information from text, and can potentially
work well even in domains not well covered by
existing resources like WordNet and Wikipedia.
Furthermore, we leverage the ontology for hierar-
chical smoothing and incorporate this smoothing
into the induction process. This facilitates more
accurate parameter estimation and better general-
ization.
Our approach can also leverage existing on-
tologies and knowledge bases to conduct semi-
supervised ontology induction (e.g., by incorpo-
rating existing structures as hard constraints or pe-
nalizing deviation from them).
2.2 Markov Logic
Combining uncertainty handling and joint infer-
ence is the hallmark of the emerging field of statis-
tical relational learning (a.k.a. structured predic-
tion), where a plethora of approaches have been
developed (Getoor and Taskar, 2007; Bakir et al,
2007). In this paper, we use Markov logic (Domin-
gos and Lowd, 2009), which is the leading unify-
ing framework, but other approaches can be used
as well. Markov logic is a probabilistic exten-
sion of first-order logic and can compactly specify
probability distributions over complex relational
domains. It has been successfully applied to un-
supervised learning for various NLP tasks such
as coreference resolution (Poon and Domingos,
2008) and semantic parsing (Poon and Domingos,
2009). A Markov logic network (MLN) is a set of
weighted first-order clauses. Together with a set
of constants, it defines a Markov network with one
node per ground atom and one feature per ground
clause. The weight of a feature is the weight of the
first-order clause that originated it. The probabil-
ity of a state x in such a network is given by the
log-linear model P (x) = 1Z exp (
?
i wini(x)),
where Z is a normalization constant, wi is the
weight of the ith formula, and ni is the number
of satisfied groundings.
2.3 Unsupervised Semantic Parsing
Semantic parsing aims to obtain a complete canon-
ical meaning representation for input sentences. It
can be viewed as a structured prediction problem,
where a semantic parse is formed by partitioning
the input sentence (or a syntactic analysis such as
a dependency tree) into meaning units and assign-
ing each unit to the logical form representing an
entity or relation (Figure 1). In effect, a semantic
297
induces
protein CD11b
nsubj dobj
IL-4nn
induces
protein CD11b
nsubj dobj
IL-4nn
INDUCE
INDUCER INDUCED
IL-4
CD11B
INDUCE(e1)INDUCER(e1,e2) INDUCED(e1,e3)IL-4(e2) CD11B(e3)
IL-4 protein induces CD11b
Structured prediction: Partition + Assignment
Figure 1: An example of semantic parsing. Top:
semantic parsing converts an input sentence into
logical form in Davidsonian semantics. Bottom: a
semantic parse consists of a partition of the depen-
dency tree and an assignment of its parts.
parser extracts knowledge from input text and con-
verts them into logical form (the semantic parse),
which can then be used in logical and probabilistic
inference and support end tasks such as question
answering.
A major challenge to semantic parsing is syn-
tactic and lexical variations of the same mean-
ing, which abound in natural languages. For ex-
ample, the fact that IL-4 protein induces CD11b
can be expressed in a variety of ways, such
as, ?Interleukin-4 enhances the expression of
CD11b?, ?CD11b is upregulated by IL-4?, etc.
Past approaches either manually construct a gram-
mar or require example sentences with meaning
annotation, and do not scale beyond restricted do-
mains.
Recently, we developed the USP system (Poon
and Domingos, 2009), the first unsupervised ap-
proach for semantic parsing.2 USP inputs de-
pendency trees of sentences and first transforms
them into quasi-logical forms (QLFs) by convert-
ing each node to a unary atom and each depen-
dency edge to a binary atom (e.g., the node for
?induces? becomes induces(e1) and the subject
dependency becomes nsubj(e1, e2), where ei?s
are Skolem constants indexed by the nodes.).3
For each sentence, a semantic parse comprises of
a partition of its QLF into subexpressions, each
of which has a naturally corresponding lambda
2In this paper, we use a slightly different formulation of
USP and its MLN to facilitate the exposition of OntoUSP.
3We call these QLFs because they are not true logical
form (the ambiguities are not yet resolved). This is related
to but not identical with the definition in Alshawi (1990).
Object Cluster: INDUCE
induces 0.1
enhances 0.4? ??
Property Cluster: INDUCER
0.5
0.4?
IL-4 0.2
IL-8 0.1?
None 0.1
One 0.8?
nsubj
agent
Core Form
Figure 2: An example of object/property clusters:
INDUCE contains the core-form property cluster
and others, such as the agent argument INDUCER.
form,4 and an assignment of each subexpression
to a lambda-form cluster.
The lambda-form clusters naturally form an IS-
PART hierarchy (Figure 2). An object cluster cor-
responds to semantic concepts or relations such as
INDUCE, and contains a variable number of prop-
erty clusters. A special property cluster of core
forms maintains a distribution over variations in
lambda forms for expressing this concept or rela-
tion. Other property clusters correspond to modi-
fiers or arguments such as INDUCER (the agent ar-
gument of INDUCE), each of which in turn con-
tains three subclusters of property values: the
argument-object subcluster maintains a distribu-
tion over object clusters that may occur in this
argument (e.g., IL ? 4), the argument-form sub-
cluster maintains a distribution over lambda forms
that corresponds to syntactic variations for this ar-
gument (e.g., nsubj in active voice and agent in
passive voice), and the argument-number subclus-
ter maintains a distribution over total numbers of
this argument that may occur in a sentence (e.g.,
zero if the argument is not mentioned).
Effectively, USP simultaneously discovers the
lambda-form clusters and an IS-PART hierarchy
among them. It does so by recursively combining
subexpressions that are composed with or by sim-
ilar subexpressions. The partition breaks a sen-
tence into subexpressions that are meaning units,
and the clustering abstracts away syntactic and
lexical variations for the same meaning. This
novel form of relational clustering is governed by
a joint probability distribution P (T, L) defined in
higher-order5 Markov logic, where T are the input
dependency trees, and L the semantic parses. The
4The lambda form is derived by replacing every Skolem
constant ei that does not appear in any unary atom in the
subexpression with a lambda variable xi that is uniquely in-
dexed by the corresponding node i. For example, the lambda
form for nsubj(e1, e2) is ?x1?x2.nsubj(x1, x2).
5Variables can range over arbitrary lambda forms.
298
main predicates are:
e ? c: expression e is assigned to cluster c;
SubExpr(s, e): s is a subexpression of e;
HasValue(s, v): s is of value v;
IsPart(c, i, p): p is the property cluster in ob-
ject cluster c uniquely indexed by i.
In USP, property clusters in different object clus-
ters use distinct index i?s. As we will see later,
in OntoUSP, property clusters with ISA relation
share the same index i, which corresponds to a
generic semantic frame such as agent and patient.
The probability model of USP can be captured
by two formulas:
x ? +p ? HasValue(x,+v)
e ? c ? SubExpr(x, e) ? x ? p
? ?1i.IsPart(c, i, p).
All free variables are implicitly universally
quantified. The ?+? notation signifies that the
MLN contains an instance of the formula, with
a separate weight, for each value combination of
the variables with a plus sign. The first formula is
the core of the model and represents the mixture
of property values given the cluster. The second
formula ensures that a property cluster must be a
part in the corresponding object cluster; it is a hard
constraint, as signified by the period at the end.
To encourage clustering, USP imposes an expo-
nential prior over the number of parameters.
To parse a new sentence, USP starts by parti-
tioning the QLF into atomic forms, and then hill-
climbs on the probability using a search operator
based on lambda reduction until it finds the max-
imum a posteriori (MAP) parse. During learn-
ing, USP starts with clusters of atomic forms,
maintains the optimal semantic parses according
to current parameters, and hill-climbs on the log-
likelihood of observed QLFs using two search op-
erators:
MERGE(c1, c2) merges clusters c1, c2 into a larger
cluster c by merging the core-form clusters
and argument clusters of c1, c2, respectively.
E.g., c1 = {?induce?}, c2 = {?enhance?},
and c = {?induce?, ?enhance?}.
COMPOSE(c1, c2) creates a new lambda-form
cluster c formed by composing the lambda
forms in c1, c2 into larger ones. E.g., c1 =
{?amino?}, c2 = {?acid?}, and c =
{?amino acid?}.
Each time, USP executes the highest-scored op-
erator and reparses affected sentences using the
new parameters. The output contains the optimal
lambda-form clusters and parameters, as well as
the MAP semantic parses of input sentences.
3 Unsupervised Ontology Induction with
Markov Logic
A major limitation of USP is that it either merges
two object clusters into one, or leaves them sepa-
rate. This is suboptimal, because different object
clusters may still possess substantial commonali-
ties. Modeling these can help extract more gen-
eral knowledge and answer many more questions.
The best way to capture such commonalities is
by forming an ISA hierarchy among the clusters.
For example, INDUCE and INHIBIT are both sub-
concepts of REGULATE. Learning these ISA rela-
tions helps answer questions like ?What regulates
CD11b??, when the text states that ?IL-4 induces
CD11b? or ?AP-1 suppresses CD11b?.
For parameter learning, this is also undesirable.
Without the hierarchical structure, each cluster es-
timates its parameters solely based on its own ob-
servations, which can be extremely sparse. The
better solution is to leverage the hierarchical struc-
ture for smoothing (a.k.a. shrinkage (McCallum et
al., 1998; Gelman and Hill, 2006)). For example,
if we learn that ?super-induce? is a verb and that in
general verbs have active and passive voices, then
even though ?super-induce? only shows up once
in the corpus as in ?AP-1 is super-induced by IL-
4?, by smoothing we can still infer that this proba-
bly means the same as ?IL-4 super-induces AP-1?,
which in turn helps answer questions like ?What
super-induces AP-1?.
OntoUSP overcomes the limitations of USP by
replacing the flat clustering process with a hier-
archical clustering one, and learns an ISA hier-
archy of lambda-form clusters in addition to the
IS-PART one. The output of OntoUSP consists
of an ontology, a semantic parser, and the MAP
parses. In effect, OntoUSP conducts ontology in-
duction, population, and knowledge extraction in a
single integrated process. Specifically, given clus-
ters c1, c2, in addition to merge vs. separate, On-
toUSP evaluates a third option called abstraction,
in which a new object cluster c is created, and ISA
links are added from ci to c; the argument clusters
in c are formed by merging that of ci?s.
In the remainder of the section, we describe the
299
details of OntoUSP. We start by presenting the
OntoUSP MLN. We then describe our inference
algorithm and how to parse a new sentence us-
ing OntoUSP. Finally, we describe the learning al-
gorithm and how OntoUSP induces the ontology
while learning the semantic parser.
3.1 The OntoUSP MLN
The OntoUSP MLN can be obtained by modifying
the USP MLN with three simple changes. First,
we introduce a new predicate IsA(c1, c2), which
is true if cluster c1 is a subconcept of c2. For con-
venience, we stipulate that IsA is reflexive (i.e.,
IsA(c, c) is true for any c). Second, we add two
formulas to the MLN:
IsA(c1, c2) ? IsA(c2, c3) ? IsA(c1, c3).
IsPart(c1, i1, p1) ? IsPart(c2, i2, p2)
? IsA(c1, c2) ? (i1 = i2 ? IsA(p1, p2)).
The first formula simply enforces the transitivity
of ISA relation. The second formula states that if
the ISA relation holds for a pair of object clusters,
it also holds between their corresponding property
clusters. Both are hard constraints. Third, we in-
troduce hierarchical smoothing into the model by
replacing the USP mixture formula
x ? +p ? HasValue(x,+v)
with a new formula
ISA(p1,+p2) ? x ? p1 ? HasValue(x,+v)
Intuitively, for each p2, the weight corresponds to
the delta in log-probability of v comparing to the
prediction according to all ancestors of p2. The
effect of this change is that now the value v of
a subexpression x is not solely determined by its
property cluster p1, but is also smoothed by statis-
tics of all p2 that are super clusters of p1.
Shrinkage takes place via interaction among the
weights of the ISA mixture formula. In particular,
if the weights for some property cluster p are all
zero, it means that values in p are completely pre-
dicted by p?s ancestors. In effect, p is backed off
to its parent.
3.2 Inference
Given the dependency tree T of a sentence, the
conditional probability of a semantic parse L is
given by Pr(L|T ) ? exp (?i wini(T,L)).
The MAP semantic parse is simply
Algorithm 1 OntoUSP-Parse(MLN, T )
Initialize semantic parse L with individual
atoms in the QLF of T
repeat
for all subexpressions e in L do
Evaluate all semantic parses that are
lambda-reducible from e
end for
L? the new semantic parse with the highest
gain in probability
until none of these improve the probability
return L
argmaxL
?
i wini(T, L). Directly enumer-
ating all L?s is intractable. OntoUSP uses the
same inference algorithm as USP by hill-climbing
on the probability of L; in each step, OntoUSP
evaluates the alternative semantic parses that
can be formed by lambda-reducing a current
subexpression with one of its arguments. The only
difference is that OntoUSP uses a different MLN
and so the probabilities and resulting semantic
parses may be different. Algorithm 1 gives
pseudo-code for OntoUSP?s inference algorithm.
3.3 Learning
OntoUSP uses the same learning objective as USP,
i.e., to find parameters ? that maximizes the log-
likelihood of observing the dependency trees T ,
summing out the unobserved semantic parses L:
L?(T ) = logP?(L)
= log?L P?(T, L)
However, the learning problem in OntoUSP is
distinct in two important aspects. First, OntoUSP
learns in addition an ISA hierarchy among the
lambda-form clusters. Second and more impor-
tantly, OntoUSP leverages this hierarchy during
learning to smooth the parameter estimation of in-
dividual clusters, as embodied by the new ISA
mixture formula in the OntoUSP MLN.
OntoUSP faces several new challenges unseen
in previous hierarchical-smoothing approaches.
The ISA hierarchy in OntoUSP is not known in
advance, but needs to be learned as well. Simi-
larly, OntoUSP has no known examples of pop-
ulated facts and rules in the ontology, but has to
infer that in the same joint learning process. Fi-
nally, OntoUSP does not start from well-formed
structured input like relational tuples, but rather
directly from raw text. In sum, OntoUSP tackles a
300
Algorithm 2 OntoUSP-Learn(MLN, T?s)
Initialize with a flat ontology, along with clus-
ters and semantic parses
Merge clusters with the same core form
Agenda ? ?
repeat
for all candidate operations O do
Score O by log-likelihood improvement
if score is above a threshold then
Add O to agenda
end if
end for
Execute the highest scoring operation O? in
the agenda
Regenerate MAP parses for affected trees and
update agenda and candidate operations
until agenda is empty
return the learned ontology and MLN, and the
semantic parses
very hard problem with exceedingly little aid from
user supervision.
To combat these challenges, OntoUSP adopts
a novel form of hierarchical smoothing by inte-
grating it with the search process for identify-
ing the hierarchy. Algorithm 2 gives pseudo-
code for OntoUSP?s learning algorithm. Like
USP, OntoUSP approximates the sum over all
semantic parses with the most probable parse,
and searches for both ? and the MAP semantic
parses L that maximize P?(T,L). In addition to
MERGE and COMPOSE, OntoUSP uses a new opera-
tor ABSTRACT(c1, c2), which does the following:
1. Create an abstract cluster c;
2. Create ISA links from c1, c2 to c;
3. Align property clusters of c1 and c2; for each
aligned pair p1 and p2, either merge them
into a single property cluster, or create an ab-
stract property cluster p in c and create ISA
links from pi to p, so as to maximize log-
likelihood.
Intuitively, c corresponds to a more abstract con-
cept that summarizes similar properties in ci?s.
To add a child cluster c2 to an existing ab-
stract cluster c1, OntoUSP also uses an operator
ADDCHILD(c1, c2) that does the following:
1. Create an ISA link from c2 to c1;
2. For each property cluster of c2, maximize the
log-likelihood by doing one of the following:
merge it with a property cluster in an exist-
ing child of c1; create ISA link from it to
an abstract property cluster in c; leave it un-
changed.
For efficiency, in both operators, the best option
is chosen greedily for each property cluster in c2,
in descending order of cluster size.
Notice that once an abstract cluster is created,
it could be merged with an existing cluster using
MERGE. Thus with the new operators, OntoUSP
is capable of inducing any ISA hierarchy among
abstract and existing clusters. (Of course, the ISA
hierarchy it actually induces depends on the data.)
Learning the shrinkage weights has been ap-
proached in a variety of ways; examples include
EM and cross-validation (McCallum et al, 1998),
hierarchical Bayesian methods (Gelman and Hill,
2006), and maximum entropy with L1 priors
(Dudik et al, 2007). The past methods either only
learn parameters with one or two levels (e.g., in
hierarchical Bayes), or requires significant amount
of computation (e.g., in EM and in L1-regularized
maxent), while also typically assuming a given
hierarchy. In contrast, OntoUSP has to both in-
duce the hierarchy and populate it, with potentially
many levels in the induced hierarchy, starting from
raw text with little user supervision.
Therefore, OntoUSP simplifies the weight
learning problem by adopting standard m-
estimation for smoothing. Namely, the weights
for cluster c are set by counting its observations
plus m fractional samples from its parent distribu-
tion. When c has few observations, its unreliable
statistics can be significantly augmented via the
smoothing by its parent (and in turn to a gradually
smaller degree by its ancestors). m is a hyperpa-
rameter that can be used to trade off bias towards
statistics for parent vs oneself.
OntoUSP also needs to balance between two
conflicting aspects during learning. On one hand,
it should encourage creating abstract clusters to
summarize intrinsic commonalities among the
children. On the other hand, this needs to be heav-
ily regularized to avoid mistaking noise for the sig-
nal. OntoUSP does this by a combination of priors
and thresholding. To encourage the induction of
higher-level nodes and inheritance, OntoUSP im-
poses an exponential prior ? on the number of pa-
rameter slots. Each slot corresponds to a distinct
property value. A child cluster inherits its parent?s
slots (and thus avoids the penalty on them). On-
301
toUSP also stipulates that, in an ABSTRACT opera-
tion, a new property cluster can be created either as
a concrete cluster with full parameterization, or as
an abstract cluster that merely serves for smooth-
ing purposes. To discourage overproposing clus-
ters and ISA links, OntoUSP imposes a large ex-
ponential prior ? on the number of concrete clus-
ters created by ABSTRACT. For abstract cluster, it
sets a cut-off tp and only allows storing a probabil-
ity value no less than tp. Like USP, it also rejects
MERGE and COMPOSE operations that improve log-
likelihood by less than to. These priors and cut-off
values can be tuned to control the granularity of
the induced ontology and clusters.
Concretely, given semantic parses L, OntoUSP
computes the optimal parameters and evaluates
the regularized log-likelihood as follows. Let
wp2,v denote the weight of the ISA mixture for-
mula ISA(p1,+p2)? x ? p1 ? HasValue(x,+v).
For convenience, for each pair of property clus-
ter c and value v, OntoUSP instead computes
and stores w?c,v =
?
ISA(c, a)wa,v, which sums
over all weights for c and its ancestors. (Thus
wc,v = w?c,v ? w?p,v, where p is the parent of
c.) Like USP, OntoUSP imposes local normal-
ization constraints that enable closed-form esti-
mation of the optimal parameters and likelihood.
Specifically, using m-estimation, the optimal w?c,v
is log((m ?ew?p,v +nc,v)/(m+nc)), where p is the
parent of c and n is the count. The log-likelihood
is ?c,v w?c,v ?nc,v, which is then augmented by the
priors.
4 Experiments
4.1 Methodology
Evaluating unsupervised ontology induction is dif-
ficult, because there is no gold ontology for com-
parison. Moreover, our ultimate goal is to aid
knowledge acquisition, rather than just inducing
an ontology for its own sake. Therefore, we
used the same methodology and dataset as the
USP paper to evaluate OntoUSP on its capabil-
ity in knowledge acquisition. Specifically, we ap-
plied OntoUSP to extract knowledge from the GE-
NIA dataset (Kim et al, 2003) and answer ques-
tions, and we evaluated it on the number of ex-
tracted answers and accuracy. GENIA contains
1999 PubMed abstracts.6 The question set con-
6http://www-tsujii.is.s.u-tokyo-
.ac.jp/GENIA/home/wiki.cgi.
tains 2000 questions which were created by sam-
pling verbs and entities according to their frequen-
cies in GENIA. Sample questions include ?What
regulates MIP-1alpha??, ?What does anti-STAT 1
inhibit??. These simple question types were used
to focus the evaluation on the knowledge extrac-
tion aspect, rather than engineering for handling
special question types and/or reasoning.
4.2 Systems
OntoUSP is the first unsupervised approach that
synergistically conducts ontology induction, pop-
ulation, and knowledge extraction. The system
closest in aim and capability is USP. We thus com-
pared OntoUSP with USP and all other systems
evaluated in the USP paper (Poon and Domingos,
2009). Below is a brief description of the systems.
(For more details, see Poon & Domingos (2009).)
Keyword is a baseline system based on keyword
matching. It directly matches the question sub-
string containing the verb and the available argu-
ment with the input text, ignoring case and mor-
phology. Given a match, two ways to derive the
answer were considered: KW simply returns the
rest of sentence on the other side of the verb,
whereas KW-SYN is informed by syntax and ex-
tracts the answer from the subject or object of the
verb, depending on the question (if the expected
argument is absent, the sentence is ignored).
TextRunner (Banko et al, 2007) is the state-of-
the-art system for open-domain information ex-
traction. It inputs text and outputs relational triples
in the form (R,A1, A2), where R is the relation
string, and A1, A2 the argument strings. To an-
swer questions, each triple-question pair is consid-
ered in turn by first matching their relation strings,
and then the available argument strings. If both
match, the remaining argument string in the triple
is returned as an answer. Results were reported
when exact match is used (TR-EXACT), or when
the triple strings may contain the question ones as
substrings (TR-SUB).
RESOLVER (Yates and Etzioni, 2009) inputs
TextRunner triples and collectively resolves coref-
erent relation and argument strings. To answer
questions, the only difference from TextRunner is
that a question string can match any string in its
cluster. As in TextRunner, results were reported
for both exact match (RS-EXACT) and substring
(RS-SUB).
DIRT (Lin and Pantel, 2001) resolves binary rela-
302
Table 1: Comparison of question answering re-
sults on the GENIA dataset. Results for systems
other than OntoUSP are from Poon & Domingos
(2009).
# Total # Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP 334 295 88%
OntoUSP 480 435 91%
tions by inputting a dependency path that signifies
the relation and returns a set of similar paths. To
use DIRT in question answering, it was queried to
obtain similar paths for the relation of the ques-
tion, which were then used to match sentences.
USP (Poon and Domingos, 2009) parses the in-
put text using the Stanford dependency parser
(Klein and Manning, 2003; de Marneffe et al,
2006), learns an MLN for semantic parsing from
the dependency trees, and outputs this MLN and
the MAP semantic parses of the input sentences.
These MAP parses formed the knowledge base
(KB). To answer questions, USP first parses the
questions (with the question slot replaced by a
dummy word), and then matches the question
parse to parses in the KB by testing subsumption.
OntoUSP uses a similar procedure as USP for ex-
tracting knowledge and answering questions, ex-
cept for two changes. First, USP?s learning and
parsing algorithms are replaced with OntoUSP-
Learn and OntoUSP-Parse, respectively. Second,
when OntoUSP matches a question to its KB, it
not only considers the lambda-form cluster of the
question relation, but also all its sub-clusters.7
4.3 Results
Table 1 shows the results comparing OntoUSP
with other systems. While USP already greatly
outperformed other systems in both precision and
recall, OntoUSP further substantially improved on
the recall of USP, without any loss in precision.
In particular, OntoUSP extracted 140 more correct
answers than USP, for a gain of 47% in absolute
7Additional details are available at
http : //alchemy.cs.washington.edu/papers/poon10.
ISA ISA
INHIBIT
induce, enhance, trigger, augment, up-regulate
INDUCE
inhibit, block, suppress, prevent, abolish, abrogate, down-regulate
activate
regulate, control, govern, modulate
ISA
ACTIVATE
REGULATE
Figure 3: A fragment of the induced ISA hierar-
chy, showing the core forms for each cluster (the
cluster labels are added by the authors for illustra-
tion purpose).
recall. Compared to TextRunner (TR-SUB), On-
toUSP gained on precision by 38 points and ex-
tracted more than five times of correct answers.
Manual inspection shows that the induced ISA
hierarchy is the key for the recall gain. Like
USP, OntoUSP discovered the following clusters
(in core forms) that represent some of the core
concepts in biomedical research:
{regulate, control, govern, modulate}
{induce, enhance, trigger, augment, up-
regulate}
{inhibit, block, suppress, prevent, abolish, ab-
rogate, down-regulate}
However, USP formed these as separate clusters,
whereas OntoUSP in addition induces ISA rela-
tions from the INDUCE and INHIBIT clusters to
the REGULATE cluster (Figure 3). This allows
OntoUSP to answer many more questions that
are asked about general regulation events, even
though the text states them with specific regula-
tion directions like ?induce? or ?inhibit?. Below
is an example question-answer pair output by On-
toUSP; neither USP nor any other system were
able to extract the necessary knowledge.
Q: What does IL-2 control?
A: The DEX-mediated IkappaBalpha induc-
tion.
Sentence: Interestingly, the DEX-mediated
IkappaBalpha induction was completely inhibited
by IL-2, but not IL-4, in Th1 cells, while the re-
verse profile was seen in Th2 cells.
OntoUSP also discovered other interesting
commonalities among the clusters. For exam-
ple, both USP and OntoUSP formed a singleton
cluster with core form ?activate?. Although this
cluster may appear similar to the INDUCE clus-
ter, the data in GENIA does not support merg-
ing the two. However, OntoUSP discovered that
303
the ACTIVATE cluster, while not completely resol-
vent with INDUCE, shared very similar distribu-
tions in their agent arguments. In fact, they are
so similar that OntoUSP merges them into a sin-
gle property cluster. It found that the patient ar-
guments of INDUCE and INHIBIT are very similar
and merged them. In turn, OntoUSP formed ISA
links from these three object clusters to REGULATE,
as well as among their property clusters. In-
tuitively, this makes sense. The positive- and
negative-regulation events, as signified by INDUCE
and INHIBIT, often target similar object entities
or processes. However, their agents tend to differ
since in one case they are inducers, and in the other
they are inhibitors. On the other hand, ACTIVATE
and INDUCE share similar agents since they both
signify positive regulation. However, ?activate?
tends to be used more often when the patient ar-
gument is a concrete entity (e.g., cells, genes, pro-
teins), whereas ?induce? and others are also used
with processes and events (e.g., expressions, inhi-
bition, pathways).
USP was able to resolve common syntactic dif-
ferences such as active vs. passive voice. How-
ever, it does so on the basis of individual verbs,
and there is no generalization beyond their clus-
ters. OntoUSP, on the other hand, formed a high-
level cluster with two abstract property clusters,
corresponding to general agent argument and pa-
tient argument. The active-passive alternation is
captured in these clusters, and is inherited by all
descendant clusters, including many rare verbs
like ?super-induce? which only occur once in GE-
NIA and for which there is no way that USP
could have learned about their active-passive al-
ternations. This illustrates the importance of dis-
covering ISA relations and performing hierarchi-
cal smoothing.
4.4 Discussion
OntoUSP is a first step towards joint ontology in-
duction and knowledge extraction. The experi-
mental results demonstrate the promise in this di-
rection. However, we also notice some limitations
in the current system. While OntoUSP induced
meaningful ISA relations among relation clusters
like REGULATE, INDUCE, etc., it was less success-
ful in inducing ISA relations among entity clus-
ters such as specific genes and proteins. This is
probably due to the fact that our model only con-
siders local features such as the parent and argu-
ments. A relation is often manifested as verbs and
has several arguments, whereas an entity typically
appears as an argument of others and has few ar-
guments of its own. As a result, in average, there
is less information available for entities than rela-
tions. Presumably, we can address this limitation
by modeling longer-ranged dependencies such as
grandparents, siblings, etc. This is straightforward
to do in Markov logic.
OntoUSP also uses a rather elaborate scheme
for regularization. We hypothesize that this can
be much simplified and improved by adopting a
principled framework such as Dudik et al (2007).
5 Conclusion
This paper introduced OntoUSP, the first unsuper-
vised end-to-end system for ontology induction
and knowledge extraction from text. OntoUSP
builds on the USP semantic parser by adding the
capability to form hierarchical clusterings of logi-
cal expressions, linked by ISA relations, and us-
ing them for hierarchical smoothing. OntoUSP
greatly outperformed USP and other state-of-the-
art systems in a biomedical knowledge acquisition
task.
Directions for future work include: exploiting
the ontological structure for principled handling of
antonyms and (more generally) expressions with
opposite meanings; developing and testing alter-
nate methods for hierarchical modeling in On-
toUSP; scaling up learning and inference to larger
corpora; investigating the theoretical properties of
OntoUSP?s learning approach and generalizing it
to other tasks; answering questions that require in-
ference over multiple extractions; etc.
6 Acknowledgements
We give warm thanks to the anonymous reviewers for
their comments. This research was partly funded by ARO
grant W911NF-08-1-0242, AFRL contract FA8750-09-C-
0181, DARPA contracts FA8750-05-2-0283, FA8750-07-D-
0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-
D030010, NSF grants IIS-0534881 and IIS-0803481, and
ONR grant N00014-08-1-0670. The views and conclusions
contained in this document are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies, either expressed or implied, of ARO, DARPA,
NSF, ONR, or the United States Government.
References
Hiyan Alshawi. 1990. Resolving quasi logical forms. Com-
putational Linguistics, 16:133?144.
G. Bakir, T. Hofmann, B. B. Scho?lkopf, A. Smola, B. Taskar,
304
S. Vishwanathan, and (eds.). 2007. Predicting Structured
Data. MIT Press, Cambridge, MA.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open informa-
tion extraction from the web. In Proceedings of the Twen-
tieth International Joint Conference on Artificial Intelli-
gence, pages 2670?2676, Hyderabad, India. AAAI Press.
Philipp Cimiano. 2006. Ontology learning and population
from text. Springer.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
Fifth International Conference on Language Resources
and Evaluation, pages 449?454, Genoa, Italy. ELRA.
Pedro Domingos and Daniel Lowd. 2009. Markov Logic:
An Interface Layer for Artificial Intelligence. Morgan &
Claypool, San Rafael, CA.
Miroslav Dudik, David Blei, and Robert Schapire. 2007. Hi-
erarchical maximum entropy density estimation. In Pro-
ceedings of the Twenty Fourth International Conference
on Machine Learning.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Andrew Gelman and Jennifer Hill. 2006. Data Analysis Us-
ing Regression and Multilevel/Hierarchical Models. Cam-
bridge University Press.
Lise Getoor and Ben Taskar, editors. 2007. Introduction to
Statistical Relational Learning. MIT Press, Cambridge,
MA.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi Tsu-
jii. 2003. GENIA corpus - a semantically annotated cor-
pus for bio-textmining. Bioinformatics, 19:180?82.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Forty First
Annual Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of
inference rules from text. In Proceedings of the Seventh
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 323?328, San Fran-
cisco, CA. ACM Press.
Alexander Maedche. 2002. Ontology learning for the se-
mantic Web. Kluwer Academic Publishers, Boston, Mas-
sachusetts.
Andrew McCallum, Ronald Rosenfeld, Tom Mitchell, and
Andrew Ng. 1998. Improving text classification by
shrinkage in a hierarchy of classes. In Proceedings of the
Fifteenth International Conference on Machine Learning.
Hoifung Poon and Pedro Domingos. 2008. Joint unsuper-
vised coreference resolution with Markov logic. In Pro-
ceedings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 649?658, Honolulu,
HI. ACL.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised
semantic parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing,
pages 1?10, Singapore. ACL.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006. Seman-
tic taxonomy induction from heterogenous evidence. In
Proceedings of COLING/ACL 2006.
S. Staab and R. Studer. 2004. Handbook on ontologies.
Springer.
Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2008. Yago - a large ontology from Wikipedia and Word-
Net. Journal of Web Semantics.
Fabian Suchanek, Mauro Sozio, and Gerhard Weikum. 2009.
Sofie: A self-organizing framework for information ex-
traction. In Proceedings of the Eighteenth International
Conference on World Wide Web.
Jun-ichi Tsujii. 2004. Thesaurus or logical ontology, which
do we need for mining text? In Proceedings of the Lan-
guage Resources and Evaluation Conference.
Fei Wu and Daniel S. Weld. 2008. Automatically refining the
wikipedia infobox ontology. In Proceedings of the Seven-
teenth International Conference on World Wide Web, Bei-
jing, China.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
305
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 933?943,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Grounded Unsupervised Semantic Parsing
Hoifung Poon
One Microsoft Way
Microsoft Research
Redmond, WA 98052, USA
hoifung@microsoft.com
Abstract
We present the first unsupervised ap-
proach for semantic parsing that rivals
the accuracy of supervised approaches
in translating natural-language questions
to database queries. Our GUSP system
produces a semantic parse by annotat-
ing the dependency-tree nodes and edges
with latent states, and learns a proba-
bilistic grammar using EM. To compen-
sate for the lack of example annotations
or question-answer pairs, GUSP adopts
a novel grounded-learning approach to
leverage database for indirect supervision.
On the challenging ATIS dataset, GUSP
attained an accuracy of 84%, effectively
tying with the best published results by su-
pervised approaches.
1 Introduction
Semantic parsing maps text to a formal mean-
ing representation such as logical forms or struc-
tured queries. Recently, there has been a bur-
geoning interest in developing machine-learning
approaches for semantic parsing (Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Mooney, 2007; Kwiatkowski et al, 2011), but
the predominant paradigm uses supervised learn-
ing, which requires example annotations that are
costly to obtain. More recently, several grounded-
learning approaches have been proposed to alle-
viate the annotation burden (Chen and Mooney,
2008; Kim and Mooney, 2010; Bo?rschinger et al,
2011; Clarke et al, 2010; Liang et al, 2011). In
particular, Clarke et al (2010) and Liang et al
(2011) proposed methods to learn from question-
answer pairs alone, which represents a significant
advance. However, although these methods exon-
erate annotators from mastering specialized logi-
cal forms, finding the answers for complex ques-
tions still requires non-trivial effort. 1
Poon & Domingos (2009, 2010) proposed the
USP system for unsupervised semantic parsing,
which learns a parser by recursively clustering
and composing synonymous expressions. While
their approach completely obviates the need for di-
rect supervision, their target logic forms are self-
induced clusters, which do not align with existing
database or ontology. As a result, USP can not be
used directly to answer complex questions against
an existing database. More importantly, it misses
the opportunity to leverage database for indirect
supervision.
In this paper, we present the GUSP system,
which combines unsupervised semantic parsing
with grounded learning from a database. GUSP
starts with the dependency tree of a sentence and
produces a semantic parse by annotating the nodes
and edges with latent semantic states derived from
the database. Given a set of natural-language
questions and a database, GUSP learns a prob-
abilistic semantic grammar using EM. To com-
pensate for the lack of direct supervision, GUSP
constrains the search space using the database
schema, and bootstraps learning using lexical
scores computed from the names and values of
database elements.
Unlike previous grounded-learning approaches,
GUSP does not require ambiguous annotations
or oracle answers, but rather focuses on lever-
aging database contents that are readily avail-
able. Unlike USP, GUSP predetermines the tar-
get logical forms based on the database schema,
which alleviates the difficulty in learning and en-
sures that the output semantic parses can be di-
rectly used in querying the database. To handle
syntax-semantics mismatch, GUSP introduces a
novel dependency-based meaning representation
1Clarke et al (2010) and Liang et al (2011) used the
annotated logical forms to compute answers for their experi-
ments.
933
by augmenting the state space to represent seman-
tic relations beyond immediate dependency neigh-
borhood. This representation also factorizes over
nodes and edges, enabling linear-time exact infer-
ence in GUSP.
We evaluated GUSP on end-to-end question
answering using the ATIS dataset for semantic
parsing (Zettlemoyer and Collins, 2007). Com-
pared to other standard datasets such as GEO and
JOBS, ATIS features a database that is an order
of magnitude larger in the numbers of relations
and instances, as well as a more irregular lan-
guage (ATIS questions were derived from spo-
ken dialogs). Despite these challenges, GUSP
attains an accuracy of 84% in end-to-end ques-
tion answering, effectively tying with the state-
of-the-art supervised approaches (85% by Zettle-
moyer & Collins (2007), 83% by Kwiatkowski et
al. (2011)).
2 Background
2.1 Semantic Parsing
The goal of semantic parsing is to map text to
a complete and detailed meaning representation
(Mooney, 2007). This is in contrast with semantic
role labeling (Carreras and Marquez, 2004) and in-
formation extraction (Banko et al, 2007; Poon and
Domingos, 2007), which have a more restricted
goal of identifying local semantic roles or extract-
ing selected information slots.
The standard language for meaning representa-
tion is first-order logic or a sublanguage, such as
FunQL (Kate et al, 2005; Clarke et al, 2010) and
lambda calculus (Zettlemoyer and Collins, 2005;
Zettlemoyer and Collins, 2007). Poon & Domin-
gos (2009, 2010) induce a meaning representa-
tion by clustering synonymous lambda-calculus
forms stemming from partitions of dependency
trees. More recently, Liang et al (2011) proposed
DCS for dependency-based compositional seman-
tics, which represents a semantic parse as a tree
with nodes representing database elements and op-
erations, and edges representing relational joins.
In this paper, we focus on semantic parsing
for natural-language interface to database (Grosz
et al, 1987). In this problem setting, a natural-
language question is first translated into a mean-
ing representation by semantic parsing, and then
converted into a structured query such as SQL to
obtain answer from the database.
2.2 Unsupervised Semantic Parsing
Unsupervised semantic parsing was first proposed
by Poon & Domingos (2009, 2010) with their
USP system. USP defines a probabilistic model
over the dependency tree and semantic parse us-
ing Markov logic (Domingos and Lowd, 2009),
and recursively clusters and composes synony-
mous dependency treelets using a hard EM-like
procedure. Since USP uses nonlocal features (e.g.,
the argument-number feature) and operates over
partitions, exact inference is intractable, and USP
resorts to a greedy approach to find the MAP parse
by searching over partitions. Titov & Klementiev
(2011) proposed a Bayesian version of USP and
Titov & Klementiev (2012) adapted it for seman-
tic role induction. In USP, the meaning is repre-
sented by self-induced clusters. Therefore, to an-
swer complex questions against a database, it re-
quires an additional ontology matching step to re-
solve USP clusters with database elements.
Popescu et al (2003, 2004) proposed the PRE-
CISE system, which does not require labeled ex-
amples and can be directly applied to question
answering with a database. The PRECISE sys-
tem, however, requires substantial amount of engi-
neering, including a domain-specific lexicon that
specifies the synonyms for names and values of
database elements, a restricted set of potential in-
terpretations for domain verbs and prepositions, as
well as a set of domain questions with manually la-
beled POS tags for retraining the tagger and parser.
It also focuses on the subset of easy questions (?se-
mantically tractable? questions), and sidesteps the
problem of dealing with complex and nested struc-
tures, as well as ambiguous interpretations. Re-
markably, while PRECISE can be very accurate
on easy questions, it does not try to learn from
these interpretations. In contrast, Goldwasser et
al. (2011) proposed a self-supervised approach,
which iteratively chose high-confidence parses to
retrain the parser. Their system, however, still
required a lexicon manually constructed for the
given domain. Moreover, it was only applied to
a small domain (a subset of GEO), and the result
still trailed supervised systems by a wide margin.
2.3 Grounded Learning for Semantic Parsing
Grounded learning is motivated by alleviating the
burden of direct supervision via interaction with
the world, where the indirect supervision may
take the form as ambiguous annotations (Chen
934
get
toronto
flight from to
diego
in
san stopping
dtw
E:flight:R
E:flight
V:city.name
V:city.name:C
E:flight_stop
V:airport.code
V:city.name + E:flight
Figure 1: End-to-end question answering by
GUSP for sentence get flight from toronto to san
diego stopping in dtw. Top: the dependency tree
of the sentence is annotated with latent semantic
states by GUSP. For brevity, we omit the edge
states. Raising occurs from flight to get and sink-
ing occurs from get to diego. Bottom: the seman-
tic tree is deterministically converted into SQL to
obtain answer from the database.
and Mooney, 2008; Kim and Mooney, 2010;
Bo?rschinger et al, 2011) or example question-
answer pairs (Clarke et al, 2010; Liang et al,
2011). In general, however, such supervision is
not always available or easy to obtain. In con-
trast, databases are often abundantly available, es-
pecially for important domains.
The database community has considerable
amount of work on leveraging databases in various
tasks such as entity resolution, schema matching,
and others. To the best of our knowledge, this ap-
proach is still underexplored in the NLP commu-
nity. One notable exception is distant supervision
(Mintz et al, 2009; Riedel et al, 2010; Hoffmann
et al, 2011; Krishnamurthy and Mitchell, 2012;
Heck et al, 2013), which used database instances
to derive training examples for relation extraction.
This approach, however, still has considerable lim-
itations. For example, it only handles binary rela-
tions, and the quality of the training examples is
inherently noisy and hard to control. Moreover,
this approach is not applicable to the question-
answering setting considered in this paper, since
entity pairs in questions need not correspond to
valid relational instances in the database.
3 Grounded Unsupervised Semantic
Parsing
In this section, we present the GUSP system for
grounded unsupervised semantic parsing. GUSP
is unsupervised and does not require example log-
ical forms or question-answer pairs. Figure 1
shows an example of end-to-end question answer-
ing using GUSP. GUSP produces a semantic parse
of the question by annotating its dependency tree
with latent semantic states. The semantic tree
can then be deterministically converted into SQL
to obtain answer from the database. Given a
set of natural-language questions and a database,
GUSP learns a probabilistic semantic grammar us-
ing EM.
To compensate for the lack of annotated ex-
amples, GUSP derives indirect supervision from
a novel combination of three key sources. First,
GUSP leverages the target database to constrain
the search space. Specifically, it defines the se-
mantic states based on the database schema, and
derives lexical-trigger scores from database ele-
ments to bootstrap learning.
Second, in contrast to most existing approaches
for semantic parsing, GUSP starts directly from
dependency trees and focuses on translating them
into semantic parses. While syntax may not al-
ways align perfectly with semantics, it is still
highly informative about the latter. In particular,
dependency edges are often indicative of semantic
relations. On the other hand, syntax and semantic
often diverge, and synactic parsing errors abound.
To combat this problem, GUSP introduces a novel
dependency-based meaning representation with an
augmented state space to account for semantic re-
lations that are nonlocal in the dependency tree.
GUSP?s approach of starting directly from de-
pendency tree is inspired by USP. However, GUSP
uses a different meaning representation defined
over individual nodes and edges, rather than par-
titions, which enables linear-time exact inference.
GUSP also handles complex linguistic phenomena
and syntax-semantics mismatch by explicitly aug-
menting the state space, whereas USP?s capability
in handling such phenomena is indirect and more
limited.
GUSP represents meaning by a semantic tree,
which is similar to DCS (Liang et al, 2011). Their
approach to semantic parsing, however, differs
from GUSP in that it induced the semantic tree di-
rectly from a sentence, rather than starting from
935
a dependency tree and annotating it. Their ap-
proach alleviates some complexity in the mean-
ing representation for handling syntax-semantics
mismatch, but it has to search over a much larger
search space involving exponentially many candi-
date trees. This might partially explain why it has
not yet been scaled up to the ATIS dataset.
Finally, GUSP recognizes that certain aspects
in semantic parsing may not be worth learn-
ing using precious annotated examples. These
are domain-independent and closed-class expres-
sions, such as times and dates (e.g., before 5pm
and July seventeenth), logical connectives (e.g.,
and, or, not), and numerics (e.g., 200 dol-
lars). GUSP preprocesses the text to detect such
expressions and restricts their interpretation to
database elements of compatible types (e.g., be-
fore 5pm vs. flight.departure time or
flight.arrival time). Short of training ex-
amples, GUSP also resolves quantifier scoping
ambiguities deterministically by a fixed ordering.
For example, in the phrase cheapest flight to Seat-
tle, the scope of cheapest can be either flight or
flight to seattle. GUSP always chooses to apply
the superlative at last, amounting to choosing the
most restricted scope (flight to seattle), which is
usually the correct interpretation.
In the remainder of this section, we first formal-
ize the problem setting and introduce the GUSP
meaning representation. We then present the
GUSP model and learning and inference algo-
rithms. Finally, we describe how to convert a
GUSP semantic parse into SQL.
3.1 Problem Formulation
Let d be a dependency tree, N(d) and E(d) be
its nodes and edges. In GUSP, a semantic parse
of d is an assignment z : N(d) ? E(d) ? S
that maps its nodes and edges to semantic states
in S. For example, in the example in Figure 1,
z(flight) = E : flight. At the core of GUSP
is a joint probability distribution P?(d, z) over the
dependency tree and the semantic parse. Seman-
tic parsing in GUSP amounts to finding the most
probable parse z? = argmaxz P?(d, z). Given
a set of sentences and their dependency trees D,
learning in GUSP maximizes the log-likelihood of
D while summing out the latent parses z:
?? = argmax logP?(D)
= argmax
?
d?D
log
?
z
P?(d, z)
3.2 Simple Semantic States
Node states GUSP creates a state E:X (E short
for entity) for each database entity X (i.e., a
database table), a state P:Y (P short for prop-
erty) and V:Y (V short for value) for each database
attribute Y (i.e., a database column). Node
states are assigned to dependency nodes. Intu-
itively, they represent database entities, proper-
ties, and values. For example, the ATIS do-
main contains entities such as flight and fare,
which may contain properties such as the depar-
ture time flight.departure time or ticket
price fare.one direction cost. The men-
tions of entities and properties are represented
by entity and property states, whereas constants
such as 9:25am or 120 dollars are repre-
sented by value states. In the semantic parse in
Figure 1, for example, flight is assigned to en-
tity state E:flight, where toronto is assigned
to value state V:city.name. There is a special
node state NULL, which signifies that the subtree
headed by the word contributes no meaning to the
semantic parse (e.g., an auxilliary verb).
Edge states GUSP creates an edge state for
each valid relational join paths connecting two
node states. Edge states are assigned to de-
pendency edges. GUSP enforces the constraints
that the node states of the dependency par-
ent and child must agree with the node states
in the edge state. For example, E:flight-
-V:flight.departure time represents a
natural join between the flight entity and the prop-
erty value departure time. For a dependency edge
e : a ? b, the assignment to E:flight-
-V:flight.departure time signifies that
a represents a flight entity, and b represents the
value of its departure time. An edge state may
also represent a relational path consisting of a
serial of joins. For example, Zettlemoyer and
Collins (2007) used a predicate from(f,c) to
signify that flight f starts from city c. In the ATIS
database, however, this amounts to a path of three
joins:
flight.from airport-airport
airport-airport service
airport service-city
In GUSP, this is represented by the edge
state flight-flight.from airport-
-airport-airport service-city.
936
GUSP only creates edge states for relational join
paths up to length four, as longer paths rarely
correspond to meaningful semantic relations.
Composition To handle compositions such as
American Airlines and New York City, it helps
to distinguish the head words (Airlines and City)
from the rest. In GUSP, this is handled by intro-
ducing, for each node state such as E:airline,
a new node state such as E:airline:C, where
C signifies composition. For example, in Figure
1, diego is assigned to V:city.name, whereas
san is assigned to V:city.name:C, since san
diego forms a single meaning unit, and should be
translated into SQL as a whole.
3.3 Domain-Independent States
These are for handling special linguistic phenom-
ena that are not domain-specific, such as negation,
superlatives, and quantifiers.
Operator states GUSP create node states for
the logical and comparison operators (OR, AND,
NOT, MORE, LESS, EQ). Additionally, to han-
dle the cases when prepositions and logical
connectives are collapsed into the label of a
dependency edge, as in Stanford dependency,
GUSP introduces an edge state for each triple
of an operator and two node states, such as
E:flight-AND-E:fare.
Quantifier states GUSP creates a node state for
each of the standard SQL functions: argmin,
argmax, count, sum. Additionally, it cre-
ates a node state for each pair of compatible func-
tion and property. For example, argmin can
be applied to any numeric property, in particular
flight.departure time, and so the node
state P:flight.departure time:argmin
is created and can be assigned to superlatives such
as earliest.
3.4 Complex Semantic States
For sentences with a correct dependency tree and
well-aligned syntax and semantics, the simple se-
mantic states suffice for annotating the correct se-
mantic parse. However, in complex sentences,
syntax and semantic often diverge, either due to
their differing goals or simply stemming from syn-
tactic parsing errors. In Figure 1, the dependency
tree contains multiple errors: from toronto and to
san diego are mistakenly attached to get, which
has no literal meaning here; stopping in dtw is also
wrongly attached to diego rather than flight. An-
notating such a tree with only simple states will
lead to incorrect semantic parses, e.g., by joining
V:city:san diego with V:airport:dtw
via E:airport service, rather than join-
ing E:flight with V:airport:dtw via
E:flight stop.
To overcome these challenges, GUSP intro-
duces three types of complex states to handle
syntax-semantics divergence. Figure 1 shows the
correct semantic parse for the above sentence us-
ing the complex states.
Raising For each simple node state N, GUSP
creates a ?raised? state N:R (R short for raised). A
raised state signifies a word that has little or none
of its own meaning, but effectively takes one of its
child states to be its own (?raises?). Correspond-
ingly, GUSP creates a ?raising? edge state N-R-N,
which signifies that the parent is a raised state and
its meaning is derived from the dependency child
of state N. For all other children, the parent be-
haves just as state N. For example, in Figure 1, get
is assigned to the raised state E:flight:R, and
the edge between get and flight is assigned to the
raising edge state E:flight-R-E:flight.
Sinking For simple node states A, B and an
edge state E connecting the two, GUSP creates
a ?sinking? node state A+E+B:S (S for sinking).
When a node n is assigned to such a sinking state,
n can behave as either A or B for its children
(i.e., the edge states can connect to either one),
and n?s parent must be of state B. In Figure 1,
for example, diego is assigned to a sinking state
V:city.name + E:flight (the edge state is
omitted for brevity). E:flight comes from its
parent get. For child san, diego behaves as in state
V:city.name, and their edge state is a simple
compositional join. For the other child stopping,
diego behaves as in state E:flight, and their
edge state is a relational join connecting flight
with flight stop. Effectively, this connects
stopping with get and eventually with flight (due to
raising), virtually correcting the syntax-semantics
mismatch stemming from attachment errors.
Implicit For simple node states A, B and an
edge state E connecting the two, GUSP also cre-
ates a node state A+E+B:I (I for implicit) with
the ?implicit? state B. In natural languages, an en-
tity is often introduced implicitly, which the reader
infers from shared world knowledge. For example,
937
to obtain the correct semantic parse for Give me
the fare from Seattle to Boston, one needs to infer
the existence of a flight entity, as in Give me the
fare (of a flight) from Seattle to Boston. Implicit
states offer candidates for addressing such needs.
As in sinking, child nodes have access to either of
the two simple states, but the implicit state is not
visible to the parent node.
3.5 Lexical-Trigger Scores
GUSP uses the database elements to automatically
derive a simple scoring scheme for lexical triggers.
If a database element has a name of k words, each
word is assigned score 1/k for the corresponding
node state. Similarly for property values and value
node states. In a sentence, if a word w triggers a
node state with score s, its dependency children
and left and right neighbors all get a trigger score
of 0.1?s for the same state. To score relevant words
not appearing in the database (due to incomplete-
ness of the database or lexical variations), GUSP
uses DASH (Pantel et al, 2009) to provide addi-
tional word-pair scoring based on lexical distribu-
tional similarity computed over general text cor-
pora (Wikipedia in this case). In the case of multi-
ple score assignments for the same word, the max-
imum score is used.
For multi-word values of property Y , and for
a dependency edge connecting two collocated
words, GUSP assigns a score 1.0 to the edge state
joining the value node state V:Y to its composi-
tion state V:Y:C, as well as the edge state joining
two composition states V:Y:C.
GUSP also uses a domain-independent list of
superlatives with the corresponding data types and
polarity (e.g., first, last, earliest, latest, cheapest)
and assigns a trigger score of 1.0 for each prop-
erty of a compatible data type (e.g., cheapest for
properties of type MONEY).
3.6 The GUSP Model
In a nutshell, the GUSP model resembles a tree-
HMM, which models the emission of words and
dependencies by node and edge states, as well as
transition between an edge state and the parent
and child node states. In preliminary experiments
on the development set, we found that the na??ve
model (with multinomials as conditional probabil-
ities) did not perform well in EM. We thus chose
to apply feature-rich EM (Berg-Kirkpatrick et al,
2010) in GUSP, which enabled the use of more
generalizable features. Specifically, GUSP defines
a probability distribution over dependency tree d
and semantic parse z by
P?(d, z) =
1
Z exp
?
i
fi(d, z) ? wi(d, z)
where fi andwi are features and their weights, and
Z is the normalization constant that sums over all
possible d, z (over the same unlabeled tree). The
features of GUSP are as follows:
Lexical-trigger scores These are implemented
as emission features with fixed weights. For ex-
ample, given a token t that triggers node state
N with score s, there is a corresponding features
1(lemma = t, state = N) with weight ??s, where
? is a parameter.
Emission features for node states GUSP uses
two templates for emission of node states: for
raised states, 1(token = ?), i.e., the emission
weights for all raised states are tied; for non-raised
states, 1(lemma = ?, state = N).
Emission features for edge states GUSP uses
the following templates for emission of edge
states:
Child node state is NULL, dependency= ?;
Edge state is RAISING, dependency= ?;
Parent node state is same as the child node state,
dependency= ?;
Otherwise, parent node state= ?, child node
state= ?, edge state type= ?, dependency= ?.
Transition features GUSP uses the following
templates for transition features, which are similar
to the edge emission features except for the depen-
dency label:
Child node state is NULL;
Edge state is RAISING;
Parent node state is same as the child node state;
Otherwise, parent node state= ?, child node
state= ?, edge state type= ?.
Complexity Prior To favor simple semantic
parses, GUSP imposes an exponential prior with
weight ? on nodes states that are not null or raised,
and on each relational join in an edge state.
3.7 Learning and Inference
Since the GUSP model factors over nodes and
edges, learning and inference can be done ef-
ficiently using EM and dynamic programming.
Specifically, the MAP parse and expectations can
938
be computed by tree-Viterbi and inside-outside
(Petrov and Klein, 2008). The parameters can be
estimated by feature-rich EM (Berg-Kirkpatrick et
al., 2010).
Because the Viterbi and inside-outside are ap-
plied to a fixed tree (i.e., the input dependency
tree), their running times are only linear in the sen-
tence length in GUSP.
3.8 Query Generation
Given a semantic parse, GUSP generates the SQL
by a depth-first traversal that recursively computes
the denotation of a node from the denotations of its
children and its node state and edge states. Each
denotation is a structured query that contains: a
list of entities for projection (corresponding to
the FROM statement in SQL); a computation tree
where the leaves are simple joins or value compar-
isons, and the internal nodes are logical or quan-
tifier operators (the WHERE statement); the salient
database elements (the SELECT statement). Be-
low, we illustrate this procedure using the seman-
tic parse in Figure 1 as a running example.
Value node state GUSP creates a semantic ob-
ject of the given type with a unique index and
the word constant. For example, the denotation
for node toronto is a city.name object with a
unique index and constant ?toronto?. The unique
index is necessary in case the SQL involves mul-
tiple instances of the same entity. For example,
the SQL in Figure 1 involves two instances of the
entity city, corresponding to the departure and
arrival cities, respectively. By default, such a se-
mantic object will be translated into an equality
constraint, such as city.name = toronto.
Entity or property node state GUSP creates a
semantic object of the given type with a unique re-
lation index. For example, the denotation for node
flight is simply a flight object with a unique in-
dex. By default, such an object will contribute to
the list of entities in SQL projection (the FROM
statement), but not any constraints.
NULL state GUSP returns an empty denotation.
Simple edge state GUSP appends the child de-
notation to that of the parent, and appends equal-
ity constraints corresponding to the relational join
path. In the case of composition, such as the join
between diego and san, GUSP simply keeps the
parent object, while adding to it the words from
the child. In the case of a more complex join,
such as that between stopping and dtw, GUSP adds
the relational constraints that join flight stop
with airport:
flight stop.stop airport = airport.airport id.
Raising edge state GUSP simply takes the child
denotation and sets that to the parent.
Implicit and sinking states GUSP maintains
two separate denotations for the two simple states
in the complex state, and processes their respec-
tive edge states accordingly. For example, the
node diego contains two denotations, one for
V:city.name, and one for E:flight, with
the corresponding child being san and stopping,
respectively.
Domain-independent states For comparator
states such as MORE or LESS, GUSP changes the
default equality constraints to an inequality one,
such as flight.depart time < 600 for before
6am. For logical connectives, GUSP combines the
projection and constraints accordingly. For quan-
tifier states, GUSP applies the given function to
the query.
Resolve scoping ambiguities GUSP delays ap-
plying quantifiers until the child semantic object
differs from the parent one or when reaching the
root. GUSP employs the following fixed ordering
in evaluating quantifiers and operators: superla-
tives and other quantifiers are evaluated at last
(i.e., after evaluating all other joins or operators
for the given object), whereas negation is evalu-
ated first, conjunctions and disjunctions are evalu-
ated in their order of appearance.
4 Experiments
4.1 Task
We evaluated GUSP on the ATIS travel planning
domain, which has been studied in He & Young
(2005, 2006) and adapted for evaluating semantic
parsing by Zettlemoyer & Collins (2007) (hence-
forth ZC07). The ZC07 dataset contains annotated
logical forms for each sentence, which we do not
use. Since our goal is not to produce a specific log-
ical form, we directly evaluate on the end-to-end
task of translating questions into database queries
and measure question-answering accuracy. The
ATIS distrbution contains the original SQL anno-
tations, which we used to compute gold answers
939
for evaluation only. The dataset is split into train-
ing, development, and test, containing 4500, 478,
and 449 sentences, respectively. We used the de-
velopment set for initial development and tuning
hyperparameters. At test time, we ran GUSP over
the test set to learn a semantic parser and output
the MAP parses.2
4.2 Preprocessing
The ATIS sentences were originally derived from
spoken dialog and were therefore in lower cases.
Since case information is important for parsers
and taggers, we first truecased the sentences us-
ing DASH (Pantel et al, 2009), which stores the
case for each phrase in Wikipedia.
We then ran the sentences through SPLAT, a
state-of-the-art NLP toolkit (Quirk et al, 2012), to
conduct tokenization, part-of-speech tagging, and
constituency parsing. Since SPLAT does not out-
put dependency trees, we ran the Stanford parser
over SPLAT parses to generate the dependency
trees in Stanford dependency (de Marneffe et al,
2006).
4.3 Systems
For the GUSP system, we set the hyperparame-
ters from initial experiments on the development
set, and used them in all subsequent experiments.
Specifically, we set ? = 50 and ? = ?0.1, and
ran three iterations of feature-rich EM with an L2
prior of 10 over the feature weights.
To evaluate the importance of complex states,
we considered two versions of GUSP : GUSP-
SIMPLE and GUSP-FULL, where GUSP-
SIMPLE only admits simple states, whereas
GUSP-FULL admits all states.
During development, we found that some
questions are inherently ambiguous that can-
not be solved except with some domain
knowledge or labeled examples. In Sec-
tion 3.2, we discuss an edge state that joins
a flight with its starting city: flight-
-flight.from airport-airport-
-airport service-city. The ATIS
database also contains another path of the same
length: flight-flight.from airport-
-airport-ground service-city. The
only difference is that air service is replaced
by ground service. In some occasions, the
2This doesn?t lead to overfitting since we did not use any
labeled information in the test set.
Table 1: Comparison of semantic parsing accu-
racy on the ATIS test dataset. Both ZC07 and
FUBL used annotated logical forms in training,
whereas GUSP-FULL and GUSP++ did not. The
numbers for GUSP-FULL and GUSP++ are end-
to-end question answering accuracy, whereas the
numbers for ZC07 and FUBL are recall on exact
match in logical forms.
Accuracy
ZC07 84.6
FUBL 82.8
GUSP-FULL 74.8
GUSP++ 83.5
answers are identical whereas in others they are
different. Without other information, neither the
complexity prior nor EM can properly discrimi-
nate one against another. (Note that this ambiguity
is not present in the ZC07 logical forms, which
use a single predicate from(f,c) for the entire
relation paths. In other words, to translate ZC07
logical forms into SQL, one also needs to decide
on which path to use.)
Another type of domain-specific ambigui-
ties involves sentences such as give me in-
formation on flights after 4pm on wednesday.
There is no obvious information to disam-
biguate between flight.departure time
and flight.arrival time for 4pm.
Such ambiguities suggest opportunities for in-
teractive learning,3 but this is clearly out of
the scope of this paper. Instead, we incor-
porated a simple disambiguation feature with a
small weight of 0.01 that fires over the sim-
ple states of flight.departure time and
airport service. We named the resulting
system GUSP++.
To gauge the difficulty of the task and the qual-
ity of lexical-trigger scores, we also considered
a deterministic baseline LEXICAL, which com-
puted semantic parses using lexical-trigger scores
alone.
3For example, after eliminating other much less likely
alternatives, the system can present to the user with both
choices and let the user to choose the correct one. The im-
plicit feedback signal can then be used to train the system for
future disambiguation.
940
Table 2: Comparison of question answering accu-
racy in ablation experiments.
Accuracy
LEXICAL 33.9
GUSP-SIMPLE 66.5
GUSP-FULL 74.8
GUSP++ 83.5
? RAISING 75.7
? SINKING 77.5
? IMPLICIT 76.2
4.4 Results
We first compared the results of GUSP-FULL and
GUSP++ with ZC07 and FUBL (Kwiatkowski et
al., 2011).4 Note that ZC07 and FUBL were eval-
uated on exact match in logical forms. We used
their recall numbers which are the percentages of
sentences with fully correct logical forms. Given
that the questions are quite specific and generally
admit nonzero number of answers, the question-
answer accuracy should be quite comparable with
these numbers.
Table 1 shows the comparison. Surprisingly,
even without the additional disambiguation fea-
ture, GUSP-FULL already attained an accuracy
broadly in range with supervised results. With the
feature, GUSP++ effectively tied with the best
supervised approach.
To evaluate the importance of various compo-
nents in GUSP, we conducted ablation test to com-
pare the variants of GUSP. Table 2 shows the re-
sults. LEXICAL can parse more than one third
of the sentences correctly, which is quite remark-
able in itself, considering that it only used the lex-
ical scores. On the other hand, roughly two-third
of the sentences cannot be correctly parsed in this
way, suggesting that the lexical scores are noisy
and ambiguous. In comparison, all GUSP variants
achieved significant gains over LEXICAL. Addi-
tionally, GUSP-FULL substantially outperformed
GUSP-SIMPLE, highlighting the challenges of
syntax-semantics mismatch in ATIS, and demon-
strating the importance and effectiveness of com-
plex states for handling such mismatch. All three
types of complex states produced significant con-
tributions. For example, compared to GUSP++,
4We should note that while the more recent system of
FUBL slightly trails ZC07, it is language-independent and
can parse questions in multiple languages.
removing RAISING dropped accuracy by almost
8 points.
4.5 Discussion
Upon manual inspection, many of the remaining
errors are due to syntactic parsing errors that are
too severe to fix. This is partly due to the fact that
ATIS sentences are out of domain compared to
the newswired text on which the syntactic parsers
were trained. For example, show, list were regu-
larly parsed as nouns, whereas round (as in round
trip) were often parsed as a verb and northwest
were parsed as an auxilliary verb. Another reason
is that ATIS sentences are typically less formal or
grammatical, which exacerbates the difficulty in
parsing. In this paper, we used the 1-best depen-
dency tree to produce semantic parse. An interest-
ing future direction is to consider joint syntactic-
semantic parsing, using k-best trees or even the
parse forest as input and reranking the top parse
using semantic information.5
5 Conclusion
This paper introduces grounded unsupervised
semantic parsing, which leverages available
database for indirect supervision and uses a
grounded meaning representation to account for
syntax-semantics mismatch in dependency-based
semantic parsing. The resulting GUSP system is
the first unsupervised approach to attain an accu-
racy comparable to the best supervised systems in
translating complex natural-language questions to
database queries.
Directions for future work include: joint
syntactic-semantic parsing, developing better fea-
tures for learning; interactive learning in a dialog
setting; generalizing distant supervision; applica-
tion to knowledge extraction from database-rich
domains such as biomedical sciences.
Acknowledgments
We would like to thank Kristina Toutanova, Chris
Quirk, Luke Zettlemoyer, and Yoav Artzi for use-
ful discussions, and Patrick Pantel and Michael
Gammon for help with the datasets.
5Note that this is still different from the currently predom-
inant approaches in semantic parsing, which learn to parse
both syntax and semantics by training from the semantic
parsing datasets alone, which are considerably smaller com-
pared to resources available for syntactic parsing.
941
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the Twentieth International Joint Con-
ference on Artificial Intelligence, pages 2670?2676,
Hyderabad, India. AAAI Press.
Taylor Berg-Kirkpatrick, John DeNero, and Dan Klein.
2010. Painless unsupervised learning with features.
In Proceedings of Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark
Johnson. 2011. Reducing grounded learning tasks
to grammatical inference. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing.
Xavier Carreras and Luis Marquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role la-
beling. In Proceedings of the Eighth Conference on
Computational Natural Language Learning, pages
89?97, Boston, MA. ACL.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language ac-
quisition. In ICML-08.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
world?s response. In Proceedings of the 2010 Con-
ference on Natural Language Learning.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation, pages 449?
454, Genoa, Italy. ELRA.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan & Claypool, San Rafael, CA.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the Forty Ninth
Annual Meeting of the Association for Computa-
tional Linguistics.
B.J. Grosz, D. Appelt, P. Martin, and F. Pereira. 1987.
Team: An experiment in the design of transportable
natural language interfaces. Artificial Intelligence,
32:173?243.
Yulan He and Steve Young. 2005. Semantic process-
ing using the hidden vector state model. In Com-
puter Speech and Language.
Yulan He and Steve Young. 2006. Spoken lan-
guage understanding using the hidden vector state
model. In Speech Communication Special Issue on
Spoken Language understanding for Conversational
Systems.
Larry Heck, Dilek Hakkani-Tur, and Gokhan Tur.
2013. Leveraging knowledge graphs for web-scale
unsupervised semantic parsing. In Proceedings of
the Interspeech 2013.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the Forty Ninth Annual Meeting of the Association
for Computational Linguistics.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages.
In Proceedings of the Twentieth National Confer-
ence on Artificial Intelligence.
Joohyun Kim and Raymond J. Mooney. 2010. Gen-
erative alignment and semantic parsing for learning
from ambiguous supervision. In COLING10.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
EMNLP-12.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in ccg grammar induction for semantic parsing.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Forty Ninth Annual Meet-
ing of the Association for Computational Linguis-
tics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Forty Seventh Annual Meeting of the Association for
Computational Linguistics.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Proceedings of the Eighth International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 311?324, Mexico City,
Mexico. Springer.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. In NIPS-08.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings of
the Twenty Second National Conference on Artificial
Intelligence, pages 913?918, Vancouver, Canada.
AAAI Press.
942
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10, Singapore. ACL.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontological induction from text. In Proceed-
ings of the Forty Eighth Annual Meeting of the As-
sociation for Computational Linguistics, pages 296?
305, Uppsala, Sweden. ACL.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In IUI-03.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
natural language interfaces to databases: Compos-
ing statistical parsing with semantic tractability. In
COLING-04.
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, and Lucy Vanderwende. 2012. MSR
SPLAT, a language analysis toolkit. In Proceedings
of NAACL HLT 2012 Demonstration Session.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the Sixteen Euro-
pean Conference on Machine Learning.
Ivan Titov and Alexandre Klementiev. 2011. A
bayesian model for unsupervised semantic parsing.
In Proceedings of the Forty Ninth Annual Meeting of
the Association for Computational Linguistics.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammers. In Proceedings of the Twenty First
Conference on Uncertainty in Artificial Intelligence,
pages 658?666, Edinburgh, Scotland. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
943
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), page 31,
Beijing, August 2010
Statistical Relational Learning for Knowledge Extraction from the Web
Hoifung Poon
University of Washington, USA
Abstract
Extracting knowledge from unstructured text has been a long-standing goal of NLP. The advent
of the Web further increases its urgency by making available billions of online documents. To
represent the acquired knowledge that is complex and heterogeneous, we need first-order logic.
To handle the inherent uncertainty and ambiguity in extracting and reasoning with knowledge,
we need probability. Combining the two has led to rapid progress in the emerging field of
statistical relational learning. In this talk, I will show that statistical relational learning of-
fers promising solutions for conquering the knowledge-extraction quest. I will present Markov
logic, which is the leading unifying framework for representing and reasoning with complex
and uncertain knowledge, and has spawned a number of successful applications for knowledge
extraction from the Web. In particular, I will present OntoUSP, an end-to-end knowledge ex-
traction system that can read text and answer questions. OntoUSP is completely unsupervised
and benefits from jointly conducting ontology induction, population, and knowledge extraction.
Experiments show that OntoUSP extracted five times as many correct answers compared to
state-of-the-art systems, with a precision of 91%.
31
