Proceedings of NAACL HLT 2009: Short Papers, pages 37?40,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Identifying Types of Claims in Online Customer Reviews
Shilpa Arora, Mahesh Joshi and Carolyn P. Rose?
Language Technologies Institute
School of Computer Science
Carnegie Mellon University, Pittsburgh PA 15213
{shilpaa,maheshj,cprose}@cs.cmu.edu
Abstract
In this paper we present a novel approach to
categorizing comments in online reviews as
either a qualified claim or a bald claim. We ar-
gue that this distinction is important based on
a study of customer behavior in making pur-
chasing decisions using online reviews. We
present results of a supervised algorithm for
learning this distinction. The two types of
claims are expressed differently in language
and we show that syntactic features capture
this difference, yielding improvement over a
bag-of-words baseline.
1 Introduction
There has been tremendous recent interest in opin-
ion mining from online product reviews and it?s ef-
fect on customer purchasing behavior. In this work,
we present a novel alternative categorization of com-
ments in online reviews as either being qualified
claims or bald claims.
Comments in a review are claims that reviewers
make about the products they purchase. A customer
reads the reviews to help him/her make a purchas-
ing decision. However, comments are often open
to interpretation. For example, a simple comment
like this camera is small is open to interpretation
until qualified by more information about whether
it is small in general (for example, based on a poll
from a collection of people), or whether it is small
compared to some other object. We call such claims
bald claims. Customers hesitate to rely on such bald
claims unless they identify (from the context or oth-
erwise) themselves to be in a situation similar to the
customer who posted the comment. The other cate-
gory of claims that are not bald are qualified claims.
Qualified claims such as it is small enough to fit
easily in a coat pocket or purse are more precise
claims as they give the reader more details, and are
less open to interpretation. Our notion of qualified
claims is similar to that proposed in the argumenta-
tion literature by Toulmin (1958). This distinction
of qualified vs. bald claims can be used to filter
out bald claims that can?t be verified. For the quali-
fied claims, the qualifier can be used in personalizing
what is presented to the reader.
The main contributions of this work are: (i) an an-
notation scheme that distinguishes qualified claims
from bald claims in online reviews, and (ii) a super-
vised machine learning approach that uses syntactic
features to learn this distinction. In the remainder
of the paper, we first motivate our work based on
a customer behavior study. We then describe the
proposed annotation scheme, followed by our su-
pervised learning approach. We conclude the paper
with a discussion of our results.
2 Customer Behavior Study
In order to study how online product reviews are
used to make purchasing decisions, we conducted
a user study. The study involved 16 pair of gradu-
ate students. In each pair there was a customer and
an observer. The goal of the customer was to de-
cide which camera he/she would purchase using a
camera review blog1 to inform his/her decision. As
the customer read through the reviews, he/she was
1http://www.retrevo.com/s/camera
37
asked to think aloud and the observer recorded their
observations.
The website used for this study had two types of
reviews: expert and user reviews. There were mixed
opinions about which type of reviews people wanted
to read. About six customers could relate more with
user reviews as they felt expert reviews were more
like a ?sales pitch?. On the other hand, about five
people were interested in only expert reviews as they
believed them to be more practical and well rea-
soned.
From this study, it was clear that the customers
were sensitive to whether a claim was qualified or
not. About 50% of the customers were concerned
about the reliability of the comments and whether
it applied to them. Half of them felt it was hard
to comprehend whether the user criticizing a feature
was doing so out of personal bias or if it represented
a real concern applicable to everyone. The other half
liked to see comments backed up with facts or ex-
planations, to judge if the claim could be qualified.
Two customers expressed interest in comments from
users similar to themselves as they felt they could
base their decision on such comments more reli-
ably. Also, exaggerations in reviews were deemed
untrustworthy by at least three customers.
3 Annotation Scheme
We now present the guidelines we used to distin-
guish bald claims from qualified claims. A claim
is called qualified if its validity or scope is limited
by making the conditions of its applicability more
explicit. It could be either a fact or a statement that
is well-defined and attributed to some source. For
example, the following comments from our data are
qualified claims according to our definition,
1. The camera comes with a lexar 16mb starter
card, which stores about 10 images in fine mode
at the highest resolution.
2. I sent my camera to nikon for servicing, took
them a whole 6 weeks to diagnose the problem.
3. I find this to be a great feature.
The first example is a fact about the camera. The
second example is a report of an event. The third
example is a self-attributed opinion of the reviewer.
Bald claims on the other hand are non-factual
claims that are open to interpretation and thus cannot
be verified. A straightforward example of the dis-
tinction between a bald claim and a qualified claim
is a comment like the new flavor of peanut butter is
being well appreciated vs. from a survey conducted
among 20 people, 80% of the people liked the new
flavor of peanut butter. We now present some exam-
ples of bald claims. A more detailed explanation is
provided in the annotation manual2:
? Not quantifiable gradable3 words such as
good, better, best etc. usually make a claim
bald, as there is no qualified definition of being
good or better.
? Quantifiable gradable words such as small,
hot etc. make a claim bald when used without
any frame of reference. For example, a com-
ment this desk is small is a bald claim whereas
this desk is smaller than what I had earlier is a
qualified claim, since the comparative smaller
can be verified by observation or actual mea-
surement, but whether something is small in
general is open to interpretation.
? Unattributed opinion or belief: A comment
that implicitly expresses an opinion or belief
without qualifying it with an explicit attribu-
tion is a bald claim. For example, Expectation
is that camera automatically figures out when
to use the flash.
? Exaggerations: Exaggerations such as on ev-
ery visit, the food has blown us away do not
have a well defined scope and hence are not
well qualified.
The two categories for gradable words defined above
are similar to what Chen (2008) describes as vague-
ness, non-objective measurability and imprecision.
4 Related work
Initial work by Hu and Liu (2004) on the product
review data that we have used in this paper focuses
on the task of opinion mining. They propose an ap-
proach to summarize product reviews by identifying
opinionated statements about the features of a prod-
uct. In our annotation scheme however, we classify
2www.cs.cmu.edu/?shilpaa/datasets/
opinion-claims/qbclaims-manual-v1.0.pdf
3http://en.wikipedia.org/wiki/English_
grammar#Semantic_gradability
38
all claims in a review, not restricting to comments
with feature mentions alone.
Our task is related to opinion mining, but with a
specific focus on categorizing statements as either
bald claims that are open to interpretation and may
not apply to a wide customer base, versus qualified
claims that limit their scope by making some as-
sumptions explicit. Research in analyzing subjec-
tivity of text by Wiebe et al (2005) involves identi-
fying expression of private states that cannot be ob-
jectively verified (and are therefore open to interpre-
tation). However, our task differs from subjectivity
analysis, since both bald as well as qualified claims
can involve subjective language. Specifically, objec-
tive statements are always categorized as qualified
claims, but subjective statements can be either bald
or qualified claims. Work by Kim and Hovy (2006)
involves extracting pros and cons from customer re-
views and as in the case of our task, these pros and
cons can be either subjective or objective.
In supervised machine learning approaches to
opinion mining, the results using longer n-grams and
syntactic knowledge as features have been both pos-
itive as well as negative (Gamon, 2004; Dave et al,
2003). In our work, we show that the qualified vs.
bald claims distinction can benefit from using syn-
tactic features.
5 Data and Annotation Procedure
We applied our annotation scheme to the product re-
view dataset4 released by Hu and Liu (2004). We
annotated the data for 3 out of 5 products. Each
comment in the review is evaluated as being quali-
fied or bald claim. The data has been made available
for research purposes5.
The data was completely double coded such that
each review comment received a code from the two
annotators. For a total of 1, 252 review comments,
the Cohen?s kappa (Cohen, 1960) agreement was
0.465. On a separate dataset (365 review com-
ments)6, we evaluated our agreement after remov-
ing the borderline cases (only about 14%) and there
4http://www.cs.uic.edu/?liub/FBS/
CustomerReviewData.zip
5www.cs.cmu.edu/?shilpaa/datasets/
opinion-claims/qbclaims-v1.0.tar.gz
6These are also from the Hu and Liu (2004) dataset, but not
included in our dataset yet.
was a statistically significant improvement in kappa
to 0.532. Since the agreement was low, we resolved
our conflict by consensus coding on the data that was
used for supervised learning experiments.
6 Experiments and Results
For our supervised machine learning experiments on
automatic classification of comments as qualified or
bald, we used the Support Vector Machine classifier
in the MinorThird toolkit (Cohen, 2004) with the de-
fault linear kernel. We report average classification
accuracy and average Cohen?s Kappa using 10-fold
cross-validation.
6.1 Features
We experimented with several different features in-
cluding standard lexical features such as word uni-
grams and bigrams; pseudo-syntactic features such
as Part-of-Speech bigrams and syntactic features
such as dependency triples7. Finally, we also used
syntactic scope relationships computed using the de-
pendency triples. Use of features based on syntactic
scope is motivated by the difference in how quali-
fied and bald claims are expressed in language. We
expect these features to capture the presence or ab-
sence of qualifiers for a stated claim. For example,
?I didn?t like this camera, but I suspect it will be a
great camera for first timers.? is a qualified claim,
whereas a comment like ?It will be a great camera
for first timers.? is not a qualified claim. Analysis of
the syntactic parse of the two comments shows that
in the first comment the word ?great? is in the scope
of ?suspect?, whereas this is not the case for the sec-
ond comment. We believe such distinctions can be
helpful for our task.
We compute an approximation to the syntactic
scope using dependency parse relations. Given
the set of dependency relations of the form
relation, headWord, dependentWord, such as
AMOD,camera,great, an in-scope feature is de-
fined as INSCOPE headWord dependentWord (IN-
SCOPE camera great). We then compute a tran-
sitive closure of such in-scope features, similar to
Bikel and Castelli (2008). For each in-scope feature
in the entire training fold, we also create a corre-
7We use the Stanford Part-of-Speech tagger and parser re-
spectively.
39
Features QBCLAIM HL-OP
Majority .694(.000) .531(.000)
Unigrams .706(.310) .683(.359)
+Bigrams .709(.321) .693(.378)
+POS-Bigrams .726*(.353*) .683(.361)
+Dep-Triples .711(.337) .692(.376)
+In-scope .706(.340) .688(.367)
+Not-in-scope .726(.360*) .687(.370)
+All-scope .721(.348) .699(.396)
Table 1: The table shows accuracy (& Cohen?s kappa in paren-
theses) averaged across ten folds. Each feature set is individ-
ually added to the baseline set of unigram features. * - Re-
sult is marginally significantly better than unigrams-only (p <
0.10, using a two-sided pairwise T-test). HL-OP - Opinion an-
notations from Hu and Liu (2004). QBCLAIM - Qualified/Bald
Claim.
sponding not-in-scope feature which triggers when
either (i) the dependent word appears in a comment,
but not in the transitive-closured scope of the head
word, or (ii) the head word is not contained in the
comment but the dependent word is present.
We evaluate the benefit of each type of feature
by adding them individually to the baseline set of
unigram features. Table 1 presents the results. We
use the majority classifier and unigrams-only perfor-
mance as our baselines. We also experimented with
using the same feature combinations to learn the
opinion category as defined by Hu and Liu (2004)
[HL-OP] on the same subset of data.
It can be seen from Table 1 that using purely
unigram features, the accuracy obtained is not
any better than the majority classifier for quali-
fied vs. bald distinction. However, the Part-of-
Speech bigram features and the not-in-scope fea-
tures achieve a marginally significant improvement
over the unigrams-only baseline.
For the opinion dimension from Hu and Liu
(2004), there was no significant improvement from
the type of syntactic features we experimented with.
Hu and Liu (2004)?s opinion category covers several
different types of opinions and hence finer linguis-
tic distinctions that help in distinguishing qualified
claims from bald claims may not apply in that case.
7 Conclusions
In this work, we presented a novel approach to re-
view mining by treating comments in reviews as
claims that are either qualified or bald. We argued
with examples and results from a user study as to
why this distinction is important. We also proposed
and motivated the use of syntactic scope as an ad-
ditional type of syntactic feature, apart from those
already used in opinion mining literature. Our eval-
uation demonstrates a marginally significant posi-
tive effect of a feature space that includes these and
other syntactic features over the purely unigram-
based feature space.
Acknowledgments
We would like to thank Dr. Eric Nyberg for the
helpful discussions and the user interface for doing
the annotations. We would also like to thank all the
anonymous reviewers for their helpful comments.
References
Daniel Bikel and Vittorio Castelli. Event Matching Using
the Transitive Closure of Dependency Relations. Pro-
ceedings of ACL-08: HLT, Short Papers, pp. 145?148.
Wei Chen. 2008. Dimensions of Subjectivity in Natural
Language. In Proceedings of ACL-HLT?08. Colum-
bus Ohio.
Jacob Cohen. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Measure-
ment, Vol. 20, No. 1., pp. 37-46.
William Cohen. 2004. Minorthird: Methods for Iden-
tifying Names and Ontological Relations in Text us-
ing Heuristics for Inducing Regularities from Data.
http://minorthird.sourceforge.net/
Kushal Dave, Steve Lawrence and David M. Pennock
2006. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
In Proc of WWW?03.
Michael Gamon. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and
the role of linguistic analysis. Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
Minqing Hu and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proc. of the ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining.
Soo-Min Kim and Eduard Hovy. 2006. Automatic Iden-
tification of Pro and Con Reasons in Online Reviews.
In Proc. of the COLING/ACL Main Conference Poster
Sessions.
Stephen Toulmin 1958 The Uses of Argument. Cam-
bridge University Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, vol-
ume 39, issue 2-3, pp. 165-210.
40
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 73?76,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Feature Based Approach to Leveraging Context for Classifying 
Newsgroup Style Discussion Segments 
Yi-Chia Wang, Mahesh Joshi 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{yichiaw,maheshj}@cs.cmu.edu 
Carolyn Penstein Ros? 
Language Technologies Institute/  
Human-Computer Interaction Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
cprose@cs.cmu.edu 
 
 
Abstract 
On a multi-dimensional text categorization 
task, we compare the effectiveness of a fea-
ture based approach with the use of a state-
of-the-art sequential learning technique that 
has proven successful for tasks such as 
?email act classification?.  Our evaluation 
demonstrates for the three separate dimen-
sions of a well established annotation 
scheme that novel thread based features 
have a greater and more consistent impact 
on classification performance.  
1 Introduction 
The problem of information overload in personal 
communication media such as email, instant mes-
saging, and on-line discussion boards is a well 
documented phenomenon (Bellotti, 2005).  Be-
cause of this, conversation summarization is an 
area with a great potential impact (Zechner, 2001). 
What is strikingly different about this form of 
summarization from summarization of expository 
text is that the summary may include more than 
just the content, such as the style and structure of 
the conversation (Roman et al, 2006).  In this pa-
per we focus on a classification task that will even-
tually be used to enable this form of conversation 
summarization by providing indicators of the qual-
ity of group functioning and argumentation. 
Lacson and colleagues (2006) describe a form of 
conversation summarization where a classification 
approach is first applied to segments of a conversa-
tion in order to identify regions of the conversation 
related to different types of information.  This aids 
in structuring a useful summary.  In this paper, we 
describe work in progress towards a different form 
of conversation summarization that similarly lev-
erages a text classification approach.  We focus on 
newsgroup style interactions.  The goal of assess-
ing the quality of interactions in that context is to 
enable the quality and nature of discussions that 
occur within an on-line discussion board to be 
communicated in a summary to a potential new-
comer or group moderators.   
We propose to adopt an approach developed in 
the computer supported collaborative learning 
(CSCL) community for measuring the quality of 
interactions in a threaded, online discussion forum 
using a multi-dimensional annotation scheme 
(Weinberger & Fischer, 2006).  Using this annota-
tion scheme, messages are segmented into idea 
units and then coded with several independent di-
mensions, three of which are relevant for our work, 
namely micro-argumentation, macro-
argumentation, and social modes of co-
construction, which categorizes spans of text as 
belonging to one of five consensus building cate-
gories.  By coding segments with this annotation 
scheme, it is possible to measure the extent to 
which group members? arguments are well formed 
or the extent to which they are engaging in func-
tional or dysfunctional consensus building behav-
ior. 
This work can be seen as analogous to work on 
?email act classification? (Carvalho & Cohen, 
2005).  However, while in some ways the structure 
of newsgroup style interaction is more straightfor-
ward than email based interaction because of the 
unambiguous thread structure (Carvalho & Cohen, 
2005), what makes this particularly challenging 
73
from a technical standpoint is that the structure of 
this type of conversation is multi-leveled, as we 
describe in greater depth below.   
We investigate the use of state-of-the-art se-
quential learning techniques that have proven suc-
cessful for email act classification in comparison 
with a feature based approach.  Our evaluation 
demonstrates for the three separate dimensions of a 
context oriented annotation scheme that novel 
thread based features have a greater and more con-
sistent impact on classification performance.  
2 Data and Coding 
We make use of an available annotated corpus of 
discussion data where groups of three students dis-
cuss case studies in an on-line, newsgroup style 
discussion environment (Weinberger & Fischer, 
2006).  This corpus is structurally more complex 
than the data sets used previously to demonstrate 
the advantages of using sequential learning tech-
niques for identifying email acts (Carvalho & 
Cohen, 2005).  In the email act corpus, each mes-
sage as a whole is assigned one or more codes.  
Thus, the history of a span of text is defined in 
terms of the thread structure of an email conversa-
tion. However, in the Weinberger and Fischer cor-
pus, each message is segmented into idea units.  
Thus, a span of text has a context within a message, 
defined by the sequence of text spans within that 
message, as well as a context from the larger 
thread structure.  
The Weinberger and Fischer annotation scheme 
has seven dimensions, three of which are relevant 
for our work.  
1. Micro-level of argumentation [4 categories] 
How an individual argument consists of a 
claim which can be supported by a ground 
with warrant and/or specified by a qualifier  
2. Macro-level of argumentation [6 categories] 
Argumentation sequences are examined in 
terms of how learners connect individual ar-
guments to create a more complex argument 
(for example, consisting of an argument, a 
counter-argument, and integration)  
3. Social Modes of Co-Construction [6 catego-
ries] To what degree or in what ways learn-
ers refer to the contributions of their learn-
ing partners, including externalizations, 
elicitations, quick consensus building, inte-
gration oriented consensus building, or con-
flict oriented consensus building, or other. 
For the two argumentation dimensions, the most 
natural application of sequential learning tech-
niques is by defining the history of a span of text in 
terms of the sequence of spans of text within a 
message, since although arguments may build on 
previous messages, there is also a structure to the 
argument within a single message.  For the Social 
Modes of Co-construction dimension, it is less 
clear.  However, we have experimented with both 
ways of defining the history and have not observed 
any benefit of sequential learning techniques by 
defining the history for sequential learning in terms 
of previous messages.  Thus, for all three dimen-
sions, we report results for histories defined within 
a single message in our evaluation below. 
3 Feature Based Approach 
In previous text classification research, more atten-
tion to the selection of predictive features has been 
done for text classification problems where very 
subtle distinctions must be made or where the size 
of spans of text being classified is relatively small.  
Both of these are true of our work. For the base 
features, we began with typical text features ex-
tracted from the raw text, including unstemmed uni-
grams and punctuation.  We did not remove stop 
words, although we did remove features that occured 
less than 5 times in the corpus.  We also included a 
feature that indicated the number of words in the 
segment. 
 
Thread Structure Features. The simplest context-
oriented feature we can add based on the threaded 
structure is a number indicating the depth in the 
thread where a message appears.  We refer to this 
feature as deep.  This is expected to improve per-
formance to the extent that thread initial messages 
may be rhetorically distinct from messages that 
occur further down in the thread.  The other con-
text oriented feature related to the thread structure 
is derived from relationships between spans of text 
appearing in the parent and child messages.  This 
feature is meant to indicate how semantically re-
lated a span of text is to the spans of text in the 
parent message.  This is computed using the mini-
mum of all cosine distance measures between the 
vector representation of the span of text and that of 
each of the spans of text in all parent messages, 
74
which is a typical shallow measure of semantic 
similarity.  The smallest such distance measure is 
included as a feature indicating how related the 
current span of text is to a parent message.  
 
Sequence-Oriented Features. We hypothesized that 
the sequence of codes within a message follows a 
semi-regular structure.  In particular, the discussion 
environment used to collect the Weinberger and 
Fischer corpus inserts prompts into the message 
buffers before messages are composed in order to 
structure the interaction.  Users fill in text under-
neath these prompts.  Sometimes they quote mate-
rial from a previous message before inserting their 
own comments.  We hypothesized that whether or 
not a piece of quoted material appears before a 
span of text might influence which code is appro-
priate.  Thus, we constructed the fsm feature, 
which indicates the state of a simple finite-state 
automaton that only has two states. The automaton 
is set to initial state (q0) at the top of a message. It 
makes a transition to state (q1) when it encounters a 
quoted span of text.  Once in state (q1), the automa-
ton remains in this state until it encounters a 
prompt. On encountering a prompt it makes a tran-
sition back to the initial state (q0).  The purpose is 
to indicate places where users are likely to make a 
comment in reference to something another par-
ticipant in the conversation has already contributed. 
4 Evaluation 
The purpose of our evaluation is to contrast our 
proposed feature based approach with a state-of-
the-art sequential learning technique (Collins, 
2002).  Both approaches are designed to leverage 
context for the purpose of increasing classification 
accuracy on a classification task where the codes 
refer to the role a span of text plays in context.   
We evaluate these two approaches alone and in 
combination over the same data but with three dif-
ferent sets of codes, namely the three relevant di-
mensions of the Weinberger and Fischer annota-
tion scheme.  In all cases, we employ a 10-fold 
cross-validation methodology, where we apply a 
feature selection wrapper in such as way as to se-
lect the 100 best features over the training set on 
each fold, and then to apply this feature space and 
the trained model to the test set.  The complete 
corpus comprises about 250 discussions of the par-
ticipants.  From this we have run our experiments 
with a subset of this data, using altogether 1250 
annotated text segments. Trained coders catego-
rized each segment using this multi-dimensional 
annotation scheme, in each case achieving a level 
of agreement exceeding .7 Kappa both for segmen-
tation and coding of all dimensions as previously 
published (Weinberger & Fischer, 2006). 
For each dimension, we first evaluate alternative 
combinations of features using SMO, Weka?s im-
plementation of Support Vector Machines (Witten 
& Frank, 2005).  For a sequential learning algo-
rithm, we make use of the Collins Perceptron 
Learner (Collins, 2002).  When using the Collins 
Perceptron Learner, in all cases we evaluate com-
binations of alternative history sizes (0 and 1) and 
alternative feature sets (base and base+AllContext).  
In our experimentation we have evaluated larger 
history sizes as well, but the performance was con-
sistently worse as the history size grew larger than 
1. Thus, we only report results for history sizes of 
0 and 1. 
Our evaluation demonstrates that we achieve a 
much greater impact on performance with carefully 
designed, automatically extractable context ori-
ented features.  In all cases we are able to achieve a 
statistically significant improvement by adding 
context oriented features, and only achieve a statis-
tically significant improvement using sequential 
learning for one dimension, and only in the ab-
sence of context oriented features. 
4.1 Feature Based Approach 
0.61
0.71
0.52
0.62
0.73
0.67
0.61
0.70
0.66
0.61
0.73
0.69
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Social Macro Micro
Dimension
Ka
pp
a 
fro
m
 
10
-
fo
ld
 
CV
Base Base+Thread Base+Seq Base+AllContext
 
Figure 1. Results with alternative features 
sets 
 
75
We first evaluated the feature based approach 
across all three dimensions and demonstrate that 
statistically significant improvements are achieved 
on all dimensions by adding context oriented fea-
tures.  The most dramatic results are achieved on 
the Social Modes of Co-Construction dimension 
(See Figure 1). All pairwise contrasts between al-
ternative feature sets within this dimension are sta-
tistically significant.  In the other dimensions, 
while Base+Thread is a significant improvement 
over Base, there is no significant difference be-
tween Base+Thread and Base+AllContext.   
4.2 Sequential Learning 
0.54
0.63
0.43
0.56
0.64
0.52
0.56
0.63
0.59
0.56
0.65
0.61
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Social Macro Micro
Dimension
Ka
pp
a 
fro
m
 
10
-
fo
ld
 
CV
Base / 0 Base /  1 Base+AllContext / 0 Base+AllContext / 1
 
Figure 2. Results with Sequential Learning 
 
The results for sequential learning are weaker than 
for the feature based (See Figure 2). While the 
Collins Perceptron learner possesses the capability 
of modeling sequential dependencies between 
codes, which SMO does not possess, it is not nec-
essarily a more powerful learner.  On this data set, 
the Collins Perceptron learner consistently per-
forms worse that SMO.  Even restricting our 
evaluation of sequential learning to a comparison 
between the Collins Perceptron learner with a his-
tory of 0 (i.e., no history) with the same learner 
using a history of 1, we only see a statistically sig-
nificant improvement on the Social Modes of Co-
Construction dimension.  This is when only using 
base features, although the trend was consistently 
in favor of a history of 1 over 0. Note that the stan-
dard deviation in the performance across folds was 
much higher with the Collins Perceptron learner, 
so that a much greater difference in average would 
be required in order to achieve statistical signifi-
cance.  Performance over a validation set was al-
ways worse with larger history sizes than 1.   
5 Conclusions  
We have described work towards an approach to 
conversation summarization where an assessment 
of conversational quality along multiple process 
dimensions is reported.  We make use of a well-
established annotation scheme developed in the 
CSCL community.  Our evaluation demonstrates 
that thread based features have a greater and more 
consistent impact on performance with this data. 
 
This work was supported by the National Sci-
ence Foundation grant number SBE0354420, and 
Office of Naval Research, Cognitive and Neural Sci-
ences Division Grant N00014-05-1-0043. 
References 
Bellotti, V., Ducheneaut, N., Howard, M. Smith, I., 
Grinter, R. (2005). Quality versus Quantity: Email-
centric task management and its relation with over-
load. Human-Computer Interaction, 2005, vol. 20 
Carvalho, V. & Cohen, W. (2005). On the Collective 
Classification of Email ?Speech Acts?, Proceedings 
of SIGIR ?2005. 
Collins, M (2002). Discriminative Training Methods for 
Hidden Markov Models: Theory and Experiments 
with Perceptron Algorithms. In Proceedings of 
EMNLP 2002.  
Lacson, R., Barzilay, R., & Long, W. (2006). Automatic 
analysis of medical dialogue in the homehemodialy-
sis domain: structure induction and summarization, 
Journal of Biomedical Informatics 39(5), pp541-555. 
Roman, N., Piwek, P., & Carvalho, A. (2006).  Polite-
ness and Bias in Dialogue Summarization : Two Ex-
ploratory Studies, in J. Shanahan, Y. Qu, & J. Wiebe 
(Eds.) Computing Attitude and Affect in Text: Theory 
and Applications, the Information Retrieval Series.   
Weinberger, A., & Fischer, F. (2006). A framework to 
analyze argumentative knowledge construction in 
computer-supported collaborative learning. Com-
puters & Education, 46, 71-95. 
Witten, I. H. & Frank, E. (2005).  Data Mining: Practi-
cal Machine Learning Tools and Techniques, sec-
ond edition, Elsevier: San Francisco. 
Zechner, K. (2001). Automatic Generation of Concise 
Summaries of Spoken Dialogues in Unrestricted 
Domains. Proceedings of ACM SIG-IR 2001. 
76
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 24?27,
Columbus, June 2008. c?2008 Association for Computational Linguistics
SIDE: The Summarization Integrated Development Environment 
 
Moonyoung Kang, Sourish Chaudhuri, Mahesh Joshi, Carolyn P. Ros? 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213 USA 
moonyoun,schaudhu,maheshj,cprose@cs.cmu.edu 
 
Abstract 
In this type-II demo, we introduce SIDE1 (the 
Summarization Integrated Development Envi-
ronment), an infrastructure that facilitates 
construction of summaries tailored to the 
needs of the user. It aims to address the issue 
that there is no such thing as the perfect sum-
mary for all purposes. Rather, the quality of a 
summary is subjective, task dependent, and 
possibly specific to a user. The SIDE frame-
work allows users flexibility in determining 
what they find more useful in a summary, 
both in terms of structure and content. As an 
educational tool, it has been successfully user 
tested by a class of 21 students in a graduate 
course on Summarization and Personal Infor-
mation Management. 
1 Introduction 
A wide range of summarization systems have 
been developed in the past 40 years, beginning 
with early work in the Library sciences field. To 
this day, a great deal of research in summarization 
focuses on alternative methods for selecting sub-
sets of text segments based on a variety of forms of 
rhetorical analysis and relevance rankings.  Never-
theless, while there is much in common between 
approaches used for summarization in a variety of 
contexts, each new summarization project tends to 
include a new system development effort, because 
a general purpose, extensible framework for sum-
                                                            
1
 The working system can be downloaded from 
http://www.cs.cmu.edu/~cprose/SIDE.html, and a video 
of an example of SIDE use can be found at 
http://ankara.lti.cs.cmu.edu/side/video.swf. 
This project is supported by ONR Cognitive and Neural 
Sciences Division, Grant number N000140510043 
 
 
marization has not been made available. As an ex-
ample, Teufel and Moens? (2002) argue that the 
summarization strategy for scientific articles must 
be different from news articles because the former 
focus on novelty of information, are much longer 
and very different in structure. 
A large proportion of summarization systems do 
not allow users to intervene in the summarization 
process so that the form of the summary could be 
tailored to the individual user?s needs (Mieskes, M., 
M?ller, C., & Strube, M., 2007). From the same 
document, many summaries can potentially be 
generated, and the most preferable one for one user 
will not, in general, be the same as what is pre-
ferred by a different user. The fact that users with 
similar backgrounds can have vastly differing in-
formation needs is highlighted by Paice and Jones? 
(1993) study where an informal sentence selection 
experiment had to be abandoned because the par-
ticipants, who were agriculture experts, were too 
influenced by their research interests to agree with 
each other. However, summarization systems tend 
to appear as black boxes from the user?s perspec-
tive and the users cannot specify what they would 
want in the summary.  
SIDE is motivated by the two scenarios men-
tioned above - the absence of a common tool for 
generating summaries from different contexts, as 
well as the fact that different users might have dif-
ferent information needs from the same document. 
Bellotti (2005) discusses the problem of informa-
tion overload in communication media such as e-
mail and online discussion boards. The rapid 
growth of weblogs, wikis and dedicated informa-
tion sources makes the problem of information 
overload more acute. It also means that summari-
zation systems have the responsibility of taking 
into account the kind of information that its user 
would be interested in. 
With SIDE, we attempt to give the user a greater 
say in deciding what kind of information and how 
much of it the user wants as part of his summary.  
24
In the following sections, we elaborate on the 
features of SIDE and its technical details.   
2 Functionality 
The design of SIDE is aimed at allowing the user 
as much involvement at every stage of the sum-
mary generation process as the user wishes. SIDE 
allows the user to select a set of documents to train 
the system upon, and to decide what aspects of 
input documents should be detected and used for 
making choices, particularly at the stage of select-
ing a subset of segments to preserve from the 
source documents. The other key feature of the 
development environment is that it allows devel-
opers to plug in custom modules using the Plugin 
Manager in the GUI. In this way, advanced users 
can extend the capabilities of SIDE for meeting 
their specific needs while still taking advantage of 
the existing, general purpose aspects of SIDE. 
     The subsequent sub-sections discuss individual 
parts of system behavior in greater detail at a con-
ceptual level. Screen shots and more step by step 
discussion of how to use the GUI are given with 
the case study that outlines the demo script. 
2.1 Filters 
To train the system and create a model, the user 
has to define a filter. Defining a filter has 4 steps ? 
creating annotated files with user-defined annota-
tions, choosing feature sets to train (unigrams, bi-
grams etc), choosing evaluation metrics (Word 
Token Counter, TF-IDF) and choosing a classifier 
to train the system. 
   Annotating Files: The GUI allows the user to 
create a set of unstructured documents. The user 
can create folders and import sets of documents or 
individual documents. The GUI allows the user to 
view the documents in their original form; alterna-
tively, the user can add it to the filter and segment 
it by sentence, paragraph, or by own definition. 
The user can define a set of annotations for each 
filter, and use those to annotate segments of the file. 
The system has sentence and paragraph segmenters 
built into it. The user can also define a segmenter 
and plug it in. 
   Feature Sets: The feature set panel allows the 
user to decide which features the user wants to use 
in training the model. It is built on top of TagHel-
per Tools (Donmez et al, 2005) and uses it to ex-
tract the features chosen by the user. The system 
has options for using unigrams, bigrams, Part-Of-
Speech bigrams and punctuation built into it, and 
the user can specify whether they wish to apply 
stemming and/or stop word removal. Like the 
segmenters, if the user wants to use a specific fea-
ture to train, the user can plug in the feature extrac-
tor for the same through the GUI. 
   Evaluation Metrics: The evaluation metric de-
cides how to order the sentences that are chosen to 
be part of the summary. In keeping with the plug-
in architecture of the system, the user can define 
own metric and plug it into the system using the 
Plugin Manager. 
   Classifier: The user can decide which classifier 
to train the model with. This functionality is built 
on top of TagHelper Tools, which uses the Weka 
toolkit (Witten & Frank, 2005) to give users a set 
of classifiers to choose from. Once the system has 
been trained, the user can see the training results in 
a panel which provides a performance summary - 
including the kappa scores computed through 10-
fold cross validation and the confusion matrix, the 
sets of features extracted from the text, and the 
settings that were used for training the model. 
    The user can choose the model for classifying 
segments in the target document. The user also can 
plug-in a machine learning algorithm to the system 
if necessary. 
2.2 Summaries 
Summaries are defined by Recipes that specify 
what types of segments should be included in the 
resulting summary, and how a subset of the ones 
that meet those requirements should be selected 
and then arranged. Earlier we discussed how filters 
are defined.  One or more filters can be applied to a 
text so that each segment has one or more labels.  
These labels can then be used to index into a text. 
For example, a Recipe might specify using a logi-
cal expression such that only a subset of segments 
whose labels meet some specified set of constraints 
should be selected. The selected subset is then op-
tionally ranked using a specified Evaluation metric. 
Finally, from this ranked list, some number or 
some percentage of segments will then finally be 
selected to be included in the resulting summary.  
The segments are then optionally re-ordered to the 
original document order before including them in 
the summary, which is then displayed to the user. 
25
3 Case Study  
The following subsections describe an example 
where the user starts with some unstructured doc-
uments and uses the system to generate a specifica-
tion for a summary, which can then be applied to 
other similar documents. 
    We illustrate a script outline of our demo pres-
entation. The demo shows how simple it is to move 
through the steps of configuring SIDE for a type of 
summary that a user would like to be able to gen-
erate.  In order to demonstrate this, we will lead the 
user through an annotation task where we assign 
dialogue acts to turns in some tutoring dialogues.  
From this annotated data, we can generate summa-
ries that pull out key actions of particular types.  
For example, perhaps we would like to look at all 
the instructions that the tutor has given to a student 
or all the questions the student has asked the tutor.  
The summarizing process consists of annotating 
training documents to define filters, deciding 
which features to use along with what machine 
learning algorithm to train the filters, training the 
actual filters, defining a summary in terms of the 
structured annotation that is accomplished by the 
defined filters, and finally, summarizing target files 
using the resulting configuration. The purpose of 
SIDE is to provide both an easy GUI interface for 
people who are not familiar with programming, 
and extensible, plug-and-play code for those who 
want to program and change SIDE into a more so-
phisticated and specialized type of summarizer. 
The demo will provide options for both novice us-
ers primarily interested in working with SIDE 
through its GUI interface and for more experienced 
users who would like to work with the code.   
3.1 Using the GUI 
The summarization process begins with loading 
unstructured training and testing documents. Next, 
filters are defined by adding training documents, 
segmenting each by choosing an automatic seg-
menter, and assigning annotations to the segments. 
   After a document is segmented, the segments are 
annotated with labels that classify segments using 
a user-defined coding scheme (Figure 1). Unanno-
tated segments are later ignored during the training 
phase. Next, a set of feature types, such as uni-
grams, bigrams, part of speech bigrams, etc., are 
selected, which together will be used to build the 
feature space that will be input to a selected ma-
chine learning algorithms, or ensemble of algo-
rithms. In this example, ?Punctuation? Feature 
Class Extractor, which can distinguish interroga-
tive sentence, is selected and for ?Evaluation Met-
rics?, ?Word Token Counter? is selected. Now, we 
train this model with an appropriate machine learn-
ing algorithm. In this example, J48 which is
 
 
Figure 1: The interface where segments are annotated. 
26
Boolean
Expression
Tree
Ranker
Limiter
 
Figure 2: The interface for defining how to build a summary from the annotated data. 
 
one of Weka?s (Witten & Frank, 2005) decision 
tree learners is chosen as the learning algorithm. 
Users can explore different ensembles of machine 
learning algorithms, compare performance over the 
training data using cross-validation, and select the 
best performing one to use for summarization. 
    Once one or more filters have been defined, we 
must define how summaries are built from the 
structured representation that is built by the filters.  
Figure 2 shows the main interface for doing this.  
Recipes consist of four parts, namely ?Selecting?, 
?Ranking?, ?Limiting?, ?Sequencing?. Selection is 
done using a boolean expression tree consisting of 
?and?, ?or?, and ?is? nodes. By doing selection, only 
those segments with proper annotations will be 
selected for inclusion in the resulting summary. 
Ranking is done by the Evaluation Metric selected 
when defining the Recipe. The size of a summary 
can be limited by limiting the number of segments 
you want in your summary. Finally, the summary 
can be reordered as you wish and displayed. 
4 Current Directions 
Currently, most of the functionality in SIDE fo-
cuses on the content selection problem.  We ac-
knowledge that to move beyond extractive forms 
of summarization, additional functionality at the 
summary generation stage is necessary.  Our cur-
rent work focuses on addressing these issues. 
References 
Bellotti, V., Ducheneaut, N., Howard, M., Smith, I., & 
Grinter, R. (2005). Quality versus Quantity: E-Mail 
Centric Task Management and Its Relation with 
Overload, Human-Computer Interaction, Volume 20,  
Donmez, P., Ros?, C. P., Stegmann, K., Weinberger, A., 
and Fischer, F. (2005). Supporting CSCL with Auto-
matic Corpus Analysis Technology , Proceedings of 
Computer Supported Collaborative Learning. 
Mieskes, M., M?ller, C., & Strube, M. (2007) Improv-
ing extractive dialogue summarization by utilizing 
human feedback, Proceedings of the 25th IASTED 
International Multi-Conference: artificial intelligence 
and applications, p.627-632 
Paice, Chris D. & Jones, Paul A. (1993) The identifica-
tion of important concepts in highly structured tech-
nical papers. In Proceedings of the 16th ACM-SIGIR 
Conference, pages 69?78 
Teufel, S. & Moens, M. (2002). Summarizing Scientific 
Articles: Experiments with Relevance and Rhetorical 
Status, Computational Linguistics, Vol 28, No. 1. 
Witten, Ian H.; Frank, Eibe (2005). Data Mining: Prac-
tical machine learning tools and techniques, 2nd Edi-
tion. Morgan Kaufmann, San Francisco.  
27
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 313?316,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Generalizing Dependency Features for Opinion Mining
Mahesh Joshi1 and Carolyn Penstein-Rose?1,2
1Language Technologies Institute
2Human-Computer Interaction Institute
Carnegie Mellon University, Pittsburgh, PA, USA
{maheshj,cprose}@cs.cmu.edu
Abstract
We explore how features based on syntac-
tic dependency relations can be utilized to
improve performance on opinion mining.
Using a transformation of dependency re-
lation triples, we convert them into ?com-
posite back-off features? that generalize
better than the regular lexicalized depen-
dency relation features. Experiments com-
paring our approach with several other ap-
proaches that generalize dependency fea-
tures or ngrams demonstrate the utility of
composite back-off features.
1 Introduction
Online product reviews are a crucial source of
opinions about a product, coming from the peo-
ple who have experienced it first-hand. However,
the task of a potential buyer is complicated by the
sheer number of reviews posted online for a prod-
uct of his/her interest. Opinion mining, or sen-
timent analysis (Pang and Lee, 2008) in product
reviews, in part, aims at automatically processing
a large number of such product reviews to identify
opinionated statements, and to classify them into
having either a positive or negative polarity.
One of the most popular techniques used for
opinion mining is that of supervised machine
learning, for which, many different lexical, syntac-
tic and knowledge-based feature representations
have been explored in the literature (Dave et al,
2003; Gamon, 2004; Matsumoto et al, 2005; Ng
et al, 2006). However, the use of syntactic fea-
tures for opinion mining has achieved varied re-
sults. In our work, we show that by altering
syntactic dependency relation triples in a partic-
ular way (namely, ?backing off? only the head
word in a dependency relation to its part-of-speech
tag), they generalize better and yield a significant
improvement on the task of identifying opinions
from product reviews. In effect, this work demon-
strates a better way to utilize syntactic dependency
relations for opinion mining.
In the remainder of the paper, we first discuss
related work. We then motivate our approach and
describe the composite back-off features, followed
by experimental results, discussion and future di-
rections for our work.
2 Related Work
The use of syntactic or deep linguistic features for
opinion mining has yielded mixed results in the lit-
erature so far. On the positive side, Gamon (2004)
found that the use of deep linguistic features ex-
tracted from phrase structure trees (which include
syntactic dependency relations) yield significant
improvements on the task of predicting satisfac-
tion ratings in customer feedback data. Mat-
sumoto et al (2005) show that when using fre-
quently occurring sub-trees obtained from depen-
dency relation parse trees as features for machine
learning, significant improvement in performance
is obtained on the task of classifying movie re-
views as having positive or negative polarity. Fi-
nally, Wilson et al (2004) use several different
features extracted from dependency parse trees to
improve performance on the task of predicting the
strength of opinion phrases.
On the flip side, Dave et al (2003) found
that for the task of polarity prediction, adding
adjective-noun dependency relationships as fea-
tures does not provide any benefit over a sim-
ple bag-of-words based feature space. Ng et al
(2006) proposed that rather than focusing on just
adjective-noun relationships, the subject-verb and
verb-object relationships should also be consid-
ered for polarity classification. However, they ob-
served that the addition of these dependency re-
lationships does not improve performance over a
feature space that includes unigrams, bigrams and
trigrams.
313
One difference that seems to separate the suc-
cesses from the failures is that of using the en-
tire set of dependency relations obtained from a
dependency parser and allowing the learning al-
gorithm to generalize, rather than picking a small
subset of dependency relations manually. How-
ever, in such a situation, one critical issue might be
the sparseness of the very specific linguistic fea-
tures, which may cause the classifier learned from
such features to not generalize. Features based on
dependency relations provide a nice way to enable
generalization to the right extent through utiliza-
tion of their structural aspect. In the next section,
we motivate this idea in the context of our task,
from a linguistic as well as machine learning per-
spective.
3 Identifying Opinionated Sentences
We focus on the problem of automatically identi-
fying whether a sentence in a product review con-
tains an opinion about the product or one of its
features. We use the definition of this task as for-
mulated by Hu and Liu (2004) on Amazon.com
and CNet.com product reviews for five different
products. Their definition of an opinion sentence
is reproduced here verbatim: ?If a sentence con-
tains one or more product features and one or
more opinion words, then the sentence is called an
opinion sentence.? Any other sentence in a review
that does not fit the above definition of an opinion
sentence is considered as a non-opinion sentence.
In general, these can be expected to be verifiable
statements or facts such as product specifications
and so on.
Before motivating the use of dependency rela-
tions as features for our task, a brief overview
about dependency relations follows.
3.1 Dependency Relations
The dependency parse for a given sentence is es-
sentially a set of triplets or triples, each of which is
composed of a grammatical relation and the pair of
words from the sentence among which the gram-
matical relation holds ({rel
i
, w
j
, w
k
}, where rel
i
is the dependency relation among words w
j
and
w
k
). The set of dependency relations is specific
to a given parser ? we use the Stanford parser1 for
computing dependency relations. The word w
j
is
usually referred to as the head word in the depen-
1http://nlp.stanford.edu/software/
lex-parser.shtml
dency triple, and the word w
k
is usually referred
to as the modifier word.
One straightforward way to use depen-
dency relations as features for machine
learning is to generate features of the form
RELATION HEAD MODIFIER and use them in a
standard bag-of-words type binary or frequency-
based representation. The indices of the head and
modifier words are dropped for the obvious reason
that one does not expect them to generalize across
sentences. We refer to such features as lexicalized
dependency relation features.
3.2 Motivation for our Approach
Consider the following examples (these are made-
up examples for the purpose of keeping the dis-
cussion succinct, but still capture the essence of
our approach):
(i) This is a great camera!
(ii) Despite its few negligible flaws, this really
great mp3 player won my vote.
Both of these sentences have an adjectival mod-
ifier (amod) relationship, the first one having
amod camera great) and the second one hav-
ing amod player great). Although both of
these features are good indicators of opinion sen-
tences and are closely related, any machine learn-
ing algorithm that treats these features indepen-
dently will not be able to generalize their rela-
tionship to the opinion class. Also, any new test
sentence that contains a noun different from either
?camera? or ?player? (for instance in the review
of a different electronic product), but is participat-
ing in a similar relationship, will not receive any
importance in favor of the opinion class ? the ma-
chine learning algorithm may not have even seen
it in the training data.
Now consider the case where we ?back off?
the head word in each of the above features to its
part-of-speech tag. This leads to a single feature:
amod NN great. This has two advantages: first,
the learning algorithm can now learn a weight for a
more general feature that has stronger evidence of
association with the opinion class, and second, any
new test sentence that contains an unseen noun in a
similar relationship with the adjective ?great? will
receive some weight in favor of the opinion class.
This ?back off? operation is a generalization of
the regular lexicalized dependency relations men-
tioned above. In the next section we describe all
such generalizations that we experimented with.
314
4 Methodology
Composite Back-off Features: The idea behind
our composite back-off features is to create more
generalizable, but not overly general back-off fea-
tures by backing off to the part-of-speech (POS)
tag of either the head word or the modifier word
(but not both at once, as in Gamon (2004) andWil-
son et al (2004)) ? hence the description ?compos-
ite,? as there is a lexical part to the feature, coming
from one word, and a POS tag coming from the
other word, along with the dependency relation it-
self.
The two types of composite back-off features
that we create from lexicalized dependency triples
are as follows:
(i) h-bo: Here we use features of the form
{rel
i
, POS
j
, w
k
}where the head word is replaced
by its POS tag, but the modifier word is retained.
(ii) m-bo: Here we use features of the form
{rel
i
, w
j
, POS
k
}, where the modifier word is re-
placed by its POS tag, but the head word is re-
tained.
Our hypothesis is that the h-bo features will
perform better than purely lexicalized dependency
relations for reasons mentioned in Section 3.2
above. Although m-bo features also generalize
the lexicalized dependency features, in a relation
such as an adjectival modifier (discussed in Sec-
tion 3.2 above), the head noun is a better candi-
date to back-off for enabling generalization across
different products, rather than the modifier adjec-
tive. For this reason, we do not expect their per-
formance to be comparable to h-bo features.
We compare our composite back-off features
with other similar ways of generalizing depen-
dency relations and lexical ngrams that have been
tried in previous work. We describe these below.
Full Back-off Features: Both Gamon (2004)
and Wilson et al (2004) utilize features based on
the following version of dependency relationships:
{rel
i
, POS
j
, POS
k
}, where they ?back off? both
the head word and the modifier word to their re-
spective POS tags (POS
j
and POS
k
). We refer
to this as hm-bo.
NGram Back-off Features: Similar to Mc-
Donald et al (2007), we utilize backed-off ver-
sions of lexical bigrams and trigrams, where all
possible combinations of the words in the ngram
are replaced by their POS tags, creating features
such as w
j
POS
k
, POS
j
w
k
, POS
j
POS
k
for
each lexical bigram and similarly for trigrams. We
refer to these as bi-bo and tri-bo features respec-
tively.
In addition to these back-off approaches, we
also use regular lexical bigrams (bi), lexical tri-
grams (tri), POS bigrams (POS-bi), POS trigrams
(POS-tri) and lexicalized dependency relations
(lexdep) as features. While testing all of our fea-
ture sets, we evaluate each of them individually by
adding them to the basic set of unigram (uni) fea-
tures.
5 Experiments and Results
Details of our experiments and results follow.
5.1 Dataset
We use the extended version of the Amazon.com /
CNet.com product reviews dataset released by Hu
and Liu (2004), available from their web page2.
We use a randomly chosen subset consisting of
2,200 review sentences (200 sentences each for
11 different products)3. The distribution is 1,053
(47.86%) opinion sentences and 1,147 (52.14%)
non-opinion sentences.
5.2 Machine Learning Parameters
We have used the Support Vector Machine (SVM)
learner (Shawe-Taylor and Cristianini, 2000) from
the MinorThird Toolkit (Cohen, 2004), along with
the ?-squared feature selection procedure, where
we reject features if their ?-squared score is not
significant at the 0.05 level. For SVM, we use
the default linear kernel with all other parameters
also set to defaults. We perform 11-fold cross-
validation, where each test fold contains all the
sentences for one of the 11 products, and the sen-
tences for the remaining ten products are in the
corresponding training fold. Our results are re-
ported in terms of average accuracy and Cohen?s
kappa values across the 11 folds.
5.3 Results
Table 1 shows the full set of results from our ex-
periments. Our results are comparable to those re-
ported by Hu and Liu (2004) on the same task;
as well as those by Arora et al (2009) on a sim-
ilar task of identifying qualified vs. bald claims
in product reviews. On the accuracy metric, the
composite features with the head word backed off
2http://www.cs.uic.edu/?liub/FBS/
sentiment-analysis.html
3http://www.cs.cmu.edu/?maheshj/
datasets/acl09short.html
315
Features Accuracy Kappa
uni .652 (?.048) .295 (?.049)
uni+bi .657 (?.066) .304 (?.089)
uni+bi-bo .650 (?.056) .299 (?.079)
uni+tri .655 (?.062) .306 (?.077)
uni+tri-bo .647 (?.051) .287 (?.075)
uni+POS-bi .676 (?.057) .349 (?.083)
uni+POS-tri .661 (?.050) .317 (?.064)
uni+lexdep .639 (?.055) .268 (?.079)
uni+hm-bo .670 (?.046) .336 (?.065)
uni+h-bo .679 (?.063) .351 (?.097)
uni+m-bo .657 (?.056) .308 (?.063)
Table 1: Shown are the average accuracy and Co-
hen?s kappa across 11 folds. Bold indicates statis-
tically significant improvements (p < 0.05, two-
tailed pairwise T-test) over the (uni) baseline.
are the only ones that achieve a statistically signif-
icant improvement over the uni baseline. On the
kappa metric, using POS bigrams also achieves
a statistically significant improvement, as do the
composite h-bo features. None of the other back-
off strategies achieve a statistically significant im-
provement over uni, although numerically hm-bo
comes quite close to h-bo. Evaluation of these
two types of features by themselves (without un-
igrams) shows that h-bo are significantly better
than hm-bo at p < 0.10 level. Regular lexical-
ized dependency relation features perform worse
than unigrams alone. These results thus demon-
strate that composite back-off features based on
dependency relations, where only the head word is
backed off to its POS tag present a useful alterna-
tive to encoding dependency relations as features
for opinion mining.
6 Conclusions and Future Directions
We have shown that for opinion mining in prod-
uct review data, a feature representation based on
a simple transformation (?backing off? the head
word in a dependency relation to its POS tag) of
syntactic dependency relations captures more gen-
eralizable and useful patterns in data than purely
lexicalized dependency relations, yielding a statis-
tically significant improvement.
The next steps that we are currently working
on include applying this approach to polarity clas-
sification. Also, the aspect of generalizing fea-
tures across different products is closely related
to fully supervised domain adaptation (Daume? III,
2007), and we plan to combine our approach with
the idea from Daume? III (2007) to gain insights
into whether the composite back-off features ex-
hibit different behavior in domain-general versus
domain-specific feature sub-spaces.
Acknowledgments
This research is supported by National Science
Foundation grant IIS-0803482.
References
Shilpa Arora, Mahesh Joshi, and Carolyn Rose?. 2009.
Identifying Types of Claims in Online Customer Re-
views. In Proceedings of NAACL 2009.
William Cohen. 2004. Minorthird: Methods for Iden-
tifying Names and Ontological Relations in Text us-
ing Heuristics for Inducing Regularities from Data.
Hal Daume? III. 2007. Frustratingly Easy Domain
Adaptation. In Proceedings of ACL 2007.
Kushal Dave, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Ex-
traction and Semantic Classification of Product Re-
views. In Proceedings of WWW 2003.
Michael Gamon. 2004. Sentiment Classification on
Customer Feedback Data: Noisy Data, Large Fea-
ture Vectors, and the Role of Linguistic Analysis. In
Proceedings of COLING 2004.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of ACM
SIGKDD 2004.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment Classification Using
Word Sub-sequences and Dependency Sub-trees. In
Proceedings of the 9th PAKDD.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. StructuredModels for
Fine-to-Coarse Sentiment Analysis. In Proceedings
of ACL 2007.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the Role of Linguistic Knowledge
Sources in the Automatic Identification and Classi-
fication of Reviews. In Proceedings of the COL-
ING/ACL 2006.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2).
John Shawe-Taylor and Nello Cristianini. 2000.
Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just How Mad Are You? Finding Strong
and Weak Opinion Clauses. In Proceedings of AAAI
2004.
316
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1302?1312, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Multi-Domain Learning: When Do Domains Matter?
Mahesh Joshi
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
maheshj@cs.cmu.edu
Mark Dredze
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, Maryland 21211
mdredze@cs.jhu.edu
William W. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
wcohen@cs.cmu.edu
Carolyn P. Rose?
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cprose@cs.cmu.edu
Abstract
We present a systematic analysis of exist-
ing multi-domain learning approaches with re-
spect to two questions. First, many multi-
domain learning algorithms resemble ensem-
ble learning algorithms. (1) Are multi-domain
learning improvements the result of ensemble
learning effects? Second, these algorithms are
traditionally evaluated in a balanced class la-
bel setting, although in practice many multi-
domain settings have domain-specific class
label biases. When multi-domain learning
is applied to these settings, (2) are multi-
domain methods improving because they cap-
ture domain-specific class biases? An under-
standing of these two issues presents a clearer
idea about where the field has had success in
multi-domain learning, and it suggests some
important open questions for improving be-
yond the current state of the art.
1 Introduction
Research efforts in recent years have demonstrated
the importance of domains in statistical natural lan-
guage processing. A mismatch between training and
test domains can negatively impact system accuracy
as it violates a core assumption in many machine
learning algorithms: that data points are indepen-
dent and identically distributed (i.i.d.). As a result,
numerous domain adaptation methods (Chelba and
Acero, 2004; Daume? III and Marcu, 2006; Blitzer et
al., 2007) target settings with a training set from one
domain and a test set from another.
Often times the training set itself violates the i.i.d.
assumption and contains multiple domains. In this
case, training a single model obscures domain dis-
tinctions, and separating the dataset by domains re-
duces training data. Instead, multi-domain learn-
ing (MDL) can take advantage of these domain la-
bels to improve learning (Daume? III, 2007; Dredze
and Crammer, 2008; Arnold et al 2008; Finkel and
Manning, 2009; Zhang and Yeung, 2010; Saha et al
2011). One such example is sentiment classification
of product reviews. Training data is available from
many product categories and while all data should
be used to learn a model, there are important differ-
ences between the categories (Blitzer et al 2007)1.
While much prior research has shown improve-
ments using MDL, this paper explores what prop-
erties of an MDL setting matter. Are previous im-
provements from MDL algorithms discovering im-
portant distinctions between features in different do-
mains, as we would hope, or are other factors con-
tributing to learning success? The key question of
this paper is: when do domains matter?
Towards this goal we explore two issues. First,
we explore the question of whether domain distinc-
tions are used by existing MDL algorithms in mean-
ingful ways. While differences in feature behaviors
between domains will hurt performance (Blitzer et
al., 2008; Ben-David et al 2009), it is not clear
if the improvements in MDL algorithms can be at-
tributed to correcting these errors, or whether they
are benefiting from something else. In particular,
there are many similarities between MDL and en-
semble methods, with connections to instance bag-
1Blitzer et al(2007) do not consider the MDL setup, they
consider a single source domain, and a single target domain,
with little or no labeled data available for the target domain.
1302
ging, feature bagging and classifier combination. It
may be that gains in MDL are the usual ensemble
learning improvements.
Second, one simple way in which domains can
change is the distribution of the prior over the la-
bels. For example, reviews of some products may be
more positive on average than reviews of other prod-
uct types. Simply capturing this bias may account
for significant gains in accuracy, even though noth-
ing is learned about the behavior of domain-specific
features. Most prior work considers datasets with
balanced labels. However, in real world applica-
tions, where labels may be biased toward some val-
ues, gains from MDL could be attributed to simply
modeling domain-specific bias. A practical advan-
tage of such a result is ease of implementation and
the ability to scale to many domains.
Overall, irrespective of the answers to these ques-
tions, a better understanding of the performance of
existing MDL algorithms in different settings will
provide intuitions for improving the state of the art.
2 Multi-Domain Learning
In the multi-domain learning (MDL) setting, exam-
ples are accompanied by both a class label and a do-
main indicator. Examples are of the form (xi, y,di),
where xi ? RN , di is a domain indicator, xi is
drawn according to a fixed domain-specific distri-
bution Ddi , and yi is the label (e.g. yi ? {?1,+1}
for binary labels). Standard learning ignores di, but
MDL uses these to improve learning accuracy.
Why should we care about the domain label? Do-
main differences can introduce errors in a number
of ways (Ben-David et al 2007; Ben-David et al
2009). First, the domain-specific distributions Ddi
can differ such that they favor different features, i.e.
p(x) changes between domains. As a result, some
features may only appear in one domain. This aspect
of domain difference is typically the focus of un-
supervised domain adaptation (Blitzer et al 2006;
Blitzer et al 2007). Second, the features may be-
have differently with respect to the label in each do-
main, i.e. p(y|x) changes between domains. As a
result, a learning algorithm cannot generalize the be-
havior of features from one domain to another. The
key idea behind many MDL algorithms is to target
one or both of these properties of domain difference
to improve performance.
Prior approaches to MDL can be broadly catego-
rized into two classes. The first set of approaches
(Daume? III, 2007; Dredze et al 2008) introduce pa-
rameters to capture domain-specific behaviors while
preserving features that learn domain-general be-
haviors. A key of these methods is that they do not
explicitly model any relationship between the do-
mains. Daume? III (2007) proposes a very simple
?easy adapt? approach, which was originally pro-
posed in the context of adapting to a specific target
domain, but easily generalizes to MDL. Dredze et al
(2008) consider the problem of learning how to com-
bine different domain-specific classifiers such that
behaviors common to several domains can be cap-
tured by a shared classifier, while domain-specific
behavior is still captured by the individual classi-
fiers. We describe both of these approaches in ? 3.2.
The second set of approaches to MDL introduce
an explicit notion of relationship between domains.
For example, Cavallanti et al(2008) assume a fixed
task relationship matrix in the context of online
multi-task learning. The key assumption is that in-
stances from two different domains are half as much
related to each other as two instances from the same
domain. Saha et al(2011) improve upon the idea
of simply using a fixed task relationship matrix by
instead learning it adaptively. They derive an online
algorithm for updating the task interaction matrix.
Zhang and Yeung (2010) derive a convex formu-
lation for adaptively learning domain relationships.
We describe their approach in ? 3.2. Finally, Daume?
III (2009) proposes a joint task clustering and multi-
task/multi-domain learning setup, where instead of
just learning pairwise domain relationships, a hier-
archical structure among them is inferred. Hierar-
chical clustering of tasks is performed in a Bayesian
framework, by imposing a hierarchical prior on the
structure of the task relationships.
In all of these settings, the key idea is to learn
both domain-specific behaviors and behaviors that
generalize between (possibly related) domains.
3 Data
To support our analysis we develop several empir-
ical experiments. We first summarize the datasets
and methods that we use in our experiments, then
1303
proceed to our exploration of MDL.
3.1 Datasets
A variety of multi-domain datasets have been used
for demonstrating MDL improvements. In this pa-
per, we focus on two datasets representative of many
of the properties of MDL.
Amazon (AMAZON) Our first dataset is the Multi-
Domain Amazon data (version 2.0), first introduced
by Blitzer et al(2007). The task is binary sentiment
classification, in which Amazon product reviews are
labeled as positive or negative. Domains are defined
by product categories. We select the four domains
used in most studies: books, dvd, electronics
and kitchen appliances.
The original dataset contained 2,000 reviews for
each of the four domains, with 1,000 positive and
1,000 negative reviews per domain. Feature extrac-
tion follows Blitzer et al(2007): we use case insen-
sitive unigrams and bigrams, although we remove
rare features (those that appear less than five times
in the training set). The reduced feature set was se-
lected given the sensitivity to feature size of some of
the MDL methods.
ConVote (CONVOTE) Our second dataset is taken
from segments of speech from United States
Congress floor debates, first introduced by Thomas
et al(2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discus-
sion in the floor debate. We select this dataset be-
cause, unlike the AMAZON data, CONVOTE can be
divided into domains in several ways based on dif-
ferent metadata attributes available with the dataset.
We consider two types of domain divisions: the bill
identifier and the political party of the speaker. Di-
vision based on the bill creates domain differences
in that each bill has its own topic. Division based on
political party implies preference for different issues
and concerns, which manifest as different language.
We refer to these datasets as BILL and PARTY.
We use Version 1.1 of the CONVOTE dataset,
available at http://www.cs.cornell.edu/
home/llee/data/convote.html. More
specifically, we combine the training, development
and test folds from the data stage three/ ver-
sion, and sub-sample to generate different versions
of the dataset required for our experiments. For
BILL we randomly sample speech segments from
three different bills. The three bills and the number
of instances for each were chosen such that we have
sufficient data in each fold for every experiment.
For PARTY we randomly sample speech segments
from the two major political parties (Democrats and
Republicans). Feature processing was identical to
AMAZON, except that the threshold for feature re-
moval was two.
3.2 Learning Methods and Features
We consider three MDL algorithms, two are repre-
sentative of the first approach and one of the second
approach (learning domain similarities) (?2). We fa-
vored algorithms with available code or that were
straightforward to implement, so as to ensure repro-
ducibility of our results.
FEDA Frustratingly easy domain adaptation
(FEDA) (Daume? III, 2007; Daume? III et al 2010b;
Daume? III et al 2010a) is an example of a classifier
combination approach to MDL. The feature space
is a cross-product of the domain and input features,
augmented with the original input features (shared
features). Prediction is effectively a linear combina-
tion of a set of domain-specific weights and shared
weights. We combine FEDA with both the SVM
and logistic regression algorithms described below
to obtain FEDA-SVM and FEDA-LR.
MDR Multi-domain regularization (MDR) (Dredze
and Crammer, 2008; Dredze et al 2009) extends the
idea behind classifier combination by explicitly for-
mulating a classifier combination scheme based on
Confidence-Weighted learning (Dredze et al 2008).
Additionally, classifier updates (which happen in
an online framework) contain an explicit constraint
that the combined classifier should perform well on
the example. Dredze et al(2009) consider several
variants of MDR. We select the two best perform-
ing methods: MDR-L2, which uses the underlying
algorithm of Crammer et al(2008), and MDR-KL,
which uses the underlying algorithm of Dredze et al
(2008). We follow their approach to classifier train-
ing and parameter optimization.
MTRL The multi-task relationship learning
(MTRL) approach proposed by Zhang and Yeung
1304
(2010) achieves states of the art performance on
many MDL tasks. This method is representative
of methods that learn similarities between domains
and in turn regularize domain-specific parameters
accordingly. The key idea in their work is the use
of a matrix-normal distribution p(X|M ,?,?) as
a prior on the matrix W created by column-wise
stacking of the domain-specific classifier weight
vectors. ? represents the covariance matrix for the
variables along the columns of X . When used as
a prior over W it models the covariance between
the domain-specific classifiers (and therefore the
tasks). ? is learned jointly with the domain-specific
classifiers. This method has similar benefits to
FEDA in terms of classifier combination, but also
attempts to model domain relationships. We use
the implementation of MTRL made available by the
authors2. For parameter tuning, we perform a grid
search over the parameters ?1 and ?2, using the fol-
lowing values for each (a total of 36 combinations):
{0.00001, 0.0001, 0.001, 0.01, 0.1, 1}.
In addition to these multi-task learning methods,
we consider a common baseline: ignoring the do-
main distinctions and learning a single classifier
over all the data. This reflects single-domain learn-
ing, in which no domain knowledge is used and will
indicate baseline performance for all experiments.
While some earlier research has included a sepa-
rate one classifier per domain baseline, it almost al-
ways performs worse, since splitting the domains
provides much less data to each classifier (Dredze
et al 2009). So we omit this baseline for simplicity.
To obtain a single classifier we use two classifica-
tion algorithms: SVMs and logistic regression.
Support Vector Machines A single SVM run
over all the training data, ignoring domain labels.
We use the SVM implementation available in the LI-
BLINEAR package (Fan et al 2008). In particular,
we use the L2-regularized L2-loss SVM (option -s
1 in version 1.8 of LIBLINEAR, and also option -B
1 for including a standard bias feature). We tune
the SVM using five-fold stratified cross-validation
on the training set, using the following values for
the trade-off parameterC: {0.0001, 0.001, 0.01, 0.1,
0.2, 0.3, 0.5, 1}.
2http://www.cse.ust.hk/?zhangyu/codes/
MTRL.zip
Logistic Regression (LR) A single logistic re-
gression model run over all the training data, ignor-
ing domain labels. Again, we use the L2-regularized
LR implementation available in the LIBLINEAR
package (option -s 0, and also option -B 1). We
tune the LR model using the same strategy as the
one used for SVM above, including the values of the
trade-off parameter C.
For all experiments, we measure average accu-
racy overK-fold cross-validation, using 10 folds for
AMAZON, and 5 folds for both BILL and PARTY.
4 When Do Domains Matter?
We now empirically explore two questions regarding
the behavior of MDL.
4.1 Ensemble Learning
Question: Are MDL improvements the result of
ensemble learning effects?
Many of the MDL approaches bear a striking
resemblance to ensemble learning. Traditionally,
ensemble learning combines the output from sev-
eral different classifiers to obtain a single improved
model (Maclin and Opitz, 1999). It is well estab-
lished that ensemble learning, applied on top of a
diverse array of quality classifiers, can improve re-
sults for a variety of tasks. The key idea behind
ensemble learning, that of combining a diverse ar-
ray of models, has been applied to settings in which
data preprocessing is used to create many different
classifiers. Examples include instance bagging and
feature bagging (Dietterich, 2000).
The core idea of using diverse inputs in making
classification decisions is common in the MDL liter-
ature. In fact, the top performing and only success-
ful entry to the 2007 CoNLL shared task on domain
adaptation for dependency parsing was a straightfor-
ward implementation of ensemble learning by cre-
ating variants of parsers (Sagae and Tsujii, 2007).
Many MDL algorithms, among them Dredze and
Crammer (2008), Daume? III (2009), Zhang and Ye-
ung (2010) and Saha et al(2011), all include some
notion of learning domain-specific classifiers on the
training data, and combining them in the best way
possible. To be clear, we do not claim that these
approaches can be reduced to an existing ensem-
ble learning algorithm. There are crucial elements
1305
in each of these algorithms that separate them from
existing ensemble learning algorithms. One exam-
ple of such a distinction is the learning of domain
relationships by both Zhang and Yeung (2010) and
Saha et al(2011). However, we argue that their
core approach, that of combining parameters that are
trained on variants of the data (all data or individual
domains), is an ensemble learning idea.
Consider instance bagging, in which multiple
classifiers are each trained on random subsets of the
data. The resulting classifiers are then combined
to form a final model. In MDL, we can consider
each domain a subset of the data, albeit non-random
and non-overlapping. The final model combines the
domain-specific parameters and parameters trained
on other instances, which in the case of FEDA are the
shared parameters. In this light, these methods are a
complex form of instance bagging, and their devel-
opment could be justified from this perspective.
However, given this justification, are improve-
ments from MDL simply the result of standard en-
semble learning effects, or are these methods re-
ally learning something about domain behavior? If
knowledge of domain was withheld from the algo-
rithm, could we expect similar improvements? As
we will do in each empirical experiment, we propose
a contrarian hypothesis:
Hypothesis: Knowledge of domains is irrelevant
for MDL.
Empirical Evaluation We evaluate this hypothe-
sis as follows. We begin by constructing a true MDL
setting, in which we attempt to improve accuracy
through knowledge of the domains. We will apply
three MDL algorithms (FEDA, MDR, and MTRL) to
our three multi-domain datasets (AMAZON, BILL,
and PARTY) and compare them against a single clas-
sifier baseline. We will then withhold knowledge
of the true domains from these algorithms and in-
stead provide them with random ?pseudo-domains,?
and then evaluate the change in their behavior. The
question is whether we can obtain similar benefits
by ignoring domain labels and relying strictly on an
ensemble learning motivation (instance bagging).
For the ?True Domain? setting, we apply the
MDL algorithms as normal. For the ?Random Do-
main? setting, we randomly shuffle the domain la-
bels within a given class label within each fold, thus
maintaining the same number of examples for each
domain label, and also retaining the same class dis-
tribution within each randomized domain. The re-
sulting ?pseudo-domains? are then similar to ran-
dom subsets of the data used in ensemble learning.
Following the standard practice in previous work,
for this experiment we use a balanced number of
examples from each domain and a balanced num-
ber of positive and negative labels (no class bias).
For AMAZON (4 domains), we have 10 folds of 400
examples per fold, for BILL (3 domains) 5 folds of
60 examples per fold, and for PARTY (2 domains) 5
folds of 80 examples per fold. In the ?Random Do-
main? setting, since we are randomizing the domain
labels, we increase the number of trials. We repeat
each cross-validation experiment 5 times with differ-
ent randomization of the domain labels each time.
Results Results are shown in Table 1. The first
row shows absolute (average) accuracy for a single
classifier trained on all data, ignoring domain dis-
tinctions. The remaining cells indicate absolute im-
provements against the baseline.
First, we note for the well-studied AMAZON
dataset that our results with true domains are con-
sistent with the previous literature. FEDA is known
to not improve upon a single classifier baseline for
that dataset (Dredze et al 2009). Both MDR-L2 and
MDR-KL improve upon the single classifier baseline,
again as per Dredze et al(2009). And finally, MTRL
also improves upon the single classifier baseline. Al-
though the MTRL improvement is not as dramatic as
in the original paper3, the average accuracy that we
achieve for MTRL (84.2%) is better than the best av-
erage accuracy in the original paper (83.65%).
The main comparison to make in Table 1 is be-
tween having knowledge of true domains or not.
?Random Domain? in the table is the case where do-
main identifiers are randomly shuffled within a given
fold. Ignoring the significance test results for now,
overall the results indicate that knowing the true do-
mains is useful for MDL algorithms. Randomiz-
ing the domains does not work better than knowing
true domains in any case. However, in all except
one case, the improvements of MDL algorithms are
3This might be due to a different version of the dataset being
used in a cross-validation setup, rather than their train/test setup,
and also because of differences in baseline approaches.
1306
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
83.93% 83.78% 66.67% 68.00% 62.75% 64.00%
FEDA
True Domain -0.35 -0.10 +2.33 + 1.00 +4.25 N +1.25
Random Domain -1.30 H -1.02 H -1.20 -2.07 -2.05 -2.10
MDR-L2
True Domain +1.87 N +2.02 N +0.00 -1.33 +2.25 +1.00
Random Domain +0.91 N +1.07 N -2.67 -4.00 -2.80 -4.05
MDR-KL
True Domain +1.85 N +2.00 N +1.00 -0.33 +3.00 +1.75
Random Domain +1.36 N +1.51 N +0.60 -0.73 -1.30 -2.55 H
MTRL
True Domain +0.27 +0.42 +0.67 -0.67 +1.50 +0.25
Random Domain -0.37 -0.21 -1.47 -2.80 -3.55 -4.80
Table 1: A comparison between MDL methods with access to the ?True Domain? labels and methods that
use ?Random Domain? information, essentially ensemble learning. The first row has raw accuracy numbers,
whereas the remaining entries are absolute improvements over the baseline. N: Significantly better than the
corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly worse than
corresponding baseline, with p < 0.05, using a paired t-test.
significantly better only for the AMAZON dataset4.
And interestingly, exactly in the same case, ran-
domly shuffling the domains also gives significant
improvements compared to the baseline, showing
that there is an ensemble learning effect in operation
for MDR-L2 and MDR-KL on the AMAZON dataset.
For FEDA, randomizing the domains significantly
hurts its performance on the AMAZON data, as is
the case for MDR-KL on the PARTY data. Therefore,
while our contrarian hypothesis about irrelevance of
domains is not completely true, it is indeed the case
that some MDL methods benefit from the ensemble
learning effect.
A second observation to be made from these re-
sults is that, while all of empirical research on MDL
assumes the definition of domains as a given, the
question of how to split a dataset into domains given
various metadata attributes is still open. For exam-
ple, in our experiments, in general, using the po-
litical party as a domain distinction gives us more
improvements over the corresponding baseline ap-
proach5.
We provide a detailed comparison of using true
4Some numbers in Table 1 might appear to be significant,
but are not. That is because of high variance in the performance
of the methods across the different folds.
5The BILL and the PARTY datasets are not directly compa-
rable to each other, although the prediction task is the same.
vs. randomized domains in Table 6, after presenting
the second set of experimental results.
4.2 Domain-specific Class Bias
Question: Are MDL methods improving because
they capture domain-specific class biases?
In previous work, and the above section, experi-
ments have assumed a balanced dataset in terms of
class labels. It has been in these settings that MDL
methods improve. However, this is an unrealistic as-
sumption. Even in our datasets, the original versions
demonstrated class bias: Amazon product reviews
are generally positive, votes on bills are rarely tied,
and political parties vote in blocs. While it is com-
mon to evaluate learning methods on balanced data,
and then adjust for imbalanced real world datasets, it
is unclear what effect domain-specific class bias will
have on MDL methods. Domains can differ in their
proportion of examples of different classes. For ex-
ample, it is quite likely that less controversial bills in
the United States Congress will have more yes votes
than controversial bills. Similarly, if instead of the
category of a product, its brand is considered as a do-
main, it is likely that some brands receive a higher
proportion of positive reviews than others.
Improvements from MDL in such settings may
simply be capturing domain-specific class biases.
1307
domain class cb1 cb2 cb3 cb4
AMAZON
b
- 20 80 60 40
+ 80 20 40 60
d
- 40 20 80 60
+ 60 80 20 40
e
- 60 40 20 80
+ 40 60 80 20
k
- 80 60 40 20
+ 20 40 60 80
BILL
031
N 16 4 8 12
Y 4 16 12 8
088
N 12 16 4 8
Y 8 4 16 12
132
N 8 12 16 4
Y 12 8 4 16
PARTY
D
N 10 30 15 25
Y 30 10 25 15
R
N 30 10 25 15
Y 10 30 15 25
Table 2: The table shows the distribution of in-
stances across domains and class labels within one
fold of each of the datasets, for four different class
bias trials. These datasets with varying class bias
across domains were used for the experiments de-
scribed in ?4.2
Consider two domains, where each domain is biased
towards the opposite label. In this case, domain-
specific parameters may simply be capturing the bias
towards the class label, increasing the weight uni-
formly of features predictive of the dominant class.
Similarly, methods that learn domain similarity may
be learning class bias similarity.
Why does the effectiveness of these domain-
specific bias parameters matter? First, if capturing
domain-specific class bias is the source of improve-
ment, there are much simpler methods for learning
that can be just as effective. This would be espe-
cially important in settings where we have many do-
mains, and learning domain-specific parameters for
each feature becomes infeasible. Second, if class
bias accounted for most of the improvement in learn-
ing, it suggests that such settings could be amenable
to unsupervised adaptation of the bias parameters.
Hypothesis: MDL largely capitalizes on
domain-specific class bias.
Empirical Evaluation To evaluate our hypothe-
sis, for each of our three datasets we create 4 random
versions, each with some domain-specific class-bias.
A summary of the dataset partitions is shown in
Table 2. For example, for the AMAZON dataset,
we create 4 versions (cb1 . . . cb4), where each do-
main has 100 examples per fold and each domain
has a different balance between positive and nega-
tive classes. For each of these settings, we conduct
a 10-fold cross validation experiment, then average
the CV results for each of the 4 settings. The re-
sulting accuracy numbers therefore reflect an aver-
age across many types of bias, each evaluated many
times. We do a similar experiment for the BILL and
PARTY datasets, except we use 5-fold CV.
In addition to the multi-domain and baseline
methods, we add a new baseline: DOM-ID. In this
setting, we augment the baseline classifier (which
ignores domain labels) with a new feature that in-
dicates the domain label. While we already include
a general bias feature, as is common in classifica-
tion tasks, these new features will capture domain-
specific bias. This is the only change to the base-
line classifier, so improvements over the baseline are
indicative of the change in domain-bias that can be
captured using these simple features.
Results Results are shown in Table 3. The table
follows the same structure as Table 1, with the ad-
dition of the results for the DOM-ID approach. We
first examine the efficacy of MDL in this setting. An
observation that is hard to miss is that MDL results
in these experiments show significant improvements
in almost all cases, as compared to only a few cases
in Table 1, despite the fact that even the baseline ap-
proaches have a higher accuracy. This shows that
MDL results can be highly influenced by systematic
differences in class bias across domains. Note that
there is also a significant negative influence of class
bias on MTRL for the AMAZON data.
A comparison of the MDL results on true domains
to the DOM-ID baseline gives us an idea of how
much MDL benefits purely from class bias differ-
ences across domains. We see that in most cases,
about half of the improvement seen in MDL is ac-
counted for by a simple baseline of using the do-
main identifier as a feature, and all but one of the
improvements from DOM-ID are significant. This
1308
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
85.52% 85.46% 70.50% 70.67% 65.44% 65.81%
FEDA
True Domain +0.11 +0.31 +4.25 N +4.00 N +4.81 N +4.69 N
Random Domain +0.94 N +1.03 N +3.68 N +4.03 N +4.24 +3.73
MDR-L2
True Domain +0.92 N +0.98 N +4.42 N +4.25 N +1.31 +0.94
Random Domain +1.86 N +1.92 N +3.93 N +3.77 N +0.65 +0.28
MDR-KL
True Domain +1.54 N +1.59 N +5.17 N +5.00 N +4.25 N +3.88 N
Random Domain +2.84 N +2.90 N +4.13 N +3.97 N +3.81 N +3.44
MTRL
True Domain -1.22 H -1.17 H +4.50 N +4.33 N +6.44 N +6.06 N
Random Domain -0.69 H -0.63 H +3.53 N +3.37 N +4.87 N +4.50 N
DOM-ID
True Domain +0.36 +0.38 N +2.83 N +2.75 N +3.75 N +4.00 N
Random Domain +1.73 N +1.76 N +4.50 N +4.98 N +5.24 N +5.31 N
Table 3: A comparison between MDL methods with class biased data. Similar to the setup where we
evaluate the ensemble learning effect, we have a setting of using randomized domains. N: Significantly
better than the corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. H: Significantly
worse than corresponding baseline, with p < 0.05, using a paired t-test.
suggests that in a real-world scenario where differ-
ence in class bias across domains is quite likely, it is
useful to consider DOM-ID as a simple baseline that
gives good empirical performance. To our knowl-
edge, using this approach as a baseline is not stan-
dard practice in MDL literature.
Finally, we also include the ?Random Domain?
evaluation in the our class biased version of exper-
iments. Each ?Random Domain? result in Table 3
is an average over 20 cross-validation runs (5 ran-
domized trials for each of the four class biased tri-
als cb1 . . . cb4). This setup combines the effects
of ensemble learning and bias difference across do-
mains. As seen in the table, for MDL algorithms the
results are consistently better as compared to know-
ing the true domains for the AMAZON dataset. For
the other datasets, the performance after randomiz-
ing the domains is still significantly better than the
baseline. This evaluation on randomized domains
further strengthens the conclusion that differences in
bias across domains play an important role, even in
the case of noisy domains. Looking at the perfor-
mance of DOM-ID with randomized domains, we
see that in all cases the DOM-ID baseline performs
better with randomized domains. While the dif-
ference is significant mostly only on the AMAZON
domain class cb5 cb6 cb7 cb8
AMAZON
b
- 20 40 60 80
+ 80 60 40 20
d
- 20 40 60 80
+ 80 60 40 20
e
- 20 40 60 80
+ 80 60 40 20
k
- 20 40 60 80
+ 20 40 60 80
Table 4: The table shows the distribution of in-
stances across domains and class labels within one
fold of the AMAZON dataset, for four different class
bias trials. For the BILL and PARTY datasets, similar
folds with consistent bias were created (number of
examples used was different). These datasets with
consistent class bias across domains were used for
the experiments described in ?4.2.1
dataset (details in Table 6, columns under ?Varying
Class Bias,?) this trend is still counter-intuitive. We
suspect this might be because randomization creates
a noisy version of the domain labels, which helps
learners to avoid over-fitting that single feature.
1309
4.2.1 Consistent Class Bias
We also performed a set of experiments that ap-
ply MDL algorithms to a setting where the datasets
have different class biases (unlike the experiments
reported in Table 1, where the classes are balanced),
but, unlike the experiments reported in Table 3, the
class bias is the same within each of the domains.
We refer to this as the case of consistent class bias
across domains. The distribution of classes within
each domain within each fold is shown in Table 4.
The results for this set of experiments are reported
in Table 5. The structure of Table 5 is identical to
that of Table 1. Comparing these results to those
in Table 1, we can see that in most cases the im-
provements seen using MDL algorithms are lower
than those seen in Table 1. This is likely due to
the higher baseline performance in the consistent
class bias case. A notable difference is in the per-
formance of MTRL ? it is significantly worse for
the AMAZON dataset, and significantly better for the
PARTY dataset. For the AMAZON dataset, we be-
lieve that the domain distinctions are less meaning-
ful, and hence forcing MTRL to learn the relation-
ships results in lower performance. For the PARTY
dataset, in the case of a class-biased setup, know-
ing the party is highly predictive of the vote (in the
original CONVOTE dataset, Democrats mostly vote
?no? and Republicans mostly vote ?yes?), and this
is rightly exploited by MTRL.
4.2.2 True vs. Randomized Domains
In Table 6 we analyze the difference in perfor-
mance of MDL methods when using true vs. ran-
domized domain information. For the three sets of
results reported earlier, we evaluated whether using
true domains as compared to randomized domains
gives significantly better, significantly worse or
equal performance. Significance testing was done
using a paired t-test with ? = 0.05 as before. As the
table shows, for the first set of results where the class
labels were balanced (overall, as well as within each
domain), using true domains was significantly better
mostly only for the AMAZON dataset. FEDA-SVM
was the only approach that was consistently better
with true domains across all datasets. Note, how-
ever, that it was significantly better than the baseline
approach only for PARTY.
For the second set of results (Table 3) where the
class bias varied across the different domains, us-
ing true domains was either no different from using
randomized domains, or it was significantly worse.
In particular, it was consistently significantly worse
to use true domains on the AMAZON dataset. This
questions the utility of domains on the AMAZON
dataset in the context of MDL in a domain-specific
class bias scenario. Since randomizing the domains
works better for all of the MDL methods on AMA-
ZON, it suggests that an ensemble learning effect
is primarily responsible for the significant improve-
ments seen on the AMAZON data, when evaluated in
a domain-specific class bias setting.
Finally, for the case of consistent class bias across
domains, the trend is similar to the case of no class
bias ? using true domains is useful. This table
further supports the conclusion that domain-specific
class bias highly influences multi-domain learning.
5 Discussion and Open Questions
Our analysis of MDL algorithms revealed new
trends that suggest further avenues of exploration.
We suggest three open questions in response.
Question: When are MDL methods most effective?
Our empirical results suggest that MDL can be more
effective in settings with domain-specific class bi-
ases. However, we also saw differences in im-
provements for each method, and for different do-
mains. Differences emerge between the AMAZON
and CONVOTE datasets in terms of the ensemble
learning hypothesis. While there has been some the-
oretical analyses on the topic of MDL (Ben-David
et al 2007; Ben-David et al 2009; Mansour et
al., 2009; Daume? III et al 2010a), our results sug-
gest performing new analyses that relate ensemble
learning results with the MDL setting. These anal-
yses could provide insights into new algorithms that
can take advantage of the specific properties of each
multi-domain setting.
Question: What makes a good domain for MDL?
To the best of our knowledge, previous work has
assumed that domain identities are provided to the
learning algorithm. However, in reality, there may
be many ways to split a dataset into domains. For
example, consider the CONVOTE dataset, which we
split both by BILL and PARTY. The choice of splits
1310
AMAZON BILL PARTY
SVM LR SVM LR SVM LR
Single Classifier
86.06% 86.22% 76.42% 75.58% 69.31% 68.38%
FEDA
True Domain -0.25 -0.33 -0.83 +0.25 +0.88 +1.25
Random Domain -1.17 H -1.26 H -1.33 -0.82 -0.55 -0.04
MDR-L2
True Domain +0.39 N +0.23 -0.42 +0.42 -2.12 -1.19
Random Domain -0.38 -0.53 H -3.57 -2.73 -4.30 H -3.36 H
MDR-KL
True Domain +0.81 N +0.65 N -0.83 +0.00 +1.31 +2.25 N
Random Domain +0.22 +0.06 -1.90 -1.07 -0.60 +0.34
MTRL
True Domain -1.52 H -1.68 H -1.92 -1.08 +3.12 N +4.06 N
Random Domain -2.12 H -2.28 H -0.95 -0.12 +0.19 +1.12 N
Table 5: A comparison between MDL methods with data that have a consistent class bias across domains.
Similar to the setup where we evaluate the ensemble learning effect, we have a setting of using randomized
domains. N: Significantly better than the corresponding SVM or LR baseline, with p < 0.05, using a paired
t-test. H: Significantly worse than corresponding baseline, with p < 0.05, using a paired t-test.
MDL Method No Class Bias (Tab. 1) Varying Class Bias (Tab. 3) Consistent Class Bias (Tab. 5)
better worse equal better worse equal better worse equal
FEDA-SVM AM, BI, PA AM BI, PA AM, PA BI
FEDA-LR AM BI, PA AM BI, PA AM, BI PA
MDR-L2 AM BI, PA AM BI, PA AM, BI PA
MDR-KL PA AM, BI AM BI, PA AM, PA BI
MTRL AM BI, PA AM BI, PA AM, PA BI
DOM-ID-SVM ? ? ? AM BI, PA ? ? ?
DOM-ID-LR ? ? ? AM, BI PA ? ? ?
Table 6: The table shows the datasets (AM:AMAZON, BI:BILL, PA:PARTY) for which a given MDL method
using true domain information was significantly better, significantly worse, or not significantly different
(equal) as compared to using randomized domain information with the same MDL method.
impacted MDL. This poses new questions: what
makes a good domain? How should we choose to di-
vide data along possible metadata properties? If we
can gain improvements simply by randomly creat-
ing new domains (?Random Domain? setting in our
experiments) then there may be better ways to take
advantage of the provided metadata for MDL.
Question: Can we learn class-bias for
unsupervised domain adaptation?
Experiments with domain-specific class biases re-
vealed that a significant part of the improvements
could be achieved by adding domain-specific bias
features. Limiting the multi-domain improvements
to a small set of parameters raises an interesting
question: can these parameters be adapted to a new
domain without labeled data? Traditionally, domain
adaptation without target domain labeled data has
focused on learning the behavior of new features;
beliefs about existing feature behaviors could not be
corrected without new training data. However, by
collapsing the adaptation into a single bias parame-
ter, we may be able to learn how to adjust this pa-
rameter in a fully unsupervised way. This would
open the door to improvements in this challenging
setting for real world problems where class bias was
a significant factor.
Acknowledgments
Research presented here is supported by the Office
of Naval Research grant number N000141110221.
1311
References
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2008. Exploiting Feature Hierarchy for Transfer
Learning in Named Entity Recognition. In Proceed-
ings of ACL-08: HLT, pages 245?253.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In Proceedings of NIPS 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440?447.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
Bounds for Domain Adaptation. In Advances in Neu-
ral Information Processing Systems (NIPS 2007).
Giovanni Cavallanti, Nicolo` Cesa-Bianchi, and Claudio
Gentile. 2008. Linear Algorithms for Online Multi-
task Classification. In Proceedings of COLT.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
Maximum Entropy Capitalizer: Little Data Can Help
a Lot. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 285?292.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2008. Exact convex confidence-weighted learning. In
Advances in Neural Information Processing Systems
(NIPS).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010a. A Co-regularization Based Semi-supervised
Domain Adaptation. In Neural Information Process-
ing Systems.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010b. Frustratingly Easy Semi-Supervised Domain
Adaptation. In Proceedings of the ACL 2010 Work-
shop on Domain Adaptation for Natural Language
Processing, pages 53?59.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263.
Hal Daume? III. 2009. Bayesian multitask learning with
latent hierarchies. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence.
Thomas G. Dietterich. 2000. An experimental compar-
ison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization.
Machine Learning, 40:139?157.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ?08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ?08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1-2).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR : A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Jenny R. Finkel and Christopher D. Manning. 2009. Hi-
erarchical Bayesian Domain Adaptation. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
602?610.
Richard Maclin and David Opitz. 1999. Popular Ensem-
ble Methods: An Empirical Study. Journal of Artifi-
cial Intelligence Research, 11:169?198.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Domain Adaptation with Multiple
Sources. In Proceedings of NIPS 2008, pages 1041?
1048.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Conference on Natural Language
Learning (Shared Task).
Avishek Saha, Piyush Rai, Hal Daume? III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
1312
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 621?626,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Correcting Keyboard Layout Errors and Homoglyphs in Queries
Derek Barnes
debarnes@ebay.com
Mahesh Joshi
mahesh.joshi@ebay.com
eBay Inc., 2065 Hamilton Ave, San Jose, CA, 95125, USA
Hassan Sawaf
hsawaf@ebay.com
Abstract
Keyboard layout errors and homoglyphs
in cross-language queries impact our abil-
ity to correctly interpret user informa-
tion needs and offer relevant results.
We present a machine learning approach
to correcting these errors, based largely
on character-level n-gram features. We
demonstrate superior performance over
rule-based methods, as well as a signif-
icant reduction in the number of queries
that yield null search results.
1 Introduction
The success of an eCommerce site depends on
how well users are connected with products and
services of interest. Users typically communi-
cate their desires through search queries; however,
queries are often incomplete and contain errors,
which impact the quantity and quality of search
results.
New challenges arise for search engines in
cross-border eCommerce. In this paper, we fo-
cus on two cross-linguistic phenomena that make
interpreting queries difficult: (i) Homoglyphs:
(Miller, 2013): Tokens such as ?case? (underlined
letters Cyrillic), in which users mix characters
from different character sets that are visually simi-
lar or identical. For instance, English and Russian
alphabets share homoglyphs such as c, a, e, o, y,
k, etc. Although the letters are visually similar or
in some cases identical, the underlying character
codes are different. (ii) Keyboard Layout Errors
(KLEs): (Baytin et al., 2013): When switching
one?s keyboard between language modes, users at
times enter terms in the wrong character set. For
instance, ?????? ????? may appear to be a Rus-
sian query. While ??????? is the Russian word
for ?case?, ?????? is actually the user?s attempt
to enter the characters ?ipad? while leaving their
keyboard in Russian language mode. Queries con-
taining KLEs or homoglyphs are unlikely to pro-
duce any search results, unless the intended ASCII
sequences can be recovered. In a test set sam-
pled from Russian/English queries with null (i.e.
empty) search results (see Section 3.1), we found
approximately 7.8% contained at least one KLE or
homoglyph.
In this paper, we present a machine learning
approach to identifying and correcting query to-
kens containing homoglyphs and KLEs. We show
that the proposed method offers superior accuracy
over rule-based methods, as well as significant im-
provement in search recall. Although we focus our
results on Russian/English queries, the techniques
(particularly for KLEs) can be applied to other lan-
guage pairs that use different character sets, such
as Korean-English and Thai-English.
2 Methodology
In cross-border trade at eBay, multilingual queries
are translated into the inventory?s source language
prior to search. A key application of this, and
the focus of this paper, is the translation of Rus-
sian queries into English, in order to provide Rus-
sian users a more convenient interface to English-
based inventory in North America. The presence
of KLEs and homoglyphs in multilingual queries,
however, leads to poor query translations, which in
turn increases the incidence of null search results.
We have found that null search results correlate
with users exiting our site.
In this work, we seek to correct for KLEs and
homoglyphs, thereby improving query translation,
reducing the incidence of null search results, and
increasing user engagement. Prior to translation
and search, we preprocess multilingual queries
by identifying and transforming KLEs and homo-
glyphs as follows (we use the query ?????? ????
2 new? as a running example):
(a) Tag Tokens: label each query token
621
with one of the following semantically moti-
vated classes, which identify the user?s informa-
tion need: (i) E: a token intended as an English
search term; (ii) R: a Cyrillic token intended as a
Russian search term; (iii) K: A KLE, e.g. ??????
for the term ?ipad?. A token intended as an En-
glish search term, but at least partially entered in
the Russian keyboard layout; (iv) H: A Russian
homoglyph for an English term, e.g. ???w? (un-
derlined letters Cyrillic). Employs visually sim-
ilar letters from the Cyrillic character set when
spelling an intended English term; (v) A: Ambigu-
ous tokens, consisting of numbers and punctuation
characters with equivalent codes that can be en-
tered in both Russian and English keyboard lay-
outs. Given the above classes, our example query
?????? ???? 2 new? should be tagged as ?R K A
E?.
(b) Transform Queries: Apply a deterministic
mapping to transform KLE and homoglyph tokens
from Cyrillic to ASCII characters. For KLEs the
transformation maps between characters that share
the same location in Russian and English keyboard
layouts (e.g. ? ? a, ? ? s). For homoglyphs the
transformation maps between a smaller set of vi-
sually similar characters (e.g. ?? e, ??m). Our
example query would be transformed into ??????
ipad 2 new?.
(c) Translate and Search: Translate the trans-
formed query (into ?case ipad 2 new? for our ex-
ample), and dispatch it to the search engine.
In this paper, we formulate the token-level tag-
ging task as a standard multiclass classification
problem (each token is labeled independently), as
well as a sequence labeling problem (a first order
conditional Markov model). In order to provide
end-to-end results, we preprocess queries by de-
terministically transforming into ASCII the tokens
tagged by our model as KLEs or homoglyphs. We
conclude by presenting an evaluation of the impact
of this transformation on search.
2.1 Features
Our classification and sequence models share a
common set of features grouped into the follow-
ing categories:
2.1.1 Language Model Features
A series of 5-gram, character-level language mod-
els (LMs) capture the structure of different types
of words. Intuitively, valid Russian terms will
have high probability in Russian LMs. In contrast,
KLEs or homoglyph tokens, despite appearing on
the surface to be Russian terms, will generally
have low probability in the LMs trained on valid
Russian words. Once mapped into ASCII (see
Section 2 above), however, these tokens tend to
have higher probability in the English LMs. LMs
are trained on the following corpora:
English and Russian Vocabulary: based on
a collection of open source, parallel En-
glish/Russian corpora (?50M words in all).
English Brands: built from a curated list of 35K
English brand names, which often have distinctive
linguistic properties compared with common En-
glish words (Lowrey et al., 2013).
Russian Transliterations: built from a col-
lection of Russian transliterations of proper
names from Wikipedia (the Russian portion of
guessed-names.ru-en made available as a
part of WMT 2013
1
).
For every input token, each of the above LMs
fires a real-valued feature ? the negated log-
probability of the token in the given language
model. Additionally, for tokens containing Cyril-
lic characters, we consider the token?s KLE and
homoglyph ASCII mappings, where available. For
each mapping, a real-valued feature fires corre-
sponding to the negated log-probability of the
mapped token in the English and Brands LMs.
Lastly, an equivalent set of LM features fires for
the two preceding and following tokens around the
current token, if applicable.
2.1.2 Token Features
We include several features commonly used in
token-level tagging problems, such as case and
shape features, token class (such as letters-only,
digits-only), position of the token within the query,
and token length. In addition, we include fea-
tures indicating the presence of characters from
the ASCII and/or Cyrillic character sets.
2.1.3 Dictionary Features
We incorporate a set of features that indicate
whether a given lowercased query token is a mem-
ber of one of the lexicons described below.
UNIX: The English dictionary shipped with Cen-
tOS, including ?480K entries, used as a lexicon
of common English words.
BRANDS: An expanded version of the curated list
of brand names used for LM features. Includes
1
www.statmt.org/wmt13/
translation-task.html#download
622
?58K brands.
PRODUCT TITLES: A lexicon of over 1.6M en-
tries extracted from a collection of 10M product
titles from eBay?s North American inventory.
QUERY LOGS: A larger, in-domain collection of
approximately 5M entries extracted from ?100M
English search queries on eBay.
Dictionary features fire for Cyrillic tokens when
the KLE and/or homoglyph-mapped version of the
token appears in the above lexicons. Dictionary
features are binary for the Unix and Brands dictio-
naries, and weighted by relative frequency of the
entry for the Product Titles and Query Logs dic-
tionaries.
3 Experiments
3.1 Datasets
The following datasets were used for training and
evaluating the baseline (see Section 3.2 below) and
our proposed systems:
Training Set: A training set of 6472 human-
labeled query examples (17,239 tokens).
In-Domain Query Test Set: A set of 2500 Rus-
sian/English queries (8,357 tokens) randomly se-
lected from queries with null search results. By
focusing on queries with null results, we empha-
size the presence of KLEs and homoglyphs, which
occur in 7.8% of queries in our test set.
Queries were labeled by a team of Russian lan-
guage specialists. The test set was also indepen-
dently reviewed, which resulted in the correction
of labels for 8 out of the 8,357 query tokens.
Although our test set is representative of the
types of problematic queries targeted by our
model, our training data was not sampled using the
same methodology. We expect that the differences
in distributions between training and test sets, if
anything, make the results reported in Section 3.3
somewhat pessimistic
2
.
3.2 Dictionary Baseline
We implemented a rule-based baseline system em-
ploying the dictionaries described in Section 2.1.3.
In this system, each token was assigned a class
k ? {E,R,K,H,A} using a set of rules: a token
among a list of 101 Russian stopwords
3
is tagged
2
As expected, cross-validation experiments on the train-
ing data (for parameter tuning) yielded results slightly higher
than the results reported in Section 3.3, which use a held-out
test set
3
Taken from the Russian Analyzer packaged with Lucene
? see lucene.apache.org.
as R. A token containing only ASCII characters is
labeled as A if all characters are common to En-
glish and Russian keyboards (i.e. numbers and
some punctuation), otherwise E. For tokens con-
taining Cyrillic characters, KLE and homoglyph-
mapped versions are searched in our dictionaries.
If found, K or H are assigned. If both mapped ver-
sions are found in the dictionaries, then either K
or H is assigned probabilistically
4
. In cases where
neither mapped version is found in the dictionary,
the token assigned is either R or A, depending on
whether it consists of purely Cyrillic characters, or
a mix of Cyrillic and ASCII, respectively.
Note that the above tagging rules allow tokens
with classes E and A to be identified with perfect
accuracy. As a result, we omit these classes from
all results reported in this work. We also note
that this simplification applies because we have
restricted our attention to the Russian ? English
direction. In the bidirectional case, ASCII tokens
could represent either English tokens or KLEs (i.e.
a Russian term entered in the English keyboard
layout). We leave the joint treatment of the bidi-
rectional case to future work.
Tag Prec Recall F1
K .528 .924 .672
H .347 .510 .413
R .996 .967 .982
Table 1: Baseline results on the test set, using
UNIX, BRANDS, and the PRODUCT TITLES dic-
tionaries.
We experimented with different combinations
of dictionaries, and found the best combination to
be UNIX, BRANDS, and PRODUCT TITLES dic-
tionaries (see Table 1). We observed a sharp de-
crease in precision when incorporating the QUERY
LOGS dictionary, likely due to noise in the user-
generated content.
Error analysis suggests that shorter words are
the most problematic for the baseline system
5
.
Shorter Cyrillic tokens, when transformed from
Cyrillic to ASCII using KLE or homoglyph map-
pings, have a higher probability of spuriously
mapping to valid English acronyms, model IDs,
or short words. For instance, Russian car brand
????? maps across keyboard layouts to ?dfp?,
4
We experimented with selecting K or H based on a prior
computed from training data; however, results were lower
than those reported, which use random selection.
5
Stopwords are particularly problematic, and hence ex-
cluded from consideration as KLEs or homoglyphs.
623
Tag
Classification Sequence
P R F1 P R F1
LR
K .925 .944 .935 .915 .934 .925
H .708 .667 .687 .686 .686 .686
R .996 .997 .996 .997 .996 .997
RF
K .926 .949 .937 .935 .949 .942
H .732 .588 .652 .750 .588 .659
R .997 .997 .997 .996 .998 .997
Table 2: Classification and sequence tagging re-
sults on the test set
a commonly used acronym in product titles for
?Digital Flat Panel?. Russian words ?????? and
????? similarly map by chance to English words
?verb? and ?her?.
A related problem occurs with product model
IDs, and highlights the limits of treating query to-
kens independently. Consider Cyrillic query ????
e46?. The first token is a Russian transliteration
for the BMW brand. The second token, ?e46?,
has three possible interpretations: i) as a Russian
token; ii) a homoglyph for ASCII ?e46?; or iii)
a KLE for ?t46?. It is difficult to discriminate
between these options without considering token
context, and in this case having some prior knowl-
edge that e46 is a BMW model.
3.3 Machine Learning Models
We trained linear classification models using lo-
gistic regression (LR)
6
, and non-linear models us-
ing random forests (RFs), using implementations
from the Scikit-learn package (Pedregosa et al.,
2011). Sequence models are implemented as first
order conditional Markov models by applying a
beam search (k = 3) on top of the LR and RF
classifiers. The LR and RF models were tuned us-
ing 5-fold cross-validation results, with models se-
lected based on the mean F1 score across R, K, and
H tags.
Table 2 shows the token-level results on our in-
domain test set. As with the baseline, we focus the
model on disambiguating between classes R, K and
H. Each of the reported models performs signifi-
cantly better than the baseline (on each tag), with
statistical significance evaluated usingMcNemar?s
test. The differences between LR and RF mod-
els, as well as sequence and classification variants,
however, are not statistically significant. Each of
the machine learning models achieves a query-
level accuracy score of roughly 98% (the LR se-
6
Although CRFs are state-of-the-art for many tagging
problems, in our experiments they yielded results slightly
lower than LR or RF models.
quence model achieved the lowest with 97.78%,
the RF sequence model the highest with 97.90%).
Our feature ablation experiments show that
the majority of predictive power comes from the
character-level LM features. Dropping LM fea-
tures results in a significant reduction in perfor-
mance (F1 scores .878 and .638 for the RF Se-
quence model on classes K and H). These results
are still significantly above the baseline, suggest-
ing that token and dictionary features are by them-
selves good predictors. However, we do not see
a similar performance reduction when dropping
these feature groups.
We experimented with lexical features, which
are commonly used in token-level tagging prob-
lems. Results, however, were slightly lower than
the results reported in this section. We suspect the
issue is one of overfitting, due to the limited size of
our training data, and general sparsity associated
with lexical features. Continuous word presenta-
tions (Mikolov et al., 2013), noted as future work,
may offer improved generalization.
Error analysis for our machine learning mod-
els suggests patterns similar to those reported in
Section 3.2. Although errors are significantly less
frequent than in our dictionary baseline, shorter
words still present the most difficulty. We note
as future work the use of word-level LM scores
to target errors with shorter words.
3.4 Search Results
Recall that we translate multilingual queries into
English prior to search. KLEs and homoglyphs
in queries result in poor query translations, often
leading to null search results.
To evaluate the impact of KLE and homoglyph
correction, we consider a set of 100k randomly se-
lected Russian/English queries. We consider the
subset of queries that the RF or baseline models
predict as containing a KLE or homoglyph. Next,
we translate into English both the original query,
as well as a transformed version of it, with KLEs
and homoglyphs replaced with their ASCII map-
pings. Lastly, we execute independent searches
using original and transformed query translations.
Table 3 provides details on search results for
original and transformed queries. The baseline
model transforms over 12.6% of the 100k queries.
Of those, 24.3% yield search results where the un-
modified queries had null search results (i.e. Null
? Non-null). In 20.9% of the cases, however, the
624
transformations are destructive (i.e. Non-null ?
Null), and yield null results where the unmodified
query produced results.
Compared with the baseline, the RF model
transforms only 7.4% of the 100k queries; a frac-
tion that is roughly in line with the 7.8% of queries
in our test set that contain KLEs or homoglyphs.
In over 42% of the cases (versus 24.3% for the
baseline), the transformed query generates search
results where the original query yields none. Only
4.81% of the transformations using the RF model
are destructive; a fraction significantly lower than
the baseline.
Note that we distinguish here only between
queries that produce null results, and those that do
not. We do not include queries for which original
and transformed queries both produce (potentially
differing) search results. Evaluating these cases
requires deeper insight into the relevance of search
results, which is left as future work.
Baseline RF model
#Transformed 12,661 7,364
Null? Non-Null 3,078 (24.3%) 3,142 (42.7%)
Non-Null? Null 2,651 (20.9%) 354 (4.81%)
Table 3: Impact of KLE and homoglyph correction
on search results for 100k queries
4 Related Work
Baytin et al. (2013) first refer to keyboard lay-
out errors in their work. However, their focus is
on predicting the performance of spell-correction,
not on fixing KLEs observed in their data. To
our knowledge, our work is the first to introduce
this problem and to propose a machine learning
solution. Since our task is a token-level tagging
problem, it is very similar to the part-of-speech
(POS) tagging task (Ratnaparkhi, 1996), only with
a very small set of candidate tags. We chose
a supervised machine learning approach in order
to achieve maximum precision. However, this
problem can also be approached in an unsuper-
vised setting, similar to the methodWhitelaw et al.
(2009) use for spelling correction. In that setup,
the goal would be to directly choose the correct
transformation for an ill-formed KLE or homo-
glyph, instead of a tagging step followed by a de-
terministic mapping to ASCII.
5 Conclusions and Future Work
We investigate two kinds of errors in search
queries: keyboard layout errors (KLEs) and ho-
moglyphs. Applying machine learning methods,
we are able to accurately identify a user?s intended
query, in spite of the presence of KLEs and ho-
moglyphs. The proposed models are based largely
on compact, character-level language models. The
proposed techniques, when applied to multilingual
queries prior to translation and search, offer signif-
icant gains in search results.
In the future, we plan to focus on additional fea-
tures to improve KLE and homoglyph discrimina-
tion for shorter words and acronyms. Although
lexical features did not prove useful for this work,
presumably due to data sparsity and overfitting
issues, we intend to explore the application of
continuous word representations (Mikolov et al.,
2013). Compared with lexical features, we expect
continuous representations to be less susceptible
to overfitting, and to generalize better to unknown
words. For instance, using continuous word rep-
resentations, Turian et al. (2010) show significant
gains for a named entity recognition task.
We also intend on exploring the use of features
from in-domain, word-level LMs. Word-level fea-
tures are expected to be particularly useful in the
case of spurious mappings (e.g. ????? vs. ?dfp?
from Section 3.2), where context from surround-
ing tokens in a query can often help in resolving
ambiguity. Word-level features may also be useful
in re-ranking translated queries prior to search, in
order to reduce the incidence of erroneous query
transformations generated through our methods.
Finally, our future work will explore KLE and ho-
moglyph correction bidirectionally, as opposed to
the unidirectional approach explored in this work.
Acknowledgments
We would like to thank Jean-David Ruvini, Mike
Dillinger, Sa?sa Hasan, Irina Borisova and the
anonymous reviewers for their valuable feedback.
We also thank our Russian language special-
ists Tanya Badeka, Tatiana Kontsevich and Olga
Pospelova for their support in labeling and review-
ing datasets.
References
Alexey Baytin, Irina Galinskaya, Marina Panina, and
Pavel Serdyukov. 2013. Speller performance pre-
625
diction for query autocorrection. In Proceedings
of the 22nd ACM International Conference on Con-
ference on Information & Knowledge Management,
pages 1821?1824.
Tina M. Lowrey, Larry J. Shrum, and Tony M. Du-
bitsky. 2013. The Relation Between Brand-name
Linguistic Characteristics and Brand-nameMemory.
Journal of Advertising, 32(3):7?17.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tristan Miller. 2013. Russian?English Homoglyphs,
Homographs, and Homographic Translations. Word
Ways: The Journal of Recreational Linguistics,
46(3):165?168.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part?of?Speech Tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 133?142.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384?394.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung,
and Ged Ellis. 2009. Using the Web for Lan-
guage Independent Spellchecking and Autocorrec-
tion. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 890?899.
626
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 293?296,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Movie Reviews and Revenues: An Experiment in Text Regression?
Mahesh Joshi Dipanjan Das Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{maheshj,dipanjan,kgimpel,nasmith}@cs.cmu.edu
Abstract
We consider the problem of predicting a
movie?s opening weekend revenue. Previous
work on this problem has used metadata about
a movie?e.g., its genre, MPAA rating, and
cast?with very limited work making use of
text about the movie. In this paper, we use
the text of film critics? reviews from several
sources to predict opening weekend revenue.
We describe a new dataset pairing movie re-
views with metadata and revenue data, and
show that review text can substitute for meta-
data, and even improve over it, for prediction.
1 Introduction
Predicting gross revenue for movies is a problem
that has been studied in economics, marketing,
statistics, and forecasting. Apart from the economic
value of such predictions, we view the forecasting
problem as an application of NLP. In this paper, we
use the text of critics? reviews to predict opening
weekend revenue. We also consider metadata for
each movie that has been shown to be successful for
similar prediction tasks in previous work.
There is a large body of prior work aimed at pre-
dicting gross revenue of movies (Simonoff and Spar-
row, 2000; Sharda and Delen, 2006; inter alia). Cer-
tain information is used in nearly all prior work on
these tasks, such as the movie?s genre, MPAA rating,
running time, release date, the number of screens on
which the movie debuted, and the presence of partic-
ular actors or actresses in the cast. Most prior text-
based work has used automatic text analysis tools,
deriving a small number of aggregate statistics. For
example, Mishne and Glance (2006) applied sen-
timent analysis techniques to pre-release and post-
release blog posts about movies and showed higher
?We appreciate reviewer feedback and technical advice
from Brendan O?Connor. This work was supported by NSF IIS-
0803482, NSF IIS-0844507, and DARPA NBCH-1080004.
correlation between actual revenue and sentiment-
based metrics, as compared to mention counts of the
movie. (They did not frame the task as a revenue
prediction problem.) Zhang and Skiena (2009) used
a news aggregation system to identify entities and
obtain domain-specific sentiment for each entity in
several domains. They used the aggregate sentiment
scores and mention counts of each movie in news
articles as predictors.
While there has been substantial prior work on
using critics? reviews, to our knowledge all of this
work has used polarity of the review or the number
of stars given to it by a critic, rather than the review
text directly (Terry et al, 2005).
Our task is related to sentiment analysis (Pang et
al., 2002) on movie reviews. The key difference is
that our goal is to predict a future real-valued quan-
tity, restricting us from using any post-release text
data such as user reviews. Further, the most im-
portant clues about revenue may have little to do
with whether the reviewer liked the movie, but rather
what the reviewer found worth mentioning. This pa-
per is more in the tradition of Ghose et al (2007) and
Kogan et al (2009), who used text regression to di-
rectly quantify review ?value? and make predictions
about future financial variables, respectively.
Our aim in using the full text is to identify partic-
ular words and phrases that predict the movie-going
tendencies of the public. We can also perform syn-
tactic and semantic analysis on the text to identify
richer constructions that are good predictors. Fur-
thermore, since we consider multiple reviews for
each movie, we can compare these features across
reviews to observe how they differ both in frequency
and predictive performance across different media
outlets and individual critics.
In this paper, we use linear regression from text
and non-text (meta) features to directly predict gross
revenue aggregated over the opening weekend, and
the same averaged per screen.
293
Domain train dev test total
Austin Chronicle 306 94 62 462
Boston Globe 461 154 116 731
LA Times 610 2 13 625
Entertainment Weekly 644 208 187 1039
New York Times 878 273 224 1375
Variety 927 297 230 1454
Village Voice 953 245 198 1396
# movies 1147 317 254 1718
Table 1: Total number of reviews from each domain for
the training, development and test sets.
2 Data
We gathered data for movies released in 2005?2009.
For these movies, we obtained metadata and a list
of hyperlinks to movie reviews by crawling Meta-
Critic (www.metacritic.com). The metadata
include the name of the movie, its production house,
the set of genres it belongs to, the scriptwriter(s),
the director(s), the country of origin, the primary
actors and actresses starring in the movie, the re-
lease date, its MPAA rating, and its running time.
From The Numbers (www.the-numbers.com),
we retrieved each movie?s production budget, open-
ing weekend gross revenue, and the number of
screens on which it played during its opening week-
end. Only movies found on both MetaCritic and The
Numbers were included.
Next we chose seven review websites that most
frequently appeared in the review lists for movies at
Metacritic, and obtained the text of the reviews by
scraping the raw HTML. The sites chosen were the
Austin Chronicle, the Boston Globe, the LA Times,
Entertainment Weekly, the New York Times, Vari-
ety, and the Village Voice. We only chose those
reviews that appeared on or before the release date
of the movie (to ensure that revenue information is
not present in the review), arriving at a set of 1718
movies with at least one review. We partitioned this
set of movies temporally into training (2005?2007),
development (2008) and test (2009) sets. Not all
movies had reviews at all sites (see Table 1).
3 Predictive Task
We consider two response variables, both in
U.S. dollars: the total revenue generated by a movie
during its release weekend, and the per screen rev-
enue during the release weekend. We evaluate these
predictions using (1) mean absolute error (MAE) in
U.S. dollars and (2) Pearson?s correlation between
the actual and predicted revenue.
We use linear regression to directly predict the
opening weekend gross earnings, denoted y, based
on features x extracted from the movie metadata
and/or the text of the reviews. That is, given an input
feature vector x ? Rp, we predict an output y? ? R
using a linear model: y? = ?0 + x>?. To learn val-
ues for the parameters ? = ??0,??, the standard
approach is to minimize the sum of squared errors
for a training set containing n pairs ?xi, yi? where
xi ? Rp and yi ? R for 1 ? i ? n:
?? = argmin
?=(?0,?)
1
2n
n?
i=1
(
yi ? (?0 + x>i ?)
)2
+?P (?)
A penalty term P (?) is included in the objective for
regularization. Classical solutions use an `2 or `1
norm, known respectively as ridge and lasso regres-
sion. Introduced recently is a mixture of the two,
called the elastic net (Zou and Hastie, 2005):
P (?) =
?p
j=1
(
1
2(1? ?)?
2
j + ?|?j |
)
where ? ? (0, 1) determines the trade-off be-
tween `1 and `2 regularization. For our experi-
ments we used the elastic net and specifically the
glmnet package which contains an implementa-
tion of an efficient coordinate ascent procedure for
training (Friedman et al, 2008).
We tune the ? and ? parameters on our develop-
ment set and select the model with the ??, ?? com-
bination that yields minimum MAE on the develop-
ment set.
4 Experiments
We compare predictors based on metadata, predic-
tors based on text, and predictors that use both kinds
of information. Results for two simple baselines of
predicting the training set mean and median are re-
ported in Table 2 (Pearson?s correlation is undefined
since the standard deviation is zero).
4.1 Metadata Features
We considered seven types of metadata features, and
evaluated their performance by adding them to our
pool of features in the following order: whether the
294
film is of U.S. origin, running time (in minutes), the
logarithm of its budget, # opening screens, genre
(e.g., Action, Comedy) and MPAA rating (e.g., G,
PG, PG-13), whether the movie opened on a holiday
weekend or in summer months, total count as well as
of presence of individual Oscar-winning actors and
directors and high-grossing actors. For the first task
of predicting the total opening weekend revenue of
a movie, the best-performing feature set in terms of
MAE turned out to be all the features. However, for
the second task of predicting the per screen revenue,
addition of the last feature subset consisting of infor-
mation related to the actors and directors hurt perfor-
mance (MAE increased). Therefore, for the second
task, the best performing set contained only the first
six types of metadata features.
4.2 Text Features
We extract three types of text features (described be-
low). We only included feature instances that oc-
curred in at least five different movies? reviews. We
stem and downcase individual word components in
all our features.
I. n-grams. We considered unigrams, bigrams, and
trigrams. A 25-word stoplist was used; bigrams
and trigrams were only filtered if all words were
stopwords.
II. Part-of-speech n-grams. As with words, we
added unigrams, bigrams, and trigrams. Tags
were obtained from the Stanford part-of-speech
tagger (Toutanova and Manning, 2000).
III. Dependency relations. We used the Stanford
parser (Klein and Manning, 2003) to parse the
critic reviews and extract syntactic dependen-
cies. The dependency relation features consist
of just the relation part of a dependency triple
?relation, head word, modifier word?.
We consider three ways to combine the collec-
tion of reviews for a given movie. The first (???)
simply concatenates all of a movie?s reviews into
a single document before extracting features. The
second (?+?) conjoins each feature with the source
site (e.g., New York Times) from whose review it was
extracted. A third version (denoted ?B?) combines
both the site-agnostic and site-specific features.
Features Site
Total Per Screen
MAE MAE
($M) r ($K) r
Predict mean 11.672 ? 6.862 ?
Predict median 10.521 ? 6.642 ?
m
et
a
Best 5.983 0.722 6.540 0.272
te
xt
I
? 8.013 0.743 6.509 0.222
+ 7.722 0.781 6.071 0.466
see Tab. 3 B 7.627 0.793 6.060 0.411
I ? II
? 8.060 0.743 6.542 0.233
+ 7.420 0.761 6.240 0.398
B 7.447 0.778 6.299 0.363
I ? III
? 8.005 0.744 6.505 0.223
+ 7.721 0.785 6.013 0.473
B 7.595 0.796 ?6.010 0.421
m
et
a
?
te
xt
I
? 5.921 0.819 6.509 0.222
+ 5.757 0.810 6.063 0.470
B 5.750 0.819 6.052 0.414
I ? II
? 5.952 0.818 6.542 0.233
+ 5.752 0.800 6.230 0.400
B 5.740 0.819 6.276 0.358
I ? III
? 5.921 0.819 6.505 0.223
+ 5.738 0.812 6.003 0.477
B 5.750 0.819 ?5.998 0.423
Table 2: Test-set performance for various models, mea-
sured using mean absolute error (MAE) and Pearson?s
correlation (r), for two prediction tasks. Within a column,
boldface shows the best result among ?text? and ?meta ?
text? settings. ?Significantly better than the meta baseline
with p < 0.01, using the Wilcoxon signed rank test.
4.3 Results
Table 2 shows our results for both prediction tasks.
For the total first-weekend revenue prediction task,
metadata features baseline result (r2 = 0.521) is
comparable to that reported by Simonoff and Spar-
row (2000) on a similar task of movie gross predic-
tion (r2 = 0.446). Features from critics? reviews
by themselves improve correlation on both predic-
tion tasks, however improvement in MAE is only
observed for the per screen revenue prediction task.
A combination of the meta and text features
achieves the best performance both in terms of MAE
and r. While the text-only models have some high
negative weight features, the combined models do
not have any negatively weighted features and only
a very few metadata features. That is, the text is able
to substitute for the other metadata features.
Among the different types of text-based features
that we tried, lexical n-grams proved to be a strong
baseline to beat. None of the ?I ? ?? feature sets are
significantly better than n-grams alone, but adding
295
the dependency relation features (set III) to the n-
grams does improve the performance enough to
make it significantly better than the metadata-only
baseline for per screen revenue prediction.
Salient Text Features: Table 3 lists some of the
highly weighted features, which we have catego-
rized manually. The features are from the text-only
model annotated in Table 2 (total, not per screen).
The feature weights can be directly interpreted as
U.S. dollars contributed to the predicted value y? by
each occurrence of the feature. Sentiment-related
features are not as prominent as might be expected,
and their overall proportion in the set of features
with non-zero weights is quite small (estimated in
preliminary trials at less than 15%). Phrases that
refer to metadata are the more highly weighted
and frequent ones. Consistent with previous re-
search, we found some positively-oriented sentiment
features to be predictive. Some other prominent
features not listed in the table correspond to spe-
cial effects (?Boston Globe: of the art?, ?and cgi?),
particular movie franchises (?shrek movies?, ?Vari-
ety: chronicle of?, ?voldemort?), hype/expectations
(?blockbuster?, ?anticipation?), film festival (?Vari-
ety: canne? with negative weight) and time of re-
lease (?summer movie?).
5 Conclusion
We conclude that text features from pre-release re-
views can substitute for and improve over a strong
metadata-based first-weekend movie revenue pre-
diction. The dataset used in this paper has been
made available for research at http://www.
ark.cs.cmu.edu/movie$-data.
References
J. Friedman, T. Hastie, and R. Tibshirani. 2008. Regular-
ized paths for generalized linear models via coordinate
descent. Technical report, Stanford University.
A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007.
Opinion mining using econometrics: A case study on
reputation systems. In Proc. of ACL.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of NAACL, pages 272?280.
Feature Weight ($M)
ra
ti
ng
pg +0.085
New York Times: adult -0.236
New York Times: rate r -0.364
se
qu
el
s this series +13.925
LA Times: the franchise +5.112
Variety: the sequel +4.224
pe
op
le Boston Globe: will smith +2.560
Variety: brittany +1.128
? producer brian +0.486
ge
nr
e
Variety: testosterone +1.945
Ent. Weekly: comedy for +1.143
Variety: a horror +0.595
documentary -0.037
independent -0.127
se
nt
im
en
t Boston Globe: best parts of +1.462
Boston Globe: smart enough +1.449
LA Times: a good thing +1.117
shame $ -0.098
bogeyman -0.689
pl
ot
Variety: torso +9.054
vehicle in +5.827
superhero $ +2.020
Table 3: Highly weighted features categorized manu-
ally. ? and $ denote sentence boundaries. ?brittany?
frequently refers to Brittany Snow and Brittany Murphy.
?? producer brian? refers to producer Brian Grazer (The
Da Vinci Code, among others).
G. Mishne and N. Glance. 2006. Predicting movie sales
from blogger sentiment. In AAAI Spring Symposium
on Computational Approaches to Analysing Weblogs.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79?86.
R. Sharda and D. Delen. 2006. Predicting box office suc-
cess of motion pictures with neural networks. Expert
Systems with Applications, 30(2):243?254.
J. S. Simonoff and I. R. Sparrow. 2000. Predicting movie
grosses: Winners and losers, blockbusters and sleep-
ers. Chance, 13(3):15?24.
N. Terry, M. Butler, and D. De?Armond. 2005. The de-
terminants of domestic box office performance in the
motion picture industry. Southwestern Economic Re-
view, 32:137?148.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63?70.
W. Zhang and S. Skiena. 2009. Improving movie gross
prediction through news analysis. In Web Intelligence,
pages 301?304.
H. Zou and T. Hastie. 2005. Regularization and variable
selection via the elastic net. Journal Of The Royal Sta-
tistical Society Series B, 67(5):768?768.
296
Proceedings of NAACL-HLT 2013, pages 685?690,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
What?s in a Domain? Multi-Domain Learning for Multi-Attribute Data
Mahesh Joshi? Mark Dredze? William W. Cohen? Carolyn P. Rose??
? School of Computer Science, Carnegie Mellon University
Pittsburgh, PA, 15213, USA
? Human Language Technology Center of Excellence, Johns Hopkins University
Baltimore, MD, 21211, USA
maheshj@cs.cmu.edu,mdredze@cs.jhu.edu
wcohen@cs.cmu.edu,cprose@cs.cmu.edu
Abstract
Multi-Domain learning assumes that a sin-
gle metadata attribute is used in order to di-
vide the data into so-called domains. How-
ever, real-world datasets often have multi-
ple metadata attributes that can divide the
data into domains. It is not always apparent
which single attribute will lead to the best do-
mains, and more than one attribute might im-
pact classification. We propose extensions to
two multi-domain learning techniques for our
multi-attribute setting, enabling them to si-
multaneously learn from several metadata at-
tributes. Experimentally, they outperform the
multi-domain learning baseline, even when it
selects the single ?best? attribute.
1 Introduction
Multi-Domain Learning (Evgeniou and Pontil,
2004; Daume? III, 2007; Dredze and Crammer, 2008;
Finkel and Manning, 2009; Zhang and Yeung, 2010;
Saha et al, 2011) algorithms learn when training in-
stances are spread across many domains, which im-
pact model parameters. These algorithms use exam-
ples from each domain to learn a general model that
is also sensitive to individual domain differences.
However, many data sets include a host of meta-
data attributes, many of which can potentially define
the domains to use. Consider the case of restaurant
reviews, which can be categorized into domains cor-
responding to the cuisine, location, price range, or
several other factors. For multi-domain learning, we
should use the metadata attribute most likely to char-
acterize a domain: a change in vocabulary (i.e. fea-
tures) that most impacts the classification decision
(Ben-David et al, 2009). This choice is not easy.
First, we may not know which metadata attribute is
most likely to fit this role. Perhaps the location most
impacts the review language, but it could easily be
the price of the meal. Second, multiple metadata
attributes could impact the classification decision,
and picking a single one might reduce classification
accuracy. Therefore, we seek multi-domain learn-
ing algorithms which can simultaneously learn from
many types of domains (metadata attributes).
We introduce the multi-attribute multi-domain
(MAMD) learning problem, in which each learning
instance is associated with multiple metadata at-
tributes, each of which may impact feature behavior.
We present extensions to two popular multi-domain
learning algorithms, FEDA (Daume? III, 2007) and
MDR (Dredze et al, 2009). Rather than selecting
a single domain division, our algorithms consider
all attributes as possible distinctions and discover
changes in features across attributes. We evaluate
our algorithms using two different data sets ? a data
set of restaurant reviews (Chahuneau et al, 2012),
and a dataset of transcribed speech segments from
floor debates in the United States Congress (Thomas
et al, 2006). We demonstrate that multi-attribute al-
gorithms improve over their multi-domain counter-
parts, which can learn distinctions from only a single
attribute.
2 MAMD Learning
In multi-domain learning, each instance x is drawn
from a domain d with distribution x ? Dd over a
vectors space RD and labeled with a domain spe-
cific function fd with label y ? {?1,+1} (for bi-
nary classification). In multi-attribute multi-domain
685
(MAMD) learning, we have M metadata attributes in
a data set, where the mth metadata attribute has Km
possible unique values which represent the domains
induced by that metadata attribute. Each instance xi
is drawn from a distribution xi ? Da specific to a
set of attribute values Ai associated with each in-
stance. Additionally, each unique set of attributes
indexes a function fA.1 Ai could contain a value for
each attribute, or no values for any attribute (which
would index a domain-agnostic ?background? distri-
bution and labeling function). Just as a domain can
change a feature?s probability and behavior, so can
each metadata attribute.
Examples of data for MAMD learning abound. The
commonly used Amazon product reviews data set
(Blitzer et al, 2007) only includes product types, but
the original reviews can be attributed with author,
product price, brand, and so on. Additional exam-
ples include congressional floor debate records (e.g.
political party, speaker, bill) (Joshi et al, 2012). In
this paper, we use restaurant reviews (Chahuneau et
al., 2012), which have upto 20 metadata attributes
that define domains, and congressional floor de-
bates, with two attributes that define domains.
It is difficult to apply multi-domain learning algo-
rithms when it is unclear which metadata attribute
to choose for defining the ?domains?. It is possible
that there is a single ?best? attribute to use for defin-
ing domains, one that when used in multi-domain
learning will yield the best classifier. To find this
attribute, one must rely on one?s intuition about the
problem,2 or perform an exhaustive empirical search
over all attributes using some validation set. Both
these strategies can be brittle, because as the nature
of data changes over time so may the ?best? do-
main distinction. Additionally, multi-domain learn-
ing was not designed to benefit from multiple helpful
attributes.
We note here that Eisenstein et al (2011), as well
as Wang et al (2012), worked with a ?multifaceted
topic model? using the framework of sparse addi-
tive generative models (SAGE). Both those models
capture interactions between topics and multiple as-
1Distributions and functions that share attributes could share
parameters.
2Intuition is often critical for learning and in some cases can
help, such as in the Amazon product reviews data set, where
product type clearly corresponds to domain. However, for other
data sets the choice may be less clear.
pects, and can be adapted to the case of MAMD. While
our problem formulation has significant conceptual
overlap with the SAGE?like multifaceted topic mod-
els framework, our proposed methods are motivated
from a fast online learning perspective.
A naive approach for MAMD would be to treat ev-
ery unique set of attributes as a domain, including
unique proper subsets of different attributes to ac-
count for the case of missing attributes in some in-
stances.3 However, introducing an exponential num-
ber of domains requires a similar increase in train-
ing data, clearly an infeasible requirement. Instead,
we develop multi-attribute extensions for two multi-
domain learning algorithms, such that the increase
in parameters is linear in the number of metadata at-
tributes, and no special handling is required for the
case where some metadata attributes might be miss-
ing from an instance.
Multi-Attribute FEDA The key idea behind
FEDA (Daume? III, 2007) is to encode each domain
using its own parameters, one per feature. FEDA
maps a feature vector x in RD to RD(K+1). This
provides a separate parameter sub-space for every
domain k ? 1 . . .K, and also maintains a domain-
agnostic shared sub-space. Essentially, each feature
is duplicated for every instance in the appropriate
sub-space of RD(K+1) that corresponds to the in-
stance?s domain. We extend this idea to the MAMD
setting by using one parameter per attribute value.
The original instance x ? RD is now mapped into
RD(1+
?
mKm); a separate parameter for each at-
tribute value and a shared set of parameters. In ef-
fect, for every metadata attribute a ? Ai, the original
features are copied into the appropriate sub-space.
This grows linearly with the number of metadata at-
tribute values, as opposed to exponentially in our
naive solution. While this is still substantial growth,
each instance retains the same feature sparsity as in
the original input space. In this new setup, FEDA al-
lows an instance to contribute towards learning the
shared parameters, and the attribute-specific param-
eters for all the attributes present on an instance. Just
like multi-domain FEDA, any supervised learning al-
gorithm can be applied to the transformed represen-
tation.
3While we used a similar setup for formulating our problem,
we did not rule out the potential for factoring the distributions.
686
Multi-Attribute MDR We make a similar change
to MDR (Dredze et al, 2009) to extend it for
the MAMD setting. In the original formulation,
Dredze et al used confidence-weighted (CW)
learning (Dredze et al, 2008) for learning shared
and domain-specific classifiers, which are combined
based on the confidence scores associated with the
feature weights. For training the MDR approaches in
a multi-domain learning setup, they found that com-
puting updates for the combined classifier and then
equally distributing them to the shared and domain-
specific classifiers was the best strategy, although it
approximated the true objective that they aimed to
optimize. In our multi-attribute setup confidence-
weighted (CW) classifiers are learned for each of the
?
mKm attribute values in addition to a shared CW
classifier. At classification time, a combined clas-
sifier is computed for every instance. However, in-
stead of combining the shared classifier and a single
domain-specific classifier, we combine the shared
CW classifier and |Ai| different attribute value-
specific CW classifiers associated with xi. The
combined classifier is found by minimizing the KL-
divergence of the combined classifier with respect to
each of the underlying classifiers.4
When learning the shared and domain-specific
classifiers, we follow the best result in Dredze et
al. and use the ?averaged update? strategy (?7.3 in
Dredze et al), where updates are computed for the
combined classifier, and are then distributed to the
shared and domain-specific classifiers. MDR-U will
indicate that the updates to the combined classifiers
are uniformly distributed to the underlying shared
and domain-specific classifiers.
Dredze et al also used another scheme called
?variance? to distribute the combined update to the
underlying classifiers (?4, last paragraph in Dredze
et al) Their idea was to give a lower portion
of the update to the underlying classifier that has
higher variance (or in their terminology, ?less con-
fidence?) since it contributed less to the combined
classifier. We refer to this as MDR-V. However, this
conflicts with the original CW intuition that features
with higher variance (lower confidence) should re-
ceive higher updates; since they are more in need
of change. Therefore, we implemented a modi-
fied ?variance? scheme, where the updates are dis-
4We also tried the l2 distance method of Dredze et al (2009)
but it gave consistently worse results.
tributed to the underlying classifiers such that higher
variance features receive the larger updates. We re-
fer to this as MDR-NV. We observed significant im-
provements with this modified scheme.
3 Experiments
To evaluate our multi-attribute algorithms we con-
sider two datasets. First, we use two subsets of the
restaurant reviews dataset (1,180,308 reviews) intro-
duced by Chahuneau et al (2012) with the goal of
labeling reviews as positive or negative. The first
subset (50K-RND) randomly selects 50,000 reviews
while the second (50K-BAL) is a class-balanced
sample. Following the approach of Blitzer et al
(2007), scores above and below 3-stars indicated
positive and negative reviews, while 3-star reviews
were discarded. Second, we use the transcribed seg-
ments of speech from the United States Congress
floor debates (Convote), introduced by Thomas
et al (2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discussion
in the floor debate.
In the WordSalad datasets, each restaurant re-
view can have many metadata attributes, including a
unique identifier, name (which may not be unique),
address (we extract the zipcode), and type (Italian,
Chinese, etc.). We select the 20 most common meta-
data attributes (excluding latitude, longitude, and the
average rating). 5 In the Convote dataset, each
speech segment is associated with the political party
affiliation of the speaker (democrat, independent, or
republican) and the speaker identifier (we use bill
identifiers for creating folds in our 10-fold cross-
validation setup).
In addition to our new algorithms, we evalu-
ate several baselines. All methods use confidence-
weighted (CW) learning (Crammer et al, 2012).
BASE A single classifier trained on all the data,
and which ignores metadata attributes and uses uni-
gram features. For CW, we use the best-performing
setting from Dredze et al (2008) ? the ?variance?
algorithm, which computes approximate but closed?
form updates, which also lead to faster learning. Pa-
rameters are tuned over a validation set within each
training fold.
5Our method requires categorical metadata attributes, al-
though real-valued attributes can be discretized.
687
metadata 1-META FEDA MDR-U MDR-V MDR-NV
5
0
K
-
R
N
D NONE (BASE) 92.29 (?0.14)
ALL (META) ? 92.69 (?0.10)
CATEGORY ? 92.48 (?0.11) 92.47 (?0.10) ?? 92.99 (?0.12) 91.16 (?0.16) ?? 93.24 (?0.13)
ZIPCODE 92.40 (?0.09) ? 92.73 (?0.09) ?? 92.99 (?0.12) 91.19 (?0.20) ?? 93.22 (?0.11)
NEIGHBORHOOD 92.42 (?0.11) ? 92.65 (?0.13) ?? 93.02 (?0.13) 91.17 (?0.21) ?? 93.21 (?0.12)
5
0
K
-
B
A
L NONE (BASE) 89.95 (?0.10)
ALL (META) ? 90.39 (?0.09)
CATEGORY 90.09 (?0.11) ? 90.50 (?0.11) ? 90.60 (?0.11) 87.89 (?0.13) ?? 91.33 (?0.08)
ZIPCODE 89.97 (?0.12) ? 90.42 (?0.13) ? 90.56 (?0.09) 87.78 (?0.16) ?? 91.30 (?0.10)
ID ? 90.42 (?0.11) ?? 90.64 (?0.11) ? 90.50 (?0.11) 87.78 (?0.25) ?? 91.27 (?0.09)
Table 1: Average accuracy (? standard error) for the best three metadata attributes, when using a single attribute at
a time. Results that are numerically the best within a row are in bold. Results significantly better than BASE are
marked with ?, and better than META are marked with ?. Significance is measured using a two-tailed paired t-test with
? = 0.05.
#attributes FEDA MDR-U MDR-V MDR-NV
5
0
K
-
R
N
D MAMD ?? 93.07 (?0.19) ?? 93.12 (?0.11) 87.08 (?1.72) ?? 93.19 (?0.12)
1-ORCL ?? 93.06 (?0.11) ?? 93.17 (?0.11) 92.37 (?0.11) ?? 93.39 (?0.12)
1-TUNE ? 92.64 (?0.12) ? 92.81 (?0.16) 92.15 (?0.17) ?? 93.07 (?0.14)
1-MEAN ? 92.61 (?0.09) ? 92.59 (?0.10) 91.41 (?0.12) ? 92.58 (?0.10)
5
0
K
-
B
A
L MAMD ?? 91.42 (?0.09) ?? 91.06 (?0.04) 81.43 (?2.79) ?? 91.40 (?0.08)
1-ORCL ?? 90.89 (?0.10) ?? 90.87 (?0.11) 89.33 (?0.13) ?? 91.45 (?0.07)
1-TUNE ? 90.33 (?0.10) ?? 90.70 (?0.14) 89.13 (?0.16) ?? 91.26 (?0.08)
1-MEAN ? 90.30 (?0.06) 89.92 (?0.07) 88.25 (?0.07) 90.06 (?0.08)
Table 2: Average accuracy (? standard error) using 10-fold cross-validation for methods that use all attributes, either
directly (our proposed methods) or for selecting the ?best? single attribute using one of the strategies described earlier.
Formatting and significance symbols are the same as in Table 1.
META Identical to BASE with a unique bias feature
added for each attribute value (Joshi et al, 2012).
1-META A special case of META where a unique
bias feature is added only for a single attribute.
To use multi-domain learning directly, we could
select a single attribute as the domain. We consider
several strategies for picking this attribute and eval-
uate both FEDA and MDR in this setting.
1-MEAN Choose an attribute randomly, equivalent
to the expected (mean) error over all attributes.
1-TUNE Select the best performing attribute on a
validation set.
1-ORCL Select the best performing attribute on
the test set. Though impossible in practice, this gives
the oracle upper bound on multi-domain learning.
All experiments use ten-fold cross-validation. We
report the mean accuracy, along with standard error.
4 Results
Table 1 shows the results of single-attribute multi-
domain learning methods for the WordSalad
datasets. The table shows the three best-performing
metadata attributes (as decided by the highest accu-
racy among all the methods across all 20 metadata
attributes). Clearly, several of the attributes can pro-
vide meaningful domains, which demonstrates that
methods that can select multiple attributes at once
are desirable. We also see that our modification to
MDR (MDR-NV) works the best.
Table 3 shows the results of single-attribute multi-
domain learning methods for the Convote dataset.
The first observation to be made on this dataset is
that neither the PARTY, nor the SPEAKER attribute
individually achieve significant improvement over
the META baseline, which uses both these attributes
as features. This is in contrast with the results on
the WordSalad dataset, where some attributes by
themselves showed an improvement over the META
baseline. Thus, this dataset represents a more chal-
lenging setup for our multi?attribute multi?domain
learning methods ? they need to exploit the two
weak attributes simultaneously.
We next demonstrate multi-attribute improve-
ments over the multi-domain baselines (Tables 2
and 4). For WordSalad datasets, our exten-
sions that can use all metadata attributes simul-
taneously are consistently better than both the
1-MEAN and the 1-TUNE strategies (except for
the case of the old variance scheme used by
(Dredze et al, 2009)). For the skewed subset
688
metadata 1-META FEDA MDR-U MDR-V MDR-NV
NONE (BASE) 67.08 (?1.74)
ALL (META) ? 82.60 (?1.95)
PARTY ? 78.81 (?1.47) ? 84.19 (?2.44) ? 83.23 (?2.48) ? 81.38 (?2.22) ? 83.92 (?2.31)
SPEAKER ? 77.49 (?1.75) ? 82.88 (?2.43) ? 78.32 (?1.91) 62.43 (?2.20) ? 72.26 (?1.37)
Table 3: Convote: Average accuracy (? standard error) when using a single attribute at a time. Results that are
numerically the best within a row are in bold. Results significantly better than BASE are marked with ?, and better
than META are marked with ?. Significance is measured using a two-tailed paired t-test with ? = 0.05.
#attributes FEDA MDR-U MDR-V MDR-NV
MAMD ?? 85.71 (?2.74) ? 84.12 (?2.56) 50.44 (?1.78) ?? 86.19 (?2.49)
1-ORCL ? 84.77 (?2.47) ? 83.88 (?2.27) ? 81.38 (?2.22) ? 83.92 (?2.31)
1-TUNE ? 84.19 (?2.44) ? 83.23 (?2.48) ? 81.38 (?2.22) ? 83.92 (?2.31)
1-MEAN ? 83.53 (?2.40) ? 80.77 (?1.92) ? 71.91 (?1.82) ? 78.09 (?1.69)
Table 4: Convote: Average accuracy (? standard error) using 10-fold cross-validation for methods that use all
attributes, either directly (our proposed methods) or for selecting the ?best? single attribute using one of the strategies
described earlier. Formatting and significance symbols are the same as in Table 3.
50K-RND, MAMD+FEDA is significantly better than
1-TUNE+FEDA; MAMD+MDR-U is significantly bet-
ter than 1-TUNE+MDR-U; MAMD+MDR-NV is not
significantly different from 1-TUNE+MDR-U. For
the balanced subset 50K-BAL, a similar pattern
holds, except that MAMD+MDR-NV is significantly
better than 1-TUNE+MDR-NV. Clearly, our multi-
attribute algorithms provide a benefit over existing
approaches. Even with oracle knowledge of the test
performance using multi-domain learning, we can
still obtain improvements (FEDA and MDR-U in the
50K-BAL set, and all the Convote results, except
MDR-V).
Although MAMD+MDR-NV is not significantly bet-
ter than 1-TUNE+MDR-NV on the 50K-RND set,
we found that in every single fold in our ten-
fold cross-validation experiments, the ?best? single
metadata attribute decided using a validation set did
not match the best-performing single metadata at-
tribute on the corresponding test set. This shows
the potential instability of choosing a single best at-
tribute. Also, note that MDR-NV is a variant that we
have proposed in the current work, and in fact for
the earlier variant of MDR (MDR-U), as well as for
FEDA, we do see significant improvements when us-
ing all metadata attributes. Furthermore, the compu-
tational cost of evaluating every metadata attribute
independently to tune the single best metadata at-
tribute can be high and often impractical. Our ap-
proach requires no such tuning. Finally, observe
that for FEDA, the 1-TUNE strategy is not signifi-
cantly different from 1-MEAN, which just randomly
picks a single best metadata attribute. For MDR-U,
1-TUNE is significantly better than 1-MEAN on the
balanced subset 50K-BAL, but not on the skewed
subset 50K-RND.
As mentioned earlier, the Convote dataset is a
challenging setting for our methods due to the fact
that no single attribute is strong enough to yield im-
provements over the META baseline. In this setting,
both MAMD+FEDA and MAMD+MDR-NV achieve a
significant improvement over the META baseline,
with MDR-NV being the best (though not signif-
icantly better than FEDA). Additionally, both of
them are significantly better than their correspond-
ing 1-TUNE strategies. This result further supports
our claim that using multiple attributes in combi-
nation for defining domains (even when any single
one of them is not particularly beneficial for multi?
domain learning) is important.
5 Conclusions
We propose multi-attribute multi-domain learning
methods that can utilize multiple metadata attributes
simultaneously for defining domains. Using these
methods, the definition of ?domains? does not have
to be restricted to a single metadata attribute. Our
methods achieve a better performance on two multi-
attribute datasets as compared to traditional multi-
domain learning methods that are tuned to use a sin-
gle ?best? attribute.
Acknowledgments
This research is supported by the Office of Naval
Research grant number N000141110221.
689
References
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440?447.
Association for Computational Linguistics.
Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge,
Lily Scherlis, and Noah A. Smith. 2012. Word
Salad: Relating Food Prices and Descriptions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning (EMNLP 2012).
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification for
text categorization. Journal of Machine Learning Re-
search (JMLR).
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263. Association for Computational Linguistics.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ?08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ?08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1?
2):123?149.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse Additive Generative Models of Text. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning (ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi?task learning. In Proceedings of
the 2004 ACM SIGKDD international conference on
Knowledge discovery and data mining - KDD ?04.
Jenny R Finkel and Christopher D Manning. 2009. Hier-
archical Bayesian Domain Adaptation. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 602?
610. Association for Computational Linguistics.
Mahesh Joshi, Mark Dredze, William W. Cohen, and Car-
olyn P. Rose?. 2012. Multi-domain learning: When do
domains matter? In Proceedings of EMNLP-CoNLL
2012, pages 1302?1312.
Avishek Saha, Piyush Rai, Hal Daume? III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
William Yang Wang, Elijah Mayfield, Suresh Naidu, and
Jeremiah Dittmar. 2012. Historical Analysis of Legal
Opinions with a Sparse Mixed-Effects Latent Variable
Model. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012).
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
690
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 131?133,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
The Ngram Statistics Package (Text::NSP) - A Flexible Tool for Identifying
Ngrams, Collocations, and Word Associations
Ted Pedersen?
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Satanjeev Banerjee
Twitter, Inc.
795 Folsom Street
San Francisco, CA 94107
Bridget T. McInnes
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Saiyam Kohli
SDL Language Weaver, Inc.
6060 Center Drive, Suite 150
Los Angeles, CA 90045
Mahesh Joshi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
Ying Liu
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Abstract
The Ngram Statistics Package (Text::NSP)
is freely available open-source software that
identifies ngrams, collocations and word as-
sociations in text. It is implemented in Perl
and takes advantage of regular expressions to
provide very flexible tokenization and to allow
for the identification of non-adjacent ngrams.
It includes a wide range of measures of associ-
ation that can be used to identify collocations.
1 Introduction
The identification of multiword expressions is a key
problem in Natural Language Processing. Despite
years of research, there is still no single best way
to proceed. As such, the availability of flexible and
easy to use toolkits remains important. Text::NSP
is one such package, and includes programs for
counting ngrams (count.pl, huge-count.pl), measur-
ing the association between the words that make up
an ngram (statistic.pl), and for measuring correlation
between the rankings of ngrams created by differ-
ent measures (rank.pl). It is also able to identify n-
th order co-occurrences (kocos.pl) and pre?specified
compound words in text (find-compounds.pl).
This paper briefly describes each component of
NSP. Additional details can be found in (Banerjee
and Pedersen, 2003) or in the software itself, which
is freely available from CPAN 1 or Sourceforge 2.
?Contact author : tpederse@d.umn.edu. Note that authors
Banerjee, McInnes, Kohli and Joshi contributed to Text::NSP
while they were at the University of Minnesota, Duluth.
1http://search.cpan.org/dist/Text-NSP/
2http://sourceforge.net/projects/ngram/
2 count.pl
The program count.pl takes any number of plain
text files or directories of such files and counts the
total number of ngrams as well their marginal to-
tals. It provides the ability to define what a token
may be using regular expressions (via the --token
option). An ngram is an ordered sequence of n to-
kens, and under this scheme tokens may be almost
anything, including space separated strings, charac-
ters, etc. Also, ngrams may be made up of nonadja-
cent tokens due to the --window option that allows
users to specify the number of tokens within which
an ngram must occur.
Counting is done using hashes in Perl which are
memory intensive. As a result, NSP also provides
the huge-count.pl program and various other huge-
*.pl utilities that carry out count.pl functionality us-
ing hard drive space rather than memory. This can
scale to much larger amounts of text, although usu-
ally taking more time in the process.
By default count.pl treats ngrams as ordered se-
quences of tokens; dog house is distinct from house
dog. However, it may be that order does not always
matter, and a user may simply want to know if two
words co-occur. In this case the combig.pl program
adjusts counts from count.pl to reflect an unordered
count, where dog house and house dog are consid-
ered the same. Finally, find-compounds.pl allows a
user to specify a file of already known multiword ex-
pressions (like place names, idioms, etc.) and then
identify all occurrences of those in a corpus before
running count.pl
131
3 statistic.pl
The core of NSP is a wide range of measures of
association that can be used to identify interest-
ing ngrams, particularly bigrams and trigrams. The
measures are organized into families that share com-
mon characteristics (which are described in detail in
the source code documentation). This allows for an
object oriented implementation that promotes inher-
itance of common functionality among these mea-
sures. Note that all of the Mutual Information mea-
sures are supported for trigrams, and that the Log-
likelihood ratio is supported for 4-grams. The mea-
sures in the package are shown grouped by family
in Table 1, where the name by which the measure is
known in NSP is in parentheses.
Table 1: Measures of Association in NSP
Mutual Information (MI)
(ll) Log-likelihood Ratio (Dunning, 1993)
(tmi) true MI (Church and Hanks, 1990)
(pmi) Pointwise MI (Church and Hanks, 1990)
(ps) Poisson-Stirling (Church, 2000)
Fisher?s Exact Test (Pedersen et al, 1996)
(leftFisher) left tailed
(rightFisher) right tailed
(twotailed) two tailed
Chi-squared
(phi) Phi Coefficient (Church, 1991)
(tscore) T-score (Church et al, 1991)
(x2) Pearson?s Chi-Squared (Dunning, 1993)
Dice
(dice) Dice Coefficient (Smadja, 1993)
(jaccard) Jaccard Measure
(odds) Odds Ratio (Blaheta and Johnson, 2001)
3.1 rank.pl
One natural experiment is to compare the output of
statistic.pl for the same input using different mea-
sures of association. rank.pl takes as input the out-
put from statistic.pl for two different measures, and
computes Spearman?s Rank Correlation Coefficient
between them. In general, measures within the same
family correlate more closely with each other than
with measures from a different family. As an ex-
ample tmi and ll as well as dice and jaccard differ
by only constant terms and therefore produce identi-
cal rankings. It is often worthwhile to conduct ex-
ploratory studies with multiple measures, and the
rank correlation can help recognize when two mea-
sures are very similar or different.
4 kocos.pl
In effect kocos.pl builds a word network by finding
all the n-th order co-occurrences for a given literal
or regular expression. This can be viewed somewhat
recursively, where the 3-rd order co-occurrences of
a given target word are all the tokens that occur with
the 2-nd order co-occurrences, which are all the to-
kens that occur with the 1-st order (immediate) co-
occurrences of the target. kocos.pl outputs chains of
the form king -> george -> washington,
where washington is a second order co-occurrence
(of king) since both king and washington are first
order co-occurrences of george. kocos.pl takes as
input the output from count.pl, combig.pl, or statis-
tic.pl.
5 API
In addition to command line support, Test::NSP of-
fers an extensive API for Perl programmers. All of
the measures described in Table 1 can be included
in Perl programs as object?oriented method calls
(Kohli, 2006), and it is also easy to add new mea-
sures or modify existing measures within a program.
6 Development History of Text::NSP
The Ngram Statistics Package was originally imple-
mented by Satanjeev Banerjee in 2000-2002 (Baner-
jee and Pedersen, 2003). Amruta Purandare in-
corporated NSP into SenseClusters (Purandare and
Pedersen, 2004) and added huge-count.pl, com-
big.pl and kocos.pl in 2002-2004. Bridget McInnes
added the log-likelihood ratio for longer ngrams
in 2003-2004 (McInnes, 2004). Saiyam Kohli
rewrote the measures of association to use object-
oriented methods in 2004-2006, and also added
numerous new measures for bigrams and trigams
(Kohli, 2006). Mahesh Joshi improved cross plat-
form support and created an NSP wrapper for Gate
in 2005-2006. Ying Liu wrote find-compounds.pl
and rewrote huge-count.pl in 2010-2011.
132
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistics Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D. Blaheta and M. Johnson. 2001. Unsupervised learn-
ing of multi-word verbs. In ACL/EACL Workshop on
Collocations, pages 54?60, Toulouse, France.
K. Church and P. Hanks. 1990. Word association norms,
mutual information and lexicography. Computational
Linguistics, pages 22?29.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Us-
ing statistics in lexical analysis. In U. Zernik, editor,
Lexical Acquisition: Exploiting On-Line Resources to
Build a Lexicon. Lawrence Erlbaum Associates, Hills-
dale, NJ.
K. Church. 1991. Concordances for parallel text. In
Seventh Annual Conference of the UW Centre for New
OED and Text Research, Oxford, England.
K. Church. 2000. Empirical estimates of adaptation:
The chance of two noriegas is closer to p/2 than p2.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
180?186, Saarbru?cken, Germany.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
S. Kohli. 2006. Introducing an object oriented design to
the ngram statistics package. Master?s thesis, Univer-
sity of Minnesota, Duluth, July.
B. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master?s thesis,
University of Minnesota, Duluth, December.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence,
pages 455?460, Portland, OR, August.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
133
