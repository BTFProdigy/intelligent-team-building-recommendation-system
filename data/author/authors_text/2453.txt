Adapting an Example-Based Translation System to
Chinese
Ying Zhang, Ralf D. Brown, and Robert E. Frederking
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213-3890 USA
fjoy,ralf,refg@cs.cmu.edu
ABSTRACT
We describe an Example-Based Machine Translation (EBMT) sys-
tem and the adaptations and enhancementsmade to create a Chinese-
English translation system from the Hong Kong legal code and var-
ious other bilingual resources available from the Linguistic Data
Consortium (LDC).
1. BACKGROUND
We describe an Example-Based Machine Translation (EBMT)
system and the adaptations and enhancements made to create a
Chinese-English translation system from the Hong Kong legal code
and various other bilingual resources available from the Linguistic
Data Consortium (LDC).
The EBMT software [1, 3] used for the experiments described
here is a shallow system which can function using nothing more
than sentence-alignedplaintext and a bilingual dictionary; and given
sufficient parallel text, the dictionary can be extracted statistically
from the corpus [2]. To perform a translation, the program looks up
all matching phrases in the source-language half of the parallel cor-
pus and performs a word-level alignment on the entries containing
matches to determine a (usually partial) translation. Portions of the
input for which there are no matches in the corpus do not generate
a translation.
Because the EBMT system does not generate translations for
100% of the text it is given as input, a bilingual dictionary and
phrasal glossary are used to fill any gaps. Selection of a ?best?
translation is guided by a trigram model of the target language [6].
Supporting Chinese required a number of changes to the program
and training procedures; those changes are discussed in the next
section.
2. ENHANCEMENTS
The first change required of the translation software was sup-
port for the two-byte encoding used for the Chinese text (GB-2312,
?GB? for short). Further, the EBMT (as well as dictionary and glos-
sary) approaches are word-based, but Chinese is ordinarily writ-
ten without breaks between words. Thus, Chinese input must be
.
segmented into individual words. The initial baseline system used
the segmenter made available by the LDC. This segmenter uses a
word-frequency list to make segmentation decisions, but although
the list provided by the LDC is large, it did not completely cover
the vocabulary of the EBMT training corpus (described below). As
a result, many sentences had incorrect segmentations or included
long sequences which were not segmented at all or were broken
into single characters. Almost every Chinese character has at least
one meaning, and its meaning may be entirely different from the
meaning of the word containing it. The mis-segmenting of Chinese
words due to the inadequate dictionary makes it very hard to build
a statistical dictionary and properly index the EBMT corpus.
To improve the performance of the Chinese segmenter, we aug-
mented its word list by finding sequences of characters in the train-
ing corpus that belong together, based on their frequency and high
mutual information. We developed a form of term extraction to
find English phrases which should be treated as atomic units for
translation, thus increasing the average length of ?words? in both
source and target languages. Finally, we also created an augmented
bilingual dictionary for use in word-level alignment for EBMT by
applying statistical dictionary extraction techniques to the training
corpus.
As the improved segmenter and the term finder may be produc-
ing excessively long phrases or phrases which are impossible to
match in the other language, we repeat the procedure of segment-
ing/bracketing/dictionary-building several times. On each succes-
sive iteration, the segmenter and bracketer are limited to words and
phrases for which the statistical dictionary from the previous itera-
tion contains translations. Through this iteration, we increased the
size of the statistical dictionary from each step and guaranteed that
all Chinese words generated by the segmenter have translations in
the dictionary. This helps ensure that the EBMT engine can per-
form word-level alignments.
3. EXPERIMENTAL DESIGN
The primary purpose of this experiment was to determine the
effect of each enhancement by operating with various subsets of
the enhancements. Since it rapidly becomes impractical to test all
possible combinations, we opted for the following test conditions:
1. baseline: parallel corpus segmented with the LDC segmenter
and LDC dictionary/glossary
2. baseline plus improved segmenter
3. baseline plus improved segmenter and term finder
4. baseline plus improved segmenter and statistical dictionary
5. baseline plus improved segmenter, term finder, and statistical
dictionary
For training, we had available two parallel Chinese-English cor-
pora distributed by the LDC: the complete Hong Kong legal code
(after cleaning: 47.86 megabytes, 5.5 million English words, 9 mil-
lion Chinese characters) where 85% of the content (by sentence) is
unique, and a collection of Hong Kong news articles (after clean-
ing: 24.58 megabytes, 2.67 million English words, 4.5 million Chi-
nese characters). In addition, LDC distributes a bilingual dictio-
nary/phrasebook, which we also used.
To determine the effects of varying amounts of training data on
overall performance, we divided the bilingual training corpus into
ten nearly equal slices. Each test condition was then run ten times,
each time increasing the number of slices used for training the sys-
tem. After each training pass, the test sentences were translated and
the system?s performance evaluated automatically; selected points
were then manually evaluated for translation quality.
The automatic performance evaluation measured coverage of the
input and average phrase length. Coverage is the percentage of the
input text for which a translation is produced by a particular trans-
lation method (since the EBMT engine does not generally produce
hypotheses that cover every word of input), while average phrase
length is a crude indication of translation quality ? the longer the
phrase that is translated, the more context is incorporated and the
less likely it is that the wrong sense will be used in the translation or
that (for EBMT) the alignment will be incorrect. Since the dictio-
nary and glossary remain constant for a given test condition, only
the EBMT coverage will be presented.
Manual grading of the output was performed using a web-based
system with which the graders could assign one of three scores
(?Good?, ?OK?, ?Bad?) in each of two dimensions: grammatical
correctness and meaning preservation. This type of quality scoring
is commonly used in assessing translation quality, and is used by
other TIDES participants. Fifty-two test sentences were translated
for each of four points from the automated evaluation and these sets
of four alternatives presented to the graders. The four points chosen
were the baseline system with 100% of the training corpus, the full
system with 20% and 100% training, and the full system trained on
a corpus of Hong Kong news text (cross-domain); only four points
were selected due to the difficulty and expense of obtaining large
numbers of manual quality judgements.
To assess the performance of the system in a different domain,
as well as the effect of the trigram language model on the selec-
tion of translated fragments for the final translation, we obtained
manual judgements for 44 sentences on an additional four test con-
ditions, each trained with the entire available parallel text and tested
on Hong Kong news text rather than legal sentences. These points
were the cross-domain case (trained on the legal corpus) and three
different language models for within-domain training: an English
language model derived from the legal corpus, one derived from
the news corpus, and a pre-existing model generated from two gi-
gabytes of newswire and broadcast news transcriptions.
4. RESULTS
We discovered that there is a certain amount of synergy between
some of the improvements, particularly the term finder and statis-
tical dictionary extraction. Applying the term finder modifies the
parallel corpus in such a way that it becomes more difficult for
the EBMT engine to find matches which it can align, while adding
dictionary entries derived from the modified corpus eliminates that
effect. As a result, we will not present the performance results for
Test Condition 3 (improved segmenter plus term finder); further,
the data for Test Conditions 2 (improved segmenter only) and 4
(improved segmenter plus statistical dictionary) may not accurately
reflect the contribution of those two components to the full system
Translating Legal Code
System Baseline Full Full X-Dom
Training 100% 20% 100% 100%
Syntactic 42.31% 54.81% 61.06% 39.42%
Semantic 43.75% 61.54% 64.42% 34.62%
Translating Hong Kong News
Training News News News Legal
LangModel Legal News Prior Legal
Syntactic 45.67% 44.71% 47.60% 34.62%
Semantic 50.00% 50.96% 51.92% 47.12%
Figure 1: Judgements ? Acceptable Translations
used for Test Condition 5.
Figure 2 shows the proportion of the words in the test sentences
for which the EBMT engine was able to produce a translation,
while Figure 3 shows the average number of source-languagewords
per translated fragment. These curves do not increase monotoni-
cally because, for performance reasons, the EBMT engine does not
attempt to align every occurrence of a phrase, only theN (currently
12) most-recently added ones; as a result, adding more text to the
corpus can cause EBMT to ignore matches that successfully align
in favor of newer occurrences which it is unable to align.
Examining Figure 3, it is clear that the fifth slice (from 40 to
50%) is much more like the test data than other slices, resulting in
longer matches. In general, the closer training and test text are to
each other, the longer the phrases they have in common.
Figure 1 summarizes the results of human quality assessments.
The ?Good? and ?OK? judgements were combined into ?Accept-
able? and the the percentage of ?Acceptable? judgements was aver-
aged across sentences and graders. As hoped and expected, the im-
provements do in fact result not only in better coverage by EBMT,
but also in better quality assessments by the human graders. Fur-
ther, the results on Hong Kong news text show that the choice of
language model does have a definite effect on quality. These results
also confirm the adage that there is no such thing as too much train-
ing text for language modeling, since the model generated from the
EBMT corpus was unable to match the performance of the pre-
existing model generated from two orders of magnitude more text.
5. CONCLUSIONS AND FUTURE WORK
As seen in Figure 2, the enhancements described here cumula-
tively provide a 12% absolute improvement in coverage for EBMT
translations without requiring any additional knowledge resources.
Further, the enhanced coverage does, in fact, result in improved
translations, as verified by human judgements. We can also con-
clude that when we combine words into larger chunks on both sides
of the corpus, the possibility of finding larger matches between the
source language and the target language increases, which leads to
the improvement of the translation quality for EBMT.
We will do further research on the interaction between the im-
proved segmenter, term finder and statistical dictionary builder, uti-
lizing the information provided by the statistical dictionary as feed-
back for the segmenter and term finder to modify their results. We
are also investigating the effects of splitting the EBMT training into
multiple sets of topic-specific sentences, automatically separated
using clustering techniques.
The relatively low slope of the coverage curve also indicates that
the training corpus is sufficiently large. Our prior experience with
Spanish (using the UN Multilingual Corpus [5]) and French (using
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
 o
f E
BM
T
Percentage of corpus used for training (%)
Improved Segmenter, term finder, statDict
Improved Segmenter, statDict
Improved Segmenter
Baseline system
Trained on News tested On Legalcode
Figure 2: EBMT Coverage with Varying Training
the Hansard corpus [7]) was that the curve flattens out at between
two and three million words of training text, which appears also to
be the case for Chinese (each training slice contains approximately
one million words of total text).
We have not yet taken full advantage of the features of the EBMT
software. In particular, it supports equivalence classes that permit
generalization of the training text into templates for improved cov-
erage. We intend to test automatic creation of equivalence classes
from the training corpus [4] in conjunction with the other improve-
ments reported herein.
6. ACKNOWLEDGEMENTS
We would like to thank Alon Lavie and Lori Levin for their com-
ments on drafts of this paper.
7. REFERENCES
[1] R. D. Brown. Example-Based Machine Translation in the
PANGLOSS System. In Proceedings of the Sixteenth
International Conference on Computational Linguistics, pages
169?174, Copenhagen, Denmark, 1996.
http://www.cs.cmu.edu/?ralf/papers.html.
[2] R. D. Brown. Automated Dictionary Extraction for
?Knowledge-Free? Example-Based Translation. In
Proceedings of the Seventh International Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI-97), pages 111?118, Santa Fe, New Mexico,
July 1997.
http://www.cs.cmu.edu/?ralf/papers.html.
[3] R. D. Brown. Adding Linguistic Knowledge to a Lexical
Example-Based Translation System. In Proceedings of the
Eighth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-99),
pages 22?32, Chester, England, August 1999.
http://www.cs.cmu.edu/?ralf/papers.html.
[4] R. D. Brown. Automated Generalization of Translation
Examples. In Proceedings of the Eighteenth International
Conference on Computational Linguistics (COLING-2000),
pages 125?131, 2000.
[5] D. Graff and R. Finch. Multilingual Text Resources at the
Linguistic Data Consortium. In Proceedings of the 1994 ARPA
Human Language Technology Workshop. Morgan Kaufmann,
1994.
00.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
7
7.5
0 10 20 30 40 50 60 70 80 90 100
Av
er
ag
e 
ph
ra
se
 le
ng
th
 (w
ord
s)
Percentage of corpus used for training (%)
Improved Segmenter, term finder, statDict
Improved Segmenter, statDict
Improved Segmenter
Baseline system
Trained on News tested On Legalcode
Figure 3: Average EBMT Match Lengths
[6] C. Hogan and R. E. Frederking. An Evaluation of the
Multi-engine MT Architecture. In Machine Translation and
the Information Soup: Proceedings of the Third Conference of
the Association for Machine Translation in the Americas
(AMTA ?98), volume 1529 of Lecture Notes in Artificial
Intelligence, pages 113?123. Springer-Verlag, Berlin, October
1998.
[7] Linguistic Data Consortium. Hansard Corpus of Parallel
English and French. Linguistic Data Consortium, December
1997. http://www.ldc.upenn.edu/.
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
JAVELIN: A Flexible, Planner-Based Architecture for Question Answering
Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
ehn@cs.cmu.edu
Robert Frederking
Language Technologies Institute
Carnegie Mellon University
ref@cs.cmu.edu
Abstract
The JAVELIN system integrates a flexible,
planning-based architecture with a variety of
language processing modules to provide an
open-domain question answering capability on
free text. The demonstration will focus on how
JAVELIN processes questions and retrieves the
most likely answer candidates from the given
text corpus. The operation of the system will be
explained in depth through browsing the repos-
itory of data objects created by the system dur-
ing each question answering session.
1 Introduction
Simple factoid questions can now be answered reason-
ably well using pattern matching. Some systems (Soub-
botin and Soubbotin, 2002) use surface patterns enhanced
with semantic categories and question types in order to
model the likelihood of answers given the question. Fur-
thermore, Hovy et al (Hovy et al, 2002) have obtained
good results using only surface patterns pre-extracted
from the web. However, pattern-based approaches don?t
represent the meaning of the patterns they use, and it is
not clear whether they can be generalized for more diffi-
cult, non-factoid questions.
Open domain question answering is a complex, multi-
faceted task, where question type, information availabil-
ity, user needs, and a combination of text processing tech-
niques (statistical, NLP, etc.) must be combined dynami-
cally to determine the optimal answer. For more complex
questions, a more flexible and powerful control mech-
anism is required. For example, LCC (D. Moldovan
and Surdeanu, 2002) has implemented feedback loops
which ensure that processing constraints are met by re-
trieving more documents or expanding question terms.
The LCC system includes a passage retrieval loop, a
lexico-semantic loop and a logic proving loop. The
IBM PIQUANT system (Carroll et al, 2002) combines
knowledge-based agents using predictive annotation with
a statistical approach based on a maximum entropy model
(Ittycheriah et al, 2001).
exe
Domain
Model
Planner 
Data
Repository
JAVELIN 
GUI
Execution
Manager
process history
and data
JAVELIN operator
(action) models
question
answer
ack
.
.
.
dialog
response
exe
results
exe
results
results
Question
Analyzer
Information
Extractor
Answer
Generator
Retrieval
Strategist
Answer
Justification
Web
Browser
Figure 1: The JAVELIN architecture. The Planner con-
trols execution of the individual components via the Ex-
ecution Manager.
Both the LCC and IBM systems represent a depar-
ture from the standard pipelined approach to QA archi-
tecture, and both work well for straightforward factoid
questions. Nevertheless, both approaches incorporate a
pre-determined set of processing steps or strategies, and
have limited ability to reason about new types of ques-
tions not previously encountered. Practically useful ques-
tion answering in non-factoid domains (e.g., intelligence
analysis) requires more sophisticated question decom-
position, reasoning, and answer synthesis. For these
hard questions, QA architectures must define relation-
ships among entities, gather information from multiple
sources, and reason over the data to produce an effec-
tive answer. As QA functionality becomes more sophis-
ticated, the set of decisions made by a system will not
be captured by pipelined architectures or multi-pass con-
straint relaxation, but must be modeled as a step-by-step
decision flow, where the set of processing steps is deter-
mined at run time for each question.
This demonstration illustrates the JAVELIN QA archi-
tecture (Nyberg et al, 2002), which includes a general,
modular infrastructure controlled by a step-by-step plan-
ning component. JAVELIN combines analysis modules,
information sources, user discourse and answer synthe-
sis as required for each question-answering interaction.
JAVELIN also incorporates a global memory, or repos-
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 19-20
                                                         Proceedings of HLT-NAACL 2003
itory, which maintains a linked set of object dependen-
cies for each question answering session. The repository
can be used to provide a processing summary or answer
justification for the user. The repository also provides a
straightforward way to compare the results of different
versions of individual processing modules running on the
same question. The modularity and flexibility of the ar-
chitecture provide a good platform for component-based
(glass box) evaluation (Nyberg and Mitamura, 2002).
2 Demonstration Outline
The demonstration will be conducted on a laptop con-
nected to the Internet. The demonstration will feature
the JAVELIN graphical user interface (a Java application
running on the laptop) and the JAVELIN Repository (the
central database of JAVELIN result objects, accessed via
a web browser). A variety of questions will be asked of
the system, and the audience will be able to view the sys-
tem?s answers along with a detailed trace of the steps that
were taken to retrieve the answers.
Figure 2: An Answer Justification.
Figure 2 shows the top-level result returned by
JAVELIN. The preliminary answer justification includes
the selected answer along with a variety of hyperlinks
that can be clicked to provide additional detail regarding
the system?s analysis of the question, the documents re-
trieved, the passages extracted, and the full set of answer
candidates. The justification also provides drill-down ac-
cess to the steps taken by the Planner module in reason-
ing about how to best answer the given question. Figure 3
shows additional detail that is exposed when the ?Docu-
ments Returned? and ?Request Fills? links are activated.
Acknowledgements
The research described in this paper was supported in part
by a grant from ARDA under the AQUAINT Program
Phase I. The current version of the JAVELIN system was
conceived, designed and constructed with past and cur-
rent members of the JAVELIN team at CMU, including:
Figure 3: Partial Answer Detail.
Jamie Callan, Jaime Carbonell, Teruko Mitamura, Kevyn
Collins-Thompson, Krzysztof Czuba, Michael Duggan,
Laurie Hiyakumoto, Ning Hu, Yifen Huang, Curtis Hut-
tenhower, Scott Judy, Jeongwoo Ko, Anna Kups?c?, Lucian
Lita, Stephen Murtagh, Vasco Pedro, David Svoboda, and
Benjamin Van Durme.
References
J. Carroll, J. Prager, C. Welty, K. Czuba, and D. Ferrucci.
2002. A multi-strategy and multi-source approach to
question answering.
S. Harabagiu D. Moldovan, M. Pasca and M. Surdeanu.
2002.
E. Hovy, U. Hermjakob, and D. Ravichandran. 2002. A
question/answer typology with surface text patterns.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi.
2001. Question answering using maximum-entropy
components.
E. Nyberg and T. Mitamura. 2002. Evaluating qa sys-
tems on multiple dimensions.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2002. The
javelin question-answering system at trec 2002.
M. Soubbotin and S. Soubbotin. 2002. Use of patterns
for detection of likely answer strings: A systematic ap-
proach.
SPEECHALATOR: TWO-WAY SPEECH-TO-SPEECH TRANSLATION IN YOUR HAND
Alex Waibel
 
, Ahmed Badran
 
, Alan W Black
 
, Robert Frederking
 
, Donna Gates
 
Alon Lavie
 
, Lori Levin
 
, Kevin Lenzo

, Laura Mayfield Tomokiyo
Juergen Reichert

, Tanja Schultz   , Dorcas Wallace   , Monika Woszczyna , Jing Zhang
 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA

Cepstral, LLC,

Multimodal Technologies Inc,

Mobile Technologies Inc.
speechalator@speechinfo.org
ABSTRACT
This demonstration involves two-way automatic speech-
to-speech translation on a consumer off-the-shelf PDA. This
work was done as part of the DARPA-funded Babylon project,
investigating better speech-to-speech translation systems for
communication in the field. The development of the Speecha-
lator software-based translation system required addressing
a number of hard issues, including a new language for the
team (Egyptian Arabic), close integration on a small device,
computational efficiency on a limited platform, and scalable
coverage for the domain.
1. BACKGROUND
The Speechalator was developed in part as the next genera-
tion of automatic voice translation systems. The Phrasalator
is a one-way device that can recognize a set of pre-defined
phrases and play a recorded translation, [1]. This device
can be ported easily to new languages, requiring only a
hand translation of the phrases and a set of recorded sen-
tences. However, such a system severely limits communica-
tion as the translation is one way, thus reducing one party?s
responses to simple pointing and perhaps yes and no.
The Babylon project addresses the issues of two-way
communication where either party can use the device for
conversation. A number of different groups throughout the
US were asked to address specific aspects of the task, such
as different languages, translation techniques and platform
specifications. The Pittsburgh group was presented with
three challenges. First, we were to work with Arabic, a lan-
guage with which the group had little experience, to test our
capabilities in moving to new languages quickly. Second,
we were instructed to use an interlingua approach to trans-
lation, where the source language is translated into an in-
termediate form that is shared between all languages. This
step streamlines expansion to new languages, and CMU has
a long history in working with interlingua based translation
systems. Third, we were constrained to one portable PDA-
class device to host the entire two-way system: two recog-
nizers, two translation engines, and two synthesizers.
2. RECOGNITION
We used an HMM-based recognizer, developed by Multi-
modal Technologies Inc, which has been specifically tuned
for PDAs. The recognizer allows a grammar to be tightly
coupled with the recognizer, which offers important effi-
ciencies considering the limited computational power of the
device. With only minor modification we were able to gen-
erate our interlingua interchange format (IF) representation
directly as output from the recognizer, removing one mod-
ule from the process.
MTI?s recognizer requires under 1M of memory with
acoustic models of around 3M per language. Special op-
timizations deal with the slow processor and ensure low
use of memory during decoding. The Arabic models were
bootstrapped from the GlobalPhone [2] Arabic collections
as well as data collected as part of this project.
3. TRANSLATION
As part of this work we investigated two different tech-
niques for translation, both interlingua based. The first was
purely knowledge-based, following our previous work [3].
The engine developed for this was too large to run on the
device, although we were able to run the generation part off-
line seamlessly connected by a wireless link from the hand-
held device. The second technique we investigated used
a statistical training method to build a model to translate
structured interlingua IF to text in the target language. Be-
cause this approach was developed with the handheld in
mind, it is efficient enough to run directly on the device,
and is used in this demo.
4. SYNTHESIS
The synthesis engine is Cepstral?s Theta system. As the
Speechalator runs on very small hardware devices (at least
small compared to standard desktops), it was important that
the synthesis footprint remained as small as possible.
The speechalator is to be used for people with little ex-
posure to synthetic speech, and the output quality must be
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 29-30
                                                         Proceedings of HLT-NAACL 2003
very high. Cepstral?s unit selection voices, tailored to the
domain, meet the requirements for both quality and size.
Normal unit selection voices may take hundreds of megabytes,
but the 11KHz voices developed by Cepstral were around 9
megabytes each.
5. ARABIC
The Arabic language poses a number of challenges for any
speech translation system. The first problem is the wide
range of dialects of the language. Just as Jamaican and
Glaswegian speakers may find it difficult to understand each
other?s dialect of English, Arabic speakers of different di-
alects may find it impossible to communicate.
Modern Standard Arabic (MSA) is well-defined and widely
understood by educated speakers across the Arab world.
MSA is principally a written language and not a spoken lan-
guage, however. Our interest was in dealing with a normal
spoken dialect, and we chose Egyptian Arabic; speakers of
that dialect were readily accessible to us, and media influ-
ences have made it perhaps the most broadly understood of
the regional dialects.
Another feature of Arabic is that the written form, ex-
cept in specific rare cases, does not include vowels. For
speech recognition and synthesis, this makes pronunciations
hard. Solutions have been tested for recognition where the
vowels are not explicitly modeled, but implicitly modeled
by context. This would not work well for synthesis; we have
defined an internal romanization, based on the CallHome
[4] romanization, from which full phonetic forms can easily
be derived. This romanization is suitable for both recog-
nizer and synthesis systems, and can easily be transformed
into the Arabic script for display.
6. SYSTEM
The end-to-end system runs on a standard Pocket PC de-
vice. We have tested it on a number of different machines,
including various HP (Compaq) iPaq machines (38xx 39xx)
and Dell Axims. It can run on 32M machines, but runs best
on a 64M machine with about 40M made available for pro-
gram space. Time from the end of spoken input to start of
translated speech is around 2-4 seconds depending on the
length of the sentence and the actual processor. We have
found StrongARM 206MHz processors, found on the older
Pocket PCs, slightly faster than XScale 400MHz, though no
optimization for the newer processors has been attempted.
Upon startup, the user is presented with the screen as
shown in Figure 1. A push-to-talk button is used and the
speaker speaks in his language. The recognized utterance
is first displayed, with the translation following, and the ut-
terance is then spoken in the target language. Buttons are
provided for replaying the output and for switching the in-
put to the other language.
7. DISCUSSION
The current demonstration is designed for the medical inter-
view domain, with the doctor speaking English and the pa-
tient speaking Arabic. At this point in the project no formal
evaluation has taken place. However, informally, in office-
like acoustic environments, accuracy within domain is well
over 80%.
Arabic input Screen
Speechalator snapshot
8. REFERENCES
[1] Sarich, A., ?Phraselator, one-way speech translation
system,? http://www.sarich.com/translator/, 2001.
[2] T. Schultz and A. Waibel, ?The globalphone project:
Multilingual lvcsr with janus-3,? in Multilingual Infor-
mation Retrieval Dialogs: 2nd SQEL Workshop, Plzen,
Czech Republic, 1997, pp. 20?27.
[3] A. Lavie, et al ?A multi-perspective evaluation of
the NESPOLE! speech-to-speech translation system,?
in Proceedings of ACL 2002 workshop on Speech-to-
speech Translation: Algorithms and Systems, Philadel-
phia, PA., 2002.
[4] Linguistic Data Consortium, ?Callhome egyptian ara-
bic speech,? 1997.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 5?8,
New York, June 2006. c?2006 Association for Computational Linguistics
The MILE Corpus for Less Commonly Taught Languages 
 
Alison Alvarez, Lori Levin, Robert 
Frederking, Simon Fung, Donna 
Gates 
Language Technologies Institute 
5000 Forbes Avenue 
Pittsburgh, PA 15213 
[nosila, lsl, ref+, 
sfung, dmg] 
@cs.cmu.edu  
Jeff Good 
Max Planck Institute for Evolutionary 
Anthropology 
Deutscher Platz 6 
04103 Leipzig 
good@eva.mpg.de 
 
 
Abstract 
This paper describes a small, struc-
tured English corpus that is 
designed for translation into Less 
Commonly Taught Languages 
(LCTLs), and a set of re-usable 
tools for creation of similar cor-
pora. 1  The corpus systematically 
explores meanings that are known to 
affect morphology or syntax in the 
world?s languages.  Each sentence 
is associated with a feature structure 
showing the elements of meaning 
that are represented in the sentence.   
The corpus is highly structured so 
that it can support machine learning 
with only a small amount of data.   
As part of the REFLEX program, 
the corpus will be translated into 
multiple LCTLs, resulting in paral-
lel corpora can be used for training 
of MT and other language technolo-
gies. Only the untranslated English 
corpus is described in this paper.  
 
1   Introduction 
 
Of the 6,000 living languages in the world 
only a handful have the necessary monolin-
gual or bilingual resources to build a 
working statistical or example-based ma-
chine translation system.  Currently, there 
                                                 
1 AVENUE/MILE is supported by the US Na-
tional Science Foundation NSF grant number 
IIS-0121-631 and the US Government?s 
REFLEX Program. 
are efforts to build language packs for Less 
Commonly Taught Languages (LCTLs).  
Each language pack includes parallel cor-
pora consisting of naturally occurring text 
translated from English into the LCTL or 
vice versa.  
This paper describes a small corpus 
that supplements naturally occurring text 
with highly systematic enumeration of 
meanings that are known to affect morphol-
ogy and syntax in the world?s languages.   
The supplemental corpus will enable the 
exploration of constructions that are sparse 
or obscured in natural data.  The corpus 
consists of 12,875 English sentences, total-
ing 76,202 word tokens.    
This paper describes the construc-
tion of the corpus, including tools and 
resources that can be used for the construc-
tion of similar corpora.   
 
2 Structure of the corpus 
 
| 247: John said "The woman is a teacher." 
| 248: John said the woman is not a teacher. 
| 249: John said "The woman is not a teacher." 
| 250: John asked if the woman is a teacher. 
| 251: John asked "Is the woman a teacher?" 
| 252: John asked if the woman is not a teacher. 
| ?
| 1488: Men are not baking cookies. 
| 1489: The women are baking cookies.
| ?
| 1537: The ladies' waiter brought appetizers. 
| 1538: The ladies' waiter will bring appetizers. 
Figure 1: A sampling of sentences from 
the complete elicitation corpus 
5
srcsent: Mary was not a doctor. 
context: Translate this as though it were spoken to a peer co-worker; 
 
((actor ((np-function fn-actor)(np-animacy anim-human)(np-biological-gender bio-gender-female) 
(np-general-type  proper-noun-type)(np-identifiability identifiable) 
 (np-specificity specific)?))     
(pred ((np-function fn-predicate-nominal)(np-animacy anim-human)(np-biological-gender bio-
gender-female) (np-general-type common-noun-type)(np-specificity specificity-neutral)?)) 
(c-v-lexical-aspect state)(c-copula-type copula-role)(c-secondary-type secondary-copula)(c-
solidarity solidarity-neutral) (c-power-relationship power-peer) (c-v-grammatical-aspect gram-
aspect-neutral)(c-v-absolute-tense past) (c-v-phase-aspect phase-aspect-neutral) (c-general-
type declarative-clause)(c-polarity polarity-negative)(c-my-causer-intentionality intentionality-
n/a)(c-comparison-type comparison-n/a)(c-relative-tense relative-n/a)(c-our-boundary boundary-
n/a)?) 
Figure 2:  An abridged feature structure, sentence and context field 
The MILE (Minor Language Elicitation) 
corpus is a highly structured set of English 
sentences.  Each sentence represents a 
meaning or combination of meanings that 
we want to elicit from a speaker of an 
LCTL.  For example, the corpus excerpts 
in Figure 1 explore quoted and non quoted 
sentential complements, embedded ques-
tions, negation, definiteness, biological 
gender, and possessive noun phrases.   
Underlying each sentence is a feature 
structure that serves to codify its meaning.  
Additionally, sentences are accompanied by 
a context field that provides information that 
may be present in the feature structure, but 
not inherent in the English sentence.  For 
example, in Figure 2, the feature structure 
specifies solidarity with the hearer and 
power relationship of the speaker and hearer, 
as evidenced by the features-value pairs (c-
solidarity solidarity-neutral) and (c-power-
relationship power-peer).  Because this is 
not an inherent part of English grammar, this 
aspect of meaning is conveyed in the context 
field.   
 
3 Building the Corpus 
 
Figure 3 shows the steps in creating the 
corpus.  Corpus creation is driven by a Fea-
ture Specification.  The Feature 
Specification defines features such as tense, 
person, and number, and values for each 
feature such past, present, future, remote 
past, recent past, for tense.  Additionally, 
the feature specification defines illegal com-
binations of features, such as the use of a 
singular number with an inclusive or exclu-
sive pronoun (We = you and me vs we = me 
and other people).  The inventory of fea-
tures and values is informed by typological 
studies of which elements of meaning are 
known to affect syntax and morphology in 
some of the world?s languages. The feature 
specification currently contains 42 features 
and 340 values and covers. In order to select 
the most relevant features we drew guidance 
from Comrie and Smith (1977) and Bouqui-
aux and Thomas (1992).  We also used the 
World Atlas of Language Structures 
(Haspelmath et al 2005) as a catalog of ex-
isting language features and their prevalence.  
In the process of corpus creation, feature 
structures are created before their corre-
sponding English sentences.   There are 
three reasons for this.  First, as mentioned 
above, the feature structure may contain 
elements of meaning that are not explicitly 
represented in the English sentence.  Sec-
ond, multiple elicitation languages can be 
generated from the same set of feature struc-
tures.  For example, when we elicit South 
American languages we use Spanish instead 
of English sentences.  Third, what we want 
to know about each LCTL is not how it 
translates the structural elements of English 
such as determiners and auxiliary verbs, but 
how it renders certain meanings such as 
6
List of semantic 
features and 
values 
The Corpus
Feature Maps:  which 
combinations of 
features and values 
are of interest 
Clause-
Level 
Noun-
Phrase
Tense &
Aspect Modality 
Feature Structure Sets
Feature 
Specification 
Reverse Annotated Feature Structure
Sets: add English sentences
Smaller   CorpusSampling
?
Figure 3: An overview of the elicitation corpus production process 
definiteness, tense, and modality, which are 
not in one-to-one correspondence with Eng-
lish words.    
Creation of feature structures takes place 
in two steps.  First, we define which com-
binations of features and values are of 
interest.  Then the feature structures are 
automatically created from the feature speci-
fication.    
Combinations of features are specified 
in Feature Maps (Figure 3).  These maps 
identify features that are known to interact 
syntactically or morphologically in some 
languages.  For example, tense in English 
is partially expressed using the auxiliary 
verb system.  An unrelated aspect of mean-
ing, whether a sentence is declarative or 
interrogative, interacts with the tense system 
in that it affects the word order of auxiliary 
verbs (He was running, Was he running), 
Thus there is an interaction of tense with 
interrogativity.   We use studies of lan-
guage typology to identify combinations of 
features that are known to interact.   
Feature Maps are written in a concise 
formalism that is automatically expanded 
into a set of feature structures.  For exam-
ple, we can formally specify that we want 
three values of tense combined with three 
values of person, and nine feature structures 
will be produced.  These are shown as Fea-
ture Structure Sets in Figure 3.   
 
 
4 Sentence Writing 
 
 As stated previously, our corpus 
consists of feature structures that have been 
human annotated with a sentence and con-
text field.  Our feature structures contain 
functional-typological information, but do 
not contain specific lexical items.  This 
means that our set of feature structures can 
be interpreted into any language using ap-
propriate word choices and used for 
elicitation.  Additionally, this leaves the 
human annotator with some freedom when 
selecting vocabulary items.  Due to feed-
back from previous elicitation subjects we 
chose basic vocabulary words while steering 
clear of overly primitive subject matter that 
may be seen as insulting.  Moreover, we 
did our best to avoid lexical gaps; for exam-
ple, many languages do not have a single 
word that means winner.   
7
 Translator accuracy was also an im-
portant objective and we took pains to 
construct natural sounding, unambiguous 
sentences.  The context field is used to 
clarify the sentence meaning and spell out 
features that may not manifest themselves in 
English. 
 
5 Tools 
 
 In conjunction with this project we 
created several tools that can be reused to 
make new corpora with other purposes. 
? An XML schema and XSLT can be used 
to make different feature specifications 
? A feature structure generator that can be 
used as a guide to specify and design 
feature maps 
? A feature structure browser can be used 
to make complicated feature structures 
easier to read and annotate 
 
6 Conclusion 
 
The basic steps for creating a func-
tional-typological corpus are: 
  
1. Combinations of features are selected 
2. Sets of feature structures representing all 
feature combinations are generated 
3. Humans write sentences with basic vo-
cabulary that represent the meaning in 
the feature structure 
4. If the corpus is too large, some or all of 
the corpus can be sampled 
 
We used sampling and assessments of 
the most crucial features in order to compile 
our corpus and restrict it to a size small 
enough to be translatable by humans.  As a 
result it is possible that this corpus will miss 
important feature combinations in some lan-
guages.  However, a corpus containing all 
possible combinations of features would 
produce hundreds of billions of feature 
structures.   
Our future research includes building a 
Corpus Navigation System to dynamically 
explore the full feature space.  Using ma-
chine learning we will use information de-
tected from translated sentences in order to 
decide what parts of the feature space are 
redundant and what parts must be explored 
and translated next. A further description of 
this process can be read in Levin et al 
(2006). 
Additionally, we will change from using 
humans to write sentences and context fields 
to having them generated by using a natural 
language generation system (Alvarez et al 
2005).   
We also ran small scale experiments to 
measure translator accuracy and consistency 
and encountered positive results. Hebrew 
and Japanese translators provided consistent, 
accurate translations.  Large scale experi-
ments will be conducted in the near future to 
see if the success of the smaller experiments 
will carry over to a larger scale. 
 
7 References 
 
Alvarez, Alison, and Lori Levin, Robert  
  Frederking, Jeff Good, Erik Peterson  
 September 2005, Semi-Automated Elicitation 
 Corpus Generation. In Proceedings of MT 
 Summit X, Phuket: Thailand. 
 
Bouquiaux, Luc and J.M.C. Thomas. 1992.  
Studying and Describing Unwritten Lan-
guages. Dallas, TX: The Summer Institute of 
Linguistcs. 
 
Comrie, Bernard and N. Smith. 1977.  
  Lingua descriptive series: Questionnaire. In:      
  Lingua, 42:1-72. 
 
Haspelmath, Martin and Matthew S. Dryer,     
  David Gil, Bernard Comrie, editors. 2005    
  World Atlas of Language Strucutures. Oxford  
  University Press. 
 
Lori Levin, Alison Alvarez, Jeff Good, and     
  Robert Frederking. 2006 "Automatic Learning    
  of Grammatical Encoding." To appear in Jane 
  Grimshaw, Joan Maling, Chris Manning, Joan    
  Simpson and Annie Zaenen (eds)  
  Architectures, Rules and Preferences: A  
  Festschrift for Joan Bresnan , CSLI Publica  
  tions.  In Press. 
 
8
 
		
 	
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 78?86,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Inductive Detection of Language Features via Clustering Minimal Pairs:
Toward Feature-Rich Grammars in Machine Translation
Jonathan H. Clark, Robert Frederking, Lori Levin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jhclark,ref,lsl}@cs.cmu.edu
Abstract
Syntax-based Machine Translation systems
have recently become a focus of research
with much hope that they will outperform
traditional Phrase-Based Statistical Machine
Translation (PBSMT). Toward this goal, we
present a method for analyzing the mor-
phosyntactic content of language from an
Elicitation Corpus such as the one included in
the LDC?s upcoming LCTL language packs.
The presented method discovers a mapping
between morphemes and linguistically rele-
vant features. By providing this tool that
can augment structure-based MT models with
these rich features, we believe the discrimina-
tive power of current models can be improved.
We conclude by outlining how the resulting
output can then be used in inducing a mor-
phosyntactically feature-rich grammar for AV-
ENUE, a modern syntax-based MT system.
1 Introduction
Recent trends in Machine Translation have begun
moving toward the incorporation of syntax and
structure in translation models in hopes of gaining
better translation quality. In fact, some structure-
based systems have already shown that they can out-
perform phrase-based SMT systems (Chiang, 2005).
Still, even the best-performing data-driven systems
have not fully explored the depth of such linguistic
features as morphosyntax.
Certainly, many have brought linguistically moti-
vated features into their models in the past. Huang
and Knight (2006) explored relabeling of non-
terminal symbols to embed more information di-
rectly into the backbone of the grammar. Bonneau-
Maynard et al (2007) argue that incorporation of
morphosyntax in the form of a part of speech (POS)
language model can improve translation. While
these approaches do make use of various linguis-
tic features, we have only begun to scratch the sur-
face of what actually occurs in the languages of the
world. We wish to address such issues as case mark-
ing, subject-verb agreement, and numeral-classifier
agreement by providing models with information
about which morphemes correspond to which gram-
matical meanings.
2 Task Overview
Feature Detection is the process of determining from
a corpus annotated with feature structures (Figure 2)
which feature values (Figure 1) have a distinct rep-
resentation in a target language in terms of mor-
phemes (Figure 3). By leveraging knowledge from
the field of language typology, we know what types
of phenomena are possible across languages and,
thus, which features to include in our feature speci-
fication.
But not every language will display each of these
phenomena. Our goal is to determine which fea-
ture values (e.g. singular, dual, plural) have a dis-
tinct encoding in a given target language. Viewed
differently, we can ask which feature values can be
clustered by similarity. For instance, in Chinese, we
would expect singular, plural and dual to be mem-
bers of the same cluster (since they are typically not
explicitly expressed), while for Arabic we should
place each of these into separate clusters to indicate
they are each grammaticalized differently. Similarly,
78
Feature Name Feature Value Comment
np-gen m ,f, n Biological Gender
np-def +, - Definiteness
np-num sg, dl, pl Number
c-ten past, pres, fut Tense
np-function act, und Actor and undergoer participant roles
c-function main, rel Main and relative clause roles
Figure 1: An example feature specification.
ID Source Language Target Language Lexical Cluster Feature Structure
s1 He loves her. El ama a ella. `1 ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s2 She loves her. Ella ama a ella. `1 ((act (np-gen f) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s3 He loved her. El *ama a ella. `1 ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten past))
s4 The boy eats. El nin?o come. `2 ((act (np-gen m) (np-num sg) (np-def +)) (c-ten pres))
s5 The girl eats. La nin?a come. `2 ((act (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s6 A girl eats. Una nin?a come. `2 ((act (np-gen f) (np-num sg) (np-def -)) (c-ten pres))
s7 The girls eat. Las nin?as comen. `2 ((act (np-gen f) (np-num pl) (np-def +)) (c-ten pres))
s8 The girls eat. Las nin?as comen. `2 ((act (np-gen f) (np-num dl) (np-def +)) (c-ten pres))
s9 Girls eat. Unas nin?as comen. `2 ((act (np-gen f) (np-num pl) (np-def -)) (c-ten pres))
Figure 2: An example of sentences that might be found in an elicitation corpus. Notice that each sentence differs from
some other sentence in the corpus by exactly one feature value. This enables us to see how the written form of the
language changes (or does not change) when the grammatical meaning changes.
English would have two clusters for the feature num-
ber: (singular) and (dual, plural). Further, we would
like to determine which morphemes express each of
these values (or value clusters). For example, En-
glish expresses negation with the morphemes no and
not, whereas questions are expressed by reordering
of the auxiliary verb or the addition of a wh-word.
Though many modern corpora contain feature-
annotated utterances, these corpora are often not
suitable for feature detection. For this purpose, we
use an Elicitation Corpus (see Figure 2), a corpus
that has been carefully constructed to provide a large
number of minimal pairs of sentences such as He
sings and She sings so that only a single feature (e.g.
gender) differs between the two sentences. Also, no-
tice that the feature structures are sometimes more
detailed than the source language sentence. For ex-
ample, English does not express dual number, but
we might want to include this feature in our Elicita-
tion Corpus (especially for a language such as Ara-
bic). For these cases, we include a context field for
the translator with an instruction such as ?Translate
this sentence as if there are two girls.?
In the past, we proposed deductive (rule-based)
methods for feature detection (Clark et al, 2008).
In this paper, we propose the use of inductive fea-
ture detection, which operates directly on the feature
set that the corpus has been annotated with, remov-
ing the need for manually written rules. We define
inductive feature detection as a recall-oriented task
since its output is intended to be analyzed by a Mor-
phosyntactic Lexicon Generator, which will address
the issue of precision. This, in turn, allows us to in-
form a rule learner about which language features
can be clustered and handled by a single set of rules
and which must be given special attention. How-
ever, due to the complexity of this component, de-
scribing it is beyond the scope of this paper. We also
note that future work will include the integration of a
morphology analysis system such as ParaMor (Mon-
son et al, 2007) to extract and annotate the valuable
morphosyntactic information of inflected languages.
An example of this processing pipeline is given in
Figure 4.
79
Feature Value Candidate Morphemes
np-gen m el, nin?o
np-gen f ella, nin?a
np-gen n *unobserved*
np-def + el, la, las
np-def - una, unas
np-num sg el, ella, la, una, come, nin?o, nin?a
np-num dl-pl las, unas, comen, nin?as
c-ten past-pres ?
c-ten fut *unobserved*
Figure 3: An example of the output of our system for the above corpus: a list of feature-morpheme pairings.
Elicitation
Corpus
Inductive
Feature
Detection
Morphosyntactic
Lexicon
Generator
Unsupervised
Morphology
Induction
Grammar
Rule
Learner
Decoder
Figure 4: An outline of the steps from an input Elicitation Corpus to the application of a morphosyntactically feature
rich grammar in a MT decoder. This paper discusses the highlighted inductive feature detection component. Note that
this is just one possible configuration for integrating inductive feature detection into system training.
3 The Need to Observe Real Data
One might argue that such information could be ob-
tained from a grammatical sketch of a language.
However, these sketches often focus on the ?inter-
esting? features of a language, rather than those that
are most important for machine translation. Fur-
ther, not all grammatical functions are encoded in
the elements that most grammatical sketches focus
on. According to Construction Grammar, such in-
formation is also commonly found in constructions
(Kay, 2002). For example, future tense is not gram-
maticalized in Japanese according to most reference
sources, yet it may be expressed with a construction
such as watashi wa gakoo ni iku yode desu (lit. ?I
have a plan to go to school.?) for I will go to school.
Feature detection informs us of such constructional-
ized encodings of language features for use in im-
proving machine translation models.
Recognizing the need for this type of data, the
LDC has included our Elicitation Corpus in their
Less Commonly Taught Languages (LCTL) lan-
guage packs (Simpson et al, 2008). Already, these
language packs have been translated into Thai, Ben-
gali, Urdu, Hungarian, Punjabi, Tamil, and Yoruba.
With structured elicitation corpora already being
produced on a wide scale, there exists plenty of data
that can be exploited via feature detection. Some of
these language packs have already been released for
use in MT competitions and they will start being re-
leased to the general research community this year
through LDC?s catalog.
4 Applications
4.1 Induction of Feature-Rich Grammars
Given these outputs, a synchronous grammar in-
duction system can then use these feature-annotated
morphemes and the knowledge of which features are
expressed to create a feature rich grammar. Consider
the example in Figure 5, which shows Urdu subject-
verb agreement taking place while being separated
by 12 words. Traditional n-gram Language Mod-
els (LM?s) would not be able to detect any disagree-
ments more than n words away, which is the nor-
mal case for a trigram LM. Even most syntax-based
systems would not be able to detect this problem
without using a huge number of non-terminals, each
marked for all possible agreements. A syntax-based
system might be able to check this sort of agree-
80
ek talb alm arshad jo mchhlyoN ke liye pani maiN aata phink raha tha . . .
a.SG student named Irshad who fish for water in flour throw PROG.SG.M be.PAST.SG.M
?A student named Irshad who was throwing flour in the water for the fish . . . ?
Figure 5: A glossed example from parallel text in LDC?s Urdu-English LCTL language pack showing subject-verb
agreement being separated by 12 words.
ment if it produced a target-side dependency tree as
in Ding and Palmer (2005). However, we are not
aware of any systems that attempt this. Therefore,
the correct hypotheses, which have correct agree-
ment, will likely be produces as hypotheses of tra-
ditional beam-search MT systems, but their features
might not be able to discern the correct hypothe-
sis, allowing it to fall below the 1-best or out of the
beam entirely. By constructing a feature-rich gram-
mar in a framework that allows unification-based
feature constraints such as AVENUE (Carbonell et
al., 2002), we can prune these bad hypotheses lack-
ing agreement from the search space.
Returning to the example of subject-verb agree-
ment, consider the following Urdu sentences taken
from the Urdu-English Elicitation Corpus in LDC?s
LCTL language pack:
Danish ne Amna ko sza di
Danish ERG Amna DAT punish give.PERF
?Danish punished Amna.?
Danish Amna ko sza dita hai
Danish Amna DAT punish give.HAB be.PRES
?Danish punishes Amna.?
These examples show the split-ergativity of Urdu
in which the ergative marker ne is used only for
the subject of transitive, perfect aspect verbs. In
particular, since these sentences have the perfect
aspect marked on the light verb di, a closed-class
word (Poornima and Koenig, 2008), feature detec-
tion will allow the induction of a grammar that per-
colates a feature up from the VP containing di in-
dicating that its aspect is perfect. Likewise, the NP
containing Danish ne will percolate a feature up in-
dicating that the use of ne requires perfect aspect.
If, during translation, a hypothesis is proposed that
does not meet either of these conditions, unification
will fail and the hypothesis will be pruned 1.
Certainly, unification-based grammars are not the
1If the reader is not familiar with Unification Grammars, we
recommend Kaplan (1995)
only way in which this rich source of linguistic infor-
mation could be used to augment a structure-based
translation system. One could also imagine a system
in which the feature annotations are simply used to
improve the discriminative power of a model. For
example, factored translation models (Koehn and
Hoang, 2007) retain the simplicity of phrase-based
SMT while adding the ability to incorporate addi-
tional features. Similarly, there exists a continuum
of degrees to which this linguistic information can
be used in current syntax-based MT systems. As
modern systems move toward integrating many fea-
tures (Liang et al, 2006), resources such as this will
become increasingly important in improving trans-
lation quality.
5 System Description
In the following sections, we will describe the pro-
cess of inductive feature detection by way of a run-
ning example.
5.1 Feature Specification
The first input to our system is a feature specification
(Figure 1). The feature specification used for this ex-
periment was written by an expert in language typol-
ogy and is stored in a human-readable XML format.
It is intended to cover a large number of phenom-
ena that are possible in the languages of the world.
Note that features beginning with np- are partici-
pant (noun) features while features beginning with
c- are clause features. The feature specification al-
lows us to know which values are unobserved during
elicitation (that is, no sentence having that feature
value was given to the bilingual person to translate).
This is the case for the first four features and their
values in Figure 1. The last two function features
and their values tell us what possible roles partici-
pants and clauses can take in sentences.
81
5.2 Elicitation Corpus
As outlined in Section 3, feature detection uses an
Elicitation Corpus (see Figure 2), a corpus that has
been carefully constructed to provide a large num-
ber of minimal pairs of sentences such as He sings
and She sings so that only a single feature (e.g. gen-
der) differs between the two sentences (Levin et al,
2006; Alvarez et al, 2006). If two features had var-
ied at once (e.g. It sang) or lexical choice varied
(e.g. She reads), then making assertions about which
features the language does and does not express be-
comes much more difficult.
Notice that each input sentence has been tagged
with an identifier for a lexical cluster as a pre-
processing step. Specifying lexical clusters ensures
that we don?t compare sentences with different con-
tent just because their feature structures match. For
example, we would not want to compare Dog bites
man and Man bites dog nor The student snored
and The professor snored. Note that bag-of-words
matching is insufficient for this purpose.
Though any feature-annotated corpus can be used
in feature detection, the amount of useful informa-
tion extracted from the corpus is directly dependent
on how many minimal pairs can be formed from the
corpus. For instance, one might consider using a
morphologically annotated corpus or even an auto-
matically parsed corpus in place of the elicitation
corpus. Even though these resources are likely to
suffer from having very sparse minimal pairs due to
their uncontrolled usage of vocabulary, they might
still contain some amount of useful information.
However, since we seek both to apply these methods
to language for which there are currently no man-
ually annotated corpora and to investigate features
that existing parsers generally cannot identify (e.g.
generic nouns and evidentiality), we will not men-
tion these types of resources any further.
5.3 Minimal Pair Clustering
Minimal pair clustering is the process of grouping
all possible sets of minimal pairs, those pairs of sen-
tences that have exactly one difference between their
feature structures. We use wildcard feature struc-
tures to represent each minimal pair cluster. We de-
fine a wildcard feature as any feature whose value
is *, which denotes that the value matches another *
rather than its original feature value. Similarly, we
define the feature context of the wildcard feature be
the enclosing participant and clause type for a np-
feature or the enclosing clause for a c- type fea-
ture. Then, for each sentence s in the corpus, we
substitute a wildcard feature for each of the values v
in its feature structure, and we append s to the list
of sentences associated with this wildcard feature
structure. A sample of some of the minimal pairs
for our running example are shown in Figure 6.
Here, we show minimal pairs for just one wild-
card, though multiple wildcards may be created if
one wishes to examine how features interact with
one another. This could be useful in cases such as
Hindi where the perfective verb aspect interacts with
the past verb tense and the actor NP function to add
the case marker ne (for split ergativity of Urdu, see
Section 4.1). That said, a downstream component
such as a Morphosyntactic Lexicon Generator would
perhaps be better suited for the analysis of feature in-
teractions. Also, note that the feature context is not
used when there is only one wildcard feature. The
feature context becomes useful when multiple wild-
cards are added in that it may also act as a wildcard
feature.
The next step is to organize the example sentences
into a table that helps us decide which examples can
be compared and stores information that will inform
our comparison. Briefly, any two sentences belong-
ing to the same minimal pair cluster and lexical clus-
ter will eventually get compared. As specified in Al-
gorithm 1, we create a table like that in Figure 7.
Having collected this information, we are now ready
to begin clustering feature values.
Algorithm 1 Organize()
Require: Minimal pairs, lexical clusters, and the
feature specification.
Ensure: A table T of comparable examples.
for all pair m ? minimalPairs do
for all sentence s ? m do
f? wildcardFeature(s, m)
v? featureValue(s, f)
c? featureContext(m)
`? lexCluster(s)
T[f,m, c, `, v]? T[f,m, c, `, v]? s
return T
82
ID Set Members Feature Feature Context Feature Structure
m1 {s1, s2} np-gen ((act)) ((act (np-gen *) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
m2 {s1, s3} np-ten () ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten *))
m3 {s4, s5, s7, s8} np-gen ((act)) ((act (np-gen *) (np-num sg) (np-def +)) (c-ten pres))
m4 {s5, s7, s8} np-num ((act)) ((act (np-gen f) (np-num *) (np-def +)) (c-ten pres))
m5 {s6, s9} np-num ((act)) ((act (np-gen f) (np-num *) (np-def -)) (c-ten pres))
m6 {s5, s6} np-def ((act)) ((act (np-gen f) (np-num sg) (np-def *)) (c-ten pres))
m7 {s7, s9} np-def ((act)) ((act (np-gen f) (np-num pl) (np-def *)) (c-ten pres))
Figure 6: An example subset of minimal pairs that can be formed from the corpus in Figure 2.
Feature Min. Pair Feat. Context Lex. Cluster Feat. Value. Sentence
np-gen m1 ((act)) `1 m s1
np-gen m1 ((act)) `1 f s2
np-ten m2 () `1 pres s1
np-ten m2 () `1 past s3
np-num m4 ((act)) `2 sg s5
np-num m4 ((act)) `2 pl s7
np-num m4 ((act)) `2 dl s8
np-num m5 ((act)) `2 sg s6
np-num m5 ((act)) `2 pl s9
Figure 7: An example subset of the organized items that can be formed from the minimal pairs in Figure 6. Each item
that has a matching minimal pair ID, feature context, and lexical cluster ID can be compared during feature detection.
5.4 Feature Value Clustering
During the process of feature value clustering, we
collapse feature values that do not have a distinct
encoding in the target language into a single group.
This is helpful both as information to components
using the output of inductive feature detection and
later as a method of reducing data sparseness when
creating morpheme-feature pairings. We represent
the relationship between the examples we have gath-
ered for each feature as a feature expression graph.
We define a feature expression graph (FEG) for a
feature f to be a graph on |v| vertices where v is
the number of possible values of f (though for most
non-trivial cases, it is more conveniently represented
as a triangular matrix).
Each vertex of the FEG corresponds to a feature
value (e.g. singular, dual) while each arc contains
the list of examples that are comparable according
to the table from the previous step. The examples at
each arc are organized into those that had the same
target language string, indicating that the feature val-
ues are not distinctly expressed, and those that had
a different target language string, indicating that the
change in grammatical meaning represented in the
feature structure has a distinct encoding in the tar-
get language. Algorithm 2 more formally specifies
the creation of a FEG. The FEG?s for our running
example are shown in Figure 8. From these statis-
tics generated from these graphs, we then estimate
the maximum likelihood probability of each feature
value pair being distinctly encoded as shown in Fig-
ure 9.
The interpretation of these probabilities might not
be obvious. They estimate the likelihood of a lan-
guage encoding a feature given that the meaning of
that feature is intended to be conveyed. These proba-
bilities should not be interpreted as a traditional like-
lihood of encountering a given lexical item.
Finally, we cluster by randomly selecting a start-
ing vertex for a new cluster and adding vertices to
that cluster, following arcs out from the cluster that
have a weight lower than some threshold ?. When
no more arcs may be followed, a new start vertex is
selected and another cluster is formed. This is re-
peated until all feature values have been assigned to
a cluster. For our running example, we use ? = 0.6,
83
fm
n
{(s1, s2, NEQ), (s4, s5, NEQ), 
(s4, s7, NEQ), (s4, s8, NEQ)}
np-gen
{} {}
pls
dl
{(s5,s7, NEQ), (s6, s9, NEQ)}
{(s5, s8, NEQ)}
{(s7, s8, EQ)}
np-num
-+
{(s5, s6, NEQ), 
(s7, s9, NEQ))}
np-def
prespast
fut
{(s1, s2, NEQ)}
c-ten
{} {}
Figure 8: An example subset of the Feature Expression Graphs that are formed from the minimal pairs in Figure 7.
fm
n
| arcs[m,f] with (s
m
,s
f
,x,NEQ) |
| arcs[m,f] |
| arcs[m,n] with (s
m
,s
n
,x,NEQ) |
| arcs[m,n] |
| arcs[f,n] with (s
f
,s
n
,x,NEQ) |
| arcs[f,n] |
Figure 9: An example of how probabilities are estimated for each feature value pair in a Feature Expression Graph for
the feature np-gender.
Algorithm 2 Collecting statistics for each FEG.
Require: The table T from the previous step.
Ensure: A complete graph as an arc list with the
observed similarities and differences for each fea-
ture value.
for all si, sj ? T s.t. (mi, ci, `i) = (mj , cj , `j)
do
(vi, vj)? (featureValue(si), featureValue(sj))
if tgt(si) = tgt(sj) then
arcs[vi, vj ]? arcs[vi, vj ] ? (si, sj ,m,EQ)
else
arcs[vi, vj ]? arcs[vi, vj ] ? (si, sj ,m,NEQ)
return arcs
which results in the following clusters being formed:
np-gen: m, f
np-num: s, pl/dl
np-def: +, -
c-ten: past, pres
5.5 Morpheme-Feature Pairing
Finally, using the information from above about
which values should be examined as a group and
which sentence pairs exemplify an orthographic dif-
ference, we examine each pair of target language
sentences to determine which words changed to re-
flect the change in grammatical meaning. This pro-
cess is outlined in Algorithm 3. The general idea is
that for each arc going out of a feature value vertex
we examine all of the target language sentence pairs
that expressed a difference. We then take the words
that were in the vocabulary of the target sentence
for the current feature value, but not in the sentence
it was being compared to and add them to the list
of words that could be used to express this feature
value (Figure 3).
6 Evaluation and Results
We evaluated the output of feature detection with
one wildcard feature as applied to the Elicitation
Corpus from the LDC?s Urdu-English LCTL lan-
guage pack. Threshold parameters were set to small
values (? = 0.05). Note that an increase in precision
might be possible by tuning this value; however, as
stated, we are most concerned with recall.
An initial attempt was made to create a gold stan-
dard against which recall could be directly calcu-
lated. However, the construction of this gold stan-
dard was both noisier and more time consuming
than expected. That is, even though the task is
based on how a linguistic field worker might col-
84
Algorithm 3 Determine which morphemes are as-
sociated with which feature values.
Require: List of clusters C and list of FEGs F
Ensure: A list of morphemes associated with each
feature value
for all feature ? F do
for all vertex ? feature do
for all arc ? vertex do
for all (s1, s2,m,NEQ) ? arc do
v1 ? featureValue(s1,m)
v2 ? featureValue(s2,m)
if v1 6= v then (s1, v1)? (s2, v2)
w1 ? vocabulary(s1)
w2 ? vocabulary(s2)
? ?W1 ?W2
for all w ? freq do
freq[w]++
for all w ? freq do
p = freq[w] / ?w freq[w]
if p ? ?? then
morphemes[v]? morphemes[v]? w
return morphemes
lect data, it was more difficult for a human than
anticipated. Therefore, we instead produced a list
of hypothesized morpheme-feature pairs and had a
human trained in linguistics who was also bilingual
in Hindi/Urdu-English mark each pair as ?Correct,?
?Incorrect,? or ?Ambiguous.? The results of this
evaluation are summarized in Figure 10. The reader
may be surprised by how many incorrect hypothe-
ses were generated, given the controlled nature of
the Elicitation Corpus. However, there are two im-
portant factors to consider. First, features can in-
teract in complex and often unexpected ways. For
instance, in English, the only feature difference in
minimal pair Cats yawned and A cat yawned is the
number of the actor. However, this causes an in-
teraction with definiteness that would cause the pre-
sented algorithms to associate a with the number of
nouns even though it is canonically associated with
definiteness. Second, the bilingual people translat-
ing the Elicitation Corpus are prone to make errors.
Though a fair number of incorrect hypotheses
were produced, the number of correct hypotheses
is encouraging. We also note that the words be-
ing identified are largely function words and multi-
Judgment Morpheme-Feature Pairings
Correct 68
Ambiguous 29
Incorrect 109
TOTAL 206
Figure 10: The results of feature detection. Being a
recall-oriented approach, inductive feature detection is
geared toward overproduction of morpheme-feature pair-
ings as shown in the number of ambiguous and incorrect
pairings.
morpheme tokens from which closed-class func-
tional morphemes will be extracted. One might
think the counts extracted seem low when compared
to the typical MT vocabulary size, but these function
words that we extract cover a much larger probabil-
ity mass of the language than content words.
We are confident that the Morphosyntactic Lex-
icon Generator designed to operate directly down-
stream from this process will be sufficiently discrim-
inant to use these morpheme-feature pairings to cre-
ate a high precision lexicon. However, since this
component is, in itself, highly complex, its specifics
are beyond the scope of this paper and so we leave it
to be discussed in future work.
7 Conclusion
We have presented a method for inductive feature
detection of an annotated corpus, which determines
which feature values have a distinct representation
in a target language and what morphemes can be
used to express these grammatical meanings. This
method exploits the unique properties of an Elici-
tation Corpus, a resource which is becoming widely
available from the LDC. Finally, we have argued that
the output of feature detection is useful for exploit-
ing these linguistic features via a feature-rich gram-
mar for a machine translation system.
Acknowledgments
We would like to thank our colleagues Alon Lavie,
Vamshi Ambati, Abhaya Agarwal, and Alok Par-
likar for their insights. Thanks to Keisuke Kamataki
for the Japanese example and to Shakthi Poornima
for her help with the Urdu examples. This work was
supported by US NSF Grant Number 0713-292.
85
References
Alison Alvarez, Lori Levin, Robert Frederking, Simon
Fung, Donna Gates, and Jeff Good. 2006. The MILE
corpus for less commonly taught languages. In HLT-
NAACL, New York, New York, June.
H. Bonneau-Maynard, A. Allauzen, D. De?chelotte, and
H. Schwenk. 2007. Combining morphosyntactic en-
riched representation with n-best reranking in statis-
tical translation. In Proceedings of the Workshop on
Structure and Syntax in Statistical Translation (SSST)
at NAACL-HLT.
Jaime Carbonell, Kathrina Probst, Erik Peterson, Chris-
tian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource limited
MT. In Association for Machine Translation in the
Americas (AMTA), October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Association for
Computational Linguistics (ACL).
Jonathan H. Clark, Robert Frederking, and Lori Levin.
2008. Toward active learning in corpus creation: Au-
tomatic discovery of language features during elicita-
tion. In Proceedings of the Language Resources and
Evaluation Conference (LREC).
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Meeting
of the Association for Computational Linguistics ACL.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of (NAACL-HLT).
Ronald Kaplan. 1995. The formal architecture of lexi-
cal functional grammar. In Mary Dalrymple, Ronald
Kaplan, J. Maxwell, and A. Zaenen, editors, Formal
Issues in Lexical Functional Grammar. CSLI Publica-
tions.
Paul Kay. 2002. An informal sketch of a formal archi-
tecture for construction grammar. In Grammars.
Phillipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Lori Levin, Jeff Good, Alison Alvarez, and Robert Fred-
erking. 2006. Parallel reverse treebanks for the dis-
covery of morpho-syntactic markings. In Proceedings
of Treebanks and Linguistic Theory, Prague.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics, Sydney.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2007. Paramor: Minimally supervised induc-
tion of paradigm structure and morphological analysis.
In Proceedings of the 9th ACL SIGMORPH.
Shakthi Poornima and Jean-Pierre Koenig. 2008. Re-
verse complex predicates in Hindi. In Proceedings of
the 24th Northwest Linguistic Conference.
Heather Simpson, Christopher Cieri, Kazuaki Maeda,
Kathryn Baker, and Boyan Onyshkevych. 2008. Hu-
man language technology resources for less commonly
taught languages: Lessons learned toward creation of
basic language resources. In Proceedings of the LREC
2008 Workshop on Collaboration: interoperability be-
tween people in the creation of language resources for
less-resourced langauges.
86
WebDIPLOMAT: A Web-Based Interactive Machine Translation 
System 
Christol)her Hogan and Robert  Frederking 
I,anguagc Techliologies \]institute 
Pittsburgh, \]~ennsylva.nia, USA 
chogan~e-l?ngo, c m, ref@cs, cmu. edu 
Abst ract  
We have implenlented a.n interactive, Wel)-based, 
chat-style machine translation system, SUpl)ort;ing 
speech recognition and synthesis, local- or third- 
party correction of speech recognition and machine 
tra.nslation output, a.nd online learning. The un- 
derlying client-server architecture, implemented in
.la.va TM, pl:ovides remote, distributed computation 
for the translation and speech sut)systems. We fur- 
ther describe our Web-based user interthces, whMi 
can easily produce different uscflfl eonfigllrartions. 
1. Introduct ion 
The World Wide Web (Berners-l,ee, 11989) seems 
to be all ideal environment for machine transla- 
tion: it is easily accessible around the world using 
freely-available, easy-to-use tools which are ava.ilable 
to persons speaking a. nlyriad of langua.ges, all of 
whom would like to I)e able to communicate with 
one another without language barriers. IlL. is there- 
fore not too surl)rising that a few companies have 
attempted to make machine translation available 
in this medium (AltaVista, 1999; FreeTranslation, 
1.q99; hlt.erTran, 1999). '.l'he l)riinary use identified 
for these tra.nslators has been that of translating 
Web pages or amusing oneself with the inadequa.- 
cies of ma.dfine translation (Yang and l,ange, 1998). 
What these systems cannot be used for is real-time, 
speech-to-speech ommunication with translation. 
l{eal-time communication over the hiternet has 
more properly been the (lomain of '<chat" l)roto- 
eels: primarily Interact Relay Chat (11{(3) (Oikari- 
nen and Reed, 1993), and similar instant messaging 
protocols developed commercially (America Online 
Inc., 2000; Microsoft Corp., 2000; ICQ Inc., 1999). 
While some portals have been developed to permit 
access to chat using the Web (iTRiBE lnc., 1996), 
the primary point of access eems to be chat-specific 
client software. Although chat defines protocols and 
provides infrastructure, it is limited ill the kind of 
data that it can transl)ort, and client software is 
tightly focussed oil the text domain. Such limita- 
tions have not, however, prevented researchers fi'om 
exl)erilnenting with the possibilities of incorporat- 
ing machine translation or speech into tile chat ex- 
perience (1,enzo, 1998; Seligma.n et al, 1998). The 
outcome of these experiments has been to show that 
comn-mrcial machine translation systems may 1)e rea- 
sonably integrated into the chat room, and that com- 
mercial speech software ca.n be connected to existing 
chat software to provide the desired experience. 
We have taken a difl~rent road. It has been noted 
(Seligman, 19.(.)7; l"rederking et al, 2000) that broad- 
coverage machine translation and speech recognition 
cannot now be usefld mdess users can interact with 
the system to improve results. While Seligman et 
al. (1998) were able to etDct user editing of speech 
recognition by editing text before submitting it for 
translation, they were unable to do the same for tile 
translation system, prilnarily due to limitations of 
commercial software. Additional imitations are en- 
countered in the communication medium: chat is 
not amenable to non-text interaction with transla- 
tion agents, and commercial chat software does not, 
in any case, support such interaction. 
To deal with these limitations, we have developed 
a fully interactive, Web-based, chat-style tra.nslation 
system, supporting sl)eech recognition and synthesis, 
local-or third-1)arty correction of speech reeognitioi, 
and machine translation, and online learning, which 
ca.n be used with nothing lllore than a Well browser 
and some simple add-ons. All intensive processing, 
including translation and speech recognition is per- 
formed a.t central servers, permitting access for those 
with limited computational resources. In a.ddition, 
tile modular design of t.he system and interface per- 
mit computa.tional tasks to be easily distributed and 
different dialog configurations to be explored. 
2 In ter face  Des ign  
The design of the Webl)IPLOMAT system is in- 
tended to facilitate the following kind of interaction: 
(numbers correspond to Figure 1) 
1. Speech fl'om the user is recognized and dis- 
played in an editing window, where it may be 
edited by respeaking or using the keyboard. 
2. When text is acceptable to the user, it is sub- 
mitted tbr translation and transfer to the other 
1041 
' ,  . . . . . . . .  . I  ; 5 . . . .  
........ I ' 
-- -v  ) 
Figure 1: User-level perspective on information flow. 
See text for explanation of labels. 
l)arty. 
3. Text to be translated is optionally presented to 
a human expert, who is able to translate, cor- 
rect and teach the system a correct translation. 
4. Upon machine translation of tlLe text, or accep- 
tance by the expert, a translation is delivered 
to the other pa.rty and synthesized. 
5. 13oth sides of the conversation are tracked a.u- 
tomatically for all users, and displayed on their 
interfaces. 
Although the above is the original vision for tihe 
system, other configurations are easily imagined. 
Configurations with more than two participants, or 
where one of the users is also simultaneously all ex- 
pert are stra.ightforwardly handled. International- 
ization of the interfaces, for use in different locales, is 
also easily handled. Many changes of this nature are 
handled by easy modifications to the HTMI, code for 
given \?eb pages. More COml)licated tasks may be 
accomplished by modifications of underlying code. 
In order to produce the above configuration, the 
current system implements two user interthces (UIs): 
the Client UI, which provides peech and text input 
capabilities to the primary end-users of the system; 
and the Editor UI, which provides translation edit- 
ing capabilities to a human translation expert, in 
the rest of this section, we describe in detail certain 
unique aspects of each interface. 
2.1 Cl ient User  Interface 
In addition to speech-input and editing capabilities, 
the Client UI is able to track the entire dialog as 
it progresses. Because the Central Communications 
Server (@ ~a.l) forwards every message to all con- 
nected clients, every component of the system can be 
aware of how the dialog turn is proceeding. Ill tile 
Client UI, this capability is used to l)rovide a run- 
ning transcript of the conversation as it occurs. By 
noting the identifiers on messages (cf. ~,3.4), the U1 
can assign appropriate labels to each of the follow- 
ing: our original utterance, translation of our utter- 
ance, other person's utterance, translation of their 
utterance. In ~ddition, we use knowledge about the 
status of the dialog to prevent the user from send- 
ing several utterances belbre the other party has re- 
sponded. 
2.2 Ed i to r  User  In ter face  
The F, ditor UI provides tools which make it possible 
for a human expert to edit translations produced 
by the machine translator betbre they are sent to 
the users. As mentioned earlier, the editing step is 
optional, and is intended to improve the quality of 
transla.tions. The Editor UI may be configured so 
that either of the two users, or a remote third party 
can act as editor. Onr motivations for providing an 
editing capability are twofold: 
? Although our MT system (@ ~3.2) dots not 
always produce the correct answer, the correct 
answer is usually available a.mong the possibili- 
ties it. considers. 
t .a l  Q ? ,H~ MT system provides for online updates of 
its knowledge base which a.llows tbr translations 
to improve over time. 
In order to take advantage of' these capabilities, we 
have designed two editing tools, the chart editor and 
a.lways-active l arning, that enable a human expert 
to rapidly produce an accurate tlJaillslatioll aud to 
store tha.t translation in the MT knowledge base for 
future use. 
As discussed in ~a.2, our MT system ma.y produce 
more than one translation for each par t  of tile input, 
from which it attempts to se\]ect the best translation. 
The entire set of translations i available to the Web- 
I ) IPLOMAT system, and ix used in the cha.rt editor. 
By double-clicking on words in the translation, the 
Original English 
My name is John . . . . . . .  
Edited Frencll 
l inen nora estJehn 
Figure 2: Popup Chart Editor 
1042 
human edit()\]: is l)resented a. pol)Ul)-menu of alterna.- 
tire tra.nslations beginning a.t a particular location 
in the sentence (see l?igure 2). When one o\[' the al- 
ternatives is sek;cted, it replaces the original word or 
words. In this way, a. sentence may be rapidly edited 
to an acceptable sta.te. 
In order to reduce develolmmnt \]line, our MT sys- 
tem can be used in a ra.pid-del)loylnent style: afl;er a. 
minimal knowledge base is constructed, the system 
is put into use with a huma.n expert supervising, so 
that domain-rel(:va.nt data ma.y be elicited (lui(:ldy. 
In order to supl)ort this, all uttera.nces a.re consid- 
ered for learning. When the editor presses the 'Ac- 
ccitt/Learn' l)utton, the original utterance and its 
tra.nslatiotl are exa.ntined to determine if they are 
suital)le for learning. (Turrently all utterances for 
which the forward tra.nslation has 1teen edited are 
su brat\]ted \['or learning, a.lthough other criteria ma.y 
also be entertained. More detail about online lea.r|> 
ing may 1)e found ill ~3.2. 
Although the editor UI is primarily i\]lte\]l(led tbr 
use by a. tra.nslation expert, it, will sometimes also 1)e 
u,qed 1)y tllose who are not as expert. For this situa- 
ti:)n, we ha.re introduced it lta('ktra.lisla.l.ion capalJil- 
ity which retra.nsla.tos the edited forward trai/sla.tioll 
into the language of the input. Although i,~iperl'ect, 
baektranslatio\]l can often give the user an idea of 
whether the forward transla.tion was suits\]ant\]ally 
(:O\]:l:eot,. 
3 System_ Design 
h, this section, we describe 1.he eOml)uta,l, io\]|al archi- 
t()etu r(" \[lllderlyin,,g the W(;b I) 11) I,O M A'I' sys| ,e l l l .  
3.1. Ar(:hite('t;m'( ~. 
The underlyil\]g arel\]itecture of the \?obl)II)I,OMAT ' 
system is shown in Figure 3. The system is organized 
arotllld three servel:s: 
The We.It Serv<'.r serves I1T\]Vll, l)ages to <:lients. 
We used an unmodified version of th<; Apache 
ll'l"l'l) Server (Apache Softwa.re l:oundation, 
1999). 
Tim SI)eech Recogniz( : r (s)  l)erform speech 
recognition for clients. 
The Cent ra l  Commmf icat ions  Server  allows 
comrmmica.tion between clients, l,hicapsulated 
oh.jeers sent to this server are forwarded to 
all connected clients. With the exception of 
speech and HTTP,  all communications between 
clients use this server. 
The servers are designed to be small, and a.re in~ 
tended to coexist on one lnachine. 1 Currently, how- 
ever, the speech server inchides a full speech recog- 
l This is necessary due to security restrictions on .\]~twt 'I'M 
Applets. 
nizer, a.nd therefore consunies a greater amount o1' 
resources than the other servers. 
Most processing is intended Co be perforumd by 
clients, which haw~' no loca.lity requirements, and 
may therefore I)e distributed across nm.chi\]les and 
networks as necessary. The User and Editor Clients 
were described in {i?2.1 and 2.2. We will now ex- 
amine the most important l~rocessing mechanisms, 
ilmluding machine translation and speech recogni- 
tion/synthesis. 
3.2 Mach ine  Trans la t ion  
l"or Machine Transla.tion, we rely on the l)anlite 
M|dti-lgl\]gine Machine Translation (MEMT) Server 
(l:rederking a.nd lh:own, 1996). This system, which 
is outlined in Figure 4, makes use of several trans- 
lation engines at once, combining their output with 
a. sta.tistica\] language model (Brown and l:rederk- 
ing, 1995). Each traiisla.tion engine makes use of a 
dill'ere|tt transla.tion technok)gy, and produ(:es multi- 
t)1% possibly overlal)ping , l.ra\]mlations for every part 
of tit(; inl)ut that it can translate. All of the trans- 
lations I)roduced 1)3: the various engines a,re pla.ced 
in a chart data struci;ure (Kay, 1967; Winograd, 
1983), indexed by the'Jr position i\]\] the input utter- 
a.nce. A statistical huiguage model is used, together 
with scores provided I)y the tra.nslation engines, to 
determine the optima.l path through the set of trans- 
lated segments, which informa,tion is also stored i\]\] 
the chart. Upon completion of tra.nslation, the chart 
data struct||re is made a.vailable For use by the rest 
o\[7 the WeM)II)I,OMA:I ' system. 
(;urrently, we enq)loy l,exica.l Transfer and Ex- 
Source Target 
Language Language 
Morphological I Analyzer i~\ [  User Interthce 
- i~  ii'ransfer-Based MT 
- i~ Example-Based MT Statistical Modeller 
Knowledge-Based MT 
Expansion slot 
Figure d: Multi-Engine Machine 3h:ansla.tion Archi- 
tecture 
1043 
MEMT 
Server 
MT 
Interface 
i Speech Synthesizer ', Central Web 
'Recognizer(s) Server i Interface Server 
..... i-'"3"r":q~'-'7"~"~i' --\]'; 7 7~'-'- -" . . . .  " ...................... "':':'i;:": \] :):')}:'i}}i"':":":':i:i :: ";" :':\]'17'ii'i ........... I l l \[el 'net 
.,.'" / ...... ",, ................ ....." . . . . . .  .,. 
1 
Speech User l Speech Speech User2 Speech / 
Plugin Client Synth. Plugin Client Synth. Editor Client 1/ Editor Client 2 
Figure 3: Server 
ample Based Machine Translation (EBMT) engines 
(Na.gao, 1984; Brown, 1996). Lexical Transfer uses 
bilingual dictionaries and phrasal glossaries to pro- 
vide phrase-for-phrase translations, while EBMT 
uses a fllzzy matching step to produce translations 
froln a bilingual corpus of matched sentence pairs. 
Because the knowledge bases for these techniques are 
simple, they both suI)port online augmentation. As 
mentioned in ?2.2, the Editor UI attempts to learn 
from utterances that have been edited. Pairs of ut- 
terances ubmitted for learning to the translator are 
placed in a Lexical Transfer glossary if less than six 
words long, and in an EBMT corpus if two words 
or longer. Higher scores are given to these newly 
created resources, so that they are preferred. 
The MT server is interfa.ced to the Central Server 
through MT interfa.ce clients, which handle, inter 
alia, character set conversions, support for learning 
and conversion of MT output into an internal ob- 
ject representation usable by other clients. It also 
ensures that outgoing translations are staml)ed with 
correct identifiers (cf. ~3.4), relative to the incoming 
text, to ensure that translations are directed to the 
appropriate clients. 
a.a Speech Recognition and Synthesis 
In the current system, speech recognition is handled 
as a private communication between a browser plug- 
in, running on the user's machine, and a speech 
recognition server, and is not routed through the 
central server. Speech is streamed over the network 
to the server, which performs the recognition, and 
returns the results as a text string. This configura- 
tion permits most of the computational resources to 
be offloaded from the client machine onto powerful 
remote servers. The speech may be streamed over 
the network as-is, or it may be lightly preprocessed 
into a feature stream for use over lower-bandwidth 
connections. The recognized text is returned di- 
Architecture 
rectly to tile user client for editing and validation 
by the user belbre heing sent for translation. Our 
speech server is a previously implemented esign 
(Issar, 1997) based on the Sphinx II speech recog- 
nizer (Huang et a l., 1992). As mentioned earlier, 
the speech server and recognizer are not currently 
designed to run in a distributed fashion. 
Unlike speech recognition, which is handled by 
the User Client, speech synthesis does not require 
human interaction, and can therefore be connected 
directly to the central server. Currently, Synthe- 
sizer Interfaces unpackage internal representations 
and send utterances to be synthesized on a speech 
synthesizer unning locally on the user's machine. 
Future plans call for speech to be synthesized at a 
central ocation and transported across the net.work 
in standard andio formats. 
3.4 Imp lementat ion  
All components of the Webl)IPLOMA'\]' except the 
speech components and Web Server were imple- 
mented in Java TM (Gosling et el., 1996), inclnding 
the Central Server. Messages between clients are 
implemented as a Java class Capsule, containing a 
S t r ing  identifier and any number of data. Objects. 
Object serialization permits simple implementation 
of message streams. User Interface clients are de- 
veloped as Applets, which are embedded in HTML 
pages served by the Web Server. 
4 Future Work and Conclusion 
The most significant change we would like to make 
to the current system is the way that speech is han- 
dled. We firmly believe that the best speech input 
device is the one people are already familiar with, 
namely the telephone. A revised system would al- 
low users to call specific phone numbers (connected 
to the central server) in order to access the system, 
which would then recognize and synthesize speech 
1044 
over tile telephone line while still using web-based in- 
terfaces. This, of COtlrse, takes us closer to the grand 
AI Challenge of the translating telephone (OAIAE, 
1996; Kurzweil, 1999; Frederking et al, 1999). We 
contend that by using interactive machine transla- 
tion, the goal of a broad-domain translating tele- 
phone Call be more easily brought o fruition. 
References  
AltaVista. 1999. Babel Fish: A SYSTI{AN transla- 
tion system, http://babelfish.altavista.com/. 
America Ojflit\e Inc. 2000.  AOI, Instant 
MessengertSm). http://www.aol.com/aim/home. 
html. 
The Apache Software Foundation. 1999. The 
Apache H' I 'T I  ) Server Project. ht tp : / /www.  
apache.org. 
Tim Berners-l,ee. 1989. Informa.tion manage- 
ment: A proposal, http://www.w3.org/History/ 
1989/proposal.html, March. CI!;RN. 
l~.alf Brown and Robert Frederking. 1995. Ap- 
plying statistical English language modeling to 
symholic machine translation. \]n Proceedings of 
the ,5'ixlh International Uo~dbrence on 7'heorctical 
and Methodological Issues in Machine Trcmslation 
(TMI-95), pages 221-239. 
Ralf Brown. 1996. Example-based inachine transla- 
tion in the Pangloss ystem. In Proccedirtg.s of the 
l(ith International Co~@rencc o1~ Computational 
Lingttistics (COIJNG-96). 
Robert l,'rcderking and l{alf Ih'own. 1996. The 
Pangloss-lAte machine translation system. In Pro- 
ceedings of the Col~ference of the Association for 
Machine 7)'anslation in the Americas (AMTA). 
Robert Frederking, Christol)hel: logan, and Alexan- 
der l{udnicky. 1999. A new approach to the trans- 
lating telephone. In l)wcccdiltfls of the Mac\ira: 
7)'anslalion 5'ummit VII: 1147' i~t i, hc G'tvat 7)'ans- 
lation I';ra, Singapore, September. 
lb)bert Frederking, Alexander Rudnicky, Christo- 
pher Hogan, and Kevin Lenzo. 2000. Interactive 
speech translation i  the DIPLOMAT project. MT 
Journal. To appear. 
l"ree'lS:anslation. 1999. Free'l'rmMation: A Trans- 
parent l,anguage translation system, http://www. 
freetranslation.com/. 
James Gosling, Bill .loy, and (luy L. Steele, Jr. 1996. 
7'he Java "pM La,~(luage ,5'pcciJication. Addison- 
Wesley Publishing Co. 
Xuedong Ihmng, Fileno Alleva, Hsiao-Wuen Hen, 
Mei-Yuh Hwang, and Ronald l{osenfeld. 1992. 
The SPHINX-II speech recognition system: An 
overview. 'l'echnicM l{el)ort CMU-CS-92-112, 
Carnegie Mellon University School of Computer 
Science. 
ICQ Inc. 1999. ICQ IRC Services. http://www.icq. 
COrn/. 
Inter'Dan. 1999. An lnterTran translation system. 
http://www.airsho.com/transLator3.htm. 
Snnil Issar. 1997. A speech interface for forms on 
WWW. In Proceedings of the 5th European Con- 
ferencc on ,5'peech Communication and 7'echnol- 
ogy, September. 
iTRiBE Inc. 1996..lil{C. http://virtual.itribe.net/ 
jirc/. 
Martin Kay. 1967. Experiments with a powerfi|l 
parser. In Proceedings of the 2~M lnternatio~ml 
COLING, Angust. 
Ray Kurzweil. 1999. The Age of ,5'piritual Ma- 
chines: I~Tten Computers Exceed tluman h~telli- 
flence. Viking Press. 
Kevin Lenzo. 1998. personal conmmnication. 
Microsoft Corp. 2000. MSN TM Messenger Service. 
http://messenger.msn.com/. 
M. Nagao. 1984. A \['ramework of a nlechanical 
translation between Japanese and English by anal- 
ogy principle. In A. Elithorn and l{. 13aneI:ii, ed- 
itors, Artificial and \]\]'uma~ Intelligc~cc. NN.I'O 
lhlblications. 
()\[\[ice of Arti\[icial Intelligence Analysis and F, vahm- 
lion OAIAE. 1996. Artificial intelligence -An ex- 
ecutive overview, http://www.ai.usma.edu:8080/ 
overview/cover.html. 
Jarkko Oikarinen and l)arren l/.eed. 1993. Internet 
relay chat protocol, ftp://ftp.demon.co.uk/pub/ 
doc/rfc/rfc1738.txt, l{equcst for Comments 1459, 
Network Worldng (-lroup. 
Mark Seligman, Mary Flanagan, and Sophie Toole. 
1998. Dictated input {'or broad-coverage speech 
tra.nslation. In Clare Voss and Fie Reeder, edi- 
tors, Workshop on Embedded MT' ,h'flstems: De- 
sign, Consh'uclion, and l'Jvahtatiol~ of 5'9slcms 
'with an 1147' Component, l,anghorne, Pennsylva- 
nia, October. AMTA. 
Mark Seligman. 1997. Six issues in speech trans- 
lation. In Steven Kra.uwer et al, editors, Spo- 
ken Language Translation Workshop, pages 83--89, 
Madrid, July. 
Terry Winograd. 1.983. Langua9e as a Co.qnitive 
Process. Volume 1: Syntax. Addison-Wesley. 
Jin Yang and Elke 1). Lange. 1998. SYSTllAN 
on AltaVista.: A user study on real-time ma- 
chine translation on tile Intcrnet. In l)avid Far- 
well et al, editors, Proceedings of the Third Con- 
ference of the Association for Machine 7;r(msla- 
lion in lhe Americas (AMTA '98), pages 275- 
285, Langhorne, Pennsylvania, October. Springer- 
Verlag. 
1045 
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 48?53
Manchester, August 2008
Speech Translation for Triage of Emergency Phonecalls in Minority 
Languages 
Udhyakumar Nallasamy, Alan W Black, Tanja 
Schultz, Robert Frederking 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213  USA 
udhay@cmu.edu, 
{awb,ref,tanja}@cs.cmu.edu 
Jerry Weltman 
Louisiana State University 
Baton Rouge,  
Louisiana 70802  USA 
jweltm2@lsu.edu 
 
Abstract 
We describe Ayudame, a system de-
signed to recognize and translate Spanish 
emergency calls for better dispatching. 
We analyze the research challenges in 
adapting speech translation technology to 
9-1-1 domain. We report our initial re-
search in 9-1-1 translation system design, 
ASR experiments, and utterance classifi-
cation for translation.  
1 Introduction 
In the development of real-world-applicable lan-
guage technologies, it is good to find an applica-
tion with a significant need, and with a complex-
ity that appears to be within the capabilities of 
current existing technology.  Based on our ex-
perience in building speech-to-speech translation, 
we believe that some important potential uses of 
the technology do not require a full, complete 
speech-to-speech translation system; something 
much more lightweight can be sufficient to aid 
the end users (Gao et al 2006). 
A particular task of this kind is dealing with 
emergency call dispatch for police, ambulance, 
fire and other emergency services (in the US the 
emergency number is 9-1-1).  A dispatcher must 
answer a large variety of calls and, due to the 
multilingual nature of American society, they 
may receive non-English calls and be unable to 
service them due to lack of knowledge of the 
caller language. 
                                               
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
    Figure 1. Ayudame system architecture 
 
As a part of a pilot study into the feasibility of 
dealing with non-English calls by a mono-lingual 
English-speaking dispatcher, we have designed a 
translation system that will aid the dispatcher in 
communicating without understanding the 
caller?s language. 
48
The fundamental idea is to use utterance clas-
sification of the non-English input.  The non-
English is first recognized by a speech recogni-
tion system; then the output is classified into a 
small number of domain-specific classes called 
Domain Acts (DAs) that can indicate directly to 
the dispatcher the general intended meaning of 
the spoken phrase.  Each DA may have a few 
important parameters to be translated, such as 
street addresses (Levin et al 2003; Langley 
2003). The dispatcher can then select from a lim-
ited number of canned responses to this through 
a simple menu system.  We believe the reduction 
in complexity of such a system compared to a 
full speech-to-speech translation will be advanta-
geous because it should be much cheaper to con-
struct, easier to port to new languages, and, im-
portantly, sufficient to do the job of processing 
emergency calls.  
In the ?NineOneOne? project, we have de-
signed an initial prototype system, which we call 
?Ayudame? (Spanish word for ?Help me?).  Fig-
ure 1 gives an overview of the system architec-
ture. 
2 The NineOneOne Domain 
Our initial interest in this domain was due to con-
tact from the Cape Coral Police Department 
(CCPD) in Florida.  They were interested in in-
vestigating how speech-to-speech translations 
could be used in emergency 9-1-1 dispatch sys-
tems.  Most current emergency dispatching cen-
ters use some proprietary human translation ser-
vice, such as Language Line (Language Line 
Services).  Although this service provides human 
translation services for some 180 languages, it is 
far from ideal.  Once the dispatcher notes that the 
caller cannot speak/understand English, they 
must initiate the call to Language Line, including 
identifying themselves to the Language Line op-
erator, before the call can actually continue.  This 
delay can be up to a minute, which is not ideal in 
an emergency situation.    
After consulting with CCPD, and collecting a 
number of example calls, it was clear that full 
speech-to-speech translation was not necessary 
and that a limited form of translation through 
utterance classification (Lavie et al 2001) might 
be sufficient to provide a rapid response to non-
English calls.  The language for our study is 
Spanish.  Cape Coral is on the Gulf Coast of  
Florida and has fewer Spanish speakers than e.g. 
the Miami area, but still sufficient that a number 
of calls are made to their emergency service in 
Spanish, yet many of their operators are not 
sufficiently fluent in Spanish to deal with the 
calls. 
There are a number of key pieces of 
information that a dispatcher tries to collect 
before passing on the information to the 
appropriate emergency service.  This includes 
things like location, type of emergency, urgency, 
if anyone is hurt, if the situation is dangerous, 
etc.  In fact many dispaching organizations have 
existing, well-defined  policies on what 
information they should collect for different 
types of emergencies.  
3 Initial system design 
Based on the domain's characteristics, in addition 
to avoiding full-blown translation, we are follow-
ing a highly asymmetrical design for the system 
(Frederking et al 2000).  The dispatcher is al-
ready seated at a workstation, and we intend to 
keep them ?in the loop?, for both technical and 
social reasons.  So in the dispatcher-to-caller di-
rection, we can work with text and menus, sim-
plifying the technology and avoiding some cog-
nitive complexity for the operator.  So in the dis-
patcher-to-caller direction we require  
 no English ASR, 
 no true English-to-Spanish MT, and 
 simple, domain-limited, Spanish speech 
synthesis. 
The caller-to-dispatcher direction is much more 
interesting. In this direction we require 
 Spanish ASR that can handle emotional 
spontaneous telephone speech in mixed 
dialects, 
 Spanish-to-English MT, but 
 no English Speech Synthesis. 
We have begun to consider the user interfaces 
for Ayudame as well.  For ease of integration 
with pre-existing dispatcher workstations, we 
have chosen to use a web-based graphical inter-
face.  For initial testing of the prototype, we plan 
to run in ?shadow? mode, in parallel with live 
dispatching using the traditional approach.  Thus 
Ayudame will have a listen-only connection to 
the telephone line, and will run a web server to 
interact with the dispatcher.  Figure 2 shows an 
initial design of the web-based interface.  There 
are sections for a transcript, the current caller 
utterance, the current dispatcher response 
choices, and a button to transfer the interaction to 
a human translator as a fall-back option.  For 
each utterance, the DA classification is displayed 
in addition to the actual utterance (in case the 
dispatcher knows some Spanish). 
49
 Figure 2. Example of initial GUI design 
 
4 Automatic Speech Recognition 
An important requirement for such a system is 
the ability to be able to recognize the incoming 
non-English speech with a word error rate suffi-
ciently low for utterance classification and pa-
rameter translation to be possible.  The issues in 
speech recognition for this particular domain in-
clude: telephone speech (which is through a lim-
ited bandwidth channel); background noise (the 
calls are often from outside or in noisy places); 
various dialects of Spanish, and potential stressed 
speech.  Although initially we expected a sub-
stantial issue with recognizing stressed speakers, 
as one might expect in emergency situations, in 
the calls we have collected so far, although it is 
not a negligible issue, it is far less important that 
we first expected. 
The Spanish ASR system is built using the 
Janus Recognition Toolkit (JRTk) (Finke et al 
1997) featuring the HMM-based IBIS decoder 
(Soltau et al 2001). Our speech corpus consists 
of 75 transcribed 9-1-1 calls, with average call 
duration of 6.73 minutes (min: 2.31 minutes, 
max: 13.47 minutes). The average duration of 
Spanish speech (between interpreter and caller) 
amounts to 4.8 minutes per call. Each call has 
anywhere from 46 to 182 speaker turns with an 
average of 113 speaker turns per call. The turns 
that have significant overlap between speakers 
are omitted from the training and test set. The 
acoustic models are trained on 50 Spanish 9-1-1 
calls, which amount to 4 hours of speech data.  
 
 
The system uses three-state, left-to-right, sub-
phonetically tied acoustic models with 400 con-
text-dependent distributions with the same num-
ber of codebooks. Each codebook has 32 gaus-
sians per state. The front-end feature extraction 
uses standard 39 dimensional Mel-scale cepstral 
coefficients and applies Linear Discriminant 
Analysis (LDA) calculated from the training 
data. The acoustic models are seeded with initial 
alignments from GlobalPhone Spanish acoustic 
models trained on 20 hours of speech recorded 
from native Spanish speakers (Schultz et al 
1997). The vocabulary size is 65K words. The 
language model consists of a trigram model 
trained on the manual transcriptions of 40 calls 
and interpolated with a background model 
trained on GlobalPhone Spanish text data con-
sisting of 1.5 million words (Schultz et al 1997). 
The interpolation weights are determined using 
the transcriptions of 10 calls (development set). 
The test data consists of 15 telephone calls from 
different speakers, which amounts to a total of 1 
hour. Both development and test set calls con-
sisted of manually segmented and transcribed 
speaker turns that do not have a significant over-
lap with other speakers. The perplexity of the test 
set according to the language model is 96.7. 
The accuracy of the Spanish ASR on the test 
set is 76.5%.  This is a good result for spontane-
ous telephone-quality speech by multiple un-
known speakers, and compares favourably to the 
ASR accuracy of other spoken dialog systems.  
We had initially planned to investigate novel 
ASR techniques designed for stressed speech and 
multiple dialects, but to our surprise these do not 
50
seem to be required for this application.  Note 
that critical information such as addresses will be 
synthesized back to the caller for confirmation in 
the full system. So, for the time-being we will 
concentrate on the accuracy of the DA classifica-
tion until we can show that improving ASR accu-
racy would significantly help. 
5 Utterance Classification 
As mentioned above, the translation approach we 
are using is based on utterance classification. The 
Spanish to English translation in the Ayudame 
system is a two-step process. The ASR 
hypothesis is first classified into domain-specific 
Domain Acts (DA). Each DA has a 
predetermined set of parameters. These 
parameters are identified and translated using a 
rule-based framework.  For this approach to be 
accomplished with reasonable effort levels, the 
total number of types of parameters and their 
complexity must be fairly limited in the domain, 
such as addresses and injury types. This section 
explains our DA tagset and classification 
experiments. 
5.1 Initial classification and results 
The initial evaluation (Nallasamy et al 2008) 
included a total of 845 manually labeled turns in 
our 9-1-1 corpus. We used a set of 10 tags to an-
notate the dialog turns. The distribution of the 
tags are listed below 
 
Tag (Representation) Frequency 
Giving Name 80 
Giving Address 118 
Giving Phone number 29 
Requesting Ambulance 8 
Requesting Fire Service 11 
Requesting Police 24 
Reporting Injury/Urgency 61 
Yes 119 
No 24 
Others 371 
Table 1. Distribution of first-pass tags in the 
corpus. 
 
We extracted bag-of-word features and trained a 
Support Vector Machine (SVM) classifier (Bur-
ges, 1998) using the above dataset. A 10-fold 
stratified cross-validation has produced an aver-
age accuracy of 60.12%. The accuracies of indi-
vidual tags are listed below. 
 
Tag Accuracy  
  (%) 
Giving Name 57.50 
Giving Address 38.98 
Giving Phone number 48.28 
Req. Ambulance 62.50 
Req. Fire Service 54.55 
Req. Police 41.67 
Reporting Injury/Urgency 39.34 
Yes 52.94 
No 54.17 
Others 75.74 
Table 2. Classification accuracies of first-pass 
tags. 
5.2 Tag-set improvements 
We improved both the DA tagset and the 
classification framework in our second-pass 
classification, compared to our initial  
experiment. We had identified several issues in 
our first-pass classification:  
 We had forced each dialog turn to have a 
single tag. However, the tags and the 
dialog turns don?t conform to this 
assumption. For example, the dialog 
?Yes, my husband has breathing prob-
lem. We are at two sixty-one Oak 
Street?1 should get 3 tags: ?Yes?, ?Giv-
ing-Address?, ?Requesting-Ambulance?.  
 Our analysis of the dataset alo showed 
that the initial set of tags are not 
exhaustive enough to cover the whole 
range of dialogs required to be translated 
and conveyed to the dispatcher.  
We made several iterations over the tagset to 
ensure that it is both compact and achieves  
requisite coverage. The final tag set consists of 
67 entries. We manually annotated 59 calls with 
our new tagset using a web interface. The 
distribution of the top 20 tags is listed below. 
The whole list of tags can be found in the 
NineOneOne project webpage: 
http://www.cs.cmu.edu/~911/ 
 
                                               
1
 The dialog is English Translation of  ?s?, mi esposo le falta 
el aire. es ac? en el dos sesenta y uno Oak Street?. It is 
extracted from the transcription of a CCPD 9-1-1 
emergency call, with address modified to protect privacy 
51
Tag (Representation) Frequency 
Yes 227 
Giving-Address 133 
Giving-Location 113 
Giving-Name 107 
No 106 
Other 94 
OK 81 
Thank-You 51 
Reporting-Conflict 43 
Describing-Vehicle 42 
Giving-Telephone-Number 40 
Hello 36 
Reporting-Urgency-Or-Injury 34 
Describing-Residence 28 
Dont-Know 19 
Dont-Understand 16 
Giving-Age 15 
Goodbye 15 
Giving-Medical-Symptoms 14 
Requesting-Police 12 
Table 3. Distribution of top 20 second-pass 
tags 
 
The new tagset is hierarchical, which allows 
us to evaluate the classifier at different levels of 
the hierarchy, and eventually select the best 
trade-off between the number of tags and 
classification accuracy. For example, the first 
level of tags for reporting incidents includes the 
five most common incidents, viz, Reporting-
Conflict, Reporting-Robbery, Reporting-Traffic-
accident, Reporting-Urgency-or-Injury and 
Reporting-Fire. The second level of tags are used 
to convey more detailed information about the 
above incidents (eg. Reporting-Weapons in the 
case of conflict) or rare incidents (eg. Reporting-
Animal-Problem). 
5.3 Second-pass classification and Results 
We also improved our classification 
framework to allow multiple tags for a single 
turn and to easily accomodate any new tags in 
the future. Our earlier DA classification used a 
multi-class classifier, as each turn was restricted 
to have a single tag. To accomodate multiple tags 
for a single turn, we trained binary classifiers for 
each tag. All the utterances of the corresponding 
tag are marked positive examples and the rest are 
marked as negative examples. Our new data set 
has 1140 dialog turns and 1331 annotations. Note 
that the number of annotations is more than the 
number of labelled turns as each turn may have 
multiple tags. We report classification accuracies 
in the following table for each tag based on 10-
fold cross-validation: 
 
Tag (Representation) Accuracy   
   (%) 
Yes 87.32 
Giving-Address 42.71 
Giving-Location 87.32 
Giving-Name 42.71 
No 37.63 
Other 54.98 
OK 72.5 
Thank-You 41.14 
Reporting-Conflict 79.33 
Describing-Vehicle 96.82 
Giving-Telephone-Number 39.37 
Hello 38.79 
Reporting-Urgency-Or-Injury 49.8 
Describing-Residence 92.75 
Dont-Know 41.67 
Dont-Understand 36.03 
Giving-Age 64.95 
Goodbye 87.27 
Giving-Medical-Symptoms 47.44 
Requesting-Police 79.94 
Table 4. Classification accuracies of 
individual second-pass tags 
 
The average accuracy of the 20 tags is 
58.42%. Although multiple classifiers increase 
the computational complexity during run-time, 
they are independent of each other, so we can run 
them in parallel. To ensure the consistency and 
clarity of the new tag set, we had a second 
annotator label 39 calls. The inter-coder 
agreement (Kappa coefficient) between the two 
annotators is 0.67. This is considered substantial 
agreement between the annotators, and confirms 
the consistency of the tag set. 
6 Conclusion 
The work reported here demonstrates that we can 
produce Spanish ASR for Spanish emergency 
calls with reasonable accuracy (76.5%), and clas-
sify manual transcriptions of these calls with rea-
sonable accuracy (60.12% on the original tagset, 
52
58.42% on the new, improved tagset).  We be-
lieve these results are good enough to justify the 
next phase of research, in which we will develop, 
user-test, and evaluate a full pilot system. We are 
also investigating a number of additional tech-
niques to improve the DA classification accura-
cies.  Further we believe that we can design the 
overall dialog system to ameliorate the inevitable 
remaining misclassifications, based in part on the 
confusion matrix of actual errors (Nallasamy et 
al, 2008).  But only actual user tests of a pilot 
system will allow us to know whether an even-
tual deployable system is really feasible. 
Acknowledgements 
This project is funded by NSF Grant No: IIS-
0627957 ?NineOneOne: Exploratory Research 
on Recognizing Non-English Speech for Emer-
gency Triage in Disaster Response?. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the 
authors and do not necessarily reflect the views 
of sponsors. 
References 
Burges C J C, A tutorial on support vector machines 
for pattern recognition, In Proc. Data Mining and 
Knowledge Discovery, pp 2(2):955-974, USA, 
1998 
Finke M, Geutner P, Hild H, Kemp T, Ries K and 
Westphal M, The Karlsruhe-Verbmobil Speech 
Recognition Engine, In Proc. IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), pp. 83-86, Germany, 1997 
Frederking R, Rudnicky A, Hogan C and Lenzo K, 
Interactive Speech Translation in the Diplomat 
Project, Machine Translation Journal 15(1-2), 
Special issue on Spoken Language Translation, pp. 
61-66, USA, 2000 
Gao Y, Zhou B, Sarikaya R, Afify M, Kuo H, Zhu W, 
Deng Y, Prosser C, Zhang W and Besacier L, IBM 
MASTOR SYSTEM: Multilingual Automatic 
Speech-to-Speech Translator, In Proc. First Inter-
national Workshop on Medical Speech Translation, 
pp. 53-56, USA, 2006 
Langley C, Domain Action Classification and Argu-
ment Parsing for Interlingua-based Spoken Lan-
guage Translation. PhD thesis, Carnegie Mellon 
University, Pittsburgh, PA, 2003 
Language Line Services http://www.languageline.com 
Lavie A, Balducci F, Coletti P, Langley C, Lazzari G, 
Pianesi F, Taddei L and Waibel A, Architecture 
and Design Considerations in NESPOLE!: a 
Speech Translation System for E-Commerce Ap-
plications,. In Proc. Human Language Technolo-
gies (HLT), pp 31-34, USA, 2001 
Levin L, Langley C, Lavie A, Gates D, Wallace D and 
Peterson K, Domain Specific Speech Acts for Spo-
ken Language Translation, In Proc. 4th SIGdial 
Workshop on Discourse and Dialogue, pp. 208-
217, Japan, 2003 
Nallasamy U, Black A, Schultz T and Frederking R, 
NineOneOne: Recognizing and Classifying Speech 
for Handling Minority Language Emergency Calls, 
In Proc. 6th International conference on Language 
Resources and Evaluation (LREC), Morocco, 2008 
NineOneOne project webpage 
[www.cs.cmu.edu/~911] 
Schultz T, Westphal M and Waibel A, The 
GlobalPhone Project: Multilingual LVCSR with 
JANUS-3, In Proc. Multilingual Information Re-
trieval Dialogs: 2nd SQEL Workshop, pp. 20-27, 
Czech Republic, 1997 
Soltau H, Metze F, F?ugen C and Waibel A, A One 
Pass-Decoder Based on Polymorphic Linguistic 
Context Assignment, In Proc. IEEE workshop on 
Automatic Speech Recognition and Understanding 
(ASRU), Italy, 2001 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
53
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136?144,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
CONE: Metrics for Automatic Evaluation of Named Entity              
Co-reference Resolution  
 
 
Bo Lin, Rushin Shah, Robert Frederking, Anatole Gershman 
Language Technologies Institute, School of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave., PA 15213, USA 
 {bolin,rnshah,ref,anatoleg}@cs.cmu.edu 
 
 
Abstract 
Human annotation for Co-reference Resolu-
tion (CRR) is labor intensive and costly, and 
only a handful of annotated corpora are cur-
rently available. However, corpora with 
Named Entity (NE) annotations are widely 
available. Also, unlike current CRR systems, 
state-of-the-art NER systems have very high 
accuracy and can generate NE labels that are 
very close to the gold standard for unlabeled 
corpora.  We propose a new set of metrics col-
lectively called CONE for Named Entity Co-
reference Resolution (NE-CRR) that use a 
subset of gold standard annotations, with the 
advantage that this subset can be easily ap-
proximated using NE labels when gold stan-
dard CRR annotations are absent. We define 
CONE B3 and CONE CEAF metrics based on 
the traditional B3 and CEAF metrics and show 
that CONE B3 and CONE CEAF scores of any 
CRR system on any dataset are highly corre-
lated with its B3 and CEAF scores respectively. 
We obtain correlation factors greater than 0.6 
for all CRR systems across all datasets, and a 
best-case correlation factor of 0.8. We also 
present a baseline method to estimate the gold 
standard required by CONE metrics, and show 
that CONE B3 and CONE CEAF scores using 
this estimated gold standard are also correlated 
with B3 and CEAF scores respectively. We 
thus demonstrate the suitability of CONE 
B3and CONE CEAF for automatic evaluation 
of NE-CRR. 
1 Introduction 
Co-reference resolution (CRR) is the problem of 
determining whether two entity mentions in a 
text refer to the same entity in real world or not. 
Noun Phrase CRR (NP-CRR) considers all noun 
phrases as entities, while Named Entity CRR 
restricts itself to noun phrases that describe a 
Named Entity. In this paper, we consider the task 
of Named Entity CRR (NE-CRR) only. Most, if 
not all, recent efforts in the field of CRR have 
concentrated on machine-learning based ap-
proaches. Many of them formulate the problem 
as a pair-wise binary classification task, in which 
possible co-reference between every pair of men-
tions is considered, and produce chains of co-
referring mentions for each entity as their output. 
One of the most important problems in CRR is 
the evaluation of CRR results. Different evalua-
tion metrics have been proposed for this task. B-
cubed (Bagga and Baldwin, 1998) and CEAF 
(Luo, 2005) are the two most popular metrics; 
they compute Precision, Recall and F1 measure 
between matched equivalent classes and use 
weighted sums of Precision, Recall and F1 to 
produce a global score. Like all metrics, B3 and 
CEAF require gold standard annotations; howev-
er, gold standard CRR annotations are scarce, 
because producing such annotations involves a 
substantial amount of human effort since it re-
quires an in-depth knowledge of linguistics and a 
high level of understanding of the particular text. 
Consequently, very few corpora with gold stan-
dard CRR annotations are available (NIST, 2003; 
MUC-6, 1995; Agirre, 2007). By contrast, gold 
standard Named Entity (NE) annotations are easy 
to produce; indeed, there are many NE annotated 
corpora of different sizes and genres. Similarly, 
there are few CRR systems and even the best 
scores obtained by them are only in the region of 
F1 = 0.5 - 0.6. There are only four such CRR 
systems freely available, to the best of our know-
ledge (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). In comparison, there are nu-
merous Named Entity recognition (NER) sys-
tems, both general-purpose and specialized, and 
many of them achieve scores better than F1 = 
0.95 (Ratinov and Roth, 2009; Finkel et al, 
136
2005). Although these facts can be partly attri-
buted to the ?hardness? of CRR compared to 
NER, they also reflect the substantial gap be-
tween NER and CRR research. In this paper, we 
present a set of metrics, collectively called 
CONE, that leverage widely available NER sys-
tems and resources and tools for the task of eva-
luating co-reference resolution systems. The ba-
sic idea behind CONE is to predict a CRR sys-
tem?s performance for the task of full NE-CRR 
on some dataset using its performance for the 
subtask of named mentions extraction and group-
ing (NMEG) on that dataset. The advantage of 
doing so is that measuring NE-CRR performance 
requires the co-reference information of all men-
tions of a Named Entity, including named men-
tions, nominal and pronominal references, while 
measuring the NMEG performance only requires 
co-reference information of named mentions of a 
NE, and this information is relatively easy to ob-
tain automatically even in the absence of gold 
standard annotations. We compute correlation 
between CONE B3, B3, CONE CEAF and CEAF 
scores for various CRR systems on various gold-
standard annotated datasets and show that the 
CONE B3 and B3 scores are highly correlated for 
all such combinations of CRR systems and data-
sets, as are CONE CEAF and CEAF scores, with 
a best-case correlation of 0.8. We produce esti-
mated gold standard annotations for the Enron 
email corpus, since no actual gold standard CRR 
annotations exist for it, and then use CONE B3 
and CONE CEAF with these estimated gold 
standard annotations to compare the performance 
of various NE-CRR systems on this corpus. No 
such comparison has been previously performed 
for the Enron corpus. 
We adopt the same terminology as in (Luo, 
2005): a mention refers to each individual phrase 
and an entity refers to the equivalence class or 
co-reference chain with several mentions. This 
allows us to note some differences between NE-
CRR and NP-CRR. NE-CRR involves indentify-
ing named entities and extracting their co-
referring mentions; equivalences classes without 
any NEs are not considered. NE-CRR is thus 
clearly a subset of NP-CRR, where all co-
referring mentions and equivalence classes are 
considered. However, we focus on NE-CRR be-
cause it is currently a more active research area 
than NP-CRR and a better fit for target applica-
tions such as text forensics and web mining, and 
also because it is more amenable to the automatic 
evaluation approach that we propose. 
The research questions that motivate our work 
are:  
(1) Is it possible to use only NER resources to 
evaluate NE-CRR systems? If so, how is this 
problem formulated?  
(2) How does one perform evaluation in a way 
that is accurate and automatic with least hu-
man intervention?  
(3) How does one perform evaluation on large 
unlabeled datasets?  
We show that our CONE metrics achieve good 
results and represent a promising first step to-
ward answering these questions.  
 
The rest of the paper is organized as follows. We 
present related work in the field of automatic 
evaluation methods for natural language 
processing tasks in Section 2. In Section 3, we 
give an overview of the standard metrics current-
ly used for evaluating co-reference resolution. 
We define our new metrics CONE B3 and CONE 
CEAF in Section 4. In section 5, we provide ex-
perimental results that illustrate the performance 
of CONE B3 and CONE CEAF compared to B3 
and CEAF respectively. In Section 6, we give an 
example of the application of CONE metrics by 
evaluating NE-CRR systems on an unlabeled 
dataset, and discuss possible drawbacks and ex-
tensions of these metrics. Finally, in section 7 we 
present our conclusions and ideas for future 
work.  
2 Related Work 
There has been a substantial amount of research 
devoted to automatic evaluation for natural lan-
guage processing, especially tasks involving lan-
guage generation. The BLEU score (Papineni et 
al., 2002) proposed for evaluating machine trans-
lation results is the best known example of this. 
It uses n-gram statistics between machine gener-
ated results and references. It inspired the 
ROUGE metric (Lin and Hovy, 2003) and other 
methods (Louis and Nenkova, 2009) to perform 
automatic evaluation of text summarization. Both 
these metrics have show strong correlation be-
tween automatic evaluation results and human 
judgments. The two metrics successfully reduce 
the need for human judgment and help speed up 
research by allowing large-scale evaluation. 
Another example is the alignment entropy (Per-
vouchine et al, 2009) for evaluating translitera-
tion alignment. It reduces the need for alignment 
gold standard and highly correlates with transli-
teration system performance. Thus it is able to 
137
serve as a good metric for transliteration align-
ment. We contrast our work with (Stoyanov et al, 
2009), who show that the co-reference resolution 
problem can be separated into different parts ac-
cording to the type of the mention. Some parts 
are relatively easy to solve. The resolver per-
forms equally well in each part across datasets. 
They use the statistics of mentions in different 
parts with test results on other datasets as a pre-
dictor for unseen datasets, and obtain promising 
results with good correlations. We approach the 
problem from a different perspective. In our 
work, we show the correlation between the 
scores on traditional metrics and scores on our 
CONE metrics, and show how to automatically 
estimate the gold standard required by CONE 
metrics. Thus our method is able to predict the 
co-reference resolution performance without 
gold standard at all. We base our new metrics on 
the standard B3 and CEAF metrics used for com-
puting CRR scores. (Vilian et al, 1995; Bagga 
and Baldwin, 1998; Luo, 2005). B3 and CEAF 
are believed to be more discriminative and inter-
pretable than earlier metrics and are widely 
adopted especially for machine-learning based 
approaches.  
 
3 Standard Metrics: B3 and CEAF 
We now provide an overview of the standard B3 
and CEAF metrics used to evaluate CRR sys-
tems. Both metrics assume that a CRR system 
produces a set of equivalence classes {O} and 
assigns each mention to only one class. Let Oi be 
the class to which the ith mention was assigned 
by the system. We also assume that we have a set 
of correct equivalence classes {G} (the gold 
standard). Let Gi be the gold standard class to 
which the ith mention should belong. Let Ni de-
note the number of mentions in Oi which are also 
in Gi ? the correct mentions. B
3 computes the 
presence rate of correct mentions in the same 
equivalent classes. The individual precision and 
recall score is defined as follows: 
|| i
i
i O
NP ?
 
|| i
i
i G
NR ?
 
Here |Oi| and |Gi| are the cardinalities of sets Oi 
and Gi.   
The final precision and recall scores are: 
?
?
?
n
i
ii PwP
1
 ?
?
?
n
i
ii RwR
1
 
Here, in the simplest case the weight wi is set to 
1/n, equal for all mentions. 
CEAF (Luo, 2005) produces the optimal 
matching between output classes and true classes 
first, with the constraint that one true class, Gi, 
can be mapped to at most one output class, say 
Of(i) and vice versa. This can be solved by the 
KM algorithm (Kuhn, 1955; Munkres, 1957) for 
maximum matching in a bipartite graph. CEAF 
then computes the precision and recall score as 
follows: 
?
?
?
i
i
i
ifi
O
M
P
)(,      
?
?
?
i
i
i
ifi
G
M
R
)(,  
jiji GOM ??,
 
We use the terms Mi,j from CEAF to re-write B
3, 
its formulas then reduce to: 
???? i j i
ji
i
i O
M
O
P
2
,1  
???? i j i
ji
i
i G
M
G
R
2
,1  
We can see that B3 simply iterates through all 
pairs of matchings instead of considering the one 
to one mappings as CEAF does. Thus, B3 com-
putes the weighted sum of the F-measures for 
each individual mention which helps alleviate the 
bias in the pure link-based F-measure, while 
CEAF computes the same as B3 but enforces at 
most one matched equivalence class for every 
class in the system output and gold standard out-
put. 
4 CONE B3 and CONE CEAF Metrics:  
We now formally define the new CONE B3 and 
CONE CEAF metrics that we propose for 
automatic evaluation of NE-CRR systems. 
      Let G denote the set of gold standard 
annotations and O denote the output of an NE-
CRR system. Let Gi denote the equivalent class 
of entity i in the gold standard and Oj denote the 
equivalence class for entity j in the system output.  
Also let Gij denote the j
th mention in the 
equivalence class of entity i in the gold standard 
and Oij denote the j
th mention in the system 
output. 
As described earlier, the standard B3 and CEAF 
metrics evaluate scores using G and O and can 
be thought of as functions of the form B3(G, O). 
and CEAF(G, O) respectively. Let us use 
Score(G, O) to collectively refer to both these 
138
functions. An equivalence class Gi in G may 
contain three types of mentions: named mentions 
gNMij, nominal mentions g
NO
ij, and pronominal 
mentions gPRij. Similarly, we can define o
NM
ij, 
oNOij and o
PR
ij for a class Oi in O. Now for each 
gold standard equivalence class Gi and system 
output equivalence class Oi, we define the 
following sets GNMi  and  O
NM
i: 
iijNMijNMiNM GggGi ??? },{,  
iijNMijNMiNM OooOi ??? },{,  
In other words, GNMi and O
NM
i are the subsets of 
Gi and Oi containing all named mentions and no 
mentions of any other type.  
Let GNM denote the set of all such equivalance 
classes GNMi and O
NM denote the set of all 
equivalence classes ONMi. It is clear that G
NM and 
ONM are pruned versions of the gold standard 
annotations and system output respectively. 
We now define CONE B3 and CONE CEAF as 
follows: 
CONE B3 = B3(GNM, ONM) 
CONE CEAF = CEAF(GNM, ONM) 
 
Following our previous notation, we denote 
CONE B3 and CONE CEAF collectively as 
Score(GNM, ONM). We observe that Score(GNM, 
ONM) measures a NE-CRR system?s  
performance for the NE-CRR subtask of named 
mentions extraction and grouping (NMEG). We 
find that Score(GNM, ONM) is highly correlated 
with Score(G, O) for all the freely available NE-
CRR systems over various datasets. This 
provides the neccessary  justification for the use 
of Score(GNM, ONM).  
We use SYNERGY (Shah et al, 2010), an 
ensemble NER system that combines the UIUC 
NER (Ritanov and Roth, 2009) and Stanford 
NER (Finkel et al, 2005) systems, to produce 
GNM and ONM from G and O by  selecting named 
mentions. However, any other good NER system 
would serve the same purpose. 
We see that while standard evaluation metrics 
require the use of G, i.e. the full set of NE-CRR 
gold standard annotations including named, 
nominal and pronimal mentions, CONE metrics 
require only GNM, i.e. gold standard annotations 
consisting of named mentions only. The key 
advantage of using CONE metrics is that GNM 
can be automatically approximated using an 
NER system with a good degree of accuracy. 
This is because state-of-the-art NER systems 
achieve near-optimal performance, exceeding F1 
= 0.95 in many cases, and after obtaining their 
output, the task of estimating GNM reduces to 
simply clustering it to seperate mentions of 
diffrerent real-world entities. This clustering can 
be thought of as a form of named entity matching, 
which is not a very hard problem. There exist 
systems that perform such matching in a 
sophisticated manner with a high degree of 
accuracy. We use simple heuristics such as exact 
matching, word matches, matches between in-
itials, etc. to design such a matching system 
ourselves and use it to obtain estimates of GNM, 
say GNM-approx. We then calculate CONE B3 and 
CONE CEAF scores using GNM-approx instead of 
GNM; in other words, we perform fully automatic 
evaluation of NE-CRR systems by using 
Score(GNM-approx, ONM) instead of Score(GNM, 
ONM). In order to show the validity of this 
evaluation, we calculate the correlation between 
the Score(GNM-approx, ONM) and Score(G, O) for  
different NE-CRR systems across different 
datasets and find that they are indeed correlated. 
CONE thus makes automatic evaluation of NE-
CRR systems possible. By leveraging the widely 
available named entity resources, it reduces the 
need for gold standard annotations in the 
evaluation process. 
4.1 Analysis 
There are two major kinds of errors that affect 
the performance of NE-CRR systems for the full 
NE-CRR task: 
? Missing Named Entity (MNE): If a named 
mention is missing from the system output, 
it is very likely that its nearby nominal and 
anaphoric mentions will be lost, too 
? Incorrectly grouped Named Entity (IGNE): 
Even if the named mention is correctly iden-
tified with its nearby nominal and anaphoric 
mentions to form a chain, it is still possible 
to misclassify the named mentions and its 
co-reference chain 
Consider the following example of these two 
types of errors. Here, the alphabets represent the 
named mentions and numbers represent other 
type of mentions: 
 
Gold standard, G: (A, B, C, 1, 2, 3, 4) 
Output from System 1, O1: (A, B, 1, 2, 3) 
Output from System 2, O2: (A, C, 1, 2, 4), (B, 3) 
O1 shows an example of an MNE error, while 
O2 shows an example of an IGNE error.  
 
Both these types of errors are in fact rooted in 
named mention extraction and grouping 
(NMEG). Therefore, we hypothesize that they 
must be preserved in a NE-CRR system?s output 
139
for the subtask of named mentions extraction and 
grouping (NMEG) and will be reflected in the 
CONE B3 and CONE CEAF metrics that eva-
luate scores for this subtask. Consider the follow-
ing extension of the previous example:  
 
GNM: (A, B, C) 
O1NM: (A, B) 
O2NM: (A, C), (B) 
 
We observe that the MNE error in O1 is pre-
served in O1NM, and the IGNE error in O2 is pre-
served in O2NM. Empirically we sample several 
output files in our experiments and observe the 
same phenomena. Therefore, we argue that it is 
possible to capture the two major kinds of errors 
described by considering only GNM and ONM in-
stead of G and O.  
 
We now provide a more detailed theoretical 
analysis of the CONE metrics. For a given NE-
CRR system and dataset, consider the system 
output O and gold standard annotation G. Let P 
and R indicate precision and recall scores ob-
tained by evaluating O against G, using CEAF. If 
we replace both G and O with their subsets GNM 
and ONM respectively, such that GNM and ONM 
contain only named mentions, we can modify the 
equations for precision and recall for CEAF to 
derive the following equations for precision PNM 
and recall RNM for CONE CEAF: 
??
i
iNMOOSum NM }{
     
??
i
iNMNM GGSum }{
 
??
i
NM
ifi
NM
NM
OSum
M
P }{
)(,     
??
i
NM
ifi
NM
NM
GSum
M
R }{
)(, 
 
The corresponding equations for CONE B3 Pre-
cision are: 
?
?
?
?
i
NM
i
NM
j
ji
NM
NM
OSumO
M
P
}{
2
,
?
?
?
?
i
NM
i
NM
j
ji
NM
RSumR
M
R
}{
'
2
,
 
 
In order to support the hypothesis that CONE 
metrics evaluated using (GNM, ONM) represent an 
effective substitute for standard metrics that use 
(G, O), we compute entity level correlation be-
tween the corresponding CONE and standard 
metrics. For example, in the case of CEAF / 
CONE CEAF Precision, we calculate correlation 
between the following quantities: 
??? }{
)(,
NM
ifi
NM
NM
SSum
M
P?
 and 
??? }{
)(,
SSum
M
P ifi?  
We perform this experiment with the LBJ and 
BART CRR systems on the ACE Phase 2 corpus. 
We illustrate the correlation results in Figure 1.  
 
Figure 1. Correlation between NMP? andP?  - 
Entity Level CEAF Precision 
From Figure 1, we can see that the two 
measures are highly correlated. In fact, we find 
that the Pearson?s correlation coefficient (Soper 
et al, 1917; Cohen, 1988) is 0.73. The points 
lining up on the x-axis and y=1.0 represent very 
small equivalence classes and are a form of noise; 
their removal doesn?t affect this coefficient. To 
show that this strong correlation is not a 
statistical anomaly, we also compute entity-level 
correlation using (Gi - G
NM
i, Oj - O
NM
j) and (Gi, 
Oj) instead of (G
NM
i, O
NM
j) and (Gi, Oj) and find 
that the coefficient drops to 0.03, which is 
obviously not correlated at all.  
We now know NMP? andP?  are highly correlated. 
Assume the correlation is linear, with the 
following equation: 
?? ?? iNMi PP  
where ? and ? are the linear regression 
parameters. 
Thus 
? ? ???? nPnPPP NM
i
iNM
i i
????? ??
   
Here, n is the number of equivalence classes.    
We conclude that the overall CEAF Precision 
and CONE CEAF Precision should be highly 
140
correlated too. We repeat this experiment with 
CEAF / CONE CEAF Recall, B3 / CONE B3 
Precision and B3 / CONE B3 Recall and obtain 
similar results, allowing us to conclude that these 
sets of measures should also be highly correlated. 
We note here some generally accepted 
terminology regarding correlation: If two 
quantities have a Pearson?s correlation 
coefficient greater than 0.7, they are considered  
"strongly correlated", if their correlation is 
between 0.5 and 0.7, they are considered "highly 
correlated", if it is between 0.3 and 0.5, they are 
considered "correlated", and otherwise they are 
considered "not correlated".  
It is important to note that like all automatic 
evaluation metrics, CONE B3 and CONE CEAF 
too can be easily ?cheated?, e.g. a NE-CRR sys-
tem that performs NER and named entity match-
ing well but does not even detect and classify 
anaphora or nominal mentions would nonethe-
less score highly on these metrics. A possible 
solution to this problem would be to create gold 
standard annotations for a small subset of the 
data, call these annotations G?, and report two 
scores: B3 / CEAF (G?), and CONE B3 / CONE 
CEAF (GNM-approx). Discrepancies between these 
two scores would enable the detection of such 
?cheating?. A related point is that designers of 
NE-CRR systems should not optimize for CONE 
metrics alone, since by using GNM-approx (or GNM 
where gold standard annotations are available), 
these metrics are obviously biased towards 
named mentions. This issue can also be ad-
dressed by having gold standard annotations G? 
for a small subset. One could then train a system 
by optimizing both B3 / CEAF (G?) and CONE 
B3 / CONE CEAF (GNM-approx). This can be 
thought of as a form of semi-supervised learning, 
and may be useful in areas such as domain adap-
tation, where we could use some annotated test-
set in a standard domain, e.g. newswire as the 
smaller set and an unlabeled large testset from 
some other domain, such as e-mail or biomedical 
documents. An interesting future direction is to 
monitor the effectiveness of our metrics over 
time. As co-reference resolution systems evolve 
in strength, our metrics might be less effective, 
however this could be a good indicator to discri-
minate on different subtasks the improvements 
gained by the co-reference resolution systems. 
5 Experimental Results 
We present experimental results in support of the 
validity and effectiveness of CONE metrics. As 
mentioned earlier, we used the following four 
publicly available CRR systems: UIUC?s LBJ 
system (L), BART from JHU Summer Workshop 
(B), LingPipe from Alias-i (LP), and OpenNLP 
(OP) (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). All these CRR systems per-
form Noun Phrase co-reference resolution (NP-
CRR), not NE-CRR. So, we must first eliminate 
all equivalences classes that do not contain any 
named mentions. We do so using the SYNERGY 
NER system to separate named mentions from 
unnamed ones. Note that this must not be con-
fused with the use of SYNERGY to produce GNM 
and ONM from G and O respectively. For that task, 
all equivalence classes in G and O already con-
tain at least one named mention and we remove 
all unnamed mentions from each class. This 
process effectively converts the NP-CRR results 
of these systems into NE-CRR ones. We use the 
ACE Phase 2 NWIRE and ACE 2005 English 
datasets. We avoid using the ACE 2004 and 
MUC6 datasets because the UIUC LBJ system 
was trained on ACE 2004 (Bengston and Roth, 
2008), while BART and LingPipe were trained 
on MUC6. There are 29 files in the test set of 
ACE Phrase 2 and 81 files in ACE 2005, sum-
ming up to 120 files with around 50,000 tokens 
with 5000 valid co-reference mentions. Tables 1 
and 2 show the Pearson?s correlation coefficients  
between CONE metric scores of the type 
Score(GNM, ONM) and standard metric scores of 
the type Score(G, O) for combinations of various 
CRR systems and datasets.  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.82 0.71 0.7 0.81 0.71 0.77 
B 0.85 0.5 0.66 0.71 0.61 0.68 
LP 0.84 0.66 0.67 0.74 0.71 0.73 
OP 0.31 0.57 0.61 0.79 0.72 0.79 
Table 1. GNM: Correlation on ACE Phase 2 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.6 0.62 0.62 0.75 0.61 0.68 
B 0.74 0.82 0.84 0.72 0.68 0.67 
LP 0.91 0.65 0.73 0.44 0.57 0.53 
OP 0.48 0.77 0.8 0.54 0.67 0.65 
Table 2. GNM: Correlation on ACE 2005 
 
We observe from Tables 1 and 2 that CONE B3 
and CONE CEAF scores are highly correlated 
141
with B3 and CEAF scores respectively, and this 
holds true for Precision, Recall and F1 scores, for 
all combinations of CRR systems and datasets. 
This justifies our assumption that a system?s per-
formance for the subtask of NMEG is a good 
predictor of its performance for the full task of 
NE-CRR. These correlation coefficients are 
graphically illustrated in Figures 2 and 3. 
We now use our baseline named entity matching 
method to automatically generate estimated gold 
standard annotations GNM-approx and recalculate 
CONE CEAF and CONE B3 scores using GNM-
approx instead of GNM. Tables 3 and 4 show the 
correlation coefficients between the new CONE 
scores and the standard metric scores. 
 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.31 0.23 0.22 0.33 0.55 0.56 
B 0.71 0.44 0.43 0.61 0.63 0.71 
LP 0.57 0.43 0.49 0.36 0.25 0.31 
OP 0.1 0.6 0.64 0.35 0.53 0.53 
Table 3. GNM-approx: Correlation on ACE Phase 2  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.33 0.32 0.42 0.22 0.34 0.36 
B 0.25 0.66 0.65 0.2 0.45 0.37 
LP 0.19 0.33 0.34 0.77 0.68 0.72 
OP 0.26 0.66 0.67 0.28 0.42 0.38 
Table 4. GNM-approx: Correlation on ACE Phase 2 
We observe from Tables 3 and 4 that these corre-
lation factors are encouraging, but not as good as 
those in Tables 1 and 2. All the corresponding 
CONE B3 and CONE CEAF scores are corre-
lated, but very few are highly correlated. We 
should note however that our baseline system to 
create GNM-approx uses relatively simple clustering 
methods and heuristics. It is easy to observe that 
a sophisticated named entity matching system 
would produce a GNM-approx that better approx-
imates GNM than our baseline method, and CONE 
B3 and CONE CEAF scores calculated using this 
GNM-approx would be more correlated with stan-
dard B3 and CEAF scores.  
We note from the above results that correlations 
scores are very similar across different systems 
and datasets. In order to formalize this assertion, 
we calculate correlation scores in a system-
independent and data-independent manner. We 
combine all the data points across all four differ-
ent systems and plot them in Figure 2 and 3 for 
ACE Phase 2 NWIRE corpus and in Figure 4 and 
5 for ACE 2005 corpus respectively. We illu-
strate only F1 scores; the results for precision 
and recall are similar. 
 
Figure 2. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2 
 
Figure 3. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2 
 
Figure 2 reflects a Pearson?s correlation coeffi-
cient of 0.70, suggesting that all the B3 F1 and 
CONE B3 F1 scores for different systems are 
highly correlated and that CONE B3 F1 does not 
bias towards any particular system. Figure 3 re-
flects a Pearson?s correlation coefficient of 0.83, 
providing similar evidence for the system-
independence of correlation between CEAF F1 
and CONE CEAF F1 scores. Figures 4 and 5 
corresponding to ACE 2005 reflect similar corre-
lation coefficients of 0.89 and 0.82, and thus 
support the idea that the correlations between B3 
F1 and CONE B3 F1, as well as between CEAF 
F1and CONE CEAF F1, are dataset-independent 
in addition to being system-independent.  
 
142
 
Figure 4. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2005 
 
 
Figure 5. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2005 
6 Application and Discussion  
To illustrate the applicability of CONE metrics, 
we consider the Enron e-mail corpus. It is of a 
different genre than the newswire corpora that 
CRR systems are usually trained on, and no CRR 
gold standard annotations exist for it. Conse-
quently, no CRR systems have been evaluated on 
it so far. We used CONE B3 and CONE CEAF to 
evaluate and compare the NE-CRR performance 
of various CRR systems on a subset of the Enron 
e-mail corpus (Klimt and Yang, 2004) that was 
cleaned and stripped of spam messages. We re-
port the results in Table 5. 
 
  CONE B3  CONE CEAF 
  P R F1 P R F1 
L 0.43 0.21 0.23 0.31 0.17 0.21 
B 0.26 0.18 0.2 0.26 0.16 0.2 
LP 0.61 0.51 0.53 0.58 0.53 0.54 
OP 0.19 0.03 0.05 0.11 0.02 0.04 
Table 5. GNM-approx Scores on Enron corpus 
 
We find that LingPipe is the best of all the sys-
tems we considered, and LBJ is slightly ahead of 
BART in all measures. We suspect that since 
LingPipe is a commercial system, it may have 
extra training resources in the form of non-
traditional corpora. Nevertheless, we believe our 
method is robust and scalable for large corpora 
without NE-CRR gold standard annotations. 
 
7 Conclusion and Future Work 
We propose the CONE B3 and CONE CEAF me-
trics for automatic evaluation of Named Entity 
Co-reference Resolution (NE-CRR). These me-
trics measures a NE-CRR system?s performance 
on the subtask of named mentions extraction and 
grouping (NMEG) and use it to estimate the sys-
tem?s performance on the full task of NE-CRR. 
We show that CONE B3 and CONE CEAF 
scores of various systems across different data-
sets are strongly correlated with their standard B3 
and CEAF scores respectively. The advantage of 
CONE metrics compared to standard ones is that 
instead of the full gold standard data G, they only 
require a subset GNM of named mentions which 
even if not available can be closely approximated 
by using a state-of-the-art NER system and clus-
tering its results. Although we use a simple base-
line algorithm for producing the approximate 
gold standard GNM-approx, CONE B3 and CONE 
CEAF scores of various systems obtained using 
this GNM-approx still prove to be correlated with 
their standard B3 and CEAF scores obtained us-
ing the full gold standard G. CONE metrics thus 
reduce the need of expensive labeled corpora. 
We use CONE B3 and CONE CEAF to evaluate 
the NE-CRR performance of various CRR sys-
tems on a subset of the Enron email corpus, for 
which no gold standard annotations exist and no 
such evaluations have been performed so far. In 
the future, we intend to use more sophisticated 
named entity matching schemes to produce better 
approximate gold standards GNM-approx. We also 
intend to use the CONE metrics to evaluate NE-
CRR systems on new datasets in domains such as 
chat, email, biomedical literature, etc. where very 
few corpora with gold standard annotations exist. 
 
Acknowledgments 
We would like to thank Prof. Ani Nenkova from 
the University of Pennsylvania for her talk about 
automatic evaluation for text summarization at 
the spring 2010 CMU LTI Colloquium and ano-
nymous reviewers for insightful comments.  
143
References  
E. Agirre, L. M?rquez and R. Wicentowski, Eds. 
2007. Proceedings of the Fourth International 
Workshop on Semantic Evaluations (SemEval).   
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. Proceedings of LREC 
Workshop on Linguistic Coreference. 
J. Baldridge and T. Morton. 2004. OpenNLP. 
http://opennlp.sourceforge.net/. 
B. Baldwin and B. Carpenter. 2003. LingPipe. Alias-i. 
E. Bengtson and D. Roth. 2008. Understanding the 
Value of Features for Coreference Resolution. Pro-
ceedings of EMNLP. 
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences. (2nd ed.) 
A.K. Elmagarmid, P.G. Ipeirotis and V.S. Verykios. 
2007. Duplicate Record Detection: A Survey. IEEE 
Transactions on Knowledge and Data Engineering, 
v.19 n.1, 2007.   
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Informa-
tion Extraction Systems by Gibbs Sampling. Pro-
ceedings of ACL. 
B. Klimt and Y. Yang. 2004. The Enron corpus: A 
new dataset for email classification research. Pro-
ceedings of ECML. 
H.W. Kuhn. 1955. The Hungarian method for the 
assignment problem. Naval Research Logistics 
Quarterly, 2(83). 
C. Lin and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. 
Proceedings of HLT-NAACL.    
C. Lin and F.J. Och. 2004. Automatic evaluation of 
machine translation quality using longest common 
subsequence and skip-bigram statistics. Proceed-
ings of ACL.  
A. Louis and A. Nenkova. 2009. Automatically Eva-
luating Content Selection in Summarization with-
out Human Models. Proceedings of EMNLP, pages 
306?314, Singapore, 6-7 August 2009. 
X. Luo. 2005. On coreference resolution performance 
metrics. Proceedings of EMNLP. 
MUC-6. 1995. Proceedings of the Sixth Understand-
ing Conference (MUC-6). 
J. Munkres. 1957. Algorithms for the assignment and 
transportation problems. Journal of SIAM, 5:32-38. 
NIST. 2003. The ACE evaluation plan. 
www.nist.gov/speech/tests/ace/index.htm. 
K Papineni, S Roukos, T Ward and W.J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. Proceedings of ACL. 
V. Pervouchine, H. Li and B. Lin. 2009. Translitera-
tion alignment. Proceedings of ACL. 
L. Ratinov and D. Roth. 2009. Design Challenges and 
Misconceptions in Named Entity Recognition. 
Proceedings of CoNLL. 
R. Shah, B. Lin, A. Gershman and R. Frederking. 
2010. SYNERGY: a named entity recognition sys-
tem for resource-scarce languages such as Swahili 
using online machine translation. Proceedings of 
LREC Workshop on African Language Technology. 
H.E. Soper, A.W. Young, B.M. Cave, A. Lee and K. 
Pearson. 1917. On the distribution of the correla-
tion coefficient in small samples. Appendix II to 
the papers of "Student" and R. A. Fisher. A co-
operative study. Biometrika, 11, 328-413. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the-Art. 
Proceedings of ACL. 
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman, A. 
Jern, J. Smith, X. Yang and A. Moschitti. 2008. 
BART: A Modular Toolkit for Coreference Reso-
lution. Proceedings of EMNLP. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. 
Hirschman. 1995. A model-theoretic coreference 
scoring scheme. Proceedings of MUC 6. 
 
 
144
