Processing Self Corrections in a speech to speech system 
J S rg  Sp i lker ,  Mar t in  K la rner ,  G f in ther  G6rz  
University of Er langen-Nuremberg - Computer  Science Institute, 
IMMD 8 - Artificial Intell igence, 
Am Weichselgarten 9, 91058 Er langen-  Tennenlohe, Germany 
{ spilker, klarner, goerz}~immd8, inf ormat ik. uni-erlangen, de 
Abstract  
Speech repairs occur often in spontaneous spo- 
ken dialogues. The ability to detect and cor- 
rect those repairs is necessary for any spoken 
language system. We present a framework to 
detect and correct speech repairs where all tel- 
evant levels of information, i.e., acoustics, lexis, 
syntax and semantics can be integrated. The 
basic idea is to reduce the search space for re- 
pairs as soon as possible by cascading filters 
that involve more and more features. At first an 
acoustic module generates hypotheses about the 
existence of a repair. Second a stochastic model 
suggests a correction for every hypothesis. Well 
scored corrections are inserted as new paths in 
the word lattice. Finally a lattice parser decides 
on accepting the repair. 
1 I n t roduct ion  
Spontaneous peech is disfluent. In contrast 
to read speech the sentences aren't perfectly 
planned before they are uttered. Speakers of- 
ten modify their plans while they speak. This 
results in pauses, word repetitions or changes, 
word fragments and restarts. Current mlto- 
rustic speech understanding systems perform 
very well in small domains with restricted 
speech but have great difficulties to deal with 
such disfluencies. A system that copes with 
these self corrections (=repairs) must recognize 
the spoken words and identify the repair to get 
the intended meaning of an utterance. To char- 
acterize a repair it is commonly segmented into 
the following four parts (el. fig.i): 
? reparandum: the "wrong" part of the ut- 
terance 
? interruption point (IP): marker at the end 
of the reparandum 
? editing term: special phrases, which indi- 
cate a repair like "well", "I mean" or filled 
pauses such as "uhln '~, "uh" 
? reparans: the correction of the reparandum 
on Thursday lcannot ? no Ican meet "ah afteronc 
/ 
/ \ - / "" 
Rct)arandmn Interruption- Editing Rcparans 
point Term 
Figure 1: Example of a self repair 
Only if reparandum and editing term are 
known, the utterance can be analyzed in the 
right way. It remains an open question whether 
the two terms should be deleted before a seman- 
tic analysis as suggested sometimes in the liter- 
ature 1. If both terms are marked it is a straight- 
forward preprocessing step to delete reparan- 
dum and editing term. In the Verbmobil 2 cor- 
pus, a corpus dealing with appointment schedul- 
ing a.nd tr~vel planning, nearly 21% of all turns 
contain at least one repair. As a consequence a 
speech understanding system thai; cannot han- 
dle repairs will lose perforlnance on these turns. 
Even if repairs are defined by syntactic and 
semantic well-formedness (Levelt, 1983) we ob- 
serve that most of them are local phenomena.. 
At this point we have to differentiate between 
restarts and other repairs a (modification re- 
pairs). Modification repairs have a strong corre- 
spondence between reparandum and reparans, 
1In most cases a reparaudum could be deleted with- 
out any loss of information. But, for exmnple, if it in- 
troduces an object which is referred to later, a deletion 
is not appropriate. 
>l?his work is part of the VERBMOBIL  project and 
was funded by the German Federal Ministry for Research 
and Technology (BMBF) in the framework of the Verb- 
mobil Project under Grant BMBF 01 IV 701 V0. The 
responsibility for the contents of this study lies with the 
authors. 
SOften a third kind of repair is defined: "abridged 
repairs". These repairs consist solely of an editing term 
and are not repairs in our sense. 
1116 
whereas restarts a.re less structured. In our be- 
lieve there is no nted for a. complete syntactic 
am@sis to detect ~md correct most modification 
repairs. Thus, in wh~tt follows, we will concen- 
tra.te on this ldnd of repa.ir. 
There are two major arguments to process 
repairs before t)arsing. Primarily spontaneous 
speech is not always syntactically well-formed 
even in the absence of sell' corrections. Sec- 
ond (Meta-) rules increase the pa.rsers' search 
space. This is perhaps acceptable for transliter- 
ated speech but not for speech recognizers out- 
put like l~ttices because they represent millions 
of possible spoken utterances. \[n addition, sys- 
tems whk;h a.re not based on a. deep syntactic 
and semantic amdysis e .g .  statistical dialog 
act prediction -- require a repa.ir processing step 
to resolve contr~dictions like the one in tit. 1. 
We propose all algorithm for word lattices 
th,~t divides repa.ir detection a.nd correction in 
three steps (of. fig. 2) l"irst, ~r trigger indi- 
cates potential 1Ps. Second, a sl;ochasl, ic model 
tries to lind an appropria.te repair h)r each IP by 
guessing 1,he mosl; l)robable segmentation, qb 
accomplish this, repair processing is seen as a 
statistical machine translation problem where 
the repa.randum is a transl~tion of the reparans. 
For every repair found, a pa.th representing the 
spcaker.' intended word sequence is inserted 
into the la.ttice. In the last step, a lattice parser 
selects the best pa.th. 
tlll 'llllll'Sday I ?iIIlllt)l IlO \[ CIIII lllCel "ah tiller t)llC 
gpeec\] l  I't'cOgtllZCi 
wllnl Io Slly ICOll i i1  il 
Oll +l'htusday 1 C;lllllOI lit) \] t'311 IIIL'L~I 'liIh alter  111112 
loca l  word  based  scope  dc lec t io l l  o f  lattice ed i t ing  1o represent  res t l l t  
l{cl)ttl' i l l/dtll l l \ ]~.c\])alans !f 
1 ?-ilJ\] 
? wll iii \[o say icoll ill , ii 
on  "\[\]lttlSdlly t'lllllK~l t'atI1 litter I / "lib ' ' tll'\[Cl "t t)llC t J 
se lec l ion  by  
1 l ingu is t i c  a l la lys l s  
s 
011 "l'htll'gday \]C/Ill nlcel "till :tiler olle 
Figure 2: An architecture for repa.ir processing 
2 Repa i r  qh ' iggers  
Because it is impossible for;t  rea.l time speech 
system to check for every word whether it can 
be part of a repair, we use triggers which indi- 
cate the potential existence of a repa.ir. These 
triggers nlllst be immediately detectable for ev- 
ery word in the lattice. Currently we art using 
two different ldnd of triggers4: 
\]. 
. 
Acoustic/prosodic cuts: Spe~kers mark the 
117 in many cases by prosodic signals like 
1)auses, hesitations, etc. A prosodic classi- 
tier 5 determines for every word the proba-  
bi l i ty of  an IP following. If it is above a cer- 
t~dn l;hreshold, the trigger becomes active. 
For a detailed description of the acoustic 
aspects ee (Batliner eL al., 1998). 
Word Dagments are a very strong repair 
indicator. Unfortunately, no speech recog- 
nizer is able to detect word fl:agmtnts to 
date. But there are some interesting ap- 
proaches to detect words which are not in 
the recognizers vocabulary (Klakow et al, 
1999). A word fi'agment is normally an un- 
known word and we hope that it can bt 
distinguished from unfra.gmented unknown 
words by the prosodic classifier. So, cur- 
rently this is a hypol;hetical trigger. We 
will elaborate on it in the evaluation sec- 
tion (cf. sect. 5) to show the impact of this 
trigger. 
If a trigger is active, a. sea, rch for an acceptable 
segmentation into rel)arandum , editing term 
a.nd reparans is initia.ted. 
3 Scope Detect ion  
As mentioned in the introduction reDfir seg- 
mentation is based mainly on a stochastic trans- 
la.tion modtl, l~el'ore we explain it in detail we 
give a short introduction to statistical machine 
translation ?. The fundalnentaJ idea. is the as- 
sumption that a given sentence S in a source 
language (e.g. English) can be translated in any 
^ 
sentence 5/' in a l;~rgel; I,~nguage (e.g. German). 
To every pair (5', ~/') a probability is assigned 
which reflects the likelihood that a tra.nsl~tor 
who sees S will produce \]' as the translation. 
The sta.tistical machine translation problem is 
4 Other triggers cal, be added as well. (Stolcke ct al., 
1999) for example integrate prosodic cues and an ex- 
tended language model in a speech recognizer to detect 
IPs. 
SThe classifier is developed by tile speech group of 
the IMM1) 5. Special thanks to Anton Batliner, Richard 
Iluber and Volker Warnke. 
~A more detailed introduction is given by (Brown el, 
al., 1990) 
I 117 
formul;~ted as: 
5~' = argmaXTI ' (T lS )  
This is reformulated by Bayes' law for a better 
search space reduction, but we are only inter- 
ested in the conditional probability P(TIS ). For 
further processing steps we have to introduce 
the concept of alignment (Brown et al, 1990). 
Let S be the word sequence S1, S 2 . . . .  5,l ~ SI 
and T = ~,T2. . .Tm ~ 77\] ~. We can link a 
word in T to a word in S. This reflects the 
assumption that the word in T is translated 
from the word in S. \]?or example, if S is "On 
Thursday" and T is "Am l)onnerstag" "Am" 
can be linked to "On" but also to "Thursday". 
If each word in T is linked to exactly one word 
in ,S' these links can be described by a vector 
a~ '~ = a l . . .  a,~ with ai E O...l. If the word 51~. 
is linked to Si then aj = i. If it is not connected 
to any word in S then aj = 0. Such a vector 
is called an alignment a. P(T\],5,) can now be 
expressed by 
 '(TIS) = al,5,) (2) 
a is alignment 
Without any further assumptions we can infer 
the tbllowing: 
) 1 * ( -45) ,  
H \])(ajl(t'{-l' r j - l '  ?'"' '5,) ~ 
J--' Tii-', m, ,5,) (3) 
Now we return to self corrections. How can this 
framework help to detect the segments of a re- 
pair? Assulne we have a lattice l)~th where the 
reparandn.  (m)) a,d the reparans( S) are 
given, then (RS, \]{D) can be seen as a. transla- 
tion pair and P(RD\]R,5,) can be expressed ex- 
actly the same way as in equation (2). Hence 
we have a method to score (ITS, P~D) pairs.. But 
the triggers only indicate the interruption point, 
not the complete segmentation. Let us first 
look at editing terms. We assume them to be 
a closed list of short phrases. Thus if an entry 
of the editing term list; is found after an 1P, the 
corresponding words are skipped. Any subse- 
quence of words befbre/after the IP conld be the 
reparanduln/reparans. Because turns ca.n h~we 
an arbitrary length it is impossible to compute 
P(I-~D\]IL5,) for every (RS, H.D) pair. Bug this 
is not necessary at all, if repairs are considered 
as local phenomena. We restrict our search to a 
window of four words before and after the IP. A 
corpus analysis showed that 98% of all repairs 
are within this window. Now we only have to 
compute probabilities for 42 difl'erent pairs. If 
the probability of a (RS, RD) pair is above a 
certain threshold, the segmentation is accepted 
as a repair. 
3.1 Parameter  Est imation 
The conditional probabilities in equation (3) 
cannot be estimated reliably fi'om any corpus 
of realistic size, because there are too many p~> 
rameters. For example both P in the product 
depend on the complete reparans R,5,. There- 
fore we simplify the probabilities by assuming 
that m depends only on l, a.i only on j ,m and 
l and finally RDj on 1L5,,.j. So equation (3) be- 
comes 
P(Z D, siZeS) : 
\]-I (4) 
j=l  
These probabilffies can be directly trained fi'orn 
a nlannally annotated corl)ns , where all repairs 
are labeled with begin, end, liP and editing term 
and for each l'eparandnnl the words are linked 
to the corresponding words in the respective 
reparalls. All distributions are smoothed by a 
simple back-off method (Katz, 1987) to avoid 
zero probabilities with the exception that the 
word replacement probability P(I~I)jIILS,j) is 
smoothed in a more sophisticated way. 
3.2 Smoothing 
Even it" we reduce the number of parameters for 
the word replacement probability by the sim- 
plifications mentioned above there are a lot of 
parameters left. With a vocabulary size of 2500 
words, 25002 paralneters have to be estimated 
for P(I~DjllL5,~j). The corpus 7 contains 3200 
repairs fi'om which we extra.ct about 5000 word 
links. So most of the possible word links never 
occur in the corpus. Some of theln are more 
likely to occur in a repair than others. For ex- 
ample, the replacement of "Thursday" by "\]M- 
clay'" is supposed to be more likely than by "eat' 
ing", even if both replacements are not in the 
training corpus. Of course, this is related to 
7~110006urns with ~240000 words 
1118 
the fact that a, repair is a syntactic and/or se- 
mantic anomaly. We make nse of it by a.dding 
two additional knowledge sources to our model. 
Minimal syntactic information is given by part- 
o f  speech (POS) tags and POS sequences, se- 
mmltic information is given by semantic word 
classes. Ilence the input is not merely a se- 
quence of words but a sequence of triples. Each 
triple has three slots (word, POS tag, seman- 
tic class). In the next section we will describe 
how we ol)tain these two information pieces \[br 
every word in the lattice. With this additional 
informa.tion, P(RDjI1LS',~ j) probability could 1)e 
smoothed by linea.r interpolation of word, POS 
and semantic la.ss replacement \])robabilities. 
= 
n,, l '(Word( l .Dj )ll4r o.rd( n,S'..j) ) 
+/3 ,  
+ 
with a '+\ [3+7=1.  
l'Vord(IM):i ) is the not~tion tbr 1;11(: selector of 
the word slot of the triple a,t position j .  
4 Integration with Lattice 
Processing 
We ca, ll llOW del ;e( ; t  a ,nd cor rec t  a, repa,ir, given a 
sentence a.nnotated with I)()S tag;s an(I seman- 
1;ic classes, l~tll, how ca.n we ('onsl;rucl, such a. 
sequence, from a wor(l la.tl;ic(<? Integrating the 
ntodel in a lattice algoril;h m requires three steps: 
? mapping the word la?tice to a. tag lattice 
? triggering IPs and extra.cting the possible 
rel)ar;md um/reparans l):~irs 
? intr<)ducing new paths to represent tile 
plausible repa.rans 
The tag lattice constrnction is adapted from 
(Samuelsson, 11997). For every word edge and 
every denoted POS tag a corresponding tag 
edge is crea,ted and tim resulting prol)ability 
is determined. \[I' a tag edge already exists, 
tile probabilities of both edges are merged. 
The original words are stored together with 
their unique semantic lass in a associated list. 
Paths through the tag graph a.re scored by a 
IX)S-trigram. If a trigger is active, all paths 
through the word before tim ll' need to be tested 
whether an acceptable rel)air segmentation ex- 
ists. Since the scope model takes at most \['our 
words for reparandum a.nd rel)a.ra.ns in account 
it is sufficient to expand only partial paths. 
l);ach of these partial paths is then processed by 
the scope model. To reduce the se~rch space, 
paths with a low score can be pruned. 
Repair processing is integrated into the Verb- 
mobil system as a. filter process between speech 
recognition a.nd syntactic analysis. This en- 
forces a rep~fir representation that ca.n be into- 
grated into a lattice. It is not possible to lna.rk 
only the words with some additional informa- 
tion, because a rel)air is a phenomenon that (le- 
pends on a path. Imagine that the system has 
detected ~ repair on ~ certain path in the btttice 
and marked all words by their top,fir function. 
Then a search process (e.g. the parser) selects a 
different D~th which shares only the words of the 
repa.randum. But these words are no reparan- 
dum for this path. A solution is to introduce a 
new path in the. lattice where reI)arandum a.nd 
editing terms a.re deleted. As we said betbre, we 
do not want l;o delete these segments, so they 
are stored in a special slot of 1;11o first word of 
the reparans. The original path can now 1)e re- 
construct if necessary. 
To ensure that these new I)aths are coml)~> 
ra.ble to other paths we score the reparandum 
the same wa.y the l)arser does, and add the re- 
suiting wdue to the \[irst wor(l of the reparaits. 
As a result, l>oth the original path a.nd the. one 
wil,h the repair get the sa.me score excel)t one 
word tra.nsition. The (proba.bly bad) transition 
in l, he original path from the last word o\[" the 
rei)arandtnn to the first word of 1;he repa.rans is 
rel)laeed by a. (proba.bly goo(t) transition From 
the repa.ran(hnn~s onset to the rel>arans. \Ve 
take the lattice in fig. 2 to give an example. 
The SCOl)e mo(M has ma.rked " l  ca.nnot" as the 
reparandum, "no" as an editing term, and "l 
ca.n" as the rel)arans. We sum tip the acoustic 
scores of "1", "can" and "no". Then we add the 
maximnm language model scores for the tra.n- 
sition to "1", to "can" given "I", and to "no" 
given 'T' and "can". This score is ~(I(le(1 as an 
offset to the acoustic score of the second "1". 
5 Resu l ts  and  Fur ther  Work  
Due to the different trigger situations we per- 
formed two tests: One where we use only 
acoustic triggers and ~mother where the exis- 
tence of a perfect word fr~gment detector is as- 
sume(1. The input were unsegmented translit- 
era.ted utterance to exclude intluences a word 
1 1 19 
recognizer. We restrict the processing time on 
a SUN/ULTI{A 300MIIZ to 10 seconds. The 
parser was simulated by a word trigram. Train- 
ing and testing were done on two separated 
parts of the German part of the Verbmobil cor- 
pus (12558 turns training / 1737 turns test). 
Detection Correct scope 
Recall Precision Recall Precision 
Test 1 49% 70% 47 % 70% 
Test 2 71% 85% 62% 83% 
A direct comparison to other groups is rather 
difficult due to very different corpora, eval- 
uation conditions and goals. (Nakatani and 
Hirschberg, 1.993) suggest a acoustic/prosodic 
detector to identify IPs but don't discuss the 
problem of finding the correct segmentation i  
depth. Also their results are obtained on a 
corpus where every utterance contains at least 
one repair. (Shriberg, 1994) also addresses the 
acoustic aspects of repairs. Parsing approaches 
like in (Bear et al, 1992; Itindle, 1983; Core and 
Schubert, 1999) must be proved to work with 
lattices rather than transliterated text. An al- 
gorithm which is inherently capable of lattice 
processing is prot)osed by Heeman (Hem-nan, 
1997). He redefines the word recognition prob- 
lem to identify the best sequence of words, cor- 
responding POS tags and special rel)air tags. 
He reports a recall rate of 81% and a precision 
of 83% for detection and 78%/80% tbr correc- 
tion. The test settings are nearly the same as 
test 2. Unibrtunately, nothing is said about the 
processing time of his module. 
We have presented an approach to score po- 
tential reparandum/reparans pairs with a rela- 
tive simple scope model. Our results show that 
repair processing with statistical methods and 
without deep syntactic knowledge is a promis- 
ing approach at least for modification repairs. 
Within this fi'alnework more sophisticated scope 
models can be evaluated. A system integration 
as a filter process is described. Mapping the 
word lattice to a POS tag lattice is not optimal, 
because word inlbrmation is lost in the search 
tbr partial paths. We plan to implement a com- 
bined combined POS/word tagger. 
References  
A. Batliner, R. Kompe, A. Kiettling, M. Mast, 
H. Niemann, and F,. NSth. 1998. M = 
syntax + prosody: A syntactic-prosodic la-
belling schema for large spontaneous speech 
databases. Epeech Communication, 25:193- 
222. 
J. Bear, J. Dowding, and E. Shriberg. 1992. 
Integrating multiple knowledge sources \["or 
detection and correction of repairs ill hu- 
man computer dialogs. In Proc. ACL, pages 
56-63, University of Delaware, Newark, 
Delaware. 
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. 
Della Pietr~, F. Jelinek, J. D. Lafferty, R. L. 
Mercer, and P. S. Roossin. 1990. A sta.tisti- 
cal approach to machine translation. Compu- 
tational Linguistics, 16(2):79-85, June. 
M. G. Core and K. Schubert. 1999. Speech re- 
pairs: A parsing perspective. Satellite meet- 
ing ICPIIS 99. 
P. A. I-Iceman. 1997. Speech Repairs, Into- 
nation Boundaries and Discourse Markers: 
Modeling Epeakcrs' Utterances in ,5'pokcn Di- 
alog. Ph.l). thesis, University of Rochester. 
D. Hindle. 1983. Deterministic parsing of syn- 
tactic nontluencies. In Proc. ACL, MIT, 
Cambridge, Massachusetts. 
S. M. Katz. 1987. Estimation of probabilities 
from sparse data for tile language model con> 
ponent of a speech recognizer. 7)'ansaction 
on Acoustics, ,5'pcech and ,5'ignal 1)rocessing, 
ASSl'-35, March. 
ill). Klakow, G Rose, and X. Aubert. 1999. 
OOV-Detection in Large Vocabulary Sys- 
tem Using Automatically Defined Word- 
Fragments as Fillers. In EUR.OSPEECII '99, 
volume 1, pages 4:9-52, Budapest. 
W. Levelt. 1983. Monitoring and self-repair in 
speech. Cognition, 14:41-104. 
C. Naka.tani and a. tlirschberg. 1993. A speech- 
tirst model for repair detection and correc- 
tion. In P,vc. ACL, Ohio State University, 
Cohmbus, Ohio. 
C. Samuelsson. 1997. A left-to-right tagger for 
word graphs. In Proc. of the 5th Inter'national 
workshop on Parsing technologies, pages 171- 
178, Bosten, Massachusetts. 
E. E. Shriberg. 1994. Preliminaries to a Theory 
of Epeech Disflucncics. Ph.D. thesis, Univer- 
sity of California. 
A. Stolcke, E. Shriberg, D. Hakkani-Tur, and 
G. Tur. 1999. Modeling the prosody of hid- 
den events for improved word recognition. In 
EUROS'PEECII '99, volume 1, pages 307- 
310, Budapest. 
1120 
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 1?9,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Content Analysis of Museum Documentation with a Transdisciplinary
Perspective
Gu?nther Goerz, Martin Scholz
University of Erlangen-Nuremberg, Computer Science Department (8)
Erlangen, Germany
goerz@informatik.uni-erlangen.de
Abstract
In many cases, museum documentation
consists of semi-structured data records
with free text fields, which usually refer
to contents of other fields, in the same
data record, as well as in others. Most of
these references comprise of person and
place names, as well as time specifica-
tions. It is, therefore, important to rec-
ognize those in the first place. We report
on techniques and results of partial pars-
ing in an ongoing project, using a large
database on German goldsmith art. The
texts are encoded according to the TEI
guidelines and expanded by structured de-
scriptions of named entities and time spec-
ifications. These are building blocks for
event descriptions, at which the next step
is aiming. The identification of named en-
tities allows the data to be linked with var-
ious resources within the domain of cul-
tural heritage and beyond. For the latter
case, we refer to a biological database and
present a solution in a transdisciplinary
perspective by means of the CIDOC Con-
ceptual Reference Model (CRM).
1 Specific Goals of Content Analysis
When we speak of museum documentation, we
address a wide variety of document types. First
of all, there are acquisition and inventory lists or
index cards, which contain more or less detailed
records of museum objects. Often these are ac-
companied by photographs, restoration records,
and further archival records. If curators prepare
exhibitions, usually they provide catalogs by com-
piling data from sources, such as those just men-
tioned, and by contributing short articles on the ex-
hibits. Last but not least there are scholarly mono-
graphs on museum objects.
With the introduction of information technol-
ogy in museums and cultural heritage institu-
tions, such records have been stored in (relational)
database systems and content management sys-
tems. At the beginning ? with the exception
of bibliographic records ? there were no meta-
data standards at all in the museum world. Since
the 1990s, many metadata schemata have been
proposed for the field of cultural heritage, some
with very detailed classification features for spe-
cific object types1. There is still an active dis-
cussion about metadata schemata and their stan-
dardisation, as can be seen with recent proposals
for CDWA Lite, museumdat and their combination
(Stein and Coburn, 2008).
Today, access to museum documentation via the
World Wide Web has become a matter of course,
in particular, if the documentation has been the
result of publicly funded research projects. Nat-
urally, printed editions are still a very important
medium of publication. However, in many cases
the data are too voluminous, which means only
abridged versions are published in print, while
the full data are available only in digital form.
Web access allows many means to retrieve and
print the data, with very little cost involved. Us-
ing controlled language defined in terminologies
and formal ontologies, different forms of ?intelli-
gent search? come within reach as well as inter-
active evaluation and visualisation methods. But
it is not only access to the data alone; interactiv-
ity opens up possibilites for Wiki-style annotation
and scholarly communication, as well as forums
for the general public. Furthermore, the technol-
ogy provides methods to link the data with other
resources, e.g. authority files containing biograph-
ical or geographical data.
1cf. Getty Foundation?s Metadata Crosswalk http:
//www.getty.edu/research/conducting_
research/standards/intrometadata/
crosswalks.html ;visited 03.12.2008.
1
A common situation in museum documenta-
tion is characterized by the fact that it is cen-
tered around museum objects, i.e. there is a
database system or content management system,
which contains structured descriptions of museum
objects and further information about their cre-
ators, provenance, use, and so forth, according
to given descriptive and administrative metadata
schemata. Besides fields in such data records en-
forcing (more or less strictly defined) data types,
e.g. for inventory numbers, there are free text
fields which contain important background infor-
mation about persons, objects, materials, stylistic
festures, etc. without any further tagging. Ba-
sically, the free text fields are open for any kind
of information which cannot be expressed in the
strictly defined parts of the schema. Therefore,
overall, the given data records at best provide a
semi-structured representation.
The free text fields and their relations to other
fields, in particular, indicate a clear need for con-
tent analysis. Firstly, named entitities must be
identified, in particular person and geographic
place names. For instance, there may be a data
field for the creator of a work of art and another
one for the place where this work was created, ad-
ditionally one or more free text fields which talk
about the artist?s family relations, when he came to
the mentioned place and how long he stayed there,
etc. As this example indicates, at least a second
type of linguistic expressions, time specifications
in a variety of forms, ought to be recognized. In
the future, we would like to identify event descrip-
tions and how they are related among each other,
for which the recognition of named entitites and
time specifications is a first step.
In the following sections we describe our ap-
proach to address these problems. The next sec-
tion outlines characteristic features of the data
with a reflection on their typicality. Section three
is the central technical part presenting the shallow
text analysis techniques we use ? word class tag-
ging, recognition of temporal specifications, place
and person names ? and the utilization of name
authorities for lexical and semantic enrichment.
In the fourth section we show how the results
achieved so far can be used to construct event-
based shallow semantic representations related to
the CIDOC CRM. Furthermore, the CRM is also
the key to transdisciplinary approaches in museum
documentation as outlined in the final section with
an example between biology and cultural history.
2 Characteristics of the Data
We are working2 with data which resulted from a
project on goldsmith art in Nuremberg, executed
at the German National Museum, providing de-
scriptions of more than 6700 objects, 2290 artists,
many locations, etc. Furthermore, with the mu-
seum?s content management system we can access
many more data records on sculptures and paint-
ings ? with a particular emphasis on the work of
Albrecht Du?rer ? up to 1800. The latter corpora
were accessed primarily to verify the general use-
fulness of the approach that will be presented in
the following sections.
For many projects in the field of cultural her-
itage in Germany, a condition for public fund-
ing has been to use the MIDAS3 data schema
(Heusinger, 1989) in combination with a spe-
cific database implementation (HiDA). MIDAS
defines a framework of record types with appro-
priate properties for terms (thesauri), time, place,
artists, other persons and organizations, objects,
content and signs, events, sources, and adminis-
trative data. The goal of MIDAS was to establish
a de facto standard based on the current documen-
tation practice in museums. Depending on what
is to be documented, the appropriate record types
can be selected. HiDA is a data administration sys-
tem, which provides a graphical user interface for
data input, editing, and search; it stores the records
not in a database system, but in a system of files,
one for each type, in a proprietary format. For
this reason and problems in handling the user in-
terface, many HiDA-encoded data are now being
converted to an XML representation. For the free
texts, we decided to follow the encoding rules of
the Text Encoding Initiative (TEI) (Ide and Vero-
nis, 1995)4 for text bodies.
The actual encoding of the XML-transformed
data sets is still very close to HiDA as far as
the ?classes? and properties are concerned. Cur-
rently, the data are in the process of being trans-
formed to the emerging museumdat/CDWA Lite
2Research project ?WissKI ? Wissenschaftliche Kom-
munikationsInfrastruktur?; funding provided by the German
Research Council (DFG)
3Acronym for ?Marburger Informations-,
Dokumentations- und Administrations-System?, not to
be confused with the MIDAS heritage standard in the UK.
4Website: http://www.tei-c.org/index.xml ;
visited 15.12.2008
2
standard (Stein and Coburn, 2008)5, which in turn
is compatible with CIDOC?s Conceptual Refer-
ence Model (Doerr, 2003)6. The CRM is the for-
mal reference ontology, which defines the con-
ceptual background for the semantic representa-
tions resulting from content analysis. We refer to
the CRM as a formally defined reference ontology
because with the ?Erlangen CRM?7 we provided
is a description logic version of the latest stan-
dard (ISO 21127:2009), implemented in OWL-DL
(Goerz et al, 2008).
As for the content of the free text fields, the texts
contain well-formed sentences in the linguistic
sense, although in some cases, one can find elliptic
formulations in telegraphic style. In most cases,
the texts refer to defined data record fields (per-
sons, creatorship, object properties, bibliographic
data), providing additional information, for which
there is no other place in the schema. A great deal
of the texts talk about family and other relations
between persons, about creatorship, techniques,
actions of the mentioned persons other than the
creation of the artwork, and the general cultural
context. As in early modern German there is a
great orthographic variation even in writing per-
son and place names, many of the texts suggest
disambiguations of different kinds. Nevertheless,
there are still many writing variants of named en-
tities. Furthermore, many texts contain quotations
from reference works, some of which are too old
to obey the actual orthographic standards.
It is important to notice that the actual data we
have to deal with are nevertheless a typical ex-
ample of the state of the art of documentation in
many cultural heritage institutions. Hence, the
techniques of content analysis and annotation pre-
sented in the following will be of a general utility
in many similar projects.
3 Content Analysis: Shallow Parsing and
Semantic Representation
The texts contained in the free text fields are en-
coded with the TEI Lite tag set, supplemented by
5cf. slide set by Georg Hohmann: http:
//www8.informatik.uni-erlangen.de/IMMD8/
Services/transdisc/cidoc2008_hohmann.pdf
; visited 03.12.2008
6The actual version of the ISO standard and a lot of
accompanying materials can be retrieved from http://
cidoc.ics.forth.gr/ ; visited 03.12.2008.
7http://www8.informatik.uni-erlangen.
de/IMMD8/Services/cidoc-crm/ ; visited 05.02.
2009
tags of the module namesdates for person and
place names. For processing, all texts in the free
text fields of a MIDAS file ? e.g., the ?object? file
containing all object descriptions in the ?database?
? are merged in a single multi-text TEI file. Each
text from a data field is represented as a <text>
element where the text proper without further an-
notations is contained in its subordinate <body>
element. The association between the TEI text el-
ements and the orginal MIDAS data fields is as-
sured by unique XML identifiers in xml:id at-
tributes. The ?raw? text data are transformed au-
tomatically into the initial TEI representation in a
rather straightforward way by a script. No further
internal structuring is provided at this stage; anno-
tations are added by subsequent processing steps.
Shallow parsing for place names and time spec-
ifications is based on sets of chunk rules imple-
mented with the Definite Clause Grammar (DCG)
formalism8 which are executed by Prolog. There
are grammars for person and place names and for
time specifications; these sets of grammar rules
define three partial ?parsers?. For the three parsers
there is only one pass, and there is in principle no
restriction on the order in which they are applied.
The parsing results of each of the parsers are rep-
resented as feature structures, which are then con-
verted to TEI tags and inserted into the file by a
separate software component. At this stage, there
is no recognition and resolution of anaphoric ref-
erences, such as pronouns. In a second and inde-
pendent pass, a lookup of person and place names
in Name Authority files is executed and the results
are collected in local files. There is no filtering ap-
plied to the lookup because, at this point, no spe-
cial knowledge about these resources is available.
3.1 Tagging
First of all, the texts encoded conforming to the
TEI guidelines are annotated with word class tags
and lemmata (base forms) by a POS tagger. Lem-
matisation is very useful in languages with a rich
inflectional system, such as German. For POS tag-
ging, we use the Stuttgart TreeTagger9 with the
STTS tagset which provides categories for Ger-
man words and delimiters.
8based on previous work by (Tantzen, 2004).
9Institute for Automatic Language Processing of the
University of Stuttgart. The tagger is available at http:
//www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/DecisionTreeTagger.
html ; visited 03.12.2008.
3
The resulting TEI tags express morphosyntac-
tic descriptions. Of particular interest are the tags
<w> for encoding words and <c> for individ-
ual punctuation marks which are very well suited
for encoding tokens: Both can accept an attribute
type for the determination of the word or charac-
ter class. Lemmata are encoded with the attribute
lemma.
3.2 Time Specifications
The ?temporal? grammar/parser recognizes a
broad variety of temporal expressions built up
from days, weeks, months, seasons of the year,
years, decades, and centuries.10 Time specifica-
tions may be given as absolute or relative.
Absolute time specifications describe unique
time points or intervals on the time line, such as
calendar dates (e.g. 28. August 1749) and open or
closed time spans (e.g. bis 1832, up to 1832). Fur-
thermore, modifying particles are recognized, e.g.
Mitte 1749 (midyear 1749) or Ende Ma?rz 1832
(end of March 1832).
To determine the missing data in relative time
specifications, such as drei Wochen spa?ter (three
weeks later), a kind of anaphoric resolution
method is applied. Therefore, we keep track of
all occurences of temporal expressions. For reso-
lution, we choose the most recently mentioned at
the appropriate level (day, month, year).
3.3 Places
The normal case of place specifications in the
goldsmith corpus consists of a geographic place
name or a preposition followed by a place name.
In some cases there are also enumerations of place
names. We distinguish between a named entity
and the corresponding linguistic phrase. Named
entities are looked up in a local dictionary which
is built from entries in Name Authorities.
Before lexical lookup, a procedure is executed
which prevents the annotation of lower case words
as named entities. It implements the simple
heuristics that ? even composite ? named en-
tities are designated by words beginning with a
capital letter, but not each word beginning with a
capital letter is a named entity as in English. In
German, a noun must be written with its first letter
in upper case.
Each named entity is associated with one out
of ten geographical types to avoid aggregations of
10The actual text corpus does not contain time of day ex-
pressions.
incompatible types as in die Sta?dte Mu?nchen und
Berlin und Finnland (the cities Munich, Berlin and
Finland). On the other hand, certain words such
as city, town, settlement, etc. are associated with
such a type (?city?) to be used as a constraint on
subsequent proper nouns.
3.4 Persons
Parsing of person names is much more difficult be-
cause they are more complex and there is a con-
siderably larger variation than with place names.
Whereas, usually, composite place names are lex-
icalized, this is not a real option for person names.
Every person in German speaking countries has
at least one first and one surname, optionally
amended by further forenames, appellations of no-
bility or ancestry or generation. We do not regard
titles and forms of address such as Ko?nig (king)
or Herr (Mister) as parts of names ? in spite of
the fact that according to German law the title of
Doktor (doctor) is a part of the name.
For name parsing, the constituents of names are
divided into four categories: forenames, surnames,
copula, and generation appellations. The class of
copula subsumes many particles which serve as
predicates of nobility or ancestry, e.g. von, van der
or French/Spanish/Italian de la. The category of
generation appellations contains words and num-
berings to distinguish persons with the same name,
e.g. Karl V., Albrecht Du?rer der ?Altere.
There are several sources of ambiguities with
person names the grammar has to deal with, as
well w.r.t. the correct interpretation of their parts
as regarding their reference:
? Persons are often referenced not by their full
name, but only by their first or surname.
? Many first names may also occur as sur-
names.
? Many surnames are also names of profes-
sions or places.
? There are several standards of equal range for
the ordering of name parts.
? The use of a comma to separate surname and
first name can be confused with an enumera-
tion and vice versa.
Therefore we use dictionaries for the four cat-
egories of name parts. There are words, which
may be members of several categories, if there are
several possibilities of interpretation. The dictio-
naries for generation appellations and copula are
4
small and have been assembled manually. For
first and surnames, several name lists were com-
piled into one dictionary file from lists available
via Name Authorities and also from the WWW.
To recognize person names containing very rare
first and surnames, as well as writing variants
which are not contained in the lexicon, we use a
system of syntactic and semantic cues ? based on
statistical analyses of the texts ? which indicate
the occurence of a name at a specific location (cf.
table).
syntax of the trigger Example
profession name Goldschmied Samuel Klemm
appellation name Frau Martha
preposition relation name mit Meister Silvester
possessive pron. rel. name Seine Tochter Katharina
relation des/der name Tochter des Christian Mahler
relation von name Sohn von Peter v. Quickelberg
relation : name Lehrling: Lang, Joh. Christoph
Table 1: Rules for person name triggers. Words to be in-
serted for profession, appellation and relation are taken from
hand-made lexica.
Statistical analysis of the goldsmith corpus has
given clear evidence for three groups of words
whose occurrence indicates an immediate follow-
ing person name: Appellations of professions, ap-
pellations plus titles, and relations between per-
sons. A relation between persons is regarded as a
cue only if certain particles occur immediately be-
fore or after it. The word sequence ?Tochter von?
(daughter of) is a good example of such a cue for
a subsequent person name.
In a first step, the name parts and the cues are
labelled separately. In a second pass, whenever a
cue or a name part is encountered, an algorithm to
assemble the parts into complete person names is
run. It tries to match the current word sequence
with different patterns of name parts which con-
stitute valid person names, i.e. it applies different
finite state machines11 to the word sequence. The
longest sequence recognized by a finite state ma-
chine is assumed to be a name (see Table 2).
3.5 Name Authorities
To achieve a normalization of appellations, per-
son and place names are looked up in name au-
thorities. There are several authorities, none of
which can claim completeness, and each has its
11Finite State Machines are formal automata which recog-
nize regular expression patterns; i.e., both notions are equiv-
alent.
Pattern Example
s Jamnitzer
s g Jamnitzer II
f+ s Hans Jamnitzer
f+ g c s *Hans II von Jamnitzer
f+ g s Hans II Jamnitzer
f+ c s *Hans von Jamnitzer
f+ g Hans II
f+ Hans
s , f+ g Jamnitzer, Hans II
s , f+ c *Jamnitzer, Hans von
s , f+ g c *Jamnitzer, Hans II von
s , f+ Jamnitzer, Hans
Table 2: Recognized name patterns with examples showing
the name of the goldsmith ?Hans II Jamnitzer?. s stands for
surname, f for forename, c for copula and g for generation
particle. The ?+? sign expresses one or more occurences; the
asterisk indicates that the name has been modified to fit the
pattern with ?von?.
strengths and weaknesses. Up to now, we have
used the following interfaces ? however, fur-
ther interfaces are in preparation: BGN: Board
on Geographic Names (German places File)12,
Diskus ?Geographie-Datei? (distributed with MI-
DAS)13, Orbis Latinus (Graesse)14, Getty TGN
(Thesaurus of Geographic Names)15, PKNAD
(Person Names) by prometheus e.V.16, and Getty
ULAN (United List of Artist Names)17
There are two modes of use for name authorities
in the process of named entity recognition:
1. Decision making: The data are used as dic-
tionaries for the person name and place name
parsers.
2. Enrichment with metadata in a second phase
once the named entities are identified.
As there are not yet unique formats and inter-
12http://earth-info.nga.mil/gns/html/
namefiles.htm ; visited 17.12.2008
13http://museum.zib.de/
museumsvokabular/index.php?main=download
; visited 17.12.2008
14http://www.columbia.edu/acis/ets/
Graesse/contents.html ; visited 17.12.2008
15http://www.getty.edu/research/
conducting_research/vocabularies/tgn/ ;
visited 17.12.2008
16http://www.prometheus-bildarchiv.
de/index.php?id=56\&L=0\&skin=0 ; visited
17.12.2008
17http://www.getty.edu/research/
conducting_research/vocabularies/ulan/
; visited 17.12.2008
5
faces for the mentioned name authorities, we im-
plemented a querying interface for each name au-
thority in both modes with the exception of the
Getty vocabularies. These are not used directly
as dictionaries, but only for metadata enrichment,
because the data must be retrieved place by place
from individual web pages due to the lack of an
appropriate API.
3.5.1 Name Authorities as Dictionaries
Name authorities can be directly accessed through
the dictionary interfaces of the place and person
name parsers. To accelerate the search for entries,
the retrieved data are stored in local dictionary
files, one for each name authority. The dictionary
files can be generated either during the recognition
process or off-line. To keep the local data up to
date, the generation process should to be repeated
from time to time, at least for some of the men-
tioned resources.
3.5.2 Name Authorities for Metadata
Harvesting
Metadata harvesting has been implemented as a
separate process; it consists of the search for anno-
tations of named entities in the TEI files, of query-
ing name authorities and collecting the metadata
through special interfaces, encoding in an appro-
priate format and storing in local files. We do not
rank name authorities and the content of the meta-
data; its structure and degree of detail are taken as
retrieved. However, with each data set the list of
IDs of the tagged findings in the TEI file is stored.
3.6 TEI-Encoding of Named Entities
Temporal expressions are encoded with the
<date> tag. For the attributes, the distinction be-
tween time spans and time points is represented by
the attributes from and to, or the attribute when,
resp.
The tag <placeName> is used to anno-
tate place expressions as a whole. To label
the named entities contained within, the TEI
module namesdates provides six tags accord-
ing to its geographical type: <district>,
<settlement>, <region>, <country>,
<bloc> und <geogName>; for some of them
there may be a refinement by means of the ten ge-
ographic types mentioned in 3.3 with the attribute
type.
For person names, the TEI tag <persName>
and several subtags are defined, among which
<surname>, <forename>, <nameLink> and
<genName> correspond exactly to the name parts
presented above.
3.7 Evaluation Results
The three partial parsers are executed in sequential
order. The best results were obtained in the order
time ? person ? place:
On the goldsmith corpus with a test set of about
2000 word types, a precision of 81.8% and a recall
of 72.6% was achieved with the described level of
granularity, i.e., accounting for the distinction of
first and last names and geographic types.
If these distinctions are dropped, as in many
other systems, precision increases to 83.0% and
recall to 82.6%.
A separate evaluation of the parsers (in paren-
theses: with distinctions) showed for
? time: precision 89.0% and recall 92.1%,
? person: precision 74.4% (71.6%) and recall
87.0% (75.5%),
? place: precision 78.9% (69.1%) and recall
76.9% (71.7%),
Depending on the choice of name authorities
used for lexicon generation, and due to a high de-
gree of ambiguity, too many words may be clas-
sified as place names. For this reason, BGN has
been left out, because it led to a considerable de-
crease of precision and a slight increase of recall.
4 Building Blocks for Event Recognition
With parsing results for person and place names
and time specifications, we have a first-level
partial semantic representation of text chunks,
which could be combined into larger representa-
tion structures. However, considering the char-
acteristics of the given free texts and the state of
the art in computational linguistics, it would be
presumptuous to aim at a deep semantic analysis.
Nevertheless, under the assumption of composi-
tionality, i.e., the assumption that semantic rep-
resentations of larger units are to be composed
from those of their parts in a systematic way, it
is possible to assemble partial semantic represen-
tations. In particular, we are interested in identi-
fying events and the involved actors, objects, and
instruments. Event recognition in texts has been
an active research area in recent years, in particu-
6
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<TEI>
<teiHeader>
...
</teiHeader>
<text>
<group>
<text xml:id="kue00020e00029">
<body>
Er ist offensichtlich identisch mit dem Ornamentstecher
<persName xml:id="persName4815108">
<forename>Theodor</forename>
<surname>B.</surname>
</persName>
und stammte wie
<persName xml:id="persName6059828">
<surname>Bang</surname>
,
<forename>Hieronymus</forename>
</persName>
<placeName type="zone" xml:id="placeName12514145">
aus
<settlement type="stadt">Osnabr&uuml;ck</settlement>
</placeName>
(Verwandtschaft?) Kein Eintrag in den Eheb&uuml;chern
<date from="1600-01-01" to="1699-12-31" xml:id="date33491090">
des 17. Jhs.
</date>,
kein Eintrag im Totenbuch St.
<placeName type="zone" xml:id="placeName3113238">
<district type="stadtteil">Sebald</district>
</placeName>
bzw.
<placeName type="zone" xml:id="placeName9131644">
<district type="stadtteil">Lorenz</district></placeName>
bis
<date from="1623-01-01" to="1630-12-31" xml:id="date24591544">
1623/30
</date>.
<date from="1611-01-01" to="1611-12-31" xml:id="date22562823">
Von 1611
</date>
stammt eine von
<persName xml:id="persName5006112"><surname>Bang</surname></persName>
gestochene Ansicht
<placeName type="zone" xml:id="placeName4837279">
von
<settlement type="stadt">Bamberg</settlement></placeName>.
<persName xml:id="persName7446303">
<forename>Balthasar</forename> <surname>Keimox</surname>
</persName>
verlegte von ihm eine Folge von
12 Stichvorlagen mit reichem Arabeskenwerk.
</body>
</text>
</group>
</text>
</TEI>
Figure 1: Parsing result: annotated text in TEI encoding. (Layout has been rearranged for readability.)
7
lar in combination with text mining.18 In previous
work (Fischer et al, 1996; Bu?cher et al, 2002), we
augmented a chart-based chunk parser with an in-
cremental construction procedure for (partial) Dis-
course Representation Structures (DRSs). DRSs
are semantic representations which contain a list
of discourse referents, introduced by named enti-
ties or definite noun phrases, and a body, which
consists of a possibly complex logical form repre-
senting the meaning of the given part of speech19.
For events, we use a neo-Davidsonian represen-
tation, i.e., the corresponding verb is a one-place
predicate whose argument is a discourse referent
representing an event, conjoined with binary re-
lations for the thematic roles. For example, the
sentence ?Albrecht Du?rer painted a self-portrait
in 1500 in Nuremberg? would get a semantic rep-
resentation in which ? extremely simplified ?
e would be the discourse referent for the event,
paint(e) the representation of the event, and ac-
tor(e,a), object(e,s), time(e,1500), etc. constitute
the body, where a and s are the discourse referents
for the artist and the self-portrait, resp. DRSs are
reaching beyond sentence limits and can in prin-
ciple be combined into larger and larger discourse
structures. Therefore, they are appropriate repre-
sentations on which reference resolution mecha-
nisms, such as those described in (Fischer et al,
1996) can be built. In our current work, a cen-
tral activity is to port this method and its imple-
mentation to the museum documentation domain
and enrich it by collocational analysis as in (Smith,
2002).
The representation of events is not only an ex-
tremely important key to content analysis, but also
the pivot which connects various objects, persons,
places, with each other, making a variety of con-
nections explicit, which are implicitly contained
in the data fields and free texts of records of dif-
ferent types. It, therefore, becomes an obvious
goal to enrich such relational structures with fur-
ther information elements from other cultural her-
itage resources ? beyond name authorities. In our
particular application, access to Getty?s Art and
Architecture Thesaurus (AAT), to other museum
and collection databases or online auction cata-
logs would be obvious. Unfortunately, many of
18To quote just one prominent example, cf. the TERQAS
(Time and Event Recognition for Question Answering)
Symposium, 2002, http://www.timeml.org/site/
terqas/index.html ; visited 05.02.2009
19cf. (Kamp and Reyle, 1993)
these resources use idiosyncratic data formats just
as MIDAS mentioned above. At best, they refer
to a formal representation of their respective do-
main, in terms of a so-called ?formal domain on-
tology?, a representative hierarchical structure of
concepts, properties and constraints of the domain.
However, to satisfy the desideratum of linking di-
verse data collections, an intermediate level of in-
teroperability is required. A well proven approach
for such information integration tasks is to link the
different domain ontologies to a generic reference
ontology, which contains just the fundamental and
most general concepts and properties for a wide
variety of applications. In fact, for the field of
cultural heritage, CIDOC?s Conceptual Reference
Model (CRM) is such a reference ontology. It is
worthwhile to notice that, among other things, the
CRM emphasizes the event-driven perspective, in
fact, events are the glue in CRM which connects
all documentation elements. As a first step, we
have already implemented a generator for CRM
instances from TEI-conformant texts with named
entity annotations.
5 Transdisciplinary Aspects
Coming back to our project on goldsmith art doc-
umentation, we recognize clues in the data, which
point beyond the domain of cultural history: there
are goblets and centerpieces (epergnes) showing
sculptered animals, such as lizards and beetles.
Two of the documented objects exhibit a beau-
tiful stag beetle, which induced interesting ques-
tions about those insects, not only on their icono-
graphic significance, but also on their determina-
tion and classification in biology, the distribution
of species, etc. This illustrates that there is a
need to connect with further knowledge sources,
such as resources from biology, biodiversity re-
search, etc. For example, we may want to con-
sult a database such as BIODAT, maintained by
the natural history museum Koenig in Bonn. Con-
sidering the completely different scientific back-
ground and the different perspectives in descrip-
tion, this task seems to be very ambitious, to say
the least. Whereas the stag beetle in the foot of the
goblet is described in terms of art history and met-
allurgy, we find a completely different description
of a pinned stag beetle in the BIODAT data base.
We may be lucky to identify it there if we know
the precise species name in advance, but in many
cases, there is a significant chance that the match-
8
ing task will fail. At this point in time, we can only
provide a sketch in terms of an example how we
would approach this challenge. But it seems ob-
vious if we could find a general way to connect to
different description systems, we would approach
the long-term goal of an ?epistemic web?.
Recent efforts showed that there is in fact a
way to a solution, indicated by the term ?trans-
disciplinarity?; first results have been presented
at the first meeting of the CIDOC working group
on ?Transdisciplinary Approaches in Documen-
tation?20. Originating from philosophy of sci-
ence (Mittelstrass, 2002), transdisciplinarity con-
centrates on problems, which cannot be solved
within a single disciplinary framework. It takes
a new view on the unity of science, focussing on
scientific rationality, not systems. Taking into ac-
count that for all sciences there are common ele-
ments in the practice of argumentation and justifi-
cation, transdisciplinarity is a research principle in
the first place. Its emphasis on rational language
use in science offers a clue to the field of docu-
mentation; as a starting point, our methodological
focus is first of all on data integration . Taking
into account that transdisciplinarity addresses the
practice of research, this framework should sup-
port an action and event perspective on a generic
level, i.e. for the tasks of classification, represen-
tation, annotation, linking, etc.
In fact, we claim that the CIDOC CRM can play
the role of such a transdisciplinary framework;
at least for the stag beetle on goblets and still
life paintings, some other insects and also birds
on drawings and paintings, the modelling task
has already been performed successfully. For the
birds ? hooded crows in Dutch winter scenes in
Brueghel paintings ? our transdisciplinary mod-
elling effort provided a nice result for biodiversity
research as a side effect: During the ?little ice age?
hooded crows lived in Western Europe, whereas
today they can only be found east of the Elbe river.
Acknowledgments
The authors are grateful for valuable hints and
discussions to Siegfried Krause, Georg Hohmann,
Karl-Heinz Lampe, and Bernhard Schiemann and
to the anonymous reviewers for valuable sugges-
tions.
20at the CIDOC 2008 conference; online materi-
als are available via http://www8.informatik.
uni-erlangen.de/IMMD8/Services/
transdisc/ ; visited 03.12.2008.
References
Kerstin Bu?cher, Gu?nther Goerz, and Bernd Ludwig.
2002. Corega Tabs: Incremental semantic compo-
sition. In Gu?nther Goerz and et al, editors, KI-2002
Workshop on Applications of Description Logics,
Proceedings, volume 63 of CEUR Workshop Pro-
ceedings, Aachen, September. Gesellschaft fu?r In-
formatik e.V.
Martin Doerr. 2003. The CIDOC Conceptual Refer-
ence Model: an ontological approach to semantic in-
teroperability of metadata. AI Magazine, 24(3):75?
92, September.
Ingrid Fischer, Bernd Geistert, and Gu?nther Goerz.
1996. Incremental semantics construction and
anaphora resolution using Lambda-DRT. In S. Bot-
ley and J. Glass, editors, Proceedings of DAARC-
96 ? Discourse Anaphora and Anaphor Resolution
Colloquium, pages 235?244, Lancaster, July.
Gu?nther Goerz, Martin Oischinger, and Bernhard
Schiemann. 2008. An Implementation of the
CIDOC Conceptual Reference Model (4.2.4) in
OWL-DL. In Proceedings of the 2008 Annual Con-
ference of CIDOC ? The Digital Curation of Cul-
tural Heritage, pages 1?14, Athens, Benaki Mu-
seum, September 15?18.
Lutz Heusinger. 1989. Marburger Informations-,
Dokumentations- und Administrations-System (MI-
DAS) / [1,2]. Saur, Mu?nchen.
Nancy Ide and Jean Veronis, editors. 1995. Text En-
coding Initiative. Background and Context. Kluwer,
Dordrecht. Also in: Computers and the Humanities.
Vol. 29, No. 1?3 (1995).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer, Dordrecht.
Ju?rgen Mittelstrass. 2002. Transdisciplinarity ? new
structures in science. In Innovative Structures in
Basic Research. Ringberg-Symposium, 4?7 October
2000, number 5 in Max Planck Forum, pages 43?54,
Mu?nchen.
David A. Smith. 2002. Detecting events with date
and place information in unstructured text. In Pro-
ceedings of the 2nd ACM+IEEE Joint Conference
on Digital Libraries, pages 191?196, Portland, OR.
Regine Stein and Erin Coburn. 2008. CDWA Lite
and museumdat: New developments in metadata
standards for cultural heritage information. In Pro-
ceedings of the 2008 Annual Conference of CIDOC,
Athens, September 15?18.
Svenja Tantzen. 2004. Ein Prologparser fu?r temporale
und lokale Ausdru?cke in einem ?Geosem-System?
fu?r das Deutsche. Technical report, Friedrich-
Alexander-Universita?t Erlangen-Nu?rnberg,
Philosophische Fakulta?t II, Erlangen. Master
Thesis.
9
