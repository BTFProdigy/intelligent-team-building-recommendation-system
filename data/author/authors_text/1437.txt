Contrast And Variability In Gene Names 
 
K. Bretonnel Cohen 
Center for Computational Pharmacology 
University of Colorado Health Sciences Center 
kevin.cohen@uchsc.edu 
Andrew E. Dolbey 
Center for Computational Pharmacology 
University of Colorado Health Sciences Center 
andrew.dolbey@uchsc.edu 
 
George K. Acquaah-Mensah 
Center for Computational Pharmacology 
University of Colorado Health Sciences Center 
george.acquaah-mensah@uchsc.edu 
Lawrence Hunter 
Center for Computational Pharmacology 
University of Colorado Health Sciences Center 
larry.hunter@uchsc.edu 
 
 
Abstract 
We studied contrast and variability in 
a corpus of gene names to identify 
potential heuristics for use in 
performing entity identification in the 
molecular biology domain.  Based on 
our findings, we developed heuristics 
for mapping weakly matching gene 
names to their official gene names.  
We then tested these heuristics against 
a large body of Medline abstracts, and 
found that using these heuristics can 
increase recall, with varying levels of 
precision.  Our findings also 
underscored the importance of good 
information retrieval and of the ability 
to disambiguate between genes, 
proteins, RNA, and a variety of other 
referents for performing entity 
identification with high precision. 
1 Introduction 
Almost all current approaches to entity 
identification are actually not tackling the 
identification per se, but rather merely the (still 
difficult) location of named entities in text.  The 
difference between these is that entity location 
consists of the (difficult enough) task of 
demarcation of the boundaries of names in text, 
whereas entity identification consists of the 
same thing, plus mapping the located names to 
the canonical entities that they refer to.  In this 
paper we present data on variability in the 
orthographic representation of gene names, and 
then show how knowledge about that variability 
can be used for heuristics that increase recall in 
the entity identification task.  (We use the term 
"gene name" as shorthand for "gene, protein, or 
RNA name.") 
 
To understand why it is important to be able 
to map located names to the canonical entities 
that they refer to, consider the outcome of 
running an information extraction routine with 
access only to entity location against a 
hypothetical document about rat somatotropin.  
It contains the synonymous names rat 
somatotropin, somatotropin, and growth 
hormone, all of which refer to the same 
biomolecule, whose canonical name we will 
assume to be somatotropin.  (The document is 
hypothetical; somatotropin and its synonyms are 
not.)  Suppose that the paper includes three 
separate assertions, of the form somatotropin is 
upregulated by X, transcription of rat 
somatotropin is blocked by Y, and growth 
hormone is expressed by cells of type Z.  The 
system correctly extracts three assertions, but 
incorrectly attributes them to three separate 
biomolecules, only one of which is the canonical 
form.  Now consider the outcome of running an 
information extraction routine with access to 
entity identification against the same document.  
Again, the system extracts three assertions, but 
this time all three assertions are correctly 
attributed to the same biomolecule, i.e. 
somatotropin.  Krauthammer et al (2000), who 
are arguably the only researchers who have 
attempted to do actual identification as we 
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 14-20.
                         Proceedings of the Workshop on Natural Language Processing in
define it, have noted that while it is possible to 
recognize gene names in the face of variability, 
it remains difficult to map the recognized names 
to their canonical referents.  They point out that 
heuristics might be helpful in doing this. 
We studied variability in gene names with 
an eye toward finding such heuristics.  Our goal 
was to differentiate between kinds of variability 
that tend to differentiate between names with 
different referents, e.g. aha vs. aho or ACE1 vs. 
ACE2, as opposed to kinds of variability that 
only differentiate between synonyms that share 
a referent, such as tumour protein homologue 
and tumor protein homolog or ACE and ACE1. 
We then use data on contrast and variability to 
suggest our heuristics.  The idea behind using 
such heuristics is that an identified entity in 
some text that differs minimally from the 
canonical name for some entity can be mapped 
to that canonically-labelled entity if such 
mapping is allowed by some heuristic. 
We use the term contrast to describe or refer 
to dimensions or features which can be used to 
distinguish between two samples of natural 
language with different meaning.  Issues of 
contrast versus variability can be discussed with 
reference to individual characters or sequences 
of characters, or with reference to more abstract 
features, such as orthographic case.  In the 
molecular biology domain, we will say that 
some feature is contrastive if it encodes the 
difference between the names of two different 
genes.  In other words, contrasts occur inter-
entity.  We will say that some feature is 
(noncontrastively) variable if it differs merely 
between synonyms; in other words, variability 
occurs within members of a synonym set. 
We can trivially identify the contrast 
between BRCA1 vs. MHC class I polypeptide-
related sequence C, or between sonic hedgehog 
vs. eyeless.  What we are really interested in is 
minimally different tuples?sets that differ with 
respect to only one feature.  For instance, we 
would want to look at BRCA1 and BRCA2, 
which differ with respect to whether the 
character at the right edge is 1 or 2, or estrogen 
receptor beta and oestrogen receptor beta, 
which differ with respect to the presence or 
absence of an o at the left edge.  Ideally, then, 
we are looking for sets of names that differ only 
by a single unit.  However, the size and scope of 
the unit needs further discussion.  When dealing 
with written language, the unit of concern will 
usually be the grapheme.  A grapheme may be 
as small as a single character, but may also be 
considerably longer, e.g. the sequence ough in 
dough or through.  In this study, we considered 
graphemes longer than a single character only in 
the case of vowels.  Sometimes we will want to 
consider tuples that differ with respect to strings 
that are considerably larger than a grapheme, 
such as a word, or a string of parenthesized 
material; this will be discussed further in the 
first Methods section.  The issue of tuple size 
will be discussed there, as well. 
2 
2.1 Corpus 
Methods I: Investigating 
dimensions of contrast and 
variability 
We examined a large corpus of gene names and 
of synonyms for those gene names to determine 
what sorts of features are contrastive in gene 
names, and what sorts of features can vary 
without affecting the referential status of a gene 
name.  The corpus was derived from the 
LocusLink LL_tmpl file (the version on the 
LocusLink download site at 2:32 p.m. on Sept. 
13, 2001), available by ftp from 
ftp://ncbi.nlm.nih.gov.  This is an 
easily readable dump of LocusLink, which 
?provides a single query interface to curated 
sequence and descriptive information about 
genetic loci.  It presents information on official 
nomenclature, aliases, sequence accessions, 
phenotypes, EC numbers, MIM numbers, 
UniGene clusters, homology, map locations, and 
related web sites? 
(www.ncbi.nlm.nih.gov/locuslink).
We then pulled out the names and synonyms for 
all LocusLink entries for the species Mus 
musculus, Rattus norvegicus, and Homo 
sapiens.  We took the fields labelled as 
OFFICIAL GENE NAME, PREFERRED 
GENE NAME, OFFICIAL SYMBOL, 
PREFERRED SYMBOL, PRODUCT, 
PREFERRED PRODUCT, ALIAS SYMBOL, 
and ALIAS PROT.  Some genes were unnamed, 
and these were excluded from the analysis.  To 
our surprise, we also found that some gene 
names were duplicated within the same genome-
-e.g., in the M. musculus genome, there are two 
genes named reciprocal translocation, Ch4 6 
and 7, Adler 17; we filtered out duplicate names 
and excluded them, as well.  This left 42,608 
genes for the mouse, 4457 for the rat, and 
25,915 for the human.  For each organism, we 
created one file containing just gene names, and 
for each organism we created a set of files 
containing all gene names and their synonyms.  
For the gene name file, we used just those fields 
labelled OFFICIAL GENE NAME or 
PREFERRED GENE NAME; for the combined 
name/synonym files, we used all of the fields 
given above. 
2.2 
2.3 
2.4 
2.5 
Finding contrasts in the corpus 
For each species, we pulled out a list of all 
names that were indicated as OFFICIAL GENE 
NAME or PREFERRED GENE NAME in the 
LL_tmpl file.  Each name in this file represents 
a different gene.  We examined the names in this 
single large file for contrastive differences. 
Finding noncontrastive variability 
in the corpus 
For each species, for each gene, we pulled out 
the list of all names that were indicated by any 
of the set of labels listed above, and stored them 
separately.  With each of the many resulting 
files (one per gene), we examined the small set 
of synonymous names for noncontrastive 
variability. 
Finding minimal tuples 
The most obvious way to find minimal tuples 
would be to first determine the minimum edit 
distance between all pairs of gene names, and 
then select all pairs with minimum edit distance 
below some cutoff value.  However, this 
approach would suffer from two obvious flaws.  
The first flaw is that it is computationally 
expensive, since it is a O(n2)-complex problem.  
The second flaw is that it is ineffective.  It only 
yields tuples of size 2, but in fact sets of 
minimally differing gene names occur in sets of 
size 3, 4, 5, and even considerably larger, e.g. 
the three-member set conserved sequence block 
I, conserved sequence block II, and conserved 
sequence block III.  We chose an alternative 
approach to the problem of finding minimal 
tuples.  It consists of the following steps: 
 
For each gene name 
 transform the gene name to some reduced 
form 
 using the reduced form as the key in a hash 
of keys  lists, add the full form to a list of 
full forms from which that reduced form 
was derived 
 
For each key in the hash 
 retrieve the list of names that is mapped to 
by that key 
 if the list of names pointed to by that key 
has more than one element, report the list 
 
For example, if the input is the list of gene 
names gamma-glutamyltransferase 1, gamma-
glutamyltransferase 2, gamma-
glutamyltransferase 3, matrix metalloproteinase 
23A, matrix metalloproteinase 23B, and acrosin, 
and the transformation that is being applied to 
each name consists of deletion of the last 
character, then the output will be two lists of > 1 
element pointed to by gamma-
glutamyltransferase and matrix 
metalloproteinase 23, and one list of a single 
element, acrosin.  The two lists with > 1 
element would be reported as minimal tuples. 
Transformations 
We applied four transformations designed to 
investigate syntagmatic, or positional, effects.  
These consisted of removing the first character, 
the first word, the last character, and the last 
word. 
 
We applied four transformations designed to 
investigate paradigmatic, or content-based, 
effects.  These consisted of mapping vowel 
sequences to a constant string (the purpose of 
this being to look at American vs. British 
dialectal differences in gene names); 
replacement of hyphens with spaces; removal of 
parenthesized material; and normalization of 
case.  These relatively simple transformations 
miss a number of categories of differences 
between gene names, e.g. single-character 
differences in non-edge positions, such as 0 
BETA-1 GLOBIN vs. 0 BETA-2 GLOBIN; 
single-word differences in non-edge positions, 
such as DOPAMINE D1A RECEPTOR vs. 
DOPAMINE D2 RECEPTOR; proper substring 
relationships, such as EYE vs. EYE2; and 
interactions between the features that we did 
examine, such as calsequestrin 1 (fast-twitch, 
skeletal muscle) vs. calsequestrin 2 (cardiac 
muscle), which is not found by any one heuristic 
but would be found by the combination of the 
parenthesized-material transformation followed 
by either of the right-edge transformations.  
Nonetheless, they seem like a reasonable 
starting point. 
Results I 3 
Table 1 and Graphs 1, 2, 3, and 4 summarize our 
findings on contrast and variability in gene 
names. 
 
One surprising finding was that every 
paradigmatic dimension of contrast that we 
examined turned out to be contrastive in at least 
some very small number of cases.  We did not 
expect hyphenation to ever be contrastive, but 
found that within the H. sapiens genome, the 
two genes at LocusLink ID's 51086 and 112858 
differ in just that feature, having the names 
putative protein-tyrosine kinase and putative 
protein tyrosine kinase, respectively.  The two 
genes at LocusLink ID's 51251 and 90859 differ 
in the same way, being named uridine 5'-
monophosphate hydrolase 1 and uridine 5' 
monophosphate hydrolase 1, respectively. 
 
Graph 1. Hyphenation: contrast and 
variability
0
78
0 04
100
0
50
100
150
Contrastive Synonymous
Na
me
s mouse
rat
human
 
Graph 2. Case: contrast and 
variability
1
327
3
848
0
452
0
100
200
300
400
500
600
700
800
900
Contrastive Synonymous
Na
me
s mouse
rat
human
 
DOC S Contrastive 
(i.e., names) 
%N Variable 
(i.e., 
synonyms) 
 
LMC M 4 .009 72
 R 4 0.090 801
 H 2 0.008 123
LMW M 2556 5.999 135
 R 836 18.757 759
 H 4013 15.485 258
RMC M 15540 36.472 360
 R 687 15.414 49
 H 8684 33.510 921
RMW M 22290 52.314 191
 R 940 21.090 25
 H 11627 44.866 675
VS M 7 0.016 37
 R 0 0.000 4
 H 4 0.015 30
HYPH M 0 0.000 78
 R 0 0.000 0
 H 4 0.015 100
CASE M 1 0.002 327
 R 3 0.067 848
 H 0 0.000 452
PM M 14 0.033 102
 R 13 0.292 25
 H 51 0.197 526
 
Table 1.  Contrastive and noncontrastive 
variability in gene names.  ?Percentage? 
columns give percentage of total names 
considered for that species, rounded to three 
decimal places.  DOC = dimension of 
contrast, L/RMC = left/right-most char, 
L/RMW = left/right-most word, VS = vowel 
sequences, HYPH = hyphenation, CASE = 
case, PM = parenthesized material.  S = 
species, M = mouse, R = rat, H = human.  %N 
= contrastive names as percentage of total 
names for that species. 
 
 
We did not expect case ever to be contrastive, 
but found that within the R. norvegicus genome, 
the two genes at LocusLink ID's 24969 and 
83789 differ with respect to just that feature, 
having the names Ribosomal protein S2 and 
ribosomal protein S2, respectively.  The two 
genes at LocusLink ID's 56764 and 65028 differ 
in the same way, having the names dnaj-like 
protein and DnaJ-like protein.  As Graphs 1 and 
2 show, these contrasts were not common, but 
we were surprised to observe them at all. 
 
(In considering these findings, it should be 
noted that these results are specific to a 
particular version of LocusLink.  We were 
interested in the extent to which these 
unexpected minimal pairs might be erroneous, 
so we examined the corresponding LOCUSID?s 
in a subsequent revision of the file from several 
months later (May 1, 2002, 10:21 a.m.).  We 
found that some of these entries had been 
combined, and some had been assigned an 
OFFICIAL_GENE_NAME, but others were 
unchanged, and so while we cannot eliminate 
the possibility that they are in error and have just 
managed to elude the editing process thus far, it 
is certainly the case that these anomalous 
contrasts continue to exist in the database, and 
we have no reason to assume that such names 
will not continue to be entered into the database, 
erroneously or otherwise, and therefore it 
behooves us to consider their implications for 
entity identification.) 
 
Graph 3. Parenthesized material: 
contrast and variability
14
102
13 25
51
526
0
100
200
300
400
500
600
Contrastive Synonymous
Na
me
s mouse
rat
human
      We found marked edge effects.  Contrasts 
are much more likely to be marked at the name 
boundary than are noncontrastive differences.  
There is a marked asymmetry in the 
directionality of the location of contrastive 
differences: they are much more likely to appear 
at the right edge of the word than at the left edge 
of the word.  There are also marked intra-species 
differences.  For example, although large edge 
effects are obvious for names (as opposed to 
synonyms) in the mouse and human genomes, 
they are not in the rat genome.  In interpreting 
variability, it will likely be helpful to have some 
awareness of what species is being discussed. 
Graph 4. Edge effects: 
contrastive on left, 
synonymous on right
0
5000
10000
15000
20000
25000
Lc
ha
r
Lw
ord
Rc
ha
r
Rw
ord
Lc
ha
r
Lw
ord
Rc
ha
r
Rw
ord
Na
me
s mouse
rat
human
4 Methods II: Testing the 
heuristics 
These findings suggested a set of heuristics for 
allowing weakened pattern matches on gene 
names.  The heuristics are stated as 
transformations applied to regular expressions 
representing gene names to generate new regular 
expressions for the same gene names, but the 
heuristics can be applied in other ways as well, 
e.g. by grammar-based generation of alternate 
forms.  The heuristics are listed below: 
 
1.  Equivalence of vowel sequences: for any 
regular expression representing a gene name, 
substitute the regular expression formed by 
replacing all vowel sequences with one or more 
of any vowel. 
 
2.  Optionality of hyphens: for any regular 
expression representing a gene name, substitute 
the regular expression formed by replacing 
every hyphen with the disjunction of a hyphen 
or a space. 
 
3.  Optionality of parenthesized material: for any 
regular expression representing a gene name, 
substitute the regular expression formed by 
making any paired parentheses and the material 
they enclose (and surrounding whitespace, as 
appropriate) optional. 
 
4.  Case insensitivity: for any regular expression, 
apply it case-insensitively. 
 
To evaluate the extent to which each of 
these heuristics led to increased entity 
recognition, we ran our heuristics against a large 
body of Medline abstracts.  We counted the 
number of entities that were found by an exact 
pattern match to a LocusLink name, and counted 
the number of additional names that were found 
by each heuristic.  Although none of our 
heuristics specifically addressed 
morphologically-induced variability, we also 
added a search for pluralized gene names, so 
that we could compare the extent to which 
recognition of plurals improved recall to the 
extent to which our heuristics improved recall.  
5 Results II 
Table 2 shows the results.  As intuition would 
suggest, all heuristics were effective in locating 
more names than strict pattern matches alone.  
For example, optional hyphenation heuristic 
allowed the official gene name alpha-2-
macroglobulin to find a match in Moreover, C5a 
also enhanced transcription of the gene for the 
type-2 acute phase protein alpha 2-
macroglobulin n HC indirectly by increasing 
LPS-dependent IL-6 release from KC. 
  
names located by strict pattern matching 1846
Additional names located by vowel 
sequence heuristic matches 
586
Additional names located by optional 
hyphen heuristic matches 
37
Additional names located by case 
insensitive heuristic matches 
864
Additional names located by optional 
parentheses heuristic matches 
432
Additional names located by plural 
matches 
87
 
Table 2.  Names found by strict pattern 
match, heuristics, and plurals. 
 
However, we were concerned about the 
possibility of poor precision, i.e. false-positives.  
For this reason, we ran our heuristics against the 
same body of Medline abstracts, then randomly 
selected up to 100 tokens of gene names 
suggested by each heuristic  (some found less 
than 100 tokens in our corpus--see Table 2 
above).  We labelled each putative gene name 
with the canonical gene name that we believed it 
to refer to, and then asked a subject matter 
expert to evaluate whether the gene names that 
we had identified were or were not the gene 
names that we believed them to be.  The expert 
was presented with a three-way forced-choice 
paradigm, the options being yes, no, and can't 
tell.  It seemed useful to be able to compare the 
precision of our technique with the incidence of 
false positives from strict pattern matches, so the 
expert was also presented with a number of 
strict matches (i.e., not identified by our 
heuristics) to evaluate, in a quantity roughly 
equivalent to the number of heuristically-
suggested names that they were asked to 
evaluate.  Table 3 shows the results. 
 
condition total 
tokens 
marked 
yes 
marked 
no 
marked 
?can?t 
tell? 
percentage 
false 
positive 
((can?t tell 
+ no)  
total 
tokens) x 
100 
Vowel 
seq. 
100 15 81 4 85.0
Hyph. 37 34 0 3 8.1
Case 
insens. 
97 72 20 5 25.8
Paren. 
Material 
99 93 0 6 6.1
   
Plurals 86 75 8 3 12.8
Strict 
pattern 
match 
500 425 40 35 15.0
 
Table 3.  False positives. 
 
We note the following: 
 
1.  Even strict pattern matches and forms that 
vary only with respect to inflectional 
morphology (i.e., the plurals) yield a nontrivial 
percentage of false positives?a percentage 
which is actually higher than two of our 
heuristics (optionality of hyphenation and 
optionality of parenthesized material). 
 
2.  Two of our heuristics (equivalence of vowel 
sequences and case insensitivity) yielded 
unexpectedly high rates of false positives.  The 
vowel sequence heuristic can probably be made 
to yield a lower rate by fine-tuning it.  For 
example, false positives from this heuristic can 
be reduced by disregarding any weak matches 
that come from one or more name-final upper-
case I?s, since these are commonly used in gene 
names to form Roman numerals.  The high false 
positive rate of the case insensitivity heuristic is 
unexpected, and will be investigated further.  
6 Conclusions 
Entity identification is a difficult task whose 
success is partly dependent on performance in 
other tasks, including disambiguation and 
information retrieval.  Disambiguation of the 
actual referent of an apparent gene or protein 
name is even more important than one might 
expect.  Hatzivassiloglou et al (2001) points out 
the benefits and the difficulties inherent in 
distinguishing between genes, proteins, and 
RNA; we found that it was also important to 
differentiate between genes, proteins, RNA, and 
receptors, promoters, antagonists, domains, and 
binding sites, as well as diseases, syndromes, 
conditions, phenotypes, and mutants, as all of 
these were noted by our subject-matter expert as 
sources of false positives.  Good information 
retrieval is clearly also a prerequisite for high-
precision entity identification.  In some cases, 
false positives arose when (abstracts of) 
irrelevant documents were used as input. 
Heuristics can be useful tools for increasing 
recall in entity identification, as well as for 
helping us ensure that we are performing true 
entity identification, as opposed to entity 
location.  Tanabe and Wilbur (in press) point out 
the value of combining knowledge sources in 
the entity identification task; our heuristics seem 
especially promising in part because they are 
based on a combination of two sources: (1) the 
expertise of NLP application developers about 
the sorts of variability that need to be dealt with 
in NLP systems (e.g. in text normalization), and 
(2) on empirical data about variability in the 
names themselves.  Future work should 
concentrate on three areas.  The first is 
extending our study of variability to include 
other dimensions of contrast, such as the ones 
that we point out that our study ignored, so that 
we can increase the inventory of heuristics.  The 
second is integrating our heuristics with a 
system that identifies weak matches with gene 
names, i.e. candidates for application of the 
heuristics.  The third is elucidating the place of 
orthographic variability within all causes of 
pattern match failure.  
References 
Hatzivassiloglou, Vasileios; Pablo A. 
Duboue; and Andrey Rzhetsky (2001). 
Disambiguating proteins, genes, and RNA in 
text: a machine learning approach.  
Bioinformatics 17, Suppl. 1: S97-S106. 
 
Krauthammer, Michael; Andrey Rzhetsky; 
Pavel Morozov; and Carol Friedman (2000).  
Using BLAST for identifying gene and protein 
names in journal articles.  Gene 259:245-252. 
 
Tanabe, Lorraine and W. John Wilbur (in 
press).  Tagging gene and protein names in 
biomedical text.  Bioinformatics. 
A resource for constructing customized test suites 
for molecular biology entity identification systems 
 
K. Bretonnel Cohen 
Center for Computational Pharmacology, 
University of Colorado School of Medicine 
kevin.cohen@uchsc.edu 
Lorraine Tanabe 
National Center for Biotechnology 
Information, NLM, NIH 
tanabe@ncbi.nlm.nih.gov 
Shuhei Kinoshita 
Fujitsu Ltd. Bio-IT Lab, and Center for 
Computational Pharmacology, University of 
Colorado School of Medicine 
shuhei.kinoshita@uchsc.edu 
Lawrence Hunter 
Center for Computational Pharmacology, 
University of Colorado School of Medicine 
larry.hunter@uchsc.edu 
 
Abstract 
This paper describes a data source and 
methodology for producing customized test 
suites for molecular biology entity 
identification systems.  The data consists of: 
(a) a set of gene names and symbols classified 
by a taxonomy of features that are relevant to 
the performance of entity identification 
systems, and (b) a set of sentential 
environments into which names and symbols 
are inserted to create test data and the 
associated gold standard.  We illustrate the 
utility of test sets producible by this 
methodology by applying it to five entity 
identification systems and describing the error 
patterns uncovered by it, and investigate 
relationships between performance on a 
customized test suite generated from this data 
and the performance of a system on two 
corpora. 
 
1 Introduction 
This paper describes a methodology and data for the 
testing of molecular biology entity identification (EI) 
systems by developers and end users.  Molecular 
biology EI systems find names of genes and gene 
products in free text.  Several years? publication history 
has established precision, recall, and F-score as the de 
facto standards for evaluating EI systems for molecular 
biology texts at the publication stage and in 
competitions like BioCreative 
(www.mitre.org/public/biocreative).  These measures 
provide important indices of a system?s overall output 
quality.  What they do not provide is the detailed sort of 
information about system performance that is useful for 
the system developer who is attempting to assess the 
strengths and weaknesses of a work in progress, nor do 
they provide detailed information to the potential 
consumer who would like to compare two systems 
against each other. Hirschman and Mani (2003) point 
out that different evaluation methods are useful at 
different points in the software life-cycle.  In particular, 
what they refer to as feature-based evaluation via test 
suites is useful at two points: in the development phase, 
and for acceptance testing.  We describe here a 
methodology and a set of data for constructing 
customized feature-based test suites for EI in the 
molecular biology domain.  The data consists of two 
sets.  One is a set of names and symbols of entities as 
that term is most commonly understood in the 
molecular biology domain?genes and gene products.  
(Sophisticated ontologies such as GENIA (Ohta et al 
2002) include other kinds of entities relevant to 
molecular biology as well, such as cell lines.)  The 
names and symbols exemplify a wide range of the 
features that characterize entities in this domain?case 
variation, presence or absence of numbers, presence or 
absence of hyphenation, etc. The other is a set of 
sentences that exemplify a range of sentential contexts 
in which the entities can appear, varying with respect to 
position of the entity in the sentence (initial, medial, or 
final), presence of keywords like gene and protein, 
tokenization issues, etc.  Both the entities and the 
sentential contexts are classified in terms of a taxonomy 
of features that are relevant to this domain in particular 
and to natural language processing and EI in general.  
The methodology consists of generating customized test 
suites that address specific performance issues by 
combining sets of entities that have particular 
characteristics with sets of contexts that have particular 
characteristics.  Logical combination of subsets of 
characteristics of entities and contexts allows the 
developer to assess the effect of specific characteristics 
on performance, and allows the user to assess 
performance of the system on types of inputs that are of 
                                            Association for Computational Linguistics.
                     Linking Biological Literature, Ontologies and Databases, pp. 1-8.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
particular interest to them.  For example, if the 
developer or end-user wants to assess the ability of a 
system to recognize gene symbols with a particular 
combination of letter case, hyphenation, and presence 
or absence of numerals, the data and associated code 
that we provide can be used to generate a test suite 
consisting of symbols with and without that 
combination of features in a variety of sentential 
contexts. 
Inspiration for this work comes on the one hand 
from standard principles of software engineering and 
software testing, and on the other hand from descriptive 
linguistics (Harris 1951, Samarin 1967).  In Hirschman 
and Mani?s taxonomy of evaluation techniques, our 
methodology is referred to as feature-based, in that it is 
based on the principle of classifying the inputs to the 
system in terms of some set of features that are relevant 
to the application of interest.  It is designed to provide 
the developer or user with detailed information about 
the performance of her EI system.  We apply it to five 
molecular biology EI and information extraction 
systems: ABGene (Tanabe and Wilbur 2002a, Tanabe 
and Wilbur 2002b); KeX/PROPER (Fukuda et al 
1997); Yapex (Franz?n et al 2002); the stochastic POS 
tagging-based system described in Cohen et al (in 
submission); and the entity identification component of 
Ono et al?s information extraction system (Ono et al 
2001), and show how it gives detailed useful 
information about each that is not apparent from the 
standard metrics and that is not documented in the cited 
publications.  (Since we are not interested in punishing 
system developers for graciously making their work 
available by pointing out their flaws, we do not refer to 
the various systems by name in the remainder of this 
paper.)   
Software testing techniques can be grouped into 
structured (Beizer 1990), heuristic (Kaner et al 2002), 
and random categories.  Testing an EI system by 
running it on a corpus of texts and calculating precision, 
recall, and F-score for the results falls into the category 
of random testing.  Random testing is a powerful 
technique, in that it is successful in finding bugs.  When 
done for the purpose of evaluation, as distinct from 
testing (see Hirschman and Thompson 1997 for the 
distinction between the two, referred to there as 
performance evaluation and diagnostic evaluation), it 
also is widely accepted as the relevant index of 
performance for publication.  However, its output lacks 
important information that is useful to a system 
developer (or consumer): it tells you how often the 
system failed, but not what it failed at; it tells you how 
often the system succeeds, but not where its strengths 
are.   
For the developer or the user, a structured test suite 
offers a number of advantages in answering these sorts 
of questions.  The utility of such test suites in general 
software testing is well-accepted.  Oepen et al (1998) 
lists a number of advantages of test suites vs. 
naturalistic corpora for testing natural language 
processing software in particular: 
? Control over test data: test suites allow for 
?focussed and fine-grained diagnosis of system 
performance? (15).  This is important to the 
developer who wants to know exactly what 
problems need to be fixed to improve performance, 
and to the end user who wants to know that 
performance is adequate on exactly the data that 
they are interested in. 
? Systematic coverage: test suites can allow for 
systematic evaluation of variations in a particular 
feature of interest.  For example, the developer 
might want to evaluate how performance varies as 
a function of name length, or case, or the presence 
or absence of hyphenation within gene symbols.  
The alternative to using a structured test suite is to 
use a corpus, and then search through it for the 
relevant inputs and hope that they are actually 
attested. 
? Control of redundancy: while redundancy in a 
corpus is representative of actual redundancy in 
inputs, test suites allow for reduction of 
redundancy when it obscures the situation, or for 
increasing it when it is important to test handling of 
a feature whose importance is greater than its 
frequency in naturally occurring data.  For 
example, names of genes that are similar to names 
of inherited diseases might make up only a small 
proportion of the gene names that occur in PubMed 
abstracts, but the user whose interests lie in 
curating OMIM might want to be able to assure 
herself that coverage of such names is adequate, 
beyond the level to which corpus data will allow.   
? Inclusion of negative data: in the molecular 
biology domain, a test suite can allow for 
systematic evaluation of potential false positives.   
? Coherent annotation: even the richest metadata is 
rarely adequate or exactly appropriate for exactly 
the questions that one wants to ask of a corpus.  
Generation of structured, feature-based test suites 
obviates the necessity for searching through 
corpora for the entities and contexts of interest, and 
allows instead the structuring of contexts and 
labeling of examples that is most useful to the 
developer. 
The goal of this paper is to describe a methodology and 
publicly available data set for constructing customized 
and refinable test suites in the molecular biology 
domain quickly and easily.  A crucial difference 
between similar work that simply documents a 
distributable test suite (e.g. Oepen (1998) and Volk 
(1998)) and the work reported in this paper is that we 
are distributing not a static test suite, but rather data for 
generating test suites?data that is structured and 
classified in such a way as to allow software developers 
and end users to easily generate test suites that are 
customized to their own assessment needs and 
development questions.  We build this methodology 
and data on basic principles of software engineering 
and of linguistic analysis.  The first such principle 
involves making use of the software testing notion of 
the catalogue.   
A catalogue is a list of test conditions, or qualities 
of particular test inputs (Marick 1997).  It corresponds 
to the features of feature-based testing, discussed in 
Hirschman and Mani (2003) and to the schedule 
(Samarin 1967:108-112) of descriptive linguistic 
technique.  For instance, a catalogue of test conditions 
for numbers might include: 
? zero, non-zero, real, integer 
? positive, negative, unsigned 
? the smallest number representable in some data 
type, language, or operating system; smaller than 
the smallest number representable 
? the largest number representable; larger than the 
largest number representable  
Note that the catalogue includes both ?clean? 
conditions and ?dirty? ones.  This approach to software 
testing has been highly successful, and indeed the best-
selling book on software testing (Kaner et al 1999) can 
fairly be described as a collection of catalogues of 
various types. 
The contributions of descriptive linguistics include 
guiding our thinking about what the relevant features, 
conditions, or categories are for our domain of interest.  
In this domain, that will include the questions of what 
features may occur in names and what features may 
occur in sentences?particularly features in the one that 
might interact with features in the other.  Descriptive 
linguistic methodology is described in detail in e.g. 
Harris (1951) and Samarin (1967); in the interests of 
brevity, we focus on the software engineering 
perspective here, but the thought process is very 
similar.  The software engineering equivalent of the 
descriptive linguist?s hypothesis is the fault model 
(Binder 1999)?an explicit hypothesis about a potential 
source of error based on ?relationships and components 
of the system under test? (p. 1088).  For instance, 
knowing that some EI systems make use of POS tag 
information, we might hypothesize that the presence of 
some parts of speech within a gene name might be 
mistaken for term boundaries (e.g. the of in bag of 
marbles, LocusID 43038).  Catalogues are used to 
develop a set of test cases that satisfies the various 
qualities.  (They can also be used post-hoc to group the 
inputs in a random test bed into equivalence classes, 
although a strong motivation for using them in the first 
place is to obviate this sort of search-based post-hoc 
analysis.)  The size of the space of all possible test 
cases can be estimated from the Cartesian product of all 
catalogues; the art of software testing (and linguistic 
fieldwork) consisting, then, of selecting the highest-
yielding subset of this often enormous space that can be 
run and evaluated in the time available for testing.   
At least three kinds of catalogues are relevant to 
testing an EI system.  They fall into one of two very 
broad categories: syntagmatic, having to do with 
combinatory properties, and paradigmatic, having to do 
with varieties of content.  The three kinds of catalogues 
are: 
1. A catalogue of environments in which gene names 
can appear.  This is syntagmatic. 
2. A catalogue of types of gene names.  This is 
paradigmatic. 
3. A catalogue of false positives.  This is both 
syntagmatic and paradigmatic. 
The catalogue of environments would include, for 
example, elements related to sentence position, such as 
sentence-initial, sentence-medial, and sentence-final; 
elements related to list position, such as a single gene 
name, a name in a comma-separated list, or a name in a 
conjoined noun phrase; and elements related to 
typographic context, such as location within 
parentheses (or not), having attached punctuation (e.g. a 
sentence-final period) (or not), etc.  The catalogue of 
types of names would include, for example, names that 
are common English words (or not); names that are 
words versus ?names? that are symbols; single-word 
versus multi-word names; and so on.  The second 
category also includes typographic features of gene 
names, e.g. containing numbers (or not), consisting of 
all caps (or not), etc.  We determined candidate features 
for inclusion in the catalogues through standard 
structuralist techniques such as examining public-
domain databases containing information about genes, 
including FlyBase, LocusLink, and HUGO, and by 
examining corpora of scientific writing about genes, 
and also by the software engineering techniques of 
?common sense, experience, suspicion, analysis, [and] 
experiment? (Binder 1999).  The catalogues then 
suggested the features by which we classified and 
varied the entities and sentences in the data.   
General format of the data 
The entities and sentences are distributed in XML 
format and are available at a supplemental web site 
(compbio.uchsc.edu/Hunter_lab/testing_ei).  A plain-
text version is also available.  A representative entity is 
illustrated in Figure 1 below, and a representative 
sentence is illustrated in Figure 2.  All data in the 
current version is restricted to the ASCII character set.   
Test suite generation   
Data sets are produced by selecting sets of entity 
features and sets of sentential context features and 
inserting the entities into slots in the sentences.    This 
can be accomplished with the user?s own tools, or using 
applications available at the supplemental web site.  
The provided applications produce two files: a file 
containing raw data for use as test inputs, and a file 
containing the corresponding gold standard data marked 
up in an SGML-like format.  For example, if the raw 
data file contains the sentence ACOX2 polymorphisms 
may be correlated with an increased risk of larynx 
cancer, then the gold standard file will contain the 
corresponding sentence <gp>ACOX2</gp> 
polymorphisms may be correlated with an increased 
risk of larynx cancer.  Not all users will necessarily 
agree on what counts as the ?right? gold standard?see 
Olsson et al (2002) and the BioCreative site for some 
of the issues.  Users can enforce their own notions of 
correctness by using our data as input to their own 
generation code, or by post-processing the output of our 
applications.   
 
ID: 136 
name_vs_symbol: n 
length: 3 
case: a 
contains_a_numeral: y 
contains_Arabic_numeral: y 
Arabic_numeral_position: f 
contains_Roman_numeral: 
<several typographic features omitted> 
contains_punctuation: 1 
contains_hyphen: 1 
contains_forward_slash: 
<several punctuation-related features omitted> 
contains_function_word: 
function_word_position: 
contains_past_participle: 1 
past_participle_position: i 
contains_present_participle: 
present_participle_position: 
source_authority: HGNC ID: 2681 "Approved Gene 
Name" field 
original_form_in_source: death-associated 
protein 6 
data: death-associated protein 6 
Figure 1  A representative entry from the entity data 
file.  A number of null-valued features are omitted for 
brevity?see the full entry at the supplemental web site.  
The data field (last line of the figure) is what is output 
by the generation software. 
ID: 25 
type: tp 
total_number_of_names: 1 
list_context:  
position: I 
typographic_context:  
appositive:  
source_id: PMID: 14702106 
source_type: title 
original_form_in_source: Stat-3 is required 
for pulmonary homeostasis during hyperoxia. 
slots: <> is required for pulmonary 
homeostasis during hyperoxia.    
 
Figure 2  A representative entry from the sentences 
file.  Features and values are explained in section 2.2 
Feature set for sentential contexts below.   The slots 
field (last line of the figure) shows where an entity 
would be inserted when generating test data.   
2   The taxonomy of features for entities 
and sentential contexts 
In this section we describe the feature sets for entities 
and sentences, and motivate the inclusion of each, 
where not obvious.   
2.1   Feature set for entities 
Conceptually, the features for describing name-inputs 
are separated into four categories: 
orthographic/typographic, morphosyntactic, source, and 
lexical.   
? Orthographic/typographic features describe the 
presence or absence of features on the level of 
individual characters, for example the case of letters, 
the presence or absence of punctuation marks, and the 
presence or absence of numerals.   
? Morphosyntactic features describe the presence or 
absence of features on the level of the morpheme or 
word, such as the presence or absence of participles, 
the presence or absence of genitives, and the presence 
or absence of function words.   
? Source features are defined with reference to the 
source of an input.  (It should be noted that in software 
engineering, as in Chomskyan theoretical linguistics, 
data need not be naturally-occurring to be useful; 
however, with the wealth of data available for gene 
names, there is no reason not to include naturalistic 
data, and knowing its source may be useful, e.g. in 
evaluating performance on FlyBase names, etc.)  
Source features include source type, e.g. literature, 
database, or invention; identifiers in a database; 
canonical form of the entity in the database; etc. 
? Lexical features are defined with respect to the 
relationship between an input and some outside source 
of lexical information, for instance whether or not an 
input is or contains a common English word.  This is 
also the place to indicate whether or not an input is 
present in a resource such as LocusLink, whether or not 
it is on a particular stoplist, whether it is in-vocabulary 
or out-of-vocabulary for a particular language model, 
etc. 
The distinction between these three broad 
categories of features is not always clear-cut.  For 
example, presence of numerals is an 
orthographic/typographic feature, and is also 
morphosyntactic when the numeral postmodifies a 
noun, e.g. in heat shock protein 60.  Likewise, features 
may be redundant?for example, the presence of a 
Greek letter in the square-bracket- or curly-bracket-
enclosed formats, or the presence of an apostrophized 
genitive, are not independent of the presence of the 
associated punctuation marks.  However, Boolean 
queries over the separate feature sets let them be 
manipulated and queried independently. So, entities 
with names like A' can be selected independently of 
names like Parkinson?s disease.   
2.1.1   Orthographic/typographic features 
Length:  Length is defined in characters for 
symbols and in whitespace-tokenized words for names.   
Case: This feature is defined in terms of five 
possible values: all-upper-case, all-lower-case, upper-
case-initial-only, each-word-upper-case-initial (e.g. 
Pray For Elves), and mixed.  The fault model 
motivating this feature hypothesizes that taggers may 
rely on case to recognize entities and may fail on some 
combinations of cases with particular sentential 
positions.  For example, one system performed well on 
gene symbols in general, except when the symbols are 
lower-case-initial and in sentence-initial position (e.g. 
p100 is abundantly expressed in liver? (PMID 
1722209) and bif displays strong genetic interaction 
with msn (PMID 12467587).    
Numeral-related features: A set of features 
encodes whether or not an entity contains a numeral, 
whether the numeral is Arabic or Roman, and the 
positions of numerals within the entity (initial, medial, 
or final).  The motivation for this feature is the 
hypothesis that a system might be sensitive to the 
presence or absence of numerals in entities.  One 
system failed when the entity was a name (vs. a 
symbol), it contained a number, and the number was in 
the right-most (vs. a medial) position in a word.  It 
correctly tagged entities like glucose 6 phosphate 
dehydrogenase but missed the boundary on 
<gp>alcohol dehydrogenase</gp> 6. This pattern was 
specific to numbers?letters in the same position are 
handled correctly.  
Punctuation-related features: A set of features 
includes whether an entity contains any punctuation, the 
count of punctuation marks, and which marks they are 
(hyphen, apostrophe, etc.).  One system failed to 
recognize names (but typically not symbols) when they 
included hyphens.  Another system had a very reliable 
pattern of failure involving apostrophes just in case they 
were in genitives. 
Greek-letter-related features:  These features 
encode whether or not an entity contains a Greek letter, 
the position of the letter, and the format of the letter.  
(This feature is an example of an orthographic feature 
which may be defined on a substring longer than a 
character, e.g. beta.)  Two systems had problems 
recognizing gene names when they contained Greek 
letters in the PubMed Central format, i.e. [beta]1 
integrin.   
2.1.2   Morphosyntactic features 
The most salient morphosyntactic feature is whether an 
entity is a name or a symbol.  The fault model 
motivating this feature suggests that a system might 
perform differently depending on whether an input is a 
name or a symbol.  The most extreme case of a system 
being sensitive to this feature was one system that 
performed very well on symbols but recognized no 
names whatsoever.   
Features related to function words: a set of 
features encodes whether or not an entity contains a 
function word, the number of function words in the 
entity, and their positions?for instance, the facts: that 
scott of the antarctic (FlyBase ID FBgn0015538) 
contains two function words; that they are of and the; 
and that they are medial to the string.  This feature is 
motivated by two fault models.  One posits that a 
system might apply a stoplist to its input and that 
processing of function words might therefore halt at an 
early stage.  The other posits that a system might 
employ shallow parsing to find boundaries of entities 
and that the shallow parser might insert boundaries at 
the locations of function words, causing some words to 
be omitted from the entity.  One system always had 
partial hits on names that were multi-word unless each 
word in it was upper-case-initial, or there was an 
alphanumeric postmodifier (i.e. a numeral, upper-cased 
singleton letter, or Greek letter) at the right edge.   
Features related to inflectional morphology: a 
set of features encodes whether or not an entity contains 
nominal number or genitive morphology or verbal 
participial morphology, and the positions of the words 
in the entity that contain those morphemes, for instance 
the facts that apoptosis antagonizing transcription 
factor (HUGO ID 19235) contains a present participle 
and that the word that contains it is medial to the string.   
Features related to parts of speech: Future 
development of the data will include features encoding 
the parts of speech present in names. 
2.1.3   Source features 
Source or authority:  This feature encodes the 
source of or authority cited for an entity.  For many of 
the entries in the current data, it is an identifier from 
some database.  For others, it is a website (e.g. 
www.flynome.org).  Other possible values include the 
PMID of a document in which it was observed.   
Original form in source:  Where there is a source 
for the entity or for some canonical form of the entity, 
the original form is given.  This is not equivalent to the 
?official? form, but rather is the exact form in which the 
entity occurs; it may even contain typographic errors 
(e.g. the extraneous space in nima ?related kinase, 
LocusID 189769 (reported to the NCBI service desk).   
2.1.4   Lexical features 
These might be better called lexicographic features.  
They can be encoded impressionistically, or can be 
defined with respect to an external source, such as 
WordNet, the UMLS, or other lexical resources.  They 
may also be useful for encoding strictly local 
information, such as whether or not a gene was attested 
in training data or whether it is present in a particular 
language model or other local resource.  These features 
are allowed in the taxonomy but are not implemented in 
the current data.  Our own use of the entity data 
suggests that it should be, especially encoding of 
whether or not names include common English words.  
(The presence of function words is already encoded.)   
2.2   Feature set for sentential contexts 
In many ways, this data is much harder to build and 
classify than the names data, for at least two reasons. 
Many more features interact with each other, and as 
soon as a sentence contains more than one gene name, 
it contains more than one environment, and the number 
of features for the sentence as a whole is multiplied, as 
are the interactions between them.  For this reason, we 
have focussed our attention so far on sentences 
containing only a single gene name, although the 
current version of the data does include a number of 
multi-name sentences.  
2.2.1   Positivity 
The fundamental distinction in the feature set for 
sentences has to do with whether the sentence is 
intended to provide an environment in which gene 
names actually appear, or whether it is intended to 
provide a non-trivial opportunity for false positives. 
True positive sentences contain some slot in which 
entities from the names data can be inserted, e.g. <> 
polymorphisms may be correlated with an increased 
risk of larynx cancer or <> interacts with <> and <> 
in the two-hybrid system.   
False positive sentences contain one or more 
tokens that are deliberately intended to pose 
challenging opportunities for false positives.  Certainly 
any sentence which does not consist all and only of a 
single gene name contains opportunities for false 
positives, but not all potential false positives are created 
equal.  We include in the data set sentences that contain 
tokens with orthographic and typographic 
characteristics that mimic the patterns commonly seen 
in gene names and symbols, e.g.  The aim of the present 
study is to evaluate the impact on QoL? where QoL is 
an abbreviation for quality of life.  We also include 
sentences that contain ?keywords? that may often be 
associated with genes, such as gene, protein, mutant, 
expression, etc., e.g. Demonstration of antifreeze 
protein activity in Antarctic lake bacteria.  
2.2.2   Features for TP sentences 
Number and positional features encode the total number 
of slots in the sentence, and their positions.  The value 
for the position feature is a list whose values range over 
initial, medial, and final.  For example, the sentence  
<> interacts with <> and <> in the two-hybrid system 
has the value I,M (initial and medial) for the position 
feature.   
Typographic context features encode issues 
related to tokenization, specifically related to 
punctuation, for example if a slot has punctuation on 
the left or right edge, and the identity of the punctuation 
marks. 
List context features encode data about position in 
lists.  These include the type of list (coordination, 
asyndetic coordination, or complex coordination).  
The appositive feature is for the special case of 
appositioned symbols or abbreviations and their full 
names or definitions, e.g. The Arabidopsis INNER NO 
OUTER (INO) gene is essential for formation and?  
For the systems that we have tested with it, it has not 
revealed problems that are independent of the 
typographic context.  However, we expect it to be of 
future use in testing systems for abbreviation expansion 
in this domain.   
Source features encode the identification and type 
of the source for the sentence and its original form in 
the source.  The source identifier is often a PubMed ID.  
It bears pointing out again that there is no a priori 
reason to use sentences with any naturally-occurring 
?source? at all, as opposed to the products of the 
software engineer?s imagination.  Our primary rationale 
for using naturalistic sources at all for the sentence data 
has more to do with convincing the user that some of 
the combinations of entity features and sentential 
features that we claim to be worth generating actually 
do occur.  For instance, it might seem counterintuitive 
that gene symbols or names would ever occur lower-
case-initial in sentence initial position, but in fact we 
found many instances of this phenomenon; or that a 
multi-word gene name would occur in text in all upper-
case letters, but see the INNER NO OUTER example 
above.   
Syntactic features encode the characteristics of 
the local environment.  Some are very lexical, such as: 
whether the following word is a keyword; whether the 
preceding word is a species name.  Others are more 
abstract, such as whether the preceding word is an 
article; whether the preceding word is an adjective; 
whether the preceding word is a conjunction; whether 
the preceding word is a preposition.  Interactions with 
the list context features are complex.  The fault model 
motivating these features hypothesizes that POS context 
and the presence of keywords might affect a system?s 
judgments about the presence and boundaries of names.   
2.2.3   Features for FP sentences 
Most features for FP sentences encode the 
characteristics that give the contents of the sentence 
their FP potential.  The keyword feature is a list of 
keywords present in the sentence, e.g. gene, protein, 
expression, etc.  The typographic features feature 
encodes whether or not the FP potential comes from 
orthographic or typographic features of some token in 
the sentence, such as mixed case, containing hyphens 
and a number, etc.  The morphological features feature 
encodes whether or not the FP potential comes from 
apparent morphology, such as words that end with ase 
or in.   
3   Testing the relationship between 
predictions from performance on a test 
suite and performance on a corpus 
Precision and recall on data in a structured test suite 
should not be expected to predict precision and recall 
on a corpus, since there is no relation between the 
prevalence of features in the test suite and prevalence of 
features in the corpus.  However, we hypothesized that 
performance on an equivalence class of inputs in a test 
suite might predict performance on the same 
equivalence class in a corpus.  To test this hypothesis, 
we ran a number of test suites through one of the 
systems and analyzed the results, looking for patterns of 
errors.  The test suites were very simple, varying only 
entity length, case, hyphenation, and sentence position.  
Then we ran two corpora through the same system and 
examined the output for the actual corpora to see if the 
predictions based on the system?s behavior on the test 
suite actually described performance on similar entities 
in the corpora.     
One corpus, which we refer to as PMC (since it 
was sampled from PubMed Central), consists of 2417 
sentences sampled randomly from a set of 1000 full-
text articles.  This corpus contains 3491 entities.  It is 
described in Tanabe and Wilbur (2002b).  The second 
corpus was distributed as training data for the 
BioCreative competition.  It consists of 10,000 
sentences containing 11,851 entities and is described in 
detail at www.mitre.org/public/biocreative.  Each 
corpus is annotated for entities.   
The predictions based on the system?s performance 
on the test suite data were: 
1. The system will have low recall on entities that 
have numerals in initial position, followed by a 
dash, e.g. 825-Oak, 12-LOX, and 18-wheeler 
(/^\d+-/ in Perl). 
2. The system will have low recall on names that 
contain stopwords, such as Pray For Elves and ken 
and barbie.   
3. The system will have low recall on sentence-
medial terms that begin with a capital letter, such 
as Always Early.   
4. The system will have low recall on three-character-
long symbols.   
5. The system will have good recall on (long) names 
that end with numerals. 
We then examined the system?s true positive, false 
positive, and false negative outputs from the two 
corpora for outputs that belonged to the equivalence 
classes in 1-5.  Table 1 shows the results. 
 
 BioCreative 
 TP FP FN P R 
1 12 57 17 .17 .41 
2 0 1 38 0.0 0.0 
4 556 278 512 .67 .52 
5 284 251 72 .53 .80 
 PubMed Central 
 TP FP FN P R 
1 8 10 0 .44 1.0 
2 1 0 2 1.0 .33 
4 163 64 188 .72 .46 
5 108 54 46 .67 .70 
 
Table 1  Performance on two corpora for the 
predictable categories  Numbers in the far left column 
refer to the predictions listed above.  Overall 
performance on the corpora was: BioCreative P = .65, 
R = .68, and PMC P = .71, R = .62.   
For equivalence classes 1, 2, and 4, the predictions 
mostly held.  Low recall was predicted, and actual 
recall was .41, 0.0, .52, 1.0 (the one anomaly), .33, and 
.46 for these classes of names, versus overall recall of 
.68 on the BioCreative corpus and .62 on the PMC 
corpus.  The prediction held for equivalence class 5, as 
well; good recall was predicted, and actual recall was 
.80 and .70?higher than the overall recalls for the two 
corpora.  The third prediction could not be evaluated 
due to the normalization of case in the gold standards.  
These results suggest that a test suite can be a good 
predictor of performance on entities with particular 
typographic characteristics.   
4   Conclusion 
We do not advocate using this approach to replace the 
quantitative evaluation of EI systems by precision, 
recall, and F-measure.  Arguably, overall performance 
on real corpora is the best evaluation metric for entity 
identification, in which case the standard metrics are 
well-suited to the task.  However, at specific points in 
the software lifecycle, viz. during development and at 
the time of acceptance testing, the standard metrics do 
not provide the right kind of information.  We can, 
however, get at this information if we bear in mind two 
things: 
1. Entity identification systems are software, and as 
such can be assessed by standard software testing 
techniques.  
2. Entity identification systems are in some sense 
instantiations of hypotheses about linguistic 
structure, and as such can be assessed by standard 
linguistic ?field methods.?  
This paper describes a methodology and a data set for 
utilizing the principles of software engineering and 
linguistic analysis to generate test suites that answer the 
right kinds of questions for developers and for end 
users.  Readers are invited to contribute their own data.   
Acknowledgments 
The authors gratefully acknowledge support for this 
work from NIH/NIAAA grant U01-AA13524-02; 
comments from Andrew E. Dolbey on an earlier 
version of this work; Philip V. Ogren for help with 
stochastic-POS-tagging-based system; the Center for 
Computational Pharmacology NLP reading group and 
the anonymous reviewers for insightful comments on 
the current version; and Fukuda et al, Ono et al, and 
Franz?n et al for generously making their systems 
publicly available. 
 
References 
Beizer, Boris (1990).  Software testing techniques, 2nd 
ed.  Van Nostrand Reinhold.   
Binder, Robert V. (1999).  Testing object-oriented 
systems: models, patterns, and tools.  Addison-
Wesley.   
Cohen, K. Bretonnel; Philip V. Ogren; Shuhei 
Kinoshita; and Lawrence Hunter (in submission).  
Entity identification in the molecular biology domain 
with a stochastic POS tagger.  ISMB 2004.   
Cole, Ronald; Joseph Mariani; Hans Uszkoreit; Annie 
Zaenen; and Victor Zue (1997).  Survey of the state of 
the art in human language technology.  Cambridge 
University Press.   
Franz?n, Kristofer; Gunnar Eriksson; Fredrik Olsson; 
Lars Asker; Per Lid?n; and Joakim C?ster (2002).  
Protein names and how to find them.  International 
Journal of Medical Informatics 67(1-3):49-61.   
Fukuda, K.; T. Tsunoda; A. Tamura; and T. Takagi 
(1997).  Toward information extraction: identifying 
protein names from biological papers.  Pacific 
Symposium on Biocomputing 1998, pp. 705-716.   
Harris, Zellig S. (1951).  Methods in structural 
linguistics.  University of Chicago Press.   
Hirschman, Lynette; and Inderjeet Mani (2003).  
Evaluation.  In Mitkov (2003), pp. 415-429.   
Hirschman, Lynette; and Henry S. Thompson (1997).  
Overview of evaluation in speech and natural 
language processing.  In Cole et al (1997), pp. 409-
414.   
Kaner, Cem; Hung Quoc Nguyen; and Jack Falk 
(1999).  Testing computer software, 2nd ed.  John 
Wiley & Sons.   
Kaner, Cem; James Bach; and Bret Pettichord (2002).  
Lessons learned in software testing: a context-driven 
approach.  John Wiley & Sons.   
Marick, Brian (1997).  The craft of software testing: 
subsystem testing including object-based and object-
oriented testing.  Prentice Hall.   
Mitkov, Ruslan (2003).  The Oxford Handbook of 
Computational Linguistics.  Oxford University Press.   
Nerbonne, John (1998).  Linguistic Databases.  CSLI 
Publications.   
Ohta, Tomoko; Yuka Tateisi; Jin-Dong Kim; Hideki 
Mima; and Jun-ichi Tsujii (2002).  The GENIA 
corpus: an annotated corpus in molecular biology.  
Proceedings of the Human Language Technology 
Conference.   
Oepen, Stephan; Klaus Netter; and Judith Klein (1998).  
TSNLP ? Test Suites for Natural Language 
Processing.  In Nerbonne (1998), pp. 13-36.   
Olsson, Fredrik; Gunnar Eriksson; Kristofer Franz?n; 
Lars Asker; and Per Lid?n (2002).  Notions of 
correctness when evaluating protein name taggers.  
Proceedings of the 19th International Conference on 
Computational Linguistics (COLING 2002), Taipei, 
Taiwan.   
Ono, Toshihide; Haretsugu Hishigaki; Akira Tanigami; 
and Toshihisa Takagi (2001).  Automated extraction 
of information on protein-protein interactions from 
the biological literature.  Bioinformatics 17(2):155-
161.   
Samarin, William J. (1967).  Field linguistics: a guide 
to linguistic field work.  Irvington.   
Tanabe, Lorraine; and W. John Wilbur (2002a).  
Tagging gene and protein names in biomedical text.  
Bioinformatics 18(8):1124-1132.   
Tanabe, Lorraine; and W. John Wilbur (2002b).  
Tagging gene and protein names in full text articles.  
Proceedings of the workshop on natural language 
processing in the biomedical domain, pp. 9-13.  
Association for Computational Linguistics.   
Volk, Martin (1998).  Markup of a test suite with 
SGML.  In Nerbonne (1998), pp. 59-76.   
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 38?45, Detroit, June 2005. c?2005 Association for Computational Linguistics
Corpus design for biomedical natural language processing
K. Bretonnel Cohen
Center for Computational Pharmacology
U. of Colorado School of Medicine
Aurora, Colorado
kevin.cohen@gmail.com
Lynne Fox
Denison Library
U. of Colorado Health Sciences Center
Denver, Colorado
lynne.fox@uchsc.edu
Philip V. Ogren
Center for Computational Pharmacology
U. of Colorado School of Medicine
Aurora, Colorado
philip.ogren@uchsc.edu
Lawrence Hunter
Center for Computational Pharmacology
U. of Colorado School of Medicine
Aurora, Colorado
larry.hunter@uchsc.edu
Abstract
This paper classifies six publicly avail-
able biomedical corpora according to var-
ious corpus design features and charac-
teristics. We then present usage data for
the six corpora. We show that corpora
that are carefully annotated with respect
to structural and linguistic characteristics
and that are distributed in standard for-
mats are more widely used than corpora
that are not. These findings have implica-
tions for the design of the next generation
of biomedical corpora.
1 Introduction
A small number of data sets for evaluating the per-
formance of biomedical language processing (BLP)
systems on a small number of task types have been
made publicly available by their creators (Blaschke
et al 19991, Craven and Kumlein 19992, Puste-
jovsky et al 20023, Franze?n et al 20024, Collier
et al 19995, Tanabe et al 20056). From a biolog-
ical perspective, a number of these corpora (PDG,
GENIA, Medstract, Yapex, GENETAG) are excep-
tionally well curated. From the perspective of sys-
1We refer to this corpus as the Protein Design Group (PDG)
corpus.
2We refer to this as the University of Wisconsin corpus.
3The Medstract corpus.
4The Yapex corpus.
5The GENIA corpus.
6Originally the BioCreative Task 1A data set, now known as
the GENETAG corpus.
tem evaluation, a number of these corpora (Wiscon-
sin, GENETAG) are very well designed, with large
numbers of both positive and negative examples for
system training and testing. Despite the positive at-
tributes of all of these corpora, they vary widely in
their external usage rates: some of them have been
found very useful in the natural language process-
ing community outside of the labs that created them,
as evinced by their high rates of usage in system
construction and evaluation in the years since they
have been released. In contrast, others have seen lit-
tle or no use in the community at large. These data
sets provide us with an opportunity to evaluate the
consequences of a variety of approaches to biomed-
ical corpus construction. We examine these corpora
with respect to a number of design features and other
characteristics, and look for features that character-
ize widely used?and infrequently used?corpora.
Our findings have implications for how the next gen-
eration of biomedical corpora should be constructed,
and for how the existing corpora can be modified to
make them more widely useful.
2 Materials and methods
Table 1 lists the publicly available biomedical cor-
pora of which we are aware. We omit discussion
here of the corpus currently in production by the
University of Pennsylvania and the Children?s Hos-
pital of Philadelphia (Kulick et al 2004), since it is
not yet available in finished form. We also omit text
collections from our discussion. By text collection
we mean textual data sets that may include metadata
about documents, but do not contain mark-up of the
document contents. So, the OHSUMED text collec-
38
Table 1: Name, date, genre, and size for the six cor-
pora. Size is in words.
Name date genre size
PDG 1999 Sentences 10,291
Wisconsin 1999 Sentences 1,529,731
GENIA 1999 Abstracts 432,560
MEDSTRACT 2001 Abstracts 49,138
Yapex 2002 Abstracts 45,143
GENETAG 2004 Sentences 342,574
Table 2: Low- and high-level tasks to which the six
corpora are applicable. SS is sentence segmentation,
T is tokenization, and POS is part-of-speech tagging.
EI is entity identification, IE is information extrac-
tion, A is acronym/abbreviation definition, and C is
coreference resolution.
Name SS T POS EI IE A C
PDG    
Wisconsin    
GENIA        
Medstract      
Yapex  
GENETAG  
tion (Hersh et al 1994) and the TREC Genomics
track data sets (Hersh and Bhupatiraju 2003, Hersh
et al 2004) are excluded from this work, although
their utility in information retrieval is clear.
Table 1 lists the corpora, and for each corpus,
gives its release date (or the year of the correspond-
ing publication), the genre of the contents of the cor-
pus, and the size of the corpus7 .
The left-hand side of Table 2 lists the data sets
and, for each one, indicates the lower-level general
language processing problems that it could be ap-
plied to, either as a source of training data or for
evaluating systems that perform these tasks. We
considered here sentence segmentation, word tok-
enization, and part-of-speech (POS) tagging.
The right-hand side of Table 2 shows the higher-
7Sizes are given in words. Published descriptions of
the corpora don?t generally give size in words, so this
data is based on our own counts. See the web site at
http://compbio.uchsc.edu/corpora for details on how we did the
count for each corpus.
level tasks to which the various corpora can be
applied. We considered here entity identifica-
tion, information (relation) extraction, abbrevia-
tion/acronym definition, and coreference resolution.
(Information retrieval is approached via text collec-
tions, versus corpora.) These tasks are directly re-
lated to the types of semantic annotation present
in each corpus. The three EI-only corpora (GE-
NIA, Yapex, GENETAG) are annotated with seman-
tic classes of relevance to the molecular biology do-
main. In the case of the Yapex and GENETAG cor-
pora, this annotation uses a single semantic class,
roughly equivalent to the gene or gene product. In
the case of the GENIA corpus, the annotation re-
flects a more sophisticated, if not widely used, on-
tology. The Medstract corpus uses multiple seman-
tic classes, including gene, protein, cell type, and
molecular process. In all of these cases, the se-
mantic annotation was carefully curated, and in one
(GENETAG) it includes alternative analyses. Two
of the corpora (PDG, Wisconsin) are indicated in Ta-
ble 2 as being applicable to both entity identification
and information extraction tasks. From a biologi-
cal perspective, the PDG corpus has exceptionally
well-curated positive examples. From a linguistic
perspective, it is almost unannotated. For each sen-
tence, the entities are listed, but their locations in
the text are not indicated, making them applicable
to some definitions of the entity identification task
but not others. The Wisconsin corpus contains both
positive and negative examples. For each example,
entities are listed in a normalized form, but without
clear pointers to their locations in the text, making
this corpus similarly difficult to apply to many defi-
nitions of the entity identification task.
The Medstract corpus is unique among these in
being annotated with coreferential equivalence sets,
and also with acronym expansions.
All six corpora draw on the same subject matter
domain?molecular biology?but they vary widely
with respect to their level of semantic restriction
within that relatively broad category. One (GE-
NIA) is restricted to the subdomain of human
blood cell transcription factors. Another (Yapex)
combines data from this domain with abstracts
on protein binding in humans. The GENETAG
corpus is considerably broader in topic, with all
of PubMed/MEDLINE serving as a potential data
39
Table 3: External usage rates. The systems column
gives the count of the number of systems that actu-
ally used the dataset, as opposed to publications that
cited the paper but did not use the data itself. Age is
in years as of 2005.
Name age systems
GENIA 6 21
GENETAG 1 8
Yapex 3 6
Medstract 4 3
Wisconsin 6 1
PDG 6 0
source. The Medstract corpus contains biomedical
material not apparently related to molecular biology.
The PDG corpus is drawn from a very narrow subdo-
main on protein-protein interactions. The Wiscon-
sin corpus is composed of data from three separate
sub-domains: protein-protein interactions, subcellu-
lar localization of proteins, and gene/disease associ-
ations.
Table 3 shows the number of systems built out-
side of the lab that created the corpus that used each
of the data sets described in Tables 1 and 2. The
counts in this table reflect work that actually used
the datasets, versus work that cites the publication
that describes the data set but doesn?t actually use
the data set. We assembled the data for these counts
by consulting with the creators of the data sets and
by doing our own literature searches8 . If a system is
described in multiple publications, we count it only
once, so the number of systems is slightly smaller
than the number of publications.
3 Results
Even without examining the external usage data, two
points are immediately evident from Tables 1 and 2:
  Only one of the currently publicly available
corpora (GENIA) is suitable for evaluating per-
formance on basic preprocessing tasks.
8In the cases of the two corpora for which we found only
zero or one external usage, this search was repeated by an expe-
rienced medical librarian, and included reviewing 67 abstracts
or full papers that cite Blaschke et al (1999) and 37 that cite
Craven and Kumlein (1999).
  These corpora include only a very limited range
of genres: only abstracts and roughly sentence-
sized inputs are represented.
Examination of Table 3 makes another point im-
mediately clear. The currently publicly available
corpora fall into two groups: ones that have had a
number of external applications (GENIA, GENE-
TAG, and Yapex), and ones that have not (Medstract,
Wisconsin, and PDG). We now consider a number
of design features and other characteristics of these
corpora that might explain these groupings9 .
3.1 Effect of age
We considered the very obvious hypothesis that it
might be length of time that a corpus has been avail-
able that determines the amount of use to which it
has been put. (Note that we use the terms ?hypothe-
sis? and ?effect? in a non-statistical sense, and there
is no significance-testing in the work reported here.)
Tables 1 and 3 show clearly that this is not the case.
The age of the PDG, Wisconsin, and GENIA data
is the same, but the usage rates are considerably
different?the GENIA corpus has been much more
widely used. The GENETAG corpus is the newest,
but has a relatively high usage rate. Usage of a cor-
pus is determined by factors other than the length of
time that it has been available.
3.2 Effect of size
We considered the hypothesis that size might be the
determinant of the amount of use to which a corpus
is put?perhaps smaller corpora simply do not pro-
vide enough data to be helpful in the development
and validation of learning-based systems. We can
9Three points should be kept in mind with respect to this
data. First, although the sample includes all of the corpora that
we are aware of, it is small. Second, there is a variety of po-
tential confounds related to sociological factors which we are
aware of, but do not know how to quantify. One of these is the
effect of association between a corpus and a shared task. This
would tend to increase the usage of the corpus, and could ex-
plain the usage rates of GENIA and GENETAG, although not
that of Yapex. Another is the effect of association between a
corpus and an influential scientist. This might tend to increase
the usage of the corpus, and could explain the usage rate of
GENIA, although not that of GENETAG. Finally, there may
be interactions between any of these factors, or as a reviewer
pointed out, there may be a separate explanation for the usage
rate of each corpus in this study. Nevertheless, the analysis of
the quantifiable factors presented above clearly provides useful
information about the design of successful corpora.
40
reject this hypothesis: the Yapex corpus is one of
the smallest (a fraction of the size of the largest, and
only roughly a tenth of the size of GENIA), but has
achieved fairly wide usage. The Wisconsin corpus
is the largest, but has a very low usage rate.
3.3 Effect of structural and linguistic
annotation
We expected a priori that the corpus with the most
extensive structural and linguistic annotation would
have the highest usage rate. (In this context, by
structural annotation we mean tokenization and sen-
tence segmentation, and by linguistic annotation we
mean POS tagging and shallow parsing.) There isn?t
a clear-cut answer to this.
The GENIA corpus is the only one with curated
structural and POS annotation, and it has the highest
usage rate. This is consistent with our initial hypoth-
esis.
On the other hand, the Wisconsin corpus could
be considered the most ?deeply? linguistically an-
notated, since it has both POS annotation and?
unique among the various corpora?shallow pars-
ing. It nevertheless has a very low usage rate. How-
ever, the comparison is not clearcut, since both the
POS tagging and the shallow parsing are fully au-
tomatic and not manually corrected. (Additionally,
the shallow parsing and the tokenization on which
it is based are somewhat idiosyncratic.) It is clear
that the Yapex corpus has relatively high usage de-
spite the fact that it is, from a linguistic perspective,
very lightly annotated (it is marked up for entities
only, and nothing else). To our surprise, structural
and linguistic annotation do not appear to uniquely
determine usage rate.
3.4 Effect of format
Annotation format has a large effect on usage. It
bears repeating that these six corpora are distributed
in six different formats?even the presumably sim-
ple task of populating the Size column in Table 1
required writing six scripts to parse the various data
files. The two lowest-usage corpora are annotated in
remarkably unique formats. In contrast, the three
more widely used corpora are distributed in rela-
tively more common formats. Two of them (GENIA
and Yapex) are distributed in XML, and one of them
(GENIA) offers a choice for POS tagging informa-
tion between XML and the whitespace-separated,
one-token-followed-by-tags-per-line format that is
common to a number of POS taggers and parsers.
The third (GENETAG) is distributed in the widely
used slash-attached format (e.g. sense/NN).
3.5 Effect of semantic annotation
The data in Table 2 and Table 3 are consistent with
the hypothesis that semantic annotation predicts us-
age. The claim would be that corpora that are
built specifically for entity identification purposes
are more widely used than corpora of other types,
presumably due to a combination of the importance
of the entity identification task as a prerequisite to
a number of other important applications (e.g. in-
formation extraction and retrieval) and the fact that
it is still an unsolved problem. There may be some
truth to this, but we doubt that this is the full story:
there are large differences in the usage rates of the
three EI corpora, suggesting that semantic annota-
tion is not the only relevant design feature. If this
analysis is in fact correct, then certainly we should
see a reduction in the use of all three of these corpora
once the EI problem is solved, unless their semantic
annotations are extended in new directions.
3.6 Effect of semantic domain
Both the advantages and the disadvantages of re-
stricted domains as targets for language processing
systems are well known, and they seem to balance
out here. The scope of the domain does not affect
usage: both the low-use and higher-use groups of
corpora contain at least one highly restricted domain
(GENIA in the high-use group, and PDG in the low-
use group) and one broader domain (GENETAG in
the high-use group, and Wisconsin in the lower-use
group).
4 Discussion
The data presented in this paper show clearly that ex-
ternal usage rates vary widely for publicly available
biomedical corpora. This variability is not related
to the biological relevance of the corpora?the PDG
and Wisconsin corpora are clearly of high biologi-
cal relevance as evinced by the number of systems
that have tackled the information extraction tasks
that they are meant to support. Additionally, from a
biological perspective, the quality of the data in the
41
PDG corpus is exceptionally high. Rather, our data
suggest that basic issues of distribution format and
of structural and linguistic annotation seem to be the
strongest predictors of how widely used a biomed-
ical corpus will be. This means that as builders of
of data sources for BLP, we can benefit from the ex-
tensive experience of the corpus linguistics world.
Based on that experience, and on the data that we
have presented in this paper, we offer a number of
suggestions for the design of the next generation of
biomedical corpora.
We also suggest that the considerable invest-
ments already made in the construction of the less-
frequently-used corpora can be protected by modify-
ing those corpora in accordance with these sugges-
tions.
Leech (1993) and McEnery and Wilson (2001),
coming from the perspective of corpus linguistics,
identify a number of definitional issues and design
maxims for corpus construction. Some of these are
quite relevant to the current state of biomedical cor-
pus construction. We frame the remainder of our
discussion in terms of these issues and maxims.
4.1 Level of annotation
From a definitional point of view, annotation is one
of the distinguishing points of a corpus, as opposed
to a text collection. Perhaps the most salient char-
acteristic of the currently publicly available corpora
is that from a linguistic or language processing per-
spective, with the exception of GENIA and GENE-
TAG, they are barely annotated at all. For example,
although POS tagging has possibly been the sine qua
non of the usable corpus since the earliest days of
the modern corpus linguistic age, five of the six cor-
pora listed in Table 2 either have no POS tagging
or have only automatically generated, uncorrected
POS tags. The GENIA corpus, with its carefully cu-
rated annotation of sentence segmentation, tokeniza-
tion, and part-of-speech tagging, should serve as a
model for future biomedical corpora in this respect.
It is remarkable that with just these levels of anno-
tation (in addition to its semantic mark-up), the GE-
NIA corpus has been applied to a wide range of task
types other than the one that it was originally de-
signed for. Eight papers from COLING 2004 (Kim
et al 2004) used it for evaluating entity identifica-
tion tasks. Yang et al (2002) adapted a subset of
the corpus for use in developing and testing a coref-
erence resolution system. Rinaldi et al (2004) used
it to develop and test a question-answering system.
Locally, it has been used in teaching computational
corpus linguistics for the past two years. We do not
claim that it has not required extension for some of
these tasks?our claim is that it is its annotation on
these structural and linguistic levels, in combination
with its format, that has made these extensions prac-
tical.
4.1.1 Formatting choices and formatting
standardization
A basic desideratum for a corpus is recoverabil-
ity: it should be possible to map from the annotation
to the raw text. A related principle is that it should
be easy for the corpus user to extract all annotation
information from the corpus, e.g. for external stor-
age and processing: ?in other words, the annotated
corpus should allow the maximum flexibility for ma-
nipulation by the user? (McEnery and Wilson, p.
33). The extent to which these principles are met
is a function of the annotation format. The currently
available corpora are distributed in a variety of one-
off formats. Working with any one of them requires
learning a new format, and typically writing code
to access it. At a minimum, none of the non-XML
corpora meet the recoverability criterion. None10 of
these corpora are distributed in a standoff annotation
format. Standoff annotation is the strategy of stor-
ing annotation and raw text separately (Leech 1993).
Table 4 contrasts the two. Non-standoff annota-
tion at least obscures?more frequently, destroys?
important aspects of the structure of the text itself,
such as which textual items are and are not imme-
diately adjacent. Using standoff annotation, there is
no information loss whatsoever. Furthermore, in the
standoff annotation strategy, the original input text
is immediately available in its raw form. In contrast,
in the non-standoff annotation strategy, the original
must be retrieved independently or recovered from
the annotation (if it is recoverable at all). The stand-
off annotation strategy was relatively new at the time
that most of the corpora in Table 1 were designed,
but by now has become easy to implement, in part
10The semantic annotation of the GENETAG corpus is in a
standoff format, but neither the tokenization nor the POS tag-
ging is.
42
Table 4: Contrasting standoff and non-standoff an-
notation
Raw text
MLK2 has a role in vesicle formation
Non-standoff annotation
MLK2/NN has/VBZ a/DT role/NN in/IN
vesicle/NN formation/NN
Standoff annotation
  POS=?NN? start=0 end=3 
  POS=?VBZ? start=5 end=7 
  POS=?DT? start=9 end=9 
  POS=?NN? start=11 end=14 
  POS=?IN? start=16 end=17 
  POS=?NN? start=19 end=25 
  POS=?NN? start=27 end=35 
due to the availability of tools such as the University
of Pennsylvania?s WordFreak (Morton and LaCivita
2003).
Crucially, this annotation should be based on
character offsets, avoiding a priori assumptions
about tokenization. See Smith et al (2005) for an
approach to refactoring a corpus to use character off-
sets.
4.1.2 Guidelines
The maxim of documentation suggests that anno-
tation guidelines should be published. Further, ba-
sic data on who did the annotations and on their
level of agreement should be available. The cur-
rently available datasets mostly lack assessments of
inter-annotator agreement, utilize a small or unspec-
ified number of annotators, and do not provide pub-
lished annotation guidelines. (We note the Yang et
al. (2002) coreference annotation guidelines, which
are excellent, but the corresponding corpus is not
publicly available.) This situation can be remedied
by editors, who should insist on publication of all
of these. The GENETAG corpus is notable for the
detailed documentation of its annotation guidelines.
We suspect that the level of detail of these guidelines
contributed greatly to the success of some rule-based
approaches to the EI task in the BioCreative compe-
tition, which utilized an early version of this corpus.
4.1.3 Balance and representativeness
Corpus linguists generally strive for a well-
structured stratified sample of language, seeking to
?balance? in their data the representation of text
types, different sorts of authors, and so on. Within
the semantic domain of molecular biology texts,
an important dimension on which to balance is the
genre or text type.
As is evident from Table 1, the extant datasets
draw on a very small subset of the types of genres
that are relevant to BLP: we have not done a good
job yet of observing the principle of balance or rep-
resentativeness. The range of genres that exist in the
research (as opposed to clinical) domain alone in-
cludes abstracts, full-text articles, GeneRIFs, defini-
tions, and books. We suggest that all of these should
be included in future corpus development efforts.
Some of these genres have been shown to have
distinguishing characteristics that are relevant to
BLP. Abstracts and isolated sentences from them
are inadequate, and also unsuited to the opportuni-
ties that are now available to us for text data mining
with the recent announcement of the NIH?s new pol-
icy on availability of full-text articles (NIH 2005).
This policy will result in the public availability of
a large and constantly growing archive of current,
full-text publications. Abstracts and sentences are
inadequate in that experience has shown that signifi-
cant amounts of data are not found in abstracts at all,
but are present only in the full texts of articles, some-
times not even in the body of the text itself, but rather
in tables and figure captions (Shatkay and Feldman
2003). They are not suited to the upcoming opportu-
nities in that it is not clear that practicing on abstracts
will let us build the necessary skills for dealing with
the flood of full-text articles that PubMedCentral
is poised to deliver to us. Furthermore, there are
other types of data?GeneRIFs and domain-specific
dictionary definitions, for instance?that are fruit-
ful sources of biological knowledge, and which may
actually be easier to process automatically than ab-
stracts. Space does not permit justifying the impor-
tance of all of these genres, but we discuss the ratio-
nale for including full text at some length due to the
recent NIH announcement and due to the large body
of evidence that can currently be brought to bear on
the issue. A growing body of recent research makes
43
two points clear: full-text articles are different from
abstracts, and full-text articles must be tapped if we
are to build high-recall text data mining systems.
Corney et al (2004) looked directly at the effec-
tiveness of information extraction from full-text ar-
ticles versus abstracts. They found that recall from
full-text articles was more than double that from ab-
stracts. Analyzing the relative contributions of the
abstracts and the full articles, they found that more
than half of the interactions that they were able to
extract were found in the full text and were absent in
the abstract.
Tanabe and Wilbur (2002) looked at the perfor-
mance on full-text articles of an entity identification
system that had originally been developed and tested
using abstracts. They found different false positive
rates in the Methods sections compared to other sec-
tions of full-text articles. This suggests that full-text
articles, unlike abstracts, will require parsing of doc-
ument structure. They also noted a range of prob-
lems related to the wider range of characters (includ-
ing, e.g., superscripts and Greek letters) that occurs
in full-text articles, as opposed to abstracts.
Schuemie et al (2004) examined a set of 3902
full-text articles from Nature Genetics and BioMed
Central, along with their abstracts. They found that
about twice as many MeSH concepts were men-
tioned in the full-text articles as in the abstracts.
They also found that full texts contained a larger
number of unique gene names than did abstracts,
with an average of 2.35 unique gene names in the
full-text articles, but an average of only 0.61 unique
gene names in the abstracts.
It seems clear that for biomedical text data min-
ing systems to reach anything like their full poten-
tial, they will need to be able to handle full-text in-
puts. However, as Table 1 shows, no publicly avail-
able corpus contains full-text articles. This is a defi-
ciency that should be remedied.
5 Conclusion
5.1 Best practices in biomedical corpus
construction
We have discussed the importance of recoverabil-
ity, publication of guidelines, balance and represen-
tativeness, and linguistic annotation. Corpus main-
tenance is also important. Bada et al (2004) point
out the role that an organized and responsive main-
tenance plan has played in the success of the Gene
Ontology. It seems likely that the continued devel-
opment and maintenance reflected in the three ma-
jor releases of GENIA (Ohta et al 2002, Kim et al
2003) have contributed to its improved quality and
continued use over the years.
5.2 A testable prediction
We have interpreted the data on the characteristics
and usage rates of the various datasets discussed in
this paper as suggesting that datasets that are devel-
oped in accordance with basic principles of corpus
linguistics are more useful, and therefore more used,
than datasets that are not.
A current project at the University of Pennsyl-
vania and the Children?s Hospital of Philadelphia
(Kulick et al 2004) is producing a corpus that fol-
lows many of these basic principles. We predict that
this corpus will see wide use by groups other than
the one that created it.
5.3 The next step: grounded references
The logical next step for BLP corpus construction
efforts is the production of corpora in which entities
and concepts are grounded with respect to external
models of the world (Morgan et al 2004).
The BioCreative Task 1B data set construction ef-
fort provides a proof-of-concept of the plausibility
of building BLP corpora that are grounded with re-
spect to external models of the world, and in partic-
ular, biological databases. These will be crucial in
taking us beyond the stage of extracting information
about text strings, and towards mining knowledge
about known, biologically relevant entities.
6 Acknowledgements
This work was supported by NIH grant R01-
LM008111. The authors gratefully acknowledge
helpful discussions with Lynette Hirschman, Alex
Morgan, and Kristofer Franze?n, and thank Sonia
Leach and Todd A. Gibson for LATEXassistance.
Christian Blaschke, Mark Craven, Lorraine Tanabe,
and again Kristofer Franze?n provided helpful data.
We thank all of the corpus builders for their gen-
erosity in sharing their valuable resources.
44
References
Bada, Michael; Robert Stevens; et al 2004. A short
study on the success of the Gene Ontology. Journal of
web semantics 1(2):235-240.
Blaschke, Christian; Miguel A. Andrade; Christos
Ouzounis; and Alfonso Valencia. 1999. Automatic
extraction of biological information from scientific
text: protein-protein interactions. ISMB-99, pp. 60-67.
AAAI Press.
Collier, Nigel, Hyun Seok Park, Norihiro Ogata, Yuka
Tateisi, Chikashi Nobata, Takeshi Sekimizu, Hisao
Imai and Jun?ichi Tsujii. 1999. The GENIA project:
corpus-based knowledge acquisition and information
extraction from genome research papers. EACL 1999.
Corney, David P.A.; Bernard F. Buxton; William B.
Langdon; and David T. Jones. 2004. BioRAT: ex-
tracting biological information from full-length pa-
pers. Bioinformatics 20(17):3206-3213.
Craven, Mark; and Johan Kumlein. 1999. Constructing
biological knowledge bases by extracting information
from text sources. ISMB-99, pp. 77-86. AAAI Press.
Franze?n, Kristofer; Gunnar Eriksson; Fredrik Olsson;
Lars Asker Per Lide?n; and Joakim Co?ster. 2002. Pro-
tein names and how to find them. International Jour-
nal of Medical Informatics, 67(1-3), pp. 49-61.
Hersh, William; Chris Buckley; TJ Leone; and David
Hickam. 1994. OHSUMED: an interactive retrieval
evaluation and new large test collection for research.
SIGIR94, pp. 192-201.
Hersh, William; and Ravi Teja Bhupatiraju. 2003. TREC
genomics track overview. TREC 2003, pp. 14-23.
Hersh et al 2004. TREC 2004 genomics track overview.
TREC Notebook.
Kim, Jin-Dong; Tomoko Ohta; Yuka Tateisi; and Jun?ichi
Tsujii. 2003. GENIA corpus?a semantically an-
notated corpus for bio-textmining. Bioinformatics
19(Suppl. 1):i180-i182.
Kim, Jin-Dong; Tomoko Ohta; Yoshimasa Tsuruoka;
and Yuka Tateisi. 2004. Introduction to the bio-
entity recognition task at JNLPBA. Proc. interna-
tional joint workshop on natural language processing
in biomedicine and its applications, pp. 70-75.
Kulick, Seth; Ann Bies; Mark Liberman; Mark Mandel;
Ryan McDonald; Martha Palmer; Andrew Schein; and
Lyle Ungar. 2004. Integrated annotation for biomedi-
cal information extraction. BioLink 2004, pp. 61-68.
Leech, G. 1993. Corpus annotation schemes. Literary
and linguistic computing 8(4):275-281.
McEnery, Tony; and Andrew Wilson. 2001. Corpus lin-
guistics, 2nd edition. Edinburgh University Press.
Morgan, Alexander A.; Lynette Hirschman; Marc
Colosimo; Alexander S. Yeh; and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. JBMI 37:396-410.
Morton, Thomas; and Jeremy LaCivita. 2003. Word-
Freak: an open tool for linguistic annotation.
HLT/NAACL 2003: demonstrations, pp. 17-18.
NIH (National Institutes of Health). 2005.
http://www.nih.gov/news/pr/feb2005/od-03.htm
Ohta, Tomoko; Yuka Tateisi; and Jin-Dong Kim. 2002.
The GENIA corpus: an annotated research abstract
corpus in molecular biology domain. HLT 2002, pp.
73-77.
Pustejovsky, James; Jose? Castan?o; R. Saur?i; A.
Rumshisky; J. Zhang; and W. Luo. 2002. Medstract:
creating large-scale information servers for biomedical
libraries. Proc. workshop on natural language pro-
cessing in the biomedical domain, pp. 85-92. Associa-
tion for Computational Linguistics.
Rinaldi, Fabio; James Dowdall; Gerold Schneider; and
Andreas Persidis. 2004. Answering questions in the
genomics domain. Proc. ACL 2004 workshop on ques-
tion answering in restricted domains, pp. 46-53.
Schuemie, M.J.; M. Weeber; B.J. Schijvenaars; E.M.
van Mulligen; C.C. van der Eijk; R. Jelier; B. Mons;
and J.A. Kors. 2004. Distribution of information in
biomedical abstracts and full-text publications. Bioin-
formatics 20(16):2597-2604.
Shatkay, Hagit; and Ronen Feldman. 2003. Mining the
biomedical literature in the genomic era: an overview.
Journal of computational biology 10(6):821-855.
Smith, Lawrence H.; Lorraine Tanabe; Thomas Rind-
flesch; and W. John Wilbur. 2005. MedTag: a col-
lection of biomedical annotations. BioLINK 2005, this
volume.
Tanabe, Lorraine; and L. John Wilbur. 2002. Tagging
gene and protein names in full text articles. Proc.
ACL workshop on natural language processing in the
biomedical domain, pp. 9-13.
Tanabe, Lorraine; Natalie Xie; Lynne H. Thom; Wayne
Matten; and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics 6(Suppl. 1):S3.
Yang, Xiaofeng; Guodong Zhou; Jian Su; and Chew Lim
Tan. Improving noun phrase coreference resolution by
matching strings. 2002. IJCNLP04, pp. 326-333.
45
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 116?117,
New York City, June 2006. c?2006 Association for Computational Linguistics
Refactoring Corpora
Helen L. Johnson
Center for Computational Pharmacology
U. of Colorado School of Medicine
helen.johnson@uchsc.edu
William A. Baumgartner, Jr.
Center for Computational Pharmacology
U. of Colorado School of Medicine
william.baumgartner@uchsc.edu
Martin Krallinger
Protein Design Group
Universidad Auto?noma de Madrid
martink@cnb.uam.es
K. Bretonnel Cohen
Center for Computational Pharmacology
U. of Colorado School of Medicine
kevin.cohen@gmail.com
Lawrence Hunter
Center for Computational Pharmacology
U. of Colorado School of Medicine
larry.hunter@uchsc.edu
Abstract
We describe a pilot project in semi-
automatically refactoring a biomedical
corpus. The total time expended was just
over three person-weeks, suggesting that
this is a cost-efficient process. The refac-
tored corpus is available for download at
http://bionlp.sourceforge.net.
1 Introduction
Cohen et al (2005) surveyed the usage rates of a
number of biomedical corpora, and found that most
biomedical corpora have not been used outside of
the lab that created them. Empirical data on corpus
design and usage suggests that one major factor af-
fecting usage is the format in which it is distributed.
These findings suggest that there would be a large
benefit to the community in refactoring these cor-
pora. Refactoring is defined in the software en-
gineering community as altering the internal struc-
ture of code without altering its external behav-
ior (Fowler et al, 1999). We suggest that in the con-
text of corpus linguistics, refactoring means chang-
ing the format of a corpus without altering its con-
tents, i.e. its annotations and the text that they de-
scribe. The significance of being able to refactor a
large number of corpora should be self-evident: a
likely increase in the use of the already extant pub-
licly available data for evaluating biomedical lan-
guage processing systems, without the attendant cost
of repeating their annotation.
We examined the question of whether corpus
refactoring is practical by attempting a proof-of-
concept application: modifying the format of the
Protein Design Group (PDG) corpus described in
Blaschke et al (1999) from its current idiosyncratic
format to a stand-off annotation format (WordF-
reak1) and a GPML-like (Kim et al, 2001) embed-
ded XML format.
2 Methods
The target WordFreak and XML-embedded formats
were chosen for two reasons. First, there is some
evidence suggesting that standoff annotation and
embedded XML are the two most highly preferred
corpus annotation formats, and second, these for-
mats are employed by the two largest extant curated
biomedical corpora, GENIA (Kim et al, 2001) and
BioIE (Kulick et al, 2004).
The PDG corpus we refactored was originally
constructed by automatically detecting protein-
protein interactions using the system described in
Blaschke et al (1999), and then manually review-
ing the output. We selected it for our pilot project
because it was the smallest publicly available cor-
pus of which we were aware. Each block of text has
a deprecated MEDLINE ID, a list of actions, a list of
proteins and a string of text in which the actions and
proteins are mentioned. The structure and contents
of the original corpus dictate the logical steps of the
refactoring process:
1. Determine the current PubMed identifier, given
the deprecated MEDLINE ID. Use the PubMed
identifier to retrieve the original abstract.
1http://venom.ldc.upenn.edu/
resources/info/wordfreak ann.html
116
2. Locate the original source sentence in the title
or abstract.
3. Locate the ?action? keywords and the entities
(i.e., proteins) in the text.
4. Produce output in the new formats.
Between each file creation step above, human cu-
rators verify the data. The creation and curation pro-
cess is structured this way so that from one step to
the next we are assured that all data is valid, thereby
giving the automation the best chance of performing
well on the subsequent step.
3 Results
The refactored PDG corpus is publicly available at
http://bionlp.sourceforge.net. Total time expended
to refactor the PDG corpus was 122 hours and 25
minutes, or approximately three person-weeks. Just
over 80% of the time was spent on the programming
portion. Much of that programming can be directly
applied to the next refactoring project. The remain-
ing 20% of the time was spent curating the program-
matic outputs.
Mapping IDs and obtaining the correct abstract
returned near-perfect results and required very little
curation. For the sentence extraction step, 33% of
the corpus blocks needed manual correction, which
required 4 hours of curation. (Here and below, ?cu-
ration? time includes both visual inspection of out-
puts, and correction of any errors detected.) The
source of error was largely due to the fact that the
sentence extractor returned the best sentence from
the abstract, but the original corpus text was some-
times more or less than one sentence.
For the protein and action mapping step, about
40% of the corpus segments required manual cor-
rection. In total, this required about 16 hours of cu-
ration time. Distinct sources of error included par-
tial entity extraction, incorrect entity extraction, and
incorrect entity annotation in the original corpus ma-
terial. Each of these types of errors were corrected.
4 Conclusion
The underlying motivation for this paper is the hy-
pothesis that corpus refactoring is practical, eco-
nomical, and useful. Erjavec (2003) converted the
GENIA corpus from its native format to a TEI P4
format. They noted that the translation process
brought to light some previously covert problems
with the GENIA format. Similarly, in the process of
the refactoring we discovered and repaired a number
of erroneous entity boundaries and spurious entities.
A number of enhancements to the corpus are now
possible that in its previous form would have been
difficult at best. These include but are not limited
to performing syntactic and semantic annotation and
adding negative examples, which would expand the
usefulness of the corpus. Using revisioning soft-
ware, the distribution of iterative feature additions
becomes simple.
We found that this corpus could be refactored with
about 3 person-weeks? worth of time. Users can take
advantage of the corrections that we made to the en-
tity component of the data to evaluate novel named
entity recognition techniques or information extrac-
tion approaches.
5 Acknowledgments
The authors thank the Protein Design Group at the Universidad
Auto?noma de Madrid for providing the original PDG protein-
protein interaction corpus, Christian Blaschke and Alfonso Va-
lencia for assistance and support, and Andrew Roberts for mod-
ifying his jTokeniser package for us.
References
Christian Blaschke, Miguel A. Andrade, and Christos Ouzou-
nis. 1999. Automatic extraction of biological information
from scientific text: Protein-protein interactions.
K. Bretonnel Cohen, Lynne Fox, Philip Ogren, and Lawrence
Hunter. 2005. Empirical data on corpus design and usage in
biomedical natural language processing. AMIA 2005 Sym-
posium Proceedings, pages 156?160.
Tomaz Erjavec, Yuka Tateisi, Jin-Dong Kim, and Tomoko Ohta.
2003. Encoding biomedical resources in TEI: the case of the
GENIA corpus.
Martin Fowler, Kent Beck, John Brant, William Opdyke, and
Don Roberts. 1999. Refactoring: improving the design of
existing code. Addison-Wesley.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, Hideki Mima, and
Jun?ichi Tsujii. 2001. Xml-based linguistic annotation of
corpus. In Proceedings of The First NLP and XML Work-
shop, pages 47?53.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. McDonald,
M. Palmer, A. Schein, and L. Ungar. 2004. Integrated anno-
tation for biomedical information extraction. Proceedings of
the HLT/NAACL.
117
BioNLP 2007: Biological, translational, and clinical language processing, pages 97?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Shared Task Involving Multi-label Classification of Clinical Free Text
John P. Pestian1, Christopher Brew2, Pawe? Matykiewicz1,4,
DJ Hovermale2, Neil Johnson1, K. Bretonnel Cohen3,
W?odzis?aw Duch4
1Cincinnati Children?s Hospital Medical Center, University of Cincinnati,
2Ohio State University, Department of Linguistics,
3University of Colorado School of Medicine,
4Nicolaus Copernicus University, Torun?, Poland.
Abstract
This paper reports on a shared task involving
the assignment of ICD-9-CM codes to radi-
ology reports. Two features distinguished
this task from previous shared tasks in the
biomedical domain. One is that it resulted in
the first freely distributable corpus of fully
anonymized clinical text. This resource is
permanently available and will (we hope) fa-
cilitate future research. The other key fea-
ture of the task is that it required catego-
rization with respect to a large and commer-
cially significant set of labels. The number
of participants was larger than in any pre-
vious biomedical challenge task. We de-
scribe the data production process and the
evaluation measures, and give a preliminary
analysis of the results. Many systems per-
formed at levels approaching the inter-coder
agreement, suggesting that human-like per-
formance on this task is within the reach of
currently available technologies.
1 Introduction
Clinical free text (primary data about patients, as op-
posed to journal articles) poses significant technical
challenges for natural language processing (NLP).
In addition, there are ethical and social demands
when working with such data, which is intended for
use by trained medical practitioners who appreciate
the constraints that patient confidentiality imposes.
State-of-the-art NLP systems handle carefully edited
text better than fragmentary notes, and clinical lan-
guage is known to exhibit unique sublanguage char-
acteristics (Hirschman and Sager, 1982; Friedman
et al, 2002; Stetson et al, 2002) (e.g. verbless
sentences, domain-specific punctuation semantics,
and unusual metonomies) that may limit the perfor-
mance of general NLP tools. More importantly, the
confidentiality requirements take time and effort to
address, so it is not surprising that much work in
the biomedical domain has focused on edited jour-
nal articles (and the genomics domain) rather than
clinical free text in medical records. The fact re-
mains, however, that the automation of healthcare
workflows can bring important benefits to treatment
(Hurtado et al, 2001) and reduce administrative bur-
den, and that free text is a critical component of
these workflows. There are economic motivations
for the task, as well. The cost of adding labels like
ICD-9-CM to clinical free text and the cost of re-
pairing associated errors is approximately $25 bil-
lion per year in the US (Lang, 2007). For these
(and many other) reasons, there have been consis-
tent attempts to overcome the obstacles which hin-
der the processing of clinical text (Uzuner et al,
2006). This paper discusses one such attempt?
The 2007 Computational Medicine Challenge, here-
after referred to as ?the Challenge?. There were two
main reasons for conducting the Challenge. One
is to facilitate advances in mining clinical free text;
shared tasks in other biomedical domains have been
shown to drive progress in the field in multiple ways
(see (Hirschman and Blaschke, 2006; Hersh et al,
2005; Uzuner et al, 2006; Hersh et al, 2006) for a
comprehensive review of biomedical challenge tasks
and their contributions). The other is a ground-
97
breaking distribution of useful, reusable, carefully
anonymized clinical data to the research commu-
nity, whose data use agreement is simply to cite the
source. The remaining sections of this paper de-
scribe how the data were prepared, the methods for
scoring, preliminary results [to be updated if sub-
mission is accepted?results are currently still under
analysis], and some lessons learned.
2 Corpus collection and coding process
Supervised methods for machine learning require
training data. Yet, due to confidentiality require-
ments, spotty electronic availability, and variance in
recording standards, the requisite clinical training
data are difficult to obtain. One goal of the chal-
lenge was to create a publicly available ?gold stan-
dard? that could serve as the seed for a larger, open-
source clinical corpus. For this we used the follow-
ing guiding principles: individual identity must be
expunged to meet United States HIPAA standards,
(U.S. Health, 2002) and approved for release by the
local Institutional Review Board (IRB); the sample
must represent problems that medical records coders
actually face; the sample must have enough data for
machine-learning-based systems to do well; and the
sample must include proportionate representations
of very low-frequency classes.
Data for the corpus were collected from the
Cincinnati Children?s Hospital Medical Center?s
(CCHMC) Department of Radiology. CCHMC?s
Institutional Review Board approved release of the
data. Sampling of all outpatient chest x-ray and re-
nal procedures for a one-year period was done us-
ing a bootstrap method (Walters, 2004). These data
are among those most commonly used, and are de-
signed to provide enough codes to cover a substan-
tial proportion of pediatric radiology activity. Ex-
punging patient identity to meet HIPAA standards
included three steps: disambiguation, anonymiza-
tion, and data scrubbing (Pestian et al, 2005).
Ambiguity and Anonymization. Not surprisingly,
some degree of disambiguation is needed to carry
out effective anonymization (Uzuner et al, 2006;
Sibanda and Uzuner, 2006). The reason is that clini-
cal text is dense with medical jargon, abbreviations,
and acronyms, many of which turn out to be ambigu-
ous between a sense that needs anonymization and a
different sense that does not. For example, in a clin-
ical setting, FT can be an abbreviation for full-term,
fort (as in Fort Bragg), feet, foot, field test, full-time
or family therapy. Fort Bragg, being a place name,
and a possible component of an address, could indi-
rectly lead to identification of the patient. Until such
occurrences are disambiguated, it is not possible to
be certain that other steps to anonymize data are ad-
equate. To resolve the relevant ambiguities found in
this free text, we relied on previous efforts that used
expert input to develop clinical disambiguation rules
(Pestian et al, 2004).
Anonymization. To assure patient privacy, clin-
ical text that is used for non-clinical reasons must
be anonymized. However, to be maximally useful
for machine-learning, this must be done in a par-
ticular way. Replacing personal names with some
unspecific value such as ?*? would lose potentially
useful information. Our goal is to replace the sensi-
tive fields with like values that obscure the identity
of the individual (Cho et al, 2002). We found that
the amount of sensitive information routinely found
in unstructured free text data is limited. In our case,
these data included patient and physician names and
sometimes dates or geographic locations, but little or
no other sensitive information turned up in the rele-
vant database fields. Using our internally developed
encryption broker software, we replaced all female
names with ?Jane?, all male names with ?John?, and
all surnames with ?Johnson?. Dates were randomly
shifted.
Manual Inspection. Once the data were disam-
biguated and anonymized, they were manually re-
viewed for the presence of any Protected Health In-
formation (PHI). If a specific token was perceived to
potentially violate PHI regulations, the entire record
was deleted from the dataset. In some case, how-
ever, a general geographic area was changed and
not deleted. For example if the data read ?patient
lived near Mr. Roger?s neighborhood? it would be
deleted, because it may be traceable. On the other
hand, if the data read ?patient was from Cincinnati?
it may have been changed to read ?patient was from
the Covington? After this process, a corpus of 2,216
records was obtained (See Table 2 for details).
ICD-9-CM Assignment. A radiology report has
multiple components. Two parts in particular are
essential for the assignment of ICD-9-CM codes:
98
clinical history?provided by an ordering physician
before a radiological procedure, and impression?
reported by a radiologist after the procedure. In the
case of radiology reports, ICD-9-CM codes serve as
justification to have a certain procedure performed.
There are official guidelines for radiology ICD-9-
CM coding (Moisio, 2000). These guidelines note
that every disease code requires a minimum num-
ber of digits before reimbursement will occur; that
a definite diagnosis should always be coded when
possible; that an uncertain diagnosis should never
be coded; and that symptoms must never be coded
when a definite diagnosis is available. Particular
hospitals and insurance companies typically aug-
ment these principles with more specific internal
guidelines and practices for coding. For these rea-
sons of policy, and because of natural variation in
human judgment, it is not uncommon for multiple
annotators to assign different codes to the same text.
Understanding the sources of this variation is impor-
tant; so too is the need to create a definite gold stan-
dard for use in the challenge. To do so, data were
annotated by the coding staff of CCHMC and two
independent coding companies: COMPANY Y and
COMPANY Z.
Majority annotation. A single gold standard was
created from these three sets of annotations. There
was no reason to adopt any a priori preference for
one annotator over another, so the democratic princi-
ple of assigning a majority annotation was used. The
majority annotation consists of those codes assigned
to the document by two or more of the annotators.
There are, however, several possible problems with
this approach. For example, it could be that the ma-
jority annotation will be empty. This will be rare
(126 records out of 2,216 in our case), because it
only happens when the codes assigned by the three
annotators form disjoint sets. In most hospital sys-
tems, including our own, the coders are required to
indicate a primary code. By convention, the primary
code is listed as the record?s first code, and has an
especially strong impact on the billing process. For
simplicity?s sake, the majority annotation process ig-
nores the distinction between primary and secondary
codes. There is space for a better solution here, but
we have not seriously explored it. We have, how-
ever, conducted an analysis of agreement statistics
(not further discussed here) that suggests that the
overall effect of the majority method is to create a
coding that shares many statistical properties with
the originals, except that it reduces the effect of the
annotators? individual idiosyncrasies. The majority
annotation is illustrated in Table 1.
Our evaluation strategy makes the simplistic as-
sumption that the majority annotation is a true gold
standard and a worthwhile target for emulation. This
is debatable, and is discussed below, but for the sake
of definiteness we simply stipulate that submissions
will be compared against the majority annotation,
and that the best possible performance is to exactly
replicate said majority annotation.
3 Evaluation
Micro- and macro-averaging. Although we rank
systems for purposes of determining the top three
performers on the basis of micro-averaged F1, we
report a variety of performance data, including the
micro-average, macro-average, and a cost-sensitive
measure of loss. Jackson and Moulinier comment
(for general text classification) that: ?No agree-
ment has been reached...on whether one should pre-
fer micro- or macro-averages in reporting results.
Macro-averaging may be preferred if a classification
system is required to perform consistently across all
classes regardless of how densely populated these
are. On the other hand, micro-averaging may be
preferred if the density of a class reflects its impor-
tance in the end-user system? (Jackson and Moulin-
ier, 2002):160-161. For the present medical ap-
plication, we are more interested in the number of
patients whose cases are correctly documented and
billed than in ensuring good coverage over the full
range of diagnostic codes. We therefore emphasize
the micro-average.
A cost-sensitive accuracy measure. While F-
measure is well-established as a method for ranking,
there are reasons for wanting to augment this with
a cost-sensitive measure. An approach that allows
penalties for over-coding (a false positive) and
under-coding (a false negative) to be manipulated
has important implications. The penalty for under-
coding is simple?the hospital loses the amount of
revenue that it would have earned if it had assigned
the code. The regulations under which coding is
done enforce an automatic over-coding penalty of
99
Table 1: Majority Annotation
Hospital Company Y Company Z Majority
Document 1 AB BC AB AB
Document 2 BC ABD CDE BCD
Document 3 EF EF E EF
Document 4 ABEF ACEF CDEF ACEF
three times what is earned from the erroneous code,
with the additional risk of possible prosecution
for fraud. This motivates a generalized version of
Jaccard?s similarity metric (Gower and Legendre,
1986), which was introduced by Boutell, Shen, Luo
and Brown (Boutell et al, 2003).
Suppose that Yx is the set of correct labels for a test
set and Px is the set of labels predicted by some
participating system. Define Fx = Px ? Yx and
Mx = Yx ? Px , i.e. Fx is the set of false positives,
and Mx is the set of missed labels or false negatives.
The score is given by
score(Px) =
(
1?
?|Mx|+ ?|Fx|
|Yx ? Px|
)?
(1)
As noted in (Boutell et al, 2003), if ? = ? = 1 this
formula reduces to the simpler case of
score(Px) =
(
1?
|Yx ? Px|
|Yx ? Px|
)?
(2)
The discussion in (Boutell et al, 2003) points out
that constraints are necessary on ? and ? to ensure
that the inner term of the expression is non-negative.
We do not understand the way that they formulate
these constraints, but note that non-negativity will be
ensured if 0 ? ? ? 1 and 0 ? ? ? 1 . Since over-
coding is three times as bad as undercoding, we use
? = 1.0 , ? = 0.33 . Varying the value of ? would
affect the range of the scores, but does not alter the
rankings of individual systems. We therefore used
? = 1 . This measure does not represent the pos-
sibility of prosecution for fraud, because the costs
involved are incommensurate with the ones that are
represented. With these parameter settings, the cost-
sensitive measure produces rankings that differ con-
siderably from those produced by macro-averaged
balanced F-measure. For example, we shall see that
the system ranked third in the competition by macro-
averaged F-measure assigns a total of 1167 labels,
where the second-ranked assigns 1232, and the cost-
sensitive measure rewards this conservatism in as-
signing labels by reversing the ranking of the two
systems. In either case, the difference between the
systems is small (0.86% difference in F-measure,
0.53% difference in the cost-sensitive measure).
4 The Data
We selected for the challenge a subset of the com-
prehensive data set described above. The subset was
created by stratified sampling, such that it contains
20% of the documents in each category. Thus, the
proportion of categories in the sample is the same as
the proportion of categories in the full data set. We
included in the initial sample only those categories
to which 100 or more documents from the compre-
hensive data set were assigned. After the process
summarized in Table 2, the data were divided into
two partitions: a training set with 978 documents,
and a testing set with 976. Forty-five ICD-9-CM
labels (e.g 780.6) are observed in these data sets.
These labels form 94 distinct combinations (e.g. the
combination 780.6, 786.2). We required that any
combination have at least two exemplars in the data,
and we split each combination between the train-
ing and the test sets. So, there may be labels and
combinations of labels that occur only one time in
the training data, but participants can be sure that
no combination will occur in the test data that has
not previously occurred at least once in the train-
ing data. Our policy here has the unintended con-
sequence that any combination that appears exactly
once in the training data is highly likely to appear
exactly once in the test data. This gives unnecessary
information to the participants. In future challenges
we will drop the requirement for two occurrences in
the data, but ensure that single-occurrence combina-
tions are allocated to the training set rather than the
100
test set. This maintains the guarantee that there will
be no unseen combinations in the test data. The full
data set may be downloaded from the official chal-
lenge web-site.
5 Results
Notice of the Challenge was distributed using elec-
tronic mailing lists supplied by the Association of
Computational Linguistics, IEEE Computer Intelli-
gence and Data Mining, and American Medical In-
formatics Association?s Natural Language Process-
ing special interest group. Interested participants
were asked to register at the official challenge web-
site. Registration began February 1, 2007 and ended
February 28, 2007. Approximately 150 individu-
als registered from 22 countries and six continents.
Upon completing registration, an automated e-mail
was sent with the location of the training data. On
March 1, 2007 participants received notice of the
location of the testing data. Participants were en-
couraged to use the data for other purposes as long
as it was non-commercial and the appropriate cita-
tion was made. There were no other data use re-
strictions. Participants had until March 18, 2007
to submit their results and an explanation of their
model. Approximately 33% (50) of the partici-
pants submitted results. During the course of the
Challenge participants asked a range of questions.
These were posted to the official challenge web-site
- www.computationalmedicine.org/challenge.
The figure below is a scatterplot relating micro-
averaged F1 to the cost-sensitive measure described
above. Each point represents a system. The top-
performing systems achieved 0.8908, the minimum
was 0.1541, and the mean was 0.7670, with a SD
of 0.1340. There are 21 systems with a micro-
averaged F1 between 0.81 and 0.90. Another 14
have F1 > 0.70 . It is noticeable that the systems
are not ranked identically by the cost-sensitive and
the micro-averaged measure, but the differences are
small in each case.
A preliminary screening using a two-factor ANOVA
with system identity and diagnostic code as predic-
tive factors for balanced F-measure revealed a sig-
nificant main effect of both system and code. Pair-
wise t-tests using Holm?s correction for multiple
comparisons revealed no statistically significant dif-
Figure 1: Scatter plot of evaluation measures
ferences between the systems performing at F=0.70
or higher. Differences between the top system and a
system with a microaveraged F-measure of 0.66 do
come out significant on this measure.
We have also calculated (Table 3) the agreement
figures for the three individual annotations that
went into the majority gold standard. We see
that CCHMC outranks COMPANY Y on the cost-
sensitive measure, but the reverse is true for micro-
and macro-averaged F1, with the agreement be-
tween the hospital and the gold standard being espe-
cially low for the macro-averaged version. To under-
stand these figures, it is necessary to recall that the
gold standard is a majority annotation that is formed
from the the three component annotations. It appears
that for rare codes, which have a disproportionate
effect on the macro-averaged F, the majority anno-
tation is dominated by cases where company Y and
company Z assign the same code, one that CCHMC
did not assign.
The agreement figures are comparable to those of
the best automatic systems. If submitted to the
competition, the components of the majority anno-
tation would not have outranked the best systems,
even though the components contributed to the ma-
jority opinion. It is tempting to conclude that the
automated systems are close to human-level perfor-
mance. Recall, however, that while the hospital and
the companies did not have the luxury of exposure
to the majority annotation, the systems did have that
access, which allowed them to explicitly model the
properties of that majority annotation. A more mod-
erate conclusion is that the hospital and the compa-
nies might be able to improve (or at least adjust)
their annotation practices by studying the majority
101
Table 2: Characteristics of the data set through the development process.
Step Removed Total documents
One-year collection of documents 20,275
20 percent sample of one-year collection 4,055
Manual inspection for anonymization problems 1,839 2,216
Removal of records with no majority code 126 2,090
Removal of records with a code occurring only once 136 1,954
Table 3: Comparison of human annotators against majority.
Annotator Cost-sensitive Micro-averaged F1 Macro-averaged F1
HOSPITAL 0.9056 0.8264 0.6124
COMPANY Y 0.8997 0.8963 0.8973
COMPANY Z 0.8621 0.8454 0.8829
annotation and adapting as appropriate.
6 Discussion
Compared to other recent text classification shared
tasks in the biomedical domain (Uzuner et al, 2006;
Hersh et al, 2004; Hersh et al, 2005), this task re-
quired categorization with respect to a set of labels
more than an order of magnitude larger than previ-
ous evaluations. This increase in the size of the set
of labels is an important step forward for the field?
systems that perform well on smaller sets of cate-
gories do not necessarily perform well with larger
sets of categories (Jackson and Moulinier, 2002), so
the data set will allow for more thorough text cat-
egorization system evaluations than have been pos-
sible in the past. Another important contribution of
the work reported here may be the distribution of
the data?the first fully distributable, freely usable
data set of clinical text. The high number of partici-
pants and final submissions was a pleasant surprise;
we attribute this, among other things, to the fact that
this was an applied challenge, that real data were
supplied, and that participants were free to use these
data in other venues.
Participants utilized a diverse range of approaches.
These system descriptions are based on brief com-
ments entered into the submission box, and are ob-
viously subject to revision. The three highest scor-
ers all mentioned ?negation,? all seemed to be us-
ing the structure of UMLS in a serious way. The
better systems frequently mentioned ?hypernyms?
or ?synonyms,? and were apparently doing signifi-
cant amounts of symbolic processing. Two of the
top three had machine-learning components, while
one of the top three used purely symbolic methods.
The most common approach seems to be thought-
ful and medically-informed feature engineering fol-
lowed by some variety of machine learning. The
top-performing system used C4.5, suggesting that
use of the latest algorithms is not a pre-requisite for
success. SVMs and related large-margin approaches
to machine learning were strongly represented, but
did not seem to be reliably predictive of high rank-
ing.
6.1 Observations on running the task and the
evaluation
The most frequently viewed question of the FAQ
was related to a script to calculate the evaluation
score. This was supplied both as a downloadable
script and as an interactive web-page with a form for
submission. In retrospect, we realize that we had not
fully thought through what would happen as people
began to use this script. If we run a similar contest
in the future, we will be better prepared for the con-
fusion that this can cause.
A novel aspect of this task was that although we only
scored a single run on the test data, we allowed par-
ticipants to submit their ?final? run up to 10 times,
and to see their score each time. Note that although
102
participants could see how their score varied on suc-
cessive submissions, they did not have access to the
actual test data or to the correct answers, and so there
were no opportunities for special-purpose hacks to
handle special cases in the test data. The average
participant tried 5.27 (SD 3.17) submissions against
the test data. About halfway through the submis-
sion period we began to realize that in a competi-
tive situation, there are risks in providing the type
of feedback given on the submission form. In fu-
ture challenges, we will be judicious in selecting the
number of attempts allowed and the provision of any
type of feedback. As far as we can tell our general
assumption that the scientific integrity of the partic-
ipants was greater than the need to game the system
is true. It is good policy for those administering the
contest, however, to keep temptations to a minimum.
Our current preference would be to provide only the
web-page interface with no more than five attempts,
and to tell participants only whether their submis-
sion had been accepted, and if so, how many items
and how many codes were recognized.
We provided an XML schema as a precise and pub-
licly visible description of the submission format.
Although we should not have been, we were sur-
prised when changes to the schema were required
in order to accommodate small but unexpected vari-
ations in participant submissions. An even simpler
submission format would have been good. The ad-
vantage of the approach that we took was that XML
validation gave us a degree of sanity-checking at lit-
tle cost. The disadvantage was that some of the nec-
essary sanity-checking went beyond what we could
see how to do in a schema.
The fact that numerous participants generated sys-
tems with high performance indicates that the task
was reasonable, and that sufficient information
about the coding task was either provided by us or
inferred by the participants to allow them to do their
work. Since this is a first attempt, it is not yet clear
what the upper limits on performance are for this
task, but preliminary indications are that automated
systems are or will soon be viable as a component of
deployed systems for this kind of application.
7 Acknowledgements
The authors thank Aaron Cohen of the Oregon
Health and Science University for observations on
the inter-rater agreement between the three sources
and its relationship to the majority assignments, and
also for his input on testing for statistically signif-
icant differences between systems. We also thank
PERSON of ORGANIZATION for helpful com-
ments on the manuscript. Most importantly we
thank all the participants for their on-going commit-
ment, professional feedback and scientific integrity.
References
[Boutell et al, 2003] Boutell M., Shen X., Luo J. and
Brown C. 2003. Multi-label Semantic Scene Clas-
sification, Technical Report 813. Department of Com-
puter Science, University of Rochester September.
[Cho et al, 2002] Cho P. S., Taira R. K., and Kangarloo
H. 2002 Text boundary detection of medical reports.
Proceedings of the Annual Symposium of the American
Medical Informatics Association, 998.
[Friedman et al, 2002] Friedman C., Kra P., and Rzhetsky
A. 2002. Two biomedical sublanguages: a descrip-
tion based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35:222?235.
[Gower and Legendre, 1986] Gower J. C. and Legendre P.
1986. Metric and euclidean properties of dissimilarity
coefficient. Journal of Classification, 3:5?48.
[Hersh et al, 2004] Hersh W., Bhupatiraju R. T., Ross L.,
Roberts P., Cohen A. M., and Kraemer D. F. 2004.
TREC 2004 Genomics track overview. Proceedings of
the 13th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hersh et al, 2006] Hersh W., Cohen A. M., Roberts P.,
and Rekapalli H. K. 2006. TREC 2006 Genomics
track overview. Proceedings of the 15th Annual Text
Retrieval Conference National Institute of Standards
and Technology.
[Hersh et al, 2005] Hersh W., Cohen A. M., Yang J.,
Bhupatiraju R. T., Roberts P., and Hearst M. 2005.
TREC 2005 Genomics track overview. Proceedings of
the 14th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hirschman and Blaschke, 2006] Hirschman L. and
Blaschke C. 2006. Evaluation of text mining in
biology. Text mining for biology and biomedicine,
Chapter 9. Ananiadou S. and McNaught J., editors.
Artech House.
103
[Hirschman and Sager, 1982] Hirschman L. and Sager S.
1982. Automatic information formatting of a medi-
cal sublanguage. Sublanguage: studies of language in
restricted semantic domains, Chapter 2. Kittredge R.
and Lehrberger J., editors. Walter de Gruyter.
[Hurtado et al, 2001] Hurtado M. P, Swift E. K., and Cor-
rigan J. M. 2001. Crossing the Quality Chasm: A
New Health System for the 21st Century. Institute of
Medicine, National Academy of Sciences.
[Jackson and Moulinier, 2002] Jackson P. and Moulinier
I. 2002. Natural language processing for online appli-
cations: text retrieval, extraction, and categorization.
John Benjamins Publishing Co.
[Lang, 2007] Lang, D. 2007. CONSULTANT REPORT
- Natural Language Processing in the Health Care In-
dustry. Cincinnati Children?s Hospital Medical Cen-
ter, Winter 2007.
[Moisio, 2000] Moisio M. 2000. A Guide to Health Care
Insurance Billing. Thomson Delmar Learning, Clifton
Park.
[Pestian et al, 2005] Pestian J. P., Itert L., Andersen C. L.,
and Duch W. 2005. Preparing Clinical Text for Use in
Biomedical Research. Journal of Database Manage-
ment, 17(2):1-12.
[Pestian et al, 2004] Pestian J. P., Itert L., and Duch W.
2004. Development of a Pediatric Text-Corpus for
Part-of-Speech Tagging. Intelligent Information Pro-
cessing and Web Mining, Advances in Soft Computing,
219?226 New York, Springer Verlag.
[Sammuelsson and Wiren, 2000] Sammuelsson C. and
Wiren M. 2000. Parsing Techniques. Handbook of
Natural Language Processing, 59?93. Dale R., Moisl
H., Somers H., editors. New York, Marcel Deker.
[Sibanda and Uzuner, 2006] Sibanda T. and Uzuner O.
2006. Role of local context in automatic deidentifica-
tion of ungrammatical, fragmented text. Proceedings
of the Human Language Technology conference of the
North American chapter of the Association for Com-
putational Linguistics, 65?73.
[Stetson et al, 2002] Stetson P. D., Johnson S. B., Scotch
M., and Hripcsak G. 2002. The sublanguage of cross-
coverage. Proceedings of the Annual Symposium of
the American Medical Informatics Association, 742?
746.
[U.S. Health, 2002] U.S. Heath & Human Services.
2002. 45 CFR Parts 160 and 164 Standards for Privacy
of Individually Identifiable Health Information Final
Rule Federal Register, 67(157):53181?53273.
[Uzuner et al, 2006] Uzuner O., Szolovits P., and Kohane
I. 2006. i2b2 workshop on natural language process-
ing challenges for clinical records. Proceedings of the
Fall Symposium of the American Medical Informatics
Association.
[Walters, 2004] Walters S. J. 2004. Sample size and
power estimation for studies with health related quality
of life outcomes: a comparison of four methods using
the SF-36 Health and Quality of Life Outcomes, 2:26.
104
Proceedings of the Workshop on BioNLP: Shared Task, pages 50?58,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
High-precision biological event extraction with a concept recognizer
K. Bretonnel Cohen?, Karin Verspoor?, Helen L. Johnson, Chris Roeder,
Philip V. Ogren, William A. Baumgartner Jr., Elizabeth White, Hannah Tipney, and Lawrence Hunter
Center for Computational Pharmacology
University of Colorado Denver School of Medicine
PO Box 6511, MS 8303, Aurora, CO 80045 USA
kevin.cohen@gmail.com, karin.verspoor@ucdenver.edu, helen.linguist@gmail.com,
chris.roeder@ucdenver.edu, philip@ogren.info, william.baumgartner@ucdenver.edu,
elizabeth.white@colorado.edu, hannah.tipney@ucdenver.edu, larry.hunter@ucdenver.edu
Abstract
We approached the problems of event detec-
tion, argument identification, and negation and
speculation detection as one of concept recog-
nition and analysis. Our methodology in-
volved using the OpenDMAP semantic parser
with manually-written rules. We achieved
state-of-the-art precision for two of the three
tasks, scoring the highest of 24 teams at pre-
cision of 71.81 on Task 1 and the highest of 6
teams at precision of 70.97 on Task 2.
The OpenDMAP system and the rule set are
available at bionlp.sourceforge.net.
*These two authors contributed equally to the
paper.
1 Introduction
We approached the problem of biomedical event
recognition as one of concept recognition and anal-
ysis. Concept analysis is the process of taking a
textual input and building from it an abstract rep-
resentation of the concepts that are reflected in it.
Concept recognition can be equivalent to the named
entity recognition task when it is limited to locat-
ing mentions of particular semantic types in text, or
it can be more abstract when it is focused on recog-
nizing predicative relationships, e.g. events and their
participants.
2 BioNLP?09 Shared Task
Our system was entered into all three of the
BioNLP?09 (Kim et al, 2009) shared tasks:
? Event detection and characterization This
task requires recognition of 9 basic biological
events: gene expression, transcription, protein
catabolism, protein localization, binding, phos-
phorylation, regulation, positive regulation and
negative regulation. It requires identification
of the core THEME and/or CAUSE participants
in the event, i.e. the protein(s) being produced,
broken down, bound, regulated, etc.
? Event argument recognition This task builds
on the previous task, adding in additional argu-
ments of the events, such as the site (protein or
DNA region) of a binding event, or the location
of a protein in a localization event.
? Recognition of negations and speculations
This task requires identification of negations of
events (e.g. event X did not occur), and specu-
lation about events (e.g. We claim that event X
should occur).
3 Our approach
We used the OpenDMAP system developed at the
University of Colorado School of Medicine (Hunter
et al, 2008) for our submission to the BioNLP
?09 Shared Task on Event Extraction. OpenDMAP
is an ontology-driven, integrated concept analysis
system that supports information extraction from
text through the use of patterns represented in a
classic form of ?semantic grammar,? freely mixing
text literals, semantically typed basal syntactic con-
stituents, and semantically defined classes of enti-
ties. Our approach is to take advantage of the high
50
quality ontologies available in the biomedical do-
main to formally define entities, events, and con-
straints on slots within events and to develop pat-
terns for how concepts can be expressed in text that
take advantage of both semantic and linguistic char-
acteristics of the text. We manually built patterns for
each event type by examining the training data and
by using native speaker intuitions about likely ways
of expressing relationships, similar to the technique
described in (Cohen et al, 2004). The patterns char-
acterize the linguistic expression of that event and
identify the arguments (participants) of the events
according to (a) occurrence in a relevant linguistic
context and (b) satisfaction of appropriate semantic
constraints, as defined by our ontology. Our solution
results in very high precision information extraction,
although the current rule set has limited recall.
3.1 The reference ontology
The central organizing structure of an OpenDMAP
project is an ontology. We built the ontology
for this project by combining elements of several
community-consensus ontologies?the Gene Ontol-
ogy (GO), Cell Type Ontology (CTO), BRENDA
Tissue Ontology (BTO), Foundational Model of
Anatomy (FMA), Cell Cycle Ontology (CCO), and
Sequence Ontology (SO)?and a small number of
additional concepts to represent task-specific aspects
of the system, such as event trigger words. Combin-
ing the ontologies was done with the Prompt plug-in
for Prote?ge?.
The ontology included concepts representing each
event type. These were represented as frames, with
slots for the various things that needed to be re-
turned by the system?the trigger word and the var-
ious slot fillers. All slot fillers were constrained to
be concepts in some community-consensus ontol-
ogy. The core event arguments were constrained in
the ontology to be of type protein from the Sequence
Ontology (except in the case of regulation events,
where biological events themselves could satisfy the
THEME role), while the type of the other event argu-
ments varied. For instance, the ATLOC argument
of a gene expression event was constrained to be
one of tissue (from BTO), cell type (from CTO), or
cellular component (from GO-Cellular Component),
while the BINDING argument of a binding event was
constrained to be one of binding site, DNA, domain,
or chromosome (all from the SO and all tagged by
LingPipe). Table 1 lists the various types.
3.2 Named entity recognition
For proteins, we used the gold standard annota-
tions provided by the organizers. For other seman-
tic classes, we constructed a compound named en-
tity recognition system which consists of a LingPipe
GENIA tagging module (LingPipe, (Alias-i, 2008)),
and several dictionary look-up modules. The dictio-
nary lookup was done using a component from the
UIMA (IBM, 2009; Ferrucci and Lally, 2004) sand-
box called the ConceptMapper.
We loaded the ConceptMapper with dictionar-
ies derived from several ontologies, including the
Gene Ontology Cellular Component branch, Cell
Type Ontology, BRENDA Tissue Ontology, and
the Sequence Ontology. The dictionaries contained
the names and name variants for each concept in
each ontology, and matches in the input documents
were annotated with the relevant concept ID for the
match. The only modifications that we made to
these community-consensus ontologies were to re-
move the single concept cell from the Cell Type On-
tology and to add the synonym nuclear to the Gene
Ontology Cell Component concept nucleus.
The protein annotations were used to constrain the
text entities that could satisfy the THEME role in the
events of interest. The other named entities were
added for the identification of non-core event partic-
ipants for Task 2.
3.3 Pattern development strategies
3.3.1 Corpus analysis
Using a tool that we developed for visualizing the
training data (described below), a subset of the gold-
standard annotations were grouped by event type
and by trigger word type (nominalization, passive
verb, active verb, or multiword phrase). This orga-
nization helped to suggest the argument structures of
the event predicates and also highlighted the varia-
tion within argument structures. It also showed the
nature of more extensive intervening text that would
need to be handled for the patterns to achieve higher
recall.
Based on this corpus analysis, patterns were de-
veloped manually using an iterative process in which
individual patterns or groups of patterns were tested
51
Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model
of Anatomy, other ontologies identified in the text.
Event Type Site AtLoc ToLoc
binding protein domain (SO),
binding site (SO), DNA
(SO), chromosome (SO)
gene expression gene (SO), biological
entity (CCO)
tissue (BTO), cell type
(CTO), cellular compo-
nent (GO)
localization cellular component
(GO)
cellular component
(GO)
phosphorylation amino acid (FMA),
polypeptide region (SO)
protein catabolism cellular component
(GO)
transcription gene (SO), biological
entity (CCO)
on the training data to determine their impact on per-
formance. Pattern writers started with the most fre-
quent trigger words and argument structures.
3.3.2 Trigger words
In the training data, we were provided annotations
of all relevant event types occurring in the training
documents. These annotations included a trigger
word specifying the specific word in the input text
which indicated the occurrence of each event. We
utilized the trigger words in the training set as an-
chors for our linguistic patterns. We built patterns
around the generic concept of, e.g. an expression
trigger word and then varied the actual strings that
were allowed to satisfy that concept. We then ran ex-
periments with our patterns and these varying sets of
trigger words for each event type, discarding those
that degraded system performance when evaluated
with respect to the gold standard annotations.
Most often a trigger word was removed from an
event type trigger list because it was also a trig-
ger word for another event type and therefore re-
duced performance by increasing the false positive
rate. For example, the trigger words ?level? and
?levels? appear in the training data trigger word lists
of gene expression, transcription, and all three regu-
lation event types.
The selection of trigger words was guided by a
frequency analysis of the trigger words provided in
the task training data. In a post-hoc analysis, we find
that a different proportion of the set of trigger words
was finally chosen for each different event type. Be-
tween 10-20% of the top frequency-ranked trigger
words were used for simple event types, with the
exception that phosphorylation trigger words were
chosen from the top 30%. For instance, for gene ex-
pression all of the top 15 most frequent trigger words
were used (corresponding to the top 16%). For com-
plex event types (the regulations) better performance
was achieved by limiting the list to between 5-10%
of the most frequent trigger words.
In addition, variants of frequent trigger words
were included. For instance, the nominalization ?ex-
pression? is the most frequent gene expression trig-
ger word and the verbal inflections ?expressed? and
?express? are also in the top 20%. The verbal inflec-
tion ?expresses? is ranked lower than the top 30%,
but was nonetheless included as a trigger word in the
gene expression patterns.
3.3.3 Patterns
As in our previous publications on OpenDMAP,
we refer to our semantic rules as patterns. For
this task, each pattern has at a minimum an event
argument THEME and an event-specific trigger
word. For example, {phosphorylation} :=
52
[phosphorylation nominalization][Theme],
where [phosphorylization nominalization]
represents a trigger word. Both elements are defined
semantically. Event THEMEs are constrained by
restrictions placed on them in the ontology, as
described above.
The methodology for creating complex event pat-
terns such as regulation was the same as for sim-
ple events, with the exception that the THEMEs
were defined in the ontology to also include bio-
logical processes. Iterative pattern writing and test-
ing was a little more arduous because these pat-
terns relied on the success of the simple event pat-
terns, and hence more in-depth analysis was re-
quired to perform performance-increasing pattern
adjustments. For further details on the pattern lan-
guage, the reader is referred to (Hunter et al, 2008).
3.3.4 Nominalizations
Nominalizations were very frequent in the train-
ing data; for seven out of nine event types, the most
common trigger word was a nominalization. In writ-
ing our grammars, we focused on these nominaliza-
tions. To write grammars for nominalizations, we
capitalized on some of the insights from (Cohen et
al., 2008). Non-ellided (or otherwise absent) argu-
ments of nominalizations can occur in three basic
positions:
? Within the noun phrase, after the nominaliza-
tion, typically in a prepositional phrase
? Within the noun phrase, immediately preceding
the nominalization
? External to the noun phrase
The first of these is the most straightforward to
handle in a rule-based approach. This is particu-
larly true in the case of a task definition like that
of BioNLP ?09, which focused on themes, since an
examination of the training data showed that when
themes were post-nominal in a prepositional phrase,
then that phrase was most commonly headed by of.
The second of these is somewhat more challeng-
ing. This is because both agents and themes can
occur immediately before the nominalization, e.g.
phenobarbital induction (induction by phenobarbi-
tal) and trkA expression (expression of trkA). To de-
cide how to handle pre-nominal arguments, we made
use of the data on semantic roles and syntactic posi-
tion found in (Cohen et al, 2008). That study found
that themes outnumbered agents in the prenominal
position by a ratio of 2.5 to 1. Based on this obser-
vation, we assigned pre-nominal arguments to the
theme role.
Noun-phrase-external arguments are the most
challenging, both for automatic processing and for
human interpreters; one of the major problems is
to differentiate between situations where they are
present but outside of the noun phrase, and situations
where they are entirely absent. Since the current im-
plementation of OpenDMAP does not have robust
access to syntactic structure, our only recourse for
handling these arguments was through wildcards,
and since they mostly decreased precision without a
corresponding increase in recall, we did not attempt
to capture them.
3.3.5 Negation and speculation
Corpus analysis of the training set revealed two
broad categories each for negation and speculation
modifications, all of which can be described in terms
of the scope of modification.
Negation
Broadly speaking, an event itself can be negated
or some aspect of an event can be negated. In other
words, the scope of a negation modification can be
over the existence of an event (first example below),
or over an argument of an existing event (second ex-
ample).
? This failure to degrade IkappaBalpha ...
(PMID 10087185)
? AP-1 but not NF-IL-6 DNA binding activity ...
(PMID 10233875)
Patterns were written to handle both types of
negation. The negation phrases ?but not? and ?but
neither? were appended to event patterns to catch
those events that were negated as a result of a
negated argument. For event negation, a more ex-
tensive list of trigger words was used that included
verbal phrases such as ?failure to? and ?absence of.?
The search for negated events was conducted in
two passes. Events for which negation cues fall out-
side the span of text that stretches from argument to
53
event trigger word were handled concurrently with
the search for events. A second search was con-
ducted on extracted events for negation cues that fell
within the argument to event trigger word span, such
as
. . . IL-2 does not induce I kappa B alpha degrada-
tion (PMID 10092783)
This second pass allowed us to capture one addi-
tional negation (6 rather than 5) on the test data.
Speculation
The two types of speculation in the training data
can be described by the distinction between ?de re?
and ?de dicto? assertions. The ?de dicto? assertions
of speculation in the training data are modifications
that call into question the degree of known truth of
an event, as in
. . . CTLA-4 ligation did not appear to affect the
CD28 - mediated stabilization (PMID 10029815)
The ?de re? speculation address the potential ex-
istence of an event rather that its degree of truth. In
these cases, the event is often being introduced in
text by a statement of intention to study the event, as
in
. . . we investigated CTCF expression
. . . [10037138]
To address these distinct types of speculation, two
sets of trigger words were developed. One set con-
sisted largely of verbs denoting research activities,
e.g. research, study, examine investigate, etc. The
other set consisted of verbs and adverbs that denote
uncertainty, and included trigger words such as sug-
gests, unknown, and seems.
3.4 Handling of coordination
Coordination was handled using the OpenNLP con-
stituent parser along with the UIMA wrappers that
they provide via their code repository. We chose
OpenNLP because it is easy to train a model, it in-
tegrates easily into a UIMA pipeline, and because
of competitive parsing results as reported by Buyko
(Buyko et al, 2006). The parser was trained using
500 abstracts from the beta version of the GENIA
treebank and 10 full-text articles from the CRAFT
corpus (Verspoor et al, In press). From the con-
stituent parse we extracted coordination structures
into a simplified data structure that captures each
conjunction along with its conjuncts. These were
provided to downstream components. The coordi-
nation component achieves an F-score of 74.6% at
the token level and an F-score of 57.5% at the con-
junct level when evaluated against GENIA. For both
measures the recall was higher than the precision by
4% and 8%, respectively.
We utilized the coordination analysis to identify
events in which the THEME argument was expressed
as a conjoined noun phrase. These were assumed to
have a distributed reading and were post-processed
to create an individual event involving each con-
junct, and further filtered to only include given (A1)
protein references. So, for instance, analysis of the
sentence in the example below should result in the
detection of three separate gene expression events,
involving the proteins HLA-DR, CD86, and CD40,
respectively.
NAC was shown to down-regulate the
production of cytokines by DC as well
as their surface expression of HLA-
DR, CD86 (B7-2), and CD40 molecules
. . . (PMID 10072497)
3.5 Software infrastructure
We took advantage of our existing infrastructure
based on UIMA (The Unstructured Information
Management Architecture) (IBM, 2009; Ferrucci
and Lally, 2004) to support text processing and data
analysis.
3.5.1 Development tools
We developed a visualization tool to enable the
linguistic pattern writers to better analyze the train-
ing data. This tool shows the source text one sen-
tence at a time with the annotated words highlighted.
A list following each sentence shows details of the
annotations.
3.6 Errors in the training data
In some cases, there were discrepancies between the
training data and the official problem definitions.
This was a source of problems in the pattern devel-
opment phase. For example, phosphorylation events
are defined in the task definition as having only a
THEME and a SITE. However, there were instances
in the training data that included both a THEME and
a CAUSE argument. When those events were identi-
fied by our system and the CAUSE was labelled, they
54
were rejected during a syntactic error check by the
test server.
4 Results
4.1 Official Results
We are listed as Team 13. Table 2 shows our re-
sults on the official metrics. Our precision was the
highest achieved by any group for Task 1 and Task
2, at 71.81 for Task 1 and 70.97 for task 2. Our re-
calls were much lower and adversely impacted our
F-measure; ranked by F-measure, we ranked 19th
out of 24 groups.
We noted that our results for the exact match met-
ric and for the approximate match metric were very
close, suggesting that our techniques for named en-
tity recognition and for recognizing trigger words
are doing a good job of capturing the appropriate
spans.
4.2 Other analysis: Bug fixes and coordination
handling
In addition to our official results, we also report in
Table 3 (see last page) the results of a run in which
we fixed a number of bugs. This represents our cur-
rent best estimate of our performance. The precision
drops from 71.81 for Task 1 to 67.19, and from 70.97
for Task 2 to 65.74, but these precisions are still
well above the second-highest precisions of 62.21
for Task 1 and 56.87 for Task 2. As the table shows,
we had corresponding small increases in our recall
to 17.38 and in our F-measure to 27.62 for Task 1,
and in our recall to 17.07 and F-measure to 27.10 for
Task 2.
We evaluated the effects of coordination handling
by doing separate runs with and without this ele-
ment of the processing pipeline. Compared to our
unofficial results, which had an overall F-measure
for Task 1 of 27.62 and for Task 2 of 27.10, a ver-
sion of the system without handling of coordination
had an overall F-measure for Task 1 of 24.72 and for
Task 2 of 24.21.
4.3 Error Analysis
4.3.1 False negatives
To better understand the causes of our low recall,
we performed a detailed error analysis of false neg-
atives using the devtest data. (Note that this section
includes a very small number of examples from the
devtest data.) We found five major causes of false
negatives:
? Intervening material between trigger words and
arguments
? Coordination that was not handled by our coor-
dination component
? Low coverage of trigger words
? Anaphora and coreference
? Appositive gene names and symbols
Intervening material For reasons that we detail
in the Discussion section, we avoided the use of
wildcards. This, and the lack of syntactic analy-
sis in the version of the system that we used (note
that syntactic analyses can be incorporated into an
OpenDMAP workflow), meant that if there was text
intervening between a trigger word and an argument,
e.g. in to efficiently [express] in developing thymo-
cytes a mutant form of the [NF-kappa B inhibitor]
(PMID 10092801), where the bracketed text is the
trigger word and the argument, our pattern would
not match.
Unhandled coordination Our coordination system
only handled coordinated protein names. Thus, in
cases where other important elements of the utter-
ance, such as the trigger word transcription in tran-
scription and subsequent synthesis and secretion
of galectin-3 (PMID 8623933) were in coordinated
structures, we missed the relevant event arguments.
Low coverage of trigger words As we discuss in
the Methods section, we did not attempt to cover
all trigger words, in part because some less-frequent
trigger words were involved in multiple event types,
in part because some of them were extremely low-
frequency and we did not want to overfit to the train-
ing data, and in part due to the time constraints of the
shared task.
Anaphora and coreference Recognition of some
events in the data would require the ability to do
anaphora and coreference resolution. For example,
in Although 2 early lytic transcripts, [BZLF1] and
[BHRF1], were also detected in 13 and 10 cases,
respectively, the lack of ZEBRA staining in any case
indicates that these lytic transcripts are most likely
55
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (18) 18 (18) 10.34 100.00 18.75 9.77 94.44 17.71
Binding 347 (44) 110 (44) 12.68 40.00 19.26 12.32 39.09 18.74
Gene expression 722 (263) 306 (263) 36.43 85.95 51.17 36.43 85.95 51.17
Transcription 137 (18) 20 (18) 13.14 90.00 22.93 13.14 90.00 22.93
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (30) 30 (30) 22.22 100.00 36.36 20.14 93.33 33.14
EVENT TOTAL 1529 (377) 490 (377) 24.66 76.94 37.35 24.30 76.12 36.84
Regulation 291 (9) 19 (9) 3.09 47.37 5.81 3.08 47.37 5.79
Positive regulation 983 (32) 65 (32) 3.26 49.23 6.11 3.24 49.23 6.08
Negative regulation 379 (10) 22 (10) 2.64 45.45 4.99 2.37 40.91 4.49
REGULATION TOTAL 1653 (51) 106 (51) 3.09 48.11 5.80 3.02 47.17 5.67
Negation 227 (4) 76 (4) 1.76 5.26 2.64
Speculation 208 (14) 105 (14) 6.73 13.33 8.95
MODIFICATION TOTAL 435 (18) 181 (18) 4.14 9.94 5.84
ALL TOTAL 3182 (428) 596 (428) 13.45 71.81 22.66 13.25 70.97 22.33
Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate span
matching/approximate recursive matching table. GS = gold standard (true positives) (given for Tasks 1/3 only), answer
= all responses (true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure. All results are as
calculated by the official scoring application.
[expressed] by rare cells in the biopsies entering
lytic cycle (PMID 8903467), where the bracketed
text is the arguments and the trigger word, the syn-
tactic object of the verb is the anaphoric noun phrase
these lytic transcripts, so even with the addition of
a syntactic component to our system, we still would
not have recognized the appropriate arguments with-
out the ability to do anaphora resolution.
Appositives The annotation guidelines for proteins
apparently specified that when a gene name was
present in an appositive with its symbol, the symbol
was selected as the gold-standard argument. For this
reason, in examples like [expression] of Fas ligand
[FasL] (PMID 10092076), where the bracketed text
is the trigger word and the argument, the gene name
constituted intervening material from the perspec-
tive of our patterns, which therefore did not match.
We return to a discussion of recall and its implica-
tions for systems like ours in the Discussion section.
4.3.2 False positives
Although our overall rate of false positives was
low, we sampled 45 false positive events distributed
across the nine event types and reviewed them with
a biologist.
We noted two main causes of error. The most
common was that we misidentified a slot filler or
were missing a slot filler completely for an actual
event. The other main reason for false positives was
when we erroneously identified a (non)event. For
example, in coexpression of NF-kappa B/Rel and
Sp1 transcription factors (PMID 7479915), we mis-
takenly identified Sp1 transcription as an event.
5 Discussion
Our results demonstrate that it is possible to achieve
state-of-the art precision over a broad range of tasks
and event types using our approach of manually
constructed, ontologically typed rules?our preci-
sion of 71.81 on Task 1 was ten points higher than
the second-highest precision (62.21), and our preci-
sion of 70.97 on Task 2 was 14 points higher than
the second-highest precision (56.87). It remains the
case that our recall was low enough to drop our F-
measure considerably. Will it be the case that a sys-
tem like ours can scale to practical performance lev-
els nonetheless? Four factors suggest that it can.
The first is that there is considerable redundancy
in the data; although we have not quantified it for
this data set, we note that the same event is often
56
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (33) 41 (33) 18.97 80.49 30.70 16.67 69.05 26.85
Binding 347 (62) 152 (62) 17.87 40.79 24.85 17.48 40.13 24.35
Gene expression 722 (290) 344 (290) 40.17 84.30 54.41 40.17 84.30 54.41
Transcription 137 (28) 31 (28) 20.44 90.32 33.33 20.44 90.32 33.33
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (47) 48 (47) 34.81 97.92 51.37 32.37 84.91 46.88
EVENT TOTAL 1529 (464) 622 (464) 30.35 74.60 43.14 29.77 72.77 42.26
Regulation 291 (11) 31 (11) 3.78 35.48 6.83 3.77 35.48 6.81
Positive regulation 983 (60) 129 (60) 6.10 46.51 10.79 6.08 46.51 10.75
Negative regulation 379 (18) 41 (18) 4.75 43.90 8.57 4.49 41.46 8.10
REGULATION TOTAL 1653 (89) 201 (89) 5.38 44.28 9.60 5.31 43.78 9.47
Negation 227 (6) 129 (6) 2.64 4.65 3.37
Speculation 208 (25) 165 (25) 12.02 15.15 13.40
MODIFICATION TOTAL 435 (31) 294 (31) 7.13 10.54 8.50
ALL TOTAL 3182 (553) 823 (553) 17.38 67.19 27.62 17.07 65.74 27.10
Table 3: Updated results on test data for Tasks 1-3, with important bug fixes in the code base. See key above.
mentioned repeatedly, but for knowledge base build-
ing and other uses of the extracted information, it is
only strictly necessary to recognize an event once
(although multiple recognition of the same assertion
may increase our confidence in its correctness).
The second is that there is often redundancy
across the literature; the best-supported assertions
will be reported as initial findings and then repeated
as background information.
The third is that these recall results reflect an ap-
proach that made no use of syntactic analysis be-
yond handling coordination. There is often text
present in the input that cannot be disregarded with-
out either using wildcards, which generally de-
creased precision in our experiments and which
we generally eschewed, or making use of syntac-
tic information to isolate phrasal heads. Syntactic
analysis, particularly when combined with analysis
of predicate-argument structure, has recently been
shown to be an effective tool in biomedical infor-
mation extraction (Miyao et al, 2009). There is
broad need for this?for example, of the thirty lo-
calization events in the training data whose trigger
word was translocation, a full eighteen had inter-
vening textual material that made it impossible for
simple patterns like translocationof [Theme] or
[ToLoc]translocation to match.
Finally, our recall numbers reflect a very short de-
velopment cycle, with as few as four patterns writ-
ten for many event types. A less time-constrained
pattern-writing effort would almost certainly result
in increased recall.
Acknowledgments
We gratefully acknowledge Mike Bada?s help in
loading the Sequence Ontology into Prote?ge?.
This work was supported by NIH
grants R01LM009254, R01GM083649, and
R01LM008111 to Lawrence Hunter and
T15LM009451 to Philip Ogren.
References
Alias-i. 2008. LingPipe 3.1.2.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically mapping an NLP
core engine to the biology domain. In Proceedings
of the ISMB 2006 joint BioLINK/Bio-Ontologies meet-
ing.
K. B. Cohen, L. Tanabe, S. Kinoshita, and L. Hunter.
2004. A resource for constructing customized test
suites for molecular biology entity identification sys-
tems. BioLINK 2004, pages 1?8.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9).
57
D. Ferrucci and A. Lally. 2004. Building an example
application with the unstructured information manage-
ment architecture. IBM Systems Journal, 43(3):455?
475, July.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
IBM. 2009. UIMA Java framework. http://uima-
framework.sourceforge.net/.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein-protein
interaction extraction. Bioinformatics, 25(3):394?400.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. In press. The textual characteristics of tradi-
tional and Open Access scientific journals are similar.
BMC Bioinformatics.
58
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 23?30,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Software testing and the naturally occurring data
assumption in natural language processing?
K. Bretonnel Cohen William A. Baumgartner, Jr. Lawrence Hunter
Abstract
It is a widely accepted belief in natural lan-
guage processing research that naturally oc-
curring data is the best (and perhaps the only
appropriate) data for testing text mining sys-
tems. This paper compares code coverage us-
ing a suite of functional tests and using a large
corpus and finds that higher class, line, and
branch coverage is achieved with structured
tests than with even a very large corpus.
1 Introduction
In 2006, Geoffrey Chang was a star of the protein
crystallography world. That year, a crucial compo-
nent of his code was discovered to have a simple
error with large consequences for his research. The
nature of the bug was to change the signs (positive
versus negative) of two columns of the output. The
effect of this was to reverse the predicted ?handed-
ness? of the structure of the molecule?an impor-
tant feature in predicting its interactions with other
molecules. The protein for his work on which Chang
was best known is an important one in predicting
things like human response to anticancer drugs and
the likelihood of bacteria developing antibiotic re-
sistance, so his work was quite influential and heav-
ily cited. The consequences for Chang were the
withdrawal of 5 papers in some of the most presti-
gious journals in the world. The consequences for
the rest of the scientific community have not been
?K. Bretonnel Cohen is with The MITRE Corporation. All
three co-authors are at the Center for Computational Pharma-
cology in the University of Colorado School of Medicine.
quantified, but were substantial: prior to the retrac-
tions, publishing papers with results that did not
jibe with his model?s predictions was difficult, and
obtaining grants based on preliminary results that
seemed to contradict his published results was dif-
ficult as well. The Chang story (for a succinct dis-
cussion, see (Miller, 2006), and see (Chang et al,
2006) for the retractions) is an object illustration of
the truth of Rob Knight?s observation that ?For sci-
entific work, bugs don?t just mean unhappy users
who you?ll never actually meet: they mean retracted
publications and ended careers. It is critical that
your code be fully tested before you draw conclu-
sions from results it produces? (personal communi-
cation). Nonetheless, the subject of software testing
has been largely neglected in academic natural lan-
guage processing. This paper addresses one aspect
of software testing: the monitoring of testing efforts
via code coverage.
1.1 Code coverage
Code coverage is a numerical assessment of the
amount of code that is executed during the running
of a test suite (McConnell, 2004). Although it is
by no means a completely sufficient method for de-
termining the completeness of a testing effort, it is
nonetheless a helpful member of any suite of met-
rics for assessing testing effort completeness. Code
coverage is a metric in the range 0-1.0. A value of
0.86 indicates that 86% of the code was executed
while running a given test suite. 100% coverage is
difficult to achieve for any nontrivial application, but
in general, high degrees of ?uncovered? code should
lead one to suspect that there is a large amount of
23
code that might harbor undetected bugs simply due
to never having been executed. A variety of code
coverage metrics exist. Line coverage indicates the
proportion of lines of code that have been executed.
It is not the most revealing form of coverage assess-
ment (Kaner et al, 1999, p. 43), but is a basic part
of any coverage measurement assessment. Branch
coverage indicates the proportion of branches within
conditionals that have been traversed (Marick, 1997,
p. 145). For example, for the conditional if $a
&& $b, there are two possible branches?one is tra-
versed if the expression evaluates to true, and the
other if it evaluates to false. It is more informative
than line coverage. Logic coverage (also known as
multicondition coverage (Myers, 1979) and condi-
tion coverage (Kaner et al, 1999, p. 44) indicates the
proportion of sets of variable values that have been
tried?a superset of the possible branches traversed.
For example, for the conditional if $a || $b,
the possible cases (assuming no short-circuit logic)
are those of the standard (logical) truth table for that
conditional. These coverage types are progressively
more informative than line coverage. Other types of
coverage are less informative than line coverage. For
example, function coverage indicates the proportion
of functions that are called. There is no guarantee
that each line in a called function is executed, and all
the more so no guarantee that branch or logic cov-
erage is achieved within it, so this type of coverage
is weaker than line coverage. With the advent of
object-oriented programming, function coverage is
sometimes replaced by class coverage?a measure
of the number of classes that are covered.
We emphasize again that code coverage is not
a sufficient metric for evaluating testing complete-
ness in isolation?for example, it is by definition
unable to detect ?errors of omission,? or bugs that
consist of a failure to implement needed functional-
ity. Nonetheless, it remains a useful part of a larger
suite of metrics, and one study found that testing in
the absence of concurrent assessment of code cov-
erage typically results in only 50-60% of the code
being executed ((McConnell, 2004, p. 526), citing
Wiegers 2002).
We set out to question whether a dominant, if of-
ten unspoken, assumption of much work in contem-
porary NLP holds true: that feeding a program a
large corpus serves to exercise it adequately. We be-
gan with an information extraction application that
had been built for us by a series of contractors, with
the contractors receiving constant remote oversight
and guidance but without ongoing monitoring of the
actual code-writing. The application had benefitted
from no testing other than that done by the develop-
ers. We used a sort of ?translucent-box? or ?gray-
box? paradigm, meaning in this case that we treated
the program under test essentially as a black box
whose internals were inaccessible to us, but with the
exception that we inserted hooks to a coverage tool.
We then monitored three types of coverage?line
coverage, branch coverage, and class coverage?
under a variety of contrasting conditions:
? A set of developer-written functional tests ver-
sus a large corpus with a set of semantic rules
optimized for that corpus.
? Varying the size of the rule set.
? Varying the size of the corpus.
We then looked for coverage differences between
the various combinations of input data and rule sets.
In this case, the null hypothesis is that no differences
would be observed. In contrast with the null hypoth-
esis, the unspoken assumption in much NLP work
is that the null hypothesis does not hold, that the
primary determinant of coverage will be the size of
the corpus, and that the observed pattern will be that
coverage is higher with the large corpus than when
the input is not a large corpus.
2 Methods and materials
2.1 The application under test
The application under test was an information ex-
traction application known as OpenDMAP. It is de-
scribed in detail in (Hunter et al, 2008). It achieved
the highest performance on one measure of the
protein-protein interaction task in the BioCreative
II shared task (Krallinger et al, 2007). Its use in
that task is described specifically in (Baumgartner
Jr. et al, In press). It contains 7,024 lines of code
spread across three packages (see Table 1). One
major package deals with representing the seman-
tic grammar rules themselves, while the other deals
with applying the rules to and extracting data from
arbitrary textual input. (A minor package deals with
24
Component Lines of code
Parser 3,982
Rule-handling 2,311
Configuration 731
Total 7,024
Table 1: Distribution of lines of code in the application.
the configuration files and is mostly not discussed in
this paper.)
The rules and patterns that the system uses are
typical ?semantic grammar? rules in that they allow
the free mixing of literals and non-terminals, with
the non-terminals typically representing domain-
specific types such as ?protein interaction verb.?
Non-terminals are represented as classes. Those
classes are defined in a Prote?ge? ontology. Rules typ-
ically contain at least one element known as a slot.
Slot-fillers can be constrained by classes in the on-
tology. Input that matches a slot is extracted as one
of the participants in a relation. A limited regular
expression language can operate over classes, liter-
als, or slots. The following is a representative rule.
Square brackets indicate slots, curly braces indicate
a class, the question-mark is a regular expression op-
erator, and any other text is a literal.
{c-interact} := [interactor1]
{c-interact-verb} the?
[interactor2]
The input NEF binds PACS-2 (PMID 18296443)
would match that rule. The result would be the
recognition of a protein interaction event, with in-
teractor1 being NEF and interactor2 being PACS-2.
Not all rules utilize all possibilities of the rule lan-
guage, and we took this into account in one of our
experiments; we discuss the rules further later in the
paper in the context of that experiment.
2.2 Materials
In this work, we made use of the following sets of
materials.
? A large data set distributed as training data for
part of the BioCreative II shared task. It is de-
scribed in detail in (Krallinger et al, 2007).
Briefly, its domain is molecular biology, and
in particular protein-protein interactions?an
important topic of research in computational
Test type Number of tests
Basic 85
Pattern/rule 67
Patterns only 90
Slots 9
Slot nesting 7
Slot property 20
Total 278
Table 2: Distribution of functional tests.
bioscience, with significance to a wide range
of topics in biology, including understanding
the mechanisms of human diseases (Kann et
al., 2006). The corpus contained 3,947,200
words, making it almost an order of mag-
nitude larger than the most commonly used
biomedical corpus (GENIA, at about 433K
words). This data set is publicly available via
biocreative.sourceforge.net.
? In conjunction with that data set: a set of 98
rules written in a data-driven fashion by man-
ually examining the BioCreative II data de-
scribed just above. These rules were used in the
BioCreative II shared task, where they achieved
the highest score in one category. The set of
rules is available on our SourceForge site at
bionlp.sourceforge.net.
? A set of functional tests created by the primary
developer of the system. Table 2 describes the
breakdown of the functional tests across vari-
ous aspects of the design and functionality of
the application.
2.3 Assessing coverage
We used the open-source Cobertura tool
(Mark Doliner, personal communication;
cobertura.sourceforge.net) to mea-
sure coverage. Cobertura reports line coverage and
branch coverage on a per-package basis and, within
each package, on a per-class basis1.
The architecture of the application is such that
Cobertura?s per-package approach resulted in three
1Cobertura is Java-specific. PyDEV provides code coverage
analysis for Python, as does Coverage.py.
25
sets of coverage reports: for the configuration file
processing code, for the rule-processing code, and
for the parser code. We report results for the appli-
cation as a whole, for the parser code, and for the
rule-processing code. We did note differences in the
configuration code coverage for the various condi-
tions, but it does not change the overall conclusions
of the paper and is omitted from most of the discus-
sion due to considerations of space and of general
interest.
3 Results
We conducted three separate experiments.
3.1 The most basic experiment: test suite
versus corpus
In the most basic experiment, we contrasted
class, line, and branch coverage when running the
developer-constructed test suite and when running
the corpus and the corpus-based rules. Tables 3 and
4 show the resulting data. As the first two lines
of Table 3 show, for the entire application (parser,
rule-handling, and configuration), line coverage was
higher with the test suite?56% versus 41%?and
branch coverage was higher as well?41% versus
28% (see the first two lines of Table 3).
We give here a more detailed discussion of the re-
sults for the entire code base. (Detailed discussions
for the parser and rule packages, including granular
assessments of class coverage, follow.)
For the parser package:
? Class coverage was higher with the test suite
than with the corpus?88% (22/25) versus 80%
(20/25).
? For the entire parser package, line coverage
was higher with the test suite than with the
corpus?55% versus 41%.
? For the entire parser package, branch cover-
age was higher with the test suite than with the
corpus?57% versus 29%.
Table 4 gives class-level data for the two main
packages. For the parser package:
? Within the 25 individual classes of the parser
package, line coverage was equal or greater
with the test suite for 21/25 classes; it was not
just equal but greater for 14/25 classes.
? Within those 21 of the 25 individual classes
that had branching logic, branch coverage was
equal or greater with the test suite for 19/21
classes, and not just equal but greater for 18/21
classes.
For the rule-handling package:
? Class coverage was higher with the test suite
than with the corpus?100% (20/20) versus
90% (18/20).
? For the entire rules package, line coverage was
higher with the test suite than with the corpus?
63% versus 42%.
? For the entire rules package, branch coverage
was higher with the test suite than with the
corpus?71% versus 24%.
Table 4 gives the class-level data for the rules
package:
? Within the 20 individual classes of the rules
package, line coverage was equal or greater
with the test suite for 19/20 classes, and not just
equal but greater for 6/20 classes.
? Within those 11 of the 20 individual classes
that had branching logic, branch coverage was
equal or greater with the test suite for all
11/11 classes, and not just equal but greater for
(again) all 11/11 classes.
3.2 The second experiment: Varying the size of
the rule set
Pilot studies suggested (as later experiments veri-
fied) that the size of the input corpus had a negligible
effect on coverage. This suggested that it would be
worthwhile to assess the effect of the rule set on cov-
erage independently. We used simple ablation (dele-
tion of portions of the rule set) to vary the size of the
rule set.
We created two versions of the original rule set.
We focussed only on the non-lexical, relational pat-
tern rules, since they are completely dependent on
the lexical rules. Each version was about half the
26
Metric Functional tests Corpus, all rules nominal rules verbal rules
Overall line coverage 56% 41% 41% 41%
Overall branch coverage 41% 28% 28% 28%
Parser line coverage 55% 41% 41% 41%
Parser branch coverage 57% 29% 29% 29%
Rules line coverage 63% 42% 42% 42%
Rules branch coverage 71% 24% 24% 24%
Parser class coverage 88% (22/25) 80% (20/25)
Rules class coverage 100% (20/20) 90% (18/20)
Table 3: Application and package-level coverage statistics using the developer?s functional tests, the full corpus with
the full set of rules, and the full corpus with two reduced sets of rules. The highest value in a row is bolded. The final
three columns are intentionally identical (see explanation in text).
Package Line coverage >= Line coverage > Branch coverage >= Branch coverage >
Classes in parser package 21/25 14/25 19/21 18/21
Classes in rules package 19/20 6/20 11/11 11/11
Table 4: When individual classes were examined, both line and branch coverage were always higher with the functional
tests than with the corpus. This table shows the magnitude of the differences. >= indicates the number of classes that
had equal or greater coverage with the functional tests than with the corpus, and > indicates just the classes that had
greater coverage with the functional tests than with the corpus.
size of the original set. The first consisted of the
first half of the rule set, which happened to consist
primarily of verb-based patterns. The second con-
sisted of the second half of the rule set, which corre-
sponded roughly to the nominalization rules.
The last two columns of Table 3 show the
package-level results. Overall, on a per-package ba-
sis, there were no differences in line or branch cov-
erage when the data was run against the full rule set
or either half of the rule set. (The identity of the last
three columns is due to this lack of difference in re-
sults between the full rule set and the two reduced
rule sets.) On a per-class level, we did note minor
differences, but as Table 3 shows, they were within
rounding error on the package level.
3.3 The third experiment: Coverage closure
In the third experiment, we looked at how cover-
age varies as increasingly larger amounts of the cor-
pus are processed. This methodology is compara-
ble to examining the closure properties of a corpus
in a corpus linguistics study (see e.g. Chapter 6 of
(McEnery and Wilson, 2001)) (and as such may be
sensitive to the extent to which the contents of the
corpus do or do not fit the sublanguage model). We
counted cumulative line coverage as increasingly
large amounts of the corpus were processed, rang-
ing from 0 to 100% of its contents. The results for
line coverage are shown in Figure 1. (The results for
branch coverage are quite similar, and the graph is
not shown.) Line coverage for the entire application
is indicated by the thick solid line. Line coverage
for the parser package is indicated by the thin solid
line. Line coverage for the rules package is indi-
cated by the light gray solid line. The broken line
indicates the number of pattern matches?quantities
should be read off of the right y axis.
The figure shows quite graphically the lack of ef-
fect on coverage of increasing the size of the cor-
pus. For the entire application, the line coverage is
27% when an empty document has been read in, and
39% when a single sentence has been processed; it
increases by one to 40% when 51 sentences have
been processed, and has grown as high as it ever
will?41%?by the time 1,000 sentences have been
processed. Coverage at 191,478 sentences?that is,
3,947,200 words?is no higher than at 1,000 sen-
tences, and barely higher, percentagewise, than at a
single sentence.
An especially notable pattern is that the huge rise
27
Figure 1: Increase in percentage of line coverage as in-
creasing amounts of the corpus are processed. Left y axis
is the percent coverage. The x axis is the number of sen-
tences. Right y axis (scale 0-12,000) is the number of
rule matches. The heavy solid line is coverage for the en-
tire package, the thin solid line is coverage for the parser
package, the light gray line is coverage for the rules pack-
age, and the broken line is the number of pattern matches.
in the number of matches to the rules (graphed by
the broken line) between 5,000 sentences and 191K
sentences has absolutely no effect on code coverage.
4 Discussion
The null hypothesis?that a synthetic test suite
and a naturalistic corpus provide the same code
coverage?is not supported by the data shown here.
Furthermore, the widely, if implicitly, held assump-
tion that a corpus would provide the best testing data
can be rejected, as well. The results reported here
are consistent with the hypothesis that code cover-
age for this application is not affected by the size of
the corpus or by the size of the rule set, and that run-
ning it on a large corpus does not guarantee thorough
testing. Rather, coverage is optimized by traditional
software testing.
4.1 Related work
Although software testing is a first-class research
object in computer science, it has received little at-
tention in the natural language processing arena. A
notable exception to this comes from the grammar
engineering community. This has produced a body
of publications that includes Oepen?s work on test
suite design (Oepen et al, 1998), Volk?s work on test
suite encoding (Volk, 1998), Oepen et al?s work on
the Redwoods project (Oepen et al, 2002), Butt and
King?s discussion of the importance of testing (Butt
and King, 2003), Flickinger et al?s work on ?seman-
tics debugging? with Redwoods data (Flickinger et
al., 2005), and Bender et al?s recent work on test
suite generation (Bender et al, 2007). Outside of
the realm of grammar engineering, work on test-
ing for NLP is quite limited. (Cohen et al, 2004)
describes a methodology for generating test suites
for molecular biology named entity recognition sys-
tems, and (Johnson et al, 2007) describes the de-
velopment of a fault model for linguistically-based
ontology mapping, alignment, and linking systems.
However, when most researchers in the NLP com-
munity refer in print to ?testing,? they do not mean
it in the sense in which that term is used in soft-
ware engineering. Some projects have publicized as-
pects of their testing work, but have not published on
their approaches: the NLTK project posts module-
level line coverage statistics, having achieved me-
dian coverage of 55% on 116 Python modules2 and
38% coverage for the project as a whole; the MAL-
LET project indicates on its web site that it en-
courages the production of unit tests during devel-
opment, but unfortunately does not go into details
of their recommendations for unit-testing machine
learning code3.
4.2 Conclusions
We note a number of shortcomings of code cov-
erage. For example, poor coding conventions
can actually inflate your line coverage. Con-
sider a hypothetical application consisting only
of the following, written as a single line of code
with no line breaks: if (myVariable ==
1) doSomething elsif (myVariable
== 2) doSomethingElse elsif
(myVariable = 3) doYetAnotherThing
and a poor test suite consisting only of inputs that
will cause myVariable to ever have the value 1.
The test suite will achieve 100% line coverage for
2nltk.org/doc/guides/coverage
3mallet.cs.umass.edu/index.php/
Guidelines for writing unit tests
28
this application?and without even finding the error
that sets myVariable to 3 if it is not valued 1
or 2. If the code were written with reasonable line
breaks, code coverage would be only 20%. And,
as has been noted by others, code coverage can not
detect ?sins of omission??bugs that consist of the
failure to write needed code (e.g. for error-handling
or for input validation). We do not claim that code
coverage is wholly sufficient for evaluating a test
suite; nonetheless, it is one of a number of metrics
that are helpful in judging the adequacy of a testing
effort. Another very valuable one is the found/fixed
or open/closed graph (Black, 1999; Baumgartner Jr.
et al, 2007).
While remaining aware of the potential shortcom-
ings of code coverage, we also note that the data
reported here supports its utility. The developer-
written functional tests were produced without mon-
itoring code coverage; even though those tests rou-
tinely produced higher coverage than a large corpus
of naturalistic text, they achieved less than 60% cov-
erage overall, as predicted by Wiegers?s work cited
in the introduction. We now have the opportunity to
raise that coverage via structured testing performed
by someone other than the developer. In fact, our
first attempts to test the previously unexercised code
immediately uncovered two showstopper bugs; the
coverage analysis also led us to the discovery that
the application?s error-handling code was essentially
untested.
Although we have explored a number of dimen-
sions of the space of the coverage phenomenon, ad-
ditional work could be done. We used a relatively
naive approach to rule ablation in the second experi-
ment; a more sophisticated approach would be to ab-
late specific types of rules?for example, ones that
do or don?t contain slots, ones that do or don?t con-
tain regular expression operators, etc.?and monitor
the coverage changes. (We did run all three experi-
ments on a separate, smaller corpus as a pilot study;
we report the results for the BioCreative II data set
in this paper since that is the data for which the rules
were optimized. Results in the pilot study were en-
tirely comparable.)
In conclusion: natural language processing appli-
cations are particularly susceptible to emergent phe-
nomena, such as interactions between the contents
of a rule set and the contents of a corpus. These
are especially difficult to control when the evalua-
tion corpus is naturalistic and the rule set is data-
driven. Structured testing does not eliminate this
emergent nature of the problem space, but it does
allow for controlled evaluation of the performance
of your system. Corpora also are valuable evalua-
tion resources: the combination of a structured test
suite and a naturalistic corpus provides a powerful
set of tools for finding bugs in NLP applications.
Acknowledgments
The authors thank James Firby, who wrote the func-
tional tests, and Helen L. Johnson, who wrote the
rules that were used for the BioCreative data. Steve
Bethard and Aaron Cohen recommended Python
coverage tools. We also thank the three anonymous
reviewers.
References
William A. Baumgartner Jr., K. Bretonnel Cohen, Lynne
Fox, George K. Acquaah-Mensah, and Lawrence
Hunter. 2007. Manual curation is not sufficient
for annotation of genomic databases. Bioinformatics,
23:i41?i48.
William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-
son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-
mann, Elizabeth K. White, Olga Medvedeva, K. Bre-
tonnel Cohen, and Lawrence Hunter. In press. Con-
cept recognition for extracting protein interaction rela-
tions from biomedical text. Genome Biology.
Emily M. Bender, Laurie Poulson, Scott Drellishak, and
Chris Evans. 2007. Validation and regression test-
ing for a cross-linguistic grammar resource. In ACL
2007 Workshop on Deep Linguistic Processing, pages
136?143, Prague, Czech Republic, June. Association
for Computational Linguistics.
Rex Black. 1999. Managing the Testing Process.
Miriam Butt and Tracy Holloway King. 2003. Grammar
writing, testing and evaluation. In Ali Farghaly, editor,
A handbook for language engineers, pages 129?179.
CSLI.
Geoffrey Chang, Christopher R. Roth, Christopher L.
Reyes, Owen Pornillos, Yen-Ju Chen, and Andy P.
Chen. 2006. Letters: Retraction. Science, 314:1875.
K. Bretonnel Cohen, Lorraine Tanabe, Shuhei Kinoshita,
and Lawrence Hunter. 2004. A resource for construct-
ing customized test suites for molecular biology entity
identification systems. In HLT-NAACL 2004 Work-
shop: BioLINK 2004, Linking Biological Literature,
Ontologies and Databases, pages 1?8. Association for
Computational Linguistics.
29
Dan Flickinger, Alexander Koller, and Stefan Thater.
2005. A new well-formedness criterion for semantics
debugging. In Proceedings of the HPSG05 Confer-
ence.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
Helen L. Johnson, K. Bretonnel Cohen, and Lawrence
Hunter. 2007. A fault model for ontology mapping,
alignment, and linking systems. In Pacific Sympo-
sium on Biocomputing, pages 233?244. World Scien-
tific Publishing Company.
Cem Kaner, Hung Quoc Nguyen, and Jack Falk. 1999.
Testing computer software, 2nd edition. John Wiley
and Sons.
Maricel Kann, Yanay Ofran, Marco Punta, and Predrag
Radivojac. 2006. Protein interactions and disease. In
Pacific Symposium on Biocomputing, pages 351?353.
World Scientific Publishing Company.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the second BioCreative PPI
task: automatic extraction of protein-protein interac-
tions. In Proceedings of the Second BioCreative Chal-
lenge Evaluation Workshop.
Brian Marick. 1997. The craft of software testing:
subsystem testing including object-based and object-
oriented testing. Prentice Hall.
Steve McConnell. 2004. Code complete. Microsoft
Press, 2nd edition.
Tony McEnery and Andrew Wilson. 2001. Corpus Lin-
guistics. Edinburgh University Press, 2nd edition.
Greg Miller. 2006. A scientist?s nightmare: software
problem leads to five retractions. Science, 314:1856?
1857.
Glenford Myers. 1979. The art of software testing. John
Wiley and Sons.
S. Oepen, K. Netter, and J. Klein. 1998. TSNLP - test
suites for natural language processing. In John Ner-
bonne, editor, Linguistic Databases, chapter 2, pages
13?36. CSLI Publications.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten
Brants. 2002. The LinGO Redwoods treebank: mo-
tivation and preliminary applications. In Proceedings
of the 19th international conference on computational
linguistics, volume 2.
Martin Volk. 1998. Markup of a test suite with SGML.
In John Nerbonne, editor, Linguistic databases, pages
59?76. CSLI Publications.
30
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 684?688, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UColorado SOM: Extraction of Drug-Drug Interactions from BioMedical
Text using Knowledge-rich and Knowledge-poor Features
Negacy D. Hailu Lawrence E. Hunter K. Bretonnel Cohen
negacy.hailu@ucdenver.edu larry.hunter@ucdenver.edu kevin.cohen@gmail.com
University of Colorado, Anschutz Medical Campus
Abstract
In this paper, we present our approach to
SemEval-2013 Task 9.2. It is a feature rich
classification using LIBSVM for Drug-Drug
Interactions detection in the BioMedical do-
main. The features are extracted considering
morphosyntactic, lexical and semantic con-
cepts. Tools like openDMAP and TEES are
used to extract semantic concepts from the
corpus. The best F-score that we got for Drug-
Drug Interaction (DDI) detection is 50% and
61% and the best F-score for DDI detection
and classification is 34% and 48% for test and
development data respectively.
Keywords: text mining, event extraction, ma-
chine learning, feature extraction.
1 Introduction
Our approach to the Semeval 2013 drug-drug in-
teraction task explored the potential for integrat-
ing knowledge-based approaches with supervised
machine learning. In practice, most supervised
machine learning systems are actually hybrids of
machine learning and some knowledge-based ap-
proach. However, the integration between the two
is typically quite loose, with the knowledge-based
approach being realized either as heuristic pre-
processing or post-processing of the results. The
work reported here is an attempt to make a tighter
coupling between knowledge-based methods and
machine learning. In particular, we took the ap-
proach of using knowledge-based methods for fea-
ture extraction.
2 Methodology
In this challenge we approach the Drug-Drug inter-
action task 9.2 as a binary classification problem. A
pair of drugs is interacting if there is some kind of
influence between the two. Our approach for Drug-
Drug interaction extraction 2013 mainly makes use
of domain specific morphosyntactic, lexical and se-
mantic features between paired drugs.
We applied Machine Learning classification tech-
niques in order to determine whether a pair of drugs
within a biomedical text is interacting or not. For a
training set of labeled instances
(
Xi, yi
)
= 1, 2, ..., l
where Xi ? Rn and y ? {1,?1}l, the support vector
machines (SVMs) optimization problem is defined
as(Boser et al, 1992) (Cortes and Vapnik , 1995):
?? = argmax
?,w,b
(1
2
W TW + C
l?
i=1
?i
)
(1)
such that yi
(
W T?(Xi) + b
)
> 1? ?i,
?i ? 0.
2.1 Materials
The corpus is provided from two data sources. There
are 572 documents describing drug-drug interac-
tions from the DrugBank database and 142 abstracts
on the subject of drug-drug interactions from Med-
Line (Isabel et.al., 2011). We prepared datasets
for the entire corpus. Each instance in the dataset
is a set of paired drugs. In our dataset, there are
27787 instances. 93.57% of them are from Drug-
bank database and the remaining are from MedLine
abstracts. DDI shared task 2013 is not only interac-
tion detection but the challenge also includes detec-
684
tion of the type of interaction. In our approach, we
treated each interaction type as one class.
2.2 Methods
LIBSVM is a library for support vector machines (
LIBSVM, 2011). We used this tool for classify-
ing the dataset. Basically, the problem is a multi-
class classification problem. We applied the concept
of one-vs-all multi-class classification technique to
handle the multiple classes.
2.3 Feature Extraction
The features that we extracted for this challenge can
be categorized into three types:
2.3.1 Morphosyntactic Features
? Distance feature: this is distance between
paired drugs in number of words. The intuition
here is that the closer two drugs are, the more
chance that they might be interacting. Since
this feature takes word count as its value, the
text is split within white space when counting
number of words. Punctuation marks are not
considered when counting words.
? Part-Of-Speech tags: we chose the GENIA
dependency parser for parsing the corpus for
two reasons.
? Dependency parser related features: we con-
struct the dependency tree using the GENIA
dependency parser. Two features are extracted
from the tree:
? Presence of interaction word in the path
from the target drug node to the root of
the tree.
? Distance from one target drug name to
another one in the tree.
2.3.2 Lexical Features
? Bigrams: a sequence of bigrams is ex-
tracted for input text.
2.3.3 Semantic Features
? Interaction words: we collected the top
100 words that indicate drug-drug inter-
action. The presence of these words is
one feature for our system. The words are
checked before and after each target drug.
Such words include: increase, decrease,
inhibit, interaction, reduce, affect.
? Presence of preposition within target
drugs: the text within the target drugs is
tested to see if it has preposition or not. If
the text has a preposition, the value is 1
otherwise it will have zero value.
? Presence of other drugs within target
drugs: firstly, we collect all drug names
into a list. The text within the target drugs
is searched for the drug names and the
value for this feature will have the num-
ber of hits.
? Concept from OpenDMAP:
OpenDMAP is an ontology-driven,
rule-based concept analysis and infor-
mation extraction system (Hunter et.al.,
2008). We used openDMAP to extract
drug-drug interaction concepts from the
DDI2013 corpus. We extracted pattern
based features using OpenDMAP only if
OpenDMAP recognizes target drugs.
3 Dataset Preparation
The challenge provided datasets from Drug-
Bank database and MedLine abstracts. We split
the dataset into 20% development data and 80%
training data. Table1 shows the percentage of
positive instances in the dataset.
DDI interaction 14.47%
Interaction type effect 6.07%
Interaction type advise 2.97%
Interaction type mechanism 4.75%
Interaction type int 0.68%
Table 1: positive instances for the different class types
The data is not balanced, as shown in table 1.
We penalized the negative classes during train-
ing in order to balance the data.
In section 4 we present results for three runs.
Run1 includes the basic features which are de-
scribed in section 2.3. In Run2 we included fea-
ture values made available by TEES ( Bjo?rne
685
et.al., 2011). In addition to the features in the
first two runs, in Run3 the list of interaction
words were considered individually as features.
In this run, weight penalty and different opti-
mized LIBSVM parameters were considered.
4 Results
Table 2 shows the results for DDI detection
only, for both development and test data. The
best F1 score is 50% for test data and 61% for
development data.
Runs
1 2 3
test data
precision 0.37 0.38 0.4
recall 0.73 0.75 0.64
F1 0.49 0.5 0.49
development data
precision 0.28 0.82 0.62
recall 0.78 0.46 0.59
F1 0.41 0.59 0.61
Table 2: Partial Evaluation: only detection of DDI
Table 3 shows results for DDI detection and
classification. The best F1 score is 34% for test
data and 48% for development data.
Runs
1 2 3
test data
precision 0.16 0.25 0.27
recall 0.32 0.5 0.44
F1 0.21 0.33 0.34
development data
precision 0.13 0.59 0.49
recall 0.37 0.33 0.46
F1 0.2 0.42 0.48
Table 3: Detection and classification of DDI
And finally, the scores for the individual DDI
type for the best run are shown in table 4. Ap-
parently, Run3 outperforms in all the scores as
can be seen in tables 2 through 4.
Run3
precision recall F1
test data
mechanism 0.39 0.29 0.33
effect 0.21 0.63 0.31
advise 0.45 0.39 0.42
int 0.4 0.28 0.334
development data
mechanism 0.5 0.29 0.37
effect 0.44 0.61 0.51
advise 0.72 0.46 0.56
int 0.08 0.1 0.09
Table 4: Best scores for DDI type, Run3
5 Discussion
Generally speaking, the performance of our
system is better for DDI detection regardless of
their types compared to classifying what kind
of DDI they are.
Among the three runs that we submitted for the
challenge, Run3 outperforms in all the scores
as can be seen in tables 2 through 4 for the fol-
lowing reasons:
? weight penalty techniques are applied in
Run3
? optimal cost and gamma parameters are
selected while training for Run3
? Bag of interaction words are considered
as individual features. This specially in-
creases scores for detecting the individual
DDI types.
The best F-score that we got for DDI detec-
tion is 61% for development data and 50% for
test data as shown in Table 2. The reason why
scores are better for DDI detection is that our
approach is feature rich DDI detection and we
believe that our features mainly target detect-
ing DDIs. A further addition of features that
distinguishes the DDI types will hopefully im-
prove the scores for DDI classification. On the
other hand, it has been observed that scores are
lower for test data compared to development
data. And the reason for this is due to opti-
mization parameters that we heuristically chose
during training are possibly favoring to devel-
opment data than to test data. Another possible
reason could be overfitting.
As shown in section 4, the knowledge-based
lexical features produced our best run. The se-
mantic parser made a smaller contribution to
performance, almost certainly because of low
coverage- - -historically, in past shared tasks
on information extraction, its behavior has been
characterized by very high precision but low re-
call.
5.1 Error Analysis
Table 5 shows false positive predictions col-
lected from the results for Run3. In FP-1, the
system predicts detecting the first pair (etan-
ercept and anakinra) correctly and then clas-
sifying as type effect but it failed to deter-
mine whether etanercept is interacting with
686
interleukin-1 antagonist. A close examina-
tion of this sentence shows that the last two
drugs are separated by parentheses and in fact
the last drug is a further explanation of the
second one. The system couldn?t distinguish
this concept ? rather it is treating all the three
drugs separately and both pairs i.e. (etanercept,
anakinra) and (etanercept, interleukin-1 antag-
onist) are predicted the same. This is happening
due the syntactic nature of the text. One possi-
ble way to avoid such confusion is to expand
the sentence. In other words, we believe initial
data clean up might improve the performance
of the system. Avoiding punctuation marks
such as parenthesis for this case and other de-
limiters and representing them in words if pos-
sible might improve the performance of the
classifier.
It is also observed that there is poor prediction
for pairs of drugs that have negation. The two
examples, i.e. FP-2 and FP-3 in table 5 are
wrongly predicted because there is no feature
that handles negation in the system.
FP-1 Concurrent administration of etanercept
(another TNF -blocking agent) and anakinra
(an interleukin-1 antagonist) has been as-
sociated with an increased risk of serious in-
fections, and increased risk of neutropenia
and no additional benefit compared to these
m edicinal products alone.
FP-2 When used in external subcutaneous infu-
sion pumps for insulin, NovoLog should not
be mixed with any other insulins or diluent.
FP-3 With the exception of albuterol, there are
no formal studies fully evaluating the in-
teraction effects of ATROVENT Inhalation
Aerosol and these drugs with respect to ef-
fectiveness.
Table 5: False positive samples. In this table false positive DDIs are in bold font.
False negative predictions have a negative ef-
fect on the recall evaluation parameter. In ta-
ble 6 we show false negative predictions and
their possible analysis for the development
data. A close analysis of FN-1 and FN-2 shows
that both sentences have a comma between the
paired drugs. From a linguistic point of view,
the punctuation mark comma can be used to
separate interdependent clauses. Represent-
ing this dependency as a feature might help to
avoid false negatives. FN-3 are a bit differ-
ent and it apprears that there is much knowl-
edge that can be extracted from the given text
which is in number format. Currently, the fea-
tures that we have don?t extract information
written in numbers. Also, the list of interac-
tion words doesn?t include words like admin-
istered, administration though words like co-
administration, coadministered are included.
A further development of the list of interaction
words will avoid such false predictions.
FN-1 Anticholinergic agents: Although iprat-
ropium bromide is minimally absorbed into
the systemic circulation, there is some poten-
tial for an additive interaction with concomi-
tantly used anticholinergic medications.
FN-2 Lymphocytopenia has been reported in pa-
tients receiving CAMPTOSAR, and it is
possible that the administration of dexam-
ethasone as antiemetic prophylaxis may
have enhanced the likelihood of this effect.
FN-3 Betaseron administration to three cancer pa-
tients over a dose range of 0.025 mg to
2.2 mg led to a dose-dependent inhibition
of antipyrine elimination.14 The effect of
alternate-day administration of 0.25 mg of
Betaseron on drug metabolism in MS pa-
tients is unknown.
Table 6: False negative samples. In this table false negative DDIs are in bold format.
6 Conclusion
Our approach to Extraction of Drug-Drug In-
teractions from BioMedical Texts task 9.2 is a
feature rich SVM classification. The perfor-
mance on detecting Drug-Drug interactions is
encouraging but it is a bit lower when it comes
to further classfying the type of the interaction.
As described in section 5.1, addition of fea-
tures such as negation will reduce false posi-
tive prediction and this will increase precision
score. Further development of the list of inter-
action words is also a important task to handle
the different forms of words that could indicate
an interaction type. We have also observed that
pattern-based semantic features are not well ex-
tracted in our system.
687
References
Segura-Bedmar, I., Mart??nez, P, Herrero-Zazo, M.
SemEval-2013 Task 9: Extraction of Drug-Drug
Interactions from Biomedical Texts. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013)
Chang, Chih-Chung and Lin, Chih-Jen 2011. LIB-
SVM: A library for support vector machines, vol-
ume 2. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm
Hunter, L, Z Lu, J Firby, WA Baumgartner, Jr., HL
Johnson, PV Ogren, KB Cohen. . OpenDMAP: An
open-source, ontology-driven concept analysis en-
gine, with applications to capturing knowledge re-
garding protein transport, protein interactions and
cell-type-specific gene expression. BMC Bioinfor-
matics 2008, 9:78.
Jari Bjo?rne, Filip Ginter, Juho Heimonen, Antti
Airola, Tapio Pahikkala and Tapio Salakoski.
2011. TEES: Event Extraction Software. Software
available at http://jbjorne.github.com/
TEES/
Isabel Segura-Bedmar, Paloma Mart??nez, Cesar de
Pablo-sachnez Using a shallow linguistic kernel for
drug-drug interaction Extraction. 2011. Journal of
Biomedical Informatics, 44(5):789-804..
Boser, Bernhard E. and Guyon, Isabelle M. and
Vapnik, Vladimir N. A training algorithm for opti-
mal margin classifiers. Proceedings of the fifth an-
nual workshop on Computational learning theory.
C. Cortes and V. Vapnik. Support-vector network.
Machine Learning, 20:273-297
Isabel Segura-Bedmar, Paloma Mart??nez, and
Daniel Sa?nchez-Cisneros The 1st DDIExtraction-
2011 challenge task: Extraction of Drug-Drug In-
teractions from biomedical texts Proceedings of the
1st Challenge task on Drug-Drug Interaction Ex-
traction
Philippe Thomas, Mariana Neves, Illes Solt,
Domonkos Tikk, and Ulf Leser. Relation Extraction
for Drug-Drug Interactions using Ensemble Learn-
ing Proceedings of the 1st Challenge task on Drug-
Drug Interaction Extraction
Md. Faisal Mahbub Chowdhury, Asma Ben
Abacha, Alberto Lavelli, and Pierre Zweigenbau.
Two Different Machine Learning Techniques for
Drug-drug Interaction Extraction Proceedings of
the 1st Challenge task on Drug-Drug Interaction
Extraction
Md. Faisal Mahbub Chowdhury, and Alberto
Lavelli. Drug-drug Interaction Extraction Using
Composite Kernels Proceedings of the 1st Chal-
lenge task on Drug-Drug Interaction Extraction
Jari Bjo?rne, Antti Airola, Tapio Pahikkala, and
Tapio Salakoski Drug-Drug Interaction Extraction
with RLS and SVM Classiffers Proceedings of the
1st Challenge task on Drug-Drug Interaction Ex-
traction
JAnne-Lyse Minard, Anne-Laure Ligozat, Brigitte
Grau, and Lamia Makour Feature selection for
Drug-Drug Interaction detection using machine-
learning based approaches Proceedings of the 1st
Challenge task on Drug-Drug Interaction Extrac-
tion
Sandra Garcia-Blasco, Santiago M. Mola-Velasco,
Roxana Danger, and Paolo Rosso Automatic Drug-
Drug Interaction Detection: A Machine Learning
Approach With Maximal Frequent Sequence Ex-
traction Proceedings of the 1st Challenge task on
Drug-Drug Interaction Extraction
Jacinto Mata Va?zquez, Ramo?n Santano, Daniel
Blanco, Marcos Lucero, and Manuel J. Man?a Lo?pez
A machine learning approach to extract drugdrug
interactions in an unbalanced cataset Proceedings
of the 1st Challenge task on Drug-Drug Interaction
Extraction
Stefania Rubrichi, Matteo Gabetta, Riccardo
Bellazzi, Cristiana Larizza, and Silvana Quaglini
Drug-Drug Interactions Discovery Based on CRFs
SVMs and Rule-Based Methods Proceedings of
the 1st Challenge task on Drug-Drug Interaction
Extraction
Man Lan, Jiang Zhao, Kezun Zhang, Honglei Shi,
and Jingli Cai An experimental exploration of drug-
drug interaction extraction from biomedical texts
Proceedings of the 1st Challenge task on Drug-
Drug Interaction Extraction
Shreyas Karnik, Abhinita Subhadarshini, Zhiping
Wang, Luis Rocha and Lang Li Extraction of drug-
drug interactions using all paths graph kernel Pro-
ceedings of the 1st Challenge task on Drug-Drug
Interaction Extraction
688
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 38?45,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Fast and simple semantic class assignment for biomedical text
K. Bretonnel Cohen
Computational Bioscience Program
U. Colorado School of Medicine
and
Department of Linguistics
U. of Colorado at Boulder
kevin.cohen@gmail.com
Tom Christiansen
Comput. Bioscience Prog.
U. Colorado Sch. of Medicine
tchrist@perl.com
William A. Baumgartner Jr.
Computational Bioscience Program
U. Colorado School of Medicine
william.baumgartner@ucdenver.edu
Karin Verspoor
Computational Bioscience Program
U. Colorado School of Medicine
karin.verspoor@ucdenver.edu
Lawrence E. Hunter
Computational Bioscience Program
U. Colorado School of Medicine
larry.hunter@ucdenver.edu
Abstract
A simple and accurate method for assigning
broad semantic classes to text strings is pre-
sented. The method is to map text strings
to terms in ontologies based on a pipeline of
exact matches, normalized strings, headword
matching, and stemming headwords. The
results of three experiments evaluating the
technique are given. Five semantic classes
are evaluated against the CRAFT corpus of
full-text journal articles. Twenty semantic
classes are evaluated against the correspond-
ing full ontologies, i.e. by reflexive match-
ing. One semantic class is evaluated against
a structured test suite. Precision, recall,
and F-measure on the corpus when evaluat-
ing against only the ontologies in the cor-
pus is micro-averaged 67.06/78.49/72.32 and
macro-averaged 69.84/83.12/75.31. Accuracy
on the corpus when evaluating against all
twenty semantic classes ranges from 77.12%
to 95.73%. Reflexive matching is generally
successful, but reveals a small number of er-
rors in the implementation. Evaluation with
the structured test suite reveals a number of
characteristics of the performance of the ap-
proach.
1 Introduction
Broad semantic class assignment is useful for a
number of language processing tasks, including
coreference resolution (Hobbs, 1978), document
classification (Caporaso et al, 2005), and informa-
tion extraction (Baumgartner Jr. et al, 2008). A
limited number of semantic classes have been stud-
ied extensively, such as assigning text strings to the
category gene or protein (Yeh et al, 2005;
Smith et al, 2008), or the PERSON, ORGANI-
ZATION, and LOCATION categories introduced in
the Message Understanding Conferences (Chinchor,
1998). A larger number of semantic classes have re-
ceived smaller amounts of attention, e.g. the classes
in the GENIA ontology (Kim et al, 2004), vari-
ous event types derived from the Gene Ontology
(Kim et al, 2009), and diseases (Leaman and Gon-
zalez, 2008). However, many semantic types have
not been studied at all. In addition, where ontolo-
gies are concerned, although there has been work
on finding mentions or evidence of specific terms in
text (Blaschke et al, 2005; Stoica and Hearst, 2006;
Davis et al, 2006; Shah et al, 2009), there has been
no work specifically addressing assigning multiple
very broad semantic classes with potential overlap.
In particular, this paper examines the problem of tak-
ing a set of ontologies and a text string (typically,
but not necessarily, a noun phrase) as input and de-
termining which ontology defines the semantic class
that that text string refers to. We make an equiva-
lence here between the notion of belonging to the
domain of an ontology and belonging to a specific
semantic class. For example, if a string in text refers
to something in the domain of the Gene Ontology,
we take it as belonging to a Gene Ontology seman-
tic class (using the name of the ontology only for
convenience); if a string in text refers to something
belonging to the domain of the Sequence Ontology,
we take it as belonging to a Sequence Ontology se-
mantic class. We focus especially on rapid, simple
methods for making such a determination.
The problem is most closely related to multi-class
38
classification, where in the case of this study we are
including an unusually large number of categories,
with possible overlap between them. A text string
might refer to something that legitimately belongs
to the domain of more than one ontology. For exam-
ple, it might belong to the semantic classes of both
the Gene Ontology and the Gene Regulation Ontol-
ogy; regulation is an important and frequent concept
in the Gene Ontology. This fact has consequences
for defining the notion of a false positive class as-
signment; we return to this issue in the Results sec-
tion.
2 Methods
2.1 Target semantic classes
The following ontologies were used to define se-
mantic classes:
? Gene Ontology
? Sequence Ontology
? Foundational Model of Anatomy
? NCBI Taxonomy
? Chemical Entities of Biological Interest
? Phenotypic Quality
? BRENDA Tissue/Enzyme Source
? Cell Type Ontology
? Gene Regulation Ontology
? Homology Ontology
? Human Disease Ontology
? Human Phenotype Ontology
? Mammalian Phenotype Ontology
? Molecule Role Ontology
? Mouse Adult Gross Anatomy Ontology
? Mouse Pathology Ontology
? Protein Modification Ontology
? Protein-Protein Interaction Ontology
? Sample Processing and Separation Techniques
Ontology
? Suggested Ontology for Pharmacogenomics
2.2 Methodology for assigning semantic class
We applied four simple techniques for attempting to
match a text string to an ontology. They are arranged
in order of decreasing stringency. That is, each sub-
sequent method has looser requirements for a match.
This both allows us to evaluate the contribution of
each component more easily and, at run time, allows
the user to set a stringency level, if the default is not
desired.
2.2.1 Exact match
The first and most stringent technique is exact
match. (This is essentially the only technique used
by the NCBO (National Center for Biomedical On-
tology) Annotator (Jonquet et al, 2009), although
it can also do substring matching.) We normalize
terms in the ontology and text strings in the input
for case and look for a match.
2.2.2 Stripping
All non-alphanumeric characters, including
whitespace, are deleted from the terms in the
ontology and from text strings in the input (e.g.
cadmium-binding and cadmium binding both
become cadmiumbinding) and look for a match.
2.2.3 Head nouns
This method involves a lightweight linguistic
analysis. We traversed each ontology and deter-
mined the head noun (see method below) of each
term and synonym in the ontology. We then pre-
pared a dictionary mapping from head nouns to lists
of ontologies in which those head nouns were found.
Head nouns were determined by two simple
heuristics (cf. (Collins, 1999)). For terms fitting the
pattern X of... (where of represents any preposi-
tion) the term X was taken as the head noun. For
all other terms, the rightmost word was taken as the
head noun. These two heuristics were applied in se-
quence when applicable, so that for example positive
regulation of growth (GO:0045927) becomes posi-
tive regulation by application of the first heuristic
and regulation by application of the second heuris-
tic. In the case of some ontologies, very limited pre-
39
processing was necessary?for example, it was nec-
essary to delete double quotes that appeared around
synonyms, and in some ontologies we had to delete
strings like [EXACT SYNONYM] from some terms
before extracting the head noun.
2.2.4 Stemming head nouns
In this technique, the headwords obtained by the
previous step were stemmed with the Porter stem-
mer.
2.3 Corpus and other materials
We made use of three sources in our evaluation.
One is the CRAFT (Colorado Richly Annotated Full
Text) corpus (Verspoor et al, 2009; Cohen et al,
2010a). This is a collection of 97 full-text journal
articles, comprising about 597,000 words, each of
which has been used as evidence for at least one an-
notation by the Mouse Genome Informatics group.
It has been annotated with a number of ontologies
and database identifiers, including:
? Gene Ontology
? Sequence Ontology
? Cell Type Ontology
? NCBI Taxonomy
? Chemical Entities of Biological Interest
(ChEBI)
In total, there are over 119,783 annotations. (For
the breakdown across semantic categories, see Ta-
ble 1.) All of these annotations were done by biolog-
ical scientists and have been double-annotated with
inter-annotator agreement in the nineties for most
categories.
The second source is the full sets of terms from
the twenty ontologies listed in the Introduction. All
of the twenty ontologies that we used were obtained
from the OBO portal. Version numbers are omitted
here due to space limitations, but are available from
the authors on request.
The third source is a structured test suite based on
the Gene Ontology (Cohen et al, 2010b). Structured
test suites are developed to test the performance
of a system on specific categories of input types.
This test set was especially designed to test diffi-
cult cases that do not correspond to exact matches
of Gene Ontology terms, as well as the full range of
types of terms. The test suite includes 300 concepts
from GO, as well as a number of transformations of
their terms, such as cells migrated derived from the
term cell migration and migration of cells derived
from cell migration, classified according to a num-
ber of linguistic attributes, such as length, whether
or not punctuation is included in the term, whether
or not it includes function (stop) words, etc. This
test suite determines at least one semantic category
that should be returned for each term. Unlike using
the entire ontologies, this evaluation method made
detailed error analysis possible. This test suite has
been used by other groups for broad characteriza-
tions of successes and failures of concept recogniz-
ers, and to tune the parameters of concept recogni-
tion systems.
2.4 Evaluation
We did three separate evaluations. In one, we com-
pared the output of our system against manually-
generated gold-standard annotations in the CRAFT
corpus (op. cit.). This was possible only for the on-
tologies that have been annotated in CRAFT, which
are listed above.
In the second evaluation, we used the entire on-
tologies themselves as inputs. In this method, all
responses should be the same?for example, every
term from the Gene Ontology should be classified
as belonging to the GO semantic class.
In the third, we utilized the structured test suite
described above.
2.4.1 Baselines
Two baselines are possible, but neither is optimal.
The first would be to use MetaMap (Aronson, 2001),
the industry standard for semantic category assign-
ment. (Note that MetaMap assigns specific cate-
gories, not broad ones.) However, MetaMap out-
puts only semantic classes that are elements of the
UMLS, which of the ontologies that we looked at,
includes only the Gene Ontology. The other is the
NCBO Annotator. The NCBO Annotator detects
only exact matches (or substring matches) to ontol-
ogy terms, so it is not clear that it is a strong enough
baseline to allow for a stringent analysis of our ap-
40
proach.
3 Results
We present our results in three sections:
? For the CRAFT corpus
? For the ontologies themselves
? For the Gene Ontology test suite
3.1 Corpus results
Table 1 (see next page) shows the results on the
CRAFT corpus if only the five ontologies that were
actually annotated in CRAFT are used as inputs.
The results are given for stemmed heads. Perfor-
mance on the four techniques that make up the ap-
proach is cumulative, and results for stemmed heads
reflects the application of all four techniques. In this
case, where we evaluate against the corpus, it is pos-
sible to determine false positives, so we can give
precision, recall, and F-measures for each semantic
class, as well as for the corpus as a whole. Micro-
averaged results were 67.06 precision, 78.49 recall,
and 72.32 F-measure. Macro-averaged results were
69.84 precision, 83.12 recall, and 75.31 F-measure.
Table 2 (see next page) shows the results for
the CRAFT corpus when all twenty ontologies are
matched against the corpus data, including the many
ontologies that are not annotated in the data. We
give results for just the five annotated ontologies
below. Rather than calculating precision, recall,
and F-measure, we calculate only accuracy. This
is because when classes other than the gold stan-
dard class is returned, we have no way of know-
ing if they are incorrect without manually examin-
ing them?that is, we have no way to identify false
positives. If the set of classes returned included the
gold standard class, a correct answer was counted. If
the classifier returned zero or more classes and none
of them was the gold standard, an incorrect answer
was counted. Results are given separately for each
of the four techniques. This allows us to evaluate
the contribution of each technique to the overall re-
sults; the value in each column is cumulative, so the
value for Stemmed head includes the contribution of
all four of the techniques that make up the general
approach. Accuracies of 77.12% to 95.73% were
achieved, depending on the ontology. We see that
the linguistic technique of locating the head noun
makes a contribution to all categories, but makes an
especially strong contribution to the Gene Ontology
and Cell Type Ontology classes. Stemming of head-
words is also effective for all five categories. We see
that exact match is effective only for those semantic
classes for which terminology is relatively fixed, i.e.
the NCBI taxonomy and chemical names. In some
of the others, matching natural language text is very
difficult by any technique. For example, of the 8,665
Sequence Ontology false negatives in the data re-
flected in the P/R/F values in Table 1, a full 2,050
are due to the single character +, which does not
appear in any of the twenty ontologies that we ex-
amined and that was marked by the annotators as a
Sequence Ontology term, wild type (SO:0000817).
3.2 Ontology results
As the second form of evaluation, we used the
terms from the ontologies themselves as the inputs
to which we attempted to assign a semantic class. In
this case, no annotation is required, and it is straight-
forwardly the case that each term in a given ontology
should be assigned the semantic class of that ontol-
ogy. We used only the head noun technique. We did
not use the exact match or stripping heuristics, since
they are guaranteed to return the correct answer, nor
did we use stemming. Thus, this section of the eval-
uation gives us a good indication of the performance
of the head noun approach.
As might be expected, almost all twenty on-
tologies returned results in the 97-100% correct
rate. However, we noted much lower performance
in two ontologies, the Sequence Ontology and the
Molecule Role Ontology. This lower performance
reflects a number of preprocessing errors or omis-
sions. The fact that we were able to detect these low-
performing ontologies indicates that our evaluation
technique in this experiment?trying to match terms
from an ontology against that ontology itself?is a
robust evaluation technique and should be used in
similar studies.
3.2.1 Structured test suite results
The third approach to evaluation involved use of
the structured test suite. The structured test suite re-
vealed a number of trends in the performance of the
system.
41
Ontology Annotations Precision Recall F-measure
Gene Ontology 39,626 66.31 73.06 69.52
Sequence Ontology 40,692 63.00 72.21 67.29
Cell Type Ontology 8,383 53.58 87.27 66.40
NCBI Taxonomy 11,775 96.24 92.51 94.34
ChEBI 19,307 70.07 90.53 79.00
Total (micro-averaged) 119,783 67.06 78.49 72.32
Total (macro-averaged) 69.84 83.12 75.31
Table 1: Results on the CRAFT corpus when only the CRAFT ontologies are used as input. Results are for stemmed
heads. Precision, recall, and F-measure are given for each semantic category in the corpus. Totals are micro-averaged
(over all tokens) and macro-averaged (over all categories), respectively. P/R/F are cumulative, so that the results for
stemmed heads reflect the application of all four techniques.
Ontology Exact Stripped Head noun Stemmed head
Gene Ontology 24.26 24.68 59.18 77.12
Sequence Ontology 44.28 47.63 56.63 73.33
Cell Type Ontology 25.26 25.80 70.09 88.38
NCBI Taxonomy 84.67 84.71 90.97 95.73
ChEBI 86.93 87.44 92.43 95.49
Table 2: Results on the CRAFT corpus when all twenty ontologies are used as input. Accuracy is given for each
technique. Accuracy is cumulative, so that accuracy in the final column reflects the application of all four techniques.
? The headword technique works very well for
recognizing syntactic variants. For example, if
the GO term induction of apoptosis is written
as apoptosis induction, the headword technique
allows it to be picked up.
? The headword technique works in situations
where text has been inserted into a term. For
example, if the GO term ensheathment of neu-
rons appears as ensheathment of some neu-
rons, the headword technique will allow it to be
picked up. If the GO term regulation of growth
shows up as regulation of vascular growth, the
headword technique will allow it to be picked
up.
? The headword stemming technique allows us to
pick up many verb phrases, which is important
for event detection and event coreference. For
example, if the GO term cell migration appears
in text as cells migrate, the technique will de-
tect it. The test suite also showed that failures
to recognize verb phrases still occur when the
morphological relationship between the nomi-
nal term and the verb are irregular, as for exam-
ple between the GO term growth and the verb
grows.
? The technique?s ability to handle coordination
is very dependent on the type of coordination.
For example, simple coordination (e.g. cell mi-
gration and proliferation) is handled well, but
complex coordination (e.g. cell migration, pro-
liferation and adhesion) is handled poorly.
? Stemming is necessary for recognition of plu-
rals, regardless of the length of the term in
words.
? The approach currently fails on irregular plu-
rals, due to failure of the Porter stemmer to han-
dle plurals like nuclei and nucleoli well.
? The approach handles classification of terms
that others have characterized as ?ungram-
matical,? such as transposition, DNA-mediated
(GO:0006313). This is important, because ex-
act matches will always fail on these terms.
42
4 Discussion
4.1 Related work
We are not aware of similar work that tries to assign
a large set of broad semantic categories to individ-
ual text strings. There is a body of work on selecting
a single ontology for a domain or text. (Mart??nez-
Romero et al, 2010) proposes a method for selecting
an ontology given a list of terms, all of which must
appear in the ontology. (Jonquet et al, 2009) de-
scribes an ontology recommender that first annotates
terms in a text with the Open Biomedical Annotator
service, then uses the sum of the scores of the indi-
vidual annotations to recommend a single ontology
for the domain as a whole.
4.2 Possible alternate approaches
Three possible alternative approaches exist, all of
which would have as their goal the returning of a sin-
gle best semantic class for every input. However, for
the use cases that we have identified?coreference
resolution, document classification, information ex-
traction, and curator assistance?we are more inter-
ested in wide coverage of a broad range of semantic
classes, so these approaches are not evaluated here.
However, we describe them for completeness and
for the use of researchers who might be interested
in pursuing single-class assignment.
4.2.1 Frequent words
One alternative approach would be to use simple
word frequencies. For example, for each ontology,
one could determine the N most frequent words, fil-
tering out stop words. At run time, check the words
in each noun phrase in the text against the lists of fre-
quent words. For every word from the text that ap-
peared in the list of frequent words from some ontol-
ogy, assign a score to each ontology in which it was
found, weighting it according to its position in the
list of frequent words. In theory, this could accom-
modate for the non-uniqueness of word-to-ontology
mappings, i.e. the fact that a single word might ap-
pear in the lists for multiple ontologies. However,
we found the technique to perform very poorly for
differentiating between ontologies and do not rec-
ommend it.
4.2.2 Measuring informativeness
If the system is desired to return only one sin-
gle semantic class per text string, then one approach
would be to determine the informativeness of each
word in each ontology. That is, we want to find the
maximal probability of an ontology given a word
from that ontology. This approach is very difficult
to normalize for the wide variability in size of the
many ontologies that we wanted to be able to deal
with.
4.2.3 Combining scores
Finally, one could conceivably combine scores for
matches obtained by the different strategies, weight-
ing them according to their stringency, i.e. exact
match receiving a higher weight than head noun
match, which in turn would receive a higher weight
than stemmed head noun match. This weighting
might also include informativeness, as described
above.
4.3 Why the linguistic method works
As pointed out above, the lightweight linguistic
method makes a large contribution to the perfor-
mance of the approach for some ontologies, partic-
ularly those for which the exact match and stripping
techniques do not perform well. It works for two
reasons, one related to the approach itself and one
related to the nature of the OBO ontologies. From
a methodological perspective, the approach is effec-
tive because headwords are a good reflection of the
semantic content of the noun phrase and they are
relatively easy to access via simple heuristics. Of
course simple heuristics will fail, as we can observe
most obviously in the cases where we failed to iden-
tify members of the ontologies in the second eval-
uation step. However, overall the approach works
well enough to constitute a viable tool for coref-
erence systems and other applications that benefit
from the ability to assign broad semantic classes to
text strings.
The approach is also able to succeed because of
the nature of the OBO ontologies. OBO ontologies
are meant to be orthogonal (Smith et al, 2007). A
distributional analysis of the distribution of terms
and words between the ontologies (data not shown
here, although some of it is discussed below), as well
as the false positives found in the corpus study, sug-
43
gests that orthogonality between the OBO ontolo-
gies is by no means complete. However, it holds
often enough for the headword method to be effec-
tive.
4.4 Additional error analysis
In the section on the results for the structured test
suite, we give a number of observations on contribu-
tions to errors, primarily related either to the char-
acteristics of individual words or to particular syn-
tactic instantiations of terms. Here, we discuss some
aspects of the distribution of lexical items and of the
corpus that contributed to errors.
? The ten most common headwords appear in
from 6-16 of the twenty ontologies. However,
they typically appear in one ontology at a fre-
quency many orders of magnitude greater than
their frequency in the other ontologies. Taking
this frequency data into account for just these
ten headwords would likely decrease false pos-
itives quite significantly.
? More than 50% of Gene Ontology terms share
one of only ten headwords. Many of our Gene
Ontology false negatives on the corpus are be-
cause the annotated text string does not contain
a word such as process or complex that is the
head word of the canonical term.
4.5 Future work
The heuristics that we implemented for extracting
headwords from OBO terms were very simple, in
keeping with our initial goal of developing an easy,
fast method for semantic class assignment. How-
ever, it is clear that we could achieve substantial per-
formance improvements from improving the heuris-
tics. We may pursue this track, if it becomes clear
that coreference performance would benefit from
this when we incorporate the semantic classification
approach into a coreference system.
On acceptance of the paper, we will make Perl and
Java versions of the semantic class assigner publicly
available on SourceForge.
4.6 Conclusion
The goal of this paper was to develop a simple ap-
proach to assigning text strings to an unprecedent-
edly large range of semantic classes, where mem-
bership in a semantic class is equated with belonging
to the semantic domain of a specific ontology. The
approach described in this paper is able to do that
at a micro-averaged F-measure of 72.32 and macro-
averaged F-measure of 75.31 as evaluated on a man-
ually annotated corpus where false positives can be
determined, and with an accuracy of 77.12-95.73%
when only true positives and false negatives can be
determined.
References
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: The MetaMap program.
In Proc AMIA 2001, pages 17?21.
William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-
son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-
mann, Elizabeth K. White, Olga Medvedeva, K. Bre-
tonnel Cohen, and Lawrence Hunter. 2008. Concept
recognition for extracting protein interaction relations
from biomedical text. Genome Biology, 9.
Christian Blaschke, Eduardo A. Leon, Martin Krallinger,
and Alfonso Valencia. 2005. Evaluation of BioCre-
ative assessment of task 2. BMC Bioinformatics, 6
Suppl 1.
J. Gregory Caporaso, William A. Baumgartner Jr..,
K. Bretonnel Cohen, Helen L. Johnson, Jesse Paque-
tte, and Lawrence Hunter. 2005. Concept recognition
and the TREC Genomics tasks. In The Fourteenth Text
REtrieval Conference (TREC 2005) Proceedings.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2.
K. Bretonnel Cohen, Helen L. Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E. Hunter. 2010a.
The structural and content aspects of abstracts versus
bodies of full text journal articles are different. BMC
Bioinformatics, 11(492).
K. Bretonnel Cohen, Christophe Roeder, William
A. Baumgartner Jr., Lawrence Hunter, and Karin Ver-
spoor. 2010b. Test suite design for biomedical ontol-
ogy concept recognition systems. In Proceedings of
the Language Resources and Evaluation Conference.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
N. Davis, H. Harkema, R. Gaizauskas, Y. K. Guo,
M. Ghanem, T. Barnwell, Y. Guo, and J. Ratcliffe.
2006. Three approaches to GO-tagging biomedical
abstracts. In Proceedings of the Second International
Symposium on Semantic Mining in Biomedicine, pages
21?28, Jena, Germany.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
44
C. Jonquet, N.H. Shah, and M.A. Musen. 2009. Pro-
totyping a biomedical ontology recommender ser-
vice. In Bio-Ontologies: Knowledge in Biology,
ISMB/ECCB SIG.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the international joint workshop on natu-
ral language processing in biomedicine and its appli-
cations, pages 70?75.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
BioNLP 2009 Companion Volume: Shared Task on En-
tity Extraction, pages 1?9.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survey of advances in biomedical
named entity recognition. In Pac Symp Biocomput.
Marcos Mart??nez-Romero, Jose? Va?zquez-Naya, Cris-
tian R. Munteanu, Javier Pereira, and Alejandro Pazos.
2010. An approach for the automatic recommendation
of ontologies using collaborative knowledge. In KES
2010, Part II, LNAI 6277, pages 74?81.
Nigam H. Shah, Nipun Bhatia, Clement Jonquet, Daniel
Rubin, Annie P. Chiang, and Mark A. Musen. 2009.
Comparison of concept recognizers for building the
Open Biomedical Annotator. BMC Bioinformatics,
10.
Barry Smith, Michael Ashburner, Cornelius Rosse,
Jonathan Bard, William Bug, Werner Ceusters,
Louis J. Goldberg, Karen Eilbeck, Amelia Ireland,
Christopher J. Mungall, The OBI Consortium, Neo-
cles Leontis, Philippe Rocca-Serra, Alan Ruttenberg,
Susanna-Assunta Sansone, Richard H. Scheuermann,
Nigam Shah, Patricia L. Whetzel, and Suzanna Lewis.
2007. The OBO Foundry: coordinated evolution of
ontologies to support biomedical data integration. Na-
ture Biotechnology, 25:1251?1255.
Larry Smith, Lorraine Tanabe, Rie Johnson nee Ando,
Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-
Shi Lin, Roman Klinger, Christof Friedrich, Kuzman
Ganchev, Manabu Torii, Hongfang Liu, Barry Had-
dow, Craig Struble, Richard Povinelli, Andreas Vla-
chos, William Baumgartner, Jr., Lawrence Hunter,
Bob Carpenter, Richard Tzong-Han Tsai, Hong-
Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun,
Sophia Katrenko, Pieter Adriaans, Christian Blaschke,
Rafael Torres Perez, Mariana Neves, Preslav Nakov,
Anna Divoli, Manuel Mana, Jacinto Mata-Vazquez,
and W. John Wilbur. 2008. Overview of BioCreative
II gene mention recognition. Genome Biology.
E. Stoica and M. Hearst. 2006. Predicting gene functions
from text using a cross-species approach. In Proceed-
ings of the 11th Pacific Symposium on Biocomputing.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of traditional
and Open Access scientific journals are similar. BMC
Bioinformatics, 10.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. BioCreatve task 1A: gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl. 1).
45
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 134?135,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
The CISP Annotation Schema Uncovers Hypotheses and Explanations in
Full-Text Scientific Journal Articles
Elizabeth White, K. Bretonnel Cohen, and Lawrence Hunter
Department of Pharmacology, Computational Bioscience Program,
University of Colorado School of Medicine, Aurora, Colorado, USA
elizabeth.white@ucdenver.edu,
kevin.cohen@gmail.com,
larry.hunter@ucdenver.edu
Abstract
Increasingly, as full-text scientific papers are
becoming available, scientific queries have
shifted from looking for facts to looking for
arguments. Researchers want to know when
their colleagues are proposing theories, out-
lining evidentiary relations, or explaining dis-
crepancies. We show here that sentence-level
annotation with the CISP schema adapts well
to a corpus of biomedical articles, and we
present preliminary results arguing that the
CISP schema is uniquely suited to recovering
common types of scientific arguments about
hypotheses, explanations, and evidence.
1 Introduction
In the scientific domain, the deluge of full-text
publications is driving researchers to find better
techniques for extracting or summarizing the main
claims and findings in a paper. Many researchers
have noted that the sentences of a paper play a small
set of different rhetorical roles (Teufel and Moens,
1999; Blais et al, 2007; Agarwal and Yu, 2009). We
are investigating the rhetorical roles of sentences in
the CRAFT corpus, a set of 97 full-text papers that
we have annotated using the CISP schema. Hand
alignment of the resulting annotations suggests that
patterns in these CISP-annotated sentences corre-
spond to common argumentative gambits in scien-
tific writing.
2 Methods
The CRAFT corpus is a set of 97 full-text papers de-
scribing the function of genes in the Mouse Genome
Informatics database (Blake et al, 2011). These
documents have already been annotated with syn-
tactic information (parse trees and part-of-speech
tags), linguistic phenomena (coreference), and se-
mantic entities (genes, chemicals, cell lines, biolog-
ical functions and molecular processes), making the
corpus a rich resource for extracting or inferring in-
formation from full scientific papers.
The CISP schema (Soldatova and Liakata, 2007;
Liakata et al, 2009) contains 11 categories, and sev-
eral of the categories describe the intentions of the
authors, making it well suited for markup of argu-
mentation. We chose to narrow these down to 9 cat-
egories (excluding Model and Object) during anno-
tation training; our guidelines are shown in Figure
1. We expect this schema to describe the pragmat-
ics in the text well, while still offering the poten-
tial for high interannotator agreement due to a man-
ageable number of categories. The process of mark-
ing the sentences in the CRAFT corpus according to
the CISP guidelines took one annotator about four
months.
3 Results and Discussion
Six of the 97 CRAFT papers do not follow the stan-
dard IMRaD paper structure (one was a review ar-
ticle, and five combined Results and Discussion);
these documents were eliminated from this analy-
sis. Annotation of the 91 remaining CRAFT papers
resulted in 20676 sentences. The distribution of the
annotated classes is shown in Table 1.
Our use of the CISP schema exposes an approach
for recovering two types of explanatory arguments.
The first sets the context with a sequence of Back-
134
Figure 1: Flow chart for CISP annotation of the CRAFT corpus.
CISP Type Count Percentage
Hypothesis 1050 5.08
Goal 992 4.80
Motivation 928 4.49
Background 2838 13.73
Method 637 3.08
Experiment 5270 25.49
Result 5471 26.46
Observation 1168 5.65
Conclusion 2322 11.23
Total 20676 100.0
Table 1: Distribution of CISP sentence types annotated in
91 CRAFT articles.
ground sentences, followed by a Hypothesis, Moti-
vation, or Goal; this echoes a motif found by Swales
(1990) and Teufel and Moens (1999). We also find
another pattern that consists of a combination of Re-
sults and Observations, either preceded or followed
by a Conclusion; Teufel and Moens (1999) also find
exemplars of this maneuver, and note that it paral-
lels Swales? notion of occupying a niche in the re-
search world. Hand alignment of CISP annotations
in Introduction and Result sections suggests that a
finite state machine may be capable of modeling the
transitions between CISP sentence types in these ar-
guments, and machine learning approaches to rep-
resent these and other patterns with hidden Markov
models or conditional random fields are underway.
References
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into Introduction, Methods, Results, and Discussion.
Bioinformatics, 25(23): 3174?3180.
Antoine Blais, Iana Atanassova, Jean-Pierre Descle?s,
Mimi Zhang, and Leila Zighem. 2007. Discourse
automatic annotation of texts: an application to sum-
marization. In Proceedings of the Twentieth Interna-
tional Florida Artificial Intelligence Research Society
Conference, May 7-9, 2007, Key West, Florida, USA,
350?355. AAAI Press.
Judith A. Blake, Carol J. Bult, James A. Kadin, Joel E.
Richardson, Janan T. Eppig, and the Mouse Genome
Database Group 2011. The Mouse Genome Database
(MGD): premier model organism resource for mam-
malian genomics and genetics. Nucleic Acids Res.,
39(Suppl. 1): D842?D848.
Maria Liakata, Claire Q, and Larisa N. Soldatova. Se-
mantic Annotation of Papers: Interface & Enrichment
Tool (SAPIENT). 2009. In Proceedings of BioNLP
2009, Boulder, Colorado,193?200.
Larisa Soldatova and Maria Liakata. 2007. An ontology
methodology and CISP - the proposed Core Informa-
tion about Scientific Papers. JISC intermediate project
report.
John M. Swales. 1990. Genre Analysis: English in
academic and research settings, 137?166. Cambridge
University Press, Cambridge.
Simone Teufel and Marc Moens. 1999. Argumentative
classification of extracted sentences as a first step to-
wards flexible abstracting. In Advances in Automatic
Text Summarization, I. Mani and D. Maybury, eds.
MIT Press, Cambridge, MA.
135
Proceedings of the Fifth Law Workshop (LAW V), pages 82?91,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A scaleable automated quality assurance technique for semantic
representations and proposition banks
K. Bretonnel Cohen
Computational Bioscience Program
U. of Colorado School of Medicine
Department of Linguistics
University of Colorado at Boulder
kevin.cohen@gmail.com
Lawrence E. Hunter
Computational Bioscience Program
U. of Colorado School of Medicine
larry.hunter@ucdenver.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
This paper presents an evaluation of an auto-
mated quality assurance technique for a type
of semantic representation known as a pred-
icate argument structure. These representa-
tions are crucial to the development of an im-
portant class of corpus known as a proposi-
tion bank. Previous work (Cohen and Hunter,
2006) proposed and tested an analytical tech-
nique based on a simple discovery proce-
dure inspired by classic structural linguistic
methodology. Cohen and Hunter applied the
technique manually to a small set of repre-
sentations. Here we test the feasibility of au-
tomating the technique, as well as the ability
of the technique to scale to a set of seman-
tic representations and to a corpus many times
larger than that used by Cohen and Hunter.
We conclude that the technique is completely
automatable, uncovers missing sense distinc-
tions and other bad semantic representations,
and does scale well, performing at an accu-
racy of 69% for identifying bad representa-
tions. We also report on the implications of
our findings for the correctness of the seman-
tic representations in PropBank.
1 Introduction
It has recently been suggested that in addition to
more, bigger, and better resources, we need a sci-
ence of creating them (Palmer et al, Download date
December 17 2010).
The corpus linguistics community has arguably
been developing at least a nascent science of anno-
tation for years, represented by publications such as
(Leech, 1993; Ide and Brew, 2000; Wynne, 2005;
Cohen et al, 2005a; Cohen et al, 2005b) that ad-
dress architectural, sampling, and procedural issues,
as well as publications such as (Hripcsak and Roth-
schild, 2005; Artstein and Poesio, 2008) that address
issues in inter-annotator agreement. However, there
is not yet a significant body of work on the subject
of quality assurance for corpora, or for that matter,
for many other types of linguistic resources. (Mey-
ers et al, 2004) describe three error-checking mea-
sures used in the construction of NomBank, and the
use of inter-annotator agreement as a quality control
measure for corpus construction is discussed at some
length in (Marcus et al, 1993; Palmer et al, 2005).
However, discussion of quality control for corpora is
otherwise limited or nonexistent.
With the exception of the inter-annotator-
agreement-oriented work mentioned above, none of
this work is quantitative. This is a problem if our
goal is the development of a true science of annota-
tion.
Work on quality assurance for computational lex-
ical resources other than ontologies is especially
lacking. However, the body of work on quality as-
surance for ontologies (Kohler et al, 2006; Ceusters
et al, 2004; Cimino et al, 2003; Cimino, 1998;
Cimino, 2001; Ogren et al, 2004) is worth consider-
ing in the context of this paper. One common theme
in that work is that even manually curated lexical re-
sources contain some percentage of errors.
The small size of the numbers of errors uncovered
in some of these studies should not be taken as a
significance-reducing factor for the development of
quality assurance measures for lexical resources?
82
rather, the opposite: as lexical resources become
larger, it becomes correspondingly more difficult to
locate errors in them. Finding problems in a very
errorful resource is easy; finding them in a mostly
correct resource is an entirely different challenge.
We present here an evaluation of a methodol-
ogy for quality assurance for a particular type of
lexical resource: the class of semantic representa-
tion known as a predicate argument structure (PAS).
Predicate argument structures are important in the
context of resource development in part because
they are the fundamental annotation target of the
class of corpus known as a proposition bank. Much
of the significance claim for this work comes from
the significance of proposition banks themselves in
recent research on natural language processing and
computational lexical semantics. The impact of
proposition banks on work in these fields is sug-
gested by the large number of citations of just the
three publications (Kingsbury and Palmer, 2002;
Kingsbury et al, 2002; Palmer et al, 2005)?at the
time of writing, 290, 220, and 567, respectively. Ad-
ditional indications of the impact of PropBank on
the field of natural language processing include its
use as the data source for two shared tasks ((Car-
reras and Ma`rquez, 2005)).
The methodology consists of looking for argu-
ments that never coo?ccur with each other. In struc-
tural linguistics, this property of non-coo?ccurrence
is known as complementary distribution. Comple-
mentary distribution occurs when two linguistic el-
ements never occur in the same environment. In
this case, the environment is defined as any sen-
tence containing a given predicate. Earlier work
showed a proof-of-concept application to a small set
of rolesets (defined below) representing the potential
PAS of 34 biomedical predicates (Cohen and Hunter
2006). The only inputs to the method are a set of
rolesets and a corpus annotated with respect to those
rolesets. Here, we evaluate the ability of the tech-
nique to scale to a set of semantic representations
137 times larger (4,654 in PropBank versus 34 in
Cohen and Hunter?s pilot project) and to a corpus
about 1500 times larger (1M words in PropBank ver-
sus about 680 in Cohen and Hunter?s pilot project)
than that considered in previous work. We also use
a set of independent judges to assess the technique,
where in the earlier work, the results were only as-
sessed by one of the authors.
Novel aspects of the current study include:
? Investigating the feasibility of automating the
previously manual process
? Scaling up the size of the set of semantic repre-
sentations evaluated
? Scaling up the size of the corpus against which
the representations are evaluated
? Using independent judges to assess the predic-
tions of the method
1.1 Definitions
For clarity, we define the terms roleset, frame file,
and predicate here. A roleset is a 2-tuple of a sense
for a predicate, identified by a combination of a
lemma and a number?e.g., love.01?and a set of in-
dividual thematic roles for that predicate?e.g., Arg0
lover and Arg1 loved. A frame file is the set of all
rolesets for a single lemma?e.g., for love, the role-
sets are love.01 (the sense whose antonym is hate)
and love.02, the ?semi-modal? sense in whether it
be melancholy or gay, I love to recall it (Austen,
1811). Finally, we refer to sense-labelled predicates
(e.g. love.01) as predicates in the remainder of the
paper.
PropBank rolesets contain two sorts of thematic
roles: (core) arguments and (non-core) adjuncts. Ar-
guments are considered central to the semantics of
the predicate, e.g. the Arg0 lover of love.01. Ad-
juncts are not central to the semantics and can occur
with many predicates; examples of adjuncts include
negation, temporal expressions, and locations.
In this paper, the arity of a roleset is determined
by its count of arguments, disregarding adjuncts.
1.2 The relationship between observed
argument distributions and various
characteristics of the corpus
This work is predicated on the hypothesis that argu-
ment distributions are affected by goodness of the fit
between the argument set and the actual semantics
of the predicate. However, the argument distribu-
tions that are observed in a specific data set can be
affected by other factors, as well. These include at
least:
? Inflectional and derivational forms attested in
the corpus
83
? Sublanguage characteristics
? Incidence of the predicate in the corpus
A likely cause of derivational effects on observed
distributions is nominalization processes. Nomi-
nalization is well known for being associated with
the omission of agentive arguments (Koptjevskaja-
Tamm, 1993). A genre in which nominalization is
frequent might therefore show fewer coo?ccurrences
of Arg0s with other arguments. Since PropBank
does not include annotations of nominalizations, this
phenomenon had no effect on this particular study.
Sublanguage characteristics might also affect ob-
served distributions. The sublanguage of recipes
has been noted to exhibit rampant deletions of def-
inite object noun phrases both in French and in En-
glish, as has the sublanguage of technical manuals
in English. (Neither of these sublanguages have
been noted to occur in the PropBank corpus. The
sublanguage of stock reports, however, presumably
does occur in the corpus; this sublanguage has been
noted to exhibit distributional subtleties of predi-
cates and their arguments that might be relevant to
the accuracy of the semantic representations in Prop-
Bank, but the distributional facts do not seem to in-
clude variability in argument coo?ccurrence so much
as patterns of argument/predicate coo?ccurrence (Kit-
tredge, 1982).)
Finally, incidence of the predicate in the corpus
could affect the observed distribution, and in partic-
ular, the range of argument coo?ccurrences that are
attested: the lower the number of observations of a
predicate, the lower the chance of observing any two
arguments together, and as the number of arguments
in a roleset increases, the higher the chance of failing
to see any pair together. That is, for a roleset with
an arity of three and an incidence of n occurrences
in a corpus, the likelihood of never seeing any two
of the three arguments together is much lower than
for a roleset with an arity of six and an incidence of
n occurrences in the corpus. The number of obser-
vations required in order to be able to draw conclu-
sions about the observed argument distributions with
some degree of confidence is an empirical question;
prior work (Cohen and Hunter 2006) suggests that
as few as ten tokens can be sufficient to uncover er-
roneous representations for rolesets with an arity of
four or less, although that number of observations
of one roleset with an arity of four showed multiple
non-coo?ccurring arguments that were not obviously
indicative of problems with the representation (i.e.,
a false positive finding).
Besides the effects of these aspects of the corpus
contents on the observed distributions, there are also
a number of theoretical and practical issues in the
design and construction of the corpus (as distinct
from the rolesets, or the distributional characteris-
tics of the contents) which have nontrivial implica-
tions for the methodology being evaluated here. In
particular, the implications of the argument/adjunct
distinction, of the choice of syntactic representation,
and of annotation errors are all discussed in Sec-
tion 4. Note that we are aware that corpus-based
studies generally yield new lexical items and us-
ages any time a new corpus is introduced, so we
do not make the naive assumption that PropBank
will give complete coverage of all coo?ccurring argu-
ments, and in fact our evaluation procedure took this
into account explicitly, as described in Section 2.3.
2 Materials and Methods
2.1 Materials
We used Rev. 1.0 of the PropBank I corpus, and the
associated framesets in the frames directory.
2.2 Methods
2.2.1 Determining the distribution of
arguments for a roleset
In determining the possible coo?ccurring argument
pairs for a roleset, we considered only arguments,
not adjuncts. As we discuss in Section 4.1, this
is a non-trivial decision with potential implications
for the ability of the algorithm to detect problem-
atic representations in general, and with implications
for PropBank in particular. The rationale behind the
choice to consider only arguments is that our goal
is to evaluate the representation of the semantics of
the predicates, and that by definition, the PropBank
arguments are essential to defining that semantics,
while by definition, the adjuncts are not.
In the first processing step, for each roleset, we
used the corresponding framefile as input and gen-
erated a look-up table of the possible argument
pairs for that predicate. For example, the predi-
cate post.01 has the three arguments Arg0, Arg1, and
84
Arg2; we generated the set {<Arg0, Arg1>, <Arg0,
Arg2>, <Arg1, Arg2>} for it.
In the second processing step, we iterated over all
annotations in the PropBank corpus, and for each to-
ken of each predicate, we extracted the complete set
of arguments that occurred in association with that
token. We then constructed the set of coo?ccurring ar-
guments for that annotation, and used it to increment
the counts of each potential argument pair for the
predicate in question. For example, the PropBank
annotation for Oils and fats also did well, posting
a 5.3% sales increase (wsj/06/wsj 0663.mrg)
contains an Arg0 and an Arg1, so we incremented
the count for that argument pair by 1; it contains no
other argument pairs, so we did not increment the
counts for <Arg0, Arg2> or <Arg1, Arg2>.
The output of this step was a table with the count
of occurrence of every potential pair of arguments
for every roleset; members of pairs whose count was
zero were then output as arguments in complemen-
tary distribution. For example, for post.01, the pairs
<Arg0, Arg2> and <Arg1, Arg2> never occurred,
even as traces, so the arguments Arg0 and Arg2 are
in complementary distribution for this predicate, as
are the arguments Arg1 and Arg2.
To manipulate the data, we used Scott Cotton?s
Java API, with some extensions, which we docu-
mented in the API?s Javadoc.
2.3 Determining the goodness of rolesets
exhibiting complementary distribution
In (Cohen and Hunter, 2006), determinations of the
goodness of rolesets were made by pointing out the
distributional data to the corpus creators, showing
them the corresponding data, and reaching consen-
sus with them about the appropriate fixes to the rep-
resentations. For this larger-scale project, one of the
goals was to obtain goodness judgements from com-
pletely independent third parties.
Towards that end, two judges with experience in
working with PropBank were assigned to judge the
predictions of the algorithm. Judge 1 had two years
of experience, and Judge 2 had four years of expe-
rience. The judges were then given a typology of
classification to assign to the predicates: good, bad,
and conditionally bad. The definitions of these cate-
gories, with the topology of the typology, were:
? Good: This label is assigned to predicates that
the algorithm predicted to have bad representa-
tions, but that are actually good. They are false
positives for the method.
? Not good: (This label was not actually as-
signed, but rather was used to group the fol-
lowing two categories.)
? Bad: This label is assigned to predicates
that the algorithm predicted to have bad
representations and that the judges agreed
were bad. They are true positives for the
method.
? Conditionally bad: This label is assigned
to predicates that the algorithm predicted
to have bad representations and that the
judges agreed were bad based on the ev-
idence available in PropBank, but that the
judges thought might be good based on
native speaker intiution or other evidence.
In all of these cases, the judges did suggest
changes to the representations, and they
were counted as not good, per the typol-
ogy, and are also true positives.
Judges were also asked to indicate whether bad
representations should be fixed by splitting predi-
cates into more word senses, or by eliminating or
merging one or more arguments.
We then took the lists of all predicted bad predi-
cates that appeared at least 50, 100, or 200 times in
the PropBank corpus. These were combined into a
single list of 107 predicates and randomized. The
judges then split the list into halves, and each judge
examined half of the list. Additionally, 31 predi-
cates, or 29% of the data set, were randomly selected
for double annotation by both judges to assess inter-
judge agreement. Judges were shown both the predi-
cates themselves and the sets of non-coo?ccurring ar-
guments for each predicate.
3 Results
3.1 Accuracy
The overall results were that out of 107 predicates,
33 were judged GOOD, i.e. were false positives.
44 were judged BAD and 30 were judged CONDI-
TIONAL, i.e. were true positives. This yields a ratio
of 2.24 of true positives to false positives: the pro-
85
Table 1: Ratios of BAD plus CONDITIONAL to GOOD
for the pooled judgements as broken down by arity
Arity Ratio
3 1.29
4 1.47
5 4.0
6 8.0
7 None found
cedure returns about two true positives for every one
false positive. Expressed in terms of accuracy, this
corresponds to 69% for correctly labelling true pos-
itives.
We broke down the data by (1) arity of the role-
set, and (2) minimum number of observations of a
role set. This allowed us to test whether predictive
power decreased as arity increased, and to test the
dependency of the algorithm on the minimum num-
ber of observations; we suspected that it might be
less accurate the fewer the number of observations.
Table 1 shows the ratios of true positives to false
positives, broken down by arity. The data confirms
that the algorithm is effective at finding bad repre-
sentations, with the number of true positives out-
numbering the number of false positives at every
arity. This data is also important because it allows
us to test a hypothesis: is it the case that predictive
power becomes worse as arity increases? As the ta-
ble shows, the ratio of true positives to false posi-
tives actually increases as the arity of the predicate
increases. Therefore, the data is consistent with the
hypothesis that not only does the predictive power of
the algorithm not lessen as arity increases, but rather
it actually becomes greater.
Table 2 shows the ratios of true positives to false
positives again, this time broken down by minimum
number of occurrences of the predicates. Again, the
data confirms that the algorithm is effective at find-
ing bad representations?it returns more bad repre-
sentations than good representations at every level of
minimum number of observations. This data is also
important because it allows us to test the hypothe-
sis of whether or not predictive power of the algo-
rithm decreases with the minimum number of obser-
vations. As we hypothesized, it does show that the
predictive power decreases as the minimum number
Table 2: Ratios of BAD plus CONDITIONAL to GOOD
for the pooled judgements as broken down by minimum
number of observations
ratio
Minimum 50 1.88
Minimum 100 2.63
Minimum 200 2.63
of observations decreases, with the ratio of true pos-
itives to false positives dropping from 2.63 with a
minimum of 200 or 100 observations to 1.88 with a
minimum of 50 observations. However, the ratio of
true positives to false positives remains close to 2:1
at every level.
3.2 Suggested fixes to the representations
Of the 74 true positives, the judges felt that 17 of
the bad representations should be fixed by splitting
the predicate into multiple senses. For the 57 re-
maining true positives, the judges felt that an argu-
ment should be removed from the representation or
converted to an adjunct. This demonstrates that the
method is applicable both to the problem of reveal-
ing missing sense distinctions and to the problem of
identifying bad arguments.
3.3 Scalability
The running time was less than one and a half min-
utes for all 4,654 rolesets on the 1-million-word cor-
pus.
3.4 Inter-judge agreement
A subset of 31 predicates was double-annotated by
the two judges to examine inter-judge agreement.
The judges then examined the cases on which they
initially disagreed, and came to a consensus where
possible. Initially, the judges agreed in 63.3% of the
cases, which is above chance but not the 80% agree-
ment that we would like to see. The judges then went
through a reconciliation process. They were able to
come to a consensus in all cases.
3.5 Putting the results in context
To help put these results in context, we give here the
distribution of arities in the PropBank rolesets and
the minimum number of observations of each in the
PropBank corpus.
86
Table 3: Distribution of arities by percentage and by
count in the 4,654 PropBank rolesets.
Arity percentage (count)
0 0.28% (13)
1 (Arg0) 155
1 (Arg1) 146
1 (all) 6.5% (301)
2 45.14% (2,101)
3 37.02% (1,723)
4 7.05% (328)
5 3.5% (163)
6 0.5% (24)
7 0.0002% (1)
Total 100% (4,654)
Table 3 shows the distribution of arities in the
PropBank rolesets. It distinguishes between non-
ergatives and ergatives (although for the purpose
of calculating percentages, they are combined into
one single-arity group). The mode is an arity of 2:
45.14% of all rolesets (2,101/4,654) have an arity of
2. 3 is a close second, with 37.02% (1,723/4,654).
(The single roleset with an arity of seven is notch.02,
with a gloss of ?move incrementally.?)
Table 4 gives summary statistics for the occur-
rence of complementary distribution, showing the
distribution of rolesets in which there were at least
one argument pair in complementary distribution
and of the total number of argument pairs in comple-
mentary distribution. Since (as noted in Section 1.2)
the incidence of a predicate has a potential effect
on the incidence of argument pairs in apparent com-
plementary distribution, we display the counts sepa-
rately for four cut-offs for the minimum number of
observations of the predicate: 200, 100, 50, and 10.
To further explicate the operation of the discovery
procedure, we give here some examples of rolesets
that were found to have arguments in complemen-
tary distribution.
3.5.1 accept.01
Accept.01 is the only roleset for the lemma ac-
cept. Its sense is take willingly. It has four argu-
ments:
? Arg0 acceptor
Table 4: Summary statistics: counts of predicates with
at least one argument pair in complementary distribution
and of total argument pairs in complementary distribution
for four different minimum numbers of observations of
the predicates.
Minimum observations Predicates Argument pairs
200 29 69
100 58 125
50 107 268
10 328 882
? Arg1 thing accepted
? Arg2 accepted-from
? Arg3 attribute
The predicate occurs 149 times in the corpus. The
algorithm found Arg2 and Arg3 to be in complemen-
tary distribution.
Manual investigation showed the following distri-
butional characteristics for the predicate and its ar-
guments:
? (Arg0 or Arg1) and Arg2: 5 tokens
? (Arg0 or Arg1) and Arg3: 8 tokens
? Arg2 with neither Arg0 nor Arg1: 0 tokens
? Arg3 with neither Arg0 nor Arg1: 0 tokens
? Arg0 or Arg1 with neither Arg2 nor Arg 3: 136
tokens
Examination of the 5 tokens in which Arg2
coo?ccurred with Arg0 or Arg1 and the 8 tokens
in which Arg3 coo?ccurred with Arg0 or Arg1 sug-
gested an explanation for the complementary distri-
bution of arguments Arg2 and Arg3. When Arg2
appeared, the sense of the verb seemed to be one
of physical transfer: Arg2 coo?ccurred with Arg1s
like substantial gifts (wsj 0051.mrg) and a $3
million payment (wsj 2071.mrg). In contrast,
when Arg3 appeared, the sense was not one of
physical transfer, but of some more metaphorical
sense?Arg3 coo?ccurred with Arg1s like the war
(wsj 0946.mrg) and Friday?s dizzying 190-point
plunge (wsj 2276.mrg). There is no accept.02;
creating one with a 3-argument roleset including the
current Arg3 seems warranted. Keeping the Arg3
for accept.01 might be warranted, as well, but prob-
ably as an adjunct (to account for usages like John
accepted it as a gift.)
87
3.5.2 affect.01
Affect.01 is one of two senses for the lemma af-
fect. Its sense is have an effect on. It has three argu-
ments:
? Arg0 thing affecting
? Arg1 thing affected
? Arg2 instrument
The predicate occurs 149 times in the corpus. The
algorithm found Arg0 and Arg2, as well as Arg1 and
Arg2, to be in complementary distribution.
Manual investigation revealed that in fact, Arg2
never appears in the corpus at all. Presumably, ei-
ther Arg0 and Arg2 should be merged, or?more
likely?Arg2 should not be an argument, but rather
an adjunct.
3.6 Incidental findings
3.6.1 Mistakes uncovered in frame files
In the process of calculating the set of possible
argument pairs for each predicate in the PropBank
frame files, we found a roleset that erroneously had
two Arg1s. The predicate in question was pro-
scribe.01. The roles in the frame file were:
? Arg0 causer
? Arg1 thing proscribed
? Arg1 proscribed from
It was clear from the annotations in the exam-
ple sentence that the ?second? Arg1 was intended to
be an Arg2: [The First AmendmentArg0] proscribes
[the governmentArg1] from [passing laws abridging
the right to free speechArg2].
3.6.2 Unlicensed arguments used in the corpus
We found eighteen tokens in the corpus that were
annotated with argument structures that were not li-
censed by the roleset for the corresponding predi-
cate. For example, the predicate zip.01 has only
a single argument in its semantic representation?
Arg0, described as entity in motion. However, the
corpus contains a token of zip.01 that is annotated
with an Arg0 and an Arg1.
4 Discussion/Conclusions
4.1 The effect of the argument/adjunct
distinction
The validity and usefulness of the distinction be-
tween arguments and adjuncts is an ongoing con-
troversy in biomedical computational lexical se-
mantics. The BioProp project (Chou et al, 2006;
Tsai et al, 2006) makes considerable use of ad-
juncts, essentially identically to PropBank; however,
most biomedical PAS-oriented projects have rela-
tively larger numbers of arguments and lesser use
of adjuncts (Wattarujeekrit et al, 2004; Kogan et al,
2005; Shah et al, 2005) than PropBank. Overall,
one would predict fewer non-coo?ccurring arguments
with a set of representations that made a stronger
distinction between arguments and adjuncts; over-
all arity of rolesets would be smaller (see above for
the effect of arity on the number of observations re-
quired for a predicate), and the arguments for such a
representation might be more ?core? to the seman-
tics of the predicate, and might therefore be less
likely to not occur overall, and therefore less likely
to not coo?ccur.
4.2 The effect of syntactic representation on
observed argument distributions
The original work by Cohen and Hunter assumed a
very simple, and very surface, syntactic representa-
tion. In particular, there was no representation of
traces. In contrast, PropBank is built on Treebank
II, which does include representation of traces, and
arguments can, in fact, be filled by traces. This could
be expected to reduce the number of tokens of appar-
ently absent arguments, and thereby the number of
non-coo?occurring arguments. This doesn?t seem to
have had a strong enough effect to interfere with the
ability of the method to uncover errors.
4.3 The effect of arity
The mode for distribution of arities in the Prop-
Bank framefiles was 2 (see Table 3). In contrast, the
modes for distribution of rolesets with at least one
argument pair in complementary distribution across
arities and for distribution of argument pairs in com-
plementary distribution across arities was 4 or 5
for the full range of minimum observations of the
predicates from 200 to 10 (data omitted for space).
88
This supports the initial assumption that higher-arity
predicates are more likely to have argument pairs in
complementary distribution?see Section 1.2 above.
One aspect of a granular analysis of the data is
worth pointing out with respect to the effects of ar-
ity: as a validation check, note that for all arities,
the number of predicates and the number of argu-
ment pairs rises as the minimum required number of
tokens of the predicate in the corpus goes down.
4.4 Conclusions
The goals of this study were to investigate the au-
tomatability and scalability of a technique for PAS
quality assurance that had previously only been
shown to work for a small lexical resource and
a small corpus, and to use it to characterize the
quality of the shallow semantic representations in
the PropBank framefiles. The evaluation procedure
was found to be automatable: the process of find-
ing argument pairs in complementary distribution is
achievable by running a single Java application. In
addition, the use of a common representation for ar-
gument sets in a framefile and argument sets in a
PropBank annotation enabled the fortuitous discov-
ery of a number of problems in the framefiles and in
the corpus (see Section 3.6) as a side-effect of appli-
cation of the technique.
The process was also found to scale well, with
a running time of less than one and a half minutes
for a set of 4,654 rolesets and a 1-million-word cor-
pus on a moderately priced laptop; additionally, the
resource maintainer?s efforts can easily be focussed
towards the most likely and the most prevalent error
sources by adjusting the minimum number of obser-
vations required before reporting a case of comple-
mentary distribution. The process was also found to
be able to identify missing sense distinctions and to
identify bad arguments.
In addition to our findings regarding the quality
assurance technique, a granular breakdown of the
errors found by the algorithm by arity and mini-
mum number of observations (data not shown due to
space) allows us to estimate the number of errors in
the PropBank framefiles. A reasonable upper-bound
estimate for the number of errorful rolesets is the
number of predicates that were observed at least 10
times and were found to have at least one pair of ar-
guments in complementary distribution (the bottom
row of Table 4), adjusted by the accuracy of the tech-
nique that we reported in Section 3.1, i.e. 0.69. This
yields a worst-case scenario of (0.69*328)/4,654
rolesets, or 4.9% of the rolesets in PropBank, be-
ing in need of revision. The best-case scenario
would assume that we can only draw conclusions
about the predicates with high numbers of observa-
tions and high arity, again adjusted downward for
the accuracy of the technique; taking 5 or more argu-
ments as high arity, this yields a best-case scenario
of (0.69*17)/4,654 rolesets, or 0.3% of the rolesets
in PropBank, being in need of revision. A different
sort of worst-case scenario assumes that the major
problem in maintaining a proposition bank is not fix-
ing inadequate representations, but finding them. On
this assumption, the problematic representations are
the ones with small numbers of tokens and low ar-
ity. Taking 3 or fewer arguments as low arity yields a
worst-case scenario of 99/4,654 rolesets (no adjust-
ment for accuracy required), or 2.13% of the rolesets
in PropBank, being essentially uncharacterizable as
to the goodness of their semantic representation1.
Besides its obvious role in quality assurance for
proposition banks, there may be other uses for this
technique, as well. The output of the technique may
also be useful in sense grouping and splitting and in
detecting metaphorical uses of verbs (e.g. the accept
example). As the PropBank model is extended to an
increasingly large set of languages (currently Ara-
bic, Basque, Catalan, Chinese, Hindi, Korean, and
Russian), the need for a quality assurance mecha-
nism for proposition banks?both to ensure the qual-
ity of their contents, and to assure funding agencies
that they are evaluatable?will only grow larger.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Jane Austen. 1811. Sense and Sensibility.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: semantic role label-
1The situation is arguably actually somewhat worse than
this, since it does not take into account predicates which occur
fewer than ten times in the corpus; however, there is a reason-
able counter-argument that those predicates are too rare for any
individual roleset to have a large impact on the overall goodness
of the resource.
89
ing. In Proceedings of the 9th conference on computa-
tional natural language learning, pages 152?164.
Werner Ceusters, Barry Smith, Anand Kumar, and
Christoffel Dhaen. 2004. Mistakes in medical on-
tologies: where do they come from and how can they
be detected? In D.M. Pisanelli, editor, Ontologies in
medicine: proceedings of the workshop on medical on-
tologies, pages 145?163. IOS Press.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedi-
cal proposition bank. In Proceedings of the workshop
on frontiers in linguistically annotated corpora 2006,
pages 5?12. Association for Computational Linguis-
tics.
J.J. Cimino, H. Min, and Y. Perl. 2003. Consistency
across the hierarchies of the UMLS Semantic Network
and Metathesaurus. Journal of Biomedical Informat-
ics, 36:450?461.
James J. Cimino. 1998. Auditing the Unified Medical
Language System with semantic methods. Journal of
the American Medical Informatics Association, 5:41?
51.
James J. Cimino. 2001. Battling Scylla and Charybdis:
the search for redundancy and ambiguity in the 2001
UMLS Metathesaurus. In Proc. AMIA annual sympo-
sium, pages 120?124.
K. Bretonnel Cohen and Lawrence Hunter. 2006. A
critical revew of PASBio?s argument structures for
biomedical verbs. BMC Bioinformatics, 7(Suppl. 3).
K. B. Cohen, Lynne Fox, Philip V. Ogren, and Lawrence
Hunter. 2005a. Corpus design for biomedical natural
language processing. In Proceedings of the ACL-ISMB
workshop on linking biological literature, ontologies
and databases, pages 38?45. Association for Compu-
tational Linguistics.
K. Bretonnel Cohen, Lynne Fox, Philip V. Ogren, and
Lawrence Hunter. 2005b. Empirical data on corpus
design and usage in biomedical natural language pro-
cessing. In AMIA 2005 symposium proceedings, pages
156?160.
George Hripcsak and Adam S. Rothschild. 2005. Agree-
ment, the F-measure, and reliability in information re-
trieval. Journal of the American Medical Informatics
Association, 12(3):296?298.
Nancy Ide and Chris Brew. 2000. Requirements, tools,
and architectures for annotated corpora. In Proc. data
architectures and software support for large corpora,
pages 1?5.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the LREC.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference.
Richard Kittredge. 1982. Variation and homogene-
ity of sublanguages. In Richard Kittredge and John
Lehrberger, editors, Sublanguage: studies of language
in restricted semantic domains, pages 107?137.
Yacov Kogan, Nigel Collier, Serguei Pakhomov, and
Michael Krauthammer. 2005. Towards semantic role
labeling & IE in the medical literature. In AMIA 2005
Symposium Proceedings, pages 410?414.
Jacob Kohler, Katherine Munn, Alexander Ruegg, An-
dre Skusa, and Barry Smith. 2006. Quality control
for terms and definitions in ontologies and taxonomies.
BMC Bioinformatics, 7(1).
Maria Koptjevskaja-Tamm. 1993. Nominalizations.
Routledge.
Geoffrey Leech. 1993. Corpus annotation schemes. Lit-
erary and linguistic computing, pages 275?281.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. Annotating noun argument structure
for NomBank. In Proceedings of Language Resources
and Evaluation, LREC.
Philip V. Ogren, K. Bretonnel Cohen, George K.
Acquaah-Mensah, Jens Eberlein, and Lawrence
Hunter. 2004. The compositional structure of Gene
Ontology terms. Pacific Symposium on Biocomputing,
pages 214?225.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: an annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Stephanie Strassel, and Randee Tangi.
Download date December 17, 2010. Historical devel-
opment and future directions in data resource develop-
ment. In MINDS 2006?2007.
Parantu K. Shah, Lars J. Jensen, Ste?phanie Boue?, and
Peer Bork. 2005. Extraction of transcript diversity
from scientific literature. PLoS Computational Biol-
ogy, 1(1):67?73.
Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun Lin,
Cheng-Lung Sung, Wei Ku, Ying-Shan Su, Ting-Yi
Sung, and Wen-Lian Hsu. 2006. BIOSMILE: adapt-
ing semantic role labeling for biomedical verbs: an
exponential model coupled with automatically gener-
ated template features. In Proceedings of the BioNLP
Workshop on Linking Natural Language Processing
and Biology, pages 57?64. Association for Computa-
tional Linguistics.
90
Tuangthong Wattarujeekrit, Parantu K. Shah, and Nigel
Collier. 2004. PASBio: predicate-argument structures
for event extraction in molecular biology. BMC Bioin-
formatics, 5(155).
Martin Wynne, editor. 2005. Developing linguistic cor-
pora: a guide to good practice. David Brown Book
Company.
91
