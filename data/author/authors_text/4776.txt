Integrating Collocation Features in Chinese Word Sense 
Disambiguation 
Wanyin Li  
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
cswyli@comp.polyu.e
du.hk 
Qin Lu 
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
csqinlu@comp.polyu.e
du.hk 
Wenjie Li  
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
cswjli@comp.polyu.ed
u.hk 
 
Abstract 
The selection of features is critical in pro-
viding discriminative information for clas-
sifiers in Word Sense Disambiguation 
(WSD). Uninformative features will de-
grade the performance of classifiers. Based 
on the strong evidence that an ambiguous 
word expresses a unique sense in a given 
collocation, this paper reports our experi-
ments on automatic WSD using collocation 
as local features based on the corpus ex-
tracted from People?s Daily News (PDN) 
as well as the standard SENSEVAL-3 data 
set. Using the Na?ve Bayes classifier as our 
core algorithm, we have implemented a 
classifier using a feature set combining 
both local collocation features and topical 
features. The average precision on the 
PDN corpus has 3.2% improvement com-
pared to 81.5% of the baseline system 
where collocation features are not consid-
ered. For the SENSEVAL-3 data, we have 
reached the precision rate of 37.6% by in-
tegrating collocation features into 
contextual features, to achieve 37% im-
provement  over  26.7% of precision in the 
baseline system. Our experiments have 
shown that collocation features can be used 
to reduce the size of human tagged corpus. 
1 Introduction 
WSD tries to resolve lexical ambiguity which 
refers to the fact that a word may have multiple 
meanings such as the word ?walk? in  ?Walk or 
Bike to school? and ?BBC Education Walk 
Through Time?, or the Chinese word  ???? in  
??????(?local government?) and ?????
????(?He is also partly right?). WSD tries to 
automatically assign an appropriate sense to an 
occurrence of a word in a given context.  
Various approaches have been proposed to deal 
with the word sense disambiguation problem 
including rule-based approaches, knowledge or 
dictionary based approaches, corpus-based ap-
proaches, and hybrid approaches. Among these 
approaches, the supervised corpus-based ap-
proach had been applied and discussed by many 
researches ([2-8]). According to [1], the corpus-
based supervised machine learning methods are 
the most successful approaches to WSD where 
contextual features have been used mainly to 
distinguish ambiguous words in these methods. 
However, word occurrences in the context are 
too diverse to capture the right pattern, which 
means that the dimension of contextual words 
will be very large when all words in the training 
samples are used for WSD [14]. Certain 
uninformative features will weaken the dis-
criminative power of a classifier resulting in a 
lower precision rate. To narrow down the con-
text, we propose to use collocations as contex-
tual information as defined in Section 3.1.2. It is 
generally understood that the sense of an am-
biguous word is unique in a given collocation 
[19]. For example, ???? means ?burden? but 
not ?baggage? when it appears in the collocation 
?????? (? burden of thought?). 
In this paper, we apply a classifier to combine 
the local features of collocations which contain 
the target word with other contextual features to 
discriminate the ambiguous words. The intuition 
is that when the target context captures a collo-
cation, the influence of other dimensions of
87
contextual words can be reduced or even ig-
nored. For example, in the expression ?????
?????? ? (?terrorists burned down the 
gene laboratory?), the influence of contextual 
word ???? (?gene?) should be reduced to work 
on the target word ???? because ?????? is 
a collocation whereas ???? and ???? are not 
collocations even though they do co-occur. Our 
intention is not to generally replace contextual 
information by collocation only. Rather, we 
would like to use collocation as an additional 
feature in WSD. We still make use of other  con-
textual features because of the following reasons. 
Firstly, contextual information is proven to be 
effective for WSD in the previous research 
works. Secondly, collocations may be independ-
ent on the training corpus and a sentence in con-
sideration may not contain any collocation. 
Thirdly, to fix the tie case such as ??????
?????? (?terrorists? gene checking?),  
???? means ?human? when presented in 
the collocation ??????, but ?particle? 
in the collocation ??????.  The primary 
purpose of using collocation in WSD is to im-
prove precision rate without any sacrifices in 
recall rate. We also want to investigate whether 
the use of collocation as an additional feature 
can reduce the size of hand tagged sense corpus. 
 The rest of this paper is organized as follows. 
Section 2 summarizes the existing Word Sense 
Disambiguation techniques based on annotated 
corpora. Section 3 describes the classifier and 
the features in our proposed WSD approach. 
Section 4 describes the experiments and the 
analysis of our results. Section 5 is the conclu-
sion. 
2 Related Work 
Automating word sense disambiguation tasks 
based on annotated corpora have been proposed. 
Examples of supervised learning methods for 
WSD appear in [2-4], [7-8]. The learning algo-
rithms applied including: decision tree, decision-
list [15], neural networks [7], na?ve Bayesian 
learning ([5],[11]) and maximum entropy [10]. 
Among these leaning methods, the most impor-
tant issue is what features will be used to con-
struct the classifier. It is common in WSD to use 
contextual information that can be found in the 
neighborhood of the ambiguous word in training 
data ([6], [16-18]). It is generally true that when 
words are used in the same sense, they have 
similar context and co-occurrence information 
[13]. It is also generally true that the nearby con-
text words of an ambiguous word give more ef-
fective patterns and features values than those 
far from it [12]. The existing methods consider 
features selection for context representation in-
cluding both local and topic features where local 
features refer to the information pertained only 
to the given context and topical features are sta-
tistically obtained from a training corpus. Most 
of the recent works for English corpus including 
[7] and [8], which combine both local and topi-
cal information in order to improve their per-
formance. An interesting study on feature 
selection for Chinese [10] has considered topical 
features as well as local collocational, syntactic, 
and semantic features using the maximum en-
tropy model. In Dang?s [10] work, collocational 
features refer to the local PoS information and 
bi-gram co-occurrences of words within 2 posi-
tions of the ambiguous word. A useful result 
from this work based on (about one million 
words) the tagged People?s Daily News shows 
that adding more features from richer levels of 
linguistic information such as PoS tagging 
yielded no significant improvement (less than 
1%) over using only the bi-gram co-occurrences 
information. Another similar study for Chinese 
[11] is based on the Naive Bayes classifier 
model which has taken into consideration PoS 
with position information and bi-gram templates 
in the local context. The system has a reported 
60.40% in both precision and recall based on the 
SENSEVAL-3 Chinese training data. Even 
though in both approaches, statistically signifi-
cant bi-gram co-occurrence information is used, 
they are not necessarily true collocations.  For 
example, in the express ?????????
????????????, the bi-grams in 
their system are (???,???, ???
?, ????, ?????, ????
??,? ????Some bi-grams such as 
????may have higher frequency but 
may introduce noise when considering it as fea-
tures in disambiguating the sense ?human|?? 
and ?symbol|??? like in the example case of 
?????????. In our system, we do not rely 
on co-occurrence information. Instead, we util-
ize true collocation information (???, ??) 
which fall in the window size of (-5, +5) as fea-
88
tures and the sense of ?human|?? can be de-
cided clearly using this features. The collocation 
information is a pre-prepared collocation list 
obtained from a collocation extraction system 
and verified with syntactic and semantic meth-
ods ([21], [24]).    
Yarowsky [9] used the one sense per collocation 
property as an essential ingredient for an unsu-
pervised Word-Sense Disambiguation algorithm 
to perform bootstrapping algorithm on a more 
general high-recall disambiguation. A few re-
cent research works have begun to pay attention 
to collocation features on WSD. Domminic [19] 
used three different methods called bilingual 
method, collocation method and UMLS (Unified 
Medical Language System) relation based 
method to disambiguate unsupervised English 
and German medical documents. As expected, 
the collocation method achieved a good preci-
sion around 79% in English and 82% in German 
but a very low recall which is 3% in English and 
1% in German. The low recall is due to the na-
ture of UMLS where many collocations would 
almost never occur in natural text.  To avoid this 
problem, we combine the contextual features in 
the target context with the pre-prepared colloca-
tions list to build our classifier.  
3 The Classifier With Topical Contex-
tual and Local Collocation Features 
3.1 The Feature Set 
As stated early, an important issue is what fea-
tures will be used to construct the classifier in 
WSD. Early researches have proven that using 
lexical statistical information, such as bi-gram 
co-occurrences was sufficient to produce close 
to the best results [10] for Chinese WSD. In-
stead of including bi-gram features as part of 
discrimination features, in our system, we con-
sider both topical contextual features as well as 
local collocation features. These features are 
extracted form the 60MB human sense-tagged 
People?s Daily News with segmentation infor-
mation.  
3.1.1 Topical Contextual Features 
Niu [11] proved in his experiments that Na?ve 
Bayes classifier achieved best disambiguation 
accuracy with small topical context window size 
(< 10 words).  We follow their method and set 
the contextual window size as 10 in our system.  
Each of the Chinese words except the stop 
words inside the window range will be consid-
ered as one topical feature. Their frequencies are 
calculated over the entire corpus with respect to 
each sense of an ambiguous word w.  The sense 
definitions are obtained from HowNet. 
3.1.2 Local Collocation Features 
We chose collocations as the local features. A 
collocation is a recurrent and conventional fixed 
expression of words which holds syntactic and 
semantic relations [21]. Collocations can be 
classified as fully fixed collocations, fixed col-
locations, strong collocations and loose colloca-
tions. Fixed collocations means the appearance 
of one word implies the co-occurrence of an-
other one such as ?????? (?burden of his-
tory?), while strong collocations allows very 
limited substitution of the components, for ex-
ample, ?????? (?local college?), or ? ???
?? (?local university?). The sense of ambiguous 
words can be uniquely determined in these two 
types of collocations, therefore are the colloca-
tions applied in our system. The sources of the 
collocations will be explained in Section 4.1. 
In both Niu [11] and Dang?s [10] work, topical 
features as well as the so called collocational 
features were used. However, as discussed in 
Section 2, they both used bi-gram co-
occurrences as the additional local features. 
However, bi-gram co-occurrences only indicate 
statistical significance which may not actually 
satisfy the conceptual definition of collocations. 
Thus instead of using co-occurrences of bi-
grams, we take the true bi-gram collocations 
extracted from our system and use this data to 
compare with bi-gram co-occurrences to test the 
usefulness of collocation for WSD. The local 
features in our system make use of the colloca-
tions using the template (wi, w) within a window 
size of ten (where i = ? 5). For example, ???
?????????? (?Government 
departments and local government commanded 
that?) fits the bi-gram collocation template (w, 
w1) with the value of (????). During the 
training and the testing processes, the counting 
of frequency value of the collocation feature will 
be increased by 1 if a collocation containing the 
ambiguous word occurs in a sentence. To have a 
good analysis on collocation features, we have 
also developed an algorithm using lonely 
adjacent bi-gram as locals features(named Sys-
89
adjacent bi-gram as locals features(named Sys-
tem A)  and another using collocation as local 
features(named System B). 
3.2 The Collocation Classifier 
We consider all the features in the features set F 
= Ft ?Fl = {f1, f2,  ? , fm } as independent, where 
Ft stands for the topical contextual features set, 
and Fl stands for the local collocation features 
set. For an ambiguous word w with n senses, let 
Sw = {ws1, ws2,  ? , wsn } be the sense set. For 
the contextual features, we directly apply the 
Na?ve Bayes algorithm using Add-Lambda 
Smoothing to handle unknown words: 
 
)|(log)(log)(1 sij
Ff
sisi wfpwpwscore
tj
?
?
+=   
(1) 
For each sense siw of an ambiguous word w:
 
)(
)()(
wfreq
wfreqwp sisi =                       (2) 
For each contextual feature fj respects to each 
sense siw of w : 
),(
),(
)|(
si
Ff
t
sij
sij wffreq
wffreq
wfp
tt
?
?
=   (3) 
To integrate the local collocation feature fj ? Fl  
with respect to each sense siw  of w, we use the 
follows formula: 
)()()( 21 sisisi wscorewscorewscore ?+= ?  (4) 
 
where ? is tuned from experiments (Section 4.5), 
score1( siw ) refers the score of the topical con-
textual features based on formula (1) and 
score2( siw ) refers the score of collocation fea-
tures with respect to the sense sjw  of w defined 
below. 
?
?
=
lj Ff
sjjsi wfwscore )|()(2 ?           (5) 
where ?(fj| sjw ) = 1 for fj ? Fl if the collocation 
occurs in the local context. Otherwise this term 
is set as 0. 
Finally, we choose the right skw so that 
)(maxarg sks wscores k=        (6) 
4 Experimental Results 
We have designed a set of experiments to com-
pare the classifier with and without the colloca-
tion features. In system A, the classifier is built 
with local bi-gram features and topical contex-
tual features. The classifier in system B is con-
structed from combining the local collocation 
features with topical features. 
4.1 Preparation the Data Set 
We have selected 20 ambiguous words from 
nouns and verbs with the sense number as 4 in 
average. The sense definition is taken from 
HowNet [22]. To show the effect of the algo-
rithm, we try to choose words with high degree 
of ambiguity, high frequency of use [23], and 
high frequency of constructing collocations. The 
selection of these 20 words is not completely 
random although within each criterion class we 
do try to pick word randomly. 
Based on the 20 words, we extracted 28,000 
sentences from the 60 MB People?s Daily News 
with segmentation information as our train-
ing/test set which is then manually sense-tagged.  
The collocation list is constructed from a 
combination of a digital collocation dictionary, a 
return result from a collocation automatic ex-
traction system [21], and a hand collection from 
the People?s Daily News. As we stated early, the 
sense of ambiguous words in the fixed colloca-
tions and strong collocations can be decided 
uniquely although they are not unique in loose 
collocations. For example, the ambiguous word 
???? in the collocation ??????? may 
have both the sense of ?appearance|??? or 
?reputation|???. Therefore, when labeling the 
sense of collocations, we filter out the ones 
which cannot uniquely determine the sense of 
ambiguous words inside. However, this does not 
mean that loose collocations have no contribu-
tion in WSD classification. We simply reduce its 
weight when combining it with the contextual 
features compared with the fixed and strong col-
locations. The sense and collocation distribution 
over the 20 words on the training examples can 
be found in Table 1. 
Table 1. Sense and Collocation Distribution of the 20 tar-
get words in the training corpus 
Am. 
W 
T# S1 
co# 
S2 
co# 
S3 
co# 
S4 
co# 
S5 
co# 
S6 
co# 
90
?? 31 1  1 
30 
10 NA  
  
?? 499 479  324 
18  
0 0 0 
NA  
?? 944 908  129 
1  
1 
17 
10 
18  
0 
0 NA 
?? 409 3  2 
389  
171 
17 
0 NA 
  
?? 110 3  0 
101 
36 
6  
9 NA 
  
?? 41 3  0 
37  
6 
1  
0 NA 
  
?? 4885 26  0 
34  
0 
72  
0 
4492 
1356 
261 
1 NA 
?? 3508 7  0 
48  
4 
3194 
1448 
259 
194 
NA  
?? 348 312  117 
22 
11 
14  
4 NA 
  
?? 4438 3983 721 
33  
10 
123  
37 
153 
123 
102 
23 
44 
5 
?? 1987 1712 723 
274 
10 NA  
  
?? 83 36  14 
47  
4 00 NA 
  
?? 995 168  108 
827 
513 NA  
  
?? 31 11  3 
20  
11 NA  
  
?? 2725 227 1772 
498 
49 
102 
424 
1898 
201 
NA  
?? 592 1  0 
208 
63 
367 
124 16 1 
NA  
?? 1155 756  571 
399 
135 NA  
  
?? 2792 691  98 
1765 
113 
336  
29 0 
NA  
?? 2460 82  63 
36 
11 
1231 
474 
877 
103 
NA  
?? 125 11  0 
64  
0 
15  
3 
32 
 4 
3  
0 NA 
T#: total number of sentences contain the ambiguous word 
s1- s6: sense no; co#: number of collocations in each sense 
4.2 The Effect of Collocation Features 
We recorded 6 trials with average precision over 
six-fold validation for each word. Their average 
precision for the six trials in the system A, and B 
can be found in Table 2 and Table 3. From Ta-
ble 3, regarding to precision, there are 16 words 
have improved and 4 words remained the same 
in the system B. The results from the both sys-
tem confirmed that collocation features do im-
prove the precision. Note that 4 words have the 
same precision in the two systems, which fall 
into two cases. In the first case, it can be seen 
that these words already have very high preci-
sion in the system A (over 93%) which means 
that one sense dominates all other senses. In this 
case, the additional collation information is not 
necessary. In fact, when we checked the inter-
mediate outputs, the score of the candidate 
senses of the ambiguous words contained in the 
collocations get improved. Even though, it 
would not change the result. Secondly, no collo-
cation appeared in the sentences which are 
tagged incorrectly in the system A. This is con-
firmed when we check the error files. For exam-
ple, the word ???? with the sense as ???? 
(?closeness?) appeared in 4492 examples over 
the total 4885 examples (91.9%). In the mean 
time, 99% of collocation in its collocation list 
has the same sense of ??? ? (?closeness?). 
Only one collocation ????? has the sense of 
??? ? (?power?). Therefore, the collocation 
features improved the score of sense ??? ? 
which is already the highest one based on the 
contextual features.  
As can be seen from Table 3, the collocation 
features work well for the sparse data. For ex-
ample, the word ???? in the training corpus 
has only one example with the sense ??? (?hu-
man?), the other 30 examples all have the sense 
???? (?management?). Under this situation, 
the topical contextual features failed to identify 
the right sense for the only appearance of the 
sense ??? (?human?) in the training instance 
??????????????????. How-
ever, it can be correctly identified in the system 
B because the appearance of the collocation ??
??????. 
To well show the effect of collocations on 
the accuracy of classifier for the task of WSD, 
we also tested both systems on SENSEVAL-3 
data set, and the result is recorded in the Table 4. 
From the difference in the relative improvement 
of both data sets, we can see that collocation 
features work well when the statistical model is 
not sufficiently built up such as from a small 
corpus like SENSEVAL-3. Actually, in this case, 
the training examples appear in the corpus only 
once or twice so that the parameters for such 
sparse training examples may not be accurate to 
forecast the test examples, which convinces us 
that collocation features are effective on han-
dling sparse training data even for unknown 
words. Fig. 1 shows the precision comparison in 
the system A, and B on SENVESAL-3. 
Table 2.  Average Precision (5/6 training, 1/6 test) of 
system A on People?s Daily News 
Amb. 
W T1 T2 T3 T4 T5 T6 
Ave. 
Prec. 
?? 1.00 1.00 1.00 1.00 1.00 .83 .972 
?? .90 .97 1.00 1.00 .97 .98 .972 
?? .97 .96 .96 .92 .98 .96 .958 
91
?? .94 .94 .97 .92 .97 .97 .951 
?? 1.00 1.00 .77 .94 .88 1.00 .932 
?? .83 1.00 1.00 1.00 .83 .90 .927 
?? .93 .95 .91 .92 .92 .92 .925 
?? .93 .94 .89 .91 .89 .90 .91 
?? .94 .93 .86 .93 .89 .87 .903 
?? .83 .94 .89 .90 .88 .94 .897 
?? .86 .88 .92 .84 .82 .87 .865 
?? .92 .84 .92 .76 .84 .72 .833 
?? .84 .83 .88 .82 .88 .71 .827 
?? .80 .60 .80 .20 1.00 1.00 .733 
?? .68 .72 .67 .77 .70 .68 .703 
?? .51 .67 .47 .60 .68 .59 .586 
?? .70 .63 .66 .64 .64 .64 .652 
?? .57 .74 .55 .64 .72 .67 .648 
?? .65 .58 .66 .64 .54 .47 .58 
?? .55 .50 .45 .45 .45 .64 .507 
Total Average Precision 0.815 
Table 3. Average Precision (5/6 training, 1/6 test) of 
system B on People?s Daily News 
Amb. 
W T1 T2 T3 T4 T5 T6 
Ave. 
Prec. 
?? 1.00 1.00 1.00 1.00 1.00 1.00 1.00 
?? .90 .97 1.00 1.00 .97 .98 .970 
?? .96 .98 .97 .96 .98 .96 .968 
?? .94 .94 .97 .94 .97 .98 .957 
?? 1.00 1.00 .77 .94 .88 1.00 .931 
?? .83 1.00 1.00 1.00 .83 .90 .927 
?? .93 .95 .91 .92 .92 .92 .925 
?? .92 .95 .92 .92 .91 .91 .922 
?? .94 .94 .86 .93 .91 .87 .908 
?? .80 .95 .89 .93 .89 .94 .902 
?? .87 .88 .92 .84 .83 .91 .875 
?? .84 1.00 .92 .76 .84 .77 .855 
?? .88 .86 .89 .84 .90 .74 .852 
?? 1.00 .80 .80 .20 1.00 1.00 .800 
?? .69 .72 .68 .79 .75 .72 .725 
?? .69 .76 .73 .74 .82 .79 .755 
?? .58 .59 .70 .67 .64 .59 .628 
?? .68 .67 .66 .63 .65 .63 .653 
?? .65 .68 .71 .61 .70 .69 .673 
?? .60 .55 .54 .54 .54 .64 .568 
Total Average Precision 0.840 
Table 4.  Average Precision of System A & B on 
SENSEVAL-3 Data Set 
Amb. 
Word 
Total 
S 
Ave. Prec. in 
Sys A 
Ave. Prec. 
in Sys B 
?? 48 .207 .290 
?? 20 .742 .742 
? 49 .165 .325 
? 25 .325 .325 
?? 36 .260 .373 
?? 30 .167 .267 
?? 30 .192 .392 
?? 36 .635 .635 
? 57 .238 .275 
?? 36 .327 .385 
?? 31 .100 .322 
? 40 .358 .442 
?? 40 .308 .308 
? 76 .110 .123 
? 28 .308 .475 
?667. 500. 30 ? 
? 42 .165 .260 
? 57 .037 .422 
?? 28 .833 .103 
Total Ave. 
Precision .276 .376 
Fig. 1. The precision comparison in system A, and B based 
on SENSEVAL-3 
 
4.3 The Effect of Collocations on the Size 
of Training Corpus Needed 
Hwee [21] stated that a large-scale, human 
sense-tagged corpus is critical for a supervised 
learning approach to achieve broad coverage 
and high accuracy WSD. He conducted a thor-
ough study on the effect of training examples on 
the accuracy of supervised corpus based WSD. 
As the result showed, WSD accuracy continues 
to climb as the number of training examples in-
creases. Similarly, we have tested the system A, 
and B with the different size of training corpus 
based on the PDN corpus we prepared. Our ex-
periment results shown in Fig 2 follow the same 
fact.  The purpose we did the testing is that we 
hope to disclose the effect of collocations on the 
size of training corpus needed. From Fig 2, we 
can see by using the collocation features, the 
precision of the system B has increased slower 
along with the growth of training examples than 
the precision of the system A.  The result is rea-
sonable because with collocation feature, the 
statistical contextual information over the entire 
corpus becomes side effect. Actually, as can be 
seen from Fig. 2, after using collocation features 
92
in the system B, even we use 1/6 corpus as train-
ing, the precision is still higher than we use 5/6 
train corpus in the system A. 
Fig. 2. The precision variation respect to the size of   train-
ing corpus in system A, and B based on PDN corpus 
 
4.4 Investigation of Sense Distribution on 
the Effect of Collocation Features 
To investigate the sense distribution on the ef-
fect of collocation features, we selected the am-
biguous words with the number of sense varied 
from 2 to 6. In each level of the sense number, 
the words are selected randomly. Table 5 shows 
the effect of sense distribution on the effect of 
collocation features. From the table, we can see 
that the collocation features work well when the 
sense distribution is even for a particular am-
biguous word under which case the classifier 
may get confused. 
Table 5.  The Effect of Sense Distribution on the Effect of 
collocation Features 
Amb. 
word 
Prec. 
Wihtout 
coll 
Prec. 
With  
coll 
Sense 
# 
Sense 
Distri. 
?? .972 1 2 97% * 
?? .97 .97 4 96% * 
?? .957 .968 5 96% * 
?? .951 .957 3 95% * 
?? .931 .931 3 92% * 
?? .927 .927 3 90% * 
?? .925 .925 5 92% * 
?? .915 .922 4 91% * 
?? .903 .908 3 90% * 
?? .902 .902 6 90% * 
?? .865 .875 2 86% o 
?? .833 .855 3 ^ 
?? .823 .852 2 83% o 
?? .733 .8 2 ^ 
?? .706 .725 4 ^ 
?? .65 .653 4 ^ 
?? .618 .755 4 ^ 
?? .582 .628 2 ^ 
?? .563 .673 4 ^ 
?? .507 .568 5 ^ 
     *: over 90% samples fall in one dominate sense 
     ^: Even distribution over all senses  
     o: 83% to 86% samples fall in one dominate sense 
4.5 The Test of ? 
We have conducted a set of experiments based 
on both the PDN corpus and SENSEVLA-3 data 
to set the best value of ? for the formula (4) de-
scribed in Section 3.2. The best start value of ? 
is tested based on the precision rate which is 
shown in Fig. 3. It is shown from the experiment 
that ? takes the start value of 0.5 for both cor-
puses.  
Fig. 3. The best value of ? vs the precision rate 
 
5 Conclusion and the Future Work 
This paper reports a corpus-based Word Sense 
Disambiguation approach for Chinese word us-
ing local collocation features and topical contex-
tual features. Compared with the base-line 
systems in which a Na?ve Bayes classifier is 
constructed by combining the contextual fea-
tures with the bi-gram features, the new system 
achieves 3% precision improvement in average 
in Peoples? Daily News corpus and 10% im-
provement in SENSEVAL-3 data set. Actually, 
it works very well when disambiguating the 
sense with sparse distribution over the entire 
corpus under which the statistic calculation 
prone to identify it incorrectly. In the same time, 
because disambiguating using collocation fea-
93
tures does not need statistical calculation, it 
makes contribution to reduce the size of human 
tagged corpus needed which is critical and time 
consuming in corpus based approach.  
Because different types of collocations may 
play different roles in classifying the sense of an 
ambiguous word, we hope to extend this work 
by integrating collocations with different weight 
based on their types in the future, which may 
need a pre-processing job to categorize the col-
locations automatically. 
6 Acknowledgements 
We would like to present our thanks to the IR 
Laboratory in HIT University of China for shar-
ing their sense number definition automatically 
extracted from HowNet with us. 
References 
1. Hwee Tou Ng, Bin Wang, Yee Seng Chan. Exploiting 
Parallel Texts for Word Sense Disambiguation. ACL-03 
(2003) 
2. Black E.: An experiment in computational discrimina-
tion of English word senses. IBM Journal of Research 
and Development, v.32, n.2, (1988) 185-194 
3. Gale, W. A., Church, K. W. and Yarowsky, D.: A 
method for disambiguating word senses in a large cor-
pus. Computers and the Humanities, v.26, (1993) 415-
439 
4. Leacock, C., Towell, G. and Voorhees, E. M.: Corpus-
based statistical sense resolution. In Proceedings of the 
ARPA Human Languages Technology Workshop (1993) 
5. Leacock, C., Chodorow, M., & Miller G. A..Using Cor-
pus Statistics and WordNet Relations for Sense Identifi-
cation. Computational Linguistics, 24:1, (1998) 147?
165  
6. Sch?tze, H.: Automatic word sense discrimination. 
Computational Linguistics, v.24, n.1, (1998) 97-124 
7. Towell, G. and Voorhees, E. M.: Disambiguating highly 
ambiguous words. Computational Linguistics, v.24, n.1, 
(1998) 125-146 
 8. Yarowsky, D.: Decision lists for lexical ambiguity reso-
lution: Application to accent restoration in Spanish and 
French. In Proceedings of the Annual Meeting of the 
Association for Computational Linguistics, (1994) 88-
95 
9. Yarowsky, D.: Unsupervised word sense disambiguation 
rivaling supervised methods. In Proceedings of the An-
nual Meeting of the Association for Computational Lin-
guistics, (1995)189-196 
 10. Dang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D., 
Simple Features for Chinese Word Sense Disambigua-
tion. In Proc. of COLING (2002) 
11. Zheng-Yu Niu, Dong-Hong Ji, Chew Lim Tan, Opti-
mizing Feature Set for Chinese Word Sense Disam-
biguation. To appear in Proceedings of the 3rd 
International Workshop on the Evaluation of Systems 
for the Semantic Analysis of Text (SENSEVAL-3). 
Barcelona, Spain (2004) 
12. Chen, Jen Nan and Jason S. Chang, A Concept-based 
Adaptive Approach to Word SenseDisambiguation, 
Proceedings of 36th Annual Meeting of the Association 
for Computational Linguistics and 17th International 
Conference on Computational linguistics. 
COLING/ACL-98 (1998) 237-243 
13.  Rigau, G., J. Atserias and E. Agirre, Combining Unsu-
pervised Lexical Knowledge Methods for Word Sense 
Disambiguation, Proceedings of joint 35th Annual 
Meeting of the Association for Computational Linguis-
tics and 8th Conference of the European Chapter of the 
Association for Computational Linguistics 
(ACL/EACL?97), Madrid, Spain (1997) 
14. Jong-Hoon Oh, and Key-Sun Choi, C02-1098.: Word 
Sense Disambiguation using Static and Dynamic Sense 
Vectors. COLING (2002) 
15. Yarowsky, D., Hierarchical Decision Lists for Word 
Sense Disambiguation. Computers and the Humanities, 
34(1-2), (2000) 179?186 
16. Agirre, E. and G. Rigau (1996) Word Sense Disam-
biguation using Conceptual Density, Proceedings of 
16th International Conference on Computational Lin-
guistics. Copenhagen, Denmark, COLING (1996) 
17. Escudero, G., L. M?rquez and G. Rigau, Boosting Ap-
plied to Word Sense Disambiguation. Proceedings of 
the 11th European Conference on Machine Learning 
(ECML 2000) Barcelona, Spain. 2000. Lecture Notes in 
Artificial Intelligence 1810. R. L. de M?ntaras and E. 
Plaza (Eds.). Springer Verlag (2000) 
18. Gruber, T. R., Subject-Dependent Co-occurrence and 
Word Sense Disambiguation. Proceedings of 29th An-
nual Meeting of the Association for Computational Lin-
guistics (1991) 
19. Dominic Widdows, Stanley Peters, Scott Cederberg, 
Chiu-Ki Chan, Diana Steffen, Paul Buitelaar, Unsuper-
vised Monolingual and Bilingual Word-Sense Disam-
biguation of Medical Documents using UMLS. 
Appeared in Natural Language Processing in Biomedi-
cine,. ACL 2003 Workshop, Sapporo, Japan (2003) 9?
16 
20. Hwee Tou Ng., Getting serious about word sense dis-
ambiguation. In Proceedings of the ACL SIGLEX 
Workshop on Tagging Text with Lexical Seman-
tics:Why, What, and How? (1997) 1?7 
21. Ruifeng Xu , Qin Lu, and Yin Li, An automatic Chi-
nese Collocation Extraction Algorithm Based On Lexi-
cal Statistics. In Proceedings of the NLPKE Workshop 
(2003) 
22.  D. Dong and Q. Dong, HowNet. 
   http://www.keenage.com, (1991) 
23.  Chih-Hao Tsai, 
 http://technology.chtsai.org/wordlist/, (1995-2004) 
24. Q. Lu, Y. Li, and R. F. Xu, Improving Xtract for Chi-
nese Collocation Extraction.  Proceedings of IEEE In-
ternational Conference on Natural Language Processing 
and Knowledge Engineering, Beijing (2003) 
 
 
94
  
Using Synonym Relations In Chinese Collocation Extraction 
Wanyin Li 
Department of Computing,  
The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
cswyli@comp.polyu.edu.hk 
Qin Lu 
Department of Computing,  
The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
csluqin @comp.polyu.edu.hk 
Ruifeng Xu 
Department of Computing, The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
csrfxu@comp.polyu.edu.hk 
 
 
Abstract 
A challenging task in Chinese collocation 
extraction is to improve both the precision and 
recall rate. Most lexical statistical methods 
including Xtract face the problem of unable to 
extract  collocations with lower frequencies than 
a given threshold. This paper presents a method 
where HowNet is used to find synonyms using a 
similarity function. Based on such synonym 
information, we have successfully extracted 
synonymous collocations which normally cannot 
be extracted using the lexical statistical 
approach. We applied synonyms mapping to 
each headword to extract more synonymous 
word bi-grams. Our evaluation over 60MB 
tagged corpus shows that we can extract 
synonymous collocations that occur with very 
low frequency, sometimes even for collocations 
that occur only once in the training set. 
Comparing to a collocation extraction system 
based on Xtract, we have reached the precision 
rate of 43% on word bi-grams for a set of 9 
headwords, almost 50% improvement from 
precision rate of 30% in the Xtract system. 
Furthermore, it  improves the recall rate of word 
bi-gram collocation extraction by 30%. 
1 Introduction 
A Chinese collocation is a recurrent and 
conventional expression of words which  holds 
syntactic and semantic relations.  A widely adopted 
definition given by Benson (Benson 1990) stated 
that ?a collocation is an arbitrary and recurrent 
word combination.?  For example, we say ?warm 
greetings? rather than ?hot greetings?, ?broad 
daylight? rather than ?bright daylight?.  Similarly, 
in Chinese ? ? ? ? ? ? are three nouns 
with similar meanings, however, we  say 
? ? rather than ? ?, 
? ?rather than ? ?.   
 
Study in collocation extraction using lexical 
statistics has gained some insights to the issues 
faced in collocation extraction (Church and Hanks 
1990, Smadja 1993, Choueka 1993, Lin 1998). As 
the lexical statistical approach is developed based 
on the ?recurrence? property of collocations, only 
collocations with reasonably good recurrence can 
be extracted. Collocations with low occurrence 
frequency cannot be extracted, thus affecting the 
recall rate. The precision rate using the lexical 
statistics approach can reach around 60% if both 
word bi-gram extraction and n-gram extractions 
are taking into account (Smadja 1993, Lin 1997 
and Lu et al 2003). The low precision rate is 
mainly due to the low precision rate of word bi-
gram extractions as only about 30% - 40% 
precision rate can be achieved for word bi-grams.  
In this paper, we propose a different approach to 
find collocations with low recurrences. The main 
idea is to make use of synonym relations to extract 
synonymous collocations. Lin (Lin 1997) 
described a distributional hypothesis that if two 
words have similar set of collocations, they are 
probably similar. In HowNet, Liu Qun (Liu et al 
2002) defined the word similarity as two words 
that can substitute each other in the context and 
keep the sentence consistent in syntax and 
semantic structure. That means, naturally, two 
similar words are very close to each other and they 
can be used in place of the other in certain context. 
For example, we may either say  ? ?or ? ? 
as  and are semantically close to each 
other. We apply this lexical phenomenal after the 
lexical statistics based extractor to find the low 
frequency synonymous collocations, thus 
increasing recall rate.  
 
  
  The rest of this paper is organized as follows. 
Section 2 describes related existing collocation 
extraction techniques based on both lexical 
statistics and synonymous collocation. Section 3 
describes our approach on collocation extraction. 
Section 4 evaluates the proposed method. Section 5 
draws our conclusion and presents possible future  
work. 
2 Related Work 
Methods have proposed to extract collocations 
based on lexical statistics. Choueka (Choueka 
1993) applied quantitative selection criteria based 
on frequency threshold to extract adjacent n-grams 
(including bi-grams). Church and Hanks (Church 
and Hanks 1990) employed mutual information to 
extract both adjacent and distant bi-grams that tend 
to co-occur within a fixed-size window. But the 
method did not extend to extract n-grams. Smadja 
(Smadja 1993) proposed a statistical model by 
measuring the spread of the distribution of co-
occurring pairs of words with higher strength. This 
method successfully extracted both adjacent and 
distant bi-grams and n-grams. However, the 
method failed to extract bi-grams with lower 
frequency. The precision rate on bi-grams 
collocation is very low, only around high 20% and 
low 30%. Even though, it is difficult to measure 
recall rate in collocation extraction (almost no 
report on recall estimation), It is understood that 
low occurrence collocations cannot be extracted. 
Our research group has further applied the Xtract 
system to Chinese (Lu et al 2003) by adjusting the 
parameters to optimize the algorithm for Chinese 
and a new weighted algorithm was developed 
based on mutual information to acquire word bi-
grams with one higher frequency word and one 
lower frequency word. The result has achieved an 
estimated 5% improvement in recall rate and a 
15% improvement in precision comparing to the 
Xtract system. 
All of the above techniques do not take 
advantage of the wide range of lexical resources 
available including synonym information. Pearce 
(Pearce 2001) presented a collocation extraction 
technique that relies on a mapping from a word to 
its synonyms for each of its senses. The underlying 
intuitions is that if the difference between the 
occurrence counts of one synonyms pair with 
respect to a particular word was at least two, then 
this was deemed sufficient to consider them as a 
collocation. To apply this approach, knowledge in 
word (concept) semantics and relations to other 
words must be available such as the use of 
WordNet. Dagan (Dagan 1997) applied similarity-
based smoothing method to solve the problem of 
data sparseness in statistical natural language 
processing. The experiments conducted in his later 
works showed that this method achieved much 
better results than back-off smoothing methods in 
word sense disambiguation. Similarly, Hua Wu 
(Wu and Zhou 2003) applied synonyms 
relationship between two different languages to 
automatically acquire English synonymous 
collocation. This is the first time that the concept 
synonymous collocation is proposed. A side 
intuition raised here is that nature language is full 
of synonymous collocations. As many of them 
have low occurrences, they are failed to be 
retrieved by lexical statistical methods. Even 
though there are Chinese synonym dictionaries, 
such as  ( Tong Yi Ci Lin), the 
dictionaries lack structured knowledge and 
synonyms are too loosely defined to be used for 
collocation extraction.  
HowNet developed by Dong et al(Dong and 
Dong 1999) is the best publicly available resource 
on Chinese semantics. By making use of semantic 
similarities of words, synonyms can be defined by 
the closeness of their related concepts and the 
closeness can be calculated. In Section 3, we 
present our method to extract synonyms from 
HowNet and using synonym relations to further 
extract collocations. 
Sun (Sun 1997) did a preliminary Quantitative 
analysis on Chinese collocations based on their 
arbitrariness, recurrence and the syntax structure. 
The purpose of this study is to help differentiate if 
a collocation is true or not according to the 
quantitative factors. By observing the existence of  
synonyms information in natural language use, we 
consider it possible to identify different types of 
collocations using more semantic and syntactic 
information available. We discuss the basic ideas 
in section 5.. 
3 Our Approach 
Our method of extracting Chinese collocations 
consists of three steps.  
Step 1: Take the output of any lexical statistical 
algorithm which extracts word bi-gram 
collocations. The data is then sorted 
according to each headword , Wh, with its co-
word, Wc, listed. 
Step 2: For each headword Wh used to extract bi-
grams, we acquire its synonyms based on a 
similarity function using HowNet. Any word 
in HowNet having similarity value over a 
threshold value is chosen as a synonym 
headword Ws for additional extractions. 
Step 3: For each synonym headword, Ws, and the 
co-word Wc of Wh, as its synonym, if the bi-
gram (Ws , Wc) is not in the output of the 
  
lexical statistical algorithm in Step one, take 
this bi-gram (Ws , Wc) as a collocation if the 
pair co-occurs in the corpus by additional 
search to the corpus. 
3.1 Structure of HowNet 
Different from WordNet or other synonyms 
dictionary, HowNet describes words as a set of 
concepts  and each concept is described by a 
set of primitives . The following lists for the 
word , one of its corresponding concepts 
 
 
In the above record, DEF is where the primitives 
are specified. DEF contains up to four types  of 
primitives: the basic independent primitive   
, the other independent 
primitive , the relation primitive  
, and the symbol primitive , 
where the basic independent primitive and the 
other independent primitive are used to indicate the 
semantics of a concept and the others are used to 
indicate syntactical relationships. The similarity 
model described in the next subsection will 
consider both of these relationships.  
The primitives are linked by a hierarchical tree 
to indicate the parent-child relationships of the 
primitives as shown in the following example:  
 
 
 
This hierarchical structure provides a way to link 
one concept with any other concept in HowNet, 
and the closeness of concepts can be simulated by 
the distance between two concepts. 
3.2 Similarity Model Based on HowNet 
Liu Qun (Liu 2002) defined word similarity as 
two words which can substitute each other in the 
same context and still maintain the sentence 
consistent syntactically and semantically. This is 
very close to our definition of synonyms. Thus we 
directly used their similarity function, which is 
stated as follows.  
A word in HowNet is defined as a set of 
concepts and each concept is represented by 
primitives.  Thus, HowNet can be described by W, 
a collection of n words, as: 
 W = { w1, w2, ? wn}Each word wi is, in 
turn, described by a set of concepts S as:  
 Wi = { Si1, Si2,?Six}, 
And, each concept Si  is, in turn, described by a 
set of primitives: 
 Si  = { pi1, pi2 ?piy } 
For each word pair, w1 and w2, the similarity 
function is defined by 
  )1(),(max),( 21...1,..121 jimjni SSSimwwSim ===     
where S1i is the list of concepts associated with W1 
and S2j is the list of concepts associated with W2.  
As any concept Si is presented by its primitives, 
the similarity of primitives for any p1, and  p2 of 
the same type, can be expressed by the following 
formula: 
 ?
?
+= ),(),( 2121 ppDis
ppSim     (2) 
where ?  is an adjustable parameter set to 1.6, 
and ),( 21 ppDis is the path length between p1 and 
p2 based on the semantic tree structure. The above 
formula where ? is a constant does not indicate 
explicitly the fact that the depth of a pair of nodes 
in the tree affects their similarity. For two pairs of 
nodes (p1 ,  p2) and  (p3 ,  p4) with the same distance,  
the deeper the depth is, the more commonly shared 
ancestros they would have which should be 
semantically closer to each other. In following two 
tree structures, the pair of nodes (p1, p2) in the left 
tree should be more similar than (p3 ,  p4)  in the 
right tree. 
 
                root 
 
 
 
 
 
                             p2 
 
             p1 
                      
                     root 
 
 
 
 
 
                          P3 
 
 
                    P4 
 
 
  
 
To indicate this observation,  ?  is modified as a 
function of tree depths of the nodes using the 
formula  ? =min(d(p1), d(p2)) . Consequently, the 
formula (2) is rewritten as formular (2?) during the 
experiment. 
 
))(),(min(),(
))(),(min(),(
2121
21
21 pdpdppDis
pdpdppSim +=
    (2?) 
 
where d(pi) is the depth of node pi  in the tree . The 
comparison of calculating the word similarity by 
applying the formula (2) and  (2?) is shown in 
Section 4.4. 
 
 Based on the DEF description in HowNet, 
different primitive types play different roles only 
some are directly related to semantics. To make 
use of both the semantic and syntactic information 
included in HowNet to describe a word, the 
similarity of two concepts should take into 
consideration of all primitive types with weighted 
considerations and thus the formula is defined as 
)3(),(),( 21
1
4
1
21 jj
i
j
j
i
i ppSimSSSim ??
==
= ?   
where ?i is a weighting factor given in (Liu 
2002) with the sum of ?1 + ?2 + ?3 + ?4 being 1? 
and ?1 ? ?2 ? ?3 ? ?4. The distribution of the 
weighting factors is given for each concept a priori 
in HowNet to indicate the importance of primitive 
pi in defining the corresponding concept S. 
 
3.3 Collocation Extraction 
In order to extract collocations from a corpus, 
and to obtain result for Step 1 of our algorithm, we 
used the collocation extraction algorithm 
developed by the research group at the Hong Kong 
Polytechnic University(Lu et al 2003). The 
extraction of bi-gram collocation is based on the 
English Xtract(Smaja 1993) with improvements. 
Based on the three Steps mentioned earlier, we will 
present the extractions in each step in the 
subsections. 
 
3.3.1 Bi-gram Extraction 
Based on the lexical statistical model proposed 
by Smadja in Xtract on extracting English 
collocations, an improved algorithm was 
developed for Chinese collocation by our research 
group and the system is called CXtract. For easy of 
understanding, we will explain the algorithm 
briefly here. According to Xtract, word 
cooccurence is denoted by a tripplet (wh, wi, d)  
where wh is a given headword, wi is a co-word 
appeared in the corpus in a distance d within the 
window of [-5, 5]. The frequency fi of the co-word   
wi   in the window of [-5, 5] is defined as: 
?
?=
=
5
5
,
j
jii ff    (4) 
where  fi, j   is the frequency of the co-word at distance 
j in the corpus within the window. The average 
frequency of  fi , denoted by if , is given by 
10/
5
5
,?
?=
=
j
jii ff    (5) 
Then, the average frequency f , and the standard 
deviation ? are defined by 
?
=
=
n
i
ifn
f
1
1
;  2
1
)(1?
=
?=
n
i
i ffn
?  (6) 
The Strength of the co-occurrence for the pair 
(wh, wi,), denoted by ki, is defined by 
?
ffk ii
?= ?   (7) 
Furthermore, the Spread of (wh, wi,),, denoted as 
Ui, which characterizes the distribution of  wi 
around  wh is define as: 
10
)( 2,? ?
=
iji
i
ff
U ;    (8) 
To eliminate the bi-grams with unlikely co-
occurrence, the following sets of threshold values 
is defined: 
0:1 K
ff
kC ii ??= ?    (9) 
0:2 UUC i ?     (10) 
)(:3 1, iiji UKffC ?+?   (11) 
However, the above statistical model given by 
Smadja fails to extract the bi-grams with a much 
higher frequency of wh but a relatively low 
frequency word of wi,,  For example,  in the bi-
gram , freq ( ) is much lower than the 
freq ( ). Therefore, we further defined a 
weighted mutual information to extract this kind of 
bi-grams: 
  ,
)(
),w(
0
h R
wf
wfR
i
i
i ?=      (12)  
As a result, the system should return a list of 
triplets (wh, wi, d), where  (wh, wi,) is considered 
collocations.  
  
3.3.2 Synonyms Set 
For each given headword wh, before taking it as 
an input to extract its bi-grams directly, we fist 
apply the similarity formula described in Equation 
(1) to generate a set of synonyms headwords Wsyn: 
 
   }),(:{ ?>= shssyn wwSimwW                             (13) 
Where 0 <? <1 is an algorithm parameter which 
is adjusted based on experience. We set it as 0.85 
from the experiment because we would like to 
balance the strength of the synonyms relationship 
and the coverage of the synonyms set. The setting 
of the parameter ? < 0.85 weaks the similarity 
strength of the extracted synonyms. For example, 
for a given collocation ? ?, that is unlikely 
to include the candidates ? ?, ? ?, 
? ?.  On the other hand, by setting the 
parameter ? > 0.85 will limit the coverage of the 
synonyms set and hence lose valuable synonyms. 
For example, for a given bi-gram ? ?, we 
hope to include the candidate synonymous 
collocations such as  ? ?, ? ?, 
? ?. We will show the test of ?  in the 
section 4.2. 
This synonyms headwords set provides the 
possibility to extract the synonymous collocation 
with the lower frequency that failed to be extracted 
by lexical statistic. 
3.3.3 Synonymous Collocations 
A phenomenal among the collocations in natural 
language is that there are many synonymous 
collocations exist. For example, ?switch on light? 
and ?turn on light?, ? ? and ? ?. 
Due to the domain specification of the corpus, 
some of the synonymous collocations may fail to 
be extracted by the lexical statistic model because 
of their lower frequency. Based on this 
observation, this paper takes a further step. The 
basic idea is for a bi-gram collocation (wh, wc, d ) 
we select the synonyms ws of wh with the 
maximum similarity respect to all the concepts 
contained by wh, we deem (ws, wc, d ) as a 
collocation if its occurrence is greater than 1 in the 
corpus. There are similar works discussed by 
Pearce (Pearce 2001). . 
For a given collocation (ws, wc,, d), if ws ? Wsyn, 
then we deem the triple (ws, wc,, d) as a 
synonymous collocation with respect to the 
collocation (wh, wc,, d) if the co-occurrence of (ws, 
wc, , d) in the corpus is greater than one. Therefore, 
we define the collection of synonymous 
collocations Csyn as: 
}1),,(:),,{( >= dwwFreqdwwC cscssyn           (14) 
where  ws ? Wsyn. 
4 Evaluation 
The performance of collocation is normally 
evaluated by precision and recall as defined below. 
nsCollocatioextractedofnumbertotal
nsCollocatioExtractedcorrectofnumberprecision
    
    = (15) 
nsCollocatioactualofnumbertotal
nsCollocatioExtractedcorrectofnumberrecall
    
    =  (16) 
To evaluate the performance of our approach, we 
conducted a set of experiments based on 9 selected 
headwords. A baseline system using only lexical 
statistics given in 3.3.1 is used to get a set of 
baseline data called Set A. The output using our 
algorithm is called Set B. Results are checked by 
hand for validation on what is true collocation and 
what is not a true collocation. 
 
Table 1. Sample table for the true collocation 
with headword ? ?  
 
Table 2. Sample table for the bi-grams that are 
not true collocations  
  
Table 1 shows samples of extracted word bi-grams 
using our algorithm that are considered 
synonymous collocations for the headword ? ?. 
Table 2 shows extracted bi-grams by our algorithm 
that are not considered true collocations. 
4.1 Test Set 
Our experiment is based on a corpus of six 
months tagged People Daily with 11 millions 
number of words. For word bi-gram extractions, 
we consider only content words, thus headwords 
are selected from noun, verb and adjective only. 
For evaluation purpose, we selected randomly 3 
nouns, 3 verbs and 3 adjectives with frequency of 
low, medium and high. Thus, in Step 1 of the 
algorithm, 9 headwords were  used to extract bi-
gram collocations from the corpus, and 253 pairs 
of collocations were extracted. Evaluation by hand 
has identified 77 true collocations in Set A test set. 
The overall precision rate is 30% (see Table 3).  
 
 Noun+Verb
+Adjective 
Headword 9 
Extracted Bi-grams 253 
True collocations using 
lexical statistics only 
77 
Precision rate 30% 
 Table 3: Statistics in test set for set A 
 
Using Step 2 of our algorithm, where ?=0.85 is 
used, we have obtained 55 synonym headwords 
(include the 9 headwords). Out of these 55 
synonyms, 614 bi-gram pairs were then extracted 
from the lexical statistics based algorithm, in 
which 179 are consider true collocations. Then, by 
applying Step 3 of our algorithm, we extracted an 
additional 201 bi-gram pairs, among them, 178 are 
considered true collocations. Therefore, using our 
algorithm, the overall precision rate has achieved 
43%, an improvement of almost 50%. The data is 
summarized in Table 4. 
 n., v, and adj. 
Synonyms headword 55 
Bi-grams (lexical statistics) 614 
Non-synonym collocations 
(lexical statistics only) 
179 
Extracted synonym 
collocations Step 2 
201 
True synonym collocations 
using Step 2 
178 
Overall precision rate 43% 
Table 4: Statistics in test set for mode B 
4.2 The choice of ? 
We also conducted a set of experiments to 
choose the best value for the similarity function?s 
threshold ?. We tested the best value of ? with both 
the precision rate and the estimated recall rate 
using the so called remainder bi-grams. The 
remainder bi-grams is the total number of bi-grams 
extracted by the algorithm. When precision goes 
up, the size of the result is smaller, which in a way 
is an indicator of less recalled collocations. Figure 
1 shows the precision rate and the estimated recall 
rate in testing the value of ?. 
 
Figure 1. Precision Rate vs. value of ? 
From Figure 1, it is obvious that at ?=0.85 the 
recall rate starts to drop more drastically without 
much incentive for precision. 
 
 Extracted Bi-
grams using 
lexical 
statistics 
Extracted 
Synonyms 
Collocations 
using Step 2 
(1.2,1.4,12) 465 328 
(1.4,1.4,12) 457 304 
(1.4,1.6,12) 394 288 
(1.2,1.2,12) 513 382 
(1.2,1.2,14) 503 407 
(1.2,1.2,16) 481 413 
      Table 5: Value of (K0, K1, U0). 
4.3 The test of (K0, K1, U0) 
The original threshold for CXtract is (1.2, 1.2, 12) 
for the parameters (K0, K1, U0). However, with 
synonyms collocations, we have also conducted 
some experiments to see whether the parameters 
should be adjusted. Table 5 shows the statistics to 
test the value of (K0, K1, U0). The similarity 
threshold ? was fixed at 0.85 throughout the 
experiments. 
  
The experimental shows that varying the value of 
(k0, k1) does not bring any benefit to our algorithm. 
However, increasing the value of u0 did improve 
the extraction of synonymous collocations. Figure 
2 shows that U0 =14 is a good trade-off for the 
precision rate and the remainder Bi-grams. The basic 
meaning behind the result is reasonable. According to 
Smadja, U0 defined in the formula (8) represents the 
co-occurrence distribution of the candidate 
collocation (wh, wc) in the position of d (-5 ? d ? 
5). For a true collocation (wh, wc,, d), its co-
occurrence  in the position d is much higher than in 
other positions which leads to a peak in the co-
occurrence distribution. Therefore, it is selected by 
the statistical algorithm based on the formula (10). 
Based on the physical meaning behind, one way to 
improve the precision rate is to increase the value of  
the threshold U0.  A side effect to an increased  value 
of U0  is that the recall is decreased because some  
true collocations do not meet the condition of co-
occurrence greater than U0. Step 2 of the new 
algorithm regains some  true collocations lost 
because of a higher U0. in Step 1.  
               Figure 2. Precision Rate vs. Value of U0 
 
4.4 The comparison of similarity calculation 
based on formula  (2) and (2?) 
Table 6 shows the similarity value given by 
formula (2) where ?  is a constant given the value 
1.6 and by formula (2?) where ? is replaced by a 
function of the depths of the nodes. Results show 
that (2?) is more fine tuned and reflects the nature 
of the data better. For example, and 
are more similar than and . 
 and are much similar but not the same. 
 
 
Table 6: comparison of similarity calculation 
5 Conclusion and Further Work 
In this paper, we have presented a method to 
extract bi-gram collocations using lexical statistics 
model with synonyms information. Our method 
reaches the precision rate of 43% for the tested data. 
Comparing to the precision of 30% using lexical 
statistics only, our improvement is close to 50%. In 
additional, the recall improved 30%. The contribution 
is that we have made use of synonym information 
which is plentiful in the natural language use and it 
works well to supplement the shortcomings of lexical 
statistical method.  
Manning claimed that the lack of valid 
substitution for a synonym is a characteristics of 
collocations in general (Manning and Schutze 
1999). To extend our work, we consider the use of 
synonym information can be further applied to 
help identify collocations of different types.  
Our preliminary study has suggested that 
collocation can be classified into 4 types:   
Type 0 Collocation:  Fully fixed collocation 
which include some idioms, proverbs and sayings 
such as ? ? ? ? and so on.  
Type 1 Collocation:  Fixed collocation in which 
the appearance of one word implies the co-
occurrence of another one such as ? ?.  
Type 2 Collocation: Strong collocation which 
allows very limited substitution of the components, 
for example, ? ?, ? ?, 
? ? and so on.  
Type 3 Collocation: Normal collocation which 
allows more substitution of the components, 
however a limitation is still required. For example, 
? ? ? ? ? ? 
? ? . 
  
By using synonym information and define 
substitutability, we can validate whether 
collocations are fixed collocations, strong 
collocations with very limited substitutions, or 
general collocations that can be substituted more 
freely. 
6 Acknowledgements 
Our great thanks to Dr. Liu Qun of the Chinese 
Language Research Center of Peking University for 
letting us share their data structure in the Synonyms 
Similarity Calculation. This work is partially 
supported by the Hong Kong Polytechnic 
University (Project Code A-P203) and CERG 
Grant (Project code 5087/01E) 
References  
M. Benson, 1990. Collocations and General 
Purpose Dictionaries. International Journal of 
Lexicography, 3(1): 23-35 
Y. Choueka, 1993. Looking for Needles in a 
Haystack or Locating Interesting Collocation 
Expressions in Large Textual Database. 
Proceedings of RIAO Conference on User-
oriented Content-based Text and Image 
Handling: 21-24, Cambridge. 
K. Church, and P. Hanks, 1990. Word Association 
Norms, Mutual Information,and Lexicography. 
Computational Linguistics, 6(1): 22-29. 
I. Dagan, L. Lee, and F. Pereira. 1997. Similarity-
based method for word sense disambiguation. 
Proceedings of the 35th Annual Meeting of 
ACL: 56-63, Madrid, Spain. 
Z. D. Dong and Q. Dong. 1999. Hownet, 
http://www.keenage.com 
D. K. Lin, 1997. Using Syntactic Dependency as 
Local Context to Resolve Word Sense Ambiguity. 
Proceedings of ACL/EACL-97: 64-71, Madrid, 
Spain 
Q. Liu, 2002. The Word Similarity Calculation on 
<<HowNet>>. Proceedings of 3rd Conference 
on Chinese lexicography, TaiBei 
Q. Lu, Y. Li, and R. F. Xu, 2003. Improving Xtract 
for Chinese Collocation Extraction.  Proceedings 
of IEEE International Conference on Natural 
Language Processing and Knowledge 
Engineering, Beijing 
C. D. Manning and H. Schutze, 1999. Foundations 
of Statistical Natural Language Processing. The 
MIT Press, Cambridge, Massachusetts  
D. Pearce, 2001. Synonymy in Collocation 
Extraction. Proceedings of NAACL'01 
Workshop on Wordnet and Other Lexical 
Resources: Applications, Extensions and 
Customizations 
F. Smadja, 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-
177 
H. Wu, and M. Zhou, 2003. Synonymous 
Collocation Extraction Using Translation 
Information. Proceeding of the 41st Annual 
Meeting of ACL 
D. K. Lin, 1998. Extracting collocations from text 
corpora. In Proc. First Workshop on 
Computational Terminology, Montreal, Canada. 
M. S. Sun, C. N. Huang and J. Fang, 1997. 
Preliminary Study on Quantitative Study on 
Chinese Collocations. ZhongGuoYuWen, No.1, 
29-38, (in Chinese). 
 
 
 The Construction of A Chinese Shallow Treebank 
Ruifeng Xu 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csrfxu@comp.polyu.edu.hk 
Qin Lu 
Dept. Computing, 
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong  
csluqin@comp.polyu.edu.hk 
Yin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csyinli@comp.polyu.edu.hk 
Wanyin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
cswyli@comp.polyu.edu.hk 
 
Abstract 
This paper presents the construction of a 
manually annotated Chinese shallow Treebank, 
named PolyU Treebank. Different from 
traditional Chinese Treebank based on full 
parsing, the PolyU Treebank is based on 
shallow parsing in which only partial syntactical 
structures are annotated. This Treebank can be 
used to support shallow parser training, testing 
and other natural language applications. 
Phrase-based Grammar, proposed by Peking 
University, is used to guide the design and 
implementation of the PolyU Treebank. The 
design principles include good resource sharing, 
low structural complexity, sufficient syntactic 
information and large data scale. The design 
issues, including corpus material preparation, 
standard for word segmentation and POS 
tagging, and the guideline for phrase bracketing 
and annotation, are presented in this paper. 
Well-designed workflow and effective 
semiautomatic and automatic annotation 
checking are used to ensure annotation accuracy 
and consistency. Currently, the PolyU Treebank 
has completed the annotation of a 
1-million-word corpus. The evaluation shows 
that the accuracy of annotation is higher than 
98%. 
1 Introduction 
A Treebank can be defined as a syntactically 
processed corpus. It is a language resource  
containing annotations of information at various 
linguistic levels such as words, phrases, clauses and 
sentences to form a ?bank of linguistic trees?. There 
are many Treebanks built for different languages 
such as the Penn Treebank (Marcus 1993), ICE-GB 
(Wallis 2003), and so on. The Penn Chinese 
Treebank is an important resource (Xia et al 2000; 
Xue et al 2002). Its annotation is based on 
Head-driven Phrase Structure Grammar (HPSG). 
The corpus of 100,000 Chinese words has been 
manually annotated with a strict quality assurance 
process. Another important work is the Sinica 
Treebank at the Academic Sinica, Taiwan ( Chen et 
al. 1999; Chen et al 2003). Information-based Case 
Grammar (ICG) was selected as the language 
framework. A head-driven chart parser was 
performed to do phrase bracketing and annotating. 
Then, manual post-editing was conducted. 
According to the report, The Sinica Treebank  
contains 38,725 parsed trees with 329,532 words.  
Most reported Chinese Treebanks, including the 
two above, are based on full parsing which requires 
complete syntactical analysis including determining 
syntactic categories of words, locating chunks that 
can be nested, finding relations between phrases and 
resolving the attachment ambiguities. The output of 
full parsing is a set of complete syntactic trees. 
Automatic full parsing, however, is difficult to 
achieve good performance. Shallow parsing (or 
partial parsing) is usually defined as a parsing 
process aiming to provide a limited amount of local 
syntactic information such as non-recursive noun 
phrases, V-O structures and S-V structures etc. Since 
shallow parsing can recognize the backbone of a 
sentence more effectively and accurately with lower 
cost, people has in recent years started to work using 
results from shallow parsing. A shallow parsed 
Treebank can be used to extract information for 
different applications especially for training shallow 
parsers. 
Different from full parsing, annotation to a 
shallow Treebank is only targeted at certain local 
structures in a sentence. The depth of ?shallowness?  
and the scope of annotation vary from different 
reported work. Thus, two issues in shallow Treebank 
annotation is (1) what information and (2) to what 
depths the syntactic information should be annotated. 
Generally speaking, the degree of ?shallowness? and 
the syntactical labeling are determined by the 
requirement of the serving applications. The choice 
of full parsing or shallow parsing is dependent on 
the need of the application including resources and 
 the capability of system to be developed (Xia et al 
2000; Chen et al 2000; Li et al 2003). Currently, 
there is no large-scale shallow annotated Treebank 
available as a publicly resource for training and 
testing.  
In this paper, we present a manually annotated 
shallow Treebank, called the PolyU Treebank. It is 
targeted to contain 1-million-word contemporary 
Chinese text. The whole work on the PolyU 
Treebank follows the Phrase-based Grammar 
proposed by Peking University (Yu et al 1998). In 
this language framework, a phrase, lead by a lexical 
word(or sometimes called a content word) as a head, 
is considered the basic syntactical unit in a Chinese 
sentence. The building of the PolyU Treebank was 
originally designed as training data for a shallow 
parser used for Chinese collocation extraction. From 
linguistics viewpoint, a collocation occurs only in 
words within a phrase, or between the headwords of 
related phrases (Zhang and Lin 1992). Therefore, the 
use of syntactic information is naturally considered 
an effective way to improve the performance of 
collocation extraction systems. The typical problems 
like doctor-nurse (Church and Hanks 1990) could be 
avoided by using such information. When 
employing syntactical information in collocation 
extraction, we restrict ourselves to identify the stable 
phrases in the sentences with certain levels of 
nesting. Thus it has motivated us to produce a 
shallow Treebank. 
A natural way to obtain a shallow Treebank is 
through extracting shallow structures from a fully 
parsed Treebank. Unfortunately, all the available 
fully parsed Treebank, such as the Penn Treebank 
and the Sinica Treebank, are annotated using 
different grammars than our chosen Phrase-based 
Grammar. Also, the sizes of these Treebank are 
much smaller in scale to be useful for training our 
shallow parser. 
This paper presents the most important design 
issues of the PolyU Treebank and the quality control 
mechanisms. The rest of this paper is organized as 
follows. Section 2 introduces the overview and 
design principles.  Section 3 to Section5, present 
the design issues on corpus material preparation, the 
standard for word segmentation and POS tagging, 
and the guideline for phrase bracketing and labeling, 
respectively. Section 6 discusses the quality 
assurance mechanisms including a carefully 
designed workflow, parallel annotation, and 
automatic and semi-automatic post-annotation 
checking. Section 7 gives the current progress and 
future work. 
2 Overview and Design Principles 
The objective of this project is to manually 
construct a large shallow Treebank with high 
accuracy and consistency.  
The design principles of The PolyU Treebank are: 
high resource sharing ability, low structural 
complexity, sufficient syntactic information and 
large data scale. First of all, the design and 
construction of The PolyU Treebank aims to provide 
as much a general purpose Treebank as possible so 
that different applications can make use of it as a 
NLP resource. With this objective, we chose to 
follow the well-known Phrase-based Grammar as 
the framework for annotation as this grammar is 
widely accepted by Chinese language researchers, 
and thus our work can be easily understood and 
accepted.  
Due to the lack of word delimitation in Chinese, 
word segmentation must be performed before any 
further syntactical annotation. High accuracy of 
word segmentation is very important for this project. 
In this project, we chose to use the segmented and 
tagged corpus of People Daily annotated by the 
Peking University. The annotated corpus contains 
articles appeared in the People Daily Newspaper in 
1998. The segmentation is based on the guidelines, 
given in the Chinese national standard GB13715, 
(Liu et al 1993) and the POS tagging specification 
was developed according to the ?Grammatical 
Knowledge-base of contemporary Chinese?. 
According to the report from Peking University, the 
accuracy of this annotated corpus in terms of 
segmentation and POS tagging are 99.9% and 99.5%, 
respectively (Yu et al 2001). The use of such mature 
and widely adopted resource can effectively reduce 
our cost, ensure syntactical annotation quality. With 
consistency in segmentation, POS, and syntactic 
annotation, the resulting Treebank can be readily 
shared by other researchers as a public resource. 
The second design principle is low structural 
complexity. That means, the annotation framework 
should be clear and simple, and the labeled syntactic 
and functional information should be commonly 
used and accepted. Considering the characteristics of 
shallow annotation, our project has focused on the 
annotation of phrases and headwords while the 
sentence level syntax are ignored.  
Following the framework of Phrase-based 
Grammar, a base-phrase is regarded as the smallest 
unit where a base-phrase is defined as a ?stable? and 
?simple? phrase without nesting components. Study 
on Chinese syntactical analysis suggests that phrases 
should be the fundamental unit instead of words in a 
sentence. This is because, firstly, the usage of 
Chinese words is very flexible. A word may have 
different POS tags serving for different functions in 
sentences. On the contrary, the use of Chinese 
phrases is much more stable. That is, a phrase has 
very limited functional use in a sentence. Secondly, 
the construction rules of Chinese phrases are nearly 
 the same as that of Chinese sentences. Therefore, the 
analysis of phrases can help identifying POS and 
grammatical functions of words. Naturally, it should 
be regarded as the basic syntactical unit. Usually, a 
base-phrase is driven by a lexical word as its 
headword. Examples of base-phrases include base 
NP, base VP and so on, such as the sample shown 
below. 
  
Using base-phrases as the start point, nested levels 
of phrases are then identified, until the maximum 
phrases (will be defined later) are identified. Since 
we do not intend to provide full parsing information, 
there has to be a limit on the level of nesting. For 
practical reasons, we choose to limit the nesting of 
brackets to 3 levels. That means, the depth of our 
shallow parsed Treebank will be limited to 3. This 
restriction can limit the structural complexity to a 
manageable level.  
Our nested bracketing is not strictly bottom up. 
That is we do not simply extend from base-phrase 
and move up until the 3rd level. Instead, we first 
identify the maximal-phrase which is used to 
identify the backbone of the sentence. The 
maximal-phrase provides the framework under 
which the base-phrases of up to 2 levels can be 
identified. The principles for the identification of 
scope and depth of phrase bracketing are briefly 
explained below and the operating procedure is 
indicated by the given order in which these 
principles are presented. More details is given in 
Section 5. 
Step 1: Annotation of maximal-phrase which is 
the shortest word sequence of maximally 
spanning non-overlapping edges which plays a 
distinct semantic role of a predicate. A 
maximal-phrase contains two or more lexical 
words. 
Step 2: Annotation of base-phrases within a 
maximal-phrase. In case a base-phrase and a 
maximal-phrase are identical and the 
maximal-phrase is already bracketed in Step 1, no 
bracketing is done in this step. For each identified 
base-phrase, its headword will be marked. 
Step 3: Annotation of next level of bracketing, 
called mid-phrase which is expended from a 
base-phrase. A mid-phrase is annotated only if it is 
deemed necessary. The process starts from the 
identified base-phrase. One more level of 
syntactical structure is then bracketed if it exists 
within the maximal-phrase.   
  
The third design principle is to provide sufficient 
syntactical information for natural language 
application even though shallow annotation does not 
necessarily contain complete syntactic information 
at sentence level. Some past research in Chinese 
shallow parsing were on single level base-phrases 
only (Sun 2001). However, for certain applications, 
such as for collocation extraction, identification of 
base-phrases only are not very useful. In this project, 
we have decided to annotate phrases within three 
levels of nesting within a sentence. For each phrase, 
a label is be given to indicate its syntactical 
information, and an optional semantic or structural 
label is given if applicable. Furthermore, the 
headword of a base-phrase is annotated. We believe 
these information are sufficient for many natural 
language processing research work and it is also 
manageable for this project within its working 
schedule. 
Fourthly, aiming to support practical language 
processing, a reasonably large annotated Treebank is 
expected. Studies on English have shown that 
Treebank of word size 500K to 1M is reasonable for 
syntactical structure analysis (Leech and Garside 
1996). In consideration of the resources available 
and the reference of studies on English, we have set 
out our Treebank size to be one million words. We 
hope such a reasonably large-scale data can 
effectively support some language research, such as  
collocation extraction.  
We chose to use the XML format to record the 
annotated data. Other information such as original 
article related information (author, date, etc.), 
annotator name, and other useful information are 
also given through the meta-tags provided by XML. 
All the meta-tags can be removed by a program to 
recover the original data. 
We have performed a small-scale experiment to 
compare the annotation cost of shallow annotation 
and full annotation (followed Penn Chinese 
Treebank specification) on 500 Chinese sentences 
by the same annotators. The time cost in shallow 
annotation is only 25% of that for full annotation. 
Meanwhile, due to the reduced structural complexity 
in shallow annotation, the accuracy of first pass 
shallow annotation is much higher than full 
annotation. 
3 Corpus Materials Preparation 
The People Daily corpus, developed by PKU, 
consists of more than 13k articles totaling 5M words. 
As we need one million words for our Treebank, we 
have selected articles covering different areas in 
different time span to avoid duplications due to 
short-lived events and news topics. Our selection 
takes each day?s news as one single unit, and then  
several distant dates are randomly selected among 
the whole 182 days in the entire collection.  We 
have also decided to keep the original articles? 
structures and topics indicators as they may be 
useful for some applications. 
 4 Word Segmentation and Part-of-Speech 
Tagging 
The articles selected from PKU corpus are already 
segmented into words following the guidelines 
given in GB13715. The annotated corpus has a basic 
lexicon of over 60,000 words. We simply use this 
segmentation without any change and the accuracy 
is claimed to be 99.9%.  
Each word in the PKU corpus is given a POS tag.  
In this tagging scheme, a total of 43 POS tags are 
listed (Yu et al 2001).  Our project takes the PKU 
POS tags with only notational changes explained as 
follows: 
The morphemes tags including Ag (Adjectives 
morphemes), Bg, Dg, Ng, Mg, Rg, Tg, Qg, and Ug 
are re-labeled as lowercase letters, ag, bg, dg, ng, mg, 
rg, tg, qg and ug, respectively. This modification is 
to ensure consistent labeling in our system where the 
lower cases are used to indicate word-level tags and 
upper cases are used to indicate phrase-level labels. 
5 Phrase Bracketing and Annotation 
Phrase bracketing and annotation is the core part 
of this project. Not only all the original annotated 
files are converted to XML files, results of our 
annotations are also given in XML form. The meta 
tags provided by XML are very helpful for further 
processing and searching to the annotated text. . 
Note that in our project, the basic phrasal analysis 
looks at the context of a clause, not a sentence. Here, 
the term clause refers the text string ended by some 
punctuations including comma (,), semicolon (;), 
colon (:), or period (.). Certain punctuation marks 
such as ? ?, ?<?, and ?>? are not considered clause 
separators. For example,  
  
is considered having two clauses and thus will be 
bracketed separately. It should be pointed out that he 
set of Chinese punctuation marks are different from 
that of English and their usage can also be different. 
Therefore, an English sentence and their Chinese 
translation may use different punctuation marks.  
For example, the sentence 
 
is the translation of the English ?Tom, John, and 
Jack go back to school together? , which uses ? ? 
rather than comma(,) to indicate parallel structures, 
and is thus considered one clause.   
Each clause will then be processed according to 
the principles discussed in Section 2. The symbols 
?[? and ?]? are used to indicate the left and right 
boundaries of a phrase. The right bracket is 
appended with syntactic labels as described in the 
general form of [Phrase]SS-FF, where SS is a 
mandatory syntactic label such as NP(noun phrase) 
and AP(adjective phrase), and FF is an optional label 
indicating internal structures and semantic functions 
such as BL(parallel), SB(a noun is the object of verb 
within a verb phrase). A total of  21 SS labels and 
20 FF labels are given in our phrase annotation 
specification. For example, the functional label BL 
identifies parallel components in a phrase as 
indicated in the example .  
As in another example shown below,  
 
the phrase  is a verb phrase, thus it is 
labeled as VP. Furthermore, the verb phrase can be 
further classified as a verb-complement type. Thus 
an additional SBU function label is marked. We 
should point out that since the FF labels are not 
syntactical information and are thus not expected to 
be used by any shallow parsers. The FF labels carry 
structural and/or semantic information which are of 
help in annotation. We consider it useful for other 
applications and thus decide to keep them in the 
Treebank. Appendix 1 lists all the FF labels used in 
the annotation. 
 
5.1  Identification of Maximal-phrase:  
The maximal-phrases are the main syntactical 
structures including subject, predicate, and objects in 
a clause. Again, maximal-phrase is defined as the 
phrase with the maximum spanning non-overlapping 
length, and it is a predicate playing a distinct 
semantic role and containing more than one lexical 
word. That means a maximal-phrase contains at least 
one base-phrase. As this is the first stage in the 
bracketing process, no nesting should occur. In the 
following annotated sentence, 
 (Eg.1) 
there are two separate maximal-phrases, 
, and 
. Note 
that  is considered a base-phrase, but not a 
maximal-phrase because it contains only one lexical 
word. Unlike many annotations where the object of 
a sentence is included as a part of the verb phrase, 
we treat them as separate maximal-phrases both due 
to our requirement and also for reducing nesting. 
If a clause is completely embedded in a larger 
clause, it is considered a special clause and given a 
special name called an internal clause .  We will 
bracket such an internal clause as a maximal phrase 
with the tag ?IC? as shown in the following example, 
 
 
5.2  Annotation of Base-phrases:  
A base-phrase is the phrase with stable, close and 
simple structure without nesting components. 
Normally a base-phrase contains a lexical word as 
 headword. Taking the  maximal-phrase 
in 
Eg.1 as an example,  and 
, are base-phrases in this 
maximal-phrase. Thus, the sentence is annotated as 
 
  
In fact, and are also 
base-phrases.  is not bracketed because it is a 
single lexical word as a base-phrase without any 
ambiguity and it is thus by default not being 
bracketed. is not further 
bracketed because it overlaps with a maximal-phrase. 
Our annotation principle here is that if a base-phrase 
overlaps with a maximal-phrase, it will not be 
bracketed twice.  
The identification of base-phrase is done only 
within an already identified maximal-phrase. In 
other words, if a base-phrase is identified, it must be 
nested inside a maximal-phrase or at most overlaps 
with it. It should be pointed out that the 
identification of a base-phrase is the most 
fundamental and most important goal of Treebank 
annotation. The identification of maximal-phrases 
can be considered as parsing a clause using a 
top-down approach. On the other hand, the 
identification of a base-phrase is a bottom up 
approach to find the most basic units within a 
maximal-phrase.  
 
5.3  Mid-Phrase Identification:  
Due to the fact that sometimes there may be more 
syntactic structures between the base-phrases and 
maximal-phrases, this step uses base-phrase as the 
starting point to further identify one more level of 
the syntactical structure in a maximal-phrase. Takes 
Eg.1 as an example, it is further annotated as 
 
where the underlined text shows the additional 
annotation. 
As we only limit our nesting to three levels, any 
further nested phrases will be ignored. The 
following sentence shows the result of our 
annotation with three levels of nesting:  
  
However, a full annotation should have 4 levels of 
nesting as shown below. The underlined text is the 
4th level annotation skipped by our system. 
 
 
5.4  Annotation of Headword 
In our system, a ?#? tag will be appended after a 
word to indicate that it is a headword of the 
base-phrase. Here, a headword must be a lexical 
word rather than a function word.  
In most cases, a headword stays in a fixed position 
of a base-phrase. For example, the headword of a 
noun phrase is normally the last noun in this phrase. 
Thus, we call this position the default position. If a 
headword is in the default position, annotation is not 
needed. Otherwise, a ?#? tag is used to indicate the 
headword. 
For example, in a clause, 
,  
 is a verb phrase, and the headword 
of the phrase is , which is not in the default 
position of a verb phrase. Thus, this phrase is further 
annotated as:  
  
Note that  is also a headword, but since it 
is in the default position, no explicit annotation is 
needed. 
6 Annotation and Quality Assurance 
Our research team is formed by four people at the 
Hong Kong Polytechnic University, two linguists 
from Beijing Language and Culture University and 
some research collaborators from Peking University. 
Furthermore, the annotation work has been 
conducted by four post-graduate students in 
language studies and computational linguistics from 
the Beijing Language and Culture University.  
The annotation work is conducted in 5 separate 
stages to ensure quality output of the annotation 
work. The preparation of annotation specification 
and corpus selection was done in the first stage. 
Researchers in Hong Kong invited two linguists 
from China to come to Hong Kong to prepare for the 
corpus collection and selection work. A thorough 
study on the reported work in this area was 
conducted. After the project scope was defined, the 
SS labels and the FF labels were then defined. A 
Treebank specification was then documented.  The 
Treebank was given the name PolyU Treebank to 
indicate that it is produced at the Hong Kong 
Polytechnic University. In order to validate the 
specifications drafted, all the six members first 
manually annotated 10k-word material, separately. 
The outputs were then compared, and the problems 
and ambiguities occurred were discussed and 
consolidated and named Version 1.0. Stage 1 took 
about 5 months to complete. Details of the 
specification can be downloaded from the project 
website www.comp.polyu.edu.hk/~cclab. 
In Stage 2, the annotators in Beijing were then 
involved. They had to first study the specification 
and understand the requirement of the annotation. 
Then, the annotators under the supervision of a team 
member in Stage 1 annotated 20k-word materials 
together and discussed the problems occurred. 
 During this two-month work, the annotators were 
trained to understand the specification. The 
emphasis at this stage was to train the annotators? 
good understanding of the specification as well as 
consistency by each annotator and consistency by 
different annotators. Further problems occurred in 
the actual annotation practice were then solved and 
the specification was also further refined or 
modified.  
In Stage 3, which took about 2 months, each 
annotator was  assigned 40k-word material each in 
which 5k-words material were duplicate annotated 
to all the annotators. Meanwhile, the team members 
in Hong Kong also developed a post-annotation 
checking tool to verify the annotation format, phrase 
bracketing, annotation tags, and phrase marks to 
remove ambiguities and mistakes. Furthermore, an 
evaluation tool was built to check the consistency of 
annotation output. The detected annotation errors 
were then sent back to the annotators for discussion 
and correction. Any further problems occurred were 
submitted for group discussion and minor 
modification on the specification was also done. 
In stage 4, each annotator was dispatched with one 
set of 50k-word material each time. For each 
distribution, 15k-word data in each set were 
distributed to more than two annotators in duplicates 
so that for any three annotators, there would be 5K 
duplicated materials. When the annotators finished 
the first pass annotation, we used the post-annotation 
checking tool to do format checking in order to 
remove the obvious annotation errors such as wrong 
tag annotation and cross bracketing. However, it was 
quite difficult to check the difference in annotation 
due to different interpretation of a sentence. What 
we did was to make use of the annotations done on 
the duplicate materials to compare for consistency. 
When ambiguity or differences were identified, 
discussions were conducted and a result used by the 
majority would be chosen as the accepted result. The 
re-annotated results were regarded as the Golden 
Standard to evaluate the accuracy of annotation and 
consistency between different annotators. The 
annotators were required to study this Golden 
Standard and go back to remove  similar mistakes. 
The annotated 50k data was accepted only after this. 
Then, a new 50k-word materials was distributed and 
repeated in the same way. During this stage, the 
ambiguous and out-of-tag-set phrase structures were 
marked as OT for further process. The annotation 
specification was not modified in order to avoid 
frequent revisit to already annotated data. About 4 
months were spent on this stage. 
In Stage 5, all the members and annotators were 
grouped and discuss the OT cases. Some typical new 
phrase structure and function types were appended 
in the specification and thus the final formal 
annotation specification was established. Using this 
final specification, the annotators had to go back to 
check their output, modify the mistakes and 
substitute the OT tags by the agreed tags. Currently, 
the project was already in Stage 5 with 2 months of 
work finished. A further 2 months was expected to 
complete this work. 
Since it is impossible to do all the checking and 
analysis manually, a series of checking and 
evaluating tools are established. One of the tools is 
to check the consistency between text corpus files 
and annotated XML files including checking the 
XML format, the filled XML header, and whether 
the original txt material is being altered by accident. 
This program ensures that the XML header 
information is correctly filled and during annotation 
process, no additional mistakes are introduced due to 
typing errors.  
Furthermore, we have developed and trained a 
shallow parser using the Golden Standard data. This 
shallow parser is performed on the original text data, 
and its output and manually annotated result are 
compared for verification to further remove errors 
Now, we are in the process of developing an 
effective analyzer to evaluate the accuracy and 
consistency for the whole annotated corpus. For the 
exactly matched bracketed phrases, we check 
whether the same phrase labels are given. Abnormal 
cases will be manually checked and confirmed. Our 
final goal is to ensure the bracketing can reach 99% 
accuracy and consistency. 
7 Current Progress and Future Work 
As mentioned earlier, we are now in Stage 5 of the 
annotation. The resulting annotation contains 2,639 
articles selected from PKU People Daily corpus. 
These articles contains 1, 035, 058 segmented 
Chinese words, with on average, around 394 words 
in each article. There are a total of 284, 665 
bracketed phrases including nested phrases. A 
summary of the different SS labels used are given in 
Table 1. 
 
Table 1. Statistics of annotated syntactical phrases 
 
For each bracketed phrase, if its FF label does not 
fit into the corresponding default pattern, (like for 
the noun phrase(NP), the default grammatical 
structure is that the last noun in the phrase is the 
headword and other components are the modifiers, 
using PZ tags), its FF labels should then be 
explicitly labeled. The statistics of annotated FF tags 
 are listed in Table 2.  
 
Table 2. Statistics of function and structure tags 
 
For the material annotated by multiple annotators 
as duplicates, the evaluation program has reported 
that the accuracy of phrase annotation is higher than 
99.5% and the consistency between different 
annotators is higher than 99.8%. As for other 
annotated materials, the quality evaluation program 
preliminarily reports the accuracy of phrase 
annotation is higher than 98%. Further checking and 
evaluation work are ongoing to ensure the final 
overall accuracy achieves 99%. 
Up to now, the FF labels of 5,255 phrases are 
annotated as OT. That means about 1.8% (5,255 out 
of a total of 284,665) of them do not fit into any 
patterns listed in Table 2. Most of them are proper 
noun phrase, syntactically labeled as PP. We are 
investigating these cases and trying to identify 
whether some of them can be in new function and 
structure patterns and give a new label. 
It is also our intention to further develop our tools 
to improve the automatic annotation analysis and 
evaluation program to find out the potential 
annotation error and inconsistency. Other 
visualization tools are also being developed to 
support keyword searching, context indexing, and 
annotation case searching. Once we complete Stage 
5, we intend to make the PolyU Treebank data 
available for public access.  Furthermore, we are 
developing a shallow parser and using The PolyU 
Treebank as training and testing data. 
 
Acknowledgement 
 
This project is partially supported by the Hong Kong 
Polytechnic University (Project Code A-P203) and 
CERG Grant (Project code 5087/01E) 
References  
Baoli Li, Qin Lu and Yin Li. 2003. Building a 
Chinese Shallow Parsed Treebank for Collocation 
Extraction, Proceedings of CICLing 2003: 
402-405 
Fei Xia, et al 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text Annotation 
Proceedings of LREC-2000, Greece 
Feng-yi Chen, et al 1999. Sinica Treebank, 
Computational Linguistics and Chinese Language 
Processing, 4(2):183-204 
G. N. Leech, R.Garside. 1996. Running a grammar 
factory: the production of syntactically analyzed 
corpora or ?treebanks?, Johansson and Stenstron. 
Honglin Sun, 2001. A Content Chunk Parser for 
Unrestricted Chinese Text, Ph.D Thesis, Peking 
University, 2001 
Keh-jiann Chen et al 2003. Building and Using 
Parsed Corpora (Anne Abeill? ed. s) KLUWER, 
Dordrecht 
Kenneth Church, and Patrick Hanks. 1990. Word 
association norms, mutual information, and 
lexicography, Computational Linguistics, 16(1): 
22-29 
Marcus, M. et al 1993. Building a Large Annotated 
Corpus of English: The Penn Treebank, 
Computational Linguistics, 19(1): 313-330. 
Nianwen Xue, et al 2002. Building a Large-Scale 
Annotated Chinese Corpus, Proceedings of 
COLING 2002, Taipei, Taiwan 
Sean Wallis, 2003. Building and Using Parsed 
Corpora (Anne Abeill? eds) KLUWER, Dordrecht 
Shiwen Yu, et al 1998. The Grammatical 
Knowledge- base of contemporary Chinese: a 
complete specification. Tsinghua University Press, 
Beijing, China 
Shiwen Yu, et al 2001. Guideline of People Daily 
Corpus Annotation, Technical report, Beijing 
University 
Shoukang Zhang and Xingguang Lin, 1992. 
Collocation Dictionary of Modern Chinese 
Lexical Words, Business Publisher, China 
Yuan Liu, et al 1993. Segmentation standard for 
Modern Chinese Information Processing and 
automatic segmentation methodology. Tsinghua 
University Press, Beijing, China 
  
Appendix 1 The structural and semantic FF labels  
 
 
Appendix 2 Example of an Annotated Article  
 
 
 
