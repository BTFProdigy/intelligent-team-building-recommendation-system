Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 162?171,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Discourse Relations for Eliminating
Intra-sentence Polarity Ambiguities
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, Kam-Fai Wong
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
Shatin, NT, Hong Kong, China
Key Laboratory of High Confidence Software Technologies
Ministry of Education, China
{ljzhou, byli, wgao, zywei, kfwong}@se.cuhk.edu.hk
Abstract
Polarity classification of opinionated sen-
tences with both positive and negative senti-
ments1 is a key challenge in sentiment anal-
ysis. This paper presents a novel unsuper-
vised method for discovering intra-sentence
level discourse relations for eliminating polar-
ity ambiguities. Firstly, a discourse scheme
with discourse constraints on polarity was de-
fined empirically based on Rhetorical Struc-
ture Theory (RST). Then, a small set of cue-
phrase-based patterns were utilized to collect
a large number of discourse instances which
were later converted to semantic sequential
representations (SSRs). Finally, an unsuper-
vised method was adopted to generate, weigh
and filter new SSRs without cue phrases for
recognizing discourse relations. Experimen-
tal results showed that the proposed methods
not only effectively recognized the defined
discourse relations but also achieved signifi-
cant improvement by integrating discourse in-
formation in sentence-level polarity classifica-
tion.
1 Introduction
As an important task of sentiment analysis, polar-
ity classification is critically affected by discourse
structure (Polanyi and Zaenen, 2006). Previous re-
search developed discourse schema (Asher et al,
2008) (Somasundaran et al, 2008) and proved that
the utilization of discourse relations could improve
the performance of polarity classification on dia-
logues (Somasundaran et al, 2009). However, cur-
1Defined as ambiguous sentences in this paper
rent state-of-the-art methods for sentence-level po-
larity classification are facing difficulties in ascer-
taining the polarity of some sentences. For example:
(a) [Although Fujimori was criticized by the international
community]?[he was loved by the domestic population]?
[because people hated the corrupted ruling class]. (??
??????????????????????
??????????????????????)
Example (a) is a positive sentence holding a Con-
trast relation between first two segments and a
Cause relation between last two segments. The po-
larity of "criticized", "hated" and "corrupted" are rec-
ognized as negative expressions while "loved" is rec-
ognized as a positive expression. Example (a) is dif-
ficult for existing polarity classification methods for
two reasons: (1) the number of positive expressions
is less than negative expressions; (2) the importance
of each sentiment expression is unknown. However,
consider Figure 1, if we know that the polarity of
the first two segments holding a Contrast relation
is determined by the nucleus (Mann and Thompson,
1988) segment and the polarity of the last two seg-
ments holding aCause relation is also determined by
the nucleus segment, the polarity of the sentence will
be determined by the polarity of "[he...population]".
Thus, the polarity of Example (a) is positive.
Statistics showed that 43% of the opinionated
sentences in NTCIR2 MOAT (Multilingual Opinion
Analysis Task) Chinese corpus3 are ambiguous. Ex-
isting sentence-level polarity classification methods
ignoring discourse structure often give wrong results
for these sentences. We implemented state-of-the-
2http://research.nii.ac.jp/ntcir/
3Including simplified Chinese and traditional Chinese cor-
pus from NTCIR-6 MOAT and NTCIR-7 MOAT
162
Figure 1: Discourse relations for Example (a). (n and s
denote nucleus and satellite segment, respectively)
art method (Xu and Kit, 2010) in NTCIR-8 Chinese
MOAT as the baseline polarity classifier (BPC) in
this paper. Error analysis of BPC showed that 49%
errors came from ambiguous sentences.
In this paper, we focused on the automation of
recognizing intra-sentence level discourse relations
for polarity classification. Based on the previous
work of Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988), a discourse scheme with dis-
course constraints on polarity was defined empiri-
cally (see Section 3). The scheme contains 5 rela-
tions: Contrast, Condition, Continuation, Cause and
Purpose. From a raw corpus, a small set of cue-
phrase-based patterns were used to collect discourse
instances. These instances were then converted to
semantic sequential representations (SSRs). Finally,
an unsupervised SSR learner was adopted to gener-
ate, weigh and filter high quality new SSRs with-
out cue phrases. Experimental results showed that
the proposed methods could effectively recognize
the defined discourse relations and achieve signifi-
cant improvement in sentence-level polarity classi-
fication comparing to BPC.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Sec-
tion 3 presents the discourse scheme with discourse
constraints on polarity. Section 4 gives the detail of
proposed method. Experimental results are reported
and discussed in Section 5 and Section 6 concludes
this paper.
2 Related Work
Research on polarity classification were generally
conducted on 4 levels: document-level (Pang et al,
2002), sentence-level (Riloff et al, 2003), phrase-
level (Wilson et al, 2009) and feature-level (Hu and
Liu, 2004; Xia et al, 2007).
There was little research focusing on the auto-
matic recognition of intra-sentence level discourse
relations for sentiment analysis in the literature.
Polanyi and Zaenen (2006) argued that valence cal-
culation is critically affected by discourse struc-
ture. Asher et al (2008) proposed a shallow se-
mantic representation using a feature structure and
use five types of rhetorical relations to build a fine-
grained corpus for deep contextual sentiment anal-
ysis. Nevertheless, they did not propose a com-
putational model for their discourse scheme. Sny-
der and Barzilay (2007) combined an agreement
model based on contrastive RST relations with a lo-
cal aspect model to make a more informed over-
all decision for sentiment classification. Nonethe-
less, contrastive relations were only one type of dis-
course relations which may help polarity classifica-
tion. Sadamitsu et al (2008) modeled polarity re-
versal using HCRFs integrated with inter-sentence
discourse structures. However, our work is on intra-
sentence level and our purpose is not to find polar-
ity reversals but trying to adapt general discourse
schemes (e.g., RST) to help determine the overall
polarity of ambiguous sentences.
The most closely related works were (Somasun-
daran et al, 2008) and (Somasundaran et al, 2009),
which proposed opinion frames as a representation
of discourse-level associations on dialogue andmod-
eled the scheme to improve opinion polarity clas-
sification. However, opinion frames was difficult
to be implemented because the recognition of opin-
ion target was very challenging in general text. Our
work differs from their approaches in two key as-
pects: (1) we distinguished nucleus and satellite in
discourse but opinion frames did not; (2) our method
for discourse discovery was unsupervised while their
method needed annotated data.
Most research works about discourse classifica-
tion were not related to sentiment analysis. Su-
pervised discourse classification methods (Soricut
and Marcu, 2003; Duverle and Prendinger, 2009)
needed manually annotated data. Marcu and Echi-
habi (2002) presented an unsupervised method to
recognize discourse relations held between arbitrary
spans of text. They showed that lexical pairs ex-
tracted from massive amount of data can have a
major impact on discourse classification. Blair-
Goldensohn et al (2007) extended Marcu's work by
using parameter opitimization, topic segmentation
and syntactic parsing. However, syntactic parsers
163
were usually costly and impractical when dealing
with large scale of text. Thus, in additional to lex-
ical features, we incorporated sequential and seman-
tic information in proposed method for discourse re-
lation classification. Moreover, our method kept the
characteristic of language independent, so it could be
applied to other languages.
3 Discourse Scheme for Eliminating
Polarity Ambiguities
Since not all of the discourse relations in RST
would help eliminate polarity ambiguities, the dis-
course scheme defined in this paper was on a much
coarser level. In order to ascertain which relations
should be included in our scheme, 500 ambigu-
ous sentences were randomly chosen from NTCIR
MOAT Chinese corpus and the most common dis-
course relations for connecting independent clauses
in compound sentences were annotated. We found
that 13 relations from RST occupied about 70% of
the annotated discourse relations which may help
eliminate polarity ambiguities. Inspired by Marcu
and Echihabi (2002), to construct relatively low-
noise discourse instances for unsupervised methods
using cue phrases, we grouped the 13 relations into
the following 5 relations:
Contrast is a union of Antithesis, Concession, Oth-
erwise and Contrast from RST.
Condition is selected from RST.
Continuation is a union of Continuation, Parallel
from RST.
Cause is a union of Evidence, Volitional-Cause,
Nonvolitional-Cause, Volitional-result and
Nonvolitional-result from RST.
Purpose is selected from RST.
The discourse constraints on polarity presented
here were based on the observation of annotated dis-
course instances: (1) discourse instances holding
Contrast relation should contain two segments with
opposite polarities; (2) discourse instances hold-
ing Continuation relation should contain two seg-
ments with the same polarity; (3) the polarity of dis-
course instances holdingContrast,Condition,Cause
or Purpose was determined by the nucleus segment;
(4) the polarity of discourse instances holding Con-
tinuation was determined by either segment.
Relation Cue Phrases(English Translation)
Contrast although1, but2, however2
Condition if1, (if1?then2)
Continuation and, further more,(not only, but also)
Cause because
1, thus2, accordingly2,
as a result2
Purpose in order to
2, in order that2,
so that2
1 means CUE1 and 2 means CUE2
Table 1: Examples of cue phrases
4 Methods
The proposed methods were based on two as-
sumptions: (1) Cue-phrase-based patterns could be
used to find limited number of high quality discourse
instances; (2) discourse relations were determined
by lexical, structural and semantic information be-
tween two segments.
Cue-phrase-based patterns could find only lim-
ited number of discourse instances with high pre-
cision (Marcu and Echihabi, 2002). Therefore, we
could not rely on cue-phrase-based patterns alone.
Moreover, there was no annotated corpus similar to
Penn Discourse TreeBank (Miltsakaki et al, 2004)
in other languages such as Chinese. Thus, we pro-
posed a language independent unsupervised method
to identify discourse relations without cue phrases
while maintaining relatively high precision. For
each discourse relation, we started with several cue-
phrase-based patterns and collected a large number
of discourse instances from raw corpus. Then, dis-
course instances were converted to semantic sequen-
tial representations (SSRs). Finally, an unsupervised
method was adopted to generate, weigh and filter
common SSRswithout cue phrases. Themined com-
mon SSRs could be directly used in our SSR-based
classifier in unsupervised manner or be employed as
effective features for supervised methods.
4.1 Gathering and representing discourse
instances
A discourse instance, denoted by Di, consists of
two successive segments (Di[1], Di[2]) within a sen-
tence. For example:
D1: [Although Boris is very brilliant at math]s, [he
164
BOS... ?[CUE2]...EOS
BOS [CUE1]... ?...EOS
BOS... ?[CUE1]...EOS
BOS [CUE1]... ?[CUE2]...EOS
Table 2: Cue-phrase-based patterns. BOS and EOS de-
noted the beginning and end of two segments.
is a horrible teacher]n
D2: [John is good at basketball]s, [but he lacks team
spirit]n
In D1, "although" indicated the satellite section
while inD2, "but" indicated the nucleus section. Ac-
cordingly, different cue phrases may indicate differ-
ent segment type. Table 1 listed some examples of
cue phrases for each discourse relation. Some cue
phrases were singleton (e.g. "although" and "as a re-
sult") and some were used as a pair (e.g. "not only,
but also"). "CUE1" indicated satellite segments and
"CUE2" indicated nucleus segments. Note that we
did not distinguish satellite from nucleus for Con-
tinuation in this paper because the polarity could be
determined by either segment.
Table 2 listed cue-phrase-based patterns for all re-
lations. To simplify the problem of discourse seg-
mentation, we split compound sentences into dis-
course segments using commas and semicolons. Al-
though we collected discourse instances from com-
pound sentences only, the number of instances for
each discourse relation was large enough for the pro-
posed unsupervised method. Note that we only col-
lected instances containing at least one sentiment
word in each segment.
In order to incorporate lexical and semantic infor-
mation in our method, we represented each word in
a discourse instance using a part-of-speech tag, a se-
mantic label and a sentiment tag. Then, all discourse
instances were converted to SSRs. The rules for con-
verting were as follows:
(1) Cue phrases and punctuations were ingored.
But the information of nucleus(n) and satellite(s)
was preserved.
(2) Adverbs(RB) appearing in sentiment lexicon,
verbs(V ), adjectives(JJ ) and nouns(NN) were repre-
sented by their part-of-speech (pos) tag with seman-
tic label (semlabel) if available.
(3) Named entities (NE; PER: person name;ORG:
organization), pronouns (PRP), and function words
were represented by their corresponding named en-
tity tags and part-of-speech tags, respectively.
(4) Added sentiment tag (P : Positive; N : Nega-
tive) to all sentiment words.
By applying above rules, the SSRs forD1 andD2
would be:
d1: [PERV|Ja01 RB|Ka01 JJ|Ee14|P IN NN|Dk03]s
, [PRP V|Ja01 DT JJ|Ga16|N NN|Ae13 ]n
d2: [PER V|Ja01 JJ|Ee14|P IN NN|Bp12]s, [PRP
V|He15|N NN|Di10 NN|Dd08 ]n
Refer to d1 and d2, "Boris" could match "John"
in SSRs because they were converted to "PER" and
they all appeared at the beginning of discourse in-
stances. "Ja01", "Ee14" etc. were semantic labels
from Chinese synonym list extended version (Che et
al., 2010). There were similar resources in other lan-
guages such asWordnet(Fellbaum, 1998) in English.
The next problem became how to start from current
SSRs and generate new SSRs for recognizing dis-
course relations without cue phrases.
4.2 Mining common SSRs
Recall assumption (2), in order to incorporate lex-
ical, structural and semantic information for the sim-
ilarity calculation of two SSRs holding the same
discourse relation, three types of matches were de-
fined for {(u, v)|u ? di[k], v ? dj[k], k = 1, 2}:
(1)Full match: (i) u = v or (ii) u.pos = v.pos and
u.semlabel=v.semlabel or (iii) u.pos=v.pos and
u had a sentiment tag and v had a sentiment tag or
(iv) u.pos and v.pos?{PRP, PER, ORG} (2) Partial
match: u.pos = v.pos but not Full match; (3) Mis-
match: u.pos ?= v.pos.
Generating common SSRs
Intuitively, a simple way of estimating the simi-
larity between two SSRs was using the number of
mismatches. Therefore, we utilized match(di, dj)
where i ?= j, which integrated the three types of
matches defined above to calculate the number of
mismatches and generate common SSRs. Consider
Table 3, in common SSRs, full matches were pre-
served, partial matches were replaced by part of
speech tags and mismatches were replaced by '*'s.
The common SSRs generated during the calculation
of match(di, dj) consisted of two parts. The first
part was generated by di[1] and dj[1] and the second
part was generated by di[2] and dj[2]. We stipulated
165
d1 d2 mis conf ssr
PER PER 0 0 PER
V|Ja01 V|Ja01 0 0 V|Ja01
RB|Ka01 +1 ?0.298 *
JJ|Ee14|P JJ|Ee14|P 0 0 JJ|Ee14|P
IN IN 0 0 IN
NN|Dk03 NN|Bp12 0 ?0.50 NN
conf(ssr[1]) = ?0.798
PRP PRP 0 0 PRP
V|Ja01 V|He15|N 0 ?0.50 V
DT +1 ?0.184 *
JJ|Ga16|N +1 ?1.0 *
NN|Ae13 NN|Di10 0 ?0.50 NN
NN|Dd08 +1 ?1.0 *
conf(ssr[2]) = ?3.184
Table 3: Calculation of match(d1, d2). ssr denoted
the common SSR between d1 and d2 , conf(ssr[1]) and
conf(ssr[2]) denoted the confidence of ssr.
that di and dj could generate a common SSR if and
only if the orders of nucleus segment and satellite
segment were the same.
In order to guarantee relatively high quality com-
mon SSRs, we empirically set the upper threshold
of the number of mismatches as 0.5 (i.e., ? 1/2 of
the number of words in the generated SSR). It's not
difficult to figure out that the number of mismatches
generated in Table 3 satisfied this requirement. As a
result, for each discourse relation rn, a correspond-
ing common SSR set Sn could be obtained by adopt-
ing match(di, dj) where i ?= j for all discourse in-
stances. An advantage of match(d1, d2) was that
the generated common SSRs preserved the sequen-
tial structure of original discourse instances. And
common SSRs allows us to build high precision dis-
course classifiers (See Section 5).
Weighing and filtering common SSRs
A problem of match(di, dj) was that it ignored
some important information by treating different
mismatches equally. For example, the adverb "very"
in "very brilliant" of D1 was not important for dis-
course recognition. In other words, the number of
mismatches inmatch(di, dj) could not precisely re-
flect the confidence of the generated common SSRs.
Therefore, it was needed to weigh different mis-
matches for the confidence calculation of common
SSRs.
Intuitively, if a partial match or a mismatch (de-
noted by um) occurred very frequently in the gener-
ation of common SSRs, the importance of um tends
to diminish. Inspired by the tf-idf model, given
ssri?Sn, we utilized the following equation to esti-
mate the weight (denoted by wm) of um.
wm = ?ufm ? log (|Sn|/ssrfm )
where ufm denoted the frequency of um during the
generation of ssri, |Sn| denoted the size of Sn and
ssrfm denoted the number of common SSRs in Sn
containing um . All weights were normalized to
[?1, 0).
Nouns (except for named entities) and verbs were
most representative words in discourse recognition
(Marcu and Echihabi, 2002). In addition, adjectives
and adverbs appearing in sentiment lexicons were
important for polarity classification. Therefore, for
these 4 kinds of words, we utilized ?1.0 for a mis-
match and ?0.50 for a partial match.
As we had got the weights for all partial matches
and mismatches, the confidence of ssri?Sn could be
calculated using the cumulation of weights of par-
tial matches and mismatches in ssri[1] and ssri[2].
Recall Table 3, conf(ssr[1]) and conf(ssr[2]) rep-
resented the confidence scores ofmatch(di[1], dj[1])
and match(di[2], dj[2]), respectively. In order to
control the quantity and quality of mined SSRs, a
threshold minconf was introduced. ssri will be
preserved if and only if conf(ssri[1]) ?minconf
and conf(ssri[2]) ? minconf . The value of
minconf was tuned using the development data.
Finally, we combined adjacent '*'s and preserved
SSRs containing at least one notional word and at
least two words in each segment to meet the de-
mand of maintaining high precision (e.g., "[* DT
*]", "[PER *]" will be dropped). Moreover, since
many of the SSRs were duplicated, we ranked all
the generated SSRs according to their occurrences
and dropped those appearing only once in order to
preserve common SSRs. At last, SSRs appearing in
more than one common SSR set were removed for
maintaining the uniqueness of each set. The com-
mon SSR set Sn for each discourse relation rn could
be directly used in SSR-based unsupervised classi-
fiers or be employed as effective features in super-
vised methods.
166
Relation Occurrence
Contrast 86 (8.2%)
Condition 27 (2.6%)
Continuation 445 (42.2%)
Cause 123 (11.7%)
Purpose 55 (5.2%)
Others 318 (30.2%)
Table 4: Distribution of discourse relations on NTC-7.
Others represents discourse relations not included in our
discourse scheme.
5 Experiments
5.1 Annotation work and Data
We extracted all compound sentences which may
contain the defined discourse relations from opinion-
ated sentences (neutral ones were dropped) of NT-
CIR7MOAT simplified Chinese training data. 1,225
discourse instances were extracted and two annota-
tors were trained to annotate discourse relations ac-
cording to the discourse scheme defined in Section 3.
Note that we annotate both explicit and implicit dis-
course relations. The overall inter annotator agree-
ment was 86.05% and the Kappa-value was 0.8031.
Table 4 showed the distribution of annotated dis-
course relations based on the inter-annotator agree-
ment. The proportion of occurrences of each dis-
course relations varied greatly. For example, Con-
tinuation was the most common relation in anno-
tated corpus, but the occurrences of Condition rela-
tion were rare.
The experiments of this paper were performed us-
ing the following data sets:
NTC-7 contained manually annotated discourse
instances (shown in Table 4). The experiments of
discourse identification were performed on this data
set.
NTC-8 contained all opinionated sentences (neu-
tral ones were dropped) extracted from NTCIR8
MOAT simplified Chinese test data. The experi-
ments of polarity ambiguity elimination using the
identified discourse relations were performed on this
data set.
XINHUA contained simplified Chinese raw news
text from Xinhua.com (2002-2005). A word seg-
mentation tool, a part-of-speech tagging tool, a
named entity recognizer and a word sense disam-
biguation tool (Che et al, 2010) were adopted to all
sentences. The common SSRs were mined from this
data set.
5.2 Experimental Settings
Discourse relation identification
In order to systematically justify the effectiveness
of proposed unsupervised method, following exper-
iments were performed on NTC-7:
Baseline used only cue-phrase-based patterns.
M&E proposed by Marcu and Echihabi (2002).
Given a discourse instance Di, the probabilities:
P (rk|(Di[1], Di[2])) for each relation rk were esti-
mated on all text from XINHUA. Then, the most
likely discourse relation was determined by taking
the maximum over argmaxk{P (rk|(Di[1], Di[2])}.
cSSR used both cue-phrase-based patterns to-
gether with common SSRs for recognizing discourse
relations. Common SSRs were mined from dis-
course instances extracted fromXINHUAusing cue-
phrase-based patterns. Development data were ran-
domly selected for tuning minconf .
SVM was trained utilizing cue phrases, probabil-
ities from M&E, topic similarity, structure overlap,
polarity of segments and mined common SSRs (Op-
tional). The parameters of the SVM classifier were
set by a grid search on the training set. We performed
4-fold cross validation on NTC-7 to get an average
performance.
The purposes of introducing SVM in our experi-
ment were: (1) to compare the performance of cSSR
to supervised method; (2) to examine the effective-
ness of integrating common SSRs as features for su-
pervised methods.
Polarity ambiguity elimination
BPC was trained mainly utilizing punctuation,
uni-gram, bi-gram features with confidence score
output. Discourse classifiers such as Baseline, cSSR
or SVM were adopted individually for the post-
processing of BPC. Given an ambiguous sentence
which contained more than one segment, an intuitive
three-step method was adopted to integrated a dis-
course classifier and discourse constraints on polar-
ity for the post-processing of BPC:
(1) Recognize all discourse relations together with
nucleus and satellite information using a discourse
classifier. The nucleus and satellite information is
167
Figure 2: Influences of different values of minconf to
the performance of cSSR
acquired by cSSR if a segment pair could match a
cSSR. Otherwise, we use the annotated nucleus and
satellite information.
(2) Apply discourse constraints on polarity to
ascertain the polarity for each discourse instance.
There may be conflicts between polarities acquired
by BPC and discourse constraints on polarity (e.g.,
Two segments with the same polarity holding a Con-
trast relation). To handle this problem, we chose
the segment with higher polarity confidence and ad-
justed the polarity of the other segment using dis-
course constraints on polarity.
(3) If there was more than one discourse instance
in a single sentence, the overall polarity of the sen-
tence was determined by voting of polarities from
each discourse instance under the majority rule.
5.3 Experimental Results
Refer to Figure 2, the performance of cSSR was
significantly affected by minconf . Note that we
performed the tuning process ofminconf on differ-
ent development data (1/4 instances randomly se-
lected from NTC-7) and Figure 2 showed the av-
erage performance. cSSR became Baseline when
minconf =0. A significant drop of precision was
observed when minconf was less than ?2.5. The
recall remained around 0.495 when minconf ?
?4.0. The best performance was observed when
minconf=?3.5. As a result, ?3.5 was utilized as
the threshold value for cSSR in the following exper-
iments.
Table 5 presented the experimental results for dis-
course relation classification. it showed that:
(1) Cue-phrase-based patterns could find only lim-
ited number of discourse relations (34.1% of average
BPC Baseline cSSR SVM+SSRs
Precision 0.7661 0.7982 0.8059 0.8113
Recall 0.7634 0.7957 0.8038 0.8091
F-score 0.7648 0.7970 0.8048 0.8102
Table 6: Performance of integrating discourse classifiers
and constraints to polarity classification. Note that the
experiments were performed on NTC-8 which contained
only opinionated sentences.
recall) with a very high precision (96.17% of average
precision). This is a proof of assumption (1) given
in Section 4. On the other side, M&E which only
considered word pairs between two segments of dis-
course instances got a higher recall with a large drop
of precision. The drop of precision may be caused
by the neglect of structural and semantic information
of discourse instances. However, M&E still outper-
formed Baseline in average F -score.
(2) cSSR enhanced Baseline by increasing the av-
erage recall by about 15% with only a small drop of
precision. The performance of cSSR demonstrated
that our method could effectively discover high qual-
ity common SSRs. The most remarkable improve-
ment was observed on Continuation in which the re-
call increased by almost 20% with only a minor drop
of precision. Actually, cSSR outperformed Baseline
in all discourse relations except forContrast. In Dis-
course Tree Bank (Carlson et al, 2001) only 26%
of Contrast relations were indicated by cue phrases
while in NTC-7 about 70% of Contrast were indi-
cated by cue phrases. A possible reason was that
we were dealing with Chinese news text which were
usually well written. Another important observation
was that the performance of cSSR was very close to
the result of SVM.
(3) SVM+SSRs achieved the best F -score on
Continuation and average performance. The integra-
tion of SSRs to the feature set of SVM contributed to
a remarkable increase in average F -score. The re-
sults of cSSR and SVM+SSRs demonstrated the ef-
fectiveness of common SSRs mined by the proposed
unsupervised method.
Table 6 presented the performance of integrat-
ing discourse classifiers to polarity classification.
For Baseline and cSSR, the information of nucleus
and satellite could be obtained directly from cue-
168
Relation Baseline M&E cSSR SVM SVM+SSRs
Contrast
P 0.9375 0.4527 0.7531 0.9375 0.9375
R 0.6977 0.7791 0.7093 0.6977 0.6977
F 0.8000 0.5726 0.7305 0.8000 0.8000
Condition
P 1.0000 0.4444 0.6774 1.0000 0.7083
R 0.5556 0.8889 0.7778 0.5185 0.6296
F 0.7143 0.5926 0.7241 0.6829 0.6667
Continuation
P 0.9831 0.6028 0.9761 0.6507 0.7266
R 0.2607 0.5865 0.4584 0.6697 0.6629
F 0.4120 0.5945 0.6239 0.6600 0.6933
Cause
P 1.0000 0.5542 0.9429 1.0000 0.9412
R 0.2114 0.3740 0.2683 0.2114 0.2602
F 0.3489 0.4466 0.4177 0.3489 0.4076
Purpose
P 0.8947 0.3704 0.8163 0.9167 0.7193
R 0.6182 0.7273 0.7273 0.6000 0.7455
F 0.7312 0.4908 0.7692 0.7253 0.7321
Average
P 0.9617 0.5302 0.8864 0.7207 0.7607
R 0.3410 0.5951 0.4878 0.5856 0.6046
F 0.5035 0.5608 0.6293 0.6461 0.6737
Table 5: Performance of recognizing discourse relations. (The evaluation criteria are Precision, Recall and F-score)
phrase-based patterns and SSRs, respectively. For
SVM+cSSR, the nucleus and satellite information
was acquired by cSSR if a segment pair could match
a cSSR. Otherwise, we used manually annotated nu-
cleus and satellite information. It's clear that the
performance of polarity classification was enhanced
with the improvement of discourse relation recogni-
tion. M&E was not included in this experiment be-
cause the performance of polarity classification was
decreased by the mis-classified discourse relations.
SVM+SSRs achieved significant (p<0.01) improve-
ment in polarity classification compared to BPC.
5.4 Discussion
Effect of weighing and filtering
To assess the contribution of weighing and filter-
ing in mining SSRs using a minimum confidence
threshold, i.e. minconf , we implemented cSSR?
without weighing and filtering on the same data set.
Consider Table 7, cSSR achieved obvious improve-
ment in Precision and F -score than cSSR?. More-
over, the total number of SSRs was greatly reduced
in cSSR with only a minor drop of recall. This was
because cSSR? was affected by thousands of low
quality common SSRs which would be filtered in
cSSR. The result in Table 7 proved that weighing and
cSSR? cSSR
Precision 0.6182 0.8864
Recall 0.5014 0.4878
F-score 0.5537 0.6293
NOS > 1 million ? 0.12 million
Table 7: Comparison of cSSR? and cSSR. "NOS" denoted
the number of mined common SSRs.
filtering were essential in our proposed method.
We further analyzed how the improvement was
achieved in cSSR. In our experiment, the most com-
mon mismatches were auxiliary words, named enti-
ties, adjectives or adverbs without sentiments (e.g.,
"green", "very", etc.), prepositions, numbers and
quantifiers. It's straightforward that these words
were insignificant in discourse relation classification
purpose. Moreover, these words did not belong to
the 4 kinds of most representative words. In other
words, the weights of most mismatches were calcu-
lated using the equation presented in Section 4.2 in-
stead of utilizing a unified value, i.e. ?1. Recall
Table 3, the weight of "RB|Ka01" (original: "very")
was ?0.298 and "DT" (original: 'a') was ?0.184.
Comparing to the weights of mismatches for most
representative words (?1.0), the proposed method
successfully down weighed the words which were
169
Figure 3: Improvement from individual discourse rela-
tions. N denoted the number of ambiguities eliminated.
not important for discourse identification. There-
fore, weighing and filtering were able to preserve
high quality SSRs while filter out low quality SSRs
by setting the confidence threshold, i.e. minconf .
Contribution of different discourse relations
We also analyzed the contribution of different dis-
course relations in eliminating polarity ambiguities.
Refer to Figure 3, the improvement of polarity classi-
fication mainly came from three discourse relations:
Contrast, Continuation and Cause. It was straight-
forward that Contrast relation could eliminate po-
larity ambiguities because it held between two seg-
ments with opposite polarities. The contribution of
Cause relation also result from two segments holding
different polarities such as example (a) in Section 1.
However, recall Table 4, although Cause occurred
more often than Contrast, only a part of discourse
instances holding Cause relation contained two seg-
ments with the opposite polarities. Another impor-
tant relation in eliminating ambiguity was Continu-
ation. We investigated sentences with polarities cor-
rected by Continuation relation. Most of them fell
into two categories: (1) sentences with mistakenly
classified sentiments by BPC; (2) sentences with im-
plicit sentiments. For example:
(b) [France and Germany have banned human cloning at
present]?[on 20th, U.S. President George W. Bush called
for regulations of the same content to Congress] (???
????????????????????? 20
???????????????????)
The first segment of example (b) was negative
("banned" expressed a negative sentiment) and a
Continuation relation held between these two seg-
ments. Consequently, the polarity of the second seg-
ment should be negative.
6 Conclusions and Future work
This paper focused on unsupervised discovery
of intra-sentence discourse relations for sentence
level polarity classification. We firstly presented a
discourse scheme based on empirical observations.
Then, an unsupervised method was proposed start-
ing from a small set of cue-phrase-based patterns to
mine high quality common SSRs for each discourse
relation. The performance of discourse classification
was further improved by employing SSRs as features
in supervisedmethods. Experimental results showed
that our methods not only effectively recognized dis-
course relations but also achieved significant im-
provement (p<0.01) in sentence level polarity clas-
sification. Although we were dealing with Chinese
text, the proposed unsupervised method could be
easily generalized to other languages.
The future work will be focused on (1) integrating
more semantic and syntactic information in proposed
unsupervised method; (2) extending our method to
inter-sentence level and then jointly modeling intra-
sentence level and inter-sentence level discourse
constraints on polarity to reach a global optimal in-
ference for polarity classification.
Acknowledgments
This work is partially supported by National 863
program of China (Grant No. 2009AA01Z150),
the Innovation and Technology Fund of Hong Kong
SAR (Project No. GHP/036/09SZ) and 2010/11
CUHK Direct Grants (Project No. EE09743).
References
N. Asher, F. Benamara, and Y.Y. Mathieu. 2008. Distill-
ing opinion in discourse: A preliminary study. Coling
2008: Companion volume: Posters and Demonstra-
tions, pages 5--8.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and refining rhetorical-semantic
relationmodels. InProceedings of NAACLHLT, pages
428--435.
L. Carlson, D.Marcu, andM.E. Okurowski. 2001. Build-
ing a discourse-tagged corpus in the framework of
170
rhetorical structure theory. In Proceedings of the Sec-
ond SIGdial Workshop on Discourse and Dialogue-
Volume 16, pages 1--10. Association for Computa-
tional Linguistics.
W. Che, Z. Li, and T. Liu. 2010. Ltp: A chinese language
technology platform. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Demonstrations, pages 13--16. Association for Com-
putational Linguistics.
D.A. Duverle and H. Prendinger. 2009. A novel dis-
course parser based on support vector machine classi-
fication. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2, pages 665--673. Associ-
ation for Computational Linguistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168--177. ACM.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text organi-
zation. Text-Interdisciplinary Journal for the Study of
Discourse, 8(3):243--281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 368--375. Associa-
tion for Computational Linguistics.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
The penn discourse treebank. InProceedings of the 4th
International Conference on Language Resources and
Evaluation. Citeseer.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing-
Volume 10, pages 79--86. Association for Computa-
tional Linguistics.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: The-
ory and applications, pages 1--10.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In Proceedings of the seventh conference on Natu-
ral language learning at HLT-NAACL 2003-Volume 4,
pages 25--32. Association for Computational Linguis-
tics.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008. Sen-
timent analysis based on probabilistic models using
inter-sentence information.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings of
NAACL HLT, pages 300--307.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 801--808. Association for
Computational Linguistics.
S. Somasundaran, G. Namata, J. Wiebe, and L. Getoor.
2009. Supervised and unsupervised methods in em-
ploying discourse relations for improving opinion po-
larity classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 170--179. Association for Compu-
tational Linguistics.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 149--156. Association for Computational Lin-
guistics.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399--433.
Y.Q. Xia, R.F. Xu, K.F. Wong, and F. Zheng. 2007. The
unified collocation framework for opinion mining. In
International Conference on Machine Learning and
Cybernetics, volume 2, pages 844--850. IEEE.
R. Xu and C. Kit. 2010. Incorporating feature-based and
similarity-based opinion mining--ctl in ntcir-8 moat.
InProceedings of the 8th NTCIRWorkshop, pages 276-
-281.
171
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 897?902,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Is Twitter A Better Corpus for Measuring Sentiment Similarity?
Shi Feng1, Le Zhang1, Binyang Li2,3, Daling Wang1, Ge Yu1, Kam-Fai Wong3
1Northeastern University, Shenyang, China
2University of International Relations, Beijing, China
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{fengshi,wangdaling,yuge}@ise.neu.edu.cn, zhang777le@gmail.com
{byli,kfwong}@se.cuhk.edu.hk
Abstract
Extensive experiments have validated the ef-
fectiveness of the corpus-based method for
classifying the word?s sentiment polarity.
However, no work is done for comparing d-
ifferent corpora in the polarity classification
task. Nowadays, Twitter has aggregated huge
amount of data that are full of people?s senti-
ments. In this paper, we empirically evaluate
the performance of different corpora in sen-
timent similarity measurement, which is the
fundamental task for word polarity classifica-
tion. Experiment results show that the Twitter
data can achieve a much better performance
than the Google, Web1T and Wikipedia based
methods.
1 Introduction
Measuring semantic similarity for words and short
texts has long been a fundamental problem for many
applications such as word sense disambiguation,
query expansion, search advertising and so on.
Determining the word?s polarity plays a critical
role in opinion mining and sentiment analysis task.
Usually we can detect the word?s polarity by mea-
suring it?s semantic similarity with a positive seed
word sep and a negative seed word sen respectively,
as shown in Formula (1):
SO(w) = sim(w, sep)? sim(w, sen) (1)
where sim(wi, wj) is the semantic similarity mea-
surement method for the given word wi and wj . A
lot of papers have been published for designing ap-
propriate similarity measurements. One direction is
to learn similarity from the knowledge base or con-
cept taxonomy (Lin, 1998; Resnik, 1999). Anoth-
er direction is to learn semantic similarity with the
help of large corpus such as Web or Wikipedia da-
ta (Sahami and Heilman, 2006; Yih and Meek, 2007;
Bollegala et al, 2011; Gabrilovich and Markovitch,
2007). The basic assumption of this kind of methods
is that the word with similar semantic meanings of-
ten co-occur in the given corpus. Extensive experi-
ments have validated the effectiveness of the corpus-
based method in polarity classification task (Turney,
2002; Kaji and Kitsuregawa, 2007; Velikovich et al,
2010). For example, PMI is a well-known similari-
ty measurement (Turney, 2002), which makes use of
the whole Web as the corpus, and utilizes the search
engine hits number to estimate the co-occurrence
probability of the give word pairs. The PMI based
method has achieved promising results. However,
according to Kanayama?s investigation, only 60%
co-occurrences in the same window in Web pages
reflect the same sentiment orientation (Kanayama
and Nasukawa, 2006). Therefore, we may ask the
question whether the choosing of corpus can change
the performance of sim and is there any better cor-
pus than the Web page data for measuring the senti-
ment similarity?
Everyday, enormous numbers of tweets that con-
tain people?s rich sentiments are published in Twit-
ter. The Twitter may be a good source for measuring
the sentiment similarity. Compared with the Web
page data, the tweets have a higher rate of subjective
text posts. The length limitation can guarantee the
polarity consistency of each tweet. Moreover, the
tweets contain graphical emoticons, which can be
897
considered as natural sentiment labels for the corre-
sponding tweets in Twitter. In this paper, we attempt
to empirically evaluate the performance of differen-
t corpora in sentiment similarity measurement task.
As far as we know, no work is done on this topic.
2 The Characteristics of Twitter Data
As the world?s second largest SNS website, at the
end of 2012 Twitter had aggregated more than 500
million registered users, among which 200 million
were active users . More than 400 million tweets are
posted every day.
Several examples of typical posts from Twitter are
shown below.
(1) She had a headache and feeling light headed
with no energy. :(
(2) @username Nice work! Looks like you had a
fun day. I?m headed there Sat or Sun. :)
(3) I seen the movie on Direc Tv. I ordered it and
I really liked it. I can?t wait to get it for blu ray!
Excellent work Rob!
We observe that comparing with the other corpus,
the Twitter data has several advantages in measuring
the sentiment similarity.
Large. Users like to record their personal feelings
and talk about the trend topics in Twitter (Java et al,
2007; Kwak et al, 2010). So there are huge amount
of subjective texts with various topics generated in
the millions of tweets everyday. Further more, the
flexible Twitter API makes these data easy to access
and collect.
Length Limitation. Twitter has a length limita-
tion of 140 characters. Users have limited space to
express their feelings. So the sentiments in tweet-
s are usually concise, straightforward and polarity
consistent.
Emoticons. Users tend to utilize emoticons to
emphasize their sentiment feelings. According to
the statistics, about 8.1% tweets contain at least one
emoticon (Yang and Leskovec, 2011). Since the
tweets have the length limitation, the sentiments ex-
pressed in these short texts are usually consistent
with the embedded emoticons, such as the word fun
and headache in above examples.
In addition to the above advantages, there are al-
so some disadvantages for measuring sentiment sim-
ilarity using Twitter data. The spam tweets that
caused by advertisements may add noise and bias
during the similarity measurement. The short length
may also bring in lower co-occurrence probability
of words. Some words may not co-occur with each
other when the corpus is small. These disadvantages
set obstacles for measuring sentiment similarity by
using Twitter data as corpus. In the experiment sec-
tion, we will see if we can overcome these draw-
backs and get benefit from the advantages of Twitter
data.
3 The Corpus-based Sentiment Similarity
Measurements
The intuition behind the corpus-based semantic sim-
ilarity measuring method is that the words with sim-
ilar meanings tend to co-occur in the corpus. Given
the word wi, wj , we use the notation P (wi) to de-
note the occurrence counts of word wi in the corpus
C. P (wi, wj) denotes the co-occurrence counts of
word wi and wj in C. In this paper we employ the
corpus-based version of the three well-known simi-
larity measurements: Jaccard, Dice and PMI.
CorpusJaccard(wi, wj)
= P (wi,wj)P (wi)+P (wj)?P (wi,wj)
(2)
CorpusDice(wi, wj) =
2? P (wi, wj)
P (wi) + P (wj)
(3)
CorpusPMI(wi, wj) = log2(
P (wi,wj)
N
P (wi)
N
P (wj)
N
) (4)
In Formula (4), N is the number of documents in
the corpus C. The above similarity measurements
may have their own strengths and weaknesses. In
this paper, we utilize these classical measurements
to evaluate the quality of the corpus in polarity clas-
sification task.
Google is the world?s largest search engine, which
has indexed a huge number of Web pages. Us-
ing the extreme large indexed Web pages as cor-
pus, Cilibrasi and Vitanyi (2007) presented a method
for measuring similarity between words and phras-
es based on information distance and Kolmogorov
complexity. The search result page counts of Google
were utilized to estimate the occurrence frequencies
of the words in the corpus. Suppose wi, wj rep-
resent the candidate words, the Normalized Google
898
Distance is defined as:
NGD(wi, wj) =
max{logP (wi),logP (wj)}?logP (wi,wj)
logN?min{logP (wi),logP (wj)}
(5)
where P (wi) denotes page counts returned by
Google using wi as keyword; P (wi, wj) denotes the
page counts by using wi and wj as joint keywords;
N is the number of Web pages indexed by Google.
Cilibrasi and Vitanyi have validated the effective-
ness of Google distance in measuring the semantic
similarity between concept words.
Based on the above formulas, we compare the
Twitter data with the Web and Wikipedia data as the
similarity measurement corpus. Given a candidate
word w, we firstly measure its sentiment similar-
ity with a positive seed word and a negative seed
word respectively in Formula (1), and the difference
of sim is used to further detect the polarity of w.
The above four similarity measurements serve as
sim with Web, Wikipedia and Twitter data as cor-
pus. Turney (2002) chose excellent and poor as
seed words. However, using isolated seed word-
s may cause the bias problem. Therefore, we fur-
ther select two groups of seed words that are lack
of sensitivity to context and form a positive seed set
PS and a negative seed set NS (Turney, 2003). The
Formula (1) can be rewritten as:
SO(w) =
?
sep?PS
sim(w, sep)?
?
sen?NS
sim(w, sen) (6)
Based on the Formula(6) and the sentiment seed
words, we can measure the sentiment polarity of the
given candidate words.
4 Experiment
4.1 Experiment Setup
Corpus Preparing. The Twitter corpus corre-
sponds to the 476 million Twitter tweets (Yang and
Leskovec, 2011), which includes over 476 million
Twitter posts from 20 million users, covering a 7
month period from June 1, 2009 to December 31,
2009. We filter out the non-English tweets and the
spam tweets that have only few words with URLs.
The tweets that contain three or more trending topics
are also removed. Finally, we construct the Twitter
corpus that consists of 266.8 million English tweets.
For calculating page counts in Web data, the candi-
date words were launched to Google from February
2013 to April 2013. We also conduct the experi-
ments on the Google Web 1T data that consists of
Google n-gram counts (frequency of occurrence of
each n-gram) for 1 ? n ? 5 (Brants and Franz,
2006). The Web 1T data provides a nice approxi-
mation to the word co-occurrence statistics in Web
pages in a predefined window size (1 ? n ? 5).
For example, the 5 gram Web1T data means the co-
occurrence window size is 5. The English Wikipedia
dump 1 we used was extracted at the end of March
2013, which contained more than 13 million articles.
We extracted the plain texts of the Wikipedia data as
the training corpus for the Formula (6).
EvaluationMethod. Two well-know sentiment lex-
icons are utilized as gold standard for polarity clas-
sification task. The statistics of Liu?s sentiment lex-
icon (Liu et al, 2005) and MPQA subjectivity lexi-
con (Wilson et al, 2005) are shown in Table 1. For
each word w in the lexicons, we employ the Formu-
la (6) to calculate the word?s polarity using different
corpora. If SO(w) > 0, the word w is classified in-
to the positive category. Otherwise if SO(w) < 0, it
is classified into the negative category. The accura-
cy of the classification result is used to measure the
quality of the corpus.
Positive# Negative#
Liu 2,006 4,783
MPQA 2,304 4,153
Table 1: Lexicon size
4.2 Experiment Results
Firstly, we chose the seed words excellent and poor
as Turney?s (2002) settings. The polarity classifica-
tion accuracies are shown in Table 2.
In Table 2, Google, Web1T, Wikipedia, Twitter
represent the corpora that used in the experiment;
CJ, CD, CP, GD represent the Formula (2) to For-
mula (5) respectively. We can see from the Table 2
that the Twitter based method can achieve the best
performance. The rich sentiment information and
1http://en.wikipedia.org/
899
Lexicon Corpus CJ CD CP GD
Liu
Google 0.5116 0.5117 0.5064 0.5076
Web1T-5gram 0.3903 0.3903 0.3897 0.3864
Web1T-4gram 0.3771 0.3771 0.3772 0.3227
Wikipedia 0.5280 0.5280 0.5350 0.5412
Twitter 0.5567 0.5567 0.5635 0.5635
MPQA
Google 0.4897 0.4890 0.4891 0.4864
Web1T-5gram 0.3843 0.3843 0.3837 0.3783
Web1T-4gram 0.3729 0.3729 0.3714 0.3225
Wikipedia 0.5181 0.5181 0.5380 0.5344
Twitter 0.5421 0.5421 0.5493 0.5494
Table 2: Polarity classification accuracies using excellent
and poor as seed words
natural window size (140 characters) have a posi-
tive impact on determining the word?s polarity. The
Google based method gets a lower accuracy, this
may be due to the length of Web documents which
can not usually guarantee the semantic consistency
in the returned data. Even though two words appear
in one page (returned by Google), they might not be
semantically related. Furthermore, the Google based
method is time-consuming, because we have to peri-
odically send queries in order to avoid being blocked
by Google. The Web1T based method gets a much
worse accuracy. After detailed analysis, we find that
although the small window size (4 or 5) can guar-
antee the semantic consistency, the short length also
brings in lower co-occurrence probability. Statistics
show that about 38% SO values are zero when using
Web1T corpus. Due to the short length, the Twitter
data also suffers from the low co-occurrence prob-
lem.
To tackle the low co-occurrence problem, the seed
word sets are selected as Turney?s (2003) settings.
The positive word set PS={good, nice, excellent,
positive, fortunate, correct, superior} and negative
word set NS = {bad, nasty, poor, negative, unfortu-
nate, wrong, inferior} for the Formula (6). These
seed words have been verified to be effective in Tur-
ney?s paper for polarity classification. The experi-
ment results are shown in Table 3.
Table 3 shows that the performance of Twitter cor-
pus is much improved since the multiple seed words
alleviate the problem of low co-occurrence probabil-
ity in tweets. Generally, when using the seed word
groups the Twitter can achieve a much better per-
formance than all the other corpora. The improve-
ments are statistically significant (p-value < 0.05).
Lexicon Corpus CJ CD CP GD
Liu
Google 0.4859 0.4936 0.4884 0.5060
Web1T-5gram 0.5785 0.5785 0.3963 0.5782
Web1T-4gram 0.5766 0.5766 0.3872 0.5775
Wikipedia 0.6226 0.6225 0.5957 0.6145
Twitter 0.6678 0.6678 0.6917 0.6457
Twitter+ 0.6921 0.6921 0.7273 0.6599
MPQA
Google 0.5108 0.5225 0.5735 0.5763
Web1T-5gram 0.5737 0.5737 0.4225 0.5718
Web1T-4gram 0.5749 0.5749 0.3329 0.4797
Wikipedia 0.6086 0.6085 0.5773 0.5985
Twitter 0.6431 0.6431 0.6671 0.6253
Twitter+ 0.6665 0.6665 0.7001 0.6383
Table 3: Polarity classification accuracies using the seed
word groups
We further add the emoticons ?:)? and ?:(? into the
seed word groups, denoted by Twitter+ in Table 3.
The emoticons are natural sentiment labels. We can
see that the performances are further improved by
considering emoticons as seed words. The above
experiment results have validated the effectiveness
of Twitter data as a better corpus for measuring the
sentiment similarity. The results also reveal the po-
tential usefulness of Twitter corpus in semantic sim-
ilarity measurement.
5 Related Work
Detecting the polarity of words is the fundamental
problem for most of sentiment analysis tasks (Hatzi-
vassiloglou and McKeown, 1997; Pang and Lee,
2007; Feldman, 2013).
Many methods have been proposed to measure
the words? or short texts similarity based on large
corpus (Sahami and Heilman, 2006; Yih and Meek,
2007; Gabrilovich and Markovitch, 2007). Bolle-
gala et al (2011) submitted the word to the search
engine, and the related result pages were employed
to represent the meaning of the original word. Mi-
halcea et al (2006) proposed a method to measure
the semantic similarity of words or short texts, con-
sidering both corpus-based and knowledge-based in-
formation. Although the previous algorithms have
achieved promising results, there are no work done
on evaluating the quality of different corpora.
Mohtarami et al (2012; 2013a; 2013b) intro-
duced the concept of sentiment similarity, which
was considered as different from the traditional se-
mantic similarity, and more focused on revealing the
underlying sentiment relations between words. Mo-
900
htarami et al (2013b) proposed a hidden emotion-
al model to calculating the sentiment similarity of
word pairs. However, the impact of the different cor-
pora is not considered for this task.
Mohammad et al (2013) generated word-
sentiment association lexicons from Tweets with the
help of hashtags and emoticons. Pak and Paroubek
(2010) collected tweets with happy and sad emoti-
cons as training corpus, and built sentiment classi-
fier based on traditional machine learning methods.
Brody and Diakopoulos (2011) showed that length-
ening was strongly associated with subjectivity and
sentiment in tweets. Davidov et al (2010) treated 50
Twitter tags and 15 smileys as sentiment labels and
a supervised sentiment classification framework was
proposed to classify the tweets. The previous litera-
tures have showed that the emoticons can be treated
as natural sentiment labels of the tweets.
6 Conclusion and Future Work
The quality of corpus may affect the performance
of sentiment similarity measurement. In this pa-
per, we compare the Twitter data with the Google,
Web1T and Wikipedia data in polarity classification
task. The experiment results validate that when us-
ing the seed word groups the Twitter can achieve a
much better performance than the other corpora and
adding emoticons as seed words can further improve
the performance. It is observed that the twitter cor-
pus is a potential good source for measuring senti-
ment similarity between words. In future work, we
intend to design new similarity measurements that
can make best of the advantages of Twitter data.
Acknowledgments
This research is partially supported by Gener-
al Research Fund of Hong Kong (No. 417112)
and Shenzhen Fundamental Research Program (J-
CYJ20130401172046450). This research is al-
so supported by the State Key Development Pro-
gram for Basic Research of China (Grant No.
2011CB302200-G), State Key Program of Nation-
al Natural Science of China (Grant No. 61033007),
National Natural Science Foundation of China
(Grant No. 61370074, 61100026), and the Funda-
mental Research Funds for the Central Universities
(N120404007).
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using
Word Lengthening to Detect Sentiment in Microblogs.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
562?570, Edinburgh, UK, ACL.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizu-
ka. 2011. A Web Search Engine-Based Approach to
Measure Semantic Similarity between Words. IEEE
Transactions on Knowledge and Data Engineering,
23(7): 977?990.
Rudi Cilibrasi and Paul Vitanyi. 2007. The Google Simi-
larity Distance. IEEE Transactions on Knowledge and
Data Engineering, 19(3): 370?383.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hashtags
and Smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 241?
249, Beijing, China, ACL.
Ronen Feldman. 2013. Techniques and Applications for
Sentiment Analysis. Communications of the ACM,
56(4):82?89.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611, Hyderabad, India.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics, pages
174?181, Madrid, Spain, ACL.
Akshay Java, Xiaodan Song, Tim Finin, and Belle L. T-
seng. 2007. Why We Twitter: An Analysis of a Mi-
croblogging Community. In Proceedings of the 9th
International Workshop on Knowledge Discovery on
the Web and 1st International Workshop on Social Net-
works Analysis, pages 118?138, San Jose, CA, USA,
Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. Building Lex-
icon for Sentiment Analysis from Massive Collec-
tion of HTML Documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pp. 1075?1083, Prague, Czech
Republic, ACL.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Ful-
ly Automatic Lexicon Expansion for Domain-oriented
901
Sentiment Analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 355?363, Sydney, Australia, ACL.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
B. Moon. 2010. What is Twitter, a Social Network or a
News Media? In Proceedings of the 19th Internation-
al Conference on World Wide Web, pages 591?600,
Raleigh, North Carolina, USA, ACM.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 768?774, Montreal, Quebec, Canada, ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: Analyzing and Comparing Opin-
ions on the Web. In Proceedings of the 14th interna-
tional conference on World Wide Web, pages 342?351,
Chiba, Japan, ACM.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence and the
18th Innovative Applications of Artificial Intelligence
Conference, pages 775?780, Boston, Massachusetts,
USA, AAAI Press.
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh Phu
Tran, and Chew Lim Tan. 2012. Sense Sentimen-
t Similarity: An Analysis. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence, pages 1706?1712, Toronto, Ontario, Canada,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013a.
From Semantic to Emotional Space in Probabilistic
Sense Sentiment Analysis. In Proceedings of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 711?717, Bellevue, Washington, USA,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013b.
Probabilistic Sense Sentiment Similarity through Hid-
den Emotions. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics, pages 983?992, Sofia, Bulgaria, ACL.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proceedings
of the seventh international workshop on Semantic E-
valuation Exercises, Atlanta, Georgia, USA, ACL.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
Corpus for Sentiment Analysis and Opinion Mining.
In Proceedings of the 2010 International Conference
on Language Resources and Evaluation, pages 1320?
1326, Valletta, Malta, ELRA.
Philip Resnik. 1999. Semantic Similarity in a Taxonomy:
An Information based Measure and Its Application to
Problems of Ambiguity in Natural Language. Journal
of Artificial Intelligence Research, 11:95?130.
Mehran Sahami and Timothy D. Heilman. 2006. A Web-
based Kernel Function for Measuring the Similarity of
Short Text Snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, pages 377?
386, Edinburgh, Scotland, UK, ACM.
Bo Pang and Lillian Lee. 2007. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Peter D. Turney. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised Classi-
fication of Reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 417?424, Philadelphia, PA, USA, ACL.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transaction Information Sys-
tem, 21(4): 315?346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan T. McDonald. The Viability of Web-
derived Polarity Lexicons. In Proceedings of the 2010
North American Chapter of the Association of Compu-
tational Linguistics, pp. 777?785, Los Angeles, Cali-
fornia, USA, ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman-
n. 2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347?354, Vancouver, British Columbi-
a, Canada, ACL.
Jaewon Yang and Jure Leskovec. 2011. Patterns of Tem-
poral Variation in Online Media. In Proceedings of
the Forth International Conference on Web Search and
Web Data Mining, pages 177?186, Hong Kong, Chi-
na, ACM.
Wen-tau Yih and Christopher Meek. 2007. Improving
Similarity Measures for Short Segments of Text. In
Proceedings of the 22nd AAAI Conference on Artificial
Intelligence, pages 1489?1494, Vancouver, British
Columbia, Canada, AAAI Press.
902
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1159?1168,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploiting Community Emotion for Microblog Event Detection
Gaoyan Ou
1,2
, Wei Chen
1,2,
, Tengjiao Wang
1,2
, Zhongyu Wei
1,3
,
Binyang Li
4
, Dongqing Yang
1,2
and Kam-Fai Wong
1,3
1
Key Laboratory of High Confidence Software Technologies, Ministry of Education, China
2
School of Electronics Engineering and Computer Science, Peking University, China
3
Shenzhen Research Institute, The Chinese University of Hong Kong, China
4
Dept. of Information Science & Technology, University of International Relations, China
pekingchenwei@pku.edu.cn
Abstract
Microblog has become a major plat-
form for information about real-world
events. Automatically discovering real-
world events from microblog has attracted
the attention of many researchers. Howev-
er, most of existing work ignore the impor-
tance of emotion information for event de-
tection. We argue that people?s emotion-
al reactions immediately reflect the occur-
ring of real-world events and should be im-
portant for event detection. In this study,
we focus on the problem of community-
related event detection by community e-
motions. To address the problem, we pro-
pose a novel framework which include
the following three key components: mi-
croblog emotion classification, community
emotion aggregation and community emo-
tion burst detection. We evaluate our ap-
proach on real microblog data sets. Exper-
imental results demonstrate the effective-
ness of the proposed framework.
1 Introduction
Microblog has become a popular and convenient
platform for people to share information about so-
cial events in real time. When an external even-
t occurs, it will be quickly propagated between
microblog users. During propagation process of
an event, sufficient amount of users will express
their emotions. Taking Sina Weibo
1
as an exam-
ple, more than 12 percent of users use emoticons
2
when reposting an event-related microblog mes-
sage.
The emotion information can not only help us
better understand a given event, but also be u-
tilized to discover new events. Figure 1 shows
1
http://weibo.com/
2
An icon to indicate user?s emotion, as shown in Table 1.
 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1367?1375,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Unified Graph Model for Sentence-based Opinion Retrieval 
 
 
Binyang Li, Lanjun Zhou, Shi Feng, Kam-Fai Wong 
Department of Systems Engineering and Engineering Management 
The Chinese University of Hong Kong 
{byli, ljzhou, sfeng, kfwong}@se.cuhk.edu.hk 
 
 
Abstract 
There is a growing research interest in opinion 
retrieval as on-line users? opinions are becom-
ing more and more popular in business, social 
networks, etc. Practically speaking, the goal of 
opinion retrieval is to retrieve documents, 
which entail opinions or comments, relevant to 
a target subject specified by the user?s query. A 
fundamental challenge in opinion retrieval is 
information representation. Existing research 
focuses on document-based approaches and 
documents are represented by bag-of-word. 
However, due to loss of contextual information, 
this representation fails to capture the associa-
tive information between an opinion and its 
corresponding target. It cannot distinguish dif-
ferent degrees of a sentiment word when asso-
ciated with different targets. This in turn se-
riously affects opinion retrieval performance. 
In this paper, we propose a sentence-based ap-
proach based on a new information representa-
tion, namely topic-sentiment word pair, to cap-
ture intra-sentence contextual information be-
tween an opinion and its target. Additionally, 
we consider inter-sentence information to cap-
ture the relationships among the opinions on 
the same topic. Finally, the two types of infor-
mation are combined in a unified graph-based 
model, which can effectively rank the docu-
ments. Compared with existing approaches, 
experimental results on the COAE08 dataset 
showed that our graph-based model achieved 
significant improvement. 
1 Introduction 
In recent years, there is a growing interest in 
sharing personal opinions on the Web, such as 
product reviews, economic analysis, political 
polls, etc. These opinions cannot only help inde-
pendent users make decisions, but also obtain 
valuable feedbacks (Pang et al, 2008). Opinion 
oriented research, including sentiment classifica-
tion, opinion extraction, opinion question ans-
wering, and opinion summarization, etc. are re-
ceiving growing attention (Wilson, et al, 2005; 
Liu et al, 2005; Oard et al, 2006). However, 
most existing works concentrate on analyzing 
opinions expressed in the documents, and none 
on how to represent the information needs re-
quired to retrieve opinionated documents. In this 
paper, we focus on opinion retrieval, whose goal 
is to find a set of documents containing not only 
the query keyword(s) but also the relevant opi-
nions. This requirement brings about the chal-
lenge on how to represent information needs for 
effective opinion retrieval. 
In order to solve the above problem, previous 
work adopts a 2-stage approach. In the first stage, 
relevant documents are determined and ranked 
by a score, i.e. tf-idf value. In the second stage, 
an opinion score is generated for each relevant 
document (Macdonald and Ounis, 2007; Oard et 
al., 2006). The opinion score can be acquired by 
either machine learning-based sentiment classifi-
ers, such as SVM (Zhang and Yu, 2007), or a 
sentiment lexicons with weighted scores from 
training documents (Amati et al, 2007; Hannah 
et al, 2007; Na et al, 2009). Finally, an overall 
score combining the two is computed by using a 
score function, e.g. linear combination, to re-rank 
the retrieved documents. 
Retrieval in the 2-stage approach is based on 
document and document is represented by 
bag-of-word. This representation, however, can 
only ensure that there is at least one opinion in 
each relevant document, but it cannot determine 
the relevance pairing of individual opinion to its 
target. In general, by simply representing a 
document in bag-of-word, contextual informa-
tion i.e. the corresponding target of an opinion, is 
neglected. This may result in possible mismatch 
between an opinion and a target and in turn af-
fects opinion retrieval performance. By the same 
token, the effect to documents consisting of mul-
1367
tiple topics, which is common in blogs and 
on-line reviews, is also significant. In this setting, 
even if a document is regarded opinionated, it 
cannot ensure that all opinions in the document 
are indeed relevant to the target concerned. 
Therefore, we argue that existing information 
representation i.e. bag-of-word, cannot satisfy 
the information needs for opinion retrieval. 
In this paper, we propose to handle opinion re-
trieval in the granularity of sentence. It is ob-
served that a complete opinion is always ex-
pressed in one sentence, and the relevant target 
of the opinion is mostly the one found in it. 
Therefore, it is crucial to maintain the associative 
information between an opinion and its target 
within a sentence. We define the notion of a top-
ic-sentiment word pair, which is composed of a 
topic term (i.e. the target) and a sentiment word 
(i.e. opinion) of a sentence. Word pairs can 
maintain intra-sentence contextual information to 
express the potential relevant opinions. In addi-
tion, inter-sentence contextual information is also 
captured by word pairs to represent the relation-
ship among opinions on the same topic. In prac-
tice, the inter-sentence information reflects the 
degree of a word pair. Finally, we combine both 
intra-sentence and inter-sentence contextual in-
formation to construct a unified undirected graph 
to achieve effective opinion retrieval. 
The rest of the paper is organized as follows. 
In Section 2, we describe the motivation of our 
approach. Section 3 presents a novel unified 
graph-based model for opinion retrieval. We 
evaluated our model and the results are presented 
in Section 4. We review related works on opi-
nion retrieval in Section 5. Finally, in Section 6, 
the paper is concluded and future work is sug-
gested.  
2 Motivation 
In this section, we start from briefly describing 
the objective of opinion retrieval. We then illu-
strate the limitations of current opinion retrieval 
approaches, and analyze the motivation of our 
method.  
2.1 Formal Description of Problem 
Opinion retrieval was first presented in the 
TREC 2006 Blog track, and the objective is to 
retrieve documents that express an opinion about 
a given target. The opinion target can be a ?tradi-
tional? named entity (e.g. a name of person, lo-
cation, or organization, etc.), a concept (e.g. a 
type of technology), or an event (e.g. presidential 
election). The topic of the document is not re-
quired to be the same as the target, but an opi-
nion about the target has to be presented in the 
document or one of the comments to the docu-
ment (Macdonald and Ounis, 2006). Therefore, 
in this paper we regard the information needs for 
opinion retrieval as relevant opinion. 
2.2 Motivation of Our Approach 
In traditional information retrieval (IR) 
bag-of-word representation is the most common 
way to express information needs. However, in 
opinion retrieval, information need target at re-
levant opinion, and this renders bag-of-word re-
presentation ineffective. 
Consider the example in Figure 1. There are 
three sentences A, B, and C in a document di. 
Now given an opinion-oriented query Q related 
to ?Avatar?. According to the conventional 
2-stage opinion retrieval approach, di is 
represented by a bag-of-word. Among the words, 
there is a topic term Avatar (t1) occurring twice, 
i.e. Avatar in A and Avatar in C, and two senti-
ment words comfortable (o1) and favorite (o2) 
(refer to Figure 2 (a)). In order to rank this doc-
ument, an overall score of the document di is 
computed by a simple combination of the rele-
vant score ( ???????? ) and the opinion score 
(???????), e.g. equal weighted linear combination, 
as follows. 
???????? ? ???????? ?     ??????? 
For simplicity, we let ???????? ? ? ?? ? ?? ?? , and 
???????  be computed by using lexicon-based 
method: ??????? ? ????????????????? ? ??????????????. 
 
 
 
 
 
Figure 1: A retrieved document di on the target 
?Avatar?. 
Although bag-of-word representation achieves 
good performance in retrieving relevant docu-
ments, our study shows that it cannot satisfy the 
information needs for retrieval of relevant opi-
nion. It suffers from the following limitations: 
(1) It cannot maintain contextual information; 
thus, an opinion may not be related to the target 
of the retrieved document is neglected. In this 
example, only the opinion favorite (o2) on Avatar 
in C is the relevant opinion. But due to loss of 
contextual information between the opinion and 
its corresponding target, Avatar in A and com-
A. ???????????? 
Tomorrow, Avatar will be shown in China. 
B. ????? IMAX??????????
I?ve reserved a comfortable seat in IMAX. 
C. ??????????? 3D??? 
Avatar is my favorite 3D movie.  
1368
fortable (o1) are also regarded as relevant opi-
nion mistakenly, creating a false positive. In re-
ality comfortable (o1) describes ?the seats in 
IMAX?, which is an irrelevant opinion, and sen-
tence A is a factual statement rather than an opi-
nion statement. 
    
     
  (a)                (b)        
Figure 2: Two kinds of information representa-
tion of opinion retrieval. (t1=?Avatar? o1= ?com-
fortable?, o2=?favorite?) 
(1) Current approaches cannot capture the re-
lationship among opinions about the same topic. 
Suppose there is another document including 
sentence C which expresses the same opinion on 
Avatar. Existing information representation 
simply does not cater for the two identical opi-
nions from different documents. In addition, if 
many documents contain opinions on Avatar, the 
relationship among them is not clearly 
represented by existing approaches.  
In this paper, we process opinion retrieval in 
the granularity of sentence as we observe that a 
complete opinion always exists within a sentence 
(refer to Figure 2 (b)). To represent a relevant 
opinion, we define the notion of topic-sentiment 
word pair, which consists of a topic term and a 
sentiment word. A word pair maintains the asso-
ciative information between the two words, and 
enables systems to draw up the relationship 
among all the sentences with the same opinion 
on an identical target. This relationship informa-
tion can identify all documents with sentences 
including the sentiment words and to determine 
the contributions of such words to the target 
(topic term). Furthermore, based on word pairs, 
we designed a unified graph-based method for 
opinion retrieval (see later in Section 3). 
3 Graph-based model 
3.1 Basic Idea 
Different from existing approaches which simply 
make use of document relevance to reflect the 
relevance of opinions embedded in them, our 
approach concerns more on identifying the re-
levance of individual opinions. Intuitively, we 
believed that the more relevant opinions appear 
in a document, the more relevant is that docu-
ment for subsequent opinion analysis operations. 
Further, since the lexical scope of an opinion 
does not usually go beyond a sentence, we pro-
pose to handle opinion retrieval in the granularity 
of sentence. 
Without loss of generality, we assume that 
there is a document set ? ? ???, ??, ??,? , ???, and 
a specific query  ? ? ???, ??, ??,? , ??? , where 
??, ??, ??,? , ?? are query keywords. Opinion re-
trieval aims at retrieving documents from ? 
with relevant opinion about the query ?. In ad-
dition, we construct a sentiment word lexicon ?? 
and a topic term lexicon ?? (see Section 4). To 
maintain the associative information between the 
target and the opinion, we consider the document 
set as a bag of sentences, and define a sentence 
set as ? ? ???, ??, ??,? , ???. For each sentence, we 
capture the intra-sentence information through 
the topic-sentiment word pair.  
Definition 1. topic-sentiment word pair ???  con-
sists of two elements, one is from ??, and the 
other one is from ??.  
??? ? ?? ??, ?? ? |?? ? ?? , ?? ? ????. 
The topic term from ?? determines relevance 
by the query term matching, and the sentiment 
word from ?? is used to express an opinion. We 
use the word pair to maintain the associative in-
formation between the topic term and the opinion 
word (also referred to as sentiment word). The 
word pair is used to identify a relevant opinion in 
a sentence. In Figure 2 (b), t1, i.e. Avatar in C, is 
a topic term relevant to the query, and o2 (?favo-
rite?) is supposed to be an opinion; and the word 
pair < t1, o2> indicates sentence C contains a re-
levant opinion. Similarly, we map each sentence 
in word pairs by the following rule, and express 
the intra-sentence information using word pairs. 
For each sentiment word of a sentence, we 
choose the topic term with minimum distance as 
the other element of the word pair: 
?? ? ?? ??, ?? ? |?? ? min???????, ??? for each ??? 
According to the mapping rule, although a 
sentence may give rise to a number of word pairs, 
only the pair with the minimum word distance is 
selected. We do not take into consideration of the 
other words in a sentence as relevant opinions 
are generally formed in close proximity. A sen-
tence is regarded non-opinionated unless it con-
tains at least one word pair. 
In practice, not all word pairs carry equal 
weights to express a relevant opinion as the con-
tribution of an opinion word differs from differ-
ent target topics, and vice versa. For example, 
the word pair < t1, o2> should be more probable 
as a relevant opinion than < t1, o1>. To consider 
1369
that, inter-sentence contextual information is ex-
plored. This is achieved by assigning a weight to 
each word pair to measure their associative de-
grees to different queries. We believe that the 
more a word pair appears the higher should be 
the weight between the opinion and the target in 
the context. 
We will describe how to utilize intra-sentence 
contextual information to express relevant opi-
nion, and inter-sentence information to measure 
the degree of each word pair through a 
graph-based model in the following section. 
3.2 HITS Model 
We propose an opinion retrieval model based on 
HITS, a popular graph ranking algorithm 
(Kleinberg, 1999). By considering both in-
tra-sentence information and inter-sentence in-
formation, we can determine the weight of a 
word pair and rank the documents.  
HITS algorithm distinguishes hubs and au-
thorities in objects. A hub object has links to 
many authorities. An authority object, which has 
high-quality content, would have many hubs 
linking to it. The hub scores and authority scores 
are computed in an iterative way. Our proposed 
opinion retrieval model contains two layers. The 
upper level contains all the topic-sentiment word 
pairs ??? ? ?? ??, ?? ? |?? ? ??, ?? ? ???? . The lower 
level contains all the documents to be retrieved. 
Figure 3 gives the bipartite graph representation 
of the HITS model.  
 
Figure 3: Bipartite link graph. 
For our purpose, the word pairs layer is consi-
dered as hubs and the documents layer authori-
ties. If a word pair occurs in one sentence of a 
document, there will be an edge between them. 
In Figure 3, we can see that the word pair that 
has links to many documents can be assigned a 
high weight to denote a strong associative degree 
between the topic term and a sentiment word, 
and it likely expresses a relevant opinion. On the 
other hand, if a document has links to many word 
pairs, the document is with many relevant opi-
nions, and it will result in high ranking. 
Formally, the representation for the bipartite 
graph is denoted as ? ?? ??, ??, ??? ? , where 
?? ? ????? is the set of all pairs of topic words 
and sentiment words, which appear in one sen-
tence. ?? ? ??? ?  is the set of documents. 
??? ? ????
? |??? ? ??, ?? ? ???  corresponds to the 
connection between documents and top-
ic-sentiment word pairs. Each edge ????  is asso-
ciated with a weight ???? ? ?0,1?  denoting the 
contribution of ???  to the document ?? . The 
weight ????  is computed by the contribution of 
word pair ??? in all sentences of ?? as follows: 
???
? ? ?
|??|
? ?? ? ??????, ??? ? ?1 ? ????????, ?????????????  ?1? 
? |??| is the number of sentences in ??; 
? ? is introduced as the trade-off parameter to 
balance the ??????, ??? and ??????, ???; 
? ??????, ??? is computed to judge the relevance 
of ??  in ?? which belongs to ??; 
??????, ??? ? ? ???,?? ? ?? ???              (2) 
where ? ???,??  is the number of ??  appears in ?? , 
and 
?? ????log?
???
?.??????
?                   (3) 
where ? ??? is the number of sentences that the 
word ?? appears in. 
? ??????, ???  is the contribution of ??  in ?? 
which belongs to ??. 
??????, ??? ?
??,????
??,????
?0.5??1.5?
???????
??? ?
      (4) 
where ??? is the average number of sentences in 
??; ? ???,?? is the number of ?? appears in ?? (Al-
lan et al, 2003; Otterbacher et al, 2005). 
It is found that the contribution of a sentiment 
word ??  will not decrease even if it appears in 
all the sentences. Therefore in Equation 4, we 
just use the length of a sentence instead of ?? ??? 
to normalize long sentences which would likely 
contain more sentiment words. 
The authority score ??????????????????  of 
document ?? and a hub score ?????????????????? 
of ???  at the ?? ? 1???  iteration are computed 
based on the hub scores and authority scores in 
the ??? iteration as follows. 
?????????????????? ? ? ???
? ? ????????????????????  (5) 
?????????????????? ? ? ???
? ? ???????????????????  (6) 
We let ? ? ???,??|??|?|??| denote the adjacency 
matrix.  
???????? ???????                 (7) 
????????? ?? ?????                (8) 
where ????? ? ????????????????|??|??  is the vector 
of authority scores for documents at the ??? ite-
ration and ?????? ? ????????????????|??|??  is the 
vector of hub scores for the word pairs at ??? 
iteration. In order to ensure convergence of the 
iterative form, ?? and ??? are normalized in each 
iteration cycle.  
1370
For computation of the final scores, the initial 
scores of all documents are set to 
?
??, and top-
ic-sentiment word pairs are set to 
?
???? . The 
above iterative steps are then used to compute 
the new scores until convergence. Usually the 
convergence of the iteration algorithm is 
achieved when the difference between the scores 
computed at two successive iterations for any 
nodes falls below a given threshold (Wan et al, 
2008; Li et al, 2009; Erkan and Radev, 2004). In 
our model, we use the hub scores to denote the 
associative degree of each word pair and the au-
thority scores as the total scores. The documents 
are then ranked based on the total scores. 
4 Experiment 
We performed the experiments on the Chinese 
benchmark dataset to verify our proposed ap-
proach for opinion retrieval. We first tested the 
effect of the parameter ?  of our model. To 
demonstrate the effectiveness of our opinion re-
trieval model, we compared its performance with 
the same of other approaches. In addition, we 
studied each individual query to investigate the 
influence of query to our model. Furthermore, 
we showed the top-5 highest weight word pairs 
of 5 queries to further demonstrate the effect of 
word pair. 
4.1 Experiment Setup  
4.1.1 Benchmark Datasets 
Our experiments are based on the Chinese 
benchmark dataset, COAE08 (Zhao et al, 2008). 
COAE dataset is the benchmark data set for the 
opinion retrieval track in the Chinese Opinion 
Analysis Evaluation (COAE) workshop, consist-
ing of blogs and reviews. 20 queries are provided 
in COAE08. In our experiment, we created re-
levance judgments through pooling method, 
where documents are ranked at different levels: 
irrelevant, relevant but without opinion, and re-
levant with opinion. Since polarity is not consi-
dered, all relevant documents with opinion are 
classified into the same level. 
4.1.2 Sentiment Lexicon  
In our experiment, the sentiment lexicon is 
composed by the following resources (Xu et al, 
2007):  
(1) The Lexicon of Chinese Positive Words, 
which consists of 5,054 positive words and 
the Lexicon of Chinese Negative Words, 
which consists of 3,493 negative words; 
(2) The opinion word lexicon provided by Na-
tional Taiwan University which consists of 
2,812 positive words and 8,276 negative 
words; 
(3) Sentiment word lexicon and comment word 
lexicon from Hownet. It contains 1836 posi-
tive sentiment words, 3,730 positive com-
ments, 1,254 negative sentiment words and 
3,116 negative comment words. 
The different graphemes corresponding to 
Traditional Chinese and Simplified Chinese are 
both considered so that the sentiment lexicons 
from different sources are applicable to process 
Simplified Chinese text. The lexicon was ma-
nually verified.  
4.1.3 Topic Term Collection 
In order to acquire the collection of topic terms, 
we adopt two expansion methods, dictio-
nary-based method and pseudo relevance feed-
back method.  
The dictionary-based method utilizes Wikipe-
dia (Popescu and Etzioni, 2005) to find an entry 
page for a phrase or a single term in a query. If 
such an entry exists, all titles of the entry page 
are extracted as synonyms of the query concept. 
For example, if we search ???? (Green Tsu-
nami, a firewall) in Wikipedia, it is re-directed to 
an entry page titled ?????? (Youth Escort). 
This term is then added as a synonym of ???? 
(Green Tsunami) in the query. Synonyms are 
treated the same as the original query terms in a 
retrieval process. The content words in the entry 
page are ranked by their frequencies in the page. 
The top-k terms are returned as potential ex-
panded topic terms. 
The second query expansion method is a 
web-based method. It is similar to the pseudo 
relevance feedback expansion but using web 
documents as the document collection. The 
query is submitted to a web search engine, such 
as Google, which returns a ranked list of docu-
ments. In the top-n documents, the top-m topic 
terms which are highly correlated to the query 
terms are returned. 
4.2 Performance Evaluation 
4.2.1 Parameter Tuning 
We first studied how the parameter ? (see Equ-
ation 1) influenced the mean average precision 
(MAP) in our model. The result is given in Fig-
ure 4. 
1371
 
Figure 4: Performance of MAP with varying ?. 
Best MAP performance was achieved in 
COAE08 evaluation, when ? was set between 
0.4 and 0.6. Therefore, in the following experi-
ments, we set ? ? 0.4. 
4.2.2 Opinion Retrieval Model Comparison 
To demonstrate the effectiveness of our proposed 
model, we compared it with the following mod-
els using different evaluation metrics: 
(1) IR: We adopted a classical information re-
trieval model, and further assumed that all re-
trieved documents contained relevant opinions. 
(2) Doc: The 2-stage document-based opinion 
retrieval model was adopted. The model used 
sentiment lexicon-based method for opinion 
identification and a conventional information 
retrieval method for relevance detection.  
(3) ROSC: This was the model which achieved 
the best run in TREC Blog 07. It employed ma-
chine learning method to identify opinions for 
each sentence, and to determine the target topic 
by a NEAR operator. 
(4) ROCC: This model was similar to ROSC, 
but it considered the factor of sentence and re-
garded the count of relevant opinionated sen-
tence to be the opinion score (Zhang and Yu, 
2007). In our experiment, we treated this model 
as the evaluation baseline. 
(5) GORM: our proposed graph-based opinion 
retrieval model. 
Approach COAE08 Evaluation metrics 
Run id MAP R-pre bPref P@10
IR 0.2797 0.3545 0.2474 0.4868
Doc 0.3316 0.3690 0.3030 0.6696
ROSC 0.3762 0.4321 0.4162 0.7089
Baseline 0.3774 0.4411 0.4198 0.6931
GORM 0.3978 0.4835 0.4265 0.7309
Table 1: Comparison of different approaches on 
COAE08 dataset, and the best is highlighted. 
Most of the above models were originally de-
signed for opinion retrieval in English, and 
re-designed them to handle Chinese opinionated 
documents. We incorporated our own Chinese 
sentiment lexicon for this purpose. In our expe-
riments, in addition to MAP, other metrics such 
as R-precision (R-prec), binary Preference (bPref) 
and Precision at 10 documents (P@10) were also 
used. The evaluation results based on these me-
trics are shown in Table 1. 
Table 1 summarized the results obtained. We 
found that GORM achieved the best performance 
in all the evaluation metrics. Our baseline, ROSC 
and GORM which were sentence-based ap-
proaches achieved better performance than the 
document-based approaches by 20% in average. 
Moreover, our GORM approach did not use ma-
chine learning techniques, but it could still 
achieve outstanding performance. 
To study GORM influenced by different que-
ries, the MAP from median average precision on 
individual topic was shown in Figure 5. 
Figure 5: Difference of MAP from Median on 
COAE08 dataset. (MAP of Median is 0.3724) 
As shown in Figure 5, the MAP performance 
was very low on topic 8 and topic 11. Topic 8, i.e. 
???? (Jackie Chan), it was influenced by topic 
7, i.e. ????? (Jet Lee) as there were a number 
of similar relevant targets for the two topics, and 
therefore many word pairs ended up the same. 
As a result, documents belonging to topic 7 and 
topic 8 could not be differentiated, and they both 
performed badly. In order to solve this problem, 
we extracted the topic term with highest relevant 
weight in the sentence to form word pairs so that 
it reduce the impact on the topic terms in com-
mon. 24% and 30% improvement were achieved, 
respectively. 
As to topic 11, i.e. ????? (Lord of King), 
there were only 8 relevant documents without 
any opinion and 14 documents with relevant 
opinions. As a result, the graph constructed by 
insufficient documents worked ineffectively.  
Except for the above queries, GORM per-
formed well in most of the others. To further in-
vestigate the effect of word pair, we summarized 
the top-5 word pairs with highest weight of 5 
queries in Table 2. 
 
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
A
P
?
COAE08
?0.4
?0.3
?0.2
?0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
1 2 3 4 5 6 7 8 9 1011121314151617181920
D
if
fe
re
nc
e
Topic
Difference from Median Average Precision per 
Topic
1372
Table 2: Top-5 highest weight word pairs for 5 queries in COAE08 dataset. 
Table 2 showed that most word pairs could 
represent the relevant opinions about the corres-
ponding queries. This showed that inter-sentence 
information was very helpful to identify the as-
sociative degree of a word pair. Furthermore, 
since word pairs can indicate relevant opinions 
effectively, it is worth further study on how they 
could be applied to other opinion oriented appli-
cations, e.g. opinion summarization, opinion 
prediction, etc. 
5 Related Work 
Our research focuses on relevant opinion rather 
than on relevant document retrieval. We, there-
fore, review related works in opinion identifica-
tion research. Furthermore, we do not support the 
conventional 2-stage opinion retrieval approach. 
We conducted literature review on unified opi-
nion retrieval models and related work in this 
area is presented in the section. 
5.1 Lexicon-based Opinion Identification 
Different from traditional IR, opinion retrieval 
focuses on the opinion nature of documents. 
During the last three years, NTICR and TREC 
evaluations have shown that sentiment lex-
icon-based methods led to good performance in 
opinion identification.  
A lightweight lexicon-based statistical ap-
proach was proposed by Hannah et al (2007). In 
this method, the distribution of terms in relevant 
opinionated documents was compared to their 
distribution in relevant fact-based documents to 
calculate an opinion weight. These weights were 
used to compute opinion scores for each re-
trieved document. A weighted dictionary was 
generated from previous TREC relevance data 
(Amati et al, 2007). This dictionary was submit-
ted as a query to a search engine to get an initial 
query-independent opinion score of all retrieved 
documents. Similarly, a pseudo opinionated 
word composed of all opinion words was first 
created, and then used to estimate the opinion 
score of a document (Na et al, 2009). This me-
thod was shown to be very effective in TREC 
evaluations (Lee et al, 2008). More recently, 
Huang and Croft (2009) proposed an effective 
relevance model, which integrated both 
query-independent and query-dependent senti-
ment words into a mixture model. 
In our approach, we also adopt sentiment lex-
icon-based method for opinion identification. 
Unlike the above methods, we generate a weight 
to a sentiment word for each target (associated 
topic term) rather than assign a unified weight or 
an equal weight to the sentiment word for the 
whole topics. Besides, in our model no training 
data is required. We just utilize the structure of 
our graph to generate a weight to reflect the as-
sociative degree between the two elements of a 
word pair in different context. 
5.2 Unified Opinion Retrieval Model 
In addition to conventional 2-stage approach, 
there has been some research on unified opinion 
retrieval models.  
Eguchi and Lavrenko proposed an opinion re-
trieval model in the framework of generative 
language modeling (Eguchi and Lavrenko, 2006). 
They modeled a collection of natural language 
documents or statements, each of which con-
sisted of some topic-bearing and some senti-
ment-bearing words. The sentiment was either 
represented by a group of predefined seed words, 
or extracted from a training sentiment corpus. 
This model was shown to be effective on the 
MPQA corpus.  
Mei et al tried to build a fine-grained opinion 
retrieval system for consumer products (Mei et 
al., 2007). The opinion score for a product was a 
mixture of several facets. Due to the difficulty in 
Top-5 MAP 
??? 
Chen Kaige 
??? 
Six States 
???? 
Macro-regulation 
??? 
Stephen Chow 
Vista 
Vista 
<??? ??> 
Chen Kaige Support 
<??? ??> 
Chen Kaige Best 
<???? ?> 
Limitless Revile 
<?? ??> 
Movie Excellent 
<?? ???> 
Cast Strong 
<?? ??> 
Room rate Rise 
<?? ??> 
Regulate Strengthen
<?? ??> 
CCP Strengthen 
<?? ??> 
Room rate Steady 
<?? ??> 
Housing Security 
<?? ??> 
Economics Steady 
<?? ??> 
Price Rise 
<?? ??> 
Development Steady
<?? ??> 
Consume Rise 
<?? ??> 
Social Security 
<?? ??> 
Movie Like 
<??? ??> 
Stephen Chow Like 
<?? ??> 
Protagonist Best 
<?? ?> 
Comedy Good 
<?? ??> 
Works Splendid 
<?? ?> 
Price Expensive 
<?? ??> 
Microsoft Like 
 <Vista ??> 
Vista Recommend
<?? ??> 
Problem Vital 
<?? ?> 
Performance No 
1373
associating sentiment with products and facets, 
the system was only tested using small scale text 
collections.  
Zhang and Ye proposed a generative model to 
unify topic relevance and opinion generation 
(Zhang and Ye, 2008). This model led to satis-
factory performance, but an intensive computa-
tion load was inevitable during retrieval, since 
for each possible candidate document, an opinion 
score was summed up from the generative prob-
ability of thousands of sentiment words. 
Huang and Croft proposed a unified opinion 
retrieval model according to the Kullback-Leib- 
ler divergence between the two probability dis-
tributions of opinion relevance model and docu-
ment model (Huang and Croft, 2009). They di-
vided the sentiment words into query-dependent 
and query-independent by utilizing several sen-
timent expansion techniques, and integrated them 
into a mixed model. However, in this model, the 
contribution of a sentiment word was its corres-
ponding incremental mean average precision 
value. This method required that large amount of 
training data and manual labeling. 
Different from the above opinion retrieval ap-
proaches, our proposed graph-based model 
processes opinion retrieval in the granularity of 
sentence. Instead of bag-of-word, the sentence is 
split into word pairs which can maintain the 
contextual information. On the one hand, word 
pair can identify the relevant opinion according 
to intra-sentence contextual information. On the 
other hand, it can measure the degree of a rele-
vant opinion by considering the inter-sentence 
contextual information. 
6 Conclusion and Future Work 
In this work we focus on the problem of opinion 
retrieval. Different from existing approaches, 
which regard document relevance as the key in-
dicator of opinion relevance, we propose to ex-
plore the relevance of individual opinion. To do 
that, opinion retrieval is performed in the granu-
larity of sentence. We define the notion of word 
pair, which can not only maintain the association 
between the opinion and the corresponding target 
in the sentence, but it can also build up the rela-
tionship among sentences through the same word 
pair. Furthermore, we convert the relationships 
between word pairs and sentences into a unified 
graph, and use the HITS algorithm to achieve 
document ranking for opinion retrieval. Finally, 
we compare our approach with existing methods. 
Experimental results show that our proposed 
model performs well on COAE08 dataset.  
The novelty of our work lies in using word 
pairs to represent the information needs for opi-
nion retrieval. On the one hand, word pairs can 
identify the relevant opinion according to in-
tra-sentence contextual information. On the other 
hand, word pairs can measure the degree of a 
relevant opinion by taking inter-sentence con-
textual information into consideration. With the 
help of word pairs, the information needs for 
opinion retrieval can be represented appropriate-
ly. 
In the future, more research is required in the 
following directions: 
(1) Since word pairs can indicate relevant opi-
nions effectively, it is worth further study on 
how they could be applied to other opinion 
oriented applications, e.g. opinion summa-
rization, opinion prediction, etc. 
(2) The characteristics of blogs will be taken 
into consideration, i.e., the post time, which 
could be helpful to create a more time sensi-
tivity graph to filter out fake opinions. 
(3) Opinion holder is another important role of 
an opinion, and the identification of opinion 
holder is a main task in NTCIR. It would be 
interesting to study opinion holders, e.g. its 
seniority, for opinion retrieval. 
Acknowledgements: This work is partially 
supported by the Innovation and Technology 
Fund of Hong Kong SAR (No. ITS/182/08) and 
National 863 program (No. 2009AA01Z150). 
Special thanks to Xu Hongbo for providing the 
Chinese sentiment resources. We also thank Bo 
Chen, Wei Gao, Xu Han and anonymous re-
viewers for their helpful comments. 
References 
James Allan, Courtney Wade, and Alvaro Bolivar. 
2003. Retrieval and novelty detection at the sen-
tence level. In SIGIR ?03: Proceedings of the 26th 
annual international ACM SIGIR conference on 
Research and development in information retrieval, 
pages 314-321. ACM. 
Giambattista Amati, Edgardo Ambrosi, Marco Bianc-
hi, Carlo Gaibisso, and Giorgio Gambosi. 2007. 
FUB, IASI-CNR and University of Tor Vergata at 
TREC 2007 Blog Track. In Proceedings of the 15th 
Text Retrieval Conference. 
Koji Eguchi and Victor Lavrenko. Sentiment retrieval 
using generative models. 2006. In EMNLP ?06, 
Proceedings of 2006 Conference on Empirical Me-
thods in Natural Language Processing, page 
345-354. 
1374
Gunes Erkan and Dragomir R. Radev. 2004. Lexpa-
gerank: Prestige in multi-document text summariza-
tion. In EMNLP ?04, Proceedings of 2004 Confe-
rence on Empirical Methods in Natural Language 
Processing. 
David Hannah, Craig Macdonald, Jie Peng, Ben He, 
and Iadh Ounis. 2007. University of Glasgow at 
TREC 2007: Experiments in Blog and Enterprise 
Tracks with Terrier. In Proceedings of the 15th Text 
Retrieval Conference. 
Xuanjing Huang, William Bruce Croft. 2009. A Uni-
fied Relevance Model for Opinion Retrieval. In 
Proceedings of CIKM. 
Jon M. Kleinberg. 1999. Authoritative sources in a 
hyperlinked environment. J. ACM, 46(5): 604-632. 
Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob 
Nam, Hun-young Jung, Jong-Hyeok Lee. 2008. 
KLE at TREC 2008 Blog Track: Blog Post and Feed 
Retrieval. In Proceedings of the 15th Text Retrieval 
Conference. 
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan 
Zhu. 2009. Answering Opinion Questions with 
Random Walks on Graphs. In ACL ?09, Proceedings 
of the 48th Annual Meeting of the Association for 
Computational Linguistics. 
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. 
Opinion observer: Analyzing and comparing opi-
nion s on the web. In WWW ?05: Proceedings of the 
14th International Conference on World Wide Web. 
Craig Macdonald and Iadh Ounis. 2007. Overview of 
the TREC-2007 Blog Track. In Proceedings of the 
15th Text Retrieval Conference. 
Craig Macdonald and Iadh Ounis. 2006. Overview of 
the TREC-2006 Blog Track. In Proceedings of the 
14th Text Retrieval Conference. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and Chengxiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In 
WWW ?07: Proceedings of the 16 International 
Conference on World Wide Web. 
Seung-Hoon Na, Yeha Lee, Sang-Hyob Nam, and 
Jong-Hyeok Lee. 2009. Improving opinion retrieval 
based on query-specific sentiment lexicon. In 
ECIR ?09: Proceedings of the 31st annual European 
Conference on Information Retrieval, pages 
734-738. 
Douglas Oard, Tamer Elsayed, Jianqiang Wang, Ye-
jun Wu, Pengyi Zhang, Eileen Abels, Jimmy Lin, 
and Dagbert Soergel. 2006. TREC-2006 at Mary-
land: Blog, Enterprise, Legal and QA Tracks. In 
Proceedings of the 15th Text Retrieval Conference. 
Jahna Otterbacher, Gunes Erkan, and Dragomir R. 
Radev. 2005. Using random walks for ques-
tion-focused sentence retrieval. In EMNLP ?05, 
Proceedings of 2005 Conference on Empirical Me-
thods in Natural Language Processing. 
Larry Page, Sergey Brin, Rajeev Motwani, and Terry 
Winograd. 1998. The pagerank citation ranking: 
Bringing order to the web. Technical report, Stan-
ford University. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2): 1-135.  
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinion s from reviews. In 
EMNLP ?05, Proceedings of 2005 Conference on 
Empirical Methods in Natural Language 
Processing. 
Xiaojun Wan and Jianwu Yang. 2008. Mul-
ti-document summarization using cluster-based link 
analysis. In SIGIR ?08: Proceedings of the 31th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, 
pages 299-306. ACM. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing contextual polarity in 
phrase-level sentiment analysis. In EMNLP ?05, 
Proceedings of 2005 Conference on Empirical Me-
thods in Natural Language Processing.  
Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2007. 
Opinmine - Opinion Analysis System by CUHK for 
NTCIR-6 Pilot Task. In Proceedings of NTCIR-6.  
Min Zhang and Xingyao Ye. 2008. A generation 
model to unify topic relevance and lexicon-based 
sentiment for opinion retrieval. In SIGIR ?08: Pro-
ceedings of the 31st Annual International ACM SI-
GIR conference on Research and Development in 
Information Retrieval, pages 411-418. ACM. 
Wei Zhang and Clement Yu. 2007. UIC at TREC 
2007 Blog Track. In Proceedings of the 15th Text 
Retrieval Conference. 
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, 
Kang Liu, and Qi Zhang. 2008. Overview of Chi-
nese Opinion Analysis Evaluation 2008. In Pro-
ceedings of the First Chinese Opinion Analysis 
Evaluation. 
 
1375
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 58?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Study on Uncertainty Identification in Social Media Context
Zhongyu Wei1, Junwen Chen1, Wei Gao2,
Binyang Li1, Lanjun Zhou1, Yulan He3, Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
3School of Engineering & Applied Science, Aston University, Birmingham, UK
{zywei,jwchen,byli,ljzhou,kfwong}@se.cuhk.edu.hk
wgao@qf.org.qa, y.he@cantab.net
Abstract
Uncertainty text detection is important
to many social-media-based applications
since more and more users utilize social
media platforms (e.g., Twitter, Facebook,
etc.) as information source to produce
or derive interpretations based on them.
However, existing uncertainty cues are in-
effective in social media context because
of its specific characteristics. In this pa-
per, we propose a variant of annotation
scheme for uncertainty identification and
construct the first uncertainty corpus based
on tweets. We then conduct experiments
on the generated tweets corpus to study the
effectiveness of different types of features
for uncertainty text identification.
1 Introduction
Social media is not only a social network tool for
people to communicate but also plays an important
role as information source with more and more
users searching and browsing news on it. People
also utilize information from social media for de-
veloping various applications, such as earthquake
warning systems (Sakaki et al, 2010) and fresh
webpage discovery (Dong et al, 2010). How-
ever, due to its casual and word-of-mouth pecu-
liarities, the quality of information in social me-
dia in terms of factuality becomes a premier con-
cern. Chances are there for uncertain information
or even rumors flooding in such a context of free
form. We analyzed a tweet dataset which includes
326,747 posts (Details are given in Section 3) col-
lected during 2011 London Riots, and result re-
veals that at least 18.91% of these tweets bear un-
certainty characteristics1. Therefore, distinguish-
ing uncertain statements from factual ones is cru-
cial for users to synthesize social media informa-
tion to produce or derive reliable interpretations,
1The preliminary study was done based on a manually de-
fined uncertainty cue-phrase list. Tweets containing at least
one hedge cue were treated as uncertain.
and this is expected helpful for applications like
credibility analysis (Castillo et al, 2011) and ru-
mor detection (Qazvinian et al, 2011) based on
social media.
Although uncertainty has been studied theoret-
ically for a long time as a grammatical phenom-
ena (Seifert and Welte, 1987), the computational
treatment of uncertainty is a newly emerging area
of research. Szarvas et al (2012) pointed out that
?Uncertainty - in its most general sense - can be
interpreted as lack of information: the receiver of
the information (i.e., the hearer or the reader) can-
not be certain about some pieces of information?.
In recent years, the identification of uncertainty
in formal text, e.g., biomedical text, reviews or
newswire, has attracted lots of attention (Kilicoglu
and Bergler, 2008; Medlock and Briscoe, 2007;
Szarvas, 2008; Light et al, 2004). However, un-
certainty identification in social media context is
rarely explored.
Previous research shows that uncertainty identi-
fication is domain dependent as the usage of hedge
cues varies widely in different domains (Morante
and Sporleder, 2012). Therefore, the employment
of existing out-of-domain corpus to social media
context is ineffective. Furthermore, compared to
the existing uncertainty corpus, the expression of
uncertainty in social media is fairly different from
that in formal text in a sense that people usu-
ally raise questions or refer to external informa-
tion when making uncertain statements. But, nei-
ther of the uncertainty expressions can be repre-
sented based on the existing types of uncertainty
defined in the literature. Therefore, a different un-
certainty classification scheme is needed in social
media context.
In this paper, we propose a novel uncertainty
classification scheme and construct the first uncer-
tainty corpus based on social media data ? tweets
in specific here. And then we conduct experi-
ments for uncertainty post identification and study
the effectiveness of different categories of features
based on the generated corpus.58
2 Related work
We introduce some popular uncertainty corpora
and methods for uncertainty identification.
2.1 Uncertainty corpus
Several text corpora from various domains have
been annotated over the past few years at different
levels (e.g., expression, event, relation, sentence)
with information related to uncertainty.
Sauri and Pustejovsky (2009) presented a cor-
pus annotated with information about the factu-
ality of events, namely Factbank, which is con-
structed based on TimeBank2 containing 3,123 an-
notated sentences from 208 news documents with
8 different levels of uncertainty defined.
Vincze et al (2008) constructed the BioSocpe
corpus, which consists of medical and biological
texts annotated for negation, uncertainty and their
linguistic scope. This corpus contains 20,924 sen-
tences.
Ganter et al (2009) generated Wikipedia
Weasels Corpus, where Weasel tags in Wikipedia
articles is adopted readily as labels for uncertainty
annotation. It contains 168,923 unique sentences
with 437 weasel tags in total.
Although several uncertainty corpora exist,
there is not a uniform set of standard for uncer-
tainty annotation. Szarvas et al (2012) normal-
ized the annotation of the three corpora aforemen-
tioned. However, the context of these corpora
is different from that of social media. Typically,
these documents annotated are grammatically cor-
rect, carefully punctuated, formally structured and
logically expressed.
2.2 Uncertainty identification
Previous work on uncertainty identification fo-
cused on classifying sentences into uncertain
or definite categories. Existing approaches are
mainly based on supervised methods (Light et
al., 2004; Medlock and Briscoe, 2007; Medlock,
2008; Szarvas, 2008) using the annotated corpus
with different types of features including Part-Of-
Speech (POS) tags, stems, n-grams, etc..
Classification of uncertain sentences was con-
solidated as a task in the 2010 edition of CoNLL
shared task on learning to detect hedge cues
and their scope in natural language text (Farkas
et al, 2010). The best system for Wikipedia
data (Georgescul, 2010) employed Support Vector
Machine (SVM), and the best system for biolog-
ical data (Tang et al, 2010) adopted Conditional
2http://www.timeml.org/site/timebank/
timebank.html
Random Fields (CRF).
In our work, we conduct an empirical study of
uncertainty identification on tweets dataset and ex-
plore the effectiveness of different types of fea-
tures (i.e., content-based, user-based and Twitter-
specific) from social media context.
3 Uncertainty corpus for microblogs
3.1 Types of uncertainty in microblogs
Traditionally, uncertainty can be divided into
two categories, namely Epistemic and Hypothet-
ical (Kiefer, 2005). For Epistemic, there are two
sub-classes Possible and Probable. For Hypotheti-
cal, there are four sub-classes including Investiga-
tion, Condition, Doxastic andDynamic. The detail
of the classification is described as below (Kiefer,
2005):
Epistemic: On the basis of our world knowledge
we cannot decide at the moment whether the
statement is true or false.
Hypothetical: This type of uncertainty includes
four sub-classes:
? Doxastic: Expresses the speaker?s be-
liefs and hypotheses.
? Investigation: Proposition under inves-
tigation.
? Condition: Proposition under condi-
tion.
? Dynamic: Contains deontic, disposi-
tional, circumstantial and buletic modal-
ity.
Compared to the existing uncertainty corpora,
social media authors enjoy free form of writing.
In order to study the difference, we annotated a
small set of 827 randomly sampled tweets accord-
ing to the scheme of uncertainty types above, in
which we found 65 uncertain tweets. And then,
we manually identified all the possible uncertain
tweets, and found 246 really uncertain ones out of
these 827 tweets, which means that 181 uncertain
tweets are missing based on this scheme. We have
the following three salient observations:
? Firstly, there is no tweet found with the type of
Investigation. We find people seldom use words
like ?examine? or ?test? (indicative words of In-
vestigation category) when posting tweets. Once
they do this, the statement should be considered
as highly certain. For example, @dobibid I have
tested the link, it is fake!
? Secondly, people frequently raise questions
about some specific topics for confirmation which
expresses uncertainty. For example, @ITVCentral59
Can you confirm that Birmingham children?s hos-
pital has/hasn?t been attacked by rioters?
? Thirdly, people tend to post message with exter-
nal information (e.g., story from friends) which re-
veals uncertainty. For example, Friend who works
at the children?s hospital in Birmingham says the
riot police are protecting it.
Based on these observations, we propose a vari-
ant of uncertainty types in social media context
by eliminating the category of Investigation and
adding the category of Question and External un-
der Hypothetical, as shown in Table 3.1. Note
that our proposed scheme is based on Kiefer?s
work (2005) which was previously extended to
normalize uncertainty corpora in different genres
by Szarvas et al (2012). But we did not try these
extended schema for specific genres since even the
most general one (Kiefer, 2005) was proved un-
suitable for social media context.
3.2 Annotation result
The dataset we annotated was collected from Twit-
ter using Streaming API during summer riots
in London during August 6-13 2011, including
326,747 tweets in total. Search criteria include
hashtags like #ukriots, #londonriots, #prayforlon-
don, and so on. We further extracted the tweets
relating to seven significant events during the riot
identified by UK newspaper The Guardian from
this set of tweets. We annotated all the 4,743 ex-
tracted tweets for the seven events3.
Two annotators were trained to annotate the
dataset independently. Given a collection of
tweets T = {t1, t2, t3...tn}, the annotation task is
to label each tweet ti as either uncertain or cer-
tain. Uncertainty assertions are to be identified
in terms of the judgements about the author?s in-
tended meaning rather than the presence of uncer-
tain cue-phrase. For those tweets annotated as un-
certain, sub-class labels are also required accord-
ing to the classification indicated in Table 3.1 (i.e.,
multi-label is allowed).
The Kappa coefficient (Carletta, 1996) indi-
cating inter-annotator agreement was 0.9073 for
the certain/uncertain binary classification and was
0.8271 for fine-grained annotation. The conflict
labels from the two annotators were resolved by a
third annotator. Annotation result is displayed in
Table 3.2, where 926 out of 4,743 tweets are la-
beled as uncertain accounting for 19.52%. Ques-
tion is the uncertainty category with most tweets,
followed by External. Only 21 tweets are labeled
3http://www.guardian.co.uk/
uk/interactive/2011/dec/07/
london-riots-twitter
Tweet# 4743
Uncertainty# 926
Epistemic Possible# 16Probable# 129
Hypothetical
Condition# 71
Doxastic# 48
Dynamic# 21
External# 208
Question# 488
Table 2: Statistics of annotation result
as Dynamic and all of them are buletic modal-
ity4 which shares similarity with Doxastic. There-
fore, we consider Dynamic together with Domes-
tic in the error analysis for simplicity. During
the preliminary annotation, we found that uncer-
tainty cue-phrase is a good indicator for uncer-
tainty tweets since tweets labeled as uncertain al-
ways contain at least one cue-phrase. Therefore,
annotators are also required identify cue-phrases
which trigger the sense of uncertainty in the tweet.
All cue-phrases appearing more than twice are col-
lected to form a uncertainty cue-phrase list.
4 Experiment and evaluation
We aim to identify those uncertainty tweets from
tweet collection automatically based on machine
learning approaches. In addition to n-gram fea-
tures, we also explore the effectiveness of three
categories of social media specific features includ-
ing content-based, user-based and Twitter-specific
ones. The description of the three categories of
features is shown in Table 4. Since the length of
tweet is relatively short, we therefore did not carry
out stopwords removal or stemming.
Our preliminary experiments showed that com-
bining unigrams with bigrams and trigrams gave
better performance than using any one or two of
these three features. Therefore, we just report the
result based on the combination of them as n-gram
features. Five-fold cross validation is used for
evaluation. Precision, recall and F-1 score of un-
certainty category are used as the metrics.
4.1 Overall performance
The overall performance of different approaches
is shown in Table 4.1. We used uncertainty cue-
phrase matching approach as baseline, denoted
by CP. For CP, we labeled tweets containing at
least one entry in uncertainty cue-phrase list (de-
scribed in Section 3) as uncertain. All the other
approaches are supervised methods using SVM
based on different feature sets. n-gram stands for
n-gram feature set, C means content-based feature
set, U denotes user-based feature set, T represents
4Proposition expresses plans, intentions or desires.60
Category Subtype Cue Phrase Example
Epistemic Possible, etc. may, etc. It may be raining.Probable likely, etc. It is probably raining.
Hypothetical
Condition if, etc. If it rains, we?ll stay in.
Doxastic believe, etc. He believes that the Earth is flat.
Dynamic hope, etc. fake picture of the london eye on fire... i hope
External someone said, etc. Someone said that London zoo was attacked.
Question seriously?, etc. Birmingham riots are moving to the children hospital?! seriously?
Table 1: Classification of uncertainty in social media context
Category Name Description
Content-based
Length Length of the tweet
Cue Phrase Whether the tweet contains a uncertainty cue
OOV Ratio Ratio of words out of vocabulary
Twitter-specific
URL Whether the tweet contains a URL
URL Count Frequency of URLs in corpus
Retweet Count How many times has this tweet been retweeted
Hashtag Whether the tweet contains a hashtag
Hashtag Count Number of Hashtag in tweets
Reply Is the current tweet a reply tweet
Rtweet Is the current tweet a retweet tweet
User-based
Follower Count Number of follower the user owns
List Count Number of list the users owns
Friend Count Number of friends the user owns
Favorites Count Number of favorites the user owns
Tweet Count Number of tweets the user published
Verified Whether the user is verified
Table 3: Feature list for uncertainty classification
Approach Precision Recall F-1
CP 0.3732 0.9589 0.5373
SVMn?gram 0.7278 0.8259 0.7737
SVMn?gram+C 0.8010 0.8260 0.8133
SVMn?gram+U 0.7708 0.8271 0.7979
SVMn?gram+T 0.7578 0.8266 0.7907
SVMn?gram+ALL 0.8162 0.8269 0.8215
SVMn?gram+Cue Phrase 0.7989 0.8266 0.8125SVMn?gram+Length 0.7372 0.8216 0.7715SVMn?gram+OOV Ratio 0.7414 0.8233 0.7802
Table 4: Result of uncertainty tweets identification
Twitter-specific feature set and ALL is the combi-
nation of C, U and T.
Table 4.1 shows that CP achieves the best recall
but its precision is the lowest. The learning based
methods with different feature sets give some sim-
ilar recalls. Compared to CP, SVMn?gram in-
creases the F-1 score by 43.9% due to the salient
improvement on precision and small drop of re-
call. The performance improves in terms of pre-
cision and F-1 score when the feature set is ex-
panded by adding C, U or T onto n-gram, where
+C brings the highest gain, and SVMn?gram+ALL
performs best in terms of precision and F-1 score.
We then study the effectiveness of the three
content-based features, and result shows that the
presence of uncertain cue-phrase is most indica-
tive for uncertainty tweet identification.
4.2 Error analysis
We analyze the prediction errors based on
SVMn?gram+ALL. The distribution of errors in
terms of different types of uncertainty is shown
Type Poss. Prob. D.&D. Cond. Que. Ext.
Total# 16 129 69 71 488 208
Error# 11 20 18 11 84 40
% 0.69 0.16 0.26 0.15 0.17 0.23
Table 5: Error distributions
in Table 4.2. Our method performs worst on the
type of Possible and on the combination of Dy-
namic and Doxastic because these two types have
the least number of samples in the corpus and the
classifier tends to be undertrained without enough
samples.
5 Conclusion and future work
In this paper, we propose a variant of classification
scheme for uncertainty identification in social me-
dia and construct the first uncertainty corpus based
on tweets. We perform uncertainty identification
experiments on the generated dataset to explore
the effectiveness of different types of features. Re-
sult shows that the three categories of social media
specific features can improve uncertainty identifi-
cation. Furthermore, content-based features bring
the highest improvement among the three and the
presence of uncertain cue-phrase contributes most
for content-based features.
In future, we will explore to use uncertainty
identification for social media applications.
6 Acknowledgement
This work is partially supported by General Re-
search Fund of Hong Kong (No. 417112).61
References
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249?254.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th International Conference
on World Wide Web, pages 675?684.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 331?340. ACM.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The conll-
2010 shared task: learning to detect hedges and their
scope in natural language text. In Proceedings of
the 14th Conference on Computational Natural Lan-
guage Learning?Shared Task, pages 1?12. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009, pages 173?
176. Association for Computational Linguistics.
Maria Georgescul. 2010. A hedgehop over a max-
margin framework using hedge cues. In Proceed-
ings of the 14th Conference on Computational Natu-
ral Language Learning?Shared Task, pages 26?31.
Association for Computational Linguistics.
Ferenc Kiefer. 2005. Lehetoseg es szuk-
segszeruseg[Possibility and necessity]. Tinta Kiado,
Budapest.
H. Kilicoglu and S. Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a
linguistically motivated perspective. BMC bioinfor-
matics, 9(Suppl 11):S10.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings
of BioLink 2004 workshop on linking biological lit-
erature, ontologies and databases: tools for users,
pages 17?24.
B. Medlock and T. Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific litera-
ture. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
992?999.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational Linguistics, 38(2):223?260.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589?1599.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, pages 851?860. ACM.
R. Saur?? and J. Pustejovsky. 2009. Factbank: A cor-
pus annotated with event factuality. Language Re-
sources and Evaluation, 43(3):227?268.
Stephan Seifert and Werner Welte. 1987. A basic bib-
liography on negation in natural language, volume
313. Gunter Narr Verlag.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the 14th Conference on Compu-
tational Natural Language Learning?Shared Task,
pages 13?17. Association for Computational Lin-
guistics.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and
J. Csirik. 2008. The bioscope corpus: biomedical
texts annotated for uncertainty, negation and their
scopes. BMC bioinformatics, 9(Suppl 11):S9.
62
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 97?102,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Web Information Mining and Decision Support Platform for the  
Modern Service Industry 
 
Binyang Li1,2, Lanjun Zhou2,3, Zhongyu Wei2,3, Kam-fai Wong2,3,4,  
Ruifeng Xu5, Yunqing Xia6 
 
1 Dept. of Information Science & Technology, University of International Relations, China 
2Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong 
Kong, Shatin, N.T., Hong Kong  
3MoE Key Laboratory of High Confidence Software Technologies, China 
4 Shenzhen Research Institute, The Chinese University of Hong Kong 
5Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China 
6Department of Computer Science & Technology, TNList, Tsinghua University, China 
{byli,ljzhou,zywei,kfwong}@se.cuhk.edu.hk  
 
 
Abstract 
This demonstration presents an intelligent infor-
mation platform MODEST. MODEST will pro-
vide enterprises with the services of retrieving 
news from websites, extracting commercial in-
formation, exploring customers? opinions, and 
analyzing collaborative/competitive social net-
works. In this way, enterprises can improve the 
competitive abilities and facilitate potential col-
laboration activities. At the meanwhile, MOD-
EST can also help governments to acquire in-
formation about one single company or the entire 
board timely, and make prompt strategies for 
better support. Currently, MODEST is applied to 
the pillar industries of Hong Kong, including 
innovative finance, modem logistics, information 
technology, etc. 
1 Introduction 
With the rapid development of Web 2.0, the 
amount of information is exploding. There are 
millions of events towards companies and bil-
lions of opinions on products generated every 
day (Liu, 2012). Such enormous information 
cannot only facilitate companies to improve their 
competitive abilities, but also help government to 
make prompt decisions for better support or 
timely monitor, e.g. effective risk management. 
For this reason, there is a growing demand of 
Web information mining and intelligent decision 
support services for the industries. Such services 
are collectively referred as modern service, 
which includes the following requirements: 
(1) To efficiently retrieve relevant information 
from the websites; 
(2) To accurately determine the latest business 
news and trends of the company; 
(3) To identify and analyze customers? opinions 
towards the company; 
(4) To explore the collaborative and competitive 
relationship with other companies; 
(5) To leverage the knowledge mined from the 
business news and company social network 
for decision support. 
In this demonstration, we will present a Web 
information mining and decision support plat-
form, MODEST1. The objective of MODEST is 
to provide modern services for both enterprises 
and government, including collecting Web in-
formation, making deep analysis, and providing 
supporting decision. The innovation of MOD-
EST is focusing on deep analysis which incor-
porates the following functions: 
? Topic detection and tracking function is to 
cluster the hot events and capture the rela-
tionship between the relevant events based on 
the collected data from websites (event also 
referred as topic in this paper). In order to re-
alize this function, Web mining techniques 
are adopted, e.g. topic clustering, heuristics 
algorithms, etc. 
? The second function is to identify and analyze 
customers? opinions about the company. 
Opinion mining technology (Zhou et al., 2010) 
is adopted to determine the polarity of those 
news, which can help the company timely and 
appropriately adjust the policy to strengthen 
the dominant position or avoid risks. 
                                                          
1 This work is supported by the Innovation and Technology 
Fund of Hong Kong SAR. 
97
? The third function is to explore and analyze 
social network based on the company centric. 
We utilize social network analysis (SNA) 
technology (Xia et al., 2010) to discover the 
relationships, and we further analyze the con-
tent in fine-grained granularity to identify its 
potential partners or competitors.  
With the help of MODEST, the companies can 
acquire modern service-related information, and 
timely adjust corporate policies and marketing 
plan ahead. Hence, the ability of information ac-
quisition and the competitiveness of the enter-
prises can be improved accordingly. 
In this paper, we will use a practical example 
to illustrate our platform and evaluate the per-
formance of main functions.  
The rest of this paper is organized as follows. 
Section 2 will introduce the system description 
as well as the main functions implementation. 
The practical case study will be illustrated in 
Section 3. The performance of MODEST will be 
evaluated in Section 4.Finally, this paper will be 
concluded in Section 5. 
2 System Description 
In this section, we first outline the system archi-
tecture of MODEST, and then describe the im-
plementation of the main functionality in detail.  
2.1 Architecture and Workflow 
The MODEST system consists of three modules: 
data acquisition, data analysis, and result display. 
The system architecture is shown in Figure 1. 
 
Figure 1: System architecture. (The module in 
blue is data acquisition, the module in orange is 
data analysis, and the module in light green is 
result display) 
(1) The core technique in the data acquisition 
module is the crawler, which is developed to 
collect raw data from websites, e.g. news portals, 
blogosphere. Then the system parse the raw web 
pages and extract information to store in the local 
database for further processing. 
(2) The data analysis module can be divided into 
two parts:  
? NLP pre-processor: utilizes NLP (natural 
language processing) techniques and some 
toolkits to perform the pre-processing on the 
raw data in (1), including word segmenta-
tion, part-of-speech (POS) tagging1, stop-
word removal, and named entity recognition 
(NER)2. We then create knowledgebase for 
individual industry, such as domain-specific 
sentiment word lexicon, name entity collec-
tion, and so on. 
? Miner?makes use of data mining techniques 
to realize four functions, topic detection and 
tracking (TDT), multi-document summari-
zation 3  (MDS), social network analysis 
(SNA), and opinion mining (OM). The re-
sults of data analysis are also stored in the 
database.  
(3) The result display module read out the analy-
sis results from the database and display them to 
users in the form of plain text, charts, figures, as 
well as video. 
2.2 Function Implementation 
Since the innovation of MODEST is focusing on 
the module of data analysis, we will describe its 
main functions in detail, including topic detec-
tion and tracking, opinion mining, and social 
networks analysis. 
2.2.1 Topic Detection and Tracking 
The TDT function targets on detecting and 
tracking the hot topics for each individual com-
pany. Given a period of data collected from web-
sites, there are various discussions about the 
company. In order to extract these topics, clus-
tering methods (Viermetz et al., 2007 and Yoon 
et al., 2009) are implemented to explore the top-
ics. Note that during the period of data collection, 
different topics with respect to the same compa-
ny may have relations. We, therefore, utilize hi-
erarchical clustering methods4to capture the po-
tential relations.  
Due to the large amount of data, it is impossi-
ble to view all the topics at a snapshot. MODEST 
utilizes topic tracking technique (Wang et al., 
2008) to identify related stories with a stream of 
                                                          
1 www.ictclas.org 
2http://ir.hit.edu.cn/demo/ltp 
3http://libots.sourceforge.net/ 
4http://dragon.ischool.drexel.edu/ 
Raw
files
Pre-processed
files
Database
Crawler UI
Word
segmentation
Web
NER
Stopword
removal
POS
tagging
NLP pre-p ocessor
TDT OM
MDS SNS
Miner
Data Layer
98
media. It is convenient for the users to see the 
latest information about the company.  
In summary, TDT function provides the ser-
vices of detecting and tracking the latest and 
emergent topics, analyzing the relationships of 
topics on the dynamics of the company. It meets 
the aforementioned demand, ?to accurately grasp 
the latest business news and trends of the com-
pany?. 
2.2.2 Opinion Mining 
The objective of OM function is to discover 
opinions towards a company and classify the 
opinions into positive, negative, or neutral. 
The opinion mining function is redesigned 
based on our own opinion mining engine (Zhou 
et al., 2010). It separates opinion identification 
and polarity classification into two stages.  
Given a set of documents that are relevant to 
the company, we first split the documents into 
sentences, and then identify whether the sentence 
is opinionated or not. We extract the features 
shown in Table 1 for opinion identification. 
(Zhou et al., 2010) 
Table 1: Features adopted in the opinionated 
sentence classifier 
Punctuation level features 
The presence of direct quote punctuation "?" and "?"  
The presence of other punctuations: "?" and "!" 
Word-Level and entity-level features 
The presence of known opinion operators 
The percentage of known opinion word in sentence 
Presence of a named entity 
Presence of pronoun 
Presence of known opinion indicators 
Presence of known degree adverbs 
Presence of known conjunctions 
Bi-gram features 
Named entities + opinion operators 
Pronouns + opinion operators 
Nouns or named entities + opinion words 
Pronouns + opinion words 
Opinion words (adjective) + opinion words(noun) 
Degree adverbs + opinion words 
Degree adverbs + opinion operators 
These features are then combined using a ra-
dial basis function (RBF) kernel and a support 
vector machine (SVM) classifier (Drucker et al., 
1997) is trained based on the NTCIR 8training 
data for opinion identification (Kando, 2010). 
For those opinionated sentences, we then clas-
sify them into positive, negative, or neutral. In 
addition to the features shown in Table 1, we 
incorporate features of s-VSM (Sentiment Vector 
Space Model) (Xia et al., 2008) to enhance the 
performance. The principles of the s-VSM are 
listed as follows: (1) Only sentiment-related 
words are used to produce sentiment features for 
the s-VSM. (2) The sentiment words are appro-
priately disambiguated with the neighboring ne-
gations and modifiers. (3) Negations and modifi-
ers are included in the s-VSM to reflect the func-
tions of inversing, strengthening and weakening. 
Sentiment unit is the appropriate element com-
plying with the above principles. (Zhou et al., 
2010) 
In addition to polarity classification, opinion 
holder and target are also recognized in OM 
function for further identifying the relationship 
that two companies have, e.g. collaborative or 
competitive. Both of the dependency parser and 
the semantic role labeling1 (SRL) tool are in-
corporated to identify the semantic roles of each 
chunk based on verbs in the sentence. 
The OM function provides the company with 
services of analyzing the social sentimental 
feedback on the dynamics of the company. It 
meets the aforementioned demand, ?to identify 
and analyze customers? opinions towards the 
company?. 
2.2.3 Social Network Analysis  
SNA function aims at producing the commercial 
network of companies that are hidden within the 
articles.  
To achieve this goal, we maintain two lexicons, 
the commercial named entity lexicon and com-
mercial relation lexicon. Commercial named en-
tity are firstly located within the text and then 
recorded in the commercial entity lexicon in the 
pre-processor NER. Commercial relation lexicon 
record the articles/documents that involve the 
commercial relations. Note that the commercial 
relation lexicon (Table 2) is manually compiled. 
In this work, we consider only two general 
commercial relations, namely cooperation and 
competition.  
 
Table 2: Statistics on relation lexicon. 
Type Amount Examples 
Competition 20 ??(challenge), ??
(compete), ? ?
(opponent) 
Collaboration 18 ??(collaborate),??
(coordinate), ? ?
(cooperate) 
SNA function produces the social network of a 
centric company, which can provide the compa-
                                                          
1http://ir.hit.edu.cn/demo/ltp 
99
ny with the impact analysis and decision-making 
chain tracking. It meets the aforementioned de-
mand, ?to explore the collaborative and competi-
tive relationship between companies?. 
3 Practical Example 
In this section, we use a case study to illustrate 
our system and further evaluate the performance 
of the main functions with respect to those com-
panies. Due to the limited space, we just illus-
trate the main functions of topic detection, opin-
ion mining and social network analysis. 
3.1 Topic Detection and Opinion Mining 
Figure 2(a) showed the results of topic detection 
and opinion mining functions for a Hong Kong 
local financial company Sun Hung Kai Proper-
ties (?????). On top of the figure are the 
results of topic detection and tracking function. 
Multi-document summary of the latest news is 
provided for the company and more news with 
the similar topics can be found by pressing the 
button ???? (more). Since there are a lot of 
duplicates of a piece of news on the websites, the 
summary is a direct way to acquire the recent 
news, which can improve the effectiveness of the 
company.  
The results of opinion mining function are 
shown at the bottom of Figure 2(a), where the 
green line indicates negative while the red line 
indicates positive. In order to give a dynamic 
insight of public opinions, we provide the 
amount changes of positive and negative articles 
with time variant. This is very helpful for the 
company to capture the feedback of their mar-
keting policies. As shown in Figure 2(a), there 
were 14 negative articles (????) on Oct. 29, 
2012, which achieved negative peak within the 6 
months. The users would probably read those 14 
articles and adjust the company strategy accord-
ingly.  
3.2 Social Network Analysis 
Figure 2(b) shows the social network based on 
the centric company in yellow, Sun Hung Kai 
Properties (?????). We only list the half 
of the connected companies with collaborative 
relationship from Sun Hung Kai Properties, and 
remove the competitive ones due to limited space. 
The thickness of the line indicates the strength of 
the collaboration between the two companies. 
The social network can explore the potential 
partners/competitors of a company. Furthermore, 
users are allowed to adjust the depth and set the 
nodes count of the network. The above analysis 
can provide a richer insight in to a company.  
In the following section, we will make exper-
iments to investigate the performance of the 
above functions.
 
(a) Topic detection and opinion mining of Sun Hung Kai Properties (?????). (For convenience, 
we translate the texts on the button in English) 
Opinion Mining 
Topic Detection 
100
 (b)Social network of Sun Hung Kai Properties (?????). (The rectangle in yellow is the centric) 
 
Figure 2: Screenshot of the MODEST system. 
4 Experiment and Result 
In our evaluation, the experiments were made 
based on 17692 articles collected from 52 Hong 
Kong websites during 6 months (1/7/2012~ 
31/12/2012). We investigate the performance of 
MODEST based on the standard metrics pro-
posed by NIST1, including precision, recall, and 
F-score. 
Precision (P) is the fraction of detected articles 
(U) that are relevantto the topic (N). 
  
 
 
      
Recall (R) is the fraction of the articles (T) that 
are relevant to the topic that are successfully de-
tected (N). 
  
 
 
      
Usually, there is an inverse relationship be-
tween precision and recall, where it is possible to 
increase one at the cost of reducing the other. 
Therefore, precision and recall scores are not 
discussed in isolation. Instead, F-Score (F) is 
proposed to combine precision and recall, which 
is the harmonic meanof precision and recall. 
  
 
 
 
 
 
 
      
     
   
      
4.1 Topic Detection and Tracking 
We first assess the performance of the topic de-
tection function. The data is divided into 6 parts 
                                                          
1http://trec.nist.gov/ 
according to the time. For different companies, 
the amount of articles vary a lot. Therefore, we 
calculate the metrics for each individual dataset, 
and then compute the weighted mean value. The 
experimental results are shown in Table 3.  
Table 3: Experimental results on topic detection. 
Dataset Recall Precision F-Score 
1/7/12-31/7/12 85.71% 89.52% 85.38% 
1/8/12-31/8/12 93.10% 93.68% 92.49% 
1/9/12-30/9/12 76.50% 83.13% 76.56% 
1/10/12-31/10/12 83.32% 88.53% 85.84% 
1/11/12-30/11/12 86.11% 89.94% 87.98% 
1/12/12-31/12/12 84.26% 87.65% 85.92% 
Average 85.13% 88.78% 85.69% 
From the experimental results, we can find 
that the average F-Score is about 85.69%.The 
dataset in the second row achieves the best per-
formance while the dataset in the third only get 
76.56% in F-Score. It is because that the amount 
of articles is smaller than the others and the re-
call value is very low. As far as we know, the 
best run of topic detection in (Allan et al., 2007) 
achieved 84%. The performance of topic detec-
tion in MODEST is comparable. 
4.2 Opinion Mining 
We then evaluate the performance of opinion 
mining function. We manually annotated 1568 
articles, which is further divided into 8 datasets 
randomly. Precision, recall, and F-score are also 
used as the metrics for the evaluation. The ex-
perimental results are shown in Table 4. 
101
  From Table 4, we can find that the average 
F-Score can reach 74.09%. Note that the opinion 
mining engine of MODEST is the implementa-
tion of (Zhou et al., 2010), which achieved the 
best run in NTCIR. However, the engine is 
trained on NTCIR corpus, which consists of arti-
cles of general domain, while the test set focuses 
on the financial domain. We further train our 
engine on the data from the financial domain and 
the average F-Score improves to over 80%. 
5 Conclusions 
This demonstration presents an intelligent infor-
mation platform designed to mine Web infor-
mation and provide decisions for modern service, 
MODEST. MODEST can provide the services of 
retrieving news from websites, extracting com-
mercial information, exploring customers? opin-
ions about a given company, and analyzing its 
collaborative/competitive social networks. Both 
enterprises and government are the target cus-
tomers. For enterprise, MODEST can improve 
the competitive abilities and facilitate potential 
collaboration. For government, MODEST can 
collect information about the entire industry, and 
make prompt strategies for better support. 
In this paper, we first introduce the system ar-
chitecture design and the main functions imple-
mentation, including topic detection and tracking, 
opinion mining, and social network analysis. 
Then a case study is given to illustrate the func-
tions of MODEST. In order to evaluate the per-
formance of MODEST, we also conduct the ex-
periments based on the data from 52 Hong Kong 
websites, and the results show the effectiveness 
of the above functions. 
In the future, MODEST will be improved in 
two directions: 
? Extend to other languages, e.g. English, 
Simplified Chinese, etc. 
? Enhance the compatibility to implement 
on mobile device.  
The demo of MODEST and the related 
toolkits can be found on the homepage: 
http://sepc111.se.cuhk.edu.hk:8080/adcom_hk/ 
Acknowledgements 
This research is partially supported by General Re-
search Fund of Hong Kong (417112), Shenzhen Fun-
damental Research Program (JCYJ201304011720464 
50, JCYJ20120613152557576), KTO(TBF1ENG007), 
National Natural Science Foundation of China 
(61203378, 61370165), and Shenzhen International 
Cooperation Funding (GJHZ20120613110641217). 
References: 
James Allan, Jaime Carbonell, George Doddington, 
Jonathan Yamron, and Yiming Yang. 1998. Topic 
Detection and Tracking Pilot Study: Final Report. 
Proceedings of the DARPA Broadcast News Tran-
scription and Understanding Workshop. 
Harris Drucker, Chris J.C. Burges, Linda Kaufman, 
Alex Smola, and Vladimir Vpnik. 1997. Support 
Vector Regression Machines. Proceedings of Ad-
vances in Neural Information Processing Systems, 
pp. 155-161. 
Noriko Kando.2010. Overview of the Eighth NTCIR 
Workshop. Proceedings of NTCIR-8 Workshop. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Proceedings of Synthesis Lectures on Hu-
man Language Technologies, pp. 1-167. 
Maximilian Viermetz, and Michal Skubacz. 2007. 
Using Topic Discovery to Segment Large Commu-
nication Graphs for Social Network Analysis. Pro-
ceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence, pp. 95-99. 
Canhui Wang, Min Zhang, Liyun Ru, and Shaoping 
Ma. 2008. Automatic Online News Topic Ranking 
Using Media Focus and User Attention based on 
Aging Theory. Proceedings of the Conference on 
Information and Knowledge Management. 
Yunqing Xia, Nianxing Ji, Weifeng Su, and Yi Liu. 
2010. Mining Commercial Networks from Online 
Financial News. Proceedings of the IEEE Interna-
tional Conference on E-Business Engineering, pp. 
17-23. 
Ruifeng Xu, Kam-fai Wong, and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining-WIA in NTCIR-7 
MOAT Task. In NTCIR-7 Workshop, pp. 307-313. 
Seok-Ho Yoon, Jung-Hwan Shin, Sang-Wook Kim, 
and Sunju Park. 2009. Extraction of a Latent Blog 
Community based on Subject. Proceeding of the 18th 
ACM Conference on Information and Knowledge 
Management, pp. 1529-1532. 
Lanjun Zhou, Yunqing Xia, Binyang Li, and Kam-fai 
Wong. 2010. WIA-Opinmine System in NTCIR-8 
MOAT Evaluation. Proceedings of NTCIR-8 
Workshop Meeting, pp. 286-292. 
Table 4: Experimental results on opinion mining. 
Dataset Size Precision Recall F-Score 
dataset-1 200 76.57% 78.26% 76.57% 
dataset-2 200 83.55% 89.64% 86.07% 
dataset-3 200 69.12% 69.80% 69.44% 
dataset-4 200 77.13% 75.40% 75.67% 
dataset-5 200 76.21% 77.65% 76.74% 
dataset-6 200 63.76% 66.22% 64.49% 
dataset-7 200 78.56% 78.41% 78.43% 
dataset-8 168 65.72% 65.15% 65.32% 
Average 196 73.83% 75.07% 74.09% 
102
