An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Integrating Information Extraction and Automatic Hyperlinking 
 
Stephan Busemann, Witold'UR G \ VNL Hans-Ulrich Krieger,  
Jakub Piskorski, Ulrich Sch?fer, Hans Uszkoreit, Feiyu Xu 
German Research Center for Artificial Intelligence (DFKI GmbH) 
Stuhlsatzenhausweg 3, D-66123 Saarbr?cken, Germany 
sprout@dfki.de 
 
 
Abstract 
This paper presents a novel information sys-
tem integrating advanced information extrac-
tion technology and automatic hyper-linking. 
Extracted entities are mapped into a domain 
ontology that relates concepts to a selection of 
hyperlinks. For information extraction, we use 
SProUT, a generic platform for the develop-
ment and use of multilingual text processing 
components. By combining finite-state and 
unification-based formalisms, the grammar 
formalism used in SProUT offers both pro-
cessing efficiency and a high degree of decal-
rativeness. The ExtraLink demo system show-
cases the extraction of relevant concepts from 
German texts in the tourism domain, offering 
the direct connection to associated web docu-
ments on demand.   
1 Introduction 
The utilization of language technology for the 
creation of hyperlinks has a long history (e.g., 
Allen et al, 1993). Information extraction (IE) is a 
technology that can be applied to identifying both 
sources and targets of new hyperlinks. IE systems 
are becoming commercially viable in supporting 
diverse information discovery and management 
tasks. Similarly, automatic hyperlinking is a matu-
ring technology designed to interrelate pieces of 
information, using ontologies to define the rela-
tionships. With ExtraLink, we present a novel 
information system that integrates both technolo-
gies in order to reach at an improved level of 
informativeness and comfort. Extraction and link 
generation occur completely in the background. 
Entities identified by the IE system are mapped 
into a domain ontology that relates concepts to a 
structured selection of predefined hyperlinks, 
which can be directly visualized on demand using 
a standard web browser. This way, the user can, 
while reading a text, immediately link up textual 
information to the Internet or to any other docu-
ment base without accessing a search engine.  
The quality of the link targets is much higher 
than with standard search engines since, first of all, 
only domain-specific interpretations are sought, 
and second, the ontology provides additional 
structure, including related information. 
ExtraLink uses as its IE system SProUT, a gene-
ric multilingual shallow analysis platform, which 
currently provides linguistic processing resources 
for English, German, Italian, French, Spanish, 
Czech, Polish, Japanese, and Chinese (Becker et 
al., 2002). SProUT is used for tokenization, mor-
phological analysis, and named entity recognition 
in free texts. In Section 2 to 4, we describe innova-
tive features of SProUT. Section 5 gives details 
about the ExtraLink demonstrator. 
2 Integrating Typed Feature Structures 
and Finite State Machines 
The main motivation for developing SProUT 
comes from the need to have a system that (i) 
allows a flexible integration of different processing 
modules and (ii) to find a good trade-off between 
processing efficiency and linguistic expressive-
ness. On the one hand, very efficient finite state 
devices have been successfully applied to real-
world applications. On the other hand, unification-
based grammars (UBGs) are designed to capture 
fine-grained syntactic and semantic constraints, 
resulting in better descriptions of natural language 
phenomena. In contrast to finite state devices, 
unification-based grammars are also assumed to be 
more transparent and more easily modifiable. 
SProUT?s mission is to take the best from these 
two worlds, having a finite state machine that 
operates on typed feature structures (TFSs). I.e., 
transduction rules in SProUT do not rely on simple 
atomic symbols, but instead on TFSs, where the 
left-hand side of a rule is a regular expression over 
TFSs, representing the recognition pattern, and the 
right-hand side is a sequence of TFSs, specifying 
the output structure. Consequently, equality of 
atomic symbols is replaced by unifiability of TFSs 
and the output is constructed using TFS unification 
w.r.t. a type hierarchy. Such rules not only recog-
nize and classify patterns, but also extract frag-
ments embedded in the patterns and fill output 
templates with them. 
Standard finite state techniques such as minimi-
zation and determinization are no longer applicable 
here, due to the fact that edges in our automata are 
annotated by TFSs, instead of atomic symbols. 
However, not every outgoing edge in such an 
automaton must be analyzed, since TFS annota-
tions can be arranged under subsumption, and the 
failure of a general edge automatically causes the 
failure of several, more specialized edges, without 
applying the unifiability test. Such information can 
in fact be precompiled. This and other optimization 
techniques are described in (Krieger and Piskorski, 
2003). 
When compared to symbol-based finite state 
approaches, our method leads to smaller grammars 
and automata, which usually better approximate a 
given language.  
3 XTDL ? The Formalism in SProUT 
XTDL combines two well-known frameworks, 
viz., typed feature structures and regular ex-
pressions. XTDL is defined on top of TDL, a defi-
nition language for TFSs (Krieger and Sch?fer, 
1994) that is used as a descriptive device in several 
grammar systems (LKB, PAGE, PET).  
Apart from the integration into the rule 
definitions, we also employ TDL in SProUT for 
the establishment of a type hierarchy of linguistic 
entities. In the example definition below, the 
morph type inherits from sign and introduces three 
more morphologically motivated attributes with 
the corresponding typed values: 
morph := sign & [ POS  atom, STEM atom, INFL infl ]. 
A rule in XTDL is straightforwardly defined as 
a recognition pattern on the left-hand side, written 
as a regular expression, and an output description 
on the right-hand side. A named label serves as a 
handle to the rule. Regular expressions over TFSs 
describe sequential successions of linguistic signs. 
We provide a couple of standard operators. Con-
catenation is expressed by consecutive items. Dis-
junction, Kleene star, Kleene plus, and optionality 
are represented by the operators |, *, +, and ?, resp. 
{n} after an expression denotes an n-fold repetition. 
{m,n} repeats at least m times and at most n times. 
The XTDL grammar rule below may illustrate 
the syntax. It describes a sequence of morphologi-
cally analyzed tokens (of type morph). The first 
TFS matches one or zero items (?) with part-of-
speech Determiner. Then, zero or more Adjective 
items are matched (*). Finally, one or two Noun 
items ({1,2}) are consumed. The use of a variable 
(e.g., #1) in different places establishes a 
coreference between features. This example enfor-
ces agreement in case, number, and gender for the 
matched items. Eventually, the description on the 
RHS creates a feature structure of type phrase, 
where the category is coreferent with the category 
Noun of the right-most token(s), and the agreement 
features corefer to features of the morph tokens. 
 np :> 
   (morph & [ POS  Determiner, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )?  
   (morph & [ POS  Adjective, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )*  
   (morph & [ POS  Noun & #4, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] ){1,2} 
 -> phrase & [CAT #4, 
 AGR agr & [CASE #1, NUM #2, GEN #3 ]]. 
 
The choice of TDL has a couple of advantages. 
TFSs as such provide a rich descriptive language 
over linguistic structures and allow for a fine-
grained inspection of input items. They represent a 
generalization over pure atomic symbols. Unifia-
bility as a test criterion in a transition is a generali-
zation over symbol equality. Coreferences in 
feature structures express structural identity. Their 
properties are exploited in two ways. They provide 
a stronger expressiveness, since they create 
dynamic value assignments on the automaton 
transitions and thus exceed the strict locality of 
constraints in an atomic symbol approach. Further-
more, coreferences serve as a means of information 
transport into the output description on the RHS of 
the rule. Finally, the choice of feature structures as 
primary citizens of the information domain makes 
composition of modules very simple, since input 
and output are all of the same abstract data type.  
Functional (in contrast to regular) operators are 
a door to the outside world of SProUT.  They 
either serve as predicates, helping to locate 
complex tests that might cancel a rule application, 
or they construct new material, involving pieces of 
information from the LHS of a rule.  The sketch of 
a rule below transfers numerals into their 
corresponding digits using the functional operator 
normalize() that is defined externally. For instance, 
"one" is mapped onto "1", "two" onto "2", etc. 
 
  ?  numeral & [ SURFACE #surf, ... ] .?  -> 
  digit & [ ID #id, ... ],  where #id = normalize(#surf). 
4 The SProUT System  
The core of SProUT comprises of the following 
components: (i) a finite-state machine toolkit for 
building, combining, and optimizing finite-state 
devices; (ii) a flexible XML-based regular com-
piler for converting regular patterns into their cor-
responding compressed finite-state representation 
(Piskorski et al, 2002); (iii) a JTFS package which 
provides standard operations for constructing and 
manipulating TFSs; and (iv) an XTDL grammar 
interpreter. 
Currently, SProUT offers three online compo-
nents: a tokenizer, a gazetteer, and a morphological 
analyzer. The tokenizer maps character sequences 
to tokens and performs fine-grained token classifi-
cation. The gazetteer recognizes named entities 
based on static named entity lexica.  
The morphology unit provides lexical resources 
for English, German (equipped with online shallow 
compound recognition), French, Italian, and 
Spanish, which were compiled from the full form 
lexica of MMorph (Petitpierre and Russell, 1995). 
Considering Slavic languages, a component for 
Czech presented in (Haji?, 2001), and Morfeusz 
(Przepi?rkowski and Wolinski, 2003) for Polish. 
For Asian languages, we integrated Chasen 
(Asahara and Matsumoto, 2000) for Japanese and 
Shanxi (Liu, 2000) for Chinese.  
The XTDL-based grammar engineering plat-
form has been used to define grammars for 
English, German, French, Spanish, Chinese and 
Japanese allowing for named entity recognition 
and extraction. To guarantee a comparable 
coverage, and to ease evaluation, an extension of 
the MUC-7 standard for entities has been adopted.   
 
ne-person := enamex & [ TITLE list-of-strings, 
                          GIVEN_NAME list-of-strings, 
                          SURNAME list-of-strings, 
                                  P-POSITION list-of-strings, 
                          NAME-SUFFIX string, 
                                    DESCRIPTOR string ]. 
 
Given the expressiveness of XTDL expressions, 
MUC-7/MET-2 named entity types can be 
enhanced with more complex internal structures. 
For instance, a person name ne-person is defined 
as a subtype of enamex with the above structure. 
The named entity grammars can handle types 
such as person, location, organization, time point, 
time span (instead of date and time defined by 
MUC), percentage, and currency.  
The core system together with the grammars 
forms a basis for developing applications. SProUT 
is being used by several sites in both research and 
industrial contexts. 
A component for resolving coreferent named 
entities disambiguates and classifies incomplete 
named entities via dynamic lexicon search, e.g., 
Microsoft is coreferent with Microsoft corporation 
and is thus correctly classified as an organization. 
5 ExtraLink: Integrating Information 
Extraction and Automatic Hyperlinking  
A methodology for automatically enriching web 
documents with typed hyperlinks has been develo-
ped and applied to several domains, among them 
the domain of tourism information. A core compo-
nent is a domain ontology describing tourist sites 
in terms of sights, accommodations, restaurants, 
cultural events, etc. The ontology was specialized 
for major European tourism sites and regions (see 
Figure 1). It is associated with a large selection of  
 
 
 
Figure 1: Link Target Page (excerpt). The instance the 
web document is associated to (Isle of Capri) is shown 
on the left, together with neighboring concepts in the 
ontology, which the user can navigate through. 
 
link targets gathered, intellectually selected and 
continuously verified. Although language techno-
logy could also be employed to prime target 
selection, for most applications quality require-
ments demand the expertise of a domain specialist. 
In the case of the tourism domain, the selection 
was performed by a travel business professional. 
The system is equipped with an XML interface and 
accessible as a server. 
The ExtraLink GUI marks the relevant entities 
(usually locations) identified by SProUT (see 
second window on the left in Figure 2). Clicking 
on a marked expression causes a query related to 
the entity being shipped to the server. Coreferent 
concepts are handled as expanded queries. The 
server returns a set of links structured according to 
the ontology, which is presented in the ExtraLink 
GUI (Figure 2). The user can choose to visualize 
any link target in a new browser window that also 
shows the respective subsection of the ontology in 
an indented tree notation (see Figure 1).  
 
 
 
Figure 2: ExtraLink GUI. The links in the right-hand 
window are generated after clicking on the marked 
named entity for Lisbon (marked in dark). The bottom 
left window shows the SProUT result for ?Lissabon?. 
 
The ExtraLink demonstrator has been imple-
mented in Java and C++, and runs under both MS 
Windows and Linux. It is operational for German, 
but it can easily be extended to other languages 
covered by SProUT. This involves the adaptation 
of the mapping into the ontology and a multi-
lingual presentation of the ontology in the link 
target page. 
Acknowledgements 
Work on ExtraLink has been partially funded 
through grants by the German Ministry for 
Education, Science, Research and Technology 
(BMBF) to the project Whiteboard (contract 01 IW 
002), by the EC to the project Airforce (contract 
IST-12179), and by the state of the Saarland to the 
project SATOURN. We are indebted to Tim vor 
der Br?ck, Thierry Declerck, Adrian Raschip, and 
Christian Woldsen for their contributions to 
developing ExtraLink. 
References 
J. Allen, J. Davis, D. Krafft, D. Rus, and D. Subrama-
nian. Information agents for building hyperlinks. J. 
Mayfield and C. Nicholas: Proceedings of the Work-
shop on Intelligent Hypertext, 1993. 
M. Asahara and Y. Matsumoto. Extended models and 
tools for high-performance part-of-speech tagger. 
Proceedings of  COLING, 21-27, 2000. 
0 %HFNHU : 'UR G \ VNL +-U. Krieger, J. 
Piskorski, U. Sch?fer, F. Xu. SProUT?Shallow Pro-
cessing with Typed Feature Structures and Unifica-
tion. In Proceedings of  ICON, 2002. 
J. +DML? Disambiguation of rich inflection?compu-
tational morphology of Czech. Prague Karolinum, 
Charles University Press, 2001. 
H.-U. Krieger and U. Sch?fer. TDL?A Type Description 
Language for Constraint-Based Grammars. Procee-
dings of COLING, 893-899, 1994. 
H.-U. Krieger and J. Piskorski. Speed-up methods for 
complex annotated finite state grammars. DFKI 
Report, 2003. 
K. Liu. Research of automatic Chinese word segmen-
tation. Proceedings of ILT&CIP, 2001. 
D. Petitpierre and G. Russell. MMORPH?the Multext 
morphology program. Multext deliverable report 
2.3.1. ISSCO, University of Geneva, 1995. 
J. PiskRUVNL:'UR G \ VNL );X DQG2 6FKHUIA 
flexible XML-based regular compiler for creation 
and converting linguistic resources. Proceedings of 
LREC 2002, Las Palmas, Spain, 2002. 
A. Przepi?rkowski and M. Wolinski. The Unbearable 
Lightness of Tagging: A Case Study in Morphosyn-
tactic Tagging of Polish. Proceedings of the Work-
shop on Linguistically Interpreted Corpora, 2003. 
 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 584?591,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
 
A Seed-driven Bottom-up Machine Learning Framework  
for Extracting Relations of Various Complexity 
Feiyu Xu, Hans Uszkoreit and Hong Li 
Language Technology Lab, DFKI GmbH 
Stuhlsatzenhausweg 3, D-66123 Saarbruecken 
{feiyu,uszkoreit,hongli}@dfki.de  
 
Abstract 
A minimally supervised machine learning 
framework is described for extracting rela-
tions of various complexity. Bootstrapping 
starts from a small set of n-ary relation in-
stances as ?seeds?, in order to automati-
cally learn pattern rules from parsed data, 
which then can extract new instances of the 
relation and its projections. We propose a 
novel rule representation enabling the 
composition of n-ary relation rules on top 
of the rules for projections of the relation. 
The compositional approach to rule con-
struction is supported by a bottom-up pat-
tern extraction method. In comparison to 
other automatic approaches, our rules can-
not only localize relation arguments but 
also assign their exact target argument 
roles. The method is evaluated in two 
tasks: the extraction of Nobel Prize awards 
and management succession events. Perfor-
mance for the new Nobel Prize task is 
strong. For the management succession 
task the results compare favorably with 
those of existing pattern acquisition ap-
proaches.  
1 Introduction 
Information extraction (IE) has the task to discover 
n-tuples of relevant items (entities) belonging to an 
n-ary relation in natural language documents. One 
of the central goals of the ACE program1 is to de-
velop a more systematically grounded approach to 
IE starting from elementary entities, binary rela-
                                                 
1 http://projects.ldc.upenn.edu/ace/ 
tions to n-ary relations such as events. Current 
semi- or unsupervised approaches to automatic 
pattern acquisition are either limited to a certain 
linguistic representation (e.g., subject-verb-object), 
or only deal with binary relations, or cannot assign 
slot filler roles to the extracted arguments, or do 
not have good selection and filtering methods to 
handle the large number of tree patterns (Riloff, 
1996; Agichtein and Gravano, 2000; Yangarber, 
2003; Sudo et al, 2003; Greenwood and Stevenson, 
2006; Stevenson and Greenwood, 2006). Most of 
these approaches do not consider the linguistic in-
teraction between relations and their projections on 
k dimensional subspaces where 1?k<n, which is 
important for scalability and reusability of rules.  
Stevenson and Greenwood (2006) present a sys-
tematic investigation of the pattern representation 
models and point out that substructures of the lin-
guistic representation and the access to the embed-
ded structures are important for obtaining a good 
coverage of the pattern acquisition. However, all 
considered representation models (subject-verb-
object, chain model, linked chain model and sub-
tree model) are verb-centered. Relations embedded 
in non-verb constructions such as a compound 
noun cannot be discovered: 
(1)  the 2005  Nobel Peace Prize 
 
(1) describes a ternary relation referring to three 
properties of a prize: year, area and prize name. 
We also observe that the automatically acquired 
patterns in Riloff (1996), Yangarber (2003), Sudo 
et al (2003), Greenwood and Stevenson (2006) 
cannot be directly used as relation extraction rules 
because the relation-specific argument role infor-
mation is missing. E.g., in the management succes-
sion domain that concerns the identification of job 
changing events, a person can either move into a 
584
job (called Person_In) or leave a job (called Per-
son_Out). (2) is a simplified example of patterns 
extracted by these systems: 
(2) <subject: person> verb <object:organisation> 
 
In (2), there is no further specification of whether 
the person entity in the subject position is Per-
son_In or Person_Out.   
The ambitious goal of our approach is to provide 
a general framework for the extraction of relations 
and events with various complexity. Within this 
framework, the IE system learns extraction pat-
terns automatically and induces rules of various 
complexity systematically, starting from sample 
relation instances as seeds. The arity of the seed 
determines the complexity of extracted relations. 
The seed helps us to identify the explicit linguistic 
expressions containing mentionings of relation in-
stances or instances of their k-ary projections 
where 1?k<n. Because our seed samples are not 
linguistic patterns, the learning system is not re-
stricted to a particular linguistic representation and 
is therefore suitable for various linguistic analysis 
methods and representation formats. The pattern 
discovery is bottom-up and compositional, i.e., 
complex patterns can build on top of simple pat-
terns for projections.  
We propose a rule representation that supports 
this strategy. Therefore, our learning approach is 
seed-driven and bottom-up. Here we use depend-
ency trees as input for pattern extraction. We con-
sider only trees or their subtrees containing seed 
arguments. Therefore, our method is much more 
efficient than the subtree model of Sudo et al, 
(2003), where all subtrees containing verbs are 
taken into account. Our pattern rule ranking and 
filtering method considers two aspects of a pattern: 
its domain relevance and the trustworthiness of its 
origin. We tested our framework in two domains: 
Nobel Prize awards and management succession. 
Evaluations have been conducted to investigate the 
performance with respect to the seed parameters: 
the number of seeds and the influence of data size 
and its redundancy property.  The whole system 
has been evaluated for the two domains consider-
ing precision and recall. We utilize the evaluation 
strategy ?Ideal Matrix? of Agichtein and Gravano 
(2000) to deal with unannotated test data.   
The remainder of the paper is organised as fol-
lows: Section 2 provides an overview of the system 
architecture. Section 3 discusses the rule represen-
tation. In Section 4, a detailed description of the 
seed-driven bottom-up pattern acquisition is pre-
sented. Section 5 describes our experiments with 
pattern ranking, filtering and rule induction. Sec-
tion 6 presents the experiments and evaluations for 
the two application domains. Section 7 provides a 
conclusion and an outline of future work.   
2 System Architecture 
Given the framework, our system architecture 
can be depicted as follows: 
 
Figure 1. Architecture 
 
This architecture has been inspired by several 
existing seed-oriented minimally supervised ma-
chine learning systems, in particular by Snowball 
(Agichtein and Gravano, 2000) and ExDisco 
(Yangarber et al, 2000). We call our system 
DARE, standing for ?Domain Adaptive Relation 
Extraction based on Seeds?. DARE contains four 
major components: linguistic annotation, classifier, 
rule learning and relation extraction. The first com-
ponent only applies once, while the last three com-
ponents are integrated in a bootstrapping loop.  At 
each iteration, rules will be learned based on the 
seed and then new relation instances will be ex-
tracted by applying the learned rules. The new re-
lation instances are then used as seeds for the next 
iteration of the learning cycle.  The cycle termi-
nates when no new relations can be acquired. 
The linguistic annotation is responsible for en-
riching the natural language texts with linguistic 
information such as named entities and depend-
ency structures.  In our framework, the depth of the 
linguistic annotation can be varied depending on 
the domain and the available resources. 
The classifier has the task to deliver relevant 
paragraphs and sentences that contain seed ele-
ments. It has three subcomponents: document re-
585
trieval, paragraph retrieval and sentence retrieval. 
The document retrieval component utilizes the 
open source IR-system Lucene2. A translation step 
is built in to convert the seed into the proper IR 
query format. As explained in Xu et al (2006), we 
generate all possible lexical variants of the seed 
arguments to boost the retrieval coverage and for-
mulate a boolean query where the arguments are 
connected via conjunction and the lexical variants 
are associated via disjunction. However, the trans-
lation could be modified. The task of paragraph 
retrieval is to find text snippets from the relevant 
documents where the seed relation arguments co-
occur. Given the paragraphs, a sentence containing 
at least two arguments of a seed relation will be 
regarded as relevant. 
As mentioned above, the rule learning compo-
nent constitutes the core of our system. It identifies 
patterns from the annotated documents inducing 
extraction rules from the patterns, and validates 
them.  In section 4, we will give a detailed expla-
nation of this component.  
The relation extraction component applies the 
newly learned rules to the relevant documents and 
extracts relation instances. The validated relation 
instances will then be used as new seeds for the 
next iteration.  
3 DARE Rule Representation  
Our rule representation is designed to specify the 
location and the role of the arguments w.r.t. the 
target relation in a linguistic construction. In our 
framework, the rules should not be restricted to a 
particular linguistic representation and should be 
adaptable to various NLP tools on demand.  A 
DARE rule is allowed to call further DARE rules 
that extract a subset of the arguments. Let us step 
through some example rules for the prize award 
domain. One of the target relations in the domain is 
about a person who obtains a special prize in a cer-
tain area in a certain year, namely, a quaternary 
tuple, see (3). (4) is a domain relevant sentence.  
(3) <recipient, prize, area, year> 
(4) Mohamed ElBaradei won the 2005 Nobel 
Peace Prize on Friday for his efforts to limit the 
spread of atomic weapons. 
(5) is a rule that extracts a ternary projection in-
stance <prize, area, year>  from a  noun phrase 
                                                 
2 http://www.lucene.de 
compound, while (6) is a rule which triggers (5) in 
its object argument and extracts all four arguments. 
(5) and (6) are useful rules for  extracting argu-
ments from (4). 
(5)  
 
 (6) 
 
 
Next we provide a definition of a DARE rule: 
A DARE rule has three components  
1. rule name: ri; 
2. output: a set A containing the n arguments 
of the n-ary relation, labelled with their ar-
gument roles; 
3. rule body in AVM format containing: 
- specific linguistic labels or attributes 
(e.g., subject, object, head, mod), de-
rived from the linguistic analysis, e.g., 
dependency structures and the named en-
tity information 
- rule: its value is a DARE rule which ex-
tracts a subset of arguments of A  
The rule in (6) is a typical DARE rule. Its sub-
ject and object descriptions call appropriate DARE 
rules that extract a subset of the output relation 
arguments.  The advantages of this rule representa-
tion strategy are that (1) it supports the bottom-up 
rule composition; (2) it is expressive enough for 
the representation of rules of various complexity; 
(3) it reflects the precise linguistic relationship 
among the relation arguments and reduces the 
template merging task in the later phase; (4) the 
rules for the subset of arguments may be reused for 
other relation extraction tasks.  
The rule representation models for automatic or 
unsupervised pattern rule extraction discussed by 
586
Stevenson and Greenwood (2006) do not account 
for these considerations.  
4 Seed-driven Bottom-up Rule Learning  
Two main approaches to seed construction have 
been discussed in the literature: pattern-oriented 
(e.g., ExDisco) and semantics-oriented (e.g., 
Snowball) strategies. The pattern-oriented method 
suffers from poor coverage because it makes the IE 
task too dependent on one linguistic representation 
construction (e.g., subject-verb-object) and has 
moreover ignored the fact that semantic relations 
and events could be dispersed over different sub-
structures of the linguistic representation. In prac-
tice, several tuples extracted by different patterns 
can contribute to one complex relation instance.   
The semantics-oriented method uses relation in-
stances as seeds. It can easily be adapted to all re-
lation/event instances. The complexity of the target 
relation is not restricted by the expressiveness of 
the seed pattern representation. In Brin (1998) and 
Agichtein and Gravano (2000),  the semantics-
oriented methods have proved to be effective in 
learning patterns for some general binary relations 
such as booktitle-author and company-headquarter 
relations. In Xu et al (2006), the authors show that 
at least for the investigated task it is more effective 
to start with the most complex relation instance, 
namely, with an n-ary sample for the target n-ary 
relation as seed, because the seed arguments are 
often centred in a relevant textual snippet where 
the relation is mentioned.  Given the bottom-up 
extracted patterns, the task of the rule induction is 
to cluster and generalize the patterns. In compari-
son to the bottom-up rule induction strategy (Califf 
and Mooney, 2004), our method works also in a 
compositional way. For reasons of space this part 
of the work will be reported in Xu and Uszkoreit 
(forthcoming).  
4.1 Pattern Extraction 
Pattern extraction in DARE aims to find linguistic 
patterns which do not only trigger the relations but 
also locate the relation arguments. In DARE, the 
patterns can be extracted from a phrase, a clause or 
a sentence, depending on the location and the dis-
tribution of the seed relation arguments.   
 
Figure 2. Pattern extraction step 1 
 
Figure 3. Pattern extraction step 2 
 
Figures 2 and 3 depict the general steps of bot-
tom-up pattern extraction from a dependency tree t 
where three seed arguments arg1, arg2 and arg3 are 
located. All arguments are assigned their relation 
roles r1, r2 and r3. The pattern-relevant subtrees are 
trees in which seed arguments are embedded: t1, t2 
and t3. Their root nodes are n1, n2 and n3.  Figure 2 
shows the extraction of a unary pattern n2_r3_i, 
while Figure 3 illustrates the further extraction and 
construction of a binary pattern n1_r1_r2_j and a 
ternary pattern n3_r1_r2_r3_k. In practice, not all 
branches in the subtrees will be kept. In the follow-
ing, we give a general definition of our seed-driven 
bottom-up pattern extraction algorithm: 
input:  (i) relation = <r1, r2, ..., rn>: the target rela-
tion tuple with n argument roles. 
 T: a set of linguistic analysis trees anno-
tated with i seed relation arguments (1?i?n) 
output: P: a set of pattern instances which can ex-
tract i or a subset of i arguments.  
Pattern extraction: 
 for each tree t ?T 
587
Step 1: (depicted in Figure 2) 
1. replace all terminal nodes that are instanti-
ated with the seed arguments by new 
nodes. Label these new nodes with the 
seed argument roles and possibly the cor-
responding entity classes; 
2. identify the set of the lowest nonterminal 
nodes N1 in t that dominate only one ar-
gument (possibly among other nodes). 
3. substitute N1 by nodes labelled with the 
seed argument roles and their entity classes 
4. prune the subtrees dominated by N1 from t 
and add these subtrees into P. These sub-
trees are assigned the argument role infor-
mation and a unique id. 
Step2: For i=2 to n: (depicted in Figure 3) 
1. find the set of the lowest nodes N1 in t that 
dominate in addition to other children only 
i seed arguments; 
2. substitute N1 by nodes labelled with the i 
seed argument role combination informa-
tion (e.g., ri_rj) and with a unique id. 
3. prune the subtrees Ti dominated by Ni in t; 
4. add Ti to P together with the argument role 
combination information and the unique id  
With this approach, we can learn rules like (6) in 
a straightforward way. 
4.2 Rule Validation: Ranking and Filtering 
Our ranking strategy has incorporated the ideas 
proposed by Riloff (1996), Agichtein and Gravano 
(2000), Yangarber (2003) and Sudo et al (2003). 
We take two properties of a pattern into account: ? domain relevance: its distribution in the rele-
vant documents and irrelevant documents 
(documents in other domains); 
? trustworthiness of its origin: the relevance 
score of the seeds from which it is extracted.   
In Riloff (1996) and Sudo et al (2003), the rele-
vance of a pattern is mainly dependent on its oc-
currences in the relevant documents vs. the whole 
corpus.  Relevant patterns with lower frequencies 
cannot float to the top. It is known that some com-
plex patterns are relevant even if they have low 
occurrence rates. We propose a new method for 
calculating the domain relevance of a pattern. We 
assume that the domain relevance of a pattern is 
dependent on the relevance of the lexical terms 
(words or collocations) constructing the pattern, 
e.g., the domain relevance of (5) and (6) are de-
pendent on the terms ?prize? and ?win? respec-
tively. Given n different domains, the domain rele-
vance score (DR) of a term t in a domain di is: 
DR(t, di)= 
0, if df(t, di) =0; 
df(t,di)
N?D ?LOG(n?
df(t,di)
df(t,dj)
j=1
n?
), otherwise 
where 
? df(t, di): is the document frequency of a 
term t in the domain di  
? D: the number of the documents in di 
? N: the total number of the terms in di 
Here the domain relevance of a term is dependent 
both on its document frequency and its document 
frequency distribution in other domains. Terms 
mentioned by more documents within the domain 
than outside are more relevant (Xu et al, 2002).   
In the case of n=3 such different domains might 
be, e.g., management succession, book review or 
biomedical texts. Every domain corpus should ide-
ally have the same number of documents and simi-
lar average document size. In the calculation of the 
trustworthiness of the origin, we follow Agichtein 
and Gravano (2000) and Yangarber (2003). Thus, 
the relevance of a pattern is dependent on the rele-
vance of its terms and the score value of the most 
trustworthy seed from which it origins. Finally, the 
score of a pattern p is calculated as follows: 
score(p)= }:)(max{)(
0
SeedsssscoretDR
T
i
i ???
=
 
where    |T|> 0 and ti ? T 
? T: is the set of the terms occur in p; 
? Seeds: a set of seeds from which the pat-
tern is extracted; 
? score(s): is the score of the seed s; 
This relevance score is not dependent on the distri-
bution frequency of a pattern in the domain corpus. 
Therefore, patterns with lower frequency, in par-
ticular, some complex patterns, can be ranked 
higher when they contain relevant domain terms or 
come from reliable seeds. 
588
5 Top down Rule Application 
After the acquisition of pattern rules, the DARE 
system applies these rules to the linguistically an-
notated corpus. The rule selection strategy moves 
from complex to simple. It first matches the most 
complex pattern to the analyzed sentence in order 
to extract the maximal number of relation argu-
ments. According to the duality principle (Yangar-
ber 2001), the score of the new extracted relation 
instance S is dependent on the patterns from which 
it origins. Our score method is a simplified version 
of that defined by Agichtein and Gravano (2000): 
score(S)=1? (1? score(Pi )
i=0
P
? )  
where P={Pi} is the set of patterns that extract S. 
 
The extracted instances can be used as potential 
seeds for the further pattern extraction iteration, 
when their scores are validated.  The initial seeds 
obtain 1 as their score. 
6 Experiments and Evaluation 
 We apply our framework to two application do-
mains: Nobel Prize awards and management suc-
cession events.  Table 1 gives an overview of our 
test data sets. 
Data Set Name Doc Number Data Amount 
Nobel Prize A  (1999-2005) 2296 12,6 MB 
Nobel Prize B (1981-1998)  1032 5,8 MB 
MUC-6 199 1 MB 
Table1. Overview of Test Data Sets.  
For the Nobel Prize award scenario, we use two 
test data sets with different sizes: Nobel Prize A 
and Nobel Prize B. They are Nobel Prize related 
articles from New York Times, online BBC and 
CNN news reports.   The target relation for the ex-
periment is a quaternary relation as mentioned in 
(3), repeated here again: 
<recipient, prize, area, year> 
 Our test data is not annotated with target rela-
tion instances. However, the entire list of Nobel 
Prize award events is available for the evaluation 
from the Nobel Prize official website3. We use it as 
our reference relation database for building our 
Ideal table (Agichtein and Gravano, 2000).      
For the management succession scenario, we use 
the test data from MUC-6 (MUC-6, 1995) and de-
                                                 
3 http://nobelprize.org/ 
fine a simpler relation structure than the MUC-6 
scenario template with four arguments:  
<Person_In, Person_Out, Position, Organisation> 
In the following tables, we use PI for Person_In, 
PO for Person_Out, POS for Position and ORG for 
Organisation. In our experiments, we attempt to 
investigate the influence of the size of the seed and 
the size of the test data on the performance. All 
these documents are processed by named entity 
recognition (Drozdzynski et al, 2004) and depend-
ency parser MINIPAR (Lin, 1998).      
6.1 Nobel Prize Domain Evaluation 
For this domain, three test runs have been evalu-
ated, initialized by one randomly selected relation 
instance as seed each time.  In the first run, we use 
the largest test data set Nobel Prize A. In the sec-
ond and third runs, we have compared two random 
selected seed samples with 50% of the data each, 
namely Nobel Prize B. For data sets in this do-
main, we are faced with an evaluation challenge 
pointed out by DIPRE (Brin, 1998) and Snowball 
(Agichtein and Gravano, 2000), because there is no 
gold-standard evaluation corpus available. We 
have adapted the evaluation method suggested by 
Agichtein and Gravano, i.e., our system is success-
ful if we capture one mentioning of a Nobel Prize 
winner event through one instance of the relation 
tuple or its projections. We constructed two tables 
(named Ideal) reflecting an approximation of the 
maximal detectable relation instances: one for No-
bel Prize A and another for Nobel Prize B. The 
Ideal tables contain the Nobel Prize winners that 
co-occur with the word ?Nobel? in the test corpus. 
Then precision is the correctness of the extracted 
relation instances, while recall is the coverage of 
the extracted tuples that match with the Ideal table. 
In Table 2 we show the precision and recall of the 
three runs and their random seed sample: 
Recall Data 
Set 
Seed Preci-
sion total time interval 
Nobel 
Prize A
[Zewail, Ahmed H], 
nobel, chemistry,1999 
71,6% 50,7% 70,9% 
(1999-2005) 
Nobel 
Prize B
[Sen, Amartya], no-
bel, economics, 1998 
87,3% 31% 43% 
(1981-1998) 
Nobel 
Prize B
[Arias, Oscar],  
nobel, peace, 1987 
83,8% 32% 45% 
(1981-1998) 
Table 2. Precision, Recall against the Ideal Table  
The first experiment with the full test data has 
achieved much higher recall than the two experi-
ments with the set Nobel Prize B. The two experi-
ments with the Nobel Prize B corpus show similar 
589
performance. All three experiments have better 
recalls when taking only the relation instances dur-
ing the report years into account, because there are 
more mentionings during these years in the corpus.  
Figure (6) depicts the pattern learning and new 
seed extracting behavior during the iterations for 
the first experiment. Similar behaviours are ob-
served in the other two experiments.   
 
Figure 6. Experiment with Nobel Prize A  
6.2 Management Succession Domain 
The MUC-6 corpus is much smaller than the Nobel 
Prize corpus. Since the gold standard of the target 
relations is available, we use the standard IE preci-
sion and recall method. The total gold standard 
table contains 256 event instances, from which we 
randomly select seeds for our experiments. Table 3 
gives an overview of performance of the experi-
ments. Our tests vary between one seed, 20 seeds 
and 55 seeds. 
Initial Seed Nr.  Precision Recall 
A 12.6% 7.0% 1  
B 15.1% 21.8% 
20  48.4%  34.2% 
55  62.0% 48.0% 
Table 3. Results for various initial seed sets  
The first two one-seed tests achieved poor per-
formance. With 55 seeds, we can extract additional  
67 instances to obtain the half size of the instances 
occurring in the corpus. Table 4 show evaluations 
of the single arguments. B works a little better be-
cause the randomly selected single seed appears a 
better sample for finding the pattern for extracting 
PI argument.  
Arg precision 
(A) 
precision 
(B) 
Recall 
(A) 
Recall 
(B) 
PI 10.9% 15.1% 8.6% 34.4% 
PO 28.6% - 2.3% 2.3% 
ORG 25.6% 100% 2.6% 2.6% 
POS 11.2% 11.2% 5.5% 5.5% 
Table 4. Evaluation of one-seed tests (A and B) 
Table 5 shows the performance with 20 and 55 
seeds respectively. Both of them are better than the 
one-seed tests, while 55 seeds deliver the best per-
formance in average, in particular, the recall value. 
  
arg precision 
(20) 
precision 
(55) 
recall 
(20) 
recall 
(55) 
PI 84% 62.8% 27.9% 56.1% 
PO 41.2% 59% 34.2% 31.2% 
ORG 82.4% 58.2% 7.4% 20.2% 
POS 42% 64.8% 25.6% 30.6% 
Table 5. Evaluation of 20 and 55 seeds tests 
Our result with 20 seeds (precision of 48.4% and 
recall of 34.2%) is comparable with the best result 
reported by Greenwood and Stevenson (2006) with 
the linked chain model (precision of 0.434 and re-
call of 0.265). Since the latter model uses patterns 
as seeds, applying a similarity measure for pattern 
ranking, a fair comparison is not possible. Our re-
sult is not restricted to binary relations and our 
model also assigns the exact argument role to the 
Person role, i.e. Person_In or Person_Out.   
We have also evaluated the top 100 event-
independent binary relations such as Person-
Organisation and Position-Organisation. The preci-
sion of these by-product relations of our IE system 
is above 98%.  
7 Conclusion and Future Work 
Several parameters are relevant for the success 
of a seed-based bootstrapping approach to relation 
extraction. One of these is the arity of the relation.  
Another one is the locality of the relation instance 
in an average mentioning. A third one is the types 
of the relation arguments:  Are they  named entities 
in the classical sense? Are they lexically marked? 
Are there several arguments of the same type? 
Both tasks we explored involved extracting quater-
nary relations. The Nobel Prize domain shows bet-
ter lexical marking because of the prize name.  The 
management succession domain has two slots of 
the same NE type, i.e., persons. These differences 
are relevant for any relation extraction approach.   
The success of the bootstrapping approach cru-
cially depends on the nature of the training data 
base.  One of the most relevant properties of this 
data base is the ratio of documents to relation in-
stances. Several independent reports of an instance 
usually yield a higher number of patterns.   
The two tasks we used to investigate our method 
drastically differ in this respect.  The Nobel Prize 
590
domain we selected as a learning domain for gen-
eral award events since it exhibits a high degree of 
redundancy in reporting.  A Nobel Prize triggers 
more news reports than most other prizes.  The 
achieved results met our expectations.  With one 
randomly selected seed, we could finally extract 
most relevant events in some covered time interval. 
However, it turns out that it is not just the aver-
age number of reports per events that matters but 
also the distribution of reportings to events.  Since 
the Nobel prizes data exhibit a certain type of 
skewed distribution, the graph exhibits properties 
of scale-free graphs.  The distances between events 
are shortened to a few steps. Therefore, we can 
reach most events in a few iterations. The situation 
is different for the management succession task 
where the reports came from a single newspaper.  
The ratio of events to reports is close to one.  This 
lack of informational redundancy requires a higher 
number of seeds.  When we started the bootstrap-
ping with a single event, the results were rather 
poor.  Going up to twenty seeds, we still did not 
get the performance we obtain in the Nobel Prize 
task but our results compare favorably to the per-
formance of existing bootstrapping methods.  
The conclusion, we draw from the observed dif-
ference between the two tasks is simple:  We shall 
always try to find a highly redundant training data 
set.  If at all possible, the training data should ex-
hibit a skewed distribution of reports to events.  
Actually, such training data may be the only realis-
tic chance for reaching a large number of rare pat-
terns.  In future work we will try to exploit the web 
as training resource for acquiring patterns while 
using the parsed domain data as the source for ob-
taining new seeds in bootstrapping the rules before 
applying these to any other nonredundant docu-
ment base.  This is possible because our seed tu-
ples can be translated into simple IR queries and 
further linguistic processing is limited to the re-
trieved candidate documents.   
Acknowledgement 
The presented research was partially supported by a 
grant from the German Federal Ministry of Education 
and Research to the project Hylap (FKZ: 01IWF02) and 
EU?funding for the project RASCALLI. Our special 
thanks go to Doug Appelt and an anonymous reviewer 
for their thorough and highly valuable comments. 
 
References 
E. Agichtein and L. Gravano. 2000. Snowball: extract-
ing relations from large plain-text collections. In 
ACM 2000, pages 85?94, Texas, USA. 
S. Brin. Extracting patterns and relations from the 
World-Wide Web. In Proc. 1998 Int'l Workshop on 
the Web and Databases (WebDB '98), March 1998. 
M. E. Califf and R. J. Mooney. 2004. Bottom-Up Rela-
tional Learning of Pattern Matching Rules for Infor-
mation Extraction. Journal of Machine Learning Re-
search, MIT Press. 
W. Drozdzynski,  H.-U.Krieger, J.  Piskorski; U. Sch?-
fer, and F. Xu. 2004. Shallow Processing with Unifi-
cation and Typed Feature Structures ? Foundations 
and Applications. K?nstliche Intelligenz 1:17?23.  
M. A. Greenwood and M. Stevenson. 2006. Improving 
Semi-supervised Acquisition of Relation Extraction 
Patterns. In Proc. of the Workshop on Information 
Extraction Beyond  the Document, Australia.  
D. Lin. 1998. Dependency-based evaluation of  MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain. 
MUC. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6), Morgan Kaufmann. 
E. Riloff. 1996. Automatically Generating Extraction 
Patterns from Untagged Text. In Proc. of the Thir-
teenth National Conference on Articial Intelligence, 
pages 1044?1049, Portland, OR, August. 
M. Stevenson and Mark A. Greenwood. 2006. Compar-
ing Information Extraction Pattern Models. In Proc. 
of the Workshop on Information Extraction Beyond  
the Document, Sydney, Australia.  
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for 
Automatic IE Pattern Acquisition. In Proc. of ACL-
03, pages 224?231, Sapporo, Japan. 
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain 
Knowledge for Information Extraction. In Proc. of 
COLING 2000, Saarbr?cken, Germany. 
R. Yangarber. 2003. Counter-training in the Discovery 
of Semantic Patterns. In Proceedings of ACL-03, 
pages 343?350, Sapporo, Japan. 
F. Xu, D. Kurz, J. Piskorski and S. Schmeier. 2002. A 
Domain Adaptive Approach to Automatic Acquisition 
of Domain Relevant Terms and their Relations with 
Bootstrapping.  In Proc. of LREC 2002, May 2002.  
F. Xu, H. Uszkoreit and H. Li. 2006. Automatic Event 
and Relation Detection with Seeds of Varying Com-
plexity. In Proceedings of AAAI 2006 Workshop 
Event Extraction and Synthesis, Boston, July, 2006.  
591
Coling 2010: Poster Volume, pages 570?578,
Beijing, August 2010
Using Syntactic and Semantic based Relations for Dialogue Act
Recognition
Tina Klu?wer, Hans Uszkoreit, Feiyu Xu
Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)
Projektbu?ro Berlin
{tina.kluewer,uszkoreit,feiyu}@dfki.de
Abstract
This paper presents a novel approach to
dialogue act recognition employing multi-
level information features. In addition to
features such as context information and
words in the utterances, the recognition
task utilizes syntactic and semantic rela-
tions acquired by information extraction
methods. These features are utilized by
a Bayesian network classifier for our dia-
logue act recognition. The evaluation re-
sults show a clear improvement from the
accuracy of the baseline (only with word
features) with 61.9% to an accuracy of
67.4% achieved by the extended feature
set.
1 Introduction
Dialogue act recognition is an essential task for
dialogue systems. Automatic dialogue act clas-
sification has received much attention in the past
years either as an independent task or as an em-
bedded component in dialogue systems. Various
methods have been tested on different corpora us-
ing several dialogue act classes and information
coming from the user input.
The work presented in this paper is part of a
dialogue system called KomParse (Klu?wer et al,
2010), which is an application of a NL dialogue
system combined with various question answering
technologies in a three-dimensional virtual world
named Twinity, a web-based online product of the
Berlin startup company Metaversum1. The Kom-
Parse NPCs provide various services through con-
1http://www.metaversum.com/
versation with game users such as selling pieces
of furniture to users via text based conversation.
The main task of the input interpretation com-
ponent of the agent is the detection of the dialogue
acts contained in the user utterances. This classi-
fication is done via a cue-based method with var-
ious features from multi-level knowledge sources
extracted from the incoming utterance considering
a small context of the previous dialogue.
In contrast to existing systems using mainly
lexical features, i.e. words, single markers such as
punctuation (Verbree et al, ) or combinations of
various features (Stolcke et al, 2000) for the dia-
logue act classification, the results of the interpre-
tation component presented in this paper are based
on syntactic and semantic relations. The system
first gathers linguistic information coming from
different levels of deep linguistic processing sim-
ilar to (Allen et al, 2007). The retrieved informa-
tion is used as input for an information extraction
component that delivers the relations embedded in
the actual utterance (Xu et al, 2007). These rela-
tions combined with additional features (a small
dialogue context and mood of the sentence) are
then utilized as features for the machine-learning
based recognition.
The classifier is trained on a corpus originating
from a Wizard-of-Oz experiment which was semi-
automatically annotated. It contains automatically
annotated syntactic relations namely, predicate ar-
gument structures, which were checked and cor-
rected manually afterwards. Furthermore these re-
lations are enriched by manual annotation with se-
mantic frame information from VerbNet to gain an
additional level of semantic richness. These two
representations of relations, the syntax-based re-
570
lations and the VerbNet semantic relations, were
used in separate training steps to detect how much
the classifier can benefit from either notations.
A systematic analysis of the data has been con-
ducted. It turns out that a comparatively small set
of syntactic relations cover most utterances, which
can moreover be expressed by an even smaller set
of semantic relations. Because of this observation
as well as the overall performance of the classifier
the interpretation is extended with an additional
rule based approach to ensure the robustness of
the system.
The paper is organized as follows: Section 2
provides an overview about existing dialogue act
recognition systems and the features they use for
classification.
Section 3 introduces the original data used as ba-
sis for the annotation and the classification task.
In Section 4 the annotation that provides the nec-
essary information for the dialogue act classifi-
cation and involves the relation extraction is de-
scribed in detail. The annotation is split into three
main steps: The annotation of dialogue informa-
tion (section 4.1), the integration of syntactic in-
formation (section 4.2) and finally the manual an-
notation of VerbNet predicate and role informa-
tion in section 4.3.
Section 5 presents the results of the actual classifi-
cation task using different feature sets and in Sec-
tion 6 the results and methods are summarized.
Finally, Section 7 provides a brief description of
the rule-based interpretation and presents an out-
look on future work.
2 Related Work
Dialogue Acts (DAs) represent the functional
level of a speaker?s utterance, such as a greeting,
a request or a statement. Dialogue acts are ver-
bal or nonverbal actions that incorporate partic-
ipant?s intentions originating from the theory of
Speech Acts by Searle and Austin (Searle, 1969).
They provide an abstraction from the original in-
put by detecting the intended action of an utter-
ance, which is not necessarily inferable from the
surface input (see the two requests in the follow-
ing example).
Can you show me a red car please?
Please show me a red car!
To detect the action included in an utterance,
different approaches have been suggested in re-
cent years which can be clustered into two main
classes: The first class uses AI planning methods
to detect the intention of the utterance based on
belief states of the communicating agents and the
world knowledge. These systems are often part of
an entire dialogue system e.g. in a conversational
agent which provides the necessary information
about current beliefs and goals of the conversa-
tion participants at runtime. One example is the
TRIPS system (Allen et al, 1996). Because of the
huge amount of reasoning, systems in this class
generally gather as much linguistic information as
possible.
The second class uses cues derived from the
actual utterance to detect the right dialogue act,
mostly using machine learning methods. This
class gained much attention due to less computa-
tional costs. The probabilistic classifications are
carried out via training on labeled examples of
dialogue acts described by different feature sets.
Frequently used cues for dialogue acts are lexi-
cal features such as the words of the utterance or
ngrams of words for example in (Verbree et al,
), (Zimmermann et al, 2005) or (Webb and Liu,
2008). Although the performance of the classi-
fication task is difficult to compare, because of
the variety of different corpora, dialogue act sets
and algorithms used, these approaches do pro-
vide considerably good results. For example (Ver-
bree et al, ) achieve accuracy values of 89% on
the ICSI Meeting Corpus containing 80.000 ut-
terances with a dialogue act set of 5 distinct di-
alogue act classes and amongst others the features
?ngrams of words? and ?ngrams of POS informa-
tion?.
Another group of systems utilizes acoustic fea-
tures derived from Automatic Speech Recognition
for automatic dialogue act tagging (Surendran and
Levow, 2006), context features like the preceding
dialogue act or ngrams of previous dialogue acts
(Keizer and Akker, 2006).
However grammatical and semantic informa-
tion is not that often incorporated into feature sets,
with the exception of single features such as the
571
Dialogue Act Meaning Frequency
REQUEST The utterance contains a wish or demand 449
REQUEST INFO The utterance contains a wish or demand regarding information 154
PROPOSE The utterance serves as suggestion or showing of an object 216
ACCEPT The utterance contains an affirmation 167
REJECT The utterance contains a rejection 88
PROVIDE INFO The utterance provides an information 156
ACKNOWLEDGE The utterance is a backchannelling 9
Table 1: The used Dialogue Act Set
type of verbs or arguments or the presence or ab-
sence of special operators e.g. wh-phrases (An-
dernach, 1996). (Keizer et al, 2002) use among
others linguistic features like sentence type for
classification with Bayesian networks. Although
(Jurafsky et al, 1998) already noticed a strong
correlation between selected dialogue acts and
special grammatical structures, approaches using
grammatical structure were not very succesful.
While grammatical and semantic features are
not often incorporated into dialogue act recogni-
tion, they are a commonly used in related fields
like automatic classification of rhetorical rela-
tions. For example (Sporleder and Lascarides,
2008) and (Lapata and Lascarides, 2004) extract
verbs as well as their temporal features derived
from parsing to infer sentence internal temporal
and rhetorical relations. Their best model for
analysing temporal relations between two clauses
achieves 70.7% accuracy. (Subba and Eugenio,
2009) also show a significant improvement of a
discourse relation classifier incorporating compo-
sitional semantics compared to a model without
semantic features. Their VerbNet based frame se-
mantics yield in a better result of 4.5%.
3 The Data
The data serving as the basis for the relation iden-
tification as well as the training corpus for the di-
alogue act classifier is taken from a Wizard-of-Oz
experiment (Bertomeu and Benz, 2009) in which
18 users furnish a virtual living room with the help
of a furniture sales agent. Users buy pieces of fur-
niture and room decoration from the agent by de-
scribing their demands and preferences in a text
chat. During the dialogue with the agent, the pre-
ferred objects are then selected and directly put to
the right location in the apartment. In the exper-
iments, users spent one hour each on furnishing
the living room by talking to a human wizard con-
trolling the virtual sales agent. The final corpus
consists of 18 dialogues containing 3,171 turns
with 4,313 utterances and 23,015 alpha-numerical
strings (words). The following example shows a
typical part of such a conversation:
USR.1: And do we have a little side table for the TV?
NPC.1: I could offer you another small table or a side-
board.
USR.2: Then I?ll take a sideboard that is similar to my
shelf.
NPC.2: Let me check if we have something like that.
Table 2: Example Conversation from the Wizard-
of-Oz Experiment
4 Annotation
The annotation of the corpus is carried out in sev-
eral steps.
4.1 Pragmatic Annotation
The first annotation step consists of annotating
discourse and pragmatic information including di-
alogue acts, projects according to (Clark, 1996),
sentence mood, the topic of the conversation and
an automatically retrieved information state for
every turn of the conversations. From the anno-
tated information the following elements were se-
lected as features in the final recognition system:
? The dialogue acts which carry the intentions
of the actual utterance as well as the last pre-
ceding dialogue act. The set used for anno-
tation is a domain specific set containing the
dialogue acts shown in table 1.
? The sentence mood. Sentence mood was
annotated with one of the following values:
declarative, imperative, interrogative.
572
? The topic of the utterance. The topic value
is coreferent with the currently discussed ob-
ject. Topic can consist of an object class
(e.g. sofa) or an special object instance
(sofa 1836). The topic of the directly pre-
ceding utterance was chosen as a feature too.
4.2 Annotation with Predicate Argument
Structure
The second annotation step, applied to the ut-
terance level of the input, automatically enriches
the annotation with predicate argument structures.
Each utterance is parsed with a predicate argu-
ment parser and annotated with syntactic relations
organized according to PropBank (Palmer et al,
2005) containing the following features: Predi-
cate, Subject, Objects, Negation, Modifiers, Cop-
ula Complements.
A single relation mainly consists of a predi-
cate and the belonging arguments. Verb modi-
fiers like attached PPs are classified as ?argM?
together with negation (?argM neg?) and modal
verbs (?argM modal?). Arguments are labeled
with numbers according to the found information
for the actual structure. PropBank is organized in
two layers, the first one being an underspecified
representation of a sentence with numbered argu-
ments, the second one containing fine-grained in-
formation about the semantic frames for the predi-
cate comparable to FrameNet (Baker et al, 1998).
While the information in the second layer is sta-
ble for each verb, the values of the numbered ar-
guments can change from verb to verb. While
for one verb the ?arg0? may refer to the subject
of the verb, another verb may encapsulate a di-
rect object behind the same notation ?arg0?. This
is very complicated to handle in a computational
setup, which needs continuous labeling for the
successive components. Therefore the arguments
were in general named as in PropBank but con-
sistently numbered by syntactic structure. This
means for example that the subject is always la-
beled as ?arg1?.
Consider the example ?Can you put posters or
pictures on the wall??. The syntactic relation will
yield in the following representation:
<predicate: put>
<ArgM_modal: can>
<Arg1: you>
<Arg2: posters or pictures>
<ArgM: on the wall>
Predicate Argument Structure Parser The
syntactic predicate argument structure that consti-
tutes the syntactic relations and serves as basis for
the VerbNet annotation, is automatically retrieved
by a rule-based predicate argument parser. The
rules utilized by the parser describe subtrees of de-
pendency structures in XML by means of relevant
grammatical functions. For detecting verbs with
two arguments in the input, for instance, a rule
can be written describing the dependency struc-
ture for a verb with a subject and an object. This
rule would then detect every occurrence of the
structure ?Verb-Subj-Obj? in a dependency tree.
This sample rule would express the following con-
straints: The matrix unit should be of the part of
speech ?Verb? , The structure belonging to this
verb must contain a ?nsubj? dependency and an
?obj? dependency.
The rules deliver raw predicate argument struc-
tures, in which the detected arguments and the
verb serve as hooks for further information lookup
in the input. If a verb fulfills all requirements
described by the rule, in a second step all modi-
ficational arguments existing in the structure are
recursively acquired. The same is done for
modal arguments as well as modifiers of the ar-
guments such as determiners, adjectives or em-
bedded prepositions. After the generation of the
main predicate argument structure from the gram-
matical functions, the last step inserts the content
values present in the actual input into the structure
to get the syntactic relations for the utterance.
Before the input can be parsed with the predi-
cate argument parser, some preprocessing steps of
the corpus are needed. These include:
Input Cleaning The input data coming from the
users contain many errors. Some string
substitutions as well as the external Google
spellchecker were applied to the input before
any further processing.
Segmentation For clausal separation we apply a
simple segmentation via heuristics based on
punctuation.
POS Tagging Then the input is processed by
573
the external part-of-speech tagger TreeTag-
ger (Schmid, 1994).
The embedded dependency parser is the Stan-
ford Dependency Parser (de Marneffe and Man-
ning, 2008), but other dependency parsers could
be employed instead. The predicate argument
parser is an standalone software and can be used
either as a system component or for batch process-
ing of a text corpus.
4.3 VerbNet Frame Annotation
The last step of annotation consists of the man-
ual annotation of semantic predicate classes and
semantic roles. Moreover, the automatically de-
termined syntactic relations are checked and cor-
rected if possible. VerbNet (Schuler, 2005) is uti-
lized as a source for semantic information. The
VerbNet role set consists of 21 general roles used
in all VerbNet classes. Examples of roles in
this general role set are ?agent?, ?patient? and
?theme?.
For the manual addition of the semantic frame
information a web-based annotation tool has been
developed. The annotation tool shows the utter-
ance which should be annotated in the context of
the dialogue including the information from the
preceding annotation steps. All VerbNet classes
containing the current predicate are listed as pos-
sibilities for the predicate classification together
with their syntactic frames. The annotators can se-
lect the appropriate predicate class and frame ac-
cording to the arguments found in the utterance.
If an argument is missing in the input that is re-
quired in the selected frame a null argument is
added to the structure. If the right predicate class
is existing, but the predicate is not yet a member
of the class, it is added to the VerbNet files. In
case the right predicate class is found but the fit-
ting frame is missing, the frame is added to the
VerbNet files. Thus during annotation 35 new
members have been added to the existing VerbNet
classes, 4 Frames and 4 new subclasses. Via these
modifications, a version of VerbNet has been de-
veloped that can be regarded as a domain-specific
VerbNet for the sales domain.
During the predicate classification, the annota-
tors also assign the appropriate semantic roles to
the arguments belonging to the selected predicate.
The semantic roles are taken from the selected
VerbNet frame.
From the annotated semantic structure, seman-
tic relations are inferred such as the one in the fol-
lowing example:
<predicate: put-3.1>
<agent: you>
<theme: posters or pictures>
<destination: on the wall>
5 Dialogue Act Recognition
Two datasets are derived from the corpus: The
dataset containing the utterances of the users
(CST) and one dataset containing the utterances
of the wizard (NPC), whereas the NPC corpus is
cleaned from the ?protocol sentences?. Protocol
sentences are canned sentences the wizard used
in every conversation, for example to initialize
the dialogue. For the experiments, the two sin-
gle datasets ?NPC? and ?CST? as well as a com-
bined dataset called ?ALL? are used. Unfortu-
nately from the original 4,313 utterances in total,
many utterances could not be used for the final ex-
periments. First, fragments are removed and only
the utterances found by the parser to contain a
valid predicate argument structure are used. After
protocol sentences are taken out too, a dataset of
1702 valid utterances remains. Moreover, 292 ut-
terances are annotated to contain no valid dialogue
act and are therefore not suitable for the recogni-
tion task. Of the remaining utterances, 171 predi-
cate argument structures were annotated as wrong
because of completely ungrammatical input. In
this way we arrive at a dataset of 804 instances for
the users and 435 for the wizard, summing up to
1239 instances in total.
The features used for dialogue act recognition
exploit the information extracted from the differ-
ent annotation steps:
? Context features: The last preceding dia-
logue act, equality between the last preced-
ing topic and the actual topic, sentence mood
? Syntactic relation features: Syntactic predi-
cate class, arguments, negation
? VerbNet semantic relation features: VerbNet
predicate class, VerbNet frame arguments,
negation
574
? Utterance features: The original utterances
without any modifications
Different sets of features for training and eval-
uation are generated from these:
DATASET Syn: All utterances of the specified
dataset described via syntactic relation and
context features.
DATASET VNSem: All utterances of the speci-
fied dataset described via VerbNet semantic
relations and context features.
DATASET Syn Only: All utterances of the
specified dataset only described via the
syntactic relations.
DATASET VNSem Only: All utterances of the
specified dataset only described via the Verb-
Net semantic relations.
DATASET Context Only: All utterances of the
specified dataset described via the context
features and negation without any informa-
tion regarding relations.
DATASET Utterances Context: The utterances
of the specified dataset as strings combined
with the whole set of context features without
further relation extraction results.
DATASET Utterances: Only the utterances of
the specified dataset as strings. This and the
last ?Utterances?-set serve as baselines.
Dialogue Act Recognition is carried out via
the Bayesian network classifier AOEDsr from the
WEKA toolkit. AODEsr augments AODE, an
algorithm averaging over all of a small space
of alternative naive-Bayes-like models that have
weaker independence assumptions than naive
Bayes, with Subsumption Resolution (Zheng and
Webb, 2006). Evaluation is performed using
crossfolded evaluation.
All results of the experiments are given in terms
of accuracy.
Results for the dataset ?All? comparing the syn-
tactic relations with VerbNet relations as well as
the pure utterances and context are shown in table
4.
Dataset Accuracy
All Syn 67.4%
All VNSem 66.8%
All Utterances Context 61.9%
All Utterances 48.1%
Table 4: Dialogue Act Classification Results for
the ?ALL? Datasets
The best result is achieved with the syntactic in-
formation, although the VerbNet information pro-
vides an abstraction over the predicate classifica-
tion. Both the set containing the VerbNet relations
as well as the syntactic relations are much better
than the set containing only the context and the
original utterances. The dataset containing only
the utterances could not reach 50%.
Although the experiments show much better re-
sults using the relations instead of the original ut-
terance, the overall accuracy is not very satisfying.
Several reasons for this phenomenon come into
consideration. While it can to a certain extend be
the fault of the classifying algorithm (see table 8
for some tests with a ROCCHIO based classifier),
the main reason might as well lie in the impre-
cise boundaries of the dialogue act classes: Sev-
eral categories are hard to distinguish even for a
human annotator as you can see from the wrongly
classified examples in table 3. Another possibil-
ity can be the comparatively small number of total
training instances.
For the NPC dataset the results are slightly bet-
ter and much better still for the set CST, which
is due to a smaller number (6) of dialogue acts:
The dialogue act ?PROPOSE?, which is the act
for showing an object or proposing a possibility,
was not used by any user, but only by the wizard.
Dataset Accuracy
CST Syn 73.1%
NPC Syn 68.5%
Table 5: Dialogue Act Classification Results for
Datasets ?CST? and ?NPC?
To find out if one sort of features is espe-
cially important for the classification we reorga-
575
Utterance Right Classification Classified As
What do you think about this one? request info propose
Let see what you have and where we can put it request info request
Table 3: Wrongly classified instances
nize the training sets to contain only the context
features without the relations (All Context Only)
on the one hand and only the relational informa-
tion without the context features on the other hand
(All Syn Only and All VNSem Only). Results
are shown in table 6.
Dataset Accuracy
All Context Only 56.6%
All VNSem Only 53.5%
All Syn Only 50.8%
Table 6: Dialogue Act Classification Results for
Context and Relation sets
Table 6 shows that the results are considerably
worse if only parts of the features are used. The
set with context feature performs 3,1% better than
the best set with the relations only. Furthermore
the VerbNet semantic relation set leads to nearly
3% better accuracy, which may mean that the ab-
straction of semantic predicates provides a better
mapping to dialogue acts after all if used without
further features which may be ranked more impor-
tant by the classifier.
Besides the experiments with the Bayesian net-
works, additional experiments are performed us-
ing a modified ROCCHIO algorithm similar to the
one in (Neumann and Schmeier, 2002). Three dif-
ferent datasets were tested (see table 7).
Dataset Accuracy
All Utterances 70.1%
All Utterances Context 73.2%
All Syn 74.4%
Table 8: Dialogue Act Classification Results using
the ROCCHIO Algorithm
Table 8 shows that the baseline dataset contain-
ing only the utterances already provides much bet-
ter results with the ROCCHIO algorithm, deliv-
ering 70.1% which is more than 10% more ac-
curacy compared to the 48.1% of the Bayesian
classifier. If tested together with the context fea-
tures the accuracy of the utterance dataset raises to
73.2% and, after including the relational informa-
tion, even to 74.4%. Thus, the results of this ROC-
CHIO experiment also prove that the employment
of the relation information leads to improved ac-
curacy of the classification.
6 Conclusion
This paper reports on a novel approach to auto-
matic dialogue act recognition using syntactic and
semantic relations as new features instead of the
traditional features such as ngrams of words.
Different feature sets are constructed via an
automatic annotation of syntactic predicate argu-
ment structures and a manual annotation of Verb-
Net frame information. On the basis of this infor-
mation, both the syntactic relations as well as the
semantic VerbNet-based relations included in the
utterances can be extracted and added to the fea-
ture sets for the recognition task. Besides the re-
lation information the employed features include
information from the dialogue context (e.g. the
last preceding dialogue act) and other features like
sentence mood.
The feature sets have been evaluated with a
Bayesian network classifier as well as a ROC-
CHIO algorithm. Both classifiers demonstrate the
benefits gained from the relations by exploiting
the additionally provided information. While the
difference between the best baseline feature set
and the best relation feature set in the Bayesian
network classifier yields a 5,5% boost in accuracy
(61.9% to 67.4%), the ROCCHIO setup exceeds
the boosted accuracy by another 1,5% , starting
from a higher baseline of 73.2%. Based on the
observed complexity of the classification task we
expect that the benefit of the relational informa-
576
Predicate Instances Example
see-30.1 59 I would like to see a table in front of the sofa
put-9.1 74 Can you put it in the corner?
reflexive appearance-48.1.2 80 Show me the red one
own-100 137 Do you have wooden chairs?
want-32.1 153 I would like some plants over here
Table 7: The Main Semantic Relations Found in the Data Sorted by Predicate
tion may turn out to be even more significant on
larger learning data.
7 Future Work
The results in section 5 show that the pure classifi-
cation cannot be used as interpretation component
in isolation, but additional methods have to be in-
corporated. In a preceding analysis of the data
it was found that certain predicates are very fre-
quently uttered by the users. In the syntactic pred-
icate scenario the total number of different predi-
cates is 80, whereas the semantic predicates build
up a total number of 66. The class containing the
predicates with one to ten occurrences constitutes
137 of 1239 instances. The remaining 1101 in-
stances are covered by only 21 different predicate
classes. These predicates together with their ar-
guments constitute a set of common domain re-
lations for the sales domain. The main domain
relations found are shown in table 7.
The figures suggest that the interpretation at
least for the domain relations can be established in
a robust manner, wherefore the agent?s interpreta-
tion component was extended to a hybrid module
including a robust rule based method. To derive
the necessary rules a rule generator was developed
and the rules covering the used feature set (includ-
ing the context features, sentence mood and the
syntactic relations) were automatically generated
from the given data.
Future work will focus on the evaluation of
these automatically derived rules on a recently
collected but not yet annotated dataset from a sec-
ond Wizard-of-Oz experiment, carried out in the
same furniture sales setting.
Additional experiments are planned for evalu-
ating the relation-based features in dialogue act
recognition on other corpora tagged with differ-
ent dialogue acts in order to test the overall per-
formance of our classification approach on more
transparent dialogue act sets.
Acknowledgements
The work described in this paper was partially
supported through the project ?KomParse? funded
by the ProFIT program of the Federal State of
Berlin, co-funded by the EFRE program of the
European Union. Additional support came from
the project TAKE, funded by the German Min-
istry for Education and Research (BMBF, FKZ:
01IW08003).
577
References
Allen, James F., Bradford W. Miller, Eric K. Ringger,
and Teresa Sikorski. 1996. A robust system for nat-
ural spoken dialogue. In Proceedings of ACL 1996.
Allen, James, Mehdi Manshadi, Myroslava Dzikovska,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In DeepLP ?07: Pro-
ceedings of the Workshop on Deep Linguistic Pro-
cessing, Morristown, NJ, USA.
Andernach, Toine. 1996. A machine learning ap-
proach to the classification of dialogue utterances.
CoRR, cmp-lg/9607022.
Baker, Collin F., Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project. In
Proceedings of COLING 1998.
Bertomeu, Nuria and Anton Benz. 2009. Annotation
of joint projects and information states in human-
npc dialogues. In Proceedings of CILC-09, Murcia,
Spain.
Clark, H.H. 1996. Using Language. Cambridge Uni-
versity Press.
de Marneffe, Marie C. and Christopher D. Manning.
2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, Manchester, UK.
Jurafsky, Daniel, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syntactic
cues for dialog acts.
Keizer, Simon and Rieks op den Akker. 2006.
Dialogue act recognition under uncertainty using
bayesian networks. Nat. Lang. Eng., 13(4).
Keizer, Simon, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with bayesian net-
works for dutch dialogues. In Proceedings of the
3rd SIGdial workshop on Discourse and dialogue,
Morristown, NJ, USA.
Klu?wer, Tina, Peter Adolphs, Feiyu Xu, Hans Uszko-
reit, and Xiwen Cheng. 2010. Talking npcs in a
virtual game world. In Proceedings of the System
Demonstrations Section at ACL 2010.
Lapata, Mirella and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In Proceed-
ings of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 153?160.
Neumann, Gu?nter and Sven Schmeier. 2002. Shal-
low natural language technology and text mining.
Ku?nstliche Intelligenz. The German Artificial Intel-
ligence Journal.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1).
Schmid, Helmut. 1994. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing.
Schuler, Karin Kipper. 2005. Verbnet: a broad-
coverage, comprehensive verb lexicon. Ph.D. the-
sis, Philadelphia, PA, USA.
Searle, John R. 1969. Speech acts : an essay in
the philosophy of language / John R. Searle. Cam-
bridge University Press, London.
Sporleder, Caroline and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: A critical assessment. Natural Lan-
guage Engineering, 14(3).
Stolcke, Andreas, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van, and Ess dykema
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26.
Subba, Rajen and Barbara Di Eugenio. 2009. An ef-
fective discourse parser that uses rich linguistic in-
formation. In NAACL ?09, Morristown, NJ, USA.
Surendran, Dinoj and Gina-Anne Levow. 2006. Di-
alog act tagging with support vector machines and
hidden markov models. In Interspeech.
Verbree, A.T., R.J. Rienks, and D.K.J. Heylen.
Dialogue-act tagging using smart feature selection:
results on multiple corpora. In Raorke, B., editor,
First International IEEE Workshop on Spoken Lan-
guage Technology SLT 2006.
Webb, Nick and Ting Liu. 2008. Investigating the
portability of corpus-derived cue phrases for dia-
logue act classification. In Proceedings of COLING
2008, Manchester, UK.
Xu, Feiyu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of ACl (07), Prague, Czech Republic.
Zheng, Fei and Geoffrey I. Webb. 2006. Efficient
lazy elimination for averaged one-dependence esti-
mators. In ICML, pages 1113?1120.
Zimmermann, Matthias, Yang Liu, Elizabeth Shriberg,
and Andreas Stolcke. 2005. Toward joint seg-
mentation and classification of dialog acts in mul-
tiparty meetings. In Proc. Multimodal Interaction
and Related Machine Learning Algorithms Work-
shop (MLMI05, page 187.
578
Coling 2010: Poster Volume, pages 1354?1362,
Beijing, August 2010
Boosting Relation Extraction with Limited Closed-World Knowledge
Feiyu Xu Hans Uszkoreit Sebastian Krause Hong Li
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI GmbH)
{feiyu,uszkoreit,sebastian.krause,lihong}@dfki.de
Abstract
This paper presents a new approach to im-
proving relation extraction based on min-
imally supervised learning. By adding
some limited closed-world knowledge for
confidence estimation of learned rules to
the usual seed data, the precision of re-
lation extraction can be considerably im-
proved. Starting from an existing base-
line system we demonstrate that utilizing
limited closed world knowledge can ef-
fectively eliminate ?dangerous? or plainly
wrong rules during the bootstrapping pro-
cess. The new method improves the re-
liability of the confidence estimation and
the precision value of the extracted in-
stances. Although recall suffers to a cer-
tain degree depending on the domain and
the selected settings, the overall perfor-
mance measured by F-score considerably
improves. Finally we validate the adapt-
ability of the best ranking method to a new
domain and obtain promising results.
1 Introduction
Minimally supervised machine-learning ap-
proaches to learning rules or patterns for relation
extraction (RE) in a bootstrapping framework are
regarded as very effective methods for building
information extraction (IE) systems and for
adapting them to new domains (e. g., (Riloff,
1996), (Brin, 1998), (Agichtein and Gravano,
2000), (Yangarber, 2001), (Sudo et al, 2003),
(Jones, 2005), (Greenwood and Stevenson,
2006), (Agichtein, 2006), (Xu et al, 2007),
(Xu, 2007)). On the one hand, these approaches
show very promising results by utilizing minimal
domain knowledge as seeds. On the other hand,
they are all confronted with the same problem,
i.e., the acquisition of wrong rules because of
missing knowledge for their validation during
bootstrapping. Various approaches to confidence
estimation of learned rules have been proposed
as well as methods for identifying ?so-called?
negative rules for increasing the precision value
(e.g., (Brin, 1998), (Agichtein and Gravano,
2000), (Agichtein, 2006), (Yangarber, 2003),
(Pantel and Pennacchiotti, 2006), (Etzioni et al,
2005), (Xu et al, 2007) and (Uszkoreit et al,
2009)).
In this paper, we present a new approach to esti-
mating or ranking the confidence value of learned
rules by utilizing limited closed-world knowl-
edge. As many predecessors, our ranking method
is built on the ?Duality Principle? (e. g., (Brin,
1998), (Yangarber, 2001) and (Agichtein, 2006)).
We extend the validation method by an evalu-
ation of extracted instances against some lim-
ited closed-world knowledge, while also allowing
cases in which knowledge for informed decisions
is not available. In comparison to previous ap-
proaches to negative examples or negative rules
such as (Yangarber, 2003), (Etzioni et al, 2005)
and (Uszkoreit et al, 2009), we implicitly gener-
ate many negative examples by utilizing the pos-
itive examples in the closed-world portion of our
knowledge. Rules extracting wrong instances are
lowered in rank.
In (Xu et al, 2007) and (Xu, 2007), we develop
a generic framework for learning rules for rela-
tions of varying complexity, called DARE (Do-
main Adaptive Relation Extraction). Furthermore,
there is a systematic error analysis of the base-
1354
line system conducted in (Xu, 2007). We employ
our system both as a baseline reference and as a
platform for implementing and evaluating our new
method.
Our first experiments conducted on the same
data used in (Xu et al, 2007) demonstrate: 1) lim-
ited closed-world knowledge is very useful and ef-
fective for improving rule confidence estimation
and precision of relation extraction; 2) integration
of soft constraints boosts the confidence value of
the good and relevant rules, but without strongly
decreasing the recall value. In addition, we val-
idate our method on a new corpus of newspaper
texts about celebrities and obtain promising re-
sults.
The remainder of the paper is organized as fol-
lows: Section 2 explains the relevant related work.
Sections 3 and 4 describe DARE and our exten-
sions. Section 5 reports the experiments with
two ranking strategies and their results. Section
6 gives a summary and discusses future work.
2 Related Work
In the existing minimally supervised rule learning
systems for relation extraction based on bootstrap-
ping, they already employ various approaches to
confidence estimation of learned rules and differ-
ent methods for identification of so-called nega-
tive rules. For estimation of confidence/relevance
values of rules, most of the approaches follow
the so-called ?Duality Principle? as mentioned by
Brin (1998) and Yangarber (2001), namely, the
confidence value of learned rules is dependent
on the confidence value of their origins, which
can be documents or relation instances. For ex-
ample, Riloff (1996), Yangarber (2001), Sudo et
al. (2003) and Greenwood and Stevenson (2006)
use domain relevance of documents in which pat-
terns are discovered as well as the distribution fre-
quency of these patterns in those relevant docu-
ments as an indication of good patterns. Their
methods are aimed at detecting all patterns for
a specific domain, but those patterns cannot be
applied directly to a specific relation. In con-
trast, systems presented by Brin (1998), Agichtein
and Gravano (2000), Agichtein (2006), Pantel
and Pennacchiotti (2006) as well as our base-
line system (Xu et al, 2007) are designed to
learn rules for a specific relation. They start with
some relation instances as their so-called ?seman-
tic seeds? and detect rules from texts matching
with these instances. The new rules are applied
to new texts for extracting new instances. These
new instances in turn are utilized as new seeds.
All these systems calculate their rule confidence
based on the confidence values of the instances
from which they stem. In addition to the confi-
dence value of the seed instances, most of them
also consider frequency information and include
some heuristics for extra validation. For exam-
ple, Agichtein (2006) intellectually defines certain
constraints for evaluating the truth value of ex-
tracted instances. But it is not clear whether this
strategy can be adapted to new domains and other
relations. In (Xu et al, 2007) we make use of do-
main relevance values of terms occurring in rules.
This method is not applicable to general relations.
Parallel to confidence estimation strategies, the
learning of negative rules is useful for identifying
wrong rules straightforwardly. Yangarber (2003)
and Etzioni et al (2005) utilize the so-called
Counter-Training for detecting negative rules for
a specific domain or a specific class by learning
from multiple domains or classes at the same time.
Examples of one certain domain or class are re-
garded as negative examples for the other ones.
Bunescu and Mooney (2007) follow a classifi-
cation-based approach to RE. They use positive
and negative sentences of a target relation for a
SVM classifier. Uszkoreit et al (2009) exploit
negative examples as seeds for learning further
negative instances and negative rules. The dis-
advantage of the above four approaches is that
the selected negative domains or classes or neg-
ative instances cover only a subset of the neg-
ative domains/classes/relations of the target do-
main/class/relation.
3 DARE Baseline System
Our baseline system DARE is a minimally super-
vised learning system for relation extraction, ini-
tialized by so-called ?semantic seeds?, i.e., exam-
ples of the target relations, labelled with their se-
mantic roles. The system supports domain adap-
tation through a compositional rule representation
and a bottom-up rule discovery strategy. In this
1355
way, DARE can handle target relations of varying
arity. The following example is a relation instance
of the target relation from (Xu, 2007) concerning
Nobel Prize awards: <Mohamed ElBaradei, No-
bel, Peace, 2005>. The target relation contains
four arguments: WINNER, PRIZE NAME, PRIZE AREA
and YEAR. This example refers to an event men-
tioned in the sentence in example (1).
(1) Mohamed ElBaradei, won the 2005 Nobel
Prize for Peace on Friday because of ....
Figure 1 is a simplified dependency tree of ex-
ample (1). DARE utilizes a bottom-up rule dis-
covery strategy to extract rules from such depen-
dency trees. All sentences are processed with
named entity recognition and dependency parsing.
?win?subject
wwnnnn
n object
''PPP
PP
Winner ?Prize?lex-mod
ssggggg
ggggg
ggg
lex-mod  mod ''O
OOOO
Year Prize ?for?
pcomp-n 
Area
Figure 1: Dependency tree for example (1)
From the tree in Figure 1, DARE learns three
rules. The first rule is dominated by the prepo-
sition ?for?, extracting the argument PRIZE AREA
(Area). The second rule is dominated by the noun
?Prize?, extracting the arguments YEAR (Year) and
PRIZE NAME (Prize), and calling the first rule for
the argument PRIZE AREA (Area). The rule ?win-
ner prize area year 1? from Figure 2 extracts all
four arguments from the verb phrase dominated
by the verb ?win? and calls the second rule to
handle the arguments embedded in the linguistic
argument ?object?.
Rule name :: winner prize area year 1
Rule body ::?
????????????
head
?
?
pos verb
mode active
lex-form ?win?
?
?
daughters <
[
subject
[
head 1 Winner
]]
,
?
??object
?
?
rule year prize area 1 ::
< 4 Year, 2 Prize,
3 Area >
?
?
?
??>
?
????????????
Output :: < 1 Winner, 2 Prize, 3 Area, 4 Year >
Figure 2: DARE extraction rule.
We conduct a systematic error analysis based
on our experiments with the Nobel Prize award
data (Xu, 2007). The learned rules are divided
into four groups: good, useless, dangerous and
bad. The good rules are rules that only extract cor-
rect instances, while bad ones exclusively produce
wrong instances. Useless rules are those that do
not detect any new instances. Dangerous rules are
dangerous because they extract both correct and
wrong instances. Most good rules are rules with
high specificity, namely, extracting all or most ar-
guments of the target relation. The 14.7% extrac-
tion errors are from bad rules and dangerous rules.
Other errors are caused by wrong reported con-
tent, negative modality, parsing and named entity
recognition errors.
4 Our Approach: Boosting Relation Ex-
traction
4.1 Closed-World Knowledge: Modeling and
Construction
The error analysis of DARE confirms that the
identification of bad rules or dangerous rules is
important for the precision of an extraction sys-
tem. Using closed-world knowledge with large
numbers of implicit negative instances opens a
possibility to detect such rules directly. In our
work, closed-world knowledge for a target rela-
tion is the total set of positive relation instances
for entire relations or for some selected subsets
of individuals. For most real world applications,
closed-world knowledge can only be obtained for
relatively small subsets of individuals participat-
ing in the relevant relations. We store the closed-
world knowledge in a relational database, which
we dub ?closed-world knowledge database? (abbr.
cwDB). Thus, a cwDB for a target relation should
fill the following condition:
A cwDB must contain all correct relation
instances (insts) for an instantiation value
(argValue) of a selected relation argument
cwArg in the target relation.
Given R (the total set of relation instances of a
target relation), a cwDB is defined as follows:
cwDB={inst ? R : cwArg(inst) = argValue}.
An example of a cwDB is the set of all prize win-
ners of a specific prize area such as Peace, where
PRIZE AREA is the selected cwArg and argValue is
Peace. Note that the merger of two cwDBs, for
example with PRIZE AREAs Peace and Literature,
is again a cwDB (with two argValues in this case).
1356
4.2 Modified Learning Algorithm
In Algorithm 1, we present the modification of the
DARE algorithm (Xu, 2007). The basic idea of
DARE is that it takes some initial seeds as input
and learns relation extraction rules from sentences
in the textual corpus matching the seeds. Given
the learned rules, it extracts new instances from
the texts. The modified algorithm adds the val-
idate step to evaluate the new instances against
the closed-world knowledge cwDB. Based on the
evaluation result, both new instances and learned
rules are ranked with a confidence value.
INPUT: initial seeds
1 i? 0 (iteration of bootstrapping)
2 seeds ? initial seeds
3 all instances ? {}
4 while (seeds 6= {})
5 rulesi ? getRules(seeds)
6 instancesi ? getInstances(rulesi)
7 new instancesi ? instancesi ? all instances
8 validate(new instances i , cwDB)
9 rank(new instancesi)
10 rank(rulesi)
11 seeds ? new instancesi
12 all instances ? all instances + new instancesi
13 i? i+ 1
OUTPUT: all instances
Algorithm 1: Extended DARE
4.3 Validation against cwDB
Given a cwDB of a target relation and its argValue
of its selected argument cwArg, the validation of
an extracted instance (inst) against the cwDB is
defined as follows.
inst correct ? inst ? cwDB (1)
inst wrong ? inst 6? cwDB ?
cwArg(inst) = argValue
inst unknown ? ( inst 6? cwDB ?
cwArg(inst) 6= argValue )
? ( inst 6? cwDB ?
cwArg(inst) is unspecified )
4.4 Rule Confidence Ranking with cwDB
We develop two rule-ranking strategies for con-
fidence estimation, in order to investigate the
best way of integrating the closed-world knowl-
edge: (a) exclusive ranking: This ranking strat-
egy excludes every rule which extracts wrong in-
stances after their validation against the closed-
world knowledge; (b) soft ranking: This ranking
strategy is built on top of the duality principle and
takes specificity and the depth of learning into ac-
count.
Exclusive Ranking The exclusive ranking
method is a very naive ranking method which
estimates the confidence value of a learned rule
(e.g., rule) depending on the truth value of its
extracted instances (getInstances(rule)) against
a cwDB. Any rule with one wrong extraction
is regarded as a bad rule in this method. This
method works effectively in a special scenario
where the total list of the instances of the target
relation is available as the cwDB.
confidence(rule) =
{
1 if getInstances(rule) ? cwDB,
0 otherwise. (2)
Soft Ranking The soft ranking method works
in the spirit of the ?Duality Principle?, the con-
fidence value of rules is dependent on the truth
value of their extracted instances and on the seed
instances from which they stem. The confi-
dence value of the extracted instances is estimated
based on their validation against the cwDB or the
confidence value of their ancestor seed instances
from which their extraction rules stem. Further-
more, the specificity of the instances (percentage
of the filled arguments) and the learning depth
(iteration step of bootstrapping) are parameters
too. The definition of instance scoring, namely,
score(inst), is given as follows:
score(inst) =
?
?
?
? > 0 if validate(inst , cwDB) = correct,
0 if validate(inst , cwDB) = wrong,
UN inst if validate(inst , cwDB) = unknown.
(3)
As defined above, if a new instance is con-
firmed as correct by the cwDB, it will obtain a
positive value. In our experiment, we set ?=10
in order to boost the precision. In the case of un-
known about its truth value, the confidence value
of a new instance (inst) is dependent on the confi-
dence values of the seed instances (ancestor seeds)
from which its mother rules (Rinst ) stem. Below,
the scoring of the unknown case, namely, UN inst ,
is defined, where Rinst are rules that extract the
new instance inst , while Irule are instances from
which a rule inRinst is learned and ? is the speci-
ficity value of inst while ? is utilized to express
the noisy potential of each further iteration during
bootstrapping.
1357
UN inst =
?
rule?Rinst
(?
j?Irule score(j)
|Irule | ? ?
irule
)
|Rinst |
? ?
where
Rinst = getMotherRulesOf(inst),
Irule = getMotherInstancesOf(rule),
? = specificity,
? = 0.8,
irule = i-th iteration where rule occurs
(4)
Given the scoring of instance inst , the confidence
estimation of a rule is the average score of all
insts extracted by this rule:
confidence(rule) =
?
inst?I score(inst)
|I|
where I = getInstances(rule) (5)
5 Experiments
5.1 Corpora and Closed-World Knowledge
We conduct our experiments with two different
domains. We start with the Nobel Prize award do-
main reported in (Xu, 2007) and apply our method
to the same corpus, a collection from various on-
line newspapers. The target relation is the one
with the four arguments as mentioned in Sec-
tion 3. In this way, we can compare our results
with those reported in (Xu, 2007). Furthermore,
all Nobel Prize winners can be found from http:
//nobelprize.org, so it is easy to construct
a cwDB for Nobel Prize winners. We take the
PRIZE AREA as our selected argument for closing
sub-relations and construct various cwDBs with
the instantiation of this argument (e.g., all win-
ners of Nobel Peace Prize). The second domain
is about celebrities. Our text corpus is collected
from tabloid newspaper texts, containing 6850 ar-
ticles from the years 2001 and 2002. The target
relation is the marriage relationship between two
persons. We construct a cwDB of 289 persons in
which we have listed all their (ex-)spouses as well
as the time span of the marriage relation.
Table 1 summarizes the size of the corpus data
of the two domains.
Domain Space #Doc.
Nobel Prize 18,4 MB 3328
Celebrity Marr. 16,6 MB 6850
Table 1: Corpus data.
5.2 Nobel Prize Domain
We apply the extended DARE system to the Nobel
Prize corpus at first and conduct two rule rank-
ing strategies with different sizes of the cwDB.
We conduct all our experiments with the seed
<Guenter Grass, Nobel, Literature, 1999>. The
DARE-Baseline performance is shown in Table 2.
Precision Absolute Recall
Baseline 77.98% 89.01%
Table 2: DARE-Baseline Performance
Exclusive Ranking
Given the complete list of Nobel Laureates, we
can apply the exclusive ranking strategy to this do-
main. Our cwDB is the total list of Nobel Prize
winners. The wrong instances will not be used as
seed for the next iteration. Rules that extracted
at least one wrong instance are marked as bad, the
other rules as good. We utilize only the good rules
for relation extraction.
Prec. Rel. Recall Rel. F-Measure
100.00% 82.88% 90.64%
Table 3: Performance of Exclusive Ranking in
Nobel Prize award domain.
In comparison to the DARE baseline system,
given the same seed setup, this experiment results
in a precision boost from 77.98% to 100% (see
Table 3). This is not surprising since the cwDB
covers all relation instances for the target rela-
tion. Nevertheless, this experiment shows that the
closed-world knowledge approach is effective to
exclude bad rules. However, the recall decreases
and is only 82.88% of the one of the baseline sys-
tem. As we explain above, not all rules extracting
wrong instances are bad rules because wrong ex-
tractions can also be caused by other error sources
such as named entity recognition. Therefore, even
good rules can be excluded because of other er-
ror sources. The exclusive ranking strategy is use-
ful for application scenarios where people want to
learn rules for achieving 100% precision perfor-
mance and do not expect high recall. It is espe-
cially effective when a big cwDB is available.
Soft Ranking
This ranking strategy does not exclude any
rules and assigns a score to each rule based on
1358
the definition in Section 4.4. Rules which extract
correct instances, more specific relation instances
and stem from high-scored seed instances obtain
a better value than others. In our approach, the
specificity is dependent on the number of the ar-
guments in the extracted instances. For this do-
main, the most specific instances contain all four
arguments. In the following, we conduct two ex-
periments with two different sizes of the cwDB:
1) with the total list of winners (complete cwDB)
and 2) with only winners in one PRIZE AREA (lim-
ited cwDB).
1) Complete closed-world database Figure 3
displays the correlation between the score of rules
and their extraction precision performance. Each
point stands for a set of rules with the same
score and extraction precision. In this setup, the
higher the score, the higher the precision. Given
the scored rules, Figure 4 depicts precision, re-
call and F-Measure for different score thresholds.
For a given threshold j we take all rules with
score(rule) ? j and use the instances they ex-
tract. The recall value here is the relative recall
w. r. t. to the DARE baseline performance: i. e. the
number of correct extracted instances divided by
the number of correct instances extracted by the
DARE baseline system. The F-Measure value is
calculated by using the relative recall values, we
therefore refer to it as the relative F-Measure. If
the system takes all rules with score ? 7, the sys-
tem achieves the best relative F-Measure.
  0 1 2 3 4 5 6 7 8 9 100,00%10,00%
20,00%30,00%
40,00%50,00%
60,00%70,00%
80,00%90,00%
100,00%
Rule-Score
Correctne
ss of extra
cted insta
nces
Figure 3: Rule scores vs. precisions with the
complete closed-world database.
2) Limited closed-world database This experi-
ment investigates the system performance in cases
in which only a limited cwDB is available. This is
the typical situation for most real world RE appli-
cations. Therefore, this experiment is much more
  0 012 3 312 4 412 5 512 6 612 2 212 7 712 8 812 9 912 , ,12 3020%00R22%00R
70%00R72%00R80%00R
82%00R90%00R92%00R
,0%00R,2%00R300%00R
ule-ScSorCtrcnsr-ec  efsnSxeC e-sff  efsnSxeCadiesc?le
??lec?of?C?olC ?fed?-ole
Figure 4: Performance with the complete closed-
world database.
important than the previous one. We construct
a smaller database containing only Peace Nobel
Prize winners, which is about 1/8 of the previous
complete cwDB.
  0 1 2 3 4 5 6 7 8 9 100,00%10,00%
20,00%30,00%
40,00%50,00%
60,00%70,00%
80,00%90,00%
100,00%
Rule-Score
Correctne
ss of extra
cted insta
nces
Figure 5: Rule score vs. precision with the lim-
ited closed-world database
  0 012 3 312 4 412 5 512 6 612 2 212 7 712 8 812 9 912 , ,12 300%00R30%00R
40%00R50%00R60%00R
20%00R70%00R80%00R
90%00R,0%00R300%00R
ule-ScSorCtrcnsr-ec  efsnSxeC e-sff  efsnSxeCadiesc?le
??lec?of?C?olC ?fed?-ole
Figure 6: Performance with the limited closed-
world database
Figure 5 shows the correlation between the
score of the rules and their extraction precision.
Although the development curve here is not as
smooth as depicted in Figure 3, the higher scored
rules have better precision values than most of the
lower scored rules. However, we can observe that
some very good rules are scored low, located in
1359
Thresh. Good Dangerous Bad
Baseline 58.94% 26.49% 14.57%
1 64.96% 29.20% 5.84%
2 66.67% 27.91% 5.43%
3 69.23% 26.50% 4.27%
4 73.27% 23.76% 2.97%
5 76.00% 22.67% 1.33%
6 77.59% 20.69% 1.72%
7 77.50% 22.50% 0.00%
8 87.50% 12.50% 0.00%
9 85.71% 14.29% 0.00%
10 90.00% 10.00% 0.00%
Table 4: Quality analysis of rules with the limited
closed-world database
the left upper corner. The reason is that many of
their extracted instances are unknown, even if their
extracted instances are mostly correct.
As shown in Figure 6, even with the limited
cwDB, the precision values are comparable with
the complete cwDB (see Figure 4). However, the
recall value drops much earlier than with the com-
plete cwDB. With a threshold of score 4, the sys-
tem achieves the best modified F-Measure 92,21%
with an improvement of precision of about 11 per-
centage points compared to the DARE baseline
system (89.39% vs. 77.98%). These results show
that even with a limited cwDB this ranking system
can help to improve the precision without loosing
too much recall.
We take a closer look on the useful (actively ex-
tracting) rules and their extraction performance,
using the same rule classification as (Xu, 2007).
As shown in Table 4, more than one fourth of
the extraction rules created by the baseline system
are dangerous ones and almost 15% are plainly
wrong. Applying the rule scoring with the limited
cwDB increases the fraction of good rules to al-
most three quarters and nearly eliminates all bad
rules at threshold 4. By choosing higher thresh-
olds, surviving good rules raises to 90%. The total
remaining set of rules then only consists of rules
that at least partially extract correct instances.
5.3 Celebrity Domain
As presented above, the soft ranking method de-
livers very promising result. In order to val-
idate this ranking method, we choose an ad-
ditional domain and decide to learn marriage
relations among celebrities, where the target
relation consists of the following arguments:
[ NAME OF SPOUSE, NAME OF SPOUSE, YEAR].
The value of the marriage year is valid when
the year is within the marriage time interval. The
motivation of selecting this target relation is the
large number of possible relations between two
persons leading to dangerous or even bad rules.
For example, the rule in Figure 7 is a very dan-
gerous rule because ?meeting? events of two mar-
ried celebrities are often reported. A good confi-
dence estimation method is very useful for boost-
ing the good rules like the one in Figure 8. From
our text corpus we extract 37.000 sentences that
mention at least two persons. The cwDB con-
sists of sample relation instances, in which one
NAME OF SPOUSE is instantiated, i. e. we manu-
ally construct a database which contains all (ex-)
spouses of 289 celebrities.
head([SPOUSE<ne_person>]),
mod({head(("meet", VB)),
subj({head([SPOUSE<ne_person>])})})
Figure 7: A dangerous extraction rule example
head(("marry", VB)),
aux({head(("be", VB))}),
dep({head([SPOUSE<ne_person>]),
dep({head([DATE<point>])})}),
nsubj({head([SPOUSE<ne_person>])})
Figure 8: Example of a positive rule
Since a gold standard of mentions for this cor-
pus is not available, we manually validate 100 ran-
dom samples from each threshold group. This
evaluation gives us an opportunity to estimate the
effect of a cwDB in this domain. Table 5 presents
the performance of the rules with different thresh-
olds. The precision value of the baseline system
is very low. Threshold 3 slightly improves the
precision of the DARE baseline without damag-
ing recall too much. Step 4 excludes dangerous
rules such as the one in Figure 7 which drastically
boosts the precision. Unfortunately, the exclusion
of such general rules leads to the loss of many cor-
rect relation instances too, therefore, the immense
drop of recall from threshold 3 to 4 as well as from
threshold 4 to 5. Positive extraction rules such as
Figure 8 are quite highly scored. Because of the
large number of rules and instances, we start the
quality analysis of rules with score 3. As the table
indicates, the use of the rule scoring in this domain
clearly improves the quality of the created extrac-
tion rules. The error analysis shows that the ma-
jor error resource for this domain is wrong coref-
erence resolution or identity resolution. For ex-
1360
Thresh. # Instances Prec. Rel. Rec. Rel. F-Meas. # Rules Good Dangerous Bad
Baseline 25183 9.00% 100.00% 16.51% 12258
1 19806 7.00% 61.17% 12.56% 562
2 14542 9.00% 57.75% 15.57% 159
3 11259 15.00% 74.51% 24.97% 121 19.83% 33.88% 46.28%
4 788 65.00% 22.60% 33.54% 72 25.00% 27.78% 47.22%
5 195 67.00% 5.76% 10.62% 29 37.93% 17.24% 44.83%
6 115 84.00% 4.26% 8.11% 11 45.45% 27.27% 27.27%
7 55 89.09% 2.16% 4.22% 6 50.00% 33.33% 16.67%
8 9 77.78% 0.31% 0.62% 4 75.00% 0.00% 25.00%
9 5 60.00% 0.13% 0.26% 3 66.67% 0.00% 33.33%
10 5 60.00% 0.13% 0.26% 3 66.67% 0.00% 33.33%
Table 5: Soft ranking for the celebrity marriage domain with a limited cwDB.
ample, the inability to distinguish Prince Charles
(former husband of British princess Diana) from
Charles Spencer (her brother) is the reason that
DARE crosses the border between the marriage
and the sibling relation. In comparison to the
Nobel Prize award event, the marriage relation
between persons is often used as additional in-
formation to a person which is involved in a re-
ported event. Therefore, anaphoric references oc-
cur more often in their mentionings, as the exam-
ple relation in (3).
(3) ?My kids, I really don?t like them to
watch that much television,? said :::::Cruise, 40, whoadopted Isabella and Connor while ::he was mar-ried to second wife Nicole Kidman.
6 Summary
We propose a new way in which prior knowledge
about domains can be efficiently used as addi-
tional criteria for confidence estimation of learned
new rules or new instances in a minimally su-
pervised machine learning framework. By intro-
ducing rule scoring on the basis of available do-
main knowledge (the cwDB), rules can be eval-
uated during the bootstrapping process with re-
spect to their extraction precision. The results
are rather promising. The rule score threshold is
an easy way for users of an extraction system to
adjust the precision-recall-trade-off to their own
needs. The rule estimation method is also general
enough to extend to integration of common sense
knowledge. Although the relation instances in
the closed-world knowledge database can also be
used as seed in the beginning, the core idea of our
research work is to develop a general confidence
estimation strategy for discovered new informa-
tion. As discussed in (Xu, 2007) and (Uszkoreit
et al, 2009), the size of seed is not always rele-
vant for the learning and extraction performance,
in particular if the data corpus exhibits the small
world property. Using all instances in the cwDB
as seed, our experiments with the baseline system
yield worse precision performance than the modi-
fied DARE algorithm with only one seed instance.
This approach is quite general and easily adapt-
able to many domains; the only prerequisite is
the existence of a database with relation instances
from the target domain with a fulfilled closed-
world property on some relational argument. A
database of this kind should be easily obtainable
for many domains, e. g. by exploiting structured
and semi-structured information sources in the In-
ternet, such as YAGO (Suchanek et al (2007)) and
DBpedia (Bizer et al (2009)). Furthermore, in
some areas, such as Business Intelligence, there
is nearly complete knowledge already present for
past years, while the task is to extract informa-
tion only from recent news articles. Construct-
ing closed-worlds out of the present knowledge to
improve the learning of new information is there-
fore a straightforward approach. Even the manual
collection of suitable data might be a reasonable
choice since appropriate closed worlds could be
rather small if cwDBis chosen properly.
Acknowledgments
The work presented here has been partially sup-
ported through the prject KomParse by the ProFIT
program of the Federal State of Berlin which in
turn is co-funded by the EFRE program of the
European Union. It is additionally supported
through a grant to the project TAKE, funded by
the German Ministry for Education and Research
(BMBF, FKZ: 01IW08003).
1361
References
Agichtein, Eugene and Luis Gravano. 2000. Snow-
ball: extracting relations from large plain-text col-
lections. In DL ?00: Proceedings of the fifth ACM
conference on Digital libraries, pages 85?94, New
York, NY, USA. ACM.
Agichtein, Eugene. 2006. Confidence estimation
methods for partially supervised information extrac-
tion. In Proceedings of the Sixth SIAM International
Conference on Data Mining, Bethesda, MD, USA,
April. SIAM.
Bizer, Christian, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crys-
tallization point for the web of data. Journal of Web
Semantics, 7(3):154?165.
Brin, Sergey. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, EDBT?98.
Bunescu, Razvan C. and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics.
Etzioni, Oren, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91 ? 134.
Greenwood, Mark A. and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation ex-
traction patterns. In Proceedings of the Workshop
on Information Extraction Beyond The Document,
pages 29?35, Sydney, Australia, July. Association
for Computational Linguistics.
Jones, R. 2005. Learning to Extract Entities from La-
beled and Unlabeled Text. Ph.D. thesis, University
of Utah.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, July. The Association for Computer Lin-
guistics.
Riloff, Ellen. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings
of Thirteenth National Conference on Artificial In-
telligence (AAAI-96), pages 1044?1049. The AAAI
Press/MIT Press.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In 16th international World Wide Web con-
ference (WWW 2007), New York, NY, USA. ACM
Press.
Sudo, K., S. Sekine, and R. Grishman. 2003. An im-
proved extraction pattern representation model for
automatic IE pattern acquisition. Proceedings of
ACL 2003, pages 224?231.
Uszkoreit, Hans, Feiyu Xu, and Hong Li. 2009. Anal-
ysis and improvement of minimally supervised ma-
chine learning for relation extraction. In 14th In-
ternational Conference on Applications of Natural
Language to Information Systems. Springer.
Xu, Feiyu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of ACL 2007, 45th Annual Meeting
of the Association for Computational Linguistics,
Prague, Czech Republic, June.
Xu, Feiyu. 2007. Bootstrapping Relation Extraction
from Semantic Seeds. Phd-thesis, Saarland Univer-
sity.
Yangarber, Roman. 2001. Scenarion Customization
for Information Extraction. Dissertation, Depart-
ment of Computer Science, Graduate School of Arts
and Science, New York University, New York, USA.
Yangarber, Roman. 2003. Counter-training in dis-
covery of semantic patterns. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 343?350, Sapporo Con-
vention Center, Sapporo, Japan, July.
1362
Proceedings of the EACL 2009 Demonstrations Session, pages 13?16,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
GOSSIP GALORE
A Self-Learning Agent for Exchanging Pop Trivia
Xiwen Cheng, Peter Adolphs, Feiyu Xu, Hans Uszkoreit, Hong Li
DFKI GmbH, Language Technology Lab
Stuhlsatzenhausweg 3, D-66123 Saarbru?cken, Germany
{xiwen.cheng,peter.adolphs,feiyu,uszkoreit,lihong}@domain.com
Abstract
This paper describes a self-learning soft-
ware agent who collects and learns knowl-
edge from the web and also exchanges her
knowledge via dialogues with the users.
The agent is built on top of information
extraction, web mining, question answer-
ing and dialogue system technologies, and
users can freely formulate their questions
within the gossip domain and obtain the
answers in multiple ways: textual re-
sponse, graph-based visualization of the
related concepts and speech output.
1 Introduction
The system presented here is developed within the
project Responsive Artificial Situated Cognitive
Agents Living and Learning on the Internet (RAS-
CALLI) supported by the European Commission
Cognitive Systems Programme (IST-27596-2004).
The goal of the project is to develop and imple-
ment cognitively enhanced artificial agents, using
technologies in natural language processing, ques-
tion answering, web-based information extraction,
semantic web and interaction driven profiling with
cognitive modelling (Krenn, 2008).
This paper describes a conversational agent
?Gossip Galore?, an active self-learning system
that can learn, update and interpret information
from the web, and can make conversations with
users and provide answers to their questions in the
domain of celebrity gossip. In more detail, by
applying a minimally supervised relation extrac-
tion system (Xu et al, 2007; Xu et al, 2008), the
agent automatically collects the knowledge from
relevant websites, and also communicates with the
users using a question-answering engine via a 3D
graphic interface.
This paper is organized as follows. Section 2
gives an overview of the system architecture and
Figure 1: Gossip Galore responding to ?Tell me
something about Carla Bruni!?
presents the design and functionalities of the com-
ponents. Section 3 explains the system setup and
discusses implementation details, and finally Sec-
tion 4 draws conclusions.
2 System Overview
Figure 1 shows a use case of the system. Given a
query ?Tell me something about Carla Bruni?, the
application would trigger a series of background
actions and respond with: ?Here, have a look at
the personal profile of Carla Bruni?. Meanwhile,
the personal profile of Carla Bruni, would be dis-
played on the screen. The design of the interface
reflects the domain of celebrity gossip: the agent
is depicted as a young lady in 3D graphics, who
communicates with users. As an additional fea-
ture, users can access the dialogue memory of the
system, which simulates the human memory in di-
alogues. An example of the dialogue memory is
sketched in Figure 2.
As shown in Figure 3, the system consists of a
number of components. In principle, first, a user?s
query is linguistically analyzed, and then inter-
13
Dialogue
State
Dialogue
Memory
MM GeneratorResponseHandler
NE RecognizerSpellChecker Parser
Anaphora
Resolver
Knowledge
Base
Web
Miner
Input
Interpreter
Input
Analyzer
Relation
Extractor
Information
Wrapper
NL Generator
Conversational
Agent
Figure 3: Agent architecture and interaction of components
Figure 2: Representation of Social Network in Di-
alogue Memory
preted with respect to the context of the dialogue.
A Response Handler will then consult the knowl-
edge base pre-constructed by extracting relevant
information from the Web, and pass the answer, in
an abstract representation, to a Multimodal Gener-
ator, which realizes and presents the answer to the
user in multiple ways. The main components are
described in the following sections.
2.1 Knowledge Base
The knowledge base is automatically built by the
Web Miner. It contains knowledge regarding prop-
erties of persons or groups and their social rela-
tionships. The persons and groups that we concern
are celebrities in the entertainment industry (e.g.,
singers, bands, or movie stars) and their relatives
(e.g., partners) and friends. Typical properties of a
person include name, gender, birthday, etc., and
profiles of celebrities contain additional proper-
ties such as sexual orientation, home pages, stage
names, genres of their work, albums, and prizes.
Social relationships between the persons/groups
such as parent-child, partner, sibling, influenc-
ing/influenced and group-member, are also stored.
2.2 Web Miner
The Web Miner fetches relevant concepts and their
relations by means of two technologies: a) infor-
mation wrapping for exaction of personal profiles
from structured and semi-structured web content,
and b) a minimally supervised machine learning
method provided by DARE (Xu et al, 2007; Xu
et al, 2008) to acquire relations from free texts.
DARE learns linguistic patterns indicating the tar-
get semantic relations by taking some relation in-
stances as initial seed. For example, assume that
the following seed for a parent-child relationship
is given to the DARE system:
(1) Seed: ?Angelina Jolie, Shiloh Nouvel Jolie-Pitt,
daughter?
One sentence that matches the entities men-
tioned in the seed above could be (2), and from
which the DARE system can derive a linguistic
pattern as shown in 3.
(2) Matched sentence: Angelina Jolie and Brad Pitt
welcome their new daughter Shiloh Nouvel Jolie-Pitt.
(3) Extracted pattern: ?subject: celebrity? welcome
?mod: ?new daughter?? ?object: person?
Given the learned pattern, new instances of the
?parent-child? relationship can be automatically
discovered, e.g.:
(4) New acquired instances: ?Adam Sandler, Sunny
Madeline? ?Cynthia Rodriguez, Ella Alexander?
Given the discovered relations among the
celebrities and other people, the system constructs
a social network, which is the basis for providing
answers to users? questions regarding celebrities?
relationships. The network also serves as a re-
source for the active dialogue memory of the agent
as shown in Figure 2.
14
2.3 Input Analyzer and Input Interpreter
The Input Analyzer is designed as both domain
and dialogue context independent. It relies on sev-
eral linguistic analysis tools: 1) a spell checker, 2)
a named entity recognizer SProUT (Drozdzynski
et al, 2004), and 3) a syntactic parsing component
for which we currently employ a fuzzy paraphrase
matcher to approximate the output of a deep syn-
tactic/semantic parser.
In contrast to the Input Analyzer, the Input In-
terpreter analyzes the input with respect to the
context of the dialogue. It contains two major
components: 1) anaphoric resolution, which refers
pronouns to previously mentioned entities with the
help of the dialogue memory, and 2) domain clas-
sification, which determines whether the entities
contained in a user query can be found in the gos-
sip knowledge base (cf. ?Carla Bruni? vs. ?Nico-
las Sarkozy?) and whether the answer focus be-
longs to the domain (cf. ?stage name? vs ?body
guard?). For example, a simple factoid query such
as ?Who is Madonna?, an embedded questions
like ?I wonder who Madonna is?, and expressions
of requests and wishes such as ?I?m interested in
Madonna?, would share the same answer focus,
i.e., the ?personal profile? of ?Madonna?. In ad-
dition to the simple answer types such as ?person
name?, ?location? and ?date/time?, our system can
also deal with complex answer focus types such as
?personal profile?, ?social network? and ?relation
path?, as well as domain-relevant concepts such as
?party affiliation? or ?sexual orientation?.
Finally, the analysis of each query is associated
with a meaning representation, an answer focus
and an expected answer type.
2.4 Response Handler
This component executes the planned action based
on the properties of the answer focus and the en-
tities in a query. In cases where the answer focus
or the entities cannot be found in the knowledge
base, the system would still attempt to provide a
constructive answer. For instance, if a question
contains a domain-specific answer focus but en-
tities unknown to the knowledge base, the agent
will automatically look for alternative knowledge
resources, e.g., Wikipedia. For example, given
the question ?Tell me something about Nicolas
Sarkozy!?, the agent would attempt a Web search
and return the corresponding page on Wikipedia
about ?Nicolas Sarkozy?, even if the knowledge
base does not contain his information since he is a
politician rather than an entertainer.
In addition, specific strategies have been devel-
oped to deal with negative answers. For instance,
the agent would answer the question: When did
Madonna die?, with ?As far as I know, Madonna
is still alive.?, as it cannot find any information re-
garding Madonna?s death.
2.5 Multimodal Generator
The agent (i.e., the young lady in Figure 1) is
equipped with multimodal capabilities to inter-
act with users. It can show the results in tex-
tual and speech forms, using body gestures, fa-
cial expressions, and finally via multimedia out-
put to an embedded screen. We currently employ
template-based generators for producing both the
natural language utterances and the instructions to
the agent that controls the multimodal communi-
cation with the user.
2.6 Dialogue State
The responsibility of this component is to keep
track of the current state of the dialogue between a
user and the agent. It models the system?s expec-
tation of the user?s next action and the system?s re-
actions. For example, if a user misspelled a name
as in the question ?Who is Roby Williams??, the
system would answer with a clarification question:
?Did you mean Robbie Williams?? The user is
then expected to react to the question with either
?yes? or ?no?, which would not be interpretable in
other dialogue contexts where the user is expected
to ask a question. The fact that the system asks a
clarification question and expects a yes/no answer
as well as the repaired question are stored in the
Dialogue State component.
2.7 Dialogue Memory
This component aims to simulate the cognitive ca-
pacity of the memory of a human being: con-
struction of a short-time memory and activation
of long-time memory (our Knowledge Base). It
records the sequence of all entities mentioned dur-
ing the conversation and their respective target
foci. Simultaneously, it retrieves all the related in-
formation from the Knowledge Base. In figure 2,
the dialogue memory for the three questions ?Tell
me something about Carla Bruni.?, ?Can you tell
me some news about her??, ?How many kids does
Brad Pitt have?? is shown. Green and yellow bub-
bles are entities mentioned in the dialogue context,
15
where the yellow one is the last mentioned entity.
White bubbles indicate the newest records which
are acquired in the last process of online QA.
3 Implementation
The system uses a client-server architecture. The
server is responsible for accepting new connec-
tions, managing accounts, processing conversa-
tions and passing responses to the clients. All
the server-side functions are implemented in Java
1.6. We use Jetty as a web server to deliver mul-
timedia representations of an answer and to pro-
vide selected functionalities of the system as web
services to our partners. The knowledge base is
stored in a MySQL database whose size is 11MB,
and contains information of 38,758 persons in-
cluding 16,532 artists and 1,407 music groups. As
for the social connection data, there are 14,909
parent-child, 16,886 partner, 4,214 sibling, 308
influence/influenced and 9,657 group-member re-
lational pairs. The social network is visualized
in JGraph, and speech output is generated by the
open-source speech synthesis system OpenMary
(Schro?der and Hunecke, 2007).
There are two interfaces realizing the client-
side of the system: a 3D software application and
a web interface. The software application uses
a 3D computer game engine, and communicates
with the server by messages in an XML format
based on BML and SSML. In addition, we provide
a web interface1, implemented using HTML and
Javascript on the browser side, and Java Servlets
on the server side, offering the same core func-
tionality as the 3D client.
Both the server and the web client are platform
independent. The 3D client runs on Windows with
a dedicated 3D graphics card. The recommended
memory for the server is 1GB.
4 Conclusions
This paper describes a fully implemented software
application, which discovers and learns informa-
tion and knowledge from the Web, and communi-
cates with users and exchanges gossip trivia with
them. The system uses many novel technologies
in order to achieve the goal of vividly chatting and
interacting with the users in a fun way. The tech-
nologies include information extraction, question
answering, dialogue modeling, response planning
and multimodal presentation generation. Please
1http://rascalli.dfki.de/live/dialogue.page
refer to (Xu et al, 2009) for additional details
about the ?Gossip Galore? system.
The planned future extensions include the in-
tegration of deeper language processing methods
to discover more precise linguistic patterns. A
prime candidate for this extension is our own deep
syntactic/semantic parser. Another plan concerns
the required temporal aspects of relations together
with credibility checking. Finally, we plan to ex-
ploit the dialogue memory for moving more of the
dialogue initiative to the agent. In cases of miss-
ing or negative answers or in cases of pauses on
the user side, the agent can use the active parts
of the dialogue memory to propose additional rel-
evant information or to guide the user to fruitful
requests within the range of user?s interests.
References
Witold Drozdzynski, Hans-Ulrich Krieger, Jakub Piskorski,
Ulrich Scha?fer, and Feiyu Xu. 2004. Shallow processing
with unification and typed feature structures ? foundations
and applications. Ku?nstliche Intelligenz, 1:17?23.
Brigitte Krenn. 2008. Responsive artificial situated cognitive
agents living and learning on the internet, April. Poster
presented at CogSys 2008.
Marc Schro?der and Anna Hunecke. 2007. Mary tts partici-
pation in the Blizzard Challenge 2007. In Proceedings of
the Blizzard Challenge 2007, Bonn, Germany.
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2007. A seed-
driven bottom-up machine learning framework for extract-
ing relations of various complexity. Proceedings of ACL-
2007, pages 584?591.
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2008. Task driven
coreference resolution for relation extraction. In Proceed-
ings of ECAI 2008, Patras, Greece.
Feiyu Xu, Peter Adolphs, Hans Uszkoreit, Xiwen Cheng, and
Hong Li. 2009. Gossip galore: A conversational web
agent for collecting and sharing pop trivia. In Joaquim
Filipe, Ana Fred, and Bernadette Sharp (eds). Proceed-
ings of ICAART 2009, Porto, Portugal.
16
Proceedings of the ACL 2010 System Demonstrations, pages 36?41,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Talking NPCs in a Virtual Game World
Tina Klu?wer, Peter Adolphs, Feiyu Xu, Hans Uszkoreit, Xiwen Cheng
Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)
Projektbu?ro Berlin
Alt-Moabit 91c
10559 Berlin
Germany
{tina.kluewer,peter.adolphs,feiyu,uszkoreit,xiwen.cheng}@dfki.de
Abstract
This paper describes the KomParse sys-
tem, a natural-language dialog system
in the three-dimensional virtual world
Twinity. In order to fulfill the various
communication demands between non-
player characters (NPCs) and users in
such an online virtual world, the system
realizes a flexible and hybrid approach
combining knowledge-intensive domain-
specific question answering, task-specific
and domain-specific dialog with robust
chatbot-like chitchat.
1 Introduction
In recent years multi-user online games in virtual
worlds such as Second Life or World of Warcraft
have attracted large user communities. Such vir-
tual online game worlds provide new social and
economic platforms for people to work and inter-
act in. Furthermore, virtual worlds open new per-
spectives for research in the social, behavioral, and
economic sciences, as well as in human-centered
computer science (Bainbridge, 2007). Depending
on the game type, non-player characters (NPCs)
are often essential for supporting the game plot,
for making the artificial world more vivid and ulti-
mately for making it more immersive. In addition,
NPCs are useful to populate new worlds by carry-
ing out jobs the user-led characters come in touch
with. The range of functions to be filled by NPCs
is currently still strongly restricted by their limited
capabilities in autonomous acting and communi-
cation. This shortcoming creates a strong need for
progress in areas such as AI and NLP, especially
their planning and dialog systems.
The KomParse system, described in this paper,
provides NPCs for a virtual online world named
Twinity, a product of the Berlin startup company
Metaversum1. The KomParse NPCs offer vari-
ous services through conversation with game users
using question-answering and dialog functional-
ity. The utilization of Semantic Web technology
with RDF-encoded generic and domain-specific
ontologies furthermore enables semantic search
and inference.
This paper is organized as follows: Section 2
presents the NPC modelling and explains the ap-
plication scenarios. Section 3 details the knowl-
edge representation and semantic inference in our
system. Section 4 explains the system architecture
and its key components. Section 5 describes the
KomParse dialog system. Section 7 gives a con-
clusion and closes off with our future work.
2 Application Scenario and NPC
Modelling
The online game Twinity extends the Second Life
idea by mirroring an urban part of the real world.
At the time of this writing, the simulated section of
reality already contains 3D models of the cities of
Berlin, Singapore and London and it keeps grow-
ing. Users can log into the virtual world, where
they can meet other users and communicate with
them using the integrated chat function or talk
to each other via Voice-over-IP. They can style
their virtual appearance, can rent or buy their own
flats and decorate them as to their preferences and
tastes.
Out of many types of NPCs useful for this appli-
cation such as pedestrians, city guides and person-
nel in stores, restaurants and bars, we start with
two specific characters: a female furniture sales
agent and a male bartender. The furniture seller
is designed for helping users furnish their virtual
apartments. Users can buy pieces of furniture and
room decoration from the NPC by describing their
demands and wishes in a text chat. During the di-
1http://www.metaversum.com/
36
Figure 1: The furniture sales NPC selling a sofa
alog with the NPC, the preferred objects are then
selected and directly put into a location in the
apartment, which can be further refined with the
user interfaces that Twinity provides.
In the second scenario, the bartender sells vir-
tual drinks. He can talk about cocktails with users,
but moreover, he can also entertain his guests by
providing trivia-type information about popular
celebrities and various relations among them.
We chose these two characters not only because
of their value for the Twinity application but also
for our research goals. They differ in many in-
teresting aspects. First of all, the furniture sales
agent is controlled by a complex task model in-
cluding ontology-driven and data-driven compo-
nents to guide the conversation. This agent also
possesses a much more fine-grained action model,
which allows several different actions to cover
the potential conversation situations for the sell-
ing task. The bartender agent on the other hand is
designed not to fulfill one strict task because his
clients do not follow a specific goal except order-
ing drinks. Our bartender has the role of a conver-
sation companion and is able to entertain clients
with his broad knowledge. Thus, he is allowed to
access to several knowledge bases and is able to
handle questions (and later conversations) about
a much larger domain called the ?gossip domain?
which enables conversation about pop stars, movie
actors and other celebrities as well as the relations
between these people. In order to achieve a high
robustness, we integrate a chatbot into the bar-
tender agent to catch chitchat utterances we cannot
handle.
Figure 2: Our bartender NPC in his bar in Twinity
3 Knowledge Representation and
Semantic Inference
Semantic Web technology is employed for mod-
elling the knowledge of the NPCs. The Resource
Description Format (RDF) serves as the base for
the actual encoding. An RDF statement is a binary
relation instance between two individuals, that is a
triple of a predicate and two arguments, called the
subject and the object, and written as subj pred obj
(e.g. f:Sofa Alatea f:hasMainColour
f:Burgundy).
All objects and properties the NPC can talk
about are modelled in this way. Therefore the
knowledge base has to reflect the physical prop-
erties of the virtual objects in Twinity as faithfully
as possible. For instance, specific pieces of furni-
ture are described by their main color, material or
style, whereas cocktails are characterized by their
ingredients, color, consistence and taste. Further-
more, references to the 3D models of the objects
are stored in order to create, find and remove such
objects in the virtual world.
The concepts and individuals of the particular
domain are structured and organized in domain-
specific ontologies. These ontologies are mod-
elled in the Web Ontology Language (OWL).
OWL allows us to define concept hierarchies, re-
lations between concepts, domains and ranges of
these relations, as well as specific relation in-
stances between instances of a concept. Our on-
tologies are defined by the freely available ontol-
ogy editor Prote?ge? 4.02. The advantage of using an
ontology for structuring the domain knowledge is
2http://protege.stanford.edu/, as accessed
27 Oct 2009
37
Twinity
Server
KomParse
Server
Twinity
Client
Conversational
AgentConversational
AgentConversational
Agent
Twinity
ClientTwinity
Client
Figure 3: Overall System Architecture ? Server/Client Architecture for NPC Control
the modular non-redundant encoding. When com-
bined with a reasoner, only a few statements about
an individual have to be asserted explicitely, while
the rest can be inferred from the ontology. We em-
ploy several ontologies, among which the follow-
ing are relevant for modelling the specific domains
of our NPCs:
? An extensive furniture ontology, created by
our project partner ZAS Berlin, defining
kinds of furniture, room parts, colors and
styles as well as the specific instances of fur-
niture in Twinity. This knowledge base con-
tains 95,258 triples, 123 furniture classes, 20
color classes, 243 color instances and various
classes defining styles and similar concepts.
? A cocktail ontology, defining 13 cocktail
classes with ingredients and tastes in 21,880
triples.
? A biographical ontology, the ?gossip on-
tology?, defining biographical and career-
specific concepts for people. This ontology is
accompanied by a huge database of celebri-
ties, which has been automatically acquired
from the Web and covers nearly 600,000 per-
sons and relations between these people like
family relationships, marriages and profes-
sional relations. (Adolphs et al, 2010)
The furniture ontology is the only knowledge
base for the furniture sales agent, whereas the bar-
tender NPC has access to both the cocktail as well
as the gossip knowledge base.
We use SwiftOwlim3 for storing and querying
the data. SwiftOwlim is a ?triple store?, a kind
of database which is specifically built for storing
and querying RDF data. It provides a forward-
chaining inference engine which evaluates the
domain definitions when loading the knowledge
repository, and makes implicit knowledge explicit
by asserting triples that must also hold true accord-
ing to the ontology. Once the reasoner is finished,
the triple store can be queried directly using the
RDF query language SPARQL.
3http://www.ontotext.com/owlim/
4 Overall System Architecture
Figure 3 shows the overall system architecture.
Twinity is a server/client application, in which the
server hosts the virtual world and coordinates the
user interactions. In order to use Twinity, users
have to download the Twinity client. The client
allows the user to control the physical represen-
tation of the user?s character in the virtual world,
also called the ?avatar?. Thus the client is respon-
sible for displaying the graphics, calculating the
effects of physical interactions, handling the user?s
input and synchronizing the 3D data and user ac-
tions with the Twinity server.
Each NPC comprises two major parts: whereas
its avatar is the physical appearance of the NPC in
the virtual world, the ?conversational agent? pro-
vides the actual control logic which controls the
avatar autonomously. It is in particular able to hold
a conversation with Twinity users in that it reacts
to a user?s presence, interprets user?s utterances in
dialog context and generates adequate responses.
The KomParse server is a multi-client, multi-
threaded server written in Java that hosts the con-
versational agents for the NPCs (section 5). The
NPC?s avatar, on the other hand, is realized by a
modified Twinity client. We utilize the Python in-
terface provided by the Twinity client to call our
own plugin which opens a bidirectional socket
connection to the KomParse server. The plugin is
started together with the Twinity client and serves
as a mediator between the Twinity server and the
KomParse server from then on (fig. 3). It sends all
in-game events relevant to our system to the server
and translates the commands sent by the server
into Twinity-specific actions.
The integration architecture allows us to be
maximally independent of the specific game plat-
form. Rather than using the particular program-
ming language and development environment of
the platform for realizing the conversational agent
or reimplementing a whole client/server proto-
col for connecting the avatar to the corresponding
agent, we use an interface tailored to the specific
needs of our system. Thus the KomParse system
38
can be naturally extended to other platforms since
only the avatar interfaces have to be adapted.
The integration architecture also has the advan-
tage that the necessary services can be easily dis-
tributed in a networked multi-platform environ-
ment. The Twinity clients require aMicrosoftWin-
dows machine with a 3D graphics card supporting
DirectX 9.0c or higher, 1 GB RAM and a CPU
core per instance. The KomParse server requires
roughly 1 GB RAM. The triple store is run as
a separate server process and is accessed by an
XML-RPC interface. Roughly 1.2 GB RAM are
required for loading our current knowledge base.
5 Conversational Agent: KomParse
Dialog System
Figure 4: Dialog System: Conversational Agent
The KomParse dialog system, the main func-
tionality of the conversational agent, consists of
the following three major components: input ana-
lyzer, dialog manager and output generator (fig.4).
The input analyzer is responsible for the lin-
guistic analysis of the user?s textual input includ-
ing preprocessing such as string cleaning, part-of-
speech tagging, named entity recognition, parsing
and semantic interpretation. It yields a semantic
representation which is the input for the dialog
manager.
The dialog manager takes the result of the input
analyzer and delivers an interpretation based on
the dialog context and the available knowledge. It
also controls the task conversation chain and han-
dles user requests. The dialog manager determines
the next system action based on the interpreted pa-
rameters.
The output generator realizes the action defined
by the dialog manager with its multimodal gener-
ation competence. The generated results can be
verbal, gestural or a combination of both.
As mentioned above, our dialog system has to
deal with two different scenarios. While the fo-
cal point of the bartender agent lies in the question
answering functionality, the furniture sales agent
is driven by a complex dialog task model based on
a dialog graph. Thus, the bartender agent relies
mainly on question answering technology, in that
it needs to understand questions and extract the
right answer from our knowledge bases, whereas
the sales agent has to accommodate various dialog
situations with respect to a sales scenario. It there-
fore has to understand the dialog acts intended
by the user and trigger the corresponding reac-
tions, such as presenting an object, memorizing
user preferences, negotiating further sales goals,
etc.
The task model for sales conversations is in-
spired by a corpus resulting from the annotation of
a Wizard-of-Oz experiment in the furniture sales
agent scenario carried out by our project partner at
ZAS (Bertomeu and Benz, 2009). In these exper-
iments, 18 users spent one hour each on furnish-
ing a virtual living room in a Twinity apartment by
talking to a human wizard controlling the virtual
sales agent. The final corpus consists of 18 di-
alogs containing 3,171 turns with 4,313 utterances
and 23,015 alpha-numerical strings (words). The
following example shows a typical part of such a
conversation:
USR.1: And do we have a little side table for the TV?
NPC.1: I could offer you another small table or a sideboard.
USR.2: Then I?ll take a sideboard thats similar to my shelf.
NPC.2: Let me check if we have something like that.
Table 1: Example Conversation from the Wizard-
of-Oz Experiment
The flow of the task-based conversation is con-
trolled by a data-driven finite-state model, which
is the backbone of the dialog manager. During
a sales conversation, objects and features of ob-
jects mentioned by the NPC and the user are ex-
tracted from the knowledge bases and added into
the underspecified graph nodes and egdes at run-
time. This strategy keeps the finite-state graph as
small as possible. Discussed objects and their fea-
tures are stored in a frame-based sub-component
named ?form?. The form contains entries which
correspond to ontological concepts in the furni-
39
ture ontology. During conversation, these entries
will be specified with the values of the properties
of the discussed objects. This frame-based ap-
proach increases the flexibility of the dialog man-
ager (McTear, 2002) and is particularly useful for
a task-driven dialog system. As long as the negoti-
ated object is not yet fully specified, the form rep-
resents the underspecified object description ac-
cording to the ontology concept. Every time the
user states a new preference or request, the form
is enriched with additional features until the set of
objects is small enough to be presented to the user
for final selection. Thus the actual flow of dia-
log according to the task model does not have to
be expressed by the graph but can be derived on
demand from the knowledge and handled by the
form which in turn activates the appropriate dia-
log subgraphs. This combination of graph-based
dialog models and form-based task modelling ef-
fectively accounts for the interaction of sequential
dialog strategies and the non-sequential nature of
complex dialog goals.
Given a resolved semantic representation, the
dialog manager triggers either a semantic search
in the knowledge bases to deliver factual answers
as needed in a gossip conversation or a further di-
alog response for example providing choices for
the user in a sales domain. The semantic search is
needed in both domains. In case that the semantic
representation can neither be resolved in the task
domain nor in the gossip domain, it gets passed to
the embedded A.L.I.C.E. chatbot that uses its own
understanding and generation components (Wal-
lace and Bush, 2001).
5.1 Semantic Representation
The input understanding of the system is imple-
mented as one single understanding pipeline.The
understanding pipeline delivers a semantic repre-
sentation which is the basis for the decision of the
dialog manager which action to perform next.
This semantic representation can be extracted
from the user input by our understanding com-
ponent via a robust hybrid approach: either via a
number of surface patterns containing regular ex-
pressions or via patterns reflecting the syntactic
analysis of a dependency parser (de Marneffe and
Manning, 2008).
The representation?s structure is inspired by our
knowledge representation design described in sec-
tion 3 as well as by predicate logic. The core of the
representation is a predicate-argument structure
limited to two arguments including message type
and the whole syntactic information found by the
analysis pipeline. The field ?Message Type? can
have one of the following values: wh-question,
yes/no-question, declarative. Predicates can often
be instantiated with the lemmatized matrix verb of
the successfully analysed piece of the input. If the
input contains a wh-question, the questioned fact
is marked as an unfilled argument slot. The gen-
eral structure can be simplified described as:
<PREDICATE, ARG1, ARG2, [message-type]>
The following examples show the structure used
for different input:
? ?Who is the boyfriend of Madonna??
<hasBoyfriend, Madonna, ?, [wh]>
? ?I want to buy a sofa.?
<buy, I, "a sofa", [declarative]>
5.2 Information Extraction
Both scenarios make use of state-of-the-art infor-
mation extraction approaches to extract the impor-
tant pieces from the user input. While the bar-
tender depends on relation extraction to detect the
fact or relation questioned by the user (Xu et al,
2007), the sales agent uses information extraction
methods to recognize user wishes and demands.
As a result, the questioned fact or the demanded
object feature equals the ontology structure con-
taining the knowledge needed to handle the user
input. The input ?Do you have any red couches??
for example needs to get processed by the system
in such a way that the information regarding the
sofa with red color is extracted.
This is done by the system in a data-driven way.
The input analysis first tries to find a demanded
object in the input via asking the ontology: Every
object which can be discussed in the scenario is
encoded in the sales agents knowledge base. This
can be seen as a Named Entity Recognition step.
In case of success, the system tries to detect one
of the possible relations of the object found in the
input. This is achieved by querying the ontology
about what kind of relations the identified object
can satisfy. Possible relations are encoded in the
class description of the given object. As a result
the system can detect a relation ?hasColour? for
the found object ?sofa? and the color value ?red?.
The found information gets inserted into the form
which gets more and more similar or if possible
equal to a search query via RDF.
40
Figure 5: Comparison of Input, Extracted Information and Knowledge Base
6 Conclusion and Future Work
The KomParse system demonstrates an attractive
application area for dialog systems that bears great
future potential. Natural language dialog with
NPCs is an important factor in making virtual
worlds more interesting, interactive and immer-
sive. Virtual worlds with conversing characters
will also find many additional applications in edu-
cation, marketing, and entertainment.
KomParse is an ambitious and nevertheless
pragmatic attempt to bring NLP into the world of
virtual games. We develop a new strategy to inte-
grate task models and domain ontologies into dia-
log models. This strategy is useful for task-driven
NPCs such as furniture sellers. With the chatty
bartender, a combination of task-specific dialog
and domain-specific question answering enables a
smart wide-domain off-task conversation. Since
the online game employs bubble-chat as a mode
of communication in addition to Voice-over-IP, we
are able to test our dialog system in a real-time
application without being hindered by imperfect
speech recognition.
The system presented here is still work in
progress. The next goals will include various eval-
uation steps. On the one hand we will focus on
single components like hybrid parsing of input ut-
terances and dialog interpretation in terms of pre-
cision and recall. On the other hand an evaluation
of the two different scenarios regarding the us-
ability are planned in experiments with end users.
Moreover we will integrate some opinion mining
and sentiment analysis functionality which can be
helpful to better detect and understand the user?s
preferences in the furniture sales agents scenario.
Acknowledgements
The project KomParse is funded by the ProFIT
programme of the Federal State of Berlin, co-
funded by the EFRE programme of the Euro-
pean Union. The research presented here is ad-
ditionally supported through a grant to the project
TAKE, funded by the German Ministry for Edu-
cation and Research (BMBF, FKZ: 01IW08003).
Many thanks go to our project partners at the Cen-
tre for General Linguistics (ZAS) in Berlin as well
as to the supporting company Metaversum.
References
Peter Adolphs, Xiwen Cheng, Tina Klu?wer, Hans
Uszkoreit, and Feiyu Xu. 2010. Question answering
biographic information and social network powered
by the semantic web. In Proceedings of LREC 2010,
Valletta, Malta.
William Sims Bainbridge. 2007. The scientific re-
search potential of virtual worlds. Science, 317.
Nuria Bertomeu and Anton Benz. 2009. Annotation of
joint projects and information states in human-npc
dialogues. In Proceedings of the First International
Conference on Corpus Linguistics (CILC-09), Mur-
cia, Spain.
Marie C. de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, Manchester, UK.
Michael F. McTear. 2002. Spoken dialogue tech-
nology: enabling the conversational user interface.
ACM Comput. Surv., 34(1).
Richard Wallace and Noel Bush. 2001. Artificial
intelligence markup language (aiml) version 1.0.1
(2001). Unpublished A.L.I.C.E. AI Foundation
Working Draft (rev 006).
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 584?
591, Prague, Czech Republic, June. Association for
Computational Linguistics.
41
