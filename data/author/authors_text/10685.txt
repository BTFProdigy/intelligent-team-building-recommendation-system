R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 357 ? 365, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Finding Taxonomical Relation from an MRD for 
Thesaurus Extension 
SeonHwa Choi and HyukRo Park 
Dept. of Computer Science,  
Chonnam National University,  
300 Youngbong-Dong, Puk-Ku Gwangju, 500-757, Korea 
csh123@dreamwiz.com, hyukro@chonnam.ac.kr 
Abstract. Building a thesaurus is very costly and time-consuming task. To 
alleviate this problem, this paper proposes a new method for extending a 
thesaurus by adding taxonomic information automatically extracted from an 
MRD. The proposed method adopts a machine learning algorithm in acquiring 
rules for identifying a taxonomic relationship to minimize human-intervention. 
The accuracy of our method in identifying hypernyms of a noun is 89.7%, and 
it shows that the proposed method can be successfully applied to the problem of 
extending a thesaurus.  
1   Introduction 
As the natural language processing (NLP) systems became large and applied to wide 
variety of application domains, the need for a broad-coverage lexical knowledge-base 
has increased more than ever before. A thesaurus, as one of these lexical knowledge-
bases, mainly represents a taxonomic relationship between nouns. However, because 
building broad-coverage thesauri is a very costly and time-consuming job, they are 
not readily available and often too general to be applied to a specific domain.  
The work presented here is an attempt to alleviate this problem by devising a new 
method for extending a thesaurus automatically using taxonomic information 
extracted from a machine readable dictionary (MRD).  
Most of the previous approaches for extracting hypernyms of a noun from the 
definition in an MRD rely on the lexico-syntactic patterns compiled by human experts. 
Not only these methods require high cost for compiling lexico-syntactic patterns but 
also it is very difficult for human experts to compile a set of lexical-syntactic patterns 
with a broad-coverage because, in natural languages, there are various different 
expressions which represent the same concept. Accordingly the applicable scope of a set 
of lexico-syntactic patterns compiled by human is very limited.  
To overcome the drawbacks of human-compiled lexico-syntactic patterns, we use 
part-of-speech (POS) patterns only and try to induce these patterns automatically 
using a small bootstrapping thesaurus and machine learning methods. 
The rest of the paper is organized as follows. We introduce the related works in 
section 2. Section 3 deals with the problem of features selection. In section 4, our 
problem is formally defined as a machine learning method and discuss 
implementation details. Section 5 is devoted to experimenal result. Finally, we come 
to the conclusion of this paper in section 6. 
358 S.H. Choi and H.R. Park 
2   Related work 
[3] introduced a method for the automatic acquisition of the hyponymy lexical 
relation from unrestricted text, and gave several examples of lexico-syntactic patterns 
for hyponymy that can be used to detect these relationships including those used here, 
along with an algorithm for identifying new patterns. Hearst?s approach is 
complementary to statistically based approaches that find semantic relations between 
terms, in that hers requires a single specially expressed instance of a relation while the 
others require a statistically significant number of generally expressed relations. The 
hyponym-hypernym pairs found by Hearst?s algorithm include some that she 
describes as ?context and point-of-view dependent?, such as ?Washington/nationalist? 
and ?aircraft/target?. [4] was somewhat less sensitive to this kind of problem since 
only the most common hypernym of an entire cluster of nouns is reported, so much of 
the noise is filtered. [3] tried to discover new patterns for hyponymy by hand, 
nevertheless it is a costly and time-consuming job. In the case of [3] and [4], since the 
hierarchy was learned from text, it got to be domain-specific different from a general-
purpose resource such as WordNet. 
[2] proposed a method that combines a set of unsupervised algorithms in order to 
accurately build large taxonomies from any MRD, and a system that 1)performs fully 
automatic extraction of a taxonomic link from MRD entries and 2) ranks the extracted 
relations in a way that selective manual refinement is allowed. In this project, they 
introduced the idea of the hyponym-hypernym relationship appears between the entry 
word and the genus term. Thus, usually a dictionary definition is written to employ a 
genus term combined with differentia which distinguishes the word being defined 
from other words with the same genus term. They found the genus term by simple 
heuristic defined using several examples of lexico-syntactic patterns for hyponymy. 
[1] presented the method to extract semantic information from standard dictionary 
definitions. Their automated mechanism for finding the genus terms is based on the 
observation that the genus term from verb and noun definitions is typically the head 
of the defining phrase. The syntax of the verb phrase used in verb definitions makes it 
possible to locate its head with a simple heuristic: the head is the single verb 
following the word to. He asserted that heads are bounded on the left and right by 
specific lexical defined by human intuition, and the substring after eliminating 
boundary words from definitions is regarded as a head. 
By the similar idea to [2], [10] introduced six kinds of rule extracting a hypernym 
from Korean MRD according to a structure of a dictionary definition. In this work, 
Moon proposed that only a subset of the possible instances of the hypernym relation 
will appear in a particular form, and she divides a definition sentence into a head term 
combined with differentia and a functional term. For extracting a hypernym, Moon 
analyzed a definition of a noun by word list and the position of words, and then 
searched a pattern coinciding with the lexico-syntactic patterns made by human 
intuition in the definition of any noun, and then extracted a hypernym using an 
appropriate rule among 6 rules. For example, rule 2 states that if a word X occurs in 
front of a lexical pattern ?leul bu-leu-deon i-leum ( the name to call )?,then X is 
extracted as a hypernym of the entry word. 
Several approaches[11][12][13] have been researched for building a semantic 
hierarchy of Korean nouns adopting the method of [2]. 
 Finding Taxonomical Relation from an MRD for Thesaurus Extension 359 
3   Features for Hypernym Identification 
Machine learning approaches require an example to be represented as a feature vector. 
How an example is represented or what features are used to represent the example has 
profound impact on the performance of the machine learning algorithms. This section 
deals with the problems of feature selection with respect to characteristics of Korean 
for successful identification of hypernyms. 
Location of a word. In Korean, a head word usually appears after its modifying 
words. Therefore a head word has tendency to be located at the end of a sentence. In 
the definition sentences in a Korean MRD, this tendency becomes much stronger. In 
the training examples, we found that 11% of the hypernyms appeared at the start, 81% 
of them appeared at the end and 7% appeared at the middle of a definition sentence. 
Thus, the location of a noun in a definition sentences is an important feature for 
determining whether the word is a hypernym or not. 
POS of a function word attached to a noun. Korean is an agglutinative language in 
which a word-phrase is generally a composition of a content word and some 
number of function words. A function word denotes the grammatical relationship 
between word-phrases, while a content word contains the central meaning of the 
word-phrase.  
In the definition sentences, the function words which attached to hypernyms are 
confined to a small number of POSs. For example, nominalization endings, objective 
case postpositions come frequently after hypernyms but dative postpositions or 
locative postpositions never appear after hypernyms. A functional word is appropriate 
feature for identifying hypernyms. 
Context of a noun. The context in which a word appears is valuable information and 
a wide variety of applications such as word clustering or word sense disambiguation 
make use of it. Like in many other applications, context of a noun is important in 
deciding hyperhyms too because hypernyms mainly appear in some limited context. 
Although lexico-syntactic patterns can represent more specific contexts, building 
set of lexco-syntactic patterns requires enormous training data. So we confined 
ourselves only to syntactic patterns in which hypernyms appear.  
We limited the context of a noun to be 4 word-phrases appearing around the noun. 
Because the relations between word-phrases are represented by the function words of 
these word-phrases, the context of a noun includes only POSs of the function words 
of the neighboring word-phrases. When a word-phrase has more than a functional 
morpheme, a representative functional morpheme is selected by an algorithm 
proposed by [8].   
When a noun appears at the start or at the end of a sentence, it does not have right 
or left context respectively. In this case, two treatments are possible. The simplest 
approach is to treat the missing context as don?t care terms. On the other hand, we 
could extend the range of available context to compensate the missing context. For 
example, the context of a noun at the start of a sentence includes 4 POSs of function 
words in its right-side neighboring word-phrases. 
360 S.H. Choi and H.R. Park 
4   Learning Classification Rules 
Decision tree learning is one of the most widely used and a practical methods for 
inductive inference such as ID3, ASSISTANT, and C4.5[14]. Because decision tree 
learning is a method for approximating discrete-valued functions that is robust to 
noisy data, it has therefore been applied to various classification problems 
successfully. 
Our problem is to determine for each noun in definition sentences of a word 
whether it is a hypernym of the word or not. Thus our problem can be modeled as 
two-category classification problem. This observation leads us to use a decision tree 
learning algorithm C4.5. 
Our learning problem can be formally defined as followings: 
? Task T : determining  whether a noun is a hypernym of an entry word  or not . 
? Performance measure P : percentage of nouns correctly classified. 
? Training examples E : a set of nouns appearing in the definition sentences of 
the MRD with their feature vectors and target values. 
To collect training examples, we used a Korean MRD provided by Korean 
TermBank Project[15] and a Korean thesaurus compiled by Electronic 
Communication Research Institute. The dictionary contains approximately 220,000 
nouns with their definition sentences while the thesaurus has approximately 120,000 
nouns and taxonomy relations between them. The fact that 46% of nouns in the 
dictionary are missing from the thesaurus shows that it is necessary to extend a 
thesaurus using an MRD. 
Using the thesaurus and the MRD, we found that 107,000 nouns in the thesaurus 
have their hypernyms in the definition sentences in the MRD. We used 70% of these 
nouns as training data and the remaining 30% of them as evaluation data.  
For each training pair of hypernym/hyponym nouns, we build a triple in the form 
of (hyponym definition-sentences hypernym) as follows. 
ga-gyeong [ a-leum-da-un gyeong-chi (a beautiful scene)] gyeong-chi 
     hyponym                        definition sentence                                        hypernym 
Morphological analysis and Part-Of-Speech tagging are applied to the definition 
sentences. After that, each noun appearing in the definition sentences is converted 
into a feature vector using features mentioned in section 3 along with a target value 
(i.e. whether this noun is a hypernym of the entry word or not). 
Table 1 shows some of the training examples. In this table, the attribute 
IsHypernym which can have a value either Y or N is a target value for given noun. 
Hence the purpose of learning is to build a classifier which will predict this value for 
a noun unseen from the training examples.  
In Table 1, Location denotes the location of a noun in a definition sentence. 0 
indicates that the noun appears at the start of the sentence, 1 denotes at the middle of 
the sentence, and 2 denotes at the end of a sentence respectively. FW of a hypernym is 
the POS of a function word attachted to the noun and context1,...,context4 denote the 
POSs of  function words appearing to the right/left of the noun. ?*? denotes a don?t 
care condition. The meanings of POS tags are list in Appendix A. 
 Finding Taxonomical Relation from an MRD for Thesaurus Extension 361 
Table 1. Some of training examples 
Noun Location FW of a 
hypernym 
context1 context2 context3 context4 IsHypernym 
N1 1 jc ecx exm nq * Y 
N2 2 * exm ecx jc nq Y 
N3 2 * exm jc nca exm Y 
N4 1 exm jc jc ecx m N 
N5 1 jc jc ecx m jca N 
N6 1 jc ecx m jca exm Y 
N7 2 * exm exm jca exm Y 
N8 1 * nc jca exm jc N 
N9 1 jca nc nc nc jc Y 
N10 2 exn a nca jc nca Y 
.. .. .. .. .. .. .. .. 
Fig. 1 shows a part of decision tree learned by C4.5 algorithm. From this tree, we 
can easily find that the most discriminating attribute is Location while the least one  
is Context. 
 
Fig. 1. A learned decision tree for task T 
5   Experiment 
To evaluate the proposed method, we measure classification accuracy as well as 
precision, recall, and F-measure which are defined as followings respectively. 
recallprecision
recallprecisionMeasureF
ca
a
recall
ba
aprecision
dcba
da
accuracytionclassifica
+
=?
+
=
+
=
+++
+
=
**2
 
362 S.H. Choi and H.R. Park 
Table 2. Contingency table for evaluating a binary classifier 
 Yes is correct No is correct 
Yes was assigned a b 
No was assigned c d 
Table 3. Evaluation result 
 
Classification  
accuracy Precesion Recall F-Measure 
A 91.91% 95.62% 92.55% 94.06% 
B 92.37% 93.67% 95.23% 94.44% 
C 89.75% 83.83% 89.92% 86.20% 
Table 4. Evaluation result 
Proposed Y.J.Moon 96[10] 
 
A B 
M.S.Kim 
95[11] C D 
Y.M.Choi 
98[13] 
Classification 
Accuracy 91.91% 92.37% 88.40% 88.40% 68.81% 89.40% 
Table 3 shows the performance of the proposed approach. We have conducted two 
suite of experiments. The purpose of the first suite of experiment is to measure the 
performance differences according to the different definitions for the context of a 
word. In the experiment denoted A in table 3, the context of a word is defined as 4 
POSs of the function words, 2 of them immediately proceeding and 2 of them 
immediately following the word. In the experiment denoted B, when the word appears 
at the beginning of a sentence or at the end of a sentence, we used only right or left 
context of the word respectively. Our experiement shows that the performance of B is 
slightly better than that of A. 
In the second suite of experiment, we measure the performance of our system for 
nouns which do not appear in the thesaurus. This performance can give us a figure about 
how well our system can be applied to the problem of extending a thesaurus. The result 
is shown in Table 3 in the row labeled with C.  As we expected, the performance is 
droped slightly, but the difference is very small. This fact convince us that the proposed 
method can be successfully applied to the problem of extending a thesuarus.   
Table 4 compares the classification accuracy of the proposed method with those of 
the previous works. Our method outperforms the performance of the previous works 
reported in the literature[10] by 3.51%.  
Because the performance of the previous works are measured with small data in a 
restricted domain, we reimplemented one of the those previous works[10] to compare 
the performances using same data. The result is shown in Table 4 under the column 
marked D. Column C is the performance of the [10] reported in the literature. This 
 Finding Taxonomical Relation from an MRD for Thesaurus Extension 363 
result shows that as the heuristic rules in [10] are dependent on lexical information, if 
the document collection is changed or the application domain is changed, the 
performance of the method degrades seriously. 
6   Conclusion 
To extend a thesaurus, it is necessary to identify hypernyms of a noun. There have 
been several works to build taxonomy of nouns from an MRD. However, most of 
them relied on the lexico-syntactic patterns compiled by human experts.  
This paper has proposed a new method for extending a thesaurus by adding a 
taxonomic relationship extracted from an MRD. The taxonomic relationship is 
identified using nouns appearing in the definition sentences of a noun in the MRD and 
syntactic pattern rules compiled by a machine learning algorithm. 
Our experiment shows that the classification accuracy of the proposed method is 
89.7% for nouns not appearing in the thesaurus.  
Throughout our research, we have found that machine learning approaches to the 
problems of identifying hypernyms from an MRD could be a competitive alternative to 
the methods using human-compiled lexico-syntactic patterns, and such taxonomy 
automatically extracted from an MRD can effectively supplement an existing thesaurus.  
References 
1. Martin S. Chodorow, Roy J. Byrd, George E. Heidorn. : Extracting Semantic Hierarchies 
From A Large On-Line Dictionary. In Proceedings of the 23rd Conference of the 
Association for Computational Linguistics (1985) 
2. Rigau G., Rodriguez H., Agirre E. : Building Accurate Semantic Taxonomies from 
Mololingual MRDs. In Proceedings of the 36th Conference of the Association for 
Computational Linguistics (1998) 
3. Marti A. Hearst. : Automatic acquisition of hyonyms from large text corpora. In 
Proceedings of the Fourteenth International Conference on Computational Linguistics 
(1992) 
4. Sharon A. Caraballo. : Automatic construction of a hypernym-labled noun hierarchy from 
text. In Proceedings of the 37th Conference of the Association for Computational 
Linguistics (1999). 
5. Fernando Pereira, Naftali Thishby, Lillian Lee. : Distributional clustering of English 
words. In Proceedings of the 31th Conference of the Association for Computational 
Linguistics (1993) 
6. Brian Roark, Eugen Charniak. : Noun-phrase co-occurrence statistics for semi-automatic 
semantic lexicon construction. In Proceedings of the 36th Conference of the Association 
for Computational Linguistics and 17th International Conference on Computational 
Linguistics (1998) 
7. Tom M. Mitchell.: Machine Learning. Carnegie Mellon University. McGraw-Hill (1997). 
8. SeonHwa Choi, HyukRo Park. : A New Method for Inducing Korean Dependency 
Grammars reflecting the Characteristics of Korean Dependency Relations. In Proceedings 
of the 3rd Conterence on East-Asian Language Processing and Internet Information 
Technology (2003) 
9. YooJin Moon, YeongTak Kim. :The Automatic Extraction of Hypernym in Korean. In 
Preceedings of Korea Information Science Society Vol. 21, NO. 2 (1994) 613-616 
364 S.H. Choi and H.R. Park 
10. YooJin Moon. : The Design and Implementation of WordNet for Korean Nouns. In 
Proceedings of Korea Information Science Society (1996) 
11. MinSoo Kim, TaeYeon Kim, BongNam Noh. : The Automatic Extraction of Hypernyms 
and the Development of WordNet Prototype for Korean Nouns using Koran MRD. In 
Proceedings of Korea Information Processing Society (1995) 
12. PyongOk Jo, MiJeong An, CheolYung Ock, SooDong Lee. : A Semantic Hierarchy of 
Korean Nouns using the Definitions of Words in a Dictionary. In Proceedings of Korea 
Cognition Society (1999) 
13. YuMi Choi and SaKong Chul. : Development of the Algorithm for the Automatic 
Extraction of Broad Term. In Proceedings of Korea Information Management Society 
(1998) 227-230 
14. Quinlan J. R.: C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufman 
(1993)  http://www.rulequest.com/Personal/ 
15. KORTERM. : KAIST language resources http://www.korterm.or.kr/ 
Appendix A. POS Tag Set 
Table 5. POS tag set 
CATEGORY  TAG DESCRIPTION 
noun common nn common noun 
  nca active common noun 
  ncs statove common noun 
  nct time common noun 
 proper nq proper noun 
 bound nb bound noun 
  nbu unit bound noun 
 numeral nn numeral 
 pronoun npp personal pronoun 
  npd demonstrative pronoun 
predicate verb pv verb 
 adjective pa adjective 
  pad demonstrative adjective 
 auxiliary px auxiliary verb 
modification adnoun m adnoun 
  md demonstrative adnoun 
  mn numeral adnoun 
 adverb a general adverb 
  ajs sentence conjunctive adverb 
  ajw word conjunctive adverb 
  ad demonstrative adverb 
independence interjection ii interjection 
particle case jc case 
  jca adverbial case particle 
  jcm adnominal case particle 
  jj conjunctive case particle 
  jcv vocative case particle 
 Finding Taxonomical Relation from an MRD for Thesaurus Extension 365 
CATEGORY  TAG DESCRIPTION 
 auxiliary jx auxiliary 
 predicative jcp predicative particle 
ending prefinal efp prefinal ending 
 conjunctive ecq coordinate conjunctive ending 
  ecs subordinate conjunctive ending 
  ecx auxiliary conjunctive ending 
 transform exn nominalizing ending 
  exm adnominalizing ending 
  exa adverbalizing ending 
 final ef final ending 
affix prefix xf prefix 
 suffix xn suffix 
  xpv verb-derivational suffix 
  xpa adjective-derivational suffix 
 
Significant Sentence Extraction by Euclidean
Distance Based on Singular Value
Decomposition
Changbeom Lee1, Hyukro Park2, and Cheolyoung Ock1
1 School of Computer Engineering & Information Technology, University of Ulsan,
Ulsan 680-749, South Korea
{chblee1225, okcy}@mail.ulsan.ac.kr
2 Department of Computer Science, Chonnam National University, 300,
Youngbong-dong, Buk-gu, Kwangju 500-757, South Korea
hyukro@chonnam.ac.kr
Abstract. This paper describes an automatic summarization approach
that constructs a summary by extracting the significant sentences. The
approach takes advantage of the cooccurrence relationships between
terms only in the document. The techniques used are principal compo-
nent analysis (PCA) to extract the significant terms and singular value
decompostion (SVD) to find out the significant sentences. The PCA can
quantify both the term frequency and term-term relationship in the doc-
ument by the eigenvalue-eigenvector pairs. And the sentence-term matrix
can be decomposed into the proper dimensional sentence-concentrated
and term-concentrated marices which are used for the Euclidean dis-
tances between the sentence and term vectors and also removed the noise
of variability in term usage by the SVD. Experimental results on Korean
newspaper articles show that the proposed method is to be preferred
over random selection of sentences or only PCA when summarization is
the goal.
keywords: Text summarization; Principal component analysis; Singular
value decomposition.
1 Introduction
Automatic text summarization is the process of reducing the length of text
documents, while retaining the essential qualities of the orginal. Many search
engines have tried to solve the problem of information overflowing by showing
either the title and beginning of of a document. However, such the title and
beginning are insufficient to decide the relevance of the documents which user
wants to search, and this is the reason that the text summarization is required
to resolve this problem.
The process of text summarization could consist of two phases: a document
interpretation phase and a summary generation phase. The primary goal of a
document interpretation phase is to find the main theme of a document and its
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 636?645, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Significant Sentence Extraction by Euclidean Distance 637
corresponding significant words. Since the significant words collectively repre-
sent the main theme of a document, it is important to find them more reason-
ably. For the purpose of this doing , the word frequency of a document might
be utilized[4,8]. But this approach is limited in that cooccurrence relationships
among words are not considered at all. In contrast to word frequency, the other
method by using WordNet or a thesaurus[2] makes good use of word relationships
such as NT(narrow term), RT(related term), and so on. However such resources
require a large cost to compile, and often represent too general relationships to
fit a specific domain.
In this paper, we propose a new summarization approach by both princi-
pal component analysis (PCA) and singular value decomposition (SVD) that
are called quantification methods or statistical analysis methods. PCA is uti-
lized to find significant words or terms in the document by term-relationships.
Since the necessary term-relationships can be acquired only from the given doc-
ument by linear transformation of PCA, the proposed method need not exploit
the additional information such as WordNet or a thesaurus. And the SVD is
used to extract the significant sentences. After performing SVD, a sentence-
term matrix is decomposed into three matrices; that is, a sentence-concentrated
matrix, a term-concentrated matrix, and a singular value matrix. The distances
between significant term vectors and sentence vectors can be calculated by using
a sentence-concentrated matrix and a term-concentrated matrix. The shorter
the distance is, the more important the sentence is. In a word, to produce the
summary of a document, we first identify significant terms by the term-term
relationships of being generated by PCA, and second extract the significant
sentences by the distances betweeen significant term vectors and all sentence
vectors.
This paper is organized as follows. Section 2 and 3 describe the way to identify
the significant terms by PCA, and extract the significant sentences by SVD,
respectively. Section 4 reports experimental results. A brief conclusion is given
in Section 5. And this paper enlarges [7] whose main content is to find out the
significant terms by PCA.
2 Significant Term Extraction by PCA
2.1 PCA Overview and Its Application
In this subsection, we will outline PCA which is adapted from [6] and which is
used to extract the significant terms in the document.
PCA is concerned with explaining the variance?covariance structure through
a few linear combinations of the original variables. Its general objective is data
reduction and interpretation. Algebraically, principal components are particular
linear combinations of the p random variables X1, X2, ? ? ? , Xp. Geometrically,
these linear combinations represent the selection of a new coordinate system
obtained by rotating the orginal system with X1, X2, ? ? ? , Xp as the coordinate
axes.
638 C. Lee, H. Park, and C. Ock
PCA uses the covariance matrix (i.e., term-term correlation or cooccurrence
matrix) instead of the obsevation-variable matrix (i.e., sentence-term matrix)
such as Table 2. Let ? be the covariance matrix associated with the random vec-
tor XT = [X1, X2, ? ? ? , Xp]. Let ? have the eigenvalue-eigenvector pairs (?1, e1),
(?2, e2), . . . , (?p, ep) where ?1 ? ?2 ? . . . ?p ? 0 . The ith principal compo-
nent(PC) is given by
Yi = e
T
i X = e1iX1 + e2iX2 + ? ? ? + epiXp, i = 1, 2, . . . , p (1)
The first PC is the linear combination with maximum variance, and has the
widest spread in a new coordinate system geometrically. Consequently, we can
say that it can cover the distribution of term frequency of a document as wide
as possible, and also say that it has the power of explanation of the distribution
as large as possible (but, not considering the meaning).
The PCs are uncorrelated and have variances equal to the eigenvalues of ?,
and the proportion of total population variance due to the ith PC, ?i, is
?i =
?i
?1 + ?2 + . . . + ?p
, i = 1, 2, . . . , p (2)
If most (80 ? 90%) of the total population variance, for large p, can be attributed
to the first one, two, or three components, then these components can ?replace?
the original p variables without much loss of information. The first i PCs have
also maximal mutual information with respect to the inputs among projections
onto all possible i directions, and mutual information is given by I(X, Y ) =
H(X) ? H(X |Y ) [5].
As we expected, all terms of the document are not necessary to represent the
content (i.e., term frequency distribution) of the document by using a few first i
PCs, because they have most of the total population variance and maximal mutual
information as noted earlier. In addition, since PCA exploits a covariance matrix,
we can use the term-term relationships by eigenvalues and eigenvectors without
additional information resources. In the next subsection, we will describe how to
extract the significant terms by eigenvalue-eigenvector pairs of a few first i PCs.
2.2 Extracting Significant Terms by Eigenvalue-Eigenvector Pairs
We assume that the candidates for the significant terms are confined only to
nouns occurred more than 2 times in a document. We also regard the sentences as
observations, the extracted nouns (terms) as variables, and the value of variables
as the number of occurrence of terms in each sentence (cf. cumulative frequency
of the document in [7] ) .
Table 1 shows the term list extracted from one of the Korean newspaper arti-
cles composed of 12 sentences, and all of these terms have the occurrences more
than twice in the document. Since the terms occurred just once are not reason-
able to be representative nouns, we do not consider such terms. In our sample
article, 9 terms are extracted for the cadidates for the significant ones as shown in
Table 1. The sample article has 12 sentences (observations) originally, but there
Significant Sentence Extraction by Euclidean Distance 639
Table 1. Variable (term) list
variable notation
dae-tong-ryeong (president) X1
mun-je (problem) X2
guk-ga (nation) X3
sa-ram (person) X4
bu-jeong-bu-pae (illegality and corruption) X5
gyeong-je (economy) X6
guk-min (people) X7
bang-beop (method) X8
dan-che-jang (administrator) X9
Table 2. Observation-variable (sentence-term) matrix
obs. \ var. X1 X2 X3 X4 X5 X6 X7 X8 X9
1 1 0 0 0 0 0 0 0 0
2 2 1 0 0 0 0 0 0 0
3 0 0 1 1 0 0 0 0 0
4 0 0 1 1 1 0 0 0 0
5 0 0 0 0 1 2 0 0 0
6 0 0 1 0 0 0 1 0 0
7 0 1 0 0 0 0 1 1 0
8 1 1 0 0 0 0 0 0 0
9 0 0 0 0 0 0 0 1 2
are 9 ones in sentence-term matrix as shown in Table 2. The only reason for this
difference was that the sentences, which did not include the 9 extracted terms at
all, were omitted. In Table 2, the column under the head X1 shows the freqeuncy
of X1, that is, X1 occurred once in first sentence, twice in second sentence, once in
eighth sentence. In Table 3, the 9 PCs are obtained after performning PCA with
the 9 variables. The column under the head PC1 shows the eigenvector of the
first PC, for instance,
???
PC1 = (?0.713, ?0.417, ? ? ? , 0.025, 0.111). Its eigenvalue
is 0.871, and its proportion of total population is 32.34% computed by Eq. (2).
There are two steps to extract the significant terms by eigenvales and eigen-
vectors as shown in Table 3. First we need to decide how many PCs are selected,
and second to find out how to express the each selected PC. In order to select a
few most salient PCs, we can make good use of cumulative ratio of eigenvalues.
For example, the first four PCs can justify more than 90% (91.28%) of the total
sample variance, we can choose them without much loss information. In other
words, sample variance is summarized very well by these four PCs and the data
from 9 observations on 9 variables can be reasonably reduced to 9 observations
on 4 PCs. Until now, we could not know what the selected PCs represent ex-
actly, but we could describe them by their coefficients approximately. A PC can
be represented by linear combination of variables multiplied by their respective
coefficient. For instance,
640 C. Lee, H. Park, and C. Ock
Table 3. Eigenvector and corresponding eigenvalue of each PC
var. \ PC PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9
X1 -0.713 0.220 0.068 -0.334 0.131 0.300 0.035 -0.467 0.000
X2 -0.417 -0.003 0.010 0.337 -0.596 0.044 0.518 0.295 0.000
X3 0.303 0.151 -0.490 -0.083 0.144 0.289 0.529 -0.139 -0.485
X4 0.234 0.149 -0.333 -0.292 -0.498 -0.248 0.074 -0.421 0.485
X5 0.278 0.268 0.212 -0.096 -0.396 0.726 -0.309 0.134 0.000
X6 0.281 0.337 0.710 0.135 0.076 -0.151 0.402 -0.310 0.000
X7 0.038 -0.111 -0.181 0.653 0.258 0.375 0.038 -0.288 0.485
X8 0.025 -0.464 0.092 0.236 -0.358 -0.028 -0.236 -0.548 -0.485
X9 0.111 -0.703 0.234 -0.416 0.050 0.267 0.362 0.043 0.243
eigenvalue(?i) 0.871 0.676 0.563 0.349 0.134 0.049 0.035 0.017 0.000
cumulative ratio(%) 32.34 57.42 78.31 91.28 96.25 98.07 99.38 100.00 100.00
PC1 = ?0.713 ? X1 ? 0.417 ? X2 + 0.303 ? X3 + ? ? ? + 0.111 ? X9 (3)
Since the coefficients represent the degree of relationship between variables and
a PC, the variables with coefficient higher than 0.5 can be reasonably used to
express the PC. When the PC has not such coefficient higher than 0.5, the
variable with the highest coefficient can be also used to represent the PC. For
example, in table 3, the correlation coefficient between PC1 and X3 is 0.303,
so X3 can be selected for the description of PC1. As the most part variance of
a document justified by some of the PCs, we selected variables which have a
strong correlation (? 0.5 or highest) with one of these PCs as significant terms.
In our example, the extracted significant terms from PC1 to PC4 are X3, X6
and X7.
3 Significant Sentence Extraction by SVD
3.1 SVD Overview and Its Application
We will give an outline of SVD adapted from [3,9] and how to make good use of
extracting the significant sentences.
Let A be any rectangular matrix, for instance an S ? T matrix of sentences
and terms, such as Table 2. The matrix A can be written as the product of an
S?R column-orthogonal matrix U , an R?R daigonal matrix W with positive or
zero elements (i.e., the singular values), and the transpose of a T ?R orthogonal
matrix V . Here, R is the rank of the matrix A (R ? min(S, T )). The SVD
decompositon is shown in Eq. (4).
A = U ? W ? V T (4)
where UT U = I, V T V = I, and W is the diagonal matrix of singualr val-
ues. In contrast to our usage of SVD, [3] used term-document matrix: our
Significant Sentence Extraction by Euclidean Distance 641
sentence-term matrix can be regarded as the transpose of term-document ma-
trix, since the documents can be thought of the sentences in the summarization
fields.
In this regard, the matrix A can be regarded as the sentence-term matrix
like Table 2, U as the sentence-concentrated matrix whose number of rows is
equal to the number of rows of the matrix A, and V as the term-concentrated
matrix whose number of rows is equal to the number of columns of the matrix
A. Then, the sentence vector si is defined as si = (ui1, ui2, ? ? ? , uiR) where R is
the rank of the matrix A. As before, the vector for a term tj is represented by
tj = (vj1, vj2, ? ? ? , vjR). Consequently, both the sentence and term vectors can
be used to calculate the distances between them.
Actually the reduced dimensionality can be used instead of the full rank, R,
by the cumulative ratio of the singular values. The cumulative ratio, ?k, can
be calculated by Eq. (5). When the ?k is more than 90%, k can be selected
for the reduced dimensionality. And this is large enough to capture most of the
important underlying structure in association of sentences and terms, and also
small enough to remove the noise of variability in term usage.
?k =
?k
i=1 wi
w1 + w2 + . . . + wR
, k = 1, 2, . . . , R (5)
To extract the significant sentences, in the first step, the Euclidean distances
can be computed between all the sentence vectors and the significant term vec-
tors (not all the term vectors). In this regard, the shorter the distance is, the
more important the sentence is, since the significant terms can be described as
representative words of a document. In the second step, the sentences are ex-
tracted by means of these Euclidean distances, and then these are included in
the summary in the order of their sequences. And the number of the included
sentences is depend on the compression rate of user?s need.
3.2 Extracting Significant Sentences by the Decomposed Matrices
In this subsection, we will illustrate how to extract the significant sentences by
examples. In [7], the importance of each sentence is computed by repeatedly
summing 1 for each occurrence of significant terms in the sentence. However,
the proposed method can be regarded as more formal or reasonable, since the
Euclidean distance between vectors is used to calculate the degree of importance
of each sentence.
Computing the SVD of the sentence-term matrix as shown in Table 2 re-
sults in the following three matrices for U ?, W ?, V ?. The matrices are reduced
by the cumulative ratio of the singular values computed by Eq. (5). Since the
first six singular values can justify 92.31% of the total, the 9-dimension can
be reduced to 6-dimension. Thus, the sentence-concentrated matrix, U ?, and
the term-concentrated matrix, V ?, can be represented only by 6-dimensional
vectors. The U ? and V ? are the vectors for the 9 sentences and 9 terms re-
spectively. The diagonal matrix W ? shows the first six values (originally, nine
values).
642 C. Lee, H. Park, and C. Ock
U? =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?0.289 0.032 ?0.086 0.022 ?0.195 0.293
?0.771 0.063 ?0.153 0.033 ?0.182 0.094
?0.015 ?0.367 ?0.019 ?0.441 ?0.209 ?0.077
?0.017 ?0.577 ?0.067 ?0.366 ?0.260 ?0.344
?0.006 ?0.657 ?0.189 0.704 0.118 0.085
?0.052 ?0.269 0.070 ?0.363 0.354 0.767
?0.280 ?0.101 0.320 ?0.094 0.750 ?0.371
?0.481 0.031 ?0.067 0.011 0.013 ?0.198
?0.094 ?0.115 0.904 0.181 ?0.340 0.092
?
?
?
?
?
?
?
?
?
?
?
?
?
?
W? =
(
2.826 2.424 2.314 2.117 1.672 0.984
)
V? =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?0.818 0.078 ?0.199 0.047 ?0.326 0.288
?0.542 ?0.003 0.043 ?0.024 0.348 ?0.483
?0.030 ?0.500 ?0.007 ?0.553 ?0.069 0.352
?0.011 ?0.389 ?0.037 ?0.381 ?0.281 ?0.427
?0.008 ?0.509 ?0.111 0.160 ?0.085 ?0.263
?0.004 ?0.542 ?0.163 0.666 0.141 0.173
?0.118 ?0.153 0.168 ?0.216 0.660 0.402
?0.132 ?0.089 0.529 0.041 0.245 ?0.284
?0.066 ?0.095 0.781 0.171 ?0.407 0.187
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The Euclidean distances between the significant term vectors and the sen-
tence vectors can be computed by above two matrices, V ? and U ?, to extract
the significant sentences. The significant term vectors are the third, sixth and
seventh rows in the V ?. The significant sentence by X3, for instance, is the third
sentence of the document, since the distance between them is the shortest. All
the distances from X3, X6 and X7 vectors are shown in Table 4. Consequently,
the three significant sentences (third, fifth and sixth) can be included in the
summary of our sample article. When the number of the selected sentences are
less than that of user?s need, the summary can be supplemented with the other
sentences by their distances.
4 Experiments
We compared the proposed method with both only PCA[7] and random selection
of sentences.
The [7] selected the significant sentences by the appearance of the signifi-
cant terms. The [7] also exploited from one to three consecutive sentences for
the observation of PCA; however, the performance of extracting the significant
sentences was similar. In this paper, we used each sentence within a document
for the observation of PCA.
To extract sentences randomly, first, random numbers amounting to 30% of
the total number of sentences in a document were created, and then the sentences
were extracted by these random numbers.
We tried out the proposed method with two ways. First, the sentences were
extracted by the distances of each significant term as described in subsection
Significant Sentence Extraction by Euclidean Distance 643
Table 4. Euclidean distances between all the sentence vectors and the significant term
vectors (X3, X6 and X7)
distance
num. of the sentence X3 X6 X7
1 0.841 0.979 0.904
2 1.144 1.210 1.201
3 0.484 1.209 1.062
4 0.752 1.226 1.293
5 1.321 0.154 1.279
6 0.668 1.260 0.526
7 1.317 1.322 0.821
8 1.057 1.071 1.026
9 1.289 1.342 1.341
Table 5. Evaluation result
Method
PCA & SVD
Measure Random PCA All Each
Average Precision 0.256 0.386 0.395 0.407
Average Recall 0.413 0.451 0.486 0.500
F-Measure 0.316 0.416 0.436 0.449
3.2. Second, the sentences were selected by all the distances of all the signif-
icant terms. In Table 5, ?All? and ?Each? denote the latter and the former,
respectively.
We used 127 documents of Korean newspaper articles for the evaluation,
which were compiled by KISTI(Korea Institute of Science & Technology In-
formation). Each document consists of orginal article and manual summary
amounting to 30% of the source text. We regarded the sentences within this
manual summary as correct ones.
We use three measures to evaluate the methods: precision, recall, and F-
measure. Let
? Count(SystemCorrect) denote the number of correct sentences that the sys-
tem extracts.
? Count(SystemExtract) denote the number of sentences that the system ex-
tracts.
? Count(Correct) denote the number of correct sentences provided in the test
collection.
The measures are defined respectively as follows.
Precision =
Count(SystemCorrect)
Count(SystemExtract)
644 C. Lee, H. Park, and C. Ock
Recall =
Count(SystemCorrect)
Count(Correct)
F ? Measure = 2 ? Precision ? Recall
Precision + Recall
Table 5 shows that, by means of F-measure, the proposed method has im-
proved the performance by about 2% ? 3.3% over only PCA, and by about 12%
? 13.3% over random selection. Table 5 also shows that the ?Each? method is
superior to ?All?. Furthermore, the performance of using PCA is better than
that of using term frequency or a thesaurus [7].
5 Conclusion
In this paper, we have proposed a summarization approach that constructs a
summary by extracting sentences in a single document. The particular tech-
niques used are PCA and SVD for extracting the significant terms and sentences
respectively.
PCA can quantify the information on both the term frequency and the term-
term cooccurrence in the document by the eigenvalue-eigenvector pairs. These
pairs were used to find out the significant terms among the nouns in the docu-
ment. In addition, these terms can be regarded as those extracted by relation-
ships between terms in the document, since PCA exploits the variance-covariance
structure.
In contrast to PCA, SVD has the inforamtion on the sentences and terms after
computing it of sentence-term matrix. In this regard, we can use the decomposed
matrices to calculate the distances between the sentence and term vectors, and
to make an effective removal of the noise of variability in term usage.
Experimental results on Korean newspaper articles show that the proposed
method is superior to the methods of both random selection and only using
PCA, and that extracting sentences by the distances per each term is better
performance than extracting by all the distances of all the terms.
In conclusion, the information on the cooccurrence relationships between
the terms by the PCA and the vector expressions of the sentences and terms
by the SVD can be helpful for the text summarization. Furthermore, the pro-
posed methods only exploited the pattern of the statistical occurrences within
a document without the additional resources like a thesaurus to find out the
relationships between the terms, and the proper dimension of the vectors.
Acknowledgements
This research was supported by the MIC(Ministry of Information and Com-
munication), Korea, under the ITRC(Information Technology Research Center)
support program supervised by the IITA(Institute of Information Technology
Assessment)
Significant Sentence Extraction by Euclidean Distance 645
References
1. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. New York: ACM
Press (1999)
2. Barzilay, R., Elhadad, M. : Using Lexical chains for Text Summarization. In: I. Mani,
& M. T. Maybury (eds.): Advances in automatic text summarization. Cambridge,
MA: The MIT Press (1999) 111?121.
3. Deerwester, S., Dumais, S. T., Harshman, R.: Indexing by latent semantic analysis.
Journal of the American Society for Information Science, 41 (6). (1990) 381?407
4. Edmundson, H. P.: New Methods in Automatic Extracting. In: I. Mani, & M. T.
Maybury (eds.): Advances in automatic text summarization. Cambridge, MA: The
MIT Press (1999) 23?42.
5. Haykin, S. S.: Neural networks: A comprehensive foundation. 2nd edn. Paramus,
NJ: Prentice Hall PTR (1998)
6. Johnson, R. A., Wichern, D. W.: Applied Multivariate Statistical Analysis. 3rd edn.
NJ: Prentice Hall (1992)
7. Lee, C., Kim, M., Park, H.: Automatic Summarization Based on Principal Compo-
nent Analysis. In: Pires, F.M., Abreu, S. (eds.): Progress in Artificial Intelligence.
Lecture Notes in Artificial Intelligence, Vol. 2902. Springer-Verlag, Berlin Heidelberg
New York (2003) 409?413
8. Luhn, H. P.: The Automatic Creation of Literature Abstracts. In: I. Mani, & M. T.
Maybury (eds.): Advances in automatic text summarization. Cambridge, MA: The
MIT Press (1999) 15?21.
9. Press, W. H., Teukolsky, S. A., Vetterling, W. T., Flannery, B. P.: Numerical recipes
in C++. 2nd edn. New York: Cambridge University Press (1992)
