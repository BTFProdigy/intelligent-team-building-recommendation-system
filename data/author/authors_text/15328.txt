Adaptive Transformation-based Learning for
Improving Dictionary Tagging
Burcu Karagol-Ayan, David Doermann, and Amy Weinberg
Institute for Advanced Computer Studies (UMIACS)
University of Maryland
College Park, MD 20742
{burcu,doermann,weinberg}@umiacs.umd.edu
Abstract
We present an adaptive technique that en-
ables users to produce a high quality dic-
tionary parsed into its lexicographic com-
ponents (headwords, pronunciations, parts
of speech, translations, etc.) using an
extremely small amount of user provided
training data. We use transformation-
based learning (TBL) as a postprocessor at
two points in our system to improve per-
formance. The results using two dictio-
naries show that the tagging accuracy in-
creases from 83% and 91% to 93% and
94% for individual words or ?tokens?, and
from 64% and 83% to 90% and 93% for
contiguous ?phrases? such as definitions
or examples of usage.
1 Introduction
The availability and use of electronic resources
such as electronic dictionaries has increased tre-
mendously in recent years and their use in
Natural Language Processing (NLP) systems is
widespread. For languages with limited electronic
resources, i.e. low-density languages, however,
we cannot use automated techniques based on par-
allel corpora (Gale and Church, 1991; Melamed,
2000; Resnik, 1999; Utsuro et al, 2002), compa-
rable corpora (Fung and Yee, 1998), or multilin-
gual thesauri (Vossen, 1998). Yet for these low-
density languages, printed bilingual dictionaries
often offer effective mapping from the low-density
language to a high-density language, such as En-
glish.
Dictionaries can have different formats and can
provide a variety of information. However, they
typically have a consistent layout of entries and a
1 Headword 5 Translation
2 POS 6 Example of usage
3 Sense number 7 Example of usage translation
4 Synonym 8 Subcategorization
Figure 1: Sample tagged dictionary entries. Eight
tags are identified and tagged in the given entries.
consistent structure within entries. Publishers of
dictionaries often use a combination of features to
impose this structure including (1) changes in font
style, font-size, etc. that make implicit the lexico-
graphic information1, such as headwords, pronun-
ciations, parts of speech (POS), and translations,
(2) keywords that provide an explicit interpreta-
tion of the lexicographic information, and (3) var-
ious separators that impose an overall structure on
the entry. For example, a boldface font may in-
dicate a headword, italics may indicate an exam-
ple of usage, keywords may designate the POS,
commas may separate different translations, and a
numbering system may identify different senses of
a word.
We developed an entry tagging system that rec-
ognizes, parses, and tags the entries of a printed
dictionary to reproduce the representation elec-
tronically (Karagol-Ayan et al, 2003). The sys-
tem aims to use features as described above and
the consistent layout and structure of the dictio-
1For the purposes of this paper, we will refer to the lexi-
cographic information as tag when necessary.
257
naries to capture and recover the lexicographic in-
formation in the entries. Each token2 or group of
tokens (phrase)3 in an entry associates with a tag
indicating its lexicographic information in the en-
try. Figure 1 shows sample tagged entries in which
eight different types of lexicographic information
are identified and marked. The system gets for-
mat and style information from a document image
analyzer module (Ma and Doermann, 2003) and
is retargeted at many levels with minimal human
assistance.
A major requirement for a human aided dic-
tionary tagging application is the need to mini-
mize human generated training data.4 This re-
quirement limits the effectiveness of data driven
methods for initial training. We chose rule-based
tagging that uses the structure to analyze and tag
tokens as our baseline, because it outperformed
the baseline results of an HMM tagger. The ap-
proach has demonstrated promising results, but we
will show its shortcomings can be improved by ap-
plying a transformation-based learning (TBL) post
processing technique.
TBL (Brill, 1995) is a rule-based machine learn-
ing method with some attractive qualities that
make it suitable for language related tasks. First,
the resulting rules are easily reviewed and under-
stood. Second, it is error-driven, thus directly min-
imizes the error rate (Florian and Ngai, 2001).
Furthermore, TBL can be applied to other annota-
tion systems? output to improve performance. Fi-
nally, it makes use of the features of the token and
those in the neighborhood surrounding it.
In this paper, we describe an adaptive TBL
based technique to improve the performance of the
rule-based entry tagger, especially targeting cer-
tain shortcomings. We first investigate how using
TBL to improve the accurate rendering of tokens?
font style affects the rule-based tagging accuracy.
We then apply TBL on tags of the tokens. In our
experiments with two dictionaries, the range of
font style accuracies is increased from 84%-94%
to 97%-98%, and the range of tagging accuracies
is increased from 83%-90% to 93%-94% for to-
kens, and from 64%-83% to 90%-93% for phrases.
Section 2 discusses the rule-based entry tagging
2Token is a set of glyphs (i.e., a visual representation of a
set of characters) in the OCRed output. Each punctuation is
counted as a token as well.
3In Figure 1, not on time is a phrase consisting of 3 tokens.
4For our experiments we required hand tagging of no
more than eight pages that took around three hours of human
effort.
method. In Section 3, we briefly describe TBL,
and Section 4 recounts how we apply TBL to im-
prove the performance of the rule-based method.
Section 5 explains the experiments and results, and
we conclude with future work.
2 A Rule-based Dictionary Entry Tagger
The rule-based entry tagger (Karagol-Ayan et al,
2003) utilizes the repeating structure of the dic-
tionaries to identify and tag the linguistic role
of tokens or sets of tokens. Rule-based tagging
uses three different types of clues?font style, key-
words and separators?to tag the entries in a sys-
tematic way. The method accommodates noise in-
troduced by the document analyzer by allowing
for a relaxed matching of OCRed output to tags.
For each dictionary, a human operator must spec-
ify the lexicographic information used in that par-
ticular dictionary, along with the clues for each
tag. This process can be performed in a few hours.
The rule-based method alone achieved token accu-
racy between 73%-87% and phrase accuracy be-
tween 75%-89% in experiments conducted using
three different dictionaries5.
The rule-based method has demonstrated prom-
ising results, but has two shortcomings. First, the
method does not consider the relations between
different tags in the entries. While not a prob-
lem for some dictionaries, for others ordering the
relations between tags may be the only informa-
tion that will tag a token correctly. Consider the
dictionary entries in Figure 1. In this dictionary,
the word ?a? represents POS when in italic font,
and part of a translation if in normal font. How-
ever if the font is incorrect (font errors are more
likely to happen with short tokens), the only way
to mark correctly the tag involves checking the
neighboring tokens and tags to determine its rel-
ative position within the entry. When the token
has an incorrect font or OCR errors exist, and
the other clues are ambiguous or inconclusive, the
rule-based method may yield incorrect results.
Second, the rule-based method can produce in-
correct splitting and/or merging of phrases. An er-
roneous merge of two tokens as a phrase may take
place either because of a font error in one of the
tokens or the lack of a separator, such as a punctu-
ation mark. A phrase may split erroneously either
5Using HMMs for entry tagging on the same set of dic-
tionaries produced slightly lower performance, resulting in
token accuracy between 73%-88% and phrase accuracy be-
tween 57%-85%.
258
as a result of a font error or an ambiguous separa-
tor. For instance, a comma may be used after an
example of usage to separate it from its translation
or within it as a normal punctuation mark.
3 TBL
TBL (Brill, 1995), a rule-based machine learning
algorithm, has been applied to various NLP tasks.
TBL starts with an initial state, and it requires a
correctly annotated training corpus, or truth, for
the learning (or training) process. The iterative
learning process acquires an ordered list of rules
or transformations that correct the errors in this
initial state. At each iteration, the transformation
which achieved the largest benefit during appli-
cation is selected. During the learning process,
the templates of allowable transformations limit
the search space for possible transformation rules.
The proposed transformations are formed by in-
stantiation of the transformation templates in the
context of erroneous tags. The learning algorithm
stops when no improvement can be made to the
current state of the training data or when a pre-
specified threshold is reached.
A transformation modifies a tag when its con-
text (such as neighboring tags or tokens) matches
the context described by the transformation. Two
parts comprise a transformation: a rewrite rule?
what to replace? and a triggering environment?
when to replace. A typical rewrite rule is: Change
the annotation from aa to ab, and a typical trig-
gering environment is: The preceding word is wa.
The system?s output is the final state of this data
after applying all transformations in the order they
are produced.
To overcome the lengthy training time associ-
ated with this approach, we used fnTBL, a fast ver-
sion of TBL that preserves the performance of the
algorithm (Ngai and Florian, 2001). Our research
contribution shows this method is effective when
applied to a miniscule set of training data.
4 Application of TBL to Entry Tagging
In this section, we describe how we used TBL in
the context of tagging dictionary entries.
We apply TBL at two points: to render correctly
the font style of the tokens and to label correctly
the tags of the tokens6. Although our ultimate goal
6In reality, TBL improves the accuracy of tags and phrase
boundary flags. In this paper, whenever we say ?application
of TBL to tagging?, we mean tags and phrase boundary flags
 
	
 	Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
 	
	
Telicity as a Cue to Tempora l  and Discourse Structure in 
Chinese-Engl ish Machine Translation* 
Mari Olsen David Traum 
Microsoft U Maryland 
molsen@microsoft.com traum@cs.umd.edu 
Carol Van Ess-Dykema 
U.S. Department ofDefense 
carol@umiacs.umd.edu 
Amy Weinberg 
U Maryland 
weinberg@umiacs.umd.edu 
Ron Dolan 
Library of Congress 
rdolan@cfar.umd.edu 
Abstract 
Machine translation between any two languages re- 
quires the generation of information that is implicit 
in the source language. In translating from Chinese 
to English, tense and other temporal information 
must be inferred from other grammatical and lex- 
ical cues. Moreover, Chinese multiple-clause sen- 
tences may contain inter-clausal relations (temporal 
or otherwise) that must be explicit in English (e.g., 
by means of a discourse marker). Perfective and im- 
perfective grammatical aspect markers can provide 
cues to temporal structure, but such information is 
not present in every sentence. We report on a project 
to use the \]exical aspect features of (a)te\]icity re- 
flected in the Lexical Conceptual Structure of the 
input text to suggest ense and discourse structure 
in the English translation of a Chinese newspaper 
corpus. 
1 Introduction 
It is commonly held that an appropriate interlingua 
must allow for the expression of argument relations 
in many languages. This paper advances the state of 
the art of designing an interlingua by showing how 
aspectual distinctions (telic versus atelic) can be de- 
rived from verb classifications primarily influenced 
by considerations of argument structure, and how 
these aspectual distinctions can be used to fill lexical 
gaps in the source language that cannot be left un- 
specified in the target language. Machine translation 
between any two languages often requires the gen- 
eration of information that is implicit in the source 
language. In translating from Chinese to English, 
tense and other temporal information must be in- 
ferred from other grammatical nd lexical cues. For 
example, Chinese verbs do not necessarily specify 
whether the event described is prior or cotempora- 
neous with the moment of speaking. While gram- 
matical aspect information can be loosely associated 
with time, with imperfective aspect (Chinese ~ zai- 
and ~ .zhe) representing present ime and perfec- 
tiv e (Chinese T le )  representing past time, (Chu, 
* We gratefully acknowledge DOD support for this work 
through contract MDA904-96-R-0738 
1998; Li and Thompson, 1981), verbs in the past 
do not need to have any aspect marking distinguish- 
ing them from present tense verbs. Th is  is unlike 
English, which much more rigidly distinguishes past 
from present ense through use of suffixes. Thus, in 
order to generate an appropriate English sentence 
from its Chinese counterpart, we need to fill in a 
potentially unexpressed tense. 
Moreover, Chinese multiple-clause ntences may 
contain implicit relations between clauses (temporal 
or otherwise) that must be made explicit in English. 
These multiple-clause ntences are often most nat- 
urally translated into English including an overt ex- 
pression of their relation, e.g., the "and" linking the 
two clauses in (1), or as multiple sentences, as in (2)). 
(1) 1 9 65  ~ ~ , ~ ,~ 
1965 year before , our_country altogether 
only have 30 ten_thousand ton de 
~t ~2 , ~ ~ ~8 
shipbuilding capacity , year output is 8 
ten_thousand ton 
Before 1965 China had a total of only 300,000 
tons of shipbuilding capacity and the annual 
output was 80,000 ~ons. 
(2)~ 8~ ~ ~ ~d Y 
this 8 ten_thousand ton actually include asp 
517 cl , ship de tonnage is very low de 
This 80,000 tons actually included 517 ships. 
Ship tonnage was very low. 
In our NLP applications, we use a level of linguis- 
tic structure driven by the argument-taking proper- 
ties of predicates and composed monotonically up to 
the sentence l vel. The resulting Lexical Conceptual 
Structures (LCS) (3ackendoff, 1983), is a language- 
neutral representation of the situation (event or 
state), suitable for use as an interlingua, e.g., for 
machine translation. The LCS represents predicate 
argument structure abstracted away from language- 
specific properties of semantics and syntax. The 
34 
primitives of the interlingua provide for monotonic 
composition that captures both conceptual nd syn- 
tactic generalities (Dorr et al, 1993) among lan- 
guages. 1 The strength of the representation derives 
from the cross-linguistic regularities in the lexical se- 
mantics encoded in the LCS. The syntactic hierarchy 
(subject, object, oblique) is mirrored in the LCS hi- 
erarchy: for example THEMES are arguments of the 
LCS predicate, and AGENTS are arguments of the 
theme-predicate composition. Syntactic divergences 
(whether the object precedes or follows the verb, for 
example) are represented in language specific lin- 
earization rules; lexical divergences (whether the lo- 
cation argument is encoded irectly in the verb, e.g. 
the English verb pocket or must be saturated by an 
exterfial argument) are stated in terms of the pieces 
of LCS struct-ure in the lexicon. SententiM repre- 
sentations derive from saturating the arguments re- 
quired by the predicates in the sentence. 
LCS rePresentations also include temporal infor- 
mation, where available in the source language: re- 
cent revisions include, for example (Dorr and Olsen, 
1997a) standardizing LCS representations for the as- 
pectual (un)boundedness ((A)TELICITY) of  events, 
either lexically or sententially represented. Although 
at present he LCS encodes no supra-sentential dis- 
course relations, we show how the lexical aspect in- 
formation may be used to generate discourse co- 
herence in temporal structure. Relations between 
clauses as constrained by temporal reference has 
been examined in an LCS framework by Dorr and 
Gaasterland (Dorr and Gaasterland, 1995). They 
explore how temporal connectives are constrained 
in interpretation, based on the tense of the clauses 
they connect. While overt temporal connectives are 
helpful when they appear, our corpus contains many 
sentences with neither tense markers nor tense con- 
nectives. We must therefore look to a new source of 
information. We rely on the lexical information of 
the verbs within a sentence to generate both tense 
and temporal connectives. 
Straightforward LCS analysis of many of the 
multi-clause sentences in our corpus leads to vio- 
lations of the wellformedness conditions, which pre- 
vent structures with events or states directly modi- 
fying other events or states. LCS, as previously con- 
ceived, prohibits an event or state from standing in a 
modifier elationship to another event or state, with- 
out mediation of a path or position (i.e., as lexically 
realized by a preposition). This restriction reflects 
the insight hat (at least in English) when events and 
states modify each other, the modification is either 
implicit, with the relevant events and states in sepa- 
rate sentences (and hence separate LCSs), as in the 
1 LCS representations in our system have been created for 
Korean, Spanish and Arabic, as well as for English and Chi- 
nese. 
first sentence below, or explicit in a single sentence, 
as in the second sentence below. Implicit event-state 
modification (sentence 3) is prohibited. 
* Wade bought a car. He needed a way to get to 
work. 
* Wade bought a car because he needed a way to 
get to work. 
* * Wade bought a car he needed a way to get to 
work. 
It is exactly these third type that are permitted 
in standard Chinese and robustly attested in our 
data. If the LCS is to be truly an interlingua, we 
must extend the representation to allow these kinds 
of sentences to be processed. One possibility is to 
posit an implicit position connecting the situations 
described by the multiple clauses. In the source lan- 
guage analysis phase, this would amount o positing 
a disjunction of all possible position relations im- 
plicitly realizable in this language. Another option 
is to relax the wellformedness constraints to allow 
an event o directly modify another event. This not 
only fails to recognize the regularities we see in En- 
glish (and other language) LCS structures, for Chi- 
nese it merely pushes the problem back one step, 
as the set of implicitly realizable relations may vary 
from language to language and may result in some 
ungrammatical or misleading translations. The sec- 
ond option can be augmented, however, by factoring 
out of the interlingua (and into the generation code) 
language-specific principles for generating connec- 
tives using information i the LCS-structure, proper. 
For the present, this is the approach we take, us- 
ing lexical aspectual information, as read from the 
LCS structure, to generate appropriate mporal re- 
lations. 
Therefore not only tense, but inter-sentential dis- 
course relations must be considered when generating 
English from Chinese, even at the sentence l vel. We 
report on a project to generate both temporal and 
discourse relations using the LCS representation. I  
particular, we focus on the encoding of the lexical as- 
pect feature TELICITY and its complement ATELIG- 
ITY to generate past and present ense, and corre- 
sponding temporal relations for modifying clauses 
within sentences. While we cannot at present di- 
rectly capture discourse relations, we can garner as- 
pectual class from LCS verb classification, which in 
turn can be used to predict he appropriate nse for 
translations ofChinese verbs into English. 
2 Use of Aspect to Provide 
Temporal Informat ion 
We begin with a discussion of aspectual features of 
sentences, and how this information can be used to 
provide information about the time of the situations 
35 
presented in a sentence. Such information can be 
used to help provide clues as to both tense and rela- 
tionships (and cue words) between connected situa- 
tions. Aspectual features can be divided into gram- 
matical aspect, which is indicated by lexical or mor- 
phological markers in a sentence, and lexical aspect, 
which is inherent in the meanings of words. 
2.1 Grammat ica l  aspect 
Grammatical aspect provides a viewpoint on situa- 
tion (event or state) structure (Smith, 1997). Since 
imperfective aspect, such as the English PROGRES- 
SIVE construction be VERB- ing,  views a situation 
from within, it is often associated with present 
or contemporaneous time reference. On the other 
hand, perfective aspect, such as the English have 
VERB-ed, Views a situation as a whole; it is there- 
fore often associated with past time reference ((Com- 
rie, 1976; Olsen, 1997; Smith, 1997) cf. (Chu, 
1998)). The temporal relations are tendencies, 
rather than an absolute correlation: although the 
perfective is found more frequently in past tenses 
(Comrie, 1976), both imperfective and perfective co- 
occur in some language with past, present, and fu- 
ture tense. 
In some cases, an English verb will specify tense 
and/or aspect for a complement. For example, con- 
tinue requires either an infinitive (3)a or progressive 
complement (3)b (and subject drop), while other 
verbs like say do not place such restrictions (3)c,d. 
(3) a. Wolfe continued to publicize the baseless 
criticism on various occasions 
b. Wolfe continued publicizing the baseless 
criticism on various occasions 
c. Wolfe continued publicizing the baseless 
criticism on various occasions 
d. He said the asia-pacific region already be- 
came a focal point region 
e. He said the asia-pacific region already is be- 
coming a focal point region 
2.2 Lexical aspect 
While grammatical spect and overt temporal cues 
are clearly helpful in translation, there are many 
cases in our corpus in which such cues are not 
present. These are the hard cases, where we must 
infer tense or grammatical spectual marking in the 
target language from a source that looks like it pro- 
vides no overt cues. We will show however, that 
Chinese does provide implicit cues through its lex- 
ical aspect classes. First, we review what lexical 
aspect is. 
Lexical aspect refers to the type of situation de- 
noted by the verb, alone or combined with other 
sentential constituents. Verbs are assigned to lexical 
aspect classes based on their behavior in a variety of 
syntactic and semantic frames that focus on three as- 
pectual features: telicity, dynamicity and durativity. 
We focus on telicity, also known as BOUNDEDNESS. 
Verbs that are telic have an inherent end: winning, 
for example, ends with the finish line. Verbs that are 
atelic do not name their end: running could end with 
a distance run a mile or an endpoint run to the store, 
for example. Olsen (Olsen, 1997) proposed that as- 
pectual interpretation be derived through monotonic 
composition of marked privative features \[?/0 dy- 
namic\], \[.4-/0 durative\] and \[-t-/0 telic\], as shown in 
Table 1 (Olsen, 1997, pp. 32-33). 
With privative features, other sentential con- 
stituents can add to features provided by the verb 
but not remove them. On this analysis, the \[.-I-du- 
rative, +dynamic\] features of run propagate to the 
sentence l vel in run ~o the store; the \[?telic\] feature 
is added by the NP or PP, yielding an accomplish- 
ment interpretation. The feature specification of this 
?ompositionally derived accomplishment is herefore 
identical to that of a sentence containing a telic ac- 
complishment verb, such as destroy. 
According to many researchers, knowledge of lex- 
ical aspect--how verbs denote situations as devel- 
oping or holding in time-=may be used to interpret 
event sequences in discourse (Dowty, 1986; Moens 
and Steedman, 1988; Passoneau, 1988). In particu- 
lar, Dowty suggests that, absent other cues, a relic 
event is interpreted as completed before the next 
event or state, as with ran into lhe room in 4a; in 
contrast, atelic situations, such as run, was hungry 
in 4b and 4% are interpreted as contemporaneous 
with the following situations: fell and made a pizza, 
respectively. 
(4) a. Mary ran into the room. She turned on her 
walkman. 
b. Mary ran. She turned on her walkman. 
c. Mary was hungry. She made a pizza. 
Smith similarly suggests that in English all past 
events are interpreted as telic (Smith, 1997) (but cf. 
(Olsen, 1997)). 
Also, these tendencies are heuristic, and not abso- 
lute, as shown by the examples in (5). While we get 
the expected prediction that the jumping occurs af- 
ter the explosion in (5)(a), we get the reverse predic- 
tion in (5)(b). Other factors such as consequences of 
described situations, discourse context, and stereo- 
typical causal relationships also play a role. 
(5) a. The building exploded. Mary jumped. 
b. The building exploded. Chunks of concrete 
flew everywhere. 
36 
Aspectual  Class 
State 
Activity 
-~Accomplishment 
Achievement 
Tel ie Dynamic  Durat ive 
+ 
+ ,, , + __ run , paint 
+ + + 
+ + 
Examples 
know, have 
destroy 
notice, win 
Table 1: Lexical Aspect Features 
3 Aspect  in Lex ica l  Conceptua l  
S t ructure  
Our implementation f Lexical ConceptuM Struc- 
ture (Dowty, 1979; Guerssel et al, 1985)--an 
augmented form of (Jackendoff, 1983; Jackendoff, 
1990)--permits lexical aspect information to be 
read directly off the lexical entries for individual 
verbs, as well-as composed representations for sen- 
tences, using uniform processes and representations. 
The LCS framework consists of primitives (GO, 
BE, STAY, etc.), types (Event, State, Path, etc.) 
and fields (Loc(ational), Temp(oral), Foss(essional), 
Ident(ificational), Perc(eptual), etc.). 
We adopt a refinement of the LCS representation, 
incorporating meaning components from the linguis- 
tically motivated notion of !ezical semantic template 
(LST), based on lexical aspect classes, as defined 
in the work of Levin and Rappaport Hovav (Levin 
and Rappaport Hovav, 1995; Rappaport l tovav and 
Levin, 1995). Verbs that appear in multiple as- 
pectual frames appear in multiple pairings between 
constants (representing the idiosyncratic meaning 
of the verb) and structures (the aspectual class). 
Since the aspectual templates may be realized in 
a variety of ways, other aspects of the structural 
meaning contribute to differentiating the verbs from 
each other. Our current database contains ome 400 
classes, based on an initial representation f the 213 
classes in (Levin, 1993). Our current working lexi- 
con includes about 10,000 English verbs and 18,000 
Chinese verbs spread out into these classes. 
Telic verbs (and sentences) contain certain types 
of Paths, or a constant, represented by ! !, filled by 
the verb constant, in the right most leaf-node argu- 
ment. Some examples are shown below: 
depart (go foe (* thing 2) 
(away_from loc (thing 2) 
(at foe (thing 2) 
(* thing 4))) 
(!!+ingly 26)) 
insert (cause (* thing 1) 
(go loc (* thing 2) 
((* toward 5) loc (thing 2) 
( \ [at \ ]  loc (thing 2) 
(thing 6))) )  
(! !+ingly 26)) 
Each of these relic verbs has a potential coun- 
terpart with an atelic verb plus the requisite path. 
Depart, for example, corresponds to move away, or 
something similar in another language. 
We therefore identify telic sentences by the algo- 
rithm, formally specified in in Figure 1 (cf. (Dorr 
and Olsen, 1997b) \[156\]). 
Given an LCS representation L: 
1. Initialize: T(L):=\[?T\], D(L):=\[0R\], R(L):=\[0D\] 
2. If Top node of L E {CAUSE, LET, GO} 
Then T(L):=\[+T\] 
If Top node of L E {CAUSE, LET} 
Then D(L):=\[+D\], R(L):=\[+R\] 
If Top node of L E {GO} 
Then D(L):=\[+D\] 
3. If Top node of L E {ACT, BE, STAY} 
Then If Internal node of 
L E {TO, TOWARD,  FORTemp} 
Then T(L):=\[+T\] 
If Top node of L E {BE, STAY} 
Then R(L):=\[+R\] 
If Top node of L E {ACT} 
Then set D(L):=\[+D\], R(L):=\[+R\] 
4. Return T(L), D(L), R(L). 
Figure 1: Algorithm for Aspectual Feature Determi- 
nation 
This algorithm applies to the structural primitives 
of the interlingua structure rather than actual verbs 
in source or target language. The first step initial- 
ized the aspectual values as unspecified: atelic f-T\], 
stative (not event: f-D\]), and adurative f-R\]. First 
the top node is examined for primitives that indicate 
telicity: if the top node is CAUSE, LET, GO, telicity 
is set to \[+T\], as with the verbs break, destroy, for 
example. (The node is further checked for dynamic- 
ity \[+D\] and durativity \[+R\] indicators, not in focus 
in this paper.)If the top node is not a relic indicator 
(i.e., the verb is a basically atelic predicate such as 
love or run, telicity may still be still be indicated 
by the presence of complement odes of particular 
types: e.g. a goal phrase (to primitive) in the case of 
run. The same algorithm may be used to determine 
felicity in either individual verbal entries (break but 
37 
not run) or composed sentences (John ran to ~he 
store but not John ran. 
Similar mismatches of telicity between represen- 
tations of particular predicates can occur between 
languages, although there is remarkable agreement 
as to the set of templates that verbs with related 
meanings will fit into (Olsen et al, 1998). In the 
Chinese-English interlingual system we describe, the 
Chinese is first mapped into the LCS, a language- 
independent representation, from which the target- 
language sentence is generated. Since telicity (and 
other aspects of event structure) are uniformly rep- 
resented at the lexical and the sentential level, telic- 
ity mismatches between verbs of different languages 
may then be compensated for by combining verbs 
with other .components. 
. o  
4 Predictions 
Based on (Dowty, 1986) and others, as discussed 
above, we predict that sentences that have a telic 
LCS will better translate into English as the past 
tense, and those that lack telic identifiers will trans- 
late as present tense. Moreover, we predict that 
verbs in the main clause that are telic, will be past 
with respect o their subordinates (X then Y). Verbs 
in the main clause that are atelic we predict will tem- 
porally overlap (X while Y). 
5 Implementation 
LCSes are used as the interlingua for our machine 
translation efforts. Following the principles in (Dorr, 
1993), lexical information and constraints on well- 
formed LCSes are used to compose an LCS for a 
complete sentence from a sentence parse in a source 
language. This composed LCS (CLCS) is then used 
as the starting points for generation into the target 
language, using lexical information and constraints 
for the target language. 
The generation component consists of the follow- 
ing subcomponents: 
Decomposi t ion and lexlcal selection First, 
primitive LCSes for words in the target lan- 
guage are matched against CLCSes, and tree 
structures of covering words are selected. Am- 
biguity in the input and analysis represented 
in the CLCS is maintained (insofar as it is 
possible to realize particular eadings using the 
target language lexicon), and new ambiguities 
are introduced when there are different ways of 
realizing a CLCS in the target language. 
AMR Construct ion This tree structure is then 
translated into a representation using the Aug- 
mented Meaning Representation (AMR) syntax 
? of instances and hierarchical relations (Langk- 
fide and Knight, 1998a); however the rela- 
tions include information present in the CLCS 
and LCSes for target language words, including 
theta roles, LCS type, and associated features. 
Real izat ion The AMR structure is then linearized, 
as described in (Dorr et al, 1998), and mor- 
phological realization is performed. The result 
is a lattice of possible realizations, represent- 
ing both the preserved ambiguity from previous 
processing phases and multiple ways of lineariz- 
ing the sentence. 
Extract ion The final stage uses a statistical bi- 
gram extractor to pick an approximation of the 
most fluentrealization (Langkilde and Knight, 
1998b). 
While there are several possible ways to address 
the tense and discourse connective issues mentioned 
above, such as modifying the LCS primitive lements 
and/or the composition of the LCS from the source 
language, we instead have been experimenting for 
the moment with solutions implemented within the 
generation component. The only extensions to the 
LCS language have been loosening of the constraint 
against direct modification of states and events by 
other states and events (thus allowing composed LC- 
Ses to be formed from Chinese with these structures, 
but creating a challenge for fluent generation into 
English), and a few added features to cover some of 
the discourse markers that are present. We are able 
to calculate telicity of a CLCS, using the algorithm 
in Figure 1 and encode this information as a binary 
te l i?  feature in the Augmented Meaning Represen- 
tation (AMR). 
The realization algorithm has been augmented 
with the rules in (6) 
(6) a. If there is no tense feature, use telicity to 
determine the tense: 
: te l i c  + -~ : tense  past  
: re l i c  -- --~ : tense present 
b. In an event or state directly modifying 
another event or state, if there is no other 
clausal connective (coming from a subor- 
dinating conjunction or post-position in 
the original), then use telicity to pick a 
connective expressing assumed temporal 
relation: 
: re l i c  -~ -~ : scon j  then  
: re l i c  -- -~ : sconj while 
6 The Corpus 
We have applied this machine translation system to 
a corpus of Chinese newspaper text from Xinhua and 
other sources, primarily in the economics domain. 
The genre is roughly comparable to the American 
38 
Wall Street Journal. Chinese newspaper genre dif- 
fers from other Chinese textual sources, in a number 
of ways, including: 
? more complex sentence structure 
? more extensive use of acronyms 
? less use of Classical Chinese 
? more representative grammar 
? more constrained vocabulary (limited lexicon) 
? abbreviations are used extensively in Chinese 
newspaper headlines 
However, the presence of multiple events and 
states in a single sentence, without explicit modifi- 
catioia is characteristic ofwritten Chinese in general. 
In the 80-sentence corpus under consideration, the 
sentence structure is complex and stylized; with an 
average of 20 words per sentence. Many sentences, 
such as (1)and (2), have multiple clauses that are 
not in a direct complement relationship or indicated 
with explicit connective words. 
7 Ground Truth 
To evaluate the extent to which our Predictions re- 
sult in an improvement in translation, we have used 
a database of human translations of  the sentences 
in our corpus as the ground truth, or gold standard. 
One of the translators is included among our au- 
thors. 
The ground truth data was created to provide a 
fluid human translation of the text early in our sys- 
tem development. It therefore includes many com- 
plex tenses and multiple sentences combined, both 
currently beyond the state of our system. Thus, 
two of the authors and an additional researcher 
also created a database of temporal relations among 
the clauses in the sentences that produced illegal 
event/state modifications. This was used to test pre- 
dictions of temporal relationships indicated by telic- 
ity. In evaluating our results, we concentrate on how 
well the System did at matching past and present, 
and on the appropriateness of temporal connectives 
generated. 
8 Results 
We have applied the rules in (6) in generating 80 sen- 
tences in the corpus (starting from often ambiguous 
CLCS analyses). Evaluation is still tricky, since, in 
many cases, the interlingua nalysis is incorrect or 
ambiguous in ways that affect the appropriateness 
of the generated translation. 
8.1 Tense 
As mentioned above, evaluation can be very diffi- 
cult in a number of cases. Concerning tense, our 
"gold standard" is the set of human translations, 
generated  tense 
past p resent  
human past  134 17 
t rans la t ion  present  17 27 
Table 2: Preliminary Tense Results 
previously constructed for these sentences. In many 
cases, there is nothing overt in the sentence which 
would specify tense, so a mismatch might not actu- 
ally be "wrong". Also, there are a number of sen- 
tences which were not directly applicable for com- 
parison, such as when the human translator chose 
a different syntactic structure or a complex tense. 
The newspaper articles were divided into 80 sen- 
tences. Since some of these sentences were conjunc- 
tions, this yielded 99 tensed main verbs. These verbs 
either appeared in simple present, past, present or 
past perfect('has or had verb-t-ed), present or past 
imperfective (is verb-l-lag , was verb--I--lag) and their 
corresponding passive (is being kicked, was being 
kicked, have been kicked) forms. For cases like the 
present perfect ('has kicked), we noted the intended 
meaning ( e.g past activity) expressed by the verb 
as well as the verb's actual present perfective form. 
We scored the form as correct if the system trans- 
lated a present perfective with past tense meaning 
as a simple past or present perfective. There were 
10 instances where a verb in the human translation 
had no corresponding verb in the machine transla- 
tion, either due to incorrect omission or correct sub- 
stitution of the corresponding nominalization. We 
excluded these forms from consideration. If the sys- 
tem fails to supply a verb for independent reasons, 
our system clearly can't mark it with tense. The 
results of our evaluation are summarized in Table 2. 
These results definitely improve over our previ- 
ous heuristic, which was to always use past tense 
(assuming this to be the default mode for newspa- 
per article reporting). Results are also better than 
always picking present ense. These results seem to 
indicate that atelicity is a fairly good cue for present 
tense. We also note that 8 out of the 14 cases where 
the human translation used the present ense while 
the system used past tense are headlines. Headlines 
are written using the historical present in English 
("Man bites Dog"). These sentences would not be 
incorrectly translated in the past ("The Man Bit 
the Dog") Therefore, a fairer judgement might leave 
only remaining 6 incorrect cases in this cell. Using 
atelicity as a cue for the present yields correct re- 
sults approximately 65incorrect results 35worst case 
results because they do not take into account pres- 
ence or absence of the grammatical perfective and 
progressive markers referred to in the introduction. 
39 
8.2 Relat ionship between clauses 
Results are more preliminary for the clausal connec- 
tives. Of the 80 sentences, 35 of them are flagged as 
(possibly) containing events or states directly mod- 
ifying other events or states. However, of this num- 
ber, some actually do have lexical connectives repre- 
sented as featural rather than structural elements in 
the LCS, and can be straightforwardly realized using 
translated English connectives such as since, after, 
and if.then. Other apparently "modifying" events 
or states should be treated as a complement rela- 
tionship (at least according to the preferred reading 
in ambiguous cases), but are incorrectly analyzed 
as being in a non-complement relationship, or have 
other structural problems rendering the interlingua 
representation and English output not directly re- 
lated to the original clause structure. 
Of the remaining clear cases, six while relation- 
ships were generated according to our heuristics, in- 
dicating cotemporaneousness of main and modifying 
situation, e.g. (7)a,b, in the automated translations 
of (1) and (2), respectively. None were inappropri- 
ate. Of the cases where then was generated, indicat- 
ing sequential events, there were four cases in which 
this was appropriate, and three cases in which the 
situations really should have been cotemporaneous. 
While these numbers are small, this preliminary data 
seems to suggest again that atelicity is a good cue for 
cotemporality, while telicity is not a sufficient cue. 
(7) a. Before 1965, China altogether only have 
the ability shipbuilding about 300 thousand 
tons , while the annual output is 80 thou- 
sand tons. 
b. this 80 thousand tons actually includes 517 
ships, while the ship tonnage is very low. 
9 Conclus ions 
We therefore conclude that lexical aspect can serve 
as a valuable heuristic for suggesting tense, in the ab- 
sence of tense and other temporal markers. We an- 
ticipate incorporation of grammatical aspect infor- 
mation to improve our temporal representation fur- 
ther. In addition, lexical aspect, as represented by 
the interlingual LCS structure, can serve as the foun- 
dation for language specific heuristics. Furthermore, 
the lexical aspect represented in the LCS can help to 
provide the beginnings of cross-sentential discourse 
information. We have suggested applications in the 
temporal domain while, then. Causality is another 
possible domain in which relevant pieces encoded in 
sentence-level LCS structures could be used to pro- 
vide links between LCSes/sentences. Thus, the in- 
terlingual representation may be used to provide not 
only shared semantic and syntactic structure, but 
"also the building blocks for language-specific heuris- 
tics for mismatches between languages. 
10 Future  Research 
There are a number of other directions we intend to 
pursue in extending this work. First, we will evalu- 
ate the role of the grammatical spect markers men- 
tioned above, in combination with the telicity fea- 
tures. Second, we will also examine the role of the 
nature of the modifying situation. Third, we will 
incorporate other lexical information present in the 
sentence, including adverbial cue words (e.g. now, 
already and specific dates that have time-related in- 
formation, and distinguishing reported speech from 
other sentences. Finally, as mentioned, these re- 
sults do not take embedded verbs or verbs in adjunct 
clauses into account. Many adjunct and embedded 
clauses are tenseless, making evaluation more diffi- 
cult. For example, is The President believed China 
to be a threat equivalent to The president believed 
China is a threat). 
References 
Chauncey C. Chu. 1998. A Discourse Grammar of 
Mandarin Chinese. Peter Lang Publishing, Inc., 
New York, NY. 
Bernard Comrie. 1976. Aspect. Cambridge Univer- 
sity Press, Cambridge, MA. 
Bonnie J. Dorr and Terry Gaasterland. 1995. Se- 
lecting Tense, Aspect, and Connecting Words in 
Language Generation. In Proceedings of IJCAI- 
95, Montreal, Canada. 
Bonnie J. Dorr and Marl Broman Olsen. 1997a. As- 
pectual Modifications to a LCS Database for NLP 
Applications. Technical Report LAMP TR 007, 
UMIACS TR 97-23, CS TR 3763, University of 
Maryland, College Park, MD. 
Bonnie J. Dorr and Marl Broman Olsen. 1997b. 
Deriving Verbal and Compositional Lexical As- 
pect for NLP Applications. In Proceedings of the 
35th Annual Meeting of the Association for Com- 
putational Linguistics (ACL-97), pages 151-158, 
Madrid, SPain , July 7-12. 
Bonnie J. Doff, James Hendler, Scott Blanksteen, 
and Barrie Migdaloff. 1993. Use of Lexical Con- 
ceptual Structure for Intelligent Tutoring. Tech- 
nical Report UMIACS TR 93-108, CS TR 3161, 
University of Maryland. 
Bonnie J. Dorr, Nizar Habash, and David Traum. 
1998. A Thematic HieJfarchy for Efficient Gener- 
ation from Lexical-ConceptM Structure. In Pro- 
ceedings of the Third Conference of the Associ- 
ation for Machine Translation in the Americas, 
AMTA-98, in Lecture Notes in Artificial Intelli- 
gence, 15~9, pages 333-343, Langhorne, PA, Oc- 
tober 28-31. 
Bonnie J. Dorr. 1993. Machine Translation: A View 
from the Lexicon. The MIT Press, Cambridge, 
MA. 
4.0 
David Dowty. 1979. Word Meaning in Montague 
Grammar. Reidel, Dordrecht. 
David Dowty. 1986. The Effects of Aspectual Class 
on the Temporal Structure of Discourse: Seman- 
tics or Pragmatics? Linguistics and Philosophy, 
9:37-61. 
Mohamed Guerssel, Kenneth Hale, Mary Laugh- 
ten, Beth Levin, and Josie White Eagle. 1985. 
A Cross-linguistic Study of Transitivity Alterna- 
tions. In W. H. Eilfort, P. D. Kroeber, and K. L. 
Peterson, editors, Papers from the Parasession on 
Causatives and Agentivity at the Twenty.First Re- 
gional Meeting, CLS P1, Part P, pages 48-63. The 
Chicago Linguistic Society, Chicago, IL, April. 
Ray Jackendoff. 1983. Semantics and Cognition. 
The MIT P.r?ss, Cambridge, MA. 
Ray Jackendoff. 1990.. Semantic Structures. The 
MIT Press, Cambridge, MA. 
Irene Langkilde and Kevin Knight. 1998a. Gen- 
eratiort that Exploits Corpus-Based Statistical 
Knowledge. In Proceedings of COLING-ACL '98, 
pages 704-710. 
Irene Langkilde and Kevin Knight. 1998b. The 
Practical Value of N-Grams in Generation. In In- 
ternational Natural Language Generation Work- 
shop. 
Beth Levin and Malka Rappaport Hovav. 1995. Un- 
accusativity: At the Syntaz-Lexical Semantics In- 
terface. The MIT Press, Cambridge, MA. LI 
Monograph 26. 
Beth Levin. 1993. English Verb Classes and Alter- 
nations: A Preliminary Investigation. University 
of Chicago Press, Chicago, IL. 
Charles Li and Sandra Thompson. 1981. Mandarin 
Chinese: A functional reference grammar. Uni- 
versity of California Press, Berkeley, CA. 
Marc Moens and Mark Steedman. 1988. Tempo- 
ral Ontology and Temporal Reference. Compu- 
lational Linguistics: Special Issue on Tense and 
Aspect, 14(2):15-28. 
Mart Broman Olsen, Bonnie J. Dorr, and Scott C. 
Thomas. 1998. Enhancing Automatic Acquisi- 
tion of Thematic Structure in a Large-Scale Lex- 
icon for Mandarin Chinese. In Proceedings of the 
Third Conference of the Association for Machine 
Translation in the Americas, AMTA-98, in Lec- 
ture Notes in Artificial Intelligence, 1529, pages 
41-50, Langhorne, PA, October 28-31. 
Mart Broman Olsen. 1997. A Semantic and Prag- 
matic Model of Lexical and Grammatical Aspect. 
Garland, New York. 
Rebecca Passoneau. 1988. A Computational Model 
of the Semantics of Tense and Aspect. Compu- 
tational Linguistics: Special Issue on Tense and 
Aspect, 14(2):44-60. 
Malka P~appaport Hovav and Beth Levin. 1995. 
The Elasticity of Verb Meaning. In Processes in 
Argument Structure, pages 1-13, Germany. SfS- 
Report-06-95, Seminar fiir Sprachwissenschaft, 
Eberhard-Karls-Universit~t Tiibingen, Tiibingen. 
Carlota Smith. 1997. The parameter of aspect. 
Kluwer, Dordrecht, 2nd edition. 
41 
Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 60?68,
New York City, USA, June 2006. c?2006 Association for Computational Linguistics
Morphology Induction from Limited Noisy Data
Using Approximate String Matching
Burcu Karagol-Ayan, David Doermann, and Amy Weinberg
Institute for Advanced Computer Studies (UMIACS)
University of Maryland
College Park, MD 20742
{burcu,doermann,weinberg}@umiacs.umd.edu
Abstract
For a language with limited resources, a
dictionary may be one of the few available
electronic resources. To make effective
use of the dictionary for translation, how-
ever, users must be able to access it us-
ing the root form of morphologically de-
formed variant found in the text. Stem-
ming and data driven methods, however,
are not suitable when data is sparse. We
present algorithms for discovering mor-
phemes from limited, noisy data obtained
by scanning a hard copy dictionary. Our
approach is based on the novel applica-
tion of the longest common substring and
string edit distance metrics. Results show
that these algorithms can in fact segment
words into roots and affixes from the lim-
ited data contained in a dictionary, and ex-
tract affixes. This in turn allows non na-
tive speakers to perform multilingual tasks
for applications where response must be
rapid, and their knowledge is limited. In
addition, this analysis can feed other NLP
tools requiring lexicons.
1 Introduction
In order to develop morphological analyzers for lan-
guages that have limited resources (either in terms of
experienced linguists, or electronic data), we must
move beyond data intensive methods developed for
rich resource languages that rely on large amounts
of data for statistical methods. New approaches that
can deal with limited, and perhaps noisy, data are
necessary for these languages.
Printed dictionaries often exist for languages be-
fore large amounts of electronic text, and provide
a variety of information in a structured format. In
this paper, we propose Morphology Induction from
Noisy Data (MIND), a natural language morphology
induction framework that operates on from informa-
tion in dictionaries, specifically headwords and ex-
amples of usage. We use string searching algorithms
to morphologically segment words and identify pre-
fixes, suffixes, circumfixes, and infixes in noisy and
limited data. We present our preliminary results on
two data sources (Cebuano and Turkish), give a de-
tailed analysis of results, and compare them to a
state-of-the-art morphology learner. We employ the
automatically induced affixes in a simple word seg-
mentation process, decreasing the error rate of in-
correctly segmented words by 35.41%.
The next section discusses prior work on mor-
phology learning. In Section 3 and 4, we describe
our approach and MIND framework in detail. Sec-
tion 6 explains the experiments and presents results.
We conclude with future work.
2 Related Work
Much of the previous work on morphology learning
has been reported on automatically acquiring affix
lists. Inspired by works of Harris (1955), Dejean
(1998) attempted to find a list of frequent affixes
for several languages. He used successor and pre-
decessor frequencies of letters in a given sequence
of letters in identifying possible morpheme bound-
60
aries. The morpheme boundaries are where the pre-
dictability of the next letter in the letter sequence is
the lowest.
Several researchers (Brent, 1993; Brent et al,
1995; Goldsmith, 2001) used Minimum Description
Length (MDL) for morphology learning. Snover
and Brent (2001) proposed a generative probabil-
ity model to identify stems and suffixes. Schone
and Jurafsky (2001) used latent semantic analysis
to find affixes. Baroni et al (2002) produced a
ranked list of morphologically related pairs from
a corpus using orthographic or semantic similarity
with minimum edit distance and mutual informa-
tion metrics. Creutz and Lagus (2002) proposed
two unsupervised methods for word segmentation,
one based on maximum description length, and one
based on maximum likelihood. In their model,
words consisted of lengthy sequences of segments
and there is no distinction between stems and af-
fixes. The Whole Word Morphologizer (Neuvel and
Fulop, 2002) uses a POS-tagged lexicon as input, in-
duces morphological relationships without attempt-
ing to discover or identify morphemes. It is also ca-
pable of generating new words beyond the learning
sample.
Mystem (Segalovich, 2003) uses a dictionary for
unknown word guessing in a morphological analysis
algorithm for web search engines. Using a very sim-
ple idea of morphological similarity, unknown word
morphology is taken from all the closest words in
the dictionary, where the closeness is the number of
letters on its end.
The WordFrame model (Wicentowski, 2004) uses
inflection-root pairs, where unseen inflections are
transformed into their corresponding root forms.
The model works with imperfect data, and can han-
dle prefixes, suffixes, stem-internal vowel shifts, and
point-of-affixation stem changes. The WordFrame
model can be used for co-training with low-accuracy
unsupervised algorithms.
Monson (2004) concentrated on languages with
limited resources. The proposed language-
independent framework used a corpus of full word
forms. Candidate suffixes are grouped into candi-
date inflection classes, which are then arranged in a
lattice structure.
A recent work (Goldsmith et al, 2005) proposed
to use string edit distance algorithm as a bootstrap-
ping heuristic to analyze languages with rich mor-
phologies. String edit distance is used for rank-
ing and quantifying the robustness of morphological
generalizations in a set of clean data.
All these methods require clean and most of the
time large amounts of data, which may not exist
for languages with limited electronic resources. For
such languages, the morphology induction is still a
problem. The work in this paper is applicable to
noisy and limited data. String searching algorithms
are used with information found in dictionaries to
extract the affixes.
3 Approach
Dictionary entries contain headwords, and the exam-
ples of how these words are used in context (i.e. ex-
amples of usage). Our algorithm assumes that each
example of usage will contain at least one instance
of the headword, either in its root form, or as one
of its morphological variants. For each headword?
example of usage pair, we find the headword occur-
rence in the example of usage, and extract the affix
if the headword is in one of its morphological vari-
ants. We should note that we do not require the data
to be perfect. It may have noise such as OCR errors,
and our approach successfully identifies the affixes
in such noisy data.
4 Framework
Our framework has two stages, exact match and ap-
proximate match, and uses three string distance met-
rics, the longest common substring (LCS), approx-
imate string matching with k differences (k-DIFF),
and string edit distance (SED). We differentiate be-
tween exact and approximate matches and assign
two counts for each identified affix, exact count
and approximate count. We require that each affix
should have a positive exact count in order to be in
the final affix list. Although approximate match can
be used to find exact matches to identify prefixes,
suffixes, and circumfixes, it is not possible to differ-
entiate between infixes and OCR errors. For these
reasons, we process the two cases separately.
First we briefly describe the three metrics we use
and the adaptations we made to find the edit opera-
tions in SED, and then we explain how we use these
metrics in our framework.
61
4.1 String Searching Algorithms
Longest Common Substring (LCS) Given two
strings p = p1...pn and q = q1...qm, LCS finds the
longest contiguous sequence appearing in p and q.
The longest common substring is not same as the
longest common subsequence because the longest
common subsequence need not be contiguous.
There is a dynamic programming solution for
LCS1 that finds the longest common substring for
two strings with length n and m in O(nm).
String Edit Distance (SED) Given two strings p
and q, SED is the minimum number of edit opera-
tions which transforms p to q. The edit operations al-
lowed are insertions, deletions, and substitutions. In
our algorithm, we set the cost of each edit operation
to 1. A solution based on dynamic programming
computes the distance between strings in O(mn),
where m and n are the lengths of the strings (Wag-
ner and Fischer, 1974).
Approximate string matching with k differ-
ences (k-DIFF) Given two strings p and q, the prob-
lem of approximate string matching with k differ-
ences is finding all the substrings of q which are
at a distance less than or equal to a given value k
from p. Insertions, deletions and substitutions are
all allowed. A dynamic programming solution to
this problem is the same as the classical string edit
distance solution with one difference: the values of
the first row of the table are initialized to 0 (Sellers,
1980). This initialization means that the cost of in-
sertions of letters of q at the beginning of p is zero.
The solutions are all the values of the last row of ta-
ble which are less or equal to k. Consequently, the
minimum value on the last row gives us the distance
of the closest occurrence of the pattern.
String Edit Distance with Edit Operations
(SED-path) In our framework, we are also inter-
ested in tracing back the editing operations per-
formed in achieving the minimum cost alignment.
In order to obtain the sequence of edit operations,
we can work backwards from the complete distance
matrix. For two strings p and q with lengths n and
m respectively, the cell L[n,m] of the distance ma-
trix L gives us the SED between p and q. To get
to the cell L[n,m], we had to come from one of 1)
L[n ? 1,m] (insertion), 2) L[n,m ? 1] (deletion),
1http://www.ics.uci.edu/ dan/class/161/notes/6/Dynamic.html
or 3) L[n ? 1,m ? 1] (substitution). Which of the
three options was chosen can be reconstructed given
these costs, edit operation costs, and the characters
p[n], q[m] of the strings. By working backwards,
we can trace the entire path and thus reconstruct the
alignment. However, there are ambiguous cases; the
same minimum cost may be obtained by a number
of edit operation sequences. We adapted the trace of
the path for our purposes as explained below.
Let path be the list of editing operations to obtain
minimum distance, and SED-path be the SED algo-
rithm that also returns a path. The length of the path
is max(n,m), and path[j] contains the edit oper-
ation to change q[j] (or p[j] if n > m). Path can
contain four different types of operations: Match
(M), substitution (S), insertion (I), and deletion (D).
Our goal is finding affixes and in case of ambiguity,
we employed the following heuristics for finding the
SED operations leading the minimum distance:
Case 1: If one string is longer than the other, choose
I for extra characters
Case 2: Until an M is found, choose I in case of
ambiguity
Case 3: If an M is found previously, choose M/S in
case of ambiguity
Case 4: If there is an M between two I?s, switch this
with the last I
Case 1 ensures that if one word has more charac-
ters than the other, an insertion operation is selected
for those characters.
If there is an ambiguity, and an M/S or I oper-
ation have the same minimum cost, Case 2 gives
priority to the insertion operation until a match
case is encountered, while Case 3 gives priority to
match/substitution operations if a match case was
seen previously.
Below example shows how Case 4 helps us
to localize all the insertion operations. For the
headword?candidate example word pair abirids ?
makaabir??ds, the path changes from (1) to (2) using
Case 4, and correct prefix is identified as we explain
in the next section.
(1) I M I I I M M M S M M? Prefix m-
(2) I I I I M M M M S M M? Prefix maka-
62
5 Morphology Induction from Noisy Data
(MIND)
The MIND framework consists of two stages. In the
exact match stage, MIND framework checks if the
headword occurs without any changes or errors (i.e.
if headword occurs exactly in the example of us-
age). If no such occurrence is found an approximate
match search is performed in second stage. Below
we describe these two stages in detail.
5.1 Exact Match
Given a list of (noisy) headword?example of usage
pairs (w,E), the exact match first checks if the head-
word occurs in E in its root form.2 If the headword
cannot be found in E in its root form, for each ei
in E, the longest common substring, LCS(w, ei),
is computed.3 Let el be the ei that has the longest
common substring (l) with w.4 If w = l, and for
some suffix s and some prefix p one of the following
conditions is true, the affix is extracted.
1. el = ws (suffix) or
2. el = pw (prefix) or
3. el = pws (circumfix)
The extracted affixes are added to the induced af-
fix list, and their exact counts are incremented. In
the third case p?s is treated together as a circumfix.
For the infixes, there is one further step. If w =
w?l and el = e?ll, we compute LCS(w?, e?l). If e?l =
w?s, for some suffix s, s is added as an infix to the
induced affix list. (This means el = w?sl wherew =
w?l.)
The following sample run illustrates how the ex-
act match part identifies affixes. Given the Ce-
buano headword?example of usage pair (abtik) ?
(naabtikan sad ku sa ba?ta?), the word naabtikan is
marked as the candidate that has the longest com-
mon substring with headword abtik. These two
words have the following alignment, and we ex-
tract the circumfix na?an. In the illustration below,
2Headwords consisting of one character are not checked.
3In order to reduce the search space, we do not check the
example words that are shorter than the headword. Although
there are some languages, such as Russian, in which headwords
may be longer than the inflected forms, such cases are not in the
scope of this paper.
4Note that the length of the longest common substring can
be at most the length of the headword, in which case the longest
common substring is the headword itself.
straight lines represent matches, and short lines end-
ing in square boxes represent insertions.
5.2 Approximate Match
When we cannot find an exact match, there may be
an approximate match resulting from an error with
OCR or morphophonemic rules5, and we deal with
such cases separately in the second part of the al-
gorithm. For each ei in E, we compute the dif-
ference between headword, and example word, k-
DIFF(w, ei). The example word that has the min-
imum difference from the headword is selected as
the most likely candidate (ecand). We then find the
sequence of the edit operations performed in achiev-
ing the minimum distance alignment to transform
ecand to w using SED-path algorithm we described
above.6
Let cnt(X) be the count of X operation in the
computed path. If cnt(I) = 0, this case is consid-
ered as an approximate root form (with OCR errors).
The following conditions are considered as possible
errors and no further analysis is done for such cases:
cnt(M) = 0 ||
cnt(M) < max(cnt(S), cnt(D), cnt(I)) ||
cnt(M) < cnt(S) + cnt(D) + cnt(I)
Otherwise, we use the insertion operations at the
beginning and at the end of the path to identify the
type of the affix (prefix, suffix, or circumfix) and the
length of the suffix (number of insertion operations).
The identified affix is added to the affix list, and
its approximate count is incremented. All the other
cases are dismissed as errors. In its current state, the
infix affixes are not handled in approximate match
case.
The following sample shows how approximate
match works with noisy data. In the Cebuano input
5At this initial version, MIND does not make any distinc-
tions between noise in the data such as OCR errors, and mor-
phophonemic rules. Making this distinction will be one of our
future focuses
6Computing k-difference, and the edit path can be done in
parallel to reduce the computing time.
63
pair (ambihas) ? (ambsha?sa pagbutang ang duha
ka silya arun makakita? ang maglingkud sa luyu), the
first word in the example of usage has an OCR er-
ror, i is misrecognized as s. Moreover, there is a
vowel change in the word caused by the affix. An
exact match of the headword cannot be found in the
example of usage. The k-DIFF algorithm returns
ambsha?sa as the candidate example of usage word,
with a distance 2. Then, the SED-path algorithm
returns the path M M M S M S M I, and algorithm
successfully concludes that a is the suffix as shown
below in illustration (dotted lines represent substitu-
tions).
6 Experiments
6.1 Dictionaries
The BRIDGE system (Ma et al, 2003) processes
scanned and OCRed dictionaries to reproduce elec-
tronic versions and extract information from dictio-
nary entries. We used the BRIDGE system to pro-
cess two bilingual dictionaries, a Cebuano-English
(CebEng) dictionary (Wolff, 1972) and a Turkish-
English (TurEng) dictionary (Avery et al, 1974),
and extract a list of headword-example of usage
pairs for our experiments. The extracted data is not
perfect: it has mistagged information, i.e. it may in-
clude some information that is not the headword or
example of usage, or some useful information may
be missing, and OCR errors may occur. OCR errors
can be in different forms: Two words can be merged
into one, one word can be split into two, or charac-
ters can be misrecognized.
Dictionary # of # of # of
Dictionary pages hw-ex pairs words
Cebuano-all 1163 27129 206149
Turkish-all 1000 27487 111334
Cebuano-20 20 562 4134
Turkish-20 20 503 1849
Table 1: Details of Data from Two Dictionaries Used
in Experiments
Along with the headword?example of usage pairs
from more than 1000 pages, we randomly selected
20 pages for detailed analysis. Table 1 provides de-
tails of the data from two dictionaries we use in our
experiments.
Both Cebuano and Turkish are morphologically
rich. Cebuano allows prefixes, suffixes, circumfixes,
infixes, while Turkish is an agglunative language.
The two dictionaries have different characteristics.
The example of usages in CebEng are complete sen-
tences given in italic font while TurEng has phrases,
idioms, or complete sentences as examples of usages
indicated in bold font.
6.2 Protocol
We ran our algorithm first on all of the data and then
on a randomly selected 20 pages from each dictio-
nary. We manually extracted the affixes from each
of the 20 pages. We then evaluated the MIND re-
sults with this ground truth. During the evaluation,
even if the number of an affix in the ground truth and
result are same, if they were extracted from different
words, this is counted as an error. We also examined
the cause of each error in this data.
We then compare our results from the whole
TurEng data with the state-of-the-art Linguistica
(Goldsmith, 2001) algorithm. Finally, we used the
suffixes extracted by MIND and Linguistica to seg-
ment words in a Turkish treebank.
6.3 Analysis
Dict. Affix Sample words
mu- galing/mugaling hiku?h??ku?/muhiku`h??ku`
C nag- kisdum/nagkisdum kugkugl/nagkugkug
E mi- iktin/miiktin k??rus/mika?rus
B i- kunsuylu/ikunsuylu paz??ha/ipar??ha
U na- p??l/nap??l ulatl/nau?lat
A gi- buga/gibuga da?lit/gida?dit
N gi-an labuk/gilabukan ??kug/giiku?gan
O -un gihay/gihayun ga?yung/gayu?ngun
-a pisar/pisara sirnpul/simpu?la
-? ad/ad? ilac?/ilae?
T -i heves/hevesi ilim/ilmi
U -a saz/saza sonsuz/sonsuza
R -e deniz/denize zmim/mime
K -?na etraf/etraf?na kolay/kolay?na
I -ya hasta/hastaya orta/ortaya
S -u? u?st/u?stu? zyu?z/yu?zu?
H -ini bel/belini zevk/zevkini
-ine derin/derinine ic?/ic?ine
Table 3: Sample Affixes Extracted from Two Dictio-
naries
Table 2 shows result of MIND runs. The total
number of affixes and number of different types of
64
Cebuano Turkish
Whole dict. 20 pages Whole dict. 20 pages
Total 26106 542 27314 502
Root form 5727 180 18416 345
Prefix (diff. type) 10300 (180) 197 (26) 6 (6) 0 (0)
Suffix (diff. type) 1315 (253) 16 (8) 6983 (447) 128 (59)
Infix (diff. type) 25 (11) 0 (0) 1 (1) 0 (0)
Circumfix (diff. type) 717 (221) 18 (11) 9 (9) 0 (0)
App. Root form 1023 14 103 1
App. Prefix (diff. type) 1697 (116) 23 (9) 8 (8) 1 (1)
App. Suffix (diff. type) 2930 (199) 63 (19) 168 (100) 5 (5)
App. Circumfix (diff. type) 1060 (207) 14 (5) 20 (20) 0 (0)
Couldn?t decide 1159 13 765 15
Table 2: Total Number and Different Types of Affixes Extracted from Two Dictionaries Using MIND
affixes (in parenthesis) are presented for two dictio-
naries, CebEng and TurEng, and two data sets, the
whole dictionary and 20 randomly selected pages.
The top part of the table gives the exact match results
and the bottom part shows the approximate match
results. For Cebuano, approximate match part of the
framework finds many more affixes than it does for
Turkish. This is due to the different structures in
the two dictionaries. We should note that although
MIND incorrectly finds a few prefixes, circumfixes,
and infixes for Turkish, these all have count one.
Table 3 contains some of the most frequent ex-
tracted affixes along with their exact and approxi-
mate counts, and samples of headword?example of
usage word pairs they were extracted from. Each
word is segmented into one root and one suffix,
therefore when a word takes multiple affixes, they
are all treated as a compound affix.
Dictionary GT cnt. Res.cnt. Misses Additions
Cebuano 311 314 17 14
Turkish 155 142 8 10
Table 4: Detailed Analysis of Affixes from 20 Pages
Table 4 shows the number of affixes in ground
truth and MIND results along with number of
missed and incorrectly added affixes on 20 of these
pages of data. MIND only missed 5% of the affixes
in the ground truth in both data sets.
We also examined the causes of each miss and ad-
dition. Table 5 presents the causes of errors in the
output of MIND with an example for each cause. We
should emphasize that a valid affix such as Turkish
suffix -m? is counted as an error since the suffix -
?n? should be extracted for that particular headword?
example of usage pair. An OCR error such as the
misrecognition of a as d, causes both the miss of the
prefix mag- and incorrect addition of mdg- for Ce-
buano. There are some cases that cannot be correctly
identified by the framework. These usually involve
dropping the last vowel because of morphophone-
mic rules. For the Cebuano dictionary, merge and
split caused several errors, while Turkish data does
not have any such errors. Main reason is the differ-
ent structure and format of the original dictionaries.
In the Cebuano dictionary, an italic font which may
result in merge and split is used to indicate example
of usages.
For the Cebuano data, five invalid suffixes, three
invalid prefixes, and two invalid circumfixes are
found, while one valid suffix and one valid circumfix
are missed. For the Turkish data, three invalid suf-
fixes, one invalid prefix, and two valid suffixes are
found while two valid suffix are missed. When we
look at the invalid affixes in the data, most of them
(six of the Cebuano, and all of the Turkish ones)
have count one, and maximum count in an invalid
affix is five. Therefore, if we use a low threshold,
we can eliminate many of the invalid affixes.
6.4 Comparison to Linguistica
We compared our system with Linguistica, a pub-
licly available unsupervised corpus-based morphol-
ogy learner (Goldsmith, 2001). Linguistica induces
paradigms in a noise-free corpus, while MIND
makes use of string searching algorithms and allows
one to deal with noise at the cost of correctness.
MIND emphasize segmenting a word into its root
and affixes. We trained Linguistica using two dif-
ferent data sets from TurEng7: 1) Whole headword-
7We would like to do the same comparison in Cebuano. For
the time being, we could not find a treebank and native speakers
65
Reason Cebuano Turkish
OCR 8 M?lbi 11 ?n??m? or ?m
Algorithm 8 (uluy, giuylan)? 7 (al?n, aln?nda)?
not gi-an, -lan is found not -?nda, -da is found
Merge 9 ??mung gila?ug???munggila?ug 0 -
Split 1 nag-ku?gus?nag- ku?gus 0 -
Other 5 apr.?april 0 -
Headword is an abbreviation
Table 5: The Distribution of the Causes of Errors in 20 Pages with Samples
example of usage sentence pairs, and 2) Headword-
candidate example words that our algorithm returns.
In the first case (Ling-all), Linguistica uses more
data than our algorithm, so to avoid any biases re-
sulting from this, we also trained Linguistica using
the headword and candidate example word (Ling-
cand). We only used the suffixes, since Turkish is a
suffix-based language. The evaluation is done by a
native speaker.
Figure 1 presents the analysis of the suffix lists
produced by Linguistica using two sets of training
data, and MIND. The suffix lists are composed of
suffixes the systems return that have counts more
than a threshold. The results are presented for six
threshold values for all of the data. We use thresh-
olding to decrease the number of invalid affixes
caused primarily by the noise in the data. For the
MIND results, the suffixes over threshold are the
ones that have positive exact counts and total counts
(sum of exact and approximate counts) more than
the threshold. Although Linguistica is not designed
for thresholding, the data we use is noisy, and we
explored if suffixes with a corpus count more than
a threshold will eliminate invalid suffixes. The ta-
ble on the left gives the total number of suffixes,
the percentage of suffixes that have a count more
than a threshold value, the percentage of invalid suf-
fixes, and percentage of missed suffixes that are dis-
carded by thresholding for the whole TurEng dictio-
nary. The number of affixes MIND finds are much
more than that of Linguistica. Furthermore, number
of invalid affixes are lower. On the other hand, the
number of missed affixes is also higher for MIND
since, for this particular data, there are many affixes
with counts less than 5. 41% of the affixes have an
exact count of 1. The main reason for this is the
agglunative nature of Turkish language. The effect
of thresholding can also be examined in the graph
for Cebuano.
on the right in Figure1 which gives the percentage
of valid suffixes as a function of threshold values.
MIND takes advantage of thresholding, and percent-
age of valid suffixes rapidly decrease for threshold
value 1.
System Th. Total Over Th. Invalid Missed
Ling-cand 0 6 100.00 0.00 0.00
Ling-all 0 4 100.00 0.00 0.00
MIND 0 60 96.67 1.72 0.00
Ling-cand 1 6 66.67 0.00 33.33
Ling-all 1 4 100.00 0.00 0.00
MIND 1 60 41.67 0.00 53.33
Ling-cand 2 6 50.00 0.00 50.00
Ling-all 2 4 75.00 0.00 25.00
MIND 2 60 18.33 0.00 76.67
Table 6: Total Number and Percentage of Over the
Threshold, Invalid, and Missed Suffixes Found by
Linguistica and MIND for Different Threshold Val-
ues for 20 pages of Turkish Data
Table 6 presents the same results for 20 pages
from TurEng for three threshold values. MIND per-
forms well even with very small data and finds many
valid affixes. Linguistica on the other hand finds
very few.
6.5 Stemming
To test the utility of the results, we perform a sim-
ple word segmentation, with the aim of stripping the
inflectional suffixes, and find the bare form of the
word. A word segmenter takes a list of suffixes, and
their counts from the morphology induction system
(Linguistica or MIND), a headword list as a dictio-
nary, a threshold value, and the words from a tree-
bank. For each word in the treebank, there is a root
form (rf ), and a usage form (uf ). The suffixes with
a count more than the threshold are indexed accord-
ing to their last letters. For each word in the tree-
bank, we first check if uf is already in the dictio-
nary, i.e. in the headword list. If we cannot find it
66
System Th. Total % Over Th. % Invalid % Missed
Ling-cand 0 116 100.00 18.10 0.00
Ling-all 0 274 100.00 34.67 0.00
MIND 0 499 89.58 13.20 3.61
Ling-cand 1 116 98.28 17.54 0.86
Ling-all 1 274 94.89 32.69 1.46
MIND 1 499 50.50 4.37 33.07
Ling-cand 2 116 92.24 16.82 5.17
Ling-all 2 274 87.96 31.12 4.74
MIND 2 499 38.48 4.17 44.49
Ling-cand 3 116 91.38 16.98 6.03
Ling-all 3 274 85.40 31.20 6.57
MIND 3 499 28.86 2.78 53.31
Ling-cand 4 116 81.03 12.77 11.21
Ling-all 4 274 81.39 30.94 9.12
MIND 4 499 25.65 3.13 56.51
Ling-cand 5 116 80.17 12.90 12.07
Ling-all 5 274 79.56 31.19 10.58
MIND 5 499 23.25 2.59 58.72
Figure 1: Total Number and Percentage of Over the Threshold, Invalid, Missed and Valid Suffixes Found by
Linguistica and MIND for Different Threshold Values
in the dictionary, we repeatedly attempt to find the
longest suffix that matches the end of uf , and check
the dictionary again. The process stops when a dic-
tionary word is found or when no matching suffixes
can be found at the end of the word. If the word the
segmenter returns is same as rf in the treebank, we
increase the correct count. Otherwise, this case is
counted as an error.
In our stemming experiments we used METU-
Sabanci Turkish Treebank8, a morphologically and
syntactically annotated treebank corpus of 7262
grammatical sentences (Atalay et al, 2003; Oflazer
et al, 2003). We skipped the punctuation and mul-
tiple parses,9 and ran our word segmentation on
14950 unique words. We also used the headword
list extracted from TurEng as the dictionary. Note
that, the headword list is not error-free, it has OCR
errors. Therefore even if the word segmenter returns
the correct root form, it may not be in the dictionary
and the word may be stripped further.
The percentage of correctly segmented words are
presented in Figure 2. We show results for six
threshold values. Suffixes with counts more than the
threshold are used in each case. Again for MIND
results, we require that the exact match counts are
more than zero, and the total of exact match and ap-
8http://www.ii.metu.edu.tr/ corpus/treebank.html
9Multiple parses are the cases where a suffix is attached not
to a single word, but to a group of words. The suffix -ti in takip
etti is attached to takip et.
Figure 2: Percentage of Correctly Segmented Words
by Different Systems for Different Threshold Values
proximate match counts are more than the thresh-
old. For Linguistica, suffixes with a corpus count
more than the threshold are used. For each thresh-
old value, MIND did much better than Ling-cand.
MIND outperformed Ling-all for thresholds 0 and
1. For the other values, the difference is small. We
should note that Ling-all uses much more training
data than MIND (503 vs. 1849 example of words),
and even with this difference the performance of
MIND is close to Ling-all. We believe the reason
for the close performance of MIND and Ling-all in
segmentation despite the huge difference in the num-
ber of correct affixes they found due to the fact that
affixes Ling-all finds are shorter, and more frequent.
In its current state, MIND does not segment com-
pound affixes, and find several long and less fre-
quent affixes. These long affixes can be composed
67
by shorter affixes Linguistica finds.
7 Conclusion and Future Work
We presented a framework for morphology induc-
tion from noisy data, that is especially useful for lan-
guages which have limited electronic data. We use
the information in dictionaries, specifically head-
word and the corresponding example of usage sen-
tences, to acquire affix lists of the language. We pre-
sented results on two data sets and demonstrated that
our framework successfully finds the prefixes, suf-
fixes, circumfixes, and infixes. We also used the ac-
quired suffix list from one data set in a simple word
segmentation process, and outperformed a state-of-
the-art morphology learner using the same amount
of training data.
At this point we are only using headword and
corresponding example of usage pairs. Dictionaries
provide much more information. We plan to make
use of other information, such as POS, to categorize
the acquired affixes. We will also investigate how
using all the words in example of usages and split-
ting the compound affixes in agglunative languages
can help us to increase the confidence of correct af-
fixes, and decrease the number of invalid affixes.
Finally we will work on identifying morphophone-
mic rules (especially stem-interval vowel shifts and
point-of-affixation stem changes).
Acknowledgments
The partial support of this research under contract
MDA-9040-2C-0406 is gratefully acknowledged.
References
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003. The an-
notation process in the Turkish Treebank. In Proceedings of
the EACL Workshop on Linguistically Interpreted Corpora?
LINC, Budapest, Hungary, April.
Robert Avery, Serap Bezmez, Anna G. Edmonds, and Mehlika
Yaylal?. 1974. Redhouse ?Ingilizce-Tu?rkc?e So?zlu?k. Red-
house Yay?nevi.
Marco Baroni, Johannes Matiasek, and Harald Trost. 2002.
Unsupervised discovery of morphologically related words
based on orthographic and semantic similarity. In Proceed-
ings of the ACL-02 Workshop on Morphological and Phono-
logical Learning, pages 48?57.
Michael R. Brent, Sreerama K. Murthy, and Andrew Lundberg.
1995. Discovering morphemic suffixes: A case study in
minimum description length induction. In Proceedings of
the 15th Annual Conference of the Cognitive Science Soci-
ety, pages 28?36, Hillsdale, NJ.
Michael R. Brent. 1993. Minimal generative models: A mid-
dle ground between neurons and triggers. In Proceedings of
the 5th International Workshop on Artificial Intelligence and
Statistics, Ft. Laudersdale, FL.
Mathias Creutz and Krista Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACL-02 Workshop
on Morphological and Phonological Learning.
H. Dejean. 1998. Morphemes as necessary concepts for struc-
tures: Discovery from untagged corpora. In Workshop on
Paradigms and Grounding in Natural Language Learning,
pages 295?299.
John Goldsmith, Yu Hu, Irina Matveeva, and Colin Sprague.
2005. A heuristic for morpheme discovery based on string
edit distance. Technical Report TR-2205-04, Department of
Computer Science, University of Chicago.
John Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguistics,
27(2):153?198.
Zellig Harris. 1955. From phoneme to morpheme. Language,
31:190?222.
Huanfeng Ma, Burcu Karagol-Ayan, David Doermann, Dou-
glas Oard, and Jianqiang Wang. 2003. Parsing and tag-
ging of bilingual dictionaries. Traitement Automatique Des
Langues, pages 125?150.
Christian Monson. 2004. A framework for unsupervised nat-
ural language morphology induction. In Proceedings of the
Student Research Workshop: ACL 2004, pages 67?72.
Sylvain Neuvel and Sean A. Fulop. 2002. Unsupervised learn-
ing of morphology without morphemes. In Proceedings of
the ACL-02 Workshop on Morphological and Phonological
Learning, pages 31?40.
Kemal Oflazer, Bilge Say, Dilek Hakkani-Tu?r, and Go?khan Tu?r.
2003. Building a Turkish Treebank. In Anne Abeille?, edi-
tor, Building and Using Parsed Corpora. Kluwer Academic
Publishers.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-free
induction of inflectional morphologies. In Second Meeting
of the NAACL, pages 183?191.
Ilya Segalovich. 2003. A fast morphological algorithm with
unknown word guessing induced by a dictionary for a web
search engine. In Proceedings of MLMTA, Las Vegas, NV.
P.H. Sellers. 1980. The theory and computation of evolution-
ary distances: pattern recognition. Journal of Algorithms,
1:359?373.
Matthew G. Snover and Michael R. Brent. 2001. A bayesian
model for morpheme and paradigm identification. In Pro-
ceedings of the 39th Annual Meeting of the ACL, pages 482?
490.
Robert A. Wagner and Michael J. Fischer. 1974. The string-
to-string correction problem. Journal of the Association for
Computing Machinery, 21(1):168?173.
Richard Wicentowski. 2004. Multilingual noise-robust super-
vised morphological analysis using the wordframe model.
In Proceedings of the 7th Meeting of the ACL Special In-
terest Group in Computational Phonology, pages 70?77,
Barcelona, Spain.
John U. Wolff. 1972. A Dictionary of Cebuano Visaya. South-
east Asia Program, Cornell University, Ithaca, New York.
68
Coling 2010: Poster Volume, pages 1095?1103,
Beijing, August 2010
?Expresses-an-opinion-about?: using corpus statistics in an information
extraction approach to opinion mining
Asad B. Sayeed, Hieu C. Nguyen,
and Timothy J. Meyer
Department of Computer Science
University of Maryland, College Park
asayeed@cs.umd.edu,
hcnguyen88@gmail.com,
tmeyer1@umd.edu
Amy Weinberg
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland, College Park
weinberg@umiacs.umd.edu
Abstract
We present a technique for identifying the
sources and targets of opinions without
actually identifying the opinions them-
selves. We are able to use an informa-
tion extraction approach that treats opin-
ion mining as relation mining; we iden-
tify instances of a binary ?expresses-an-
opinion-about? relation. We find that
we can classify source-target pairs as be-
longing to the relation at a performance
level significantly higher than two relevant
baselines.
This technique is particularly suited to
emerging approaches in corpus-based so-
cial science which focus on aggregating
interactions between sources to determine
their effects on socio-economically sig-
nificant targets. Our application is the
analysis of information technology (IT)
innovations. This is an example of a
more general problem where opinion is
expressed using either sub- or supersets
of expressive words found in newswire.
We present an annotation scheme and an
SVM-based technique that uses the lo-
cal context as well as the corpus-wide
frequency of a source-target pair as data
to determine membership in ?expresses-
an-opinion-about?. While the presence
of conventional subjectivity keywords ap-
pears significant in the success of this
technique, we are able to find the most
domain-relevant keywords without sacri-
ficing recall.
1 Introduction
Two problems in sentiment analysis consist of
source attribution and target discovery?who has
an opinion, and about what? These problems are
usually presented in terms of techniques that re-
late them to the actual opinion expressed. We have
a social science application in which the identifi-
cation of sources and targets over a large volume
of text is more important than identifying the ac-
tual opinions particularly in experimenting with
social science models of opinion trends. Con-
sequently, we are able to use lightweight tech-
niques to identify sources and targets without us-
ing resource-intensive techniques to identify opin-
ionated phrases.
Our application for this work is the discovery
of networks of influence among opinion leaders
in the IT field. We are interested in answering
questions about who the leaders in the field are
and how their opinion matches the social and eco-
nomic success of IT innovation. Consequently,
it became necessary for us to construct a system
(figure 1) that finds the expressions in text that re-
fer to an opinion leader?s activities in promoting
or deprecating a technology.
In this paper, we demonstrate an information
extraction (Mooney and Bunescu, 2005) approach
based in relation mining (Girju et al, 2007) that
is effective for this purpose. We describe a tech-
nique by which corpus statistics allow us to clas-
sify pairs of entities and sentiment analysis targets
as instances of an ?expresses-an-opinion-about?
relation in documents in the IT business press.
This genre has the characteristic that many enti-
ties and targets are represented within individual
sentences and paragraphs. Features based on the
1095
Figure 1: Opinion relation classification system.
frequency counts of query results allow us to train
classifiers that allow us to extract ?expresses-an-
opinion-about? instances, using a very simple an-
notation strategy to acquire training examples.
In the IT business press, the opinionated lan-
guage is different from the newswire text for
which many extant sentiment tools were devel-
oped. We use an existing sentiment lexicon along-
side other non-sentiment-specific measures that
adapt resources from newswire-developed senti-
ment analysis projects without imposing the full
complexity of those techniques.
1.1 Corpus-based social science
The ?expresses-an-opinion-about? relation is a bi-
nary relation between opinion sources and tar-
gets. Sources include both people?typically
known experts, corporate representatives, and
other businesspeople?as well as organizations
such as corporations and government bodies. The
targets are the innovation terms. Therefore, the
use of named-entity recognition in this project
only focuses on persons and organizations, as the
targets are a fixed list.
1.2 Reifying opinion in an application
context
A hypothesis implicit in our social science task
is that opinion leaders create trends in IT innova-
tion adoption partly by the text that their activi-
ties generate in the IT business press. This text
has an effect on readers, and these readers act in
such a way that in turn may generate more or less
prominence for a given innovation?and may also
generate further text.
Some of these text-generating activities include
expressions of private states in an opinion source
(e.g., ?I believe that Web 2.0 is the future?). These
kinds of expressions suggest a particular ontol-
ogy of opinion analysis involving discourse re-
lations across various types of clauses (Wilson
and Wiebe, 2005; Wilson et al, 2005a). How-
ever, if we are to track the relative adoption of
IT innovations, we must take into account the
effect of the text on the reader?s opinion about
these innovations?there are expressions other
than those of private states that have an effect on
the reader. These can be considered to be ?opin-
ionated acts1.?
Opinionated acts can include things like pur-
chasing and adoption decisions by organizations.
For example:
And like other top suppliers to Wal-
Mart Stores Inc., BP has been in-
volved in a mandate to affix radio
frequency identification tags with em-
bedded electronic product codes to its
crates and pallets. (ComputerWorld,
January 2005)
In this case, both Wal-Mart and BP have expressed
implicit approval for radio frequency identifica-
tion by adopting it. This may affect the reader?s
own likelihood of support or adoption of the tech-
nology. In this context, we do not directly con-
sider the subjectivity of the opinion source, even
though that may be present.
Opinionated acts include things like implica-
tions of technology use, not just adoption. We
thus define opinion expressions as follows: any
expression involving some actor that is likely to
affect a reader?s own potential to adopt, reject, or
speak positively or negatively of a target. This
would include ?conventional? expressions of pri-
vate states as well as opinionated acts.
Our definition of ?expresses-an-opinion-about?
follows immediately. SourceA expresses an opin-
ion about target B if an interested third party C?s
actions towards B may be affected by A?s textu-
ally recorded actions, in a context where actions
1Somasundaran and Wiebe (2009) mention a related cate-
gory of ?pragmatic opinions? that involve world knowledge.
1096
have positive or negative weight (e.g. purchasing,
promotion, etc.).
1.3 Domain-specific sentiment detection
We construct a system that uses named-entity
recognition and supervised machine learning via
SVMs to automatically discover instances of
?expresses-an-opinion-about? as a binary relation
at reasonably high accuracy and precision.
The advantage of our approach is that, outside
of HMM-based named-entity detection (BBN?s
IdentiFinder), we evade the need for resource-
intensive techniques such as sophsticated gram-
matical models, sequence models, and semantic
role labelling (Choi et al, 2006; Kim and Hovy,
2006) by removing the focus on the actual opinion
expressed. Then we can use a simple supervised
discriminative technique with a joint model of lo-
cal term frequency information and corpus-wide
co-occurrence distributions in order to discover
the raw data for opinion trend modelling. The
most complex instrument we use from sentiment
analysis research on conventional newswire is a
sentiment keyword lexicon (Wilson et al, 2005b);
furthermore, our techniques allow us to distin-
guish sentiment keywords that indicate opinion in
this domain from keywords that actually indicate
that there is no opinion relation between source
and target.
While we show that this lightweight technique
works well at a paragraph level, it can also be used
in conjunction with more resource-intensive tech-
niques used to find ?conventional? opinion ex-
pressions. Also, the use of topic aspects (Soma-
sundaran and Wiebe, 2009) in conjunction with
target names has been associated with an improve-
ment in recall. However, our technique still per-
forms well above the baseline without these im-
provements.
2 Methodology
2.1 Article preparation
We have a list of IT innovations on which our
opinion leader research effort is most closely fo-
cused. This list contains common names that re-
fer to these technologies as well as some alternate
names and abbreviations. We selected articles at
random from the ComputerWorld IT journal that
contained mentions of members of the given list.
These direct mentions were tagged in the docu-
ment as XML entities.
Each article was processed by BBN?s Identi-
Finder 3.3 (Bikel et al, 1999), a named entity
recognition (NER) system that tags named men-
tions of person and organization entities2.
The articles were then divided into paragraphs.
For each paragraph, we generated candidate rela-
tions from the entities and innovations mentioned
therein. To generate candidates, we paired every
entity in the paragraph with every innovation. Re-
dundant pairs are sometimes generated when an
entity is mentioned in multiple ways in the para-
graph. We eliminated most of these by removing
entities whose mentions were substrings of other
mentions. For example, ?Microsoft? and ?Mi-
crosoft Corp.? are sometimes found in the same
paragraph; we eliminate ?Microsoft.?
2.2 Annotation
We processed 20 documents containing 157 rela-
tions in the manner described in the previous sec-
tion. Then two domain experts (chosen from the
authors) annotated every candidate pair in every
document according to the following scheme (il-
lustrated in figure 2):
? If the paragraph associated with the candi-
date pair describes a valid source-target rela-
tion, the experts annotated it with Y.
? If the paragraph does not actually contain
that source-target relation, the experts anno-
tated it with N.
? If either the source or the target is misidenti-
fied (e.g., errors in named entity recognition),
the experts annotated it with X.
The Cohen?s ? score was 0.6 for two annotators.
While this appears to be only moderate agree-
ment, we are still able to achieve good perfor-
mance in our experiments with this value.
2In a separate research effort, we found that IdentiFinder
has a high error rate on IT business press documents, so we
built a system to reduce the error post hoc. We ran this sys-
tem over the IdentiFinder annotations.
1097
Davis says she has especially enjoyed work-
ing with the PowerPad?s bluetooth interfaces to
phones and printers. ?It?s nice getting into new
wireless technology,? she says. The bluetooth
capability will allow couriers to transmit data
without docking their devices in their trucks.
Source Target Class
Davis bluetooth Y/N/X
PowerPad bluetooth Y/N/X
Figure 2: Example paragraph annotation exercise.
We then selected 75 different documents for
each annotator and processed and annotated them
as above. At this point we have the instances and
the classes to which they belong. We labelled 466
instances of Y, 325 instances of N, and 280 in-
stances of X, for a total of 1071 relations.
2.3 Feature vector generation
We have four classes of features for every rela-
tion instance. Each type of feature consists of
counts extracted from an index of 77,227 Comput-
erWorld articles from January 1988 to June 2008
generated by the University of Massachusetts
search engine Indri (Metzler and Croft, 2004).
Each vector is normalized to the unit vector. The
index is not stemmed for performance reasons.
The first type of feature consists of simple doc-
ument frequency statistics for source-target pairs
throughout the corpus. The second type consists
of document frequency counts of source-target
pairs when they are in particularly close proxim-
ity to one another. The third type consists of docu-
ment frequency counts of source target pairs prox-
imate to keywords that reflect subjectivity. The
fourth and final type consist of TFIDF scores of
vocabulary items in the paragraph containing the
putative opinion-holding relation (unigram con-
text features). We use the first three features types
to represent the likelihood in the ?world? that the
source has an opinion about the target and the last
feature type to represent the likelihood of the spe-
cific paragraph containing an opinion that reflects
the source-target relation.
We have a total of 7450 features. Each vec-
tor is represented as a sparse array. 806 features
represent queries on the Indri index. For all the
features, we therefore have 863,226 index queries.
We perform the queries in parallel on 25 proces-
sors to generate the full feature array, which takes
approximately an hour on processors running at
8Ghz. We eliminate all values that are smaller in
magnitude than 0.000001 after unit vector normal-
ization.
2.3.1 Frequency statistics
There are two simple frequency statistics fea-
tures generated from Indri queries. The first is
the raw frequency counts of within-document co-
occurrences of the source and target in the rela-
tion. The second is the mean co-occurrence fre-
quency of the source and target per Computer-
World document.
2.3.2 Proximity counts
For every relation, we query Indri to check how
often the source and the target appear in the same
document in the ComputerWorld corpus within
four word ranges: 5, 25, 100, and 500. That is
to say, if a source and a target appear within five
words of one another, this is included in the five-
word proximity feature. This generates four fea-
tures per relation.
2.3.3 Subjectivity keyword proximity counts
We augment the proximity counts feature with
a third requirement: that the source and target ap-
pear within one of the ranges with a ?subjectivity
keyword.? The keywords are taken from Univer-
sity of Pittsburgh subjectivity lexicon; the utility
of this lexicon is supported in recent work (Soma-
sundaran and Wiebe, 2009).
For performance reasons, we did not use all of
the entries in the subjectivity lexicon. Instead,
we used a TFIDF-based measure to rank the key-
words by their prevalence in the ComputerWorld
corpus where the term frequency is defined over
the entire corpus. Then we selected 200 keywords
with the highest score.
For each keyword, we use the same proximity
ranges (5, 25, 100, and 500) in queries to Indri
where we obtain counts of each keyword-source-
target triple for each range. There are threfore 800
subjectivity keyword features.
1098
Positive class Negative class System Prec / Rec / F Accuracy
Y N Random baseline 0.60 / 0.53 / 0.56 0.52
Y N Maj.-class (Y) baseline 0.59 / 1.00 / 0.74 0.59
Y N Linear kernel 0.70 / 0.73 / 0.72 0.66
Y N RBF kernel 0.72 / 0.76 / 0.75 0.69
Y N/X Random baseline 0.44 / 0.50 / 0.47 0.50
Y N/X RBF kernel 0.65 / 0.55 / 0.59 0.67
Table 1: Results with all features against majority class and random baselines. All values are mean
averages under 10-fold cross validation.
2.3.4 Word context (unigram) features
For each relation, we take term frequency
counts of the paragraph to which the relation be-
longs. We multiply them by the IDF of the term
across the ComputerWorld corpus. This yields
6644 features over all paragraphs.
2.4 Machine learning
On these feature vectors, we trained SVM models
using Joachims? (1999) svmlight tool. We use a
radial basis function kernel with an error cost pa-
rameter of 100 and a ? of 0.25. We also use a lin-
ear kernel with an error cost parameter of 100 be-
cause it is straightforwardly possible with a linear
kernel to extract the top features from the model
generated by svmlight.
3 Experiments
We conducted most of our experiments with only
the Y and N classes, discarding all X; this re-
stricted most of our results to those assuming cor-
rect named entity recognition. Y was the posi-
tive class for training the svmlight models, and
N was the negative class. We also performed ex-
periments with N and X together being the nega-
tive class; this represents the condition that we are
seeking ?expresses-an-opinion-about? even with a
higher named-entity error rate.
We use two baselines. One is a random base-
line with uniform probability for the positive and
negative classes. The other is a majority-class as-
signer (Y is the majority class).
The best system for the Y vs. N experiment was
subjected to feature ablation. We first systemati-
cally removed each of the four feature types indi-
vidually. The feature type whose removal had the
largest effect on performance was removed per-
manently, and the rest of the features were tested
without it. This was done once more, at which
point only one feature type was present in the
models tested.
3.1 Evaluation
All evaluation was performed under 10-fold cross
validation, and we report the mean average of all
performance metrics (precision, recall, harmonic
mean F-measure, and accuracy) across folds.
We define these measures in the standard infor-
mation retrieval form. If tp represents true pos-
itives, tn true negatives, fp false positives, and
fn false negatives, then precision is tp/(tp+fp),
recall tp/(tp + fn), F-measure (harmonic mean)
is 2(prec ? rec)/(prec + rec), and accuracy is
(tp+ tn)/(tp+ fp+ fn+ tn).
4 Results and discussion
The results of the experiments with all features are
listed in table 1.
4.1 ?Perfect? named entity recognition
We achieve best results in the Y versus N case us-
ing the radial basis function kernel. We find im-
provement in F-measure and accuracy at 19% and
17% respectively. Simply assigning the majority
class to all test examples yields a very high re-
call, by definition, but poor precision and accu-
racy; hence its relatively high F-measure does not
reflect high applicability to further processing, as
the false positives would amplify errors in our so-
cial science application.
The linear kernel has results that are below the
RBF kernel for all measures, but are relatively
close to the RBF results.
1099
Subjectivity Proximity Frequency Unigram Prec / Rec / F Accuracy
X X X X 0.72 / 0.76 / 0.75 0.69
X X X 0.67 / 0.89 / 0.76 0.67
X X X 0.71 / 0.77 / 0.73 0.68
X X X 0.70 / 0.78 / 0.74 0.67
X X X 0.69 / 0.77 / 0.73 0.67
X X 0.63 / 0.91 / 0.75 0.64
X X 0.66 / 0.89 / 0.76 0.67
X X 0.65 / 0.90 / 0.76 0.66
X 0.61 / 0.92 / 0.73 0.60
X 0.61 / 0.94 / 0.74 0.60
Table 2: Feature ablation results for RBF kernel on Y vs. N case. The first line is the RBF result with
all features from table 1.
4.2 Introducing erroneous named entities
The case of Y versus N and X together unsurpris-
ingly performed worse than the case where named
entity errors were eliminated. However, relative to
its own random baseline, it performed well, with
a 12% and 17% improvement in F-measure and
accuracy using the RBF kernel. This suggests that
the errors do not introduce enough noise into the
system to produce a large decline in performance.
As X instances are about 26% of the total and
we see a considerable drop in recall, we can say
that some of the X instances are likely to be similar
to valid Y ones; indeed, examination of the named
entity recognizer?s errors suggests that some in-
correct organizations (e.g. product names) occur
in contexts where valid organizations occur. How-
ever, precision and accuracy have not fallen nearly
as far, so that the quality of the output for further
processing is not hurt in proportion to the intro-
duction of X class noise.
4.3 Feature ablation
Table 2 contains the result of our feature abla-
tion experiments. Overall, the removal of features
causes the SVM models to behave increasingly
like a majority class assigner. As we mentioned
earlier, higher recall at the expense of precision
and accuracy is not an optimal outcome for us
even if the F-measure is preserved. In our results,
the F-measure values are remarkably stable.
In the first round of feature removal, the sub-
jectivity keyword features have the biggest ef-
fect with the largest drop in precision and the
largest increase in recall; high-TFIDF words from
a general-purpose subjectivity lexicon allow the
model to assign more items to the negative class.
The next round of feature removal shows
that the proximity features have the next largest
amount of influence on the classifier, as precision
drops by 4%. The proximity features are very sim-
ilar to the subjectivity features in that they too in-
volve queries over windows of limited word sizes;
the subjectivity keyword features only differ in
that a subjectivity keyword must be within the
window as well. That the proximity features are
not more important than the subjectivity features,
implies that the subjectivity keywords matter to
the classifier, even though they are not specific to
the IT domain. However, the proximity of sources
and targets also matters, even in the absence of the
subjectivity keywords.
Finally, we are left with the frequency features
and the unigram context features. Either set of
features supports a level of performance greater
than the random baseline in table 1. However,
the unigram features allow for slightly better re-
call than the frequency features without loss of
precision, but this may not be very surprising, as
there are many more unigram features than fre-
quency features. More importantly, however, ei-
ther of these feature types is sufficient to prevent
the classifier from assigning the majority class all
of the time, although they come close.
1100
Feature type Range Keyword
Subjectivity 500 agreement
Subjectivity 500 critical
Subjectivity 500 want
Subjectivity 100 will
Subjectivity 100 able
Subjectivity 500 worth
Subjectivity 500 benefit
Subjectivity 100 trying
Subjectivity 500 large
Subjectivity 500 competitive
Table 3: The 10 most positive features via a linear
kernel in descending order.
Feature type Range Keyword
Subjectivity 500 low
Subjectivity 500 ensure
Subjectivity 25 want
Subjectivity 100 vice
Subjectivity 500 slow
Subjectivity 100 large
Subjectivity 500 ready
Subjectivity 100 actually
Subjectivity 100 ready
Subjectivity 100 against
Table 4: The 10 most negative features via a linear
kernel in descending order.
4.4 Most discriminative features
The models generated by svmlight under a lin-
ear kernel allow for the extraction of feature
weights by a script written by svmlight?s creator.
We divided the instances into a single 70%/30%
train/test split and trained a classifier with a linear
kernel and an error cost parameter of 100, with re-
sults similar to those reported under 10-fold cross-
validation in table 1. We used all features.
Then we were able to extract the 10 most pos-
itive (table 3) and 10 most negative (table 4) fea-
tures from the model.
Interestingly, all of these are subjectivity key-
word features, even the negatively weighted fea-
tures. The top positive features are often evocative
of business language, such as ?agreement?, ?crit-
ical?, and ?competitive?. Most of them emerge
from queries at the 500-word range, suggesting
that their presence in the document itself is evi-
dence that a source is expressing an opinion about
a target. That most of them are subjectivity fea-
tures is reflected in the feature ablation results in
the previous section.
It is less clear why ?ensure? and ?against?
should be evidence that a source-target pair is not
an instance of ?expresses-an-opinion-about?. On
the other hand, words like ?ready? (which appears
twice) and ?actually? can conceivably reflect sit-
uations in the IT domain that are not matters of
opinion. In either case, this demonstrates one of
the advantages of our technique, as these are fea-
tures that actively assist in classifying some rela-
tion instances as not expressing sentiment. For ex-
ample, contrary to what we would expect, ?want?
in a 25-word window with a source and a tar-
get is actually evidence against an ?expresses-an-
opinion-about? relation in text about IT innova-
tions (ComputerWorld, July 2007):
But Klein, who is director of infor-
mation services and technology, didn?t
want IT to become the blog police.
In this example, Klein is expressing a desire,
but not about the innovation (blogs) in question.
5 Conclusions and future work
5.1 Summary
We constructed and evaluated a system that de-
tects at paragraph level whether entities relevant
to the IT domain have expressed an opinion about
a list of IT innovations of interest to a larger social
science research program. To that end, we used
a combination of co-occurrence statistics gleaned
from a document indexing tool and TFIDF val-
ues from the local term context. Under these
novel conditions, we successfully exceeded sim-
ple baselines by large margins.
Despite only moderate annotator agreement, we
were able to produce results coherent enough to
successfully train classifiers and conduct experi-
ments.
Our feature ablation study suggests that all of
the feature types played a role in improving the
performance of the system over the random and
1101
majority-class baselines. However, the subjec-
tivity keyword features from an existing lexicon
played the largest role, followed by the proxim-
ity and unigram features. Subjectivity keyword
features dominated the ranks of feature weights
under a linear kernel, and the features most pre-
dictive of membership in ?expresses-an-opinion-
about? are words with semantic significance in the
context of the IT business press.
5.2 Application to other domains
We used somewhat na??ve statistics in a simple
machine learning system in order to implement a
form of opinion mining for a particular domain.
The most direct linguistic guidance we provided
our system were the query ranges and the sub-
jectivity lexicon. The generality of this approach
yields the advantage that it can be applied to other
domains where there are ways of expressing senti-
ment unique to those domains outside of newswire
text and product reviews.
5.3 Improving the features
Our use of an existing sentiment lexicon opens the
door in future work for the use of techniques to
bootstrap a larger sentiment lexicon that empha-
sizes domain-specific language in the expression
of opinion, including opinionated acts. In fact,
our results suggest that terminology in the exist-
ing lexicon that is most prominently weighted in
our classifier also tends to be domain-relevant. In
a further iteration, we might also improve perfor-
mance by using terms outside the lexicon that tend
to co-occur with terms from the lexicon.
5.4 Data generation
Our annotation exercise was a very simple one in-
volving a short reading exercise and the selection
of one of three choices per relation instance. This
type of exercise is ideally suited to the ?crowd-
sourcing? technique of paying many individuals
small amounts of money to perform these simple
annotations over the Internet. Previous research
(Snow et al, 2008) suggests that we can generate
very large datasets very quickly in this way; this
is a requirement for expanding to other domains.
5.5 Scalability
In order to classify on the order of 1000 instances,
it took nearly a million queries to the Indri index,
which took a little over an hour to do in parallel
on 25 processors by calling the Indri query engine
afresh at each query. While each query is nec-
essary to generate each feature value, there are a
number of optimizations we could implement to
accelerate the process. Various types of dynamic
programming and caching could be used to han-
dle related queries. One way of scaling up to
larger datasets would be to use the MapReduce
and cloud computing paradigms on which text
processing tools have already been implemented
(Moreira et al, 2007).
The application for this research is a social sci-
ence exercise in exploring trends in IT adoption
by analysing the IT business press. In the end, the
perfect discovery of all instances of ?expresses-
an-opinion-about? is not as important as finding
enough reliable data over a large number of docu-
ments. This work brings us several steps closer in
finding the right combination of features in order
to acquire trend-representative data.
Acknowledgements
This paper is based upon work supported by the
National Science Foundation under Grant IIS-
0729459.
References
Bikel, Daniel M., Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3).
Choi, Yejin, Eric Breck, and Claire Cardie. 2006.
Joint extraction of entities and relations for opinion
recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP).
Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: classification of semantic re-
lations between nominals. In SemEval ?07: Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 13?18, Morristown, NJ,
USA. Association for Computational Linguistics.
Joachims, T. 1999. Making large-scale SVM learn-
ing practical. In Scho?lkopf, B., C. Burges, and
1102
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
Kim, Soo-Min and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In SST ?06: Proceedings of
the Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Metzler, Donald and W. Bruce Croft. 2004. Combin-
ing the language model and inference network ap-
proaches to retrieval. Information Processing and
Management, 40(5):735 ? 750.
Mooney, Raymond J. and Razvan Bunescu. 2005.
Mining knowledge from text using information ex-
traction. SIGKDD Explor. Newsl., 7(1):3?10.
Moreira, Jose? E., Maged M. Michael, Dilma Da Silva,
Doron Shiloach, Parijat Dube, and Li Zhang. 2007.
Scalability of the nutch search engine. In Smith,
Burton J., editor, ICS, pages 3?12. ACM.
Rogers, Everett M. 2003. Diffusion of Innovations,
5th Edition. Free Press.
Snow, Rion, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In EMNLP 2008, Morristown,
NJ, USA.
Somasundaran, Swapna and Janyce Wiebe. 2009.
Recognizing stances in online debates. In ACL-
IJCNLP ?09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 1. Association
for Computational Linguistics.
Wilson, Theresa and Janyce Wiebe. 2005. Annotating
attributions and private states. In ACL 2005 Work-
shop: Frontiers in Corpus Annotation II: Pie in the
Sky, pages 53?60.
Wilson, Theresa, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. OpinionFinder: A system for subjec-
tivity analysis. In HLT/EMNLP.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP.
1103
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 345?348,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Crowdsourcing the evaluation of a domain-adapted named entity
recognition system
Asad B. Sayeed, Timothy J. Meyer,
Hieu C. Nguyen, Olivia Buzek
Department of Computer Science
University of Maryland
College Park, MD 20742
asayeed@cs.umd.edu,
tmeyer1@umd.edu,
{hcnguyen88,olivia.buzek}
@gmail.com
Amy Weinberg
Department of Linguistics
University of Maryland
College Park, MD 20742
weinberg@umiacs.umd.edu
Abstract
Named entity recognition systems sometimes
have difficulty when applied to data from do-
mains that do not closely match the training
data. We first use a simple rule-based tech-
nique for domain adaptation. Data for robust
validation of the technique is then generated,
and we use crowdsourcing techniques to show
that this strategy produces reliable results even
on data not seen by the rule designers. We
show that it is possible to extract large im-
provements on the target data rapidly at low
cost using these techniques.
1 Introduction
1.1 Named entities and errors
In this work, we use crowdsourcing to generate eval-
uation data to validate simple techniques designed to
adapt a widely-used high-performing named entity
recognition system to new domains. Specifically, we
achieve a roughly 10% improvement in precision on
text from the information technology (IT) business
press via post hoc rule-based error reduction. We
first tested the system on a small set of data that we
annotated ourselves. Then we collected data from
Amazon Mechanical Turk in order to demonstrate
that the gain is stable. To our knowledge, there is no
previous work on crowdsourcing as a rapid means
of evaluating error mitigation in named entity rec-
ognizer development.
Named entity recognition (NER) is a well-known
problem in NLP which feeds into many other re-
lated tasks such as information retrieval (IR) and
machine translation (MT) and more recently social
network discovery and opinion mining. Generally,
errors in the underlying NER technology correlate
with a steep price in performance in the NLP sys-
tems further along a processing pipeline, as incor-
rect entities propagate into incorrect translations or
erroneous graphs of social networks.
Not all errors carry the same price. In some ap-
plications, omitting a named entity has the conse-
quence of reducing the availability of training data,
but including an incorrectly identified piece of text
as as a named entity has the consequence of pro-
ducing misleading results. Our application would
be opinion mining; an omitted entity may prevent
the system from attributing an opinion to a source,
but an incorrect entity reveals non-existent opinion
sources.
Machine learning is currently used extensively in
building NER systems. One such system is BBN?s
Identifinder (Bikel et al, 1999). The IdentiFinder al-
gorithm, based on Hidden Markov Models, has been
shown to achieve F-measure scores above 90% when
the training and testing data happen to be derived
from Wall Street Journal text produced in the 1990s.
We use IdentiFinder 3.3 as a starting point for per-
formance improvement in this paper.
The use of machine learning in existing systems
requires us to produce new and costly training data
if we want to adapt these systems directly to other
domains. Our post hoc error reduction strategy is
therefore profoundly different: it relieves us of the
burden of generating complete training examples.
The data we generate are strictly corrections of the
existing system?s output. Our thus cheaper evalua-
tion is therefore primarily on improvements to pre-
345
cision, while minimizing damage to recall, unlike
an evaluation based on retraining with new, fully-
annotated text.
1.2 Crowdsourcing
Crowdsourcing is the use of the mass collabora-
tion of Internet passers-by for large enterprises on
the World Wide Web such as Wikipedia and survey
companies. However, a generalized way to mon-
etize the many small tasks that make up a larger
task is relatively new. Crowdsourcing platforms
like Amazon Mechanical Turk have allowed some
NLP researchers to acquire data for small amounts
of money from large, unspecified groups of Internet
users (Snow et al, 2008; Callison-Burch, 2009).
The use of crowdsourcing for an NLP annotation
task required careful definition of the specifics of
the task. The individuals who perform these tasks
have no specific training, and they are trying to get
through as many tasks as they can, so each task must
be specified very simply and clearly.
Part of our work was to define a named entity
error detection task simply enough that the results
would be consistent across anonymous annotators.
2 Methodology
2.1 Process overview
The overall process for running this experiment was
as follows (figure 1).
Figure 1: Diagram of data pipeline.
First, we performed an initial performance assess-
ment of IdentiFinder on our domain. We selected
200 articles from an IT trade journal. IdentiFinder
was used to tag persons and organizations in these
documents. Domain experts (in this case, the au-
thors of this paper) analyzed the entity tags pro-
duced by the NER system and annotated the erro-
neous tags. We built an error reduction system based
on our error analysis. We then ran the IdentiFinder
output through the error reduction system and eval-
uated its performance against our annotations.
Next, we constructed an Amazon Mechanical
Turk-based interface for na??ve web users or ?Turk-
ers? to annotate the IdentiFinder entities for errors.
We measured the interannotator agreement between
the Turkers and the domain experts, and we evalu-
ated the IdentiFinder output and the repaired output
against the expert-generated and Turker gold stan-
dards.
We selected a new batch of 800 articles and ran
IdentiFinder and the filters on them, and we again
ran our Mechanical Turk application on the Iden-
tiFinder output. We measured the performance of
IdentiFinder and filtered output against the Turker
annotations.
2.2 Performance evaluation
Performance is evaluated in terms of standard pre-
cision and recall of entities. If the system output
contains a person or organization labelled correctly
as such, it considers this to be a hit. If it contains a
person or organization that is mislabelled or other-
wise incorrect in the gold standard annotation, it is
a miss. We compute the F-measure as the harmonic
mean of precision and recall.
As the IdentiFinder output is the baseline, and we
ignore missed entities, by definition the baseline re-
call is 100%.
3 Experiments and results
Here we delve into further detail about the tech-
niques we used and the results that they yielded. The
results are summarized in table 1.
3.1 Baseline performance assessment
We randomly selected 200 documents from Infor-
mationWeek, a major weekly magazine in the IT
business press. Running them through IdentiFinder
produces NIST ACE-standard XML entity markup.
We focused on the ENAMEX tags of person and or-
ganization type that IdentiFinder produces.
After we annotated the ENAMEX tags for errors,
we found that closer inspection of the errors in the
IdentiFinder output allowed us to classify the major-
ity of them into three major categories:
346
Annotator Collection System Precision Recall F-measure
Authors 200 document IdentiFinder only 0.74 1 0.85
Authors 200 document Filtered 0.86 0.98 0.92
MTurk 200 document IdentiFinder only 0.69 1 0.82
MTurk 200 document Filtered 0.79 0.97 0.87
MTurk 800 document IdentiFinder only 0.67 1 0.80
MTurk 800 document Filtered 0.77 0.95 0.85
Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.
? IdentiFinder tags words that are simply not
named entities.
? IdentiFinder assigns the wrong category (per-
son or organization) to an entity.
? IdentiFinder includes extraneous words in an
otherwise correct entity.
The second and third types of error are particu-
larly challenging. An example of the second type is
the following:
Yahoo is a reasonably strong competitor
to Google. It gets about half as much on-
line revenue and search traffic as Google,
. . .
Google is marked twice incorrectly as being a person
rather than an organization.
Finally, here is an example of the third error type:
A San Diego bartender reported that Bill
Gates danced the night away in his bar on
Nov. 11.
IdentiFinder incorrectly marks ?danced? as part of a
person tag.
We were able to find the precision of IdentiFinder
against our annotations: 0.74. This is poorer than the
reported performance of IdentiFinder on Wall Street
Journal text (Bikel et al, 1999).
3.2 Domain-specific error reduction
We wrote a series of rule-based filters to remove
instances of the error types?of which there were
many subtypes?described in the previous sec-
tion. For instance, the third example above was
eliminated via the use of a part-of-speech tagger;
?danced? was labelled as a verb, and entities with
tagged verbs were removed. In the second case,
the mislabelling of Google as a person rather than
an organization is identified by looking at Identi-
Finder?s majority labelling of Google throughout the
corpus?as an organization. Simple rules about cap-
italization allow instances like the first example to
be identified as errors.
This step increases the precision of the system
output to 86%, while only sacrificing a tiny amount
of recall. We see that this 10% increase is main-
tained even on the Mechanical Turk-generated an-
notations.
3.3 Mechanical Turk tasks
The basic unit of Mechanical Turk is the Human In-
telligence Task (HIT). Turkers select HITs presented
as web pages and perform the described task. Data-
collectors create HITs and pay Amazon to disburse
small amounts of money to Turkers who complete
them.
We designed our Mechanical Turk process so that
every HIT we create corresponds to an IdentiFinder-
marked document. Within its corresponding HIT,
each document is broken up into paragraphs. Fol-
lowing every paragraph is a table whose rows con-
sist of every person/organization ENAMEX discov-
ered by IdentiFinder and whose columns consist of
one of the four categories: ?Person,? ?Organization,?
?Neither,? and ?Don?t Know.? Then for each entity,
the user selects exactly one of the four options.
Each HIT is assigned to three different Turkers.
Every entity in that HIT is assigned a person or or-
ganization ENAMEX tag if two of the three Turkers
agreed it was one of those (majority vote); other-
wise, it is marked as an invalid entity.
We calculated the agreement between our annota-
tions and those developed from the Turker majority
347
vote scheme. This yields a Cohen?s ? of 0.68. We
considered this to be substantial agreement.
After processing the same 200 document set from
our own annotation, we found that the precision
of IdentiFinder was 69%, but after error reduction,
it increased to 79% with only a miniscule loss of
known valid entities (recall).
We then took another 800 documents from Infor-
mationWeek and ran them through IdentiFinder. We
did not annotate these documents ourselves, but in-
stead turned them over to Turkers. IdentiFinder out-
put alone has a 67% precision, but after error reduc-
tion, it rises to 77%, and recall is still minimally af-
fected.
4 Discussion
4.1 Benefits
It appears that high-performing NER systems ex-
hibit rather severe domain adaption problems. The
performance of IdentiFinder is quite low on the IT
business press. However, a simple rule-based sys-
tem was able to gain 10% improvement in precision
with little recall sacrificed. This is a particularly im-
portant improvement in applications with low toler-
ance for erroneous entities.
However, rule-based systems built by experts are
known to be vulnerable to new data unseen by the
experts. In order to apply this domain-specific error
reduction reliably, it has to be tested on data gathered
elsewhere. We used crowdsourced data to show that
the rule-based system was robust when confronted
with data that the designers did not see.
One danger in crowdsourcing is a potential lack
of commitment on the part of the annotators, as they
attempt to get through tasks as quickly as possible.
It turns out that in an NER context, we can design a
crowdsourced task that yields relatively reliable re-
sults across data sets by ensuring that for every data
point, there were multiple annotators making only
simple decisions about entity classification.
This method also provides us with a source of eas-
ily acquired supervised training data for testing more
advanced techniques, if required.
4.2 Costs
It took not more than an estimated two person weeks
to complete this work. This includes doing the
expert annotations, designing the Mechanical Turk
tasks, and building the domain-specific error reduc-
tion rules.
For each HIT, each annotator was paid 0.05 USD.
For three annotators for 1000 documents, that is
150.00 USD (plus additional small Amazon sur-
charges and any taxes that apply).
5 Conclusions and Future Work
This work was done on a single publication in a sin-
gle domain. One future experiment would be to see
whether these results are reliable across other pub-
lications in the domain. Another set of experiments
would be to determine the optimum number of an-
notators; we assumed three, but cross-domain results
may be more stable with more annotators.
Retraining an NER system for a particular domain
can be expensive if new annotations must be gen-
erated from scratch. While there is work on using
advanced machine learning techniques for domain
transfer (Guo et al, 2009), simply repairing the the
errors post hoc via a rule-based system can have a
low cost for high gains. This work shows a case
where the results are reliable and the verification
simple, in a context where reducing false positives
is a high priority.
Acknowledgements
This paper is based upon work supported by the Na-
tional Science Foundation under Grant IIS-0729459.
This research was also supported in part by NSF
award IIS-0838801.
References
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP 2009, Singapore, August.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL 2009, Morristown, NJ, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008, Morristown, NJ, USA.
348
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 667?676,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Grammatical structures for word-level sentiment detection
Asad B. Sayeed
MMCI Cluster of Excellence
Saarland University
66123 Saarbru?cken, Germany
asayeed@coli.uni-sb.de
Jordan Boyd-Graber,
Bryan Rusk, Amy Weinberg
{iSchool / UMIACS, Dept. of CS, CASL}
University of Maryland
College Park, MD 20742 USA
{jbg@umiacs,brusk@,
aweinberg@casl}.umd.edu
Abstract
Existing work in fine-grained sentiment anal-
ysis focuses on sentences and phrases but ig-
nores the contribution of individual words and
their grammatical connections. This is because
of a lack of both (1) annotated data at the word
level and (2) algorithms that can leverage syn-
tactic information in a principled way. We ad-
dress the first need by annotating articles from
the information technology business press via
crowdsourcing to provide training and testing
data. To address the second need, we propose
a suffix-tree data structure to represent syntac-
tic relationships between opinion targets and
words in a sentence that are opinion-bearing.
We show that a factor graph derived from this
data structure acquires these relationships with
a small number of word-level features. We
demonstrate that our supervised model per-
forms better than baselines that ignore syntac-
tic features and constraints.
1 Introduction
The terms ?sentiment analysis? and ?opinion mining?
cover a wide body of research on and development of
systems that can automatically infer emotional states
from text (after Pang and Lee (2008) we use the two
names interchangeably). Sentiment analysis plays a
large role in business, politics, and is itself a vibrant
research area (Bollen et al, 2010).
Effective sentiment analysis for texts such as
newswire depends on the ability to extract who
(source) is saying what (target). Fine-grained sen-
timent analysis requires identifying the sources and
targets directly relevant to sentiment bearing expres-
sions (Ruppenhofer et al, 2008). For example, con-
sider the following sentence from a major informa-
tion technology (IT) business journal:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtualiza-
tion also opens up a slew of potential net-
work access control issues.
There are three entities in the sentence that have the
capacity to express an opinion: Lloyd Hession, BT
Radianz, and New York. These are potential opinion
sources. There are also a number of mentioned con-
cepts that could serve as the topic of an opinion in
the sentence, or target. These include all the sources,
but also ?virtualization?, ?network access control?,
?network?, and so on.
The challenging task is to discriminate between
these mentions and choose the ones that are rele-
vant to the user. Furthermore, such a system must
also indicate the content of the opinion itself. This
means that we are actually searching for all triples
{source, target, opinion} in this sentence (Kim and
Hovy, 2006) and throughout each document in the
corpus. In this case, we want to identify that Lloyd
Hession is the source of an opinion, ?slew of network
issues,? about a target, virtualization. Providing such
fine-grained annotations would enrich information
extraction, question answering, and corpus explo-
ration applications by letting users see who is saying
what with what opinion (Wilson et al, 2005; Stoy-
anov and Cardie, 2006).
We motivate the need for a grammatically-focused
approach to fine-grained opinion mining and situate it
667
within the context of existing work in Section 2. We
propose a supervised technique for learning opinion-
target relations from dependency graphs in a way that
preserves syntactic coherence and semantic compo-
sitionality. In addition to being theoretically sound
? a lacuna identified in many sentiment systems1
? such approaches improve downstream sentiment
tasks (Moilanen and Pulman, 2007).
There are multiple types of downstream tasks that
potentially require the retrieval of {source, target,
opinion} relations on a sentence-by-sentence basis.
An increasingly significant application area is in the
use of large corpora in social science. This area of
research requires the exploration and aggregation of
data about the relationships between discourses, orga-
nizations, and people. For example, the IT business
press data that we use in this work belongs to a larger
research program (Tsui et al, 2009; Sayeed et al,
2010) of exploring industry opinion leadership. IT
business press text is one type of text in which many
entities and opinions can appear intermingled with
one another in a small amount of text.
Another application for fine-grained sentiment re-
lation retrieval of this type is paraphrasing, where
attribution of which opinion belongs to which entities
may be important for producing useful and accurate
output, since source and target identification errors
can change the entire meaning of an output text.
Unlike previous approaches that ignore syntax, we
use a sentence?s syntactic structure to build a proba-
bilistic model that encodes whether a word is opinion
bearing as a latent variable. We build a data structure
we call a ?syntactic relatedness trie? (Section 3) that
serves as the skeleton for a graphical model over the
sentiment relevance of words (Section 4). This ap-
proach allows us to learn features that predict opinion
bearing constructions from grammatical structures.
Because of a dearth of resources for this fine-grained
task, we also develop new crowdsourcing techniques
for labeling word-level, syntactically informed sen-
1Alm (2011) recently argued that work on sentiment anal-
ysis needs to de-emphasize the goal of building systems that
are ?high-performing? by traditional measures, because the field
risks sacrificing ?opportunities that may lead to a more thorough
understanding of language uses and users? in relation to subjec-
tive phenomena. The work we present in this paper therefore
focuses on extracting meaningful features as an investment in
future work that directly improves retrieval performance.
timent (Section 5). We use inference techniques to
uncover grammatical patterns that connect opinion-
expressing words and target entities (Section 6) per-
forming better than using syntactically uninformed
methods.
2 Background and existing work
We call opinion mining ?fine-grained? when it re-
trieves many different {source, target, opinion}
triples per document. This is particularly challenging
when there are multiple triples even within a sen-
tence. There is considerable work on identifying the
source of an opinion. However, it is much harder
to find obvious features that tell us whether ?virtual-
ization? is the target of an opinion. The most recent
target identification techniques use machine learning
to determine the presence of a target from known
opinionated language (Jakob and Gurevych, 2010).
Even when targets are identified we must decide if
an opinion is expressed, since not all target mentions
will necessarily be accompanied by opinion expres-
sions. Returning to the first example sentence, we
could say that the negative opinion about virtualiza-
tion is expressed by the words ?slew? and ?issues?.
A system that could automatically make this dis-
covery must draw on grammatical relationships be-
tween targets and the opinion bearing words. Parsers
reveal these relationships, but the relationships are
often indirect. The variability of language prevents
a complete enumeration of all intervening items that
make the relationships indirect, but examples include
negation and intensifiers, which change opinion, and
sentiment-neutral words, which fill syntactic or stylis-
tic needs. In this paper, we cope with the variability
of expression by using supervised machine learning
to generalize across observations and learn which fea-
tures best enable us to identify opinionated language.
Existing work in this area often uses semantic
frames and role labeling (Kim and Hovy, 2006; Choi
et al, 2006), but resources typically used in these
tasks (e.g. FrameNet) are not exhaustive. More gen-
eral approaches (Ruppenhofer et al, 2008) describe
semantic and discourse contexts of opinion sources
and targets cannot recognize them.
When techniques do identify targets via syntax,
they often only use grammar as a feature in an oth-
erwise syntax-agnostic model. Some work of this
668
nature merely identifies targets without providing the
syntactic evidence necessary to find domain-relevant
opinionated language (Jakob and Gurevych, 2010),
relying on lists of opinion keywords. There is also
work (Qiu et al, 2011) that uses predefined heuristics
over dependency parses to identify both targets and
opinion keywords but does not acquire new syntactic
heuristics. Other work (Nakagawa et al, 2010) is sim-
ilar to ours in that it uses factor graph modeling over
a dependency parse formalism, but it assumes that
opinionated language is known a priori and focuses
on polarity classification, while our work tackles the
more fundamental problem of identifying the opin-
ionated language itself.
Little work has been done to perform target and
opinion-expression extraction jointly, especially in a
way that extracts features for downstream processing.
This dearth persists despite evidence that such infor-
mation improves sentiment analysis (Moilanen and
Pulman, 2007).
An advantage of our proposed approach is that we
can use dependency paths in order to capture situa-
tions where the relations are non-compositional or
semantically motivated. In Section 5, we describe a
data set that has the additional property that opinion
is expressed in ways that require external pragmatic
knowledge of the domain. An advantage of arbi-
trary, non-local dependencies is that we can treat this
knowledge as part of the model we learn via long-
distance chains, which can capture pragmatics.
3 Syntactic relatedness tries
We now describe how we build the syntactic related-
ness trie (SRT) that forms the scaffolding for the prob-
abilistic models needed to identify sentiment-bearing
words via syntactic constraints extracted from a de-
pendency parse (Ku?bler et al, 2009).
We use the Stanford Parser (de Marneffe and Man-
ning, 2008) to produce a dependency graph and con-
sider the resulting undirected graph structure over
words. We construct a trie for each possible target
word in a sentence (it is possible for a sentence to
induce multiple tries if the sentence contains multi-
ple potential targets). Each trie encodes paths from
the possible target word to other words, and each
path represents a sequence of words connected by
undirected edges in the parse.
3.1 Encoding Dependencies in an SRT
SRTs enable us to encode the connections between
a single linguistic object of interest?in this appli-
cation, a possible target word?and a set of related
objects. SRTs are data structures consisting of nodes
and edges.
This description is very similar to the definition
of a dependency parse. The key difference is that
while a token only appears once as a node in a de-
pendency parse, an SRT can contain multiple nodes
that originate from the same token. This encodes the
possible connections between an opinion target and
opinion-conveying words.
The object of interest is the opinion target, defined
as the SRT root node (e.g. in Figure 1 ?policy? is a
known target, so it becomes the root of an SRT). Each
SRT edge corresponds to a grammatical relationship
between words and is labeled with that relationship.
We use the notation a
R
?? b to signify that node a has
the relationship (?role?) R with b. We say in this case
that node b is a descendent of node a with the role
R. The directed edges constitute a trie or suffix tree
that represents the fact that multiple paths may share
elements that all provide evidence for the relevance
of multiple leaves. 2
In the remainder of this section we describe the
necessary steps to create a training corpus for fine-
grained sentiment analysis. We provide an example
of how to create an SRT from a dependency parse and
then to attach latent variable assignments to an SRT
based on human annotations in a way that respects
syntactic constraints.
3.2 Using sentiment flow to label an SRT
Our goal is to discriminate between parts of the struc-
ture that are relevant to target-opinion word relations
and those that are not. We use the term sentiment
flow (shortened to ?flow? when space is an issue)
for relevant sentiment-bearing words in the SRT and
inert for the remainder of the sentence. We use the
term ?flow? because our invariant (section 3.3) con-
strains a sentiment flow in a SRT to be a contiguous
subgraph; this corresponds to linguistic intuitions
that, for example, in the sentence ?Linux with Wine
2The SRT will be used to create an undirected graphical
model; the notion of directedness refers to the traversal of paths
used to construct the SRT.
669
the dominant
role
the european climate protection
policy
has
benefits
our
economy
policy
policy
policy
protection
role
role
has
dominant
benefits
Dependency Parse
Paths for "policy" SRT
Figure 1: Dependency parse example. A dependency
parse (top) is used to generate a syntactic relatedness
trie for all possible targets of a sentiment-bearing
expression. For the target word ?policy?, there are a
number of paths (colors are consistent in paths to be
added to the SRT and in the dependency parse) that
connect it to other words; once extracted, these paths
will be inserted into a target-specific SRT.
is very usable?, {?Linux?, ?is?, ?very?} could not
be part of a sentiment flow without also including
{?usable?}.
Now that we have the structure of the model, we
need training data: sentences where sentiment bear-
ing words have been labeled. We describe how to go
from sentiment-labeled words to valid flows using
this sentence from the MPQA:
The dominant role of the European climate
protection policy has benefits for our econ-
omy.
In this sentence, the target word ?policy? is con-
nected to multiple sentiment-bearing words via paths
in the dependency parse (Figure 1). We can represent
these relationships using paths through the graph as
in Figure 2(a). (For clarity, we do not show some
paths.)
Suppose that an annotator decides that ?protec-
tion? and ?benefits? are directly expressing an opin-
ion about the policy, but ?dominant? is ambiguous (it
has some negative connotations). The nodes ?protec-
tion? and ?benefits? are a flow, and the ?dominant?
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
(a)
(b)
(c)
(d)
Figure 2: Labeled SRTs rooted on the target word
?policy?; green-filled nodes represent words that are
part of a sentiment flow and nodes with a red outline
represent inert nodes. (a) Initial labels for SRT (e.g.
as provided by annotators) (b) propagating labels to
yield a valid sentiment flow (c) a change of ?role? to
inert also renders its children inert (d) a change of
?dominant? to be part of a sentiment flow also causes
its parents to be part of a flow.
node is inert. However, there is considerable overlap
between the ?dominant? path and the ?benefits? path.
That is the motivation for combining them into a trie
structure and labeling them in such a way that the
path remains a flow until there is no path element that
leads to a flow leaf (Figure 2).
In other words, we want the path elements com-
mon to a flow path and an inert path to reinforce
sentiment flow. The transition from flow to inert is
learned by the classifier.
We enforce this requirement through the procedure
shown in Figure 2, which is equivalent to finding the
depth first search tree of the dependency graph and
applying the node-labeling scheme as above.
3.3 Invariant
Anything that follows a node with an inert label is
by definition not reachable from the root of the tree.
670
Consequently, any node that is part of a sentiment
flow that follows an inert node is not reachable along
a path and is actually inert itself. We specify this
directly as an invariant on the data structure:
Invariant: no node descending from a
node labeled inert can be labeled as a part
of a sentiment flow.
This specifies that flow labels spread out from the
root of the SRT. Our inference algorithm requires
that we be able to change the labels of nodes for
test data, thus we need to define invariant-respecting
operations for switching labels from flow to flow and
vice-versa. A flow label switched to inert will require
all the descendents of that particular node to switch
to inert as well as in figure 2(c). Similarly, an inert
label switched to flow will require all of the ancestors
of that node to switch to flow as in 2(d).
4 Encoding SRTs as a factor graph
In this section, we develop supervised machine learn-
ing tools to produce a labeled SRT from unlabeled,
held-out data in a single, unified model, without per-
mitting the sorts of inconsistencies that may be ad-
mitted by using a local classifier at each node.
4.1 Sampling labels
A factor graph (Kschischang et al, 1998) is a rep-
resentation of a joint probability distribution in the
form of a graph with two types of vertices: vari-
able vertices and factor vertices. Given a set of vari-
ables Z = {z1 . . . zn}, we connect them via factors
F = {f1 . . . fm}. Factors are functions that repre-
sent relationships, i.e. probabilistic dependencies,
among the variables; the product of all factors gives
the complete joint distribution p. Each factor fi can
take as input some corresponding subset of variables
Yi from Z. We can then write the relationship as
follows:
p(Z) ?
?m
k=1 fk(Yk)
Our goal is to discover the values for the variables
that best explain a dataset. While there are many
approaches for inference in statistical models, we
turn to MCMC methods (Neal, 1993) to discover the
underlying structure of the model. More specifically,
we seek a posterior distribution over latent variables
parent
node
child
1
child
2
child
3
h
g
f
Figure 3: Graphical model of SRT factors
that partition words in a sentence into flow and in-
ert groups; we estimate this posterior using Gibbs
sampling (Finkel et al, 2005).
The sampler requires an initial state that respects
the invariant. Our initial setting is produced by iterat-
ing through all labels in the SRT forest and randomly
setting them as either flow or inert with uniform
probability.
A Gibbs sampler samples new variable assign-
ments from the conditional distribution, treating the
variable assignments for all other variables fixed.
However, the assignment of a single node is highly
coupled with its neighbors, so a block sampler is used
to propose changes to groups nodes that respect the
flow labeling of the overall assignments. This was
implemented by changing the proposal distribution
used by the FACTORIE framework (McCallum et al,
2009).
We can thus represent a node and its contribution
to the overall score using the graph in Figure 3. This
graph contains the given node, its parent, and a vari-
able number of children. The factors that go into the
labeling decision for each node are thus constrained
to a small, computationally tractable space around
the given node. This graph contains three factors:
? g represents a function over features of the given
node itself, or ?node features.?
? f represents a function over a bigram of features
taken from the parent node and the given node,
or ?parent-node? features.
? h represents a function over a combination fea-
tures on the node and features of all its children,
or ?node-child? features.
We provide further details about these factors in the
next section.
671
In addition to the latent value associated with each
word, we associate each node with features derived
from the dependency parse: the word from the sen-
tence itself, the part-of-speech (POS) tag assigned
by the Stanford parser, and the label of the incoming
dependency edge. We treat the edge labels from the
original dependency parse as a feature of the node.
We can represent the set of possible observed lin-
guistic feature classes as the set of features ?. Fig-
ure 3 induces a scoring function with contributions
of each node to the score(label|node) =
?
???
(
f(parent?, node?|label)g(node?|label)
h(node?, child1?, . . . , childn?|label)
)
.
After assignments for the latent variables are sampled,
the weights for the factors (which when combined
create individual factors f that define the joint) must
be learned. This is accomplished via the sample-rank
algorithm (Wick et al, 2009).
5 Data source
Our goal is to identify opinion-bearing words and tar-
gets using supervised machine learning techniques.
Sentiment corpora with sub-sentential annotations,
such as the Multi-Perspective Question-Answering
(MPQA) corpus (Wilson and Wiebe, 2005) and the
J. D. Power and Associates (JDPA) blog post cor-
pus (Kessler et al, 2010), exist, but most of these
annotations are at a phrase level. Within a phrase,
however, some words may contribute more than oth-
ers to the statement of an opinion. We developed our
own annotations to discover such distinctions3. We
describe these briefly here; more information about
the development of the data source can be found in
Sayeed et al (2011).
5.1 Information technology business press
Our work is part of a larger collaboration with so-
cial scientists to study the diffusion of information
technology (IT) innovations through society by iden-
tifying opinion leaders and IT-relevant opinionated
language Rogers (2003). Thus, we focus on a col-
lection of articles from the IT professional maga-
zine, Information Week, from the years 1991 to 2008.
3To download the corpus, visit http://www.umiacs.
umd.edu/?asayeed/naacl12data/.
This consists of 33K articles including news bulletins
and opinion columns. Our IT concept target list (59
terms) comes from our application. Thus, we con-
struct a trie for each appearance of any of these possi-
ble target terms. We consider this list of target terms
to be complete, which allows us to focus on discover-
ing opinion-bearing text associated with these targets.
5.2 Crowdsourced annotation process
Our process for obtaining gold standard data involves
multiple levels of human annotation including on
crowdsourcing platforms Hsueh et al (2009).
There are 75K sentences with IT concept mentions,
only a minority of which express relevant opinions.
Hired undergraduate students searched a random se-
lection of these sentences and found 219 that contain
these opinions. We used cosine-similarity to rank the
remaining sentences against the 219.
We then needed to identify which of the words
contained an opinion. We excluded all words that
were common function words (e.g.,?the?, ?in?) but
left negations. We engineered tasks so that only
a randomly-selected five or six words appear high-
lighted for classification in order to limit annotator
boredom. We called this group a ?highlight group?.
The virtualization example would look like this:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtual-
ization also opens up a slew of potential
network access control issues.
In the virtualization example, the worker would see
that virtualization is highlighted as the IT concept
target. Other words are highlighted as candidates that
the worker must classify as being opinion-relevant to
?virtualization?. Each highlight group corresponds to
a syntactic relatedness trie (Section 3).
A task was presented to a worker in the form of
a highlight group and some list boxes that represent
classes for the highlighted words: ?positive?, ?nega-
tive?, ?not opinion-relevant?, and ?ambiguous?. The
worker was required to drag each highlighted can-
didate word to exactly one of the boxes. As we are
not doing opinion polarity classification, the ?posi-
tive? and ?negative? boxes were intended as a form
of misdirection intended to avoid having the worker
consider what an opinion is; we treated this input as
a single ?opinion-relevant? category.
672
Three or more users annotated each highlight
group, and an aggregation scheme was applied af-
terwards: ?ambiguous? answers were rolled into ?not
opinion-relevant? and ties were dropped. Our qual-
ity control process involved filtering out workers
who performed poorly on a small subset of gold-
standard answers We annotated 30 evaluation units to
determine that our process retrieved opinion-relevant
words at 85% precision and 74% recall.
Annotators labeled 700 highlight groups for the
results in this paper. The total cost of this exercise
was approximately 250 USD, which includes the fees
charged by Amazon and CrowdFlower. These last
highlight groups were converted to SRTs and divided
into training and testing groups, 465 and 196 SRTs
respectively, with a small number lost to fatal errors
in the Stanford parser.
6 Experiments and discussion
During the training phase, we evaluate the quality
of a candidate labeling based on label accuracy. We
need to identify both flow nodes and inert nodes in
order to distinguish between relevant and irrelevant
subcomponents. We thus also employ precision and
recall as performance metrics.
An example of how this works can be seen by com-
paring figure 2(b) to figure 2(d), viewing the former
as the gold standard and the latter as a hypothetical
system output. If we run the evaluation over that
single SRT and treat flow as the positive class, we
find that 3 true positives, 1 false positive, 2 false neg-
atives, and no true negatives. There are 6 labels in
total. That yields 0.50 accuracy, 0.75 precision, 0.60
recall, and 0.67 F-measure.
We run every experiment (training a model and
testing on held-out data) 10 times and take the mean
average and range of all measures. F-measure is
calculated for each run and averaged post hoc.
6.1 Experiments
Our baseline system is the initial setting of the labels
for the sampler: uniform random assignment of flow
labels, respecting the invariant. This leads to a large
class imbalance in favor of inert as any switch to
inert converts all nodes downstream from the root to
convert to inert, while a switch to flow causes only
one ancestor branch to convert to flow.
Our next systems involve combinations of our SRT
factors with the observed linguistic features. All our
experiments include the factor g that pertains only to
the features of the node. Then we add factor f?the
parent-node ?bigram? features?and finally factor h,
the variable-length node-child features. We also ex-
periment with including and excluding combinations
of POS, role, and word features. We also explored
models that only made local decisions, ignoring the
consistency constraints over sentiment flows. Al-
though such models cannot be used in techniques
such as Nakagawa et al?s polarity classifier, they
function as a baseline and inform whether syntactic
constraints help performance.
We ran the inferencer for 200 iterations to train a
model with a particular factor-feature combination.
We use the learned model to predict the labels on
the held-out testing data by running the inference
algorithm (sampling labels only) for 50 iterations.
6.2 Discussion
We present a sampling of possible feature-factor com-
binations in table 1 in order to show trends in the
performance of the system.
Unsurprisingly, the invariant-respecting baseline
had very high precision but low recall. Simply includ-
ing the node-only g factor with all features increases
the recall while hurting precision. On removing word
features, recall increases without changing precision.
This suggests that some words in some SRTs are as-
sociated with flow labels in the training data, but not
as much in the testing data.
Including parent-node f features with the g fea-
tures yields higher precision and lower recall, sug-
gesting that parent-node word features support preci-
sion. Including all features on all factors (f , g, and h)
preserves most of the precision but improves recall.
Excluding h features increases recall slightly more
than it hurts precision. Excluding both word features
for all factors and role h features hurts all measures.
The accuracy measure, however, does show over-
all improvement with the inclusion of more feature-
factor combinations. In particular, the node-child h
factor does appear to have an effect on the perfor-
mance. The presence of some combinations of child
word, POS tags, and roles appear to provide some
indication of the flow labeling of some of the nodes.
The best models in terms of accuracy include all or
673
Experiment Features Invariant? Precision Recall F Accuracy
Baseline N/A
Yes 0.78 ? 0.05 0.06 ? 0.01 0.11 ? 0.02 0.51 ? 0.01
No 0.50 ? 0.00 0.49 ? 0.00 0.50 ? 0.00 0.50 ? 0.00
Node only
All
Yes 0.63 ? 0.10 0.34 ? 0.10 0.42 ? 0.07 0.54 ? 0.03
No 0.51 ? 0.00 0.88 ? 0.03 0.65 ? 0.01 0.51 ? 0.01
All but word
Yes 0.63 ? 0.16 0.40 ? 0.22 0.42 ? 0.19 0.53 ? 0.03
No 0.57 ? 0.04 0.56 ? 0.17 0.55 ? 0.07 0.55 ? 0.03
Parent, node
Parent: all but word
Yes 0.71 ? 0.06 0.21 ? 0.04 0.31 ? 0.05 0.55 ? 0.01
Node: all
All Yes 0.84 ? 0.07 0.11 ? 0.04 0.19 ? 0.06 0.53 ? 0.01
Full graph
Parent: all but word
Yes 0.59 ? 0.06 0.39 ? 0.11 0.46 ? 0.07 0.54 ? 0.03Node: all but word
Children: POS only
Parent: all
Yes 0.67 ? 0.05 0.39 ? 0.08 0.47 ? 0.06 0.59 ? 0.02Node: all
Children: all but word
All
Yes 0.70 ? 0.05 0.35 ? 0.08 0.46 ? 0.07 0.59 ? 0.02
No 0.70 ? 0.03 0.20 ? 0.05 0.36 ? 0.06 0.56 ? 0.01
Table 1: Performance using different feature combinations, including some without enforcing the invariant.
Mean averages and standard deviation for 10 runs.
almost all of the features.
Our non-invariant-respecting baseline unsurpris-
ingly was nearly 50% on all measures. Including the
node-only features dramatically increases recall, less
if we exclude word features. The word features ap-
pear to have an effect on recall just as in the invariant-
respecting case with node-only features. With all
features, precision is dramatically improved, but with
a large cost to recall. However, it underperforms
the equivalent invariant-respecting model in recall,
F-measure, and accuracy.
Though these invariant-violating models are un-
constrained in the way they label the graph, our
invariant-respecting models still outperform them.
A coherent path contains more information than an
incoherent one; it is important to find negating and
intensifying elements in context. Our SRT invariant
allows us to achieve better performance and will be
more useful to downstream tasks.
Finally, it appears that using more factors and lin-
guistic features promotes stability in performance
and decreases sensitivity to the initial setting.
6.3 Manual inspection
One pattern that prominently stood out in the testing
data with the full-graph model was the misclassifica-
tion of flow labels as inert in the vicinity of Stanford
dependency labels such as conj and. These kinds
of labels have high ?fertility?; the labels immediately
following them in the SRT could be a variety of types,
creating potential data sparsity issues.
This problem could be resolved by making some
features transparent to the learner. For example, if
node q has an incoming conj and dependency edge
label, then q?s parent could also be directly connected
to q?s children, as a conjunction should be linguisti-
cally transparent to the status of the children in the
sentiment flow.
There are many fewer incidents of inert labels be-
ing classified as flow. There are paths through an
SRT where a flow candidate word is the ancestor of
an inert candidate word from the set of crowdsourced
candidates. The model sometimes appears to ?over-
shoot? the flow candidate. Considering that recall is
already fairly low, attempts to address this problem
risks making the model too conservative. One poten-
tial solution is to prune or separate paths that contain
multiple flow candidates.
6.3.1 Paths found
We examined the labeling on the held-out testing
data of the best-performing model of the full graph
system with all linguistic features. For example, con-
sider the following highlight group:
But Microsoft?s informal approach may not be
enough as the number of blogs at the company
grows, especially since the line between ?personal?
Weblogs and those done as part of the job can be
hard to distinguish.
In this case, the Turkers decided that ?distinguish?
expressed a negative opinion about blogs, in the sense
674
that something that was difficult to distinguish was
a problem: the modifier ?hard? is what makes it
negative. The system found an entirely flow path that
connected these attributes into a single unit:
Blog:flow prepof????? number:flow nsubj????
grows:flow ccomp????? hard:flow xcomp?????
distinguish:flow
In this path, ?blog? and ?distinguish? are both con-
nected to one another by ?hard?, giving ?distinguish?
its negative spin. There are two non-local dependen-
cies in this example: xcomp, ccomp. Very often,
more than one unique path connects the concept to
the opinion candidate word.
7 Conclusions and future work
In this work, we have applied machine learning to
produce a robust modeling of syntactic structure for
an information extraction application. A solution to
the problem of modeling these structures requires the
development of new techniques that model complex
linguistic relationships in an application-dependent
way. We have shown that we can mine these relation-
ships without being overcome by the data-sparsity
issues that typically stymie learning over complex
linguistic structure.
The limitations on these techniques ultimately find
their root in the difficulty in modeling complex syn-
tactic structures that simultaneously exclude irrel-
evant portions of the structure while maintaining
connected relations. Our technique uses a structure-
labelling scheme that enforces connectedness. En-
forcing connected structure is not only necessary to
produce useful results but also to improve accuracy.
Further performance gains might be possible by en-
riching the feature set. For example, the POS tagset
used by the Stanford parser contains multiple verb
tags that represent different English tenses and num-
bers. For the purpose of sentiment relations, it is
possible that the differences between verb tags are
too small to matter and are causing data sparsity is-
sues. Thus, we could additional features that ?back
off? to general verb tags.
Acknowledgements
This paper is based upon work supported by the
US National Science Foundation under Grant IIS-
0729459. Additional support came from the Cluster
of Excellence ?Multimodal Computing and Innova-
tion?, Germany. Jordan Boyd-Graber is also sup-
ported by US National Science Foundation Grant
NSF grant #1018625 and the Army Research Labora-
tory through ARL Cooperative Agreement W911NF-
09-2-0072. Any opinions, findings, conclusions, or
recommendations expressed are the authors? and do
not necessarily reflect those of the sponsors.
References
Alm, C. O. (2011). Subjective natural language prob-
lems: Motivations, applications, characterizations,
and implications. In ACL (Short Papers).
Bollen, J., Mao, H., and Zeng, X.-J. (2010). Twit-
ter mood predicts the stock market. CoRR,
abs/1010.3003.
Choi, Y., Breck, E., and Cardie, C. (2006). Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
de Marneffe, M.-C. and Manning, C. D. (2008). The
stanford typed dependencies representation. In
CrossParser ?08: Coling 2008: Proceedings of
the workshop on Cross-Framework and Cross-
Domain Parser Evaluation, Morristown, NJ, USA.
Association for Computational Linguistics.
Finkel, J. R., Grenager, T., and Manning, C. (2005).
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Hsueh, P.-Y., Melville, P., and Sindhwani, V. (2009).
Data quality from crowdsourcing: a study of anno-
tation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learn-
ing for Natural Language Processing, HLT ?09,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jakob, N. and Gurevych, I. (2010). Extracting opin-
ion targets in a single and cross-domain setting
with conditional random fields. In EMNLP.
Kessler, J. S., Eckert, M., Clark, L., and Nicolov,
N. (2010). The 2010 ICWSM JDPA sentment
675
corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Kim, S.-M. and Hovy, E. (2006). Extracting opinions,
opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the
Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Kschischang, F. R., Frey, B. J., and andrea Loeliger,
H. (1998). Factor graphs and the sum-product algo-
rithm. IEEE Transactions on Information Theory,
47:498?519.
Ku?bler, S., McDonald, R., and Nivre, J. (2009). De-
pendency parsing. Synthesis Lectures on Human
Language Technologies, 2(1).
McCallum, A., Schultz, K., and Singh, S. (2009).
Factorie: Probabilistic programming via impera-
tively defined factor graphs. In Neural Information
Processing Systems (NIPS).
Moilanen, K. and Pulman, S. (2007). Sentiment com-
position. In Proceedings of the Recent Advances in
Natural Language Processing International Con-
ference (RANLP-2007), Borovets, Bulgaria.
Nakagawa, T., Inui, K., and Kurohashi, S. (2010). De-
pendency tree-based sentiment classification using
crfs with hidden variables. In HLT-NAACL.
Neal, R. M. (1993). Probabilistic inference using
Markov chain Monte Carlo methods. Technical
Report CRG-TR-93-1, University of Toronto.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2).
Qiu, G., Liu, B., Bu, J., and Chen, C. (2011). Opin-
ion word expansion and target extraction through
double propagation. Computational linguistics,
37(1):9?27.
Rogers, E. M. (2003). Diffusion of Innovations, 5th
Edition. Free Press.
Ruppenhofer, J., Somasundaran, S., and Wiebe, J.
(2008). Finding the sources and targets of sub-
jective expressions. In Calzolari, N., Choukri, K.,
Maegaard, B., Mariani, J., Odjik, J., Piperidis, S.,
and Tapias, D., editors, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. European Lan-
guage Resources Association (ELRA).
Sayeed, A. B., Nguyen, H. C., Meyer, T. J., and
Weinberg, A. (2010). Expresses-an-opinion-about:
using corpus statistics in an information extraction
approach to opinion mining. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10.
Sayeed, A. B., Rusk, B., Petrov, M., Nguyen, H. C.,
Meyer, T. J., and Weinberg, A. (2011). Crowd-
sourcing syntactic relatedness judgements for opin-
ion mining in the study of information technology
adoption. In Proceedings of the Association for
Computational Linguistics 2011 workshop on Lan-
guage Technology for Cultural Heritage, Social
Sciences, and the Humanities (LaTeCH). Associa-
tion for Computational Linguistics.
Stoyanov, V. and Cardie, C. (2006). Partially su-
pervised coreference resolution for opinion sum-
marization through structured rule learning. In
EMNLP ?06: Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 336?344, Morristown, NJ, USA.
Association for Computational Linguistics.
Tsui, C.-J., Wang, P., Fleischmann, K., Oard, D.,
and Sayeed, A. (2009). Understanding IT innova-
tions by computational analysis of discourse. In
International conference on information systems.
Wick, M., Rohanimanesh, K., Culotta, A., and Mccal-
lum, A. (2009). SampleRank: Learning preference
from atomic gradients. In NIPS WS on Advances
in Ranking.
Wilson, T. and Wiebe, J. (2005). Annotating attribu-
tions and private states. In CorpusAnno ?05: Pro-
ceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association
for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In HLT/EMNLP.
676
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 69?77,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Crowdsourcing syntactic relatedness judgements for opinion mining in the
study of information technology adoption
Asad B. Sayeed, Bryan Rusk, Martin Petrov,
Hieu C. Nguyen, Timothy J. Meyer
Department of Computer Science
University of Maryland
College Park, MD 20742 USA
asayeed@cs.umd.edu,brusk@umd.edu,
martin@martinpetrov.com,
{hcnguyen88,tmeyer88}@gmail.com
Amy Weinberg
Center for the Advanced
Study of Language
and Department of Linguistics
University of Maryland
College Park, MD 20742 USA
aweinberg@casl.umd.edu
Abstract
We present an end-to-end pipeline including
a user interface for the production of word-
level annotations for an opinion-mining task
in the information technology (IT) domain.
Our pre-annotation pipeline selects candidate
sentences for annotation using results from a
small amount of trained annotation to bias the
random selection over a large corpus. Our
user interface reduces the need for the user to
understand the ?meaning? of opinion in our
domain context, which is related to commu-
nity reaction. It acts as a preliminary buffer
against low-quality annotators. Finally, our
post-annotation pipeline aggregates responses
and applies a more aggressive quality filter.
We present positive results using two differ-
ent evaluation philosophies and discuss how
our design decisions enabled the collection of
high-quality annotations under subjective and
fine-grained conditions.
1 Introduction
Crowdsourcing permits us to use a bank of anony-
mous workers with unknown skill levels to perform
complex tasks given a simple breakdown of these
tasks with user interface design that hides the full
task complexity. Use of these techniques is growing
in the areas of computational linguistics and infor-
mation retrieval, particularly since these fields now
rely on the collection of large datasets for use in ma-
chine learning. Considering the variety of applica-
tions, a variety of datasets is needed, but trained,
known workers are an expense in principle that must
be furnished for each one. Consequently, crowd-
sourcing offers a way to collect this data cheaply and
quickly (Snow et al, 2008; Sayeed et al, 2010a).
We applied crowdsourcing to perform the fine-
grained annotation of a domain-specific corpus. Our
user interface design and our annotator quality con-
trol process allows these anonymous workers to per-
form a highly subjective task in a manner that cor-
relates their collective understanding of the task to
our own expert judgements about it. The path to
success provides some illustration of the pitfalls in-
herent in opinion annotation. Our task is: domain
and application-specific sentiment classification at
the sub-sentence level?at the word level.
1.1 Opinions
For our purposes, we define opinion mining (some-
times known as sentiment analysis) to be the re-
trieval of a triple {source, target, opinion} (Sayeed
et al, 2010b; Pang and Lee, 2008; Kim and Hovy,
2006) in which the source is the entity that origi-
nated the opinionated language, the target is a men-
tion of the entity or concept that is the opinion?s
topic, and the opinion is a value (possibly a struc-
ture) that reflects some kind of emotional orientation
expressed by the source towards the target.
In much of the recent literature on automatic
opinion mining, opinion is at best a gradient be-
tween positive and negative or a binary classifica-
tion thereof; further complexity affects the reliability
of machine-learning techniques (Koppel and Schler,
2006).
We call opinion mining ?fine-grained? when we
are attempting to retrieve potentially many different
69
{source, target, opinion} triples per document. This
is particularly challenging when there are multiple
triples even at a sentence level.
1.2 Corpus-based social science
Our work is part of a larger collaboration with social
scientists to study the diffusion of information tech-
nology (IT) innovations through society by identify-
ing opinion leaders and IT-relevant opinionated lan-
guage (Rogers, 2003). A key hypothesis is that the
language used by opinion leaders causes groups of
others to encourage the spread of the given IT con-
cept in the market.
Since the goal of our exercise is to ascertain the
correlation between the source?s behaviour and that
of others, then it may be more appropriate to look
at opinion analysis with the view that what we are
attempting to discover are the views of an aggregate
reader who may otherwise have an interest in the IT
concept in question. We thus define an expression of
opinion in the following manner:
A expresses opinion about B if an in-
terested third party C?s actions towards B
may be affected by A?s textually recorded
actions, in a context where actions have
positive or negative weight.
This perspective runs counter to a widespread view
(Ruppenhofer et al, 2008) which has assumed a
treatment of opinionated language as an observation
of a latent ?private state? held by the source. This
definition reflects the relationship of sentiment and
opinion with the study of social impact and market
prediction. We return to the question of how to de-
fine opinion in section 6.2.
1.3 Crowdsourcing in sentiment analysis
Paid crowdsourcing is a relatively new trend in com-
putational linguistics. Work exists at the paragraph
and document level, and it exists for the Twitter and
blog genres (Hsueh et al, 2009).
A key problem in crowdsourcing sentiment analy-
sis is the matter of quality control. A crowdsourced
opinion mining task is an attempt to use untrained
annotators over a task that is inherently very subjec-
tive. It is doubly difficult for specialized domains,
since crowdsourcing platforms have no way of di-
rectly recruiting domain experts.
Hsueh et al (2009) present results in quality con-
trol over snippets of political blog posts in a task
classifying them by sentiment and political align-
ment. They find that they can use a measurement of
annotator noise to eliminate low-quality annotations
at this coarse level by reweighting snippet ambigu-
ity scores with noise scores. We demonstrate that we
can use a similar annotator quality measure alone to
eliminate low-quality annotations on a much finer-
grained task.
1.4 Syntactic relatedness
We have a downstream application for this annota-
tion task which involves acquiring patterns in the
distribution of opinion-bearing words and targets us-
ing machine learning (ML) techniques. In partic-
ular, we want to acquire the syntactic relationships
between opinion-bearing words and within-sentence
targets. Supervised ML techniques require gold
standard data annotated in advance.
The Multi-Perspective Question-Answering
(MPQA) newswire corpus (Wilson and Wiebe,
2005) and the J. D. Power & Associates (JDPA)
automotive review blog post (Kessler et al, 2010)
corpus are appropriate because both contain sub-
sentence annotations of sentiment-bearing language
as text spans. In some cases, they also include links
to within-sentence targets. This is an example of an
MPQA annotation:
That was the moment at which the fabric
of compassion tore, and worlds cracked
apart; when the contrast and conflict of
civilisational values became so great as
to remove any sense of common ground -
even on which to do battle.
The italicized portion is intended to reflect a negative
sentiment about the bolded portion. However, while
it is the case that the whole italicized phrase repre-
sents a negative sentiment, ?remove? appears to rep-
resent far more of the negativity than ?common? and
?ground?. While there are techniques that depend
on access to entire phrases, our project is to identify
sentiment spans at the length of a single word.
2 Data source
Our corpus for this task is a collection of arti-
cles from the IT professional magazine, Information
70
Week, from the years 1991 to 2008. This consists
of 33K articles of varying lengths including news
bulletins, full-length magazine features, and opin-
ion columns. We obtained the articles via an institu-
tional subscription, and reformatted them in XML1.
Certain IT concepts are particularly significant in
the context of the social science application. Our tar-
get list consists of 59 IT innovations and concepts.
The list includes plurals, common variations, and
abbreviations. Examples of IT concepts include ?en-
terprise resource planning? and ?customer relation-
ship management?. To avoid introducing confound-
ing factors into our results, we only include explicit
mentions and omit pronominal coreference.
3 User interface
Our user interface (figure 1) uses a drag-and-drop
process through which workers make decisions
about whether particular highlighted words within
a given sentence reflect an opinion about a particu-
lar mentioned IT concept or innovation. The user
is presented with a sentence from the corpus sur-
rounded by some before and after context. Under-
neath the text are four boxes: ?No effect on opin-
ion? (none), ?Affects opinion positively? (postive),
?Affects opinion negatively? (negative), and ?Can?t
tell? (ambiguous).
The worker must drag each highlighted word in
the sentence into one of the boxes, as appropriate. If
the worker cannot determine the appropriate box for
a particular word, she is expected to drag this to the
ambiguous box. The worker is presented with de-
tailed instructions which also remind her that most
of words in the sentence are not actually likely to be
involved in the expression of an opinion about the
relevant IT concept2. The worker is not permitted
to submit the task without dragging all of the high-
lighted words to one of the boxes. When a word
is dragged to a box, the word in context changes
colour; the worker can change her mind by clicking
an X next to the word in the box.
1We will likely be able to provide a sample of sentence data
annotated by our process as a resource once we work out docu-
mentation and distribution issues.
2We discovered when testing the interface that workers can
feel obliged to find a opinion about the selected IT concept. We
reduced it by explicitly reminding them that most words do not
express a relevant opinion and by placing the none box first.
We used CrowdFlower to manage the task with
Amazon Mechanical Turk as its distribution chan-
nel. We set CrowdFlower to present three sentences
at a time to users. Only users with USA-based IP
addresses were permitted to perform the final task.
4 Procedure
In this section, we discuss the data processing
pipeline (figure 3) through which we select candi-
dates for annotations and the crowdsourcing inter-
face we present to the end user for classifying indi-
vidual words into categories that reflect the effect of
the word on the worker.
4.1 Data preparation
4.1.1 Initial annotation
Two social science undergraduate students were
hired to do annotations on Information Week with
the original intention of doing all the annotations
this way. There was a training period where they an-
notated about 60 documents in sets of 20 in iterative
consultation with one of the authors. Then they were
given 142 documents to annotate simultaneously in
order to assess their agreement after training.
Annotation was performed in Atlas.ti, an anno-
tation tool popular with social science researchers.
It was chosen for its familiarity to the social sci-
entists involved in our project and because of their
stated preference for using tools that would allow
them to share annotations with colleagues. Atlas.ti
has limitations, including the inability to create hier-
archical annotations. We overcame these limitations
using a special notation to connect related annota-
tions. An annotator highlights a sentence that she
believes contains an opinion about a mentioned tar-
get on one of the lists. She then highlights the men-
tion of the target and, furthermore, highlights the in-
dividual words that express the opinion about the tar-
get, using the notation to connect related highlights.
4.1.2 Candidate selection
While the use of trained annotators did not pro-
duce reliable results (section 6.2) in acceptable time
frames, we decided to use the annotations in a pro-
cess for selecting candidate sentences for crowd-
sourcing. All 219 sentences that the annotators se-
lected as having opinions about within-sentence IT
71
Figure 1: A work unit presented in grayscale. ?E-business? is the IT concept and would be highlighted in blue. The
words in question are highlighted in gray background and turn red after they are dragged to the boxes.
concepts were concatenated into a single string and
converted into a TFIDF unit vector.
We then selected all the sentences that contain
IT concept mentions from the entire Information
Week corpus using an OpenNLP 1.4.3 model as
our sentence-splitter. This produced approximately
77K sentences. Every sentence was converted into a
TFIDF unit vector, and we took the cosine similar-
ity of each sentence with the TFIDF vector. We then
ranked the sentences by cosine similarity.
4.1.3 Selecting highlighted words
We ran every sentence through the Stanford
part-of-speech tagger. Words that belonged to
open classes such as adjectives and verbs were se-
lected along with certain closed-class words such as
modals and negation words. These candidate words
were highlighted in the worker interface.
We did not want to force workers to classify every
single word in a sentence, because this would be too
tedious. So we instead randomly grouped the high-
lighted words into non-overlapping sets of six. (Re-
mainders less than five were dropped from the task.)
We call these combinations of sentence, six words,
and target IT concept a ?highlight group? (figure 2).
Each highlight group represents a task unit which
we present to the worker in our crowdsourcing ap-
plication. We generated 1000 highlight groups from
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
Figure 2: Two highlight groups consisting of the
same sentence and concept (ERP) but different non-
overlapping sets of candidate words.
the top-ranked sentences.
4.2 Crowdsourced annotation
4.2.1 Training gold
We used CrowdFlower partly because of its au-
tomated quality control process. The bedrock of
this process is the annotation of a small amount of
gold standard data by the task designers. Crowd-
Flower randomly selects gold-annotated tasks and
presents them to workers amidst other unannotated
tasks. Workers are evaluated by the percentage of
gold-annotated tasks they perform correctly. The re-
sult of a worker performing a task unit is called a
?judgement.?
Workers are initially presented their gold-
annotated tasks without knowing that they are an-
swering a test question. If they get the question
wrong, CrowdFlower presents the correct answer to
72
them along with a reason why their answer was an
error. They are permitted to write back to the task
designer if they disagree with the gold judgement.
This process functions in a manner analogous to
the training of a machine-learning system. Further-
more, it permits CrowdFlower to exclude or reject
low-quality results. Judgements from a worker who
slips below 65% correctness are rated as untrustwor-
thy and not included in the CrowdFlower?s results.
We created training gold in the manner recom-
mended by CrowdFlower. We randomly selected
50 highlight groups from the 1000 mentioned in the
previous section. We ran these examples through
CrowdFlower using the interface we discuss in the
next section. Then we used the CrowdFlower gold
editor to select 30 highlight groups that contained
clear classification decisions where it appeared that
the workers were in relative consensus and where we
agreed with their decision. Of these, we designated
only the clearest-cut classifications as gold, leav-
ing more ambiguous-seeming ones up to the users.
For example, in the second highlight group in 2, we
would designate software and systems as none and
extend as positive in the training gold and the re-
mainder as up to the workers. That would be a ?min-
imum effort? to indicate that the worker understands
the task the way we do.
Unfortunately, CrowdFlower has some limita-
tions in the way it processes the responses to gold?
it is not possible to define a minimum effort pre-
cisely. CrowdFlower?s setting either allow us to pass
workers based on getting at least one item in each
class correct or by placing all items in their correct
classes. The latter is too strict a criterion for an in-
herently subjective task. So we accepted the former.
We instead applied our minimum effort criterion in
some of our experiments as described in section 4.3.
4.2.2 Full run
We randomly selected another 200 highlight
groups and posted them at 12 US cents for each set
of three highlight groups, with at least three Me-
chanical Turk workers seeing each highlight group.
The 30 training gold highlight groups were posted
along with them. Including CrowdFlower and Ama-
zon fees, the total cost was approximately 60 USD.
We permitted only USA-based workers to access the
task. Once initiated, the entire task took approxi-
Figure 3: Schematic view of pipeline.
mately 24 hours to complete.
4.3 Post-processing
4.3.1 Aggregation
Each individual worker?s ambiguous annotations
are converted to none annotations, as the ambigu-
ous box is intended as an outlet for a worker?s un-
certainty, but we choose to interpret anything that
a worker considers too uncertain to be classified
as positive or negative as something that is not
strongly opinionated under our definitions.
Aggregation is performed by majority vote of the
annotators on each word in each highlight group. If
no classification obtains more than 50% for a given
word, the word is dropped as too ambiguous to be
accepted either way as a result. This aggregation
has the effect of smoothing out individual annotator
differences.
4.3.2 Extended quality control
While CrowdFlower provides a first-pass quality
control system for selecting annotators who are do-
ing the task in good faith and with some understand-
ing of the instructions, we wanted particularly to
select annotators who would be more likely to be
consistent on the most obvious cases without overly
constraining them. Even with the same general idea
of our intentions, some amount of variation among
the annotators is unavoidable; how do we then reject
annotations from those workers who pass Crowd-
Flower?s liberal criteria but still do not have an idea
of annotation close enough to ours?
73
Our solution was to score the annotators post hoc
by their accuracy on our minimum-effort training
gold data. Then we progressively dropped the worst
n annotators starting from n = 0 and measured the
quality of the aggregated annotations as per the fol-
lowing section.
5 Results
This task can be interpreted in two different ways:
as an annotation task and as a retrieval system. An-
notator reliability is an issue insofar as it is impor-
tant that the annotations themselves conform to a
predetermined standard. However, for the machine
learning task that is downstream in our processing
pipeline, obtaining a consistent pattern is more im-
portant than conformance to an explicit definition.
We can thus interpret the results as being the out-
put of a system whose computational hardware hap-
pens to be a crowd of humans rather than silicon,
considering that the time of the ?run? is compara-
ble to many automated systems; Amazon Mechani-
cal Turk?s slogan is ?artificial artificial intelligence?
for a reason.
Nevertheless, we evaluated our procedure under
both interpretations by comparing against our own
annotations in order to assess the quality of our col-
lection, aggregation, and filtering process:
1. As an annotation task: we use Cohen?s ?
between the aggregated and filtered data vs.
our annotations in the belief that higher above-
chance agreement would imply that the aggre-
gate annotation reflected collective understand-
ing of our definition of sentiment. Consider-
ing the inherently subjective nature of this task
and the interdependencies inherent in within-
sentence judgements, Cohen?s ? is not a defini-
tive proof of success or failure.
2. As a retrieval task: Relative to our own an-
notations, we use the standard information re-
trieval measures of precision, recall, and F-
measure (harmonic mean) as well as accuracy.
We merge positive and negative annotations
into a single opinion-bearing class and measure
whether we can retrieve opinion-bearing words
while minimizing words that are, in context,
not opinion-bearing relative to the given target.
(We do not merge the classes for agreement-
based evaluation as there was not much over-
lap between positive and negative classifica-
tions.) The particular relative difference be-
tween precision and recall will suggest whether
the workers had a consistent collective under-
standing of the task.
It should be noted that the MPQA and the JDPA do
not report Cohen?s ? for subjective text spans partly
for the reason we suggest above: the difficulty of as-
sessing objective agreement on a task in which sub-
jectivity is inherent and desirable. There is also a
large class imbalance problem. Both these efforts
substitute retrieval-based measures into their assess-
ment of agreement.
We annotated a randomly-selected 30 of the 200
highlight groups on our own. Those 30 had 169
annotated words of which 117 were annotated as
none, 35 as positive, and 17 as negative. The re-
sults of our process are summarized in table 1.
In the 30 highlight groups, there were 155 total
words for which a majority consensus (>50%) was
reached. 48 words were determined by us in our
own annotation to have opinion weight (positive or
negative). There are only 22 annotators who passed
CrowdFlower?s quality control.
The stringent filter on workers based on their ac-
curacy on our minimum-effort gold annotations has
a remarkable effect on the results. As we exclude
workers, the F-measure and the Cohen?s ? appear
to rise, up to a point. By definition, each exclu-
sion raises the threshold score for acceptance. As
we cross the 80% threshold, the performance of the
system drops noticeably, as the smoothing effect of
voting is lost. Opinion-bearing words also reduce
in number as the threshold rises as some highlight
groups simply have no one voting for them. We
achieve our best result in terms of Cohen?s ? on
dropping the 7 lowest workers. We achieve our high-
est precision and accuracy after dropping the 10 low-
est workers.
Between the 7th and 10th underperforming an-
notator, we find that precision starts to exceed re-
call, possibly due to the loss of retrievable words as
some highlight groups lose all their annotators. Lost
words can be recovered in another round of annota-
tion.
74
Workers excluded No. of words lost (of 48) Prec/Rec/F Acc Cohen?s ? Score threshold
(prior polarity) N/A 0.87 / 0.38 / 0.53 0.79 -0.26 N/A
0 0 0.64 / 0.71 / 0.67 0.79 0.48 0.333
1 0 0.64 / 0.71 / 0.67 0.79 0.48 0.476
3 0 0.66 / 0.73 / 0.69 0.80 0.51 0.560
5 0 0.69 / 0.73 / 0.71 0.81 0.53 0.674
7 2 0.81 / 0.76 / 0.79 0.86 0.65 0.714
10 9 0.85 / 0.74 / 0.79 0.88 0.54 0.776
12 11 0.68 / 0.68 / 0.68 0.82 0.20 0.820
Table 1: Results by number of workers excluded from the task. The prior polarity baseline comes from a lexicon by
Wilson et al (2005) that is not specific to the IT domain.
6 Discussion
We have been able to show that crowdsourcing a
very fine-grained, domain-specific sentiment analy-
sis task with a nonstandard, application-specific def-
inition of sentiment is possible with careful user in-
terface design and mutliple layers of quality control.
Our techniques succeed on two different interpreta-
tions of the evaluation measure, and we can reclaim
any lost words by re-running the task. We used an
elaborate processing pipeline before and after anno-
tation in order to accomplish this. In this section, we
discuss some aspects of the pipeline that led to the
success of this technique.
6.1 Quality
There are three major aspects of our procedure that
directly affect the quality of our results: the first-
pass quality control in CrowdFlower, the majority-
vote aggregation, and the stringent post hoc filtering
of workers. These interact in particular ways.
The first-pass quality control interacts with the
stringent filter in that even if it were possible to
have run the stringent filter on CrowdFlower itself,
it would probably not have been a good idea. Al-
though we intended the stringent filter to be a min-
imum effort, it would have rejected workers too
quickly. It is technically possible to implement the
stringent filtering directly without the CrowdFlower
built-in control, but that would have entailed spend-
ing an unpredictable amount more money paying for
additional unwanted annotations from workers.
Furthermore, the majority-vote aggregation re-
quires that there not be too few annotators; our re-
sults show that filtering the workers too aggressively
harms the aggregation?s smoothing effect. The les-
son we take from this is that it can be beneficial to
accept some amount of ?bad? with the ?good? in im-
plementing a very subjective crowdsourcing task.
6.2 Design decisions
Our successful technique for identifying opinionated
words was developed after multiple iterations using
other approaches which did not succeed in them-
selves but produced outputs that were amenable to
refinement, and so these techniques became part of
a larger pipeline. However, the reasons why they did
not succeed on their own are illustrative of some of
the challenges in both fine-grained domain-specific
opinion annotation and in annotation via crowd-
sourcing under highly subjective conditions.
6.2.1 Direct annotation
We originally intended to stop with the trained an-
notation we described in 4.1.1, but collecting opin-
ionated sentences in this corpus turned out to be very
slow. Despite repeated training rounds, the annota-
tors had a tendency to miss a large number of sen-
tences that the authors found to be relevant. On dis-
cussion with the annotators, it turned out that the
variable length of the articles made it easy to miss
relevant sentences, particularly in the long feature
articles likely to contain opinionated language?a
kind of ?needle-in-a-haystack? problem.
Even worse, however, the annotators were vari-
ably conservative about what constituted an opinion.
One annotator produced far fewer annotations than
the other one?but the majority of her annotations
were also annotated by the other one. Discussion
with the annotators revealed that one of them simply
had a tighter definition of what constituted an opin-
ion. Attempts to define opinion explicitly for them
still led to a situations in which one was far more
conservative than the other.
75
6.2.2 Cascaded crowdsourcing technique
Insofar as we were looking for training data for
use in downstream machine learning techniques,
getting uniform sentence-by-sentence coverage of
the corpus was not necessary. There are 77K sen-
tences in this corpus which mention the relevant IT
concepts; even if only a fraction of them mention the
IT concepts with opinionated language, we would
still have a potentially rich source of training data.
Nevertheless the direct annotation with trained
annotators provided data for selecting candidate sen-
tences for a more rapid annotation. We used the
process in section 4.1.2 and chose the top-ranked
sentences. Then we constructed a task design that
divided the annotation into two phases. In the first
phase, for each candidate sentence, we ask the anno-
tator whether or not the sentence contains opinion-
ated language about the mentioned IT concept. (We
permit ?unsure? answers.)
In the second phase, for each candidate sentence
for which a majority vote of annotators decided that
the sentence contained a relevant opinion, we run
a second task asking whether particular words (se-
lected as per section 4.1.3) were words directly in-
volved in the expression of the opinion.
We tested this process with the 90 top-ranked
sentences. Four individuals in our laboratory an-
swered the ?yes/no/unsure? question of the first
phase. However, when we took their pairwise Co-
hen?s ? score, no two got more than approximately
0.4. We also took majority votes of each subset of
three annotators and found the Cohen?s ? between
them and the fourth. The highest score was 0.7, but
the score was not stable, and we could not trust the
results enough to move onto the second phase.
We also ran this first phase through Amazon Me-
chanical Turk. It turned out that it was far too easy
to cheat on this yes/no question, and some workers
simply answered ?yes? or ?no? all the time. Agree-
ment scores of a Turker majority vote vs. one of the
authors turned out to yield a Cohen?s ? of 0.05?
completely unacceptable.
Discussion with the in-laboratory annotators sug-
gested the roots of the problem: it was the same
problem as with the direct Atlas.ti annotation we re-
ported in the previous section. It was very difficult
for them to agree on what it meant for a sentence to
contain an opinion expressed about a particular con-
cept. Opinions about the nature of opinion ranged
from very ?conservative? to very ?liberal.? Even
explicit definition with examples led annotators to
reach very different conclusions. Furthermore, the
longer the annotators thought about it, the more con-
fused and uncertain they were about the criterion.
What is an opinion can itself be a matter of opin-
ion. It became clear that without very tight review
of annotation and careful task design, asking users
an explicit yes/no question about whether a particu-
lar concept has a particular opinion mentioned in a
particular sentence has the potential to induce over-
thinking by annotators, despite our variations on the
task. The difficulty may also lead to a tendency to
cheat. Crowdsourcing allows us to make use of non-
expert labour on difficult tasks if we can break the
tasks down into simple questions and aggregate non-
expert responses, but we needed a somewhat more
complex task design in order to eliminate the diffi-
culty of the task and the tendency to cheat.
7 Future work
Foremost among the avenues for future work is ex-
perimentation with other vote aggregration and post
hoc filtering schemes. For example, one type of ex-
periment could be the reweighting of votes by an-
notator quality rather than the wholesale dropping
of annotators. Another could involve the use of
general-purpose sentiment analysis lexica to bias the
vote aggregation in the manner of work in sentiment
domain transfer (Tan et al, 2007).
This work also points to the potential for crowd-
sourcing in computational linguistics applications
beyond opinion mining. Our task is a sentiment-
specific instance of a large class of syntactic relat-
edness problems that may suitable for crowdsourc-
ing. One practical application would be in obtaining
training data for coreference detection. Another one
may be in the establishment of empirical support for
theories about syntactic structure.
Acknowledgements
This paper is based on work supported by the Na-
tional Science Foundation under grant IIS-0729459.
76
References
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, HLT ?09, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA sent-
ment corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media Data
Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Moshe Koppel and Jonathan Schler. 2006. The im-
portance of neutral examples for learning sentiment.
Computational Intelligence, 22(2).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2).
Everett M. Rogers. 2003. Diffusion of Innovations, 5th
Edition. Free Press.
Josef Ruppenhofer, Swapna Somasundaran, and Janyce
Wiebe. 2008. Finding the sources and targets of
subjective expressions. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08), Marrakech, Morocco. Eu-
ropean Language Resources Association (ELRA).
Asad B. Sayeed, Timothy J. Meyer, Hieu C. Nguyen,
Olivia Buzek, and Amy Weinberg. 2010a. Crowd-
sourcing the evaluation of a domain-adapted named
entity recognition system. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Asad B. Sayeed, Hieu C. Nguyen, Timothy J. Meyer, and
Amy Weinberg. 2010b. Expresses-an-opinion-about:
using corpus statistics in an information extraction ap-
proach to opinion mining. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, New York, NY, USA.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In CorpusAnno ?05:
Proceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association for
Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
77
