Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 220?229,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
MEANT: An inexpensive, high-accuracy, semi-automatic metric for
evaluating translation utility via semantic frames
Chi-kiu Lo and DekaiWu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo,dekai}@cs.ust.hk
Abstract
We introduce a novel semi-automated metric,
MEANT, that assesses translation utility by match-
ing semantic role fillers, producing scores that cor-
relate with human judgment as well as HTER but
at much lower labor cost. As machine transla-
tion systems improve in lexical choice and flu-
ency, the shortcomings of widespread n-gram based,
fluency-oriented MT evaluation metrics such as
BLEU, which fail to properly evaluate adequacy,
become more apparent. But more accurate, non-
automatic adequacy-oriented MT evaluation metrics
like HTER are highly labor-intensive, which bottle-
necks the evaluation cycle. We first show that when
using untrained monolingual readers to annotate se-
mantic roles in MT output, the non-automatic ver-
sion of the metric HMEANT achieves a 0.43 corre-
lation coefficient with human adequacy judgments at
the sentence level, far superior to BLEU at only 0.20,
and equal to the far more expensive HTER. We then
replace the human semantic role annotators with au-
tomatic shallow semantic parsing to further automate
the evaluation metric, and show that even the semi-
automated evaluation metric achieves a 0.34 corre-
lation coefficient with human adequacy judgment,
which is still about 80% as closely correlated as
HTER despite an even lower labor cost for the evalu-
ation procedure. The results show that our proposed
metric is significantly better correlated with human
judgment on adequacy than current widespread au-
tomatic evaluation metrics, while being much more
cost effective than HTER.
1 Introduction
In this paper we show that evaluating machine trans-
lation by assessing the translation accuracy of each argu-
ment in the semantic role framework correlates with hu-
man judgment on translation adequacy as well as HTER,
at a significantly lower labor cost. The correlation of this
new metric, MEANT, with human judgment is far supe-
rior to BLEU and other automatic n-gram based evalua-
tion metrics.
We argue that BLEU (Papineni et al, 2002) and other
automatic n-gram basedMT evaluation metrics do not ad-
equately capture the similarity in meaning between the
machine translation and the reference translation?which,
ultimately, is essential for MT output to be useful. N-
gram based metrics assume that ?good? translations tend
to share the same lexical choices as the reference trans-
lations. While BLEU score performs well in captur-
ing the translation fluency, Callison-Burch et al (2006)
and Koehn and Monz (2006) report cases where BLEU
strongly disagree with human judgment on translation
quality. The underlying reason is that lexical similarity
does not adequately reflect the similarity in meaning. As
MT systems improve, the shortcomings of the n-gram
based evaluation metrics are becoming more apparent.
State-of-the-art MT systems are often able to output flu-
ent translations that are nearly grammatical and contain
roughly the correct words, but still fail to express mean-
ing that is close to the input.
At the same time, although HTER (Snover et al, 2006)
is more adequacy-oriented, it is only employed in very
large scale MT system evaluation instead of day-to-day
research activities. The underlying reason is that it re-
quires rigorously trained human experts to make difficult
combinatorial decisions on the minimal number of edits
so as to make the MT output convey the same meaning as
the reference translation?a highly labor-intensive, costly
process that bottlenecks the evaluation cycle.
Instead, with MEANT, we adopt at the outset the
principle that a good translation is one that is useful,
in the sense that human readers may successfully un-
derstand at least the basic event structure??who did
what to whom, when, where and why? (Pradhan et al,
2004)?representing the central meaning of the source ut-
terances. It is true that limited tasks might exist for which
inadequate translations are still useful. But for meaning-
ful tasks, generally speaking, for a translation to be use-
ful, at least the basic event structure must be correctly un-
derstood. Therefore, our objective is to evaluate trans-
lation utility: from a user?s point of view, how well is220
the most essential semantic information being captured
by machine translation systems?
In this paper, we detail the methodology that underlies
MEANT, which extends and implements preliminary di-
rections proposed in (Lo andWu, 2010a) and (Lo andWu,
2010b). We present the results of evaluating translation
utility by measuring the accuracy within a semantic role
labeling (SRL) framework. We show empirically that our
proposed SRL based evaluation metric, which uses un-
trained monolingual humans to annotate semantic frames
inMT output, correlates with human adequacy judgments
as well as HTER, and far better than BLEU and other
commonly used metrics. Finally, we show that replacing
the human semantic role labelers with an automatic shal-
low semantic parser in our proposed metric yields an ap-
proximation that is about 80% as closely correlated with
human judgment as HTER, at an even lower cost?and
is still far better correlated than n-gram based evaluation
metrics.
2 Related work
Lexical similarity based metrics BLEU (Papineni et
al., 2002) is the most widely used MT evaluation met-
ric despite the fact that a number of large scale meta-
evaluations (Callison-Burch et al, 2006; Koehn and
Monz, 2006) report cases where it strongly disagree with
human judgment on translation accuracy. Other lexi-
cal similarity based automatic MT evaluation metrics,
like NIST (Doddington, 2002), METEOR (Banerjee and
Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch
et al, 2006) and WER (Nie?en et al, 2000), also per-
form well in capturing translation fluency, but share the
same problem that although evaluation with these metrics
can be done very quickly at low cost, their underlying as-
sumption?that a ?good? translation is one that shares the
same lexical choices as the reference translation?is not
justified semantically. Lexical similarity does not ade-
quately reflect similarity in meaning. State-of-the-art MT
systems are often able to output translations containing
roughly the correct words, yet expressing meaning that is
not close to that of the input.
We argue that a translation metric that reflects meaning
similarity is better based on similarity in semantic struc-
ture, rather than simply flat lexical similarity.
HTER (non-automatic) Despite the fact that Human-
targeted Translation Edit Rate (HTER) as proposed by
Snover et al (2006) shows a high correlation with human
judgment on translation adequacy, it is not widely used in
day-to-day machine translation evaluation because of its
high labor cost. HTER not only requires human experts
to understand the meaning expressed in both the refer-
ence translation and the machine translation, but also re-
quires them to propose the minimum number of edits to
the MT output such that the post-edited MT output con-
veys the same meaning as the reference translation. Re-
quiring such heavy manual decision making greatly in-
creases the cost of evaluation, bottlenecking the evalua-
tion cycle.
To reduce the cost of evaluation, we aim to reduce any
human decisions in the evaluation cycle to be as simple
as possible, such that even untrained humans can quickly
complete the evaluation. The human decisions should
also be defined in a way that can be closely approximated
by automatic methods, so that similar objective functions
might potentially be used for tuning in MT system devel-
opment cycles.
Task based metrics (non-automatic) Voss and Tate
(2006) proposed a task-based approach to MT evaluation
that is in some ways similar in spirit to ours, but rather
than evaluating how well people understand the mean-
ing as a whole conveyed by a sentence translation, they
measured the recall with which humans can extract one of
the who, when, or where elements from MT output?and
without attaching them to any predicate or frame. A
large number of human subjects were instructed to extract
only one particular type of wh-item from each sentence.
They evaluated only whether the role fillers were cor-
rectly identified, without checking whether the roles were
appropriately attached to the correct predicate. Also, the
actor, experiencer, and patient were all conflated into the
undistinguished who role, while other crucial elements,
like the action, purpose, manner, were ignored.
Instead, we argue, evaluating meaning similarity
should be done by evaluating the semantic structure as
a whole: (a) all core semantic roles should be checked,
and (b) not only should we evaluate the presence of se-
mantic role fillers in isolation, but also their relations to
the frames? predicates.
Syntax based metrics Unlike Voss and Tate, Liu and
Gildea (2005) proposed a structural approach, but it was
based on syntactic rather than semantic structure, and fo-
cused on checking the correctness of the role structure
without checking the correctness of the role fillers. Their
subtree metric (STM) and headword chain metric (HWC)
address the failure of BLEU to evaluate translation gram-
maticality; however, the problem remains that a gram-
matical translation can achieve a high syntax-based score
even if contains meaning errors arising from confusion of
semantic roles.
STM was the first proposed metric to incorporate syn-
tactic features in MT evaluation, and STM underlies most
other recently proposed syntactic MT evaluation met-
rics, for example the evaluation metric based on lexical-
functional grammar of Owczarzak et al (2008). STM is
a precision-based metric that measures what fraction of
subtree structures are shared between the parse trees of221
machine translations and reference translations (averag-
ing over subtrees up to some depth threshold). Unlike
Voss and Tate, however, STM does not check whether the
role fillers are correctly translated.
HWC is similar, but is based on dependency trees con-
taining lexical as well as syntactic information. HWC
measures what fraction of headword chains (a sequence
of words corresponding to a path in the dependency tree)
also appear in the reference dependency tree. This can be
seen as a similarity measure on n-grams of dependency
chains. Note that the HWC?s notion of lexical similarity
still requires exact word match.
Although STM-like syntax-based metrics are an im-
provement over flat lexical similarity metrics like BLEU,
they are still more fluency-oriented than adequacy-
oriented. Similarity of syntactic rather than semantic
structure still inadequately reflects meaning preservation.
Moreover, properly measuring translation utility requires
verifying whether role fillers have been correctly trans-
lated?verifying only the abstract structures fails to pe-
nalize when role fillers are confused.
Semantic roles as features in aggregate metrics
Gime?nez and Ma`rquez (2007, 2008) introduced ULC, an
automatic MT evaluation metric that aggregates many
types of features, including several shallow semantic sim-
ilarity features: semantic role overlapping, semantic role
matching, and semantic structure overlapping. Unlike Liu
and Gildea (2007) who use discriminative training to tune
the weight on each feature, ULC uses uniform weights.
Although the metric shows an improved correlation with
human judgment of translation quality (Callison-Burch et
al., 2007; Gime?nez and Ma`rquez, 2007; Callison-Burch
et al, 2008; Gime?nez and Ma`rquez, 2008), it is not com-
monly used in large-scale MT evaluation campaigns, per-
haps due to its high time cost and/or the difficulty of in-
terpreting its score because of its highly complex combi-
nation of many heterogenous types of features.
Specifically, note that the feature based representations
of semantic roles used in these aggregate metrics do not
actually capture the structural predicate-argument rela-
tions. ?Semantic structure overlapping? can be seen as
the shallow semantic version of STM: it only measures
the similarity of the tree structure of the semantic roles,
without considering the lexical realization. ?Semantic
role overlapping? calculates the degree of lexical overlap
between semantic roles of the same type in the machine
translation and its reference translation, using simple bag-
of-words counting; this is then aggregated into an average
over all semantic role types. ?Semantic role matching?
is just like ?semantic role overlapping?, except that bag-
of-words degree of similarity is replaced (rather harshly)
by a boolean indicating whether the role fillers are an ex-
act string match. It is important to note that ?semantic
role overlapping? and ?semantic role matching? both use
flat feature based representations which do not capture the
structural relations in semantic frames, i.e., the predicate-
argument relations.
Like system combination approaches, ULC is a vastly
more complex aggregate metric compared to widely used
metrics like BLEU or STM. We believe it is important
to retain a focus on developing simpler metrics which
not only correlate well with human adequacy judgments,
but nevertheless still directly provide representational
transparency via simple, clear, and transparent scoring
schemes that are (a) easily human readable to support er-
ror analysis, and (b) potentially directly usable for auto-
matic credit/blame assignment in tuning tree-structured
SMT systems. We also believe that to provide a foun-
dation for better design of efficient automated metrics,
making use of humans for annotating semantic roles and
judging the role translation accuracy in MT output is an
essential step that should not be bypassed, in order to ade-
quately understand the upper bounds of such techniques.
We agree with Przybocki et al (2010), who observe
in the NIST MetricsMaTr 2008 report that ?human [ade-
quacy] assessments only pertain to the translations evalu-
ated, and are of no use even to updated translations from
the same systems?. Instead, we aim for MT evaluation
metrics that provide fine-grained scores in a way that also
directly reflects interpretable insights on the strengths and
weaknesses of MT systems rather than simply replicating
human assessments.
3 MEANT: SRL for MT evaluation
A good translation is one from which human readers
may successfully understand at least the basic event struc-
ture??who did what to whom, when, where and why?
(Pradhan et al, 2004)?which represents the most essen-
tial meaning of the source utterances.
MEANT measures this as follows. First, semantic role
labeling is performed (either manually or automatically)
on both the reference translation and the machine transla-
tion. The semantic frame structures thus obtained for the
MT output are compared to those in the reference transla-
tions, frame by frame, argument by argument. The frame
translation accuracy is a weighted sum of the number of
correctly translated arguments. Conceptually, MEANT
is defined in terms of f-score, with respect to the preci-
sion/recall for sentence translation accuracy as calculated
by averaging the translation accuracy for all frames in the
MT output across the number of frames in the MT out-
put/reference translations. Details are given below.
3.1 Annotating semantic frames
In designing a semantic MT evaluation metric, one im-
portant issue that should be addressed is how to evaluate
the similarity of meaning objectively and systematically222
Figure 1: Example of source sentence and reference translation with reconstructed semantic frames in Propbank format and MT
output with reconstructed semantic frames by minimal trained human annotators. Following Propbank, there are no semantic frames
for MT3 because there is no predicate.
using fine-grained measures. We adopted the Propbank
SRL style predicate-argument framework, which captures
the basic event structure in a sentence in a way that clearly
indicates many strengths and weaknesses of MT. Figure 1
shows the reference translationwith reconstructed seman-
tic frames in Propbank format and the corresponding MT
output with reconstructed semantic frames by minimal
trained human annotators.
3.2 Comparing semantic frames
After annotating the semantic frames, we must deter-
mine the translation accuracy for each semantic role filler
in the reference and machine translations. Although ulti-
mately it would be nice to do this automatically, it is es-
sential to first understand extremely well the upper bound
of accuracy for MT evaluation via semantic frame theory.
Thus, instead of resorting to excessively permissive bag-
of-words matching or excessively restrictive exact string
matching, for the experiments reported here we employed
a group of human judges to evaluate the correctness of
each role filler translation between the reference and ma-
chine translations.
In order to facilitate a finer-grained measurement of
utility, the human judges were not only allowed to mark
each role filler translation as ?correct? or ?incorrect?, but
also ?partial?. Translations of role fillers are judged ?cor-
rect? if they express the same meaning as that of the refer-
ence translations (or the original source input, in the bilin-
guals experiment discussed later). Translations may also
be judged ?partial? if only part of the meaning is correctly
translated. Extra meaning in a role filler is not penalized
unless it belongs in another role. We also assume that a
wrongly translated predicate means that the entire seman-
tic frame is incorrect; therefore, the ?correct? and ?par-
tial? argument counts are collected only if their associated
predicate is correctly translated in the first place.
Table 1 shows an example of SRL annotation of MT1
in Figure 1 by one of the annotators, along with the human
judgment on translation accuracy of each argument. The
predicate ceased in the reference translation did not match
with any predicate annotated in MT1, while the predicate
resumed matched with the predicate resume annotated in
MT1. All arguments of the untranslated ceased are auto-
matically considered incorrect (with no need to consider
each argument individually), under our assumption that a
wrongly translated predicate causes the entire event frame
to be considered mistranslated. The ARGM-TMP argu-
ment, Until after their sales had ceased in mainland China for
almost two months, in the reference translation is partially
translated to ARGM-TMP argument, So far , nearly two
months, inMT1. Similar decisions are made for the ARG1
argument and the other ARGM-TMP argument; now in
the reference translation is missing in MT1.
3.3 Quantifying semantic frame match
To quantify the above in a summary metric, we define
MEANT in terms of an f-score that balances the precision
and recall analysis of the comparative matrices collected
from the human judges, as follows.
Ci,j = # correct fillers of ARG j for PRED i in MT
Pi,j = # partial fillers of ARG j for PRED i in MT
Mi,j = total # fillers of ARG j for PRED i in MT
Ri,j = total # fillers of ARG j of PRED i in REF223
Table 1: SRL annotation of MT1 in Figure 1 and the human judgment of translation accuracy for each argument (see text).
SRL REF MT1 Decision
PRED (Action) ceased ? no match
PRED (Action) resumed resume match
ARG0 (Agent) ? sk - ii the sale of products in
the mainland of China
incorrect
ARG1 (Experiencer) sales of complete range of SK - II products sales partial
ARGM-TMP (Temporal) Until after , their sales had ceased in mainland
China for almost two months
So far , nearly two months partial
ARGM-TMP (Temporal) now ? incorrect
Cprecision =
?
matched i
wpred +
?
j wjCi,j
wpred +
?
j wjMi,j
Crecall =
?
matched i
wpred +
?
j wjCi,j
wpred +
?
j wjRi,j
Pprecision =
?
matched i
?
j wjPi,j
wpred +
?
j wjMi,j
Precall =
?
matched i
?
j wjPi,j
wpred +
?
j wjRi,j
precision = Cprecision + (wpartial ? Pprecision)total # predicates in MT
recall = Crecall + (wpartial ? Precall)total # predicates in REF
f-score = 2 ? precision ? recallprecision+ recall
Cprecision, Pprecision, Crecall and Precall are the sum of the
fractional counts of correctly or partially translated se-
mantic frames in theMT output and the reference, respec-
tively, which can be viewed as the true positive for pre-
cision and recall of the whole semantic structure in one
source utterence. Therefore, the SRL based MT evalua-
tion metric is equivalent to the f-score, i.e., the translation
accuracy for the whole predicate-argument structure.
Note that wpred, wj and wpartial are the weights for the
matched predicate, arguments of type j, and partial trans-
lations. These weights can be viewed as the importance
of meaning preservation for each different category of se-
mantic roles, and the penalty for partial translations. We
will describe below how these weights are estimated.
If all the reconstructed semantic frames in the MT out-
put are completely identical to those annotated in the ref-
erence translation, and all the arguments in the recon-
structed frames express the same meaning as the corre-
sponding arguments in the reference translations, then the
f-score will be equal to 1.
For instance, consider MT1 in Figure 1. The number
of frames in MT1 and the reference translation are 1 and
2, respectively. The total number of participants (includ-
ing both predicates and arguments) of the resume frame
in both MT1 and the reference translation is 4 (one pred-
icate and three arguments), with 2 of the arguments (one
ARG1/experiencer and one ARGM-TMP/temporal) only
partially translated. Assuming for now that the metric ag-
gregates ten types of semantic roles with uniform weight
for each role (optimization of weights will be discussed
later), then wpred = wj = 0.1, and so Cprecision and Crecall
are both zero while Pprecision and Precallare both 0.5. If we
further assume thatwpartial = 0.5, then precison and recall
are 0.25 and 0.125 respectively. Thus the f-score for this
example is 0.17.
Both human and semi-automatic variants of the
MEANT translation evaluation metric were meta-
evaluated, as described next.
4 Meta-evaluation methodology
4.1 Evaluation Corpus
We leverage work from Phase 2.5 of the DARPA
GALE program in which both a subset of the Chinese
source sentences, as well as their English reference, are
being annotated with semantic role labels in Propbank
style. The corpus also includes three participating state-
of-the-art MT systems? output. For present purposes, we
randomly drew 40 sentences from the newswire genre of
the corpus to form a meta-evaluation corpus. To maintain
a controlled environment for experiments and consistent
comparison, the evaluation corpus is fixed throughout this
work.
4.2 Correlation with human judgements on
adequacy
We followed the benchmark assessment procedure in
WMT and NIST MetricsMaTr (Callison-Burch et al,
2008, 2010), assessing the performance of the proposed
evaluation metric at the sentence level using ranking pref-
erence consistency, which also known as Kendall?s ? rank
correlation coefficient, to evaluate the correlation of the
proposed metric with human judgments on translation ad-
equacy ranking. A higher value for ? indicates more simi-
larity to the ranking by the evaluation metric to the human
judgment. The range of possible values of correlation co-
efficient is [-1,1], where 1 means the systems are ranked224
Table 2: List of semantic roles that human judges are requested
to label.
Label Event Label Event
Agent who Location where
Action did Purpose why
Experiencer what Manner how
Patient whom Degree or Extent how
Temporal when Other adverbial arg. how
in the same order as the human judgment and -1 means
the systems are ranked in the reverse order as the human
judgment.
5 Experiment: Using human SRL
The first experiment aims to provide a more concrete
understanding of one of the key questions as to the upper
bounds of the proposed evaluation metric: how well can
human annotators perform in reconstructing the semantic
frames in MT output? This is important since MT out-
put is still not close to perfectly grammatical for a good
syntactic parsing?applying automatic shallow semantic
parsers, which are trained on grammatical input and valid
syntactic parse trees, on MT output may significantly un-
derestimate translation utility.
5.1 Experimental setup
We thus introduce HMEANT, a variant of MEANT
based on the idea that semantic role labeling can be sim-
plified into a task that is easy and fast even for untrained
humans. The human annotators are given only very sim-
ple instructions of less than half a page, along with two
examples. Table 2 shows the list of labels annotators are
requested to annotate, where the semantic role labeling
instructions are given in the intuitive terms of ?who did
what to whom, when, where, why and how?. To facili-
tate the inter-annotator agreement experiments discussed
later, each sentence is independently assigned to at least
two annotators.
After calculating the SRL scores based on the confu-
sion matrix collected from the annotation and evaluation,
we estimate the weights using grid search to optimize cor-
relation with human adequacy judgments.
5.2 Results: Correlation with human judgement
Table 3 shows results indicating that HMEANT corre-
lates with human judgment on adequacy as well as HTER
does (0.432), and is far superior to BLEU (0.198) or other
surface-oriented metrics.
Inspection of the cross validation results shown in Ta-
ble 4 indicates that the estimated weights are not over-
fitting. Recall that the weights used in HMEANT are
globally estimated (by grid search) using the evaluation
Table 3: Sentence-level correlation with human adequacy judg-
ments, across the evaluation metrics.
Metrics Kendall ?
HMEANT 0.4324
HTER 0.4324
NIST 0.2883
BLEU 0.1982
METEOR 0.1982
TER 0.1982
PER 0.1982
CDER 0.1171
WER 0.0991
Table 4: Analysis of stability for HMEANT?s weight settings,
withRHMEANT rank and Kendall?s ? correlation scores (see text).
Fold 0 Fold 1 Fold 2 Fold 3
RHMEANT 3 1 3 5
distinct R 16 29 19 17
?HMEANT 0.33 0.48 0.48 0.40
?HTER 0.59 0.41 0.44 0.30
?CV train 0.45 0.42 0.40 0.43
?CV test 0.33 0.37 0.48 0.40
corpus. To analyze stability, the corpus is also parti-
tioned randomly into four folds of equal size. For each
fold, another grid search is also run. RHMEANT is the
rank at which the Kendall?s correlation for HMEANT
is found, if the Kendall?s correlations for all points in
the grid search space are sorted. Many similar weight-
vectors produce the same Kendall?s correlation score, so
?distinct R? shows how many distinct Kendall?s corre-
lation scores exist in each case?between 16 and 29.
HMEANT?s weight settings always produce Kendall?s
correlation scores among the top 5, regardless of which
fold is chosen, indicating good stability of HMEANT?s
weight-vector.
Next, Kendall?s ? correlation scores are shown for
HMEANT on each fold. They vary from 0.33 to 0.48,
and are at least as stable as those shown for HTER, where
? varies from 0.30 to 0.59.
Finally, ?CV shows Kendall?s correlations if the weight-
vector is instead subjected to full cross-validation training
and testing, again demonstrating good stability. In fact,
the correlations for the training set in three of the folds (0,
2, and 3) are identical to those for HMEANT.
5.3 Results: Cost of evaluating
The time needed for training non-expert humans to
carry out our annotation protocol is significantly less than
HTER and gold standard Propbank annotation. The half-
page instructions given to annotators required only be-
tween 5 to 15 minutes for all annotators, including time225
for asking questions if necessary. Aside from providing
two annotated examples, no further training was given.
Similarly, the time needed for running the evaluation
metric is also significantly less than HTER?under at
most 5 minutes per sentence, even for non-expert humans
using no computer-assisted UI tools. The average time
used for annotating each sentence was lower bounded by
2 minutes and upper bounded by 3 minutes, and the time
used for determing the translation accuracy of role fillers
averaged under 2 minutes.
Note that these figures are for unskilled non-experts.
These times tend to diminish significantly after annotators
acquire experience.
6 Experiment: Monolinguals vs. bilinguals
We now show that using monolingual annotators is es-
sentially just as effective as using more expensive bilin-
gual annotators. We study the cost/benefit trade-off of
using human annotators from different language back-
grounds for the proposed evaluation metric, and compare
whether providing the original source text helps. Note
that this experiment focuses on the SRL annotation step,
rather than the judgments of role filler paraphrasing accu-
racy, because the latter is only a simple three-way deci-
sion between ?correct?, ?partial?, and ?incorrect? that is
far less sensitive to the annotators? language backgrounds.
MT output is typically poor. Therefore, readers of
MT output often guess the original meaning in the source
input using their own language background knowledge.
Readers? language background thus affects their under-
standing of the translation, which could affect the accu-
racy of capturing the key semantic roles in the translation.
6.1 Experimental Setup
Both English monolinguals and Chinese-English bilin-
guals (Chinese as first language and English as second
language) were employed to annotate the semantic roles.
For bilinguals, we also experimented with the difference
in guessing constraints by optionally providing the origi-
nal source input together with the translation. Therefore,
there are three variations in the experiment setup: mono-
linguals seeing translation output only; bilinguals seeing
translation output only; and bilinguals seeing both input
and output.
The aim here is to do a rough sanity check on the effect
of the variation of language background of the annotators;
thus for these experiments we have not run the weight es-
timation step after SRL based f-score calculation. Instead,
we simply assigned a uniform weight to all the seman-
tic elements, and evaluated the variation under the same
weight settings. (The correlation scores reported in this
section are thus expected to be lower than that reported in
the last section.)
Table 5: Sentence-level correlation with human adequacy judg-
ments, for monolinguals vs. bilinguals. Uniform rather than op-
timized weights are used.
Metrics Kendall ?
HMEANT - bilinguals 0.3514
HMEANT - monolinguals 0.3153
HMEANT - bilinguals with input 0.3153
6.2 Results
Table 5 of our results shows that using more expen-
sive bilinguals for SRL annotation instead of monolin-
guals improves the correlation only slightly. The cor-
relation coefficient of the SRL based evaluation metric
driven by bilingual human annotators (0.351) is slightly
better than that driven by monolingual human annotators
(0.315); however, using bilinguals in the evaluation pro-
cess is more costly than using monolinguals.
The results show that even allowing the bilinguals to
see the input as well as the translation output for SRL
annotation does not help the correlation. The correlation
coefficient of the SRL based evaluation metric driven by
bilingual human annotators who see also the source in-
put sentences is 0.315 which is the same as that driven by
monolingual human annotators. We find that the correla-
tion coefficient of the proposed with human judgment on
adequacy drops when bilinguals are shown to the source
input sentence during annotation. Error analyses lead
us to believe that annotators will drop some parts of the
meaning in the translations when trying to align them to
the source input.
This suggests that HMEANT requires only monolin-
gual English annotators, who can be employed at low
cost.
7 Inter-annotator agreement
One of the concerns of the proposed metric is that,
given only minimal training on the task, humans would
annotate the semantic roles so inconsistently as to reduce
the reliability of the evaluation metric. Inter-annotator
agreement (IAA) measures the consistency of human in
performing the annotation task. A high IAA suggests that
the annotation is consistent and the evaluation results are
reliable and reproducible.
To obtain a clear analysis on where any inconsistency
might lie, we measured IAA in two steps: role identifica-
tion and role classification.
7.1 Experimental setup
Role identification Since annotators are not consistent
in handling articles or punctuation at the beginning or
the end of the annotated arguments, the agreement of se-
mantic role identification is counted over the matching of226
Table 6: Inter-annotator agreement rate on role identification
(matching of word span)
Experiments REF MT
bilinguals working on output only 76% 72%
monolinguals working on output only 93% 75%
bilinguals working on input-output 75% 73%
Table 7: Inter-annotator agreement rate on role classification
(matching of role label associated with matched word span)
Experiments Ref MT
bilinguals working on output only 69% 65%
monolinguals working on output only 88% 70%
bilinguals working on input-output 70% 69%
word span in the annotated role fillers with a tolerance
of ?1 word in mismatch. The inter-annotator agreement
rate (IAA) on the role identification task is calculated as
follows. A1 andA2 denote the number of annotated pred-
icates and arguments by annotator 1 and annotator 2 re-
spectively. Mspan denotes the number of annotated pred-
icates and arguments with matching word span between
annotators.
Pidentification =
Mspan
A1
Ridentification =
Mspan
A2
IAAidentification =
2 ? Pidentification ?Ridentification
Pidentification +Ridentification
Role classification The agreement of classified roles
is counted over the matching of the semantic role labels
within two aligned word spans. The IAA on the role clas-
sification task is calculated as follows. Mlabel denotes
the number of annotated predicates and arguments with
matching role label between annotators.
Pclassification =
Mlabel
A1
Rclassification =
Mlabel
A2
IAAclassification =
2 ? Pclassification ?Rclassification
Pclassification +Rclassification
7.2 Results
The high inter-annotator agreement suggests that the
annotation instructions provided to the annotators are in
general sufficient and the evaluation is repeatable and
could be automated in the future. Table 6 and 7 show the
annotators reconstructed the semantic frames quite con-
sistently, even they were given only simple and minimal
training.
We have noticed that the agreement on role identifica-
tion is higher than that on role classification. This sug-
gests that there are role confusion errors among the an-
notators. We expect a slightly more detailed instructions
and explanations on different roles will further improve
the IAA on role classification.
The results also show that monolinguals seeing output
only have the highest IAA in semantic frame reconstruc-
tion. Data analyses lead us to believe the monolinguals
are the most constrained group in the experiments. The
monolingual annotators can only guess the meaning in
the MT output using their English language knowledge.
Therefore, they all understand the translation almost the
same way, even if the translation is incorrect.
On the other hand, bilinguals seeing both the input and
output discover the mistranslated portions, and often un-
consciously try to compensate by re-interpreting the MT
output with information not necessarily appearing in the
translation, in order to better annotate what they think
it should have conveyed. Since there are many degrees
of freedom in this sort of compensatory re-interpretation,
this group achieved a lower IAA than the monolinguals.
Bilinguals seeing only output appear to take this even a
step further: confrontedwith a poor translation, they often
unconsciously try to guess what the original input might
have been. Consequently, they agree the least, because
they have the most freedom in applying their own knowl-
edge of the unseen input language, when compensating
for poor translations.
8 Experiment: Using automatic SRL
In the previous experiment, we showed that the pro-
posed evaluation metric driven by human semantic role
annotators performed as well as HTER. It is now worth
asking a deeper question: can we further reduce the la-
bor cost of MEANT by using automatic shallow semantic
parsing instead of humans for semantic role labeling?
Note that this experiment focuses on understanding the
cost/benefit trade-off for the semantic frame reconstruc-
tion step. For SRL annotation, we replace humans with
automatic shallow semantic parsing. We decouple this
from the ternary judgments of role filler accuracy, which
are still made by humans. However, we believe the eval-
uation of role filler accuracy will also be automatable.
8.1 Experimental setup
We performed three variations of the experiments to
assess the performance degradation from the automatic
approximation of semantic frame reconstruction in each
translation (reference translation and MT output): we ap-
plied automatic shallow semantic parsing on the MT out-
put only; on the reference translation only; and on both
reference translation and MT output. For the semantic227
Table 8: Sentence-level correlation with human adequacy judg-
ments. *The weights for individual roles in the metric are tuned
by optimizing the correlation.
Metrics Kendall ?
HTER 0.4324
HMEANT gold - monolinguals * 0.4324
HMEANT auto - monolinguals * 0.3964
MEANT gold - auto * 0.3694
MEANT auto - auto * 0.3423
NIST 0.2883
BLEU / METEOR / TER / PER 0.1982
CDER 0.1171
WER 0.0991
parser, we used ASSERT (Pradhan et al, 2004) which
achieves roughly 87% semantic role labeling accuracy.
8.2 Results
Table 8 shows that the proposed SRL based evaluation
metric correlates slightly worse than HTER with a much
lower labor cost. The correlation with human judgment
on adequacy of the fully automated SRL annotation ver-
sion, i.e., applying ASSERT on both the reference transla-
tion and the MT output, of the SRL based evaluation met-
ric is about 80% of that of HTER. The results also show
that the correlation with human judgment on adequacy of
either one side of translation using automatic SRL is in
the 85% to 95% range of that HTER.
9 Conclusion
We have presented MEANT, a novel semantic MT
evaluation metric that assesses the translation accuracy
via Propbank-style semantic predicates, roles, and fillers.
MEANT provides an intuitive picture on how much in-
formation is correctly translated in the MT output.
MEANT can be run using inexpensive untrainedmono-
linguals and yet correlates with human judgments on ad-
equacy as well as HTER with a lower labor cost. In con-
trast to HTER, which requires rigorous training of human
experts to find aminimum edit of the translation (an expo-
nentially large search space), MEANT requires untrained
humans to make well-defined, bounded decisions on an-
notating semantic roles and judging translation correct-
ness. The process by which MEANT reconstructs the se-
mantic frames in a translation and then judges translation
correctness of the role fillers conceptually models how
humans read and understand translation output.
We also showed that using automatic shallow seman-
tic parser to further reduce the labor cost of the pro-
posed metric successfully approximates roughly 80% of
the correlation with human judgment on adequacy. The
results suggest future potential for a fully automatic vari-
ant of MEANT that could out-perform current automatic
MT evaluation metrics and still perform near the level of
HTER.
Numerous intriguing questions arise from this work. A
further investigation into the correlation of each of the in-
dividual roles to human adequacy judgments is detailed
elsewhere, along with additional improvements to the
MEANT family of metrics (Lo and Wu, 2011). Another
interesting investigation would then be to similarly repli-
cate this analysis of the impact of each individual role, but
using automatically rather than manually labeled seman-
tic roles, in order to ascertain whether the more difficult
semantic roles for automatic semantic parsers might also
correspond to the less important aspects of end-to-endMT
utility.
Acknowledgments
This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under GALE Contract Nos. HR0011-06-
C-0022 and HR0011-06-C-0023 and by the Hong
Kong Research Grants Council (RGC) research
grants GRF621008, GRF612806, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the views of the Defense Advanced
Research Projects Agency.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An Au-
tomatic Metric for MT Evaluation with Improved Cor-
relation with Human Judgments. In Proceedings of the
43th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in Machine Transla-
tion Research. In Proceedings of the 13th Conference
of the European Chapter of the Association for Compu-
tational Linguistics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. (Meta-) evalua-
tion of Machine Translation. In Proceedings of the 2nd
Workshop on Statistical Machine Translation, pages
136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. Further Meta-
evaluation of Machine Translation. In Proceedings of
the 3rd Workshop on Statistical Machine Translation,
pages 70?106, 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Pryzbocki, and Omar Zaidan.228
Findings of the 2010 Joint Workshop on Statistical Ma-
chine Translation andMetrics for Machine Translation.
In Proceedings of the Joint 5th Workshop on Statistical
Machine Translation and MetricsMATR, pages 17?53,
Uppsala, Sweden, 15-16 July 2010.
G. Doddington. Automatic Evaluation of Machine Trans-
lation Quality using N-gram Co-occurrence Statistics.
In Proceedings of the 2nd International Conference
on Human Language Technology Research (HLT-02),
pages 138?145, San Francisco, CA, USA, 2002. Mor-
gan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu?is Ma`rquez. Linguistic Features
for Automatic Evaluation of Heterogenous MT Sys-
tems. In Proceedings of the 2nd Workshop on Sta-
tistical Machine Translation, pages 256?264, Prague,
Czech Republic, June 2007. Association for Computa-
tional Linguistics.
Jesu?s Gime?nez and Llu?is Ma`rquez. A Smorgasbord of
Features for Automatic MT Evaluation. In Proceed-
ings of the 3rd Workshop on Statistical Machine Trans-
lation, pages 195?198, Columbus, OH, June 2008. As-
sociation for Computational Linguistics.
Philipp Koehn and Christof Monz. Manual and Auto-
matic Evaluation of Machine Translation between Eu-
ropean Languages. In Proceedings of the Workshop on
Statistical Machine Translation, pages 102?121, 2006.
Gregor Leusch, Nicola Ueffing, andHermannNey. CDer:
Efficient MT Evaluation Using Block Movements. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), 2006.
Ding Liu and Daniel Gildea. Syntactic Features for Eval-
uation of Machine Translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, page 25, 2005.
Ding Liu and Daniel Gildea. Source-Language Fea-
tures and Maximum Correlation Training for Machine
Translation Evaluation. In Proceedings of the 2007
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (NAACL-07),
2007.
Chi-kiu Lo and Dekai Wu. Evaluating machine transla-
tion utility via semantic role labels. In Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2010), pages 2873?2877, Malta, May
2010.
Chi-kiu Lo and Dekai Wu. Semantic vs. syntactic vs.
n-gram structure for machine translation evaluation.
In Dekai Wu, editor, Proceedings of SSST-4, Fourth
Workshop on Syntax and Structure in Statistical Trans-
lation (at COLING 2010), pages 52?60, Beijing, Aug
2010.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
22nd International Joint Conference on Artificial In-
telligence (IJCAI-11), Barcelona, Jul 2011. To appear.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. A Evaluation Tool forMachine Translation:
Fast Evaluation forMTResearch. InProceedings of the
2nd International Conference on Language Resources
and Evaluation (LREC-2000), 2000.
Karolina Owczarzak, Josef van Genabith, and AndyWay.
Evaluating machine translation with LFG dependen-
cies. Machine Translation, 21:95?119, 2008.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: A Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th An-
nualMeeting of the Association for Computational Lin-
guistics (ACL-02), pages 311?318, 2002.
Sameer Pradhan, WayneWard, Kadri Hacioglu, James H.
Martin, and Dan Jurafsky. Shallow Semantic Parsing
Using Support Vector Machines. In Proceedings of
the 2004 Conference on Human Language Technology
and the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-04), 2004.
Mark Przybocki, Kay Peterson, Se?bastien Bronsart, and
Gregory Sanders. The NIST 2008Metrics for Machine
Translation Challenge - Overview, Methodology, Met-
rics, and Results. Machine Tr, 23:71?103, 2010.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A Study of Trans-
lation Edit Rate with Targeted Human Annotation. In
Proceedings of the 7th Conference of the Association
for Machine Translation in the Americas (AMTA-06),
pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Arkaitz Zubiaga, and Hassan Sawaf. Accelerated
DP Based Search For Statistical Translation. In Pro-
ceedings of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH-97),
1997.
Clare R. Voss and Calandra R. Tate. Task-based Evalua-
tion of Machine Translation (MT) Engines: Measuring
HowWell People ExtractWho,When,Where-Type El-
ements in MT Output. In Proceedings of the 11th An-
nual Conference of the European Association for Ma-
chine Translation (EAMT-2006), pages 203?212, Oslo,
Norway, June 2006.
229
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375?381,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving machine translation by training against
an automatic semantic frame based evaluation metric
Chi-kiu Lo and Karteek Addanki and Markus Saers and Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|vskaddanki|masaers|dekai}@cs.ust.hk
Abstract
We present the first ever results show-
ing that tuning a machine translation sys-
tem against a semantic frame based ob-
jective function, MEANT, produces more
robustly adequate translations than tun-
ing against BLEU or TER as measured
across commonly used metrics and human
subjective evaluation. Moreover, for in-
formal web forum data, human evalua-
tors preferredMEANT-tuned systems over
BLEU- or TER-tuned systems by a sig-
nificantly wider margin than that for for-
mal newswire?even though automatic se-
mantic parsing might be expected to fare
worse on informal language. We argue
that by preserving themeaning of the trans-
lations as captured by semantic frames
right in the training process, an MT sys-
tem is constrained to make more accu-
rate choices of both lexical and reorder-
ing rules. As a result, MT systems tuned
against semantic frame based MT evalu-
ation metrics produce output that is more
adequate. Tuning a machine translation
system against a semantic frame based ob-
jective function is independent of the trans-
lation model paradigm, so, any transla-
tion model can benefit from the semantic
knowledge incorporated to improve trans-
lation adequacy through our approach.
1 Introduction
We present the first ever results of tuning a statis-
tical machine translation (SMT) system against a
semantic frame based objective function in order
to produce a more adequate output. We compare
the performance of our system with that of two
baseline SMT systems tuned against BLEU and
TER, the commonly used n-gram and edit distance
based metrics. Our system performs better than
the baseline across seven commonly used evalu-
ation metrics and subjective human evaluation on
adequacy. Surprisingly, tuning against a seman-
tic MT evaluation metric also significantly out-
performs the baseline on the domain of informal
web forum data wherein automatic semantic pars-
ing might be expected to fare worse. These results
strongly indicate that using a semantic frame based
objective function for tuning would drive develop-
ment of MT towards direction of higher utility.
Glaring errors caused by semantic role confu-
sion that plague the state-of-the-art MT systems
are a consequence of using fast and cheap lexi-
cal n-gram based objective functions like BLEU
to drive their development. Despite enforcing flu-
ency it has been established that these metrics do
not enforce translation utility adequately and often
fail to preservemeaning closely (Callison-Burch et
al., 2006; Koehn and Monz, 2006).
We argue that instead of BLEU, a metric that fo-
cuses on getting the meaning right should be used
as an objective function for tuning SMT so as to
drive continuing progress towards higher utility.
MEANT (Lo et al, 2012), is an automatic seman-
tic MT evaluation metric that measures similarity
between the MT output and the reference transla-
tion via semantic frames. It correlates better with
human adequacy judgment than other automatic
MT evaluation metrics. Since a high MEANT
score is contingent on correct lexical choices as
well as syntactic and semantic structures, we be-
lieve that tuning against MEANT would improve
both translation adequacy and fluency.
Incorporating semantic structures into SMT by
tuning against a semantic frame based evaluation
metric is independent of the MT paradigm. There-
fore, systems from different MT paradigms (such
as hierarchical, phrase based, transduction gram-
mar based) can benefit from the semantic informa-
tion incorporated through our approach.
375
2 Related Work
Relatively little work has been done towards bi-
asing the translation decisions of an SMT system
to produce adequate translations that correctly pre-
servewho did what to whom, when, where and why
(Pradhan et al, 2004). This is because the devel-
opment of SMT systemswas predominantly driven
by tuning against n-gram based evaluation met-
rics such as BLEU or edit distance based metrics
such as TER which do not sufficiently bias SMT
system?s decisions to produce adequate transla-
tions. Although there has been a recent surge of
work aimed towards incorporating semantics into
the SMT pipeline, none attempt to tune against a
semantic objective function. Below, we describe
some of the attempts to incorporate semantic in-
formation into the SMT and present a brief survey
on evaluation metrics that focus on rewarding se-
mantically valid translations.
Utilizing semantics in SMT In the past few
years, there has been a surge of work aimed at in-
corporating semantics into various stages of the
SMT. Wu and Fung (2009) propose a two-pass
model that reorders the MT output to match the
SRL of the input, which is too late to affect the
translation decisions made by the MT system dur-
ing decoding. In contrast, training against a se-
mantic objective function attempts to improve the
decoding search strategy by incorporating a bias
towards meaningful translations into the model in-
stead of postprocessing its results.
Komachi et al (2006) and Wu et al (2011) pre-
process the input sentence to match the verb frame
alternations in the output side. Liu and Gildea
(2010) and Aziz et al (2011) use input side SRL
to train a tree-to-string SMT system. Xiong et al
(2012) trained a discriminative model to predict
the position of the semantic roles in the output.
All these approaches are orthogonal to the present
question of whether to train toward a semantic ob-
jective function. Any of the above models could
potentially benefit from tuning with semantic met-
rics.
MT evaluation metrics As mentioned previ-
ously, tuning against n-gram based metrics such
as BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie,
2005) does not sufficiently drive SMT into mak-
ing decisions to produce adequate translations
that correctly preserve ?who did what to whom,
when, where and why?. In fact, a number of
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) report cases
where BLEU strongly disagrees with human judg-
ments of translation accuracy. Tuning against edit
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al, 2000), and TER
(Snover et al, 2006) also fails to sufficiently bias
SMT systems towards producing translations that
preserve semantic information.
We argue that an SMT system tuned against an
adequacy-oriented metric that correlates well with
human adequacy judgement produces more ade-
quate translations. For this purpose, we choose
MEANT, an automatic semantic MT evaluation
metric that focuses on getting the meaning right by
comparing the semantic structures of the MT out-
put and the reference. We briefly describe some
of the alternative semantic metrics below to justify
our choice.
ULC (Gim?nez and M?rquez, 2007, 2008) is
an aggregated metric that incorporates several se-
mantic similarity features and shows improved
correlation with human judgement on translation
quality (Callison-Burch et al, 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al, 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an MT system against
ULC perhaps due to its expensive running time.
Lambert et al (2006) did tune on QUEEN, a sim-
plified version of ULC that discards the seman-
tic features and is based on pure lexical features.
Although tuning on QUEEN produced slightly
more preferable translations than solely tuning on
BLEU, themetric does not make use of any seman-
tic features and thus fails to exploit any potential
gains from tuning to semantic objectives.
Although TINE (Rios et al, 2011) is an recall-
oriented automatic evaluation metric which aims
to preserve the basic event structure, no work has
been done towards tuning an SMT system against
it. TINE performs comparably to BLEU andworse
than METEOR on correlation with human ade-
quacy judgment.
In contrast to TINE, MEANT (Lo et al, 2012),
which is the weighted f-score over the matched se-
mantic role labels of the automatically aligned se-
mantic frames and role fillers, outperforms BLEU,
NIST, METEOR, WER, CDER and TER. This
makes it more suitable for tuning SMT systems to
produce much adequate translations.
376
newswire BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 29.85 8.84 52.10 55.42 67.88 55.67 58.40 0.1667
TER-tuned 25.37 6.56 48.26 51.24 66.18 52.58 56.96 0.1578
MEANT-tuned 25.91 7.81 50.15 53.60 67.76 54.56 58.61 0.1676
Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
forum BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 9.58 4.10 31.77 34.63 80.09 64.54 76.12 0.1711
TER-tuned 6.94 2.21 28.55 30.85 76.15 57.96 74.73 0.1539
MEANT-tuned 7.92 3.11 30.40 33.08 77.32 61.01 74.64 0.1727
Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
3 Tuning SMT against MEANT
We now show that using MEANT as an objec-
tive function to drive minimum error rate training
(MERT) of state-of-the-art MT systems improves
MT utility not only on formal newswire text, but
even on informal forum text, where automatic se-
mantic parsing is difficult.
Toward improving translation utility of state-of-
the-art MT systems, we chose to use a strong and
competitive system in the DARPA BOLT program
as our baseline. The baseline system is a Moses
hierarchical model trained on a collection of LDC
newswire and a small portion of Chinese-English
parallel web forum data, together with a 5-gram
language model. For the newswire experiment, we
used a collection of NIST 02-06 test sets as our de-
velopment set and NIST 08 test set for evaluation.
The development and test sets contain 6,331 and
1,357 sentences respectively with four references.
For the forum data experiment, the development
and test sets were a held-out subset of the BOLT
phase 1 training data. The development and test
sets contain 2,000 sentences and 1,697 sentences
with one reference.
We use ZMERT (Zaidan, 2009) to tune the base-
line because it is a widely used, highly competi-
tive, robust, and reliable implementation of MERT
that is also fully configurable and extensible with
regard to incorporating new evaluation metrics. In
this experiment, we use aMEANT implementation
along the lines described in Lo et al (2012).
In each experiment, we tune two contrastive
conventional 100-best MERT tuned baseline sys-
tems on both newswire and forum data genres; one
tuned against BLEU, an n-gram based evaluation
metric and the other using TER, an edit distance
based metric. As semantic role labeling is expen-
sive we only tuned using 10-best list for MEANT-
tuned system. Tuning against BLEU and TER took
around 1.5 hours and 5 hours per iteration respec-
tively whereas tuning against MEANT took about
1.6 hours per iteration.
4 Results
Of course, tuning against any metric would maxi-
mize the performance of the SMT system on that
particular metric, but would be overfitting. For
example, something would be seriously wrong
if tuning against BLEU did not yield the best
BLEU scores. A far more worthwhile goal would
be to bias the SMT system to produce adequate
translations while achieving the best scores across
all the metrics. With this as our objective, we
present the results of comparing MEANT-tuned
systems against the baselines as evaluated on com-
monly used automatic metrics and human ade-
quacy judgement.
Cross-evaluation using automatic metrics Ta-
bles 1 and 2 show that MEANT-tuned systems
achieve the best scores across all other metrics in
both newswire and forum data genres, when avoid-
ing comparison of the overfit metrics too similar to
the one the system was tuned on (the cells shaded
in grey in the table: NIST and METEOR are n-
gram based metrics, similar to BLEU while WER
and CDER are edit distance based metrics, similar
to TER). In the newswire domain, however, our
system achieves marginally lower TER score than
BLEU-tuned system.
Figure 1 shows an example where the MEANT-
tuned system produced a more adequate transla-
tion that accurately preserves the semantic struc-
ture of the input sentence than the two baseline
systems. The MEANT scores for the MT output
from the BLEU-, TER- and MEANT-tuned sys-
tems are 0.0635, 0.1131 and 0.2426 respectively.
Both the MEANT score and the human evaluators
rank the MT output from the MEANT-tuned sys-
377
Figure 1: Examples of machine translation output and the corresponding semantic parses from the [B]
BLEU-, [T] TER-and [M]MEANT-tuned systems together with [IN] the input sentence and [REF] the
reference translation. Note that the MT output of the BLEU-tuned system has no semantic parse output
by the automatic shallow semantic parser.
tem as the most adequate translation. In this exam-
ple, the MEANT-tuned system has translated the
two predicates ???? and ???? in the input sen-
tence into the correct form of the predicates ?at-
tack? and ?adopted? in theMT output, whereas the
BLEU-tuned system has translated both of them
incorrectly (translates the predicates into nouns)
and the TER-tuned system has correctly translated
only the first predicate (into ?seized?) and dropped
the second predicate. Moreover, for the frame ??
?? in the input sentence, the MEANT-tuned sys-
tem has correctly translated the ARG0 ??????
??? into ?Hamas militants? and the ARG1 ??
???? into ?Gaza?. However, the TER-tuned
system has dropped the predicate ???? so that
the corresponding arguments ?The Palestinian Au-
thority? and ?into a state of emergency? have all
been incorrectly associated with the predicate ??
? /seized?. This example shows that the transla-
tion adequacy of SMT has been improved by tun-
ing against MEANT because the MEANT-tuned
system is more accurately preserving the semantic
structure of the input sentence.
Our results show that MEANT-tuned system
maintains a balance between lexical choices and
word order because it performs well on n-gram
based metrics that reward lexical matching and
edit distance metrics that penalize incorrect word
order. This is not surprising as a high MEANT
score relies on a high degree of semantic structure
matching, which is contingent upon correct lexi-
cal choices as well as syntactic and semantic struc-
tures.
Human subjective evaluation In line with our
original objective of biasing SMT systems towards
producing adequate translations, we conduct a hu-
man evaluation to judge the translation utility of
the outputs produced by MEANT-, BLEU- and
TER-tuned systems. Following the manual eval-
uation protocol of Lambert et al (2006), we ran-
domly draw 150 sentences from the test set in each
domain to form the manual evaluation set. Table
3 shows the MEANT scores of the two manual
evaluation sets. In both evaluation sets, like in the
test sets, the output from the MEANT-tuned sys-
tem score slightly higher inMEANT than that from
the BLEU-tuned system and significantly higher
than that from the TER-tuned system. The output
of each tuned MT system along the input sentence
and the reference were presented to human evalu-
ators. Each evaluation set is ranked by two evalu-
ators for measuring inter-evaluator agreement.
Table 4 indicates that output of the MEANT-
tuned system is ranked adequate more frequently
compared to BLEU- and TER-tuned baselines for
both newswire and web forum genres. The inter-
378
newswire forum
BLEU-tuned 0.1564 0.1663
TER-tuned 0.1203 0.1453
MEANT-tuned 0.1633 0.1737
Table 3: MEANT scores of each system in the 150-
sentence manual evaluation set.
newswire forum
Eval 1 Eval 2 Eval 1 Eval 2
BLEU-tuned (B) 37 42 47 42
TER-tuned (T) 22 24 28 23
MEANT-tuned (M) 55 56 59 68
B=T 14 12 0 0
M=B 5 4 8 9
M=T 4 4 4 4
M=B=T 13 9 4 4
Table 4: No. of sentences ranked the most ade-
quate by human evaluators for each system.
H1 newswire forum
MEANT-tuned > BLEU-tuned 80% 95%
MEANT-tuned > TER-tuned 99% 99%
Table 5: Significance level of accepting the alter-
native hypothesis.
evaluator agreement is 84% and 70% for newswire
and forum data genres respectively.
We performed the right-tailed two proportion
significance test on human evaluation of the SMT
system outputs for both the genres. Table 5 shows
that the MEANT-tuned system generates more ad-
equate translations than the TER-tuned system at
the 99% significance level for both newswire and
web forum genres. The MEANT-tuned system is
ranked more adequate than the BLEU-tuned sys-
tem at the 95% significance level on the web fo-
rum genre and for the newswire genre the hypoth-
esis is accepted at a significance level of 80%.
The high inter-evaluator agreement and the signif-
icance tests confirm that MEANT-tuned system is
better at producing adequate translations compared
to BLEU- or TER-tuned systems.
Informal vs. formal text The results of table
4 and 5 also show that?surprisingly?the human
evaluators preferred MEANT-tuned system out-
put over BLEU-tuned and TER-tuned system out-
put by a far wider margin on the informal forum
text compared to the formal newswire text. The
MEANT-tuned system is better than both base-
lines at the 80% significance level for the formal
text genre. For the informal text genre, it per-
forms the two baselines at the 95% significance
level. Although one might expect an semantic
frame dependent metric such as MEANT to per-
form poorly on the domain of informal text, sur-
prisingly, it nonetheless significantly outperforms
the baselines at the task of generating adequate out-
put. This indicates that the design of the MEANT
evaluation metric is robust enough to tune an SMT
system towards adequate output on informal text
domains despite the shortcomings of automatic
shallow semantic parsing.
5 Conclusion
We presented the first ever results to demon-
strate that tuning an SMT system against MEANT
produces much adequate translation than tuning
against BLEU or TER, as measured across all
other commonly used metrics and human subjec-
tive evaluation. We also observed that tuning
against MEANT succeeds in producing adequate
output significantly more frequently even on the
informal text such as web forum data. By pre-
serving the meaning of the translations as captured
by semantic frames right in the training process,
an MT system is constrained to make more accu-
rate choices of both lexical and reordering rules.
The performance of our system as measured across
all commonly used metrics indicate that tuning
against a semantic MT evaluation metric does pro-
duce output which is adequate and fluent.
We believe that tuning onMEANTwould prove
equally useful for MT systems based on any
paradigm, especially where the model does not
incorporate semantic information to improve the
adequacy of the translations produced and using
MEANT as an objective function to tune SMT
would drive sustainable development of MT to-
wards the direction of higher utility.
Acknowledgment
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
ment no. 287658; and by the Hong Kong
Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
379
References
Wilker Aziz, Miguel Rios, and Lucia Specia. Shal-
low semantic trees for SMT. In Proceedings
of the Sixth Workshop on Statistical Machine
Translation (WMT2011), 2011.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?
72, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational Linguis-
tics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation. In
Proceedings of the 2nd Workshop on Statistical
Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Transla-
tion. In Proceedings of the 3rd Workshop on
Statistical Machine Translation, pages 70?106,
2008.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the
2nd International Conference on Human Lan-
guage Technology Research, pages 138?145,
San Diego, California, 2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic
features for automatic evaluation of heteroge-
nous MT systems. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 256?264, Prague, Czech Republic,
June 2007.
Jes?s Gim?nez and Llu?sM?rquez. A smorgasbord
of features for automaticMT evaluation. In Pro-
ceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198, Colum-
bus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation
between European Languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation (WMT-06), pages 102?121, 2006.
Mamoru Komachi, Yuji Matsumoto, and Masaaki
Nagata. Phrase reordering for statistical ma-
chine translation based on predicate-argument
structure. In Proceedings of the 3rd Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2006), 2006.
Patrik Lambert, Jes?s Gim?nez, Marta R Costa-
juss?, Enrique Amig?, Rafael E Banchs, Llu?s
M?rquez, and JAR Fonollosa. Machine Transla-
tion system development based on human like-
ness. In Spoken Language Technology Work-
shop, 2006. IEEE, pages 246?249. IEEE, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT Evaluation Using
Block Movements. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
06), 2006.
Ding Liu and Daniel Gildea. Semantic role fea-
tures for machine translation. In Proceedings of
the 23rd international conference on Computa-
tional Linguistics (COLING-10), 2010.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic MT Evaluation.
In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation (WMT2012), 2012.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International
Conference on Language Resources and Evalu-
ation (LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H.Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
In Proceedings of the 2004 Conference on Hu-
man Language Technology and the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL-04), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine:
A metric to assess mt adequacy. In Proceed-
380
ings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 116?122. Association
for Computational Linguistics, 2011.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In Proceedings of the 7th Conference
of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, Cam-
bridge, Massachusetts, August 2006.
Dekai Wu and Pascale Fung. Semantic Roles for
SMT: A Hybrid Two-Pass Model. In Proceed-
ings of the 2009 Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics - Human Language Technolo-
gies (NAACL-HLT-09), pages 13?16, 2009.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. Extract-
ing preordering rules from predicate-argument
structures. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP-11), 2011.
Deyi Xiong, Min Zhang, and Haizhou Li. Mod-
eling the Translation of Predicate-Argument
Structure for SMT. In Proceedings of the Joint
conference of the 50th AnnualMeeting of the As-
sociation for Computational Linguistics (ACL-
12), 2012.
Omar F. Zaidan. Z-MERT: A Fully Config-
urable Open Source Tool for Minimum Error
Rate Training of Machine Translation Systems.
The Prague Bulletin of Mathematical Linguis-
tics, 91:79?88, 2009.
381
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765?771,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
XMEANT: Better semantic MT evaluation without reference translations
Lo, Chi-kiu Beloucif, Meriem Saers, Markus Wu, Dekai
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce XMEANT?a new cross-lingual
version of the semantic frame based MT
evaluation metric MEANT?which can cor-
relate even more closely with human ade-
quacy judgments than monolingual MEANT
and eliminates the need for expensive hu-
man references. Previous work established
that MEANT reflects translation adequacy
with state-of-the-art accuracy, and optimiz-
ing MT systems against MEANT robustly im-
proves translation quality. However, to go
beyond tuning weights in the loglinear SMT
model, a cross-lingual objective function that
can deeply integrate semantic frame crite-
ria into the MT training pipeline is needed.
We show that cross-lingual XMEANT out-
performs monolingual MEANT by (1) replac-
ing the monolingual context vector model in
MEANT with simple translation probabilities,
and (2) incorporating bracketing ITG con-
straints.
1 Introduction
We show that XMEANT, a new cross-lingual ver-
sion of MEANT (Lo et al, 2012), correlates with
human judgment even more closely than MEANT
for evaluating MT adequacy via semantic frames,
despite discarding the need for expensive human
reference translations. XMEANT is obtained by
(1) using simple lexical translation probabilities,
instead of the monolingual context vector model
used in MEANT for computing the semantic role
fillers similarities, and (2) incorporating bracket-
ing ITG constrains for word alignment within the
semantic role fillers. We conjecture that the rea-
son that XMEANT correlates more closely with
human adequacy judgement than MEANT is that
on the one hand, the semantic structure of the
MT output is closer to that of the input sentence
than that of the reference translation, and on the
other hand, the BITG constraints the word align-
ment more accurately than the heuristic bag-of-
word aggregation used in MEANT. Our results
suggest that MT translation adequacy is more ac-
curately evaluated via the cross-lingual semantic
frame similarities of the input and the MT output
which may obviate the need for expensive human
reference translations.
The MEANT family of metrics (Lo and Wu,
2011a, 2012; Lo et al, 2012) adopt the princi-
ple that a good translation is one where a human
can successfully understand the central meaning
of the foreign sentence as captured by the basic
event structure: ?who did what to whom, when,
where and why? (Pradhan et al, 2004). MEANT
measures similarity between the MT output and
the reference translations by comparing the simi-
larities between the semantic frame structures of
output and reference translations. It is well estab-
lished that the MEANT family of metrics corre-
lates better with human adequacy judgments than
commonly used MT evaluation metrics (Lo and
Wu, 2011a, 2012; Lo et al, 2012; Lo and Wu,
2013b; Mach?a?cek and Bojar, 2013). In addition,
the translation adequacy across different genres
(ranging from formal news to informal web fo-
rum and public speech) and different languages
(English and Chinese) is improved by replacing
BLEU or TER with MEANT during parameter
tuning (Lo et al, 2013a; Lo and Wu, 2013a; Lo
et al, 2013b).
In order to continue driving MT towards better
translation adequacy by deeply integrating seman-
tic frame criteria into theMT training pipeline, it is
necessary to have a cross-lingual semantic objec-
tive function that assesses the semantic frame sim-
ilarities of input and output sentences. We there-
fore propose XMEANT, a cross-lingual MT evalu-
ation metric, that modifies MEANT using (1) sim-
ple translation probabilities (in our experiments,
765
from quick IBM-1 training), to replace the mono-
lingual context vector model in MEANT, and (2)
constraints from BITGs (bracketing ITGs). We
show that XMEANT assesses MT adequacy more
accurately than MEANT (as measured by correla-
tion with human adequacy judgement) without the
need for expensive human reference translations in
the output language.
2 Related Work
2.1 MT evaluation metrics
Surface-form oriented metrics such as BLEU (Pa-
pineni et al, 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), CDER
(Leusch et al, 2006), WER (Nie?en et al, 2000),
and TER (Snover et al, 2006) do not correctly re-
flect the meaning similarities of the input sentence.
In fact, a number of large scale meta-evaluations
(Callison-Burch et al, 2006; Koehn and Monz,
2006) report cases where BLEU strongly dis-
agrees with human judgments of translation ade-
quacy.
This has caused a recent surge of work to de-
velop better ways to automatically measure MT
adequacy. Owczarzak et al (2007a,b) improved
correlation with human fluency judgments by us-
ing LFG to extend the approach of evaluating syn-
tactic dependency structure similarity proposed by
Liu and Gildea (2005), but did not achieve higher
correlation with human adequacy judgments than
metrics like METEOR. TINE (Rios et al, 2011) is
a recall-oriented metric which aims to preserve the
basic event structure but it performs comparably
to BLEU and worse than METEOR on correlation
with human adequacy judgments. ULC (Gim?enez
and M`arquez, 2007, 2008) incorporates several
semantic features and shows improved correla-
tion with human judgement on translation quality
(Callison-Burch et al, 2007, 2008) but no work
has been done towards tuning an SMT system us-
ing a pure form of ULC perhaps due to its expen-
sive run time. Similarly, SPEDE (Wang and Man-
ning, 2012) predicts the edit sequence for match-
ing the MT output to the reference via an inte-
grated probabilistic FSM and PDA model. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps, con-
tain several dozens of parameters to tune, and em-
ploy expensive linguistic resources like WordNet
Figure 1: Monolingual MEANT algorithm.
or paraphrase tables; the expensive training, tun-
ing, and/or running time makes them hard to in-
corporate into the MT development cycle.
2.2 The MEANT family of metrics
MEANT (Lo et al, 2012), which is the weighted f-
score over the matched semantic role labels of the
automatically aligned semantic frames and role
fillers, that outperforms BLEU, NIST, METEOR,
WER, CDER and TER in correlation with human
adequacy judgments. MEANT is easily portable
to other languages, requiring only an automatic se-
mantic parser and a large monolingual corpus in
the output language for identifying the semantic
structures and the lexical similarity between the
semantic role fillers of the reference and transla-
tion.
Figure 1 shows the algorithm and equations for
computing MEANT. q
0
i,j
and q
1
i,j
are the argument
of type j in frame i in MT and REF respectively.
w
0
i
and w
1
i
are the weights for frame i in MT/REF
respectively. These weights estimate the degree of
contribution of each frame to the overall meaning
of the sentence. w
pred
and w
j
are the weights of
the lexical similarities of the predicates and role
fillers of the arguments of type j of all frame be-
tween the reference translations and the MT out-
766
Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow
semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate.
put. There is a total of 12 weights for the set
of semantic role labels in MEANT as defined in
Lo and Wu (2011b). For MEANT, they are de-
termined using supervised estimation via a sim-
ple grid search to optimize the correlation with
human adequacy judgments (Lo and Wu, 2011a).
For UMEANT (Lo and Wu, 2012), they are es-
timated in an unsupervised manner using relative
frequency of each semantic role label in the refer-
ences and thus UMEANT is useful when human
judgments on adequacy of the development set are
unavailable.
s
i,pred
and s
i,j
are the lexical similarities based
on a context vector model of the predicates and
role fillers of the arguments of type j between the
reference translations and the MT output. Lo et al
(2012) and Tumuluru et al (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al, 2013a; Lo
and Wu, 2013a; Lo et al, 2013b). In this paper,
we employ a newer version of MEANT that uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of seman-
tic role fillers, as our experiments indicate this is
more accurate than the previously used aggrega-
tion functions.
Recent studies (Lo et al, 2013a; Lo and Wu,
2013a; Lo et al, 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
2.3 MT quality estimation
Evaluating cross-lingual MT quality is similar to
the work of MT quality estimation (QE). Broadly
speaking, there are two different approaches to
QE: surface-based and feature-based.
Token-based QE models, such as those in Gan-
drabur et al (2006) and Ueffing and Ney (2005)
fail to assess the overall MT quality because trans-
lation goodness is not a compositional property. In
contrast, Blatz et al (2004) introduced a sentence-
level QE system where an arbitrary threshold is
used to classify the MT output as good or bad.
The fundamental problem of this approach is that
it defines QE as a binary classification task rather
than attempting to measure the degree of goodness
of the MT output. To address this problem, Quirk
(2004) related the sentence-level correctness of the
QE model to human judgment and achieved a high
correlation with human judgement for a small an-
notated corpus; however, the proposed model does
not scale well to larger data sets.
Feature-based QE models (Xiong et al, 2010;
He et al, 2011; Ma et al, 2011; Specia, 2011;
Avramidis, 2012; Mehdad et al, 2012; Almaghout
and Specia, 2013; Avramidis and Popovi?c, 2013;
Shah et al, 2013) throw a wide range of linguis-
tic and non-linguistic features into machine learn-
767
Figure 3: Cross-lingual XMEANT algorithm.
ing algorithms for predicting MT quality. Al-
though the feature-based QE system of Avramidis
and Popovi?c (2013) slightly outperformed ME-
TEOR on correlation with human adequacy judg-
ment, these ?black box? approaches typically lack
representational transparency, require expensive
running time, and/or must be discriminatively re-
trained for each language and text type.
3 XMEANT: a cross-lingual MEANT
Like MEANT, XMEANT aims to evaluate how
well MT preserves the core semantics, while
maintaining full representational transparency.
But whereas MEANT measures lexical similar-
ity using a monolingual context vector model,
XMEANT instead substitutes simple cross-lingual
lexical translation probabilities.
XMEANT differs only minimally from
MEANT, as underlined in figure 3. The same
weights obtained by optimizing MEANT against
human adequacy judgement were used for
XMEANT. The weights can also be estimated in
unsupervised fashion using the relative frequency
of each semantic role label in the foreign input, as
in UMEANT.
To aggregate individual lexical translation prob-
abilities into phrasal similarities between cross-
lingual semantic role fillers, we compared two nat-
ural approaches to generalizing MEANT?s method
of comparing semantic parses, as described below.
3.1 Applying MEANT?s f-score within
semantic role fillers
The first natural approach is to extend MEANT?s
f-score based method of aggregating semantic
parse accuracy, so as to also apply to aggregat-
ing lexical translation probabilities within seman-
tic role filler phrases. However, since we are miss-
ing structure information within the flat role filler
phrases, we can no longer assume an injective
mapping for aligning the tokens of the role fillers
between the foreign input and the MT output. We
therefore relax the assumption and thus for cross-
lingual phrasal precision/recall, we align each to-
ken of the role fillers in the output/input string
to the token of the role fillers in the input/output
string that has the maximum lexical translation
probability. The precise definition of the cross-
lingual phrasal similarities is as follows:
e
i,pred
? the output side of the pred of aligned frame i
f
i,pred
? the input side of the pred of aligned frame i
e
i,j
? the output side of the ARG j of aligned frame i
f
i,j
? the input side of the ARG j of aligned frame i
p(e, f) =
?
t (e|f) t (f |e)
prec
e,f
=
?
e?e
max
f?f
p(e, f)
|e|
rec
e,f
=
?
f?f
max
e?e
p(e, f)
|f|
s
i,pred
=
2 ? prec
e
i,pred
,f
i,pred
? rec
e
i,pred
,f
i,pred
prec
e
i,pred
,f
i,pred
+ rec
e
i,pred
,f
i,pred
s
i,j
=
2 ? prec
e
i,j
,f
i,j
? rec
e
i,j
,f
i,j
prec
e
i,j
,f
i,j
+ rec
e
i,j
,f
i,j
where the joint probability p is defined as the har-
monized the two directions of the translation table
t trained using IBM model 1 (Brown et al, 1993).
prec
e,f
is the precision and rec
e,f
is the recall of
the phrasal similarities of the role fillers. s
i,pred
and s
i,j
are the f-scores of the phrasal similarities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
3.2 Applying MEANT?s ITG bias within
semantic role fillers
The second natural approach is to extend
MEANT?s ITG bias on compositional reorder-
ing, so as to also apply to aggregating lexical
translation probabilities within semantic role filler
phrases. Addanki et al (2012) showed empiri-
cally that cross-lingual semantic role reordering of
the kind that MEANT is based upon is fully cov-
ered within ITG constraints. In Wu et al (2014),
we extend ITG constraints into aligning the tokens
within the semantic role fillers within monolingual
MEANT, thus replacing its previous monolingual
phrasal aggregation heuristic. Here we borrow the
768
idea for the cross-lingual case, using the length-
normalized inside probability at the root of a BITG
biparse (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009) as follows:
G ? ?{A} ,W
0
,W
1
,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 0.25
p (e/f |A) =
1
2
?
t (e|f) t (f |e)
s
i,pred
=
1
1?
ln
(
P
(
A
?
?e
i,pred
/f
i,pred
|G
))
max(|e
i,pred
|,|f
i,pred
|)
s
i,j
=
1
1?
ln
(
P
(
A
?
?e
i,j
/f
i,j
|G
))
max(|e
i,j
|,|f
i,j
|)
where G is a bracketing ITG, whose only nonter-
minal is A, and where R is a set of transduction
rules where e ? W
0
? {?} is an output token
(or the null token), and f ? W
1
? {?} is an in-
put token (or the null token). The rule probabil-
ity function p is defined using fixed probabilities
for the structural rules, and a translation table t
trained using IBM model 1 in both directions. To
calculate the inside probability of a pair of seg-
ments, P
(
A
?
? e/f|G
)
, we use the algorithm de-
scribed in Saers et al (2009). s
i,pred
and s
i,j
are
the length normalized BITG parsing probabilities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
4 Results
Table 1 shows that for human adequacy judgments
at the sentence level, the f-score based XMEANT
(1) correlates significantly more closely than other
commonly used monolingual automatic MT eval-
uation metrics, and (2) even correlates nearly as
well as monolingual MEANT. This suggests that
the semantic structure of the MT output is indeed
closer to that of the input sentence than that of the
reference translation.
Furthermore, the ITG-based XMEANT (1) sig-
nificantly outperforms MEANT, and (2) is an au-
tomatic metric that is nearly as accurate as the
HMEANT human subjective version. This indi-
cates that BITG constraints indeed provide a more
robust token alignment compared to the heuris-
tics previously employed in MEANT. It is also
consistent with results observed while estimating
word alignment probabilities, where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Table 1: Sentence-level correlation with HAJ
(GALE phase 2.5 evaluation data)
Metric Kendall
HMEANT 0.53
XMEANT (BITG) 0.51
MEANT (f-score) 0.48
XMEANT (f-score) 0.46
MEANT (2013) 0.46
NIST 0.29
BLEU/METEOR/TER/PER 0.20
CDER 0.12
WER 0.10
5 Conclusion
We have presented XMEANT, a new cross-lingual
variant of MEANT, that correlates even more
closely with human translation adequacy judg-
ments thanMEANT, without the expensive human
references. This is (1) accomplished by replacing
monolingual MEANT?s context vector model with
simple translation probabilities when computing
similarities of semantic role fillers, and (2) fur-
ther improved by incorporating BITG constraints
for aligning the tokens in semantic role fillers.
While monolingual MEANT alone accurately re-
flects adequacy via semantic frames and optimiz-
ing SMT against MEANT improves translation,
the new cross-lingual XMEANT semantic objec-
tive function moves closer toward deep integration
of semantics into the MT training pipeline.
The phrasal similarity scoring has only been
minimally adapted to cross-lingual semantic role
fillers in this first study of XMEANT. We expect
further improvements to XMEANT, but these first
results already demonstrate XMEANT?s potential
to drive research progress toward semantic SMT.
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT con-
tract nos. HR0011-12-C-0014 and HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of DARPA, the EU, or
RGC.
769
References
Karteek Addanki, Chi-Kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-lingual
verb frame alternations. In 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT-2012), Trento, Italy, May 2012.
Hala Almaghout and Lucia Specia. A CCG-based qual-
ity estimation metric for statistical machine transla-
tion. In Machine Translation Summit XIV (MT Sum-
mit 2013), Nice, France, 2013.
Eleftherios Avramidis and Maja Popovi?c. Machine
learning methods for comparative and time-oriented
quality estimation of machine translation output. In
8th Workshop on Statistical Machine Translation
(WMT 2013), 2013.
Eleftherios Avramidis. Quality estimation for machine
translation output using linguistic analysis and de-
coding features. In 7th Workshop on Statistical Ma-
chine Translation (WMT 2012), 2012.
Satanjeev Banerjee and Alon Lavie. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. Confidence estimation
for machine translation. In 20th international con-
ference on Computational Linguistics, 2004.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, 1993.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. (meta-)
evaluation of machine translation. In Second Work-
shop on Statistical Machine Translation (WMT-07),
2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third
Workshop on Statistical Machine Translation (WMT-
08), 2008.
Julio Castillo and Paula Estrella. Semantic textual sim-
ilarity for MT evaluation. In 7th Workshop on Sta-
tistical Machine Translation (WMT 2012), 2012.
George Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In The second international conference on
Human Language Technology Research (HLT ?02),
San Diego, California, 2002.
Simona Gandrabur, George Foster, and Guy Lapalme.
Confidence estimation for nlp applications. ACM
Transactions on Speech and Language Processing,
2006.
Jes?us Gim?enez and Llu??s M`arquez. Linguistic features
for automatic evaluation of heterogenous MT sys-
tems. In Second Workshop on Statistical Machine
Translation (WMT-07), pages 256?264, Prague,
Czech Republic, June 2007.
Jes?us Gim?enez and Llu??s M`arquez. A smorgasbord
of features for automatic MT evaluation. In Third
Workshop on Statistical Machine Translation (WMT-
08), Columbus, Ohio, June 2008.
Yifan He, Yanjun Ma, Andy Way, and Josef van
Genabith. Rich linguistic features for translation
memory-inspired consistent translation. In 13th Ma-
chine Translation Summit (MT Summit XIII), 2011.
Philipp Koehn and Christof Monz. Manual and auto-
matic evaluation of machine translation between eu-
ropean languages. In Workshop on Statistical Ma-
chine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), 2006.
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on
Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. super-
vised weight estimation for semantic MT evalua-
tion metrics. In Sixth Workshop on Syntax, Seman-
tics and Structure in Statistical Translation (SSST-
6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT
Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013:
A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Sta-
tistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT
2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by train-
ing against an automatic semantic frame based eval-
770
uation metric. In 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2013),
2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by tun-
ing against Chinese MEANT. In International
Workshop on Spoken Language Translation (IWSLT
2013), 2013.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. Consistent translation using discriminative
learning: a translation memory-inspired approach.
In 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL HLT 2011). Association for Computa-
tional Linguistics, 2011.
Matou?s Mach?a?cek and Ond?rej Bojar. Results of the
WMT13 metrics shared task. In Eighth Workshop on
Statistical Machine Translation (WMT 2013), Sofia,
Bulgaria, August 2013.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
Match without a referee: evaluating mt adequacy
without reference translations. In 7th Workshop on
Statistical Machine Translation (WMT 2012), 2012.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first Na-
tional Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. A evaluation tool for machine trans-
lation: Fast evaluation for MT research. In The
Second International Conference on Language Re-
sources and Evaluation (LREC 2000), 2000.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Dependency-based automatic evaluation for
machine translation. In Syntax and Structure in Sta-
tistical Translation (SSST), 2007.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-02), pages 311?318, Philadelphia, Pennsylva-
nia, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow seman-
tic parsing using support vector machines. In Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2004), 2004.
Christopher Quirk. Training a sentence-level machine
translation confidence measure. In Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal, May 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine: A
metric to assess MT adequacy. In Sixth Workshop on
Statistical Machine Translation (WMT 2011), 2011.
Markus Saers and Dekai Wu. Improving phrase-based
translation via word alignments from stochastic in-
version transduction grammars. In Third Workshop
on Syntax and Structure in Statistical Translation
(SSST-3), Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu. Learning
stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th
International Conference on Parsing Technologies
(IWPT?09), Paris, France, October 2009.
Kashif Shah, Trevor Cohn, and Lucia Specia. An in-
vestigation on the effectiveness of features for trans-
lation quality estimation. In Machine Translation
Summit XIV (MT Summit 2013), Nice, France, 2013.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A study of trans-
lation edit rate with targeted human annotation. In
7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages
223?231, Cambridge, Massachusetts, August 2006.
Lucia Specia. Exploiting objective annotations for
measuring translation post-editing effort. In 15th
Conference of the European Association for Ma-
chine Translation, pages 73?80, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu.
Accuracy and robustness in measuring the lexical
similarity of semantic role fillers for automatic se-
mantic MT evaluation. In 26th Pacific Asia Confer-
ence on Language, Information, and Computation
(PACLIC 26), 2012.
Nicola Ueffing and Hermann Ney. Word-level con-
fidence estimation for machine translation using
phrase-based translation models. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 763?770, 2005.
Mengqiu Wang and Christopher D. Manning. SPEDE:
Probabilistic edit distance metrics for MT evalua-
tion. In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Dekai Wu, Chi-kiu Lo, Meriem Beloucif, and Markus
Saers. IMEANT: Improving semantic frame based
MT evaluation via inversion transduction grammars.
Forthcoming, 2014.
Dekai Wu. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403, 1997.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detec-
tion for statistical machine translation using linguis-
tic features. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2010),
2010.
Richard Zens and Hermann Ney. A comparative
study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2003),
pages 144?151, Stroudsburg, Pennsylvania, 2003.
771
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 52?60,
COLING 2010, Beijing, August 2010.
7IQERXMG ZW 7]RXEGXMG ZW 2KVEQ 7XVYGXYVI
JSV 1EGLMRI 8VERWPEXMSR )ZEPYEXMSR
'LMOMY 03 ERH (IOEM ;9
,/978
,YQER 0ERKYEKI 8IGLRSPSK] 'IRXIV
(ITEVXQIRX SJ 'SQTYXIV 7GMIRGI ERH )RKMRIIVMRK
,SRK /SRK 9RMZIVWMX] SJ 7GMIRGI ERH 8IGLRSPSK]
NEGOMIPSHIOEM $GWYWXLO
%FWXVEGX
;I TVIWIRX VIWYPXW SJ ER IQTMVMGEP WXYH]
SR IZEPYEXMRK XLI YXMPMX] SJ XLI QEGLMRI
XVERWPEXMSR SYXTYX F] EWWIWWMRK XLI EGGY
VEG] [MXL [LMGL LYQER VIEHIVW EVI EFPI
XS GSQTPIXI XLI WIQERXMG VSPI ERRSXEXMSR
XIQTPEXIW 9RPMOI XLI [MHIP]YWIH PI\M
GEP ERH RKVEQ FEWIH SV W]RXEGXMG FEWIH
18 IZEPYEXMSR QIXVMGW [LMGL EVI xYIRG]
SVMIRXIH SYV VIWYPXW WLS[ XLEX YWMRK WI
QERXMG VSPI PEFIPW XS IZEPYEXI XLI YXMPMX]
SJ 18 SYXTYX EGLMIZI LMKLIV GSVVIPEXMSR
[MXL LYQER NYHKQIRXW SR EHIUYEG] -R
XLMW WXYH] LYQER VIEHIVW [IVI IQTPS]IH
XS MHIRXMJ] XLI WIQERXMG VSPI PEFIPW MR XLI
XVERWPEXMSR *SV IEGL VSPI XLI wPPIV MW
GSRWMHIVIH ER EGGYVEXI XVERWPEXMSR MJ MX I\
TVIWWIW XLI WEQI QIERMRK EW XLEX ERRS
XEXIH MR XLI KSPH WXERHEVH VIJIVIRGI XVERW
PEXMSR 3YV 760 FEWIH JWGSVI IZEPYEXMSR
QIXVMG LEW E  GSVVIPEXMSR GSIJwGMIRX
[MXL XLI LYQER NYHKIQIRX SR EHIUYEG]
[LMPI MR GSRXVEWX &0)9 LEW SRP] E 
GSVVIPEXMSR GSIJwGMIRX ERH XLI W]RXEGXMG
FEWIH18 IZEPYEXMSR QIXVMG 781LEW SRP]
 GSVVIPEXMSR GSIJwGMIRX [MXL XLI LY
QER NYHKIQIRX SR EHIUYEG] 3YV VIWYPXW
WXVSRKP] MRHMGEXI XLEX YWMRK WIQERXMG VSPI
PEFIPW JSV 18 IZEPYEXMSR GER FI WMKRMw
GERXP] QSVI IJJIGXMZI ERH FIXXIV GSVVIPEXIH
[MXL LYQER NYHKIQIRX SR EHIUYEG] XLER
&0)9 ERH 781
 -RXVSHYGXMSR
-R XLMW TETIV [I WLS[ XLEX IZEPYEXMRK QEGLMRI
XVERWPEXMSR UYEPMX] F] EWWIWWMRK XLI EGGYVEG] SJ
LYQER TIVJSVQERGI MR VIGSRWXVYGXMRK XLI WIQER
XMG JVEQIW JVSQ XLI 18 SYXTYX LEW E LMKLIV GSV
VIPEXMSR [MXL LYQER NYHKQIRX SR XVERWPEXMSR EH
IUYEG] XLER  XLI [MHIP]YWIH PI\MGEP RKVEQ
TVIGMWMSR FEWIH 18 IZEPYEXMSR QIXVMG &0)9 4E
TMRIRM IX EP  EW [IPP EW  XLI FIWXORS[R
W]RXEGXMG XVII TVIGMWMSR FEWIH 18 IZEPYEXMSR QIX
VMG 781 0MY ERH +MPHIE  %X XLI WEQI
XMQI YRPMOI WSQI LMKLP] PEFSV MRXIRWMZI IZEPYE
XMSR QIXVMGW WYGL EW ,8)6 7RSZIV IX EP Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10?20,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Structured vs. Flat Semantic Role Representations
for Machine Translation Evaluation
Chi-kiu Lo and DekaiWu
HKUST
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|dekai}@cs.ust.hk
Abstract
We argue that failing to capture the degree of
contribution of each semantic frame in a sen-
tence explains puzzling results in recent work
on the MEANT family of semantic MT eval-
uation metrics, which have disturbingly in-
dicated that dissociating semantic roles and
fillers from their predicates actually improves
correlation with human adequacy judgments
even though, intuitively, properly segregat-
ing event frames should more accurately re-
flect the preservation of meaning. Our anal-
ysis finds that both properly structured and
flattened representations fail to adequately ac-
count for the contribution of each seman-
tic frame to the overall sentence. We then
show that the correlation of HMEANT, the hu-
man variant of MEANT, can be greatly im-
proved by introducing a simple length-based
weighting scheme that approximates the de-
gree of contribution of each semantic frame
to the overall sentence. The new results
also show that, without flattening the struc-
ture of semantic frames, weighting the degree
of each frame?s contribution gives HMEANT
higher correlations than the previously best-
performing flattened model, as well as HTER.
1 Introduction
In this paper we provide a more concrete answer
to the question: what would be a better represen-
tation, structured or flat, of the roles in semantic
frames to be used in a semantic machine transla-
tion (MT) evaluation metric? We compare recent
studies on the MEANT family of semantic role la-
beling (SRL) based MT evaluation metrics (Lo and
Wu, 2010a,b, 2011a,b) by (1) contrasting their vari-
ations in semantic role representation and observing
disturbing comparative results indicating that segre-
gating the event frames in structured role representa-
tion actually damages correlation against human ad-
equacy judgments and (2) showing how SRL based
MT evaluation can be improved beyond the current
state-of-the-art compared to previous MEANT vari-
ants as well as HTER, through the introduction of
a simple weighting scheme that reflects the degree
of contribution of each semantic frame to the overall
sentence. The weighting scheme we propose uses
a simple length-based heuristic that reflects the as-
sumption that a semantic frame that covers more to-
kens contributes more to the overall sentence transla-
tion. We demonstrate empirically that when the de-
gree of each frame?s contribution to its sentence is
taken into account, the properly structured role rep-
resentation is more accurate and intuitive than the
flattened role representation for SRL MT evaluation
metrics.
For years, the task of measuring the performance
of MT systems has been dominated by lexical n-
gram based machine translation evaluation met-
rics, such as BLEU (Papineni et al, 2002), NIST
(Doddington, 2002), METEOR (Banerjee and Lavie,
2005), PER (Tillmann et al, 1997), CDER (Leusch
et al, 2006) and WER (Nie?en et al, 2000). These
metrics are excellent at ranking overall systems by
averaging their scores over entire documents. How-
ever, as MT systems improve, the shortcomings of
such metrics are becoming more apparent. Though
containing roughly the correct words, MT output at
the sentence remains often quite incomprehensible,
and fails to preserve the meaning of the input. This
results from the fact that n-gram based metrics are
not as reliable at ranking the adequacy of transla-
tions of individual sentences, and are particularly
10
poor at reflecting translation quality improvements
involving more meaningful word sense or semantic
frame decisions?which human judges have no trou-
ble distinguishing. Callison-Burch et al (2006) and
Koehn and Monz (2006), for example, study situ-
ations where BLEU strongly disagrees with human
judgment of translation quality.
Newer avenues of research seek substitutes for
n-gram based MT evaluation metrics that are bet-
ter at evaluating translation adequacy, particularly at
the sentence level. One line of research emphasizes
more the structural correctness of translation. Liu
and Gildea (2005) propose STM, a metric based on
syntactic structure, that addresses the failure of lex-
ical similarity based metrics to evaluate translation
grammaticality. However, the problem remains that
a grammatical translation can achieve a high syntax-
based score yet still make significant errors arising
from confusion of semantic roles. On the other hand,
despite the fact that non-automatic, manually evalu-
ated metrics, such as HTER (Snover et al, 2006), are
more adequacy oriented exhibit much higher correla-
tion with human adequacy judgment, their high labor
cost prohibits widespread use. There has also been
work on explicitly evaluating MT adequacy by ag-
gregating over a very large set of linguistic features
(Gime?nez and Ma`rquez, 2007, 2008) and textual en-
tailment (Pado et al, 2009).
2 SRL based MT evaluation metrics
A blueprint for more direct assessment of mean-
ing preservation across translation was outlined by
Lo and Wu (2010a), in which translation utility is
manually evaluated with respect to the accuracy of
semantic role labels. A good translation is one from
which human readers may successfully understand
at least the basic event structure??who did what
to whom, when, where and why? (Pradhan et al,
2004)?which represents the most essential meaning
of the source utterances. Adopting this principle,
the MEANT family of metrics compare the seman-
tic frames in reference translations against those that
can be reconstructed from machine translation out-
put.
Preliminary results reported in (Lo and Wu,
2010b) confirm that the blueprint model outper-
forms BLEU and similar n-gram oriented evalu-
ation metrics in correlation against human ade-
quacy judgments, but does not fare as well as
HTER. The more complete study of Lo and Wu
(2011a) introduces MEANT and its human variants
HMEANT, which implement an extended version of
blueprint methodology. Experimental results show
that HMEANT correlates against human adequacy
judgments as well as the more expensive HTER,
even though HMEANT can be evaluated using low-
cost untrained monolingual semantic role annotators
while still maintaining high inter-annotator agree-
ment (both are far superior to BLEU or other sur-
face oriented evaluation metrics). The study also
shows that replacing the human semantic role la-
belers with an automatic shallow semantic parser
yields an approximation that is still vastly superior
to BLEU while remaining about 80% as closely cor-
related with human adequacy judgments as HTER.
Along with additional improvements to the accu-
racy of the MEANT family of metrics, Lo and Wu
(2011b) study the impact of each individual seman-
tic role to themetric?s correlation against human ade-
quacy judgments, as well as the time cost for humans
to reconstruct the semantic frames and compare the
translation accuracy of the role fillers.
In general, the MEANT family of SRL MT eval-
uation metrics (Lo and Wu, 2011a,b) evaluate the
translation utility as follows. First, semantic role
labeling is performed (either manually or automat-
ically) on both the reference translation (REF) and
the machine translation output (MT) to obtain the
semantic frame structure. Then, the semantic pred-
icates, roles and fillers reconstructed from the MT
output are compared to those in the reference trans-
lations. The number of correctly and partially cor-
rectly annotated arguments of each type in each
frame of the MT output are collected in this step:
Ci,j ? # correct ARG i of PRED i in MT
Pi,j ? # partially correct ARG j of PRED i in MT
Mi,j ? total # ARG j of PRED i in MT
Ri,j ? total # ARG j of PRED i in REF
In the following three subsections, we describe
how the translation utility is calculated using these
counts in (a) the original blueprint model, (b) the
first version of HMEANT and MEANT using struc-
tured role representations, and (c) the more accu-
11
Figure 1: The structured role representation for the
blueprint SRL-based MT evaluation metric as proposed
in Lo and Wu (2010a,b), with arguments aggregated into
core and adjunct classes.
rate flattened-role implementation of HMEANT and
MEANT.
2.1 Structured core vs. adjunct role
representation
Figure 1 depicts the semantic role representation
in the blueprint model of SRL MT evaluation metric
proposed by Lo and Wu (2010a,b). Each sentence
consists of a number of frames, and each frame con-
sists of a predicate and two classes of arguments, ei-
ther core or adjunct. The frame precision/recall is
the weighted sum of the number of correctly trans-
lated roles (where arguments are grouped into the
core and adjunct classes) in a frame normalized by
the weighted sum of the total number of all roles in
that frame in the MT/REF respectively. The sen-
tence precision/recall is the sum of the frame preci-
sion/recall for all frames averaged by the total num-
ber of frames in the MT/REF respectively. The SRL
evaluation metric is then defined in terms of f-score
in order to balance the sentence precision and recall.
More precisely, assuming the above definitions of
Ci,j , Pi,j , Mi,j and Ri,j , the sentence precision and
recall are defined as follows.
precision =
?
i
wpred+
?
t
wt
(
?
j?t
(Ci,j+wpartialPi,j)
)
wpred+
?
t
wt
(
?
j?t
Mi,j
)
# frames in MT
recall =
?
i
wpred+
?
t
wt
(
?
j?t
(Ci,j+wpartialPi,j)
)
wpred+
?
t
wt
(
?
j?t
Ri,j
)
# frames in REF
Figure 2: The structured role representation for the
MEANT family of metrics as proposed in Lo and Wu
(2011a).
where wpred is the weight for predicates, and wt
where t ? {core, adj} is the weight for core argu-
ments and adjunct arguments. These weights rep-
resent the degree of contribution of the predicate
and different classes of arguments (either core or ad-
junct) to the overall meaning of the semantic frame
they attach to. In addition,wpartial is a weight control-
ling the degree to which ?partially correct? transla-
tions are penalized. All the weights can be automat-
ically estimated by optimizing the correlation with
human adequacy judgments.
We conjecture that the reason for the low correla-
tion with human adequacy judgments of this model
as reported in Lo and Wu (2010b) is that the ab-
straction of arguments actually reduces the repre-
sentational power of the original predicate-argument
structure in SRL. Under this representation, all the
arguments in the same class, e.g. all adjunct argu-
ments, are weighted uniformly. The assumption that
all types of arguments in the same class have the
same degree of contribution to their frame is obvi-
ously wrong, and the empirical results confirm that
the assumption is too coarse.
2.2 Structured role representation
Figure 2 shows the structured role representation
used in the MEANT family of metrics as proposed
in Lo and Wu (2011a), which avoids aggregating ar-
guments into core and adjunct classes. The design
of the MEANT family of metrics addresses the in-
correct assumption in the blueprint model by assum-
ing each type of argument has a unique weight repre-
senting its degree of contribution to the overall sen-
tence translation. Thus, the number of dimensions of
12
the weight vector is increased to allow an indepen-
dent weight to be assigned to each type of argument.
Unlike the previous representation in the blueprint
model, there is no aggregation of arguments into
core and adjunct classes. Each sentence consists of a
number of frames, and each frame consists of a pred-
icate and a number of arguments of type j.
Under the new approach, the frame preci-
sion/recall is the weighted sum of the number of cor-
rectly translated roles in a frame normalized by the
weighted sum of the total number of all roles in that
frame in the MT/REF respectively. Similar to the
previous blueprint representation, the sentence pre-
cision/recall is the sum of the frame precision/recall
for all frames averaged by the total number of frames
in the MT/REF respectively. More precisely, fol-
lowing the previous definitions of Ci,j , Pi,j , Mi,j ,
Ri,j ,wpred andwpartial, the sentence precision and re-
call are redefined as follows.
precision =
?
i
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjMi,j
#frames in MT
recall =
?
i
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjRi,j
# frames in REF
where wj is the weight for the arguments of type j.
Theseweights represent the degree of contribution of
different types of arguments to the overall meaning
of their semantic frame.
2.3 Flat role representation
Figure 3 depicts the flat role representation used in
themore accurate variants ofMEANT as proposed in
Lo andWu (2011b). This representation ismotivated
by the studies of the impact of individual seman-
tic role. The highly significant difference between
this flat representation and both of the previous two
structured role representations is that the semantic
frames in the sentence are no longer segregated.
The flat role representation desegregates the frame
structure, resulting in a flat, single level structure.
Therefore, there is no frame precision/recall. The
sentence precision/recall is the weighted sum of the
number of correctly translated roles in all frames nor-
malized by the weighted sum of the total number of
roles in all frames in theMT/REF respectively. More
precisely, again assuming the previous definitions of
Ci,j , Pi,j , Mi,j , Ri,j and wpartial, the sentence preci-
sion and recall are redefined as follows.
Cpred ? total # correctly translated predicates
Mpred ? total # predicates in MT
Rpred ? total # predicates in REF
precision =
wpredCpred +
?
j wj
(
?
i(Ci,j + wpartialPi,j)
)
wpredMpred +
?
j wj
(
?
i Mi,j
)
recall =
wpredCpred +
?
j wj
(
?
i(Ci,j + wpartialPi,j)
)
wpredRpred +
?
j wj
(
?
i Ri,j
)
Note that there is a small modification of the defini-
tion of wpred and wj . Instead of the degree of contri-
bution to the overall meaning of the semantic frame
that the roles attached to, wpredand wj now represent
the degree of contribution of the predicate and the ar-
guments of type j to the overall meaning of the entire
sentence.
It is worth noting that the semantic role features in
the ULC metric proposed by Gime?nez and Ma`rquez
(2008) also employ a flat feature-based represen-
tation of semantic roles. However, the definition
of those semantic role features adopts a different
methodology for determining the role fillers? transla-
tion accuracy, which prevents a controlled consistent
environment for the comparative experiments that
the present work focuses on.
3 Experimental setup
The evaluation data for our experiments consists
of 40 sentences randomly drawn from the DARPA
GALE program Phase 2.5 newswire evaluation cor-
pus containing Chinese input sentence, English ref-
erence translations, and themachine translation from
three different state-of-the-art GALE systems. The
Chinese and the English reference translation have
both been annotated with gold standard PropBank
(Palmer et al, 2005) semantic role labels. The
weightswpred, wcore, wadj, wj and wpartial can be esti-
mated by optimizing correlation against human ade-
quacy judgments, using any of themany standard op-
timization search techniques. In the work of Lo and
13
Figure 3: The flat role representation for the MEANT family of metrics as proposed in Lo and Wu (2011b) .
Wu (2011b), the correlations of all individual roles
with the human adequacy judgments were found to
be non-negative, therefore we found grid search to
be quite adequate for estimating the weights. We use
linear weighting because we would like to keep the
metric?s interpretation simple and intuitive.
Following the benchmark assessment in NIST
MetricsMaTr 2010 (Callison-Burch et al, 2010), we
assess the performance of the semantic MT evalua-
tion metric at the sentence level using the summed-
diagonal-of-confusion-matrix score. The human ad-
equacy judgments were obtained by showing all
three MT outputs together with the Chinese source
input to a human reader. The human reader was in-
structed to order the sentences from the three MT
systems according to the accuracy of meaning in
the translations. For the MT output, we ranked the
sentences from the three MT systems according to
their evaluation metric scores. By comparing the
two sets of rankings, a confusion matrix is formed.
The summed diagonal of confusion matrix is the per-
centage of the total count when a particular rank by
the metric?s score exactly matches the human judg-
ments. The range of possible values of summed di-
agonal of confusion matrix is [0,1], where 1 means
all the systems? ranks determined by the metric are
identical with that of the human judgments and 0
means all the systems? ranks determined by the met-
ric are different from that of the human judgment.
Since the summed diagonal of confusion matrix
scores only assess the absolute ranking accuracy,
we also report the Kendall?s ? rank correlation co-
efficients, which measure the correlation of the pro-
posed metric against human judgments with respect
to their relative ranking of translation adequacy. A
higher the value for ? indicates the more similar the
ranking by the evaluation metric to the human judg-
ment. The range of possible values of correlation
Table 1: Sentence-level correlations against human ade-
quacy judgments as measured by Kendall?s ? and summed
diagonal of confusion matrix as used in MetricsMaTr
2010. ?SRL - blueprint? is the blueprint model described
in section 2.1. ?HMEANT (structured)? is HMEANT us-
ing the structured role representation described in sec-
tion 2.2. ?HMEANT (flat)? is HMEANT using the flat
role representation described in section 2.3.
Metric Kendall MetricsMaTr
HMEANT (flat) 0.4685 0.5583
HMEANT (structured) 0.4324 0.5083
SRL - blueprint 0.3784 0.4667
coefficient is [-1,1], where 1 means the systems are
ranked in the same order as the human judgment and
-1 means the systems are ranked in the reverse order
as the human judgment.
4 Round 1: Flat beats structured
Our first round of comparative results quantita-
tively assess whether a structured role representation
(that properly preserves the semantic frame struc-
ture, which is typically hierarchically nested in com-
positional fashion) outperforms the simpler (but less
intuitive, and certainly less linguistically satisfying)
flat role representation.
As shown in table 1, disturbingly, HMEANT us-
ing flat role representations yields higher correla-
tions against human adequacy judgments than us-
ing structured role representations, regardless of
whether role types are aggregated into core and
adjunct classes. The results are consistent for
both Kendall?s tau correlation coefficient and Met-
ricsMaTr?s summed diagonal of confusion matrix.
HMEANT using a flat role representation achieved
a Kendall?s tau correlation coefficient and summed
diagonal of confusion matrix score of 0.4685 and
0.5583 respectively, which is superior to both
14
Figure 4: The new proposed structured role representa-
tion, incorporating a weighting scheme reflecting the de-
gree of contribution of each semantic frame to the overall
sentence.
HMEANT using a structured role representation
(0.4324 and 0.5083 respectively) and the blueprint
model (0.3784 and 0.4667 respectively).
Error analysis, in light of these surprising results,
strongly suggests that the problem lies in the design
which uniformly averages the frame precision/recall
over all frames in a sentence when computing the
sentence precision/recall. This essentially assumes
that each frame in a sentence contributes equally
to the overall meaning in the sentence translation.
Such an assumption is trivially wrong and could well
hugely degrade the advantages of using a structured
role representation for semanticMT evaluation. This
suggests that the structured role representation could
be improved by also capturing the degree of contri-
bution of each frame to the overall sentence transla-
tion.
5 Capturing the importance of each frame
To address the problem in the previousmodels, we
introduce a weighting scheme to reflect the degree
of contribution of each semantic frame to the overall
sentence. However, unlike the contribution of each
role to a frame, the contribution of each frame to
the overall sentence cannot be estimated across sen-
tences. This is because unlike semantic roles, which
can be identified by their types, frames do not neces-
sarily have easily defined types, and their construc-
tion is also different from sentence to sentence so that
the positions of their predicates in the sentence are
the only way to identify the frames. However, the
degree of contribution of each frame does not depend
on the position of the predicate in the sentence. For
example, the two sentences I met Tom when I was go-
ing home andWhen I was walking home, I saw Tom have
similar meanings. The verbs met and saw are the
predicates of the key event frames which contribute
more to the overall sentences, whereas going and
walking are the predicates of the minor nested event
frames (in locative manner roles of the key event
frames) and contribute less to the overall sentences.
However, the two sentences are realized with differ-
ent surface constructions, and the two key frames are
in different positions. Therefore, the weights learned
from one sentence cannot directly be applied to the
other sentence.
Instead of estimating the weight of each frame us-
ing optimization techniques, wemake an assumption
that a semantic frame filled with more word tokens
expresses more concepts and thus contributes more
to the overall sentence. Following this assumption,
we determine the weights of each semantic frame by
its span coverage in the sentence. In other words,
the weight of each frame is the percentage of word
tokens it covers in the sentence.
Figure 4 depicts the structured role representa-
tion with the proposed new frame weighting scheme.
The significant difference between this representa-
tion and the structured role representation in the
MEANT variants proposed in Lo and Wu (2011a)
is that each frame is now assigned an independent
weight, which is its span coverage in the MT/REF
when obtaining the frame precision/recall respec-
tively.
As in Lo and Wu (2011a), each sentence consists
of a number of frames, and each frame consists of
a predicate and a number of arguments of type j.
Each type of argument is assigned an independent
weight to represent its degree of contribution to the
overall meaning of the semantic frame they attached
to. The frame precision/recall is the weighted sum
of the number of correctly translated roles in a frame
normalized by the weighted sum of the number of all
roles in that frame in the MT/REF. The sentence pre-
cision/recall is the weighted sum of the frame preci-
sion/recall for all frames normalized by the weighted
sum of the total number of frames in MT/REF re-
spectively. More precisely, again assuming the ear-
15
lier definitions of Ci,j , Pi,j , Mi,j , Ri,j , wpred and
wpartial in section 2, the sentence precision and recall
are redefined as follows.
mi ?
# tokens filled in frame i of MT
total # tokens in MT
ri ?
# tokens filled in frame i of REF
total # tokens in REF
precision =
?
imi
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjMi,j
?
imi
recall =
?
i ri
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjRi,j
?
i ri
where mi and ri are the weights for frame i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence.
6 Round 2: Structured beats flat
We now assess the performance of the new pro-
posed structured role representation, by comparing
it with the previous models under the same experi-
mental setup as in section 4. We have also run con-
trastive experiments against BLEU and HTER un-
der the same experimental conditions. In addition,
to investigate the consistency of results for the au-
tomated variants of MEANT, we also include com-
parative experiments where shallow semantic pars-
ing (ASSERT) replaces human semantic role label-
ers for each model of role representation.
Figure 5 shows an example where HMEANTwith
the frame weighting scheme outperforms HMEANT
using other role representations in correlation against
human adequacy judgments. IN is the Chinese
source input. REF is the corresponding refer-
ence translation. MT1, MT2 and MT3 are the
three corresponding MT output. The human ade-
quacy judgments for this set of translation are that
MT1>MT3>MT2. HMEANT with the proposed
frame weighting predicts the same ranking order
as the human adequacy judgment, while HMEANT
with the flat role representation and HMEANT
with the structured role representation without frame
weighting both predict MT3>MT1>MT2. There
are four semantic frames in IN while there are only
three semantic frames in the REF. This is because
the predicate ?? in IN is translated in REF as had
which is not a predicate. However, for the same
frame, both MT1 and MT2 translated ARG1???
? into the predicate affect, while MT3 did not trans-
late the predicate ?? and translated the ARG1 ?
??? into the noun phrase adverse impact. There-
fore, using the flat role representation or the previ-
ous structured role representation which assume all
frames have an identical degree of contribution to the
overall sentence translation, MT1?s and MT2?s sen-
tence precision is greatly penalized for having one
more extra frame than the reference. In contrast, ap-
plying the frame weighting scheme, the degree of
contribution of each frame is adjusted by its token
coverage. Therefore, the negative effect of the less
important extra frames is minimized, allowing the
positive effect of correctly translating more roles in
more important frames to be more appropriately re-
flected.
Table 2 shows that HMEANT with the proposed
new frameweighting scheme correlatesmore closely
with human adequacy judgments than HMEANT
using the previous alternative role representations.
The results from Kendall?s tau correlation coeffi-
cient and MetricsMaTr?s summed diagonal of con-
fusion matrix analysis are consistent. HMEANT
using the frame-weighted structured role represen-
tation achieved a Kendall?s tau correlation coef-
ficient and summed diagonal of confusion matrix
score of 0.2865 and 0.575 respectively, bettering
both HMEANT using the flat role representation
(0.4685 and 0.5583) and HMEANT using the pre-
vious un-frame-weighted structured role representa-
tion (0.4324 and 0.5083).
HMEANT using the improved structured role rep-
resentation also outperforms other commonly used
MT evaluation metrics. It correlates with human ad-
equacy judgments more closely than HTER (0.4324
and 0.425 in Kendall?s tau correlation coefficient and
summed diagonal of confusionmatrix, respectively).
It also correlates with human adequacy judgments
significantly more closely than BLEU (0.1982 and
0.425).
Turning to the variants that replace human SRL
with automated SRL, table 2 shows that MEANT
16
Figure 5: Example input sentence along with reference and machine translations, annotated with semantic frames in
Propbank format. The MT output is annotated with semantic frames by minimally trained humans. HMEANT with
the new frame-weighted structured role representation successfully ranks the MT output in an order that matches with
human adequacy judgments (MT1>MT3>MT2), whereas HMEANT with a flat role representation or the previous
un-frame-weighted structured role representation fails to rank MT1 and MT3 in an order that matches with human
adequacy judgments. See section 6 for details.
17
Table 2: Sentence-level correlations against human ade-
quacy judgments as measured by Kendall?s ? and summed
diagonal of confusion matrix as used in MetricsMaTr
2010. ?SRL - blueprint?, ?HMEANT (structured)? and
?HMEANT (flat)? are the same as in table 1. ?MEANT
(structured)? and ?MEANT (flat)? use automatic rather
than human SRL. ?MEANT (frame)? and ?HMEANT
(frame)? are MEANT/HMEANT using the structured
role representation with the frame weighting scheme de-
scribed in section 5.
Metric Kendall MetricsMaTr
HMEANT (frame) 0.4865 0.575
HMEANT (flat) 0.4685 0.5583
HMEANT (structured) 0.4324 0.5083
HTER 0.4324 0.425
SRL - blueprint 0.3784 0.4667
MEANT (frame) 0.3514 0.4333
MEANT (structured) 0.3423 0.425
MEANT (flat) 0.3333 0.425
BLEU 0.1982 0.425
using the new frame-weighted structured role repre-
sentation yields an approximation that is about 81%
as closely correlated with human adequacy judgment
as HTER, and is better than all previous MEANT
variants using alternative role representations. All
results consistently confirm that using a structured
role representation with the new frame weighting
scheme, which captures the event structure and an
approximate degree of contribution of each frame to
the overall sentence, outperforms using a flat role
representation for SRL based MT evaluation met-
rics.
7 Conclusion
We have shown how the MEANT family of SRL
based MT evaluation metrics is significantly im-
proved beyond the state-of-the-art for both HTER
and previous variants of MEANT, through the in-
troduction of a simple but well-motivated weight-
ing scheme to reflect the degree of contribution of
each semantic frame to the overall sentence trans-
lation. Following the assumption that a semantic
frame filled with more word tokens tends to express
more concepts, the new model weight each frame
by its span coverage. Consistent experimental re-
sults have been demonstrated under conditions uti-
lizing both human and automatic SRL. Under the
new frame weighted representation, properly nested
structured semantic frame representations regain an
empirically preferred position over the less intuitive
and linguistically unsatisfying flat role representa-
tions.
One future direction of this work will be to com-
pare MEANT against the feature based and string
based representations of semantic relations in ULC.
Such a comparison could yield a more complete
credit/blame perspective on the representationmodel
when operating under the condition of using auto-
matic SRL.
Another interesting extension of this work would
be to investigate the discriminative power of the
MEANT family of metrics to distinguish distances
in translation adequacy. In this paper we confirmed
that the MEANT family of metrics are stable in cor-
relation with human ranking judgments of transla-
tion adequacy. Further studies could focus on the
correlation of the MEANT family of metrics against
human scoring. We also plan to experiment on meta-
evaluating MEANT on a larger scale in other genres
and for other language pairs.
Acknowledgments
This material is based uponwork supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under GALE Contract Nos. HR0011-06-
C-0022 and HR0011-06-C-0023 and by the Hong
Kong Research Grants Council (RGC) research
grants GRF621008, GRF612806, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In
43th AnnualMeeting of the Association of Compu-
tational Linguistics (ACL-05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In 13th Confer-
18
ence of the European Chapter of the Association
for Computational Linguistics (EACL-06), pages
249?256, 2006.
Chris Callison-Burch, Philipp Koehn, Christof
Monz, Kay Peterson, Mark Pryzbocki, and Omar
Zaidan. Findings of the 2010 Joint Workshop
on Statistical Machine Translation and Metrics
for Machine Translation. In Joint 5th Workshop
on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, 15-16
July 2010.
G. Doddington. Automatic Evaluation of Machine
Translation Quality using N-gram Co-occurrence
Statistics. In 2nd International Conference on Hu-
man Language Technology Research (HLT-02),
pages 138?145, San Francisco, CA, USA, 2002.
Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu?is Ma`rquez. Linguistic Fea-
tures for Automatic Evaluation of Heterogenous
MT Systems. In 2nd Workshop on Statistical Ma-
chine Translation, pages 256?264, Prague, Czech
Republic, June 2007. Association for Computa-
tional Linguistics.
Jesu?s Gime?nez and Llu?is Ma`rquez. A Smorgas-
bord of Features for Automatic MT Evaluation. In
3rd Workshop on Statistical Machine Translation,
pages 195?198, Columbus, OH, June 2008. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation be-
tween European Languages. In Workshop on
Statistical Machine Translation, pages 102?121,
2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Efficient MT Evaluation Using Block
Movements. In 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL-06), 2006.
Ding Liu and Daniel Gildea. Syntactic Features for
Evaluation of Machine Translation. In ACLWork-
shop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summariza-
tion, page 25, 2005.
Chi-Kiu Lo and Dekai Wu. Evaluating Machine
Translation Utility via Semantic Role Labels. In
7th International Conference on Language Re-
sources and Evaluation (LREC-2010), 2010.
Chi-Kiu Lo and Dekai Wu. Semantic vs. Syntac-
tic vs. N-gram Structure for Machine Translation
Evaluation. In Proceedings of the 4th Workshop
on Syntax and Structure in Statistical Translation
(SSST-4), 2010.
Chi-Kiu Lo and Dekai Wu. MEANT: An Inexpen-
sive, High-Accuracy, Semi-Automatic Metric for
Evaluating Translation Utility based on Seman-
tic Roles. In Joint conference of the 49th Annual
Meeting of the Association for Computational Lin-
guistics : Human Language Technologies (ACL
HLT 2011), 2011.
Chi-Kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
To appear in 22nd International Joint Conference
on Artificial Intelligence, 2011.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. A Evaluation Tool for Machine
Translation: Fast Evaluation for MT Research. In
2nd International Conference on Language Re-
sources and Evaluation (LREC-2000), 2000.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Chris Manning. Robust Machine Translation
Evaluation with Entailment Features. In Joint
conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics and the
4th International Joint Conference on Natural
Language Processing of the Asian Federation of
Natural Language Processing (ACL-IJCNLP-09),
2009.
Martha Palmer, Daniel Gildea, and Paul Kings-
bury. The Proposition Bank: an Annotated Cor-
pus of Semantic Roles. Computational Linguis-
tics, 31(1):71?106, 2005.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-02), pages 311?318,
2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
19
In 2004 Conference on Human Language Tech-
nology and the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL-04), 2004.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A Study of
Translation Edit Rate with Targeted Human An-
notation. In 7th Conference of the Association for
Machine Translation in the Americas (AMTA-06),
pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Arkaitz Zubiaga, and Hassan Sawaf. Accelerated
DP Based Search For Statistical Translation. In
5th European Conference on Speech Communica-
tion and Technology (EUROSPEECH-97), 1997.
20
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 243?252,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Fully Automatic Semantic MT Evaluation
Chi-kiu LO, Anand Karthik TUMULURU and Dekai WU
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo,aktumuluru,dekai}@cs.ust.hk
Abstract
We introduce the first fully automatic, fully seman-
tic frame based MT evaluation metric, MEANT,
that outperforms all other commonly used auto-
matic metrics in correlating with human judgment
on translation adequacy. Recent work on HMEANT,
which is a human metric, indicates that machine
translation can be better evaluated via semantic
frames than other evaluation paradigms, requiring
only minimal effort from monolingual humans to an-
notate and align semantic frames in the reference and
machine translations. We propose a surprisingly ef-
fective Occam?s razor automation of HMEANT that
combines standard shallow semantic parsing with
a simple maximum weighted bipartite matching al-
gorithm for aligning semantic frames. The match-
ing criterion is based on lexical similarity scoring
of the semantic role fillers through a simple con-
text vector model which can readily be trained us-
ing any publicly available large monolingual cor-
pus. Sentence level correlation analysis, following
standard NIST MetricsMATR protocol, shows that
this fully automated version of HMEANT achieves
significantly higher Kendall correlation with hu-
man adequacy judgments than BLEU, NIST, ME-
TEOR, PER, CDER, WER, or TER. Furthermore,
we demonstrate that performing the semantic frame
alignment automatically actually tends to be just as
good as performing it manually. Despite its high
performance, fully automated MEANT is still able
to preserve HMEANT?s virtues of simplicity, repre-
sentational transparency, and inexpensiveness.
1 Introduction
We introduce the first fully automatic semantic-frame-
based MT evaluation metric capable of outperforming
all other commonly used automatic metrics like BLEU,
NIST, METEOR, PER, CDER, WER, and TER for eval-
uating translation adequacy. This work, MEANT, can be
seen as a fully automated version of HMEANT, which is
a human metric, introduced by Lo and Wu (2011b). De-
spite its high performance, MEANT is still able to pre-
serve HMEANT?s virtues of Occam?s razor simplicity,
representational transparency, and inexpensiveness.
For the past decade, MT evaluation has relied heavily
on inexpensive automatic metrics such as BLEU (Pap-
ineni et al, 2002), NIST (Doddington, 2002), METEOR
(Banerjee and Lavie, 2005), PER (Tillmann et al, 1997),
CDER (Leusch et al, 2006), WER (Nie?en et al, 2000),
and TER (Snover et al, 2006). In large part, this is be-
cause automatic metrics significantly shorten the evalua-
tion cycle by providing a fast, easy and cheap quantita-
tive evaluation which can be effectively incorporated into
modern SMT training methods.
Despite the fact that HMEANT, a human metric re-
cently proposed by Lo and Wu (2011b,c,d), was shown
to reflect translation adequacy more accurately than all
of these automatic metrics, it is unfortunately infeasible
to incorporate the HMEANT metrics directly into SMT
training methods, due to the non-automatic processes of
(1) semantic parsing and (2) aligning semantic frames.
In this paper we introduce an automatic metric in which
both the semantic parsing and the alignment of semantic
frames are fully automated. Our aim is to show that even
with full automation, this new metric still outperforms all
the previous automatic metrics mentioned, thus provid-
ing a foundation for future incorporation into the training
of SMT to drive system improvements in providing more
adequate translation output.
N-gram oriented automatic MT evaluation metrics like
BLEU perform well at capturing translation fluency, and
ranking overall systems with respect to each other when
their scores are averaged over entire documents or cor-
pora. However, they do not fare so well in ranking trans-
lations of individual sentences. As MT systems improve,
the n-gram based evaluation metrics have begun to show
their limits. State-of-the-art MT systems are often able to
output translations containing roughly the correct words,
while failing to convey important aspects of the meaning
of the input sentence. Cases where BLEU strongly dis-
agrees with human judgment of translation quality were
243
reported in large scale MT evaluation tasks by Callison-
Burch et al (2006) and Koehn and Monz (2006).
Motivated by the goal of addressing the weaknesses
of n-gram oriented automatic MT evaluation metrics at
evaluating translation adequacy, the HMEANT metric
assesses translation utility by matching the basic event
structure??who did what to whom, when, where and
why? (Pradhan et al, 2004)?representing the central
meaning conveyed by sentences. As mentioned above,
however, HMEANT requires humans to manually anno-
tate semantic frames in the reference and machine trans-
lations, and then to align the semantic frames?making
it difficult to incorporate HMEANT as an objective func-
tion in the MT system training, evaluating, and optimiz-
ing cycle.
We argue in this paper that both the human seman-
tic parsing and the semantic frame alignment tasks per-
formed within HMEANT can be successfully automated
to produce a state-of-the-art automatic metric. Moreover,
we show that the spirit of Occam?s razor can be preserved
even for the semantic frame alignment, by demonstrating
the effectiveness of a simple maximum weighted bipar-
tite matching algorithm based on the lexical similarity be-
tween semantic frames. In addition, we show empirically
that performing this semantic frame alignment automati-
cally tends to be just as good as performing it manually.
Our results indicate that MEANT, the fully automatic
version of HMEANT, achieves levels of correlation with
human adequacy judgment (in our experiments, approx-
imately 0.37) which significantly outperforms the com-
monly used automatic metrics BLEU, NIST, METEOR,
PER, CDER, WER, and TER (in our experiments, rang-
ing between 0.20 and 0.29).
2 Related Work
2.1 Automatic lexical similarity based metrics
BLEU (Papineni et al, 2002) remains the most widely
used MT evaluation metric despite the fact that a num-
ber of large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) report cases where it
strongly disagrees with human judgments of translation
accuracy. Other lexical similarity based automatic MT
evaluation metrics, like NIST (Doddington, 2002), ME-
TEOR (Banerjee and Lavie, 2005), PER (Tillmann et al,
1997), CDER (Leusch et al, 2006), WER (Nie?en et al,
2000), and TER (Snover et al, 2006), also perform well
in capturing translation fluency, but share the same prob-
lem that although evaluation with these metrics can be
done very quickly at low cost, their underlying assump-
tionthat a good translation is one that shares the
same lexical choices as the reference translationis not
justified semantically. Lexical similarity does not ade-
quately reflect similarity in meaning.
Generating a translation that contains roughly the cor-
rect words may be necessary?but is far from sufficient?
to preserve the essence of the meaning. We argue that a
translation metric that reflects meaning similarity needs
to be based on similarity of semantic structure, and not
merely flat lexical similarity.
2.2 HMEANT (human SRL based metric)
As mentioned above, despite the fact that the semi-
automatic HMEANT metric recently proposed by Lo and
Wu (2011b,c,d) shows a higher correlation with human
adequacy judgments than all commonly used automatic
MT evaluation metrics, as with other human metrics like
HTER (Snover et al, 2006), it is unfortunately infeasible
to incorporate the HMEANT metrics directly into SMT
training methods. HMEANT requires non-automatic
manual steps of (1) semantic parsing and (2) aligning
semantic frames. Monolingual (or bilingual) annotators
must label the semantic roles in both the reference and
machine translations, and then to align the semantic pred-
icates and role fillers in the MT output to the reference
translations. These annotations allow HMEANT to then
look at the aligned role fillers, and aggregate the trans-
lation accuracy for each role. In the spirit of Occam?s
razor and representational transparency, the HMEANT
score is defined simply in terms of a weighted f-score
over these aligned predicates and role fillers. More pre-
cisely, HMEANT is defined as follows:
1. Human annotators annotate the shallow semantic
structures of both the references and MT output.
2. Human judges align the semantic frames between
the references and MT output by judging the cor-
rectness of the predicates.
3. For each pair of aligned semantic frames,
(a) Human judges determine the translation cor-
rectness of the semantic role fillers.
(b) Human judges align the semantic role fillers
between the reference and MT output accord-
ing to the correctness of the semantic role
fillers.
4. Compute the weighted f-score over the matching
role labels of these aligned predicates and role
fillers.
mi ?
#tokens filled in aligned frame i of MT
total #tokens in MT
ri ?
#tokens filled in aligned frame i of REF
total #tokens in REF
Mi,j ? total # ARG j of aligned frame i in MT
Ri,j ? total # ARG j of aligned frame i in REF
Ci,j ? # correct ARG j of aligned frame i in MT
Pi,j ? # partially correct ARG j of aligned frame i in MT
244
Figure 1: Examples of human semantic frame annotation. Semantic parses of the Chinese input and the English reference trans-
lation are from the Propbank gold standard. The MT output is semantically parsed by monolingual lay annotators according to the
HMEANT guidelines. There are no semantic frames for MT3 because there is no predicate.
precision =
?
i mi
wpred+
?
j wj(Ci,j+wpartialPi,j)
wpred+
?
j wjMi,j
?
i mi
recall =
?
i ri
wpred+
?
j wj(Ci,j+wpartialPi,j)
wpred+
?
j wjRi,j
?
i ri
where mi and ri are the weights for frame, i, in the
MT/REF respectively. These weights estimate the degree
of contribution of each frame to the overall meaning of
the sentence. Mi,j and Ri,j are the total counts of argu-
ment of type j in frame i in the MT and REF respec-
tively. Ci,j and Pi,j are the count of the correctly and
partial correctly translated argument of type j in frame i
in the MT output. Figure 1 shows examples of human se-
mantic frame annotation on reference and machine trans-
lations as used in HMEANT. Table 1 shows examples of
human judges? decisions for semantic frame alignment
and translation correctness for each semantic roles, for
the ?MT2? output from Figure 1.
Unlike HMEANT, MEANT is fully automatic; but
nevertheless, it adheres to HMEANT?s principles of Oc-
cam?s razor simplicity and representational transparency.
These properties crucially facilitate error analysis and
credit/blame assignment that are invaluable for MT sys-
tem modeling.
Furthermore, being fully automatic, MEANT is even
less expensive than HMEANT, which was already shown
by Lo and Wu (2011b,c,d) to be significantly less ex-
pensive than HTER. This makes MEANT a much bet-
ter candidate than HMEANT for future incorporation into
the automatic training of SMT systems to drive improve-
ments in translation adequacy.
2.3 Semantic role labels as features in aggregate
metrics
Gime?nez and Ma`rquez (2007, 2008) introduced ULC, an
automatic MT evaluation metric that aggregates many
types of features, including several shallow semantic sim-
ilarity features. However, unlike Lo and Wu (2011b),
245
Table 1: Example of SRL annotation for the MT2 output from figure 1 along with the human judgements of translation correctness
for each argument. *Notice that although the decision made by the human judge for ?in mainland China? in the reference translation
and ?the mainland of China? in MT2 is ?correct?, nevertheless the HMEANT computation will not count this as a match since their
role labels do not match.
REF roles REF MT2 roles MT2 decision
PRED ceased Action stop match
ARG0 their sale ? ? incorrect
ARGM-LOC in mainland China Agent the mainland of China correct*
ARGM-TMP for almost two months Temporal nearly two months correct
? ? Experiencer SK - 2 products incorrect
PRED resumed Action resume match
ARG0 sales of complete range of SK
- II products
Experiencer in the mainland of China to
stop selling nearly two months
of SK - 2 products sales
incorrect
ARGM-TMP Until after , their sales had
ceased in mainland China for
almost two months
Temporal So far partial
ARGM-TMP now ? ? incorrect
the ULC representation is based on flat semantic role
label features that do not capture the structural rela-
tions in semantic frames, i.e., the predicate-argument re-
lations. Also unlike HMEANT, which weights each se-
mantic role type according to its empirically determined
relative importance to the adequate preservation of mean-
ing, ULC uses uniform weights. Although the automatic
ULC metric shows an improved correlation with human
judgment of translation quality (Callison-Burch et al,
2007; Gime?nez and Ma`rquez, 2007; Callison-Burch et
al., 2008; Gime?nez and Ma`rquez, 2008), it is not com-
monly used in large-scale MT evaluation campaigns, per-
haps due to its high time cost and/or the difficulty of in-
terpreting its score because of its highly complex combi-
nation of many heterogeneous types of features.
Like system combination approaches, ULC is a vastly
more complex aggregate metric compared to widely used
metrics like BLEU. We believe it is important for auto-
matic semantic MT evaluation metrics to provide rep-
resentational transparency via simple, clear, and trans-
parent scoring schemes that are (a) easily human read-
able to support error analysis, and (b) potentially directly
usable for automatic credit/blame assignment in tuning
tree-structured SMT systems.
3 MEANT: A fully automatic semantic
MT evaluation metric
Like HMEANT, our guiding principle is that a good
translation is one that is useful, in the sense that hu-
man readers may successfully understand at least the ba-
sic event structurewho did what to whom, when, where
and why (Pradhan et al, 2004)representing the central
meaning of the source utterances. Whereas HMEANT
measures this using a f-score of correctly translated
semantic roles in MT output that are annotated and
compared by monolingual human annotators, MEANT
automates HMEANT as follows (the differences from
HMEANT are italicized):
1. Apply an automatic shallow semantic parser on both
the references and MT output.
2. Apply maximum weighted bipartite matching algo-
rithm to align the semantic frames between the ref-
erences and MT output by the lexical similarity of
the predicates.
3. For each pair of aligned semantic frames,
(a) Lexical similarity scores determine the similar-
ity of the semantic role fillers.
(b) Apply maximum weighted bipartite matching
algorithm to align the semantic role fillers be-
tween the reference and MT output according
to their lexical similarity.
4. Compute the weighted f-score over the matching
role labels of these aligned predicates and role
fillers.
3.1 Automatic semantic parsing
To automate the process of human semantic role label-
ing, we apply an automatic shallow semantic parser on
both the reference and MT output that takes the raw trans-
lation as input and outputs the corresponding predicate-
argument structure. We choose to semantically parse the
translation independently, instead of inducing the parses
246
Figure 2: Examples of automatic shallow semantic parses. The Chinese input is parsed by a Chinese automatic shallow semantic
parser. The English reference and machine translations are parsed by an English automatic shallow semantic parser. There are no
semantic frames for mt3 since there is no predicate.
from the input, because it captures the raw meaning con-
veyed in the translation rather than predicting the mean-
ing conveyed in the translation from the input. Figure 2
shows examples of automatic shallow semantic parses on
both reference and machine translations.
3.2 Automatic semantic frame alignment
After reconstructing the shallow semantic parse, the man-
ual semantic frame alignment process is automated by
applying the maximum weighted bipartite matching algo-
rithm where the weights of the edges represent the lexical
similarity of the predicates. A wide range of lexical sim-
ilarity measures are available to us, including for exam-
ple BLEU, METEOR, cosine similarity based on context
vector models (Dagan, 2000), and so forth. In Section
4, we will show the performance of the fully automatic
semantic MT evaluation metric, MEANT ,couple with
different lexical similarity metrics and other commonly
used automatic MT evaluation metrics. In Section 6, we
will discuss aligning the semantic frames according to all
semantic role fillers, instead of the predicates only.
Then, for each pair of aligned semantic frames, we es-
timate the similarity of the semantic role fillers by sum-
ming all the lexical similarity of all the pairwise combi-
nation of tokens between the references and MT output.
After obtaining the similarity of the semantic role fillers,
we again apply the maximum weighted bipartite match-
ing algorithm to align the semantic role fillers between
the references and MT output. Table 2 shows examples
of the human judges? decisions on semantic frame align-
ment and translation correctness for each semantic role in
the ?MT2? output from Figure 2.
3.3 Scoring the semantic similarity
After aligning the semantic frames automatically, the
computation of the MEANT score is largely the same as
stated in Lo and Wu (2011d), except that we now replace
the counts of correctly and partially correctly translated
semantic role fillers by the similarity scores of the predi-
cates and arguments between the references and MT out-
put.
mi ?
#tokens filled in aligned frame i of MT
total #tokens in MT
ri ?
#tokens filled in aligned frame i of REF
total #tokens in REF
Mi,j ? total # ARG j of aligned frame i in MT
Ri,j ? total # ARG j of aligned frame i in REF
Si,pred ? sim. of pred of REF and MT in aligned frame i
Si,j ? sim. of ARG j of REF and MT in aligned frame i
precision =
?
i mi
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjMi,j
?
i mi
recall =
?
i ri
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjRi,j
?
i ri
247
Table 2: Automatic semantic frame alignment of the MT2 output from figure 2, along with the automatic lexical similarity scoring
on translation correctness for each argument.
REF roles REF MT2 roles MT2 similarity
PRED ceased PRED stop 0.0377
ARG0 their sales ? ? ?
ARGM-LOC in mainland China ? ? ?
ARGM-TMP for almost two months ? ? ?
? ? PRED selling ?
? ? ARG1 nearly two months of SK ?
PRED resumed PRED resumed 1.0
ARG1 sales of complete range of SK
- II products
ARG1 2 products sales 0.0836
ARGM-TMP now ARGM-TMP So far 0.0459
where mi, ri, Mi,j, Ri,j are defined the same as in
HMEANT, and Si,pred and Si,j are the lexical similarities
(BLEU, METEOR, cosine similarity based on a context
vector model, and so on, as discussed in the following
section) of the predicates and arguments of type j be-
tween the reference translations and the MT output.
4 MEANT outperforms all automatic
metrics
We will first show that the fully automatic semantic MT
evaluation metric, MEANT, outperforms all the other
commonly used automatic metrics.
4.1 Experimental setup
For assessing lexical similarity, a wide range of lexi-
cal similarity scoring models are available. We describe
a representative subset of a wide range of experiments
we have performed using all the most typical and com-
monly used measures. On one hand, we report experi-
ments with integrating two commonly used MT evalua-
tion metrics, BLEU and METEOR, as the lexical simi-
larity. On the other hand, we also report experiments
on integrating two common similarity measures?cosine
similarity measure and min/max with mutual information
(Dagan, 2000)?that are based on context vector models,
and trained from the Gigaword corpus with window sizes
of 3 and 5.
The cosine similarity between two sequences of word
tokens, ??u and ??v , is defined as follows:
??wx = context vector of word token x
wxi = attribute i of context vector?
??wx
f(x,wxi) =
count(x,wxi)
count (wxi)
cosine(x, y) =
?
i
f(x,wxi)? f(y, wyi)
?
?
i
f(x,wxi)
2
?
?
i
f(y, wyi)
2
cosine(??u ,??v ) = ?
i
?
j
cosine(ui, vj)
Using the same definition of wxi, the min/max with
mutual information similarity between two sequences of
word tokens, ??u and ??v , is defined as follows:
P (wxi ? x) =
count(x,wxi)?
i count(x,wxi)
P (wxi) =
?
y count(y, wxi)
?
y
?
j count(y, wxj)
MI(x,wxi) = log
(
P (wxi ? x)
P (wxi)
)
MinMax-MI(x, y) =
?
i
min (MI(x,wxi),MI(y, wyi ))
?
i
max (MI(x,wxi),MI(y, wyi ))
MinMax-MI(??u ,??v ) = ?
i
?
j
MinMax-MI(ui, vj)
For our benchmark comparison, the evaluation data
for our experiments is the same two sets of sentences,
GALE-A and GALE-B that were used in Lo and Wu
(2011d), where GALE-A is used for estimating the
weight parameters of the metric by optimizing the cor-
relation with human adequacy judgment, and then the
learned weights are applied to testing on GALE-B.
For the automatic semantic role labeling, we used the
publicly available off-the-shelf shallow semantic parser,
ASSERT (Pradhan et al, 2004).
The correlation with human adequacy judgments on
sentence-level system ranking is assessed by the stan-
dard NIST MetricsMaTr procedure (Callison-Burch et
al., 2010) using Kendall correlation coefficients.
248
Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing all
commonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metric
integrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and
(d) MinMax with mutual information.
GALE-A (training) GALE-B (testing)
Human metrics
HMEANT 0.49 0.27
HTER 0.43 0.20
Automatic metrics
MEANT ? ?
- with MinMax-MI on context vector model of window size 3 0.37 0.19
- with MinMax-MI on context vector model of window size 5 0.37 0.17
- with Cosine on context vector model of window size 3 0.32 0.13
- with Cosine on context vector model of window size 5 0.30 0.08
- with METEOR 0.17 ?
- with BLEU 0.00 ?
METEOR 0.20 0.21
NIST 0.29 0.09
TER 0.20 0.10
BLEU 0.20 0.12
PER 0.20 0.07
WER 0.10 0.11
CDER 0.12 0.10
4.2 Results
Table 3 shows that MEANT significantly outperforms all
the other automatic MT evaluation metrics when inte-
grated with a simple similarity measure based on word
context vectors trained from a large monolingual corpus.
We can also see that using min/max with mutual infor-
mation is significantly better than using cosine similarity.
Furthermore, context vector models using a window size
of 3 appear to be as good or better than those using a
window size of 5.
Although the human metrics, HMEANT and HTER,
obviously remain superior, MEANT performs far better
than almost all other automatic metrics. The only excep-
tion is the GALE-B dataset, where METEOR performs
marginally better than MEANT and even HTER. Data
analysis shows that the marginally higher correlation of
METEOR on the GALE-B dataset is a statistical outlier;
it is quite rare for a lexically based automatic metric to
outperform even the human-driven HTER metric.
Interestingly and somewhat surprisingly, using the n-
gram based MT evaluation metrics BLEU and METEOR
as lexical similarity scores does not work well at all for
this purpose, even on the training data (thus obviating the
need to obtain results on the testing data). Analysis in-
dicates that the reason for this is that variation between
alternative paraphrasing of the role fillers makes the num-
ber of matching n-grams quite small, since there are many
synonyms and few exact consecutive n-gram matches.
Table 4: Sentence-level correlation with human adequacy judg-
ment on GALE-A (training) and GALE-B (testing) for aligning
sematnic frame automatically and manually.
Semantic frame alignment GALE-A GALE-B
Automatic 0.37 0.19
Manual 0.35 0.17
In the following sections, we turn to considering sev-
eral questions that naturally arise following these strong
results.
5 Don?t align semantic frames manually
One obvious question is whether the automatic alignment
of semantic frames degrades MEANT?s accuracy, and if
so, the extent to which it hurts.
5.1 Experimental setup
To test this question, we compare the best fully automatic
results of the previous section against a semi-automatic
variant of our proposed metric. In the semi-automatic
variant, the semantic parsing is still performed automati-
cally. However, the semantic frame alignment is instead
done manually by human annotators.
The rest of the experimental setup is the same as that
used in Section 4.
249
5.2 Results
Table 4 shows that performing the alignment of semantic
frames automatically is as good?or even better than?
doing the alignment manually. We believe the success of
automatic semantic frame alignment reflects the high de-
gree of reliability of our chosen lexical similarity metric,
when the candidates for role fillers are restricted to the
fairly small set defined by the sentence pairs.
6 Look only at predicates when aligning
semantic frames
Given the positive results of the previous sections, it is
worth asking a deeper question: would it further improve
the correlation with human adequacy judgment of the
metric if the semantic frames were aligned not only by
matching predicates (as HMEANT did), but in addition
by trying to also maximize the match of the semantic role
fillers?
The reason to revisit this question is that even though
Lo and Wu (2011a) showed that in the case of HMEANT
it is effective for human annotators to align semantic
frames according to the predicates only, this could eas-
ily be due to the mental challenge for lay annotators to
compare and keep in mind all the semantic role fillers at
the same time. But in the case of a fully automatic metric,
on the other hand, it is easy for an algorithm to compute
the individual similarities between all the semantic role
fillers and consider the aggregate similarity when opti-
mizing the alignment of semantic frames.
Surprisingly, however, the results will show that even
in the automated case, this still does not help improve the
correlation with human adequacy judgments.
6.1 Experimental setup
To align semantic frames using all semantic roles, we
aggregate the lexical similarity of all the semantic role
fillers into a semantic frame similarity score. We exper-
iment on two variations of the aggregation function (1)
simple linear average of the lexical similarity over the
number of aligned semantic roles in the frames; or (2) the
inverse of the sum of the negative log of the role fillers
similarity.
The rest of the experimental setup is the same as that
used in Section 4.
6.2 Results
Table 5 shows that to align semantic frames, using only
the lexical similarity of the predicates between the frames
in the reference translations and the MT output (0.37
Kendall in GALE-A and 0.19 Kendall in GALE-B) is
more robust than either of the two natural ways of ag-
gregating the lexical similarity of the aligned semantic
role fillers. Aggregating by linear average yields a lower
Table 5: Sentence-level correlation with human adequacy judg-
ments on GALE-A (training set) and GALE-B (testing set) for
aligning semantic frames using predicate only vs. using all se-
mantic role fillers aggregated by (1) the linear average of the
lexical similarity vs. (2) the inverse of the sum of negative log
of the lexical similarity.
Frame alignment GALE-A GALE-B
Predicate only 0.37 0.19
Linear average 0.35 0.10
Inverse of sum of neg. log 0.30 0.17
0.35 Kendall in GALE-A and 0.10 Kendall in GALE-B.
Aggregating by the inverse of the sum of negative logs
yields a lower 0.30 Kendall in GALE-A and 0.17 Kendall
in GALE-B.
What might explain this perhaps surprising result? Our
conjecture is that aggregating the lexical similarities of
the semantic role fillers fails to help find better seman-
tic frame alignments because the lexical similarities are
aggregated with uniform weight across different types of
role fillers. Therefore, the aggregation ignores the fact
that different types of role types contribute to a widely
varying degree to the meaning of an entire semantic
frame?in reality, some role types are much more impor-
tant than others. However, the complexity of the met-
ric would be greatly increased if we added weights for
each semantic roles type for semantic frame alignment
process, and this would not be likely to be worthwhile
given that automatic alignment is already performing as
well as human alignment of semantic frames.
7 Don?t word align semantic role fillers
Another question that naturally arises from the positive
results above is: when aligning the semantic frames,
would word-aligning the tokens within role fillers help?
Specifically, if we had word alignments for every candi-
date pair of role filler strings, we could sum the lexical
similarities only between the aligned tokens?instead of
what we did above, which was to sum the lexical similar-
ities of all pairwise combinations of tokens.
However, experimental results will show that, surpris-
ingly, to judge the similarity of semantic role fillers,
summing the lexical similarities over only word-aligned
tokens?instead of all pairwise combinations of tokens?
does not help to improve the correlation of the semantic
MT evaluation with human adequacy judgment.
7.1 Experimental setup
To avoid the danger of aligning a token in one segment
to excessive numbers of tokens in the other segment,
we adopt a variant of competitive linking by Melamed
(1996). Competitive linking is a greedy best-first word
alignment algorithm.
250
Table 6: Sentence-level correlation with human adequacy judg-
ments on GALE-A (training set) and GALE-B (testing set) for
judging semantic role fillers similarity using pairwise tokens vs.
only aligned tokens.
Semantic role filler similarity GALE-A GALE-B
All pairwise tokens 0.37 0.19
Only aligned tokens 0.36 0.17
The rest of the experimental setup is the same as that
used in Section 4.
7.2 Results
Table 6 shows that, surprisingly, judging semantic role
filler similarity using only the aligned tokens (selected
by competitive linking word alignment algorithm) does
not help the correlation with human adequacy judgment.
This is surprising as, intuitively, using only the aligned
tokens should avoid the introduction of noise in judg-
ing the similarity between semantic role fillers because
it avoids adding in similarities for words within semantic
role fillers whose meanings are not close to each other.
How might this outcome be explained? We conjecture
that the word alignments over-constrain the calculation
of segment similarities. The individual lexical similari-
ties are already weighted fairly accurately, so the lexical
similarities between words that do not correspond do not
hurt since they are already close to zero. On the other
hand, in cases where the word alignment is ambiguous,
it is better to aggregate over different possible pairwise
alignments?strictly obeying a hard word alignment un-
desirably forces dropping of some individual lexical sim-
ilarity scores that are actually relevant.
8 Conclusion
We have introduced a new fully automatic semantic MT
evaluation metric, MEANT, that is fundamentally based
on semantic frames, that is the first such metric to out-
perform all other commonly used automatic MT evalu-
ation metrics. Experimental results following the stan-
dard NIST MetricsMATR protocol indicate that our pro-
posed metric achieves levels of correlation with human
adequacy judgment (in our experiments, approximately
0.37) that significantly outperform BLEU, NIST, ME-
TEOR, PER, CDER, WER, and TER (in our experiments,
ranging between 0.20 and 0.29).
We have also shown in this paper that the spirit of Oc-
cam?s razor of HMEANT can be preserved even under
full automation by (1) replacing human semantic role an-
notation with automatic shallow semantic parsing and (2)
replacing human semantic frame alignment with a simple
maximum weighted bipartite matching algorithm based
on the lexical similarity between semantic frames. Under
analysis, we have further shown empirically that perform-
ing this semantic frame alignment automatically tends to
be just as good as performing it manually. Furthermore,
we have shown surprisingly that (1) for aligning seman-
tic frames, using only the similarity of predicates is more
accurate than also taking into account the similarity of se-
mantic role fillers, and (2) to judge similarity between se-
mantic role fillers, aggregating similarity of all pairwise
combination of word tokens is more accurate than con-
sidering only the similarity of the tokens that obey word
alignments.
Papineni et al (2002) stated in their conclusion that
?We believe that BLEU will accelerate the MT R&D cy-
cle by allowing researchers to rapidly home in on effec-
tive modeling ideas.? since fully automatic metrics allow
inexpensive training and tuning of SMT systems. Devel-
opments in the past decade have more than borne witness
to this statement. However, SMT has progressed to the
stage where simple metrics like BLEU are no longer ca-
pable of driving progress toward preservation of meaning
with respect to proper event structure. We believe that
MEANT that rapidly and accurately reflects the transla-
tion adequacy of MT output by directly assessing who did
what to whom, when, where and why is needed to bring
MT R&D to a new level of improvement in generating
more meaningful MT output.
Acknowledgments
This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-12-C-
0016, and GALE contract nos. HR0011-06-C-0022
and HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by the
Hong Kong Research Grants Council (RGC) research
grants GRF621008, GRF612806, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions, find-
ings and conclusions or recommendations expressed in
this material are those of the authors and do not necessar-
ily reflect the views of the RGC, EU, or DARPA.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An Automatic
Metric for MT Evaluation with Improved Correlation with
Human Judgments. In Proceedings of the 43th Annual Meet-
ing of the Association of Computational Linguistics (ACL-
05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-
evaluating the role of BLEU in Machine Translation Re-
search. In Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. (Meta-) evaluation of
251
Machine Translation. In Proceedings of the 2nd Workshop
on Statistical Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. Further Meta-evaluation
of Machine Translation. In Proceedings of the 3rd Workshop
on Statistical Machine Translation, pages 70?106, 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Pe-
terson, Mark Pryzbocki, and Omar Zaidan. Findings of the
2010 Joint Workshop on Statistical Machine Translation and
Metrics for Machine Translation. In Proceedings of the Joint
5th Workshop on Statistical Machine Translation and Met-
ricsMATR, pages 17?53, Uppsala, Sweden, 15-16 July 2010.
Ido Dagan. Contextual word similarity. In Robert Dale, Her-
man Moisl, and Harold Somers, editors, Handbook of Nat-
ural Language Processing, pages 459?476. Marcel Dekker,
New York, 2000.
G. Doddington. Automatic Evaluation of Machine Translation
Quality using N-gram Co-occurrence Statistics. In Proceed-
ings of the 2nd International Conference on Human Lan-
guage Technology Research (HLT-02), pages 138?145, San
Francisco, CA, USA, 2002. Morgan Kaufmann Publishers
Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. Linguistic Features for Au-
tomatic Evaluation of Heterogenous MT Systems. In Pro-
ceedings of the 2nd Workshop on Statistical Machine Trans-
lation, pages 256?264, Prague, Czech Republic, June 2007.
Association for Computational Linguistics.
Jesu?s Gime?nez and Llu??s Ma`rquez. A Smorgasbord of Features
for Automatic MT Evaluation. In Proceedings of the 3rd
Workshop on Statistical Machine Translation, pages 195?
198, Columbus, OH, June 2008. Association for Computa-
tional Linguistics.
Philipp Koehn and Christof Monz. Manual and Automatic
Evaluation of Machine Translation between European Lan-
guages. In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 102?121, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Effi-
cient MT Evaluation Using Block Movements. In Proceed-
ings of the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-06), 2006.
Chi-kiu Lo and Dekai Wu. A Radically Simple, Effective An-
notation and Alignment Methodology for Semantic Frame
Based SMT and MT Evaluation. In Proceedings of Interna-
tional Workshop on Using Linguistic Information for Hybrid
Machine Translation (LiHMT 2011), organized by OpenMT-
2., 2011.
Chi-kiu Lo and Dekai Wu. MEANT: An Inexpensive, High-
Accuracy, Semi-Automatic Metric for Evaluating Transla-
tion Utility based on Semantic Roles. In Proceedings of the
Joint conference of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics : Human Language Tech-
nologies (ACL-HLT-11), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How seman-
tic frames evaluate MT more accurately. In Proceedings of
the 22nd International Joint Conference on Artificial Intelli-
gence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Structured vs. Flat Semantic Role
Representations for Machine Translation Evaluation. In Pro-
ceedings of the 5th Workshop on Syntax and Structure in Sta-
tistical Translation (SSST-5), 2011.
I. Dan Melamed. Automatic construction of clean broad-
coverage translation lexicons. In Proceedings of the 2nd
Conference of the Association for Machine Translation in the
Americas (AMTA-1996), 1996.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Hermann
Ney. A Evaluation Tool for Machine Translation: Fast Eval-
uation for MT Research. In Proceedings of the 2nd Inter-
national Conference on Language Resources and Evaluation
(LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics (ACL-
02), pages 311?318, 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Mar-
tin, and Dan Jurafsky. Shallow Semantic Parsing Using Sup-
port Vector Machines. In Proceedings of the 2004 Confer-
ence on Human Language Technology and the North Amer-
ican Chapter of the Association for Computational Linguis-
tics (HLT-NAACL-04), 2004.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. A Study of Translation Edit
Rate with Targeted Human Annotation. In Proceedings of the
7th Conference of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zu-
biaga, and Hassan Sawaf. Accelerated DP Based Search
For Statistical Translation. In Proceedings of the 5th Euro-
pean Conference on Speech Communication and Technology
(EUROSPEECH-97), 1997.
252
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49?56,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Unsupervised vs. supervised weight estimation
for semantic MT evaluation metrics
Chi-kiu LO and Dekai WU
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo,dekai}@cs.ust.hk
Abstract
We present an unsupervised approach to esti-
mate the appropriate degree of contribution of
each semantic role type for semantic transla-
tion evaluation, yielding a semantic MT eval-
uation metric whose correlation with human
adequacy judgments is comparable to that of
recent supervised approaches but without the
high cost of a human-ranked training corpus.
Our new unsupervised estimation approach
is motivated by an analysis showing that the
weights learned from supervised training are
distributed in a similar fashion to the relative
frequencies of the semantic roles. Empiri-
cal results show that even without a training
corpus of human adequacy rankings against
which to optimize correlation, using instead
our relative frequency weighting scheme to
approximate the importance of each semantic
role type leads to a semantic MT evaluation
metric that correlates comparable with human
adequacy judgments to previous metrics that
require far more expensive human rankings of
adequacy over a training corpus. As a result,
the cost of semantic MT evaluation is greatly
reduced.
1 Introduction
In this paper we investigate an unsupervised ap-
proach to estimate the degree of contribution of each
semantic role type in semantic translation evalua-
tion in low cost without using a human-ranked train-
ing corpus but still yields a evaluation metric that
correlates comparably with human adequacy judg-
ments to that of recent supervised approaches as in
Lo and Wu (2011a, b, c). The new approach is
motivated by an analysis showing that the distri-
bution of the weights learned from the supervised
training is similar to the relative frequencies of the
occurrences of each semantic role in the reference
translation. We then introduce a relative frequency
weighting scheme to approximate the importance of
each semantic role type. With such simple weight-
ing scheme, the cost of evaluating translation of lan-
guages with fewer resources available is greatly re-
duced.
For the past decade, the task of measuring the per-
formance of MT systems has relied heavily on lex-
ical n-gram based MT evaluation metrics, such as
BLEU (Papineni et al, 2002), NIST (Doddington,
2002), METEOR (Banerjee and Lavie, 2005), PER
(Tillmann et al, 1997), CDER (Leusch et al, 2006)
and WER (Nie?en et al, 2000) because of their sup-
port on fast and inexpensive evaluation. These met-
rics are good at ranking overall systems by averaging
their scores over the entire document. As MT sys-
tems improve, the focus of MT evaluation changes
from generally reflecting the quality of each system
to assisting error analysis on each MT output in de-
tail. The failure of such metrics in evaluating trans-
lation quality on sentence level are becoming more
apparent. Though containing roughly the correct
words, the MT output as a whole sentence is still
quite incomprehensible and fails to express mean-
ing that is close to the input. Lexical n-gram based
evaluation metrics are surface-oriented and do not
do so well at ranking translations according to ad-
equacy and are particularly poor at reflecting sig-
nificant translation quality improvements on more
meaningful word sense or semantic frame choices
which human judges can indicate clearly. Callison-
Burch et al (2006) and Koehn and Monz (2006)
even reported cases where BLEU strongly disagrees
with human judgment on translation quality.
49
Liu and Gildea (2005) proposed STM, a struc-
tural approach based on syntax to addresses the fail-
ure of lexical similarity based metrics in evaluating
translation grammaticality. However, a grammatical
translation can achieve a high syntax-based score but
still contains meaning errors arising from confusion
of semantic roles. On the other hand, despite the
fact that non-automatic, manually evaluations, such
as HTER (Snover et al, 2006), are more adequacy
oriented and show a high correlation with human ad-
equacy judgment, the high labor cost prohibits their
widespread use. There was also work on explicitly
evaluating MT adequacy with aggregated linguistic
features (Gime?nez and Ma`rquez, 2007, 2008) and
textual entailment (Pado et al, 2009).
In the work of Lo and Wu (2011a), MEANT
and its human variants HMEANT were introduced
and empirical experimental results showed that
HMEANT, which can be driven by low-cost mono-
lingual semantic roles annotators with high inter-
annotator agreement, correlates as well as HTER
and far superior than BLEU and other surfaced ori-
ented evaluation metrics. Along with additional im-
provements to the MEANT family of metrics, Lo
and Wu (2011b) detailed the studies of the impact of
each individual semantic role to the metric?s corre-
lation with human adequacy judgments. Lo and Wu
(2011c) further discussed that with a proper weight-
ing scheme of semantic frame in a sentence, struc-
tured semantic role representation is more accurate
and intuitive than flattened role representation for se-
mantic MT evaluation metrics.
The recent trend of incorporating more linguistic
features into MT evaluation metrics raise the dis-
cussion on the appropriate approach in weighting
and combining them. ULC (Gime?nez and Ma`rquez,
2007, 2008) uses uniform weights to aggregate lin-
guistic features. This approach does not capture the
importance of each feature to the overall translation
quality to the MT output. One obvious example of
different semantic roles contribute differently to the
overall meaning is that readers usually accept trans-
lations with errors in adjunct arguments as a valid
translation but not those with errors in core argu-
ments. Unlike ULC, Liu and Gildea (2007); Lo and
Wu (2011a) approach the weight estimation prob-
lem by maximum correlation training which directly
optimize the correlation with human adequacy judg-
Figure 1: HMEANT structured role representation with a
weighting scheme reflecting the degree of contribution of
each semantic role type to the semantic frame. (Lo and
Wu, 2011a,b,c).
ments. However, the shortcomings of this approach
is that it requires a human-ranked training corpus
which is expensive, especially for languages with
limited resource.
We argue in this paper that for semantic MT eval-
uation, the importance of each semantic role type
can easily be estimated using a simple unsupervised
approach which leverage the relative frequencies of
the semantic roles appeared in the reference transla-
tion. Our proposed weighting scheme is motivated
by an analysis showing that the weights learned
from supervised training are distributed in a similar
fashion to the relative frequencies of the semantic
roles. Our results show that the semantic MT eval-
uation metric using the relative frequency weight-
ing scheme to approximate the importance of each
semantic role type correlates comparably with hu-
man adequacy judgments to previous metrics that
use maximum correlation training, which requires
expensive human rankings of adequacy over a train-
ing corpus. Therefore, the cost of semantic MT eval-
uation is greatly reduced.
2 Semantic MT evaluation metrics
Adopting the principle that a good translation is one
from which human readers may successfully un-
derstand at least the basic event structure-?who did
what to whom, when, where and why? (Pradhan et
al., 2004)-which represents the most essential mean-
ing of the source utterances, Lo and Wu (2011a,b,c)
50
proposed HMEANT to evaluate translation utility
based on semantic frames reconstructed by human
reader of machine translation output. Monolingual
(or bilingual) annotators must label the semantic
roles in both the reference and machine translations,
and then to align the semantic predicates and role
fillers in the MT output to the reference translations.
These annotations allow HMEANT to then look at
the aligned role fillers, and aggregate the transla-
tion accuracy for each role. In the spirit of Oc-
cam?s razor and representational transparency, the
HMEANT score is defined simply in terms of a
weighted f-score over these aligned predicates and
role fillers. More precisely, HMEANT is defined as
follows:
1. Human annotators annotate the shallow seman-
tic structures of both the references and MT
output.
2. Human judges align the semantic frames be-
tween the references and MT output by judging
the correctness of the predicates.
3. For each pair of aligned semantic frames,
(a) Human judges determine the translation
correctness of the semantic role fillers.
(b) Human judges align the semantic role
fillers between the reference and MT out-
put according to the correctness of the se-
mantic role fillers.
4. Compute the weighted f-score over the match-
ing role labels of these aligned predicates and
role fillers.
mi ?
#tokens filled in frame i of MT
total #tokens in MT
ri ?
#tokens filled in frame i of REF
total #tokens in REF
Mi, j ? total # ARG j of PRED i in MT
Ri, j ? total # ARG j of PRED i in REF
Ci, j ? # correct ARG j of PRED i in MT
Pi, j ? # partially correct ARG j of PRED i in MT
precision =
?i mi
wpred+? j w j(Ci, j+wpartialPi, j)
wpred+? j w jMi, j
?i mi
recall =
?i ri
wpred+? j w j(Ci, j+wpartialPi, j)
wpred+? j w jRi, j
?i ri
HMEANT =
2?precision? recall
precision+ recall
where mi and ri are the weights for frame,i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence. Mi, j and Ri, j are the to-
tal counts of argument of type j in frame i in the
MT/REF respectively. Ci, j and Pi, j are the count of
the correctly and partial correctly translated argu-
ment of type j in frame i in the MT. wpred is the
weight for the predicate and wj is the weights for the
arguments of type j. These weights estimate the de-
gree of contribution of different types of semantic
roles to the overall meaning of the semantic frame
they attached to. The frame precision/recall is the
weighted sum of the number of correctly translated
roles in a frame normalized by the weighted sum
of the total number of all roles in that frame in the
MT/REF respectively. The sentence precision/recall
is the weighted sum of the frame precision/recall for
all frames normalized by the weighted sum of the to-
tal number of frames in MT/REF respectively. Fig-
ure 1 shows the internal structure of HMEANT.
In the work of Lo and Wu (2011b), the correla-
tion of all individual roles with the human adequacy
judgments were found to be non-negative. There-
fore, grid search was used to estimate the weights
of each roles by optimizing the correlation with hu-
man adequacy judgments. This approach requires
an expensive human-ranked training corpus which
may not be available for languages with sparse re-
sources.Unlike the supervised training approach, our
proposed relative frequency weighting scheme does
not require additional resource other than the SRL
annotated reference translation.
3 Which roles contribute more in the
semantic MT evaluation metric?
We begin with an investigation that suggests that the
relative frequency of each semantic role (which can
be estimated in unsupervised fashion without human
rankings) approximates fairly closely its importance
as determined by previous supervised optimization
approaches. Since there is no ground truth on which
51
Role Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)
Agent -0.09 -0.05 0.03
Experiencer 0.23 0.05 0.02
Benefactive 0.02 0.04 -0.01
Temporal 0.11 0.08 0.03
Locative -0.05 -0.05 -0.07
Purpose -0.01 0.03 -0.01
Manner -0.01 0.00 -0.01
Extent -0.02 0.00 -0.01
Modal ? 0.04 0.01
Negation ? 0.01 -0.01
Other -0.12 0.05 -0.01
Table 1: Deviation of relative frequency from optimized weight of each semantic role in GALE-A, GALE-B and
WMT12
semantic role contribute more to the overall meaning
in a sentence for semantic MT evaluation, we first
show that the unsupervised estimation are close to
the weights obtained from the supervised maximum
correlation training on a human-ranked MT evalua-
tion corpus. More precisely, the weight estimation
function is defined as follows:
c j ? # count of ARG j in REF of the test set
w j =
c j
? j c j
3.1 Experimental setup
For our benchmark comparison, the evaluation data
for our experiment is the same two sets of sentences,
GALE-A and GALE-B that were used in Lo and Wu
(2011b). The translation in GALE-A is SRL an-
notated with 9 semantic role types, while those in
GALE-B are SRL annotated with 11 semantic role
types (segregating the modal and the negation roles
from the other role).
To validate whether or not our hypothesis is lan-
guage independent, we also construct an evalua-
tion data set by randomly selecting 50 sentences
from WMT12 English to Czech (WMT12) transla-
tion task test corpus, in which 5 systems (out of
13 participating systems) were randomly picked for
translation adequacy ranking by human readers. In
total, 85 sets of translations (with translations from
some source sentences appear more than once in dif-
ferent sets) were ranked. The translation in WMT12
are also SRL annotated with the tag set as GALE-B,
i.e., 11 semantic role types.
The weights wpred, w j and wpartial were estimated
using grid search to optimize the correlation against
human adequacy judgments.
3.2 Results
Inspecting the distribution of the trained weights and
the relative frequencies from all three data sets, as
shown in table 1, we see that the overall pattern of
weights from unsupervised estimation has a fairly
small deviation from the those learned via super-
vised optimization. To visualize more clearly the
overall pattern of the weights from the two estima-
tion methods, we show the deviation of the unsuper-
vised estimation from the supervised estimation. A
deviation of 0 for all roles would mean that unsu-
pervised and supervised estimation produce exactly
identical weights. If the unsupervised estimation is
higher than the supervised estimation, the deviation
will be positive and vice versa.
What we see is that in almost all cases, the de-
viation between the trained weight and the relative
frequency of each role is always within the range [-
0.1, 0.1].
Closer inspection also reveals the following more
detailed patterns:
? The weight of the less frequent adjunct argu-
ments (e.g. purpose, manner, extent, modal and
negation) from the unsupervised estimation is
highly similar to that learned from the super-
52
PRED estimation Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)
Method (i) 0.16 0.16 0.31
Method (ii) 0.02 0.01 0.01
Table 2: Deviation from optimized weight in GALE-A, GALE-B and WMT12 of the predicate?s weight as estimated
by (i) frequency of predicates in frames, relative to predicates and arguments; and (ii) one-fourth of agent?s weight.
vised maximum correlation training.
? The unsupervised estimation usually gives a
higher weight to the temporal role than the su-
pervised training would.
? The unsupervised estimation usually gives a
lower weight to the locative role than the super-
vised training would but the two weights from
the two approach are still high similar to each
other, yielding a deviation within the range of
[-0.07, 0.07].
? There is an obvious outlier found in GALE-A
where the deviation of the relative frequency
from the optimized weight is unusually high.
This suggests that the optimized weights in
GALE-A may be at the risk of over-fitting the
training data.
4 Estimating the weight for the predicate
The remaining question left to be investigated
is how we are to estimate the importance of the
predicate in an unsupervised approach. One obvious
approach is to treat the predicate the same way as
the arguments. That is, just like with arguments,
we could weight predicates by the relative fre-
quency of how often predicates occur in semantic
frames. However, this does not seem well motivated
since predicates are fundamentally different from
arguments: by definition, every semantic frame is
defined by one predicate, and arguments are defined
relative to the predicate.
On the other hand, inspecting the weights on the
predicate obtained from the supervised maximum
correlation training, we find that the weight of the
predicate is usually around one-fourth of the weight
of the agent role. More precisely, the two weight
estimation functions are defined as follows:
cpred ? # count of PRED in REF of the test set
Method (i) =
cpred
cpred +? j c j
Method (ii) = 0.25 ?wagent
We now show that the supervised estimation of
the predicate?s weight is closely approximated by
unsupervised estimation.
4.1 Experimental setup
The experimental setup is the same as that used in
section 3.
4.2 Results
The results in table 2 show that the trained weight
of the predicate and its unsupervised estimation of
one-fourth of the agent role?s weight are highly sim-
ilar to each other. In all three data sets, the devia-
tion between the trained weight and the heuristic of
one-fourth of the agent?s weight is always within the
range [0.1, 0.2].
On the other hand, treating the predicate the same
as arguments by estimating the unsupervised weight
using relative frequency largely over-estimates and
has a large deviation from the weight learned from
supervised estimation.
5 Semantic MT evaluation using
unsupervised weight estimates
Having seen that the weights of the predicate and
semantic roles estimated by the unsupervised ap-
proach fairly closely approximate those learned
from the supervised approach, we now show that the
unsupervised approach leads to a semantic MT eval-
uation metric that correlates comparably with hu-
man adequacy judgments to one that is trained on
a far more expensive human-ranked training corpus.
5.1 Experimental setup
Following the benchmark assessment in NIST Met-
ricsMaTr 2010 (Callison-Burch et al, 2010), we as-
sess the performance of the semantic MT evaluation
53
Metrics GALE-A GALE-B WMT12
HMEANT (supervised) 0.49 0.27 0.29
HMEANT (unsupervised) 0.42 0.23 0.20
NIST 0.29 0.09 0.12
METEOR 0.20 0.21 0.22
TER 0.20 0.10 0.12
PER 0.20 0.07 0.02
BLEU 0.20 0.12 0.01
CDER 0.12 0.10 0.14
WER 0.10 0.11 0.17
Table 3: Average sentence-level correlation with human adequacy judgments of HMEANT using supervised and
unsupervised weight scheme on GALE-A, GALE-B and WMT12, (with baseline comparison of commonly used
automatic MT evaluation metric.
metric at the sentence level using Kendall?s rank
correlation coefficient which evaluate the correla-
tion of the proposed metric with human judgments
on translation adequacy ranking. A higher the value
for indicates a higher similarity to the ranking by
the evaluation metric to the human judgment. The
range of possible values of correlation coefficient is
[-1,1], where 1 means the systems are ranked in the
same order as the human judgment and -1 means the
systems are ranked in the reverse order as the hu-
man judgment. For GALE-A and GALE-B, the hu-
man judgment on adequacy was obtained by show-
ing all three MT outputs together with the Chinese
source input to a human reader. The human reader
was instructed to order the sentences from the three
MT systems according to the accuracy of meaning in
the translations. For WMT12, the human adequacy
judgments are provided by the organizers.
The rest of the experimental setup is the same as
that used in section 3.
5.2 Results
Table 3 shows that HMEANT with the proposed un-
supervised semantic role weighting scheme corre-
late comparably with human adequacy judgments to
that optimized with a more expensive human-ranked
training corpus, and, outperforms all other com-
monly used automatic metrics (except for METEOR
in Czech). The results from GALE-A, GALE-B and
WMT12 are consistent. These encouraging results
show that semantic MT evaluation metric could be
widely applicable to languages other than English.
6 Conclusion
We presented a simple, easy to implement yet well-
motivated weighting scheme for HMEANT to esti-
mate the importance of each semantic role in eval-
uating the translation adequacy. Unlike the previ-
ous metrics, the proposed metric does not require
an expensive human-ranked training corpus and still
outperforms all other commonly used automatic MT
evaluation metrics. Interestingly, the distribution of
the optimal weights obtained by maximum correla-
tion training, is similar to the relative frequency of
occurrence of each semantic role type in the refer-
ence translation. HMEANT with the new weight-
ing scheme showed consistent results across differ-
ent language pairs and across different corpora in
the same language pair. With the proposed weight-
ing scheme, the semantic MT evaluation metric is
ready to be used off-the-shelf without depending on
a human-ranked training corpus. We believe that our
current work reduces the barrier for semantic MT
evaluation for resource scarce languages sufficiently
so that semantic MT evaluation can be applied to
most other languages.
Acknowledgments
We would like to thank Ondr?ej Bojar and all the
annotators from the Charles University in Prague
for participating in the experiments. This ma-
terial is based upon work supported in part by
the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-
12-C-0016, and GALE contract nos. HR0011-
06-C-0022 and HR0011-06-C-0023; by the Eu-
54
ropean Union under the FP7 grant agreement
no. 287658; and by the Hong Kong Research
Grants Council (RGC) research grants GRF621008,
GRF612806, DAG03/04.EG09, RGC6256/00E, and
RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the RGC, EU, or DARPA.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An Auto-
matic Metric for MT Evaluation with Improved Corre-
lation with Human Judgments. In Proceedings of the
43th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Machine
Translation Research. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL-06), pages 249?
256, 2006.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Pryzbocki, and Omar Zaidan.
Findings of the 2010 Joint Workshop on Statistical
Machine Translation and Metrics for Machine Transla-
tion. In Proceedings of the Joint 5th Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
17?53, Uppsala, Sweden, 15-16 July 2010.
G. Doddington. Automatic Evaluation of Machine Trans-
lation Quality using N-gram Co-occurrence Statistics.
In Proceedings of the 2nd International Conference
on Human Language Technology Research (HLT-02),
pages 138?145, San Francisco, CA, USA, 2002. Mor-
gan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. Linguistic Features
for Automatic Evaluation of Heterogenous MT Sys-
tems. In Proceedings of the 2nd Workshop on Sta-
tistical Machine Translation, pages 256?264, Prague,
Czech Republic, June 2007. Association for Computa-
tional Linguistics.
Jesu?s Gime?nez and Llu??s Ma`rquez. A Smorgasbord of
Features for Automatic MT Evaluation. In Proceed-
ings of the 3rd Workshop on Statistical Machine Trans-
lation, pages 195?198, Columbus, OH, June 2008. As-
sociation for Computational Linguistics.
Philipp Koehn and Christof Monz. Manual and Auto-
matic Evaluation of Machine Translation between Eu-
ropean Languages. In Proceedings of the Workshop on
Statistical Machine Translation, pages 102?121, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer:
Efficient MT Evaluation Using Block Movements. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), 2006.
Ding Liu and Daniel Gildea. Syntactic Features for Eval-
uation of Machine Translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, page 25, 2005.
Ding Liu and Daniel Gildea. Source-Language Fea-
tures and Maximum Correlation Training for Machine
Translation Evaluation. In Proceedings of the 2007
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (NAACL-07),
2007.
Chi-kiu Lo and Dekai Wu. MEANT: An Inexpensive,
High-Accuracy, Semi-Automatic Metric for Evaluat-
ing Translation Utility based on Semantic Roles. In
Proceedings of the Joint conference of the 49th Annual
Meeting of the Association for Computational Linguis-
tics : Human Language Technologies (ACL-HLT-11),
2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How se-
mantic frames evaluate MT more accurately. In Pro-
ceedings of the 22nd International Joint Conference
on Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Structured vs. Flat Semantic
Role Representations for Machine Translation Evalu-
ation. In Proceedings of the 5th Workshop on Syn-
tax and Structure in Statistical Translation (SSST-5),
2011.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. A Evaluation Tool for Machine Transla-
tion: Fast Evaluation for MT Research. In Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation (LREC-2000), 2000.
Sebastian Pado, Michel Galley, Dan Jurafsky, and Chris
Manning. Robust Machine Translation Evaluation
with Entailment Features. In Proceedings of the Joint
conference of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP-09), 2009.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: A Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-02), pages 311?318, 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.
Martin, and Dan Jurafsky. Shallow Semantic Parsing
Using Support Vector Machines. In Proceedings of
55
the 2004 Conference on Human Language Technology
and the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-04), 2004.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A Study of Trans-
lation Edit Rate with Targeted Human Annotation. In
Proceedings of the 7th Conference of the Association
for Machine Translation in the Americas (AMTA-06),
pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Arkaitz Zubiaga, and Hassan Sawaf. Accelerated
DP Based Search For Statistical Translation. In Pro-
ceedings of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH-97),
1997.
56
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 422?428,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
MEANT at WMT 2013: A tunable, accurate yet inexpensive
semantic frame based MT evaluation metric
Chi-kiu LO and Dekai WU
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|dekai}@cs.ust.hk
Abstract
The linguistically transparentMEANT and
UMEANT metrics are tunable, simple
yet highly effective, fully automatic ap-
proximation to the human HMEANT MT
evaluation metric which measures seman-
tic frame similarity between MT output
and reference translations. In this pa-
per, we describe HKUST?s submission
to the WMT 2013 metrics evaluation
task, MEANT and UMEANT. MEANT
is optimized by tuning a small number
of weights?one for each semantic role
label?so as to maximize correlation with
human adequacy judgment on a devel-
opment set. UMEANT is an unsuper-
vised version where weights for each se-
mantic role label are estimated via an in-
expensive unsupervised approach, as op-
posed to MEANT?s supervised method re-
lying on more expensive grid search. In
this paper, we present a battery of exper-
iments for optimizing MEANT on differ-
ent development sets to determine the set
of weights that maximize MEANT?s accu-
racy and stability. Evaluated on test sets
from the WMT 2012/2011 metrics evalua-
tion, bothMEANT and UMEANT achieve
competitive correlations with human judg-
ments using nothing more than a monolin-
gual corpus and an automatic shallow se-
mantic parser.
1 Introduction
We evaluate in the context of WMT 2013 the
MEANT (Lo et al, 2012) and UMEANT (Lo
andWu, 2012) semantic machine translation (MT)
evaluation metrics?tunable, simple yet highly ef-
fective, fully-automatic semantic frame based ob-
jective functions that score the degree of similarity
between the MT output and the reference transla-
tions via semantic role labels (SRL). Recent stud-
ies (Lo et al, 2013; Lo and Wu, 2013) show that
tuningMT systems againstMEANTmore robustly
improves translation adequacy, compared to tun-
ing against BLEU or TER.
In the past decade, the progress of machine
translation (MT) research is predominantly driven
by the fast and cheap n-gram based MT eval-
uation metrics, such as BLEU (Papineni et al,
2002), which assume that a good translation is one
that shares the same lexical choices as the ref-
erence translation. Despite enforcing fluency, it
has been established that these metrics do not en-
force translation utility adequately and often fail to
preserve meaning closely (Callison-Burch et al,
2006; Koehn and Monz, 2006). Unlike BLEU,
or other n-gram based MT evaluation metrics,
MEANT adopts at outset the principle that a good
translation is one from which the human readers
may successfully understand at least the central
meaning of the input sentence as captured by the
basic event structure??who did what to whom,
when, where and why?(Pradhan et al, 2004).
Lo et al (2012) show that MEANT correlates
better with human adequacy judgment than other
commonly used automatic MT evaluation metrics,
such as BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie,
2005), CDER (Leusch et al, 2006), WER (Nie?en
et al, 2000), and TER (Snover et al, 2006). Re-
cent studies (Lo et al, 2013; Lo andWu, 2013) also
show that tuning MT system against MEANT pro-
duces more robustly adequate translations on both
formal news text genre and informal web forum
or public speech genre compared to tuning against
BLEU or TER. These studies show thatMEANT is
a tunable and highly-accurate MT evaluation met-
ric that drives MT system development towards
higher utility.
As described in Lo and Wu (2011a), the pa-
422
rameters in MEANT, i.e. the weight for each se-
mantic role label, could be estimated using simple
grid search to optimize the correlation with human
adequacy judgments. Later, Lo and Wu (2012)
described an unsupervised approach for estimat-
ing the parameters of MEANT using relative fre-
quency of each semantic role label in the reference
translations under the situation when the human
judgments for the development set are unavailable.
In this paper, we refer the version of MEANT us-
ing the unsupervised approach of weight estima-
tion as UMEANT.
In this paper, we present a battery of exper-
iments for optimizing MEANT on different de-
velopment sets to determine the set of weights
that maximizes MEANT?s accuracy and stability.
Evaluated on the test sets ofWMT 2012/2011 met-
rics evaluation, MEANT and UMEANT achieve
a competitive correlation score with human judg-
ments by nothing more than a monolingual corpus
and an automatic shallow semantic parser.
2 Related work
2.1 Lexical similarity based metrics
N-gram or edit distance based metrics such as
BLEU (Papineni et al, 2002), NIST (Dodding-
ton, 2002), METEOR (Banerjee and Lavie, 2005),
CDER (Leusch et al, 2006), WER (Nie?en et
al., 2000), and TER (Snover et al, 2006) do not
correctly reflect the similarity of the basic event
structure? ?who did what to whom, when, where
and why?? of the input sentence. In fact, a
number of large scale meta-evaluations (Callison-
Burch et al, 2006; Koehn and Monz, 2006) report
cases where BLEU strongly disagrees with human
judgments of translation adequacy.
Although AMBER (Chen et al, 2012) shows a
high correlation with human adequacy judgment
(Callison-Burch et al, 2012) and claims to pre-
serve the simplicity of BLEU, the modifications it
incurred on BLEU through four different n-gram
matching strategies and several different penalties
makes it very hard to interpret and indicate what
errors the MT systems are making.
2.2 Linguistic feature based metrics
ULC (Gim?nez and M?rquez, 2007, 2008) is
an automatic metric that incorporates several se-
mantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al, 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al, 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an SMT system using a
pure form of ULC perhaps due to its expensive run
time. Lambert et al (2006) did tune on QUEEN,
a simplified version of ULC that discards the se-
mantic features of ULC and is based on pure lexi-
cal similarity. Therefore, QUEEN suffers from the
problem of failing to reflect translation adequacy
similar to other n-gram based metrics.
Similarly, SPEDE (Wang andManning, 2012) is
an integrated probabilistic FSM and probabilistic
PDA model that predicts the edit sequence needed
for the MT output to match the reference. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps; con-
tain several dozens of parameters to tune and em-
ploy expensive linguistic resources, like WordNet
and paraphrase table. Like ULC, these matrices
are not useful in the MT system development cy-
cle for tuning due to expensive running time. The
metrics themselves are also expensive in training
and tuning due to the large number of parameters
to be estimated. Although ROSE (Song and Cohn,
2011) is a weighted linear model of shallow lin-
guistic features which is cheaper in run time but it
still contains several dozens of weights that need to
be tuned which affects the portability of the metric
for evaluating translations across domains.
Rios et al (2011) introduced TINE, an auto-
matic recall-oriented evaluationmetric which aims
to preserve the basic event structure, but no work
has been done toward tuning an SMT system
against it. TINE performs comparably to BLEU
and worse than METEOR on correlation with hu-
man adequacy judgment.
3 MEANT and UMEANT
MEANT (Lo et al, 2012), which is the weighted
f-measure over the matched semantic role labels
of the automatically aligned semantic frames and
role fillers, outperforms BLEU, NIST, METEOR,
WER, CDER and TER. Recent studies (Lo et al,
2013; Lo andWu, 2013) also show that tuning MT
system against MEANT produces more robustly
adequate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech. Pre-
423
Figure 1: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow se-
mantic parser. There are no semantic frames for MT3 since there is no predicate.
cisely, MEANT is computed as follows:
1. Apply an automatic shallow semantic parser
on both the references and MT output. (Fig-
ure 1 shows examples of automatic shallow
semantic parses on both reference and MT
output.)
2. Applymaximumweighted bipartite matching
algorithm to align the semantic frames be-
tween the references and MT output by the
lexical similarity of the predicates.
3. For each pair of aligned semantic frames,
(a) Lexical similarity scores determine the
similarity of the semantic role fillers.
(b) Apply maximum weighted bipartite
matching algorithm to align the seman-
tic role fillers between the reference and
MT output according to their lexical
similarity.
4. Compute the weighted f-measure over the
matching role labels of these aligned predi-
cates and role fillers.
Mi,j ? total # ARG j of aligned frame i in MT
Ri,j ? total # ARG j of aligned frame i in REF
Si,pred ? similarity of predicate in aligned frame i
Si,j ? similarity of ARG j in aligned frame i
wpred ? weight of similarity of predicates
wj ? weight of similarity of ARG j
mi ? #tokens filled in aligned frame i of MTtotal #tokens in MT
ri ? #tokens filled in aligned frame i of REFtotal #tokens in REF
precision =
?
imi
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjMi,j?
imi
recall =
?
i ri
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjRi,j?
i ri
wheremi and ri are the weights for frame, i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence. Mi,j and Ri,j are the to-
tal counts of argument of type j in frame i in the
MT and REF respectively. Si,pred and Si,j are the
lexical similarities of the predicates and role fillers
of the arguments of type j between the reference
translations and the MT output. wpred and wj are
the weights of the lexical similarities of the predi-
cates and role fillers of the arguments of typej be-
tween the reference translations and the MT out-
put. There are in total 12 weights for the set of
424
semantic role labels in MEANT as defined in Lo
and Wu (2011b).
For MEANT, wpred and wj are determined us-
ing supervised estimation via a simple grid search
to optimize the correlation with human adequacy
judgments (Lo and Wu, 2011a). For UMEANT,
wpred and wj are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the reference translations when the hu-
man judgments on adequacy of the development
set were unavailable (Lo and Wu, 2012).
In this experiment, we use a MEANT /
UMEANT implementation along the lines de-
scribed in Lo et al (2012) and Tumuluru et al
(2012) but we incorporate a variant of the aggre-
gation function proposed in Mihalcea et al (2006)
for phrasal similarity of role fillers as it normal-
izes the phrase length better than geometric mean
as described in Tumuluru et al (2012). In case
there is no semantic frame in the sentence, we treat
the whole sentence as a phrase and calculate the
phrasal similarity, like the role fillers in step 3.1,
as the MEANT score.
4 Experimental setup
We tune the 12 weights for the set of semantic role
labels in MEANT using grid search to maximize
the correlationwith human judgment on 6 develop-
ment sets. Following the protocol inWMT12 met-
rics evaluation task (Callison-Burch et al, 2012),
we use Kendall?s correlation coefficient for the
sentence-level correlation with human judgments.
The GALE development set consists of 40 sen-
tences randomly drawn from the DARPA GALE
P2.5 Chinese-English evaluation set alng with the
outputs from 3 participating MT systems and the
corresponding human adequacy judgments. The
WMT12-A development set consists of 800 sen-
tences randomly drawn from the Czech-English
test set in WMT12 metrics evaluation task along
with the output from 5 participating systems and
the corresponding human judgments. Similarly,
each of theWMT12-B,WMT12-C andWMT12-D
development sets consists of 800 randomly drawn
sentences from the WMT12 metrics evaluation
test set on German-English, Spanish-English and
French-English respectively. The WMT12-E de-
velopment set consists of 800 sentences out of
which 200 sentences were randomly drawn from
each of WMT12-A, WMT12-B, WMT12-C and
WMT12-D data set.
We evaluated MEANT and UMEANT on 3
groups of test sets. The first group is the original
(without partition) test data for each language pair
(translated in English) in WMT12. This group of
test sets is used for comparing MEANT?s perfor-
mance with the reported results from other partic-
ipants of WMT12. The second group is the held
out subset of the test data for each language pair in
WMT12. The third group is the original set of test
data for each language pair in WMT11. The lat-
ter 2 groups are used for determining which set of
tuned weights maximize the accuracy and stability
of MEANT.
5 Results
Table 1 shows that the best and the worst sentence-
level correlations reported in Callison-Burch et al
(2012) on the original WMT12 test sets (without
partitioning) for translations into English, together
the sentence-level correlation of MEANT tuned
on different development sets and UMEANT. The
grey boxes mark the results of experiments in
which there was an overlap between parts of the
development data and the test data. A study of the
values for the 12 weights associated with the se-
mantic role labels show that a general trend of the
importance of different labels in MEANT: ?who?
is always the most important; ?did?, ?what?,
?where?, ?why?, ?extent?, ?modal? and ?other?
are quite important too; ?when?, ?manner? and
?negation? fluctuate where they are quite impor-
tant in some development sets but not quite im-
portant in some development sets; ?whom? is usu-
ally not important. Given the fact that MEANT
employs significantly less expensive linguistic re-
sources and less sophisticated machine learning al-
gorithm in tuning the parameters, the performance
of MEANT is very competitive with other partici-
pants last year.
Table 2 shows the sentence-level correlation on
the WMT12 held-out test sets and the original
WMT11 test sets of MEANT tuned on different
development sets and UMEANT together with the
average sentence-level correlation on all test sets.
The results show that MEANT tuning onWMT12-
C development set achieve the highest sentence-
level correlation with human judgments on aver-
age. UMEANT, the unsupervised wight estimated
version of MEANT, achieves a very competitive
correlation score when compared with MEANT
tuned on different development sets. As a result,
425
Table 1: The best and the worst sentence-level correlation reported in Callison-Burch et al (2012) on the
original WMT12 test sets (without partitioning) for translations into English together the sentence-level
correlation of MEANT tuned on different development sets and UMEANT. The grey box marked results
of experiments in which parts of the development data and the test data are overlapped.
WMT12 cz-en WMT12 de-en WMT12 es-en WMT12 fr-en
Best reported 0.21 0.28 0.26 0.26
MEANT (GALE) 0.13 0.16 0.15 0.15
MEANT (WMT12-A) 0.12 0.17 0.16 0.15
MEANT (WMT12-B) 0.11 0.18 0.15 0.14
MEANT (WMT12-C) 0.12 0.17 0.17 0.15
MEANT (WMT12-D) 0.12 0.17 0.16 0.16
MEANT (WMT12-E) 0.12 0.17 0.17 0.15
UMEANT 0.12 0.17 0.16 0.14
Worst reported 0.06 0.08 0.08 0.07
Table 2: Sentence-level correlation on the WMT12 held-out test sets and the original WMT11 test sets
of MEANT tuned on different development sets and UMEANT together with the average sentence-level
correlation on all test sets.
WMT12 held-out WMT11 Average
cz-en de-en es-en fr-en cz-en de-en es-en fr-en -
MEANT (GALE) 0.0657 0.1251 0.1762 0.1719 0.3460 0.1123 0.2416 0.1913 0.1788
MEANT (WMT12-A) 0.0652 0.1117 0.1663 0.1540 0.3764 0.1101 0.2314 0.1944 0.1762
MEANT (WMT12-B) 0.0458 0.1294 0.1556 0.1548 0.3992 0.1479 0.2571 0.2037 0.1867
MEANT (WMT12-C) 0.0746 0.1278 0.1833 0.1592 0.3764 0.1324 0.2674 0.1882 0.1887
MEANT (WMT12-D) 0.0628 0.1164 0.1826 0.1655 0.3802 0.1168 0.2339 0.1975 0.1820
MEANT (WMT12-E) 0.0496 0.1353 0.1791 0.1619 0.3840 0.1101 0.2596 0.1851 0.1831
UMEANT 0.0477 0.1333 0.1606 0.1548 0.3764 0.1257 0.2828 0.1913 0.1841
we submitted two metrics to WMT 2013 metrics
evaluation task. One is MEANT with weights
learned from tuning on WMT12-C development
sets and the other submission is UMEANT.
6 Conclusion
In this paper, we have evaluated in the context of
WMT2013 the MEANT and UMEANT metrics,
which are tunable, accurate yet inexpensive fully
automatic machine translation evaluation metrics
that measure similarity between theMT output and
the reference via semantic frames. Recent stud-
ies show that tuning MT system against MEANT
produces more robustly adequate translations than
the common practice of tuning against BLEU or
TER across different data genres, such as formal
newswire text, informal web forum text and infor-
mal public speech. The weight for each seman-
tic role label in MEANT is estimated by maximiz-
ing the correlation with human adequacy judgment
on a development set. UMEANT is a version of
MEANT in which weight for each semantic role
label is estimated in an unsupervised fashion us-
ing the relative frequency of the semantic role la-
bels in the reference. We present the experimen-
tal results for determining the set of weights that
maximize MEANT?s accuracy and stability by op-
timizing MEANT on different development sets.
We disagree with the notion ?a good evaluation
metric is not necessarily a good tuning metric, and
vice versa? (Chen et al, 2012). Instead, we be-
lieve that a good evaluation metric should be one
that is a good objective function to drive the devel-
opment of MT systems towards higher utility. In
other words, a good evaluation metric should cor-
relate well with human adequacy judgment and at
the same time, be inexpensive in running time so as
to fit into the MT pipeline to improve MT quality.
Our results shows that MEANT is a good evalu-
ation/tuning metric because it achieves a competi-
tive correlation scorewith human judgments by us-
ing less expensive linguistic resources and training
algorithms making it possible to tune MT system
against MEANT to improve MT quality.
7 Acknowledgment
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
426
ment no. 287658; and by the Hong Kong Re-
search Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
References
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?
72, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational Linguis-
tics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation. In
Proceedings of the 2nd Workshop on Statistical
Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Transla-
tion. In Proceedings of the 3rd Workshop on
Statistical Machine Translation, pages 70?106,
2008.
Chris Callison-Burch, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Spe-
cia. Findings of the 2012 Workshop on Statisti-
cal Machine Translation. In Proceedings of the
7th Workshop on Statistical Machine Transla-
tion (WMT 2012), pages 10?51, 2012.
Julio Castillo and Paula Estrella. Semantic Textual
Similarity for MT evaluation. In Proceedings of
the 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), pages 52?58, 2012.
Boxing Chen, Roland Kuhn, and George Foster.
Improving AMBER, an MT Evaluation Metric.
In Proceedings of the 7th Workshop on Statis-
tical Machine Translation (WMT 2012), pages
59?63, 2012.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the
2nd International Conference on Human Lan-
guage Technology Research, pages 138?145,
San Diego, California, 2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic
features for automatic evaluation of heteroge-
nous MT systems. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 256?264, Prague, Czech Republic,
June 2007.
Jes?s Gim?nez and Llu?sM?rquez. A smorgasbord
of features for automaticMT evaluation. In Pro-
ceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198, Colum-
bus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation
between European Languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation (WMT-06), pages 102?121, 2006.
Patrik Lambert, Jes?s Gim?nez, Marta R Costa-
juss?, Enrique Amig?, Rafael E Banchs, Llu?s
M?rquez, and JAR Fonollosa. Machine Transla-
tion system development based on human like-
ness. In Spoken Language Technology Work-
shop, 2006. IEEE, pages 246?249. IEEE, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT Evaluation Using
Block Movements. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
06), 2006.
Chi-kiu Lo and Dekai Wu. MEANT: An Inexpen-
sive, High-Accuracy, Semi-Automatic Metric
for Evaluating Translation Utility based on Se-
mantic Roles. In Proceedings of the Joint con-
ference of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics : Human
Language Technologies (ACL-HLT-11), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelli-
gence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs.
supervised weight estimation for semantic MT
evaluation metrics. In Proceedings of the 6th
Workshop on Syntax and Structure in Statistical
Translation (SSST-6), 2012.
427
Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic seman-
tic metrics? In Proceedings of the 14th Machine
Translation Summit (MTSummit-XIV), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic MT Evaluation.
In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation (WMT2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics (ACL-13), 2013.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In Pro-
ceedings of the national conference on artificial
intelligence, volume 21, page 775. Menlo Park,
CA; Cambridge, MA; London; AAAI Press;
MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International
Conference on Language Resources and Evalu-
ation (LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H.Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
In Proceedings of the 2004 Conference on Hu-
man Language Technology and the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL-04), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine:
A metric to assess MT adequacy. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation (WMT-2011), pages 116?122,
2011.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In Proceedings of the 7th Conference
of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, Cam-
bridge, Massachusetts, August 2006.
Xingyi Song and Trevor Cohn. Regression and
Ranking based Optimisation for Sentence Level
Machine Translation Evaluation. In Proceed-
ings of the 6th Workshop on Statistical Machine
Translation (WMT 2011), pages 123?129, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring
the lexical similarity of semantic role fillers
for automatic semantic mt evaluation. In Pro-
ceeding of the 26th Pacific Asia Conference
on Language, Information, and Computation
(PACLIC-26), 2012.
Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic Edit Distance Metrics for
MT Evaluation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation (WMT
2012), pages 76?83, 2012.
428
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22?33,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Better Semantic Frame Based MT Evaluation
via Inversion Transduction Grammars
Dekai Wu Lo Chi-kiu Meriem Beloucif Markus Saers
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce an inversion transduc-
tion grammar based restructuring of
the MEANT automatic semantic frame
based MT evaluation metric, which,
by leveraging ITG language biases, is
able to further improve upon MEANT?s
already-high correlation with human
adequacy judgments. The new metric,
called IMEANT, uses bracketing ITGs to
biparse the reference and machine transla-
tions, but subject to obeying the semantic
frames in both. Resulting improvements
support the presumption that ITGs, which
constrain the allowable permutations
between compositional segments across
the reference and MT output, score the
phrasal similarity of the semantic role
fillers more accurately than the simple
word alignment heuristics (bag-of-word
alignment or maximum alignment) used
in previous version of MEANT. The
approach successfully integrates (1) the
previously demonstrated extremely high
coverage of cross-lingual semantic frame
alternations by ITGs, with (2) the high
accuracy of evaluating MT via weighted
f-scores on the degree of semantic frame
preservation.
1 Introduction
There has been to date relatively little use of in-
version transduction grammars (Wu, 1997) to im-
prove the accuracy of MT evaluation metrics, de-
spite long empirical evidence the vast majority of
translation patterns between human languages can
be accommodated within ITG constraints (and the
observation that most current state-of-the-art SMT
systems employ ITG decoders). We show that
ITGs can be used to redesign the MEANT seman-
tic frame based MT evaluation metric (Lo et al.,
2012) to produce improvements in accuracy and
reliability. This work is driven by the motiva-
tion that especially when considering semanticMT
metrics, ITGs would be seem to be a natural basis
for several reasons.
To begin with, it is quite natural to think of
sentences as having been generated from an ab-
stract concept using a rewriting system: a stochas-
tic grammar predicts how frequently any particu-
lar realization of the abstract concept will be gen-
erated. The bilingual analogy is a transduction
grammar generating a pair of possible realizations
of the same underlying concept. Stochastic trans-
duction grammars predict how frequently a partic-
ular pair of realizations will be generated, and thus
represent a good way to evaluate how well a pair
of sentences correspond to each other.
The particular class of transduction gram-
mars known as ITGs tackle the problem that
the (bi)parsing complexity for general syntax-
directed transductions (Aho and Ullman, 1972)
is exponential. By constraining a syntax-directed
transduction grammar to allow only monotonic
straight and inverted reorderings, or equivalently
permitting only binary or ternary rank rules, it is
possible to isolate the low end of that hierarchy into
a single equivalence class of inversion transduc-
tions. ITGs are guaranteed to have a two-normal
form similar to context-free grammars, and can
be biparsed in polynomial time and space (O
(
n
6
)
time andO
(
n
4
)
space). It is also possible to do ap-
proximate biparsing in O
(
n
3
)
time (Saers et al.,
2009). These polynomial complexities makes it
feasible to estimate the parameters of an ITG us-
ing standard machine learning techniques such as
expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have
also been directly shown to be more than sufficient
to account for the reordering that occur within se-
mantic frame alternations (Addanki et al., 2012).
This makes ITGs an appealing alternative for eval-
22
uating the possible links between both semantic
role fillers in different languages as well as the
predicates, and how these parts fit together to form
entire semantic frames. We believe that ITGs are
not only capable of generating the desired struc-
tural correspondences between the semantic struc-
tures of two languages, but also provide meaning-
ful constraints to prevent alignments from wander-
ing off in the wrong direction.
In this paper we show that IMEANT, a newmet-
ric drawing from the strengths of both MEANT
and inversion transduction grammars, is able to
exploit bracketing ITGs (also known as BITGs
or BTGs) which are ITGs containing only a sin-
gle non-differentiated non terminal category (Wu,
1995a), so as to produce even higher correlation
with human adequacy judgments than any auto-
matic MEANT variants, or other common auto-
matic metrics. We argue that the constraints pro-
vided by BITGs over the semantic frames and ar-
guments of the reference and MT output sentences
are essential for accurate evaluation of the phrasal
similarity of the semantic role fillers.
In common with the various MEANT semantic
MT evaluation metrics (Lo and Wu, 2011a, 2012;
Lo et al., 2012; Lo and Wu, 2013b), our proposed
IMEANT metric measures the degree to which
the basic semantic event structure is preserved
by translation?the ?who did what to whom, for
whom, when, where, how and why? (Pradhan et
al., 2004)?emphasizing that a good translation
is one that can successfully be understood by a
human. In the other versions of MEANT, sim-
ilarity between the MT output and the reference
translations is computed as a modified weighted f-
score over the semantic predicates and role fillers.
Across a variety of language pairs and genres, it
has been shown thatMEANT correlates better with
human adequacy judgment than both n-gram based
MT evaluation metrics such as BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Banerjee and Lavie, 2005), as well as edit-
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al., 2000), and TER
(Snover et al., 2006) when evaluating MT output
(Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and
Wu, 2013b; Mach??ek and Bojar, 2013). Further-
more, tuning the parameters of MT systems with
MEANT instead of BLEU or TER robustly im-
proves translation adequacy across different gen-
res and different languages (English and Chinese)
(Lo et al., 2013a; Lo and Wu, 2013a; Lo et al.,
2013b). This has motivated our choice of MEANT
as the basis on which to experiment with deploying
ITGs into semantic MT evaluation.
2 Related Work
2.1 ITGs and MT evaluation
Relatively little investigation into the potential
benefits of ITGs is found in previous MT eval-
uation work. One exception is invWER, pro-
posed by Leusch et al. (2003) and Leusch and Ney
(2008). The invWER metric interprets weighted
BITGs as a generalization of the Levenshtein edit
distance, in which entire segments (blocks) can be
inverted, as long as this is done strictly compo-
sitionally so as not to violate legal ITG biparse
tree structures. The input and output languages
are considered to be those of the reference and ma-
chine translations, and thus are over the same vo-
cabulary (say,English). At the sentence level, cor-
relation of invWER with human adequacy judg-
ments was found to be among the best.
Our current approach differs in several key
respects from invWER. First,invWER operates
purely at the surface level of exact token match,
IMEANT mediates between segments of refer-
ence translation andMT output using lexical BITG
probabilities.
Secondly, there is no explicit semantic model-
ing in invWER. Providing they meet the BITG
constraints, the biparse trees in invWER are com-
pletely unconstrained. In contrast, IMEANT em-
ploys the same explicit, strong semantic frame
modeling as MEANT, on both the reference and
machine translations. In IMEANT, the semantic
frames always take precedence over pure BITG
biases. Compared to invWER, this strongly con-
strains the space of biparses that IMEANT permits
to be considered.
2.2 MT evaluation metrics
Like invWER, other common surface-form ori-
ented metrics like BLEU (Papineni et al., 2002),
NIST (Doddington, 2002), METEOR (Banerjee
and Lavie, 2005; Denkowski and Lavie, 2014),
CDER (Leusch et al., 2006), WER (Nie?en et
al., 2000), and TER (Snover et al., 2006) do
not correctly reflect the meaning similarities of
the input sentence. There are in fact several
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) reporting cases
23
where BLEU strongly disagrees with human judg-
ments of translation adequacy.
Such observations have generated a recent surge
of work on developing MT evaluation metrics that
would outperform BLEU in correlation with hu-
man adequacy judgment (HAJ). Like MEANT, the
TINE automatic recall-oriented evaluation metric
(Rios et al., 2011) aims to preserve basic event
structure. However, its correlation with human ad-
equacy judgment is comparable to that of BLEU
and not as high as that of METEOR. Owczarzak
et al. (2007a,b) improved correlation with human
fluency judgments by using LFG to extend the ap-
proach of evaluating syntactic dependency struc-
ture similarity proposed by Liu and Gildea (2005),
but did not achieve higher correlation with hu-
man adequacy judgments than metrics like ME-
TEOR. Another automatic metric, ULC (Gim?nez
and M?rquez, 2007, 2008), incorporates several
semantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al., 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al., 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an SMT system using
a pure form of ULC perhaps due to its expensive
run time. Likewise, SPEDE (Wang and Manning,
2012) predicts the edit sequence needed to match
the machine translation to the reference translation
via an integrated probabilistic FSM and probabilis-
tic PDA model. The semantic textual similarity
metric Sagan (Castillo and Estrella, 2012) is based
on a complex textual entailment pipeline. These
aggregated metrics require sophisticated feature
extraction steps, contain many parameters that
need to be tuned, and employ expensive linguis-
tic resources such asWordNet or paraphrase tables.
The expensive training, tuning and/or running time
renders these metrics difficult to use in the SMT
training cycle.
3 IMEANT
In this section we give a contrastive description
of IMEANT: we first summarize the MEANT ap-
proach, and then explain how IMEANT differs.
3.1 Variants of MEANT
MEANT and its variants (Lo et al., 2012) measure
weighted f-scores over corresponding semantic
frames and role fillers in the reference andmachine
translations. The automatic versions of MEANT
replace humans with automatic SRL and align-
ment algorithms. MEANT typically outperforms
BLEU, NIST, METEOR, WER, CDER and TER
in correlation with human adequacy judgment, and
is relatively easy to port to other languages, re-
quiring only an automatic semantic parser and a
monolingual corpus of the output language, which
is used to gauge lexical similarity between the se-
mantic role fillers of the reference and translation.
MEANT is computed as follows:
1. Apply an automatic shallow semantic parser
to both the reference and machine transla-
tions. (Figure 1 shows examples of auto-
matic shallow semantic parses on both refer-
ence and MT.)
2. Apply the maximum weighted bipartite
matching algorithm to align the semantic
frames between the reference and machine
translations according to the lexical similari-
ties of the predicates. (Lo and Wu (2013a)
proposed a backoff algorithm that evaluates
the entire sentence of theMT output using the
lexical similarity based on the context vector
model, if the automatic shallow semantic
parser fails to parse the reference or machine
translations.)
3. For each pair of the aligned frames, apply the
maximum weighted bipartite matching algo-
rithm to align the arguments between the ref-
erence andMT output according to the lexical
similarity of role fillers.
4. Compute the weighted f-score over the
matching role labels of these aligned predi-
cates and role fillers according to the follow-
ing definitions:
q
0
i,j
? ARG j of aligned frame i in MT
q
1
i,j
? ARG j of aligned frame i in REF
w
0
i
?
#tokens filled in aligned frame i of MT
total #tokens in MT
w
1
i
?
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ? weight of similarity of predicates
w
j
? weight of similarity of ARG j
e
i,pred ? the pred string of the aligned frame i of MT
f
i,pred ? the pred string of the aligned frame i of REF
e
i,j
? the role fillers of ARG j of the aligned frame i of MT
f
i,j
? the role fillers of ARG j of the aligned frame i of REF
s(e, f) = lexical similarity of token e and f
24
[IN] ?? ? ? ?? ?? ?? ? ? ? ? ? ? ????? ?? ?? ?? ?? ?  
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed . 
ARGM-TMP PRED ARGM-LOC PRED ARG1 
ARGM-LOC PRED ARG1 PRED ARG1 
ARG0 ARGM-TMP 
[MT1] So far , nearly two months sk - ii the sale of products in the mainland of China to resume sales .  
PRED ARG0 ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 
[MT3] So far , the sale in the mainland of China for nearly two months of SK - II line of products .  
PRED 
PRED ARG0 
ARG1 ARGM-TMP 
ARGM-ADV 
ARG0 
ARGM-EXT 
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames forMT3 since there is no predicate
in the MT output.
prece,f =
?
e?e max
f?f
s(e, f)
| e |
rece,f =
?
f?f max
e?e
s(e, f)
| f |
s
i,pred =
2 ? prece
i,pred,fi,pred ? recei,pred,fi,pred
prece
i,pred,fi,pred + recei,pred,fi,pred
s
i,j
=
2 ? prece
i,j
,f
i,j
? rece
i,j
,f
i,j
prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
precision =
?
i
w
0
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
0
i,j
|
?
i
w
0
i
recall =
?
i
w
1
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
1
i,j
|
?
i
w
1
i
MEANT = 2 ? precision ? recallprecision + recall
where q0
i,j
and q1
i,j
are the argument of type j in
frame i inMT andREF respectively.w0
i
andw1
i
are
the weights for frame i in MT/REF respectively.
These weights estimate the degree of contribution
of each frame to the overall meaning of the sen-
tence. wpred and wj are the weights of the lexical
similarities of the predicates and role fillers of the
arguments of type j of all frame between the ref-
erence translations and the MT output.There is a
total of 12 weights for the set of semantic role la-
bels in MEANT as defined in Lo and Wu (2011b).
For MEANT, they are determined using super-
vised estimation via a simple grid search to opti-
mize the correlation with human adequacy judg-
ments (Lo andWu, 2011a). For UMEANT (Lo and
Wu, 2012), they are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the references and thus UMEANT is
useful when human judgments on adequacy of the
development set are unavailable.
s
i,pred and si,j are the lexical similarities based
on a context vectormodel of the predicates and role
fillers of the arguments of type j between the ref-
erence translations and the MT output. Lo et al.
(2012) and Tumuluru et al. (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al., 2013a; Lo and
Wu, 2013a; Lo et al., 2013b). In this paper, we
will assess IMEANT against the latest version of
MEANT (Lo et al., 2014) which, as shown, uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of semantic
role fillers,since this has been shown to bemore ac-
curate than the previously used aggregation func-
tions.
Recent studies (Lo et al., 2013a; Lo and Wu,
2013a; Lo et al., 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
25
In an alternative quality-estimation oriented line
of research, Lo et al. (2014) describe a cross-
lingual variant called XMEANT capable of eval-
uating translation quality without the need for ex-
pensive human reference translations, by utiliz-
ing semantic parses of the original foreign in-
put sentence instead of a reference translation.
Since XMEANT?s results could have been due
to either (1) more accurate evaluation of phrasal
similarity via cross-lingual translation probabili-
ties, or (2) better match of semantic frames with-
out reference translations, there is no direct evi-
dencewhether ITGs contribute to the improvement
in MEANT?s correlation with human adequacy
judgment. For the sake of better understanding
whether ITGs improve semantic MT evaluation,
we will also assess IMEANT against cross-lingual
XMEANT.
3.2 The IMEANT metric
Although MEANT was previously shown to pro-
duce higher correlation with human adequacy
judgments compared to other automatic metrics,
our error analyses suggest that it still suffers from a
common weakness among metrics employing lex-
ical similarity, namely that word/token alignments
between the reference and machine translations
are severely under constrained. No bijectivity or
permutation restrictions are applied, even between
compositional segments where this should be nat-
ural. This can cause role fillers to be aligned even
when they should not be. IMEANT, in contrast,
uses a bracketing inversion transduction grammar
to constrain permissible token alignment patterns
between aligned role filler phrases. The semantic
frames above the token level also fits ITG com-
positional structure, consistent with the aforemen-
tioned semantic frame alternation coverage study
of Addanki et al. (2012). Figure 2 illustrates how
the ITG constraints are consistent with the needed
permutations between semantic role fillers across
the reference and machine translations for a sam-
ple sentence from our evaluation data, which as
we will see leads to higher HAJ correlations than
MEANT.
Subject to the structural ITG constraints,
IMEANT scores sentence translations in a spirit
similar to the way MEANT scores them: it utilizes
an aggregated score over the matched semantic
role labels of the automatically aligned semantic
frames and their role fillers between the reference
and machine translations. Despite the structural
differences, like MEANT, at the conceptual level
IMEANT still aims to evaluate MT output in
terms of the degree to which the translation has
preserved the essential ?who did what to whom,for
whom, when, where, how and why? of the foreign
input sentence.
Unlike MEANT, however, IMEANT aligns and
scores under ITG assumptions. MEANT uses a
maximum alignment algorithm to align the tokens
in the role fillers between the reference and ma-
chine translations, and then scores by aggregating
the lexical similarities into a phrasal similarity us-
ing an f-measure. In contrast, IMEANT aligns and
scores by utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009; Addanki et al., 2012). To be precise in
this regard, we can see IMEANT as differing from
the foregoing description of MEANT in the defi-
nition of s
i,pred and si,j , as follows.
G ? ?{A} ,W0,W1,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 1
p (e/f |A) = s(e, f)
s
i,pred = lg?1
?
?
lg
(
P
(
A ?? e
i,pred/fi,pred|G
))
max(| e
i,pred |, | fi,pred |)
?
?
s
i,j
= lg?1
?
?
lg
(
P
(
A ?? e
i,j
/f
i,j
|G
))
max(| e
i,j
|, | f
i,j
|)
?
?
where G is a bracketing ITG whose only non ter-
minal is A, andR is a set of transduction rules with
e ? W
0
?{?} denoting a token in theMToutput (or
the null token) and f ? W1?{?} denoting a token
in the reference translation (or the null token). The
rule probability (or more accurately, rule weight)
function p is set to be 1 for structural transduction
rules, and for lexical transduction rules it is de-
fined using MEANT?s context vector model based
lexical similarity measure. To calculate the inside
probability (or more accurately, inside score) of a
pair of segments, P
(
A ?? e/f|G
)
, we use the al-
gorithm described in Saers et al. (2009). Given
this, s
i,pred and si,j now represent the length nor-
malized BITG parse scores of the predicates and
role fillers of the arguments of type j between the
reference and machine translations.
4 Experiments
In this section we discuss experiments indicating
that IMEANT further improves upon MEANT?s
26
[REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work .  
[MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency .  
ARG0 ARG1 PRED 
ARG0 PRED ARG1 
The 
level 
of 
reduction 
is 
conducive 
to 
raising 
the 
inspection 
and 
supervision 
work 
efficiency 
. 
Th
e 
red
uct
ion
 in 
hie
rar
chy
 
hel
ps rais
e the
 
effi
cie
ncy
 of 
ins
pec
tion
 
and
 
sup
erv
iso
ry . 
wo
rk 
pred 
ARG0 
ARG1 
pr
ed
 
AR
G0
 
AR
G1
 
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence ????????????
????????. Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
already-high correlation with human adequacy
judgments.
4.1 Experimental setup
We perform the meta-evaluation upon two differ-
ent partitions of the DARPA GALE P2.5 Chinese-
English translation test set. The corpus includes
the Chinese input sentences, each accompanied by
one English reference translation and three partic-
ipating state-of-the-art MT systems? output.
For the sake of consistent comparison, the first
evaluation partition, GALE-A, is the same as the
one used in Lo and Wu (2011a), and the second
evaluation partition, GALE-B, is the same as the
one used in Lo and Wu (2011b).
For both reference and machine translations, the
ASSERT (Pradhan et al., 2004) semantic role la-
beler was used to automatically predict semantic
parses.
27
Table 1: Sentence-level correlation with human
adequacy judgements on different partitions of
GALE P2.5 data. IMEANT always yields top
correlations, and is more consistent than either
MEANT or its recent cross-lingual XMEANT
quality estimation variant. For reference, the hu-
man HMEANT upper bound is 0.53 for GALE-A
and 0.37 for GALE-B?thus, the fully automated
IMEANT approximation is not far from closing the
gap.
metric GALE-A GALE-B
IMEANT 0.51 0.33
XMEANT 0.51 0.20
MEANT 0.48 0.33
METEOR 1.5 (2014) 0.43 0.10
NIST 0.29 0.16
METEOR 0.4.3 (2005) 0.20 0.29
BLEU 0.20 0.27
TER 0.20 0.19
PER 0.20 0.18
CDER 0.12 0.16
WER 0.10 0.26
4.2 Results
The sentence-level correlations in Table 1 show
that IMEANT outperforms other automatic met-
rics in correlation with human adequacy judgment.
Note that this was achieved with no tuning what-
soever of the default rule weights (suggesting that
the performance of IMEANT could be further im-
proved in the future by slightly optimizing the ITG
weights).
On the GALE-A partition, IMEANT shows 3
points improvement over MEANT, and is tied
with the cross-lingual XMEANT quality estimator
discussed earlier.IMEANT produces much higher
HAJ correlations than any of the other metrics.
On the GALE-B partition, IMEANT is tied with
MEANT, and is significantly better correlated with
HAJ than the XMEANT quality estimator. Again,
IMEANT produces much higher HAJ correlations
than any of the other metrics.
We note that we have also observed this pattern
consistently in smaller-scale experiments?while
the monolingual MEANT metric and its cross-
lingual XMEANT cousin vie with each other on
different data sets, IMEANT robustly and consis-
tently produces top HAJ correlations.
In both the GALE-A and GALE-B partitions,
IMEANT comes within a few points of the human
upper bound benchmark HAJ correlations com-
puted using the human labeled semantic frames
and alignments used in the HMEANT.
Data analysis reveals two reasons that IMEANT
correlates with human adequacy judgement more
closely than MEANT. First, BITG constraints in-
deed provide more accurate phrasal similarity ag-
gregation, compared to the naive bag-of-words
based heuristics employed in MEANT. Similar re-
sults have been observed while trying to estimate
word alignment probabilities where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Secondly, the permutation and bijectivity con-
straints enforced by the ITG provide better lever-
age to reject token alignments when they are not
appropriate, compared with the maximal align-
ment approach which tends to be rather promiscu-
ous. A case of this can be seen in Figure 3, which
shows the result on the same example sentence as
in Figure 1. Disregarding the semantic parsing er-
rors arising from the current limitations of auto-
matic SRL tools, the ITG tends to provide clean,
sparse alignments for role fillers like the ARG1
of the resumed PRED, preferring to leave tokens
like complete and range unaligned instead of aligning
them anyway as MEANT?s maximal alignment al-
gorithm tends to do. Note that it is not simply a
matter of lowering thresholds for accepting token
alignments: Tumuluru et al. (2012) showed that
the competitive linking approach (Melamed, 1996)
which also generally produces sparser alignments
does not work as well inMEANT, whereas the ITG
appears to be selective about the token alignments
in a manner that better fits the semantic structure.
For contrast, Figure 4 shows a case where
IMEANT appropriately accepts dense alignments.
5 Conclusion
We have presented IMEANT, an inversion trans-
duction grammar based rethinking of the MEANT
semantic frame based MT evaluation approach,
that achieves higher correlation with human ad-
equacy judgments of MT output quality than
MEANT and its variants, as well as other com-
mon evaluation metrics. Our results improve upon
previous research showing that MEANT?s explicit
use of semantic frames leads to state-of-the-art au-
tomatic MT evaluation. IMEANT achieves this
by aligning and scoring semantic frames under a
simple, consistent ITG that provides empirically
28
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed .  
ARG0 PRED ARGM-LOC PRED ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 PRED 
ARGM-TMP ARGM-TMP 
So 
far 
, 
in 
the 
mainland 
of 
China 
to 
stop 
selling 
nearly 
two 
months 
of 
SK 
- 
2 
products 
sales 
resumed 
. 
Un
til 
afte
r 
the
ir 
sal
e had
 
cea
sed
 in 
ma
inla
nd 
Ch
ina
 for 
alm
ost
 
two
 , 
sal
es of the
 
com
ple
te 
PRED 
PRED 
ARG1 
ARGM-TMP 
ARG1 
PRED 
PR
ED
 
PR
ED
 
AR
G1
 
AR
GM
-L
OC
 
AR
G0
 
ran
ge of SK
 - II 
pro
duc
ts 
hav
e 
now
 
bee
n 
res
um
ed . 
mo
nth
s 
AR
GM
-T
MP
 
AR
GM
-T
MP
 
Figure 3: An example where the ITG helps produce correctly sparse alignments by rejecting inappro-
priate token alignments in the ARG1 of the resumed PRED, instead of wrongly aligning tokens like the,
complete, and range as MEANT tends to do. (The semantic parse errors are due to limitations of automatic
SRL.)
informative permutation and bijectivity biases, in-
stead of the maximal alignment and bag-of-words
assumptions used by MEANT. At the same time,
IMEANT retains the Occam?s Razor style simplic-
ity and representational transparency characteris-
tics of MEANT.
Given the absence of any tuning of ITG weights
in this first version of IMEANT, we speculate that
29
[REF] Australian Prime Minister Howard said the government could cancel AWB 's monopoly in the wheat business next week . 
[MT2] Australian Prime Minister John Howard said that the Government might cancel the AWB company wheat monopoly next week . 
ARG0 
ARG0 
PRED 
ARG0 PRED ARG1 PRED 
ARG0 
ARGM-MOD ARGM-TMP 
ARG1 
PRED ARGM-MOD ARG1 ARGM-TMP 
ARG1 
Australian 
Prime 
Minister 
John 
Howard 
said 
the 
Government 
might 
cancel 
the 
AWB 
company 
wheat 
monopoly 
Au
str
alia
n 
Pri
me
 
Min
iste
r 
Ho
wa
rd sai
d the
 
go
ver
nm
en
t 
cou
ld 
can
cel
 
AW
B 's 
mo
no
po
ly the
 in 
next 
week 
that 
. 
wh
ea
t 
bu
sin
ess
 
ne
xt 
we
ek . 
pr
ed
 
pr
ed
 
AR
G0
 
AR
G0
 
AR
GM
-M
OD
 
AR
G1
 
AR
GM
-T
MP
 
AR
G1
 
pred 
pred 
ARG0 
ARG0 
ARGM-MOD 
ARGM-TMP 
ARG1 
ARG1 
Figure 4: An example of dense alignments in IMEANT, for the Chinese input sentence ???????
?????????????? AWB?????????? (The semantic parse errors are due to limitations
of automatic SRL.)
IMEANT could perform even better than it already
does here.We plan to investigate simple hyperpa-
rameter optimizations in the near future.
30
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessar-
ily reflect the views of DARPA, the EU, or RGC.
Thanks to Karteek Addanki for supporting work,
and to Pascale Fung, Yongsheng Yang and Zhao-
jun Wu for sharing the maximum entropy Chinese
segmenter and C-ASSERT, the Chinese semantic
parser.
References
Karteek Addanki, Chi-kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-
lingual verb frame alternations. In 16th An-
nual Conference of the European Association
for Machine Translation (EAMT-2012), Trento,
Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in ma-
chine translation research. In 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(meta-) evaluation of machine translation. In
Second Workshop on Statistical Machine Trans-
lation (WMT-07), 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further meta-evaluation of machine transla-
tion. In Third Workshop on Statistical Machine
Translation (WMT-08), 2008.
Julio Castillo and Paula Estrella. Semantic tex-
tual similarity for MT evaluation. In 7th Work-
shop on Statistical Machine Translation (WMT
2012), 2012.
Michael Denkowski and Alon Lavie. METEOR
universal: Language specific translation eval-
uation for any target language. In 9th Work-
shop on Statistical Machine Translation (WMT
2014), 2014.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In The second interna-
tional conference on Human Language Technol-
ogy Research (HLT ?02), San Diego, California,
2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic fea-
tures for automatic evaluation of heterogenous
MT systems. In Second Workshop on Statisti-
cal Machine Translation (WMT-07), pages 256?
264, Prague, Czech Republic, June 2007.
Jes?s Gim?nez and Llu?s M?rquez. A smorgas-
bord of features for automaticMT evaluation. In
Third Workshop on Statistical Machine Transla-
tion (WMT-08), Columbus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
automatic evaluation of machine translation be-
tween european languages. InWorkshop on Sta-
tistical Machine Translation (WMT-06), 2006.
Gregor Leusch and Hermann Ney. Bleusp, invwer,
cder: Three improved mt evaluation measures.
In NIST Metrics for Machine Translation Chal-
lenge (MetricsMATR), at Eighth Conference of
the Association for Machine Translation in the
Americas (AMTA 2008), Waikiki, Hawaii, Oct
2008.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. A novel string-to-string distance measure
with applications to machine translation evalu-
ation. In Machine Translation Summit IX (MT
Summit IX), New Orleans, Sep 2003.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT evaluation using
block movements. In 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL-2006), 2006.
31
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. InWorkshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
Ann Arbor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpen-
sive, high-accuracy, semi-automatic metric for
evaluating translation utility based on seman-
tic roles. In 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Twenty-second International Joint
Conference on Artificial Intelligence (IJCAI-
11), 2011.
Chi-kiu Lo andDekaiWu. Unsupervised vs. super-
vised weight estimation for semantic MT evalu-
ation metrics. In Sixth Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres
be better translated by tuning on automatic se-
mantic metrics? In 14th Machine Translation
Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT
2013: A tunable, accurate yet inexpensive se-
mantic frame based mt evaluation metric. In
8th Workshop on Statistical Machine Transla-
tion (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully automatic semantic MT evaluation.
In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by
tuning against Chinese MEANT. In Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. XMEANT: Better semantic MT
evaluation without reference translations. In
52nd Annual Meeting of the Association for
Computational Linguistics (ACL 2014), 2014.
Matou?Mach??ek andOnd?ej Bojar. Results of the
WMT13 metrics shared task. In Eighth Work-
shop on Statistical Machine Translation (WMT
2013), Sofia, Bulgaria, August 2013.
I. Dan Melamed. Automatic construction of
clean broad-coverage translation lexicons. In
2nd Conference of the Association for Ma-
chine Translation in the Americas (AMTA-
1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In The
Twenty-first National Conference on Artificial
Intelligence (AAAI-06), volume 21, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A evaluation tool for ma-
chine translation: Fast evaluation for MT re-
search. In The Second International Conference
on Language Resources and Evaluation (LREC
2000), 2000.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Dependency-based automatic eval-
uation for machine translation. In Syntax
and Structure in Statistical Translation (SSST),
2007.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Evaluating machine translation
with LFG dependencies. Machine Translation,
21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In 40th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL-02), pages 311?318,
Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow se-
mantic parsing using support vector machines.
In Human Language Technology Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-
NAACL 2004), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia.
TINE: A metric to assess MT adequacy. In
Sixth Workshop on Statistical Machine Transla-
tion (WMT 2011), 2011.
32
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
stochastic inversion transduction grammars. In
Third Workshop on Syntax and Structure in
Statistical Translation (SSST-3), pages 28?36,
Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT?09), pages 29?32,
Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In 7th Biennial Conference Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 2006), pages 223?231, Cambridge,
Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring the
lexical similarity of semantic role fillers for au-
tomatic semantic MT evaluation. In 26th Pa-
cific Asia Conference on Language, Informa-
tion, and Computation (PACLIC 26), 2012.
Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic edit distance metrics for
MT evaluation. In 7th Workshop on Statistical
Machine Translation (WMT 2012), 2012.
DekaiWu. An algorithm for simultaneously brack-
eting parallel texts by aligning words. In 33rd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL 95), pages 244?251,
Cambridge, Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars
for parallel text bracketing. In Third Annual
Workshop on Very Large Corpora (WVLC-3),
pages 69?81, Cambridge, Massachusetts, June
1995.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Richard Zens and Hermann Ney. A compara-
tive study on reordering constraints in statisti-
cal machine translation. In 41st Annual Meeting
of the Association for Computational Linguis-
tics (ACL-2003), pages 144?151, Stroudsburg,
Pennsylvania, 2003.
33
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144?153,
Dublin, Ireland, August 23, 2014.
Lexical Access Preference and Constraint Strategies for Improving
Multiword Expression Association within Semantic MT Evaluation
Dekai Wu Lo Chi-kiu Markus Saers
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{dekai|jackielo|masaers|dekai}@cs.ust.hk
Abstract
We examine lexical access preferences and constraints in computing multiword expression asso-
ciations from the standpoint of a high-impact extrinsic task-based performance measure, namely
semantic machine translation evaluation. In automated MT evaluation metrics, machine transla-
tions are compared against human reference translations, which are almost never worded exactly
the sameway except in the most trivial of cases. Because of this, one of the most important factors
in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative
lexical realizations of the same multiword expressions in semantic role fillers. Our results com-
paring bag-of-words, maximum alignment, and inversion transduction grammars indicate that
cognitively motivated ITGs provide superior lexical access characteristics for multiword expres-
sion associations, leading to state-of-the-art improvements in correlation with human adequacy
judgments.
1 Introduction
We investigate lexical access strategies in the context of computing multiword expression associations
within automatic semantic MT evaluation metrics?a high-impact real-world extrinsic task-based per-
formance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious
issues in machine translation and automatic MT evaluation; there are simply too many forms to enumer-
ate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a
decade and a half, but until recently little has been done to use lexical semantics as the main foundation
for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nie?en et al.,
2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference
and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and
Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation
adequacy.
Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et
al., 2012; Lo andWu, 2013b), have instead directly couchedMT evaluation in the more cognitive terms of
semantic frames, by measuring the degree to which the basic event structure is preserved by translation?
the ?who did what to whom, for whom, when, where, how and why? (Pradhan et al., 2004)?emphasizing
that a good translation is one that can successfully be understood by a human. Across a variety of language
pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both n-
gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and
METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al., 2000), and TER (Snover et al., 2006) when evaluatingMT output (Lo and
Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach??ek and Bojar, 2013). Furthermore, tuning
the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/
144
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL
decided to drop the predicate.
adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English
and Chinese) and different genres (formal newswire text, informal web forum text and informal public
speech).
Because of this, we have chosen to run our lexical association experiments in the context of the neces-
sity of recognizingmatching semantic role fillers, approximately 85%ofwhich aremultiword expressions
in our data, the overwhelming majority of which would not be enumerated within conventional lexicons.
We compare four common lexical access approaches to aggregation, preferences, and constraints: bag-
of-words, two different types of maximal alignment, and inversion transduction grammar based methods.
2 Background
The MEANT metric measures weighted f-scores over corresponding semantic frames and role fillers
in the reference and machine translations. Whereas HMEANT uses human annotation, the automatic
versions of MEANT instead replace humans with automatic SRL and alignment algorithms. MEANT
typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human ade-
quacy judgment, and is relatively easy to port to other languages, requiring only an automatic semantic
parser and a monolingual corpus of the output language, which is used to gauge lexical similarity between
the semantic role fillers of the reference and translation. More precisely, MEANT computes scores as
follows:
1. Apply an automatic shallow semantic parser to both the references and MT output. (Figure 1 shows
examples of automatic shallow semantic parses on both reference and MT.)
2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between
the references and MT output according to the lexical similarities of the predicates.
3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to
align the arguments between the reference and MT output according to the lexical similarity of role
fillers.
4. Compute the weighted f-score over the matching role labels of these aligned predicates and role
fillers according to the following definitions:
145
q0
i,j
? ARG j of aligned frame i in MT
q
1
i,j
? ARG j of aligned frame i in REF
w
0
i
?
#tokens filled in aligned frame i of MT
total #tokens in MT
w
1
i
?
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ? weight of similarity of predicates
w
j
? weight of similarity of ARG j
e
i,pred ? the pred of the aligned frame i of the machine translation
f
i,pred ? the pred of the aligned frame i of the reference translation
e
i,j
? the ARG j of the aligned frame i of the machine translation
f
i,j
? the ARG j of the aligned frame i of the reference translation
s(e, f) = lexical similarity of token e and f
prece,f =
?
e?e max
f?f
s(e, f)
| e |
rece,f =
?
f?f max
e?e
s(e, f)
| f |
precision =
?
i
w
0
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
0
i,j
|
?
i
w
0
i
recall =
?
i
w
1
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
1
i,j
|
?
i
w
1
i
MEANT = 2 ? precision ? recallprecision + recall
where the possible approaches to defining the lexical associations s
i,pred and si,j are discussed in the
following section. q0
i,j
and q1
i,j
are the argument of type j in frame i in MT and REF, respectively. w0
i
and w1
i
are the weights for frame i in MT and REF, respectively. These weights estimate the degree
of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of
the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between
the reference translations and the MT output. There is a total of 12 weights for the set of semantic role
labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised
estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and
Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using
relative frequency of each semantic role label in the references and thus UMEANT is useful when human
judgments on adequacy of the development set are unavailable.
3 Comparison of multiword expression association approaches
To assess alternative lexical access preferences and constraints for computing multiword expression
associations, we now consider four alternative approaches to defining the lexical similarities s
i,pred and
s
i,j
, all of which employ a standard context vector model of the individual words/tokens in the multiword
expression arguments between the reference and machine translations, as descibed by Lo et al. (2012)
and Tumuluru et al. (2012).
3.1 Bag of words (geometric mean)
The original MEANT approaches employed standard a bag-of-words strategy for lexical association.
This baseline approach applies no alignment constraints on multiword expressions:
s
i,pred = e
?
e?e
i,pred
?
f?f
i,pred
lg(s(e,f))
|e
i,pred|?|fi,pred|
s
i,j
= e
?
e?e
i,j
?
f?f
i,j
lg(s(e,f))
|e
i,j
|?|f
i,j
|
146
3.2 Maximum alignment (precision-recall average)
In the first maximum alignment based approach we will consider, the definitions of s
i,pred and si,j are
inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length.
s
i,pred =
1
2
(prece
i,pred,fi,pred + recei,pred,fi,pred)
s
i,j
=
1
2
(prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
)
3.3 Maximum alignment (f-score)
The second of the maximum alignment based approaches replaces the above linear averaging of pre-
cision and recall with a proper f-score. Although this is less consistent with the previous literature, such
as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT,
and thus we include it in our comparison as a variant of the maximum alignment strategy.
s
i,pred =
2 ? prece
i,pred,fi,pred ? recei,pred,fi,pred
prece
i,pred,fi,pred + recei,pred,fi,pred
s
i,j
=
2 ? prece
i,j
,f
i,j
? rece
i,j
,f
i,j
prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
3.4 Inversion transduction grammar based
There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve
the accuracy of MT evaluation metrics?despite (1) long empirical evidence the vast majority of transla-
tion patterns between human languages can be accommodated within ITG constraints, and (2) the obser-
vation thatmost current state-of-the-art SMT systems employ ITG decoders. Especially when considering
semanticMTmetrics, ITGs would seem to be a natural strategy for multiword expression association for
several cognitively motivated reasons, having to do with language universal properties of cross-linguistic
semantic frame structure.
To begin with, it is quite natural to think of sentences as having been generated from an abstract concept
using a rewriting system: a stochastic grammar predicts how frequently any particular realization of the
abstract concept will be generated. The bilingual analogy is a transduction grammar generating a pair
of possible realizations of the same underlying concept. Stochastic transduction grammars predict how
frequently a particular pair of realizations will be generated, and thus represent a good way to evaluate
how well a pair of sentences correspond to each other.
The particular class of transduction grammars known as ITGs tackle the problem that the (bi)parsing
complexity for general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By
constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted
reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low
end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to
have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and
space (O
(
n
6
)
time and O
(
n
4
)
space). It is also possible to do approximate biparsing in O
(
n
3
)
time
(Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an
ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have also been directly shown to be more than sufficient
to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This
language universal property has an evolutionary explanation in terms of computational efficiency and
cognitive load for language learnability and interpretability (Wu, 2014).
ITGs are thus an appealing alternative for evaluating the possible links between both semantic role
fillers in different languages as well as the predicates, and how these parts fit together to form entire
semantic frames. We believe that ITGs are not only capable of generating the desired structural corre-
spondences between the semantic structures of two languages, but also provide meaningful constraints
to prevent alignments from wandering off in the wrong direction.
Following this reasoning, alternate definitions of s
i,pred and si,j can be constructed in terms of brack-
eting ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated
147
nonterminal category (Wu, 1995a). The idea is to attack a potential weakness of the foregoing three
lexical association strategies, namely that word/token alignments between the reference and machine
translations are severely underconstrained. No bijectivity or permutation restrictions are applied, even
between compositional segments where this should be natural. This can cause multiword expressions of
semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inver-
sion transduction grammar can potentially better constrain permissible token alignment patterns between
aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed
permutations between semantic role fillers across the reference and machine translations for a sample
sentence from the evaluation data.
In this approach, both alignment and scoring are performed utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define s
i,pred and
s
i,j
as follows.
s
i,pred = lg?1
?
?
lg
(
P
(
A ?? e
i,pred/fi,pred|G
))
max(| e
i,pred |, | fi,pred |)
?
?
s
i,j
= lg?1
?
?
lg
(
P
(
A ?? e
i,j
/f
i,j
|G
))
max(| e
i,j
|, | f
i,j
|)
?
?
where
G ? ?{A} ,W0,W1,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 1
p (e/f |A) = s(e, f)
Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with
e ? W
0
? {?} denoting a token in the MT output (or the null token) and f ? W1 ? {?} denoting
a token in the reference translation (or the null token). The rule probability (or more accurately, rule
weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is
defined by MEANT?s lexical similarity measure on English Gigaword context vectors. To calculate the
inside probability (or more accurately, inside score) of a pair of segments, P
(
A ?? e/f|G
)
, we use the
algorithm described in Saers et al. (2009). Given this, s
i,pred and si,j now represent the length normalized
BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and
machine translations.
4 Experiments
In this section we discuss experiments comparing the four alternative lexical access preference and
constraint strategies.
4.1 Experimental setup
We compared using the DARPA GALE P2.5 Chinese-English translation test set, as used in Lo and
Wu (2011a). The corpus includes the Chinese input sentences, each accompanied by an English reference
translation and three participating state-of-the-art MT systems? output.
We computed sentence-level correlations following the benchmark assessment procedure used by
WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; Mach??ek and Bojar,
2013), which use Kendall?s ? correlation coefficient, to evaluate the correlation of evaluation metrics
against human judgment on ranking the translation adequacy of the three systems? output. A higher
value for Kendall?s ? indicates more similarity to the human adequacy rankings by the evaluation met-
rics. The range of possible values of Kendall?s ? correlation coefficient is [-1, 1], where 1 means the
148
Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE
P2.5 data. For reference, the human HMEANT upper bound is 0.53?so the fully automatic ITG based
MEANT approximation is not far from closing the gap.
Kendall correlation
MEANT + ITG based 0.51
MEANT + maximum alignment (f-score) 0.48
MEANT + maximum alignment (average of precision & recall) 0.46
MEANT + bag of words (geometric mean) 0.38
NIST 0.29
METEOR 0.20
BLEU 0.20
TER 0.20
PER 0.20
CDER 0.12
WER 0.10
systems are ranked in the same order as the human judgment by the evaluation metric; and -1 means the
systems are ranked in the reverse order as human judgment by the evaluation metric.
For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler
was used to automatically predict semantic parses.
4.2 Results and discussion
The sentence-level correlations in Table 1 show that the ITG based strategy outperforms other auto-
matic metrics in correlation with human adequacy judgment. Note that this was achieved with no tuning
whatsoever of the rule weights (suggesting that the performance could be further improved in the future
by slightly optimizing the ITG weights).
The ITG based strategy shows 3 points improvement over the next best strategy, which is maximal
alignment under f-score aggregation. The ITG based approach produces much higher HAJ correlations
than any of the other metrics.
In fact, the ITG based strategy even comes within a few points of the human upper bound bench-
mark HAJ correlations computed using the human labeled semantic frames and alignments used in the
HMEANT.
Data analysis reveals two reasons that the ITG based strategy correlates with human adequacy judge-
ment more closely than the other approaches. First, BITG constraints indeed provide more accurate
phrasal similarity aggregation, compared to the naive bag-of-words based heuristics. Similar results
have been observed while trying to estimate word alignment probabilities where BITG constraints out-
performed alignments from GIZA++ (Saers and Wu, 2009). Secondly, the permutation and bijectivity
constraints enforced by the ITG provide better leverage to reject token alignments when they are not
appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The
ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave
tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not
simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed
that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered
in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better
fits the semantic structure.
5 Conclusion
We have compared four alternative lexical access strategies for aggregation, preferences, and con-
straints in scoringmultiword expression associations that are far too numerous to be explicitly enumerated
in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words,
149
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence ????????????
???????? Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
150
two maximum alignment based approaches, and an inversion transduction grammar based approach.
Controlled experiments within the MEANT semantic MT evaluation framework shows that the cog-
nitively motivated ITG based strategy achieves significantly higher correlation with human adequacy
judgments of MT output quality than the more typically used lexical association approaches. The results
show how to improve upon previous research showing that MEANT?s explicit use of semantic frames
leads to state-of-the-art automatic MT evaluation, by aligning and scoring semantic frames under a sim-
ple, consistent ITG that provides empirically informative permutation and bijectivity biases, instead of
more naive maximal alignment or bag-of-words assumptions.
Cognitive studies of the lexicon are often described using intrinsic measures of quality. Our exper-
iments complement this by situating the empirical comparisons within extrinsic real-world task-based
performance measures. We believe that progress can be accelerated via a combination of intrinsic and
extrinsic measures of lexicon acquisition and access models.
Acknowledgments
This material is based upon work supported in part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract nos. HR0011-12-C-0014 and HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-0023; by the European Union under the FP7
grant agreement no. 287658; and by the Hong Kong Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA,
the EU, or RGC.
References
Karteek Addanki, Chi-kiu Lo, Markus Saers, and Dekai Wu. LTG vs. ITG coverage of cross-lingual verb
frame alternations. In 16th Annual Conference of the European Association for Machine Translation
(EAMT-2012), Trento, Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation, and Compiling. Prentice-Halll,
Englewood Cliffs, New Jersey, 1972.
Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the European Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third Workshop on Statistical Machine Translation (WMT-
08), 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Pryzbocki, and Omar Zaidan.
Findings of the 2010 joint workshop on statistical machine translation and metrics for machine trans-
lation. In Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (WMT10), pages
17?53, Uppsala, Sweden, 15-16 July 2010.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan. Findings of the 2011
Workshop on Statistical Machine Translation. In 6th Workshop on Statistical Machine Translation
(WMT 2011), 2011.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. Find-
ings of the 2012 workshop on statistical machine translation. In 7th Workshop on Statistical Machine
Translation (WMT 2012), pages 10?51, 2012.
151
George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence
statistics. In The second international conference on Human Language Technology Research
(HLT ?02), San Diego, California, 2002.
Philipp Koehn and Christof Monz. Manual and automatic evaluation of machine translation between
european languages. InWorkshop on Statistical Machine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL-2006), 2006.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluat-
ing translation utility based on semantic roles. In 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. supervised weight estimation for semantic MT evaluation
metrics. In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6),
2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and Dekai Wu. Improving machine translation by training
against an automatic semantic frame based evaluationmetric. In 51st AnnualMeeting of the Association
for Computational Linguistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Improving machine translation into Chinese by tuning
against Chinese MEANT. In International Workshop on Spoken Language Translation (IWSLT 2013),
2013.
Matou? Mach??ek and Ond?ej Bojar. Results of the WMT13 metrics shared task. In Eighth Workshop
on Statistical Machine Translation (WMT 2013), Sofia, Bulgaria, August 2013.
I. DanMelamed. Automatic construction of clean broad-coverage translation lexicons. In 2nd Conference
of the Association for Machine Translation in the Americas (AMTA-1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first National Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Hermann Ney. A evaluation tool for machine transla-
tion: Fast evaluation forMT research. In The Second International Conference on Language Resources
and Evaluation (LREC 2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics
(ACL-02), pages 311?318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow semantic
parsing using support vector machines. In Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), 2004.
Markus Saers and Dekai Wu. Improving phrase-based translation via word alignments from stochastic
inversion transduction grammars. In Third Workshop on Syntax and Structure in Statistical Translation
(SSST-3), pages 28?36, Boulder, Colorado, June 2009.
152
Markus Saers, JoakimNivre, and DekaiWu. Learning stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies
(IWPT?09), pages 29?32, Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and JohnMakhoul. A study of trans-
lation edit rate with targeted human annotation. In 7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages 223?231, Cambridge, Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lex-
ical similarity of semantic role fillers for automatic semantic MT evaluation. In 26th Pacific Asia
Conference on Language, Information, and Computation (PACLIC 26), 2012.
Dekai Wu. An algorithm for simultaneously bracketing parallel texts by aligning words. In 33rd An-
nual Meeting of the Association for Computational Linguistics (ACL 95), pages 244?251, Cambridge,
Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In Third Annual Workshop
on Very Large Corpora (WVLC-3), pages 69?81, Cambridge, Massachusetts, June 1995.
Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, 1997.
Dekai Wu. The magic number 4: Evolutionary pressures on semantic frame structure. In 10th Interna-
tional Conference on the Evolution of Language (Evolang X), Vienna, Apr 2014.
Richard Zens and Hermann Ney. A comparative study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),
pages 144?151, Stroudsburg, Pennsylvania, 2003.
153
