Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 716?721,
Dublin, Ireland, August 23-24, 2014.
UMCC_DLSI_SemSim: Multilingual System for Measuring  
Semantic Textual Similarity 
 
 
Alexander Ch?vez 
H?ctor D?vila 
DI, University of Matanzas, Cuba.  
{alexander.chavez,  
hector.davila}@umcc.cu 
Yoan Guti?rrez 
Antonio Fern?ndez-Orqu?n 
Andr?s Montoyo 
Rafael Mu?oz 
DLSI, University of Alicante, Spain. 
{ygutierrez 
montoyo,rafael}@dlsi.ua.es, 
antonybr@yahoo.com 
 
Abstract 
In this paper we describe the 
specifications and results of 
UMCC_DLSI system, which was 
involved in Semeval-2014 addressing two 
subtasks of Semantic Textual Similarity 
(STS, Task 10, for English and Spanish), 
and one subtask of Cross-Level Semantic 
Similarity (Task 3). As a supervised 
system, it was provided by different types 
of lexical and semantic features to train a 
classifier which was used to decide the 
correct answers for distinct subtasks. 
These features were obtained applying the 
Hungarian algorithm over a semantic 
network to create semantic alignments 
among words. Regarding the Spanish 
subtask of Task 10 two runs were 
submitted, where our Run2 was the best 
ranked with a general correlation of 0.807. 
However, for English subtask our best run 
(Run1 of our 3 runs) reached 16th place of 
38 of the official ranking, obtaining a 
general correlation of 0.682. In terms of 
Task 3, only addressing Paragraph to 
Sentence subtask, our best run (Run1 of 2 
runs) obtained a correlation value of 0.760 
reaching 3rd place of 34. 
1 Introduction 
Many applications of language processing rely on 
measures of proximity or remoteness of various 
kinds of linguistic units (words, meanings, 
sentences, documents). Thus, issues such as 
disambiguation of meanings, detection of lexical 
chains, establishing relationships between 
documents, clustering, etc., require accurate 
similarity measures. 
The problem of formalizing and quantifying an 
intuitive notion of similarity has a long history in 
philosophy, psychology, artificial intelligence, 
and through the years has followed many different 
perspectives (Hirst, 2001). Recent research in the 
field of Computational Linguistics has 
emphasized the perspective of semantic relations 
between two lexemes in a lexical resource, or its 
inverse, semantic distance. The similarity of 
sentences is a confidence score that reflects the 
relationship between the meanings of two 
sentences. This similarity has been addressed in 
the literature with terminologies such as affinity, 
proximity, distance, difference and divergence 
(Jenhani, et al., 2007). The different applications 
of text similarity have been separated into a group 
of similarity tasks: between two long texts, for 
document classification; between a short text with 
a long text, for Web search; and between two short 
texts, for paraphrase recognition, automatic 
machine translation, etc. (Han, et al., 2013). 
At present, the calculation of the similarity 
between texts has been tackled from different 
points of views. Some have opted for a single 
measure to capture all the features of texts and 
other models have been trained with various 
measures to take text features separately. In this 
work, we addressed the combination of several 
measures using a Supervised Machine Learning 
(SVM) approach. Moreover, we intend to 
introduce a new approach to calculate textual 
similarities using a knowledge-based system, 
which is based on a set of cases composed by a 
vector with values of several measures. We also 
combined both approaches. 
This work is licensed under a Creative Commons 
Attribution 4.0 International Licence. Page numbers and 
proceedings footer are added by the organisers. Licence 
details: http://creativecommons.org/licenses/by/4.0/ 
716
After this introduction, the rest of the paper is 
organized as follows. Section 2 shows the Pre-
processing stage. Subsequently, in Section 3 we 
show the different features used in our system. In 
Section 4 we describe our knowledge-based 
system. Tasks and runs are provided in Section 5. 
Finally, the conclusions and further work can be 
found in Section 6. 
2 Pre-processing 
Below are listed the pre-processing steps 
performed by our system. In bold we emphasize 
some cases which were used in different tasks. 
? All brackets were removed.  
? The abbreviations were expanded to their 
respective meanings. It was applied using a 
list of the most common abbreviations in 
English, with 819 and Spanish with 473. 
Phrases like ?The G8? and ?The Group of 
Eight? are detected as identical. 
? Deletion of hyphen to identify words 
forms. For example, ?well-studied? was 
replaced by ?well studied?. Example taken 
from line 13 of MSRpar corpus in test set 
of Semeval STS 2012 (Agirre, et al., 2012). 
? The sentences were tokenized and POS-
tagged using Freeling 3.0 (Padr? and 
Stanilovsky, 2012). 
? All contractions were expanded. For 
example: n't, 'mand 's. In the case of 's was 
replaced with ?is? or ?of?, ?Tom's bad? to 
?Tom is bad? and ?Tom's child? by "Child 
of Tom". (Only for English tasks). 
? Punctuation marks were removed from the 
tokens except for the decimal point in 
numbers. 
? Stop words were removed. We used a list 
of the most common stop words. (28 for 
English and 48 for Spanish). 
? The words were mapped to the most 
common sense of WordNet 3.0. (Only for 
Spanish task). 
? A syntactic tree was built for every 
sentence using Freeling 3.0. 
                                                 
1 The windows is the number of intermediate words 
between two words. 
2 Dataset of high quality English paragraphs containing over 
three billion words and it is available in 
http://ebiquity.umbc.edu/resource/html/id/351 
3 Features Extraction 
Measures of semantic similarity have been 
traditionally used between words or concepts, and 
much less between text segments, (i.e. two or 
more words). The emphasis on word to word 
similarity is probably due to the availability of 
resources that specifically encode relations 
between words or concepts (e.g. WordNet) 
(Mihalcea, et al., 2006). Following we describe 
the similarity measures used in this approach. 
3.1 Semantic Similarity of Words 
A relatively large number of word to word 
similarity metrics have previously been proposed 
in the literature, ranging from distance-oriented 
measures computed on semantic networks, to 
metrics based on models of distributional 
similarity learned from large text collections 
(Mihalcea, et al., 2006). 
3.2 Corpus-based Measures 
Corpus-based measures of word semantic 
similarity try to identify the degree of similarity 
between words using information exclusively 
derived from large corpora (Mihalcea, et al., 
2006). We considered one metric named Latent 
Semantic Analysis (LSA) (Landauer, et al., 1998). 
Latent Semantic Analysis: The Latent 
semantic analysis (LSA) is a corpus/document 
based measure proposed by Landauer in 1998. In 
LSA term co-occurrences in a corpus are captured 
by means of a dimensionality reduction operated 
by singular value decomposition (SVD) on the 
term-by-document matrix ?  representing the 
corpus (Mihalcea, et al., 2006). There is a 
variation of LSA called HAL (Hyperspace 
Analog to Language) (Burgess, et al., 1998) that 
is based on the co-occurrence of words in a 
common context. The variation consists of 
counting the number of occurrences in that two 
words appear at n1 distance (called windows). 
For the co-occurrence matrix of words we took 
as core the UMBC WebBase corpus2 (Han, et al., 
2013), which is derived from the Stanford 
WebBase project3 . For the calculation of HAL 
measure we used the Cosine Similarity between 
the vectors for each pair of words. 
3 Stanford WebBase 2001. http://bit.ly/WebBase.  
 
717
3.3 Knowledge-based Measures 
There are many measures developed to quantify 
the degree of semantic relation between two 
words senses using semantic network 
information. For example: 
Leacock & Chodorow Similarity: The 
Leacock & Chodorow (LC) similarity is 
determined as follows: 
????? = ? log (
??????
2??
)        (1) 
Where length is the length of the shortest path 
between senses using node-counting and D is the 
maximum depth of the taxonomy (Leacock and 
Chodorow, 1998) 
Wu and Palmer: The Wu and Palmer 
similarity metric (Wup) measures the depth of two 
given senses in the WordNet taxonomy, and the 
depth of the least common subsumer (LCS), and 
combine them into a similarity score (Wu and 
Palmer, 1994): 
?????? =
2??????(???)
?????(?????1)+?????(?????2)
      (2) 
 
Resnik: The Resnik similarity (Res) returns 
the information content (IC) of the LCS of two 
senses: 
?????? = ??(???)      (3) 
 
Where IC is defined as: 
 
??(?) = ? log?(?)       (4) 
 
And P(c) is the probability of encountering an 
instance of sense c in a large corpus (Resnik, 
1995) (Mihalcea, et al., 2006). 
Lin: The Lin similarity builds on Resnik?s 
measure and adds a normalization factor 
consisting of the information content of the two 
inputs senses (Lin, 1998): 
 
?????? =
2???(???)
??(?????)+??(?????2)
   (5) 
 
Jiang & Conrath: The Jiang and Conrath 
similarity (JC) is defined as follows (Jiang and 
Conrath, 1997): 
????? =
1
??(?????1)+??(?????2)?2???(???)
   (6) 
 
PathLen: The PathLen similarity (Len) 
involves the path lengths between two senses in 
the taxonomy (Pedersen, et al., 2004). 
                                                 
4 Copyright (c) 2006 by Chris Parkinson, available in 
http://sourceforge.net/projects/simmetrics/ 
 
??????? = ? log ???????(?????1, ?????1)(7) 
 
Where ???????(?????1, ?????1)  is the 
number of edges in the shortest path between 
?????1and ?????2. 
Word Similarity: In order to calculate the 
similarity between two words (WS) we used the 
following expression: 
 
??(?1,?2) =????1 ? ??????(?1)
?2 ? ??????(?2)
???(?1, ?2) 
(8) 
 
Where  ???(?1, ?2)  is one of the similarity 
metrics at sense level previously described. 
3.4 Lexical Features 
We used a well-known lexical attributes similarity 
measures based on distances between strings. 
Dice-Similarity, Euclidean-Distance, Jaccard-
Similarity, Jaro-Winkler, Levenstein Distance, 
Overlap-Coefficient, QGrams Distance, Smith-
Waterman, Smith-Waterman-Gotoh, Smith-
Waterman-Gotoh-Windowed-Affine. 
These metrics have been obtained from an API 
(Application Program Interface) SimMetrics 
library v1.5 for.NET4 2.0. 
3.5 Word Similarity Models 
With the purpose of calculating the similarity 
between two words, we developed two models 
involving the previous word similarity metrics. 
These were defined as: 
Max Word Similarity: The Max Word 
Similarity (MaxSim) is defined as follows: 
 
??????(?1,?2) =         
                
{
1              ????????????(?1,?2) = 1
??? (??????(?1,?2), ??????(?1,?2))
 
(9) 
Where ??????????(?1,?2) is the QGram-
Distance between w1 and w2. 
Statistics and Weight Ratio: For calculating 
the weight ratio in this measure of similarity was 
used WordNet 3.0 and it was defined in (10): 
????????? (?1,?2) =   
(??????(?1,?2) + (
1
??????(?1,?2)
))
2
 
 
(10) 
718
 
Where ??????(?1,?2) takes a value based 
on the type of relationship between w1 and w2. 
The possible values are defined in Table 1. 
Value Relation between ?1 and ?2  
10 Antonym. 
1 Synonym. 
2 Direct Hypernym, Similar_To or 
Derivationally Related Form. 
3 Two-links indirect Hypernym, Similar_To 
or Derivationally Related Form. 
3 One word is often found in the gloss of the 
other. 
9 Otherwise. 
Table 1: Values of Weight Ratio. 
3.6 Sentence Alignment 
In the recognition of texts? similarities, several 
methods of lexical alignment have been used and 
can be appreciated by different point of views 
(Brockett, 2007) (Dagan, et al., 2005). Glickman 
(2006) used the measurement of the overleap 
grade between bags of words as a form of 
sentence alignment. Rada et al. (2006) made 
reference to an all-for-all alignment, leaving open 
the possibility when the same word of a sentence 
is aligned with several sentences. For this task, we 
used the Hungarian assignment algorithm as a 
way to align two sentences (Kuhn, 1955). Using 
that, the alignment cost between the sentences was 
reduced. To increase the semantic possibilities we 
used all word similarity metrics (including the two 
word similarity models) as a function cost. 
3.7 N-Grams Alignment 
Using the Max Word Similarity model, we 
calculated three features based on 2-gram, 3-gram 
and 4-gram alignment with the Hungarian 
algorithm. 
4 Knowledge-based System 
For similarity calculation between two phrases, 
we developed a knowledge-based system using 
SemEval-2012, SemEval-2013 and SemEval-
2014 training corpus (Task 10 and Task 1 for the 
last one). For each training pair of phrases we 
obtained a vector with all measures explained 
above. Having it, we estimated the similarity 
value between two new phrases by applying the 
Euclidian distance between the new vector (made 
with the sentence pair we want to estimate the 
similarity value) and each vector in the training 
corpus. Then, the value of the instance with minor 
Euclidian Distance was assigned to the new pair 
of phrases. 
5 Tasks and runs 
Our system participated in Sentence to Phrase 
subtask of Task 3: ?Cross-Level Semantic 
Similarity? (Jurgens, et al., 2014) and in two 
subtasks of Task 10: ?Multilingual Semantic 
Textual Similarity? of SemEval-2014. It is 
important to remark that our system, using SVM 
approach, did not participate in Task 1: 
?Evaluation of compositional distributional 
semantic models on full sentences through 
semantic relatedness and textual entailment?, due 
to deadline issues. We compared our system 
results with the final ranking of Task 1 and we 
could have reached the 6th place of the ranking for 
Relatedness Subtask with a 0.781 of correlation 
coefficient, and the 9th place for Entailment 
Subtask with an accuracy of 77.41%. 
 
 
 
 
Task 
10 
Sp 
Task 10 
En 
Task 3 
Sentence 
to 
Phrase 
Features/Runs 1 2 1 2 3 1 2 
PathLenAlign x  x x  x x 
ResAlign x  x x  x x 
LcAlign x  x x  x x 
WupAlign x  x x  x x 
Res x  x x  x x 
Lc x  x x  x x 
DiceSimilarity x x x x  x x 
EuclideanDistance x x x x  x x 
JaccardSimilarity x x x x  x x 
JaroWinkler x x x x  x x 
Levenstein x x x x  x x 
Overlap- 
Coefficient 
x x x x  x x 
QGramsDistance x x x x  x x 
SmithWaterman x x x x  x x 
SmithWatermanGotoh x x x x  x x 
SmithWatermanGotoh- 
WindowedAffine 
x x x x  x x 
BiGramAlingHungMax x  x x  x x 
TriGramAlingHungMax x  x x  x x 
TetraGramAlingHungMax x  x x  x x 
WordAlingHungStatWeigthRatio x  x x  x x 
SentenceLengthPhrase1 x  x x  x x 
SentenceLengthPhrase2 x  x x  x x 
Table 2: Features and runs. Spanish (Sp) and 
English (En). 
In Table 2 is important to remark the 
following situations: 
? In Task 10 Spanish (two runs), we used the 
training corpus of Task 10 English. 
719
? In Run2 of Task 10 English, the similarity 
score was replaced for the knowledge-
based system value if Euclidean Distance 
of the most similar case was less than 0.30.  
? Run3 of Task 10 English was a knowledge-
based system. 
? In Run1 of Sentence to Phrase of Task 3, 
we trained the SVM model using only the 
training corpus of this task. 
? In Run2 of Sentence to Phrase of Task 3, 
we trained the SVM model using the 
training corpus of this task and the training 
corpus of Task 10 English. 
6 Conclusion 
In this paper we introduced a new framework for 
recognizing Semantic Textual Similarity, 
involving feature extraction for SVM model and a 
knowledge-based system. We analyzed different 
ways to estimate textual similarities applying this 
framework. We can see in Table 3 that all runs 
obtained encouraging results. Our best run was 
first position of the ranking for task 10 (Spanish) 
and other important positions were reached in the 
others subtasks. According to our participation, 
we used a SVM which works with features 
extracted from six different strategies: String-
Based Similarity Measures, Semantic Similarity 
Measures, Lexical-Semantic Alignment, 
Statistical Similarity Measures and Semantic 
Alignment. Finally, we can conclude that our 
system achieved important results and it is able to 
be applied on different scenarios, such as task 10, 
task 3.1 and task 1. See Table 3 and the beginning 
of Section 5. 
Subtask Run 
SemEval-
2014 
Position 
Task 10-
Spanish 
Run1 4 
Run2 1 
Task 10-
English 
Run1 16 
Run2 18 
Run3 33 
Task-3 
Run1 3 
Run2 16 
Table 3: SemEval-2014 results. 
As further work, we plan to analyze the main 
differences between task 10 for Spanish and 
English in order to homogenise both system?s 
results. 
Acknowledgments 
This research work has been partially funded by 
the University of Alicante, Generalitat 
Valenciana, Spanish Government and the 
European Commission through the projects, 
"Tratamiento inteligente de la informaci?n para la 
ayuda a la toma de decisiones" (GRE12-44), 
ATTOS (TIN2012-38536-C03-03), LEGOLANG 
(TIN2012-31224), SAM (FP7-611312), FIRST 
(FP7-287607) and ACOMP/2013/067. 
Reference 
Eneko Agirre, Mona Diab, Daniel Cer and Aitor 
Gonzalez-Agirre, 2012. SemEval 2012 Task 6: A 
Pilot on Semantic Textual Similarity.. s.l., First 
Join Conference on Lexical and Computational 
Semantic (*SEM), Montr?al, Canada. 2012., pp. 
385-393. 
Chris Brockett, 2007. Aligning the RTE 2006 Corpus. 
Microsoft Research, p. 14. 
Curt Burgess, Kay Livesay and Kevin Lund, 1998. 
Explorations in Context Space: Words, 
Sentences, Discourse. Discourse Processes, Issue 
25, pp. 211 - 257. 
Ido Dagan, Oren Glickman and Bernardo Magnini, 
2005. The PASCAL Recognising Textual 
Entailment Challenge. En: Proceedings of the 
PASCAL Challenges Workshop on Recognising 
Textual Entailment. 
Oren Glickman, Ido Dagan and Moshe Koppel, 2006. 
A Lexical Alignment Model for Probabilistic 
Textual Entailment. In: Proceedings of the First 
International Conference on Machine Learning 
Challenges: Evaluating Predictive Uncertainty 
Visual Object Classification, and Recognizing 
Textual Entailment. Southampton, UK: Springer-
Verlag, pp. 287--298. 
Lushan Han et al., 2013. UMBC_EBIQUITY-CORE: 
Semantic Textual Similarity Systems. s.l., s.n. 
Alexander B. Hirst and Graeme, 2001. Semantic 
distance in WordNet: An experimental, 
application-oriented evaluation of five measures. 
Ilyes Jenhani, Nahla Ben Amor and Zi Elouedi, 2007. 
Information Affinity: A New Similarity Measure 
for Possibilistic Uncertain Information. En: 
Symbolic and Quantitative Approaches to 
Reasoning with Uncertainty. s.l.:Springer Berlin 
Heidelberg, pp. 840-852. 
Jay Jiang and David Conrath, 1997. Semantic 
similarity based on corpus statistics and lexical 
taxonomy. s.l., Proceedings of the International 
Conference on Research in Computational 
Linguistics. 
David Jurgens, Mohammad Taher and Roberto 
Navigli, 2014. SemEval-2014 Task 3: Cross-
720
Level Semantic Similarity. Dublin, Ireland, In 
Proceedings of the 8th International Workshop on 
Semantic Evaluation., pp. 23-24. 
Harold W. Kuhn, 1955. The Hungarian Method for the 
assignment problem. Naval Research Logistics 
Quarterly. 
Thomas K. Landauer, Peter W. Foltz and Darrell 
Laham, 1998. Introduction to latent semantic 
analysis. Discourse Processes, Issue 25, pp. 259-
284. 
Claudia Leacock and Martin Chodorow, 1998. 
Combining local context and WordNet sense 
similarity for word sense identification. s.l.:s.n. 
Lin Dekang, 1998. An information-theoretic definition 
of similarity. s.l., Proceedings of the International 
Conf. on Machine Learning. 
Rada Mihalcea, Courtney Corley and Carlo 
Strapparava, 2006. Corpus-based and 
knowledge-based measures of text semantic 
similarity. In: IN AAAI?06. s.l.:21st National 
Conference on Artificial Intelligence, pp. 775--
780. 
Lu?s Padr? and Evgeny Stanilovsky, 2012. FreeLing 
3.0: Towards Wider Multilinguality. Istanbul, 
Turkey, Proceedings of the Language Resources 
and Evaluation Conference (LREC 2012) ELRA. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi, 2004. WordNet::Similarity - 
Measuring the Relatedness of Concepts. 
American Association for Artificial Intelligence. 
Philip Resnik, 1995. Using information content to 
evaluate semantic similarity. s.l., Proceedings of 
the 14th International Joint Conference on 
Artificial Intelligence. 
Zhibiao Wu and Martha Palmer, 1994. Verb semantics 
and lexical selection. 
 
 
721
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 722?726,
Dublin, Ireland, August 23-24, 2014.
UMCC_DLSI_Prob: A Probabilistic Automata for Aspect Based 
Sentiment Analysis
Yenier Casta?eda 
 Armando Collazo 
 Elvis Crego 
Jorge L. Garcia 
Yoan Guti?rrez 
 David Tom?s 
Andr?s Montoyo 
 Rafael Mu?oz
DI, University of Matanzas 
Matanzas, Cuba 
DLSI, University of Alicante 
Alicante, Spain
{yenier.castaneda, 
armando.collazo}@umcc.cu, 
elvis.crego@mtz.cu, 
jorge.garcia@infonet.umcc.cu 
{ygutierrez,dtomas,montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
This work introduces a new approach for 
aspect based sentiment analysis task. Its main 
purpose is to automatically assign the correct 
polarity for the aspect term in a phrase. It is a 
probabilistic automata where each state 
consists of all the nouns, adjectives, verbs and 
adverbs found in an annotated corpora. Each 
one of them contains the number of 
occurrences in the annotated corpora for the 
four required polarities (i.e. positive, negative, 
neutral and conflict). Also, the transitions 
between states have been taken into account. 
These values were used to assign the predicted 
polarity when a pattern was found in a 
sentence; if a pattern cannot be applied, the 
probabilities of the polarities between states 
were computed in order to predict the right 
polarity. The system achieved results around 
66% and 57% of recall for the restaurant and 
laptop domain respectively. 
1 Introduction 
Sentiment analysis is increasingly viewed as a 
vital task from both an academic and a 
commercial standpoint. Textual information has 
become one of the most important sources of data 
to extract useful and heterogeneous knowledge. 
?Texts can provide factual information, such as: 
descriptions, lists of characteristics, or even 
instructions to opinion-based information, which 
would include reviews, emotions, or feelings. 
These facts have motivated dealing with the 
identification and extraction of opinions and 
sentiments in texts that require special attention.? 
(Guti?rrez, et al., 2014). Sentiment Analysis or 
?Subjectivity Analysis? in (Liu, 2010) is defined 
as the computational treatment of opinions, 
sentiments and emotions expressed in a text. In 
order to automatically treat the subjectivity, we 
need lexical resources that allow the detection and 
evaluation of the affective/ subjective charges in 
texts, its polarity and intensity.  
Regarding research carried out for linguistic 
patterns identification and its polarity in texts, it is 
worth mentioning works on: adjectives 
(Hatzivassiloglou and McKeown, 1997) (Wiebe, 
2000); adjectives and verbs (Turney, 2002) 
(Wilson, et al., 2005) (Takamura, et al., 2007); 
and also verbs and names (Esuli and Sebastini, 
2006). WordNet (Fellbaum, 1998) has also been 
used for the collection of opinion adjectives and 
verbs (Kim and Hovy, 2005) to determine the 
semantic orientation of the terms depending on 
their notes (Esuli and Sebastiani, 2005), for the 
adjective extraction (Andreevskaia and Bergler, 
2006) or opinion mining (Esuli and Sebastiani, 
2007).  
Inspired on Hidden Markov models (Baum 
and Petrie, 1966) and following the idea that 
words combinations are finite in an evaluation 
text, we decided to create a finite automata in 
graph form to represent all these relations 
extracted from a training corpus. For the creation 
of this automata we utilised different resources, 
such as WordNet and OpinionFinder Subjectivity 
Lexicon. Also, different extracted patterns based 
on (Cazab?n, 1973) were applied. 
This paper is structured as follows: In section 
1.1 is described the task 4 of SemEval2014 
(Pontiki, et al., 2014) where this system was 
presented. Section 2 presents the description of 
the automata and how it was built. The polarity 
assignation method using the trained automata is 
_________________________ 
This work is licensed under a Creative Commons 
Attribution 4.0 International Licence. Page numbers and 
proceedings footer are added by the organisers. Licence 
details: http://creativecommons.org/licenses/by/4.0/ 
722
described in section 3. Finally, in section 4 and 5 
are shown the results and conclusions, 
respectively. 
1.1 Task Description 
The SemEval2014 task 4 (Pontiki, et al., 2014) 
was divided into four subtasks: 4.1 Aspect term 
extraction; 4.2 Aspect term polarity; 4.3 Aspect 
category detection; and 4.4 Aspect category 
polarity. 
This paper is focused on subtask 4.2 which is 
described as follows: 
Given one or more Aspect Terms within a 
sentence, it is necessary to determine whether the 
polarity of each Aspect Term is positive, negative, 
neutral or conflict (i.e., both positive and 
negative). For example: 
?I loved their fajitas? ? 
?fajitas?: positive 
?I hated their fajitas, but 
their salads were great? ? 
?fajitas?: negative, 
?salads?: positive 
?The fajitas are their first 
plate? ? ?fajitas?: neutral 
?The fajitas were great to 
taste, but not to see? ? 
?fajitas?: conflict. 
Each participant was permitted to submit two 
kinds of runs for this task: 
Constrained: Using only the provided training 
data and other resources, such as lexicons. 
Unconstrained: Using additional data for 
training. Teams were asked to report what 
resources they used for each submitted run. 
The training dataset, provided by the organiser of 
the Task 4 challenge, consists of two domain-
specific datasets which contain over 6,500 
sentences with fine-grained aspect-level human-
authored annotations. These domains are: 
Restaurant reviews: This dataset consists of 
over 3000 English sentences from the restaurant 
reviews of (Ganu, et al., 2009) that were adapted 
to the task. 
Laptop reviews: This dataset consists of over 
3000 English sentences extracted from customer 
reviews of laptops. 
2 The automata 
The automata was represented as a graph ? =
 (?, ?) whose vertexes constitute the group of 
finite states ? =  [?1, ?2, ?3, ? , ??] while the 
                                                          
1http://alt.qcri.org/semeval2014/task4/ 
edges represent the transitions ? =
 [?1, ?2, ?3, ? , ??] of going from one state to 
another. 
Our finite automata involves the following 
features: 
1. Group of finite states: all the verbs, nouns, 
adverbs, adjectives that were extracted from 
the training dataset (see Section 2.1) using 
Freeling 3.1 language analyser (Atserias, et 
al., 2006), or Aspect Terms (that may be 
formed by several words). In every state the 
automata stores the occurrences ??
?
, where 
p is one of the following polarity classes: 
positive, negative, neutral, conflict or 
undefined, i being the index of the current 
state in the graph. 
2. Finite alphabet: a sentences set which 
contains one or more Aspect Terms to 
which should be assigned a polarity. 
3. Initial state: first word of the sentence. 
4. Transition state (???,? and ???,?): each 
transition between two states contains ??,?
?   
and ??,?
?
, where p is  positive, negative, 
neutral  or conflict, i is the current state, and 
j is the next state. 
5. End state: last word of the sentence. 
If we could not determine the polarity 
classification for a state or transition, then we set 
it as undefined polarity. 
2.1 Training the automata 
In order to create the automata the training dataset 
provided for the SemEval2014 taks 41 was used. 
In the automata, each word of a sentence forms 
a state which is connected to the following word. 
This connection forms a transition between the 
two words. This method is repeated until the last 
word of the sentence is reached. If the word 
already exists in the automata, both its state and 
all the transitions (from and to that word) are 
adjusted, increasing in one the ??
?
, ??,?
?
and ??,?
?
 
of the polarity value initially assigned in the 
corpus.  
The transitions from words to Aspect Terms 
with their respective polarities allow to go through 
those words with undefined polarities to the target 
Aspect Terms. This event is done for finding the 
most probably polarity according to the training 
discoveries. Same thing happens with transitions 
from an Aspect Term to a word, but in this case 
from the polarity of the Aspect Term to undefined 
polarity. 
723
On the other hand, if the word is not an Aspect 
Term its state do not change at all, since the 
dataset only annotates the Aspect Terms, so we do 
not know the polarity of those words that are not 
an Aspect Term. 
To solve this issue we decided to make use of 
other resources to enhance the automata, so that 
the probability for finding a polarity for a word in 
the automata increases with the expansion of the 
dictionary. We used the Opinion Finder 
Subjectivity Lexicon (OFSL) (Wilson, et al., 
2005) to adjust the state and transitions of the 
words in the automata. To address the adjustment, 
for every word of OFSL (according to the 
classification of the sentiment polarity) that exists 
in the graph represented by automata, the 
respective value of polarity of ??
?,??,?
?
and ??,?
?
 is 
increased in one. We also used WordNet 3.0 to 
obtain the synonyms and antonyms of the words 
in the automata to form new states and transitions. 
Synonyms were given the same polarity as the 
related word, whereas antonyms took the opposite 
polarity. The subjectivity clues extracted by the 
patterns detected in the training dataset were used 
as well (See section 3.2). 
In Table 1 we show the terminology used for 
the patterns. 
Symbol Description 
[] Optional word 
/! Subjectivity clue 
/l Compare by lemma 
AT Aspect Term 
Table 1: Pattern symbols 
Examples: 
[DT] AT [PRP] [RB] be/l [VBG/!] 
RB/! [JJ/!] [RB/!] 
[RB/!] [DT] JJS/! [DT] [NN] AT [VB] 
[NN/!] 
[DT] JJ/! NN PRP VBD VB [DT] AT 
AT be/l [DT/!] JJ/! [PRP/!] [RB/!] 
Note the use of the POS tags such as DT, NN, 
VBD, and others were taken from the result of the 
pos-tagging process performed by Freeling 3.1. 
Using this tool the incoming texts were split into 
parts (sentences) for the following processes. 
For instance, in the sentence ?This MacBook 
Pro is excellent? the subjectivity clue for the 
Aspect Term MacBook Pro is excellent; so its 
states and transitions get adjusted the same way as 
the Aspect Term. Figure 1 describes this example, 
where pi is ??
?
 , pij is ??,?
?
and pji is ??,?
?
 means the 
occurrence for positive polarity (negative, neutral 
and conflict polarities were omitted by lack of 
space). Both states and transitions are represented. 
 
Figure 1: Adjusting states and transitions after 
pattern analysis. 
3 Polarity Assignation 
Before predicting the polarity of the Aspect 
Terms, each sentence is divided by its connectors 
(conjunctions, prepositions and adverbs, extracted 
using Freeling), forming the corresponding 
phrases. For instance, the sentence ?Where 
Gabriela personally greets you and recommends 
you what to eat? is divided into the phrase ?Where 
Gabriela personally greets you? and the phrase 
?recommends you what to eat? by connector and. 
3.1 Selection criteria 
If only one polarity is found then that is the 
polarity for the Aspect Term. On the other hand, 
if more than one polarity is found, the polarity for 
the Aspect Term is the most repeated one.  
Note that if both positive and negative are the 
most repeated polarities we set conflict as the 
polarity for the Aspect Term. 
If no polarities are found at all, we assignee 
neutral to the Aspect Term. 
3.2 Assigning polarity using patterns 
We detected different patterns which allowed us 
to extract those words that influence on the Aspect 
Term polarity in the phrase (See section 2.1). 
For each phrase subjectivity clue i, we 
calculate the most probable polarity 
???=????(??
?), if i has a state in the automata. 
After that, we apply our selection criteria 
described in section 3.1.  
If no polarities are found at all, we process the 
phrase in the next steps. 
3.3 Assigning polarity using the automata  
For each Aspect Term in the phrase we get the 
sentence it belongs to and we calculate ???,?= 
??,?
?
???,?
?  in that sentence, where ???,? is the most 
probable polarity of ???,? (j being the Aspect 
Term), if such a transition existed. If no polarity 
724
is found, then we calculate ?? ?,?=
??,?
?
???,?
?  again if 
such a transition existed. 
In case of applying aforementioned processes 
without finding out a concrete polarity for the 
target Aspect Terms, we perform other steps to try 
to find one or more polarities for the Aspect Term. 
First, we verify whether the Aspect Term is 
part of a phrase which was matched to a pattern 
but no polarity was found as explained in section 
3.2, if so we get the subjectivity clues of the 
phrase and for each subjectivity clue we calculate 
???,?, where i is the Aspect Term index and j 
corresponds to the subjectivity clue index. If no 
polarity is found, then we calculate ???,?. 
If no polarities are found after this step, we 
proceed to do the same as above, but this time for 
each word in the sentence. 
Lastly, if no polarities are found, ??? is 
obtained for each word i in the sentence if i has a 
state in the automata. 
After performing these steps we apply our 
selection criteria to assign the polarity to the 
Aspect Term in question. As can be seen, our 
proposal is focused on the application of an 
exhaustive exploration of the automata in order to 
classify Aspect Terms with the target polarities. 
4 Results and Discussion 
In order to evaluate the accuracy of the system 
several tests were run. Table 2 shows some of the 
tests using SemEval2014 task4 Baseline for the 
Restaurant reviews. We did the same evaluation 
for Laptop reviews and the results obtained were 
very similar to those shown in Table 2 for 
Restaurant. We used semeval_base.py2 script to 
split the dataset into a train and a test part using an 
80:20 ratio. Despite tests 1, 2 and 3 results do not 
vary much, it is evident that using the three 
training resources yields our best accuracy. 
 
Training Evaluation 
Test Patterns WordNet OFSL Pattern/Automata only Automata only Accuracy (%) 
1 X X X X  58.0 
2 X   X  57.9 
3 X  X X  57.9 
4 X X X  X 54.0 
Table 2: Evaluation over restaurant domain 
With test 4 it is evident that it is better to use 
two methods combined than only one of them, 
since the patterns indicate the words that assign 
polarity to the Aspect Term, making the automata 
more precise with this information at the time of 
assigning the correct polarity. Otherwise, if a 
pattern is not encountered we need to analyse the 
words that are closer to the Aspect Term 
determining the polarity according to the context. 
In addition, not always is assigned a polarity to it 
in case of the pattern found in the context is 
empty. Table 3 shows the results of our system in 
comparison with the best of the challenge 
SemEval2014 subtask 4.2. 
Test Constrained 
Accuracy (%) 
Unconstrained 
Accuracy (%) Rest 66.5 66.8 
Laptop 56.1 57.0 
BRR 80.9 77.6 
BRL 70.4 66.6 
Table 3: Test subtask 4.2 (BRR: Best Ranked for 
Restaurant; BRL Best Ranked for Laptop) 
The system behaved the same as the training 
stage on the competition although the accuracy 
increased. 
                                                          
2 http://alt.qcri.org/semeval2014/task4/data/semeval14-absa-
base-eval-valid.zip 
5 Conclusions and future works 
This work introduces a new approach for aspect 
based sentiment analysis. For that, a probabilistic 
automata was created where the states are formed 
by the nouns, adjectives, verbs and adverbs found 
in the annotated corpora, based on their 
occurrence. The transitions between states are 
also taken into account. A set of patterns were 
defined in order to extract the words that influence 
on an Aspect Term, also known as subjectivity 
clues, and then we predicted their polarity using 
the automata?s probabilities. A system was 
developed following this approach to participate 
on SemEval2014 competition, obtaining an 
accuracy of 66% for restaurant reviews and 57% 
for laptop reviews. 
As future works we plan to deal with the fact 
that this automata only involves states represented 
by the words lack extracted from the training data. 
So, the previously unseen aspect terms which do 
not correspond to any state in the automata, are 
not recognised in many cases as far as the polarity 
is concerned. To address this issue we plan to 
725
expand the aspect term dictionary using 
Wikipedia definitions. On the other hand, we plan 
to use a disambiguation method to select the exact 
WordNet synset and then to reduce the polysemy 
of the automata?s words. Finally, to smooth the 
probabilities it would be interesting to study 
different balances in order to get new 
improvements for the system. 
Acknowledgments 
This research work has been partially funded by 
the University of Alicante, Generalitat 
Valenciana, Spanish Government and the 
European Commission through the projects, 
?Tratamiento inteligente de la informaci?n para la 
ayuda a la toma de decisiones? (GRE12-44), 
ATTOS (TIN2012-38536-C03-03), LEGOLANG 
(TIN2012-31224), SAM (FP7-611312), FIRST 
(FP7-287607) and ACOMP/2013/067. 
References 
Alina Andreevskaia and Sabine Bergler, 2006. Mining 
WordNet for Fuzzy Sentiment: Sentiment Tag 
Extraction from WordNet Glosses. Trento, Italia, 
s.n. 
Jordi Atserias et al., 2006. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Genoa, Italy, s.n. 
Leonard  Baum and Ted Petrie, 1966. Statistical 
Inference for Probabilistic Functions of Finite State 
Markov Chains. The Annals of Mathematical 
Statistics, pp. 1554--1563. 
Mar?a Cazab?n, 1973. Patterns of English. 
s.l.:Editorial Pueblo y Educaci?n. 
Andrea Esuli and Fabrizio Sebastiani , 2005. 
Determining the semantic orientation of terms 
through gloss classification. Proceedings of the 
14th ACM International Conference on 
Information and Knowledge Management, pp. 617-
624. 
Andrea Esuli and Fabrizio Sebastiani, 2007. 
PageRanking WordNet Synsets: An Application to 
Opinion Mining. Prague, Czeck Republic, s.n., pp. 
424-431. 
Andrea Esuli and Fabrizio Sebastiani, 2006. 
SentiWordNet: A Publicly Available Lexical 
Resource for Opinion Mining. Genova, IT, s.n., pp. 
417-422. 
Christiane Fellbaum, 1998. WordNet. An Electronic 
Lexical Database. University of Cambridge: s.n. 
Gayatree Ganu, Noemie Elhadad and Am?lie Marian, 
2009. Beyond the stars: Improving rating 
predictions using review text content. Rhode Island, 
s.n. 
Yoan Guti?rrez, Andy Gonz?lez, Roger P?rez, Jos? I. 
Abreu, Antonio Fern?ndez Orqu?n, Alejandro  
Mosquera, Andr?s Montoyo, Rafael Mu?oz and 
Franc Camara, 2014. UMCC_DLSI-(SA): Using a 
ranking algorithm and informal features to solve 
Sentiment Analysis in Twitter. Second Joint 
Conference on Lexical and Computational 
Semantics (*SEM), Volume 2: Proceedings of the 
Seventh International Workshop on Semantic 
Evaluation (SemEval 2013), pp. 443--449. 
Vasileios Hatzivassiloglou and Kathleen McKeown, 
1997. Predicting the Semantic Orientation of 
Adjectives. Madrid, Spain, s.n., pp. 174-181. 
Soo-Min Kim and Eduard Hovy, 2005. Automatic 
Detection of Opinion Bearing Words and 
Sentences. Jeju Island, Republic of Korea, s.n. 
Bing Liu, 2010. Sentiment Analysis and Subjectivity. 
In: Handbook of Natural Language Processing. 
Boca Raton: s.n., pp. 627-666. 
Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, 
Haris Papageorgiou, Ion Androutsopoulos, 
and Suresh Manandhar, 2014. SemEval-2014 Task 4:  
     Aspect Based Sentiment Analysis. In Proceedings 
     of the 8th International Workshop on Semantic  
     Evaluation (SemEval 2014), Dublin, Ireland. 
Hiroya Takamura, Takashi Inui and Manabu Okumura, 
2007. Extracting Semantic Orientations of Phrases 
from Dictionary. s.l., s.n., pp. 292-299. 
Peter Turney, 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. Philadelphia, 
Pennsylvania, s.n., pp. 417-424. 
Janyce Wiebe, 2000. Learning Subjective Adjectives 
from Corpora. Austin, Texas, s.n. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann, 
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. Vancouver, Canada., s.n. 
 
 
726
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 727?731,
Dublin, Ireland, August 23-24, 2014.
UMCC DLSI: Sentiment Analysis in Twitter using Polirity Lexicons and
Tweet Similarity
Pedro Aniel S
?
anchez-Mirabal,
Yarelis Ruano Torres,
Suilen Hern
?
andez Alvarado
University of Matanzas / Cuba
pedroasm@umcc.cu
yara@umcc.cu
suilen.alvarado@umcc.cu
Yoan Guti
?
errez,
Andr
?
es Montoyo,
Rafael Mu
?
noz
University of Alicante/Spain
ygutierrez@dlsi.ua.es
montoyo@dlsi.ua.es
rafael@dlsi.ua.es
Abstract
This paper describes a system sub-
mitted to SemEval-2014 Task 4B:
Sentiment Analysis in Twitter, by the
team UMCC DLSI Sem integrated by
researchers of the University of Matanzas,
Cuba and the University of Alicante,
Spain. The system adopts a cascade
classification process that uses two classi-
fiers, K-NN using the lexical Levenshtein
metric and a Dagging model trained over
attributes extracted from annotated cor-
pora and sentiment lexicons. Phrases that
fit the distance thresholds were automat-
ically classified by the KNN model, the
others, were evaluated with the Dagging
model. This system achieved over 52.4%
of correctly classified instances in the
Twitter message-level subtask.
1 Introduction
Nowadays, one of the most important sources of
data to extract useful and heterogeneous knowl-
edge is Textual Information. Daily, millions
of Tweets, SMS and blog comments increase
the huge volume of information available for re-
searchers. Texts can provide factual information,
such as: descriptions, lists of characteristics, or
even instructions to opinion-based information,
which would include reviews, emotions, or feel-
ings (Guti?errez et al., 2013). These facts have
motivated that dealing with the identification and
extraction of opinions and sentiments in texts re-
quires special attention. Applications of Senti-
ment Analysis are now more common than ever
in fields like politics and business. More than 50
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
systems participating in this task, clearly indicate
the increase of interest in the scientific community.
Twitter messages can be found among of the
most used corpora nowadays for Sentiment Anal-
ysis (SA). This kind of messages involves an evi-
dent informality which has been addressed in dif-
ferent ways. For example, there are some works
like (Guti?errez et al., 2013) that apply normali-
sation textual tools to reduce the informality of
the twitter messages. Authors such as (Go et al.,
2009), (Guti?errez et al., 2013), (Fern?andez et al.,
2013) and others are focused on the application
of preprocessing processes and feature reduction
to be able to standardise twitter messages and re-
duce different types of elements like hashtags, user
nicks, urls, etc.
In terms of those techniques that can be used
for SA, we can cite (Pang et al., 2002) who built
a lexicon with associated polarity value, starting
with a set of classified seed adjectives and using
conjunctions (and) disjunctions (or, but) to deduce
the orientation of new words in a corpus. This re-
search was based on machine learning techniques
to address Sentiment Classification. Other inter-
esting research is (Turney, 2002), which classi-
fies words according to their polarity based on
the idea that terms with similar orientation tend
to co-occur in documents. There are a large quan-
tity of approaches to deal with SA, and basically
most of them are based on word bags and/or an-
notated corpora as knowledge base. Based on this
information the SA systems are able to apply dif-
ferent types of evaluation techniques such as ma-
chine learning or statistic formulas to predict the
correct classification. As part of machine learn-
ing approaches we would like to mention those
works such as (Go et al., 2009), (Mohammad et
al., 2013) and others that were based on feature
vectors and which cover a wide range settings of
SA. As a starting point, we based this work on
the (Mohammad et al., 2013) approach, adding
727
new features extracted from the sentiment repos-
itories Sentiment 140
1
and NRC-Hashtag Senti-
ment (Mohammad and Turney, 2013).
The remainder of this paper is structured as fol-
lows: section 2 describes in detail the approach
presented. In section 3 we explain the experiments
we carried out. Finally in section 4 conclusions
and future works are expounded.
2 System Description
In this section we present our system in detail
which is able to classify the polarity of tweets as
positive, negative, or neutral.
The system is structured in two main stages.
The first stage consists of classifying a given
tweet. For that, we first recovered all the tweets
from the training corpus that have a similarity
value greater than a fixed threshold T . The sec-
ond stage consists of classifying using the K-NN
rule (Coomans and Massart, 1982), considering as
K all tweets recovered. The process begins with
T = 0.9 decreasing it until T = 0.6. In section 3
we will explain how these values were determined.
As similarity metric we use the Levenshtein
(Levenshtein, 1966) lexical distance. In case that
we cannot find any tweet fulfilling the condition,
the tweet polarity is assigned using a second clas-
sifier trained using Dagging which combines sev-
eral Logistic classifiers set by WEKA as default.
2.1 Preprocessing
The first step in our system is to pre-process all
tweets. The following operations were applied in
the given order.
- Replacing emoticons: Each emoticon is
replaced by a word according to a
lexicon of emoticons. The mean-
ings of the emoticons were taken from
http://en.wikipedia.org/wiki/
List_of_emoticons.
- Replacing acronyms: Each acronym is re-
placed by its meaning. The meanings of the
acronyms were taken from http://www.
acronymfinder.com/.
- Cleaning text: Remove not alphanumeric char-
acters from the tweet.
- Replacing abbreviations: Each abbrevia-
tion is replaced by its respective words.
1
http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
The abbreviations were taken from
http://en.wikipedia.org/wiki/
Abbreviation.
- Lemmatising: Each word is replaced by its
lemma. We use Freeling 3.0 (Padr?o and
Stanilovsky, 2012) for this purpose. We only
retain lemmas corresponding to adjectives,
adverbs, interjections, nouns and verbs.
- Expanding contractions: Each contraction
is replaced by its respective word. The
contractions were taken from http://
www.softschools.com/language_
arts/grammar/contractions/
contractions_list/.
- Deleting punctuation marks.
- Deleting stop words. The stop words
were taken from http://www.ranks.
nl/stopwords.
2.2 Recovering tweets from similarity
As it was explained before, in a first step we tried
to classify tweets using the K-NN rule. To recover
the K similar tweets we used the Levenshtein met-
ric (Levenshtein, 1966). This measure allows to
compute the similarity of two strings of symbols
counting the minimum number of deletions, sub-
stitutions and insertions necessary to transform
one string into another. In our case, each word in
the string is considered as a symbol. In the future
we plan to improve this metric using Levenshtein
at word level and then at sentence level. This met-
ric is known as DLED (Double Levenshteins Edit
Distance) and will be taken from (Fern?andez et al.,
2012).
2.3 Features for Dagging classifier
We represented each tweet as a vector of features
based in (Mohammad et al., 2013) plus other new
ones. Also we used the lexicons Sentiment 140
and NRC-Hashtag Sentiment as it was defined
by Mohammad.
Also two new lexicons, named NRC Emotion
Lexicon 1.0 and NRC Emotion Lexicon 2.0 were
derived from the NRC Emotion Lexicon (Mo-
hammad and Turney, 2013). In the first case we
associated to each word just the values in the
columns positive and negative of NRC Emotion
Lexicon, thus, no sentiment score was computed.
728
For the second lexicon, the positive score was cal-
culated as the sum of the values for the classifica-
tions positive, anticipation, joy, surprise and trust.
On the other hand, the negative score was com-
puted as the sum of the values for the classifica-
tions negative, anger, disgust, fear, sadness and
trust.
In each case we computed the following at-
tributes:
- Pos: Sum of the positive scores of each token
in the tweet over the number of tokens in the
tweet.
- Neg: Sum of the negative scores of each token
in the tweet over the number of tokens in the
tweet.
- PercentPos:
100?Pos
Pos+Neg
- MissNGram: Percent of tokens in the tweet that
were not found in the lexicon.
For the Sentiment 140 and NRC-Hashtag Sen-
timent lexicons we also computed the feature:
- SSE: Sum of the sentiment score of each token
in the tweet over the number of tokens in the
tweet.
Based on the information involved into Senti-
ment 140 and NRC-Hashtag Sentiment lexicons,
unigrams, bigrams and pairs were tokenised in-
volving any non-contiguous combination of the
previous n-grams. With respect to the pairs extrac-
tion were considered the following possibilities:
unigram-unigram, unigram-bigram and bigram-
bigram. Similar to (Mohammad et al., 2013) dif-
ferent set of attributes were generated for each
type of token. As result an initial set of 50 at-
tributes were obtained.
In the case of the new lexicons (NRC Emotion
Lexicon 1.0 and NRC Emotion Lexicon 2.0), only
unigrams were considered. Moreover, the feature
SSE was not computed. So, another 8 features
were taken into account with respect to these lexi-
cons.
Finally we computed:
- NCL: Percent of tokens in capital letters.
- NoE: Number of emoticons in the tweet.
- NoA: Number of acronyms in the tweet.
In general the system works with a total of 61
attributes.
2.4 Classifier Design
As training set, we joined the preprocessed tweets
from both the train and development sets pro-
vided by the Task9B of Semeval-2014. The
Dagging classifier was trained using this set
with the following parameters -F 15 -S 1 -W
weka.classifiers.functions.Logistic ? -R 1.0E-8 -
M -1 using a 10 fold cross-validation as evaluation
method.
3 Experiments
The experiments were evaluated over the training
dataset provided by Task 9: Sentiment Analysis in
Twitter, subtask B. Based on the explanation pro-
vided in section 2 according to the initialisation of
the threshold T to ensure that the K similar tweets
are in fact similar enough, we carried out an exper-
iment for different values of T . These experiments
refer an analysis to know how the variation of T
affects the classification results.
T % CCI
0.9 86.7
0.8 83.3
0.7 74.1
0.6 67.2
0.5 61.1
0.4 55.0
0.3 56.0
Table 1: Results of the K-NN classifier using Lev-
enshtein metric.
T % CCI
0.9 81.2
0.8 83.3
0.7 74.1
0.6 66.7
0.5 63.1
0.4 60.6
0.3 54.2
Table 2: Results of the K-NN classifier using
Matching Coefficient metric.
The first stage of the system was applied to
compute the number of instances which have at
least one instance with a similarity value greater
than T . We computed the percent of instances
correctly classified (%CCI). Table 1 shows the
behaviour of the system when T changes. Table
2 shows the results of the K-NN classifier using
729
System LiveJournal2014 SMS2013 Twitter2013 Twitter2014 Twitter2014Sarcasm
Best result 74.8 70.3 72.1 71.0 58.2
Average result 63.5 55.6 59.8 60.6 45.4
UMCC-DLSI-Sem 53.1 50.0 52.0 55.4 42.8
Worse result 29.3 24.6 34.2 33.0 29.0
Table 3: Results in the SemEval-2014 Task 4B.
Matching Coefficient metric (http://www.
coli.uni-saarland.de/courses/LT1/
2011/slides/stringmetrics.pdf).
This metric counts the quantity of matched
symbols (words in this case) between two
sentences.
Furthermore, we repeated this experiment using
the Matching Coefficient similarity metric to bet-
ter tunning the algorithm and to evaluate if the re-
sults behave in a similar way when T changes. In
both cases, we use the implementation provided in
the SimMetrics library.
As those results shows, when T decrease the ac-
curacy decrease too. In practice, for the values of
T lower than 0.6 the results are worse than 61.4%
using the Dagging classifier in the 10 fold cross-
validation. For that reason, as was mentioned in 2,
we only tried to apply the first stage for values of
T ? 0.6 .
We evaluated our system in the challenge Task
4B: Sentiment Analysis in Twitter, using the pro-
vided training and test data of this challenge.
Based on the classifier obtained in the training pro-
cess we tested our system over the test dataset
achieving values of %CCI up to 55.4. Table 3
show detailed results for each of the 5 different
sources.
4 Conclusions and Future Works
Our system was based on an approach that follows
two stages to classify the polarity of tweets. Re-
gardless the fact that our system behaves worse
than the average, we consider that the approach is
suitable to deal with SA, since our results are close
to the average. As future works we will study
other approaches in order to encourage further de-
velopments of this proposal. Several issues could
be adjusted, for example, other distances should be
tested and evaluated such as DLED (Double Lev-
enshteins Edit Distance) (Fern?andez et al., 2012).
Also, features that encode information about the
presence of negation and opposition words could
be very useful.
Acknowledgements
This research work has been partially funded
by the University of Alicante, Generalitat Va-
lenciana, Spanish Government and the European
Commission through the projects, ?Tratamiento
inteligente de la informacin para la ayuda a la toma
de decisiones? (GRE12-44), ATTOS (TIN2012-
38536-C03-03), LEGOLANG (TIN2012-31224),
SAM (FP7-611312), FIRST (FP7-287607) and
ACOMP/2013/067.
References
D. Coomans and D.L. Massart. 1982. Alternative k-
nearest neighbour rules in supervised pattern recog-
nition : Part 1. k-nearest neighbour classification by
using alternative voting rules. Analytica Chimica
Acta, 136(0):15?27.
Antonio Fern?andez, Yoan Guti?errez, H?ector D?avila,
Alexander Ch?avez, Andy Gonz?alez, Rainel Estrada,
Yenier Casta?neda, Sonia V?azquez, Andr?es Montoyo,
and Rafael Mu?noz. 2012. Umcc dlsi: Multidimen-
sional lexical-semantic textual similarity. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
608?616, Montr?eal, Canada, 7-8 June. Association
for Computational Linguistics.
Javi Fern?andez, Yoan Guti?errez, Jos?e M G?omez, Patri-
cio Mart?nez-Barco, Andr?es Montoyo, and Rafael
Munoz. 2013. Sentiment analysis of spanish tweets
using a ranking algorithm and skipgrams. Proc. of
the TASS workshop at SEPLN 2013. IV Congreso
Espa?nol de Inform?atica, pages 17?20.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Yoan Guti?errez, Andy Gonz?alez, Roger P?erez, Jos?e I.
Abreu, Antonio Fern?andez Orqu??n, Alejandro Mos-
quera, Andr?es Montoyo, Rafael Mu?noz, and Franc
Camara. 2013. Umcc dlsi-(sa): Using a ranking
algorithm and informal features to solve sentiment
analysis in twitter. In Second Joint Conference on
Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International
730
Workshop on Semantic Evaluation (SemEval 2013),
pages 443?449, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Vladimir Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and rever-
sals. Cybernetics and Control Theory, 10(8):707?
710. Original in Doklady Akademii Nauk SSSR
163(4): 845?848 (1965).
Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436?465.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 417?424, Stroudsburg, PA,
USA. Association for Computational Linguistics.
731
