Proceedings of the 5th Workshop on Important Unresolved Matters, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Syllable-Based Speech Recognition for Amharic
Solomon Teferra Abate
solomon_teferra_7@yahoo.com
Wolfgang Menzel
menzel@informatik.uni-hamburg.de
Uniformity of Hamburg, Department of Informatik Natural Language Systems Groups
Vogt-K?lln-Strasse. 30, D-22527 Hamburg, Germany
Abstract
Amharic  is  the  Semitic  language  that  has  the 
second  large  number  of  speakers  after  Arabic 
(Hayward and Richard 1999). Its writing system is 
syllabic  with  Consonant-Vowel  (CV)  syllable 
structure. Amharic orthography has more or less a 
one  to  one correspondence with  syllabic  sounds. 
We have used this feature of Amharic to develop a 
CV syllable-based speech recognizer,  using  Hid-
den  Markov  Modeling  (HMM),  and  achieved 
90.43% word recognition accuracy.
1 Introduction
Most of the Semitic languages are technologically 
unfavored. Amharic is one of these languages that 
are looking for technological considerations of re-
searchers and developers in the area of natural lan-
guage  processing  (NLP).  Automatic  Speech  Re-
cognition (ASR) is one of the major areas of NLP 
that is understudied in Amharic. Only few attempts 
(Solomon,  2001;  Kinfe,  2002;  Zegaye,  2003; 
Martha,  2003;  Hussien  and  Gamb?ck,  2005; 
Solomon et al, 2005; Solomon, 2006)  have been 
made.
We have  developed an  ASR for  the  language 
using  CV  syllables  as  recognition  units.  In  this 
paper  we  present  the  development  and  the 
recognition  performance  of  the  recognizer 
following  a  brief  description  of  the  Amharic 
language and speech recognition technology.
2 The Amharic Language
Amharic,  which belongs to the Semitic  language 
family, is the official language of Ethiopia. In this 
family,  Amharic  stands  second in  its  number  of 
speakers  after  Arabic  (Hayward  and  Richard 
1999). Amharic has five dialectical variations (Ad-
dis  Ababa,  Gojjam,  Gonder,  Wollo,  and  Menz) 
spoken in different regions of the country (Cowley, 
et.al.  1976).  The  speech  of  Addis  Ababa  has 
emerged as the standard dialect and has wide cur-
rency  across  all  Amharic-speaking  communities 
(Hayward and Richard 1999).
As with all of the other languages, Amharic has 
its own characterizing phonetic, phonological and 
morphological properties. For example, it has a set 
of  speech sounds that  is  not  found in  other lan-
guages. For example the following sounds are not 
found in English: [p`], [t?`], [s`], [t`], and [q].
Amharic also has its  own inventory of  speech 
sounds.  It  has  thirty  one  consonants  and  seven 
vowels. The consonants are generally classified as 
stops, fricatives,  nasals,  liquids, and semi-vowels 
(Leslau 2000). Tables 1 and 2 show the classifica-
tion of Amharic consonants and vowels1.
Man 
of 
Art 
Voic
ing 
Place of Articulation
Lab Den Pal Vel Glot
Stops Vs  [p] [t] [t?] [k] [?]
Vd [b] [d] [d?] [g] 
Glott [p`] [t`] [t?`] [q] 
Rd       [kw]
[gw]
[qw] 
Fric Vs [f] [s] [?]   [h]
Vd   [z] [?]   
Glott   [s`]     
Rd         [hw]
Nas-
als 
Vd [m] [n] [?]   
Liq   Vd   [l] 
[r]
Sv Vd [w]     [j] 
Table 1: Amharic Consonants
Key: Lab = Labials; Den = Dentals; Pal = Palat-
als; Vel = Velars; Glot = Glottal; Vs = Voiceless; 
1International  Phonetic  Association's  (IPA)  standard  has 
been used for representation.
33
Vd = Voiced; Rd = Rounded; Fric = Fricatives; Liq 
= Liquids; Sv = Semi-Vowels.
Positions front center back
high            [i]    [u]
mid             [e]    [?] [o]
low [a]
Table 2: Amharic Vowels
Amharic is one of the languages that have their 
own writing system, which is used across all Am-
haric dialects. Getachew (1967) stated that the Am-
haric writing system is phonetic. It allows any one 
to write Amharic texts if s/he can speak Amharic 
and has knowledge of the Amharic alphabet. Un-
like most known languages, no one needs to learn 
how  to  spell  Amharic  words.  In  support  of  the 
above  point,  Leslaw  (1995)  noted  that  no  real 
problems exist in Amharic orthography, as there is 
more or less, a one-to-one correspondence between 
the sounds and the graphic symbols, except for the 
gemination  of  consonants and  some  redundant 
symbols.
Many (Bender 1976; Cowley 1976; Baye 1986) 
have claimed the Amharic orthography as a syllab-
ary for a relatively long period of time. Recently, 
however, Taddesse (1994) and Baye (1997), who 
apparently modified his view, have argued it is not. 
Both of these arguments are based on the special 
feature of the orthography; the possibility of rep-
resenting  speech  using  either  isolated  phoneme 
symbols or concatenated symbols.
In the concatenated feature, commonly known to 
most of the population, each orthographic symbol 
represents a consonant and a vowel, except for the 
sixth order2, which is sometimes realized as a con-
sonant without a vowel and at other times a con-
sonant with a vowel. This representation of concat-
enated speech sounds by a single symbol has been 
the basis for the claim made of the writing system, 
as syllabary. 
Amharic  orthography  does  not  indicate 
gemination,  but  since  there  are  relatively  few 
2An order in Amharic writing system is a combination of a 
consonant with a vowel represented by a symbol. A consonant 
has therefore, 7 orders or different symbols that represent its 
combination with 7 Amharic vowels.
minimal pairs of geminations, Amharic readers do 
not find this to be a problem. This property of the 
writing  system  is  analogous  to  the  vowels  of 
Arabic  and  Hebrew,  which  are  not  normally 
indicated in writing.
The Amharic orthography, as represented in the 
Amharic  Character  set  -  also called [fid?lI]  con-
sists of 276 distinct symbols. In addition, there are 
twenty numerals and eight  punctuation marks.  A 
sample  of  the  orthographic  symbols  is  given  in 
Table 3.
? u i a e o
h ? ? ? ? ? ? ?
l ? ? ? ? ? ? ?
m ? ? ? ? ? ? ?
r ? ? ? ? ? ? ?
Table 3: Some Orthographic Symbols of Amharic 
However, research in speech recognition should 
only consider distinct sounds instead of all the or-
thographic symbols, unless there is a need to de-
velop a dictation machine that includes all of the 
orthographic symbols. Therefore, redundant ortho-
graphic  symbols  that  represent  the  same syllabic 
sounds can be eliminated. Thus, by eliminating re-
dundant graphemes, we are left with a total of 233 
distinct  CV syllable  characters.  In  our  work,  an 
HMM model has been developed for each of these 
CV syllables.
3 HMM-Based Speech Recognition
The  most  well  known  and  well  performing  ap-
proach for speech recognition are Hidden Markov 
Models (HMM). An HMM can be classified on the 
basis  of  the type of its  observation  distributions, 
the structure in its transition matrix and the number 
of states.
The observation distributions of HMMs can be 
either discrete, or continuous. In discrete HMMs, 
distributions are defined on finite spaces while in 
continuous  HMMs,  distributions  are  defined  as 
probability  densities  on  continuous  observation 
spaces,  usually  as  a  mixture  of  several  Gaussian 
distributions.
The model topology that is generally adopted for 
speech recognition is a left-to-right or Bakis model 
34
because the speech signal varies in time from left 
to right (Deller, Proakis and Hansen 1993).
An HMM is flexible in its size, type, or architec-
ture to model words as well as any sub-word unit.
3.1 Sub-word Units of Speech Recognition
Large Vocabulary Automatic Speech Recognition 
Systems (LVASRSs)  require modeling of  speech 
in smaller  units  than words  because the acoustic 
samples of most words will never be seen during 
training,  and  therefore,  can  not  be  trained. 
Moreover,  in  LVASRSs  there  are  thousands  of 
words  and most  of  them occur very rarely,  con-
sequently  training of  models  for  whole  words  is 
generally impractical.  That  is  why LVASRSs re-
quire a segmentation of each word in the vocabu-
lary into sub-word units that occur more frequently 
and can be trained more robustly than words. Us-
ing sub-word based models enables us to deal with 
words  which have not  been seen during training 
since they can just  be decomposed into the sub-
word units. As a word can be decomposed in sub-
word units of different granularities, there is a need 
to choose the most suitable sub-word unit that fits 
the purpose of the system.
Lee et al (1992) pointed out that there are two 
alternatives  for  choosing  the  fundamental  sub-
word units, namely acoustically-based and linguist-
ically-based units . The acoustic units are the labels 
assigned  to  acoustic  segment  models,  which  are 
defined on the basis of procuring a set of segment 
models that spans the acoustic space determined by 
the given, unlabeled training data. The linguistic-
ally-based  units  include  the  linguistic  units,  e.g. 
phones, demi-syllables, syllables and morphemes.
It should be clear that there is no ideal (perfect) 
set  of  sub-word units.  Although phones  are very 
small in number and relatively easy to train, they 
are much more sensitive to contextual influences 
than larger units. The use of triphones, which mod-
el both the right and left context of a phone, has 
become the dominant  solution to  the problem of 
the context sensitivity of phones. 
Triphones  are  also  relatively  inefficient  sub-
word units due to their large number.  Moreover, 
since a triphone unit spans a short time-interval, it 
is  not  suitable for the integration of spectral and 
temporal dependencies.
An other alternative is the syllable. Syllables are 
longer and less context sensitive than phones and 
capable of exploiting both the spectral and tempor-
al  characteristics  of  continuous  speech 
(Ganapathiraju et al 1997). Moreover, the syllable 
has  a  close  connection  to  articulation,  integrates 
some co-articulation phenomena, and has the po-
tential  for  a  relatively  compact  representation  of 
conversational speech.
Therefore, different attempts have been made to 
use syllables as a unit of recognition for the devel-
opment of ASR. To mention a few: Ganapathiraju 
et al (1997) have explored techniques to accentu-
ate the strengths of syllable-based modeling with a 
primary interest of integrating finite-duration mod-
eling and monosyllabic word modeling. Wu et al 
(1998) tried to extract the features of speech over 
the syllabic duration (250ms), considering syllable-
length interval to be 100-250ms. Hu et al (1996) 
used  a  pronunciation  dictionary  of  syllable-like 
units that are created from sequences of phones for 
which the boundary is difficult to detect. Kanok-
phara  (2003)  used  syllable-structure-based  tri-
phones as speech recognition units for Thai.
However, syllables are too many in a number of 
languages, such as English, to be trained properly. 
Thus  ASR researchers  in  languages  like  English 
are led to choose phones where as for Amharic it 
seems promising to consider syllables as an altern-
ative, because Amharic has only 233 distinct CV 
syllables.
4 Syllable-Based Speech  Recognition  for 
Amharic
In  the  development  of  syllable-based  LVASRSs 
for Amharic we need to deal with a  language mod-
el,  pronunciation  dictionary,  initialization  and 
training of the HMM models, and identification of 
the proper HMM topologies that can be properly 
trained  with  the  available  data.  This  section 
presents the development and the performance of 
syllable based speech recognizers.
4.1 The Language Model
One of the required elements in the development of 
LVASRSs is the language model.  As there is no 
usable  language  model  for  Amharic,  we  have 
trained  bigram language  models  using  the  HTK 
statistical  language  model  development  modules. 
Due to  the  inflectional  and derivativational  mor-
phological feature of Amharic our language mod-
els have relatively high perplexities.
35
4.2 The Pronunciation Dictionary
The development of a large vocabulary speaker in-
dependent recognition system requires the availab-
ility of an appropriate pronunciation dictionary. It 
specifies the finite set of words that may be output 
by the speech recognizer and gives, at  least,  one 
pronunciation for each. A pronunciation dictionary 
can be classified as a canonical or alternative on 
the basis of the pronunciations it includes.
A  canonical  pronunciation  dictionary  includes 
only  the  standard  phone  (or  other  sub-word)  se-
quence assumed to be pronounced in read speech. 
It does not consider pronunciation variations such 
as speaker variability, dialect, or co-articulation in 
conversational  speech.  On the other hand,  an al-
ternative pronunciation dictionary uses the actual 
phone (or other sub-word) sequences pronounced 
in speech. In an alternative pronunciation diction-
ary,  various  pronunciation  variations  can  be  in-
cluded (Fukada et al 1999).
We have used the pronunciation dictionary that 
has been developed by Solomon et al (2005). They 
have developed a canonical and an alternative pro-
nunciation dictionaries. Their canonical dictionary 
transcribes  50,000 words  and the  alternative  one 
transcribes 25,000 words in terms of CV syllables. 
Both  these  pronunciation  dictionaries  do  not 
handle the difference between geminated and non-
geminated consonants; the variation of the pronun-
ciation  of  the  sixth  order  grapheme,  with  or 
without vowel; and the absence or presence of the 
glottal  stop  consonant.  Gemination  of  Amharic 
consonants  range  from  a  slight  lengthening  to 
much  more  than  doubling.  In  the  dictionary, 
however, they are represented with the same tran-
scription symbols. 
The sixth order grapheme may be realized with 
or without vowel but the pronunciation dictionaries 
do not  indicate  this  difference.  For  example,  the 
dictionaries used the same symbol for the syllable 
[rI]  in  the  word  [d??m?rInI]  'we  started',  whose 
vowel part may not be realized, and in the word 
[b?rIzo] 'he diluted with water' that is always real-
ized with its vowel sound. That forces a syllable 
model to capture two different sounds: a sound of a 
consonant followed by a vowel, and a sound of the 
consonant only. A similar problem occurs with the 
glottal stop consonant [?] which may be uttered or 
not. 
A sample of pronunciations in the canonical and 
alternative  pronunciation  dictionaries  is  given  in 
Table 43. The alternative pronunciation dictionary 
contains up to 25 pronunciation variants per word 
form. Table  5  illustrates  some cases of  the vari-
ation.
        
Words
 Canonical  Pro-
nunciation
 Alternative Pronun-
ciation
CAmA 
  
 CA mA sp 
 
 CA mA sp
 Ca mA sp
Hitey-
oPeyA 
  
  
  
Hi te yo Pe yA 
sp 
 Hi te yo Pe yA sp
 Hi te yo Pi yA sp
 Hi to Pe yA sp
 te yo Pe yA sp
 to Pe yA sp
Table 4: Canonical and Alternative Pronunciation
Words   Number of pronun-
ciation variants
HiteyoPeyAweyAne       25
HiheHadEge       16
yaHiteyoPeyAne       7
miniseteru       7
yaganezabe       6
HegeziHabehEre       6
yehenene       5
Table 5: Number of Pronunciation variants
Although it does not handle gemination and pro-
nunciation  variabilities,  the  canonical  pronunci-
ation dictionary contains all  233 distinct CV syl-
lables of Amharic, which is 100% syllable cover-
age. 
Pronunciation  dictionaries  of  development  and 
evaluation test sets have been extracted from the 
canonical pronunciation dictionary. These test dic-
tionaries have 5,000 and 20,000 words each.
4.3  The Acoustic Model
For training and evaluation of our recognizers, we 
have used the Amharic read speech corpus that has 
been developed by  Solomon et al (2005). 
The speech corpus consists of a training set, a 
speaker adaptation set, development test sets (for 
5,000 and 20,000 vocabularies), and evaluation test 
sets (for 5,000 and 20,000 vocabularies).   It  is  a 
medium size speech corpus of 20 hours of training 
speech that has been read by 100 training speakers 
who  read  a  total  of  10850  different  sentences. 
Eighty of the training speakers are from the Addis 
3In tables 4 and 5, we used our own transcription
36
Ababa dialect while the other twenty are from the 
other four dialects.
Test and speaker adaptation sets were read by 
twenty other speakers of the Addis Ababa dialect 
and four speakers of the other four dialects. Each 
speaker read 18 different sentences for the 5,000 
vocabulary (development and evaluation sets each) 
and 20 different sentences for the 20,000 vocabu-
lary  (development  and  evaluation  sets  each)  test 
sets. For the adaptation set al of these readers read 
53 adaptation sentences that consist of all Amharic 
CV syllables.
Initialization:  Training  HMM  models  starts 
with initialization. Initialization of the model for a 
set of sub-word HMMs prior to re-estimation can 
be achieved in two different ways: bootstrapping 
and flat start. The latter implies that during the first 
cycle of embedded re-estimation, each training ut-
terance will be uniformly segmented. The hope of 
using such a procedure is that in the second and 
subsequent iterations, the models align as intended.
We have initialized HMMs with both methods 
and trained them in the same way. The HMMs that 
have been initialized with the flat start method per-
formed better (40% word recognition accuracy) on 
development test set of 5,000 words.
The problem with the bootstrapping approach is 
that  any  error  of  the  labeler  strongly  affects  the 
performance of the resulting model because con-
secutive training steps are influenced by the initial 
value of the model. As a result, we did not benefit 
from the use of the segmented speech, which has 
been transcribed with a speech recognizer that has 
low word recognition accuracy, and edited by non-
linguist  listeners.  We  have,  therefore,  continued 
our subsequent experiments with the flat start ini-
tialization method.
Training: We have used the Baum-Welch re-es-
timation procedure for the training. In training sub-
word HMMs that are initialized using the flat-start 
procedure,  this  re-estimation  procedure  uses  the 
parameters of continuously spoken utterances as an 
input source. A transcription, in terms of sub-word 
units, is also needed for each input utterance. Us-
ing the speech parameters and their transcription, 
the complete set of sub-word HMMs are re-estim-
ated  simultaneously.  Then  all  of  the  sub-word 
HMMs  corresponding  to  the  sub-word  list  are 
joined together to make a single composite HMM. 
It is important to emphasize that in this process the 
transcriptions are only needed to identify the se-
quence  of  sub-word  units  in  each  utterance.  No 
boundary  information  is  required  (Young  et  al. 
2002). 
The major problem with HMM training is that it 
requires a great amount of speech data. To over-
come  the  problem  of  training  with  insufficient 
speech data, a variety of sharing mechanisms can 
be  implemented.  For  example,  HMM parameters 
are tied together so that the training data is pooled 
and more robust estimates result. It is also possible 
to restrict the model to a variance vector for the de-
scription of output probabilities,  instead of a full 
covariance matrix. Rabiner and Juang(1993) poin-
ted out that for the continuous HMM models, it is 
preferable to use diagonal covariance matrices with 
several mixtures, rather than fewer mixtures with 
full covariance matrices to perform reliable re-es-
timation of the components of the model from lim-
ited  training  data.  The  diagonal  covariance 
matrices have been used in our work.
HMM Topologies:  To our knowledge, there is 
no topology of HMM model that can be taken as a 
rule of thumb for modeling syllable HMMs, espe-
cially, for Amharic CV syllables. To have a good 
HMM model for Amharic CV syllables, one needs 
to conduct experiments to select the optimal model 
topology. Designing  an HMM topology  has to be 
done with proper consideration of the size of the 
unit of recognition and the amount of the training 
speech data. This is because as the size of the re-
cognition unit increases and the size of the model 
(in terms of the number of parameters to be re-es-
timated) grows, the model requires more training 
data.
We,  therefore,  carried  out  a  series  of  experi-
ments using a left-to-right HMM with and without 
jumps and skips, with a different number of emit-
ting states (3, 5, 6, 7, 8, 9, 10 and 11) and different 
number of Gaussian mixtures (from 2 to 98).  By 
jump we mean skips  from the first  non-emitting 
state  to  the  middle  state  and/or  from the  middle 
state to the last non-emitting state. Figure 1 shows 
a  left-to-right  HMM  of  5  emitting  states  with 
jumps and skips.
Figure 1: An example of HMM topologies
37
We have assumed that the problem of gemina-
tion  may  be  compensated  by  the  looping  state 
transitions of the HMM. Accordingly, CV syllables 
containing  geminated  consonants  should  have  a 
higher  loop  probability  than those  with the  non-
geminated consonants. 
To develop a solution for the problem of the ir-
regularities  in  the  realization  of  the  sixth  order 
vowel [I] and the glottal stop consonant [?], HMM 
topologies with jumps have been used. 
We conducted an experiment using HMMs with 
a jump from the middle state to the last (non-emit-
ting) state for all of the CV syllables with the sixth 
order  vowel,  and  a  jump from the  first  emitting 
state to the middle state for all of the CV syllables 
with the glottal stop consonant.  The CV syllable 
with the  glottal  stop consonant  and the  6th order 
vowel  have  both  jumps.  These  topologies  have 
been chosen so that the models recognize the ab-
sence of the vowel and the glottal stop consonant 
of  CV syllables.  This assumption was confirmed 
by  the  observation  that  the  trained models  favor 
such a jump. A model, which has 5 emitting states, 
of the glottal stop consonant with the sixth order 
vowel tends to start emitting with the 3rd emitting 
state with a probability of 0.72. The model also has 
accumulated  a  considerable  probability  (0.38)  to 
jump from the 3rd emitting state to the last (non-
emitting) state.
A similar model of this consonant with the other 
vowels (our example is the 5th order vowel) tend to 
start  emitting  with  the  3rd emitting  state  with  a 
probability of 0.68. This is two times the probabil-
ity (0.32) of its transition from the starting (non-
emitting state) to the 1st emitting state.
The  models  of  the  other  consonants  with  the 
sixth order vowel,  which are  exemplified by  the 
model of the syllable [jI], tend to jump from the 3rd 
emitting state to the last (non-emitting) state with a 
probability of 0.39, which is considerably greater 
than that of continuing with the next state (0.09).
Since the amount of available training speech is 
not enough to train transition probabilities for skip-
ping two or more states, the number of states to be 
skipped have been limited to one. 
To determine the  optimal  number of  Gaussian 
mixtures for the syllable models, we have conduc-
ted a series of experiments by adding two Gaussian 
mixtures for all the models until the performance 
of the model starts to degrade. Considering the dif-
ference in the frequency of the CV syllables,  a hy-
brid number of Gaussian mixtures has been tried. 
By hybrid, we mean that Gaussian mixtures are as-
signed  to  different  syllables  based  on  their  fre-
quency.  For example:  the frequent  syllables,  like 
[nI], are assigned up to fifty-eight while rare syl-
lables, like [p`i], are assigned not more than two 
Gaussian mixtures.
4.4 Performance of the Recognizers
We present recognition results of only those recog-
nizers which have competitive performance to the 
best performing models. For example: the perform-
ance  of  the  model  with  11  emitting  states  with 
skips and hybrid Gaussian mixtures is more com-
petitive  than those with 7,  8,  9,  and 10 emitting 
states. We have also systematically left out test res-
ults which are worse than those presented in Table 
6. Table 64 shows evaluation results made on the 
5k development test set. 
States Transition 
Topolo-
gies
Mix. Models
AM AM + 
LM
AM + 
LM + 
SA
3 No skip
and jump
18 62.85 88.82
Hy 60.87 87.63 88.50
skip 12 69.20
jump 12 43.74 79.94
5 No skip
and jump
12 69.29 88.99 89.80
Hy 60.04
skip 12 85.77
jump 12 54.53 84.60
11 skip 12 55.04
Hy 71.83 89.21 89.04
Table 6: Recognition Performance on 5k Develop-
ment test set
From Table 6, we can see that the models with 
five  emitting  states,  with  twelve  Gaussian  mix-
tures,  without  skips  and  jumps  has  the  best 
(89.80%)  word  recognition  accuracy.  It  has 
87.69% word recognition accuracy on the 20k de-
velopment test set.
Since  the  most  commonly  used  number  of 
HMM states for phone-based speech recognizers is 
three emitting states, one may expect a model of 
six emitting states to be the best for an HMM of 
4In tables 6 and 7, States refers to the number of emitting 
states;  Mix  refers  to  the  number  of  Gaussian  mixtures  per 
state; Hy refers to hybrid; AM refers to acoustic model; LM 
refers to language model; and SA refers to speaker adaptation.
38
concatenated consonant and vowel. But the result 
of our experiment shows that a CV syllable-based 
recognizer with only five emitting states performed 
better than all the other recognizers.
As we can see from Table 6, models with three 
emitting states do have a competitive performance 
with 18 and hybrid Gaussian mixtures. They have 
the least number of states of all our models. Never-
theless,  they  require  more  storage  space  (33MB 
with 18 Gaussian mixtures and 34MB with hybrid 
Gaussian mixtures) than the best performing mod-
els (32MB). Models with three emitting states also 
have  larger  number  of  total  Gaussian  mixtures5 
(30,401  with  18  Gaussian  mixtures  and  31,384 
with hybrid Gaussian mixtures) than the best per-
forming models (13,626 Gaussian mixtures).
The other model topology that is competitive in 
word  recognition  performance is  the  model  with 
eleven emitting states, with skip and hybrid Gaus-
sian mixtures, which has a word recognition accur-
acy  of  89.21%.  It  requires  the  biggest  memory 
space (40MB) and uses the largest number of total 
Gaussian mixtures (36,619) of all the models we 
have developed.
We have evaluated the top two models with re-
gard  to  their  word  recognition  accuracy  on  the 
evaluation test sets. Their performance is presented 
in Table 7. As it can be seen from the table, the 
models with the better performance on the devel-
opment test sets also showed better results with the 
evaluation test sets. We can, therefore, say that the 
model with five emitting states without skips and 
twelve  Gaussian  mixtures  is  preferable  not  only 
with regard to its word recognition accuracy, but 
also with regard to its memory requirements.
Sta
tes
Mix. Models
AM + LM AM + LM + SA
5k 20k 5k 20k
5 12 90.43 87.26
11 Hy 89.36 87.13   
Table 7:  Recognition Performance on 5k and 20k 
Evaluation test sets
For a comparison purpose, we have developed a 
baseline  word-internal  triphone-based  recognizer 
using the same corpus. The models of 3 emitting 
states, 12 Gaussian mixtures, with skips have the 
5We  counted  the  Gaussian  mixtures  that  are  physically 
saved, instead of what should actually be. 
best word recognition accuracy (91.31%) of all the 
other triphone-based recognizers that we have de-
veloped. This recognizer  also has better word re-
cognition accuracy than that of our syllable-based 
recognizer (90.43%). But tying is applied only for 
the triphone-based recognizers. 
However the triphone-based recognizer requires 
much  more  storage  space  (38MB)  than  the  syl-
lable-based  recognizer  that  requires  only  15MB 
space. With regard to their speed of processing, the 
syllable-based  model  was  37%  faster  than  tri-
phone-based one. 
These  are  encouraging  results  as  compared  to 
the performance  reported by Afify et al (2005) for 
Arabic speech recognition (14.2% word error rate). 
They have used a trigram language model with a 
lexicon of 60k vocabulary.
4.5 Conclusions  and  Research  Areas  in  the 
Future
We conclude  that  the  use  of  CV  syllables  is  a 
promising  alternative  in  the  development  of 
ASRSs for Amharic. Although there are still pos-
sibilities  of  performance  improvement,  we  have 
got  an  encouraging  word  recognition  accuracy 
(90.43%). Some of the possibilities of performance 
improvement are:
? The pronunciation dictionary that we have used 
does not  handle the  problem of gemination of 
consonants  and  the  irregular  realization  of  the 
sixth order vowel and the glottal stop consonant, 
which has a direct effect on the quality of the 
sub-word transcriptions.  Proper editing (use of 
phonetic transcription) of the pronunciation dic-
tionaries  which,  however,  requires  a  consider-
able amount of work, certainly will  result  in a 
higher  quality  of  sub-word  transcription  and 
consequently in the improvement of the recog-
nizers'  performance.  By  switching  from  the 
grapheme-based  recognizer  to  phonetic-based 
recognizer in Arabic, Afif et  al.  (2005) gained 
relative  word  error  rate  reduction  of  10%  to 
14%.
? Since tying is one way of minimizing the prob-
lem of shortage of training speech, tying the syl-
lable-based  models  would  possibly  result  in  a 
gain of  some degree  of  performance improve-
ment. 
39
5 References
Afif, Mohamed, Long Nguyen, Bing Xiang, Sherif Ab-
dou, and John Makhoul. 2005. Recent progress in Ar-
abic broadcast news transcription at BBN. In INTER?
SPEECH?2005, 1637-1640
Baye  Yimam and  TEAM 503  students.  1997.  "??? 
?????" Ethiopian Journal of Languages and Literat-
ure 7(1997): 1-32.
Baye Yimam.  1986.  "????? ????".  Addis  Ababa. 
?.?.?.?.?. 
Bender,  L.M.  and  Ferguson  C.  1976.  The  Ethiopian 
Writing System. In Language in Ethiopia. Edited by 
M.L.  Bender,  J.D.  Bowen,  R.L.  Cooper,  and  C.A. 
Ferguson. London: Oxford University press.
Cowley, Roger, Marvin L. Bender and Charles A. Fer-
gusone. 1976.  The Amharic Language-Description. 
In  Language  in  Ethiopia.  Edited  by  M.L.  Bender, 
J.D. Bowen, R.L. Cooper, and C.A. Ferguson. Lon-
don: Oxford University press.
Deller, J.R. Jr., Hansen, J.H.L. and Proakis, J.G., Dis-
crete-time Processing of Speech Signals. Macmillan 
Publishing Company, New York, 2000.
Fukada, Toshiaki, Takayoshi Yoshimura and Yoshinori 
Sagisa. 1999. Automatic generation of multiple pro-
nunciations based on neural networks. Speech Com-
munication  27:63?73 
http://citeseer.ist.psu.edu/fukada99automatic.html.
Ganapathiraju,  Aravind; Jonathan Hamaker;  Mark Or-
dowski; and George R. Doddington. 1997. Joseph Pi-
cone.  Syllable-based  Large  Vocabulary  Continuous 
Speech Recognition.
Getachew Haile.  1967. The Problems of the Amharic 
Writing System. A paper presented in advance for the 
interdisciplinary seminar of the Faculty of Arts and 
Education. HSIU.
Hayward, Katrina and Richard J. Hayward. 1999. Am-
haric. In Handbook of the International Phonetic As-
sociation:  A  guide  to  the  use  of  the  International 
Phonetic Alphabet. Cambridge: the University Press.
Hu, Zhihong; Johan Schalkwyk; Etienne Barnard; and 
Ronald Cole. 1996. Speech recognition using syllable 
like units. Proc. Int'l Conf. on Spoken Language Pro-
cessing (ICSLP), 2:426-429.
Kanokphara,  Supphanat;  Virongrong  Tesprasit  and 
Rachod Thongprasirt.  2003. Pronunciation Variation 
Speech  Recognition  Without  Dictionary  Modifica-
tion on Sparse Database, IEEE International Confer-
ence  on  Acoustics,  Speech,  and  Signal  Processing 
(ICASSP 2003, Hong Kong).
Kinfe Tadesse.  2002. Sub-word Based Amharic Word 
Recognition: An Experiment  Using Hidden Markov 
Model  (HMM),  M.Sc  Thesis.   Addis  Ababa  Uni-
versity Faculty of Informatics. Addis Ababa. 
Lee, C-H., Gauvain, J-L., Pieraccini, R. and Rabiner, L. 
R.. 1992. Large vocabulary speech recognition using 
subword units. Proc. ICSST-92, Brisbane, Australia, 
pp. 342-353.
Leslau,  W.  2000.  Introductory  Grammar  of  Amharic, 
Wiesbaden: Harrassowitz.
Martha Yifiru. 2003. Application of Amharic speech re-
cognition system  to command and control computer: 
An experiment with Microsoft  Word, M.Sc Thesis. 
Addis Ababa University Faculty of Informatics. Ad-
dis Ababa.
Rabiner, L. and Juang, B. 1993. Fundamentals of speech 
recognition. Englewood Cliffs, NJ.
Hussien Seid and Bj?rn. Gamb?ck  2005. A Speaker In-
dependent  Continuous  Speech  Recognizer  for  Am-
haric. In: INTERSPEECH 2005,  9th European Con-
ference on Speech Communication and Technology. 
Lisbon, September 4-9.
Solomon Birihanu. 2001. Isolated Amharic Consonant-
Vowel (CV) Syllable Recognition, M.Sc Thesis.  Ad-
dis Ababa University Faculty of Informatics. Addis 
Ababa.
Solomon Teferra Abate.  2006.  Automatic  Speech Re-
cognition for  Amharic.  Ph.D. Thesis.  University  of 
Hamburg. Hamburg.
Solomon Teferra  Abate,  Wolfgang Menzel  and  Bairu 
Tafla.  2005. An Amharic Speech Corpus for Large 
Vocabulary Continuous Speech Recognition. In: IN-
TERSPEECH  2005,  9th  European  Conference  on 
Speech  Communication  and  Technology.  Lisbon, 
September 4-9.
Tadesse Beyene. 1994. The Ethiopian Writing System. 
Paper presented at the 12th International Conference 
of Ethiopian Studies, Michigan State University.
Wu, Su-Lin. 1998. Incorporating Information from Syl-
lable-length Time Scales into Automatic Speech Re-
cognition.  PhD  thesis,  University  of  California, 
Berkeley, CA.
Young, Steve; Dan Kershaw; Julian Odell and Dave Ol-
lason. 2002. The HTK Book.
Zegaye Seyifu. 2003. Large vocabulary, speaker inde-
pendent, continuous  Amharic speech recognition, 
M.Sc Thesis.  Addis Ababa University Faculty of In-
formatics. Addis Ababa.
40
Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 1?7,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Boosting N-gram Coverage for Unsegmented Languages  Using 
Multiple Text Segmentation Approach
Solomon Teferra Abate
LIG Laboratory, 
CNRS/UMR-5217
solomon.abate@imag.fr
Laurent Besacier
LIG Laboratory, 
CNRS/UMR-5217
laurent.besacier@imag.fr
Sopheap Seng
LIG Laboratory, 
CNRS/UMR-5217
MICA Center, CNRS/UMI-
2954
sopheap.seng@imag.fr
Abstract
Automatic  word  segmentation  errors, 
for  languages  having  a  writing  system 
without word boundaries, negatively af-
fect the performance of language mod-
els.  As a solution, the use of  multiple, 
instead of unique, segmentation has re-
cently  been  proposed.  This  approach 
boosts  N-gram  counts  and  generates 
new  N-grams.  However,  it  also  pro-
duces bad N-grams that  affect the lan-
guage models' performance. In this pa-
per, we study more deeply the contribu-
tion  of  our  multiple  segmentation  ap-
proach  and experiment on an efficient 
solution to minimize the effect of adding 
bad N-grams.
1 Introduction
A language model  is  a  probability assignment 
over  all  possible  word  sequences  in  a  natural 
language. It assigns a relatively large probabili-
ty to meaningful, grammatical, or frequent word 
sequences and a low probability or a zero proba-
bility  to  nonsensical,  ungrammatical  or  rare 
ones.  The statistical  approach used in N-gram 
language modeling requires a large amount of 
text data in order to make an accurate estimation 
of probabilities. These data are not available in 
large  quantities  for  under-resourced  languages 
and the lack of text data has a direct impact on 
the performance of language models. While the 
word is  usually  a  basic  unit  in  statistical  lan-
guage  modeling,  word  identification  is  not  a 
simple  task  even  for  languages  that  separate 
words by a special character (a white space in 
general).  For  unsegmented  languages,  which 
have a writing system without obvious word de-
limiters, the N-grams of words are usually esti-
mated  from  the  text  corpus  segmented  into 
words employing automatic methods. Automat-
ic segmentation of text is not a trivial task and 
introduces errors due to the ambiguities in natu-
ral language and the presence of out of vocabu-
lary words in the text. 
While the lack of text resources has a nega-
tive  impact  on  the  performance  of  language 
models,  the  errors  produced by the  word seg-
mentation make those data even less usable. The 
word N-grams not found in the training corpus 
could be due not only to the errors introduced 
by the automatic segmentation but  also to the 
fact  that  a  sequence  of  characters  could  have 
more than one correct segmentation. 
In  previous  article  (Seng  et  al.,  2009),  we 
have proposed a method to estimate an N-gram 
language  model  from  the  training  corpus  on 
which each sentence is segmented into multiple 
ways instead of a unique segmentation. The ob-
jective of multiple segmentation is to generate 
more N-grams from the training corpus to use in 
language modeling. It was possible to show that 
this  approach  generates  more  N-grams  (com-
pared  to  the  classical  dictionary-based  unique 
segmentation method) that are potentially useful 
and relevant in language modeling. The applica-
tion of multiple segmentation in language mod-
eling  for  Khmer  and  Vietnamese  showed im-
provement in terms of tri-gram hits and recogni-
tion error rate in Automatic Speech Recognition 
(ASR) systems. 
This work is a continuation of our previous 
work on the use of multiple segmentation. It is 
conducted on Vietnamese only. A close analysis 
of N-gram counts shows that the approach has 
in fact two contributions: boosting the N-gram 
1
counts that are generated with first best segmen-
tation  and  generating  new N-grams.  We have 
also identified that there are N-grams that nega-
tively  affect  the  performance  of  the  language 
models. In this paper, we study the contribution 
of boosting N-gram counts and  of new N-grams 
to the performance of the language models and 
consequently  to  the  recognition  performance. 
We also present experiments where rare or bad 
N-grams are cut off in order to minimize their 
negative effect on the performance of the lan-
guage models.
The paper is organized as follows: section 2 
presents the theoretical background of our mul-
tiple  segmentation  approach;  in  section  3  we 
point out the set up of our experiment; in sec-
tion 4 we present the results of our detailed sta-
tistical analysis of N-grams generated by multi-
ple  segmentation  systems.  Section  5  presents 
the  evaluation  results  of  our  language  models 
for  ASR  and  finally,  we  give  concluding  re-
marks.
2 Multiple Text Segmentation
Text segmentation is a fundamental task in nat-
ural language processing (NLP). Many NLP ap-
plications require the input text segmented into 
words before making further progress because 
the word is considered the basic semantic unit in 
natural  languages.  For unsegmented languages 
segmenting text into words is not a trivial task. 
Because of ambiguities in human languages, a 
sequence  of  characters  may  be  segmented  in 
more than one way to  produce a  sequence of 
valid words.  This is due to the fact  that there 
are different segmentation conventions and the 
definition of  word in a language is  often am-
biguous. 
Text  segmentation  techniques  generally  use 
an  algorithm  which  searches  in  the  text  the 
words corresponding to those in a dictionary. In 
case of ambiguity, the algorithm selects the one 
that  optimizes  a  parameter  dependent  on  the 
chosen  strategy.  The  most  common optimiza-
tion strategies consist of maximizing the length 
of  words  (?longest  matching?)  or  minimizing 
the  number  of  words  in  the  entire  sentence 
(?maximum matching?). These techniques rely 
heavily on the availability and the quality of the 
dictionaries and while it is possible to automati-
cally generate a dictionary from an unsegment-
ed text corpus using unsupervised methods, dic-
tionaries are often created manually. The state-
of-the-art methods generally use a combination 
of hand-crafted, dictionary and statistical tech-
niques to obtain a better result. However, statis-
tical  methods  need  a  large  corpus  segmented 
manually  beforehand.  Statistical  methods  and 
complex training methods are not appropriate in 
the context of under-resourced languages as the 
resources  needed to  implement  these  methods 
do not exist. For an under-resourced language, 
we seek segmentation methods that allow better 
exploitation of the limited resources. In our pre-
vious paper (Seng et al, 2009) we have indicat-
ed the  problems of  existing text  segmentation 
approaches  and  introduced  a  weighted  finite 
state  transducer  (WFST)  based  multiple  text 
segmentation algorithm.
Our approach is implemented using the AT & 
T FSM Toolkit (Mohri et al, 1998). The algo-
rithm is inspired with the work on the segmen-
tation of Arabic words (Lee et al, 2003).  The 
multiple segmentation of a sequence of charac-
ters is made using the composition of three con-
trollers.  Given  a  finite  list  of  words  we  can 
build a finite state transducer M (or word trans-
ducer) that, once composed with an acceptor I 
of the input string that represent a single charac-
ter  with  each  arc,  generates  a  lattice  of  the 
words that represent all of the possible segmen-
tations. To handle out-of-vocabulary entries, we 
make a model of any string of characters by a 
star closure operation over all the possible char-
acters.  Thus,  the  unknown  word  WFST  can 
parse any sequence of characters and generate a 
unique  unk word symbol. The word transducer 
can,  therefore,  be  described  in  terms  of  the 
WFST  operations  as  M  =  (WD   UNK)+?  
where WD is a WFST that represents the dictio-
nary  and  UNK represents  the  unknown  word 
WFST. Here,  and + are the union and Kleene?  
?+? closure operations. A language model L is 
used to score the lattice of all possible segmen-
tations obtained by the composition of our word 
transducer M and the input string I. A language 
model  of  any  order  can  be  represented  by  a 
WFST. In our case, it is important to note that 
only a simple uni-gram language model is used. 
The uni-gram model is estimated from a small 
training  corpus  segmented  automatically  into 
words  using  a  dictionary  based  method.  The 
composition  of  the  sequence  of  input  string  I 
2
with the word transducer M yields a transducer 
that represents all possible segmentations. This 
transducer is then composed with the language 
model  L,  resulting  in  a  transducer  that  repre-
sents  all  possible  segmentations  for  the  input 
string  I,  scored  according  to  L.  The  highest 
scoring paths of the compound transducer is the 
segmentation m that can be defined as:
P ?m ?=maxP ?mk ?  
The segmentation procedure can then be ex-
pressed formally as:
 m=bestpath ? I?M?L ?
where ? is the composition operator. The N-
best segmentations are obtained by decoding the 
final lattice to output the N-best highest scoring 
paths and will be used for the N-gram count.
3 Experimental Setup
3.1 Language Modeling
First,  it  is  important  to  note  that  Vietnamese 
texts are naturally segmented into syllables (not 
words). Each  syllable  tends  to  have  its  own 
meaning and thus  a  strong identity.  However, 
the  Vietnamese  monosyllable  is  not  automati-
cally a word as we would define a word in Eng-
lish. Often, two syllables go together to form a 
single word, which can be identified by the way 
it  functions  grammatically  in  a  sentence.  To 
have a word-based language model, word seg-
mentation would, therefore, be a must in Viet-
namese.
A Vietnamese training corpus that contains 3 
millions sentences from broadcast news domain 
has been used in this experiment. A Vietnamese 
dictionary of 30k words has been used both for 
the  segmentation  and  counting  the  N-grams. 
Therefore, in the experiments, the ASR vocabu-
lary always remains the same and only the lan-
guage model is changing. The segmentation of 
the  corpus  with  dictionary  based,  ?longest 
matching? unique segmentation method gives a 
corpus  of  46  millions  words.  A  development 
corpus of 1000 sentences, which has been seg-
mented automatically to obtain 44k words, has 
been used to evaluate the tri-gram hits and the 
perplexity.  The performance of  each language 
model  produced will  be evaluated in  terms of 
the tri-gram hits and perplexity on the develop-
ment corpus and in terms of ASR performance 
on a separate speech test set (different from the 
development set).
First of all, a language model named lm_1 is 
trained using the SRILM toolkit (Stolcke 2002) 
from  the  first  best  segmentation  (Segmul1), 
which has the highest scoring paths (based on 
the transducer explained in section 2) of  each 
sentence in the whole corpus. Then,  additional 
language  models  have  been  trained  using  the 
corpus  segmented  with  N-best  segmentation: 
the number of N-best segmentations to generate 
for each sentence is fixed to 2, 5, 10, 50, 100 
and 1000. The resulting texts are named accord-
ingly  as  Segmul2,  Segmul5,  Segmul10,  Seg-
mul50,  Segmul100,  Segmul1000.  Using  these 
as  training  data,  we  have  developed  different 
language models. Note that a tri-gram that ap-
pears several times in multiple segmentations of 
a single sentence has a count set to one. 
3.2 ASR System
Our automatic speech recognition systems use 
the CMU?s Sphinx3 decoder. The decoder uses 
Hidden Markov Models (HMM) with continu-
ous  output  probability  density  functions.  The 
model topology is a 3-state, left-to-right HMM 
with 16 Gaussian mixtures per state.  The pre-
processing of the system consists of extracting a 
39 dimensional  features vector  of  13 MFCCs, 
the  first  and  second  derivatives.  The  CMU?s 
SphinxTrain has been used to train the acoustic 
models used in our experiment. 
The  Vietnamese  acoustic  modeling  training 
corpus is  made up of  14 hours  of  transcribed 
read  speech.  More  details  on  the  automatic 
speech recognition system for Vietnamese lan-
guage can be found in (Le et al, 2008). While 
the evaluation metric WER (Word Error Rate) 
is  generally used to evaluate and compare the 
performance  of  the  ASR  systems,  this  metric 
does not fit well for unsegmented languages be-
cause the errors introduced during the segmen-
tation of the references and the output hypothe-
sis may prevent a fair comparison of different 
ASR system outputs.  We,  therefore,  used  the 
Syllable Error Rate (SER) as Vietnamese text is 
composed  of  syllables  naturally  separated  by 
white space. The automatic speech recognition 
is  done  on  a  test  corpus  of  270  utterances 
(broadcast news domain). 
3
4 Statistical  Analysis  of  N-grams  in 
Multiple Text Segmentation
The  change  in  the  N-gram  count  that  results 
from  multiple  segmentation  is  two  fold:  first 
there is a boosting of the counts of the N-grams 
that  are  already found with the first  best  seg-
mentation,  and  secondly  new  N-grams  are 
added.  As  we have made a  closed-vocabulary 
counting, there are no new uni-grams resulting 
from  multiple segmentation. For the counting, 
the SRILM toolkit  (Stolcke 2002) is used set-
ting the -gtnmin option to zero so that all the N-
gram counts can be considered.
Figure  1  shows  the  distribution  of  tri-gram 
counts for the unique and multiple segmentation 
of the training corpus.  It  can be seen that  the 
majority  of  the  tri-grams  have  counts  in  the 
range of one to three.
Figure 1: Distribution of tri-gram counts
The boosting (the counts of the tri-grams that 
are already found with the first best segmenta-
tion) effect of the multiple segmentation is indi-
cated in table 1. We can see from the table that 
Segmul2, for example, reduced the  number of 
rare tri-grams (count range 1-3) from 19.04 to 
16.15  million.  Consequently,  the  ratio  of  rare 
tri-grams to all tri-grams that are in Segmul1 is 
reduced  from 94% (19.04/20.31*100)  of  Seg-
mul1  only  to  79%  (15.96/20.31*100)  by  the 
boosting effect of Segmul1000, which increased 
the number of tri-grams with count range of 4-9 
from 0.91M to 3.34M. This implies, in the con-
text of under-resourced languages, that multiple 
segmentation  is  boosting  the  N-gram  counts. 
However, one still has to verify if this boosting 
is relevant or not for ASR.
Multiple
Seg.
Counts Range
1?3 
(M)
4-9 
(M) 
10-99
(M)
100-999
(M)
?1000
(M)
Segmul1 19.04 0.91 0.34 0.016 0.00054
Segmul2 16.15 3.23 0.89 0.043 0.0017
Segmul5 16.06 3.28 0.92 0.045 0.0017
Segmul10 16.03 3.30 0.93 0.045 0.0017
Segmul50 15.99 3.33 0.95 0.046 0.0017
Segmul100 15.98 3.33 0.95 0.046 0.0017
Segmul1000 15.96 3.34 0.96 0.046 0.0017
Table 1. boosting tri-gram counts 
We have also analyzed the statistical behav-
ior of the newly added tri-grams with regard to 
their count distribution (see figure 2). As we can 
see from the figure, the distribution of the new 
tri-grams is somehow similar to the distribution 
of the whole tri-grams that is indicated in figure 
1. 
As  shown  in  table  2,  the  total  number  of 
newly  added  tri-grams  is  around  15  millions. 
We can see from the table that the rate of new 
tri-gram contribution of each segmentation in-
creases as N increases in the N-best segmenta-
tion. However, as it is indicated in figure 2, the 
major  contribution  is  in  the  area  of  rare  tri-
grams.
Figure 2: Distribution of new tri-gram counts
1?3 4?9 10?99 100-999 ?1000
0
5,000,000
10,000,000
15,000,000
20,000,000
25,000,000
30,000,000
35,000,000
40,000,000
Segmul1
Segmul2
Segmul5
Segmul10
Segmul50
Segmul100
Segmul1000
Counts Range
No
. o
f tr
i-g
ra
m s
1?3 4?9 10?99 100-999 ?1000
0
2,000,000
4,000,000
6,000,000
8,000,000
10,000,000
12,000,000
14,000,000
16,000,000
Segmul 2
Segmul 5
Segmul 10
Segmul 50
Segmul 100
Segmul 1000
Counts Range
Nu
mb
er
 of
 tri
-g
ra
ms
4
Mul. Segmentation No. %
Segmul2 4,125,881 26,05
Segmul5 8,249,684 52,09
Segmul10 10,355,433 65,39
Segmul50 13,002,700 82,11
Segmul100 14,672,827 92,65
Segmul1000 15,836,120 100,0
Table 2. tri-gram contribution of multiple seg-
mentation
5 Experimental Results
In this section we present the various language 
models  we  have  developed  and  their  perfor-
mance in terms of perplexity, tri-gram hits and 
ASR performance (syllable error rate).
We use the results obtained with the method 
presented in (Seng et al, 2009) as baseline. This 
method  consists  in  re-estimating  the  N-gram 
counts  using the  multiple  segmentation of  the 
training data and add one to the count of a tri-
gram that appears several times in multiple seg-
mentations of a single sentence. These baseline 
results  are  presented  in  Table  3. The  results 
show an increase of the tri-gram coverage and 
slight improvements of the ASR performance.
Language 
Models
3gs(M) 3g hit(% ) Ppl SER 
Lm_1 20.31 46.9 126.6 27
lm_2 24.06 48.6 118.1 26.2
Lm_5 28.92 49.2 125.9 27
Lm_10 32.82 49.4 129.0 26.5
Lm_50 34.20 49.7 133.4 26.7
lm_100 34.93 49.7 134.8 26.9
lm_1000 36.11 49.88 137.7 27.3
Table 3. Results of experiments using the base-
line method presented in (Seng et al, 2009)
5.1 Separate  effect  of  boosting  tri-gram 
counts
To see  the  effect  of  boosting  tri-gram counts 
only,  we  have  updated  the  counts  of  the  tri-
grams  obtained  from  the  1-best  segmentation 
(baseline  approach)  by  the  tri-gram counts  of 
different  multiple  segmentations.  Note  that  no 
new tri-grams are added here, and we evaluate 
only the effect  and,  therefore,  the tri-gram hit 
remains the same as that of lm_1.
We have then developed different  language 
models using the uni-gram and bi-gram counts 
of  the first  best  segmentation and the updated 
trigram counts after multiple segmentation. The 
performance of the language models have been 
evaluated in terms of perplexity and their contri-
bution  to  the  performance  improvement  of  a 
speech recognition system.  We have observed 
(detailed  results  are  not  reported  here)  that 
boosting only the tri-gram counts has not con-
tributed any improvement in the performance of 
the  language  models.  The  reason  is  probably 
due  to  the  fact  that  simply  updating  tri-gram 
counts without updating the uni-grams and the 
bi-grams lead to a biased and inefficient LM.
5.2 Separate effect of new tri-grams
To  explore  the  contributions  of  only  newly 
added tri-grams, we have added their counts to 
the N-gram counts of Segmul1. It is important 
to note that the model obtained in that case is 
different from the baseline model whose results 
are presented in Table 3 (the counts of the tri-
grams already found in the unique segmentation 
are different between models). As it is presented 
in table 4, including only the newly added tri-
grams  consistently  improved  tri-gram  hits, 
while the improvement in perplexity stopped at 
Segmul10. Moreover, the use of only new tri-
grams do not reduce the speech recognition er-
ror rate.
Language 
Models
3gs
(M)
3g 
hit(% )
ppl SER 
lm_1 20.3 46.9 126.6 27
lm_2_new 24.4 48.7 119.1 26.9
lm_5_new 28.6 49.0 122.5 27.8
lm_10_new 30.7 49.2 124.2 27.9
lm_50_new 33.3 49.4 126.8 27.8
lm_100_new 35 49.8 127.8 28
lm_1000_new 36.1 49.9 129.7 27.9
Table 4. Contributions of new tri-grams
5.3 Pooling unique and multiple segmenta-
tion models
We have developed language models by pooling 
unique and multiple segmentation models alto-
gether.  For  instance,  all  the  N-grams of  lm_5 
multiple  segmentation  are  pooled  with  all  N-
grams of lm_1 unique segmentation before esti-
mating the language model probabilities. In oth-
er  words,  ngram-count  command is  used with 
multiple count files. The results are presented in 
table 5.
As it can be noted from table 5, we have got a 
significant  improvement  in  all  the  evaluation 
criteria  as  compared  with  the  performance  of 
lm_1 that has perplexity of 126.6, tri-gram hit 
5
of 46.91% and SER of 27. The best result ob-
tained (25.4) shows a 0.8 absolute SER reduc-
tion  compared  to  the  best  result  presented  in 
(Seng et al, 2009).
Language 
Models
3gs
(M)
3g 
hit(% )
ppl SER 
lm_1 20.31 46.9 126.6 27
lm_2+lm_1 24.4 48.7 120.9 25.4
lm_5+lm_1 29.12 49.2 123.2 26.2
lm_10+lm_1 31.4 49.4 124.2 26
lm_50+lm_1 34.3 49.7 126 26
lm_100+lm_1 35 49.8 126.5 26.2
lm_1000+lm_1 36.2 49.9 128 26.2
Table 5. Performance with pooling
5.4 Cutting off rare tri-grams
With  the  assumption  that  bad  N-grams  occur 
rarely, we cut off rare tri-grams from the counts 
in developing language models. We consider all 
tri-grams with a count of 1 to be rare. Our hope, 
here, is that using this cut off we will remove 
bad  N-grams  introduced  by  the  multiple  seg-
mentation approach, while keeping correct new 
N-grams in the model. Table 6 shows the per-
formance  of  the  language  models  developed 
with or without tri-gram cut off for the baseline 
method (the results presented on the lines indi-
cating All3gs are the same as the ones presented 
in Table 3) . 
Language models Evaluation Criteria
3gs
(M)
3g hit 
(%)
ppl SER
lm_1 All 3gs 20.31 46.91 126.6 27
Cut off 4.17 38.09 129.3 26.6
lm_2 All 3gs 24.06 48.6 118.1 26.2
Cut off 5.11 39.6 121.0 26.7
lm_5 All 3gs 28.92 49.2 125.9 27
Cut off 6.4 40.11 129.2 26.6
lm_10 All 3gs 32.82 49.41 129.0 26.5
Cut off 6.98 40.27 132.4 26.6
lm_50 All 3gs 34.20 49.68 133.4 26.7
Cut off 7.8 40.51 136.9 26.9
lm_100 All 3gs 34.93 49.74 134.8 26.9
Cut off 7.98 40.59 138.4 26.8
lm_1000 All 3gs 36.11 49.88 137.7 27.3
Cut off 8.33 40.71 141.3 26.8
Table 6. Performance with cut off.
The result shows that cutting off reduced the 
number of tri-grams highly (4 tri-grams over 5 
are removed in that case). It, therefore, reduces 
the  size  of  the  language  models  significantly. 
Although  the  results  obtained  are  not  conclu-
sive, a reduction of  recognition error rate has 
been  observed  in  four  out  of  the  seven  cases 
while the perplexity increased and the tri-gram 
hits decreased in all cases. 
5.5 Hybrid  of  pooling  and  cutting  off 
methods
As it has been already indicated, cutting off in-
creased the perplexity of  the language models 
and decreased the tri-gram hits. To reduce the 
negative  effect  of  cutting  off  on  tri-gram hits 
and  perplexity,  we  have  developed  language 
models using both pooling and cut off methods. 
We then cut off tri-grams of count 1 from the 
pooled N-grams. The result, as presented in ta-
ble 7, shows that we can gain significant reduc-
tion in recognition error rate and  improvement 
in tri-gram hits as compared to lm_1 that is de-
veloped with cut off, even if no improvement in 
perplexity is observed. 
The best  result  obtained (25.9) shows a 0.3 
absolute  SER reduction  compared  to  the  best 
system presented in (Seng et al, 2009).
Language Models 3gs
(M)
3g hit
(% )
ppl SE
R 
lm_1 (no cutoff) 20.3 46.9 126.6 27
lm_1 (cutoff) 4.2 38.1 129.3 26.6
lm_2+lm_1 (cutoff) 5.2 39.7 126.4 26.8
lm_5+lm_1 (cutoff) 6.4 40.2 129.5 25.9
lm_10+lm_1 (cutoff) 7.0 40.3 131.1 26.3
lm_50+lm_1 (cutoff) 7.8 40.5 133.5 26.4
lm_100+lm_1 (cutoff) 8.0 40.6 134.3 26.4
lm_1000+lm_1 (cutoff) 8.3 40.7 161.5 26.7
Table 7. Performance with hybrid method
6 Conclusion
The  two  major  contributions  of  multiple  seg-
mentation are generation of new N-grams and 
boosting N-gram counts of those found in first 
best  segmentation.  However,  it  also  produces 
bad N-grams that affect the performance of lan-
guage models. In this paper, we studied the con-
tribution  of  multiple  segmentation  approach 
more deeply and conducted experiments on effi-
cient solutions to minimize the effect of adding 
bad N-grams. Since only boosting the tri-gram 
counts  of  first  best  segmentation  and  adding 
only new tri-grams did not  reduce recognition 
error  rate,  we  have  proposed  to  pool  all  N-
grams of N-best  segmentations to that  of  first 
best  segmentation  and  got  a  significant  im-
provement in perplexity and tri-gram hits from 
6
which we obtained the maximum (0.8 absolute) 
reduction in recognition error rate. 
To   minimize  the  effect  of  adding  bad  N-
grams,  we  have  cut  off  rare  tri-grams in  lan-
guage modeling and got  reduction in  recogni-
tion error rate. The significant reduction of tri-
grams that  resulted  from the  cut  off  revealed 
that the majority of tri-grams generated by mul-
tiple  segmentation  have  counts  1.  Cutting  off 
such a big portion of the trigrams reduced tri-
gram hits and as a solution, we  proposed a hy-
brid of both pooling  and cutting off tri-grams 
from which we obtained a significant reduction 
in recognition error rate. 
It  is  possible  to  conclude  that  our  methods 
make the multiple segmentation approach more 
useful by minimizing the effect of bad N-grams 
that it generates and utilizing the contribution of 
different multiple segmentations. 
However,  we  still  see  rooms  for  improve-
ment.  A systematic selection of new tri-grams 
(for example, based on the probabilities of the 
N-grams and/or application of simple linguistic 
criteria  to  evaluate  the  usefulness  of  new tri-
grams), with the aim of reducing bad tri-grams, 
might lead to performance improvement. Thus, 
we will do experiments in this line. We will also 
apply these methods to other languages, such as 
Khmer.
References
Lee, Young-Suk, Papineni, Kishore, Roukos, Salim 
Emam,  Ossama  and  Hassan,  Hany.  2003.  Lan-
guage model based arabic word segmentation. In 
Proceedings of the ACL?03, pp. 399?406. 
Le,  Viet-Bac,  Besacier,  Laurent,  Seng,  Sopheap, 
Bigi,  Brigite and Do, Thi-Ngoc-Diep. 2008.  Re-
cent  advances  in  automatic  speech  recognition  
for vietnamese. SLTU?08, Hanoi Vietnam. 
Mohri,  Mehryar,  Fernando  C.  N.  Pereira,  and 
Michael Riley, ?A rational design for a weighted 
finite-state transducer library,? in Lecture Notes in 
Computer Science. Springer, 1998, pp. 144?158. 
Seng,  Sopheap,  Besacier,  Laurent,  Bigi,  Brigitte, 
Castelli,  Eric.  2009.  Multiple Text Segmentation  
for  Statistical  Language  Modeling. InterSpeech, 
Brighton, UK,  
Stolcke, Andreas.  2002. SRILM: an extensible lan-
guage  modeling  toolkit.  Proceedings  of  Interna-
tional Conference on Spoken Language Process-
ing, volume II, 901?904 . 129.88.65.115
7
JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines, pages 53?62,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Analyse des performances de mod?les de langage sub-lexicale
pour des langues peu-dot?es ? morphologie riche
Hadrien Gelas1, 2 Solomon Teferra Abate2
Laurent Besacier2 Fran?ois Pellegrino1
(1) Laboratoire Dynamique Du Langage, CNRS - Universit? de Lyon, France
(2) Laboratoire Informatique de Grenoble, CNRS - Universit? Joseph Fourier Grenoble 1, France
{hadrien.gelas, francois.pellegrino}@univ-lyon2.fr
{solomon.abate, laurent.besacier@imag.fr}
R?SUM?
Ce papier ?tudie l?impact de l?utilisation d?unit?s sous-lexicales sur les performances d?un syst?me
de RAP pour deux langues africaines peu-dot?es et morphologiquement riches (l?amharique et le
swahili). Deux types de sous-unit?s sous-lexicales sont consider?s : la syllabe et le morph?me,
ce dernier ?tant obtenu de mani?re supervis?e ou non-supervis?e. La reconstruction en mots
? partir de sorties de RAP en syllabes ou morph?mes est aussi prise en compte. Pour les deux
langues, les meilleurs r?sultats sont obtenus avec les morph?mes non-supervis?s. Le taux
d?erreur de mots est grandement r?duit pour la reconnaissance de l?amharique dont les donn?es
d?entrainement du LM sont tr?s faibles (2,3M de mots). Les scores pour la RAP du swahili sont
aussi am?lior?s (28M de mots pour l?entrainement). Il est ausi pr?sent? une analyse d?taill?e de
la reconstruction des mots hors vocabulaires, un pourcentage important de ceux-ci (jusqu?? 75%
pour l?amharique) sont retrouv?s ? l?aide de mod?les de langage ? base de morph?mes et la
m?thode de reconstruction appropri?e.
ABSTRACT
Performance analysis of sub-word language modeling for under-resourced languages with
rich morphology : case study on Swahili and Amharic
This paper investigates the impact on ASR performance of sub-word units for two under-
resourced african languages with rich morphology (Amharic and Swahili). Two subword
units are considered : syllable and morpheme, the latter being obtained in a supervised or
unsupervised way. The important issue of word reconstruction from the syllable (or morpheme)
ASR output is also discussed. For both languages, best results are reached with morphemes
got from unsupervised approach. It leads to very significant WER reduction for Amharic ASR
for which LM training data is very small (2.3M words) and it also slightly reduces WER over
a Word-LM baseline for Swahili ASR (28M words for LM training). A detailed analysis of the
OOV word reconstruction is also presented ; it is shown that a high percentage (up to 75% for
Amharic) of OOV words can be recovered with morph-based language model and appropriate
reconstruction method.
MOTS-CL?S : Mod?le de langage, Morph?me, Hors vocabulaire, Langues peu-dot?es.
KEYWORDS: Language model, Morpheme, Out-of-Vocabulary , Under-resourced languages.
53
1 Introduction
Due to world?s globalisation and answering the necessity of bridging the numerical gap with
the developing world, speech technology for under-resourced languages is a challenging issue.
Applications and usability of such tools in developing countries are proved to be numerous
and are highlighted for information access in Sub-Saharan Africa (Barnard et al, 2010a,b),
agricultural information in rural India (Patel et al, 2010), or health information access by
community health workers in Pakistan (Kumar et al, 2011).
In order to provide a totally unsupervised and language independent methodology to develop an
automatic speech recognition (ASR) system, some particular language characteristics should be
taken into account. Such specific features as tones ((Lei et al, 2006) on Mandarin Chinese) or
writing systems without explicit word boundaries ((Seng et al, 2008) on Khmer) need a specific
methodology adaptation. This is especially true when dealing with under-resourced languages,
where only few data are available.
During recent years, many studies tried to deal with morphologically rich languages (whether
they are agglutinative, inflecting and compounding languages) in NLP (Sarikaya et al, 2009).
Such a morphology results in data sparsity and in a degraded lexical coverage with a similar
lexicon size than state-of-the-art speech recognition setup (as one for English). It yields high
Out-of-Vocabulary (OOV) rates and degrades Word-Error rate (WER) as each OOV words will not
be recognized but can also affect their surrounding words and strongly increase WER.
When the corpus size is limited, a common approach to overcome the limited lexical coverage
is to segment words in sub-word units (morphemes or syllables). Segmentation in morphemes
can be obtained in a supervised or unsupervised manner. Supervised approaches were mainly
used through morphological analysers built on carefully annotated corpora requiring impor-
tant language-specific knowledge (as in (Ar?soy et al, 2009)). Unsupervised approaches are
language-independent and do not require any linguistic-knowledge. In (Kurimo et al, 2006),
several unsupervised algorithms have been compared, including their own public method called
Morfessor ((Creutz et Lagus, 2005)) for two ASR tasks in Turkish and Finnish (see also (Hirsimaki
et al, 2009) for a recent review of morh-based approaches). The other sub-word type that is also
utilized for reducing high OOV rate is the syllable. Segmentation is mainly rule-based and was
used in (Shaik et al, 2011b) and (Shaik et al, 2011a), even if outperformed in WER by ASR
morpheme-based recognition for Polish and German.
In this work, we investigate those different methodologies and see how to apply them for two
different speech recognition tasks : read speech ASR in Amharic and broadcast speech trans-
cription in Swahili. These tasks represents two different profiles of under-resourced languages
cases. Amharic with an acoustic model (AM) trained on 20h of read-speech but limited text data
(2.3M) and on the opposite, Swahili with a weaker acoustic model (12h of broadcast news from
internet mixing genre and quality) but a more robust LM (28M words of web-mining news, still
without any adaptation to spoken broadcast news). If such study on sub-unit has already been
conducted on Amharic (Pellegrini et Lamel, 2009), no prior work are known to us for Swahili.
But, the main goal of this study is to better understand what does really impact performance
of ASR using sub-word unit through a comparison of different methodologies. Both supervised
and unsupervised segmentation strategies are explored as well as different approaches to tag
segmentation.
54
The next section describes the target languages and the available corpora. Then, we introduce
several segmentation approaches in section 3. Section 4 presents the analysis of experimental
results for Swahili and Amharic while section 5 concludes this work.
2 Experiment description
2.1 Languages
Amharic is a Ethio-Semitic language from the Semitic branch of the Afroasiatic super family. It
is related to Hebrew, Arabic, and Syrian. According to the 1998 census, it is spoken by over
17 million people as a first language and by over 5 million as a second language throughout
Ethiopia. Amharic is also spoken in other countries such as Egypt, Israel and the United States.
It has its own writing system which is syllabary. It exhibits non-concatenative, inflectional and
derivational morphology. Like other Semitic languages such as Arabic, Amharic exhibits a root-
pattern morphological phenomenon. Case, number, definiteness, and gender-marking affixes
inflect nouns. Some adverbs can be derived from adjectives but adverbs are not inflected. Nouns
are derived from other basic nouns, adjectives, stems, roots, and the infinitive form of a verb is
obtained by affixation and intercalation.
Swahili is a Bantu language often used as a vehicular language in a wide area of East Africa. It
is not only the national language of Kenya and Tanzania but it is also spoken in different parts
of Democratic Republic of Congo, Mozambique, Somalia, Uganda, Rwanda and Burundi. Most
estimations give over 50 million speakers (with only less than 5 million native speakers). It has
many typical Bantu features, such as noun class and agreement systems and complex verbal
morphology. Structurally, it is often considered as an agglutinative language (Marten, 2006).
2.2 Speech corpora description
Both Amharic and a small part of Swahili training audio corpora were collected following the
same protocol. Texts were extracted from news websites and segmented by sentence. Native
speakers were recorded using a self-paced reading interface (with possible rerecordings). The
Amharic speech corpus (Abate et al, 2005) consists of 20 hours of training speech collected
from 100 speakers who read a total of 10,850 sentences. Swahili corpus corresponds to 2 hours
and a half read by 5 speakers (3 male and 2 female) along with almost 10 hours of web-mining
broadcast news representing various types of recording quality (noisy speech, telephone speech,
studio speech) and speakers. They were transcribed using a collaborative transcription process
based on the use of automatic pre-transcriptions to increase productivity gains (See details in
(Gelas et al, 2012)). Test corpora are made of 1.5 hours (758 sentences) of read speech for
Amharic and 2 hours (1,997 sentences) of broadcast news for Swahili.
55
2.3 Text corpora description
We built all statistical N-gram language model (LM) using the SRI 1 language model toolkit.
Swahili text corpus is made of data collected from 12 news websites (over 28M words). To
generate a pronunciation dictionary, we extracted the 65k most frequent words from the text
corpus and automatically created pronunciations taking benefit of the regularity of the grapheme
to phoneme conversion in Swahili. The same methodology and options have been applied to all
sub-words LM. For Amharic, we have used the data (2.3M words text) described in (Tachbelie
et al, 2010).
3 Segmenting text data
3.1 Unsupervised morphemic segmentation
For the unsupervised word segmentation, we used a publicly available tool called Morfessor 2.
Its data-driven approach learns a sub-word lexicon from a training corpus of words by using a
Minimum Description Length (MDL) algorithm (Creutz et Lagus, 2005). It has been used with
default options and without any adaptation.
3.2 Supervised morphemic and syllabic segmentation
For Amharic, we used the manually-segmented text described in (Tachbelie et al, 2011a) to train
an FSM-based segmenter (a composition of morpheme transducer and 12gram consonant vowel
syllable-based language model) using the AT&T FSM Library (FiniteState Machine Library) and
GRM Library (Grammar Library)(Mohri et al, 1998). The trained segmenter with the language
model is applied to segment the whole text mentioned in (Tachbelie et al, 2010).
The supervised decomposition for Swahili is performed with the public Part-Of-Speech tagger
named TreeTagger 3. It is using the parameters available for Swahili to extract sub-word units.
As for as syllable segmentation is concerned, we designed rule-based algorithms following
structural and phonological restrictions of the respective languages.
3.3 Segmentation tagging and vocabulary size
While working on sub-word unit, one should think on how to incorporate the segmentation
information. Morphological information can be included within factored LM as in (Tachbelie
et al, 2011b) or directly as a full unit in the n-gram LM itself. By choosing the latter, the ASR
decoder output is a sequence of sub-word units and an additional step is needed to recover
1. www.speech.sri.com/projects/srilm/
2. The unit obtained with Morfessor is referred here as morpheme even if it do not automatically corresponds to the
linguistic definition of morpheme (the smallest semantically meaningful unit)
3. www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTree
Tagger.html
56
words from sub-units. In (Diehl et al, 2011), a n-gram SMT-based morpheme-to-word conversion
approach is proposed.
In this work, we evaluate how the recognition performance is affected by different ways of
tagging the segmentation information straightly in the training text. In (Ar?soy et al, 2009), it is
noticed that this aspect need to be considered as it impacts WER. In (Guijarrubia et al, 2009),
a similar methodology is applied without reading any conclusion since a too small and easy
recognition task was performed.
Three distinct types of tagging are evaluated here :
? UNIT_AFX : A morpheme boundary (MB) is added on left and/or right side of segmentation
leaving the (so-called with Morfessor) root alone. To rebuild up to words, we reconnect every
units containing MB to the one next to it.
(ex. kiMB tabu? kitabu)
? UNIT_ALL : A MB tag is added on each side of segmentation, in other words, we add to the
lexicon the information to distinguish roots from their context (we can get up to four different
entries for a same root : ROOT, MBROOT, ROOTMB, MBROOTMB). To rebuild, we reconnect
every time two MB appearing consecutively.
(ex. kiMB MBtabu? kitabu)
? UNIT_POS : For syllables, we add to the unit the position of the syllable in the word.
(ex. 1ki 2ta 3bu? kitabu)
In table 1, it is shown that each choice has an influence on the size of the full text vocabulary
and thus on the lexical coverage of the 65k lexicon. As expected from a language with rich
morphology, the word baseline 65k lexicon shows a dramatically low lexical coverage (13.95%).
For the same text information, syllables logically reduce the size of vocabulary and got a full
theoretical lexical coverage without reaching the 65k limits, but with the cost of really short
length unit. Concerning both morpheme segmentation types, as expected the supervised approach
leads to a larger number of units than the unsupervised statistical approach, the latter leads to a
better theoretical lexical coverage. The average token length do not reduce much compared to
word unit as most frequent words are already short mono-morphemic grammatical words. The
influence of different tagging techniques is also shown on the same table. Detailed comments on
WER will be given in 4.2.
FullVoc 65k Cov. Token WER
LM (%) (%) length (%)
Word 100 13.95 5.5 35.7
Syl_Pos (V=27k) 5.79 100 2.0 51.7
Treetag_All 79.38 17.57 4.4 44.7
Treetag_Afx 78.61 17.74 4.4 43.3
Morf_All 45.24 30.83 5.3 34.8
Morf_Afx 38.07 36.64 5.3 35.4
TABLE 1 ? Swahili - Size of full text corpus vocabulary in comparison with a word level baseline
(FullVoc) ; lexical coverage of a 65k lexicon on the full vocabulary (65k Cov.) ; average token length
in character for the text corpus ; word error rate depending on the choice of unit and segmentation
tag (WER), all systems using 3gram LM and 65k lexicon except when specified
57
4 Results
4.1 ASR system description
We used SphinxTrain 4 toolkit from Sphinx project for building Hidden Markov Models (HMM)
based acoustic models (AMs) for Swahili. With the speech training database described in 2.2,
we trained a context-dependent model with 3,000 tied states. The acoustic models have 36 and
40 phones for Swahili and Amharic, respectively. We used the HDecode decoder of the HTK for
Amharic. The Amharic acoustic model is more precisely described in (Tachbelie et al, 2010).
4.2 Analysis of Sub-word units performance for Swahili
Comparing all results for Swahili broadcast speech transcription task (table 1), Morfessor based
segmentation ASR system is the only one, with 34.8% WER, performing significantly better than
the 35.7% word baseline. As in (Ar?soy et al, 2009) and (Hirsimaki et al, 2006), segmentation
based on a morphological analyser reaches lower results (43.3% WER) than words and unsu-
pervised based segmentation. Finally, rule-based syllabic system have the worst performance
with 51.7% WER. Those scores in table 1 gives a good indication on how to choose the most
performing unit. It seems that one need to balance and optimise two distinct criteria : n-gram
length coverage and lexical coverage.
The importance of n-gram length coverage can be seen with poor performance of too short
units, like syllables in this work. A syllable trigram (average 6.0 character-long) is approximately
equivalent to a word unigram in Swahili (average 5.5 character-long), thus such a short trigram
length is directly impacting ASR system performance even if lexical coverage is maximized
(100%). The importance to use higher order n-gram LM when dealing with short units is also
shown in (Hirsimaki et al, 2009). However, if a lattice rescoring framework is often used, it is
difficult to recover enough information if the first trigram pass do not perform well enough. It is
then recommended to directly implement the higher order n-gram LM in the decoder.
In the same time, a larger lexical coverage (lex.cov.), allows better performance if not used
with too short units as shows the difference of performance between word-based LM (13.95%
lex.cov. and 35.7% WER) and Morfessor-based LM (30.83% lex.cov. and 34.8% WER), both
having similar average token lengths.
Concerning the different tagging techniques, they have an impact on WER. The better choice
seems to be influenced by the lexical coverage. When lexical coverage is good enough (Morfessor-
based system), one can get advantage of having more different and precise contexts (tag on all
units, separating roots alone and roots with affixes in the vocabulary and on n-gram estimations),
whereas for low lexical coverage (TreeTagger-based system), having more various words is better
(tag only on affixes, regrouping all same roots together allowing more distinct units in the
lexicon).
4. cmusphinx.sourceforge.net/
58
4.3 Sub-word units performance for Amharic
For the read speech recognition task for Amharic, only the best performing systems are presented
in table 2. Similar trend is found concerning the tagging techniques (better systems are tagged
ALL for Morfessor and tagged AFX for FSM) and by the fact that Morfessor system outperforms
the others. Even if the unit length in Morfessor is 40% shorter than average word length, it
gets important benefits from a 100% lexical coverage of the training corpus. However, for this
task, the supervised segmentation (FSM) has better results than word baseline system. It can be
explained by a slightly increased lexical coverage and still a reasonable token length. Through
this task, we also considered several vocabulary sizes. Results show that WER greatly benefits
from sub-units in smaller lexicon tasks. Finally, as for Amharic sub-word units being notably
shorter than word units, we rescored output lattices from the trigram LM system with a 5gram
LM. It leads to an absolute WER decrease of 2.0% for Morfessor.
65k Cov. Token Word Error Rate (%)
LM (%) length 5K 20K 65K
Word_3g 30.79 8.3 52.4 29.6 15.9
FSM_Afx_3g 45.13 6.3 39.3 20.8 12.2
FSM_Afx_5g 45.13 6.3 39.1 20.3 11.4
Morf_All-3g 100 4.9 36.7 14.8 9.9
Morf_All-5g 100 4.9 34.9 12.6 7.9
TABLE 2 ? Amharic - Lexical coverage of a 65k lexicon on the full vocabulary (65k Cov.) ; average
token length in the whole text corpus ; word error rate depending on the choice of unit, segmentation
tag and vocabulary size
4.4 OOV benefits of using sub-word units
Making good use of sub-word units for ASR has been proved efficient in many research to
recognize OOV words over baseline word LMs (as in (Shaik et al, 2011a)). Table 3 presents
the different OOV rates considering both token and type for each LM (OOV morphemes for
Morfessor-based LM). We also present the proportion of correctly recognized words (COOV)
which were OOVs in the word baseline LM. Results show important OOV rate reduction and
correctly recognised OOV rate for both languages (Morfessor-based outputs). For Amharic, the
difference of COOV rate between each lexicon is correlated with the possible OOVs each system
can recognized.
Swahili obtain less benefits for COOV. It can be explained by the specificity of the broadcast news
task, leading to important OOV entity names or proper names (the 65k Morfessor-based lexicon
is still having 11.36% of OOV types). But if we consider only the OOVs that can possibly be
recognized (i.e. only those which are not also OOVs in the Morfessor-based lexicon), 36.04% of
them are rebuilt. Due to decoder limitations we restrained this study to a 65k lexicon, but for a
Swahili 200k word vocabulary we get a type OOV rate of 12.46% and still 10.28% with a full
vocab (400k). Those numbers are really close to those obtained with the 65k Morfessor lexicon
and could only be reached with the cost of more computational power and less robust LM. In the
59
OOV (%) OOV (%) COOV (%)
LM Token Type
Amharic
Word-5k 35.21 57.14 -
Word-20k 19.48 32.18 -
Word-65k 9.06 14.99 -
Morf_All-5k 13.67 40.58 33.76
Morf_All-20k 2.50 7.88 66.95
Morf_All-65k 0.12 2.81 75.30
Swahili
Word-65k 5.73 19.17 -
Morf_All-65k 3.67 11.36 8.77
TABLE 3 ? Amharic and Swahili - Token and type OOV rate in test reference transcriptions depending
on LM (OOV morphemes for Morfessor-based LM) ; correctly recognised baseline OOV words rate in
ASR outputs (COOV)
same time, growing Morfessor lexicon to 200k would be more advantageous as it reduces the
type OOV rate to 1.61%.
While using sub-word system outputs rebuilt to word level reduces OOV words, in contrary, it can
also generate non words by ungrammatical or non-sense concatenation. We checked the 5029
words generated by the best Amharic Morfessor output to see if they exist in the full training
text vocabulary. It appears that only 37 are non-words (33 after manual validation). Among
those 33, there were 26 isolated affixes and 7 illegal concatenations, all due to poor acoustic
estimation from the system. Considering this small amount of non-words and with no possibility
to retrieve good ones in lattices, we did not process to constraint illegal concatenation as in
(Ar?soy et Sara?lar, 2009).
5 Conclusion
We investigated the use of sub-word units in n-gram language modeling through different metho-
dologies. The best results are obtained using unsupervised segmentation with Morfessor. This
tool outperforms supervised methodologies (TreeTagger, FSM or rule-based syllables) because
the choice of sub-word units optimise two essential criteria which are n-gram length coverage
and lexical coverage. In the same time, it appears that the way one implements the segmentation
information affects the speech recognition performance. As expected, using sub-word units brings
major benefits to the OOV problem. It shows to be effective in two very different tasks for two
under-resourced African languages with rich morphology (one being highly inflectional, Amharic
and the other being agglutinative, Swahili). The Amharic read speech recognition task, get the
more advantages of it, since the word baseline LM suffers from data sparsity. But results are also
improved for a broadcast speech transcription task for Swahili.
60
R?f?rences
ABATE, S., MENZEL, W. et TAFILA, B. (2005). An Amharic speech corpus for large vocabulary
continuous speech recognition. In Interspeech, pages 67?76.
ARISOY, E., CAN, D., PARLAK, S., SAK, H. et SARA?LAR, M. (2009). Turkish broadcast news
transcription and retrieval. Audio, Speech, and Language Processing, IEEE Transactions on,
17(5):874?883.
ARISOY, E. et SARA?LAR, M. (2009). Lattice extension and vocabulary adaptation for Turkish
LVCSR. Audio, Speech, and Language Processing, IEEE Transactions on, 17(1):163?173.
BARNARD, E., DAVEL, M. et van HUYSSTEEN, G. (2010a). Speech technology for information
access : a South African case study. In AAAI Symposium on Artificial Intelligence, pages 22?24.
BARNARD, E., SCHALKWYK, J., van HEERDEN, C. et MORENO, P. (2010b). Voice search for develop-
ment. In Interspeech.
CREUTZ, M. et LAGUS, K. (2005). Unsupervised morpheme segmentation and morphology
induction from text corpora using morfessor 1.0. Rapport technique, Computer and Information
Science, Report A81, Helsinki University of Technology.
DIEHL, F., GALES, M., TOMALIN, M. et WOODLAND, P. (2011). Morphological decomposition in
Arabic ASR systems. Computer Speech & Language.
GELAS, H., BESACIER, L. et PELLEGRINO, F. (2012). Developments of swahili resources for an
automatic speech recognition system. In SLTU.
GUIJARRUBIA, V., TORRES, M. et JUSTO, R. (2009). Morpheme-based automatic speech recognition
of basque. Pattern Recognition and Image Analysis, pages 386?393.
HIRSIMAKI, T., CREUTZ, M., SIIVOLA, V., KURIMO, M., VIRPIOJA, S. et PYLKKONEN, J. (2006). Unlimi-
ted vocabulary speech recognition with morph language models applied to Finnish. Computer
Speech & Language.
HIRSIMAKI, T., PYLKKONEN, J. et KURIMO, M. (2009). Importance of high-order n-gram models in
morph-based speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on,
17(4):724?732.
KUMAR, A., TEWARI, A., HORRIGAN, S., KAM, M., METZE, F. et CANNY, J. (2011). Rethinking speech
recognition on mobile devices. In IUI4DR. ACM.
KURIMO, M., CREUTZ, M., VARJOKALLIO, M., ARISOY, E. et SARACLAR, M. (2006). Unsupervised
segmentation of words into morphemes?morpho challenge 2005, application to automatic
speech recognition. In Interspeech.
LEI, X., SIU, M., HWANG, M., OSTENDORF, M. et LEE, T. (2006). Improved tone modeling for
Mandarin broadcast news speech recognition. In Interspeech.
MARTEN, L. (2006). Swahili. In BROWN, K., ?diteur : The Encyclopedia of Languages and Linguistics,
2nd ed., volume 12, pages 304?308. Oxford : Elsevier.
MOHRI, M., PEREIRA, F. et RILEY, M. (1998). A rational design for a weighted finite-state
transducer library. In Lecture Notes in Computer Science, pages 144?158. Springer.
PATEL, N., CHITTAMURU, D., JAIN, A., DAVE, P. et PARIKH, T. (2010). Avaaj otalo : a field study of
an interactive voice forum for small farmers in rural India. In CHI, pages 733?742. ACM.
61
PELLEGRINI, T. et LAMEL, L. (2009). Automatic word decompounding for ASR in a morphologically
rich language : Application to Amharic. Audio, Speech, and Language Processing, IEEE Transactions
on, 17(5):863?873.
SARIKAYA, R., KIRCHHOFF, K., SCHULTZ, T. et HAKKANI-TUR, D. (2009). Introduction to the special
issue on processing morphologically rich languages. Audio, Speech, and Language Processing,
IEEE Transactions on, 17(5).
SENG, S., SAM, S., BESACIER, L., BIGI, B. et CASTELLI, E. (2008). First broadcast news transcription
system for Khmer language. In LREC.
SHAIK, M., MOUSA, A., SCHLUTER, R. et NEY, H. (2011a). Hybrid language models using mixed
types of sub-lexical units for open vocabulary German LVCSR. In Interspeech.
SHAIK, M., MOUSA, A., SCHLUTER, R. et NEY, H. (2011b). Using morpheme and syllable based
sub-words for Polish LVCSR. In ICASSP.
TACHBELIE, M., ABATE, S. et BESACIER, L. (2011a). Part-of-speech tagging for under-resourced
and morphologically rich languages - the case of Amharic. In HLTD.
TACHBELIE, M., ABATE, S. et MENZEL, W. (2010). Morpheme-based automatic speech recognition
for a morphologically rich language - amharic. In SLTU.
TACHBELIE, M., ABATE, S. et MENZEL, W. (2011b). Morpheme-based and factored language
modeling for Amharic speech recognition. In VETULANI, Z., ?diteur : Human Language Technology.
Challenges for Computer Science and Linguistics, volume 6562 de Lecture Notes in Computer
Science, pages 82?93. Springer.
62
