Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 593?600
Manchester, August 2008
Robust Similarity Measures for Named Entities Matching
Erwan Moreau1
Institut Te?le?com ParisTech
& LTCI CNRS
erwan.moreau@enst.fr
Franc?ois Yvon
Univ. Paris Sud
& LIMSI CNRS
yvon@limsi.fr
Olivier Cappe?
Institut Te?le?com ParisTech
& LTCI CNRS
cappe@enst.fr
Abstract
Matching coreferent named entities with-
out prior knowledge requires good similar-
ity measures. Soft-TFIDF is a fine-grained
measure which performs well in this task.
We propose to enhance this kind of met-
rics, through a generic model in which
measures may be mixed, and show experi-
mentally the relevance of this approach.
1 Introduction
In this paper, we study the problem of matching
coreferent named entities (NE in short) in text col-
lections, focusing primarily on orthographic vari-
ations in nominal groups (we do not handle the
case of pronominal references). Identifying textual
variations in entities is useful in many text min-
ing and/or information retrieval tasks (see for ex-
ample (Pouliquen et al, 2006)). As described in
the literature (e.g. (Christen, 2006)), textual dif-
ferences between entities are due to various rea-
sons: typographical errors, names written in dif-
ferent ways (with/without first name/title, etc.),
abbreviations, lack of precision in organization
names, transliterations, etc. For example, one
wants ?Mr. Rumyantsev? to match with ?Alexan-
der Rumyanstev? but not with ?Mr. Ryabev?.
Here we do not address the related problem of dis-
ambiguation2 (e.g. knowing whether a given oc-
currence of ?George Bush? refers to the 41st or
43rd president of the USA), because it is techni-
cally very different from the matching problem.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1Now at LIPN - Univ. Paris 13 & UMR CNRS 7030.
2Which is essential in the Web People Search task.
There are different ways to tackle the problem
of NE matching: the first and certainly most reli-
able one consists in studying the specific features
of the data, and then use any available tool to de-
sign a specialized method for the matching task.
This approach will generally take advantage of
language-specific (e.g. in (Freeman et al, 2006))
and domain-specific knowledge, of any external
resources (e.g. database, names dictionaries, etc.),
and of any information about the entities to pro-
cess, e.g. their type (person name, organization,
etc.), or internal structure (e.g. in (Prager et al,
2007)). In such an in-depth approach, supervised
learning is helpful: it has been used for example
in a database context3 in (Bilenko et al, 2003), but
this approach requires labeled data which is usu-
ally costly. All those data specific appproaches
would necessitate some sort of human expertise.
The second approach is the robust one: we
propose here to try to match any kind of NE,
extracted from ?real world? (potentially noisy)
sources, without any kind of prior knowledge4.
One looks for coreferent NE, whatever their type,
source, language5 or quality6. Such robust simi-
larity methods may be useful for a lot of generic
tasks, in which maximum accuracy is not the main
criterion, or simply where the required resources
are not available.
The literature on string comparison metrics is
abundant, containing both general techniques and
3The matching task is quite different in this framework,
because one observes records (structured information).
4In this kind of knowledge are included the need for hand-
tuning parameters or defining specific thresholds.
5Actually we have only studied English and French (our
approach is neither ?multilingual?, in the sense that it is not
specific to multilingual documents).
6In particular, this task clearly depends on the NE recog-
nition step, which may introduce errors.
593
more linguistically motivated measures, see e.g.
(Cohen et al, 2003) for a review. From a bird?s eye
view, these measures can be sorted in two classes:
?Sequential character-based methods? and ?Bag-
of-words methods?7. Both classes show relevant
results, but do not capture the same kind of simi-
larity. In a robust approach for NE matching, one
needs a more fine-grained method, which performs
at least as well as bag-of-words methods, without
ignoring coreferent pairs that such methods miss.
A first attempt in this direction was introduced
in (Cohen et al, 2003), in the form of a measure
called Soft-TFIDF. We will show that this measure
has theoretical pitfalls and a few practical draw-
backs. Nevertheless, Soft-TFIDF outperforms the
better standard string similarity measures in the
NE matching task. That is why we propose to gen-
eralize and improve its principle, and show exper-
imentally that this approach is relevant.
In section 2 we introduce standard similar-
ity measures and enhance the definition of Soft-
TFIDF. Then we define a generic model in which
similarity measures may be combined (section 3).
Finally, section 4 shows that experiments with two
different corpora validate our approach.
2 Approximate matching methods
We present below some of the main string similar-
ity measures used to match named entities (Chris-
ten, 2006; Cohen et al, 2003; Bilenko et al, 2003).
2.1 Classical metrics
2.1.1 Sequential character based methods
Levenshtein edit distance. This well-known dis-
tance metric d represents the minimum number
of insertions, deletions or substitutions needed to
transform a string x into another string y. For ex-
ample, d(kitten, sitting) = 3 (k 7? s, e 7? i,
? 7? g). The corresponding normalized similarity
measure is defined as s = 1 ? d/max(|x|, |y|). A
lot of variants and/or improvements exist (Navarro,
2001), among which:
? Damerau. One basic edit operation is added:
a transposition consists in swapping two
characters;
? Needleman-Wunch. Basic edit operation
costs are parameterized: G is the cost of a gap
7We omit measures based on phonetic similarity such
as Soundex, because they are language-specific and/or type-
specific (person names).
(insertion or deletion), and there is a function
cost(c, c
?
) which gives the cost of substituting
c with c? for any pair of characters (c, c?).
Jaro metric (Winkler, 1999). This measure is
based on the number and the order of common
characters. Given two strings x = a
1
. . . a
n
and
y = b
1
. . . b
m
, let H = min(n,m)/2: a
i
is in com-
mon with y if there exists b
j
in y such that a
i
= b
j
and i ? H ? j ? i + H . Let x? = a?
1
. . . a
?
n
?
(resp. y? = b?
1
. . . b
?
m
?
) be the sequence of charac-
ters from x (resp. y) in common with y (resp. x),
in the order they appear in x (resp. y). Any posi-
tion i such that a?
i
6= b
?
i
is called a transposition.
Let T be the number of transpositions between x?
and y? divided by 2:
Jaro(x, y) =
1
3
?
(
|x
?
|
|x|
+
|y
?
|
|y|
+
|y
?
|?T
|y
?
|
)
2.1.2 Bag-of-words methods
With these methods, each NE is represented as
a set of features (generally words or characters n-
grams8). Let X = {x
i
}
1?i?n
and Y = {y
i
}
1?i?m
be the sets representing the entities x, y. Simplest
measures only count the number of elements in
common9, e.g:
Overlap(x, y) =
|X ? Y |
min(|X|, |Y |)
Some more subtle techniques are based on a
vector representation of entities x and y, which
may take into account parameters that are are
not included in the sets themselves. Let A =
(a
1
, . . . , a
|?|
) and B = (b
1
, . . . , b
|?|
) be such vec-
tors10, the widely used cosine similarity is:
cos(A,B) =
?
|?|
i=1
a
i
b
i
?
?
|?|
i=1
a
2
i
?
?
|?|
i=1
b
2
i
Traditionally, TF-IDF weights are used in
vectors (Term Frequency-Inverse Document Fre-
quency). In the NE case, this value represents the
importance each feature w (e.g. word) has for an
entity x belonging to the set E of entities:
tf(w, x) =
n
w,x
?
w
?
??
n
w
?
,x
, idf(w) = log
|E|
|{x ? E|w ? x}|
,
tfidf(w, x) = tf(w, x) ? idf(w).
with n
w,x
the number of times w appears in x.
Thus the similarity score is CosTFIDF(x, y) =
Cos(A,B), where each a
i
(resp. b
i
) in A (resp. in
B) is tfidf(w
i
, x) (resp. tfidf(w
i
, y)).
8In the remaining the term n-grams is always used for
characters n-grams.
9
|E| denotes the number of elements in E.
10
? is the vocabulary, containing all possible features.
594
2.2 Special measures for NE matching
Experiments show that sequential character-based
measures catch mainly coreferent pairs of long NE
that differ only by a few characters. Bag-of-words
methods suit better to the NE matching problem,
since they are more flexible about word order and
position. But a lot of coreferent pairs can not be
identified by such measures, because of small dif-
ferences between words: for example, ?Director
ElBaradei? and ?Director-General ElBareidi? is
out of reach for such methods. That is why ?sec-
ond level? measures are relevant: their principle is
to apply a sub-measure sim? to all pairs of words
between the two NE and to compute a final score
based on these values. This approach is possible
because NE generally contain only a few words.
Monge-Elkan measure belongs to this category:
it simply computes the average of the better pairs
of words according to the sub-measure:
sim(x, y) =
1
n
n
?
i=1
m
max
j=1
(sim
?
(x
i
, y
j
)).
But experiments show that Monge-Elkan does
not perform well. Actually, its very simple behav-
ior favors too much short entities, because averag-
ing penalizes a lot every non-matching word.
A more elaborated measure is proposed in (Co-
hen et al, 2003): Soft-TFIDF is intended precisely
to take advantage of the good results obtained with
Cosine/TFIDF, without automatically discarding
words which are not strictly identical. The original
definition is the following: let CLOSE(?,X, Y )
be the set of words w ? X such that there ex-
ists a word v ? Y such that sim?(w, v) > ?. Let
N(w, Y ) = max({sim
?
(w, v)|v ? Y }). For any
w ? CLOSE(?,X, Y ), let
S
w,X,Y
= weight(w,X) ? weight(w, Y ) ?N(w, Y ),
where weight(w,Z) = tfidf(w,Z)?
?
w?Z
tfidf(w,Z)
2
.
Finally,
SoftTFIDF(X,Y ) =
?
w?CLOSE(?,X,Y )
S
w,X,Y
.
This definition is not entirely correct, be-
cause weight(w, Y ) = 0 if w /? Y (in other
words, w must appear in both X and Y , thus
SoftTFIDF(X,Y ) would always be equal to
CosTFIDF(X,Y )). We propose instead the fol-
lowing corrected definition, which corresponds to
the implementation the authors provided in the
package SecondString11:
11http://secondstring.sourceforge.net
Let CLOSEST(?,w,Z) = {v ? Z | ?v? ? Z :
sim
?
(w, v) ? sim
?
(w, v
?
) ? sim
?
(w, v) > ?}.
SoftTFIDF(X,Y ) =
?
w?X
weight(w,X) ? ?
w,Y
,
where ?
w,Z
= 0 if CLOSEST(?,w,Z) = ?, and
?
w,Z
= weight(w
?
, Z) ? sim
?
(w,w
?
) otherwise,
with12 w? ? CLOSEST(?,w,Z).
As one may see, SoftTFIDF relies on the same
principle than Monge-Elkan: for each word x
i
in the first entity, find a word y
j
in the second
one that maximizes sim?(x
i
, y
j
). Therefore, these
measures have both the drawback not to be sym-
metric. Furthermore, there is another theoretical
pitfall with SoftTFIDF: in Monge-Elkan, the fi-
nal score is simply normalized in [0, 1] using the
average among words of the first entity. Accord-
ing to the principle of the Cosine angle of TF-
IDF-weighted vectors, SoftTFIDF uses both vec-
tors norms. However the way words are ?approx-
imately matched? does not forbid the matching of
a given word in the second entity twice: in this
case, normalization is wrong because this word is
counted only once in the norm of the second vec-
tor. Consequently there is a potential overflow: ac-
tually it is not hard to find simple examples where
the final score is greater than 1, even if this case is
unlikely with real NE and a high threshold ?.
3 Generalizing Soft-TFIDF
3.1 A unifying framework for similarity
measures
We propose to formalize similarity measures in the
generic model below. This model is intended to
define, compare and possibly mix different kinds
of measures. The underlying idea is simply that
most measures may be viewed as a process follow-
ing different steps: representation as a sequence of
features13 (e.g. tokenization), alignment and a way
to compute the final score. We propose to define a
similarity measure sim through these three steps,
each of them is modeled as a function14:
Representation. Given a set F of features, let
features(e) = ?a
1
, . . . , a
n
? be a function that as-
12If |CLOSEST(?, w, Z)| > 1, pick any such w? in the
set. In the case of matching words between NE, this should
almost never happen.
13We use the word feature for the sake of generality.
14Of course, alternative definitions may be relevant. In par-
ticular one may wish to allow the alignment function to return
a set of graphs instead of only one. In the same way, one may
wish to add a special vertex ? to the graph, in order to repre-
sent the fact that a feature is not matched by adding an edge
between this feature and ?.
595
signs an (ordered) sequence of features to any en-
tity e (a
i
? F for any i). Features may be of any
kind (e.g. characters, words, n-grams, or even con-
textual elements of the entity) ;
Alignment. Given a function simF : F 2 7? R
which defines similarity between any pair of fea-
tures, let algn(?a
1
, . . . , a
n
?, ?a
?
1
, . . . , a
?
n
?
?) = G
be a function which assigns a graph G to any pair
of features sequences. G = (V,E) is a bipartite
weighted graph where:
? The set of vertices is V = A ? A?, where
A and A? are the partitions defined as A =
{v
1
, . . . , v
n
} and A? = {v?
1
, . . . , v
?
n
?
}. Each
v
i
(resp. v?
i
) represents (the position of) the
corresponding feature a
i
(resp. a?
i
) ;
? The set of weighted edges is E =
{(v
i
j
, v
?
i
?
j
, s
j
)}
1?j?|E|
, where v
i
j
? A,
v
?
i
?
j
? A
?
. Weights s
j
generally depend on
sim
F
(a
i
j
, a
?
i
?
j
).
Scoring. Finally sim = score(G), where score
assigns a real value (possibly normalized in [0, 1])
to the alignment G.
The representation step is not particularly origi-
nal, since different kinds of representation have al-
ready been used both with sequential methods and
?bag-of-features? methods. However our model
also entails an alignment step, which does not exist
with bag-of-features methods. Actually, the align-
ment is implicit with such methods, and we will
show that making it visible is essential in the case
of NE matching.
In the remaining of this paper we will only con-
sider normalized metrics (scores belong to [0, 1]).
3.2 Revisiting classical similarity measures
Measures presented in section 2 may be defined
within the model presented above. This mod-
elization is only intended to provide a theoretical
viewpoint on the measures: for all practical pur-
poses, standard implementations are clearly more
efficient. Below we do not detail the represen-
tation step, because there is no difficulty with it,
and also because it is interesting to consider that
any measure may be used with different kinds
of features, as we will show in the next section.
Let S = ?a
1
, . . . , a
n
? = features(e) and S? =
?a
?
1
, . . . , a
?
n
?
? = features(e
?
) for any pair of enti-
ties (e, e?).
3.2.1 Levenshtein-like similarity
The function align
lev
(S, S
?
) is defined in the
following way: let G
lev
be the set of all graphs
G = (V,E) such that any pair of edges
(v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E satisfies (i
j
<
i
k
? i
?
j
< i
?
k
) ? (i
j
> i
k
? i
?
j
> i
?
k
). This
constraint ensures that the sequential order of fea-
tures is respected15 , and that no feature may be
matched twice. In the simplest form of Leven-
shtein16, simF (a, b) = 1 if a = b and 0 otherwise:
for any (v
i
j
, v
?
i
?
j
, s
j
) ? E, s
j
= sim
F
(a
i
j
, a
?
i
?
j
).
Let
sim(G) = M ?n
g
?cost
g
?|E|+
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
,
where M = max(n, n?) and n
g
is the number of
vertices that are not connected (i.e. the number of
inserted or deleted words). cost
g
= 1 in the simple
Levenshtein form, but may be a parameter in the
Needleman-Wunch variant (gap cost). In brief, the
principle in this definition is to count the positions
where no edit operation is needed: thus maximiz-
ing sim(G) is equivalent to minimizing the cost of
an alignment:
align
lev
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?)|G? ? G
lev
}).
Finally, the function score
lev
is simply defined
as score
lev
(G) = sim(G)/max(n, n
?
). It is not
hard to see that this definition is equivalent to the
usual one (see section 2): basically, the graph rep-
resents the concept called trace in (Wagner and
Fischer, 1974), except that the cost function is ?re-
versed? to become a similarity function.
Figure 1: Example of Levenshtein alignment
k
i
t
t
s
i
t
t
i
n
g
e
n
0
1
1
1
0
1
Suppose cost
g
= 1:
sim(G) = M ?n
g
?|E|+
?
e
j
?E
s
j
sim(G) = 7 ? 1 ? 6 + 4
sim(G) = 4
score
lev
(G) = 4/7.
3.2.2 Bag of features
For all simple measures using only sets of fea-
tures, the function align
bag
(S, S
?
) is defined in
the following way: let G be the set of all graphs
15Constraints are a bit more complex for Damerau.
16In the Needleman-Wunch variant, simF should depend
on the cost function, e.g.: simF (a, b) = 1? cost(a, b).
596
G = (V,E) such that if (v
i
j
, v
?
i
?
j
, s
j
) ? E then
a
i
j
= a
?
i
?
j
(equivalently simF (a
i
j
, a
?
i
?
j
) = 1). Now
let once(G) be the set of all G ? G such that
any pair of edges (v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E
satisfies i
j
6= i
k
? i
?
j
6= i
?
k
(at most one match
for each feature), and a
i
j
6= a
i
k
(a feature oc-
curring several times is matched only once). Let
sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
for any G = (V,E).
align
bag
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?) |G? ? once(G)}).
Since all weights are equal to 1, one may show
that sim(G) = |S ? S?| for any G ? once(G).
Thus the score function is simply used for nor-
malization, depending on the given measure: for
example, score
overlap
(G) =
sim(G)
min(n, n
?
)
.
3.2.3 Soft-TFIDF
The case of Cosine measure with TFIDF
weighted vectors is a bit different. Here we define
the SoftTFIDF version: let algn
soft
(S, S
?
) be the
graph G = (V,E) defined as17 (v
i
j
, v
?
i
?
j
, s
j
) ? E if
and only if a?
i
?
j
= select(CLOSEST(?, a
i
j
, S
?
)),
where CLOSEST is the function defined in sec-
tion 2 and select(E) is a function returning the
first element in E if |E| > 0, and is undefined
otherwise18. For any such edge, the weight s
j
is
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Once again, let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
.
score
soft
(G) = sim(G)/(?S|| ? ?S
?
?), where
??a
1
, . . . , a
n
?|| =
?
?
?
?
n
?
i=1
(
idf(a
i
)
n
)
2
.
Although it is not explicitly used in this defini-
tion, term frequency is taken into account through
the number of edges: suppose a given term t ap-
pears m times in S and m? times in S?, all m ver-
tices corresponding to t in A (the partition repre-
senting S) will be connected to all m? vertices cor-
responding to t in A?. Thus there will be m ? m?
edges, which is exactly the unnormalized product
17In the simple case of CosTFIDF, the condition would be:
(v
i
j
, v
?
i
?
j
, s
j
) ? E if and only if a
i
j
= a
?
i
?
j
. In other words,
all identical features (and only they) are connected.
18
?the first element? means that select(E) may return any
e ? E, provided the same element is always returned for the
same set.
of term frequencies tf(t, S) ? tf(t, S?) ?n ?n?. Thus
summing m ? m? times idf(t)/n ? idf(t)/n? in
sim(G) is equal to tfidf(t, S) ? tfidf(t, S?) (nor-
malization is computed in the same way).
3.3 Meta-Levenshtein: Soft-TFIDF with
Levenshtein alignment
We have shown in part 2.2 that there are some
pitfalls in Soft-TFIDF, especially in the way the
alignment is computed: no symmetry, possible
score overflow. But experiments show that tak-
ing words IDF into account increases performance,
and that Soft-TFIDF, i.e. the possible matching
of words that are not strictly identical, increases
performance (see section 4). That is why improv-
ing this kind of measure is interesting. Follow-
ing the model we proposed above, we propose to
mix the cosine-like similarity used in Soft-TFIDF
with a Levenshtein-like alignment. The following
measure, called Meta-Levenshtein (ML for short),
takes IDFs into account but is not a bag-of-features
metrics.
Let us define align
ML
in the following way: let
G
ML
be defined exactly as the set of graphs G
lev
(see part 3.2.1), except that weights are defined as
in the case of Soft-TFIDF: for any G = (V,E) ?
G
lev
and for any edge (v
i
j
, v
?
i
?
j
, s
j
) ? E, let
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
, and
align
ML
(S, S
?
) = G, where G is such that
sim(G) = max({sim(G
?
) |G
?
? G
ML
}). Finally,
score
ML
(G) = sim(G)/(?S|| ? ?S
?
?).
Compared to Soft-TFIDF, ML solves the prob-
lem of symmetry (ML(S, S?) = ML(S?, S)), and
also the potential overflow, because no feature may
be matched twice (see fig. 2). Of course, the align-
ment is less flexible in ML, since it must satisfy the
sequential order of features. Practically, this mea-
sure may be efficiently implemented in the same
way as Levenshtein similarity, including option-
ally the Damerau extension for transpositions. We
have also tested a simple variant with possible ex-
tended transpositions, i.e. cases like ABC com-
pared to CA, where both C and A are matched.
3.4 Recursive combinations for NE matching
One of the points we want to emphasize through
the generic framework presented above is the mod-
597
Figure 2: Soft-TFIDF vs. ML alignment
With sim(A,D) ? ?, and sim(C,E) ? sim(B,E) ? ?:
A
C
B
A
D
E
F
Soft-TFIDF
A
C
B
A
D
E
F
ML
ularity of similarity measures. Our viewpoint is
that traditional measures may be seen not only in
their original context, but also as modular param-
eterized functions. The first application of such a
definition is already in use in the form of measures
like Monge-Elkan or Soft-TFIDF, which rely on
some sub-measure to compare words inside NEs.
But we will show that modularity is also useful
at a lower level: measures concerning words may
rely on similarity between (for example) n-grams,
and even at this restricted level numerous possible
kinds of similarity may be used.
Moreover, from the viewpoint of applications it
is not very costly to compute similarities between
n-grams and even between words. The number
of n-grams is clearly bounded, and the number of
words is not so high because there are only about 2
words by entity in average, and overall some words
appear very often in entities19.
4 Experiments
4.1 Data
Two corpora were used. Both contain mainly news
and press articles, collected from various interna-
tional sources. The first one, called ?Iran Nu-
clear Threat? (INT in short), is in English and
was extracted from the NTI (Nuclear Threat Ini-
tiative) web site20. It is 236,000 words long. Our
second corpus, called ?French Speaking Medias?
(FSM in short), is 856,000 words long. It was ex-
tracted from a regular crawling of a set of French-
speaking international newspapers web sites dur-
ing a short time-frame (in July 2007). GATE21
was used as the named entities recognizer for INT,
whereas Arisem22 performed the tagging of NEs
19In the corpora we studied, 1172 NE (resp. 2533) contain
1107 distinct words (resp. 2785).
20http://www.nti.org
21http://gate.ac.uk
22http://www.arisem.com
for FSM. Recognition errors23 appear in both cor-
pora, but significantly less in FSM. We restricted
the sets of NEs to those recognized as locations,
organizations and persons, and decided to work
only on entities appearing at least twice. Finally
for INT (resp. FSM) we obtain 1,588 distinct
NE (resp. 3,278) accounting altogether for 33,147
(resp. 23,725) occurrences.
Of course, it would be too costly to manually
label as match (positive) or non-match (negative)
the whole set containing n ? (n ? 1)/2 pairs, for
the observed values of n. The approach consist-
ing in labeling only a randomly chosen subset of
pairs is ineffective, because of the disproportion
between the number of negative and positive pairs
(less than 0.1%). Therefore we tried to find all pos-
itive pairs, assuming the remaining lot are nega-
tive. Practically, the labeling step was based only
on the best pairs as identified by a large set of
measures24. The guidelines we used for labeling
are the following: any incomplete, over-tagged or
simply wrongly recognized NE is discarded. Then
remaining pairs are classified as positive (corefer-
ent), negative (non-coreferent), or ?don?t know?25.
Corpus Discarded Pos. Neg. Don?t know
INT 416 / 1,588 764 2,821 302
FSM 745 / 3,278 741 32,348 419
According to our initial hypotheses, all non-
tagged pairs are considered as negative in the ex-
periments below. ?Don?t know? pairs are ignored.
As a further note, about 20% of the pairs are not
orthographically similar (e.g. acronyms and their
expansion): these pairs are out of reach of our tech-
niques, and would require additional knowledge.
4.2 Observations
4.2.1 Taking IDF into account
To evaluate the contribution of IDF26 in scor-
ing the coreference degree between NE, let us ob-
23Mainly truncated entities, over-tagged entities, and com-
mon nouns beginning with a capital letter.
24This is a potential methodological bias, but we hope to
have kept its effect as low as possible: the measures we used
are quite diverse and do not assign good scores to the same
pairs; therefore, for each measure, we expect that the poten-
tial misses (false negatives) will be matched by some other
measure, thus allowing a fair evaluation of its performance.
A few positive pairs are manually added (mainly acronyms).
25All ambiguous cases, mainly due to some missing preci-
sion (e.g. ?Ministry of Foreign Affairs? and ?Russian Min-
istry of Foreign Affairs?), and more rarely homonymy (e.g.
?Lebedev? and ?[Valery|Oleg] Lebedev?)
26It may be noticed that the Term Frequency in TFIDF is
rarely important, since a given word appear almost always
only once in a NE.
598
serve the differences among best scored pairs for
measures Bag-of-words Cosine and Cosine over
TFIDF weighted vectors. For example, the for-
mer will assign 0.5 to pair ?Prime Minister Tony
Blair?/?Blair? (from corpus INT), whereas the
latter gives 0.61. As expected, IDF weights lighten
the effect of non-informative words and strengthen
important words. In both corpora, The F1-measure
for TFIDF Cosine is about 10 points (in average)
better than for Bag-of-words Cosine (see fig. 3).
4.2.2 Soft-TFIDF problems: normalization,
threshold and sub-measure
As we have explained in section 2.2, the Soft-
TFIDF measure (Cohen et al, 2003) may suffer
from normalization problems. This is probably
the reason why the authors seem to use it parsi-
moniously, i.e. only in the case words are very
close (which is verified using a high threshold
?). Indeed, problems occur when the sub-measure
and/or the threshold are not carefully chosen, caus-
ing performances drop: using Jaro measure with
a very low threshold (0.2 here), performances
are even worst than Bag-of-words cosine (see fig.
3). This is due to the double matching problem:
for example, pair ?Tehran Times (Tehran)?/?Inter
Press Service? (from INT) is scored more than 1.0
because ?Tehran? matches ?Inter? twice: even
with a low score as a coefficient, ?Inter? has a
high IDF compared to ?Press? and ?Service?, so
counting it twice makes normalization wrong.
However, this problem may be solved by choos-
ing a more adequate sub-measure: experiments
show that using the CosTFIDF measure with bi-
grams or trigrams outperforms standard CosT-
FIDF. Of course, there are some positive pairs
that are found ?later? by Soft-TFIDF, since it may
only increase score. But the ?soft? comparison
brings back to the top ranked pairs a lot of positive
ones. In both corpora, the best sub-measure found
is CosTFIDF with trigrams. ?Mohamed ElBa-
radei?/?Director Mohammad ElBaradei? (INT)
or ?Chine?/?China? (FSM) are typical positive
pairs found by this measure but not by standard
CosTFIDF. Here no threshold is needed anymore
because the sub-measure has been chosen with
care, depending on the data, in order to avoid the
normalization problem. This is clearly a drawback
for Soft-TFIDF: it may perform well, but only with
hand-tuning sub-measure and/or threshold.
4.2.3 Beyond Soft-TFIDF: (recursive) ML
In the FSM corpus, replacing Soft-TFIDF with
(simple) Meta-Levenshtein at the word level does
not decrease performance, even though the align-
ment is more constrained in the latter case. Us-
ing the same sub-measure to compare words (tri-
grams CosTFIDF), it does neither increase perfor-
mance. A few positive pairs are missed in the INT
corpus, due to the more flexible word order in En-
glish: ?U.S. State Department?/?US Department
of State? is such an example (12 among 764 are
concerned). This problem is easily solved with the
ML variant with extended transposition (see part
3.3): in both corpora, there are no positive pairs
requiring more than a gap of one word in the align-
ment. Thus this measure is not only performant but
also robust, since it does not need any hand-tuning.
As a second step, we want to improve results
by selecting a more fine-grained sub-measure. We
have tried several ideas, such as using different
kinds of n-grams similarity inside the words sim-
ilarity measure. Firstly, trigrams performed bet-
ter than bigrams or simple characters. Secondly,
the best trigrams similarity method found is actu-
ally very simple: it consists in using CosTFIDF
computed on the trigrams contexts, i.e. the set of
closest27 trigrams of all occurrences of the given
trigram. Unsurprisingly, good scores are generally
obtained for pairs of trigrams that have common
characters. But it seems that this approach also
enhances robustness, because it finds similarities
between ?close characters?: in the French corpus,
one observes quite good scores between trigrams
containing an accentuated version and the non ac-
centuated version of the same character. Further-
more, some character encoding errors are some-
how corrected this way28. This is possibly the rea-
son why the improvement of results is better in
FSM than in INT (see table 1).
Finally, using also ML to compute similarity
between words29 yields the best results. This
means that compared to the simple CosTFIDF sub-
measure, one does not compare bags of trigrams
but ordered sequences of trigrams30.
27We have tried different window sizes for such contexts,
from 2 to 10 trigrams long: performances were approximately
the same. We only consider trigrams found in the entities.
28For example, the ?? in the name ?Lugovo??? appears also in
FSM as i, as y, as a`, and is sometimes deleted.
29i.e. not only between sequences of words: in this case
ML is run between trigrams at the word level, and then an-
other time between words at the NE level.
30It is hard to tell whether it is the sequential alignment or
599
Figure 3: F1-Measures for FSM (percentages)
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
F1
-M
ea
su
re
n best scored pairs (considered as positive)
Bag of words Cosine
Cosine TFIDF (words)
Soft-TFIDF (Jaro)
Soft-TFIDF (TFIDF 3g)
ML (ML/contexts 3g)
Example: for Cosine TFIDF with words, if the threshold is
set in such a way that (only) the 1000 top ranked pairs are
classified as positive, then the F1-measure is around 60%.
Table 1: Best F1-measures (percentages)
INT FSM
Measure F1 P R F1 P R
Cosine 51.6 63.2 43.6 59.5 76.2 48.7
CosTFIDF 62.6 71.7 55.6 69.9 84.2 59.8
Soft TFIDF/3g 68.6 74.2 63.9 73.1 79.8 67.6
ML/ML-context 70.6 72.6 68.7 77.0 82.5 72.2
P/R: Corresponding Precision/Recall.
4.3 Global results
Results are synthesized in table 1, which is based
on the maximum F1-measure for each measure.
One observes that F1-measure is 3 to 6 points bet-
ter for Soft-TFIDF than for standard TF-IDF, and
that our measure still increases F1-measure by 2
(INT) to 4 points (FSM). Results show that its
contribution consists mainly in improving the re-
call, which means that our measure is able to catch
more positive pairs than Soft-TFIDF: for exam-
ple, the pair ?Fatah Al Islam?/ ?Fateh el-Islam?
(FSM) is scored 0.54 by SoftTFIDF and 0.70 by
ML. Our measure remains the best for all values of
n in fig. 3, and results are similar for F0.5-measure
and F2-measure: thus, irrespective of specific ap-
plication needs which may favor precision or re-
call, ML seems preferable.
5 Conclusion
In conclusion, we have proposed a generic model
to show that similarity measures may be combined
in numerous ways. We have tested such a combi-
nation, based on Soft-TFIDF, which performs bet-
the ?right? use of the trigrams sub-measure which is responsi-
ble for the improvement, since the only possible comparison
at this level is Soft-TFIDF.
ter than all existing similarity metrics on two cor-
pora. Our measure is robust, since it does not rely
on any kind of prior knowledge. Thus it may be
easily used, in particular in applications where NE
matching is useful but is not the essential task.
Acknowledgements
This work has been funded by the National Project
Cap Digital - Infom@gic. We thank Lo??s Rigouste
(Pertimm) and Nicolas Dessaigne and Aure?lie Mi-
geotte (Arisem) for providing us with the anno-
tated French corpus.
References
Bilenko, Mikhail, Raymond J. Mooney, William W.
Cohen, Pradeep Ravikumar, and Stephen E. Fien-
berg. 2003. Adaptive name matching in information
integration. IEEE Intelligent Systems, 18(5):16?23.
Christen, Peter. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Department of Computer
Science, The Australian National University, Can-
berra 0200 ACT, Australia, September.
Cohen, William W., Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks. In
Kambhampati, Subbarao and Craig A. Knoblock,
editors, Proceedings of IJCAI-03 Workshop on
Information Integration on the Web (IIWeb-03),
August 9-10, 2003, Acapulco, Mexico, pages 73?78.
Freeman, Andrew, Sherri L. Condon, and Christopher
Ackerman. 2006. Cross linguistic name matching
in English and Arabic. In Moore, Robert C., Jeff A.
Bilmes, Jennifer Chu-Carroll, and Mark Sanderson,
editors, Proc. HLT-NAACL.
Navarro, Gonzalo. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88.
Pouliquen, Bruno, Ralf Steinberger, Camelia Ignat,
Irina Temnikova, Anna Widiger, Wajdi Zaghouani,
and Jan Zizka. 2006. Multilingual person name
recognition and transliteration. CORELA - Cogni-
tion, Representation, Langage.
Prager, John, Sarah Luger, and Jennifer Chu-Carroll.
2007. Type nanotheories: a framework for term
comparison. In Proceedings of CIKM ?07, pages
701?710, New York, NY, USA. ACM.
Wagner, R. and M. Fischer. 1974. The string-to-string
correction problem. JACM, 21(1):168?173.
Winkler, W. E. 1999. The state of record linkage
and current research problems. Technical Report
RR99/04, US Bureau of the Census.
600
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 91?96,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Crotal SRL System : a Generic Tool Based on Tree-structured CRF?
Erwan Moreau
LIPN - CNRS UMR 7030 & Univ. Paris 13
Erwan.Moreau@lipn.univ-paris13.fr
Isabelle Tellier
LIFO - Univ. Orle?ans
Isabelle.Tellier@univ-orleans.fr
Abstract
We present the Crotal system, used in the
CoNLL09 Shared Task. It is based on XCRF,
a highly configurable CRF library which can
take into account hierarchical relations. This
system had never been used in such a context
thus the performance is average, but we are
confident that there is room for progression.
1 Introduction
In this paper we present the Crotal Semantic Role
Labelling (SRL) system, which has been used in
the CoNLL 2009 Shared Task (Hajic? et al, 2009)1.
This system is based on Conditional Random Fields
(CRF) (Lafferty et al, 2001; Sutton and McCallum,
2006): our idea is that we can use the provided
dependency structure as the skeleton of a graphi-
cal model expressing independence asumptions in
a CRF model. CRF are a powerful machine learn-
ing technique that has been successfully applied to
a large number of natural language tasks, mainly
to tag sequences. Compared to classification tech-
niques, CRF can easily take into account dependen-
cies among annotations: it is therefore possible to
represent tree-like structures in the input of the al-
gorithm. Recently, CRF using tree structures were
used in (Finkel et al, 2008) in the case of parsing.
Before participating to this Shared Task, our pro-
totype had only been used to annotate function tags
in a French Treebank: these data were drastically
?This work has been funded by the French National project
ANR-07-MDCO-03 ?CRoTAL?.
1We have participated in the SRL-only category.
smaller, and the task was simpler. Therefore CoNLL
2009 ST is the first time the Crotal System is run
for a quite complex task, with so many data as in-
put, and seven different languages (Catalan, Span-
ish (Taule? et al, 2008), Chinese (Palmer and Xue,
2009), Czech (Hajic? et al, 2006), English (Surdeanu
et al, 2008), German (Burchardt et al, 2006) and
Japanese (Kawahara et al, 2002)). In this context,
the performance we obtained seems reasonable: our
average F1-measure is 66.49% (evaluation dataset).
One of the advantages we want to emphasise
about our system is its genericity: the system does
not need a lot of information as input (we mainly
use pos and deprel columns, and the frame sets have
not been used), and it was able to achieve satisfy-
ing results for the seven different languages using
nearly the same parameters (differences were essen-
tially due to the volume of data, since it was some-
times necessary to reduce the processing time). Of
course, we hope to improve this prototype thanks to
this experience: it may become necessary to lose in
genericity in order to gain in performance, but our
goal is to maintain as much as possible this advan-
tage.
In section 2 we explain the general architecture
for Crotal, then we explain how features are selected
in our system in section 3, and finally we detail and
discuss the results in section 4.
2 The Crotal System Architecture
2.1 General principle
The system we propose is based on the public library
XCRF (Gilleron et al, 2006; Jousse, 2007), which
91
implements CRF model(s) to learn to annotate trees
represented by XML documents. Of course, its per-
formance depends on the way it is used, and espe-
cially on how features are chosen to reliably repre-
sent the labeled data. In order to keep the system
as generic as possible, features are generated auto-
matically and only a few parameters may vary. The
global process has been divided into a sequence of
steps, by creating clusters (one for each predicate,
except the less frequent ones). Indeed, one expects
that the behaviour of the arguments for a given pred-
icate is more regular than for all predicates put to-
gether. Moreover, the size of the training set for
all seven languages allows such a clustering, and it
would even be difficult to process the whole set of
predicates due to time and memory limitations. Thus
the global process is2:
1. Data conversion from CoNLL format to XCRF
format:
? For each sentence containing n predicates,
generate n different XML trees3.
? The tree is simply built following the
dependencies (as provided by the head
column). Therefore the possible non-
projectivity of a tree is ignored, though the
order of words is of course prefered when-
ever possible. An artificial root node is al-
ways added (useful for languages where
several roots are possible).
? In each such XML tree, there is only one
(marked) predicate, and in the annotated
version its arguments (extracted from the
corresponding column) and only them are
reported in the corresponding nodes.
Figure 1 shows the labeled XML tree obtained
for a (part of) example sentence.
2. Clustering by lemma: all dependency trees hav-
ing the same lemma as predicate are put to-
gether if the number of such trees is at least a
2Remark: unless stated otherwise, we will use terms
?lemma?, ?POS tag? ?dependency relation? or ?head? to refer
to the information contained in the corresponding ?P-columns?
for each word. It is worth noticing that performance would be
better using the ?real? columns, but we have followed the in-
structions given by the organizers.
3Thus sentences with no predicate are skipped and several
trees possibly correspond to the same sentence.
given threshold (generally 3, also tested with
2 to 5). There is a special cluster for less fre-
quent lemmas4. Then, for each cluster, in train-
ing mode the process consists of:
(a) Generation of features for the arguments
training step.
(b) The CRF model for arguments is trained
with XCRF.
(c) Generation of features for the senses train-
ing step.
(d) The CRF model for senses5 is trained with
XCRF.
In annotation mode, the CRF model for argu-
ments is first applied to the input tree, then the
CRF model for senses (if possible, an individ-
ual evaluation is also computed).
3. Back conversion from XCRF format to CoNLL
format (in annotation mode).
In the framework of this task, features generation
is crucial for improving performance. That is why
we will mainly focus on that point in the remaining
of this paper.
2.2 The XCRF Library
XCRF (Gilleron et al, 2006; Jousse, 2007) is a pub-
lic library which has been applied successfully to
HTML documents in order to extract information or
translate the tree structure into XML (Jousse, 2007).
More recently we have applied it to annotate func-
tion tags in a French Treebank.
In a CRF model, a feature is a function (usually
providing a boolean result) whose value depends on
the annotations present in a special clique of the
graph, and on the value of the observed data. In
our system, each feature is defined by a pair (C, T ),
where:
? C is the set of annotations present in a given
clique, i.e. a completely connected subgraph
of the graphical structure between annotations.
4This special cluster is used as a default case. In particular,
if an unknown lemma is encoutered during annotation, it will
be annotated using the model learned for this default cluster.
5Steps 2c and 2d are skipped if the lemma has only one pos-
sible sense (or no sense is needed, like in Japanese data and for
some Czech predicates).
92
Several solutions are possible to choose this
graph. In most of our experiments, we have
chosen a graph where only the node-parent
relationship between nodes is taken into ac-
count (denoted FT2), as illustrated by Figure
2. XCRF is also able to deal with simple one-
node cliques (no dependency between annota-
tion, denoted FT1) and node-parent-sibling re-
lationship (denoted FT3).
? T = {t1, . . . , tn} is a (possibly empty) set
of boolean tests on the observation (i.e. not
depending on the annotations). Each ti is an
atomic test6: for example, the test ?pos attribute
for first left sibling is NNS? is satisfied for node
3 in fig. 1. T is the conjunction of all ti.
For example, let us define the following FT2 fea-
ture (C, T ), that would be true for node 4 in fig.
1: C is {apredparent = PRED ? apredcurrent =
C-A1} and T is {poschild1 = VB ? deprelparent =
VC}.
3 Selecting Features
Our goal is somehow to ?learn? features from the
training set, in the sense that we do not explicitly
define them but generate them from the corpus. The
main parameters we use for generating a set of fea-
tures are the following:
? The feature type n, with n ? 3. All FT n?,
with n? ? n, are also considered, because some
function tags possibly appear in FT n and not
(or more rarely) in FT n + 1.
? Various kind of accessible information (decom-
posed through two distinct parameters informa-
tion and neighbourhood):
? Information: form, lemma, POS tags, de-
pendency relation and various secondary
attributes (column features) are available
for all nodes (i.e. word), in every tree ex-
tracted from the corpus.
? Neighbourhood: Given a current node, the
?neighbourhood? defines the set of nodes
6A test is provided to XCRF as an XPath expression, which
will be applied to the current node in the XML tree correspond-
ing to the sentence.
Sentence
2, are
VBP, ROOT
1, Exports
NNS, SBJ
A1
3, thought
VBN, VC
PRED
4, to
TO, OPRD
C-A1
5, have
VB, IM
6, risen
VBN, VC
7, strongly
RB, MNR
8, in
IN, TMP
9, August
NNP, PMOD
[...]
Figure 1: a labeled example for the (part of) sentence
?Exports are thought to have risen strongly in August
[...]?: the nodes are represented with their POS tags, and
in bold face the corresponding annotation associated with
the predicate ?thought? (label PRED was added during
preprocessing, see 3.1)
?
?
A1 PRED
C-A1
?
?
? ?
?
[...]
Figure 2: graph for a FT2-CRF for the annotation of the
sentence of Figure 1 (where ? means ?no annotation?)
93
that will be observed to help deduce its an-
notation: only this node, or also its parent,
possibly its siblings, etc.
? The maximum number of (atomic) tests in the
set T for these nodes: combining several tests
makes features more precise (conjunction), but
also more numerous.
A few other parameters may be added to speed up
learning:
? minimum proportion for an argument label
which is present in the data to be taken into ac-
count,
? minimum proportion for a feature which is
present in the data to be included in the model,
? and maximum number of sentences to process
by XCRF in the training step.
We try to use as less linguistic knowledge as pos-
sible, because we are interested in testing to what
extent the model is able to learn such knowledge by
itself. Moreover, we observe that using too many
features and/or examples as input in XCRF requires
a lot of time and memory (sometimes too much), so
we have to restrict the selection to the most relevant
kind of information in order to get a tractable ma-
chinery. This is why we use only POS tags (pos)
and dependency relations (deprel) (as one can see in
fig. 1). Finally the process of generating features
consists in parsing the training data in the follow-
ing way: for each encoutered clique, all the possible
(combinations of) tests concerning the given neigh-
bourhood are generated, and each of them forms a
feature together with the observed clique.
3.1 Learning Argument Roles
In our system, the arguments and the sense of a pred-
icate are trained (or annotated) one after the other:
the former is always processed before the latter, thus
the dependency holds only in the direction from ar-
guments to sense. Therefore the training of argu-
ments only relies on the observed trees (actually
only the neighbourhood considered and the argu-
ments cliques). In order to help the learner locate
the right arguments, a special label PRED is added
as ?argument? to the node corresponding to the tar-
get predicate: by this way cliques can more easily
take the tree structure into account in the neighbour-
hood of the predicate.
After some tests using the development set as
test set, we observed that the following parameters
were the best suited to build a reliable CRF model
(for the arguments) in a reasonable time (and thus
used them to learn the final models): the neigh-
bourhood consists in the node itself, its parent and
grand-parent, first and second siblings on both sides
and first child; the FT2 model performs quite cor-
rectly (FT3 has been discarded because it would
have taken too much time), and at most two tests
are included in a feature.
3.2 Learning Predicate Senses
The step of predicting senses can use the arguments
that have been predicted in the previous step. In par-
ticular, the list of all arguments that have been found
is added and may be used as a test in any feature.
We did not use at all the frame sets provided with
the data: our system is based only on the sentences.
This choice is mainly guided by our goal to build a
generic system, thus does not need a lot of input in-
formation in various formats. The lemma part of the
predicate is simply copied from the lemma column
(this may cause a few errors due to wrong lemmas,
as observed in the English data).
The fact that sentences have been classified by
lemma makes it convenient to learn/annotate senses:
of course lemmas which can not have more than one
sense are easily processed. In the general case, we
also use XCRF to learn a model to assign senses for
each lemma, using the following parameters: there
is no need to use another model than FT1, since in
each tree there is only one (clearly identified) node
to label; a close neighbourhood (parent, first left and
right siblings and first child) and only two tests are
enough to obtain satisfactory results.
4 Results and Discussion
4.1 General Results
Due to limited time and resources, we had to relax
some time-consuming constraints for some clusters
of sentences (concerning mainly the biggest training
sets, namely Czech and English): in some cases, the
94
threshold for a feature to be selected has been in-
creased, resulting in a probably quite lower perfor-
mance for these models. Ideally we would also have
done more tests with all languages to fine-tune pa-
rameters. Nevertheless, we have obtained quite sat-
isfying results for such a generic approach: the av-
erage F1-measure is 66.49%, ranging from 57.75%
(Japanese) to 72.14% (English). These results show
that the system is generic enough to work quite cor-
rectly with all seven languages7 .
4.2 Internal Evaluation
Here we report detailed results obtained in anno-
tating the development set. Since we process the
task in two distinct steps, we can evaluate both
separately: for the arguments step, the F1-measure
ranges from 56.0% (Czech) to 61.8% (German), ex-
cept for Japanese data where it is only 27%. For the
senses step, the F1-measure is generally better: it
ranges from 61.5% for the Czech case8 to 93.3% for
Chinese.
It is also interesting to observe the difference
between using ?real? indicators (i.e. lemma, pos,
deprel and head columns) versus predicted ones
(i.e. P-columns): for example, with German data
(respectively Catalan data) the F1-measure reaches
73.6% (resp. 70.8%) in the former case, but only
61.8% (resp. 60.6%) in the latter case (for the argu-
ment labeling step only).
4.3 Impact of Parameters
At first we intended to use the most precise CRF
model (namely FT3), but the fact that it generates
many more features (thus taking too much time) to-
gether with the fact that it does not improve perfor-
mance a lot made impossible to use it for the whole
data. More precisely, it was possible but only by set-
ting restrictive values for other parameters (neigh-
bourhood, thresholds), which would have decreased
performance. This is why we had to use FT2 as a
7Actually detailed evaluation shows that the system does not
deal very well with Japanese, since locating arguments is harder
in this language.
8Counting only ?real senses?: it is worth noticing that Czech
data were a bit different from the other languages concerning
senses, since most predicates do not have senses (not counted
here and easy to identify) and the set of possible senses is dif-
ferent for each lemma.
compromise, thus making possible to use better val-
ues for the other parameters. We have also tested us-
ing 3 tests instead of only 2, but it does not improve
performance, or not enough to compensate for the
huge number of generated features, which requires
excessive time and/or memory for XCRF learning
step.
One of the most important parameters is the
neighbourhood, since it specifies the location (and
consequently the amount) of the information taken
into account in the features. We have tried differ-
ent cases for both the argument labeling step and the
sense disambiguation step: in the former case, ob-
serving children nodes is useless, whereas observing
the parent and grand-parent nodes together with two
siblings in both left and right handside improves the
model. On the contrary, in the senses step observing
more than close nodes is useless. These facts are not
surprising, since arguments are generally hierarchi-
cally lower than predicates in the dependency trees.
We have also studied the problem of finding an
optimal threshold for the minimum number of sen-
tences by cluster (all sentences in a given cluster
having the same lemma for predicate): if this thresh-
old is too low some clusters will not contain enough
examples to build a reliable model, and if it is too
high a lot of sentences will fall in the default clus-
ter (for which the model could be less precise). But
surprisingly the results did not show any significant
difference between using a threshold of 2, 3 or 5:
actually individual results differ, but the global per-
formance remains the same.
Finally a word has to be said about ?efficiency pa-
rameters?: the most important one is the minimum
proportion for a generated feature to be included in
the final set of features for the model. Clearly, the
lower this threshold is, the better the performance
is. Nevertheless, in the framework of a limited time
task, it was necessary to set a value of 0.0005% in
most cases, and sometimes a higher value (up to
0.001%) for the big clusters: these values seem low
but prevent including a lot of features (and probably
sometimes useful ones).
5 Problems, Discussion and Future Work
Since there was a time limit and the system was used
for the first time for such a task, we had to face
95
several unexpected problems and solve them quite
rapidly. Therefore one may suppose that our system
could perform better, provided more tests are done to
fine-tune parameters, especially to optimize the bal-
ance between efficiency and performance. Indeed,
there is a balance to find between the amount of in-
formation (number of features and/or examples) and
the time taken by XCRF to process the training step.
Generally speaking, performance increases with the
amount of information, but practically XCRF can
not handle a huge number of features and/or exam-
ples in a reasonable time. This is why selecting the
?right? features as soon as possible is so important.
Among various possible ways to improve the sys-
tem, we should benefit from the fact that CRF do not
need a lot of examples as input to learn quite cor-
rectly. Informally, the XCRF library seems to have
some kind of ?optimal point?: before this point the
model learned could be better, but beyond this point
time and/or memory are excessive. Thus one can
try for example to apply an iterative process using a
sufficiently low number of features at each step, to
select the more useful ones depending on the weight
XCRF assigns to them.
Since the Crotal system obtained reasonable re-
sults in this ?non ideal? context, we are quite confi-
dent in the fact that it can be significantly improved.
The CoNLL 09 Shared Task has been a good op-
portunity to validate our approach with a non trivial
problem. Even if the performance is not excellent,
several important points are satisfying: this experi-
ence shows that the system is able to handle such a
task, and that it is generic enough to deal with very
different languages.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08:HLT, pages 959?967, Columbus, Ohio. Associa-
tion for Computational Linguistics.
Re?mi Gilleron, Florent Jousse, Isabelle Tellier, and Marc
Tommasi. 2006. Conditional random fields for xml
trees. In Proceeding of ECML workshop on Mining
and Learning in Graphs.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague
Dependency Treebank 2.0. Linguistic Data Con-
sortium, Philadelphia, Pennsylvania, USA. URL:
http://ldc.upenn.edu. Cat. No. LDC2006T01, ISBN 1-
58563-370-4.
Florent Jousse. 2007. Transformations d?Arbres XML
avec des Mode`les Probabilistes pour l?Annotation.
Ph.D. thesis, Universite? Charles de Gaulle - Lille 3,
October.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
ICML?01: Proceedings of the 18th International Conf.
on Machine Learning, pages 282?289.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
96
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2205?2216, Dublin, Ireland, August 23-29 2014.
Limitations of MT Quality Estimation Supervised Systems:
The Tails Prediction Problem
Erwan Moreau
CNGL and Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
moreaue@cs.tcd.ie
Carl Vogel
Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
vogel@cs.tcd.ie
Abstract
In this paper we address the question of the reliability of the predictions made by MT Quality
Estimation (QE) systems. In particular, we show that standard supervised QE systems, usually
trained to minimize MAE, make serious mistakes at predicting the quality of the sentences in the
tails of the quality range. We describe the problem and propose several experiments to clarify
their causes and effects. We use the WMT12 and WMT13 QE Shared Task datasets to prove that
our claims hold in general and are not specific to a dataset or a system.
1 Introduction
Machine Translation (MT) Quality Estimation (QE) has become an important subject of study in the past
few years (Callison-Burch et al., 2012; Bojar et al., 2013). This follows directly from the erratic quality
of MT output in general: although MT is now widely used in professional contexts, it is still prone to
many errors; therefore a careful post-editing stage, performed by human experts, is usually needed. In
this context, QE can help carrying out this process more efficiently, and more specifically to help in
the decision process between the automatic and the manual stages: if a reliable indication of quality is
provided for every machine-translated sentence, the human effort can be reduced. For example, a very
bad translation is worthless because the translator usually has to spend more time fixing it than she or he
would have spent translating the sentence from scratch; thus it makes more sense in such cases to either
send the sentence back to an alternative MT system (e.g. trained on a different corpus), or simply leave
it untranslated for the translator. Clearly the advantage of using a QE system depends on the reliability
of its predictions. If it makes too many errors, then it only confuses the translation workflow; in this case
the translators would perform better without it.
The quality of an (automatic) QE system cannot be perfect, but it should be at least controllable. That
is, it should be possible to assess the reliability of the predictions made by a system, for instance by
estimating the level of confidence of the predictions. Hopefully, QE systems will progress towards this
kind of behaviour, but currently the evaluation methods are not entirely satisfactory from this perspective.
In particular, after describing our experimental setting in ?2, we will observe in ?3 that the use of the
Mean Absolute Error
1
(MAE) as a global evaluation measure hides huge discrepancies in the distribution
of errors among the range of scores. More precisely, supervised systems optimized to minimize the MAE
have intrinsic flaws in the way they assess the tails of the quality range, i.e. the ?very good? and the ?very
bad? sentences. In ?4 we propose different ways to evaluate the impact of this problem, and also clarify
what might be an important misunderstanding in what a QE system actually does (?4.2). Finally we
propose in ?5 several experiments: in ?5.1 we show that the problem is not system-specific, and we test
two ways to circumvent it in ?5.2 and ?5.3, but the price to pay in global performance is high.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The MAE is defined as the mean over all instances of the absolute error, where the absolute error is the absolute value of
the difference between the predicted and the actual value of the instance. Thus, the MAE score depends on the range of possible
values (i.e., two datasets using different ranges cannot be compared).
2205
2 Experimental Setup
2.1 Data
In this paper we use the three datasets from the WMT12 and WMT13 QE Shared Task (Callison-Burch
et al., 2012; Bojar et al., 2013) which are intended to predict the quality of individual machine-translated
sentences: the WMT12 task and the WMT13 task 1.1 and 1.3. The last two datasets are renamed wmt13a
and wmt13b in the rest of this paper. These three datasets differ by the way quality is measured:
? wmt12: effort scores, which have been assigned by three professional post-editors according to
predefined guidelines; scores range from 1: ?the MT output is incomprehensible [..]? to 5: ?the
MT output is perfectly clear [..]?. The dataset was cleaned to avoid the cases with a high level of
disagreement, and the scores were post-processed to harmonize the scale between the judges.
? wmt13a: HTER scores, which measure the distance between the MT output and the post-edited
sentence (Snover et al., 2006).
? wmt13b: post-editing time, that is, the time that the post-editor has spent correcting the MT output.
As a consequence, the set of scores have different characteristics: in wmt12, the distribution is highly
discrete due to the integer values assigned by the judges. In wmt13a the distribution is more dense,
whereas in wmt13b some values are spread extremely far from the mean.
2
General statistics for the
datasets are given in table 1. In all datasets the input and MT output sentences are available to the system;
the post-edited version of the sentences is also available, but it cannot be used by the QE systems (the
test set post-edited sentences were provided only after the end of the task). We focus on predicting
an absolute indication of quality rather than only ranking the sentences by quality; this is why we use
the Mean Absolute Error (MAE) as the main evaluation measure rather than Spearman?s correlation or
DeltaAvg (Callison-Burch et al., 2012).
2.2 Supervised QE System
In the observations and experiments described in this paper we use a QE system which follows a standard
supervised learning approach: it was trained on the full training set for every task considered; when the
performance on the training set is observed, it was assessed using 10-fold cross-validation (thus obtaining
a prediction for every sentence in the train set based on a 90% subset). We have used Quest
3
(Shah et
al., 2013), an open-source tool for QE, to compute the 17 ?black box features? which are also used in the
WMT QE ?baseline? system (see below). We have used Weka (Hall et al., 2009) (version 3.6.10), and
after testing several options
4
we found that using the SMOreg algorithm (Smola and Sch?olkopf, 2004;
Shevade et al., 2000) with an RBF kernel
5
was optimal with respect to the performance on the three
datasets.
We did not perform any feature selection or parameter tuning, because our main goal was to build a
generic system. Additionally we favor the ease of reproducibility over optimal performance, which is out
of the scope of this paper. We want our system to be as generic as possible (but still performing decently,
of course), because we need it to be fairly representative of standard, state-of-the-art, supervised learning
QE systems. This is very important, since our observations and experiments are supposed to generalize
to the current most common approaches in QE.
Our task of making the system representative of state-of-the-art QE systems has been greatly facilitated
by the fact that the organizers of the WMT12 and WMT13 QE Shared Task provide for every task the
performance of a so-called ?baseline system?. We can use exactly the same set of features and compare
the results of our system against these obtained by this baseline system, which in turn does not deserve
2
This is why we exclude the most striking outlier from the training set: 1115.906, line 294. The test set is left unchanged.
3
http://staffwww.dcs.shef.ac.uk/people/L.Specia/projects/quest.html ? last verified 05/14.
4
In particular, M5P regression trees generally achieve nearly as good performance as SVM regression. We have also
observed that at least the most important characteristics reported in this paper for an SVM system hold for M5P regression as
well.
5
With the default value C=1 and standardization of the features values.
2206
its name since it has actually always performed well in every task: it ranked 8th out of 20 in the WMT12
official ranking, 12th out of 17 in WMT13a, and 6th out of 14 in WMT13b (MAE ranking). Thus, we
can simply check that our system performs as well as this baseline system to ensure that it is equivalent,
and therefore probably reasonably similar to the other supervised systems submitted to the Shared Tasks
which perform similarly.
6
Table 1 shows that our system performs roughly the same as the baseline
system on the three datasets.
Range Statistics Performance (test set)
Dataset of Quality Train set Test set Our system Baseline system
values direction instances mean std. dev. instances mean std. dev. cor. MAE cor. MAE
wmt12 [1, 5] ? 1832 3.44 0.88 422 3.29 0.98 0.56 0.69 0.58 0.69
wmt13a [0, 1] ? 2254 0.32 0.17 500 0.26 0.19 0.44 0.15 0.46 0.15
wmt13b [0,+ inf[ ? 802 95.6 84.2 284 116.9 108.3 0.70 50.9 0.70 51.9
Table 1: Datasets: statistics and performance. Quality direction: ? means that the quality is better
when the score is higher,? means the opposite; ?cor.? is the Spearman?s correlation.
3 The Tails Prediction Problem
In this section we mostly observe the training set (using cross-validation), in order to dismiss the possi-
bility that the observed phenomenon is caused by the differences in the distributions of scores between
the training set and the test set. Since it is easier for a supervised learning algorithm to annotate some
data from the set it was trained on than from a different dataset, problems which appear with the former
are very likely to appear as well (possibly accentuated) with the latter.
2
3
4
5
1 2 3 4 5gold
predict
(a) wmt12. Spearman cor.: 0.53
0.2
0.3
0.4
0.5
0.00 0.25 0.50 0.75 1.00gold
predict
(b) wmt13a. Spearman cor.: 0.37
0
100
200
0 100 200 300 400 500gold
predict
(c) wmt13b. Spearman cor.: 0.62
Figure 1: Scatter plots showing how predicted scores differ from gold scores (test set). Every point
(X,Y) corresponds to one sentence for which X is the gold score and Y the predicted score. Darker
areas correspond to more dense areas; the vertical and horizontal lines indicate the frontiers of 20%-
quantiles for both variables (for instance, the points which are on the right side of the rightmost vertical
line account for the 20% highest gold scores). Remark: a few outliers are not visible on the wmt13b plot
(their gold scores are higher than 500, and their predicted scores are lower than 250).
Figure 1 shows that the points are very scattered and do not follow the diagonal very closely, but
also that the range of predicted scores is significantly different from the range of gold scores: no sen-
tence is predicted below 2 for wmt12, above 0.55 for wmt13a and above 260 for wmt13b, whereas the
corresponding range of gold scores is much wider. Figure 2, which shows the distribution of gold vs.
predicted scores for the training sets, gives a more precise picture of this difference: in all three datasets,
the predicted scores tend to belong to a smaller set of values centered approximately around the mean.
There are clearly more predicted values than gold values in this area, and this is confirmed by the much
smaller standard deviation for the predicted scores.
It is possible to obtain a clearer picture by ?flattening? the distribution, that is, instead of drawing
histograms in which points with the same value (or a close value) are accumulated, we represent every
6
In section 5.1 we also check more specifically that our observations hold for most of the systems submitted to WMT12.
2207
0
100
200
300
1 2 3 4 5score
count groupgoldpredict
(a) wmt12. ?
G
= 0.88, ?
P
= 0.50
0100
200300
400
0.00 0.25 0.50 0.75 1.00score
count groupgoldpredict
(b) wmt13a. ?
G
= 0.17, ?
P
= 0.07
0
50
100
150
0 200 400 600score
count groupgoldpredict
(c) wmt13b. ?
G
= 84.2, ?
P
= 46.5
Figure 2: Combined distributions of the gold scores and predicted scores on the training set for the
three datasets. ?
G
(resp. ?
P
) is the standard deviation for gold (resp. predicted) scores.
point on the X axis and sort the values on this axis, so that their actual value can be observed on the Y axis,
as shown on figure 3. This figure shows that, in all three cases, the predicted scores are tightly clustered
around the median, which is the point where the two curves cross each other. If the system predicted
scores according to the distribution it observed on the training set, the two curves would be close; instead,
they clearly diverge from each other as the distance to the median increases. This means that the model
tends globally to overestimate the points below the median and, symmetrically, underestimate the points
above the median (though the symmetry is degraded in 3c, since the range is unbound to the right).
In figure 3 the two sets of points are sorted independently: the sentence (x, y) on the curve of gold
scores is different from the one with the same x on the curve of predicted scores. Yet this observation
of ?tightened? predicted scores cannot be fully understood without taking into account the risk of error
in the prediction process, as it was visible on the scatter plots in figure 1. Thus it is also useful to look
at the sorted scores, but with their corresponding predicted score (for the same sentence) plotted on the
same x coordinate; this what is shown on figure 4, for the wmt13a dataset only (because the phenomenon
is the most accentuated in this dataset, and scores conveniently belong to [0, 1]). On figure 4a one can
see that the set of predicted scores are mostly contained in a slightly inclined rectangle; clearly they do
not follow the curve of gold scores, but here one can see why: the fact that there are many points at the
same level on the Y axis along the whole X axis shows that the algorithm cannot make a clear distinction
between the different levels of quality. For example, there are approximately as many scores predicted
around 0.3 which correspond to actually very good (rank near 0) and very bad sentences (rank near 1).
From a different perspective, figure 4b shows very clearly that the farther the gold score of a sentence is
from the mean (0.32), the more likely it is to be predicted with a large error.
2
3
4
5
0.00 0.25 0.50 0.75 1.00rank
score seriesgoldpredict
(a) wmt12
0.00
0.25
0.50
0.75
1.00
0.00 0.25 0.50 0.75 1.00rank
score seriesgoldpredict
(b) wmt13a
0
100
200
300
400
0.00 0.25 0.50 0.75 1.00rank
score seriesgoldpredict
(c) wmt13b
Figure 3: Sorted gold and predicted scores for the three datasets. The two sets of scores are sorted
independently. The X axis is the normalized rank (0 to 1 instead of 1 to the total number of sentences),
so that it is easier to observe the quantiles. Example: for wmt13a, the lowest fourth of gold scores ranges
from 0 to around 0.20, whereas the lowest fourth of predicted scores ranges from 0.125 to around 0.27.
Remark: on the wmt13b plot the scores higher than 400 are not visible (all are gold scores).
2208
0.000.25
0.500.75
1.00
0.00 0.25 0.50 0.75 1.00rank
score groupgoldpredict
(a) Gold scores as reference. Sentences are sorted by their
gold score; the X axis gives their corresponding rank; the
predicted score of a sentence is plotted on the same abscissa,
thus showing both the gold (in red) and predicted score (in
blue) of the sentence on the Y axis. The predicted scores
which appear on the same vertical line correspond to different
sentences which have the same (or very close) gold scores.
0.000.25
0.500.75
1.00
0.00 0.25 0.50 0.75 1.00rank
score groupabsErrgold
(b) Absolute error as reference. Sentences are sorted by
their absolute error; the X axis gives their corresponding
rank; the gold score of a given sentence is plotted on the same
abscissa, thus showing both the error (in red) and gold score
(in blue) of the sentence on the Y axis. The gold scores which
appear on the same vertical line correspond to different sen-
tences which have the same (or a very close) absolute error.
Figure 4: wmt13a, training set: sentences sorted by gold score (left) or absolute error (right).
The issue is constant among the datasets, but with a variable impact. To some extent, it could be
summarized in the following way: it appears that the system does not try to predict the actual quality
of the sentences, but instead applies a simple optimization strategy; since a large majority of sentences
belong to a relatively small range of values in the middle of the full possible range of scores, predicting
any score outside this range is taking a big risk. Consequently it is safer, in order to minimize the error
rate, to ignore (or barely take into account) the rare cases which belong to the tails. Hence the system
ends doing the opposite of what is usually expected from a quality estimation system: the most common
cases are rather accurately recognized, but the most striking anomalies are left undetected or poorly
labelled as such. This behaviour can be explained by the following reasons:
7
? The supervised learning optimization criterion is very often the minimization of the MAE,
8
as in our system. This leads the algorithm to favor the interval of scores where there are many
instances, since their weight is more important in the average.
? The datasets are unbalanced, which is certainly realistic in terms of application, but it also en-
courages the algorithm to assign scores in the interval which would be the ?default class? in a
classification problem; that is, without any clear indication in the features, it is strategically wiser
to bet on the most probable answer.
? The risk is lower with respect to MAE to assign a score in the middle of the range of possible
values rather than at the extremes. For instance if the range is [1, 5] the maximum absolute error at
3 is 2, whereas it is 4 at 1 or 5. However, at least for wmt13a, the data shows that, if this hypothesis
had a real impact, the predicted scores would be closer to 0.5 than to the mean 0.32.
4 Detecting and Evaluating the Tails Quality
4.1 Possible Measures
We propose below different measures intended to evaluate the impact of the tails prediction problem.
Since it can be defined as an increased level of error for sentences which are far from the mean, a simple
first measure is the correlation between the distance from the gold score to the mean and the absolute
error: this value reflects whether the errors are higher in the tails than close to the mean and to what
extent (in other words, it measures how strong the divergence observed on the right part of figure 4b is).
Table 2 shows how high Pearson?s correlation is in our data.
A simple way to measure the performance locally in the tails is to consider the task as a binary classi-
fication problem, as if we were only interested in recognizing whether a sentence belongs to a particular
7
The first two reasons are actually closely related, they only show different aspects of the same problem.
8
Especially in the WMT QE tasks, since this is the main evaluation measure for the scoring task.
2209
wmt12 wmt13a wmt13b
train test train test train test
all > 0 < 0 all > 0 < 0 all > 0 < 0 all > 0 < 0 all > 0 < 0 all > 0 < 0
0.58 0.54 0.64 0.62 0.64 0.65 0.82 0.84 0.78 0.76 0.86 0.76 0.80 0.89 -0.18 0.83 0.91 -0.01
Table 2: Correlation between the absolute error and the distance to the mean of the gold score.
?> 0? (resp. ?< 0?) is the correlation when taking only into account the scores above (resp. below) the
mean; this gives a more precise picture for the top/bottom quality scores. For example, in wmt13b the
top quality (lowest) scores are very well predicted, as opposed to the bottom quality (highest) scores.
subset of scores. For example, the frontier between the classes can be fixed between the 90% lowest
scores (negative) and the 10% highest (positive): it is then possible to observe the last 10% using the
standard evaluation measures: precision (proportion of true positive among the sentences labeled as
positive), recall (proportion of sentences labeled as positive among all positive sentences) and F1-score
(harmonic mean of the precision and recall).
9
The values of these measures are given for three thresh-
olds in table 3. As expected, the recall is extremely low in the tails; it is even 0 in most cases for the
5% threshold, which means that the system does not assign any score in the 5% top/bottom of the range
observed on the training data.
Data+tail 5% 10% 20%
limit P R F1 limit P R F1 limit P R F1
wmt12 B ? 2.0 ? 0.0 ? ? 2.3 0.33 0.01 0.02 ? 2.7 0.73 0.16 0.26
T ? 5.0 ? 0.0 ? ? 4.7 0.50 0.02 0.04 ? 4.2 0.65 0.18 0.28
wmt13a B ? 0.62 ? 0.0 ? ? 0.54 ? 0.0 ? ? 0.47 0.62 0.11 0.19
T ? 0.06 ? 0.0 ? ? 0.11 ? 0.0 ? ? 0.17 0.50 0.01 0.01
wmt13b B ? 272 ? 0.0 ? ? 186 0.76 0.26 0.39 ? 134 0.71 0.43 0.54
T ? 18.2 0.5 0.05 0.09 ? 24.8 0.18 0.06 0.10 ? 35.7 0.52 0.30 0.38
Table 3: Local classification measures (test set). ?T? (resp. ?B?) refers to the top (resp. bottom)
quality tail; P/R/F1 are the standard Precision/Recall/F1-score.
10
Example: 10% of the scores for the
wmt12 training data are higher than 4.7 (top quality tail); among the gold scores in the test set which are
higher than this 4.7 threshold, only 2% are predicted as higher than 4.7 (recall); and among the scores
predicted as higher than 4.7, exactly 50% are actually higher than 4.7 (precision).
Additionally, we have separately proposed a measure which aims to evaluate the ranking error locally
(Moreau and Vogel, 2013). The same idea can be applied to scoring errors: the Local MAE (LMAE)
can be computed on a particular range of scores. The difference with global MAE is that, for a given
sentence, the gold score or the predicted score can belong to the range while the other does not. This is
why there are two versions of this measure: gold-based LMAE and prediction-based LMAE, which, as
their names suggest, take into account only the gold scores (resp. predicted scores) which belong to the
range in the absolute difference | gold? predicted |, as defined in definition 4.1.
Definition 1 (Local MAE (LMAE)). Let S be a set of sentences, and D the interval of possible scores:
9
In the observations which follow we choose to set the limits (5%, etc.) based on the training set even though the test set
is observed. In other words, the absolute score corresponding to the percentage is calculated using the training set gold scores,
which might differ from the value calculated from the test set. The disadvantage is that the number of values in the test set in
the corresponding range does not necessarily correspond to the percentage, but this way the limits do not depend on the test set,
so that values obtained on different test sets would be comparable.
10
We consider theN% limits computed from the range of gold scores, and not from the range of predicted scores: this makes
more sense because otherwise the system is not evaluated against the actual scores in the tails, but since the range of predicted
scores is actually smaller than the range of gold scores, sometimes there are no predicted scores at all in this range of values
(especially for the lowest values of N , e.g. 5%). For example in the wmt12 dataset 5% of the gold scores are below 2, but the
system does not predict any value below 2. In such a case we consider that this is equivalent to a classifier which decides not to
label any instance in a given category. Since there are no instances labelled as positive at all, the precision is undefined, which
makes the F1-score undefined as well. The corresponding cells are marked as ??? in tables 3 and 6.
2210
for every sentence s ? S, predicted(s) ? D, gold(s) ? D. For any subinterval I ? D:
11
LMAE
gold
= mean
( {
?
?
gold(s)? predicted(s)
?
?
?
?
?
s such that gold(s) ? I
} )
LMAE
pred
= mean
( {
?
?
gold(s)? predicted(s)
?
?
?
?
?
s such that predicted(s) ? I
} )
To some extent, the gold-based LMAE (resp. prediction-based) is similar to a recall measure (resp.
precision) because it takes into account the true positive and the false negative (resp. the true positive and
the false positive) with respect to the range. This can be observed in table 4, which gives the values of
these two measures for three thresholds on the three datasets: LMAE
gold
is almost always much higher
than the global MAE, whereas there LMAE
pred
is often close to or lower than the global MAE. This is
because, compared to the gold scores, the top or bottom predicted scores are closer to the centre of the
range. Therefore the sentences taken into account include some actual ?tails sentences? (for which the
absolute error is high), but they can also contain many sentences which actually belong to the area (for
which the absolute error is low).
5% 10% 20%
Data+tail (Global) MAE LMAE
gold
LMAE
pred
LMAE
gold
LMAE
pred
LMAE
gold
LMAE
pred
wmt12 B
0.69
1.37 0.47 1.02 0.57 1.02 0.62
T 1.08 0.68 1.08 0.67 0.89 0.68
wmt13a B
0.15
0.35 0.18 0.28 0.18 0.19 0.17
T 0.28 0.12 0.28 0.13 0.24 0.13
wmt13b B
50.9
264 154 192 129 135 90.3
T 26.1 22.9 24.5 27.4 27.4 25.2
Table 4: Local MAE evaluation (test set). ?T? (resp. ?B?) refers to the top (resp. bottom) quality tail.
Example: for the wmt13b data, among the 10% actual top quality sentences (i.e. the 10% lowest gold
scores), the mean absolute error is 26.1. This is lower than the global MAE (50.9), as opposed to all
the other cases; this confirms that the top quality tail in wmt13b is particularly well predicted (this is
certainly a consequence of the strongly skewed distribution in this dataset).
4.2 The Post-edited Sentences Test
A good way to evaluate the discrepancies in the reliability of the quality scores in the tails is to apply
the QE system to a set of very good or very bad sentences. Thankfully the post-edited versions of the
sentences were provided with the WMT datasets; since by definition their quality is perfect, they make
a perfect case for such a test.
12
In theory, all these sentences should be assigned a score close to top
quality.
13
For every dataset we run the same QE system, i.e., we compute the features for the post-edited
sentences using Quest, then apply the model built with the regular training data to these features. We
tried with both the post-edited version of the training set and test set, when provided.
14
Our original goal was to observe how high the error rate was globally, but it turned out that the pre-
dicted scores follow a distribution which is very similar to the one followed by the MT output (the means
are very close as well, which implies that the MAE is very high). This led us to observe how the MT out-
put scores and the post-edited version scores are correlated. In most cases the two scores are very close,
as shown on figure 5. This is obviously a very serious issue, since it means that, in general, the system is
not able to distinguish between a sentence which needs correction and the same sentence after correction.
11
Remark: if I = D, LMAE
gold
= LMAE
pred
= MAE.
12
Independent assessment of the post-edited sentences is, of course, not guaranteed to yield the judgement that they would
not benefit from further editing, though.
13
That is, 5 for the wmt12 dataset and 0 for the wmt13a dataset (since HTER scores measure the distance against the post-
edited version, and here we compare the post-edited sentence against itself); the wmt13b dataset is based on post-edited time,
so there is no exact value corresponding to perfect sentences but the scores should very low.
14
The post-edited version was not available for the wmt13b test set.
2211
23
45
2 3 4 5MT.output.predictpos
ted.predict
(a) wmt12, train set;
cor.=0.977;
mean diff.=-0.03=-0.03?
34
5
2 3 4 5MT.output.predictpos
ted.predict
(b) wmt12, test set;
cor.=0.986;
mean diff.=-0.02=-0.02?
0.10.20.3
0.40.50.6
0.2 0.3 0.4 0.5 0.6MT.output.predictpos
ted.predict
(c) wmt13a, train set;
cor.=0.962;
mean diff.=0.004=0.02?
0.20.3
0.40.5
0.2 0.3 0.4 0.5MT.output.predictpos
ted.predict
(d) wmt13a, test set;
cor.=0.987;
mean diff.=0.002=0.01?
0100
200300
0 100 200 300MT.output.predictpos
ted.predict
(e) wmt13b, train set;
cor.=0.985;
mean diff.=3.14=0.04?
Figure 5: MT output predicted scores vs. post-edited predicted scores ?mean diff.? is the mean of the
difference between the post-edited score and the MT output score; it is also expressed as a multiple of ?,
where ? is the standard deviation of the MT output gold scores (specific to each particular dataset).
It is however able to see a slight difference at the document level: we have performed a paired Student?s
test for each dataset, which shows that the mean of the scores predicted for the post-edited sentences
is significantly lower in the wmt12 case and higher in the wmt13a and wmt13b cases (as expected by
the definition of scores) than the scores predicted for the MT output sentences. Nevertheless, the mean
difference is extremely low (see figure 5), never higher than 0.04 standard deviations.
Furthermore, there is no visible impact of the quality of the MT output, although one would expect the
correlation to be lower for low quality sentences: by definition, there are more differences between the
MT output and the post-edited version for these sentences, so it should be easier for the system to detect
the different level of quality between the two. In other words, it is quite understandable that the system
does not detect the difference for an MT output of relatively good quality, but the fact the post-edited
version of the really bad translations are also rated as really bad is a major issue. It must be remembered
that we are not refering to a flaw solely in our own system, but nearly across the board in the state of the
art systems.
These observations, which hold for every dataset, show that QE systems do not capture the actual
quality of the sentences: instead, it seems that what they measure is probably the difficulty of machine-
translating a sentence. Indeed, the set of Quest features that we use contains many features which depend
only on the source sentences. Moreover, this conclusion is consistent with the fact that Bic?ici et al. (2013)
obtain very good results on the WMT12 dataset using only the source sentences.
Explaining this observation with precision would require a more detailed analysis which is out of the
scope of this paper. Nevertheless, it is fairly clear that the features which are used fail to capture the
subtlety and/or the diversity of the difference between a faulty sentence and its corrected version; this
might be because a single sentence does not offer enough clues for the system to make such a fine-grained
distinction, in which case it would be necessary to rethink the definition of the QE problem.
In other work, we examine linguistic quality of items in relation to reference corpora (Moreau and
Vogel, 2013; ?). By comparison to the supervised learning studied here, such work is weakly supervised
since there is no use of absolute scores. This yields a version of the QE problem that may be deemed too
relativistic, but does represent an alternative approach. Unfortunately, because of the very difference in
the use of absolute scores, they cannot be directly compared on this. Thus, we focus here on empirical
exploration of the nature of the problem in estimating quality in the case of supervised learning.
5 Experiments
In this section we devise several experiments intended to explore different aspects of the problem in more
detail. In particular, we try to evaluate the impact of the possible causes described in ?3: first we show
in ?5.1 that it affects most QE systems, especially those optimized to minimize MAE. Then in ?5.2 and
?5.3 we confirm that the distribution of the training set is a major cause of the issue by showing that
alternative distributions have different effects.
2212
5.1 Tails Prediction for WMT12 Participating Systems
In order to test if the tails prediction problem is general to most supervised QE systems, we apply the local
performance measures to the scores predicted on the test set by the participating systems in WMT12.
15
Table 5 shows some detailed results for the four best systems at WMT12. It confirms that the predictions
made for the tails are generally significantly worse than they are globally, and especially that the systems
tend to predict very few values at the ends of the range of values: recall in the 10% bottom or top scores
is never higher than 12%.
16
It is also worth noticing that the first system, which performs significantly
better than the others, is the only one which was not optimized to minimize the MAE but to maximize
the DeltaAvg score (Soricut et al., 2012). In particular, this system obtains a recall higher than the others
in most cases (especially in the 5% and 10% tails), which is certainly due to the fact that it assigns
more scores far from the mean (in other words, this system takes more risk). This tends to confirm our
hypothesis that the minimization of the MAE as learning criterion is one of the causes of the problem.
Correlation Bottom Top
System ID Global dist.mean. 5% 10% 20% 5% 10% 20%
MAE vs abs.err. R G-LMAE R G-LMAE R G-LMAE R G-LMAE R G-LMAE R G-LMAE
SDLLW M5PbestDeltaAvg 0.61 0.49 0.05 1.02 0.07 0.76 0.32 0.76 0.02 0.96 0.12 0.99 0.26 0.84
UU best 0.64 0.53 0.0 1.21 0.07 0.91 0.26 0.91 0.0 1.02 0.04 1.01 0.22 0.81
SDLLW SVM 0.64 0.55 0.0 1.33 0.0 0.98 0.17 0.98 0.02 0.89 0.06 0.91 0.32 0.75
UU bltk 0.64 0.58 0.0 1.22 0.06 0.91 0.27 0.91 0.0 1.07 0.02 1.05 0.27 0.83
Table 5: Tails prediction quality for the 4 best systems at WMT12 (test set). The second column
contains the correlation between the distance to the mean and the absolute error; the columns R and
G-LMAE contain respectively the recall and the gold-based local MAE scores (see ?4.1).
5.2 Adding the Post-edited Sentences to the Training Set
In this experiment we use the post-edited sentences again (see ?4.2), but this time adding them to the
training set in order to observe the impact on the test set.
17
These instances are progressively added to
the official training set (in random order). We focus on the top quality tail, since it is the one which is
expected to benefit from adding sentences with top scores to the training set. Figure 6 shows how the
local MAE scores improve as post-edited instances are added. Only the gold-based LMAE scores are
represented, because these provide a recall-like information and the observations show that recall (in the
tails) is the main weakness of QE systems (see ?4.1).
As expected, in all cases adding top quality sentences to the training set makes the system decrease
the error rate in the top quality tail. Of course this local improvement comes at the price of degrading
the global performance, although for the wmt13a dataset (fig. 6b) the global error even improves until
almost half of the sentences have been added. In the case of the wmt13b dataset (fig. 6c), since the QE
system was already very good in predicting the top quality sentences (the LMAE is even better than the
global MAE), the improvement is smaller and proportionally more costly for the global performance.
5.3 Balancing the Training Set
In this final experiment, we resample the training set (with replacement), in order to balance the gold
scores over the full range of values. Since we can only use the discrete gold scores provided with the
original training set, we compute a (random) uniform distribution but select the closest available score
(randomly picking an instance among those with this score). The resulting distribution is not uniform,
and the training set contains many duplicate instances; therefore, the resulting training set is unlikely to
yield very good results in general, but it is no longer subject to the ?statistical attraction? towards the
mean that we have observed.
15
These values were kindly provided by the organizers of the WMT12 QE Shared Task.
16
This is true for all but 3 participating systems, and these exceptions correspond to systems which performed worse globally.
17
We assign perfect scores to all these sentences: 5 for wmt12, 0 for wmt13a; for wmt13b, we use the mean of the time spent
for the sentences in the training set which were left unmodified: there are 23 such sentences, and the mean is 16.19s.
2213
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
0.4
0.8
1.2
0% 25% 50% 75% 100%sentences
LMAE proportionl 0.050.10.2global
(a) wmt12.
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
0.1
0.2
0% 25% 50% 75% 100%sentences
LMAE proportionl 0.050.10.2global
(b) wmt13a.
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
2040
6080
0% 25% 50% 75% 100%sentences
LMAE proportionl 0.050.10.2global
(c) wmt13b.
Figure 6: Improvement of gold-based LMAE as post-edited sentences are added to the train set.
Example: in wmt12, the gold-based LMAE for the top 20% sentences is higher than 0.8 when the system
is trained only on the official train set (0% of the post-edited sentences added), but reaches 0.4 when
about half of the post-edited sentences are added to the training set. However the global MAE (which
takes all the sentences into account) increases from 0.7 (0%) to 0.9 (50% of the post-edited sentences
added): since the system assigns more scores in the top tail, it makes larger errors globally. Remark: the
MAE and LMAE values are measured on the same set of sentences for every percentage on the X axis.
The model obtained from the balanced training set has been applied to the original test set. Table 6
gives the local results observed in the tails: in most cases, the recall increases drastically compared to
using the regular training set, or is at least identical,
18
causing a great increase in the F1-scores as well.
The LMAE scores do not show such an improvement, in fact the mean error is often higher than with the
regular training set. This is due to the fact that the system is forced to assign scores far from the ?easy
cases? around the mean, therefore makes much bigger mistakes than in the previous case. As expected,
the global MAE scores for wmt13a and wmt13b are much higher than the original MAE values (0.27 and
110.7 respectively, i.e. about twice the original values). Interestingly, the MAE stays almost constant
(0.71 instead of 0.69) for wmt12. The correlation between the distance to the mean and the mean absolute
decreases to 0.42, 0.05 and 0.26 for wmt12, wmt13a and wmt13b, respectively.
Classification measures Local MAE measures
Data+tail 5% 10% 20% 5% 10% 20%
limit P R F1 limit P R F1 limit P R F1 gold pred. gold pred. gold pred.
wmt12 B ? 2.0 0.31 0.18 0.23 ? 2.3 0.49 0.20 0.28 ? 2.7 0.66 0.46 0.54 0.94 0.82 0.73 0.75 0.73 0.67
T ? 5.0 ? 0.0 ? ? 4.7 0.50 0.02 0.04 ? 4.2 0.69 0.19 0.29 1.28 0.72 1.26 0.65 1.07 0.71
wmt13a B ? 0.62 0.15 0.70 0.24 ? 0.54 0.17 0.75 0.28 ? 0.47 0.21 0.83 0.33 0.14 0.60 0.13 0.55 0.14 0.49
T ? 0.06 ? 0.0 ? ? 0.11 ? 0.0 ? ? 0.17 0.67 0.02 0.04 0.58 0.11 0.57 0.14 0.44 0.15
wmt13b B ? 272 0.15 0.55 0.23 ? 186 0.27 0.78 0.41 ? 134 0.38 0.91 0.54 156 243 118 235 101 199
T ? 18.2 0.20 0.20 0.20 ? 24.8 0.30 0.19 0.24 ? 35.7 0.55 0.26 0.35 128 61 114 45 110 41
Table 6: Local evaluation of the test set using a balanced training set. Cells in bold show an im-
provement over the corresponding value with the original training set, as given in tables 3 and 4. The
classification limits were computed on the original training set.
6 Conclusion and Future Work
To conclude, we have shown that there are very serious issues with the way supervised QE systems
are built: they tend to be unable to reliably evaluate both the worst and the best quality sentences.
Furthermore, they cannot distinguish between a faulty MT output sentence and its post-edited version.
We have also shown that it is possible to improve the detection of the best/worst sentences by altering
the distribution of the training set; however the question whether this can be achieved while maintaining
a decent level of global performance remains open. But even if the cost in global performance is high,
18
The only exception is the 20% top quality recall of the wmt13b dataset. This is certainly due to the very particular
distribution of scores in this dataset, and to the fact that the top quality tail was already predicted reliably in the regular version.
2214
the techniques that we have tested could be useful in some specific applications of QE (for example, if
the recall in the tails is more important than the precision).
We think that these observations raise questions about the definition of the QE problem. It might
actually be necessary to define different kinds of QE tasks: depending on the targeted application (e.g.
estimating post-editing time, retraining the MT model, discarding the worst sentences, etc.), there could
be a specific setting which is more appropriate in terms of supervised/unsupervised learning, evaluation
measure, precision/recall trade-off, etc. For instance, minimizing the MAE does not seem compatible
with detecting anomalies, but might be relevant for estimating the cost of post-editing. Similarly, under
the hypothesis that the sentence level is not sufficiently rich in information in order to obtain accurate
predictions, an intermediate level of granularity might be considered (e.g. at paragraph level).
Finally, another great challenge with respect to the reliability of QE systems is their consistency when
applied to different test sets, or more generally their dependency on the training set: in the perspective of
applications, it is very important to know what level of confidence can be expected when applying a QE
system or model to a new document.
Acknowledgements
We are grateful to Lucia Specia, Radu Soricut and Christian Buck, the organizers of the WMT 2012 and
2013 Shared Task on Quality Estimation, for releasing all the data related to the competition, including
post-edited sentences, features sets, etc.
This research is supported by Science Foundation Ireland (Grant 12/CE/I2267) as part of the Centre
for Next Generation Localisation (www.cngl.ie) funding at Trinity College, University of Dublin.
The graphics in this paper were created with R (R Core Team, 2012), using the ggplot2 library
(Wickham, 2009).
References
Ergun Bic?ici, Declan Groves, and Josef Genabith. 2013. Predicting sentence translation quality using extrinsic
and language independent features. Machine Translation, 27(3-4):171?192.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Association for Computational Linguistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10?18.
Erwan Moreau and Carl Vogel. 2013. Weakly supervised approaches for quality estimation. Machine Translation,
27(3):pp 257?280, September.
R Core Team, 2012. R: A Language and Environment for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria. ISBN 3-900051-07-0.
Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, and Lucia Specia. 2013. Quest - design, implementation and
extensions of a framework for machine translation quality estimation. Prague Bull. Math. Linguistics, 100:19?
30.
S.K. Shevade, SS Keerthi, C. Bhattacharyya, and K.R.K. Murthy. 2000. Improvements to the SMO algorithm for
SVM regression. Neural Networks, IEEE Transactions on, 11(5):1188?1193.
A.J. Smola and B. Sch?olkopf. 2004. A tutorial on support vector regression. Statistics and computing, 14(3):199?
222.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
2215
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012. The SDL Language Weaver systems in the WMT12
Quality Estimation shared task. In Proceedings of the Seventh Workshop on Statistical Machine Translation,
pages 145?151, Montr?eal, Canada, June. Association for Computational Linguistics.
Hadley Wickham. 2009. ggplot2: elegant graphics for data analysis. Springer New York.
2216
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 257?262,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Naive Bayes classifier for automatic correction of preposition
and determiner errors in ESL text
Gerard Lynch, Erwan Moreau and Carl Vogel
Centre for Next Generation Localisation
Integrated Language Technology Group
School of Computer Science and Statistics
Trinity College Dublin, Ireland
gplynch,moreaue,vogel@scss.tcd.ie
Abstract
This is the report for the CNGL ILT team en-
try to the HOO 2012 shared task. A Naive-
Bayes-based classifier was used in the task
which involved error detection and correction
in ESL exam scripts. The features we use in-
clude n-grams of words and POS tags together
with features based on the external Google N-
Grams corpus. Our system placed 11th out
of 14 teams for the detection and recognition
tasks and 11th out of 13 teams for the correc-
tion task based on F-score for both preposition
and determiner errors.
1 Introduction
The HOO 2012 shared task seeks to apply compu-
tational methods to the correction of certain types
of errors in non-native English texts. The previous
year?s task, (Dale and Kilgarriff, 2011), focused on
a larger scale of errors and a corpus of academic ar-
ticles. This year?s task focuses on six error types in a
corpus of non-native speaker text. The scope of the
errors is as follows:1
Error Code Description Example
RT Replace Preposition When I arrived at London
MT Missing preposition I gave it John
UT Unnecessary preposition I told to John that
RD Replace determiner Have the nice day
MD Missing determiner I have car
UD Unnecessary determiner There was a lot of the traffic
Table 1: Error types for HOO 2012 Shared Task
In Section 2, we give a brief summary of the data
for the shared task and in Section 3 we explain the
1http://correcttext.org/hoo2012/
errortypes.html last verified, May 10, 2012
individual steps in the system. Section 4 details the
different configurations for each of the runs submit-
ted and finally, Section 5 presents the results.
2 Training data
The training data for this shared task has been pro-
vided by Cambridge University Press and consists of
scripts from students sitting the Cambridge ESOL
First Certificate in English (FCE) exams. The top-
ics of the texts are comparable as they have been
drawn from two consecutive exam years. The data is
provided in XML format and contains 1000 original
exam scripts, together with a standoff file containing
edits of the type described in Section 1 above, also
in XML format. These edits consist of offset infor-
mation, edit type information and before and after
text for correction. The results for the shared task
were presented in this format.
The test data consists of 100 exam scripts drawn
from a new corpus of exam scripts.
Some extra metadata is present in the source files,
including information about the student?s mother
tongue and the age-range of the student, however the
mother tongue data is not present in the test set.
3 Approach
The approach we have chosen for this task involves
the use of supervised machine-learning algorithms
in a four-part classification task.
3.1 Overview of the system
The first part of the task involves identification of
edits in the training data, perhaps the most challeng-
257
ing given the large imbalance of edits vs non-edits
in the data.
The next step concerns classification of edits into
the six types described above, and the final task
involves correction of edits, replacing or adding
prepositions and determiners, and possibly in some
cases removal of same.
There is a fourth step involved which reassesses
the classification and correction based on some sim-
ple heuristics, using POS tags of the head word of
each instance. If the headword is not a preposition
and the system has marked a replace preposition er-
ror at that position, this error will be removed from
the system. Likewise when the headword is not a
determiner and a replace determiner error has been
marked. If the replacement suggested is the same
as the original text (in some cases this occurs), the
edit is also removed. Another case for removal in
this fashion includes an error type involving a miss-
ing determiner error where the head word is neither
a noun or an adjective. In some cases the system
reported and corrected an error suggesting the same
text as was originally there, i.e no change. These
cases are also removed from the end result.
3.2 Classification
We utilise the freely available Weka machine learn-
ing toolkit (Hall et al, 2009), and the algorithm used
for classification in each step is Naive Bayes.
3.2.1 Representing the data
We represent each word in the training data as a
vector of features. There are 39 basic features used
in the detection process, and 42 in the classification
and training step. The first 7 features contain in-
formation which is not used for classification but is
used to create the edit structures, such as start offset,
end offset, native language, age group and source
filename and part information. These features in-
clude the current word plus the four preceding and
following words, POS and spell-checked versions of
each, together with bigrams of the two following and
two preceding words with spell-checked and POS
versions for these. Information on speaker age and
native language is also included although native lan-
guage information is not present in the test set.
3.2.2 Additional processing
All tokens have been lower-cased and punctuation
has been removed. POS information for each token
has been added. The open-source POS tagger from
the OpenNLP tools package (OpenNLP, 2012) has
been used to this end. Spell correction facility has
been provided using the basic spellchecker in the
Lucene information retrieval API(Gospodnetic and
Hatcher, 2005) and the top match string as provided
by this spell correcting software is used in addition
to each feature. The basic maximum entropy model
for English is used for the POS tagger.
We had also planned to include features based
on the Google Books n-gram corpus, (Michel et al,
2011) which is freely available on the web, but un-
fortunately did not get to include them in the ver-
sion submitted due to errors which were found in the
scripts for generating the features late in the process.
Nevertheless, we describe these features in Section
3.3 and present some cross-validation results from
the training data for the detection step in Section 5.1.
3.3 Google N-grams Features
3.3.1 Motivation
The Google Books N-Grams2 is a collection of
datasets which consist of all the sequences of words
(n-grams) extracted from millions of books (Michel
et al, 2011). The ?English Million? dataset contains
more more than 500 millions distinct n-grams3, from
size 1 to 5. for every n-gram, its frequency, page
frequency (number of pages containing it) and book
frequency (number of books containing it) are pro-
vided.
In this Shared Task, we aim to use the Google N-
grams as a reference corpus to help detecting the
errors in the input. The intuition is the following:
if an error occurs, comparing the frequency of the
input n-grams against the frequency of other possi-
bilities in the Google N-grams data might provide
useful indication on the location/type of the error.
For example, given the input ?I had to go in a li-
brary?, The Google N-grams contain only 36,716
occurrences of the trigram ?go in a?, but 244,098
occurrences of ?go to a?, which indicates that the
latter is more likely.
2http://books.google.com/ngrams/datasets
3The least frequent n-grams were discarded.
258
However there are several difficulties in using
such a dataset:
? Technical limitations. Extracting information
from the dataset can take a lot of time because
of the size of the data, thus the range of ap-
proaches is restricted by efficiency constraints.
? Quality of the data. The Google N-grams were
extracted automatically using OCR, which
means that the dataset can contain errors or un-
expected data (for example, the English dataset
contains a significant number of non-English
words).
This is why the Google N-grams must be used
cautiously, and only as an indication among others.
3.3.2 Method
Our goal is to add features extracted from the
Google N-grams dataset to the features described
above, and feed the supervised classification process
with these. Before computing the features, a list L
of ?target expressions? is extracted from the train-
ing data, which contains all the words or sequences
of words (determiners and prepositions) which oc-
cur in a correction. Then, given an input sentence
A1 . . . Am and a position n in this sentence, two
types of information are extracted from the Google
data:
? Specific indications of whether an error exists
at this position:
1. No change: the frequency of the input se-
quence An?1An and An?1AnAn+1 ;
2. Unnecessary word(s): the frequency of the
sequence An?1An+1 if A ? L;
3. Missing word(s): the frequency of the se-
quence XAn (resp. An?1XAn for tri-
grams) for any target expression X ? L;
4. Replacement: if A ? L, the frequency of
XAn+1 (resp. An?1XAn+1 for trigrams)
for any target expression X ? L;
? Generic indications taking the context into ac-
count: for length N from 1 to 5 in a window
An?4 . . . An+4, 16 combinations are computed
based only on the fact the n-grams appear in the
Google data; for example, one of these combi-
nations is the normalized sum for the 4 5-grams
in this window of 0 or 1 (the n-gram occurs or
does not).
Additionally, several variants are considered:
? bigrams or trigrams for ?specific? features;
? binary values for ?specific? features: 1 if the
n-gram appears, 0 otherwise;
? keep only the ?generic? features and the first
three features.
4 Run configurations
Ten runs were submitted to the organisers based on
different configurations. Modification of the data
was carried out using both instance reduction and
feature selection techniques. The system facilitated
the use of different training data for each of the three
main classification steps.
4.1 Least frequent words filter
Before classification, the data is preprocessed by re-
placing all the least frequent words with a default
value (actually treated as missing values by the clas-
sifier). This is intended to help the classifier focus
on the most relevant indications and to prevent over-
specification of the classification model.
4.2 Instance reduction filters
4.2.1 POSTrigrams filter
The POS trigrams filter works as follows: during
the training stage, the sequences of POS tags for the
words current-1.current.current+1 are extracted for
each instance, together with its corresponding class.
Every POS trigram is then associated with the fol-
lowing ratio:
Frequency of true instances
Frequency of false instances
Then, when predicting the class, the filter is applied
before running the classifier: the sequences of tri-
grams are extracted for each instance, and are com-
pared against the corresponding ratio observed dur-
ing the training stage; the instance is filtered out if
the ratio is lower than some threshold N%. In Table
259
Run Detection Classification Correction
0 R1 Normal Normal
1 R20 Normal Normal
2 Full F12 Normal
3 R10 Normal Normal
4 R30 Normal Normal
5 F12 F12 Normal
6 R4new Normal Normal
7 R4 + F12 F12 Normal
8 R4 Normal Normal
9 R2 Normal Normal
Table 2: Run configurations
2, the label RN refers to the percentage (N) used as
cut-off in the experiments.
This filter is intended to reduce the impact of the
fact that the classes are strongly unbalanced. It per-
mits discarding a high number of false instances,
while removing only a small number of true in-
stances. However, as a side effect, it can cause the
classifier to miss some clues which were in the dis-
carded instances.
4.2.2 CurrentPlusOrMinusOne filter
The current plusorminus one filter works as fol-
lows: A list of all current.current+1 word bigrams
is made from the error instances in the training data,
along with all current-1.current bigrams. The non-
error instances in the training data are then filtered
based on whether an instance contains an occur-
rence of any current.current+1 or current-1.current
bigram in the list.
4.3 Feature selection filters
4.3.1 F12
During preliminary experiments, selecting a sub-
set of 12 features produced classification accuracy
gains in the detection and classification steps of the
process using ten-fold cross validation on the train-
ing set. These twelve features were: current, cur-
rent+1.current+2, current-1.current-2, currentSC,
currentPOS, current-1, current-2, current+1, cur-
rent+2, current+1SC, and current-1SC. The SC
postfix refers to the spell-corrected token, with POS
referring to the part-of-speech tag. The F12 config-
uration filter removes all other features except these.
5 Results
Table 3 displays the results for both preposition and
determiner errors which were obtained by the sys-
tem on the preliminary test set before teams sub-
mitted their revisions. Table 4 refers to the results
obtained by the system after the revised errors were
removed/edited.
Task Rank Run Precision Recall F-Score
Detection 11 9 5.33 25.61 8.82
Recognition 11 9 4.18 20.09 6.92
Correction 11 9 2.66 12.8 4.41
Table 3: Overall results on original data: TC
Task Rank Run Precision Recall F-Score
Detection 11 8 6.56 26.0 10.48
Recognition 11 8 4.91 19.45 7.84
Correction 11 8 3.09 12.26 4.94
Table 4: Overall results on revised data: TC
5.1 Some detailed results (detection)
The results reported here were obtained on the train-
ing data only, using 5-fold cross-validation, and only
for the detection task. We have studied various set-
tings for the parameters; figure 1 shows a global
overview of the performance depending on several
parameters (we show only a few different values in
order to keep the graph readable).
The results show that the Google features con-
tribute positively to the performance, but only
slightly: the F1 score is 0.6% better on average. This
overview also hides the fact that some combinations
of values work better together; for instance, contrary
to the fact that not filtering the POS trigrams per-
Run3 Recall Precision F
Detection 9.05 7.42 8.15
Correction 4.19 3.44 3.78
Recognition 9.05 7.42 8.15
Run8 Recall Precision F
Detection 22.51 5.44 8.76
Correction 11.25 2.72 4.38
Recognition 22.51 5.44 8.76
Run9 Recall Precision F
Detection 25.61 5.33 8.82
Correction 12.80 2.66 4.41
Recognition 20.09 4.18 6.92
Table 5: Top results on original test data
260
Figure 1: Average F-score depending on several parameters.
10
11
12
13
14
15
16
mea
n of 
f1
20
50
100
500
1000
POS?trigrams.0POS?trigrams.1
POS?trigrams.10
POS?trigrams.3 2?3?binary2?binary3 i r
none
window0
window2
window4
factor(minFreq) filter googleFeatures attributes
forms better on average, the best performances are
obtained when filtering, as shown in figure 2.
Figure 2: F-score (%) w.r.t POS trigrams filter threshold.
Parameters: window 2, Google features with bigrams and
trigrams.
0 2 4 6 8 10
0
5
10
15
20
filter threshold
f1 sc
ore
min. frequency 20min. frequency 50min. frequency 100min. frequency 500min. frequency 1000
? Minimum frequency4 (preprocessing, see 4.1).
4Remark: the values used as ?minimum frequencies? re-
ported in this paper can seem unusually high. This is due to
the fact that, for technical reasons, the thresholds were applied
globally to the data after it had been formatted as individual in-
stances, each instance containing a context window of 9 words.
As a consequence a threshold of N means that a given word
must occur at least N/9 times in the original input data.
As shown in Figure 2, using a high threshold
helps the classifier build a better model.
? POS trigrams filter (see 4.2.1.) Even if not fil-
tering at all performs better on average, the best
cases are obtained with a low threshold. Addi-
tionally, this parameter can be used to balance
between recall and precision (when one wants
to favor one or the other).
? Size of the context window. Results can show
important differences depending on the size
of the window, but no best configuration was
found in general for this parameter.
? Google features (see 3.3.2.) The Google fea-
tures help slightly in general, and are used in
the best cases that we have obtained. How-
ever there is no significantly better approach
between using the original frequencies, simpli-
fying these to binary values, or even not using
the list of target expressions.
6 Conclusions
The task of automated error correction is a difficult
one, with the best-performing systems managing ap-
prox. 40 % F-score for the detection, recognition
and correction (Dale et al, 2012). There are several
areas where our system?s performance might be im-
proved. The spellcheck dictionary which was used
261
was a general one and this resulted in many spelling
corrections which were out of context. A more tai-
lored dictionary employing contextual awareness in-
formation could be beneficial for the preprocessing
step.
Multi-word corrections were not supported by the
system due to how the instances were constructed
and these cases were simply ignored, to the detri-
ment of the results.
In the basic feature set, the majority of features
were based on word unigrams, however more n-
gram features could improve results as these were
found to perform well during classification.
There were many different ways to exploit the
Google N-Grams features and it may be the case
that better combinations of features can be found for
each of the classification steps.
Finally, very little time was spent tuning the
datasets for the classification and correction step as
opposed to the detection phase, this is another part of
the system where fine-tuning parameters could im-
prove performance.
Acknowledgments
This material is based upon works supported by
the Science Foundation Ireland under Grant No.[SFI
07/CE/I 1142.].
References
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, Dublin, Ireland.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition and
Determiner Error Correction Shared Task. In Pro-
ceedings of the Seventh Workshop on Innovative Use
of NLP for Building Educational Applications, Mon-
treal, Canada.
O. Gospodnetic and E. Hatcher. 2005. Lucene. Man-
ning.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K.
Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, et al 2011. Quantitative analysis of
culture using millions of digitized books. Science,
331(6014):176.
OpenNLP. 2012. Website: http://opennlp. apache. org.
262
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 120?126,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Quality Estimation:
an experimental study using unsupervised similarity measures
Erwan Moreau
CNGL and Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
moreaue@cs.tcd.ie
Carl Vogel
Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
vogel@cs.tcd.ie
Abstract
We present the approach we took for our par-
ticipation to the WMT12 Quality Estimation
Shared Task: our main goal is to achieve rea-
sonably good results without appeal to super-
vised learning. We have used various simi-
larity measures and also an external resource
(Google N -grams). Details of results clarify
the interest of such an approach.
1 Introduction
Quality Estimation (or Confidence Estimation)
refers here to the task of evaluating the quality of
the output produced by a Machine Translation (MT)
system. More precisely it consists in evaluating the
quality of every individual sentence, in order (for in-
stance) to decide whether a given sentence can be
published as it is, should be post-edited, or is so bad
that it should be manually re-translated.
To our knowledge, most approaches so far (Spe-
cia et al, 2009; Soricut and Echihabi, 2010; He et
al., 2010; Specia et al, 2011) use several features
combined together using supervised learning in or-
der to predict quality scores. These features be-
long to two categories: black box features which
can be extracted given only the input sentence and
its translated version, and glass box features which
rely on various intermediate steps of the internal MT
engine (thus require access to this internal data).
For the features they studied, Specia et al (2009)
have shown that black box features are informative
enough and glass box features do not significantly
contribute to the accuracy of the predicted scores.
In this study, we use only black box features, and
further, eschew supervised learning except in the
broadest sense. Our method requires some refer-
ence data, all taken to be equally good exemplars
of a positive reference category, against which the
experimental sentences are compared automatically.
This is the extent of broader-sense supervision. The
method does not require a training set of items each
annotated by human experts with quality scores (ex-
cept for the purpose of evaluation of course).
Successful unsupervised learning averts risks of
the alternative: supervised learning necessarily
makes the predicting system dependent on the an-
notated training data, i.e. less generic, and requires
a costly human evalution stage to obtain a reliable
model. Of course, our approach is likely not to per-
form as well as supervised approaches: here the goal
is to find a rather generic robust way to measure
quality, not to achieve the best accuracy. Neverthe-
less, in the context of this Quality Evaluation Shared
task (see (Callison-Burch et al, 2012) for a detailed
description) we have also used supervised learning
as a final stage, in order to submit results which can
be compared to other methods (see ?4).
We investigate the use of various similarity mea-
sures for evaluating the quality of machine translated
sentences. These measures compare the sentence
to be evaluated against a reference text, providing
a similarity score result. The reference data is sup-
posed to represent standard (well-formed) language,
so that the score is expected to reflect how complex
(source side) or how fluent (target side) the given
sentence is.
After presenting the similarity measures in sec-
120
tion 2, we will show in section 3 how they perform
individually on the ranking task; finally we will ex-
plain in section 4 how the results that we submitted
were obtained using supervised learning.
2 Approach
Our method consists in trying to find the best mea-
sure(s) to estimate the quality of machine translated
sentences, i.e. the ones which show the highest cor-
relation with the human annotators scores. The mea-
sures we have tested work always as follows.
Given a sentence to evaluate (source or target),
a score is computed by comparing the sentence
against a reference dataset (usually a big set of sen-
tences). This dataset is assumed to represent stan-
dard and/or well-formed language.1 This score rep-
resents either the quality (similarity measure) or the
faultiness (distance measure) of the sentence. It is
not necessarily normalized, and in general cannot be
interpreted straightforwardly (for example like the 1
to 5 scale used for this Shared Task, in which every
value 1, 2, 3, 4, 5 has a precise meaning). In the con-
text of the Shared task, this means that we focus on
the ?ranking? evaluation measures provided rather
than the ?scoring? measures. These scores are rather
intended to compare sentences relatively to one an-
other: for instance, they can be used to discard the
N% lowest quality sentences from post-editing.
The main interest in such an approach is in
avoiding dependence on costly-to-annotate training
data?correspondingly costly to obtain and which
risk over-tuning the predicting system to the articu-
lated features of the training items. Our method still
depends on the dataset used as reference, but this
kind of dependency is much less constraining, be-
cause the reference dataset can be any text data. To
obtain the best possible results, the reference data
has to be representative enough of what the eval-
uated sentences should be (if they were of perfect
quality), which implies that:
? a high coverage (common words or n-grams) is
preferable; this also means that the size of this
dataset is important;
1We use this definition of ?reference? in this article. Please
notice that this differs from the sense ?human translation of a
source sentence?, which is more common in the MT literature.
? the quality (grammaticality, language register,
etc.) must be very good: errors in the reference
data will infect the predicted scores.
It is rather easy to use different reference datasets
with our approach (as opposed to obtain new human
scores and training a new model on this data), since
nowadays numerous textual resources are available
(at least for the most common languages).
2.1 Similarity measures
All the measures we have used compare (in different
ways) the n-grams of the tested sentence against the
reference data (represented as a big bag of n-grams).
There is a variety of parameters for each measure;
here are the parameters which are common to all:
Length of n-grams: from unigrams to 6-grams;
Punctuation: with or without punctuation marks;
Case sensitivity: binary;
Sentence boundaries: binary signal of whether
special tokens should be added to mark the start
and the end of sentences.2 This permits:
? that there is the same number of n-grams
containing a token w, for every w in the
sentence;
? to match n-grams starting/ending a
sentence only against n-grams which
start/end a sentence.
Most configurations of parameters presented in this
paper are empirical (i.e. only the parameter set-
tings which performed better during our tests were
retained). Below are the main measures explored.3
2.1.1 Okapi BM25 similarity (TF-IDF)
Term Frequency-Inverse Document Frequency
(TF-IDF) is a widely used similarity measure in
Information Retrieval(IR). It has also been shown
to perform significantly better than only term fre-
quency in tasks like matching coreferent named
entities (see e.g. Cohen et al (2003)), which is
2With trigrams, ?Hello World !? (1 trigram) becomes
?# # Hello World ! # #? (5 trigrams).
3One of the measures is not addressed in this paper for IP
reasons (this measure obtained good results but was not best).
121
technically not very different from comparing sen-
tences. The general idea is to compare two docu-
ments4 using their bags of n-grams representations,
but weighting the frequency of every n-gram with
the IDF weight, which represents ?how meaning-
ful? the n-gram is over all documents based on its
inverse frequency (because the n-grams which are
very common are not very meaningful in general).
There are several variants of TF-IDF compari-
son measures. The most recent ?Okapi BM25? ver-
sion was shown to perform better in general than the
original (more basic) definition (Jones et al, 2000).
Moreover, there are different ways to actually com-
bine the vectors together (e.g. L1 or L2 distance). In
these experiments we have only used the Cosine dis-
tance, with Okapi BM25 weights. The weights are
computed as usual (using the number of sentences
containing X for any n-gram X), but are based only
on the reference data.
2.1.2 Multi-level matching
For a given length N, ?simple matching? is de-
fined as follows: for every N -gram in the sentence,
the score is incremented if this N -gram appears at
least once in the reference data. The score is then
relativized to the sentence N -gram length.
?Multi-level matching? (MLM) is similar but with
different lengths of n-grams. For (maximum) length
N , the algorithm is as follows (for every n-gram):
if the n-gram appears in the reference data the score
is incremented; otherwise, for all n-grams of length
N ? 1 in this n-gram, apply recursively the same
method, but apply a penalty factor p (p < 1) to
the result.5 This is intended to overcome the bi-
nary behaviour of the ?simple matching?. This way
short sentences can always be assigned a score, and
more importantly the score is smoothed according
to the similarity of shorter n-grams (which is the be-
haviour one wants to obtain intuitively).
4In this case every sentence is compared against the refer-
ence data; from an IR viewpoint, one can see the reference data
as the request and each sentence as one of the possible docu-
ments.
5This method is equivalent to computing the ?simple match-
ing? for different lengths N of N -grams, and then combine the
scores sN in the following way: if sN < sN?1, then add
p ? (sN?1 ? sN ) to the score, and so on. However this ?ex-
ternal? combination of scores can not take into account some of
the extensions (e.g. weights).
Two main variants have been tested. The first one
consists in using skip-grams.6 Different sizes and
configurations were tested (combining skip-grams
and standard sequential n-grams), but none gave
better results than using only sequential n-grams.
The second variant consists in assigning a more fine-
grained value, based on different parameters, instead
of always assigning 1 to the score when n-gram oc-
curs in the reference data. An optimal solution is not
obvious, so we tried different strategies, as follows.
Firstly, using the global frequency of the ngram
in the reference data: intuitively, this could be in-
terpreted as ?the more an n-gram appears (in the
reference data), the more likely it is well-formed?.
However there are obviously n-grams which appear
a lot more than others (especially for short n-grams).
This is why we also tried using the logarithm of the
frequency, in order to smooth discrepancies.
Secondly, using the inverse frequency: this is
the opposite idea, thinking that the common n-
grams are easy to translate, whereas the rare n-
grams are harder. Consequently, the critical parts
of the sentence are the rare n-grams: assigning them
more weight focuses on these. This works in both
cases (if the n-gram is actually translated correctly
or not), because the weight assigned to the n-gram
is taken into account in the normalization factor.
Finally, using the Inverse Document Frequency
(IDF): this is a similar idea as the previous one, ex-
cept that instead of considering the global frequency
the number of sentences containing the n-gram is
taken into account. In most cases (and in all cases
for long n-grams), this is very similar to the previ-
ous option because the cases where an n-gram (at
least with n > 1) appears several times in the same
sentence are not common.
2.2 Resources used as reference data
The reference data against which the sentences
are compared is crucial to the success of our ap-
proach. As the simplest option, we have used the
Europarl data on which the MT model was trained
(source/target side for source/target sentences). Sep-
arately we tested a very different kind of data,
namely the Google Books N -grams (Michel et al,
6The true-false-true skip-grams in ?There is
no such thing?: There no, is such and no thing.
122
2011): it is no obstacle that the reference sentences
themselves are unavailable, since our measures only
need the set of n-grams and possibly their frequency
(Google Books N -gram data contains both).
3 Individual measures only
In this section we study how our similarity measures
and the baseline features (when used individually)
perform on the ranking task. This evaluation can
only be done by means of DeltaAvg and Spearman
correlation, since the values assigned to sentences
are not comparable to quality scores. We have tested
numerous combinations of parameters, but show be-
low only the best ones (for every case).
3.1 General observations
Method Ref. data DeltaAvg Spearman
MLM,1-4 Google, eng 0.26 0.22
Baseline feature 1 0.29 0.29
Baseline feature 2 0.29 0.29
MLM,1-3,lf Google, spa 0.32 0.28
Okapi,3,b EP, spa 0.33 0.27
Baseline feature 8 0.33 0.32
Okapi,2,b EP, eng 0.34 0.30
Baseline feature 12 0.34 0.32
Baseline feature 5 0.39 0.39
MLM,1-5,b EP, spa 0.39 0.39
MLM,1-5,b EP, eng 0.39 0.40
Baseline feature 4 0.40 0.40
Table 1: Best results by method and by resource on train-
ing data. b = sentence boundaries ; lf = log frequency
(Google) ; EP = Europarl.
Table 1 shows the best results that every method
achieved on the whole training data with different
resources, as well as the results of the best base-
line features.7 Firstly, one can observe that the lan-
guage model probability (baseline features 4 and 5)
performs as good or slightly better than our best
measure. Then the best measure is the one which
combines different lengths of n-grams (multi-level
matching, combining unigrams to 5-grams), fol-
lowed by baseline feature 12 (percentage of bigrams
7 Baseline 1,2: length of the source/target sentence;
Baseline features 4,5: LM probability of source/target sentence;
Baseline feature 8: average number of translations per source
word with threshold 0.01, weighted by inverse frequency;
Baseline feature 12: percentage of bigrams in quartile 4 of fre-
quency of source words in a corpus of the source language.
in quartile 4 of frequency), and then Okapi BM25
applied to bigrams. It is worth noticing that compar-
ing either the source sentence or the target sentence
(against the source/target training data) gives very
similar results. However, using Google Ngrams as
reference data shows a significantly lower correla-
tion. Also using skip-grams or any of our ?fined-
grained? scoring techniques (see ?2.1.2) did not im-
prove the correlation, even if in most cases these
were as good as the standard version.
3.2 Detailed analysis: how measures differ
Even when methods yield strongly correlated re-
sults, differences can be significant. For example,
the correlation between the rankings obtained with
the two best methods (baseline 4 and MLM Eng.) is
0.53. The methods do not make the same errors.8 A
method may tend to make a lot of small errors, or on
the contrary, very few but big errors.
0 20 40 60 80 100
0
20
40
60
80
% sentences within error range
relat
ive r
ank 
erro
r (%)
Baseline feature 4MLM EP SpaMLM Google EngBaseline ranking
Figure 1: Percentage of best segments within an error
range. For every measure, the X axis represents the sen-
tences sorted by the difference between the predicted rank
and the actual rank (?rank error?), in such a way that for
any (relative) number of sentences x, the y value repre-
sents the maximum (relative) rank error for all prior sen-
tences: for instance, 80% of the ranks predicted by these
three measures are at most 40% from the actual rank.
Let R and R? be the actual and predicted ranks9
of sentence, respectively. Compute the difference
8This motivates use of supervised learning (but see ?1).
9It is worth noticing that ties are taken into account here: two
123
D = |R?R?|; then relativize to the total number of
sentences (the upper bound for D): D? = D/N .
D? is the relative rank error. On ascending sort
by D?, the predicted ranks for the first sentences
are closest to their actual rank. Taking the relative
rank error D?j for the sentence at position Mj , one
knows that all ?lower? sentences (?Mi, Mi ? Mj)
are more accurately assigned (D?i ? D
?
j). Thus, if
the position is also relativized to the total number
sentences: M ?k = Mk/N , M
?
k is the proportion of
sentences for which the predicted rank is at worst
D?k% from the real rank. Figure 1 shows the percent-
age of sentences withing a rank error range for three
good methods:10 the error distributions are surpris-
ingly similar. A baseline ranking is also represented,
which shows the same if all sentences are assigned
the same rank (i.e. all sentences are considered of
equal quality)11.
We have also studied effects of some parameters:
? Taking punctuation into account helps a little;
? Ignoring case gives slightly better results;
? Sentences boundaries significantly improve the
performance;
? Most of the refinements of the local score (fre-
quency, IDF, etc.) do not perform better than
the basic binary approach.
4 Individual measures as features
In this section we explain how we obtained the sub-
mitted results using supervised learning.
4.1 Approach
We have tested a wide range of regression algo-
rithms in order to predict the scores, using the
Weka12 toolkit (Hall et al, 2009). All tests were
sentences which are assigned the same score are given the same
rank. The ranking sum is preserved by assigning the average
rank; for instance if s1 > s2 = s3 > s4 the corresponding
ranks are 1, 2.5, 2.5, 4).
10Some are not shown, because the curves were too close.
11Remark: the plateaus are due to the ties in the actual ranks:
there is one plateau for each score level. This is not visible on
the predicted rankings because it is less likely that an impor-
tant number of sentences have both the same actual rank and
the same predicted rank (whereas they all have the same ?pre-
dicted? rank in the baseline ranking, by definition).
12www.cs.waikato.ac.nz/ml/weka ? l.v., 04/2012.
done using the whole training data in a 10 folds
cross-validation setting. The main methods were:
? Linear regression
? Pace regression (Wang and Witten, 2002)
? SVM for regression (Shevade et al, 2000)
(SMOreg in Weka)
? Decision Trees for regression (Quinlan, 1992)
(M5P in Weka)
We have tested several combinations of features
among the features provided as baseline and our
measures. The measures were primarily selected
on their individual performance (worst measures
were discarded). However we also had to take the
time constraint into account, because some measures
require a fair amount of computing power and/or
memory and some were not finished early enough.
Finally we have also tested several attributes selec-
tion methods before applying the learning method,
but they did not achieve a better performance.
4.2 Results
Table 2 shows the best results among the config-
urations we have tested (expressed using the offi-
cial evaluation measures, see (Callison-Burch et al,
2012) for details). These results were obtained using
the default Weka parameters.In this table, the differ-
ent features sets are abbreviated as follows:
? B: Baseline (17 features);
? M1: All measures scores (45 features);
? M2: Only scores obtained using the provided
resources (33 features);
? L: Lengths (of source and target sentence, 2
features).
For every method, the best results were obtained
using all possible features (baseline and our mea-
sures). The following results can also be observed:
? our measures increase the performance over
use of baseline features only (B+M1 vs. B);
? using an external resource (here Google n-
grams) with some of our measures increases the
performance (B+M1 vs. B+M2);
124
Features Method DeltaAvg Spearman MAE RMSE
B SVM 0.398 0.445 0.616 0.761
B Pace Reg. 0.399 0.458 0.615 0.757
L + M1 SVM 0.401 0.439 0.615 0.764
L + M1 Lin. Reg 0.408 0.441 0.610 0.757
B Lin. Reg. 0.408 0.461 0.614 0.754
L + M1 M5P 0.409 0.441 0.610 0.757
B + M2 SVM 0.409 0.447 0.605 0.753
B + M2 Pace Reg. 0.417 0.466 0.603 0.744
B + M2 M5P 0.419 0.472 0.601 0.746
L + M1 Pace Reg. 0.426 0.454 0.603 0.751
B + M2 Lin. Reg. 0.428 0.481 0.598 0.740
B M5P 0.434 0.487 0.586 0.729
B + M1 SVM 0.444 0.489 0.585 0.734
B + M1 Pace Reg. 0.453 0.505 0.584 0.724
B + M1 Lin. Reg. 0.456 0.507 0.583 0.724
B + M1 M5P 0.457 0.508 0.583 0.724
Table 2: Best results on 10-folds cross-validation on the
training data (sorted by DeltaAvg score).
? the baseline features contribute positively to the
performance (B+M1 vs. L+M1);
? The M5P (Decision trees) method works best
in almost all cases (3 out of 4).
Based on these training results, the two systems
that we used to submit the test data scores were:
? TCD-M5P-resources-only, where scores were
predicted from a model trained using M5P on
the whole training data, taking only the base-
line features (B) into account;
? TCD-M5P-all, where scores were predicted
from a model trained using M5P on the whole
training data, using all features (B+M1).
The TCD-M5P-resources-only submission
ranked 5th (among 17) in the ranking task, and
5th among 19 (tied with two other systems) in
the scoring task (Callison-Burch et al, 2012).
Unfortunately the TCD-M5P-all submission con-
tained an error.13 Below are the official results
for TCD-M5P-resources-only and the corrected
results for TCD-M5P-all :
13In four cases in which Google n-grams formed the refer-
ence data, the scores were computed using the wrong language
(Spanish instead of English) as the reference. Since this error
occured only for the test data (not the training data used to com-
pute the model), it made the predictions totally meaningless.
Submission DeltaAvg Spearman MAE RMSE
resources-only 0.56 0.58 0.68 0.82
all 0.54 0.54 0.70 0.84
Contrary to previous observations using the train-
ing data, these results show a better performance
without our measures. We think that this is mainly
due to the high variability of the results depending
on the data, and that the first experiments are more
significant because cross-validation was used.
5 Conclusion
In conclusion, we have shown that the robust ap-
proach that we have presented can achieve good re-
sults: the best DeltaAvg score reaches 0.40 on the
training data, when the best supervised approach is
at 0.45. We think that this robust approach com-
plements the more fine-grained approach with su-
pervised learning: the former is useful in the cases
where the cost to use the latter is prohibitive.
Additionally, it is interesting to see that using ex-
ternal data (here the Google N -grams) improves the
performance (when using supervised learning). As
future work, we plan to investigate this question
more precisely: when does the external data help?
What are the differences between using the training
data (used to produce the MT engine) and another
dataset? How to select such an external data in order
to maximize the performance? In our unsupervised
framework, is it possible to combine the score ob-
tained with the external data with the score obtained
from the training data? Similarly, can we combine
scores obtained by comparing the source side and
the target side?
Acknowledgments
This research is supported by Science Foundation
Ireland (Grant 07/CE/I1142) as part of the Centre for
Next Generation Localisation (www.cngl.ie) fund-
ing at Trinity College, University of Dublin.
We thank the organizers who have accepted to apply
a bug-fix (wrong numbering of the sentences) in the
official results, and for organizing the Shared task.
References
[Callison-Burch et al2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
125
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
[Cohen et al2003] W.W. Cohen, P. Ravikumar, and S.E.
Fienberg. 2003. A comparison of string distance met-
rics for name-matching tasks. In Proceedings of the
IJCAI-2003 Workshop on Information Integration on
the Web (IIWeb-03), pages 73?78.
[Hall et al2009] M. Hall, E. Frank, G. Holmes,
B. Pfahringer, P. Reutemann, and I.H. Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?18.
[He et al2010] Y. He, Y. Ma, J. van Genabith, and
A. Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630. Association for Computational
Linguistics.
[Jones et al2000] Karen Sparck Jones, Steve Walker, and
Stephen E. Robertson. 2000. A probabilistic model
of information retrieval: development and comparative
experiments - parts 1 and 2. Inf. Process. Manage.,
36(6):779?840.
[Michel et al2011] J.B. Michel, Y.K. Shen, A.P. Aiden,
A. Veres, M.K. Gray, J.P. Pickett, D. Hoiberg,
D. Clancy, P. Norvig, J. Orwant, et al 2011. Quan-
titative analysis of culture using millions of digitized
books. science, 331(6014):176.
[Quinlan1992] J.R. Quinlan. 1992. Learning with con-
tinuous classes. In Proceedings of the 5th Australian
joint Conference on Artificial Intelligence, pages 343?
348. Singapore.
[Shevade et al2000] S.K. Shevade, SS Keerthi, C. Bhat-
tacharyya, and K.R.K. Murthy. 2000. Improvements
to the smo algorithm for svm regression. Neural Net-
works, IEEE Transactions on, 11(5):1188?1193.
[Soricut and Echihabi2010] R. Soricut and A. Echihabi.
2010. Trustrank: Inducing trust in automatic trans-
lations via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 612?621. Association for Computational
Linguistics.
[Specia et al2009] Lucia Specia, Marco Turchi, Nicola
Cancedda, Marc Dymetman, and Nello Cristianini.
2009. Estimating the sentence-level quality of ma-
chine translation systems. In Proceedings of the 13th
Conference of the European Association for Machine
Translation, pages 28?35.
[Specia et al2011] L. Specia, N. Hajlaoui, C. Hallett, and
W. Aziz. 2011. Predicting machine translation ade-
quacy. In Machine Translation Summit XIII, Xiamen,
China.
[Wang and Witten2002] Y. Wang and I.H. Witten. 2002.
Modeling for optimal probability prediction. In Pro-
ceedings of the Nineteenth International Conference
on Machine Learning, pages 650?657. Morgan Kauf-
mann Publishers Inc.
126
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 429?434,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
An approach using style classification features for Quality Estimation
Erwan Moreau
CNGL and Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
moreaue@cs.tcd.ie
Raphael Rubino
NCLT
Dublin City University
Dublin 9, Ireland
rrubino@computing.dcu.ie
Abstract
In this paper we describe our participation
to the WMT13 Shared Task on Quality Es-
timation. The main originality of our ap-
proach is to include features originally de-
signed to classify text according to some
author?s style. This implies the use of ref-
erence categories, which are meant to rep-
resent the quality of the MT output.
Preamble
This paper describes the approach followed in the
two systems that we submitted to subtask 1.3 of
the WMT13 Shared Task on Quality Estimation,
identified as TCD-DCU-CNGL 1-3 SVM1 and
TCD-DCU-CNGL 1-3 SVM2. This approach
was also used by the first author in his submissions
to subtask 1.1, identified as TCD-CNGL OPEN
and TCD-CNGL RESTRICTED1. In the remain-
ing of this paper we focus on subtask 1.3, but there
is very little difference in the application of the ap-
proach to task 1.1.
1 Introduction
Quality Estimation (QE) aims to provide a quality
indicator for machine translated sentences. There
are many cases where such an indicator would be
useful in a translation process: to compare differ-
ent Machine Translation (MT) models on a given
set of sentences, to tune automatically the param-
eters of a MT model, to select the bad sentences
for human translation or post-editing, to select the
good sentences for immediate publication and try
to apply automatic post-editing to the others, or
simply to provide users who are not fluent in the
source language information about the fluency of
1The second author?s submission to subtask 1.1 is inde-
pendent from this approach and is described in a different
paper in this volume.
the translated text they are reading. As long as ma-
chine translated text cannot be of reasonably con-
sistent quality, QE is helpful in indicating linguis-
tic quality variability.2
After focusing on automatic prediction of ad-
hoc quality scores (as estimated by professional
annotators) in the previous edition (Callison-
Burch et al, 2012), the WMT Shared Task on
Quality Estimation 2013 proposes several variants
of the task. We participated in task 1.1 which aims
to predict HTER scores (edit distance between the
MT output and its manually post-edited version),
and in task 1.3 which aims to predict the expected
time needed to post-edit the MT output.
The originality of our participation lies in the
fact that we intended to test ?style classification?
features for the task of QE: the idea is to select a
set of n-grams which are particularly representa-
tive of a given level of quality. In practice we use
only two levels which simply represent low and
high quality. We explore various ways to build
these two reference categories and to select the n-
grams, as described in ?2. The goal was to see
if such features can contribute to the task of pre-
dicting quality of MT. As explained in ?3, how-
ever, various constraints forced us to somehow cut
corners in some parts of the features selection and
training process; therefore we think that the mod-
est results presented and discussed in ?4 might not
necessarily reflect the real contribution of these
features.
2 Features
2.1 Classical features
We extract a set of features inspired by the ones
provided by the shared task organisers in their 17
baseline feature set. Using the corpora provided
for the task, we extract for each source and target
2We focus on translation fluency rather than target lan-
guage faithfulness to sources.
429
segments pair:
? 24 surface features, such as the segment
length, the number of punctuation marks and
uppercased letters, words with mixed case,
etc.
? 30 language Model (LM) features, n-gram
log-probability and perplexity (with and
without start and end of sentence tags) with
n ? [1; 5].
? 30 backward LM features, n-gram log-
probability and perplexity (with and without
start and end of sentence tags) with n ?
[1; 5].
? 44 n-gram frequency features, with n ?
[1; 5], extracted from frequency quartiles.
? 24 word-alignment features according to the
alignment probability thresholds: 0.01, 0.1,
0.25, 0.5, 0.75 and 1.0, with or without words
frequency weighting.
For all these features, except the ones with binary
values, we compute the ratio between the source
and target feature values and add them to our fea-
ture set, which contains 223 classical features.
2.2 Style classification features
We call the features described below ?style
classification? features because they have been
used recently in the context of author identifica-
tion/profiling (Moreau and Vogel, 2013a; Moreau
and Vogel, 2013b) (quite sucessfully in some
cases). The idea consists in representing the n-
grams which are very specific to a given ?cate-
gory?, a category being a level of quality in the
context of QE, and more precisely we use only the
?good? and ?bad? categories here.
Thus this approach requires the following pa-
rameters:
? At least two datasets used as reference for the
categories;
? Various n-grams patterns, from which com-
parisons based on frequency can be done;
? One or several methods to compare a sen-
tence to a category.
2.2.1 Reference categories
As reference categories we use both the training
datasets provided for task 1.1 and 1.3: both are
used in each task, that is, categories are extracted
from subtasks 1.1 dataset and 1.3 dataset and used
in task 1.1 and 1.3 as well. However we use only
half of the sentences of task 1.1 in 1.1 and sim-
ilarly in 1.3, in order to keep the other half for
the classical training process. This is necessary to
avoid using (even indirectly) a sentence as both a
fixed parameter from which features are extracted
(the category data) and an actual instance on which
features are computed. In other words this simply
follows the principle of keeping the training and
test data independent, but in this case there are two
stages of training (comparing sentences to a refer-
ence category is also a supervised process).
The two datasets are used in three different
ways, leading to three distinct pairs of categories
?good/bad?:3
? The sentences for which the quality is below
the median form the ?bad? category, the one
above form the ?good? category;
? The sentences for which the quality is below
the first quartile form the ?bad? category, the
one above the third quartile form the ?good?
category;
? The complete set of MT output sentences
form the ?bad? category, their manually
post-edited counterpart form the ?good? cat-
egory.
We use these three different ways to build cate-
gories because there is no way to determine a pri-
ori the optimal choice. For instance, on the one
hand the opposite quartiles probably provide more
discriminative power than the medians, but on the
other hand the latter contains more data and there-
fore possibly more useful cases.4 In the last ver-
sion the idea is to consider that, in average, the
machine translated sentences are of poor quality
compared to the manually post-edited sentences;
in this case the categories contain more data, but it
might be a problem that (1) some of the machine-
translated sentences are actually good and (2) the
3Below we call ?quality? the value given by the HTER
score (1.1) or post-editing time (1.3), the level of quality be-
ing of course conversely proportional to these values.
4The datasets are not very big: only 803 sentences in task
1.3 and 2,254 sentences in task 1.1 (and we can only use half
of these for categories, as explained above).
430
right translation of some difficult phrases in the
post-edited sentences might never be found in MT
output. We think that the availability of differ-
ent categories built in various ways is potentially a
good thing, because it lets the learning algorithm
decide which features (based on a particular cate-
gory) are useful and which are not, thus tuning the
model automatically while possibly using several
possibilities together, rather than relying on some
predefined categories.
It is important to notice that the correspondence
between an MT output and its post-edited version
is not used5: in all categories the sentences are
only considered as an unordered set. For instance
it would be possible to use a third-party corpus as
well (provided it shares at least a common domain
with the data).
We use only the target language (Spanish) of the
translation and not the source language in order
not to generate too many categories, and because
it has been shown that there is a high correlation
between the complexity of the source sentence and
the fluency of the translation (Moreau and Vogel,
2012). However it is possible to do so for the cat-
egories based on quantiles.
2.2.2 n-grams patterns, thresholds and
distance measures
We use a large set of 30 n-grams patterns based on
tokens and POS tags. POS tagging has been per-
formed with TreeTagger (Schmid, 1995). Various
combinations of n-grams are considered, includ-
ing standard sequential n-grams, skip-grams, and
combinations of tokens and POS tags.
Since the goal is to compare a sentence to a
category, we consider the frequency in terms of
number of sentences in which the n-gram appears,
rather than the global frequency or the local fre-
quency by sentence.6
Different frequency thresholds are considered,
from 1 to 25. Additionally we can also filter out
n-grams for which the relative frequency is too
5in the categories used as reference data; but it is used in
the final features during the (supervised) training stage (see
?3).
6The frequency by sentence is actually also taken into ac-
count in the following way: instead of considering only the
n-gram, we consider a pair (n-gram, local frequency) as an
observation. This way if a particular frequency is observed
more often in a given category, it can be interpreted as a clue
in favor of this category. However in most cases (long n-
grams sequences) the frequency by sentence is almost always
one, sometimes two. Thus this is only marginally a relevant
criterion to categorize a sentence.
similar between the ?good? and ?bad? categories.
For instance it is possible to keep only the n-grams
for which 80% of the occurrencies belong to the
?bad? category, thus making it a strong marker
for low quality. Once again different thresholds
are considered, in order to tradeoff between the
amount of cases and their discriminative power.
We use only three simple distance/similarity
measures when comparing a sentence to a cate-
gory:
? Binary match: for each n-gram in the sen-
tence, count 1 if it belongs to the category, 0
otherwise, then divide by the number of n-
grams in the sentence;
? Weighted match: same as above but sum the
proportion of occurrences belonging to the
category instead of 1 (this way an n-gram
which is more discriminative is given more
weight);
? Cosine similarity.
Finally for every tuple formed by the combina-
tion of
? a category,
? a quality level (?good/bad?),
? an n-gram pattern,
? a frequency threshold,
? a threshold for the proportion of the occur-
rences in the given category,
? and a distance measure
a feature is created. For every sentence the value
of the feature is the score computed using the pa-
rameters defined in the tuple. From our set of
parameters we obtain approximately 35,000 fea-
tures.7 It is worth noticing that these features
are not meant to represent the sentence entirely,
but rather particularly noticeable parts (in terms of
quality) of the sentence.
7The number of features depends on the data in the cate-
gory, because if no n-gram at all in the category satisfies the
conditions given by the parameters (which can happen with
very high thresholds), then the feature does not exist.
431
2.3 Features specific to the dataset
In task 1.3 we are provided with a translator id
and a document id for each sentence. The distribu-
tion of the time spent to post-edit the sentence de-
pending on these parameters shows some signifi-
cant differences among translators and documents.
This is why we add several features intended to ac-
count for these parameters: the id itself, the mean
and the median for both the translator and the doc-
ument.
3 Design and training process
The main difficulty with so many features (around
35,000) is of course to select a subset of reason-
able size, in order to train a model which is not
overfitted. This requires an efficient optimization
method, since it is clearly impossible to explore
the search space exhaustively in this case.
Initially it was planned to use an ad-hoc genetic
algorithm to select an optimal subset of features.
But unfortunately the system designed in this goal
did not work as well as expected8, this is why we
had to switch to a different strategy: the two fi-
nal sets of features were obtained through several
stages of selection, mixing several different kinds
of correlation-based features selection methods.
The different steps described below were car-
ried out using the Weka Machine Learning toolkit9
(Hall et al, 2009). Since we have used half of the
training data as a reference corpus for some of the
categories (see ?2), we use the other half as train-
ing instances in the selection and learning process,
with 10 folds cross-validation for the latter.
3.1 Iterative selection of features
Because of the failure of the initial strategy, in or-
der to meet the time constraints of the Shared Task
we had to favor speed over performance in the pro-
cess of selecting features and training a model.
This probably had a negative impact on the final
results, as discussed in section ?4.
In particular the amount of features was too
big to be processed in the remaining time by a
subset selection method. This is why the fea-
tures were first ranked individually using the Re-
lief attribute estimation method (Robnik-Sikonja
8At the time of writing it is still unclear if this was due to
a design flaw or a bug in the implementation.
9Weka 3.6.9, http://www.cs.waikato.ac.nz/
ml/weka.
and Kononenko, 1997). Only the 20,00010 top fea-
tures were extracted from this ranking and used
further in the selection process.
From this initial subset of features, the follow-
ing heuristic search algorithms combined with a
correlation-based method11 to evaluate subsets of
features (Hall, 1998) are applied iteratively to a
given input set of features:
? Best-first search (forward, backward, bi-
directional);
? Hill-climbing search (forward and back-
ward);
? Genetic search with Bayes Networks.
Each of these algorithms was used with differ-
ent predefined parameters in order to trade off be-
tween time and performance. This selection pro-
cess is iterated as long as the number of features
left is (approximately) higher than 200.
3.2 Training the models
When less than 200 features are obtained, the it-
erative selection process is still applied but a 10
folds cross-validated evaluation is also performed
with the following regression algorithms:
? Support Vector Machines (SVM) (Smola and
Scho?lkopf, 2004; Shevade et al, 2000);
? Decision trees (Quinlan, 1992; Wang and
Witten, 1996);
? Pace regression (Wang and Witten, 2002).
These learning algorithms are also run with
several possible sets of parameters. Eventually
the submitted models are chosen among those
for which the set of features can not be reduced
anymore without decreasing seriously the perfor-
mance. Most of the best models were obtained
with SVM, although the decision trees regression
algorithm performed almost as well. It was not
possible to decrease the number of features below
60 for task 1.3 (80 for task 1.1) without causing a
loss in performance.
10For subtask 1.3. Only the 8,000 top features for subtask
1.1.
11Weka class
weka.attributeSelection.CfsSubsetEval.
432
4 Results and discussion
The systems are evaluated based on the Mean Av-
erage Error, and every team was allowed to submit
two systems. Our systems ranked 10th and 11th
among 14 for task 1.1, and 13th and 15th among
17 for task 1.1.
4.1 Possible causes of loss in performance
We plan to investigate why our approach does not
perform as well as others, and in particular to
study more exhaustively the different possibilities
in the features selection process.12 It is indeed
very probable that the method can perform better
with an appropriate selection of features and opti-
mization of the parameters, in particular:
? The final number of features is too large,
which can cause overfitting. Most QE system
do not need so many features (only 15 for the
best system in the WMT12 Shared Task on
QE (Soricut et al, 2012)).
? We had to perform a first selection to discard
some of the initial features based on their in-
dividual contribution. This is likely to be a
flaw, since some features can be very useful
in conjuction with other even if poorly infor-
mative by themselves.
? We also probably made a mistake in apply-
ing the selection process to the whole set of
features, including both classical features and
style classification features: it might be rel-
evant to run two independent selection pro-
cesses at first and then gather the resulting
features together only for a more fine-grained
final selection. Indeed, the final models that
we submitted include very few classical fea-
tures; we believe that this might have made
these models less reliable, since our initial
assumption was rather that the style classifi-
cation features would act as secondary clues
in a model primarily relying on the classical
features.
4.2 Selected features
The following observations can be made on the fi-
nal models obtained for task 1.3, keeping in mind
that the models might not be optimal for the rea-
sons explained above:
12Unfortunately the results of this study are not ready yet
at the time of writing.
? Only 5% of the selected features are classical
features;
? The amount of data used in the category
seems to play an important role: most fea-
tures correspond to categories built from the
1.1 dataset (which is bigger), and the pro-
portions between the different kinds of cate-
gories are: 13% for first quartile vs. fourth
quartile (smallest dataset), 25% for below
median vs. above median, and 61% for
MT output vs. postedited sentence (largest
dataset);
? It seems more interesting to identify the low
quality n-grams (i.e. errors) rather than the
high quality ones: 76% of the selected fea-
tures represent the ?bad? category;
? 81% of the selected features represent an
n-grams containing at least one POS tag,
whereas only 40% contain a token;
? Most features correspond to selecting n-
grams which are very predictive of the
?good/bad? category (high difference of the
relative proportion between the two cate-
gories), although a significant number of less
predictive n-grams are also selected;
? The cosine distance is selected about three
times more often than the two other distance
methods.
5 Conclusion and future work
In conclusion, the approach performed decently on
the Shared Task test data, but was outperformed
by most other participants systems. Thus cur-
rently it is not proved that style classification fea-
tures help assessing the quality of MT. However
the approach, and especially the contribution of
these features, have yet to be evaluated in a less
constrained environment in order to give a well-
argued answer to this question.
Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of the
Centre for Next Generation Localisation (www.
cngl.ie) funding at Trinity College, University
of Dublin.
433
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
M. A. Hall. 1998. Correlation-based Feature Subset
Selection for Machine Learning. Ph.D. thesis, Uni-
versity of Waikato, Hamilton, New Zealand.
Erwan Moreau and Carl Vogel. 2012. Quality esti-
mation: an experimental study using unsupervised
similarity measures. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
120?126, Montre?al, Canada, June. Association for
Computational Linguistics.
Erwan Moreau and Carl Vogel. 2013a. Participation
to the pan author identification task. In to appear in
the proceeding of CLEF 2013.
Erwan Moreau and Carl Vogel. 2013b. Participation
to the pan author profiling task. In to appear in the
proceeding of CLEF 2013.
J.R. Quinlan. 1992. Learning with continuous classes.
In Proceedings of the 5th Australian joint Confer-
ence on Artificial Intelligence, pages 343?348. Sin-
gapore.
Marko Robnik-Sikonja and Igor Kononenko. 1997.
An adaptation of relief for attribute estimation in
regression. In Douglas H. Fisher, editor, Four-
teenth International Conference on Machine Learn-
ing, pages 296?304. Morgan Kaufmann.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
S.K. Shevade, SS Keerthi, C. Bhattacharyya, and
K.R.K. Murthy. 2000. Improvements to the SMO
algorithm for SVM regression. Neural Networks,
IEEE Transactions on, 11(5):1188?1193.
A.J. Smola and B. Scho?lkopf. 2004. A tutorial on
support vector regression. Statistics and computing,
14(3):199?222.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver systems in the WMT12
Quality Estimation shared task. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 145?151, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Y. Wang and I.H. Witten. 1996. Induction of model
trees for predicting continuous classes.
Y. Wang and I.H. Witten. 2002. Modeling for optimal
probability prediction. In Proceedings of the Nine-
teenth International Conference on Machine Learn-
ing, pages 650?657. Morgan Kaufmann Publishers
Inc.
434
