Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 360?368, Prague, June 2007. c?2007 Association for Computational Linguistics
Syntactic Re-Alignment Models for Machine Translation
Jonathan May
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
jonmay@isi.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
We present a method for improving word
alignment for statistical syntax-based ma-
chine translation that employs a syntacti-
cally informed alignment model closer to
the translation model than commonly-used
word alignment models. This leads to ex-
traction of more useful linguistic patterns
and improved BLEU scores on translation
experiments in Chinese and Arabic.
1 Methods of statistical MT
Roughly speaking, there are two paths commonly
taken in statistical machine translation (Figure 1).
The idealistic path uses an unsupervised learning
algorithm such as EM (Demptser et al, 1977)
to learn parameters for some proposed translation
model from a bitext training corpus, and then di-
rectly translates using the weighted model. Some
examples of the idealistic approach are the direct
IBM word model (Berger et al, 1994; Germann
et al, 2001), the phrase-based approach of Marcu
and Wong (2002), and the syntax approaches of Wu
(1996) and Yamada and Knight (2001). Idealistic
approaches are conceptually simple and thus easy to
relate to observed phenomena. However, as more
parameters are added to the model the idealistic ap-
proach has not scaled well, for it is increasingly dif-
ficult to incorporate large amounts of training data
efficiently over an increasingly large search space.
Additionally, the EM procedure has a tendency to
overfit its training data when the input units have
varying explanatory powers, such as variable-size
phrases or variable-height trees.
The realistic path also learns a model of transla-
tion, but uses that model only to obtain Viterbi word-
for-word alignments for the training corpus. The
bitext and corresponding alignments are then used
as input to a pattern extraction algorithm, which
yields a set of patterns or rules for a second trans-
lation model (which often has a wider parameter
space than that used to obtain the word-for-word
alignments). Weights for the second model are then
set, typically by counting and smoothing, and this
weighted model is used for translation. Realistic ap-
proaches scale to large data sets and have yielded
better BLEU performance than their idealistic coun-
terparts, but there is a disconnect between the first
model (hereafter, the alignment model) and the sec-
ond (the translation model). Examples of realistic
systems are the phrase-based ATS system of Och
and Ney (2004), the phrasal-syntax hybrid system
Hiero (Chiang, 2005), and the GHKM syntax sys-
tem (Galley et al, 2004; Galley et al, 2006). For
an alignment model, most of these use the Aachen
HMM approach (Vogel et al, 1996), the implemen-
tation of IBM Model 4 in GIZA++ (Och and Ney,
2000) or, more recently, the semi-supervised EMD
algorithm (Fraser and Marcu, 2006).
The two-model approach of the realistic path has
undeniable empirical advantages and scales to large
data sets, but new research tends to focus on devel-
opment of higher order translation models that are
informed only by low-order alignments. We would
like to add the analytic power gained from mod-
ern translation models to the underlying alignment
model without sacrificing the efficiency and empiri-
cal gains of the two-model approach. By adding the
360
u n s u p e r v i s e d
l
e a r n i n g
t
a r g e
t
s e n
t
e n c e s
s o u r c e
s e n
t
e n c e s
u n w e i g h
t
e d
m o d e
l
w e i g h
t
e d
m o d e
l
p a
t t
e r n s
(
u n w e i g h
t
e d
m o d e
l )
c o u n
t
i n g
a n d
s m o o
t
h i n g
w e i g h
t
e d
m o d e
l
d e c o d e r
s o u r c e
s e n
t
e n c e s
t
a r g e
t
s e n
t
e n c e s
p a
t t
e r n
e x
t
r a c
t
i o n
t
a r g e
t
s e n
t
e n c e s
s o u r c e
s e n
t
e n c e s
V
i
t
e r b i
a
l
i g n m e n
t
s
I d e a l i s t i c
S y
s t e m
R e a l i s t i c
S y
s t e m
d e c o d e r
s o u r c e
s e n
t
e n c e s
t
a r g e
t
s e n
t
e n c e s
Figure 1: General approach to idealistic and realistic statistical MT systems
syntactic information used in the translation model
to our alignment model we may improve alignment
quality such that rule quality and, in turn, system
quality are improved. In the remainder of this work
we show how a touch of idealism can improve an
existing realistic syntax-based translation system.
2 Multi-level syntactic rules for syntax MT
Galley et al (2004) and Galley et al (2006) de-
scribe a syntactic translation model that relates En-
glish trees to foreign strings. The model describes
joint production of a (tree, string) pair via a non-
deterministic selection of weighted rules. Each rule
has an English tree fragment with variables and a
corresponding foreign string fragment with the same
variables. A series of rules forms an explanation (or
derivation) of the complete pair.
As an example, consider the parsed English and
corresponding Chinese at the top of Figure 2. The
three columns underneath the example are different
rule sequences that can explain this pair; there are
many other possibilities. Note how rules specify ro-
tation (e.g. R10, R5), direct translation (R12, R8),
insertion and deletion (R11, R1), and tree traversal
(R7, R15). Note too that the rules explain variable-
size fragments (e.g. R6 vs. R14) and thus the possi-
ble derivation trees of rules that explain a sentence
pair have varying sizes. The smallest such deriva-
tion tree has a single large rule (which does not ap-
pear in Figure 2; we leave the description of such
a rule as an exercise for the reader). A string-to-
tree decoder constructs a derivation forest of deriva-
tion trees where the right sides of the rules in a tree,
taken together, explain a candidate source sentence.
It then outputs the English tree corresponding to the
highest-scoring derivation in the forest.
3 Introducing syntax into the alignment
model
We now lay the ground for a syntactically motivated
alignment model. We begin by reviewing an align-
ment model commonly seen in realistic MT systems
and compare it to a syntactically-aware alignment
model.
3.1 The traditional IBM alignment model
IBM Model 4 (Brown et al, 1993) learns a set of 4
probability tables to compute p(f |e) given a foreign
sentence f and its target translation e via the follow-
ing (greatly simplified) generative story:
361
NP-C
NPB
NPB
NNP
taiwan
POS
?s
NN
surplus
PP
IN
in
NP-C
NPB
NN
trade
PP
IN
between
NP-C
NPB
DT
the
CD
two
NNS
shores
? l ? ? ? ? 4 ? ~ ?
TAIWAN IN TWO-SHORES TRADE MIDDLE SURPLUS
R1: NP-C
NPB
x0:NPB x1:NN
x2:PP
? x0 x2 ? x1 R10: NP-C
NPB
x0:NPB x1:NN
x2:PP
? x0 x2 x1 R10: NP-C
NPB
x0:NPB x1:NN
x2:PP
? x0 x2 x1
R2: NPB
NNP
taiwan
POS
?s
? ? l R11: NPB
x0:NNP POS
?s
? x0 R17: NPB
NNP
taiwan
x0:POS
? x0
R12: NNP
taiwan
? ? l R18: POS
?s
? ? l
R3: PP
x0:IN x1:NP-C
? x0 x1 R13: PP
IN
in
x0:NP-C
? ? x0 ? R19: PP
IN
in
x0:NP-C
? x0
R4: IN
in
? ?
R5: NP-C
x0:NPB x1:PP
? x1 x0 R5: NP-C
x0:NPB x1:PP
? x1 x0 R20: NP-C
x0:NPB PP
x1:IN x2:NP-C
? x2 x0 x1
R6: PP
IN
between
NP-C
NPB
DT
the
CD
two
NNS
shores
? ? ? R14: PP
IN
between
x0:NP-C
? x0 R21: IN
between
? ?
R15: NP-C
x0:NPB
? x0 R15: NP-C
x0:NPB
? x0
R16: NPB
DT
the
CD
two
NNS
shores
? ? ? R22: NPB
x0:DT CD
two
x1:NNS
? x0 x1
R23: NNS
shores
? ? ? R24: DT
the
? ?
R7: NPB
x0:NN
? x0 R7: NPB
x0:NN
? x0 R7: NPB
x0:NN
? x0
R8: NN
trade
? ? 4 R9: NN
surplus
? ~ ? R8: NN
trade
? ? 4 R9: NN
surplus
? ~ ? R8: NN
trade
? ? 4 R9: NN
surplus
? ~ ?
Figure 2: A (English tree, Chinese string) pair and three different sets of multilevel tree-to-string rules that
can explain it; the first set is obtained from bootstrap alignments, the second from this paper?s re-alignment
procedure, and the third is a viable, if poor quality, alternative that is not learned.
362
S-C
NP-C
NPB
NNP
guangxi
POS
?s
VP
VBG
opening
PRT
RP
up
PP
TO
to
NP-C
NPB
DT
the
JJ
outside
NN
world
 ? ? i  8
GUANGXI OUTSIDE-WORLD OPENING-UP
R24: S-C
NP-C
NPB
x0:NNP POS
?s
VP
VBG
opening
PRT
RP
up
PP
TO
to
NP-C
NPB
DT
the
JJ
outside
NN
world
? x0 ? i  8 R25: NNP
guangxi
?  ?
R26: S-C
x0:NP-C x1:VP
? x0 x1 R15: NP-C
x0:NPB
? x0 R11: NPB
x0:NNP POS
?s
? x0 R27: VP
VBG
opening
PRT
RP
up
x0:PP
? x0  8
R28: PP
TO
to
x0:NP-C
? x0 R15: NP-C
x0:NPB
? x0 R29: NPB
DT
the
JJ
outside
NN
world
? ? i R25: NNP
guangxi
?  ?
Figure 3: The impact of a bad alignment on rule extraction. Including the alignment link indicated by the
dotted line in the example leads to the rule set in the second row. The re-alignment procedure described in
Section 3.2 learns to prefer the rule set at bottom, which omits the bad link.
1. A fertility y for each word ei in e is chosen
with probability pfert(y|ei).
2. A null word is inserted next to each
fertility-expanded word with probability
pnull.
3. Each token ei in the fertility-expanded
word and null string is translated into
some foreign word fi in f with probability
ptrans(fi|ei).
4. The position of each foreign word
fi that was translated from ei is
changed by ? (which may be posi-
tive, negative, or zero) with probability
pdistortion(?|A(ei),B(fi)), where A and
B are functions over the source and target
vocabularies, respectively.
Brown et al (1993) describes an EM algorithm
for estimating values for the four tables in the gener-
ative story. However, searching the space of all pos-
sible alignments is intractable for EM, so in practice
the procedure is bootstrapped by models with nar-
rower search space such as IBM Model 1 (Brown et
al., 1993) or Aachen HMM (Vogel et al, 1996).
363
3.2 A syntax re-alignment model
Now let us contrast this commonly used model for
obtaining alignments with a syntactically motivated
alternative. We recall the rules described in Section
2. Our model learns a single probability table to
compute p(etree, f) given a foreign sentence f and
a parsed target translation etree. In the following
generative story we assume a starting variable with
syntactic type v.
1. Choose a rule r to replace v, with proba-
bility prule(r|v).
2. For each variable with syntactic type vi in
the partially completed (tree, string) pair,
continue to choose rules ri with probabil-
ity prule(ri|vi) to replace these variables
until there are no variables remaining.
In Section 5.1 we discuss an EM learning proce-
dure for estimating these rule probabilities.
As in the IBM approach, we must miti-
gate intractability by limiting the parameter space
searched, which is potentially much wider than in
the word-to-word case. We would like to supply to
EM all possible rules that explain the training data,
but this implies a rule relating each possible tree
fragment to each possible string fragment, which is
infeasible. We follow the approach of bootstrapping
from a model with a narrower parameter space as is
done in, e.g. Och and Ney (2000) and Fraser and
Marcu (2006).
To reduce the model space we employ the rule ac-
quisition technique of Galley et al (2004), which
obtains rules given a (tree, string) pair as well as
an initial alignment between them. We are agnos-
tic about the source of this bootstrap alignment and
in Section 5 present results based on several differ-
ent bootstrap alignment qualities. We require an ini-
tial set of alignments, which we obtain from a word-
for-word alignment procedure such as GIZA++ or
EMD. Thus, we are not aligning input data, but
rather re-aligning it with a syntax model.
4 The appeal of a syntax alignment model
Consider the example of Figure 2 again. The left-
most derivation is obtained from the bootstrap align-
ment set. This derivation is reasonable but there are
some poorly motivated rules, from a linguistic stand-
point. The Chinese word ? ? roughly means ?the
SENTENCE PAIRS
DESCRIPTION CHINESE ARABIC
TUNE NIST 2002 short 925 696
TEST NIST 2003 919 663
Table 1: Tuning and testing data sets for the MT
system described in Section 5.2.
two shores? in this context, but the rule R6 learned
from the alignment incorrectly includes ?between?.
However, other sentences in the training corpus have
the correct alignment, which yields rule R16. Mean-
while, rules R13 and R14, learned from yet other
sentences in the training corpus, handle the ? ... ?
structure (which roughly translates to ?in between?),
thus allowing the middle derivation.
EM distributes rule probabilities in such a way as
to maximize the probability of the training corpus.
It thus prefers to use one rule many times instead
of several different rules for the same situation over
several sentences, if possible. R6 is a possible rule
in 46 of the 329,031 sentence pairs in the training
corpus, while R16 is a possible rule in 100 sentence
pairs. Well-formed rules are more usable than ill-
formed rules and the partial alignments behind these
rules, generally also well-formed, become favored
as well. The top row of Figure 3 contains an exam-
ple of an alignment learned by the bootstrap align-
ment model that includes an incorrect link. Rule
R24, which is extracted from this alignment, is a
poor rule. A set of commonly seen rules learned
from other training sentences provide a more likely
explanation of the data, and the consequent align-
ment omits the spurious link.
5 Experiments
In this section, we describe the implementation of
our semi-idealistic model and our means of evaluat-
ing the resulting re-alignments in an MT task.
5.1 The re-alignment setup
We begin with a training corpus of Chinese-English
and Arabic-English bitexts, the English side parsed
by a reimplementation of the standard Collins model
(Bikel, 2004). In order to acquire a syntactic rule set,
we also need a bootstrap alignment of each training
sentence. We use an implementation of the GHKM
364
BOOTSTRAP GIZA CORPUS RE-ALIGNMENT EXPERIMENT
ENGLISH WORDS CHINESE WORDS TYPE RULES TUNE TEST
9,864,294 7,520,779
baseline 19,138,252 39.08 37.77
initial 18,698,549 39.49 38.39
adjusted 26,053,341 39.76 38.69
Table 2: A comparison of Chinese BLEU performance between the GIZA baseline (no re-alignment), re-
alignment as proposed in Section 3.2, and re-alignment as modified in Section 5.4
algorithm (Galley et al, 2004) to obtain a rule set for
each bootstrap alignment.
Now we need an EM algorithm for learn-
ing the parameters of the rule set that maximize
?
corpus
p(tree, string). Such an algorithm is pre-
sented by Graehl and Knight (2004). The algorithm
consists of two components: DERIV, which is a pro-
cedure for constructing a packed forest of derivation
trees of rules that explain a (tree, string) bitext cor-
pus given that corpus and a rule set, and TRAIN,
which is an iterative parameter-setting procedure.
We initially attempted to use the top-down DE-
RIV algorithm of Graehl and Knight (2004), but as
the constraints of the derivation forests are largely
lexical, too much time was spent on exploring dead-
ends. Instead we build derivation forests using the
following sequence of operations:
1. Binarize rules using the synchronous bina-
rization algorithm for tree-to-string trans-
ducers described in Zhang et al (2006).
2. Construct a parse chart with a CKY parser
simultaneously constrained on the foreign
string and English tree, similar to the
bilingual parsing of Wu (1997) 1.
3. Recover all reachable edges by traversing
the chart, starting from the topmost entry.
Since the chart is constructed bottom-up, leaf lex-
ical constraints are encountered immediately, result-
ing in a narrower search space and faster running
time than the top-down DERIV algorithm for this
application. Derivation forest construction takes
around 400 hours of cumulative machine time (4-
processor machines) for Chinese. The actual run-
ning of EM iterations (which directly implements
the TRAIN algorithm of Graehl and Knight (2004))
1In the cases where a rule is not synchronous-binarizable
standard left-right binarization is performed and proper permu-
tation of the disjoint English tree spans must be verified when
building the part of the chart that uses this rule.
takes about 10 minutes, after which the Viterbi
derivation trees are directly recoverable. The Viterbi
derivation tree tells us which English words produce
which Chinese words, so we can extract a word-
to-word alignment from it. We summarize the ap-
proach described in this paper as:
1. Obtain bootstrap alignments for a training
corpus using GIZA++.
2. Extract rules from the corpus and align-
ments using GHKM, noting the partial
alignment that is used to extract each rule.
3. Construct derivation forests for each (tree,
string) pair, ignoring the alignments, and
run EM to obtain Viterbi derivation trees,
then use the annotated partial alignments
to obtain Viterbi alignments.
4. Use the new alignments as input to the MT
system described below.
5.2 The MT system setup
A truly idealistic MT system would directly apply
the rule weight parameters learned via EM to a ma-
chine translation task. As mentioned in Section 1,
we maintain the two-model, or realistic approach.
Below we briefly describe the translation model, fo-
cusing on comparison with the previously described
alignment model. Galley et al (2006) provides a
more complete description of the translation model
and DeNeefe et al (2007) provides a more complete
description of the end-to-end translation pipeline.
Although in principle the re-alignment model and
translation model learn parameter weights over the
same rule space, in practice we limit the rules used
for re-alignment to the set of smallest rules that ex-
plain the training corpus and are consistent with the
bootstrap alignments. This is a compromise made
to reduce the search space for EM. The translation
model learns multiple derivations of rules consistent
with the re-alignments for each sentence, and learns
365
(a) Chinese re-alignment corpus has 9,864,294 English and 7,520,779 Chinese words
BOOTSTRAP GIZA CORPUS RE-ALIGNMENT EXPERIMENT
ENGLISH WORDS CHINESE WORDS TYPE RULES TUNE TEST
9,864,294 7,520,779 baseline 19,138,252 39.08 37.77
re-alignment 26,053,341 39.76 38.69
221,835,870 203,181,379 baseline 23,386,535 39.51 38.93
re-alignment 33,374,646 40.17 39.96
(b) Arabic re-alignment corpus has 4,067,454 English and 3,147,420 Arabic words
BOOTSTRAP GIZA CORPUS RE-ALIGNMENT EXPERIMENT
ENGLISH WORDS ARABIC WORDS TYPE RULES TUNE TEST
4,067,454 3,147,420 baseline 2,333,839 47.92 47.33
re-alignment 2,474,737 47.87 47.89
168,255,347 147,165,003 baseline 3,245,499 49.72 49.60
re-alignment 3,600,915 49.73 49.99
Table 3: Machine Translation experimental results evaluated with case-insensitive BLEU4.
weights for these by counting and smoothing. A
dozen other features are also added to the rules. We
obtain weights for the combinations of the features
by performing minimum error rate training (Och,
2003) on held-out data. We then use a CKY decoder
to translate unseen test data using the rules and tuned
weights. Table 1 summarizes the data used in tuning
and testing.
5.3 Initial results
An initial re-alignment experiment shows a reason-
able rise in BLEU scores from the baseline (Table
2), but closer inspection of the rules favored by EM
implies we can do even better. EM has a tendency
to favor few large rules over many small rules, even
when the small rules are more useful. Referring to
the rules in Figure 2, note that possible derivations
for (taiwan ?s, ? l)2 are R2, R11-R12, and R17-
R18. Clearly the third derivation is not desirable,
and we do not discuss it further. Between the first
two derivations, R11-R12 is preferred over R2, as
the conditioning for possessive insertion is not re-
lated to the specific Chinese word being inserted.
Of the 1,902 sentences in the training corpus where
this pair is seen, the bootstrap alignments yield the
R2 derivation 1,649 times and the R11-R12 deriva-
tion 0 times. Re-alignment does not change the re-
sult much; the new alignments yield the R2 deriva-
tion 1,613 times and again never choose R11-R12.
The rules in the second derivation themselves are
2The Chinese gloss is simply ?taiwan?.
not rarely seen ? R11 is in 13,311 forests other than
those where R2 is seen, and R12 is in 2,500 addi-
tional forests. EM gives R11 a probability of e?7.72
? better than 98.7% of rules, and R12 a probability
of e?2.96. But R2 receives a probability of e?6.32
and is preferred over the R11-R12 derivation, which
has a combined probability of e?10.68.
5.4 Making EM fair
The preference for shorter derivations containing
large rules over longer derivations containing small
rules is due to a general tendency for EM to pre-
fer derivations with few atoms. Marcu and Wong
(2002) note this preference but consider the phe-
nomenon a feature, rather than a bug. Zollmann
and Sima?an (2005) combat the overfitting aspect
for parsing by using a held-out corpus and a straight
maximum likelihood estimate, rather than EM. We
take a modeling approach to the phenomenon.
As the probability of a derivation is determined by
the product of its atom probabilities, longer deriva-
tions with more probabilities to multiply have an in-
herent disadvantage against shorter derivations, all
else being equal. EM is an iterative procedure and
thus such a bias can lead the procedure to converge
with artificially raised probabilities for short deriva-
tions and the large rules that comprise them. The
relatively rare applicability of large rules (and thus
lower observed partial counts) does not overcome
the inherent advantage of large coverage. To com-
bat this, we introduce size terms into our generative
story, ensuring that all competing derivations for the
366
LANGUAGE PAIR TYPE RULES TUNE TEST
CHINESE-ENGLISH baseline 55,781,061 41.51 40.55EMD re-align 69,318,930 41.23 40.55
ARABIC-ENGLISH baseline 8,487,656 51.90 51.69EMD re-align 11,498,150 51.88 52.11
Table 4: Re-alignment performance with semi-supervised EMD bootstrap alignments
same sentence contain the same number of atoms:
1. Choose a rule size s with cost csize(s)s?1.
2. Choose a rule r (of size s) to replace the
start symbol with probability prule(r|s, v).
3. For each variable in the partially com-
pleted (tree, string) pair, continue to
choose sizes followed by rules, recur-
sively to replace these variables until there
are no variables remaining.
This generative story changes the derivation com-
parison from R2 vs R11-R12 to S2-R2 vs R11-R12,
where S2 is the atom that represents the choice of
size 2 (the size of a rule in this context is the number
of non-leaf and non-root nodes in its tree fragment).
Note that the variable number of inclusions implied
by the exponent in the generative story above en-
sures that all derivations have the same size. For ex-
ample, a derivation with one size-3 rule, a derivation
with one size-2 and one size-1 rule, and a deriva-
tion with three size-1 rules would each have three
atoms. With this revised model that allows for fair
comparison of derivations, the R11-R12 derivation
is chosen 1636 times, and S2-R2 is not chosen. R2
does, however, appear in the translation model, as
the expanded rule extraction described in Section 5.2
creates R2 by joining R11 and R12.
The probability of size atoms, like that of rule
atoms, is decided by EM. The revised generative
story tends to encourage smaller sizes by virtue of
the exponent. This does not, however, simply ensure
the largest number of rules per derivation is used in
all cases. Ill-fitting and poorly-motivated rules such
as R22, R23, and R24 in Figure 2 are not preferred
over R16, even though they are smaller. However,
R14 and R16 are preferred over R6, as the former
are useful rules. Although the modified model does
not sum to 1, it leads to an improvement in BLEU
score, as can be seen in the last row of Table 2.
5.5 Results
We performed primary experiments on two different
bootstrap setups in two languages: the initial exper-
iment uses the same data set for the GIZA++ initial
alignment as is used in the re-alignment, while an
experiment on better quality bootstrap alignments
uses a much larger data set. For each bootstrap-
ping in each language we compared the baseline
of using these alignments directly in an MT sys-
tem with the experiment of using the alignments ob-
tained from the re-alignment procedure described in
Section 5.4. For each experiment we report: the
number of rules extracted by the expanded GHKM
algorithm of Galley et al (2006) for the translation
model, converged BLEU scores on the tuning set,
and finally BLEU performance on the held-out test
set. Data set specifics for the GIZA++ bootstrapping
and BLEU results are summarized in Table 3.
5.6 Discussion
The results presented demonstrate we are able to
improve on unsupervised GIZA++ alignments by
about 1 BLEU point for Chinese and around 0.4
BLEU point for Arabic using an additional unsu-
pervised algorithm that requires no human aligned
data. If human-aligned data is available, the EMD
algorithm provides higher baseline alignments than
GIZA++ that have led to better MT performance
(Fraser and Marcu, 2006). As a further experi-
ment we repeated the experimental conditions from
Table 3, this time bootstrapped with the semi-
supervised EMD method, which uses the larger
bootstrap GIZA corpora described in Table 3 and
an additional 64,469/48,650 words of hand-aligned
English-Chinese and 43,782/31,457 words of hand-
aligned English-Arabic. The results of this advanced
experiment are in Table 4. We show a 0.42 gain in
BLEU for Arabic, but no movement for Chinese. We
believe increasing the size of the re-alignment cor-
pora will increase BLEU gains in this experimental
367
condition, but leave those results for future work.
We can see from the results presented that the im-
pact of the syntax-aware re-alignment procedure of
Section 3.2, coupled with the addition of size param-
eters to the generative story from Section 5.4 serves
to remove links from the bootstrap alignments that
cause less useful rules to be extracted, and thus in-
crease the overall quality of the rules, and hence the
system performance. We thus see the benefit to in-
cluding syntax in an alignment model, bringing the
two models of the realistic machine translation path
somewhat closer together.
Acknowledgments
We thank David Chiang, Steve DeNeefe, Alex
Fraser, Victoria Fossum, Jonathan Graehl, Liang
Huang, Daniel Marcu, Michael Pust, Oana Pos-
tolache, Michael Pust, Jason Riesa, Jens Vo?ckler,
and Wei Wang for help and discussion. This re-
search was supported by NSF (grant IIS-0428020)
and DARPA (contract HR0011-06-C-0022).
References
Adam Berger, Peter Brown, Stephen Della Pietra, Vin-
cent Della Pietra, John Gillett, John Lafferty, Robert
Mercer, Harry Printz, and Lubos? Ures?. 1994. The
candide system for machine translation. In Proc. HLT,
pages 157?162, Plainsboro, New Jersey, March.
Daniel Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL, pages
263?270, Ann Arbor, Michigan, June.
Arthur P. Demptser, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proc. EMNLP/CONLL, Prague,
June.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proc. COLING-ACL, pages 769?776, Sydney, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
HLT-NAACL, pages 273?280, Boston, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steven DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic models. In Proc. COLING-
ACL, pages 961?968, Sydney, July.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proc.
ACL, pages 228?235, Toulouse, France, July.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112,
Boston, May.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP, pages 133?139, Philadelphia,
July.
Franz Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proc. ACL, pages 440?447,
Hong Kong, October.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
putational Linguistics, 30(4):417?449.
Franz Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Proc. ACL, pages 160?
167, Sapporo, Japan, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. COLING, pages 836?841, Copen-
hagen, August.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proc. ACL, pages 152?
158, Santa Cruz, California, June.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. ACL, pages 523?
530, Toulouse, France, July.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. HLT-NAACL, pages 256?263,
New York City, June.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics,
10(2/3):367?388.
368
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 755?763, Prague, June 2007. c?2007 Association for Computational Linguistics
What Can Syntax-based MT Learn from Phrase-based MT?
Steve DeNeefe and Kevin Knight
Information Sciences Institute
The Viterbi School of Engineering
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{sdeneefe,knight}@isi.edu
Wei Wang and Daniel Marcu
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
{wwang,dmarcu}@languageweaver.com
Abstract
We compare and contrast the strengths
and weaknesses of a syntax-based machine
translation model with a phrase-based ma-
chine translation model on several levels.
We briefly describe each model, highlight-
ing points where they differ. We include a
quantitative comparison of the phrase pairs
that each model has to work with, as well
as the reasons why some phrase pairs are
not learned by the syntax-based model. We
then evaluate proposed improvements to the
syntax-based extraction techniques in light
of phrase pairs captured. We also compare
the translation accuracy for all variations.
1 Introduction
String models are popular in statistical machine
translation. Approaches include word substitution
systems (Brown et al, 1993), phrase substitution
systems (Koehn et al, 2003; Och and Ney, 2004),
and synchronous context-free grammar systems (Wu
and Wong, 1998; Chiang, 2005), all of which train
on string pairs and seek to establish connections be-
tween source and target strings. By contrast, ex-
plicit syntax approaches seek to directly model the
relations learned from parsed data, including models
between source trees and target trees (Gildea, 2003;
Eisner, 2003; Melamed, 2004; Cowan et al, 2006),
source trees and target strings (Quirk et al, 2005;
Huang et al, 2006), or source strings and target trees
(Yamada and Knight, 2001; Galley et al, 2004).
It is unclear which of these important pursuits will
best explain human translation data, as each has ad-
vantages and disadvantages. A strength of phrase
models is that they can acquire all phrase pairs con-
sistent with computed word alignments, snap those
phrases together easily by concatenation, and re-
order them under several cost models. An advan-
tage of syntax-based models is that outputs tend to
be syntactically well-formed, with re-ordering influ-
enced by syntactic context and function words intro-
duced to serve specific syntactic purposes.
A great number of MT models have been re-
cently proposed, and other papers have gone over the
expressive advantages of syntax-based approaches.
But it is rare to see an in-depth, quantitative study
of strengths and weaknesses of particular models
with respect to each other. This is important for a
scientific understanding of how these models work
in practice. Our main novel contribution is a com-
parison of phrase-based and syntax-based extraction
methods and phrase pair coverage. We also add to
the literature a new method of improving that cover-
age. Additionally, we do a careful study of several
syntax-based extraction techniques, testing whether
(and how much) they affect phrase pair coverage,
and whether (and how much) they affect end-to-end
MT accuracy. The MT accuracy tests are needed
because we want to see the individual effects of par-
ticular techniques under the same testing conditions.
For this comparison, we choose a previously estab-
lished statistical phrase-based model (Och and Ney,
2004) and a previously established statistical string-
to-tree model (Galley et al, 2004). These two mod-
els are chosen because they are the basis of two of
the most successful systems in the NIST 2006 MT
755
evaluation1.
2 Phrase-based Extraction
The Alignment Template system (ATS) described by
Och and Ney (2004) is representative of statistical
phrase-based models. The basic unit of translation
is the phrase pair, which consists of a sequence of
words in the source language, a sequence of words
in the target language, and a vector of feature val-
ues which describe this pair?s likelihood. Decod-
ing produces a string in the target language, in or-
der, from beginning to end. During decoding, fea-
tures from each phrase pair are combined with other
features (e.g., re-ordering, language models) using a
log-linear model to compute the score of the entire
translation.
The ATS phrase extraction algorithm learns these
phrase pairs from an aligned, parallel corpus.
This corpus is conceptually a list of tuples of
<source sentence, target sentence, bi-directional
word alignments> which serve as training exam-
ples, one of which is shown in Figure 1.
Figure 1: a phrase-based training example
For each training example, the algorithm identi-
fies and extracts all pairs of <source sequence, tar-
get sequence> that are consistent with the align-
ments. It does this by first enumerating all source-
side word sequences up to a length limit L, and for
each source sequence, it identifies all target words
aligned to those source words. For example, in Fig-
ure 1, for the source phrase ? 	  , the target
words it aligns to are felt, obliged, and do.
These words, and all those between them, are the
proposed target phrase. If no words in the proposed
target phrase align to words outside of the source
phrase, then this phrase pair is extracted.
The extraction algorithm can also look to the left
and right of the proposed target phrase for neighbor-
ing unaligned words and extracts phrases. For ex-
ample, for the phrase pair ? 	  ? felt obliged,
1http://www.nist.gov/speech/tests/mt/
mt06eval official results.html
the word to is a neighboring unaligned word. It
constructs new target phrases by adding on con-
secutive unaligned words in both directions, and
extracts those in new pairs, too (e.g., ? 	  ?
felt obliged to). For efficiency reasons, imple-
mentations often skip this step.
Figure 2 shows the complete set of phrase pairs
up to length 4 that are extracted from the Figure 1
training example. Notice that no extracted phrase
pair contains the character ?. Because of the align-
ments, the smallest legal phrase pair, ? ? 	  
? i felt obliged to do my, is beyond the size
limit of 4, so it is not extracted in this example.
? ? felt
? 	  ? felt obliged
? 	   ? felt obliged to do
	  ? obliged
	   ? obliged to do
 ? do
 P ? part
 P ? ? part
 P ? . ? part .
? . ? .
. ? .
Figure 2: phrases up to length 4 extracted from the
example in Figure 1
Phrase pairs are extracted over the entire train-
ing corpus. Due to differing alignments, some
phrase pairs that cannot be learned from one exam-
ple may be learned from another. These pairs are
then counted, once for each time they are seen in a
training example, and these counts are used as the
basis for maximum likelihood probability features,
such as p(f |e) and p(e|f).
3 Syntax-based Extraction
The GHKM syntax-based extraction method for
learning statistical syntax-based translation rules,
presented first in (Galley et al, 2004) and expanded
on in (Galley et al, 2006), is similar to phrase-based
extraction in that it extracts rules consistent with
given word alignments. A primary difference is the
use of syntax trees on the target side, rather than se-
quences of words. The basic unit of translation is the
translation rule, consisting of a sequence of words
756
and variables in the source language, a syntax tree
in the target language having words or variables at
the leaves, and again a vector of feature values which
describe this pair?s likelihood. Translation rules can:
? look like phrase pairs with syntax decoration:
NPB(NNP(prime)
NNP(minister)
NNP(keizo)
NNP(obuchi))
? B ? ? ? D #
? carry extra contextual constraints:
VP(VBD(said)
x0:SBAR-C)
? ? x0
(according to this rule, ? can translate to
said only if some Chinese sequence to the
right of ? is translated into an SBAR-C)
? be non-constituent phrases:
VP(VBD(said)
SBAR-C(IN(that)
x0:S-C))
? ? x0
VP(VBD(pointed)
PRT(RP(out))
x0:SBAR-C)
? ? ? x0
? contain non-contiguous phrases, effectively
?phrases with holes?:
PP(IN(on)
NP-C(NPB(DT(the)
x0:NNP))
NN(issue))))
? ? x0 ?  ?
PP(IN(on)
NP-C(NPB(DT(the)
NN(issue))
x0:PP))
? ? x0 ?  ?
? be purely structural (no words):
S(x0:NP-C x1:VP)? x0 x1
? re-order their children:
NP-C(NPB(DT(the)
x0:NN)
PP(IN(of)
x1:NP-C))
? x1 { x0
Decoding with this model produces a tree in the
target language, bottom-up, by parsing the foreign
string using a CYK parser and a binarized rule set
(Zhang et al, 2006). During decoding, features from
each translation rule are combined with a language
model using a log-linear model to compute the score
of the entire translation.
The GHKM extractor learns translation rules from
an aligned parallel corpus where the target side has
been parsed. This corpus is conceptually a list of tu-
ples of <source sentence, target tree, bi-directional
word alignments> which serve as training exam-
ples, one of which is shown in Figure 3.
Figure 3: a syntax-based training example
For each training example, the GHKM extrac-
tor computes the set of minimally-sized translation
rules that can explain the training example while re-
maining consistent with the alignments. This is, in
effect, a non-overlapping tiling of translation rules
over the tree-string pair. If there are no unaligned
words in the source sentence, this is a unique set.
This set, ordered into a tree of rule applications, is
called the derivation tree of the training example.
Unlike the ATS model, there are no inherent size
limits, just the constraint that the rules be as small
as possible for the example.
Ignoring the unaligned ? for the moment, there
are seven minimal translation rules that are extracted
from the example in Figure 3, as shown in Fig-
ure 4. Notice that rule 6 is rather large and applies
to a very limited syntactic context. The only con-
stituent node that covers both i and my is the S,
so the rule rooted at S is extracted, with variables
for every branch below this top constituent that can
be explained by other rules. Note also that to be-
757
comes a part of this rule naturally. If the alignments
were not as constraining (e.g., if my was unaligned),
then instead of this one big rule many smaller rules
would be extracted, such as structural rules (e.g.,
VP(x0:VBD x1:VP-C)? x0 x1) and function word in-
sertion rules (e.g., VP(TO(to) x0:VP-C)? x0).
1. VBD(felt)? ?
2. VBN(obliged)? 	 
3. VB(do)? 
4. NN(part)?  P
5. PERIOD(.)? .
6. S(NP-C(NPB(PRP(I)))
VP(x0:VBD
VP-C(x1:VBN
SG-C(VP(TO(to)
VP-C(x2:VB
NP-C(NPB(PRP$(my)
x3:NN)))))))
x4:PERIOD)? ? x0 x1 x2 x3 x4
7. TOP(x0:S)? x0
Figure 4: rules extracted from training example
We ignored unaligned source words in the exam-
ple above. Galley et al (2004) attach the unaligned
source word to the highest possible location, in our
example, the S. Thus it is extracted along with our
large rule 6, changing the target language sequence
to ?? x0 x1 x2 x3 ? x4?. This treatment still re-
sults in a unique derivation tree no matter how many
unaligned words are present.
In Galley et al (2006), instead of a unique deriva-
tion tree, the extractor computes several derivation
trees, each with the unaligned word added to a dif-
ferent rule such that the data is still explained. For
example, for the tree-string pair in Figure 3, ?
could be added not only to rule 6, but alternatively
to rule 4 or 5, to make the new rules:
NN(part)?  P ?
PERIOD(.)? ? .
This results in three different derivations, one
with the ? character in rule 4 (with rules 5 and 6
as originally shown), another with the ? character
in rule 5 (with rules 4 and 6 as originally shown),
and lastly one with the ? character in rule 6 (with
rules 4 and 5 as originally shown) as in the origi-
nal paper (Galley et al, 2004). In total, ten different
rules are extracted from this training example.
As with ATS, translation rules are extracted and
counted over the entire training corpus, a count of
one for each time they appear in a training example.
These counts are used to estimate several features,
including maximum likelihood probability features
for p(etree, fwords|ehead), p(ewords|fwords), and
p(fwords|ewords).
4 Differences in Phrasal Coverage
Both the ATS model and the GHKM model extract
linguistic knowledge from parallel corpora, but each
has fundamentally different constraints and assump-
tions. To compare the models empirically, we ex-
tracted phrase pairs (for the ATS model) and transla-
tion rules (for the GHKM model) from parallel train-
ing corpora described in Table 1. The ATS model
was limited to phrases of length 10 on the source
side, and length 20 on the target side. A super-
set of the parallel data was word aligned by GIZA
union (Och and Ney, 2003) and EMD (Fraser and
Marcu, 2006). The English side of training data was
parsed using an implementation of Collins? model 2
(Collins, 2003).
Chinese Arabic
Document IDs LDC2003E07 LDC2004T17
LDC2003E14 LDC2004T18
LDC2005T06 LDC2005E46
# of segments 329,031 140,511
# of words in foreign corpus 7,520,779 3,147,420
# of words in English corpus 9,864,294 4,067,454
Table 1: parallel corpora used to train both models
Table 2 shows the total number of GHKM rules
extracted, and a breakdown of the different kinds
of rules. Non-lexical rules are those whose source
side is composed entirely of variables ? there are
no source words in them. Because of this, they
potentially apply to any sentence. Lexical rules
(their counterpart) far outnumber non-lexical rules.
Of the lexical rules, a rule is considered a phrasal
rule if its source side and the yield of its target
side contain exactly one contiguous phrase each, op-
tionally with one or more variables on either side
of the phrase. Non-phrasal rules include structural
rules, re-ordering rules, and non-contiguous phrases.
These rules are not easy to directly compare to any
phrase pairs from the ATS model, so we do not focus
on them here.
Phrasal rules can be directly compared to ATS
phrase pairs, the easiest way being to discard the
758
Statistic Chinese Arabic
total translation rules 2,487,110 662,037
non-lexical rules 110,066 15,812
lexical rules 2,377,044 646,225
phrasal rules 1,069,233 406,020
distinct GHKM-derived phrase pairs 919,234 352,783
distinct corpus-specific
GHKM-derived phrase pairs 203,809 75,807
Table 2: a breakdown of how many rules the
GHKM extraction algorithm produces, and how
many phrase pairs can be derived from them
syntactic context and look at the phrases contained
in the rules. The second to last line of Table 2 shows
the number of phrase pairs that can be derived from
the above phrasal rules. The number of GHKM-
derived phrase pairs is lower than the number of
phrasal rules because some rules represent the same
phrasal translation, but with different syntactic con-
texts. The last line of Table 2 shows the subset of
phrase pairs that contain source phrases found in our
development corpus.
Table 3 compares these corpus-specific GHKM-
derived phrase pairs with the corpus-specific ATS
phrase pairs. Note that the number of phrase pairs
derived from the GHKM rules is less than the num-
ber of phrase pairs extracted by ATS. Moreover, only
slightly over half of the phrase pairs extracted by the
ATS model are common to both models. The lim-
its and constraints of each model are responsible for
this difference in contiguous phrases learned.
Source of phrase pairs Chinese Arabic
GHKM-derived 203,809 75,807
ATS 295,537 133,576
Overlap between models 160,901 75,038
GHKM only 42,908 769
ATS only 134,636 58,538
ATS-useful only 1,994 2,199
Table 3: comparison of corpus-specific phrase pairs
from each model
GHKM learns some contiguous phrase pairs that
the phrase-based extractor does not. Only a small
portion of these are due to the fact that the GHKM
model has no inherent size limit, while the phrase
based system has limits. More numerous are cases
where unaligned English words are not added to an
ATS phrase pair while GHKM adopts them at a syn-
tactically motivated location, or where a larger rule
contains mostly syntactic structure but happens to
have some unaligned words in it. For example, con-
sider Figure 5. Because basic and will are un-
aligned, ATS will learn no phrase pairs that translate
to these words alone, though they will be learned as
a part of larger phrases.
Figure 5: Situation where GHKM is able to learn
rules that translate into basic and will, but ATS
is not
GHKM, however, will learn several phrasal rules
that translate to basic, based on the syntactic con-
text
NPB(x0:DT
JJ(basic)
x1:NN)
? x0  x1
NPB(x0:DT
JJ(basic)
x1:NN)
? x0  ? ? x1
NPB(x0:DT
JJ(basic)
x1:NN)
? x0 ? ? x1
and one phrasal rule that translates into will
VP(MD(will)
x0:RB
x1:VP-C)
? x0 ? ? x1
The quality of such phrases may vary. For example,
the first translation of  (literally: ?one? or ?a?) to
basic above is a phrase pair of poor quality, while
the other two for basic and one for will are ar-
guably reasonable.
However, Table 3 shows that ATS was able to
learn many more phrase pairs that GHKM was not.
Even more significant is the subset of these missing
phrase pairs that the ATS decoder used in its best2
2i.e. highest scoring
759
translation of the corpus. According to the phrase-
based system these are the most ?useful? phrase
pairs and GHKM could not learn them. Since this is
a clear deficiency, we will focus on analyzing these
phrase pairs (which we call ATS-useful) and the rea-
sons they were not learned.
Table 4 shows a breakdown, categorizing each of
these missing ATS-useful phrase pairs and the rea-
sons they were not able to be learned. The most
common reason is straightforward: by extracting
only the minimally-sized rules, GHKM is unable to
learn many larger phrases that ATS learns. If GHKM
can make a word-level analysis, it will do that, at
the expense of a phrase-level analysis. Galley et
al. (2006) propose one solution to this problem and
Marcu et al (2006) propose another, both of which
we explore in Sections 5.1 and 5.2.
Category of missing ATS-useful phrase pairs Chinese Arabic
Not minimal 1,320 1,366
Extra target words in GHKM rules 220 27
Extra source words in GHKM rules 446 799
Other (e.g. parse failures) 8 7
Total missing useful phrase pairs 1,994 2,199
Table 4: reasons that ATS-useful phrase pairs could
not be extracted by GHKM as phrasal rules
The second reason is that the GHKM model is
sometimes forced by its syntactic constraints to in-
clude extra words. Sometimes this is only target lan-
guage words, and this is often useful ? the rules are
learning to insert these words in their proper context.
But most of the time, source language words are also
forced to be part of the rule, and this is harmful ? it
makes the rules less general. This latter case is often
due to poorly aligned target language words (such as
the ? in our Section 3 rule extraction example), or
unaligned words under large, flat constituents.
Another factor here: some of the phrase pairs are
learned by both systems, but GHKM is more specific
about the context of use. This can be both a strength
and a weakness. It is a strength when the syntactic
context helps the phrase to be used in a syntactically
correct way, as in
VP(VBD(said)
x0:SBAR-C)
? ? x0
where the syntax rule requires a constituent of type
SBAR-C. Conversely its weakness is seen when the
context is too constrained. For example, ATS can
easily learn the phrase
 ? ? prime minister
and is then free to use it in many contexts. But
GHKM learns 45 different rules, each that translate
this phrase pair in a unique context. Figure 6 shows
a sampling. Notice that though many variations are
present, the decoder is unable to use any of these
rules to produce certain noun phrases, such as ?cur-
rent Japanese Prime Minister Shinzo Abe?, because
no rule has the proper number of English modifiers.
NPB(NNP(prime) NNP(minister) x0:NNP)? x0  ?
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(x0:JJ NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(NNP(prime) NNP(minister) x0:NNP)?  ? x0
NPB(NNP(prime) NNP(minister))?  ?
NPB(NNP(prime) NNP(minister) x0:NNP x1:NNP)? x0 x1  ?
NPB(x0:DT x1:JJ JJ(prime) NN(minister))? x0 x1  ?
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
Figure 6: a sampling of the 45 rules that translate
 ? to prime minister
5 Coverage Improvements
Each of the models presented so far has advantages
and disadvantages. In this section, we consider ideas
that make up for deficiencies in the GHKM model,
drawing our inspiration from the strong points of the
ATS model. We then measure the effects of each
idea empirically, showing both what is gained and
the potential limits of each modification.
5.1 Composed Rules
Galley et al (2006) proposed the idea of composed
rules. This removes the minimality constraint re-
quired earlier: any two or more rules in a parent-
child relationship in the derivation tree can be com-
bined to form a larger, composed rule. This change
is similar in spirit to the move from word-based to
phrase-based MT models, or parsing with a DOP
model (Bod et al, 2003) rather than a plain PCFG.
Because this results in exponential variations, a
size limit is employed: for any two or more rules
to be allowed to combine, the size of the resulting
rule must be at most n. The size of a rule is de-
fined as the number of non-part-of-speech, non-leaf
760
constituent labels in a rule?s target tree. For exam-
ple, rules 1-5 shown in Section 3 have a size of 0,
and rule 6 has a size of 10. Composed rules are ex-
tracted in addition to minimal rules, which means
that a larger n limit always results in a superset of
the rules extracted when a smaller n value is used.
When n is set to 0, then only minimal rules are ex-
tracted. Table 5 shows the growth in the number of
rules extracted for several size limits.
Size limit (n) Chinese Arabic
0 (minimal) 2,487,110 662,037
2 12,351,297 2,742,513
3 26,917,088 4,824,928
4 55,781,061 8,487,656
Table 5: increasing the size limit of composed rules
significantly increases the number of rules extracted
In our previous analysis, the main reason that
GHKM did not learn translations for ATS-useful
phrase pairs was due to its minimal-only approach.
Table 6 shows the effect that composed rule extrac-
tion has on the total number of ATS-useful phrases
missing. Note that as the allowed size of composed
rule increases, we are able to extract an greater per-
centage of the missing ATS-useful phrase pairs.
Size limit (n) Chinese Arabic
0 (minimal) 1,994 2,199
2 1,478 1,528
3 1,096 1,210
4 900 1,041
Table 6: number of ATS-useful phrases still missing
when using GHKM composed rule extraction
Unfortunately, a comparison of Tables 5 and 6 in-
dicates that the number of ATS-useful phrase pairs
gained is growing at a much slower rate than the total
number of rules. From a practical standpoint, more
rules means more processing work and longer de-
coding times, so there are diminishing returns from
continuing to explore larger size limits.
5.2 SPMT Model 1 Rules
An alternative for extracting larger rules called
SPMT model 1 is presented by Marcu et al (2006).
Though originally presented as a separate model,
the method of rule extraction itself builds upon the
minimal GHKM method just as composed rules do.
For each training example, the method considers all
source language phrases up to length L. For each of
these phrases, it extracts the smallest possible syn-
tax rule that does not violate the alignments. Ta-
ble 7 shows that this method is able to extract rules
that cover useful phrases, and can be combined with
size 4 composed rules to an even better effect. Since
there is some overlap in these methods, when com-
bining the two methods we eliminate any redundant
rules.
Method Chinese Arabic
composed alone (size 4) 900 1,041
SPMT model 1 alone 676 854
composed + SPMT model 1 663 835
Table 7: ATS-useful phrases still missing after dif-
ferent non-minimal methods are applied
Note that having more phrasal rules is not the only
advantage of composed rules. Here, combining both
composed and SPMT model 1 rules, our gain in use-
ful phrases is not very large, but we do gain addi-
tional, larger syntax rules. As discussed in (Galley
et al, 2006), composed rules also allow the learning
of more context, such as
ADJP(ADVP(RB(far)
CC(and)
RB(away)
x0:JJ)
? ? ? x0
This rule is not learned by SPMT model 1 because
it is not the smallest rule that can explain the phrase
pair, but it is still valuable for its syntactic context.
5.3 Restructuring Trees
Table 8 updates the causes of missing ATS-useful
phrase pairs. Most are now caused by syntactic con-
straints, thus we need to address these in some way.
GHKM translation rules are affected by large,
flat constituents in syntax trees, as in the prime
minister example earlier. One way to soften this
constraint is to binarize the trees, so that wide con-
stituents are broken down into multiple levels of tree
structure. The approach we take here is head-out bi-
narization (Wang et al, 2007), where any constituent
with more than two children is split into partial con-
stituents. The children to the left of the head word
761
Category of ATS-useful phrase pairs Chinese Arabic
Too large 12 9
Extra target words in GHKM rules 218 27
Extra source words in GHKM rules 424 792
Other (e.g. parse failures) 9 7
Total missing useful phrase pairs 663 835
Table 8: reasons that ATS-useful phrase pairs are
still not extracted as phrasal rules, with composed
and SPMT model 1 rules in place
are binarized one direction, while the children to
the right are binarized the other direction. The top
node retains its original label (e.g. NPB), while the
new partial constituents are labeled with a bar (e.g.
NPB). Figure 7 shows an example.
Figure 7: head-out binarization in the target lan-
guage: S, NPB, and VP are binarized according to
the head word
Table 9 shows the effect of binarization on phrasal
coverage, using both composed and SPMT rules. By
eliminating some of the syntactic constraints we al-
low more freedom, which allows increased phrasal
coverage, but generates more rules.
Category of missing ATS-useful phrase pairs Chinese Arabic
Too large 16 12
Extra target words in GHKM rules 123 12
Extra source words in GHKM rules 307 591
Other (e.g. parse failures) 12 7
Total missing useful phrase pairs 458 622
Table 9: reasons that ATS-useful phrase pairs still
could not be extracted as phrasal rules after bina-
rization
6 Evaluation of Translations
To evaluate translation quality of each of these mod-
els and methods, we ran the ATS decoder using its
extracted phrase pairs and the syntax-based decoder
using all the rule sets mentioned above. Table 10 de-
scribes the development and test datasets used, along
with four references for measuring BLEU. Tun-
ing was done using Maximum BLEU hill-climbing
(Och, 2003). Features used for the ATS system were
the standard set. For the syntax-based translation
system, we used a similar set of features.
# of lines
Dataset Chinese Arabic
Development set NIST 2002 MT eval 925 696
(sentences < 47 tokens)
Test set NIST 2003 MT eval 919 663
Table 10: development and test corpora
Table 11 shows the case-insensitive NIST BLEU4
scores for both our development and test decod-
ings. The BLEU scores indicate, first of all, that
the syntax-based system is much stronger in trans-
lating Chinese than Arabic, in comparison to the
phrase-based system. Also, the ideas presented here
for improving phrasal coverage generally improve
the syntax-based translation quality. In addition,
composed rules are shown to be helpful as com-
pared to the minimal runs. This is true even when
SPMT model 1 is added, which indicates that the
size 4 composed rules bring more than just improved
phrasal coverage.
Chinese Arabic
Experiment Dev Test Dev Test
Baseline ATS 34.94 32.83 50.46 50.52
Baseline GHKM (minimal only) 38.02 37.67 49.34 49.99
GHKM composed size 2 40.24 39.75 50.76 50.94
GHKM composed size 3 40.95 40.44 51.56 51.48
GHKM composed size 4 41.36 40.69 51.60 51.71
GHKM minimal + SPMT model 1 39.78 39.16 50.17 51.27
GHKM composed + SPMT model 1 42.04 41.07 51.73 51.53
With binarization 42.17 41.26 52.50 51.79
Table 11: evaluation results (reported in case-
insensitive NIST BLEU4)
7 Conclusions
Both the ATS model for phrase-based machine
translation and the GHKM model for syntax-based
machine translation are state-of-the-art methods.
Each extraction method has strengths and weak-
nesses as compared to the other, and there are sur-
prising differences in phrasal coverage ? neither is
merely a superset of the other. We have shown that
it is possible to gain insights from the strengths of
the phrase-based extraction model to increase both
762
the phrasal coverage and translation accuracy of the
syntax-based model.
However, there is still room for improvement in
both models. For syntax models, there are still holes
in phrasal coverage, and other areas are needing
progress, such as decoding efficiency. For phrase-
based models, incorporating syntactic knowledge
and constraints may lead to improvements as well.
8 Acknowledgments
The authors wish to acknowledge our colleagues at
ISI, especially David Chiang, for constructive criti-
cism on an early draft of this document, and several
reviewers for their detailed comments which helped
us make the paper stronger. We are also grateful to
Jens-So?nke Vo?ckler for his assistance in setting up
an experimental pipeline, without which this work
would have been much more tedious and difficult.
This research was supported under DARPA Contract
No. HR0011-06-C-0022.
References
Rens Bod, Remko Scha, and Khalil Sima?an, editors. 2003.
Data-Oriented Parsing. CSLI Publications, University of
Chicago Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL 2005.
Michael Collins. 2003. Head-driven statistical models for nat-
ural language parsing. Computational Linguistics, 29(4).
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins. 2006.
A discriminative model for tree-to-tree translation. In Proc.
EMNLP 2006.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. ACL 2003.
Alexander Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In Proc. ACL 2006.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc. HLT-
NAACL 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proc. ACL 2006.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. ACL 2003, companion volume.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Sta-
tistical syntax-directed translation with extended domain of
locality. In Proc. AMTA 2006.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. HLT-NAACL 2003.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proc. EMNLP 2006.
I. Dan Melamed. 2004. Statistical machine translation by pars-
ing. In Proc. ACL 2004.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1).
Franz Josef Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics, 30.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. ACL 2003.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proc. ACL 2005.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing
syntax trees to improve syntax-based machine translation ac-
curacy. In Proc. EMNLP and CoNLL 2007.
Dekai Wu and Hongsing Wong. 1998. Machine translation
with a stochastic grammatical channel. In Proc. ACL 1998.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proc. ACL 2001.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proc. NAACL HLT 2006.
763
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 812?819,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Attacking Decipherment Problems Optimally with Low-Order N-gram
Models
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We introduce a method for solving substi-
tution ciphers using low-order letter n-gram
models. This method enforces global con-
straints using integer programming, and it
guarantees that no decipherment key is over-
looked. We carry out extensive empirical ex-
periments showing how decipherment accu-
racy varies as a function of cipher length and
n-gram order. We also make an empirical in-
vestigation of Shannon?s (1949) theory of un-
certainty in decipherment.
1 Introduction
A number of papers have explored algorithms
for automatically solving letter-substitution ciphers.
Some use heuristic methods to search for the best de-
terministic key (Peleg and Rosenfeld, 1979; Gane-
san and Sherman, 1993; Jakobsen, 1995; Olson,
2007), often using word dictionaries to guide that
search. Others use expectation-maximization (EM)
to search for the best probabilistic key using letter
n-gram models (Knight et al, 2006). In this paper,
we introduce an exact decipherment method based
on integer programming. We carry out extensive de-
cipherment experiments using letter n-gram models,
and we find that our accuracy rates far exceed those
of EM-based methods.
We also empirically explore the concepts in Shan-
non?s (1949) paper on information theory as applied
to cipher systems. We provide quantitative plots for
uncertainty in decipherment, including the famous
unicity distance, which estimates how long a cipher
must be to virtually eliminate such uncertainty.
We find the ideas in Shannon?s (1949) paper rel-
evant to problems of statistical machine translation
and transliteration. When first exposed to the idea
of statistical machine translation, many people natu-
rally ask: (1) how much data is needed to get a good
result, and (2) can translation systems be trained
without parallel data? These are tough questions by
any stretch, and it is remarkable that Shannon was
already in the 1940s tackling such questions in the
realm of code-breaking, creating analytic formulas
to estimate answers.
Our novel contributions are as follows:
? We outline an exact letter-substitution deci-
pherment method which:
- guarantees that no key is overlooked, and
- can be executed with standard integer pro-
gramming solvers
? We present empirical results for decipherment
which:
- plot search-error-free decipherment results at
various cipher lengths, and
- demonstrate accuracy rates superior to EM-
based methods
? We carry out empirical testing of Shannon?s
formulas for decipherment uncertainty
2 Language Models
We work on letter substitution ciphers with spaces.
We look for the key (among 26! possible ones)
that, when applied to the ciphertext, yields the most
English-like result. We take ?English-like? to mean
812
most probable according to some statistical lan-
guage model, whose job is to assign some proba-
bility to any sequence of letters. According to a 1-
gram model of English, the probability of a plaintext
p1...pn is given by:
P (p1...pn) = P (p1) ? P (p2) ? ... ? P (pn)
That is, we obtain the probability of a sequence
by multiplying together the probabilities of the in-
dividual letters that make it up. This model assigns
a probability to any letter sequence, and the proba-
bilities of all letter sequences sum to one. We col-
lect letter probabilities (including space) from 50
million words of text available from the Linguistic
Data Consortium (Graff and Finch, 1994). We also
estimate 2- and 3-gram models using the same re-
sources:
P (p1...pn) = P (p1 | START ) ? P (p2 | p1) ? P (p3 | p2) ?
... ? P (pn | pn?1) ? P (END | pn)
P (p1...pn) = P (p1 | START ) ? P (p2 | START p1) ?
P (p3 | p1 p2) ? ... ? P (pn | pn?2 pn?1) ?
P (END | pn?1 pn)
Unlike the 1-gram model, the 2-gram model will
assign a low probability to the sequence ?ae? be-
cause the probability P (e | a) is low. Of course, all
these models are fairly weak, as already known by
(Shannon, 1949). When we stochastically generate
text according to these models, we get, for example:
1-gram: ... thdo detusar ii c ibt deg irn toihytrsen ...
2-gram: ... itariaris s oriorcupunond rke uth ...
3-gram: ... ind thnowelf jusision thad inat of ...
4-gram: ... rece bence on but ther servier ...
5-gram: ... mrs earned age im on d the perious ...
6-gram: ... a party to possible upon rest of ...
7-gram: ... t our general through approve the ...
We can further estimate the probability of a whole
English sentence or phrase. For example, the prob-
abilities of two plaintext phrases ?het oxf? and
?the fox? (which have the same letter frequency
distribution) is shown below. The 1-gram model
which counts only the frequency of occurrence of
each letter in the phrase, estimates the same proba-
bility for both the phrases ?het oxf? and ?the fox?,
since the same letters occur in both phrases. On the
other hand, the 2-gram and 3-gram models, which
take context into account, are able to distinguish be-
tween the English and non-English phrases better,
and hence assign a higher probability to the English
phrase ?the fox?.
Model P(het oxf) P(the fox)
1-gram 1.83? 10?9 1.83? 10?9
2-gram 3.26? 10?11 1.18? 10?7
3-gram 1.89? 10?13 1.04? 10?6
Over a longer sequence X of length N , we can
also calculate ?log2(P (X))/N , which (per Shan-
non) gives the compression rate permitted by the
model, in bits per character. In our case, we get:1
1-gram: 4.19
2-gram: 3.51
3-gram: 2.93
3 Decipherment
Given a ciphertext c1...cn, we search for the key that
yields the most probable plaintext p1...pn. There are
26! possible keys, too many to enumerate. How-
ever, we can still find the best one in a guaranteed
fashion. We do this by taking our most-probable-
plaintext problem and casting it as an integer pro-
gramming problem.2
Here is a sample integer programming problem:
variables: x, y
minimize:
2x+ y
subject to:
x+ y < 6.9
y ? x < 2.5
y > 1.1
We require that x and y take on integer values. A
solution can be obtained by typing this integer pro-
gram into the publicly available lp solve program,
1Because spacing is fixed in our letter substitution ciphers,
we normalize P (X) by the sum of probabilities of all English
strings that match the spacing pattern of X .
2For an overview of integer and linear programming, see for
example (Schrijver, 1998).
813
1    2    3    4    5    6    7    8  ?
_    Q    W    B    S    Q    W    _  ?
a    a    a    a    a    a    a    a  ?
b    b    b    b    b    b    b    b  ?
c    c    c    c    c    c    c    c  ?
d    d    d    d    d    d    d    d  ?
e    e    e    e    e    e    e    e  ?
?    ?    ?    ?    ?    ?    ?    ?  ?
z    z    z    z    z    z    z    z  ?
_    _    _    _    _    _    _    _  ?
ciphertext
network of
possible
plaintexts
link-2de link-5ad link-7e_
Figure 1: A decipherment network. The beginning of the ciphertext is shown at the top of the figure (underscores
represent spaces). Any left-to-right path through the network constitutes a potential decipherment. The bold path
corresponds to the decipherment ?decade?. The dotted path corresponds to the decipherment ?ababab?. Given a
cipher length of n, the network has 27 ? 27 ? (n ? 1) links and 27n paths. Each link corresponds to a named variable
in our integer program. Three links are shown with their names in the figure.
or the commercially available CPLEX program,
which yields the result: x = 4, y = 2.
Suppose we want to decipher with a 2-gram lan-
guage model, i.e., we want to find the key that yields
the plaintext of highest 2-gram probability. Given
the ciphertext c1...cn, we create an integer program-
ming problem as follows. First, we set up a net-
work of possible decipherments (Figure 1). Each
of the 27 ? 27 ? (n ? 1) links in the network is a
binary variable in the integer program?it must be
assigned a value of either 0 or 1. We name these
variables linkXY Z , where X indicates the column
of the link?s source, and Y and Z represent the rows
of the link?s source and destination (e.g. variables
link1aa, link1ab, link5qu, ...).
Each distinct left-to-right path through the net-
work corresponds to a different decipherment. For
example, the bold path in Figure 1 corresponds to
the decipherment ?decade?. Decipherment amounts
to turning some links ?on? (assigning value 1 to the
link variable) and others ?off? (assigning value 0).
Not all assignments of 0?s and 1?s to link variables
result in a coherent left-to-right path, so we must
place some ?subject to? constraints in our integer
program.
We observe that a set of variables forms a path if,
for every node in columns 2 through n?1 of the net-
work, the following property holds: the sum of the
values of the link variables entering the node equals
the sum of the link variables leaving the node. For
nodes along a chosen decipherment path, this sum
will be 1, and for others, it will be 0.3 Therefore,
we create one ?subject to? constraint for each node
(? ? stands for space). For example, for the node in
column 2, row e we have:
subject to:
link1ae + link1be + link1ce + ...+ link1 e
= link2ea + link2eb + link2ec + ...+ link2e
Now we set up an expression for the ?minimize?
part of the integer program. Recall that we want
to select the plaintext p1...pn of highest probability.
For the 2-gram language model, the following are
equivalent:
(a) Maximize P (p1...pn)
(b) Maximize log2 P (p1...pn)
(c) Minimize ?log2 P (p1...pn)
(d) Minimize ?log2 [ P (p1 |START )
3Strictly speaking, this constraint over nodes still allows
multiple decipherment paths to be active, but we can rely on
the rest of our integer program to select only one.
814
?P (p2 | p1)
? ...
?P (pn | pn?1)
?P (END | pn) ]
(e) Minimize ?log2 P (p1 |START )
?log2 P (p2 | p1)
? ...
?log2 P (pn | pn?1)
?log2 P (END | pn)
We can guarantee this last outcome if we con-
struct our minimization function as a sum of 27 ?27 ?
(n? 1) terms, each of which is a linkXY Z variable
multiplied by ?log2P (Z|Y ):
Minimize link1aa ? ?log2 P (a | a)
+ link1ab ? ?log2 P (b | a)
+ link1ac ? ?log2 P (c | a)
+ ...
+ link5qu ? ?log2 P (u | q)
+ ...
When we assign value 1 to link variables along
some decipherment path, and 0 to all others, this
function computes the negative log probability of
that path.
We must still add a few more ?subject to? con-
straints. We need to ensure that the chosen path im-
itates the repetition pattern of the ciphertext. While
the bold path in Figure 1 represents the fine plain-
text choice ?decade?, the dotted path represents the
choice ?ababab?, which is not consistent with the
repetition pattern of the cipher ?QWBSQW?. To
make sure our substitutions obey a consistent key,
we set up 27 ? 27 = 729 new keyxy variables to
represent the choice of key. These new variables
are also binary, taking on values 0 or 1. If variable
keyaQ = 1, that means the key maps plaintext a to
ciphertext Q. Clearly, not all assignments to these
729 variables represent valid keys, so we augment
the ?subject to? part of our integer program by re-
quiring that for any letter x,
subject to:
keyxA + keyxB + ...+ keyxZ + keyx = 1
keyAx + keyBx + ...+ keyZx + key x = 1
That is, every plaintext letter must map to exactly
one ciphertext letter, and every ciphertext letter must
map to exactly one plaintext letter. We also add a
constraint to ensure that the ciphertext space charac-
ter maps to the plaintext space character:
subject to:
key = 1
Finally, we ensure that any chosen decipherment
path of linkXY Z variables is consistent with the
chosen key. We know that for every node A along
the decipherment path, exactly one active link has
A as its destination. For all other nodes, zero active
links lead in. Suppose node A represents the de-
cipherment of ciphertext letter ci as plaintext letter
pj?for all such nodes, we stipulate that the sum of
values for link(i?1)xpj (for all x) equals the value of
keypjci . In other words, whether a node lies along
the chosen decipherment path or not, the chosen key
must support that decision.
Figure 2 summarizes the integer program that we
construct from a given ciphertext c1...cn. The com-
puter code that transforms any given cipher into a
corresponding integer program runs to about one
page. Variations on the decipherment network yield
1-gram and 3-gram decipherment capabilities. Once
an integer program is generated by machine, we
ask the commercially-available CPLEX software
to solve it, and then we note which keyXY variables
are assigned value 1. Because CPLEX computes
the optimal key, the method is not fast?for ciphers
of length 32, the number of variables and constraints
encoded in the integer program (IP) along with aver-
age running times are shown below. It is possible to
obtain less-than-optimal keys faster by interrupting
the solver.
Model # of IP # of IP Average
variables constraints running time
1-gram 1, 755 1, 083 0.01 seconds
2-gram 27, 700 2, 054 50 seconds
3-gram 211, 600 27, 326 450 seconds
4 Decipherment Experiments
We create 50 ciphers each of lengths 2, 4, 8, ..., 256.
We solve these with 1-gram, 2-gram, and 3-gram
language models. We record the average percentage
of ciphertext tokens decoded incorrectly. 50% error
means half of the ciphertext tokens are deciphered
wrong, while 0% means perfect decipherment. Here
815
variables:
linkipr 1 if the ith cipher letter is deciphered as plaintext letter p AND the (i+1)th cipher letter is
deciphered as plaintext letter r
0 otherwise
keypq 1 if decipherment key maps plaintext letter p to ciphertext letter q
0 otherwise
minimize:?n?1
i=1
?
p,r
linkipr ? ?log P (r|p) (2-gram probability of chosen plaintext)
subject to:
for all p:
?
r
keypr = 1 (each plaintext letter maps to exactly one ciphertext letter)
for all p:
?
r
keyrp = 1 (each ciphertext letter maps to exactly one plaintext letter)
key = 1 (cipher space character maps to plain space character)
for (i=1...n-2), for all r: [
?
p
linkipr =
?
p
link(i+1)rp ] (chosen links form a left-to-right path)
for (i=1...n-1), for all p:
?
r
linkirp = keypci+1 (chosen links are consistent with chosen key)
Figure 2: Summary of how to build an integer program for any given ciphertext c1...cn. Solving the integer program
will yield the decipherment of highest probability.
we illustrate some automatic decipherments with er-
ror rates:
42% error: the avelage ongrichman hal cy wiof a
sevesonme qus antizexty that he buprk lathes we blung
than soment - fotes mmasthes
11% error: the average englishman has so week a
reference for antiality that he would rather be prong than
recent - deter quarteur
2% error: the average englishman has so keep a
reference for antiquity that he would rather be wrong than
recent - peter mcarthur
0% error: the average englishman has so deep a
reverence for antiquity that he would rather be wrong
than recent - peter mcarthur
Figure 3 shows our automatic decipherment re-
sults. We note that the solution method is exact, not
heuristic, so that decipherment error is not due to
search error. Our use of global key constraints also
leads to accuracy that is superior to (Knight et al,
2006). With a 2-gram model, their EM algorithm
gives 10% error for a 414-letter cipher, while our
method provides a solution with only 0.5% error.
At shorter cipher lengths, we observe much higher
improvements when using our method. For exam-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
De
cip
he
rm
en
t E
rro
r (
%)
Cipher Length (letters)2 4 8 16 32 64 128 256 512 1024
1-gram
2-gram
3-gram
Figure 3: Average decipherment error using integer pro-
gramming vs. cipher length, for 1-gram, 2-gram and 3-
gram models of English. Error bars indicate 95% confi-
dence intervals.
ple, on a 52-letter textbook cipher, using a 2-gram
model, the solution from our method resulted in 21%
error as compared to 85% error given by the EM so-
lution.
We see that deciphering with 3-grams works well
on ciphers of length 64 or more. This confirms
816
that such ciphers can be attacked with very limited
knowledge of English (no words or grammar) and
little custom programming.
The 1-gram model works badly in this scenario,
which is consistent with Bauer?s (2006) observation
that for short texts, mechanical decryption on the ba-
sis of individual letter frequencies does not work. If
we had infinite amounts of ciphertext and plaintext
drawn from the same stochastic source, we would
expect the plain and cipher frequencies to eventually
line up, allowing us to read off a correct key from the
frequency tables. The upper curve in Figure 3 shows
that convergence to this end is slow.
5 Shannon Equivocation and Unicity
Distance
Very short ciphers are hard to solve accurately.
Shannon (1949) pinpointed an inherent difficulty
with short ciphers, one that is independent of the so-
lution method or language model used; the cipher
itself may not contain enough information for its
proper solution. For example, given a short cipher
like XY Y X , we can never be sure if the answer is
peep, noon, anna, etc. Shannon defined a mathemat-
ical measure of our decipherment uncertainty, which
he called equivocation (now called entropy).
Let C be a cipher, M be the plaintext message it
encodes, and K be the key by which the encoding
takes place. Before even seeing C, we can compute
our uncertainty about the key K by noting that there
are 26! equiprobable keys:4
H(K) = ?(26!) ? (1/26!) ? log2 (1/26!)
= 88.4 bits
That is, any secret key can be revealed in 89 bits.
When we actually receive a cipher C, our uncer-
tainty about the key and the plaintext message is re-
duced. Shannon described our uncertainty about the
plaintext message, letting m range over all decipher-
ments:
H(M |C) = equivocation of plaintext message
= ?
?
m
P (m|C) ? log2 P (m|C)
4(Shannon, 1948) The entropy associated with a set of pos-
sible events whose probabilities of occurrence are p1, p2, ..., pn
is given by H = ?
?n
i=1 pi ? log2(pi).
P (m|C) is probability of plaintext m (according
to the language model) divided by the sum of proba-
bilities of all plaintext messages that obey the repeti-
tion pattern of C. While integer programming gives
us a method to find the most probable decipherment
without enumerating all keys, we do not know of a
similar method to compute a full equivocation with-
out enumerating all keys. Therefore, we sample up
to 100,000 plaintext messages in the neighborhood
of the most probably decipherment5 and compute
H(M |C) over that subset.6
Shannon also described H(K|C), the equivoca-
tion of key. This uncertainty is typically larger than
H(M |C), because a given message M may be de-
rived from C via more than one key, in case C does
not contain all 26 letters of the alphabet.
We compute H(K|C) by letting r(C) be the
number of distinct letters in C, and letting q(C) be
(26 ? r(C))!. Letting i range over our sample of
plaintext messages, we get:
H(K|C) = equivocation of key
= ?
?
i
q(C) ? (P (i)/q(C)) ? log2 (P (i)/q(C))
= ?
?
i
P (i) ? log2 (P (i)/q(C))
= ?
?
i
P (i) ? (log2 P (i)? log2 q(C))
= ?
?
i
P (i) ? log2 P (i) +
?
i
P (i) ? log2 q(C)
= H(M |C) + log2 q(C)
Shannon (1949) used analytic means to roughly
sketch the curves for H(K|C) and H(M |C), which
we reproduce in Figure 4. Shannon?s curve is drawn
for a human-level language model, and the y-axis is
given in ?decimal digits? instead of bits.
5The sampling used to compute H(M |C) starts with the
optimal key and expands out a frontier, by swapping letters in
the key, and recursing to generate new keys (and corresponding
plaintext message decipherments). The plaintext messages are
remembered so that the frontier expands efficiently. The sam-
pling stops if 100,000 different messages are found.
6Interestingly, as we grow our sample out from the most
probable plaintext, we do not guarantee that any intermediate
result is a lower bound on the equivocation. An example is pro-
vided by the growing sample (0.12, 0.01, 0.01, 0.01, 0.01, 0.01,
0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01), whose entropy steadily
increases. However, if we add a 14th item whose P (m) is 0.12,
the entropy suddenly decreases from 2.79 to 2.78.
817
Unicity Distance
Key Equivocation
Message Equivocation
Figure 4: Equivocation for simple substitution on English (Shannon, 1949).
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0  10  20  30  40  50  60  70  80  90  100
 110
 120
 130
 140
 150
 160
 170
 180
 190
 200
 210
 220
 230
 240
 250
 260
Eq
uiv
oc
ati
on
 of
 ke
y (
bit
s)
Cipher Length
1-gram
2-gram
3-gram
Figure 5: Average key equivocation observed (bits) vs.
cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
For comparison, we plot in Figures 5 and 6 the av-
erage equivocations as we empirically observe them
using our 1-, 2-, and 3-gram language models.
The shape of the key equivocation curve follows
Shannon, except that it is curved from the start,
rather than straight.
The message equivocation curve follows Shan-
non?s prediction, rising then falling. Because very
short ciphers have relatively few solutions (for ex-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0  10  20  30  40  50  60  70  80  90  100
 110
 120
 130
 140
 150
 160
 170
 180
 190
 200
 210
 220
 230
 240
 250
 260
Eq
uiv
oc
ati
on
 of
 m
es
sa
ge
 (b
its
)
Cipher Length
1-gram
2-gram
3-gram
Figure 6: Average message equivocation observed (bits)
vs. cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
ample, a one-letter cipher has only 26), the overall
uncertainty is not that great.7 As the cipher gets
longer, message equivocation rises. At some point,
it then decreases, as the cipher begins to reveal its
secret through patterns of repetition.
Shannon?s analytic model also predicts a sharp
decline of message equivocation towards zero. He
7Uncertainty is only loosely related to accuracy?even if we
are quite certain about a solution, it may still be wrong.
818
defines the unicity distance (U ) as the cipher length
at which we have virtually no more uncertainty
about the plaintext. Using analytic means (and vari-
ous approximations), he gives:
U = H(K)/(A?B)
where:
A = bits per character of a 0-gram model (4.7)
B = bits per character of the model used to decipher
For a human-level language model (B ? 1.2), he
concludes U ? 25, which is confirmed by practice.
For our language models, the formula gives:
U = 173 (1-gram)
U = 74 (2-gram)
U = 50 (3-gram)
These numbers are in the same ballpark as
Bauer (2006), who gives 167, 74, and 59. We note
that these predicted unicity distances are a bit too
rosy, according to our empirical message equivoca-
tion curves. Our experience confirms this as well, as
1-gram frequency counts over a 173-letter cipher are
generally insufficient to pin down a solution.
6 Conclusion
We provide a method for deciphering letter substi-
tution ciphers with low-order models of English.
This method, based on integer programming, re-
quires very little coding and can perform an opti-
mal search over the key space. We conclude by not-
ing that English language models currently used in
speech recognition (Chelba and Jelinek, 1999) and
automated language translation (Brants et al, 2007)
are much more powerful, employing, for example,
7-gram word models (not letter models) trained on
trillions of words. Obtaining optimal keys accord-
ing to such models will permit the automatic deci-
pherment of shorter ciphers, but this requires more
specialized search than what is provided by gen-
eral integer programming solvers. Methods such
as these should also be useful for natural language
decipherment problems such as character code con-
version, phonetic decipherment, and word substitu-
tion ciphers with applications in machine translation
(Knight et al, 2006).
7 Acknowledgements
The authors wish to gratefully acknowledge
Jonathan Graehl, for providing a proof to support
the argument that taking a larger number of samples
does not necessarily increase the equivocation. This
research was supported by the Defense Advanced
Research Projects Agency under SRI International?s
prime Contract Number NBCHD040058.
References
Friedrich L. Bauer. 2006. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer-Verlag.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Ciprian Chelba and Frederick Jelinek. 1999. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB: 4th International Conference on
Applications of Natural Language to Information Sys-
tems.
Ravi Ganesan and Alan T. Sherman. 1993. Statistical
techniques for language recognition: An introduction
and guide for cryptanalysts. Cryptologia, 17(4):321?
366.
David Graff and Rebecca Finch. 1994. Multilingual text
resources at the linguistic data consortium. In Pro-
ceedings of the HLT Workshop on Human Language
Technology.
Thomas Jakobsen. 1995. A fast method for cryptanalysis
of substitution ciphers. Cryptologia, 19(3):265?274.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the COLING/ACL.
Edwin Olson. 2007. Robust dictionary attack of short
simple substitution ciphers. Cryptologia, 31(4):332?
342.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Comm. ACM, 22(11):598?605.
Alexander Schrijver. 1998. Theory of Linear and Integer
Programming. John Wiley & Sons.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379?423 and 623?656.
Claude E. Shannon. 1949. Communication theory
of secrecy systems. Bell System Technical Journal,
28:656?715.
819
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887?896,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic Prediction of Parser Accuracy
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Radu Soricut
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, California 90292
rsoricut@languageweaver.com
Abstract
Statistical parsers have become increasingly
accurate, to the point where they are useful in
many natural language applications. However,
estimating parsing accuracy on a wide variety
of domains and genres is still a challenge in
the absence of gold-standard parse trees.
In this paper, we propose a technique that au-
tomatically takes into account certain charac-
teristics of the domains of interest, and ac-
curately predicts parser performance on data
from these new domains. As a result, we have
a cheap (no annotation involved) and effective
recipe for measuring the performance of a sta-
tistical parser on any given domain.
1 Introduction
Statistical natural language parsers have recently
become more accurate and more widely available.
As a result, they are being used in a variety of
applications, such as question answering (Herm-
jakob, 2001), speech recognition (Chelba and Je-
linek, 1998), language modeling (Roark, 2001), lan-
guage generation (Soricut, 2006) and, most notably,
machine translation (Charniak et al, 2003; Galley et
al., 2004; Collins et al, 2005; Marcu et al, 2006;
Huang et al, 2006; Avramidis and Koehn, 2008).
These applications are employed on a wide range of
domains and genres, and therefore the question of
how accurate a parser is on the domain and genre of
interest becomes acute. Ideally, one would want to
have available a recipe for precisely answering this
question: ?given a parser and a particular domain of
interest, how accurate are the parse trees produced??
The only recipe that is implicitly given in the large
literature on parsing to date is to have human anno-
tators build parse trees for a sample set from the do-
main of interest, and consequently use them to com-
pute a PARSEVAL (Black et al, 1991) score that is
indicative of the intrinsic performance of the parser.
Given the wide range of domains and genres for
which NLP applications are of interest, combined
with the high expertise required from human anno-
tators to produce parse tree annotations, this recipe
is, albeit precise, too expensive. The other recipe
that is currently used on a large scale is to measure
the performance of a parser on existing treebanks,
such as WSJ (Marcus et al, 1993), and assume that
the accuracy measure will carry over to the domains
of interest. This recipe, albeit cheap, cannot provide
any guarantee regarding the performance of a parser
on a new domain, and, as experiments in this paper
show, can give wrong indications regarding impor-
tant decisions for the design of NLP systems that
use a syntactic parser as an important component.
This paper proposes another method for measur-
ing the performance of a parser on a given domain
that is both cheap and effective. It is a fully auto-
mated procedure (no expensive annotation involved)
that uses properties of both the domain of interest
and the domain on which the parser was trained in
order to measure the performance of the parser on
the domain of interest. It is, in essence, a solution to
the following prediction problem:
Input: (1) a statistical parser and its training data,
(2) some chunk of text from a new domain or genre
Output: an estimate of the accuracy of the parse
trees produced for that chunk of text
887
Accurate estimations for this prediction problem
will allow a system designer to make the right de-
cisions for the given domain of interest. Such deci-
sions include, but are not restricted to, the choice of
the parser, the choice of the training data, the choice
of how to implement various components such as the
treatment of unknown words, etc. Altogether, a cor-
rect estimation of the impact of such decisions on the
resulting parse trees can guide a system designer in a
hill-climbing scenario for which an extrinsic metric
(such as the impact on the overall quality of the sys-
tem) is usually too expensive to be employed often
enough. To provide an example, a machine transla-
tion engine that requires parse trees as training data
in order to learn syntax-based translation rules (Gal-
ley et al, 2006) needs to employ a syntactic parser
as soon as the training process starts, but it can take
up to hundreds and even thousands of CPU hours
(for large training data sets) to train the engine be-
fore translations can be produced and measured. Al-
though a real estimate of the impact of a parser de-
sign decision in this scenario can only be gauged
from the quality of the translations produced, it is
impractical to create such estimates for each design
decision. On the other hand, estimates using the so-
lution proposed in this paper can be obtained fast,
before submitting the parser output to a costly train-
ing procedure.
2 Related Work and Experimental
Framework
There have been previous studies which explored the
problem of automatically predicting the task diffi-
culty for various NLP applications. (Albrecht and
Hwa, 2007) presented a regression based method
for developing automatic evaluation metrics for ma-
chine translation systems without directly relying on
human reference translations. (Hoshino and Nak-
agawa, 2007) built a computer-adaptive system for
generating questions to teach English grammar and
vocabulary to students, by predicting the difficulty
level of a question using various features. There
have been a few studies of English parser accuracy
in domains/genres other than WSJ (Gildea, 2001;
Bacchiani et al, 2006; McClosky et al, 2006), but
in order to make measurements for such studies, it
is necessary to have gold-standard parses in the non-
WSJ domain of interest.
Gildea (2001) studied how well WSJ-trained
parsers do on the Brown corpus, for which a gold
standard exists. He looked at sentences with 40
words or less. (Bacchiani et al, 2006) carried out
a similar experiment on sentences of all lengths,
and (McClosky et al, 2006) report additional re-
sults. The table below shows results from our own
measurements of Charniak parser1 (Charniak and
Johnson, 2005) accuracy (F-measure on sentences of
all lengths), which are consistent with these studies.
For the Brown corpus, the test set was formed from
every tenth sentence in the corpus (Gildea, 2001).
Training Set Test Set Sent.
count
Charniak
accuracy
WSJ sec. 02-21 WSJ sec. 24 1308 90.48
(39,832 sent.) WSJ sec. 23 2343 91.13
Brown-test 2186 86.34
Here we investigate algorithms for predicting the
accuracy of a parser P on sentences, chunks of sen-
tences, and whole corpora. We also investigate and
contrast several scenarios for prediction: (1) the pre-
dictor looks at the input text only, (2) the predictor
looks at the input text and the output parse trees of
P , and (3) the predictor looks at the input text, the
output parse trees of P , and the outputs of other pro-
grams, such as the output parse trees of a different
parser Pref used as a reference. Under none of these
scenarios is the predictor allowed to look at gold-
standard parses in the new domain/genre.
The intuition behind what we are trying to achieve
here can be compared to an analogous task?trying
to assess the performance of a median student from
a math class on a given test, without having access to
the answer sheet. Looking at the test only, we could
probably tell whether the test looks hard or not, and
therefore whether the student will do well or not.
Looking at the student?s answers will likely give us
an even better idea of the performance. Finally, the
answers of a second student with similar proficiency
will provide even better clues: if the students agree
on every answer, then they probably both did well,
but if they disagree frequently, then they (and hence
our student) probably did not do as well.
Our first experiments are concerned with validat-
ing the idea itself: can a predictor be trained such
1Downloaded from ftp.cs.brown.edu/pub/nlparser/reranking-
parserAug06.tar.gz in February, 2007.
888
that it predicts the same F-scores as the ones ob-
tained using gold-trees? We first validate this using
the WSJ corpus itself, by dividing the WSJ treebank
into several sections:
1. Training (WSJ section 02-21). The parser P is
trained on this data.
2. Development (WSJ section 24). We use this
data for training our predictor.
3. Test (WSJ section 23). We use this data for
measuring our predictions. For each test sentence,
we compute (1) the PARSEVAL F-measure score
using the test gold standard, and (2) our predicted
F-measure. We report the correlation coefficient (r)
between the actual F-scores and our predicted F-
scores. We will also use a root-mean-square error
(rms error) metric to compare actual and predicted
F-scores.
Section 3 describes the features used by our pre-
dictor. Given these features, as well as actual
F-scores computed for the development data, we
use supervised learning to set the feature weights.
To this end, we use SVM-Regression2 (Smola and
Schoelkopf, 1998) with an RBF kernel, to learn the
feature weights and build our predictor system.3 We
validate the accuracy of the predictor trained in this
fashion on both WSJ (Section 4) and the Brown cor-
pus (Section 5).
3 Features Used for Predicting Parser
Accuracy
3.1 Text-based Features
One hypothesis we explore is that (all other things
being equal) longer sentences are harder to parse
correctly than shorter sentences. When exposed
to the development set, SVM-Regression learns
weights to best predict F-scores using the values for
this feature corresponding to each sentence in the
corpus.
Does the predicted F-score correlate with actual
F-score on a sentence by sentence basis? There was
a positive but weak correlation:
2Weka software (http://www.cs.waikato.ac.nz/ml/weka/)
3We compared a few regression algorithms like SVM-
Regression (using different kernels and parameter settings) and
Multi-Layer Perceptron (neural networks) ? we trained the al-
gorithms separately on dev data and picked the one that gave
the best cross-validation accuracy (F-measure).
Feature set dev (r) test (r)
Length 0.13 0.19
Another hypothesis is that the parser performance
is influenced by the number of UNKNOWN words
in the sentence to be parsed, i.e., the number of
words in the test sentence that were never seen be-
fore in the training set. Training the predictor with
this feature produces a positive correlation, slightly
weaker compared to the Length feature.
Feature set dev (r) test (r)
UNK 0.11 0.11
Unknown words are not the only ones that can in-
fluence the performance of a parser. Rare words,
for which statistical models do not have reliable es-
timates, are also likely to impact parsing accuracy.
To test this hypothesis, we add a language model
perplexity?based (LM-PPL) feature. We extract the
yield of the training trees, on which we train a tri-
gram language model.4 We compute the perplexity
of each test sentence with respect to this language
model, and use it as feature in our predictor system.
Note that this feature is meant as a refinement of the
previous UNK feature, in the sense that perplexity
numbers are meant to signal the occurrence of un-
known words, as well as rare (from the training data
perspective) words. However, the correlation we ob-
serve for this feature is similar to the correlation ob-
served for the UNK feature, which seems to suggest
that the smoothing techniques used by the parsers
employed in these experiments lead to correct treat-
ment of the rare words.
Feature set dev (r) test (r)
LM-PPL 0.11 0.10
We also look at the possibility of automatically
detecting certain ?cue? words that are appropriate
for our prediction problem. That is, we want to see
if we can detect certain words that have a discrimi-
nating power in deciding whether parsing a sentence
that contains them is difficult or easy. To this end,
we use a subset of the development data, which con-
tains the 200 best-parsed and 200 worst-parsed sen-
tences (based on F-measure scores). For each word
in the development dataset, we compute the infor-
mation gain (IG) (Yang and Pedersen, 1997) score
for that word with respect to the best/worst parsed
4We trained using the SRILM language modeling toolkit,
with default settings.
889
dataset. These words are then ranked by their IG
scores, and the top 100 words are included as lex-
ical features in our predictor system. As expected,
the correlation on the development set is quite high
(given that these lexical cues are extracted from this
particular set), but a positive correlation holds for
the test set as well.
Feature set dev (r) test (r)
lexCount100 0.43 0.18
3.2 Parser P?based Features
Besides exploiting the information present in the in-
put text, we can also inspect the output tree of the
parser P for which we are interested in predicting
accuracy. We create a rootSYN feature based on
the syntactic category found at the root of the out-
put tree (?is it S??, ?is it FRAG??). We also create
a puncSYN feature based on the number of words
labeled as punctuation tags (based on the intuition
that heavy use of punctuation can be indicative of
the difficulty of the input sentences), and a label-
SYN feature in which we bundled together informa-
tion regarding the number of internal nodes in the
parse tree output that have particular labels (?how
many nodes are labeled with PP??). In our predictor,
we use 72 such labelSYN features corresponding to
all the syntactic labels found in the parse tree out-
put for the development set. The test set correlation
given by the rootSYN and the labelSYN features are
higher than some of the text-based features, whereas
the puncSYN feature seems to have little discrimi-
native power.
Feature set dev (r) test (r)
rootSYN 0.21 0.17
puncSYN 0.09 0.01
labelSYN 0.33 0.28
3.3 Reference Parser Pref ?based Features
In addition to the text-based features and parser P?
based features, we can bring in an additional parser
Pref whose output is used as a reference against
which the output of parser P is measured. For the
reference parser feature, our goal is to measure how
similar/different are the results from the two parsers.
We find that if the parses are similar, they are more
likely to be right. In order to compute similarity, we
can compare the constituents in the two parse trees
from P and Pref , and see how many constituents
match. This is most easily accomplished by consid-
ering Pref to be a ?gold standard? (even though it is
not necessarily a correct parse) and computing the
F-measure score of parser P against Pref . We use
this F-measure score as a feature for prediction.
For the experiments presented in this section we
use as Pref , the parser from (Bikel, 2002). Intu-
itively, the requirement for choosing parser Pref in
conjunction with parser P seems to be that they
are different enough to produce non-identical trees
when presented with the same input, and at the
same time to be accurate enough to produce reli-
able parse trees. The choice of P as (Charniak and
Johnson, 2005) and Pref as (Bikel, 2002) fits this
bill, but many other choices can be made regarding
Pref , such as (Klein and Manning, 2003; Petrov and
Klein, 2007; McClosky et al, 2006; Huang, 2008).
We leave the task of creating features based on the
consensus of multiple parsers as future work.
The correlation given by the reference parser?
based feature Pref on the test set is the highest
among all the features we explored.
Feature set dev (r) test (r)
Pref 0.40 0.36
3.4 The Aggregated Power of Features
The table below lists all the individual features we
have described in this section, sorted according to
the correlation value obtained on the test set.
Feature set dev (r) test (r)
Pref 0.40 0.36
labelSYN 0.33 0.28
lexCount500 0.56 0.23
lexBool500 0.58 0.20
lexCount1000 0.67 0.20
lexBool1000 0.58 0.20
Length 0.13 0.19
lexCount100 0.43 0.18
lexBool100 0.43 0.18
rootSYN 0.21 0.17
UNK 0.11 0.11
LM-PPL 0.11 0.10
puncSYN 0.09 0.01
Note how the lexical features tend to over-fit the
development data?the words were specifically cho-
sen for their discriminating power on that particular
set. Hence, adding more lexical features to the pre-
dictor system improves the correlation on develop-
ment (due to over-fitting), but it does not produce
consistent improvement on the test set. However,
890
Method (using 3 features:
Length, UNK, Pref )
# of random
restarts
dev (r)
SVM Regression 0.42
1 0.138
5 0.136
Maximum Correlation 10 0.166
Training (MCT) 25 0.178
100 0.232
1000 0.27
10,000 0.401
Table 1: Comparison of correlation (r) obtained using MCT versus
SVM-Regression on development corpus.
there is some indication that the counts of the lex-
ical features are important, and count-based lexical
features tend to have similar or better performance
compared to their boolean-based counterparts.
Since these features measure different but over-
lapping pieces of the information available, it is to
be expected that some of the feature combinations
would provide better correlation that the individual
features, but the gains are not strictly additive. By
taking the individual features that provide the best
discriminative power, we are able to get a correla-
tion score of 0.42 on the test set.
Feature set dev (r) test (r)
Pref + labelSYN + Length + lexCount100 +
rootSYN + UNK + LM-PPL
0.55 0.42
3.5 Optimizing for Maximum Correlation
If our goal is to obtain the highest correlations
with the F-score measure, is SVM regression the
best method? Liu and Gildea (2007) recently in-
troduced Maximum Correlation Training (MCT), a
search procedure that follows the gradient of the for-
mula for correlation coefficient (r). We implemented
MCT, but obtained no better results. Moreover, it
required many random re-starts just to obtain results
comparable to SVM regression (Table 1).
4 Predicting Accuracy on Multiple
Sentences
The results for the scenario presented in Section 3
are encouraging, but other scenarios are also im-
portant from a practical perspective. For instance,
we are interested in predicting the performance of a
particular parser not on a sentence-by-sentence ba-
sis, but for a representative chunk of sentences from
the new domain. In order to predict the F-measure
on multiple sentences, we modify our feature set to
generate information on a whole chunk of sentences
Sentences in
chunk (n)
WSJ-test (r) WSJ-test
(rms error)
1 0.42 0.098
20 0.61 0.026
50 0.62 0.019
100 0.69 0.015
500 0.79 0.011
Table 2: Performance of predictor on n-sentence chunks from WSJ-test
(Correlation and rms error between actual/predicted accuracies).
rather than a single sentence. Predicting the corre-
lation at chunk level is, not unexpectedly, an eas-
ier problem than predicting correlation at sentence
level, as the results in the first two columns of Ta-
ble 2 show.
For 100-sentence chunks, we also plot the pre-
dicted accuracies versus actual accuracies for the
WSJ-test set in Figure 1. This scatterplot brings to
light an artifact of using correlation metric (r) for
evaluating our predictor?s performance. Although
our objective is to improve correlation between ac-
tual and predicted F-scores, the correlation metric (r)
does not tell us directly how well the predictor is
doing. In Figure 1, the system predicts that on
an average, most sentence chunks can be parsed
with an accuracy of 0.9085 (which is the mean pre-
dicted F-score on WSJ-test). But the range of pre-
dictions from our system [0.89,0.92] is smaller than
the actual F-score range [0.86,0.95]. Hence, even
though the correlation scores are high, this does not
necessarily mean that our predictions are on target.
An additional metric, root-mean-square (rms) error,
which measures the distance between actual and pre-
dicted F-measures, can be used to gauge the qual-
ity of our predictions. For a particular chunk-size,
lowering the rms error translates into aligning the
points of a scatterplot as the one in Figure 1, closer
to the x=y line, implying that the predictor is getting
better at exactly predicting the F-score values. The
third column in Table 2 shows the rms error for our
predictor at different chunk sizes. The results using
this metric also show that the prediction problem be-
comes easier as the chunk size increases.
Assuming that we have the test set of WSJ sec-
tion 23, but without the gold-standard trees, how
can we get an approximation for the overall accu-
racy of a parser P on this test set? One possibility,
which we use here as a baseline, is to compute the
F-score on a set for which we do have gold-standard
trees. If we use our development set (WSJ section
891
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95
Act
ual
 Ac
cur
acy
Predicted Accuracy
per-chunk-accuracy
x=y lineFitted-line
Figure 1: Plot showing Actual vs. Predicted accuracies for
WSJ-test (100-sentence chunks). Each plot point represents a
100-sentence chunk. (rms error = 0.015)
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95
Act
ual
 Ac
cur
acy
Predicted Accuracy
per-chunk-accuracy
x=y line
Figure 2: Plot showing Actual vs. Adjusted Predicted accu-
racies (shifting with ? = 0.757, skewing with ? = 1.0) for
WSJ-test (100-sentence chunks). (rms error = 0.014)
System F-measure
Charniak F-measure on WSJ-dev (baseline) 90.48 (fd)
Predictor (feature weights set with WSJ-dev) 90.85 (fp)
Actual Charniak accuracy 91.13 (ft)
Table 3: Comparing Charniak parser accuracy (from different systems)
on entire WSJ-test corpus
24) for this purpose, and (Charniak and Johnson,
2005) as the parser P , the baseline is an F-score of
90.48 (fd), which is the actual Charniak parser accu-
racy on WSJ section 24. Instead, if we run our pre-
dictor on the test set (a single chunk containing all
the sentences in the test set), it predicts an F-score
of 90.85 (fp). These two predictions are listed as
the first two rows in Table 3. Of course, having the
actual gold-standard trees for WSJ section 23 helps
us decide which prediction is better: the actual ac-
curacy of the Charniak parser on WSJ section 23 is
an F-score of 91.13 (ft), which makes our prediction
better than the baseline.
4.1 Shifting Predictions to Match Actual
Accuracy
We correctly predict (in Table 3) that the
WSJ-test is easier to parse than the WSJ-
dev (90.85 > 90.48). However, our predictor is too
conservative?the WSJ-test is actually even easier
to parse (91.13 > 90.85). We can fix this by shift-
ing the mean predicted F-score (which is equal to
fp) further away from the dev F-measure (fd), and
closer to the actual F-measure (ft). This is achieved
by shifting all the individual predictions by a certain
amount as shown below.
Let p be an individual prediction from our system.
The shifted prediction p? is given by:
p? = p+ ?(fp ? fd) (1)
We can tune ? to make the new mean predic-
tion (f ?p) to be equal to the actual F-measure (ft).
f ?p = fp + ?(fp ? fd) (2)
? =
ft ? fp
fp ? fd
(3)
Using the F-score values from Table 3, we get an
? = 0.757 and an exact prediction of 91.13. Of
course, this is because we tune on test, so we need
to validate this idea on a new test set to see if it leads
to improved predictions (Section 5).
4.2 Skewing to Widen Prediction Range
Our predictor is also too conservative about its dis-
tribution (see Figure 1). It knows (roughly) which
chunks are easier to parse and which are harder, but
its range of predictions is lower than the range of
actual F-measure scores.
We can skew individual predictions so that sen-
tences predicted to be easy are re-predicted to be
even easier (and those that are hard to be even
harder). For each prediction p? (from Equation 1),
we compute
p?? = p? + ?(p? ? f ?p) (4)
We simply set ? to 1.0, doubling the distance
of each prediction p? (in Equation 1) from the (ad-
justed) mean prediction f ?p, to obtain the skewed pre-
diction p??.
Figure 2 shows how the points representing 100-
sentence chunks in Figure 1 look after the predic-
tions have been shifted (? = 0.757) and skewed
(? = 1.0). These two operations have the desired
effect of changing the range of predictions from
[0.89,0.92] to [0.87,0.94], much closer to the actual
892
Sentences
in chunk
(n)
WSJ-test
(rms error)
Brown-test
Prediction
(rms error)
Brown-test
Adjusted
Prediction
(rms error)
1 0.098 0.129 0.139
20 0.026 0.039 0.036
50 0.019 0.032 0.029
100 0.015 0.025 0.020
500 0.011 0.038 0.024
Table 4: Performance of predictor on n-sentence chunks from WSJ-test
and Brown-test (rms error between actual/predicted accuracies).
range of [0.86,0.95]. The points in the new plot (Fig-
ure 2) also align closer to the ?x=y? line than in the
original graph (Figure 1). The rms error also drops
from 0.015 to 0.014 (7% relative reduction), show-
ing that the predictions have improved.
Since we use the WSJ-test corpus to tune the pa-
rameter values for shifting and skewing, we need to
apply our predictor on a different test set to see if we
get similar improvements by using these techniques,
which we do in the next section.
5 Predicting Accuracy on the Brown
Corpus
The Brown corpus represents a genuine challenge
for our predictor, as it presents us with the oppor-
tunity to test the performance of our predictor in
an out-of-domain scenario. Our predictor, trained
on WSJ data, is now employed to predict the per-
formance of a WSJ-trained parser P on the Brown-
test corpus. As in the previous experiments, we use
(Charniak and Johnson, 2005) trained on WSJ sec-
tions 02-21 as parser P . The feature weights for our
predictor are again trained on section 24 of WSJ, and
the shifting and skewing parameters (? = 0.757,
? = 1.0) are determined using section 23 of WSJ.
The results on the Brown-test, both the origi-
nal predictions and after they have been adjusted
(shifted/skewed), are shown in Table 4, at different
level of chunking. For chunks of size n > 1, the
shifting and skewing techniques help in lowering the
rms error. On 100-sentence chunks from the Brown
test, shifting and skewing (? = 0.757, ? = 1.0)
leads to a 20% relative reduction in the rms error.
In a similar vein with the evaluation done in Sec-
tion 4, we are interested in estimating the overall ac-
curacy of a WSJ-trained parser P given an out-of-
domain set such as the Brown test set (for which, at
least for now, we do not have access to gold-standard
System F-measure
Baseline1 (F-measure on WSJ sec. 23) 91.13
Baseline2 (F-measure on WSJ sec. 24) 90.48
Predictor (base) 88.48
Adjusted Predictor (shifting using ? = 0.757) 86.96
Actual accuracy 86.34
Table 5: Charniak parser accuracy on entire Brown-test corpus
trees). If we use (Charniak and Johnson, 2005) as
parser P , a cheap and readily-available answer is
to approximate the performance using the Charniak
parser performance on WSJ section 23, which has
an F-score of 91.13. Another cheap and readily-
available answer is to take the Charniak parser per-
formance on WSJ section 24 with an F-score of
90.48. Table 5 lists these baselines, along with the
prediction made by our system when using a single
chunk containing all the sentences in the Brown test
set (both base predictions and adjusted predictions,
i.e. shifting using ? = 0.757). Again, having gold-
standard trees for the Brown test set helps us decide
which prediction is better. Our predictions are much
closer to the actual Charniak parser performance on
the Brown-test set, with the adjusted prediction at
86.96 compared to the actual F-score of 86.34.
6 Ranking Parser Performance
One of the main goals for computing F-score figures
(either by traditional PARSEVAL evaluation against
gold standards or by methods such as the one pro-
posed in this paper) is to compare parsing accu-
racy when confronted with a choice between vari-
ous parser deployments. Not only are there many
parsing techniques available (Collins, 2003; Char-
niak and Johnson, 2005; Petrov and Klein, 2007;
McClosky et al, 2006; Huang, 2008), but recent
annotation efforts in providing training material for
statistical parsing (LDC, 2005; LDC, 2006a; LDC,
2006b; LDC, 2006c; LDC, 2007) have compounded
the difficulty of the choices (?Do I parse using parser
X??, ?Do I train parser X using the treebank Y or
Z??). In this section, we show how our predictor can
provide guidance when dealing with some of these
choices, namely the choice of the training material
to use with a statistical parser, prior to its applica-
tion in an NLP task.
For the experiments reported in this paper, we
use as parser P , our in-house implementation of
the Collins parser (Collins, 2003), to which various
893
speed-related enhancements (Goodman, 1997) have
been applied. This choice has been made to better
reflect a scenario in which parser P would be used
in a data-intensive application such as syntax-driven
machine translation, in which the parser must be
able to run through hundreds of millions of training
words in a timely manner. We use the more accurate,
but slower Charniak parser (Charniak and Johnson,
2005) as the reference parser Pref in our predictor
(see Section 3.3). In order to predict the Collins-
style parser behavior on the ranking task, we use the
same predictor model (including feature weights and
adjustment parameters) that was used for predicting
Charniak parser behavior on the Brown corpus (Sec-
tion 5).
We compare three training scenarios that make for
three different parsers:
(1) PWSJ - trained on sections 02-21 of WSJ.
(2) PNews - trained on the union of the English
Chinese Translation Treebank (LDC, 2007) (news
stories from Xinhua News Agency translated from
Chinese into English) and the English Newswire
Translation Treebank (LDC, 2005; LDC, 2006a;
LDC, 2006b; LDC, 2006c) (An-Nahar new stories
translated from Arabic into English).
(3) PWSJ?News - trained on the union of all the
above training material.
When comparing the performance of these three
parsers on a development set from WSJ (section 0),
we get the following F-scores.5
Parser WSJ (sec. 0) Accuracy
(F-scores)
PWSJ 88.25
PNews 83.00
PWSJ?News 88.00
Consider now that we are interested in compar-
ing the parsing accuracy of these parsers on a do-
main completely different from WSJ. The ranking
PWSJ>PWSJ?News>PNews, given by the evalua-
tion above, provides some guidance, but is this guid-
ance accurate? The intuition here is that the in-
formation that we already have about the new do-
main of interest (which implicitly appears in texts
5Because of tokenization differences between the different
treebanks involved in these experiments, we have to adopt a to-
kenization scheme different from the one used in the Penn Tree-
bank, and therefore the F-scores, albeit in the same range, are
not directly comparable with the ones in the parsing literature.
Parser Xinhua News
Prediction
(F-scores)
Xinhua News
Accuracy
(F-scores)
PWSJ 85.1 79.14
PNews 87.0 84.84
PWSJ?News 89.4 85.14
Table 6: Performance of predictor on the Xinhua News domain, com-
pared with actual F-scores.
extracted from this domain), can be used to bet-
ter guide this decision. Our predictor is able to
capitalize on this information, and provide domain-
informed guidance for choosing the most accurate
parser to use with the new data, which in this case
relates to choosing the best training strategy for the
parser P . If we consider as our domain of interest,
news stories from Xinhua News Agency, then using
our predictor on a chunk of 1866 sentences from this
domain gives the F-scores shown in the second col-
umn of Table 6.
As with the previous experiments, we can com-
pute the actual PARSEVAL F-scores (using gold-
standard) for this particular 1866-sentence test set,
as it happens to be part of the English Chinese Trans-
lation Treebank (LDC, 2007). These F-score fig-
ures are shown in the third column of Table 6. As
these results show, for this particular domain the cor-
rect ranking is PWSJ?News>PNews>PWSJ , which
is exactly the ranking predicted by our method, with-
out the aid of gold-standard trees.
We observe that even though the system predicts
the ranking correctly, the predictions in the Xinhua
News domain might not be as accurate in compar-
ison to the predictions on Brown corpus (predicted
F-score = 86.96, actual F-score = 86.34). One pos-
sible reason for this lower accuracy is that we use
the same prediction model without optimizing for
the particular parser on which we wish to make pre-
dictions. Still, the model was able to make distinc-
tions between multiple parsers for the ranking task
correctly, and decide the best parser to use with the
given data. We believe this to be useful in typical
NLP applications which use parsing as a component,
and where making the right choice between differ-
ent parsers can affect the end-to-end accuracy of the
system.
7 Conclusion
The steady advances in statistical parsing over the
last years have taken this technology to the point
894
where it is accurate enough to be useful in a va-
riety of natural language applications. However,
due to large variations in the characteristics of the
domains for which these applications are devel-
oped, estimating parsing accuracy becomes more
involved than simply taking for granted accuracy
estimates done on a certain well-studied domain,
such as WSJ. As the results in this paper show, it
is possible to take into account these variations in
the domain characteristics (encoded in our predictor
as text-based, syntax-based, and agreement-based
features)?to make better predictions about the ac-
curacy of certain statistical parsers (and under dif-
ferent training scenarios), instead of relying on accu-
racy estimates done on a standard domain. We have
provided a mechanism to incorporate these domain
variations for making predictions about parsing ac-
curacy, without the costly requirement of creating
human annotations for each of the domains of inter-
est. The experiments shown in the paper were lim-
ited to readily available statistical parsers (which are
widely deployed in a number of applications), and
certain domains/genres (because of ready access to
gold-standard data on which we could verify predic-
tions). However, the features we use in our predic-
tor are independent of the particular type of parser
or domain, and the same technique could be applied
for making predictions on other parsers as well.
There are many avenues for future work opened
up by the work presented here. The accuracy of the
predictor can be further improved by incorporating
more complex syntax-based features and multiple-
agreement features. Moreover, rather than predict-
ing an intrinsic metric such as the PARSEVAL F-
score, the metric that the predictor learns to pre-
dict can be chosen to better fit the final metric on
which an end-to-end system is measured, in the style
of (Och, 2003). The end-result is a finely-tuned tool
for predicting the impact of various parser design de-
cisions on the overall quality of a system.
8 Acknowledgements
We wish to acknowledge our colleagues at ISI, who
provided useful suggestions and constructive criti-
cism on this work. We are also grateful to all the
reviewers for their detailed comments. This work
was supported in part by NSF grant IIS-0428020.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references.
In Proc. of ACL.
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proc. of ACL.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech & Language, 20(1).
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Proc.
of HLT.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cover-
age of english grammars. In Proc. of Speech and Nat-
ural Language Workshop.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of ACL.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proc. of MT Summit IX. IAMT.
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting
syntactic structure for language modeling. In Proc. of
ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4).
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. of
ACL.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proc. of EMNLP.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proc. of ACL Work-
shop on Open-Domain Question Answering.
Ayako Hoshino and Hiroshi Nakagawa. 2007. A cloze
test authoring system and its automation. In Proc. of
ICWL.
895
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. of ACL.
LDC. 2005. English newswire translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2005E85.
LDC. 2006a. English newswire translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2006E36.
LDC. 2006b. GALE Y1 Q3 release - English translation
treebank. Linguistic Data Consortium, Catalog num-
ber LDC2006E82.
LDC. 2006c. GALE Y1 Q4 release - English translation
treebank. Linguistic Data Consortium, Catalog num-
ber LDC2006E95.
LDC. 2007. English chinese translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2007T02.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In Proc. of NAACL-HLT.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phraases. In
Proc. of EMNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2).
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proc. of COLING-ACL.
Franz Joseph Och. 2003. Minimum error rate training in
machine translation. In Proc. of ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of HLT/NAACL.
Brian Roark. 2001. Probabilistic top-down parsing
and language modelling. Computational Linguistics,
27(2).
A.J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Report
NC2-TR-1998-030.
Radu Soricut. 2006. Natural Language Generation us-
ing an Information-Slim Representation. Ph.D. thesis,
University of Southern California,.
Y. Yang and J. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proc. of
ICML.
896
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 727?736,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Synchronous Tree Adjoining Machine Translation
Steve DeNeefe and Kevin Knight
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
{sdeneefe,knight}@isi.edu
Abstract
Tree Adjoining Grammars have well-known
advantages, but are typically considered too
difficult for practical systems. We demon-
strate that, when done right, adjoining im-
proves translation quality without becoming
computationally intractable. Using adjoining
to model optionality allows general translation
patterns to be learned without the clutter of
endless variations of optional material. The
appropriate modifiers can later be spliced in as
needed.
In this paper, we describe a novel method
for learning a type of Synchronous Tree Ad-
joining Grammar and associated probabilities
from aligned tree/string training data. We in-
troduce a method of converting these gram-
mars to a weakly equivalent tree transducer
for decoding. Finally, we show that adjoining
results in an end-to-end improvement of +0.8
BLEU over a baseline statistical syntax-based
MTmodel on a large-scale Arabic/EnglishMT
task.
1 Introduction
Statistical MT has changed a lot in recent years.
We have seen quick progress from manually
crafted linguistic models to empirically learned
statistical models, from word-based models to
phrase-based models, and from string-based mod-
els to tree-based models. Recently there is a swing
back to incorporating more linguistic information
again, but this time linguistic insight carefully
guides the setup of empirically learned models.
Shieber (2007) recently argued that proba-
bilistic Synchronous Tree Adjoining Grammars
(Shieber and Schabes, 1990) have the right com-
bination of properties that satisfy both linguists
and empirical MT practitioners. So far, though,
most work in this area has been either more lin-
guistic than statistical (Abeille et al, 1990) or
statistically-based, but linguistically light (Nesson
et al, 2006).
Current tree-based models that integrate lin-
guistics and statistics, such as GHKM (Galley et
al., 2004), are not able to generalize well from
a single phrase pair. For example, from the data
in Figure 1, GHKM can learn rule (a) to translate
nouns with two pre-modifiers, but does not gener-
alize to learn translation rules (b) - (d) without the
optional adjective or noun modifiers. Likewise,
none of these rules allow extra material to be intro-
duced, e.g. ?Pakistan?s national defense minister?.
In large enough training data sets, we see many
examples of all the common patterns, but the rarer
patterns have sparse statistics or poor coverage.
NP
JJ
national
NN
defense
NN
minister
wzyr AldfAE AlwTnY
(a)
NP
JJ
1
NN
2
NN
3
? NN
3
NN
2
JJ
1
(b)
NP
NN
1
NN
2
? NN
2
NN
1
(c)
NP
JJ
1
NN
2
? NN
2
JJ
1
(d)
NP
NN
1
? NN
1
Figure 1: Rule (a) can be learned from this training
example. Arguably, the more general rules (b) -
(d) should also be learnable.
To mitigate this problem, the parse trees used
as training data for these systems can be binarized
(Wang et al, 2007). Binarization allows rules with
partial constituents to be learned, resulting in more
general rules, richer statistics, and better phrasal
coverage (DeNeefe et al, 2007), but no principled
required vs. optional decision has been made. This
method?s key weakness is that binarization always
keeps adjacent siblings together, so there is no way
to group the head with a required complement if
optional information intervenes between the two.
Furthermore, if all kinds of children are consid-
ered equally optional, then we have removed im-
portant syntactic constraints, which may end up
permitting too much freedom. In addition, spu-
rious alignments may limit the binarization tech-
727
nique?s effectiveness.
In this paper, we present a method of learning
a type of probabilistic Synchronous Tree Adjoin-
ing Grammar (STAG) automatically from a cor-
pus of word-aligned tree/string pairs. To learn this
grammar we use linguistic resources to make the
required vs. optional decision. We then directly
model the optionality in the translation rules by
learning statistics for the required parts of the rule
independently from the optional parts. We also
present a method of converting these rules into a
well-studied tree transducer formalism for decod-
ing purposes. We then show that modeling option-
ality using adjoining results in a statistically sig-
nificant BLEU gain over our baseline syntax-based
model with no adjoining.
2 Translation Model
2.1 Synchronous Tree Insertion Grammars
Tree Adjoining Grammars (TAG), introduced by
Joshi et al (1975) and Joshi (1985), allow inser-
tion of unbounded amounts of material into the
structure of an existing tree using an adjunction
operation. Usually they also include a substitution
operation, which has a ?fill in the blank? seman-
tics, replacing a substitution leaf node with a tree.
Figure 2 visually demonstrates TAG operations.
Shieber and Schabes (1990) offer a synchronous
version of TAG (STAG), allowing the construc-
tion of a pair of trees in lockstep fashion using the
TAG operations of substitution and adjunction on
tree pairs. To facilitate this synchronous behav-
ior, links between pairs of nodes in each tree pair
define the possible sites for substitution and ad-
junction to happen. One application of STAG is
machine translation (Abeille et al, 1990).
One negative aspect of TAG is the compu-
tational complexity: O(n6) time is required
for monolingual parsing (and thus decoding),
and STAG requires O(n12) for bilingual parsing
(which might be used for training the model di-
rectly on bilingual data). Tree Insertion Grammars
(TIG) are a restricted form of TAG that was in-
troduced (Schabes and Waters, 1995) to keep the
same benefits as TAG (adjoining of unbounded
material) without the computational complexity?
TIG parsing is O(n3). This reduction is due to a
limitation on adjoining: auxiliary trees can only
introduce tree material to the left or the right of
the node adjoined to. Thus an auxiliary tree can
be classified by direction as left or right adjoining.
adjunction
NP
DT
the
NP
NN?
NP
JJ? NP*
substitution substitution
NN
minister
JJ
defense
=?
NP
DT
the
NP
JJ
defense
NP
NN
minister
Figure 2: TAG grammars use substitution and ad-
junction operations to construct trees. Substitu-
tion replaces the substitution node (marked with
?) with another tree. Adjunction inserts an aux-
iliary tree?a special kind of tree fragment with a
foot node (marked with *)?into an existing tree at
a permitted non-terminal node. Note that in TAG,
adjunctions are permitted at any non-terminal with
the same label as the root and foot node of the
auxiliary tree, while in STAG adjunctions are re-
stricted to linked sites.
Nesson et al (2006) introduce a probabilis-
tic, synchronous variant of TIG and demonstrate
its use for machine translation, showing results
that beat both word-based and phrase-based MT
models on a limited-vocabulary, small-scale train-
ing and test set. Training the model uses an
O(n
6
) bilingual parsing algorithm, and decoding
is O(n3). Though this model uses trees in the for-
mal sense, it does not create Penn Treebank (Mar-
cus et al, 1993) style linguistic trees, but uses only
one non-terminal label (X) to create those trees us-
ing six simple rule structures.
The grammars we use in this paper share some
properties in common with those of Nesson et al
(2006) in that they are of the probabilistic, syn-
chronous tree-insertion variety. All pairs of sites
(both adjunction and substitution in our case) are
explicitly linked. Adjunction sites are restricted by
direction: at each linked site, the source and target
side each specify one allowed direction. The re-
sult is that each synchronous adjunction site can be
classified into one of four direction classes: {LR,
LL, RR, RL}. For example, LR means the source
side site only allows left adjoining trees and the
target side site only allows right adjoining trees.
There are several important differences between
our grammars and the ones of Nesson et al (2006):
Richer, Linguistic Trees: Our grammars have a
728
Penn Treebank-style linguistic tree on the En-
glish (target) side, and a hierarchical structure
using only a single non-terminal symbol (X)
on the source side. We believe this provides
the rich information needed in the target lan-
guage without over-constraining the model.
Substitution Sites/Non-lexical trees: We use
both substitution and adjunction (Nesson
et al (2006) only used adjunction) and do
not require all trees to contain lexical items
as is commonly done in TIG (Schabes and
Waters, 1995).
Single Adjunction/Multiple Sites: Each non-
terminal node in a tree may allow multiple
adjunction sites, but every site only allows at
most one adjunction,1 a common assumption
for TAG as specified in the Vijay-Shanker
(1987) definition.
Here are some examples of automatically
learned translation rules with interpretations of
how they work:
1. simple lexical rules for translating words or
phrases:
IN
without
??
X
AlA
interpretation: translate the Arabic word
?AlA? as the preposition ?without?
2. rules with substitution for translating phrases
with holes (substitution sites are designated
by an arrow and numeric subscript, e.g.
NP?
1
):
PP
PP
IN
of
NP?
1
??
X
X?
1
interpretation: insert ?of? to turn a noun
phrase into a prepositional phrase
3. simple adjoining rules for inserting optional
modifiers (adjoining sites are designated by
1An adjoined rule may itself have adjoining sites allowing
further adjunction.
an alphabetic subscript before or after a non-
terminal to indicate direction of adjoining,
e.g.
a
NP):
a
NP
JJ?
1
NP*
??
X
X* X
a
X?
1
interpretation: adjoin an adjective before a
noun in English but after in Arabic, and al-
lowing further adjoinings in those same di-
rections afterward
4. rules with multiple adjunction and substitu-
tion sites:
a
S
NP?
1 b
S
c
VP
d
VP
e
VBD?
2
NP?
3
??
X
a
X
X?
2
X
X?
1
e,b
X
d,c
X?
3
interpretation: translate an Arabic sentence in
VSO form into an English sentence in SVO
form, with multiple adjoining options
2.2 Generative Story
When we use these rules to translate from a for-
eign sentence f into an English sentence e, we
use several models together in a log-linear fash-
ion, but our primary model is a joint model of
P (e
tree
, f
tree
), which is our surrogate for directly
modeling P (e|f). This can be justified because
P (e|f) =
P (e,f)
P (f)
, and P (f) is fixed for a given
foreign sentence. Therefore:
argmax
e
P (e|f) = argmax
e
P (e, f)
? yield(argmax
e
tree
P (e
tree
, f
tree
))
? yield(argmax
e
tree
P (d
e
tree
,f
tree
))
where d
e
tree
,f
tree
is a derivation tree of rules that
generates e
tree
and f
tree
. In other words, e, the
highest probability translation of f , can be approx-
imated by taking the yield of the highest proba-
bility tree e
tree
that is a translation of the high-
est probability tree of f . This can further be ap-
proximated by the highest probability derivation
of rules translating between f and e via trees.
Now we define the probability of generating
d
e
tree
,f
tree
. Starting with an initial symbol pair
729
representing a rule with a single substitution site,2
?TOP?, X??, a tree pair can be generated by the
following steps:
1. For each substitution site s
i
in the current rule
r
1
:
(a) Choose with probability
P
sub
(r
2
|?label
L
(s
i
), label
R
(s
i
)?) a rule
r
2
having root node labels label
L
(s
i
)
and label
R
(s
i
) that match the left and
right labels at s
i
.
2. For each adjunction site s
i,r
1
in the current
rule r
1
:
(a) Choose with rule-specific probability
Pifadj(decisionadjoin|si,r
1
, r
1
) choose
whether or not to adjoin at the current
site s
i,r
1
.
(b) If we are adjoining at site
s
i,r
1
, choose with probability
P
adj
(r
2
|d, ?label
L
(s
i,r
1
), label
R
(s
i,r
1
)?)
a rule r
2
of direction class d having
root node labels label
L
(s
i,r
1
) and
label
R
(s
i,r
1
) that match the left and
right labels at s
i,r
1
.
3. Recursively process each of the added rules
For all substitution rules r
s
, adjoining rules r
a
,
and adjoining sites s
i,r
, the probability of a deriva-
tion tree using these rules is the product of all the
probabilities used in this process, i.e.:
P
deriv
=
?
r
s
(
P
sub
(r
s
|?root
L
(r
s
), root
R
(r
s
)?) ?
?
s
i,r
s
Pifadj(decisionadjoin|si,r
s
, r
s
)
)
?
?
r
a
(
P
adj
(r
a
|dir(r
a
), ?root
L
(r
a
), root
R
(r
a
)?) ?
?
s
i,r
a
Pifadj(decisionadjoin|si,r
a
, r
a
)
)
Note that while every new substitution site re-
quires an additional rule to be added, adjunction
sites may or may not introduce an additional rule
based on the rule-specific Pifadj probability. This
allows adjunction to represent linguistic optional-
ity.
2Here and in the following, we use site as shorthand for
synchronous site pair.
3 Learning the Model
Instead of using bilingual parsing to directly train
our model from strings as done by Nesson et al
(2006), we follow the method of Galley et al
(2004) by dividing the training process into steps.
First, we word align the parallel sentences and
parse the English (target) side. Then, we transform
the aligned tree/string training data into derivation
trees of minimal translation rules (Section 3.1). Fi-
nally, we learn our probability models P
sub
, Pifadj ,
and P
adj
by collecting counts over the derivation
trees (Section 3.2). This method is quick enough
to allow us to scale our learning process to large-
scale data sets.
3.1 Generating Derivation Trees and Rules
There are four steps in transforming the training
data into derivation trees and rules, the first two
operating only on the English parse tree itself:3
A. Marking Required vs. Optional. For each
constituent in the English parse tree, we mark chil-
dren as (H)ead, (R)equired, or (O)ptional elements
(see step (a) in Figure 3). The choice of head, re-
quired, or optional has a large impact on the gen-
erality and applicability of our grammar. If all
children are considered required, the result is the
same as the GHKM rules of Galley et al (2004)
and has the same problem?lots of low count,
syntactically over-constrained rules. Too many
optional children, on the other hand, allows un-
grammatical output. Our proposed model is a lin-
guistically motivated middle ground: we consider
the linguistic heads and complements selected by
Collins? (2003) rules to be required and all other
children to be optional.
B. Parse tree to TIG tree. Next, we re-
structure the English tree to form a TIG deriva-
tion where head and required elements are substi-
tutions, and optional elements are adjunctions (see
step (b) in Figure 3). To allow for adjoining be-
tween siblings under a constituent, we first do a
head-out binarization of the tree. This is followed
by excising4 any children marked as optional and
replacing them with an adjunction site, as shown
in Figure 4. Note that we excise a chain of op-
tional children as one site with each optional child
3These first two steps were inspired by the method Chiang
(2003) used to automatically extract a TIG from an English
parse tree.
4Excising is the opposite of adjoining: extracting out an
auxiliary rule from a tree to form two smaller trees.
730
SADVP , NP VP .
(a)
=?
S
ADVP
O
,
O
NP
R
VP
H
.
O
(b)
=?
S
ADVP , NP VP .
Figure 3: Parse tree to TIG transformation: (a) mark constituent children with (H)ead, (R)equired, and
(O)ptional, then (b) restructure the tree so that head and required elements are substitutions, while op-
tional elements are adjoined (shown with dotted lines).
NT
1
NT
1
ABC
NT
2
XYZ
=?
NT
1
ABC NT
1
NT
1
* NT
2
XYZ
NT
1
NT
3
XYZ
NT
1
NT
2
DEF
NT
1
ABC
=?
NT
1
NT
1
NT
1
NT
3
XYZ
NT
1
*
NT
2
DEF
NT
1
*
ABC
(a) excising one optional child (XYZ) (b) excising a series of optional children (DEF, then XYZ)
Figure 4: Two examples of excising auxiliary trees from a head-out binarized parse tree: (a) excising one
optional left branch, (b) excising a chain of optional branches in the same (right) direction into a series
of adjunctions. In both examples, the ?ABC? child is the head, while the other children are optional.
adjoined to the previous child, as in Figure 4(b).
C. Extracting rules and derivation trees. We
now have a TIG derivation tree, with each elemen-
tary tree attached to its parent by a substitution or
adjunction link. We can now extract synchronous
rules allowed by the alignments and syntactic con-
stituents. This can be done using a method in-
spired by the rule-extraction approach of Galley et
al. (2004), but instead of directly operating on the
parse tree we process the English TIG derivation
tree. In bottom-up fashion, we visit each elemen-
tary tree in the derivation, allowing a rule rooted
at this tree to be extracted if its words or those
of its descendants are aligned such that they are
the English side of a self-contained parallel phrase
(i.e., the foreign text of this phrase is not aligned to
English leaves outside of the set of descendants).
Otherwise, this elementary tree is rejoined with its
parent to form a larger elementary tree. At the end
of this process we have a new set of linked ele-
mentary trees which make up the English side of
the grammar, where each substitution or adjunc-
tion link becomes a substitution or adjunction site
in the synchronous grammar.
On the foreign side we start with the foreign text
of the self-contained parallel phrase and replace
any parts of this phrase covered by substituted or
adjoined children of the English side tree with sub-
stitution sites or adjunction site markers. From
this, we produce a tree with a simple, regular form
by placing all items under a root node labeled X.
In the case of more than one foreign word or sub-
stitution site, we introduce an intermediate level of
X-labeled non-terminals to allow for possible ad-
junction between elements, otherwise the adjoin-
ing sites attach to the single root node. We attach
all foreign-side adjoining sites to be left adjoining,
except on the right side of the right-hand child.
It is possible to have the head child tree on the
English side not aligned to anything, while the ad-
joined children are. This may lead to rules with no
foreign non-terminal from which to anchor the ad-
junctions, so in this case, we attach adjoined child
elementary trees starting from the head and mov-
ing out until we attach a some child with a non-
empty foreign side.
D. Generalizing rules. We need to clarify
what makes one rule distinct from another. Con-
sider the example in Figure 5, which shows se-
lected rules learned in the case of two different
noun phrases. If the noun phrase consists of just
a single noun, we learn rule (a), while if the noun
phrase also has an adjective, we learn rules (b) and
(c). Since adjoining the adjective is optional, we
731
consider rules (a) and (c) to be the same rule, the
latter with an adjoining seen, and the former with
the same adjoining not seen.
3.2 Statistical Models
Once we have the derivation trees and list of rules,
we learn our statistical models using maximum
likelihood estimation. By counting and normal-
izing appropriately over the entire corpus, we can
straightforwardly learn the P
sub
and P
adj
distribu-
tions. However, recall that in our model Pifadj is a
rule-specific probability, which makes it more dif-
ficult to estimate accurately. For common rules,
we see plenty of examples of adjoining, while for
other rules, we need to learn from only a handful
of examples. Smoothing and generalization are es-
pecially important for these low frequency cases.
Two options present themselves for how to esti-
mate adjoining:
(a) A joint model of adjoining. We assume that
adjoining decisions are made in combination
with each other, and so learn non-zero proba-
bilities only for adjoining combinations seen
in data
(b) An independent model of adjoining. We as-
sume adjoining decisions are made indepen-
dently, and learn a model for each adjoining
site separately
Option (a) may be sufficient for frequent rules,
and will accurately model dependencies between
different kinds of adjoining. However, it does not
allow us to generalize to unseen patterns of adjoin-
ing. Consider the low frequency situation depicted
in Figure 6, rules (d)-(f). We may have seen this
rule four times, once with adjoining site a, twice
with adjoining sites a and b, and once with a third
adjoining site c. The joint model will give a zero
probability to unseen patterns of adjoining, e.g. no
adjoining at any site or adjoining at site b alone.
Even if we use a discounting method to give a non-
zero probability to unseen cases, we still have no
way to distinguish one from another.
Option (b) allows us to learn reasonable esti-
mates for these missing cases by separating out
adjoining decisions and letting each speak for it-
self. To properly learn non-zero probabilities for
unseen cases5 we use add k smoothing (k = 1
2
).
5For example, low frequency rules may have always been
observed with a single adjoining pattern, and never without
adjoining.
A weakness of this approach still remains: ad-
joining is not a truly independent process, as we
observe empirically in the data. In real data, fre-
quent rules have many different observed adjoin-
ing sites (10 or 20 in some cases), many of which
represent already infrequent sites in combinations
never seen together. To reduce the number of in-
valid combinations produced, we only allow ad-
joinings to be used at the same time if they have
occurred together in the training data. This restric-
tion makes it possible to do less adjoining than ob-
served, but not more. For the example in Figure 6,
in addition to the observed patterns, we would also
allow site b to be used alone, and we would allow
no adjoinings, but we would not allow combina-
tions of site c with either a or b. Later, we will
see that this makes the decoding process more ef-
ficient.
Because both option (a) and (b) above have
strengths and weaknesses, we also explore a third
option which builds upon the strengths of each:
(c) A log-linear combination of the joint model
and independent model. We assume the prob-
ability has both a dependent and indepen-
dent element, and learn the relative weight
between them automatically
To help smooth this model we add two addi-
tional binary features: one indicating adjoining
patterns seen in data and one indicating previously
unseen patterns.
4 Decoding
To translate with these rules, we do a monolingual
parse using the foreign side of the rules (constrain-
ing the search using non-terminal labels from both
sides), while keeping track of the English side
string and structure for language modeling pur-
poses. This produces all valid derivations of rules
whose foreign side yield is the input string, from
which we simply choose the one with the high-
est log-linear model score. Though this process
could be done directly using a specialized parsing
algorithm, we note that these rules have weakly
equivalent counterparts in the Synchronous Tree
Substitution Grammar (STSG) and Tree-to-string
transducer (xLNTs6) worlds, such that each STIG
rule can be translated into one equivalent rule, plus
some helper rules to model the adjoin/no-adjoin
6xLNTs is shorthand for extended linear non-deleting top-
down tree-to-string transducer.
732
Case 1: Case 2:
NP
NN
health
AlSHp
? (a)
NP
NN?
1
??
X
X?
1
NP
JJ
national
NP
NN
defense
AldfAE AlwTnY
?
(b)
NP
JJ?
1
NP*
??
X
X* X
X?
1
(c)
a
NP
NN?
1
??
X
a
X?
1
Figure 5: Selected rules learned in two cases. Rule (a) and (c) are considered the same rule, where (c)
has the optional synchronous adjoining site marked with a. From these (limited) examples alone we
would infer that adjective adjoining happens half the time, and is positioned before the noun in English,
but after the noun in Arabic (thus the positioning of site a).
(d)
a
QP
b
IN?
1
??
a
X
b
X?
1
(e)
a
QP
IN?
1
??
a
X
X?
1
(f)
c
QP
IN?
1
??
X
c
X?
1
(seen once) (seen twice) (seen once)
Figure 6: For a low frequency rule, we may see only a few different adjoining patterns, but we want to
infer more.
decision. Conversion to a better known and ex-
plored formalism allows us to take advantage of
existing code and algorithms. Here we describe
the conversion process to xLNTs rules, though
conversion to STSG is similar.
Algorithm 1 describes the process of converting
one of our automatically learned STIG rules. On
each side of the rule, we traverse the tree in a top-
down, left-to-right order, recording words, substi-
tution sites, and adjoining sites in the order en-
countered (left adjoinings before the node?s chil-
dren and right adjoinings after). We make these
words and sites as the children under a single root
node. The substitution sites are given states made
up of a combination of their source and target la-
bels as are the roots of non-adjoining rules. Ad-
joining sites are labeled with a combination of the
rule id and a site id. Adjoining rule roots are la-
beled with a combination of the source and target
root labels and the direction class. To allow for the
adjoining/no-adjoining decision, two helper rules
are created for each adjoining site, their root state
a combination of the rule and site ids. One of these
rules has only epsilon leaf nodes (representing no
adjoining), while the other has leaf nodes and a
state that match with the corresponding adjoining
rule root (labeled with the site?s source and target
labels and the direction class).
For each rule, the algorithm generates one
main rule and pairs of helper rules to facilitate
adjoining/non-adjoining. For computational effi-
ciency reasons, our decoder supports neither ep-
silon rules nor non-binary rules. So we remove ep-
silons using an exponential expansion of the rules:
combine each main rule with an adjoining or non-
adjoining helper rule for each adjunction site, then
remove epsilon-only branches. For k adjunction
sites this could possibly results in 2k rules. But as
discussed previously (at the end of Section 3.2),
we only allow subsets of adjoining combinations
seen in training data, so this number is substan-
tially lower for large values of k.
5 Experiments
All experiments are trained with a subset (171,000
sentences or 4 million words) of the Arabic-
English training data from the constrained data
track of the NIST 2008 MT Evaluation, leav-
ing out LDC2004T18, LDC2007E07, and the UN
data. The training data is aligned using the LEAF
technique (Fraser and Marcu, 2007). The English
side of the training data is parsed with an imple-
mentation of Collins Model 2 (Collins, 2003)
then head-out binarized. The tuning data (1,178
sentences) and devtest data (1,298 sentences) are
733
Input: Synchronous TIG rule r with j adjoining sites, S ? T , where S and T are trees
Output: a weakly equivalent xLNTs rule S? ? t
1
. . . t
n
, where S? is a one-level tree, and 2 ? j
helper rules for adjoining
Run time: O(|S| + |T |)
begin
rules ? {}, lhs-state ? concat(?q?, get-root(S), get-root(T ))
site-and-word-list-s ? get-sites-and-words-in-order(S)
site-and-word-list-t ? get-sites-and-words-in-order(T )
if r is adjoining then lhs-state ? concat(lhs-state, get-adjoin-dir(S), get-adjoin-dir(T ))
lhs? construct-LHS(lhs-state, get-root(S), site-and-word-list-s)
rhs? construct-RHS(add-states(id(r), site-and-word-list-t))
add(rules, ?lhs ? rhs?) /* main rule */
foreach adjoining site i ? 1 . . . k do
lhs-state ? concat(?q?, id(r), i), rhs-state ? concat(?q?, lhs-root)
lhs-root ? concat(source-label(i), target-label(i), source-dir(i), target-dir(i))
lhs ? construct-LHS(lhs-state, lhs-root, lhs-root)
rhs ? construct-RHS({(rhs-state, lhs-root)})
rhs-eps ? construct-RHS(!)
add(rules, {?lhs ? rhs?, ?lhs ? rhs-eps?}) /* helper rules for site i */
return rules
end
function get-sites-and-words-in-order(node)
y ? {}
if node is substitution site or word then append site or word to y else
append left adjoining sites to y in outside-to-inside order
foreach child c of node do append result of get-yield(c) to y
append right adjoining sites to y in inside-to-outside order
return y
end
function add-states(ruld-id, node-list)
foreach substitution or adjunction site s
i
and in node-list do
if s
i
is substitution site then state = concat(?q?, source-site-label(s
i
), target-site-label(s
i
))
else state = concat(?q?, rule-id, i)
replace s
i
with (state, s
i
)
return modified node-list
end
Algorithm 1: Conversion from synchronous TIG rules to weakly equivalent xLNTs rules
BLEU
description DevTest NIST06
(1) baseline: all required (GHKM minimal, head-out binarized parse trees) 48.0 47.0
(2) joint adjoining prob model alone (only observed adjoining patterns) 48.0 46.6
(3) independent adjoining prob model alone (only observed adjoining patterns) 48.1 46.7
(4) independent adjoining prob model alone (with new adjoining patterns) 48.5 47.6
(5) independent model alone + features (adjoining pattern, direction) 48.4 47.7
(6) log-linear combination of joint & independent models + features 48.7 47.8
Table 1: End-to-end MT results show that the best adjoining model using a log-linear combination
of joint and independent models (line 6) outperforms the baseline (line 1) by +0.7 and +0.8 BLEU, a
statistically significant difference at the 95% confidence level.
734
made up of newswire documents drawn from the
NIST MT evaluation data from 2004, 2005, and
2006 (GALE part). We use the newswire docu-
ments from the NIST part of the 2006 evaluation
data (765 sentences) as a held-out test set.
We train our feature weights using max-BLEU
(Och, 2003) and decode with a CKY-based de-
coder that supports language model scoring di-
rectly integrated into the search.
In addition to P
sub
, P
adj
, and Pifadj , we
use several other features in our log-linear
model during decoding, including: lexical and
phrase-based translation probabilities, a model
similar to conditional probability on the trees
(P (f
tree
(rule)|e
tree
(rule))), a probability model
for generating the top tree non-terminal, a 5-gram
language model7, and target length bonus. We
also have several binary features?lexical rule,
rule with missing or spurious content words?and
several binary indicator features for specialized
rules: unknown word rules; name, number, and
date translation rules; and special fail-safe mono-
tone translation rules in case of parse failures and
extremely long sentences.
Table 1 shows the comparison between our
baseline model (minimal GHKM on head-out bi-
narized parse trees) and different models of ad-
joining, measured with case-insensitive, NIST-
tokenized BLEU (IBM definition). The top section
(lines 1?4) compares the joint adjoining probabil-
ity model to the independent adjoining probabil-
ity model and seen vs. unseen adjoining combi-
nations. While the joint model results in a BLEU
score at the same level as our baseline (line 2),
the independent model (line 4) improves BLEU by
+0.5 and +0.6, which are significant differences
at the 95% confidence level. Since with the in-
dependent model we introduce both new adjoin-
ing patterns and a different probability model for
adjoining (each site is independent), we also use
the independent model with only previously seen
adjoining patterns (line 3). The insignificant dif-
ference in BLEU between lines 2 and 3 leads us
to think that the new adjoining patterns are where
the improvement comes from, rather than the in-
dependent probability model alone.
We also test several other features and combi-
nations. First, we add binary features to indicate
a new adjoining combination vs. one previously
7The 5-gram LM was trained on 2 billion words of auto-
matically selected collections taken from the NIST 08 allow-
able data.
seen in data. We also add features to indicate the
direction class of adjoining to test if there is a sys-
tematic bias toward particular directions. These
features cause no significant difference in score
(line 5). We also add the joint-adjoining proba-
bility as a feature, allowing it to be combined in a
log-linear fashion with the independent probabil-
ity (line 6). This results in our best BLEU gain:
+0.7 and +0.8 over our non-adjoining baseline.
6 Conclusion
We have presented a novel method for learning
the rules and probabilities for a new statistical,
linguistically-informed, syntax-based MT model
that allows for adjoining. We have described a
method to translate using this model. And we have
demonstrated that linguistically-motivated adjoin-
ing improves the end-to-end MT results.
There are many potential directions for research
to proceed. One possibility is to investigate other
methods of making the required vs. optional de-
cision, either using linguistic resources such as
COMLEX or automatically learning the distinc-
tion using EM (as done for tree binarization by
Wang et al (2007)). In addition, most ideas pre-
sented here are extendable to rules with linguistic
trees on both sides (using insights from Lavie et
al. (2008)). Also worth investigating is the direct
integration of bilingual dictionaries into the gram-
mar (as suggested by Shieber (2007)). Lastly, rule
composition and different amounts of lexicaliza-
tion (Galley et al, 2006; Marcu et al, 2006; De-
Neefe et al, 2007) or context modeling (Marin?o et
al., 2006) have been successful with other mod-
els.
Acknowledgments
We thank David Chiang for suggestions about
adjoining models, Michael Pust and Jens-So?nke
Vo?ckler for developing parts of the experimen-
tal framework, and other colleagues at ISI for
their helpful input. We also thank the anony-
mous reviewers for insightful comments and sug-
gestions. This research is financially supported
under DARPA Contract No. HR0011-06-C-0022,
BBN subcontract 9500008412.
References
Anne Abeille, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized TAGs for machine trans-
lation. In Proc. COLING, volume 3.
735
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. Data-
Oriented Parsing.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4).
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. EMNLP-CoNLL.
Alexander Fraser and Daniel Marcu. 2007. Getting the
structure right for word alignment: LEAF. In Proc.
EMNLP-CoNLL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL.
Aravind K. Joshi, L. S. Levy, and M. Takahashi. 1975.
Tree adjunct grammars. Journal of Computer and
System Sciences, 10(1).
Aravind K. Joshi. 1985. How much context-
sensitivity is necessary for characterizing structural
descriptions?tree adjoining grammars. Natural
Language Processing?Theoretical, Computational,
and Psychological Perspectives.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. SSST.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2).
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4).
Rebecca Nesson, Stuart M. Shieber, and Alexander
Rush. 2006. Induction of probabilistic synchronous
tree-insertion grammars for machine translation. In
Proc. AMTA.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: A cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, 21(4).
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. COL-
ING.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Proc.
SSST Wkshp., NAACL-HLT.
Kumar Vijay-Shanker. 1987. A study of tree adjoining
grammars. Ph.D. thesis.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-basedma-
chine translation accuracy. In Proc. EMNLP and
CoNLL.
736
187
188
189
190
191
192
193
194
Training Tree Transducers
Jonathan Graehl?
University of Southern California
Kevin Knight??
University of Southern California
Jonathan May?
University of Southern California
Many probabilistic models for natural language are now written in terms of hierarchical tree
structure. Tree-based modeling still lacks many of the standard tools taken for granted in
(finite-state) string-based modeling. The theory of tree transducer automata provides a possible
framework to draw on, as it has been worked out in an extensive literature. We motivate the
use of tree transducers for natural language and address the training problem for probabilistic
tree-to-tree and tree-to-string transducers.
1. Introduction
Much natural language work over the past decade has employed probabilistic finite-
state transducers (FSTs) operating on strings. This has occurred somewhat under the
influence of speech recognition research, where transducing acoustic sequences to word
sequences is neatly captured by left-to-right stateful substitution. Many conceptual tools
exist, such as Viterbi decoding (Viterbi 1967) and forward?backward training (Baum
and Eagon 1967), as well as software toolkits like the AT&T FSM Library and USC/ISI?s
Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from
part-of-speech tagging to letter-to-sound conversion to name transliteration.
However, language problems like machine translation break this mold, because
they involve massive re-ordering of symbols, and because the transformation processes
seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based
models have been proposed not only for machine translation (Wu 1997; Alshawi,
Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but
also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and
Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and
Rambow 2000; Corston-Oliver et al 2002), parsing, and language modeling (Baker
1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein
? Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu.
?? Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu.
? Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu.
1 www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel.
Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for
publication: 20 October 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
and Manning 2003). It is useful to understand generic algorithms that may support all
these tasks and more.
Rounds (1970) and Thatcher (1970) independently introduced tree transducers as a
generalization of FSTs. Rounds was motivated by natural language:
Recent developments in the theory of automata have pointed to an extension of
the domain of definition of automata from strings to trees . . . parts of mathematical
linguistics can be formalized easily in a tree-automaton setting . . .We investigate
decision problems and closure properties. Our results should clarify the nature of
syntax-directed translations and transformational grammars . . . (Rounds 1970)
The Rounds/Thatcher tree transducer is very similar to a left-to-right FST, except that
it works top-down, pursuing subtrees independently, with each subtree transformed
depending only on its own passed-down state. This class of transducer, called R in
earlier works (G?cseg and Steinby 1984; Graehl and Knight 2004) for ?root-to-frontier,?
is often nowadays called T, for ?top-down?.
Rounds uses a mathematics-oriented example of a T transducer, which we repeat
in Figure 1. At each point in the top-down traversal, the transducer chooses a produc-
tion to apply, based only on the current state and the current root symbol. The traversal
continues until there are no more state-annotated nodes. Non-deterministic transducers
may have several productions with the same left-hand side, and therefore some free
choices to make during transduction.
A T transducer compactly represents a potentially infinite set of input/output tree
pairs: exactly those pairs (T1, T2) for which some sequence of productions applied to
T1 (starting in the initial state) results in T2. This is similar to an FST, which compactly
represents a set of input/output string pairs; in fact, T is a generalization of FST. If
we think of strings written down vertically, as degenerate trees, we can convert any
FST into a T transducer by automatically replacing FST transitions with T produc-
tions, as follows: If an FST transition from state q to state r reads input symbol A
and outputs symbol B, then the corresponding T production is q A(x0) ? B(r x0). If
the FST transition output is epsilon, then we have instead q A(x0) ? r x0, or if the
input is epsilon, then q x0? B(r x0). Figure 2 depicts a sample FST and its equivalent
T transducer.
T does have some extra power beyond path following and state-based record-
keeping. It can copy whole subtrees, and transform those subtrees differently. It can
also delete subtrees without inspecting them (imagine by analogy an FST that quits and
accepts right in the middle of an input string). Variants of T that disallow copying and
deleting are called LT (for linear) and NT (for nondeleting), respectively.
One advantage to working with tree transducers is the large and useful body of
literature about these automata; two excellent surveys are G?cseg and Steinby (1984)
and Comon et al (1997). For example, it is known that T is not closed under composition
(Rounds 1970), and neither are LT or B (the ?bottom-up? cousin of T), but the non-
copying LB is closed under composition. Many of these composition results are first
found in Engelfriet (1975).
The power of T to change the structure of an input tree is surprising. For example,
it may not be initially obvious how a T transducer can transform the English structure
S(PRO, VP(V, NP)) into the Arabic equivalent S(V, PRO, NP), as it is difficult to move
the subject PRO into position between the verb V and the direct object NP. First,
T productions have no lookahead capability?the left-hand-side of the S production
392
Graehl, Knight, and May Training Tree Transducers
Figure 1
Part of a sample T tree transducer, adapted from Rounds (1970).
consists only of q S(x0, x1), although we want the English-to-Arabic transformation to
apply only when it faces the entire structure q S(PRO, VP(V, NP)). However, we can
simulate lookahead using states, as in these productions:
q S(x0, x1)? S(qpro x0, qvp.v.np x1)
qpro PRO? PRO
qvp.v.np VP(x0, x1)? VP(qv x0, qnp x1)
393
Computational Linguistics Volume 34, Number 3
Figure 2
An FST and its equivalent T transducer.
By omitting rules like qpro NP? ..., we ensure that the entire production sequence
will dead-end unless the first child of the input tree is in fact PRO. So finite lookahead
(into inputs we don?t delete) is not a problem. But these productions do not actually
move the subtrees around. The next problem is how to get the PRO to appear between
the V and NP, as in Arabic. This can be carried out using copying. We make two copies
of the English VP, and assign them different states, as in the following productions.
States encode instructions for extracting/positioning the relevant portions of the VP.
For example, the state qleft.vp.v means ?assuming this tree is a VP whose left child is V,
output only the V, and delete the right child?:
q S(x0, x1)? S(qleft.vp.v x1, qpro x0, qright.vp.np x1)
qpro PRO? PRO
qleft.vp.v VP(x0, x1)? qv x0
qright.vp.np VP(x0, x1)? qnp x1
With these rules, the transduction proceeds as in Figure 3. This ends our informal pre-
sentation of tree transducers.
Although general properties of T are understood, there are many algorithmic ques-
tions. In this article, we take on the problem of training probabilistic T transducers. For
many language problems (machine translation, paraphrasing, text compression, etc.),
it is possible to collect training data in the form of tree pairs and to distill linguistic
knowledge automatically. Our problem statement is: Given (1) a particular transducer
394
Graehl, Knight, and May Training Tree Transducers
Figure 3
Multilevel re-ordering of nodes in a T-transducer.
with rules R, and (2) a finite training set of sample input/output tree pairs, we want
to produce (3) a probability estimate for each rule in R such that we maximize the
probability of the output trees given the input trees. As with the forward?backward
algorithm, we seek at least a local maximum. Tree transducers with weights have been
studied (Kuich 1999; Engelfriet, F?l?p, and Vogler 2004; F?l?p and Vogler 2004) but we
know of no existing training procedure.
Sections 2?4 of this article define basic concepts and recall the notions of relevant au-
tomata and grammars. Sections 5?7 describe a novel tree transducer training algorithm,
and Sections 8?10 describe a variant of that training algorithm for trees and strings.
Section 11 presents an example linguistic tree transducer and provides empirical evi-
dence of the feasibility of the training algorithm. Section 12 describes how the training
algorithm may be used for training context-free grammars. Section 13 discusses related
and future work.
2. Trees
T? is the set of (rooted, ordered, labeled, finite) trees over alphabet ?. An alphabet is a finite
set. (see Table 1)
T?(X) are the trees over alphabet ?, indexed by X?the subset of T??X where only
leaves may be labeled by X (T?(?) = T?). Leaves are nodes with no children.
The nodes of a tree t are identified one-to-one with its paths: pathst ? paths ? N
? ?
??
i=0 N
i (N0 ? {()}). The size of a tree is the number of nodes: |t| = |pathst|. The path
to the root is the empty sequence (), and p1 extended by p2 is p1 ? p2, where ? is the
concatenation operator:
(a1, . . . , an) ? (b1, . . . , bm) ? (a1, . . . , an, b1, . . . , bm)
For p ? pathst, rankt(p) is the number of children, or rank, of the node at p,
and labelt(p) ? ? is its label. The ranked label of a node is the pair labelandrankt(p) ?
(labelt(p), rankt(p)). For 1 ? i ? rankt(p), the ith child of the node at p is located at
395
Computational Linguistics Volume 34, Number 3
Table 1
Notation guide.
Notation Meaning
(w)FS(T,A) (weighted) finite-state string (transducers,acceptors)
(w)RTG (weighted) regular tree grammars (generalizes PCFG)
(x)(L)(N)T(s) (extended) (linear) (nondeleting) top?down tree(-to-string) transducers
(S)T(A,S)G (synchronous) tree (adjoining,substitution) grammars
(S,P)CFG (synchronous,probabilistic) context-free grammars
R+ positive real numbers
N natural numbers: {1, 2, 3, . . .}
? empty set
? equals (by definition)
|A| size of finite set A
X? Kleene star of X, i.e., strings over alphabet X: {(x1, . . . , xn) | n ? 0}
a ? b String concatenation: (1) ? (2, 3) = (1, 2, 3)
<lex lexicographic (dictionary) order: () < (1) < (1, 1) < . . . < (1, 2) < . . .
? alphabet (set of symbols) (commonly: input tree alphabet)
t ? T? t is a tree with label alphabet ?
T?(X) ... and with variables from additional leaf label alphabet X
A(t) tree constructed by placing a unary A above tree t
A((x1, . . . , xn)) tree constructed by placing an n-ary A over leaves (x1, . . . , xn)
p tree path, e.g., (a, b) is the bth child of the ath child of root
paths the set of all tree paths (? N?)
pathst subset of paths that lead to actual nodes in t
pathst({A,B}) paths that lead to nodes labeled A or B in t
t ? p the subtree of twith root at p, so that (t ? p) ? q = t ? (p ? q)
rankt(p) the number of children of the node p of t
labelt(p) the label of node p of t
labelandrankt(p) the pair (labelt(p), rankt(p))
t[p? t?] substitution of tree t? for the subtree t ? p
t[p? t?p,?p ?P] parallel substitution of tree t
?
p for each t ? p
yieldt(X) the left? right concatenation of the X labels of the leaves of t
S ? N start nonterminal of a regular tree grammar
P,R productions of a regular tree grammar, rules of a tree transducer
D(M) derivations (keeping a list of applied rewrites) ofM
LD(M) leftmost derivations ofM
wM(d ? D(M)) weight of a derivation d: product of weight of each rule usage
WM(x) total weight of x inM: sum of weight of all LD(M) producing x
L(M) weighted tree set, tree relation, or tree-to-string relation ofM
? output tree alphabet
Qi ? Q initial (start) state of a transducer
? ? xTPAT? functions from T? to {0, 1} that examine finitely many paths
True the tree pattern True(t) ? 1,?t
s ? ?? s is a string from alphabet ?, e.g., () the empty string
s[i] ith letter of string s - the ith projection ?i
indicess i such that s[i] exists: (1, . . . , |s|)
letterss set of all letters s[i] in s
|s| length of string; |s| = |indicess|, not |letterss|
spanss Analogous to tree paths, pairs (i,j) denoting substrings
s ? (i, j) The substring (s[i], . . . , s[j? 1]) indicated by the span (i, j) ? spanss
s ? [i] same as s[i]; [i] stands for the span (i, i+ 1)
s[p? s?] Substitution of string s? for span p of s
s[p? s?p,?p ?P] Parallel (non-overlapping) substitution of string s
?
p for each s ? p
396
Graehl, Knight, and May Training Tree Transducers
path p ? (i). The subtree at path p of t is t ? p, defined by pathst?p ? {q | p ? q ? pathst} and
labelandrankt?p(q) ? labelandrankt(p ? q).
The paths to X in t are pathst(X) ? {p ? pathst | labelt(p) ? X}.
A set of paths F ? paths is a frontier iff it is pairwise prefix-independent:
?p1, p2 ? F, p ? paths : p1 = p2 ? p =? p1 = p2
We write F for the set of all frontiers. F is a frontier of t, if F ? Ft is a frontier whose
paths are all valid for t?Ft ? F ? pathst.
For t, s ? T?(X), p ? pathst, t[p? s] is the substitution of s for p in t, where the subtree
at path p is replaced by s. For a frontier F of t, the parallel substitution of t?p for the frontier
F ? Ft in t is written t[p? t?p,?p ? F], where there is a t
?
p ? T?(X) for each path p. The
result of a parallel substitution is the composition of the serial substitutions for all p ? F,
replacing each t ? pwith t?p. (If Fwere not a frontier, the result would vary with the order
of substitutions sharing a common prefix.) For example: t[p? t ? p ? (1),?p ? F] would
splice out each node p ? F, replacing it by its first subtree.
Trees may be written as strings over? ? {(, )} in the usual way. For example, the tree
t = S(NP,VP(V,NP)) has labelandrankt((2)) = (VP, 2) and labelandrankt((2, 1)) = (V, 0).
Commas, written only to separate symbols in ? composed of several typographic
letters, should not be considered part of the string. For example, if we write ?(t) for
? ? ?, t ? T?, we mean the tree with label?(t)(()) ? ?, rank?(t)(()) ? 1 and ?(t) ? (1) ? t.
Using this notation, we can give a definition of T?(X):
If x ? X, then x ? T?(X) (1)
If ? ? ?, then ? ? T?(X) (2)
If ? ? ? and t1, . . . , tn ? T?(X), then ?(t1, . . . , tn) ? T?(X) (3)
The yield of X in t is yieldt(X), the concatenation (in lexicographic order
2) over paths
to leaves l ? pathst (such that rankt(l) = 0) of labelt(l) ? X?that is, the string formed by
reading out the leaves labeled with X in left-to-right order. The usual case (the yield of t)
is yieldt ? yieldt(?). More precisely,
yieldt(X) ?
?
?
?
l if r = 0 ? l ? X where (l, r) ? labelandrankt(())
() if r = 0 ? l ? X
?ri=1yieldt?(i)(X) otherwise where ?
r
i=1si ? s1 ? . . . ? sr
3. Regular Tree Grammars
In this section, we describe the regular tree grammar, a common way of compactly
representing a potentially infinite set of trees (similar to the role played by the regu-
lar grammar for strings). We describe the version where trees in a set have different
weights, in the same way that a weighted finite-state acceptor gives weights for strings
2 () <lex (a), (a1) <lex (a2) iff a1 < a2, (a1) ? b1 <lex (a2) ? b2 iff a1 < a2 ? (a1 = a2 ? b1 <lex b2).
397
Computational Linguistics Volume 34, Number 3
? = {S, NP, VP, PP, PREP, DET, N, V, run, the, of, sons, daughters}
N = {qnp, qpp, qdet, qn, qprep}
S = q
P = {q?1.0 S(qnp, VP(VB(run))),
qnp?0.6 NP(qdet, qn),
qnp?0.4 NP(qnp, qpp),
qpp?1.0 PP(qprep, np),
qdet?1.0 DET(the),
qprep?1.0 PREP(of),
qn?0.5 N(sons),
qn?0.5 N(daughters)}
Sample generated trees:
S(NP(DT(the), N(sons)),
VP(V(run)))
(with probability 0.3)
S(NP(NP(DT(the), N(sons)),
PP(PREP(of), NP(DT(the), N(daughters)))),
VP(V(run)))
(with probability 0.036)
Figure 4
A sample weighted regular tree grammar (wRTG).
in a regular language; when discussing weights, we assume the commutative semiring
({r ? R | r ? 0},+, ?, 0, 1) of nonnegative reals with the usual sum and product.
A weighted regular tree grammar (wRTG) G is a quadruple (?,N,S,P), where ? is
the alphabet, N is the finite set of nonterminals, S ? N is the start (or initial) nonterminal,
and P ? N ? T?(N)? R+ is a finite set of weighted productions (R+ ? {r ? R | r > 0}). A
production (lhs, rhs,w) is written lhs?w rhs (if w is omitted, the multiplicative identity
1 is assumed). Productions whose rhs contains no nonterminals (rhs ? T?) are called
terminal productions, and rules of the form A?w B, for A,B ? N are called -productions,
or state-change productions, and can be used in lieu of multiple initial nonterminals.
Figure 4 shows a sample wRTG. This grammar generates an infinite number of trees.
We define the binary derivation relation on terms T?(N) and derivation histories
(T?(N ? (paths? P)?):
?G?
{
((a, h), (b, h ? (p, (l, r,w)))) | (l, r,w) ? P?
p ? pathsa({l})?
b = a[p? r]
}
398
Graehl, Knight, and May Training Tree Transducers
That is, (a, h)?G (b, h ? (p, (l, r,w))) iff b may be derived from a by using the rule
l?w r to replace the nonterminal leaf l at path p with r. The reflexive, transitive closure
of ?G is written ??G, and the derivations of G, written D(G), are the ways the start
nonterminal may be expanded into entirely terminal trees:
D(G) ?
{
(t, h) ? T? ? (paths? P)? | (S, ())??G (t, h)
}
We also project the ??G relation so that it refers only to trees: t
? ??G t iff ?h
?, h ?
(paths? P)? : (t?, h?)??G (t, h).
We take the product of the used weights to get the weight of a derivation d ? D(G):
wG((t, (h1, . . . , hn)) ? D(G)) ?
n
?
i=1
wi where hi = (pi, (li, ri,wi))
The leftmost derivations of G build a tree preorder from left to right (always expand-
ing the leftmost nonterminal in its string representation):
LD(G) ?
{
(t, ((p1, r1), . . . , (pn, rn))) ? DG | ?1 ? i < n : pi+1 ?lex pi
}
The total weight of t in G is given byWG : T? ? R, the sum of the weights of leftmost
derivations producing t: WG(t) ?
?
(t,h)?LD(G) wG((t, h)). Collecting the total weight of
every possible (nonzero weight) output tree, we call L(G) the weighted tree language of
G, where L(G) = {(t,w) |WG(t) = w ? w > 0} (the unweighted tree language is simply
the first projection).
For every weighted context-free grammar, there is an equivalent wRTG that gener-
ates its weighted derivation trees (whose yield is a string in the context-free language),
and the yield of any regular tree language is a context-free string language (G?cseg
and Steinby 1984). We can also interpret a regular tree grammar as a context-free string
grammar with alphabet ? ? {(, )}.
wRTGs generate (ignoring weights) exactly the recognizable tree languages, which
are sets of trees accepted by a non-transducing automaton version of T. This acceptor
automaton is described in Doner (1970) and is actually a closer mechanical analogue
to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection
(G?cseg and Steinby 1984), and the constructive proof also applies to weighted wRTG
intersection. There is a normal form for wRTGs analogous to that of regular grammars:
Right-hand sides are a single terminal root with (optional) nonterminal children. What
is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof
and Satta 2002) is a finite wRTG without loops?for all valid derivation trees, each
nonterminal may only occur once in any path from root to a leaf:
?n ? N, t ? T?(N), h ? (paths? P)? : (n, ())??G (t, h) =? pathst({n}) = ?
RTGs produce tree sets equivalent to those produced by tree substitution grammars
(TSGs) (Schabes 1990) up to relabeling. The relabeling is necessary because RTGs distin-
guish states and tree symbols, which are conflated in TSGs at the elementary tree root.
Regular tree languages are strictly contained in tree sets of tree adjoining grammars
(TAG; Joshi and Schabes 1997), which generate string languages strictly between the
context-free and indexed languages. RTGs are essentially TAGs without auxiliary trees
399
Computational Linguistics Volume 34, Number 3
and their adjunction operation; the productions correspond exactly to TAG?s initial trees
and the elementary tree substitution operation.
4. Extended-LHS Tree Transducers (xT)
Section 1 informally described the root-to-frontier transducer class T. We saw that T
allows, by use of states, finite lookahead and arbitrary rearrangement of non-sibling
input subtrees removed by a finite distance. However, it is often easier to write rules that
explicitly represent such lookahead and movement, relieving the burden on the user to
produce the requisite intermediary rules and states. We define xT, a generalization of
weighted T. Because of its good fit to natural language problems, xT is already briefly
touched on, though not defined, in Section 4 of Rounds (1970).
A weighted extended-lhs top-down tree transducer M is a quintuple (?,?,Q,Qi,R)
where ? is the input alphabet, and ? is the output alphabet, Q is a finite set of states,
Qi ? Q is the initial (or start, or root) state, and R ? Q? xTPAT? ? T?(Q? paths)? R+
is a finite set of weighted transformation rules. xTPAT? is the set of finite tree patterns:
predicate functions f : T? ? {0, 1} that depend only on the label and rank of a finite
number of fixed paths of their input. A rule (q, ?, rhs,w) is written q ? ?w rhs, mean-
ing that an input subtree matching ? while in state q is transformed into rhs, with
Q? paths leaves replaced by their (recursive) transformations. TheQ? paths leaves of a
rhs are called nonterminals (there may also be terminal leaves labeled by the output tree
alphabet ?).
xT is the set of all such transducers T; the set of conventional top-down trans-
ducers, is a subset of xT where the rules are restricted to use finite tree patterns that de-
pend only on the root: TPAT? ? {p?,r(t)}where p?,r(t) ? (labelt(()) = ? ? rankt(()) = r).
Rules whose rhs are a pure T? with no states/paths for further expansion are called
terminal rules. Rules of the form q ? ?w q? () are -rules, or state-change rules, which
substitute state q? for state q without producing output, and stay at the current input
subtree. Multiple initial states are not needed: we can use a single start state Qi, and
instead of each initial state qwith starting weight w add the rule Qi True?w q () (where
True(t) ? 1,?t).
We define the binary derivation relation for xT transducer M on partially trans-
formed terms and derivation histories T????Q ? (paths? R)?:
?M?
{
((a, h), (b, h ? (i, (q, ?, rhs,w)))) | (q, ?, rhs,w) ? R ?
i ? pathsa ? q = labela(i) ? ?(a ? (i ? (1))) = 1 ?
b = a
[
i? rhs
[
p? q?(a ? (i ? (1) ? i?)),
?p ? pathsrhs : labelrhs(p) = (q
?, i?)
]]
}
That is, b is derived from a by application of a rule q ? ?w rhs to an unprocessed
input subtree a ? i which is in state q, replacing it by output given by rhs with variables
(q?, i?) replaced by the input subtree at relative path i? in state q?.3
Let ??M, D(M), LD(M), wM, WM, and L(M) (the weighted tree relation of M) follow
from the single-step?M exactly as they did in Section 3, except that the arguments are
3 Recall that q(a) is the tree whose root is labeled q and whose single child is the tree a.
400
Graehl, Knight, and May Training Tree Transducers
input and output instead of just output, with initial terms Qi(t) for each input t ? T? in
place of S:
D(M) ?
{
(t, t?, h) ? T? ? T? ? (paths? R)? | (Qi(t), ())??M (t
?, h)
}
We have given a rewrite semantics for our transducer, similar to wRTG. In the
intermediate terms of a derivation, the active frontier of computation moves top-down,
with everything above that frontier forming the top portion of the final output. The next
rewrite always occurs somewhere on the frontier, and in a complete derivation, the frontier
finally shrinks and disappears. In wRTG, the frontier consisted of the nonterminal-
labeled leaves. In xT, the frontier items are not nonterminals, but pairs of state and input
subtrees. We choose to represent these pairs as subtrees of terms with labels taken from
? ?? ?Q, where the state is the parent of the input subtree. In fact, given an M ? xT
and an input tree t, we can take all the (finitely many) pairs of input subtrees and states
as nonterminals in a wRTGG, with all the (finitely many) possible single-step derivation
rewrites ofM applied to t as productions (taking the weight of the xT rule used), and the
initial term Qi(t) as the start nonterminal, isomorphic to the derivations of theMwhich
start withQi(t): (d, h) ? D(G) iff (t, d, h) ? D(M). Such derivations are exactly how all the
outputs of an input tree t are produced: when the resulting term d is in T?, we say that
(t, d) is in the tree relation and that d is an output of t.
Naturally, there may be input trees for which no complete derivation exists?such
inputs are not in the domain of the weighted tree relation, having no output. It is known
that domain(M) ? {i | ?o,w : (i, o,w) ? L(M)}, the set of inputs that produce any output,
is always a recognizable tree language (Rounds 1970).
The sources of a rule r = (q, l, rhs,w) ? R are the input-paths in the rhs:
sources(r) ? {i? | ?p ? pathsrhs(Q? paths), q
? ? Q : labelrhs(p) = (q?, i?)}
If the sources of a rule refer to input paths that do not exist in the input, then the
rule cannot apply (because a ? (i ? (1) ? i?) would not exist). In the traditional statement
of T, sources(r) are the variables xi, standing for the i
th child of the root at path (i),
and the right hand sides of rules refer to them by name: (qi, xi). In xT, however, we
refer to the mapped input subtrees by path (and we are not limited to the immediate
children of the root of the subtree under transformation, but may choose any frontier
of it).
A transducer is linear if for all its rules r, sources(r) are a frontier and occur at most
once: ?p1, p2 ? pathsrhs(Q? paths), p ? paths? {()} : p1 = p2 ? p. A transducer is determin-
istic if for any input, at most one rule matches per state:
?q ? Q, t ? T?, r = (q, p, r,w), r? = (q?, p?, r?,w?) ? R :
p(t) = 1 ? p?(t) = 1 =? r = r?
or in other words, the rules for a given state have patterns that partition possible input
trees. A transducer is deleting if there are rules in which (for some matching inputs)
entire subtrees are not used in their rhs.
In practice, we will be interested mostly in concrete transducers, where the patterns
fully specify the labels and ranks of an input subtree including all the ancestors
of sources(r). Naturally, T are concrete. We have taken to writing concrete rules?
patterns as trees with variables X in the leaves (at the sources), and using those same
401
Computational Linguistics Volume 34, Number 3
variables in the rhs instead of writing the corresponding path in the lhs. For example:
q A(x0:B,C) ?w q? x0 means a xT rule (q, ?, rhs,w) with rhs = (q?, (1)) and
? ? (labelandrankt(()) = (A, 1) ? labelt((1)) = B ? labelandrankt((2)) = (C, 0))
It might be convenient to convert any xT transducer to an equivalent T transducer,
then process it with T-based algorithms?in such a case, xT would just be syntactic sugar
for T. We can automatically generate T productions that use extra states to emulate the
finite lookahead and movement available in xT (as demonstrated in Section 1), but with
one fatal flaw: Because of the definition of ?M, xT (and thus T) only has the ability
to process input subtrees that produce corresponding output subtrees (alas, there is no
such thing as an empty tree), and because TPAT can only inspect the root node while
deriving replacement subtrees, T can check only the parts of the input subtree that lie
along paths that are referenced in the rhs of the xT rule. For example, suppose we want
to transform NP(DET, N) (but not, say, NP(ADJ, N)) into the tree N using rules in T.
Although this is a simple xT rule, the closest we can get with T would be q NP(x0,
x1) ? q.N x1, but we cannot check both subtrees without emitting two independent
subtrees in the output (which rules out producing just N). Thus, xT is a bit more
powerful than T.
5. Parsing an xT Tree Relation
Derivation trees for a transducer M = (?,?,Q,Qi,R) are TR (trees labeled by rules)
isomorphic to complete leftmost M-derivations. Figure 5 shows derivation trees for a
particular transducer. In order to generate derivation trees forM automatically, we build
a modified transducerM?. This new transducer produces derivation trees on its output
instead of normal output trees.M? is (?,R,Q,Qi,R
?), with4
R? ? {(q, ?, r(yieldrhs(Q? paths)),w) | r = (q, ?, rhs,w) ? R}
That is, the original rhs of rules are flattened into a tree of depth 1, with the root labeled
by the original rule, and all the non-expanding ?-labeled nodes of the rhs removed, so
that the remaining children are the nonterminal yield in left to right order. Derivation
trees deterministically produce a single weighted output tree, and for concrete trans-
ducers, a single input tree.
For every leftmost derivation there is exactly one corresponding derivation tree: We
start with a sequence of leftmost derivations and promote rules applied to paths that
are prefixes of rules occurring later in the sequence (the first will always be the root), or,
in the other direction, list out the rules of the derivation tree in order.5 The weights of
derivation trees are, of course, just the product of the weights of the rules in them.6
The derived transducer M? nicely produces derivation trees for a given input, but
in explaining an observed (input/output) pair, we must restrict the possibilities further.
Because the transformations of an input subtree depend only on that subtree and its
state, we can build a compact wRTG that produces exactly the weighted derivation
trees corresponding toM-transductions (I, ())??M (O, h) (Algorithm 1).
4 By r((t1, . . . , tn )), we mean the tree r(t1, . . . , tn ).
5 Some path concatenation is required, because paths in histories are absolute, whereas the paths in rule rhs
are relative to the input subtree.
6 Because our product is commutative, the order does not matter.
402
Graehl, Knight, and May Training Tree Transducers
Figure 5
Derivation trees for a T tree transducer.
Algorithm 1 makes use of memoization?the possible derivations for a given (q, i, o)
are constant, so we store answers for all past queries in a lookup table and return them,
avoiding needless recomputation. Even if we prove that there are no derivations for
some (q, i, o), successful subhypotheses met during the proof may recur and are kept,
but we do avoid adding productions we know can?t succeed. We have in the worst case
to visit all |Q| ? |I| ? |O| (q, i, o) pairs and apply all |R| transducer rules successfully at
each of them, so time and space complexity, proportional to the size of the (unpruned)
output wRTG, are both O(|Q| ? |I| ? |O| ? |R|), or O(Gn2), where n is the total size of the
403
Computational Linguistics Volume 34, Number 3
Algorithm 1. Deriv (derivation forest for I??xT O)
Input: xT transducerM = (?,?,Q,Qi,R) and observed tree pair I ? T?, O ? T?.
Output: derivation wRTG G = (R,N ? Q? pathsI ? pathsO,S,P) generating all
weighted derivation trees forM that produce O from I. Returns false instead if
there are no such trees. O(G|I||O|) time and space complexity, where G is a
grammar constant.
begin
S? (Qi, (), ()), N??, P??, memo??
if PRODUCEI,O(S) then
N?{n | ?(n?, rhs,w) ? P : n = n? ? n ? yieldrhs(Q? pathsI ? pathsO)}
return G = (R,N,S,P)
else
return false
end
PRODUCEI,O(? = (q, i, o) ? Q? pathsI ? pathsO) returns boolean ? begin
if ?(?, r) ? memo then return r
memo?memo ? {(?, true)}
anyrule?? false
for r = (q, ?, rhs,w) ? R : ?(I ? i) = 1 ?MatchO,?(rhs, o) do
(o1, . . . , on)? pathsrhs(Q? paths) sorted by o1 <lex . . . <lex on //n = 0 if there are
no rhs variables
labelandrankderivrhs(())? (r,n) //derivrhs is a newly created tree
for j? 1 to n do
(q?, i?)? labelrhs(oj)
?? (q?, i ? i?, o ? oj)
if ?PRODUCEI,O(?) then next r
labelandrankderivrhs((j))? (?, 0)
anyrule?? true
P?P ? {(?, derivrhs,w)}
memo?memo ? {(?,anyrule?)}
return anyrule?
end
Matcht,?(t
?, p) ? ?p? ? path(t?) : label(t?, p?) ? ? =? labelandrankt? (p?) =
labelandrankt(p ? p?)
input and output trees, and G is the grammar constant accounting for the states and
rules (and their size).
If the transducer contains cycles of state-change rules, then the generated derivation
forest may have infinitely many trees in it, and thus the memoization of PRODUCE
must temporarily assume that the alignment (q, i, o) under consideration will succeed
upon reaching itself, through such a cycle, even though the answer is not yet conclusive
(it may be conclusively true, but not false). Although it would be possible to detect these
cycles (setting ?pending? rather than true for the interim in memo) and deal with them
more severely, we can just remove the surplus later in linear time, using Algorithm 2,
which is an implementation (for wRTG) of a well-known method of pruning useless
404
Graehl, Knight, and May Training Tree Transducers
Algorithm 2. RTGPrune (wRTG useless nonterminal/production identification)
Input: wRTG G = (?,N,S,P), with P = (p1, . . . , pm) and pi = (qi, ti,wi).
Output: For all n ? N, B[n] = (?t ? T? : n??G t) (true if n derives some output tree t
with no remaining nonterminals, false if it?s useless), and
A[n] = (?t ? T?, t? ? T?({n}) : S??G t
? ??G t) (n additionally can be produced
from an S using only productions that can appear in complete derivations).
Time and space complexity are linear in the total size of the input:
O(|N|+
?m
i=1 (1+ |pathsti |)
begin
M??
for n ? N do B[n]? false, Adj[n]??
for i? 1 to m do
Y?{labelti (p) | p ? pathsti (N)}
// Y are the unique N in rhs of rule i
for n ? Y do Adj[n]?Adj[n] ? {i}
if |Y| = 0 thenM?M ? {i}
r[i]?|Y|
for n ?M do REACH(n)
/* Now that B[n] are decided, compute A[n] */
for n ? N do A[n]? false
USE(S)
end
REACH(n) ? begin
B[n]? true
for i ? Adj[n] do
if ?B[qi] then
r[i]? r[i]? 1
if r[i] = 0 then REACH(qi)
end
USE(n) ? begin
A[n]? true
for n? s.t. ?(n, t,w) ? R : n? ? yieldt(N) do
/* for n? that are in the rhs of rules whose lhs is n */
if ?A[n?] ? B[n?] then USE(n?)
end
productions from a CFG (Hopcroft and Ullman 1979).7 We eliminate all the remains
of failed subforests, by removing all nonterminals n, and any productions involving n,
where Algorithm 2 gives A[n] = false.
In the next section, we show how to compute the contribution of a nonterminal to
the weighted trees produced by a wRTG, in a generalization of Algorithm 2 that gives
us weights that we accumulate per rule over the training examples, for EM training.
7 The idea is to first remove all nonterminals (and productions referring to them) that don?t yield any
terminal string, and after that, to remove those which are not reachable top-down from S.
405
Computational Linguistics Volume 34, Number 3
6. Inside?Outside for wRTG
Given a wRTGG = (?,N,S,P), we can compute the sums of weights of trees derived us-
ing each production by adapting the well-known inside?outside algorithm for weighted
context-free (string) grammars (Lari and Young 1990).
Inside weights ?G for a nonterminal or production are the sum of weights of all trees
that can be derived from it:
?G(n ? N) ?
?
(n,r,w)?P
w ? ?G(r)
?G(r ? T?(N) | (n, r,w) ? P}) ?
?
p?pathsr(N)
?G(labelr(p))
By definition, ?G(S) gives the sum of the weights of all trees generated by G. For the
wRTG generated by Deriv(M, I,O), this is exactlyWM(I,O).
The recursive definition of ? does not assume a non-recursive wRTG. In the
presence of derivation cycles with weights less than 1, ? can still be evaluated as a
convergent sum over an infinite number of trees.
The output of Deriv will always be non-recursive provided there are no cycles of
-rules in the transducer. There is usually no reason to build such cycles, as the effect
(in the unweighted case) is just to make all implicated states equivalent.
Outside weights ?G are for each nonterminal the sums over all its occurrences in
complete derivations in the wRTG of the weight of the whole tree, excluding the
occurrence subtree weight (we define this without resorting to division for cancellation,
but in practice we may use division by ?G(n) to achieve the same result).
?G(n ? N) ?
?
?
?
?
?
?
?
?
?
?
?
?
?
1 if n = S
uses of n in productions
? ?? ?
?
p,(n?,r,w)?P:labelr(p)=n
w ? ?G(n?) ?
?
p??pathsr(N)?{p}
?G(labelr(p
?))
? ?? ?
sibling nonterminals
otherwise.
Provided that useless nonterminals and productions were removed by Algorithm 2,
and none of the rule weights are 0, all of the nonterminals in a wRTG will have nonzero
? and ?. Conversely, if useless nonterminals weren?t removed, they will be detected
when computing inside?outside weights by virtue of their having zero values, so they
may be safely pruned without affecting the generated weighted tree language.
Finally, given inside and outside weights, the sum of weights of trees using a
particular production is ?G((n, r,w) ? P) ? ?G(n) ? w ? ?G(r). Here we rely on the com-
mutativity of the product (the left-out inside part reappears on the right of the inside
part, even when it wasn?t originally the last term).
Computing ?G and ?G for nonrecursive wRTG is a straightforward translation of
the recursive definitions (using memoization to compute each result only once) and is
O(|G|) in time and space. Or, without using memoization, we can take a topological sort
406
Graehl, Knight, and May Training Tree Transducers
using the dependencies induced by the equations for the particular forest, and compute
in that order. In case of a recursive wRTG, the equations may still be solved (usually
iteratively), and it is easy to guarantee that the sums converge by appropriately keeping
the rule weights of state-change productions less than one.
7. EM Training
Expectation-Maximization (EM) training (Dempster, Laird, and Rubin 1977) works on
the principle that the likelihood (product over all training examples of the sum of all
model derivations for it) can be maximized subject to some normalization constraint on
the parameters,8 by repeatedly:
1. Computing the expectation of decisions taken for all possible ways of
generating the training corpus given the current parameters, accumulating
(over each training example) parameter counts c of the portion of all
possible derivations using that parameter?s decision:
?? ? parameters :
c? ? Et?training
?
?
?
?
?
d?derivationst
(# of times ? used in d) ? pparameters(d)
?
d?derivationst
pparameters(d)
?
?
?
?
2. Maximizing by assigning the counts to the parameters and renormalizing:
?? ? parameters : ?? c?
Z?(c )
Each iteration is guaranteed to increase the likelihood until a local maximum is
reached. Normalization may be affected by tying or fixing of parameters. The deriva-
tions for training examples do not change, but the model weights for them do. Us-
ing inside?outside weights, we can efficiently compute these weighted sums over all
derivations for a wRTG, and thus, using Algorithm 1, over all xT derivations explaining
a given input/output tree pair.
A simpler version of Deriv that computes derivation trees for a wRTG given an
output tree could similarly be used to train weights for wRTG rules.9
Each EM iteration takes time linear in the size of the transducer and linear in the
size of the derivation tree grammars for the training examples. The size of the derivation
trees is at worstO(Gn2), so for a corpus ofN examples with maximum input/output size
n, an iteration takes at worst timeO(NGn2). Typically, we expect only a small fraction of
possible states and rules will apply to a given input/output subtree mapping.
8 Each parameter gives the probability of a single model decision, and a derivation?s probability is the
product of all the decisions producing it.
9 One may also use Deriv unmodified to train an identity (or constant-input) transducer with one rule
per wRTG production, having exactly the range of the wRTG in question, and of course transforming
training trees to appropriate tree pairs.
407
Computational Linguistics Volume 34, Number 3
The recommended normalization function computes the sum of all the counts for
rules having the same state, which results in trained model weights that give a joint
probability distribution over input/output tree pairs.
Attempts at conditional normalization can be problematic, unless the patterns for all
the rules of a given state can be partitioned into sets so that for any input, only patterns
from at most one set may match. For example, if all the patterns specify the label and
rank of the root, then they may be partitioned along those lines. Input-epsilon rules,
which always match (with pattern True), would make the distribution inconsistent by
adding extra probability mass, unless they are required (in what is no longer a partition)
to have their counts normalized against all the partitions for their state (because they
transform inputs that could fall in any of them).
One can always marginalize a joint distribution for a particular input to get true
conditional probabilities. In fact, no method of assigning rule weights can generally
compute exact conditional probabilities; remarginalization is already required: take as
the normalization constant the inside weight of the root derivation forest corresponding
to all the derivations for the input tree in question.
Even using normalization groups that lead to inconsistent probability distributions,
EM may still compute some empirically useful local maximum. For instance, placing
each q lhs in its own normalization group might be of interest; although the inside
weights of a derivation forest would sum to some s > 1, Train would divide the counts
earned by each participating rule by s (Algorithm 3).
8. Strings
We have covered tree-to-tree transducers; we now turn to tree-to-string transducers.
In the automata literature, such transductions are called generalized syntax-directed
translation (Aho and Ullman 1971), and are used to specify compilers that (deter-
ministically) transform high-level source-language trees into linear target-language
code. Tree-to-string transducers have also been applied to the machine translation of
natural languages (Yamada and Knight 2001; Eisner 2003). Tree-to-string transduction
is appealing when trees are only available on the input side of a training corpus.
Furthermore, tree/string relationships are less constrained than tree/tree, allowing
the possibility of simpler models to account for natural language transformations.
(Though we will not pursue it here, string-to-string training should also be possible
with tree-based models, if only string-pair data is available; string/string relations
induced by tree transformations are sometimes called translations in the automata
literature.)
? are the strings over alphabet ?. For s = (s1, . . . , sn), the length of s is |s| ? n and
the ith letter is s[i] ? si, for all i ? indicess ? {i ? N | 1 ? i ? n}. indicess(X) is the subset
{i ? indicess | i[s] ? X}. The letters in s are letterss = {l|?i ? indicess : s[i] = l}. The spans
of s are spanss = {(a, b) ? {N
2 | 1 ? a ? b ? n+ 1}, and the substring at span p = (a, b) of
s is s ? p ? (sa, . . . sb?1), with s ? (a, a) = (). We use the shorthand [i] ? (i, i+ 1) for all
i ? N, so s ? [i] = s[i]. The substitution of t for a span (a, b) ? spanss in s is s[(a, b)? t] ?
(s ? (1, a)) ? t ? (s ? (b,n+ 1)).10
A partition is a set of non-overlapping spans P??(a, b), (c, d) ? P : c ? d ? a ? b ?
c ? d ? (a, b) = (c, d), and the parallel substitution of s?p for the partition P of s is writ-
ten s[p? s?p,?p ? P]. In contrast to parallel tree substitution, we cannot take any
10 a ? b is string concatenation, defined already in Section 2.
408
Graehl, Knight, and May Training Tree Transducers
composition of the individual substitutions, because the replacement substrings may
be of different length, changing the referent of subsequent spans. It suffices to perform
a series of individual substitutions, in right to left order?(an, bn), . . . , (ai, bi), . . . , (a1, b1)
(ai ? bi+1,?1 ? i < n).
Algorithm 3. Train (EM training for tree transducers)
Input: xR transducerM = (?,?,Q,Qd,R) with initial rule weights, observed weighted
tree pairs T ? T? ? T? ? R+, minimum relative log-likelihood change for
convergence  ? R+, maximum number of iterations maxit ? N, and for each
rule r ? R: prior counts (for a Dirichlet prior) prior : R ? R for smoothing, and
normalization function Zr : (R ? R) ? R used to update weights from counts
w?r? count(r)/Zr(count).
Output: New rule weightsW ? {wr | r ? R}.
begin
for (i, o,w) ? T do
di,o?Deriv(M, i, o) // Algorithm 1
if di,o = false then
T?T ? {(i, o,w)}
Warn(more rules are needed to explain (i, o))
Compute inside?outside weights for di,o
If Algorithm 2 (RTGPrune) has not already been used to do so, remove all useless
nonterminals n (and associated rules) whose ?di,o (n) = 0 or ?di,o (n) = 0
i? 0, L???, ?? 
for r = (q, ?, rhs,w) ? R do wr?w
while ? ?  ? i < maxit do
for r ? R do count[r]? prior(r)
L?? 0
for (i, o,wexample) ? T / / Estimate
do
let D ? di,o ? (R,N,S,P)
compute ?D,?D using latestW ? {wr | r ? R} // see Section 6
for ? = (n, rhs,w) ? P do
?D(?)??D(n) ? w ? ?D(rhs)
let r ? labelrhs(())
count[r]? count[r]+ wexample ?
?D(?)
?D(S)
L?? L? + log?D(S) ? wexample
for r ? R / / Maximize
do
wr?
count[r]
Zr(count)
// e.g., joint
Zr(c ) ?
?
r?=(qr,d,e,f )?R
c(r?),?r = (qr, ?, rhs,w) ? R
?? L
? ? L
|L?|
L? L?, i? i+ 1
end
409
Computational Linguistics Volume 34, Number 3
9. Extended Tree-to-String Transducers (xTs)
A weighted extended-lhs root-to-frontier tree-to-string transducer M is a quintuple
(?,?,Q,Qi,R) where ? is the input alphabet, ? is the output alphabet, Q is a finite
set of states, Qi ? Q is the initial (or start, or root) state, and R ? Q? xTPAT? ? (? ?
(Q? paths)) ? R+ is a finite set of weighted transformation rules, written q ??w rhs. A
rule says that to transform an input subtree matching ? while in state q, replace it by
the string of rhs with its nonterminal (Q? paths) letters replaced by their (recursive)
transformation.
xTs is the same as xT, except that the rhs are strings containing some nonterminals
instead of trees containing nonterminal leaves. By taking the yields of the rhs of an xT
transducer?s rules, we get an xTs that derives exactly the weighted strings that are the
yields of the weighted trees generated by its progenitor.
As discussed in Section 1, we may consider strings as isomorphic to degener-
ate, monadic-spined right-branching trees, for example, the string (a, b, c) is the tree
C(a,C(b,C(c,END))). Taking the yield of such a tree, but with END yielding the empty
string, we have the corresponding string. We choose this correspondence instead of flat
trees (e.g., C(a, b, c)) because our derivation steps proceed top-down, choosing the states
for all the children at once (what?s more, we don?t allow symbols C to have arbitrary
rank). If all the rhs of an xTs transducer are transformed into such trees, then we have
an xT transducer. The yields of that transducer?s output trees for any input are the
same as the outputs of the xTs transducer for the same input, but again, only if END is
considered to yield the empty string. Note that in general the produced output trees will
not have the canonical right-branching monadic spine that we use to encode strings,11 so
that yield-taking is a nontrivial operation. Finally, consider that for a given transducer,
the same output yield may be derived via many output trees, which may differ in the
number and location of END, and in the branching structure induced by multi-variable
rhs. Because this leads to additional difficulties in inferring the possible derivations
given an observed output string, we must study tree-to-string relations apart from tree
relations.
Just as wRTG can generate PCFG derivation trees, xTs can generate tree/string pairs
comparable to a Synchronous CFG (SCFG), with the tree being the CFG derivation tree
of the SCFG input string, with one caveat: an epsilon leaf symbol (we have used END)
must be introduced which must be excluded from yield-taking, after which the string-
to-string translations are identical.
We define the binary derivation relation on (? ? (Q? T?)) ? (N? R)? (strings of
output letters and state-labeled input trees and their derivation history)
?M?
{
((a, h), (b, h ? (i, (q, ?, rhs,w)))) | ?(q, ?, rhs,w) ? R, i ? indicesa :
a[i] = (q, I) ? Q? T? ?
?(I) = 1 ?
b = a
[
[i]? rhs
[
[p]? (q?, I ? i?),
?p ? indicesrhs : rhs[p] = (q?, i?) ? Q? paths
]]
}
11 In the special case that all rhs contain at most one variable, and that every variable appears in the final
position of its rhs, the output trees do, in fact, have the same canonical monadic-spined form. For these
transducers there is no meaningful difference between xTs and xT.
410
Graehl, Knight, and May Training Tree Transducers
where at position i, an input tree I (labeled by state q) in the string a is replaced by
a rhs from a rule that matches it. Of course, the variables (q?, i?) ? Q? paths in the rhs
get replaced by the appropriate pairing of (q?, I ? i?). Each rewrite flattens the string of
trees by breaking one of the trees into zero or more smaller trees, until (in a complete
derivation) only letters from the output alphabet ? remain. As with xT, rules may only
apply if the paths in them exist in the input (if i? ? pathsI), even if the tree pattern doesn?t
mention them.
Let??M, D(M), LD(M), wM,WM, and L(M) (the weighted tree-to-string relation ofM)
follow from the single-step?M exactly as they did in Section 4.12
10. Parsing an xTs Tree-to-String Relation
Derivation trees for an xTs transducer are defined by an analogous xT transducer,
exactly as they were for derivation trees for xT, where the nodes are labeled by rules
to be applied preorder, with the ith child rewriting the ith variable in the rhs of its parent
node.
Algorithm 4 (SDeriv) is the tree-to-string analog of Algorithm 1 (Deriv), building a
tree grammar that generates all the weighted derivation trees explaining an observed
input tree/output string pair for an xTs transducer.
SDeriv differs from Deriv in the use of arbitrary output string spans instead of
output subtrees. The looser alignment constraint causes additional complexity: There
areO(m2) spans of an observed output stringO of lengthm, and each binary production
over a span has O(m) ways of dividing the span in two (we also have the n different
input subtrees and q different rule states).
There is no way to fix in advance a tree structure over the training example and
transducer rule output strings without constraining the derivations to be consistent with
the bracketing. Another way to think of this is that any xTs derivation implies a specific
tree bracketing over the output string. In order to compute the derivations using the
tree-to-tree Deriv, we would have to take the union of forests for all the possible output
trees with the given output yield.
SDeriv takes time and space linear to the size of the output: O(Gnm3) where G
combines the states and rules into a single grammar constant, and n is the size of the
input tree. The reduced O(m2) space bound from 1-best CFG parsing does not apply,
because we want to keep all successful productions and split points, not only the best
for each item.
We use the presence of terminals in the right hand side of rules to constrain the
alignments of output subspans to nonterminals, giving us minimal-sized subproblems
tackled by VarsToSpan.
The canonicalization of same-substring spans is most obviously applicable to zero-
length spans (which become (1, 1), no matter where they arose), but in the worst case,
every input label and output letter is unique, so nothing further is gained. Canonical-
ization may also be applied to input subtrees. By canonicalizing, we effectively name
subtrees and substrings by value, instead of by path/span, increasing best-case sharing
and reducing the size of the output. In practice, we still use paths and spans, and hash
to a canonical representative if desired.
12 Because the locations in derivation histories are string indexes now rather than tree paths, we use the
usual < on naturals as the ordering constraint for leftmost derivations.
411
Computational Linguistics Volume 34, Number 3
Algorithm 4. SDeriv (derivation forest for I??xTs O)
Input: xTs transducerM = (?,?,Q,Qi,R), observed input tree I ? T?, and output
string O = (o1, . . . , on) ? ??
Output: derivation wRTG G = (R ? {},N ? N?,S,P) generating all weighted
derivation trees forM that produce O from I, with
N? ? ((Q? pathsI ? spansO)?
(pathsI ? spansO ? (Q? paths)
?)). Returns false instead if there are no such trees.
begin
S? (Qi, (), (1,n)), N? ?, P??,memo??
if PRODUCEI,O(S) then
N?{n | ?(n?, rhs,w) ? P : n = n? ? n ? yieldrhs(N
?)}
return G = (R ? {},N,S,P)
else
return false
end
PRODUCEI,O(? = (q ? Q, in ? pathsI, out = (a, b) ? spansO)) returns boolean ? begin
if ?(?, r) ? memo then return r
memo?memo ? {(?, true)}
anyrule?? false
for rule = (q, pat, rhs,w) ? R : pat(I ? in) = 1 ? FeasibleO(rhs, out) do
(r1, . . . , rk)? indicesrhs(?) in increasing order
/* k? 0 if there are none */
p0? a? 1, pk+1? b
r0? 0, rk+1?|rhs|+ 1
for p = (p1, . . . , pk) : (?1 ? i ? k : O[pi] = rhs[ri])?
(?0 ? i ? k : pk < pk+1 ? (rk+1 ? rk = 1 =? pk+1 ? pk = 1)) do
/* for all alignments p between rhs[ri] and O[pi], such that
order, beginning/end, and immediate adjacencies in rhs
are observed in O. The degenerate k = 0 has just p = ().
*/
labelderivrhs(())? (rule)
v? 0
for i? 0 to k do
/* variables rhs ? (ri + 1, ri+1) must generate O ? (pi + 1, pi+1)
*/
if ri + 1 = ri+1 then next i
v? v+ 1
spangen? (in, (pi + 1, pi+1), rhs ? (ri + 1, ri+1))
n?VarsToSpanI,O(spangen)
if n = false then next p
labelandrankderivrhs((v))? (n, 0)
anyrule?? true
rankderivrhs(()) = v
P?P ? {?, derivrhs,w)}
memo?memo ? {(?,anyrule?)}
return anyrule?
end
FeasibleO(rhs, span) ? ?l ? lettersrhs : l ? ? =? l ? lettersO?span
412
Graehl, Knight, and May Training Tree Transducers
Algorithm SDeriv (cont.) -labeled nodes are generated as artifacts of sharing by
cons-nonterminals of derivations for the same spans.
VarsToSpanI,O
(wholespan = (in ? pathsI, out = (a, b) ? spansO,nonterms ? (Q? paths)
?)) returns
N? ? {false} ?
/* Adds all the productions that can be used to map from parts of the
nonterminal string referring to subtrees of I ? in into O ? out and
returns the appropriate derivation-wRTG nonterminal if there was a
completely successful derivation, or false otherwise. */
begin
ret? false
if |nonterms| = 1 then
(q?, i?)? nonterms[1]
if PRODUCEI,O(q
?, in ? i?, out) then return (q?, in ? i?, out)
return false
wholespan? (in,CANONICALO(out),nonterms)
if ?(wholespan, r) ? memo then return r
for s? b to a do
/* the first nonterminal will cover the span (a,s) */
(q?, i?)? nonterms[1] /* nonterms will never be empty */
spanfirst? (q?, i ? i?, (a, s))
if ?PRODUCEI,O(spanfirst) then next s
labelspanlist(())? 
/* cons node for sharing; left child expands to rules used for this
nonterminal, right child expands to rest of nonterminal/span
derivation */
labelandrankspanlist((1))? (spanfirst, 0)
/* first child: expansions of first nonterminal */
rankspanlist(())? 2
spanrest? (in, (s, b),nonterms ? (2, |nonterms|+ 1))
/* second child: expansions of rest of nonterminals */
n?VarsToSpanI,O(spanrest)
if n = false then next s
labelandrankspanlist((2))? (n, 0)
P?P ? (wholespan, spanlist, 1)
ret?wholespan
memo?memo ? {(wholespan,ret)}
return ret
end
CANONICALO((a, b)) ? min{(x, y) | O ? (x, y) = O ? (a, b) ? x ? 1}
The enumeration of matching rules and alignments of terminals in the rule rhs to
positions in the output substring is best interleaved; the loops are nested for clarity
of presentation only. We use an FSA of subsequences of the output string (skipping
forward to a desired letter in constant time with an index on outgoing transitions), and
a trie of the rules? outputs (grouping by collapsing rhs variable sequences into a single
?skip? symbol), and intersect them, visiting alignments and sets of rules in the rule
413
Computational Linguistics Volume 34, Number 3
index. The choice of expansion sites against an input subtree proceeds by exhaustive
backtracking, since we want to enumerate all matching patterns. Each of these sets of
rules is further indexed against the input tree in a kind of leftmost trie.13 Feasible is
redundant in the presence of such indexing.
Static grammar analysis could also show that certain transducer states always (or
never) produce an empty string, or can only produce a certain subset of the terminal al-
phabet. Such proofs would be used to restrict the alignments considered in VarsToSpan.
We have modified the usual derivation tree structure to allow sharing the ways
an output span may align to a rhs substring of multiple consecutive variables; as a
consequence, we must create some non-rule-labeled nodes, labeled by  (with rank 2).
Train collects counts only for rule-labeled nodes, and the inside?outside weight compu-
tations proceed in ignorance of the labels, so we get the same sums and counts as if we
had non-binarized derivation trees. Instead of a consecutive rhs variable span of length
n generating n immediate rule-labeled siblings, it generates a single right-branching
binarized list of length n with each suffix generated from a (shared) nonterminal. As
in LISP, the left child is the first value in the list, and the right child is the (binarized)
rest of the list. As the base case, we have (n1,n2) as a list of two nonterminals (single
variable runs refer to their single nonterminal directly without any  wrapping; we use
no explicit null list terminator). Just as in CFG parsing, it would be necessary without
binarization to consider exponentially many productions, corresponding to choosing
an n-partition of the span length; the binarized nonterminals in our derivation RTG
effectively share the common suffixes of the partitions.
SDeriv could be restated in terms of parsing with a binarized set of rules, where
only some of the binary nonterminals have associated input trees; however, this would
complicate collecting counts for the original, unbinarized transducer rules.
If there are many cyclical state-change transitions (e.g., q x0 ? q? x0), a nearly
worst-case results for the memoized top-down recursive descent parsing of SDeriv,
because for every reachable alignment, nearly every state would apply (but after prun-
ing, the training proceeds optimally). An alternative bottom-up SDeriv would be better
suited in general to input-epsilon heavy transducers (where there is no tree structure
consumed to guide the top-down choice of rules). The worst-case time and space
bounds would be the same, but (output) lexical constraints would be used earlier.
The weighted derivation tree grammar produced by SDeriv may be used (after re-
moving useless productions with Algorithm 2) exactly as before to perform EM train-
ing with Train. In doing so, we generalize the standard inside?outside training of
probabilistic context-free grammar (PCFG) on raw text (Baker 1979). In Section 12,
we demonstrate this by creating an xTs transducer that transforms a fixed single-node
dummy tree to the strings of some arbitrary CFG, and train it on a corpus in which the
dummy input tree is paired with each training string as its output.
11. Translation Modeling Experiment
It is possible to cast many current probabilistic natural language models as T-type tree
transducers. In this section, we implement the translation model of Yamada and Knight
(2001) and train it using the EM algorithm.
13 To make a trie of complete tree patterns, represent them canonically as strings interleaving paths leftmost
for expansion, and labelandrank that must agree with the concurrent location in the input tree.
414
Graehl, Knight, and May Training Tree Transducers
Figure 6 shows a portion of the bilingual English-tree/Japanese-string corpus used
in Yamada and Knight (2001) and here. Figures 7 and 8 show the generative model and
parameters; the parameter values shown were learned via specialized EM re-estimation
formulae described in this article?s appendix. According to the model, an English tree
becomes a Japanese string in four steps.
First, every node is re-ordered, that is, its children are permuted probabilistically.
If there are three children, then there are six possible permutations whose probabilities
add up to 1. The re-ordering depends only on the child label sequence, and not on any
wider or deeper context. Note that the English trees in Figure 6 are already flattened in
pre-processing because the model cannot perform complex re-orderings such as the one
we described in Section 1, S(PRO,VP(V,NP))? V, PRO, NP.
Figure 6
A portion of a bilingual tree/string training corpus.
415
Computational Linguistics Volume 34, Number 3
Figure 7
The translation model of Yamada and Knight (2001).
Figure 8
The parameter tables of Yamada and Knight (2001).
Second, at every node, a decision is made about inserting a Japanese function word.
This is a three-way decision at each node?insert to the left, insert to the right, or do not
insert?and it depends on the labels of the node and its parent.
Third, English leaf words are translated probabilistically into Japanese, independent
of context.
Fourth, the internal nodes are removed, leaving only the Japanese string.
416
Graehl, Knight, and May Training Tree Transducers
This model effectively provides a formula for P(Japanese string | English tree) in
terms of individual parameters, and EM training seeks to maximize the product of these
conditional probabilities across the whole tree/string corpus.
We now build a trainable xTs tree-to-string transducer that embodies the same
P(Japanese string | English tree).
It is a four-state transducer. For the main state (and start state) q, meaning ?translate
this (sub)tree,? we have three rules:
q x0? i x0, r x0
q x0? r x0, i x0
q x0? r x0
State i means ?produce a Japanese function word out of thin air.? We include an i
rule for every Japanese word in the vocabulary:
i x0? ?de?
i x0? ?kuruma?
i x0? ?wa?
. . .
State r means ?re-order my children and then recurse.? For internal nodes, we
include a rule for every parent/child sequence and every permutation thereof:
r NN(x0:CD, x1:NN)? q x0, q x1
r NN(x0:CD, x1:NN)? q x1, q x0
. . .
The rhs sends the child subtrees back to state q for recursive processing. However,
for English leaf nodes, we instead transition to a different state t, so as to prohibit any
subsequent Japanese function word insertion:
r NN(x0:?car?)? t x0
r CC(x0:?and?)? t x0
. . .
State t means ?translate this word,? and we have a rule for every pair of co-
occurring English and Japanese words:
t ?car?? ?kuruma?
t ?car?? ?wa?
t ?car?? *e*
. . .
This follows Yamada and Knight (2001) in also allowing English words to disappear
(the rhs of the last rule is an empty string).
Every rule in the xTs transducer has an associated weight and corresponds to
exactly one of the model parameters.
The transducer just described, which we will subsequently call simple, is unfaithful
in one respect so far: The insert-function-word decision is independent of context,
whereas Yamada and Knight (2001) specifies it is conditioned on the node and parent
labels. We modify the simple transducer into a new exact transducer by replacing the q
417
Computational Linguistics Volume 34, Number 3
state with a set of states of the form q.parent, indicating the parent symbol of the current
node being processed. The start state then becomes q.TOP, and the q rules are rewritten
to specify the current node. Thus, every parent/child pair in the corpus gets its own set
of insert-function-word rules:
q.TOP x0:VB? i x0, r x0
q.TOP x0:VB? r x0, i x0
q.TOP x0:VB? r x0
q.VB x0:NN? i x0, r x0
q.VB x0:NN? r x0, i x0
q.VB x0:NN? r x0
. . .
The r rules now need to send parent information when they recurse to the q.parent
states:
r NN(x0:CD, x1:NN)? q.NN x0, q.NN x1
r NN(x0:CD, x1:NN)? q.NN x1, q.NN x0
. . .
The i and t rules stay the same.
This modification adds to our new transducer model all the contextual information
specified in Yamada and Knight (2001). However, upon closer inspection one can see
that the exact transducer is in fact overspecified in the reordering, or r rules. Yamada
and Knight only conditions reordering on the child sequence, thus, for example, the
reordering of JJ(JJ NN) is not distinct from the reordering of NN(JJ NN). As specified
in Train a separate parameter is estimated for each rule in the transducer. We thus
introduce rule tying to ensure the exact transducer is not misnamed. By designating
a set of transducer rules as tied we indicate that a single count collection and parameter
estimation is performed for the entire set during Train. We denote tied rules by marking
each rule in the same tied class with the symbol @ and a common integer. Thus the JJ(JJ
NN) and NN(JJ NN) reordering rules described previously are modified as follows:
r JJ(x0:JJ, x1:NN)? q.JJ x0, q.JJ x1 @ 1
r JJ(x0:JJ, x1:NN)? q.JJ x1, q.JJ x0 @ 2
r NN(x0:JJ, x1:NN)? q.NN x0, q.NN x1 @ 1
r NN(x0:JJ, x1:NN)? q.NN x1, q.NN x0 @ 2
All reordering rules with the same input and output variable sequence are in the
same tied class, and thus receive the same probability, independent of their parent
symbols. We consider the four-state transducer initially specified as our simple model,
and the modification that introduces parent-dependent q states and tied reordering
rules as the exact model, since it is a precise xTs transducer formulation of the model
of Yamada and Knight (2001).
As a means of providing empirical evidence of the utility of this approach, we
built both the simple and exact transducers and trained them using the EM algorithm
described in Section 7. We next compare the alignments and transition probabilities
achieved by generic tree transducer operations with the model-specific implementation
of Yamada and Knight (2001).
We obtained the corpus used as training data in Yamada and Knight (2001). This
corpus is a set of 2,060 Japanese/English sentence pairs from a dictionary, preprocessed
418
Graehl, Knight, and May Training Tree Transducers
Table 2
A comparison of the three transducer models used to simulate the model of Yamada and
Knight (2001).
model states initial rules rules after training time % link match % sent. match
training (hours)
simple 4 98,033 12,413 16.95 87.42 52.66
exact 28 98,513 12,689 17.42 96.58 81.46
perfect 29 186,649 24,492 53.19 99.85 99.47
as described in Yamada and Knight. There are on average 6.9 English words per sen-
tence and sentences range in size from 2 to 20 words. We built the simple and exact
unweighted transducers described above; Table 2 summarizes their initial sizes. The
exact model has 24 more states than the simple; this is due to the parent-dependent
modification to q. The 480 additional rules are due to insertion rules dependent on
parent and child information.
We then ran our training algorithm on the unweighted transducers and the training
corpus. Because the derivation tree grammars produced by SDeriv can be large and
time-intensive to compute, we calculated them once prior to training, saved them
to disk, and then read them at each iteration of the training algorithm.14 Following
Yamada and Knight (2001), we chose a normalization partition (Z in Train) such that
we obtain the probabilities of all the rules given their complete left hand side,15 and
set the Dirichlet prior counts uniformly to 0. We ran 20 iterations of the EM algorithm
using Train. The time to construct derivation forests and run 20 iterations of EM for
the various models is in Table 2. Note also the size of the transducers after training in
Table 2; a rule is considered to be no longer in the transducer if it is estimated to have
conditional probability 0.0001 or less.
Because we are trying to duplicate the training experiment of Yamada and Knight
(2001), we wish to compare the word-to-word alignments discovered by that work to
those discovered by ours. We recovered alignments from our trained transducers as
follows: For each tree/string pair we obtained the most likely sequence of rules that
derives the output string from an input tree, the Viterbi derivation. Figure 9 shows the
Viterbi derivation tree and rules for an example sentence. By following the sequence of
applied rules we can also determine which English words translate to which Japanese
words, and thus construct the Viterbi word alignment. We obtained the full set of align-
ments induced in Yamada and Knight and compared them to the alignments learned
from our transducers.
In Table 2 we report link match accuracy16 as well as sentence match accuracy.
The simple transducer is clearly only a rough approximation of the model of Yamada
and Knight (2001). The exact model is much closer, but the low percentage of exact
sentence matches is a concern. When comparing the parameter table values reported
by Yamada and Knight with our rule weights we see that the two systems learned
14 In all models the size on disk in native Java binary object format was about 2.7 GB.
15 Zr(c ) ?
?
r?=(qr ,?,e,f )?R c(r
? ),?r = (qr, ?, g, h) ? R.
16 As this model induces 1-to-1 word alignments, we report accuracy as the number of links matching those
reported by Yamada and Knight (2001) as a percentage of the total number of links.
419
Computational Linguistics Volume 34, Number 3
Figure 9
A Viterbi derivation tree and the referenced rules.
different probability distributions in multiple instances. A sample of these parameter
value differences can be seen in Figure 10.
In an effort to determine the reason for the discrepancy in weights between the
parameter values learned in our exact transducer representation of Yamada and Knight
(2001), we contacted the authors17 and learned that, unreported in the paper, the original
code contained a constraint that specifically bars an unaligned foreign word insertion
immediately prior to a NULL English word translation. We incorporate this change to
our model by simply modifying our transducer, rather than by changing our program-
ming code. The new transducer, which we call perfect, is a modification of the exact
transducer as follows.
We introduce an additional state s, denoting a translation taking place immediately
after an unaligned foreign function word insertion. We then introduce the following
additional rules.
For every rule that inserts a foreign function word, add an additional rule denoting
an insertion immediately before a translation, and tie these rules together, for example:
q.VB x0:NN? i x0, r x0 @ 23
q.VB x0:NN? i x0, s x0 @ 23
q.VB x0:NN? r x0, i x0 @ 24
q.VB x0:NN? s x0, i x0 @ 24
. . .
To allow subsequent translation, ?transition? rules for state s analogous to the
transition rules described previously must also be added, for example:
s NN(x0:?car?)? s x0
s CC(x0:?and?)? s x0
. . .
17 We are grateful to Kenji Yamada for providing full parameter tables and Viterbi alignments from the
original source.
420
Graehl, Knight, and May Training Tree Transducers
Figure 10
Rule probabilities corresponding to the parameter tables of Yamada and Knight (2001).
Finally, for each non-null translation rule, add an identical translation rule starting
with s instead of t, and tie these rules, for example:
t ?car?? ?kuruma? @ 54
t ?car?? ?wa? @ 55
t ?car?? *e*
s ?car?? ?kuruma? @ 54
s ?car?? ?wa? @ 55
. . .
Note that there is no corresponding null translation rule from state s; this is in
accordance with the insertion/NULL translation restriction.
As can be seen in Table 2 the Viterbi alignments learned from this ?perfect?
transducer are virtually identical to those reported in Yamada and Knight (2001). No
421
Computational Linguistics Volume 34, Number 3
rule probability in the learned transducer differs from its corresponding parameter
value in the original table by more than 0.000066. The 11 sentences with different
alignments, which account for 0.53% of the corpus, were due to two derivations
having the same probability; this was true in Yamada and Knight (2001) as well,
and the choice between equal-scoring derivations is arbitrary. Transducer rules that
correspond to the parameter tables presented in Figure 8 and a comparison of their
learned weights over the three models with the weight learned in Yamada and Knight
are in Figure 10. Note that the final perfect model matches the original parameter
tables perfectly, indicating we can reproduce complicated models with our transducer
formalism.
There are several benefits to this xTs formulation. First, it makes the model very
clear, in the same way that Knight and Al-Onaizan (1998) and Kumar and Byrne (2003)
elucidate other machine translation models in easily grasped FST terms. Second, the
model can be trained with generic, off-the-shelf tools?versus the alternative of working
out model-specific re-estimation formulae and implementing custom training software,
whose debugging is a significant engineering challenge. Third, we can easily extend the
model in interesting ways. For example, we can add rules for multi-level and lexical
re-ordering:
r NP(x0:NP, PP(IN(?of?), x1:NP))? q x1, ?no?, q x0
We can eschew pre-processing that flattens trees prior to training, and instead
incorporate flattening rules into the explicit model.
We can add rules for phrasal translations:
r NP(JJ(?big?), NN(?cars?))? ?ooki?, ?kuruma?
This can include non-constituent phrasal translations:
r S(NP(PRO(?there?)), VP(VB(?are?)), x0:NP)? q x0, ?ga?, ?arimasu?
Such non-constituent phrase pairs are commonly used in statistical machine translation
(Och, Tillmann, and Ney 1999; Marcu and Wong 2002) and are vital to accuracy (Koehn,
Och, and Marcu 2003). We can also eliminate many epsilon word-translation rules in
favor of more syntactically-controlled ones, for example:
r NP(DT(?the?), x0:NN)? q x0
Removing epsilons serves to reduce practical complexity in training and especially in
decoding (Yamada and Knight 2002).
We can make many such changes without modifying the training procedure, as long
as we stick to the tree automata.
The implementation of EM training we describe here is part of Tiburon, a generic
weighted tree automata toolkit described in May and Knight (2006) and available at
http://www.isi.edu/licensed-sw/tiburon/.
422
Graehl, Knight, and May Training Tree Transducers
12. PCFG Modeling Experiment
In this section, we demonstrate another application of the xTs training algorithm. We
show its generality by applying it to the standard task of training a probabilistic context-
free grammar (PCFG) on string examples. Consider the following grammar:
S? NP VP
NP? DT N
NP? NP PP
PP? P NP
VP? V NP
VP? V NP PP
DT? the N? the V? the P? the
DT? window N? window V? window P? window
DT? father N? father V? father P? father
DT? mother N? mother V? mother P? mother
DT? saw N? saw V? saw P? saw
DT? sees N? sees V? sees P? sees
DT? of N? of V? of P? of
DT? through N? through V? through P? through
Also consider the following observed string data:
the father saw the window
the father saw the mother through the window
the mother sees the father of the mother
We would like to assign probabilities to the grammar rules such that the probability of
the string data is maximized (Baker 1979; Lari and Young 1990). We can exploit the xTs
training algorithm by pretending that each string was probabilistically transduced from
a tree consisting of the single node ?. All we require is to transform the grammar into
an xTs transducer:
Start state: qs
qs x0? qnp x0, qvp x0
qnp x0?0.99 qdt x0, qn x0
qnp x0?0.01 qnp x0, qpp x0
qpp x0? qp x0, qnp x0
qvp x0?0.99 qv x0, qnp x0
qvp x0?0.01 qv x0, qnp x0, qpp x0
qdt ? ? the qn ? ? the qv ? ? the qp ? ? the
qdt ? ? window qn ? ? window qv ? ? window qp ? ? window
qdt ? ? father qn ? ? father qv ? ? father qp ? ? father
qdt ? ? mother qn ? ? mother qv ? ? mother qp ? ? mother
qdt ? ? saw qn ? ? saw qv ? ? saw qp ? ? saw
qdt ? ? sees qn ? ? sees qv ? ? sees qp ? ? sees
qdt ? ? of qn ? ? of qv ? ? of qp ? ? of
qdt ? ? through qn ? ? through qv ? ? through qp ? ? through
423
Computational Linguistics Volume 34, Number 3
We also transform the observed string data into tree/string pairs:
? ? the father saw the window
? ? the father saw the mother through the window
? ? the mother sees the father of the mother
After running the xTs training algorithm, we obtain maximum likelihood values for the
rules. For example, after one iteration, we find the following values for rules that realize
verbs:
qv ? ?0.11 of
qv ? ?0.11 through
qv ? ?0.22 sees
qv ? ?0.56 saw
After more iterations, values converge to:
qv ? ?0.33 sees
qv ? ?0.67 saw
Viterbi parses for the strings can also be obtained from the derivations forests computed
by the SDeriv procedure. We note that our use of xTs training relies on copying.18
13. Related and Future Work
Concrete xLNT transducers are similar to (weighted) Synchronous TSG (STSG). STSG,
like TSG, conflate tree labels with states, and so cannot reproduce all the relations in
L(xLNT) without a subsequent relabeling step, although in some versions the root labels
of the STSG rules? input and output trees are allowed to differ. Regular lookahead19 for
deleted input subtrees could be added explicitly to xT. Eisner (2003) briefly discusses
training for STSG. For bounded trees, xTs can be represented as an FST (Bangalore and
Riccardi 2002).
Our training algorithm is a generalization of forward?backward EM training
for finite-state (string) transducers, which is in turn a generalization of the origi-
nal forward?backward algorithm for Hidden Markov Models. Eisner (2002) describes
string-based training under different semirings, and Carmel (Graehl 1997) imple-
ments FST string-to-string training. In our tree-based training algorithm, inside?outside
weights replace forward?backward, and paths in trees replace positions in strings. Ex-
plicit construction and pruning of derivation trees saves time over many EM iterations,
and could accelerate string-to-string training as well.
Yamada and Knight (2001) give a training algorithm for a specific tree-to-string
machine translation model. Gildea (2003) introduces a variation of tree-to-tree mapping
that allows for cloning (copying a subtree into an arbitrary position elsewhere), in order
18 Curiously, these rules can have ?x0? in place of ???, because the training routine also supports deleting
transducers. Such a transducer would transform any input tree to the output PCFG.
19 Tree patterns ? of arbitrary regular tree languages, as described in Engelfriet (1977).
424
Graehl, Knight, and May Training Tree Transducers
to better robustly model the substantial tree transformations found in human language
translation data.
Using a similar approach to Deriv, exploiting the independence (except on state) of
input-subtree/output-subtree mappings, we can build wRTG for the xT derivation trees
matching an observed input tree (forward application), or matching an observed output
tree (backward application).20 For backward application through concrete transducers,
each derivation tree implies a unique input tree, except where deletion occurs (the
deleted input subtree could have been anything). For copying transducers, backward
application requires wRTG intersection in order to ensure that only input-subtree hy-
potheses possible for all their derived output subtrees are allowed. For noncopying
xTs transducers with complete tree patterns, backward application is just exhaustive
context-free grammar parsing, generating a wRTG production from the left-hand-side
of each xTs rule instance applied in parsing. Training and backward application algo-
rithms for xTs can be extended in the usual way to parse given finite-state output lattices
instead of single strings.21
14. Conclusion
We have motivated the use of tree transducers for natural language processing, and
presented algorithms for training them. The tree-input/tree-output algorithm runs in
O(Gn2) time and space, whereG is a grammar constant, n is the total size of the tree pair,
and the tree-input/string-output algorithm runs in O(Gnm3) time and space, where n is
the size of the input tree and m is the size of the output string. Training works in both
cases by building the derivation forest for each example, pruning it, and then (until
convergence) collecting fractional counts for rules from those forests and normalizing.
We have also presented an implementation and experimental results.
References
Aho, A. V. and J. D. Ullman. 1971.
Translations of a context-free grammar.
Information and Control, 19:439?475.
Alshawi, Hiyan, Srinivas Bangalore,
and Shona Douglas. 2000. Learning
dependency translation models as
collections of finite state head transducers.
Computational Linguistics, 26(1):45?60.
Baker, J. K. 1979. Trainable grammars
for speech recognition. In Speech
Communication Papers for the 97th Meeting
of the Acoustical Society of America,
pages 547?550, Boston, MA.
Bangalore, Srinivas and Owen Rambow.
2000. Exploiting a probabilistic hierarchical
model for generation. In International
Conference on Computational Linguistics
(COLING 2000), pages 42?48, Saarbrucken,
Germany.
Bangalore, Srinivas and Giuseppe Riccardi.
2002. Stochastic finite-state models for
spoken language machine translation.
Machine Translation, 17(3):165?184.
Baum, L. E. and J. A. Eagon. 1967. An
inequality with application to statistical
estimation for probabilistic functions
of Markov processes and to a model
for ecology. Bulletin of the American
Mathematicians Society, 73:360?363.
Charniak, Eugene. 2001. Immediate-head
parsing for language models. In
Proceedings of the 39th Annual Meeting
of the Association for Computational
Linguistics, pages 116?123, Tolouse,
France.
Chelba, C. and F. Jelinek. 2000. Structured
language modeling. Computer Speech and
Language, 14(4):283?332.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the ACL (jointly with the 8th Conference
of the EACL), pages 16?23, Madrid, Spain.
20 In fact, forward and backward application can also be made to work on wRTG tree sets, with the result
still being a wRTG of possible derivations, except in the case of forward application with copying.
21 Instead of pairs of string indices, spans are pairs of lattice states.
425
Computational Linguistics Volume 34, Number 3
Comon, H., M. Dauchet, R. Gilleron,
F. Jacquemard, D. Lugiez, S. Tison,
and M. Tommasi. 1997. Tree automata
techniques and applications. Available at
http://www.grappa.univ-lille3.fr/tata.
Release of 12 October 2007.
Corston-Oliver, Simon, Michael Gamon,
Eric K. Ringger, and Robert Moore.
2002. An overview of Amalgam: A
machine-learned generation module.
In Proceedings of the International Natural
Language Generation Conference,
pages 33?40, New York.
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete data via the em algorithm.
Journal of the Royal Statistical Society,
Series B, 39(1):1?38.
Doner, J. 1970. Tree acceptors and some of
their applications. Journal of Computer and
System Sciences, 4:406?451.
Eisner, Jason. 2002. Parameter estimation for
probabilistic finite-state transducers. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 1?8, Philadelphia, PA.
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(companion volume), pages 205?208,
Sapporo, Japan.
Engelfriet, Joost. 1975. Bottom-up and
top-down tree transformations?a
comparison.Mathematical Systems
Theory, 9(3):198?231.
Engelfriet, Joost. 1977. Top-down tree
transducers with regular look-ahead.
Mathematical Systems Theory, 10:289?303.
Engelfriet, Joost, Zolt?n F?l?p, and Heiko
Vogler. 2004. Bottom-up and top-down
tree series transformations. Journal of
Automata, Languages and Combinatorics,
7(1):11?70.
F?l?p, Zolt?n and Heiko Vogler. 2004.
Weighted tree transducers. Journal of
Automata, Languages and Combinatorics,
9(1):31?54.
G?cseg, Ferenc and Magnus Steinby. 1984.
Tree Automata. Akad?miai Kiad?,
Budapest.
Gildea, Daniel. 2003. Loosely tree-based
alignment for machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 80?87, Sapporo, Japan.
Graehl, Jonathan. 1997. Carmel finite-state
toolkit. Available at http://www.isi.edu/
licensed-sw/carmel/.
Graehl, Jonathan and Kevin Knight. 2004.
Training tree transducers. In HLT-NAACL
2004: Main Proceedings, pages 105?112,
Boston, MA.
Hopcroft, John and Jeffrey Ullman. 1979.
Introduction to Automata Theory, Languages,
and Computation. Addison-Wesley Series
in Computer Science. Addison-Wesley,
London.
Joshi, Aravind and Yves Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of
Formal Languages, volume 3. Springer,
Berlin, pages 69?124.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Knight, Kevin and Yaser Al-Onaizan. 1998.
Translation with finite-state devices. In
Proceedings of the 3rd Conference of the
Association for Machine Translation in
the Americas on Machine Translation
and the Information Soup (AMTA-98),
pages 421?437, Berlin.
Knight, Kevin and Daniel Marcu. 2002.
Summarization beyond sentence
extraction: A probabilistic approach
to sentence compression. Artificial
Intelligence, 139(1):91?107.
Koehn, Phillip, Franz Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In HLT-NAACL 2003: Main
Proceedings, pages 127?133, Edmonton,
Alberta, Canada.
Kuich, Werner. 1999. Tree transducers and
formal tree series. Acta Cybernetica,
14:135?149.
Kumar, Shankar and William Byrne. 2003.
A weighted finite state transducer
implementation of the alignment template
model for statistical machine translation.
In HLT-NAACL 2003: Main Proceedings,
pages 142?149, Edmonton, Alberta,
Canada.
Langkilde, Irene. 2000. Forest-based
statistical sentence generation. In
Proceedings of the 6th Applied Natural
Language Processing Conference,
pages 170?177, Seattle, WA.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings
of the Conference of the Association for
Computational Linguistics (COLING/ACL),
pages 704?710, Montreal, Canada.
Lari, K. and S. J. Young. 1990. The estimation
of stochastic context-free grammars using
426
Graehl, Knight, and May Training Tree Transducers
the inside?outside algorithm. Computer
Speech and Language, 4(1):35?56.
Marcu, Daniel and William Wong. 2002.
A phrase-based, joint probability
model for statistical machine translation.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 133?139,
Philadelphia, PA.
May, Jonathan and Kevin Knight. 2006.
Tiburon: A weighted tree automata
toolkit. Implementation and Application of
Automata: 10th International Conference,
CIAA 2005, volume 4094 of Lecture
Notes in Computer Science, pages 102?113,
Taipei, Taiwan.
Nederhof, Mark-Jan and Giorgio Satta. 2002.
Parsing non-recursive CFGs. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 112?119, Philadelphia, PA.
Och, Franz, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint Conference of
Empirical Methods in Natural Language
Processing and Very Large Corpora,
pages 20?28, College Park, MD.
Pang, Bo, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple
translations extracting paraphrases and
generating new sentences. In HLT-NAACL
2003: Main Proceedings, pages 181?188,
Edmonton, Alberta, Canada.
Rounds, William C. 1970. Mappings and
grammars on trees.Mathematical Systems
Theory, 4(3):257?287.
Schabes, Yves. 1990.Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, Department of
Computer and Information Science,
University of Pennsylvania.
Thatcher, James W. 1970. Generalized2
sequential machine maps. Journal of
Computer and System Sciences, 4:339?367.
Viterbi, Andrew. 1967. Error bounds for
convolutional codes and an asymptotically
optimum decoding algorithm. IEEE
Transactions on Information Theory,
IT-13:260?269.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics,
pages 523?530, Tolouse, France.
Yamada, Kenji and Kevin Knight. 2002. A
decoder for syntax-based statistical MT. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 303?310, Philadelphia, PA.
427

Syntax-based Alignment of Multiple Translations: Extracting Paraphrases
and Generating New Sentences
Bo Pang
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
pabo@cs.cornell.edu
Kevin Knight and Daniel Marcu
Information Sciences Institute
University of Southern California
Marina Del Rey, CA 90292 USA
{knight,marcu}@isi.edu
Abstract
We describe a syntax-based algorithm that au-
tomatically builds Finite State Automata (word
lattices) from semantically equivalent transla-
tion sets. These FSAs are good representa-
tions of paraphrases. They can be used to ex-
tract lexical and syntactic paraphrase pairs and
to generate new, unseen sentences that express
the same meaning as the sentences in the input
sets. Our FSAs can also predict the correctness
of alternative semantic renderings, which may
be used to evaluate the quality of translations.
1 Introduction
In the past, paraphrases have come under the scrutiny
of many research communities. Information retrieval re-
searchers have used paraphrasing techniques for query re-
formulation in order to increase the recall of information
retrieval engines (Sparck Jones and Tait, 1984). Natural
language generation researchers have used paraphrasing
to increase the expressive power of generation systems
(Iordanskaja et al, 1991; Lenke, 1994; Stede, 1999).
And researchers in multi-document text summarization
(Barzilay et al, 1999), information extraction (Shinyama
et al, 2002), and question answering (Lin and Pantel,
2001; Hermjakob et al, 2002) have focused on identi-
fying and exploiting paraphrases in the context of recog-
nizing redundancies, alternative formulations of the same
meaning, and improving the performance of question an-
swering systems.
In previous work (Barzilay and McKeown, 2001; Lin
and Pantel, 2001; Shinyama et al, 2002), paraphrases
are represented as sets or pairs of semantically equiva-
lent words, phrases, and patterns. Although this is ade-
quate in the context of some applications, it is clearly too
weak from a generative perspective. Assume, for exam-
ple, that we know that text pairs (stock market rose, stock
market gained) and (stock market rose, stock prices rose)
have the same meaning. If we memorized only these two
pairs, it would be impossible to infer that, in fact, con-
sistent with our intuition, any of the following sets of
phrases are also semantically equivalent: {stock market
rose, stock market gained, stock prices rose, stock prices
gained } and {stock market, stock prices } in the con-
text of rose or gained; {market rose }, {market gained
}, {prices rose } and {prices gained } in the context of
stock; and so on.
In this paper, we propose solutions for two problems:
the problem of paraphrase representation and the problem
of paraphrase induction. We propose a new, finite-state-
based representation of paraphrases that enables one to
encode compactly large numbers of paraphrases. We also
propose algorithms that automatically derive such repre-
sentations from inputs that are now routinely released in
conjunction with large scale machine translation evalu-
ations (DARPA, 2002): multiple English translations of
many foreign language texts. For instance, when given
as input the 11 semantically equivalent English transla-
tions in Figure 1, our algorithm automatically induces the
FSA in Figure 2, which represents compactly 49 distinct
renderings of the same semantic meaning. Our FSAs
capture both lexical paraphrases, such as {fighting, bat-
tle}, {died, were killed} and structural paraphrases such
as {last week?s fighting, the battle of last week}. The
contexts in which these are correct paraphrases are also
conveniently captured in the representation.
In previous work, Langkilde and Knight (1998) used
word lattices for language generation, but their method
involved hand-crafted rules. Bangalore et al (2001) and
Barzilay and Lee (2002) both applied the technique of
multi-sequence alignment (MSA) to align parallel cor-
pora and produced similar FSAs. For their purposes,
they mainly need to ensure the correctness of consensus
among different translations, so that different constituent
orderings in input sentences do not pose a serious prob-
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 102-109
                                                         Proceedings of HLT-NAACL 2003
1. At least 12 people were killed in the battle last week. 2. At least 12 people lost their lives in last week?s fighting.
3. Last week?s fight took at least 12 lives. 4. The fighting last week killed at least 12.
5. The battle of last week killed at least 12 persons. 6. At least 12 persons died in the fighting last week.
7. At least 12 died in the battle last week. 8. At least 12 people were killed in the fighting last week.
9. During last week?s fighting, at least 12 people died. 10. Last week at least twelve people died in the fighting.
11. Last week?s fighting took the lives of twelve people.
Figure 1: Sample Sentence Group from the Chinese-English DARPA Evaluation Corpus: 11 English translations of
the same Chinese sentence.
 
 
at
 
during
 last
 
the
 
least
 
last
 
week
 
battle
 
fighting
 
 were
 
died
 lost
killed
 
in
 
their
 12
persons
*e*
people
 
the
  
last
 
weekbattle
fighting
 
lives
 
in
 
last
 
week
 
fighting
?s
 
at
 
?s
 
least
 
fighting
fight
 
 
died
 
in
 
peopletwelve
the
 
 
at
 
least
 
died
 
week
 
fighting?s
 
people12
 
killed
took
 
the
 
at
of
 
last
week  
lives
 
least
 
of
 
twelve people
 
12
lives
persons
*e*
Figure 2: FSA produced by our syntax-based alignment algorithm from the input in Figure 1.
  
*e*
 the
 
during
*e*
fighting
 
battle
 last
 
*e*
week
weeks
 
fight
fighting
*e*
 
killed
of took
*e*
 
the
 
at
 
lives
 
least
of
 
twelve
 
12
 
people
persons
*e*
 
lives
died
*e*
 in
 
*e*
the  
*e*
battle
fighting
 
*e*
 
last
weeks
week
 
fighting
*e*
*e*
 
people
 
lost
 
were
their
killed
Figure 3: FSA produced by a Multi-Sequence Alignment algorithm from the input in Figure 1.
lem. In contrast, we want to ensure the correctness of
all paths represented by the FSAs, and direct application
of MSA in the presence of different constituent orderings
can be problematic. For example, when given as input the
same sentences in Figure 1, one instantiation of the MSA
algorithm produces the FSA in Figure 3, which contains
many ?bad? paths such as the battle of last week?s fight-
ing took at least 12 people lost their people died in the
fighting last week?s fighting (See Section 4.2.2 for a more
quantitative analysis.). It?s still possible to use MSA if,
for example, the input is pre-clustered to have the same
constituent ordering (Barzilay and Lee (2003)). But we
chose to approach this problem from another direction.
As a result, we propose a new syntax-based algorithm to
produce FSAs.
In this paper, we first introduce the multiple transla-
tion corpus that we use in our experiments (see Section
2). We then present the algorithms that we developed to
induce finite-state paraphrase representations from such
data (see Section 3). An important part of the paper is
dedicated to evaluating the quality of the finite-state rep-
resentations that we derive (see Section 4). Since our rep-
resentations encode thousands and sometimes millions of
equivalent verbalizations of the same meaning, we use
both manual and automatic evaluation techniques. Some
of the automatic evaluations we perform are novel as
well.
2 Data
The data we use in this work is the LDC-available
Multiple-Translation Chinese (MTC) Corpus1 developed
for machine translation evaluation, which contains 105
news stories (993 sentences) from three sources of jour-
nalistic Mandarin Chinese text. These stories were inde-
pendently translated into English by 11 translation agen-
cies. Each sentence group, which consists of 11 semanti-
cally equivalent translations, is a rich source for learning
lexical and structural paraphrases. In our experiments,
we use 899 of the sentence groups ? the sentence groups
with sentences longer than 45 words were dropped.
3 A Syntax-Based Alignment Algorithm
Our syntax-based alignment algorithm, whose pseu-
docode is shown in Figure 4, works in three steps. In the
first step (lines 1-5 in Figure 4), we parse every sentence
in a sentence group and merge all resulting parse trees
into a parse forest. In the second step (line 6), we extract
1Linguistic Data Consortium (LDC) Catalog Number
LDC2002T01, ISBN 1-58563-217-1.
1. ParseForest = 
2. foreach s ? SentenceGroup
3. t = parseTree(s);
4. ParseForest = Merge(ParseForest, t);
5. endfor
6. Extract FSA from ParseForest;
7. Squeeze FSA;
Figure 4: The Syntax-Based Alignment Algorithm.
an FSA from the parse forest and then we compact it fur-
ther using a limited form of bottom-up alignment, which
we call squeezing (line 7). In what follows, we describe
each step in turn.
Top-down merging. Given a sentence group, we pass
each of the 11 sentences to Charniak?s (2000) parser to
get 11 parse trees. The first step in the algorithm is to
merge these parse trees into one parse-forest-like struc-
ture using a top-down process.
Let?s consider a simple case in which the parse for-
est contains one single tree, Tree 1 in Figure 5, and we
are adding Tree 2 to it. Since the two trees correspond
to sentences that have the same meaning and since both
trees expand an S node into an NP and a V P , it is rea-
sonable to assume that NP1 is a paraphrase of NP2 and
V P1 is a paraphrase of V P2. We merge NP1 with NP2
and V P1 with V P2 and continue the merging process on
each of the subtrees recursively, until we either reach the
leaves of the trees or the two nodes that we examine are
expanded using different syntactic rules.
When we apply this process to the trees in Figure 5,
the NP nodes are merged all the way down to the leaves,
and we get ?12? as a paraphrase of ?twelve? and ?people?
as a paraphrase of ?persons?; in contrast, the two V P s
are expanded in different ways, so no merging is done
beyond this level, and we are left with the information
that ?were killed? is a paraphrase of ?died?.
We repeat this top-down merging procedure with each
of the 11 parse trees in a sentence group. So far, only
constituents with same syntactic type are treated as para-
phrases. However, later we shall see that we can match
word spans whose syntactic types differ.
Keyword checking. The matching process described
above appears quite strict ? the expansions must match
exactly for two nodes to be merged. But consider the fol-
lowing parse trees:
1.(S (NP1 people)(V P1 were killed in this battle))
2.(S (NP2 this battle)(V P2 killed people))
If we applied the algorithm described above, we would
mistakenly align NP1 with NP2 and V P1 with V P2 ?
the algorithm described so far makes no use of lexical
12
twelve
people
persons were killed
died
Merge
Linearization
Tree 1 Tree 2
Parse Forest
FSA / Word Lattice
BEG END
+
S
NP VP
CD12 NNpersons AUXwere VP
VBkilled
S
NP VP
CD
twelve NNpeople VBdied
NP VP
CD NN AUX VPVB
12
twelve
people
persons
...
were
...killed...died
Figure 5: Top-down merging of parse trees and FSA ex-
traction.
information.
To prevent such erroneous alignments, we also imple-
ment a simple keyword checking procedure. We note
that since the word ?battle? appears in both V P1 and
NP2, this can serve as an evidence against the merging of
(NP1, NP2) and (V P1, V P2). A similar argument can
be constructed for the word ?people?. So in this exam-
ple we actually have double evidence against merging; in
general, one such clue suffices to stop the merging.
Our keyword checking procedure acts as a filter. A list
of keywords is maintained for each node in a syntactic
tree. This list contains all the nouns, verbs, and adjectives
that are spanned by a syntactic node. Before merging two
nodes, we check to see whether the keyword lists asso-
ciated with them share words with other nodes. That is,
supposed we just merged nodes A and B, and they are ex-
panded with the same syntactic rule into A1A2...An and
B1B2...Bn respectively; before we merge each Ai with
Bi, we check for each Bi if its keyword list shares com-
mon words with any Aj (j 6= i). If they do not, we con-
tinue the top-down merging process; otherwise we stop.
  
detroit
 a
 
building
 
detroit
 
detroit
 
a
 building
building
 
in
 
?s
 building
 
building
 
reduced
 
to
 
rubble
flattened
razed
 
was
 
blasted
leveled
razed
 
razed
 
leveled
 into
 
to
detroitbuilding
 
to down  
the ground
ashes
ground
 
the ground
  
levelled
 
to
 
in detroit ground
a. Before squeezing
 
 
detroit
 a
*e*  
?s
*e*
 
building  
building
 
reduced
 
*e*
was
 
flattened
 
blasted
leveled
 levelled
 
to
razed
 
leveled
*e*
 
into
 
to  
to
rubble
 
in detroit
down ashes
the
*e*
ground
b. After squeezing
Figure 6: Squeezing effect
In our current implementation, a pair of synonyms can
not stop an otherwise legitimate merging, but it?s possi-
ble to extend our keyword checking process with the help
of lexical resources such as WordNet in future work.
Mapping Parse Forests into Finite State Automata.
The process of mapping Parse Forests into Finite State
Automata is simple. We simply traverse the parse forest
top-down and create alternative paths for every merged
node. For example, the parse forest in Figure 5 is mapped
into the FSA shown at the bottom of the same figure. In
the FSA, there is a word associated with each edge. Dif-
ferent paths between any two nodes are assumed to be
paraphrases of each other. Each path that starts from the
BEGIN node and ends at the END node corresponds
to either an original input sentence or a paraphrase sen-
tence.
Squeezing. Since we adopted a very strict matching
criterion in top-down merging, a small difference in the
syntactic structure of two trees prevents some legitimate
mergings from taking place. This behavior is also exacer-
bated by errors in syntactic parsing. Hence, for instance,
three edges labeled detroit at the leftmost of the top FSA
in Figure 6 were kept apart. To compensate for this ef-
fect, our algorithm implements an additional step, which
we call squeezing. If two different edges that go into (or
out of) the same node in an FSA are labeled with the same
word, the nodes on the other end of the edges are merged.
We apply this operation exhaustively over the FSAs pro-
duced by the top-down merging procedure. Figure 6 il-
lustrates the effect of this operation: the FSA at the top
of this figure is compressed into the more compact FSA
shown at the bottom of it. Note that in addition to reduc-
ing the redundant edges, this also gives us paraphrases
not available in the FSA before squeezing (e.g. {reduced
to rubble, blasted to ground}). Therefore, the squeezing
operation, which implements a limited form of lexically
driven alignment similar to that exploited by MSA algo-
rithms, leads to FSAs that have a larger number of paths
and paraphrases.
4 Evaluation
The evaluation for our finite state representations and al-
gorithm requires careful examination. Obviously, what
counts as a good result largely depends on the applica-
tion one has in mind. If we are extracting paraphrases for
question-reformulation, it doesn?t really matter if we out-
put a few syntactically incorrect paraphrases, as long as
we produce a large number of semantically correct ones.
If we want to use the FSA for MT evaluation (for exam-
ple, comparing a sentence to be evaluated with the pos-
sible paths in FSA), we would want all paths to be rela-
tively good (which we will focus on in this paper), while
in some other applications, we may only care about the
quality of the best path (not addressed in this paper). Sec-
tion 4.1 concentrates on evaluating the paraphrase pairs
that can be extracted from the FSAs built by our system,
while Section 4.2 is dedicated to evaluating the FSAs di-
rectly.
4.1 Evaluating paraphrase pairs
4.1.1 Human-based evaluation of paraphrases
By construction, different paths between any two
nodes in the FSA representations that we derive are para-
phrases (in the context in which the nodes occur). To
evaluate our algorithm, we extract paraphrases from our
FSAs and ask human judges to evaluate their correctness.
We compare the paraphrases we collect with paraphrases
that are derivable from the same corpus using a co-
training-based paraphrase extraction algorithm (Barzilay
and McKeown, 2001). To the best of our knowledge, this
is the most relevant work to compare against since it aims
at extracting paraphrase pairs from parallel corpus. Un-
like our syntax-based algorithm which treats a sentence
as a tree structure and uses this hierarchical structural in-
formation to guide the merging process, their algorithm
treats a sentence as a sequence of phrases with surround-
ing contexts (no hierarchical structure involved) and co-
trains classifiers to detect paraphrases and contexts for
paraphrases. It would be interesting to compare the re-
sults from two algorithms so different from each other.
For the purpose of this experiment, we randomly se-
lected 300 paraphrase pairs (Ssyn) from the FSAs pro-
duced by our system. Since the co-training-based al-
gorithm of Barzilay and McKeown (2001) takes paral-
lel corpus as input, we created out of the MTC corpus
55 ? 993 sentence pairs (Each equivalent translation set
of cardinality 11 was mapped into
(11
2
)
equivalent trans-
lation pairs.). Regina Barzilay kindly provided us the list
of paraphrases extracted by their algorithm from this par-
allel corpus, from which we randomly selected another
set of 300 paraphrases (Scotr).
Correct Partial Incorrect
Ssyn 85% 12% 3%
Judge 1 Scotr 68% 13% 19%
Ssyn 80% 13% 7%
Judge 2 Scotr 63% 13% 24%
Ssyn 81% 5% 13%
Judge 3 Scotr 68% 3% 29%
Ssyn 77% 17% 5%
Judge 4 Scotr 68% 16% 16%
Average of Ssyn 81% 12% 7%
All Judges Scotr 66% 11% 22%
Table 1: A comparison of the correctness of the para-
phrases produced by the syntax-based alignment (Ssyn)
and co-training-based (Scotr) algorithms.
The resulting 600 paraphrase pairs were mixed and
presented in random order to four human judges. Each
judge was asked to assess the correctness of 150 para-
phrase pairs (75 pairs from each system) based on the
context, i.e., the sentence group, from which the para-
phrase pair was extracted. Judges were given three
choices: ?Correct?, for perfect paraphrases, ?Partially
correct?, for paraphrases in which there is only a par-
tial overlap between the meaning of two paraphrases (e.g.
while {saving set, aid package} is a correct paraphrase
pair in the given context, {set, aide package} is consid-
ered partially correct), and ?Incorrect?. The results of the
evaluation are presented in Table 1.
Although the four evaluators were judging four differ-
ent sets, each clearly rated a higher percentage of the out-
puts produced by the syntax-based alignment algorithm
as ?Correct?. We should note that there are parameters
specific to the co-training algorithm that we did not tune
to work for this particular corpus. In addition, the co-
training algorithm recovered more paraphrase pairs: the
syntax-based algorithm extracted 8666 pairs in total with
1051 of them extracted at least twice (i.e. more or less
reliable), while the numbers for the co-training algorithm
is 2934 out of a total of 16993 pairs. This means we are
not comparing the accuracy on the same recall level.
Aside from evaluating the correctness of the para-
phrases, we are also interested in the degree of overlap
between the paraphrase pairs discovered by the two algo-
rithms so different from each other. We find that out of
the 1051 paraphrase pairs that were extracted from more
than one sentence group by the syntax-based algorithm,
62.3% were also extracted by the co-training algorithm;
and out of the 2934 paraphrase pairs from the results of
co-training algorithm, 33.4% were also extracted by the
syntax-based algorithm. This shows that in spite of the
very different cues the two different algorithms rely on,
range of ASL 1-10 10-20 20-30 30-45
recall 30.7% 16.3% 7.8% 3.8%
Table 2: Recall of WordNet-consistent synonyms.
they do discover a lot of common pairs.
4.1.2 WordNet-based analysis of paraphrases
In order to (roughly) estimate the recall (of lexical syn-
onyms) of our algorithm, we use the synonymy relation
in WordNet to extract all the synonym pairs present in
our corpus. This extraction process yields the list of all
WordNet-consistent synonym pairs that are present in our
data. (Note that some of the pairs identified as synonyms
by WordNet, like ?follow/be?, are not really synonyms in
the contexts defined in our data set, which may lead to
artificial deflation of our recall estimate.) Once we have
the list of WordNet-consistent paraphrases, we can check
how many of them are recovered by our method. Table 2
gives the percentage of pairs recovered for each range of
average sentence length (ASL) in the group.
Not surprisingly, we get higher recall with shorter sen-
tences, since long sentences tend to differ in their syn-
tactic structures fairly high up in the parse trees, which
leads to fewer mergings at the lexical level. The recall
on the task of extracting lexical synonyms, as defined
by WordNet, is not high. But after all, this is not what
our algorithm has been designed for. It?s worth notic-
ing that the syntax-based algorithm also picks up many
paraphrases that are not identified as synonyms in Word-
Net. Out of 3217 lexical paraphrases that are learned by
our system, only 493 (15.3%) are WordNet synonyms,
which suggests that paraphrasing is a much richer and
looser relation than synonymy. However, the WordNet-
based recall figures suggest that WordNet can be used as
an additional source of information to be exploited by our
algorithm.
4.2 Evaluating the FSA directly
We noted before that apart from being a natural represen-
tation of paraphrases, the FSAs that we build have their
own merit and deserve to be evaluated directly. Since our
FSAs contain large numbers of paths, we design auto-
matic evaluation metrics to assess their qualities.
4.2.1 Language Model-based evaluation
If we take our claims seriously, each path in our FSAs
that connects the start and end nodes should correspond to
a well-formed sentence. We are interested in both quan-
tity (how many sentences our automata are able to pro-
duce) and quality (how good these sentences are). To an-
swer the first question, we simply count the number of
paths produced by our FSAs.
average N (# of paths) logN
length max ave max ave
1 - 10 22749 775 10.0 5.2
10 - 20 172386 4468 12.1 6.2
20 - 30 3479544 29202 15.1 5.8
30 - 45 684589 4135 13.4 4.5
Table 3: Statistics on Number of Paths in FSAs
random variable mean std. dev
ent(FSA)? ent(SG) ?0.11586 1.25162
ent(MTS)? ent(SG) 1.74259 1.05749
Table 4: Quality judged by LM
Table 3 gives the statistics on the number of paths pro-
duced by our FSAs, reported by the average length of
sentences in the input sentence groups. For example, the
sentence groups that have between 10 and 20 words pro-
duce, on average, automata that can yield 4468 alterna-
tive, semantically equivalent formulations.
Note that if we always get the same degree of merging
per word across all sentence groups, the number of paths
would tend to increase with the sentence length. This is
not the case here. Apparently we are getting less merg-
ing with longer sentences. But still, given 11 sentences,
we are capable of generating hundreds, thousands, and in
some cases even millions of sentences.
Obviously, we should not get too happy with our abil-
ity to boost the number of equivalent meanings if they are
incorrect. To assess the quality of the FSAs generated by
our algorithm, we use a language model-based metric.
We train a 4-gram model over one year of the Wall
Street Journal using the CMU-Cambridge Statistical Lan-
guage Modeling toolkit (v2). For each sentence group
SG, we use this language model to estimate the aver-
age entropy of the 11 original sentences in that group
(ent(SG)). We also compute the average entropy of
all the sentences in the corresponding FSA built by our
syntax-based algorithm (ent(FSA)). As the statistics in
Table 4 show, there is little difference between the av-
erage entropy of the original sentences and the average
entropy of the paraphrase sentences we produce. To bet-
ter calibrate this result, we compare it with the average
entropy of 6 corresponding machine translation outputs
(ent(MTS)), which were also made available by LDC
in conjunction with the same corpus. As one can see, the
difference between the average entropy of the machine
produced output and the average entropy of the origi-
nal 11 sentences is much higher than the difference be-
tween the average entropy of the FSA-produced outputs
and the average entropy of the original 11 sentences. Ob-
viously, this does not mean that our FSAs only produce
well-formed sentences. But it does mean that our FSAs
produce sentences that look more like human produced
sentences than machine produced ones according to a lan-
guage model.
4.2.2 Word repetition analysis
Not surprisingly, the language model we used in Sec-
tion 4.2.1 is far from being a perfect judge of sentence
quality. Recall the example of ?bad? path we gave in Sec-
tion 1: the battle of last week?s fighting took at least 12
people lost their people died in the fighting last week?s
fighting. Our 4-gram based language model will not find
any fault with this sentence. Notice, however, that some
words (such as ?fighting? and ?people?) appear at least
twice in this path, although they are not repeated in any
of the source sentences. These erroneous repetitions in-
dicate mis-alignment. By measuring the frequency of
words that are mistakenly repeated, we can now examine
quantitatively whether a direct application of the MSA
algorithm suffers from different constituent orderings as
we expected.
For each sentence group, we get a list of words that
never appear more than once in any sentence in this
group. Given a word from this list and the FSA built
from this group, we count the total number of paths that
contain this word (C) and the number of paths in which
this word appears at least twice (Cr, i.e. number of er-
roneous repetitions). We define the repetition ratio to
be Cr/C, which is the proportion of ?bad? paths in this
FSA according to this word. If we compute this ra-
tio for all the words in the lists of the first 499 groups2
and the corresponding FSAs produced by an instantia-
tion of the MSA algorithm3, the average repetition ra-
tio is 0.0304992 (14.76% of the words have a non-zero
repetition ratio, and the average ratio for these words is
0.206671). In comparison, the average repetition ratio for
our algorithm is 0.0035074 (2.16% of the words have a
non-zero repetition ratio4, and the average ratio for these
words is 0.162309). The presence of different constituent
orderings does pose a more serious problem to the MSA
algorithm.
4.2.3 MT-based evaluation
Recently, Papineni et al (2002) have proposed an au-
tomatic MT system evaluation technique (the BLEU
score). Given an MT system output and a set of refer-
2MSA runs very slow for longer sentences, and we believe
using the first 499 groups should be enough to make our point.
3We thank Regina Barzilay for providing us this set of re-
sults
4Note that FSAs produced right after keyword checking will
not yield any non-zero repetition ratio. However, if there are
mis-alignment not prevented by keyword checking in an FSA,
it may contain paths with erroneous repetition of words after
squeezing.
range 0-1 1-2 2-3 3-4 4-5
count 546 256 80 15 2
Table 5: Statistics for edgain
ence translations, one can estimate the ?goodness? of the
MT output by measuring the n-gram overlap between the
output and the reference set. The higher the overlap, i.e.,
the closer an output string is to a set of reference transla-
tions, the better a translation it is.
We hypothesize that our FSAs provide a better repre-
sentation against which the outputs of MT systems can
be evaluated because they encode not just a few but thou-
sands of equivalent semantic formulations of the desired
meaning. Ideally, if the FSAs we build accept all and
only the correct renderings of a given meaning, we can
just give a test sentence to the reference FSA and see if
it is accepted by it. Since this is not a realistic expecta-
tion, we measure the edit distance between a string and
an FSA instead: the smaller this distance is, the closer it
is to the meaning represented by the FSA.
To assess whether our FSAs are more appropriate rep-
resentations for evaluating the output of MT systems, we
perform the following experiment. For each sentence
group, we hold out one sentence as test sentence, and try
to evaluate how much of it can be predicted from the other
10 sentences. We compare two different ways of estimat-
ing the predictive power. (a) we compute the edit distance
between the test sentence and the other 10 sentences in
the set. The minimum of this distance is ed(input). (b)
we use dynamic programming to efficiently compute the
minimum distance (ed(FSA)) between the test sentence
and all the paths in the FSA built from the other 10 sen-
tences. The smaller the edit distance is, the better we
are predicting a test sentence. Mathematically, the differ-
ence between these two measures ed(input)? ed(FSA)
characterizes how much is gained in predictive power by
building the FSA.
We carry out the experiment described above in a
?leave-one-out? fashion (i.e. each sentence serves as
a test sentence once). Now let edgain be the average
of ed(input) ? ed(FSA) over the 11 runs for a given
group. We compute this for all 899 groups and find the
mean for edgain to be 0.91 (std. dev = 0.78). Table 5
gives the count for groups whose edgain falls into the
specified range. We can see that the majority of edgain
falls under 2.
We are also interested in the relation between the pre-
dictive power of the FSAs and the number of reference
translations they are derived from. For a given group, we
randomly order the sentences in it, set the last one as the
test sentence, and try to predict it with the first 1, 2, 3,
... 10 sentences. We investigate whether more sentences
ed(FSAn) ed(inputn)
?ed(FSA10) ?ed(FSAn)
n mean std. dev mean std. dev
1 5.65 3.86 0 0
2 3.66 3.02 0.19 0.60
3 2.71 2.55 0.33 0.76
4 2.10 2.33 0.46 0.90
5 1.56 2.01 0.56 0.95
6 1.18 1.79 0.65 1.02
7 0.79 1.48 0.75 1.09
8 0.49 1.10 0.81 1.11
9 0.21 0.74 0.89 1.16
10 0 0 0.93 1.21
Table 6: Effect of monotonically increasing the number
of reference sentences
yield an increase in the predictive power.
Let ed(FSAn) be the edit distance from the test sen-
tence to the FSA built on the first n sentences; similarly,
let ed(inputn) be the minimum edit distance from the
test sentence to an input set that consists of only the first
n sentences. Table 6 reports the effect of using differ-
ent number of reference translations. The first column
shows that each translation is contributing to the predic-
tive power of our FSA. Even when we add the tenth trans-
lation to our FSA, we still improve its predictive power.
The second column shows that the more sentences we add
to the FSA the larger the difference between its predic-
tive power and that of a simple set. The results in Table 6
suggest that our FSA may be used in order to refine the
BLEU metric (Papineni et al, 2002).
5 Conclusion & Future Work
In this paper, we presented a new syntax-based algorithm
that learns paraphrases from a newly available dataset.
The multiple translation corpus that we use in this paper
is the first instance in a series of similar corpora that are
built and made publicly available by LDC in the context
of a series of DARPA-sponsored MT evaluations. The
algorithm we proposed constructs finite state represen-
tations of paraphrases that are useful in many contexts:
to induce large lists of lexical and structural paraphrases;
to generate semantically equivalent renderings of a given
meaning; and to estimate the quality of machine transla-
tion systems. More experiments need to be carried out
in order to assess extrinsically whether the FSAs we pro-
duce can be used to yield higher agreement scores be-
tween human and automatic assessments of translation
quality.
In our future work, we wish to experiment with more
flexible merging algorithms and to integrate better the
top-down and bottom-up processes that are used to in-
duce FSAs. We also wish to extract more abstract para-
phrase patterns from the current representation. Such pat-
terns are more likely to get reused ? which would help us
get reliable statistics for them in the extraction phase, and
also have a better chance of being applicable to unseen
data.
Acknowledgments
We thank Hal Daume? III, Ulrich Germann, and Ulf Herm-
jakob for help and discussions; Eric Breck, Hubert Chen,
Stephen Chong, Dan Kifer, and Kevin O?Neill for par-
ticipating in the human evaluation; and the Cornell NLP
group and the reviewers for their comments on this pa-
per. We especially want to thank Regina Barzilay and
Lillian Lee for many valuable suggestions and help at var-
ious stages of this work. Portions of this work were done
while the first author was visiting Information Sciences
Institute. This work was supported by the Advanced
Research and Development Activity (ARDA)?s Advance
Question Answering for Intelligence (AQUAINT) Pro-
gram under contract number MDA908-02-C-0007, the
National Science Foundation under ITR/IM grant IIS-
0081334 and a Sloan Research Fellowship to Lillian Lee.
Any opinions, findings, and conclusions or recommen-
dations expressed above are those of the authors and do
not necessarily reflect the views of the National Science
Foundation or the Sloan Foundation.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Workshop on
Automatic Speech Recognition and Understanding.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of the ACL/EACL, pages 50?57.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
ACL, pages 550?557.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the NAACL.
DARPA. 2002. In DARPA IAO Machine Translation
Workshop, Santa Monica, CA, July 22-23.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answer-
ing. In Proceedings of the Text Retrieval Conference
(TREC?2002). November.
Lidija Iordanskaja, Richard Kittredge, and Alain Polge?re.
1991. Lexical selection and paraphrase in a meaning-
text generation model. In Ce?cile L. Paris, William R.
Swartout, and William C. Mann, editors, Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293?312. Kluwer Aca-
demic Publisher.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of of ACL/COLING.
Nils Lenke. 1994. Anticipating the reader?s problems
and the automatic generation of paraphrases. In Pro-
ceedings of the 15th International Conference on Com-
putational Linguistics, volume 1, pages 319?323, Ky-
oto, Japan, August 5?9.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Proceedings
of ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining 2001, pages 323?328.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the Human Language Technology Confer-
ence, pages 124?127, San Diego, CA, March 24-27.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of the Hu-
man Language Technology Conference (HLT?02), San
Diego, CA, March 24-27. Poster presentation.
Karen Sparck Jones and John I. Tait. 1984. Automatic
search term variant generation. Journal of Documen-
tation, 40(1):50?66.
Manfred Stede. 1999. Lexical Semantics and
Knowledge Representation in Multilingual Text
Generation. Kluwer Academic Publishers,
Boston/Dordrecht/London.
Cognates Can Improve Statistical Translation Models
Grzegorz Kondrak
Department of Computing Science
University of Alberta
221 Athabasca Hall
Edmonton, AB, Canada T6G 2E8
kondrak@cs.ualberta.edu
Daniel Marcu and Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
marcu,knight@isi.edu
Abstract
We report results of experiments aimed at im-
proving the translation quality by incorporating
the cognate information into translation mod-
els. The results confirm that the cognate iden-
tification approach can improve the quality of
word alignment in bitexts without the need for
extra resources.
1 Introduction
In the context of machine translation, the term cognates
denotes words in different languages that are similar
in their orthographic or phonetic form and are possible
translations of each other. The similarity is usually due
either to a genetic relationship (e.g. English night and
German nacht) or borrowing from one language to an-
other (e.g. English sprint and Japanese supurinto). In
a broad sense, cognates include not only genetically re-
lated words and borrowings but also names, numbers, and
punctuation. Practically all bitexts (bilingual parallel cor-
pora) contain some kind of cognates. If the languages are
represented in different scripts, a phonetic transcription
or transliteration of one or both parts of the bitext is a
pre-requisite for identifying cognates.
Cognates have been employed for a number of bitext-
related tasks, including sentence alignment (Simard et
al., 1992), inducing translation lexicons (Mann and Ya-
rowsky, 2001), and improving statistical machine trans-
lation models (Al-Onaizan et al, 1999). Cognates are
particularly useful when machine-readable bilingual dic-
tionaries are not available. Al-Onaizan et al (1999) ex-
perimented with using bilingual dictionaries and cog-
nates in the training of Czech?English translation mod-
els. They found that appending probable cognates to the
training bitext significantly lowered the perplexity score
on the test bitext (in some cases more than when using a
bilingual dictionary), and observed improvement in word
alignments of test sentences.
In this paper, we investigate the problem of incorpo-
rating the potentially valuable cognate information into
the translation models of Brown et al (1990), which, in
their original formulation, consider lexical items in ab-
straction of their form. For training of the models, we
use the GIZA program (Al-Onaizan et al, 1999). A list
of likely cognate pairs is extracted from the training cor-
pus on the basis of orthographic similarity, and appended
to the corpus itself. The objective is to reinforce the co-
ocurrence count between cognates in addition to already
existing co-ocurrences. The results of experiments con-
ducted on a variety of bitexts show that cognate iden-
tification can improve word alignments, which leads to
better translation models, and, consequently, translations
of higher quality. The improvement is achieved without
modifying the statistical training algorithm.
2 The method
We experimented with three word similarity measu-
res: Simard?s condition, Dice?s coefficient, and LCSR.
Simard et al (1992) proposed a simple condition for de-
tecting probable cognates in French?English bitexts: two
words are considered cognates if they are at least four
characters long and their first four characters are iden-
tical. Dice?s coefficient is defined as the ratio of the
number of shared character bigrams to the total num-
ber of bigrams in both words. For example, colour and
couleur share three bigrams (co, ou, and ur), so their
Dice?s coefficient is 6
11
' 0:55. The Longest Common
Subsequence Ratio (LCSR) of two words is computed
by dividing the length of their longest common subse-
quence by the length of the longer word. For example,
LCSR(colour,couleur) = 5
7
' 0:71, as their longest com-
mon subsequence is ?c-o-l-u-r?.
In order to identify a set of likely cognates in a tok-
enized and sentence-aligned bitext, each aligned segment
is split into words, and all possible word pairings are
stored in a file. Numbers and punctuation are not con-
sidered, since we feel that they warrant a more specific
approach. After sorting and removing duplicates, the file
represents all possible one-to-one word alignments of the
bitext. Also removed are the pairs that include English
function words, and words shorter than the minimum
length (usually set at four characters). For each word pair,
a similarity measure is computed, and the file is again
sorted, this time by the computed similarity value. If the
measure returns a non-binary similarity value, true cog-
nates are very frequent near the top of the list, and be-
come less frequent towards the bottom. The set of likely
cognates is obtained by selecting all pairs with similarity
above a certain threshold. Typically, lowering the thresh-
old increases recall while decreasing precision of the set.
Finally, one or more copies of the resulting set of likely
cognates are concatenated with the training set.
3 Experiments
We induced translation models using IBM Model 4
(Brown et al, 1990) with the GIZA toolkit (Al-Onaizan
et al, 1999). The maximum sentence length in the train-
ing data was set at 30 words. The actual translations
were produced with a greedy decoder (Germann et al,
2001). For the evaluation of translation quality, we used
the BLEU metric (Papineni et al, 2002), which measures
the n-gram overlap between the translated output and one
or more reference translations. In our experiments, we
used only one reference translation.
3.1 Word alignment quality
In order to directly measure the influence of the added
cognate information on the word alignment quality, we
performed a single experiment using a set of 500 man-
ually aligned sentences from Hansards (Och and Ney,
2000). Giza was first trained on 50,000 sentences from
Hansards, and then on the same training set augmented
with a set of cognates. The set consisted of two copies of
a list produced by applying the threshold of 0:58 to LCSR
list. The duplication factor was arbitrarily selected on the
basis of earlier experiments with a different training and
test set taken from Hansards.
The incorporation of the cognate information resulted
in a 10% reduction of the word alignment error rate,
from 17.6% to 15.8%, and a corresponding improvement
in both precision and recall. An examination of ran-
domly selected alignments confirms the observation of
Al-Onaizan et al (1999) that the use of cognate informa-
tion reduces the tendency of rare words to align to many
co-occurring words.
In another experiment, we concentrated on co-oc-
curring identical words, which are extremely likely to
represent mutual translations. In the baseline model,
links were induced between 93.6% of identical words. In
the cognate-augmented model, the ratio rose to 97.2%.
3.2 Europarl
Europarl is a tokenized and sentence-aligned multilingual
corpus extracted from the Proceedings of the European
0.202
0.203
0.204
0.205
0.206
0.207
0.208
0 1 2 3 4 5 6
BL
EU
 s
co
re
Duplication factor
"Simard"
"DICE"
"LCSR"
Figure 1: BLEU scores as a function of the duplication
factor for five methods of cognates identification aver-
aged over nine language pairs.
Parliament (Koehn, 2002). The eleven official European
Union languages are represented in the corpus. We con-
sider the variety of languages as important for a valida-
tion of the cognate-based approach as general, rather than
language-specific.
As the training data, we arbitrarily selected a subset of
the corpus that consisted the proceedings from October
1998. By pairing English with the remaining languages,
we obtained nine bitexts1, each comprising about 20,000
aligned sentences (500,000 words). The test data con-
sisted of 1755 unseen sentences varying in length from 5
to 15 words from the 2000 proceedings (Koehn, 2002).
The English language model was trained separately on a
larger set of 700,000 sentences from the 1996 proceed-
ings.
Figure 1 shows the BLEU scores as a function of the
duplication factor for three methods of cognates identi-
fication averaged over nine language pairs. The results
averaged over a number of language pairs are more in-
formative than results obtained on a single language pair,
especially since the BLEU metric is only a rough approx-
imation of the translation quality, and exhibits consider-
able variance. Three different similarity measures were
compared: Simard, DICE with a threshold of 0.39, and
LCSR with a threshold of 0.58. In addition, we experi-
mented with two different methods of extending the train-
ing set with with a list of cognates: one pair as one sen-
tence (Simard), and thirty pairs as one sentence (DICE
and LCSR).2
1Greek was excluded because its non-Latin script requires a
different type of approach to cognate identification.
2In the vast majority of the sentences, the alignment links are
correctly induced between the respective cognates when multi-
Threshold Pairs Score
Baseline 0 0.2027
0.99 863 0.2016
0.71 2835 0.2030
0.58 5339 0.2058
0.51 7343 0.2073
0.49 14115 0.2059
Table 1: The number of extracted word pairs as a func-
tion of the LCSR threshold, and the corresponding BLEU
scores, averaged over nine Europarl bitexts.
The results show a statistically significant improve-
ment3 in the average BLEU score when the duplication
factor is greater than 1, but no clear trend can be discerned
for larger factors. There does not seem to be much differ-
ence between various methods of cognate identification.
Table 1 shows results of augmenting the training set
with different sets of cognates determined using LCSR.
A threshold of 0.99 implies that only identical word
pairs are admitted as cognates. The words pairs with
LCSR around 0.5 are more likely than not to be unre-
lated. In each case two copies of the cognate list were
used. The somewhat surprising result was that adding
only ?high confidence? cognates is less effective than
adding lots of dubious cognates. In that particular set
of tests, adding only identical word pairs, which almost
always are mutual translations, actually decreased the
BLEU score. Our results are consistent with the results
of Al-Onaizan et al (1999), who observed perplexity im-
provement even when ?extremely low? thresholds were
used. It seems that the robust statistical training algo-
rithm has the ability of ignoring the unrelated word pairs,
while at the same time utilizing the information provided
by the true cognates.
3.3 A manual evaluation
In order to confirm that the higher BLEU scores reflect
higher translation quality, we performed a manual evalua-
tion of a set of a hundred six-token sentences. The models
were induced on a 25,000 sentences portion of Hansards.
The training set was augmented with two copies of a cog-
nate list obtained by thresholding LCSR at 0.56. Results
ple pairs per sentence are added.
3Statistical significance was estimated in the following way.
The variance of the BLEU score was approximated by randomly
picking a sample of translated sentences from the test set. The
size of the test sample was equal to the size of the test set (1755
sentences). The score was computed in this way 200 times for
each language. The mean and the variance of the nine-language
average was computed by randomly picking one of the 200
scores for each language and computing the average. The mean
result produced was 0.2025, which is very close to the baseline
average score of 0.2027. The standard deviation of the average
was estimated to be 0.0018, which implies that averages above
0.2054 are statistically significant at the 0.95 level.
Evaluation Baseline Cognates
Completely correct 16 21
Syntactically correct 8 7
Semantically correct 14 12
Wrong 62 60
Total 100 100
Table 2: A manual evaluation of the translations gener-
ated by the baseline and the cognate-augmented models.
of a manual evaluation of the entire set of 100 sentences
are shown in Table 2. Although the overall translation
quality is low due to the small size of the training corpus
and the lack of parameter tuning, the number of com-
pletely acceptable translations is higher when cognates
are added.
4 Conclusion
Our experimental results show that the incorporation of
cognate information can improve the quality of word
alignments, which in turn result in better translations, In
our experiments, the improvement, although statistically
significant, is relatively small, which can be attributed to
the relative crudeness of the approach based on append-
ing the cognate pairs directly to the training data. In the
future, we plan to develop a method of incorporating the
cognate information directly into the training algorithm.
We foresee that the performance of such a method will
also depend on using more sophisticated word similarity
measures.
References
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,
D. Melamed, F. Och, D. Purdy, N. Smith, and D. Yarowsky.
1999. Statistical machine translation. Technical report,
Johns Hopkins University.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1990.
The mathematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics, 19(2):263?311.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada.
2001. Fast decoding and optimal decoding for machine
translation. In Proceedings of ACL-01.
P. Koehn. 2002. Europarl: A multilingual corpus for evaluation
of machine translation. In preparation.
G. Mann and D. Yarowsky. Multipath translation lexicon induc-
tion via bridge languages. In Proceedings of NAACL 2001.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of ACL-00.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In
Proceedings of ACL-02.
M. Simard, G. F. Foster, and P. Isabelle. 1992. Using cognates
to align sentences in bilingual corpora. In Proceedings of
TMI-92.
Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 5
                                                              Proceedings of HLT-NAACL
Training Tree Transducers
Jonathan Graehl
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292
graehl@isi.edu
Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Many probabilistic models for natural language
are now written in terms of hierarchical tree
structure. Tree-based modeling still lacks many
of the standard tools taken for granted in (finite-
state) string-based modeling. The theory of tree
transducer automata provides a possible frame-
work to draw on, as it has been worked out in an
extensive literature. We motivate the use of tree
transducers for natural language and address
the training problem for probabilistic tree-to-
tree and tree-to-string transducers.
1 Introduction
Much of natural language work over the past decade has
employed probabilistic finite-state transducers (FSTs)
operating on strings. This has occurred somewhat under
the influence of speech recognition, where transducing
acoustic sequences to word sequences is neatly captured
by left-to-right stateful substitution. Many conceptual
tools exist, such as Viterbi decoding (Viterbi, 1967) and
forward-backward training (Baum and Eagon, 1967), as
well as generic software toolkits. Moreover, a surprising
variety of problems are attackable with FSTs, from part-
of-speech tagging to letter-to-sound conversion to name
transliteration.
However, language problems like machine transla-
tion break this mold, because they involve massive re-
ordering of symbols, and because the transformation pro-
cesses seem sensitive to hierarchical tree structure. Re-
cently, specific probabilistic tree-based models have been
proposed not only for machine translation (Wu, 1997;
Alshawi, Bangalore, and Douglas, 2000; Yamada and
Knight, 2001; Gildea, 2003; Eisner, 2003), but also for
This work was supported by DARPA contract F49620-00-
1-0337 and ARDA contract MDA904-02-C-0450.
summarization (Knight and Marcu, 2002), paraphras-
ing (Pang, Knight, and Marcu, 2003), natural language
generation (Langkilde and Knight, 1998; Bangalore and
Rambow, 2000; Corston-Oliver et al, 2002), and lan-
guage modeling (Baker, 1979; Lari and Young, 1990;
Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001;
Klein and Manning, 2003). It is useful to understand
generic algorithms that may support all these tasks and
more.
(Rounds, 1970) and (Thatcher, 1970) independently
introduced tree transducers as a generalization of FSTs.
Rounds was motivated by natural language. The Rounds
tree transducer is very similar to a left-to-right FST, ex-
cept that it works top-down, pursuing subtrees in paral-
lel, with each subtree transformed depending only on its
own passed-down state. This class of transducer is often
nowadays called R, for ?Root-to-frontier? (G?cseg and
Steinby, 1984).
Rounds uses a mathematics-oriented example of an R
transducer, which we summarize in Figure 1. At each
point in the top-down traversal, the transducer chooses
a production to apply, based only on the current state
and the current root symbol. The traversal continues
until there are no more state-annotated nodes. Non-
deterministic transducers may have several productions
with the same left-hand side, and therefore some free
choices to make during transduction.
An R transducer compactly represents a potentially-
infinite set of input/output tree pairs: exactly those pairs
(T1, T2) for which some sequence of productions applied
to T1 (starting in the initial state) results in T2. This is
similar to an FST, which compactly represents a set of
input/output string pairs, and in fact, R is a generalization
of FST. If we think of strings written down vertically, as
degenerate trees, we can convert any FST into an R trans-
ducer by automatically replacing FST transitions with R
productions.
R does have some extra power beyond path following
Figure 1: A sample R tree transducer that takes the
derivative of its input.
and state-based record keeping. It can copy whole sub-
trees, and transform those subtrees differently. It can also
delete subtrees without inspecting them (imagine by anal-
ogy an FST that quits and accepts right in the middle of
an input string). Variants of R that disallow copying and
deleting are called RL (for linear) and RN (for nondelet-
ing), respectively.
One advantage of working with tree transducers is the
large and useful body of literature about these automata;
two excellent surveys are (G?cseg and Steinby, 1984) and
(Comon et al, 1997). For example, R is not closed under
composition (Rounds, 1970), and neither are RL or F (the
?frontier-to-root? cousin of R), but the non-copying FL
is closed under composition. Many of these composition
results are first found in (Engelfriet, 1975).
R has surprising ability to change the structure of an
input tree. For example, it may not be initially obvious
how an R transducer can transform the English structure
S(PRO, VP(V, NP)) into the Arabic equivalent S(V, PRO,
NP), as it is difficult to move the subject PRO into posi-
tion between the verb V and the direct object NP. First, R
productions have no lookahead capability?the left-hand-
side of the S production consists only of q S(x0, x1), al-
though we want our English-to-Arabic transformation to
apply only when it faces the entire structure q S(PRO,
VP(V, NP)). However, we can simulate lookahead using
states, as in these productions:
- q S(x0, x1) ? S(qpro x0, qvp.v.np x1)
- qpro PRO ? PRO
- qvp.v.np VP(x0, x1) ? VP(qv x0, qnp x1)
By omitting rules like qpro NP? ..., we ensure that the
entire production sequence will dead-end unless the first
child of the input tree is in fact PRO. So finite lookahead
is not a problem. The next problem is how to get the PRO
to appear in between the V and NP, as in Arabic. This can
be carried out using copying. We make two copies of the
English VP, and assign them different states:
- q S(x0,x1) ? S(qleft.vp.v x1, qpro x0,
qright.vp.np x1)
- qpro PRO ? PRO
- qleft.vp.v VP(x0, x1) ? qv x0
- qright.vp.np VP(x0, x1) ? qnp x1
While general properties of R are understood, there
are many algorithmic questions. In this paper, we take
on the problem of training probabilistic R transducers.
For many language problems (machine translation, para-
phrasing, text compression, etc.), it is possible to collect
training data in the form of tree pairs and to distill lin-
guistic knowledge automatically.
Our problem statement is: Given (1) a particular
transducer with productions P, and (2) a finite training set
of sample input/output tree pairs, we want to produce (3)
a probability estimate for each production in P such that
we maximize the probability of the output trees given the
input trees.
As organized in the rest of this paper, we accomplish
this by intersecting the given transducer with each in-
put/output pair in turn. Each such intersection produces a
set of weighted derivations that are packed into a regular
tree grammar (Sections 3-5), which is equivalent to a tree
substitution grammar. The inside and outside probabili-
ties of this packed derivation structure are used to com-
pute expected counts of the productions from the original,
given transducer (Sections 6-7). Section 9 gives a sample
transducer implementing a published machine translation
model; some readers may wish to skip to this section di-
rectly.
2 Trees
T? is the set of (rooted, ordered, labeled, finite) trees over
alphabet ?. An alphabet is just a finite set.
T?(X) are the trees over alphabet ?, indexed by X?
the subset of T??X where only leaves may be labeled by
X . (T?(?) = T?.) Leaves are nodes with no children.
The nodes of a tree t are identified one-to-one with its
paths: pathst ? paths ? N? ?
??
i=0 Ni (A0 ? {()}).
The path to the root is the empty sequence (), and p1
extended by p2 is p1 ? p2, where ? is concatenation.
For p ? pathst, rankt(p) is the number of chil-
dren, or rank, of the node at p in t, and labelt(p) ?
? ? X is its label. The ranked label of a node is the
pair labelandrankt(p) ? (labelt(p), rankt(p)). For
1 ? i ? rankt(p), the ith child of the node at p is
located at path p ? (i). The subtree at path p of t is
t ? p, defined by pathst?p ? {q | p ? q ? pathst} and
labelandrankt?p(q) ? labelandrankt(p ? q).
The paths to X in t are pathst(X) ? {p ?
pathst | labelt(p) ? X}. A frontier is a set of paths
f that are pairwise prefix-independent:
?p1, p2 ? f, p ? paths : p1 = p2 ? p =? p1 = p2
A frontier of t is a frontier f ? pathst.
For t, s ? T?(X), p ? pathst, t[p? s] is the substitu-
tion of s for p in t, where the subtree at path p is replaced
by s. For a frontier f of t, the mass substitution of X
for the frontier f in t is written t[p ? X, ?p ? f ] and
is equivalent to substituting the X(p) for the p serially in
any order.
Trees may be written as strings over ? ? {(, )}
in the usual way. For example, the tree t =
S(NP,VP(V,NP)) has labelandrankt((2)) = (VP, 2)
and labelandrankt((2, 1)) = (V, 0). For t ? T?, ? ? ?,
?(t) is the tree whose root has label ? and whose single
child is t.
The yield of X in t is yieldt(X), the string formed by
reading out the leaves labeled with X in left-to-right or-
der. The usual case (the yield of t) is yieldt ? yieldt(?).
? = {S, NP, VP, PP, PREP, DET, N, V, run, the, of, sons,
daughters}
N = {qnp, qpp, qdet, qn, qprep}
S = q
P = {q?1.0 S(qnp, VP(V(run))),
qnp?0.6 NP(qdet, qn),
qnp?0.4 NP(qnp, qpp),
qpp?1.0 PP(qprep, qnp),
qdet?1.0 DET(the),
qprep?1.0 PREP(of),
qn?0.5 N(sons),
qn?0.5 N(daughters)}
Figure 2: A sample weighted regular tree grammar
(wRTG)
3 Regular Tree Grammars
In this section, we describe the regular tree grammar, a
common way of compactly representing a potentially in-
finite set of trees (similar to the role played by the finite-
state acceptor FSA for strings). We describe the version
(equivalent to TSG (Schabes, 1990)) where the generated
trees are given weights, as are strings in a WFSA.
A weighted regular tree grammar (wRTG) G is a
quadruple (?, N, S, P ), where ? is the alphabet, N is
the finite set of nonterminals, S ? N is the start (or ini-
tial) nonterminal, and P ? N?T?(N)?R+ is the finite
set of weighted productions (R+ ? {r ? R | r > 0}). A
production (lhs, rhs, w) is written lhs?w rhs. Produc-
tions whose rhs contains no nonterminals (rhs ? T?)
are called terminal productions, and rules of the form
A ?w B, for A,B ? N are called ?-productions, or
epsilon productions, and can be used in lieu of multiple
initial nonterminals.
Figure 2 shows a sample wRTG. This grammar ac-
cepts an infinite number of trees. The tree S(NP(DT(the),
N(sons)), VP(V(run))) comes out with probability 0.3.
We define the binary relation?G (single-step derives
in G) on T?(N)?(paths?P )?, pairs of trees and deriva-
tion histories, which are logs of (location, production
used):
?G?
{
((a, h), (b, h ? (p, (l, r, w)))
?
?
(l, r, w) ? P ? p ? pathsa({l}) ? b = a[p? r]
}
where (a, h)?G (b, h ? (p, (l, r, w))) iff tree b may be
derived from tree a by using the rule l ?w r to replace
the nonterminal leaf l at path p with r. For a derivation
history h = ((p1, (l1, r1, w1)), . . . , (pn, (l1, r1, w1))),
the weight of h is w(h) ??ni=1 wi, and call h leftmost if
L(h) ? ?1 ? i < n : pi+1 ?lex pi.1
1() <lex (a), (a1) <lex (a2) iff a1 < a2, (a1) ? b1 <lex
(a2) ? b2 iff a1 < a2 ? (a1 = a2 ? b1 <lex b2)
The reflexive, transitive closure of?G is written??G
(derives in G), and the restriction of ??G to leftmost
derivation histories is?L?G (leftmost derives in G).
The weight of a becoming b in G is wG(a, b) ?
?
h:(a,())?L?G (b,h)
w(h), the sum of weights of all unique
(leftmost) derivations transforming a to b, and the weight
of t in G is WG(t) = wG(S, t). The weighted regu-
lar tree language produced by G is LG ? {(t, w) ?
T? ? R+ |WG(t) = w}.
For every weighted context-free grammar, there is an
equivalent wRTG that produces its weighted derivation
trees with yields being the string produced, and the yields
of regular tree grammars are context free string languages
(G?cseg and Steinby, 1984).
What is sometimes called a forest in natural language
generation (Langkilde, 2000; Nederhof and Satta, 2002)
is a finite wRTG without loops, i.e., ?n ? N(n, ()) ??G
(t, h) =? pathst({n}) = ?. Regular tree languages
are strictly contained in tree sets of tree adjoining gram-
mars (Joshi and Schabes, 1997).
4 Extended-LHS Tree Transducers (xR)
Section 1 informally described the root-to-frontier trans-
ducer class R. We saw that R allows, by use of states,
finite lookahead and arbitrary rearrangement of non-
sibling input subtrees removed by a finite distance. How-
ever, it is often easier to write rules that explicitly repre-
sent such lookahead and movement, relieving the burden
on the user to produce the requisite intermediary rules
and states. We define xR, a convenience-oriented gener-
alization of weighted R. Because of its good fit to natu-
ral language problems, xR is already briefly touched on,
though not defined, in (Rounds, 1970).
A weighted extended-lhs root-to-frontier tree trans-
ducer X is a quintuple (?,?, Q,Qi, R) where ? is the
input alphabet, and ? is the output alphabet, Q is a fi-
nite set of states, Qi ? Q is the initial (or start, or root)
state, and R ? Q ? XRPAT? ? T?(Q ? paths) ? R+
is a finite set of weighted transformation rules, written
(q, pattern) ?w rhs, meaning that an input subtree
matching pattern while in state q is transformed into
rhs, with Q? paths leaves replaced by their (recursive)
transformations. The Q?paths leaves of a rhs are called
nonterminals (there may also be terminal leaves la-
beled by the output tree alphabet ?).
XRPAT? is the set of finite tree patterns: predicate
functions f : T? ? {0, 1} that depend only on the la-
bel and rank of a finite number of fixed paths their in-
put. xR is the set of all such transducers. R, the set
of conventional top-down transducers, is a subset of xR
where the rules are restricted to use finite tree patterns
that depend only on the root: RPAT? ? {p?,r(t)} where
p?,r(t) ? (labelt(()) = ? ? rankt(()) = r).
Rules whose rhs are a pure T? with no states/paths
for further expansion are called terminal rules. Rules
of the form (q, pat) ?w (q?, ()) are ?-rules, or epsilon
rules, which substitute state q? for state q without produc-
ing output, and stay at the current input subtree. Multiple
initial states are not needed: we can use a single start
state Qi, and instead of each initial state q with starting
weight w add the rule (Qi,TRUE) ?w (q, ()) (where
TRUE(t) ? 1, ?t).
We define the binary relation?X for xR tranducer X
on T????Q?(paths?R)?, pairs of partially transformed
(working) trees and derivation histories:
?X?
{
((a, h), (b, h ? (i, (q, pat, r, w))))
?
?
(q, pat, r, w) ? R ? i ? pathsa ?
q = labela(i) ? pat(a ? (i ? (1))) = 1 ?
b = a
[
i? r
[
p? q?(a ? (i ? (1) ? i?)),
?p : labelr(p) = (q?, i?)
]]
}
That is, b is derived from a by application of a rule
(q, pat) ?w r to an unprocessed input subtree a ? i
which is in state q, replacing it by output given by r, with
its nonterminals replaced by the instruction to transform
descendant input subtrees at relative path i? in state q?.
The sources of a rule r = (q, l, rhs, w) ? R are the input-
path parts of the rhs nonterminals:
sources(rhs) ?
?
i?
?
? ?p ? pathsrhs(Q? paths),
q? ? Q : labelrhs(p) = (q?, i?)
?
If the sources of a rule refer to input paths that do not
exist in the input, then the rule cannot apply (because
a ? (i ? (1) ? i?) would not exist). In the traditional state-
ment of R, sources(rhs) is always {(1), . . . , (n)}, writ-
ing xi instead of (i), but in xR, we identify mapped input
subtrees by arbitrary (finite) paths.
An input tree is transformed by starting at the root
in the initial state, and recursively applying output-
generating rules to a frontier of (copies of) input subtrees
(each marked with their own state), until (in a complete
derivation, finishing at the leaves with terminal rules) no
states remain.
Let ??X , ?L?X , and wX(a, b) follow from ?X ex-
actly as in Section 3. Then the weight of (i, o) in X
is WX(i, o) ? wX(Qi(i), o). The weighted tree trans-
duction given by X is XX ? {(i, o, w) ? T? ? T? ?
R+|WX(i, o) = w}.
5 Parsing a Tree Transduction
Derivation trees for a transducer X = (?,?, Q,Qi, R)
are trees labeled by rules (R) that dictate the choice of
rules in a complete X-derivation. Figure 3 shows deriva-
tion trees for a particular transducer. In order to generate
Figure 3: Derivation trees for an R tree transducer.
derivation trees for X automatically, we build a modified
transducer X ?. This new transducer produces derivation
trees on its output instead of normal output trees. X ? is
(?, R,Q,Qi, R?), with
R? ? {(q, pattern, rule(yieldrhs(Q? paths)), w) |
rule = (q, pattern, rhs, w) ? R}
That is, the original rhs of rules are flattened into a
tree of depth 1, with the root labeled by the original rule,
and all the non-expanding ?-labeled nodes of the rhs re-
moved, so that the remaining children are the nonterminal
yield in left to right order. Derivation trees deterministi-
cally produce a single weighted output tree.
The derived transducer X ? nicely produces derivation
trees for a given input, but in explaining an observed
(input/output) pair, we must restrict the possibilities fur-
ther. Because the transformations of an input subtree
depend only on that subtree and its state, we can (Al-
gorithm 1) build a compact wRTG that produces ex-
actly the weighted derivation trees corresponding to X-
transductions (I, ()) ??X (O, h) (with weight equal to
wX(h)).
6 Inside-Outside for wRTG
Given a wRTG G = (?, N, S, P ), we can compute
the sums of weights of trees derived using each produc-
tion by adapting the well-known inside-outside algorithm
for weighted context-free (string) grammars (Lari and
Young, 1990).
The inside weights using G are given by ?G : T? ?
(R?R?), giving the sum of weights of all tree-producing
derivatons from trees with nonterminal leaves:
?G(t) ?
?
?
?
?
?
?
?
?
(t,r,w)?P
w ? ?G(r) if t ? N
?
p?pathst(N)
?G(labelt(p)) otherwise
By definition, ?G(S) gives the sum of the weights of
all trees generated by G. For the wRTG generated by
DERIV(X, I,O), this is exactly WX(I,O).
Outside weights ?G for a nonterminal are the sums of
weights of trees generated by the wRTG that have deriva-
tions containing it, but excluding its inside weights (that
is, the weights summed do not include the weights of
rules used to expand an instance of it).
?G(n ? N) ? 1 if n = S, else:
uses of n in productions
z }| {
X
p,(n?,r,w)?P :labelr(p)=n
w ? ?G(n?) ?
Y
p??pathsr(N)?{p}
?G(labelr(p?))
| {z }
sibling nonterminals
Algorithm 1: DERIV
Input: xR transducer X = (?,?, Q,Qi, R) and ob-
served tree pair I ? T?, O ? T?.
Output: derivation wRTG G = (R,N ? Q? pathsI ?
pathsO, S, P ) generating all weighted deriva-
tion trees for X that produce O from I . Returns
false instead if there are no such trees.
begin
S ? (Qi, (), ()), N ? ?, P ? ?
if PRODUCEI,O(S) then
return (R,N, S, P )
else
return false
end
memoized PRODUCEI,O(q, i, o) returns boolean?
begin
anyrule?? false
for r = (q, pattern, rhs, w) ? R : pattern(I ? i) =
1 ?MATCHO,?(rhs, o) do
(o1, . . . , on)? pathsrhs(Q? paths) sorted by
o1 <lex . . . <lex on
//n = 0 if there are none
labelandrankderivrhs(())? (r, n)
for j ? 1 to n do
(q?, i?)? labelrhs(oj)
c? (q?, i ? i?, o ? oi)
if ?PRODUCEI,O(c) then next r
labelandrankderivrhs((j))? (c, 0)
anyrule?? true
P ? P ? {((q, i, o), derivrhs, w)}
if anyrule? then N ? N ? {(q, i, o)}
return anyrule?
end
MATCHt,?(t?, p) ? ?p? ? path(t?) : label(t?, p?) ?
? =? labelandrankt?(p?) = labelandrankt(p ? p?)
The possible derivations for a given
PRODUCEI,O(q, i, o) are constant and need not be
computed more than once, so the function is memoized.
We have in the worst case to visit all |Q| ? |I| ? |O|
(q, i, o) pairs and have all |R| transducer rules match at
each of them. If enumerating rules matching transducer
input-patterns and output-subtrees has cost L (constant
given a transducer), then DERIV has time complexity
O(L ? |Q| ? |I| ? |O| ? |R|).
Finally, given inside and outside weights, the sum
of weights of trees using a particular production is
?G((n, r, w) ? P ) ? ?G(n) ? w ? ?G(r).
Computing ?G and ?G for nonrecursive wRTG is a
straightforward translation of the above recursive defi-
nitions (using memoization to compute each result only
once) and is O(|G|) in time and space.
7 EM Training
Estimation-Maximization training (Dempster, Laird, and
Rubin, 1977) works on the principle that the corpus like-
lihood can be maximized subject to some normalization
constraint on the parameters by repeatedly (1) estimating
the expectation of decisions taken for all possible ways of
generating the training corpus given the current parame-
ters, accumulating parameter counts, and (2) maximizing
by assigning the counts to the parameters and renormal-
izing. Each iteration is guaranteed to increase the like-
lihood until a local maximum is reached.
Algorithm 2 implements EM xR training, repeatedly
computing inside-outside weights (using fixed transducer
derivation wRTGs for each input/output tree pair) to ef-
ficiently sum each parameter contribution to likelihood
over all derivations. Each EM iteration takes time linear
in the size of the transducer and linear in the size of the
derivation tree grammars for the training examples. The
size of the derivation trees is at worst O(|Q|?|I|?|O|?|R|).
For a corpus of K examples with average input/output
size M , an iteration takes (at worst) O(|Q| ? |R| ?K ?M2)
time?quadratic, like the forward-backward algorithm.
8 Tree-to-String Transducers (xRS)
We now turn to tree-to-string transducers (xRS). In the
automata literature, these were first called generalized
syntax-directed translations (Aho and Ullman, 1971) and
used to specify compilers. Tree-to-string transducers
have also been applied to machine translation (Yamada
and Knight, 2001; Eisner, 2003).
We give an explicit tree-to-string transducer example
in the next section. Formally, a weighted extended-lhs
root-to-frontier tree-to-string transducer X is a quintuple
(?,?, Q,Qi, R) where ? is the input alphabet, and ?
is the output alphabet, Q is a finite set of states, Qi ?
Q is the initial (or start, or root) state, and R ? Q ?
XRPAT?? (?? (Q?paths))??R+ are a finite set of
weighted transformation rules, written (q, pattern) ?w
rhs. A rule says that to transform (with weight w) an
input subtree matching pattern while in state q, replace
it by the string of rhs with its nonterminal (Q ? paths)
letters replaced by their (recursive) transformation.
xRS is the same as xR, except that the rhs are strings
containing some nonterminals instead of trees containing
nonterminal leaves (so the intermediate derivation objects
Algorithm 2: TRAIN
Input: xR transducer X = (?,?, Q,Qd, R), observed
weighted tree pairs T ? T? ? T? ? R+, normal-
ization function Z({countr | r ? R}, r? ? R),
minimum relative log-likelihood change for con-
vergence ? ? R+, maximum number of iterations
maxit ? N, and prior counts (for a so-called
Dirichlet prior) {priorr | r ? R} for smoothing
each rule.
Output: New rule weights W ? {wr | r ? R}.
begin
for (i, o, w) ? T do
di,o ?
DERIV(X, i, o)//Alg. 1
if di,o = false then
T ? T ? {(i, o, w)}
warn(more rules are needed to explain (i,o))
compute inside/outside weights for di,o and
remove all useless nonterminals n whose
?di,o(n) = 0 or ?di,o(n) = 0
itno? 0, lastL? ??, ? ? ?
for r = (q, pat, rhs, w) ? R do wr ? w
while ? ? ? ? itno < maxit do
for r ? R do countr ? priorr
L? 0
for (i, o, wexample) ? T
//Estimate
do
let D ? di,o ? (R,N, S, P )
compute ?D, ?D using latest
W ? {wr | r ? R}
//see Section 6
for prod = (n, rhs, w) ? P do
?D(prod)? ?D(n) ? w ? ?D(rhs)
let rule ? labelrhs(())
countrule ? countrule+wexample ? ?D(prod)?D(S)
L? L + log ?D(S) ? wexample
for r = (q, pattern, rhs, w) ? R
//Maximize
do
wr ?
countr
Z({countr|r ? R}, r)
//e.g.Z((q, a, b, c)) ?
?
r=(q,d,e,f)?R
countr
? ? L? lastL|L|
lastL? L, itno? itno+ 1
end
are strings containing state-marked input subtrees). We
have developed an xRS training procedure similar to the
xR procedure, with extra computational expense to con-
sider how different productions might map to different
spans of the output string. Space limitations prohibit a
detailed description; we refer the reader to a longer ver-
sion of this paper (submitted). We note that this algo-
rithm subsumes normal inside-outside training of PCFG
on strings (Lari and Young, 1990), since we can always
fix the input tree to some constant for all training exam-
ples.
9 Example
It is possible to cast many current probabilistic natural
language models as R-type tree transducers. In this sec-
tion, we implement the translation model of (Yamada
and Knight, 2001). Their generative model provides
a formula for P(Japanese string | English tree), in terms
of individual parameters, and their appendix gives spe-
cial EM re-estimation formulae for maximizing the prod-
uct of these conditional probabilities across the whole
tree/string corpus.
We now build a trainable xRS tree-to-string transducer
that embodies the same P(Japanese string | English tree).
First, we need start productions like these, where q is the
start state:
- q x:S ? q.TOP.S x
- q x:VP ? q.TOP.VP x
These set up states like q.TOP.S, which means ?translate
this tree, whose root is S.? Then every q.parent.child pair
gets its own set of three insert-function-word productions,
e.g.:
- q.TOP.S x ? i x, r x
- q.TOP.S x ? r x, i x
- q.TOP.S x ? r x
- q.NP.NN x ? i x, r x
- q.NP.NN x ? r x, i x
- q.NP.NN x ? r x
State i means ?produce a Japanese function word out of
thin air.? We include an i production for every Japanese
word in the vocabulary, e.g.:
- i x ? de
- i x ? kuruma
- i x ? wa
State r means ?re-order my children and then recurse.?
For internal nodes, we include a production for ev-
ery parent/child-sequence and every permutation thereof,
e.g.:
- r NP(x0:CD, x1:NN) ? q.NP.CD x0, q.NP.NN x1
- r NP(x0:CD, x1:NN) ? q.NP.NN x1, q.NP.CD x0
The rhs sends the child subtrees back to state q for re-
cursive processing. However, for English leaf nodes, we
instead transition to a different state t, so as to prohibit
any subsequent Japanese function word insertion:
- r NN(x0:car) ? t x0
- r CC(x0:and) ? t x0
State t means ?translate this word,? and we have a produc-
tion for every pair of co-occurring English and Japanese
words:
- t car ? kuruma
- t car ? wa
- t car ? *e*
This follows (Yamada and Knight, 2001) in also allowing
English words to disappear, or translate to epsilon.
Every production in the xRS transducer has an associ-
ated weight and corresponds to exactly one of the model
parameters.
There are several benefits to this xRS formulation.
First, it clarifies the model, in the same way that (Knight
and Al-Onaizan, 1998; Kumar and Byrne, 2003) eluci-
date other machine translation models in easily-grasped
FST terms. Second, the model can be trained with
generic, off-the-shelf tools?versus the alternative of
working out model-specific re-estimation formulae and
implementing custom training software. Third, we can
easily extend the model in interesting ways. For exam-
ple, we can add productions for multi-level and lexical
re-ordering:
- r NP(x0:NP, PP(IN(of), x1:NP)) ? q x1, no, q x0
We can add productions for phrasal translations:
- r NP(JJ(big), NN(cars)) ? ooki, kuruma
This can now include crucial non-constituent phrasal
translations:
- r S(NP(PRO(there),VP(VB(are), x0:NP) ? q x0, ga,
arimasu
We can also eliminate many epsilon word-translation
rules in favor of more syntactically-controlled ones, e.g.:
- r NP(DT(the),x0:NN) ? q x0
We can make many such changes without modifying the
training procedure, as long as we stick to tree automata.
10 Related Work
Tree substitution grammars or TSG (Schabes, 1990)
are equivalent to regular tree grammars. xR transduc-
ers are similar to (weighted) Synchronous TSG, except
that xR can copy input trees (and transform the copies
differently), but does not model deleted input subtrees.
(Eisner, 2003) discusses training for Synchronous TSG.
Our training algorithm is a generalization of forward-
backward EM training for finite-state (string) transducers,
which is in turn a generalization of the original forward-
backward algorithm for Hidden Markov Models.
11 Acknowledgments
Thanks to Daniel Gildea and Kenji Yamada for comments
on a draft of this paper, and to David McAllester for help-
ing us connect into previous work in automata theory.
References
Aho, A. V. and J. D. Ullman. 1971. Translations of a context-free grammar.
Information and Control, 19:439?475.
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000. Learning de-
pendency translation models as collections of finite state head transducers.
Computational Linguistics, 26(1):45?60.
Baker, J. K. 1979. Trainable grammars for speech recognition. In D. Klatt and
J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America. Boston, MA, pages 547?550.
Bangalore, Srinivas and Owen Rambow. 2000. Exploiting a probabilistic hierar-
chical model for generation. In Proc. COLING.
Baum, L. E. and J. A. Eagon. 1967. An inequality with application to statistical
estimation for probabilistic functions of Markov processes and to a model for
ecology. Bulletin of the American Mathematicians Society, 73:360?363.
Charniak, Eugene. 2001. Immediate-head parsing for language models. In Proc.
ACL.
Chelba, C. and F. Jelinek. 2000. Structured language modeling. Computer
Speech and Language, 14(4):283?332.
Collins, Michael. 1997. Three generative, lexicalised models for statistical pars-
ing. In Proc. ACL.
Comon, H., M. Dauchet, R. Gilleron, F. Jacquemard, D. Lugiez, S. Tison, and
M. Tommasi. 1997. Tree automata techniques and applications. Available on
www.grappa.univ-lille3.fr/tata. release October, 1st 2002.
Corston-Oliver, Simon, Michael Gamon, Eric K. Ringger, and Robert Moore.
2002. An overview of Amalgam, a machine-learned generation module. In
Proc. IWNLG.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society,
Series B, 39(1):1?38.
Eisner, Jason. 2003. Learning non-isomorphic tree mappings for machine trans-
lation. In Proc. ACL (companion volume).
Engelfriet, J. 1975. Bottom-up and top-down tree transformations?a compari-
son. Math. Systems Theory, 9(3):198?231.
G?cseg, F. and M. Steinby. 1984. Tree Automata. Akad?miai Kiad?, Budapest.
Gildea, Daniel. 2003. Loosely tree-based alignment for machine translation. In
Proc. ACL.
Joshi, A. and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and
A. Salomaa, editors, Handbook of Formal Languages (Vol. 3). Springer, NY.
Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing.
In Proc. ACL.
Knight, K. and Y. Al-Onaizan. 1998. Translation with finite-state devices. In
Proc. AMTA.
Knight, K. and D. Marcu. 2002. Summarization beyond sentence extraction?
a probabilistic approach to sentence compression. Artificial Intelligence,
139(1).
Kumar, S. and W. Byrne. 2003. A weighted finite state transducer implemen-
tation of the alignment template model for statistical machine translation. In
Proceedings of HLT-NAACL.
Langkilde, I. 2000. Forest-based statistical sentence generation. In Proc. NAACL.
Langkilde, I. and K. Knight. 1998. Generation that exploits corpus-based statisti-
cal knowledge. In Proc. ACL.
Lari, K. and S. J. Young. 1990. The estimation of stochastic context-free gram-
mars using the inside-outside algorithm. Computer Speech and Language, 4.
Nederhof, Mark-Jan and Giorgio Satta. 2002. Parsing non-recursive CFGs. In
Proc. ACL.
Pang, Bo, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of
multiple translations extracting paraphrases and generating new sentences. In
Proc. HLT/NAACL.
Rounds, William C. 1970. Mappings and grammars on trees. Mathematical
Systems Theory, 4(3):257?287.
Schabes, Yves. 1990. Mathematical and Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, Department of Computer and Information Science,
University of Pennsylvania.
Thatcher, J. W. 1970. Generalized2 sequential machine maps. J. Comput. System
Sci., 4:339?367.
Viterbi, A. 1967. Error bounds for convolutional codes and an asymptotically
optimum decoding algorithm. IEEE Trans. Information Theory, IT-13.
Wu, Dekai. 1997. Stochastic inversion transduction grammars and bilingual pars-
ing of parallel corpora. Computational Linguistics, 23(3):377?404.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation
model. In Proc. ACL.
What?s in a translation rule?
Michel Galley
Dept. of Computer Science
Columbia University
New York, NY 10027
galley@cs.columbia.edu
Mark Hopkins
Dept. of Computer Science
University of California
Los Angeles, CA 90024
mhopkins@cs.ucla.edu
Kevin Knight and Daniel Marcu
Information Sciences Institute
University of Southern California
Marina Del Rey, CA 90292
{knight,marcu}@isi.edu
Abstract
We propose a theory that gives formal seman-
tics to word-level alignments defined over par-
allel corpora. We use our theory to introduce a
linear algorithm that can be used to derive from
word-aligned, parallel corpora the minimal set
of syntactically motivated transformation rules
that explain human translation data.
1 Introduction
In a very interesting study of syntax in statistical machine
translation, Fox (2002) looks at how well proposed trans-
lation models fit actual translation data. One such model
embodies a restricted, linguistically-motivated notion of
word re-ordering. Given an English parse tree, children
at any node may be reordered prior to translation. Nodes
are processed independently. Previous to Fox (2002), it
had been observed that this model would prohibit certain
re-orderings in certain language pairs (such as subject-
VP(verb-object) into verb-subject-object), but Fox car-
ried out the first careful empirical study, showing that
many other common translation patterns fall outside the
scope of the child-reordering model. This is true even
for languages as similar as English and French. For
example, English adverbs tend to move outside the lo-
cal parent/children in environment. The English word
?not? translates to the discontiguous pair ?ne ... pas.?
English parsing errors also cause trouble, as a normally
well-behaved re-ordering environment can be disrupted
by wrong phrase attachment. For other language pairs,
the divergence is expected to be greater.
In the face of these problems, we may choose among
several alternatives. The first is to abandon syntax in
statistical machine translation, on the grounds that syn-
tactic models are a poor fit for the data. On this view,
adding syntax yields no improvement over robust phrase-
substitution models, and the only question is how much
does syntax hurt performance. Along this line, (Koehn
et al, 2003) present convincing evidence that restricting
phrasal translation to syntactic constituents yields poor
translation performance ? the ability to translate non-
constituent phrases (such as ?there are?, ?note that?, and
?according to?) turns out to be critical and pervasive.
Another direction is to abandon conventional English
syntax and move to more robust grammars that adapt to
the parallel training corpus. One approach here is that of
Wu (1997), in which word-movement is modeled by rota-
tions at unlabeled, binary-branching nodes. At each sen-
tence pair, the parse adapts to explain the translation pat-
tern. If the same unambiguous English sentence were to
appear twice in the corpus, with different Chinese trans-
lations, then it could have different learned parses.
A third direction is to maintain English syntax and
investigate alternate transformation models. After all,
many conventional translation systems are indeed based
on syntactic transformations far more expressive than
what has been proposed in syntax-based statistical MT.
We take this approach in our paper. Of course, the broad
statistical MT program is aimed at a wider goal than
the conventional rule-based program ? it seeks to under-
stand and explain human translation data, and automati-
cally learn from it. For this reason, we think it is impor-
tant to learn from the model/data explainability studies of
Fox (2002) and to extend her results. In addition to being
motivated by rule-based systems, we also see advantages
to English syntax within the statistical framework, such
as marrying syntax-based translation models with syntax-
based language models (Charniak et al, 2003) and other
potential benefits described by Eisner (2003).
Our basic idea is to create transformation rules that
condition on larger fragments of tree structure. It is
certainly possible to build such rules by hand, and we
have done this to formally explain a number of human-
translation examples. But our main interest is in collect-
ing a large set of such rules automatically through corpus
analysis. The search for these rules is driven exactly by
the problems raised by Fox (2002) ? cases of crossing
and divergence motivate the algorithms to come up with
better explanations of the data and better rules. Section
2 of this paper describes algorithms for the acquisition
of complex rules for a transformation model. Section 3
gives empirical results on the explanatory power of the
acquired rules versus previous models. Section 4 presents
examples of learned rules and shows the various types of
transformations (lexical and nonlexical, contiguous and
noncontiguous, simple and complex) that the algorithms
are forced (by the data) to invent. Section 5 concludes.
Due to space constraints, all proofs are omitted.
2 Rule Acquisition
Suppose that we have a French sentence, its translation
into English, and a parse tree over the English translation,
as shown in Figure 1. Generally one defines an alignment
as a relation between the words in the French sentence
and the words in the English sentence. Given such an
alignment however, what kinds of rules are we entitled
to learn from this instance? How do we know when it is
valid to extract a particular rule, especially in the pres-
ence of numerous crossings in the alignment? In this sec-
tion, we give principled answers to these questions, by
constructing a theory that gives formal semantics to word
alignments.
2.1 A Theory of Word Alignments
We are going to define a generative process through
which a string from a source alphabet is mapped to a
rooted tree whose nodes are labeled from a target alha-
bet. Henceforth we will refer to symbols from our source
alphabet as source symbols and symbols from our target
alphabet as target symbols. We define a symbol tree over
an alphabet ? as a rooted, directed tree, the nodes of
which are each labeled with a symbol of ?.
We want to capture the process by which a symbol tree
over the target language is derived from a string of source
symbols. Let us refer to the symbol tree that we want to
derive as the target tree. Any subtree of this tree will be
called a target subtree. Furthermore, we define a deriva-
tion string as an ordered sequence of elements, each of
which is either a source symbol or a target subtree.
Now we are ready to define the derivation process.
Given a derivation string S, a derivation step replaces
a substring S? of S with a target subtree T that has the
following properties:
1. Any target subtree in S ? is a subtree of T .
2. Any target subtree in S but not in S ? does not share
nodes with T .
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 1: A French sentence aligned with an English
parse tree.
il ne va pas
ne va pas
he
PRP
NP
ne pas
he
PRP
NP
S
NP VP
PRP RBAUX VB
he notdoes go
VB
go
il ne va pas
ne va pasRB
not
ne heRB
not
S
NP VP
PRP RBAUX VB
he notdoes go
il ne va pas
S
NP VP
PRP RBAUX VB
he notdoes go
NP VP
PRP RBAUX VB
he notdoes go
Figure 2: Three alternative derivations from a source sen-
tence to a target tree.
Moreover, a derivation from a string S of source sym-
bols to the target tree T is a sequence of derivation steps
that produces T from S.
Moving away from the abstract for a moment, let us
revisit the example from Figure 1. Figure 2 shows three
derivations of the target tree from the source string ?il
ne va pas?, which are all consistent with our defini-
tions. However, it is apparent that one of these deriva-
tions seems much more ?wrong? than the other. Specif-
ically, in the second derivation, ?pas? is replaced by the
English word ?he,? which makes no sense. Given the vast
space of possible derivations (according to the definition
above), how do we distinguish between good ones and
bad ones? Here is where the notion of an alignment be-
comes useful.
Let S be a string of source symbols and let T be a target
tree. First observe the following facts about derivations
from S to T (these follow directly from the definitions):
1. Each element of S is replaced at exactly one step of
the derivation.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 3: The alignments induced by the derivations in
Figure 2
2. Each node of T is created at exactly one step of the
derivation.
Thus for each element s of S, we can define
replaced(s, D) to be the step of the derivation D during
which s is replaced. For instance, in the leftmost deriva-
tion of Figure 2, ?va? is replaced by the second step of the
derivation, thus replaced(va, D) = 2. Similarly, for each
node t of T , we can define created(t, D) to be the step
of derivation D during which t is created. For instance,
in the same derivation, the nodes labeled by ?AUX? and
?VP? are created during the third step of the derivation,
thus created(AUX, D) = 3 and created(VP, D) = 3.
Given a string S of source symbols and a target tree
T , an alignment A with respect to S and T is a relation
between the leaves of T and the elements of S. Choose
some derivation D from S to T . The alignment A in-
duced by D is created as follows: an element s of S is
aligned with a leaf node t of T iff replaced(s, D) =
created(t, D). In other words, a source word is aligned
with a target word if the target word is created during the
same step in which the source word is replaced. Figure 3
shows the alignments induced by the derivations of Fig-
ure 2.
Now, say that we have a source string, a target tree,
and an alignment A. A key observation is that the set
of ?good? derivations according to A is precisely the set
of derivations that induce alignments A? such that A is
a subalignment of A?. By subalignment, we mean that
A ? A? (recall that alignments are simple mathematical
relations). In other words, A is a subalignment of A? if A
aligns two elements only if A? also aligns them.
We can see this intuitively by examining Figures 2 and
3. Notice that the two derivations that seem ?right? (the
first and the third) are superalignments of the alignment
given in Figure 1, while the derivation that is clearly
wrong is not. Hence we now have a formal definition
of the derivations that we are interested in. We say that
a derivation is admitted by an alignment A if it induces a
superalignment of A. The set of derivations from source
string S to target tree T that are admitted by alignment A
can be denoted ?A(S, T ). Given this, we are ready to ob-
tain a formal characterization of the set of rules that can
ne pas
he
PRP
NP
VB
go
NP VP
PRP RBAUX VB
he notdoes go
Derivationstep: Inducedrule:
input: ne VB?pas
output: VP
RBAUX x2
notdoes
S
NP VP
PRP RBAUX VB
he notdoes go
input: NP?VP
output: S
x1 x2
Figure 4: Two derivation steps and the rules that are in-
duced from them.
be inferred from the source string, target tree, and align-
ment.
2.2 From Derivations to Rules
In essence, a derivation step can be viewed as the applica-
tion of a rule. Thus, compiling the set of derivation steps
used in any derivation of ?A(S, T ) gives us, in a mean-
ingful sense, all relevant rules that can be extracted from
the triple (S, T, A). In this section, we show in concrete
terms how to convert a derivation step into a usable rule.
Consider the second-last derivation step of the first
derivation in Figure 2. In it, we begin with a source sym-
bol ?ne?, followed by a target subtree rooted at V B, fol-
lowed by another source symbol ?pas.? These three ele-
ments of the derivation string are replaced with a target
subtree rooted at V P that discards the source symbols
and contains the target subtree rooted at V B. In general,
this replacement process can be captured by the rule de-
picted in Figure 4. The input to the rule are the roots
of the elements of the derivation string that are replaced
(where we define the root of a symbol to be simply the
symbol itself), whereas the output of the rule is a symbol
tree, except that some of the leaves are labeled with vari-
ables instead of symbols from the target alhabet. These
variables correspond to elements of the input to the rule.
For instance, the leaf labeled x2 means that when this rule
is applied, x2 is replaced by the target subtree rooted at
V B (since V B is the second element of the input). Ob-
serve that the second rule induced in Figure 4 is simply
a CFG rule expressed in the opposite direction, thus this
rule format can (and should) be viewed as a strict gener-
alization of CFG rules.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
{ il, ne, va,pas}
{ ne, va,pas}{ il }
{ il }
{ il }
{ il }
{ne,pas} {ne,pas}
{ne,pas} {ne,pas}
{ va }
{ ne } { va }
{ va }
{pas}
Figure 5: An alignment graph. The nodes are annotated
with their spans. Nodes in the frontier set are boldfaced
and italicized.
Every derivation step can be mapped to a rule in this
way. Hence given a source string S, a target tree T , and
an alignment A, we can define the set ?A(S, T ) as the set
of rules in any derivation D ? ?A(S, T ). We can regard
this as the set of rules that we are entitled to infer from
the triple (S, T, A).
2.3 Inferring Complex Rules
Now we have a precise problem statement: learn the set
?A(S, T ). It is not immediately clear how such a set can
be learned from the triple (S, T, A). Fortunately, we can
infer these rules directly from a structure called an align-
ment graph. In fact, we have already seen numerous ex-
amples of alignment graphs. Graphically, we have been
depicting the triple (S, T, A) as a rooted, directed, acyclic
graph (where direction is top-down in the diagrams). We
refer to such a graph as an alignment graph. Formally,
the alignment graph corresponding to S, T , and A is just
T , augmented with a node for each element of S, and
edges from leaf node t ? T to element s ? S iff A aligns
s with t. Although there is a difference between a node
of the alignment graph and its label, we will not make a
distinction, to ease the notational burden.
To make the presentation easier to follow, we assume
throughout this section that the alignment graph is con-
nected, i.e. there are no unaligned elements. All of the
results that follow have generalizations to deal with un-
aligned elements, but unaligned elements incur certain
procedural complications that would cloud the exposi-
tion.
It turns out that it is possible to systematically con-
vert certain fragments of the alignment graph into rules
of ?A(S, T ). We define a fragment of a directed, acyclic
graph G to be a nontrivial (i.e. not just a single node) sub-
graph G? of G such that if a node n is in G? then either n
is a sink node of G? (i.e. it has no children) or all of its
children are in G? (and it is connected to all of them). In
VP
RBAUX VB
notdoes
ne pas
S
NP VP
input: ne VB?pas
output: VP
RBAUX x2
notdoes
input: NP?VP
output: S
x1 x2
{ ne } {pas}
{ va }
{ ne, va,pas}
{ il } { ne, va,pas}
{ il, ne, va,pas}
Figure 6: Two frontier graph fragments and the rules in-
duced from them. Observe that the spans of the sink
nodes form a partition of the span of the root.
Figure 6, we show two examples of graph fragments of
the alignment graph of Figure 5.
The span of a node n of the alignment graph is the
subset of nodes from S that are reachable from n. Note
that this definition is similar to, but not quite the same
as, the definition of a span given by Fox (2002). We
say that a span is contiguous if it contains all elements
of a contiguous substring of S. The closure of span(n)
is the shortest contiguous span which is a superset of
span(n). For instance, the closure of {s2, s3, s5, s7}
would be {s2, s3, s4, s5, s6, s7} The alignment graph in
Figure 5 is annotated with the span of each node.
Take a look at the graph fragments in Figure 6. These
fragments are special: they are examples of frontier
graph fragments. We first define the frontier set of an
alignment graph to be the set of nodes n that satisfy the
following property: for every node n? of the alignment
graph that is connected to n but is neither an ancestor nor
a descendant of n, span(n?) ? closure(span(n)) = ?.
We then define a frontier graph fragment of an align-
ment graph to be a graph fragment such that the root and
all sinks are in the frontier set. Frontier graph fragments
have the property that the spans of the sinks of the frag-
ment are each contiguous and form a partition of the span
of the root, which is also contiguous. This allows the fol-
lowing transformation process:
1. Place the sinks in the order defined by the partition
(i.e. the sink whose span is the first part of the span
of the root goes first, the sink whose span is the sec-
ond part of the span of the root goes second, etc.).
This forms the input of the rule.
2. Replace sink nodes of the fragment with a variable
corresponding to their position in the input, then
take the tree part of the fragment (i.e. project the
fragment on T ). This forms the output of the rule.
Figure 6 shows the rules derived from the given graph
fragments. We have the following result.
Theorem 1 Rules constructed according to the above
procedure are in ?A(S, T ).
Rule extraction: Algorithm 1. Thus we now have a
simple method for extracting rules of ?A(S, T ) from the
alignment graph: search the space of graph fragments for
frontier graph fragments.
Unfortunately, the search space of all fragments of a
graph is exponential in the size of the graph, thus this
procedure can also take a long time to execute. To ar-
rive at a much faster procedure, we take advantage of the
following provable facts:
1. The frontier set of an alignment graph can be identi-
fied in time linear in the size of the graph.
2. For each node n of the frontier set, there is a unique
minimal frontier graph fragment rooted at n (ob-
serve that for any node n? not in the frontier set,
there is no frontier graph fragment rooted at n?, by
definition).
By minimal, we mean that the frontier graph fragment
is a subgraph of every other frontier graph fragment with
the same root. Clearly, for an alignment graph with k
nodes, there are at most k minimal frontier graph frag-
ments. In Figure 7, we show the seven minimal frontier
graph fragments of the alignment graph of Figure 5. Fur-
thermore, all other frontier graph fragments can be cre-
ated by composing 2 or more minimal graph fragments,
as shown in Figure 8. Thus, the entire set of frontier graph
fragments (and all rules derivable from these fragments)
can be computed systematically as follows: compute the
set of minimal frontier graph fragments, compute the set
of graph fragments resulting from composing 2 minimal
frontier graph fragments, compute the set of graph frag-
ments resulting from composing 3 minimal graph frag-
ments, etc. In this way, the rules derived from the min-
imal frontier graph fragments can be regarded as a ba-
sis for all other rules derivable from frontier graph frag-
ments. Furthermore, we conjecture that the set of rules
derivable from frontier graph fragments is in fact equiva-
lent to ?A(S, T ).
Thus we have boiled down the problem of extracting
complex rules to the following simple problem: find the
set of minimal frontier graph fragments of a given align-
ment graph.
The algorithm is a two-step process, as shown below.
Rule extraction: Algorithm 2
1. Compute the frontier set of the alignment graph.
2. For each node of the frontier set, compute the mini-
mal frontier graph fragment rooted at that node.
VP
RBAUX VB
notdoes
ne pas
S
NP VP
NP
PRP
PRP
he
VB
go
go
vahe
il
Figure 7: The seven minimal frontier graph fragments of
the alignment graph in Figure 5
VP
RBAUX VB
notdoes
ne pas
VB
go
+ =
VP
RBAUX VB
notdoes
ne pas
go
S
NP VP
+ + =
NP
PRP
PRP
he
S
NP VP
PRP
he
Figure 8: Example compositions of minimal frontier
graph fragments into larger frontier graph fragments.
Step 1 can be computed in a single traversal of the
alignment graph. This traversal annotates each node with
its span and its complement span. The complement span
is computed as the union of the complement span of its
parent and the span of all its siblings (siblings are nodes
that share the same parent). A node n is in the frontier
set iff complement span(n) ? closure(span(n)) = ?.
Notice that the complement span merely summarizes the
spans of all nodes that are neither ancestors nor descen-
dents of n. Since this step requires only a single graph
traversal, it runs in linear time.
Step 2 can also be computed straightforwardly. For
each node n of the frontier set, do the following: expand
n, then as long as there is some sink node n? of the result-
ing graph fragment that is not in the frontier set, expand
n?. Note that after computing the minimal graph frag-
ment rooted at each node of the frontier set, every node
of the alignment graph has been expanded at most once.
Thus this step also runs in linear time.
For clarity of exposition and lack of space, a couple of
issues have been glossed over. Briefly:
? As previously stated, we have ignored here the is-
sue of unaligned elements, but the procedures can
be easily generalized to accommodate these. The
results of the next two sections are all based on im-
plementations that handle unaligned elements.
? This theory can be generalized quite cleanly to in-
clude derivations for which substrings are replaced
by sets of trees, rather than one single tree. This
corresponds to allowing rules that do not require the
output to be a single, rooted tree. Such a general-
ization gives some nice power to effectively explain
certain linguistic phenomena. For instance, it allows
us to immediately translate ?va? as ?does go? in-
stead of delaying the creation of the auxiliary word
?does? until later in the derivation.
3 Experiments
3.1 Language Choice
We evaluated the coverage of our model of transforma-
tion rules with two language pairs: English-French and
English-Chinese. These two pairs clearly contrast by
the underlying difficulty to understand and model syntac-
tic transformations among pairs: while there is arguably
a fair level of cohesion between English and French,
English and Chinese are syntactically more distant lan-
guages. We also chose French to compare our study with
that of Fox (2002). The additional language pair provides
a good means of evaluating how our transformation rule
extraction method scales to more problematic language
pairs for which child-reordering models are shown not to
explain the data well.
3.2 Data
We performed experiments with two corpora, the FBIS
English-Chinese Parallel Text and the Hansard French-
English corpus.We parsed the English sentences with
a state-of-the-art statistical parser (Collins, 1999). For
the FBIS corpus (representing eight million English
words), we automatically generated word-alignments us-
ing GIZA++ (Och and Ney, 2003), which we trained on
a much larger data set (150 million words). Cases other
than one-to-one sentence mappings were eliminated. For
the Hansard corpus, we took the human annotation of
word alignment described in (Och and Ney, 2000). The
corpus contains two kinds of alignments: S (sure) for
unambiguous cases and P (possible) for unclear cases,
e.g. idiomatic expressions and missing function words
(S ? P ). In order to be able to make legitimate com-
parisons between the two language pairs, we also used
GIZA++ to obtain machine-generated word alignments
for Hansard: we trained it with the 500 sentences and
additional data representing 13.7 million English words
(taken from the Hansard and European parliament cor-
pora).
3.3 Results
From a theoretical point of view, we have shown that our
model can fully explain the transformation of any parse
tree of the source language into a string of the target lan-
guage. The purpose of this section is twofold: to pro-
vide quantitative results confirming the full coverage of
our model and to analyze some properties of the trans-
formation rules that support these derivations (linguistic
analyses of these rules are presented in the next section).
Figure 9 summarizes the coverage of our model with
respect to the Hansard and FBIS corpora. For the for-
mer, we present results for the three alignments: S align-
ments, P alignments, and the alignments computed by
GIZA++. Each plotted value represents a percentage of
parse trees in a corpus that can be transformed into a tar-
get sentence using transformation rules. The x-axis rep-
resents different restrictions on the size of these rules: if
we use a model that restrict rules to a single expansion
of a non-terminal into a sequence of symbols, we are in
the scope of the child-reordering model of (Yamada and
Knight, 2001; Fox, 2002). We see that its explanatory
power is quite poor, with only 19.4%, 14.3%, 16.5%, and
12.1% (for the respective corpora). Allowing more ex-
pansions logically expands the coverage of the model,
until the point where it is total: transformation rules no
larger than 17, 18, 23, and 43 (in number of rule expan-
sions) respectively provide enough coverage to explain
the data at 100% for each of the four cases.
It appears from the plot that the quality of alignments
plays an important role. If we compare the three kinds of
alignments available for the Hansard corpus, we see that
much more complex transformation rules are extracted
from noisy GIZA++ alignments. It also appears that the
language difference produces quite contrasting results.
Rules acquired for the English-Chinese pair have, on av-
erage, many more nodes. Note that the language differ-
ence in terms of syntax might be wider than what the plot
seems to indicate, since word alignments computed for
the Hansard corpus are likely to be more errorful than the
ones for FBIS because the training data used to induce the
latter is more than ten times larger than for the former.
In Figure 10, we show the explanatory power of our
model at the node level. At each node of the frontier
set, we determine whether it is possible to extract a rule
that doesn?t exceed a given limit k on its size. The plot-
ted values represent the percentage of frontier set inter-
nal nodes that satisfy this condition. These results appear
more promising for the child-reordering model, with cov-
erage ranging from 72.3% to 85.1% of the nodes, but we
should keep in mind that many of these nodes are low in
the tree (e.g. base NPs); extraction of 1-level transfor-
mation rules generally present no difficulties when child
nodes are pre-terminals, since any crossings can be re-
solved by lexicalizing the elements involved in it. How-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
Pa
rse
 tr
ee
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 9: Percentage of parse trees covered by the model
given different constraints on the maximum size of the
transformation rules.
0.7
0.75
0.8
0.85
0.9
0.95
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
No
de
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 10: Same as Figure 9, except that here coverage is
evaluated at the node level.
ever, higher level syntactic constituents are more prob-
lematic for child-reordering models, and the main rea-
sons they fail to provide explanation of the parses at the
sentence level.
Table 1 shows that the extraction of rules can be per-
formed quite efficiently. Our first algorithm, which has an
exponential running time, cannot scale to process large
corpora and extract a sufficient number of rules that a
syntax-based statistical MT system would require. The
second algorithm, which runs in linear time, is on the
other hand barely affected by the size of rules it extracts.
k=1 3 5 7 10 20 50
I 4.1 10.2 57.9 304.2 - - -
II 4.3 5.4 5.9 6.4 7.33 9.6 11.8
Table 1: Running time in seconds of the two algorithms
on 1000 sentences. k represent the maximum size of rules
to extract.
NPB
DT NN RB
that Government simply tells
ADVP
VBZ
NPB
DT NNS
the people what is themgood for
WP VBZ JJ IN PRP
NPB
ADJP
VP
SG-A
SBAR-A
VHPN
VP
S
le gouvernement dit tout simplement ? les gens ce qui est bon pour eux
input:
VBZ ADVP ?NPB SBAR -S
output: S
VPx2
x1 x3 x4
Figure 11: Adverb-verb reordering.
4 Discussions
In this section, we present some syntactic transformation
rules that our system learns. Fox (2002) identified three
major causes of crossings between English and French:
the ?ne ... pas? construct, modals and adverbs, which a
child-reordering model doesn?t account for. In section 2,
we have already explained how we learn syntactic rules
involving ?ne ... pas?. Here we describe the other two
problematic cases.
Figure 11 presents a frequent cause of crossings be-
tween English and French: adverbs in French often ap-
pear after the verb, which is less common in English.
Parsers generally create nested verb phrases when ad-
verbs are present, thus no child reordering can allow a
verb and an adverb to be permuted. Multi-level reodering
as the rule in the figure can prevent crossings. Fox?s solu-
tion to the problem of crossings is to flatten verb phrases.
This is a solution for this sentence pair, since this ac-
counts for adverb-verb reorderings, but flattening the tree
structure is not a general solution. Indeed, it can only ap-
ply to a very limited number of syntactic categories, for
which the advantage of having a deep syntactic structure
is lost.
Figure 12 (dotted lines are P alignments) shows an in-
teresting example where flattening the tree structure can-
not resolve all crossings in node-reordering models. In
these models, a crossing remains between MD and AUX
no matter how VPs are flattened. Our transformation rule
model creates a lexicalized rule as shown in the figure,
where the transformation of ?will be? into ?sera? is the
only way to resolve the crossing.
In the Chinese-English domain, the rules extracted by
our algorithm often have the attractive quality that they
are the kind of common-sense constructions that are used
in Chinese language textbooks to teach students. For in-
stance, there are several that illustrate the complex re-
orderings that occur around the Chinese marker word
?de.?
NPB
DT JJ NN
the full report will
MD AUX VB
be coming in before the fall
RB IN DT NN
NPB
PP
VP-A
ADVP
VP
S
le rapport complet sera d?pos? de ici le automne prochain
input: sera  VP-A
output:
VP
VP-Awill/MD
be/AUX
VP-A
x2
Figure 12: Crossing due to a modal.
5 Conclusion
The fundamental assumption underlying much recent
work in statistical machine translation (Yamada and
Knight, 2001; Eisner, 2003; Gildea, 2003) is that lo-
cal transformations (primarily child-node re-orderings)
of one-level parent-children substructures are an adequate
model for parallel corpora. Our empirical results suggest
that this may be too strong of an assumption. To explain
the data in two parallel corpora, one English-French, and
one English-Chinese, we are often forced to learn rules
involving much larger tree fragments. The theory, algo-
rithms, and transformation rules we learn automatically
from data have several interesting aspects.
1. Our rules provide a good, realistic indicator of the
complexities inherent in translation. We believe that
these rules can inspire subsequent developments of
generative statistical models that are better at ex-
plaining parallel data than current ones.
2. Our rules put at the fingertips of linguists a very
rich source of information. They encode translation
transformations that are both syntactically and lex-
ically motivated (some of our rules are purely syn-
tactic; others are lexically grounded). A simple sort
on the counts of our rules makes explicit the trans-
formations that occur most often. A comparison of
the number of rules extracted from parallel corpora
specific to multiple language pairs provide a quanti-
tative estimator of the syntactic ?closeness? between
various language pairs.
3. The theory we proposed in this paper is independent
of the method that one uses to compute the word-
level alignments in a parallel corpus.
4. The theory and rule-extraction algorithm are also
well-suited to deal with the errors introduced by
the word-level alignment and parsing programs one
uses. Our theory makes no a priori assumptions
about the transformations that one is permitted to
learn. If a parser, for example, makes a systematic
error, we expect to learn a rule that can neverthe-
less be systematically used to produce correct trans-
lations.
In this paper, we focused on providing a well-founded
mathematical theory and efficient, linear algorithms
for learning syntactically motivated transformation rules
from parallel corpora. One can easily imagine a range
of techniques for defining probability distributions over
the rules that we learn. We suspect that such probabilis-
tic rules could be also used in conjunction with statistical
decoders, to increase the accuracy of statistical machine
translation systems.
Acknowledgements
This work was supported by DARPA contract N66001-
00-1-9814 and MURI grant N00014-00-1-0617.
References
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proc. MT Summit IX.
M. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. of the 41st Meeting
of the Association for Computational Linguistics.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41th Annual Confer-
ence of the Association for Computational Linguistics.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT/NAACL.
F. Och and H. Ney. 2000. Improved statistical alignment
models. Proc. of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics.
F. Och and H Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 240?247,
New York, June 2006. c?2006 Association for Computational Linguistics
Relabeling Syntax Trees to Improve Syntax-Based Machine Translation
Quality
Bryant Huang
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
bhuang@languageweaver.com
Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292
knight@isi.edu
Abstract
We identify problems with the Penn Tree-
bank that render it imperfect for syntax-
based machine translation and propose
methods of relabeling the syntax trees to
improve translation quality. We develop a
system incorporating a handful of relabel-
ing strategies that yields a statistically sig-
nificant improvement of 2.3 BLEU points
over a baseline syntax-based system.
1 Introduction
Recent work in statistical machine translation (MT)
has sought to overcome the limitations of phrase-
based models (Marcu and Wong, 2002; Koehn et
al., 2003; Och and Ney, 2004) by making use
of syntactic information. Syntax-based MT of-
fers the potential advantages of enforcing syntax-
motivated constraints in translation and capturing
long-distance/non-contiguous dependencies. Some
approaches have used syntax at the core (Wu, 1997;
Alshawi et al, 2000; Yamada and Knight, 2001;
Gildea, 2003; Eisner, 2003; Hearne and Way, 2003;
Melamed, 2004) while others have integrated syn-
tax into existing phrase-based frameworks (Xia and
McCord, 2004; Chiang, 2005; Collins et al, 2005;
Quirk et al, 2005).
In this work, we employ a syntax-based model
that applies a series of tree/string (xRS) rules (Gal-
ley et al, 2004; Graehl and Knight, 2004) to a source
language string to produce a target language phrase
structure tree. Figure 1 exemplifies the translation
process, which is called a derivation, from Chinese
into English. The source string to translate (       
   	 


 .) is shown at the top left. Rule 1?
replaces the Chinese word    (shaded) with the
English NP-C police. Rule 2? then builds a VP over
the    NP-C   

 
 sequence. Next,        is translated
as the NP-C the gunman by rule 3?. Finally, rule 4?
combines the sequence of NP-C VP . into an S, denot-
ing a complete tree. The yield of this tree gives the
target translation: the gunman was killed by police .
The Penn English Treebank (PTB) (Marcus et al,
1993) is our source of syntactic information, largely
due to the availability of reliable parsers. It is not
clear, however, whether this resource is suitable, as
is, for the task of MT. In this paper, we argue that the
overly-general tagset of the PTB is problematic for
MT because it fails to capture important grammati-
cal distinctions that are critical in translation. As a
solution, we propose methods of relabeling the syn-
tax trees that effectively improve translation quality.
Consider the derivation in Figure 2. The output
translation has two salient errors: determiner/noun
number disagreement (*this Turkish positions) and
auxiliary/verb tense disagreement (*has demon-
strate). The first problem arises because the DT
tag, which does not distinguish between singular and
plural determiners, allows singular this to be used
with plural NNS positions. In the second problem,
the VP-C tag fails to communicate that it is headed by
the base verb (VB) demonstrate, which should pre-
vent it from being used with the auxiliary VBZ has.
Information-poor tags like DT and VP-C can be rela-
beled to encourage more fluent translations, which
is the thrust of this paper.
240
Figure 1: A derivation from a Chinese sentence to
an English tree.
Section 2 describes our data and experimental
procedure. Section 3 explores different relabeling
approaches and their impact on translation qual-
ity. Section 4 reports a substantial improvement in
BLEU achieved by combining the most effective re-
labeling methods. Section 5 concludes.
2 Experimental Framework
Our training data consists of 164M+167M words of
parallel Chinese/English text. The English half was
parsed with a reimplementation of Collins? Model
2 (Collins, 1999) and the two halves were word-
aligned using GIZA++ (Och and Ney, 2000). These
three components ? Chinese strings, English parse
trees, and their word alignments ? were inputs
to our experimental procedure, which involved five
steps: (1) tree relabeling, (2) rule extraction, (3) de-
coding, (4) n-best reranking, (5) evaluation.
This paper focuses on step 1, in which the orig-
inal English parse trees are transformed by one or
more relabeling strategies. Step 2 involves extract-
ing minimal xRS rules (Galley et al, 2004) from
the set of string/tree/alignments triplets. These rules
are then used in a CKY-type parser-decoder to trans-
late the 878-sentence 2002 NIST MT evaluation test
set (step 3). In step 4, the output 2,500-sentence n-
best list is reranked using an n-gram language model
trained on 800M words of English news text. In
the final step, we score our translations with 4-gram
BLEU (Papineni et al, 2002).
Separately for each relabeling method, we ran
these five steps and compared the resulting BLEU
score with that of a baseline system with no re-
labeling. To determine if a BLEU score increase
or decrease is meaningful, we calculate statistical
significance at 95% using paired bootstrap resam-
pling (Koehn, 2004; Zhang et al, 2004) on 1,000
samples.
Figure 3 shows the results from each relabel-
ing experiment. The second column indicates the
change in the number of unique rules from the base-
line number of 16.7M rules. The third column gives
the BLEU score along with an indication whether it
is a statistically significant increase (s), a statisti-
cally significant decrease (t), or neither (?) over
the baseline BLEU score.
241
Figure 2: A bad translation fixable by relabeling.
242
Relabeling Variant ? # Rules BLEU ?
BASELINE ? 20.06 ?
LEX_PREP 1 +301.2K 20.2 s
2 +254.8K 20.36 s
3 +188.3K 20.14 s
LEX_DT 1 +36.1K 20.15 s
2 +29.6K 20.18 s
LEX_AUX 1 +5.1K 20.09 s
2 +8.0K 20.09 ?
3 +1.6K 20.11 s
4 +13.8K 20.07 ?
LEX_CC +3.3K 20.03 t
LEX_% +0.3K 20.14 s
TAG_VP +123.6K 20.28 s
SISTERHOOD 1 +1.1M 21.33 s
2 +935.5K 20.91 s
3 +433.1K 20.36 s
4 +407.0K 20.59 s
PARENT 1 +1.1M 19.77 t
2 +9.0K 20.01 t
3 +2.9M 15.63 t
COMP_IN +17.4K 20.36 s
REM_NPB ?3.5K 19.93 t
REM_-C ?143.4K 19.3 t
REM_SG ?9.4K 20.01 t
Figure 3: For each relabeling method and variant,
the impact on ruleset size and BLEU score over the
baseline.
3 Relabeling
The small tagset of the PTB has the advantage of
being simple to annotate and to parse. On the other
hand, this can lead to tags that are overly generic.
Klein and Manning (2003) discuss this as a prob-
lem in parsing and demonstrate that annotating ad-
ditional information onto the PTB tags leads to im-
proved parsing performance. We similarly propose
methods of relabeling PTB trees that notably im-
prove MT quality. In the next two subsections, we
explore relabeling strategies that fall under two cate-
gories introduced by Klein and Manning ? internal
annotation and external annotation.
3.1 Internal Annotation
Internal annotation reveals information about a
node and its descendants to its surrounding nodes
(ancestors, sisters, and other relatives) that is other-
wise hidden. This is paramount in MT because the
contents of a node must be understood before the
node can be reliably translated and positioned in a
sentence. Here we discuss two such strategies: lexi-
Figure 4: Rules before and after lexicalization.
calization and tag annotation.
3.1.1 Lexicalization
Many state-of-the-art statistical parsers incor-
porate lexicalization to effectively capture word-
specific behavior, which has proved helpful in our
system as well. We generalize lexicalization to al-
low a lexical item (terminal word) to be annotated
onto any ancestor label, not only its parent.
Let us revisit the determiner/noun number dis-
agreement problem in Figure 2 (*this Turkish po-
sitions). If we lexicalize all DTs in the parse trees,
the problematic DT is relabeled more specifically as
DT_this, as seen in rule 2?? in Figure 4. This also
produces rules like 4??, where both the determiner
and the noun are plural (notice the DT_these), and
4???, where both are singular. With such a ruleset, 2??
could only combine with 4???, not 4??, enforcing the
grammatical output this Turkish position.
We explored five lexicalization strategies, each
targeting a different grammatical category. A com-
mon translation mistake was the improper choice of
prepositions, e.g., responsibility to attacks. Lexical-
izing prepositions proved to be the most effective
lexicalization method (LEX_PREP). We annotated
a preposition onto both its parent (IN or TO) and its
grandparent (PP) since the generic PP tag was often
at fault. We tried lexicalizing all prepositions (vari-
ant 1), the top 15 most common prepositions (variant
2), and the top 5 most common (variant 3). All gave
statistically significant BLEU improvements, espe-
cially variant 2.
The second strategy was DT lexicalization
243
(LEX_DT), which we encountered previously in Fig-
ure 4. This addresses two features of Chinese that
are problematic in translation to English: the infre-
quent use of articles and the lack of overt number in-
dicators on nouns. We lexicalized these determiners:
the, a, an, this, that, these, or those, and grouped to-
gether those with similar grammatical distributions
(a/an, this/that, and these/those). Variant 1 included
all the determiners mentioned above and variant 2
was restricted to the and a/an to focus only on arti-
cles. The second slightly improved on the first.
The third type was auxiliary lexicalization
(LEX_AUX), in which all forms of the verb be
are annotated with _be, and similarly with do and
have. The PTB purposely eliminated such distinc-
tions; here we seek to recover them. However,
auxiliaries and verbs function very differently and
thus cannot be treated identically. Klein and Man-
ning (2003) make a similar proposal but omit do.
Variants 1, 2, and 3, lexicalize have, be, and do, re-
spectively. The third variant slightly outperformed
the other variants, including variant 4, which com-
bines all three.
The last two methods are drawn directly from
Klein and Manning (2003). In CC lexicalization
(LEX_CC), both but and & are lexicalized since
these two conjunctions are distributed very differ-
ently compared to other conjunctions. Though help-
ful in parsing, it proved detrimental in our system.
In % lexicalization (LEX_%), the percent sign (%) is
given its own PCT tag rather than its typical NN tag,
which gave a statistically significant BLEU increase.
3.1.2 Tag Annotation
In addition to propagating up a terminal word, we
can also propagate up a nonterminal, which we call
tag annotation. This partitions a grammatical cat-
egory into more specific subcategories, but not as
fine-grained as lexicalization. For example, a VP
headed by a VBG can be tag-annotated as VP_VBG
to represent a progressive verb phrase.
Let us once again return to Figure 2 to address
the auxiliary/verb tense disagreement error (*has
demonstrate). The auxiliary has expects a VP-C, per-
mitting the bare verb phrase demonstrate to be incor-
rectly used. However, if we tag-annotate all VP-Cs,
rule 6? would be relabeled as VP-C_VB in rule 6??
and rule 7? as 7?? in Figure 5. Rule 6?? can no longer
Figure 5: Rules before and after tag annotation.
join with 7??, while the variant rule 6??? can, which
produces the grammatical result has demonstrated.
We noticed many wrong verb tense choices, e.g.,
gerunds and participles used as main sentence verbs.
We resolved this by tag-annotating every VP and VP-
C with its head verb (TAG_VP). Note that we group
VBZ and VBP together since they have very similar
grammatical distributions and differ only by number.
This strategy gave a healthy BLEU improvement.
3.2 External Annotation
In addition to passing information from inside a
node to the outside, we can pass information from
the external environment into the node through ex-
ternal annotation. This allows us to make transla-
tion decisions based on the context in which a word
or phrase is found. In this subsection, we look at
three such methods: sisterhood annotation, parent
annotation, and complement annotation.
3.2.1 Sisterhood Annotation
The single most effective relabeling scheme we
tried was sisterhood annotation. We annotate each
nonterminal with #L if it has any sisters to the left,
#R if any to the right, #LR if on both sides, and noth-
ing if it has no sisters. This distinguishes between
words that tend to fall on the left or right border of
a constituent (often head words, like NN#L in an NP
or IN#R in a PP), in the middle of a constituent (of-
ten modifiers, like JJ#LR in an NP), or by themselves
244
Figure 6: A bad translation fixable by sisterhood or
parent annotation.
(often particles and pronouns, like RP and PRP). In
our outputs, we frequently find words used in posi-
tions where they should be disallowed or disfavored.
Figure 6 presents a derivation that leads to the
ungrammatical output *deeply love she. The sub-
ject pronoun she is incorrectly preferred over the ob-
ject form her because the most popular NP-C trans-
lation for       is she. We can sidestep this mistake
through sisterhood-annotation, which yields the re-
labeled rules 3?? and 4?? in Figure 7. Rule 4?? ex-
pects an NP-C on the right border of the constituent
(NP-C#L). Since she never occurs in this position in
the PTB, it should never be sisterhood-annotated as
an NP-C#L. It does occur with sisters to the right,
which gives the NP-C#R rule 3??. The object NP-C
her, on the other hand, is frequently rightmost in a
constituent, which is reflected in the NP-C#L rule 3???.
Using this rule with rule 4?? gives the desired result
deeply love her.
We experimented with four sisterhood annotation
(SISTERHOOD) variants of decreasing complexity.
The first was described above, which includes right-
most (#L), leftmost (#R), middle (#LR), and alone (no
annotation). Variant 2 omitted #LR, variant 3 kept
only #LR, and variant 4 only annotated nodes with-
out sisters. Variants 1 and 2 produced the largest
gains from relabeling: 1.27 and 0.85 BLEU points,
respectively.
Figure 7: Rules before and after sisterhood annota-
tion.
Figure 8: Rules before and after parent annotation.
3.2.2 Parent Annotation
Another common relabeling method in parsing is
parent annotation (Johnson, 1998), in which a node
is annotated with its parent?s label. Typically, this
is done only to nonterminals, but Klein and Man-
ning (2003) found that annotating preterminals as
well was highly effective. It seemed likely that such
contextual information could also benefit MT.
Let us tackle the bad output from Figure 6 with
parent annotation. In Figure 8, rule 4? is relabeled as
rule 4?? and expects an NP-C?VP, i.e., an NP-C with a
VP parent. In the PTB, we observe that the NP-C she
never has a VP parent, while her does. In fact, the
most popular parent for the NP-C her is VP, while the
most popular parent for she is S. Rule 3? is relabeled
as the NP-C?S rule 3?? and her is expressed as the NP-
C?VP rule 3???. Only rule 3??? can partner with rule 4??,
which produces the correct output deeply love her.
We tested three variants of parent annota-
tion (PARENT): (1) all nonterminals are parent-
annotated, (2) only S nodes are parent-annotated,
and (3) all nonterminals are parent- and grandparent-
annotated (the annotation of a node?s parent?s par-
ent). The first and third variants yielded the largest
ruleset sizes of all relabeling methods. The second
variant was restricted only to S to capture the dif-
ference between top-level clauses (S?TOP) and em-
245
bedded clauses (like S?S-C). Unfortunately, all three
variants turned out to be harmful in terms of BLEU.
3.2.3 Complement Annotation
In addition to a node?s parent, we can also anno-
tate a node?s complement. This captures the fact that
words have a preference of taking certain comple-
ments over others. For instance, 96% of cases where
the IN of takes one complement in the PTB, it takes
NP-C. On the other hand, although never takes NP-C
but takes S-C 99% of the time.
Consider the derivation in Figure 9 that results in
the bad output *postponed out May 6. The IN out
is incorrectly allowed despite the fact that it almost
never takes an NP-C complement (0.6% of cases in
the PTB). A way to restrict this is to annotate the
IN?s complement. Complement-annotated versions
of rules 2? and 3? are given in Figure 10. Rule
2? is relabeled as the IN/PP-C rule 2?? since PP-C
is the most common complement for out (99% of
the time). Since rule 3??? expects an IN/NP-C, rule 2??
is disqualified. The preposition from (rule 2???), on
the other hand, frequently takes NP-C as complement
(82% of the time). Combining rule 2??? with rule 3??
ensures the correct output postponed from May 6.
Complement-annotating all IN tags with their
complement if they had one and only one comple-
ment (COMP_IN) gave a significant BLEU improve-
ment with only a modest increase in ruleset size.
3.3 Removal of Parser Annotations
Many parsers, though trained on the PTB, do not
preserve the original tagset. They may omit func-
tion tags (like -TMP), indices, and null/gap elements
or add annotations to increase parsing accuracy and
provide useful grammatical information. It is not
obvious whether these modifications are helpful for
MT, so we explore the effects of removing them.
The statistical parser we used makes three re-
labelings: (1) base NPs are relabeled as NPB, (2)
argument nonterminals are suffixed with -C, and
(3) subjectless sentences are relabeled from S to
SG. We tried removing each annotation individually
(REM_NPB, REM_-C, and REM_SG), but doing so
significantly dropped the BLEU score. This leads us
to conclude these parser additions are helpful in MT.
Figure 9: A bad translation fixable by complement
annotation.
Figure 10: Rules before and after complement anno-
tation.
4 Evaluation
To maximize the benefit of relabeling, we incorpo-
rated five of the most promising relabeling strategies
into one additive system: LEX_%, LEX_DT variant
246
? # Rules BLEU
Relabeling Variant Ind. Cum. Ind. Cum.
BASELINE ? ? 20.06 20.06
LEX_% +0.3K +0.3K 20.14 20.14
LEX_DT 2 +29.6K +29.9K 20.18 20.3
TAG_VP +123.6K +153.5K 20.28 20.43
LEX_PREP 2 +254.8K +459.0K 20.36 21.25
SISTERHOOD 1 +1.1M +1.5M 21.33 22.38
Figure 11: Relabelings in the additive system and
their individual/cumulative effects over the baseline.
2, TAG_VP, LEX_PREP variant 2, and SISTERHOOD
variant 1. These relabelings contributed to a 2.3 ab-
solute (11.6% relative) BLEU point increase over
the baseline, with a score of 22.38. Figure 11 lists
these relabelings in the order they were added.
5 Conclusion
We have demonstrated that relabeling syntax trees
for use in syntax-based machine translation can sig-
nificantly boost translation performance. It is na?ve
to assume that linguistic resources can be immedi-
ately useful out of the box, in our case, the Penn
Treebank for MT. Rather, we targeted features of the
PTB tagset that impair translatability and proposed
relabeling strategies to overcome these weaknesses.
Many of our ideas effectively raised the BLEU score
over a baseline system without relabeling. Finally,
we demonstrated through an additive system that re-
labelings can be combined together to achieve an
even greater improvement in translation quality.
Acknowledgments
This research was supported in part by NSF grant
IIS-0428020. We would like to thank Greg Lang-
mead, Daniel Marcu, and Wei Wang for helpful
comments. This paper describes work conducted
while the first author was at the University of South-
ern California/Information Sciences Institute.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL-05.
Michael Collins, Philipp Koehn, and Ivona Kuc?erov?. 2005.
Clause restructuring for statistical machine translation. In
Proceedings of ACL-05, pages 531?540.
Michael Collins. 1999. Head-driven statistical models for nat-
ural language parsing. Ph.D. thesis, University of Pennsyl-
vania.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL-03 (Com-
panion Volume), pages 205?208.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proceed-
ings of HLT/NAACL-04, pages 273?280.
Dan Gildea. 2003. Loosely tree-based alignment for machine
translation. In Proceedings of ACL-03.
Jonathan Graehl and Kevin Knight. 2004. Training tree trans-
ducers. In Proceedings of HLT/NAACL-04, pages 105?112.
Mary Hearne and Andy Way. 2003. Seeing the wood for
the trees: Data-Oriented Translation. In Proceedings of MT
Summit IX.
Mark Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4):613?632.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of ACL-03, pages 423?430.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL-03.
Philipp Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of EMNLP-04.
Daniel Marcu and William Wong. 2002. A phrase-based, joint
probability model for statistical machine translation. In Pro-
ceedings of EMNLP-02.
Mitchell Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 2004. Statistical machine translation by pars-
ing. In Proceedings of ACL-04, pages 653?660.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL-00.
Franz Josef Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of ACL-02.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: syntactically informed phrasal
SMT. In Proceedings of ACL-05, pages 271?279.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a statistical
MT system with automatically learned rewrite patterns. In
Proceedings of COLING-04.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of ACL-01.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: how much improvement do we
need to have a better system? In Proceedings of LREC-04.
247
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 256?263,
New York, June 2006. c?2006 Association for Computational Linguistics
Synchronous Binarization for Machine Translation
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Systems based on synchronous grammars
and tree transducers promise to improve
the quality of statistical machine transla-
tion output, but are often very computa-
tionally intensive. The complexity is ex-
ponential in the size of individual gram-
mar rules due to arbitrary re-orderings be-
tween the two languages, and rules ex-
tracted from parallel corpora can be quite
large. We devise a linear-time algorithm
for factoring syntactic re-orderings by bi-
narizing synchronous rules when possible
and show that the resulting rule set signif-
icantly improves the speed and accuracy
of a state-of-the-art syntax-based machine
translation system.
1 Introduction
Several recent syntax-based models for machine
translation (Chiang, 2005; Galley et al, 2004) can
be seen as instances of the general framework of
synchronous grammars and tree transducers. In this
framework, both alignment (synchronous parsing)
and decoding can be thought of as parsing problems,
whose complexity is in general exponential in the
number of nonterminals on the right hand side of a
grammar rule. To alleviate this problem, we investi-
gate bilingual binarization to factor the synchronous
grammar to a smaller branching factor, although it is
not guaranteed to be successful for any synchronous
rule with arbitrary permutation. In particular:
? We develop a technique called synchronous bi-
narization and devise a fast binarization algo-
rithm such that the resulting rule set alows ef-
ficient algorithms for both synchronous parsing
and decoding with integrated n-gram language
models.
? We examine the effect of this binarization
method on end-to-end machine translation
quality, compared to a more typical baseline
method.
? We examine cases of non-binarizable rules in a
large, empirically-derived rule set, and we in-
vestigate the effect on translation quality when
excluding such rules.
Melamed (2003) discusses binarization of multi-
text grammars on a theoretical level, showing the
importance and difficulty of binarization for efficient
synchronous parsing. One way around this diffi-
culty is to stipulate that all rules must be binary
from the outset, as in inversion-transduction gram-
mar (ITG) (Wu, 1997) and the binary synchronous
context-free grammar (SCFG) employed by the Hi-
ero system (Chiang, 2005) to model the hierarchical
phrases. In contrast, the rule extraction method of
Galley et al (2004) aims to incorporate more syn-
tactic information by providing parse trees for the
target language and extracting tree transducer rules
that apply to the parses. This approach results in
rules with many nonterminals, making good bina-
rization techniques critical.
Suppose we have the following SCFG, where su-
perscripts indicate reorderings (formal definitions of
256
S
NP
Baoweier
PP
yu
Shalong
VP
juxing le
huitan
S
NP
Powell
VP
held
a meeting
PP
with
Sharon
Figure 1: A pair of synchronous parse trees in the
SCFG (1). The dashed curves indicate pairs of syn-
chronous nonterminals (and sub trees).
SCFGs can be found in Section 2):
(1)
S? NP(1) VP(2) PP(3), NP(1) PP(3) VP(2)
NP? Powell, Baoweier
VP? held a meeting, juxing le huitan
PP? with Sharon, yu Shalong
Decoding can be cast as a (monolingual) parsing
problem since we only need to parse the source-
language side of the SCFG, as if we were construct-
ing a CFG projected on Chinese out of the SCFG.
The only extra work we need to do for decoding
is to build corresponding target-language (English)
subtrees in parallel. In other words, we build syn-
chronous trees when parsing the source-language in-
put, as shown in Figure 1.
To efficiently decode with CKY, we need to bi-
narize the projected CFG grammar.1 Rules can be
binarized in different ways. For example, we could
binarize the first rule left to right or right to left:
S? VNP-PP VP
VNP-PP? NP PP or
S? NP VPP-VP
VPP-VP ? PP VP
We call those intermediate symbols (e.g. VPP-VP) vir-
tual nonterminals and corresponding rules virtual
rules, whose probabilities are all set to 1.
These two binarizations are no different in the
translation-model-only decoding described above,
just as in monolingual parsing. However, in the
source-channel approach to machine translation, we
need to combine probabilities from the translation
model (an SCFG) with the language model (an n-
gram), which has been shown to be very impor-
tant for translation quality (Chiang, 2005). To do
bigram-integrated decoding, we need to augment
each chart item (X, i, j) with two target-language
1Other parsing strategies like the Earley algorithm use an
internal binary representation (e.g. dotted-rules) of the original
grammar to ensure cubic time complexity.
boundary words u and v to produce a bigram-item
like
( u ??? vX
i j
)
, following the dynamic program-
ming algorithm of Wu (1996).
Now the two binarizations have very different ef-
fects. In the first case, we first combine NP with PP:
( Powell ??? PowellNP
1 2
)
: p
( with ??? SharonPP
2 4
)
: q
( Powell ??? Powell ??? with ??? Sharon
VNP-PP
1 4
)
: pq
where p and q are the scores of antecedent items.
This situation is unpleasant because in the target-
language NP and PP are not contiguous so we can-
not apply language model scoring when we build the
VNP-PP item. Instead, we have to maintain all fourboundary words (rather than two) and postpone the
language model scoring till the next step where VNP-PP
is combined with ( held ??? meetingVP
2 4
) to form an S item.
We call this binarization method monolingual bina-
rization since it works only on the source-language
projection of the rule without respecting the con-
straints from the other side.
This scheme generalizes to the case where we
have n nonterminals in a SCFG rule, and the decoder
conservatively assumes nothing can be done on lan-
guage model scoring (because target-language spans
are non-contiguous in general) until the real nonter-
minal has been recognized. In other words, target-
language boundary words from each child nonter-
minal of the rule will be cached in all virtual non-
terminals derived from this rule. In the case of
m-gram integrated decoding, we have to maintain
2(m ? 1) boundary words for each child nontermi-
nal, which leads to a prohibitive overall complex-
ity of O(|w|3+2n(m?1)), which is exponential in rule
size (Huang et al, 2005). Aggressive pruning must
be used to make it tractable in practice, which in
general introduces many search errors and adversely
affects translation quality.
In the second case, however:
( with ??? SharonPP
2 4
)
: r
( held ??? meetingVP
4 7
)
: s
( held ??? Sharon
VPP-VP
2 7
)
: rs ? Pr(with | meeting)
Here since PP and VP are contiguous (but
swapped) in the target-language, we can include the
257
NP
NP
PP
VP
VP
PP
target (English)
source (Chinese)
VPP-VP
NP
PP
VP
Chinese indices
English
boundary
w
o
rds 1 2 4 7Powell
Powellheld
meetingwith
Sharon
VPP-VP
Figure 2: The alignment pattern (left) and alignment
matrix (right) of the synchronous production.
language model score by adding Pr(with | meeting),
and the resulting item again has two boundary
words. Later we add Pr(held | Powell) when the
resulting item is combined with ( Powell ??? PowellNP
1 2
) to
form an S item. As illustrated in Figure 2, VPP-VP hascontiguous spans on both source and target sides, so
that we can generate a binary-branching SCFG:
(2) S? NP(1) VPP-VP(2), NP(1) VPP-VP(2)VPP-VP ? VP(1) PP(2), PP(2) VP(1)
In this case m-gram integrated decoding can be
done in O(|w|3+4(m?1)) time which is much lower-
order polynomial and no longer depends on rule size
(Wu, 1996), allowing the search to be much faster
and more accurate facing pruning, as is evidenced in
the Hiero system of Chiang (2005) where he restricts
the hierarchical phrases to be a binary SCFG. The
benefit of binary grammars also lies in synchronous
parsing (alignment). Wu (1997) shows that parsing
a binary SCFG is in O(|w|6) while parsing SCFG is
NP-hard in general (Satta and Peserico, 2005).
The same reasoning applies to tree transducer
rules. Suppose we have the following tree-to-string
rules, following Galley et al (2004):
(3)
S(x0:NP, VP(x2:VP, x1:PP))? x0 x1 x2NP(NNP(Powell))? Baoweier
VP(VBD(held), NP(DT(a) NPS(meeting)))
? juxing le huitan
PP(TO(with), NP(NNP(Sharon)))? yu Shalong
where the reorderings of nonterminals are denoted
by variables xi.Notice that the first rule has a multi-level left-
hand side subtree. This system can model non-
isomorphic transformations on English parse trees
to ?fit? another language, for example, learning that
the (S (V O)) structure in English should be trans-
formed into a (V S O) structure in Arabic, by look-
ing at two-level tree fragments (Knight and Graehl,
2005). From a synchronous rewriting point of view,
this is more akin to synchronous tree substitution
grammar (STSG) (Eisner, 2003). This larger locality
is linguistically motivated and leads to a better pa-
rameter estimation. By imagining the left-hand-side
trees as special nonterminals, we can virtually cre-
ate an SCFG with the same generative capacity. The
technical details will be explained in Section 3.2.
In general, if we are given an arbitrary syn-
chronous rule with many nonterminals, what are the
good decompositions that lead to a binary grammar?
Figure 2 suggests that a binarization is good if ev-
ery virtual nonterminal has contiguous spans on both
sides. We formalize this idea in the next section.
2 Synchronous Binarization
A synchronous CFG (SCFG) is a context-free
rewriting system for generating string pairs. Each
rule (synchronous production) rewrites a nontermi-
nal in two dimensions subject to the constraint that
the sequence of nonterminal children on one side is
a permutation of the nonterminal sequence on the
other side. Each co-indexed child nonterminal pair
will be further rewritten as a unit.2 We define the
language L(G) produced by an SCFG G as the pairs
of terminal strings produced by rewriting exhaus-
tively from the start symbol.
As shown in Section 3.2, terminals do not play
an important role in binarization. So we now write
rules in the following notation:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
where each Xi is a variable which ranges over non-terminals in the grammar and pi is the permutation
of the rule. We also define an SCFG rule as n-ary
if its permutation is of n and call an SCFG n-ary if
its longest rule is n-ary. Our goal is to produce an
equivalent binary SCFG for an input n-ary SCFG.
2In making one nonterminal play dual roles, we follow the
definitions in (Aho and Ullman, 1972; Chiang, 2005), origi-
nally known as Syntax Directed Translation Schema (SDTS).
An alternative definition by Satta and Peserico (2005) allows
co-indexed nonterminals taking different symbols in two di-
mensions. Formally speaking, we can construct an equivalent
SDTS by creating a cross-product of nonterminals from two
sides. See (Satta and Peserico, 2005, Sec. 4) for other details.
258
(2,3,5,4)
(2,3)
2 3
(5,4)
5 4
(2,3,5,4)
2 (3,5,4)
3 (5,4)
5 4
(a) (b) (c)
Figure 3: (a) and (b): two binarization patterns
for (2, 3, 5, 4). (c): alignment matrix for the non-
binarizable permuted sequence (2, 4, 1, 3)
However, not every SCFG can be binarized. In
fact, the binarizability of an n-ary rule is determined
by the structure of its permutation, which can some-
times be resistant to factorization (Aho and Ullman,
1972). So we now start to rigorously define the bi-
narizability of permutations.
2.1 Binarizable Permutations
A permuted sequence is a permutation of consec-
utive integers. For example, (3, 5, 4) is a permuted
sequence while (2, 5) is not. As special cases, single
numbers are permuted sequences as well.
A sequence a is said to be binarizable if it is a
permuted sequence and either
1. a is a singleton, i.e. a = (a), or
2. a can be split into two sub sequences, i.e.
a = (b; c), where b and c are both binarizable
permuted sequences. We call such a division
(b; c) a binarizable split of a.
This is a recursive definition. Each binarizable
permuted sequence has at least one hierarchical bi-
narization pattern. For instance, the permuted se-
quence (2, 3, 5, 4) is binarizable (with two possible
binarization patterns) while (2, 4, 1, 3) is not (see
Figure 3).
2.2 Binarizable SCFG
An SCFG is said to be binarizable if the permu-
tation of each synchronous production is binariz-
able. We denote the class of binarizable SCFGs as
bSCFG. This set represents an important subclass
of SCFG that is easy to handle (parsable in O(|w|6))
and covers many interesting longer-than-two rules.3
3Although we factor the SCFG rules individually and de-
fine bSCFG accordingly, there are some grammars (the dashed
SCFG bSCFG SCFG-2
O(|w|6) parsable
Figure 4: Subclasses of SCFG. The thick arrow de-
notes the direction of synchronous binarization. For
clarity reasons, binary SCFG is coded as SCFG-2.
Theorem 1. For each grammar G in bSCFG, there
exists a binary SCFG G?, such that L(G?) = L(G).
Proof. Once we decompose the permutation of n
in the original rule into binary permutations, all
that remains is to decorate the skeleton binary parse
with nonterminal symbols and attach terminals to
the skeleton appropriately. We explain the technical
details in the next section.
3 Binarization Algorithms
We have reduced the problem of binarizing an SCFG
rule into the problem of binarizing its permutation.
This problem can be cast as an instance of syn-
chronous ITG parsing (Wu, 1997). Here the parallel
string pair that we are parsing is the integer sequence
(1...n) and its permutation (pi(1)...pi(n)). The goal
of the ITG parsing is to find a synchronous tree that
agrees with the alignment indicated by the permu-
tation. In fact, as demonstrated previously, some
permutations may have more than one binarization
patterns among which we only need one. Wu (1997,
Sec. 7) introduces a non-ambiguous ITG that prefers
left-heavy binary trees so that for each permutation
there is a unique synchronous derivation (binariza-
tion pattern).
However, this problem has more efficient solu-
tions. Shapiro and Stephens (1991, p. 277) infor-
mally present an iterative procedure where in each
pass it scans the permuted sequence from left to right
and combines two adjacent sub sequences whenever
possible. This procedure produces a left-heavy bi-
narization tree consistent with the unambiguous ITG
and runs in O(n2) time since we need n passes in the
worst case. We modify this procedure and improve
circle in Figure 4), which can be binarized only by analyzing
interactions between rules. Below is a simple example:
S? X(1) X(2) X(3) X(4), X(2) X(4) X(1) X(3)
X? a , a
259
iteration stack input action
1 5 3 4 2
1 5 3 4 2 shift
1 1 5 3 4 2 shift
2 1 5 3 4 2 shift
3 1 5 3 4 2 shift
1 5 3-4 2 reduce [3, 4]
1 3-5 2 reduce ?5, [3, 4]?
4 1 3-5 2 shift
1 2-5 reduce ?2, ?5, [3, 4]??
1-5 reduce [1, ?2, ?5, [3, 4]??]
Figure 5: Example of Algorithm 1 on the input
(1, 5, 3, 4, 2). The rightmost column shows the
binarization-trees generated at each reduction step.
it into a linear-time shift-reduce algorithm that only
needs one pass through the sequence.
3.1 The linear-time skeleton algorithm
The (unique) binarization tree bi(a) for a binariz-
able permuted sequence a is recursively defined as
follows:
? if a = (a), then bi(a) = a;
? otherwise let a = (b; c) to be the rightmost
binarizable split of a. then
bi(a) =
{
[bi(b), bi(c)] b1 < c1
?bi(b), bi(c)? b1 > c1.
For example, the binarization tree for (2, 3, 5, 4)
is [[2, 3], ?5, 4?], which corresponds to the binariza-
tion pattern in Figure 3(a). We use [] and ?? for
straight and inverted combinations respectively, fol-
lowing the ITG notation (Wu, 1997). The rightmost
split ensures left-heavy binary trees.
The skeleton binarization algorithm is an instance
of the widely used left-to-right shift-reduce algo-
rithm. It maintains a stack for contiguous subse-
quences discovered so far, like 2-5, 1. In each it-
eration, it shifts the next number from the input and
repeatedly tries to reduce the top two elements on
the stack if they are consecutive. See Algorithm 1
for details and Figure 5 for an example.
Theorem 2. Algorithm 1 succeeds if and only if the
input permuted sequence a is binarizable, and in
case of success, the binarization pattern recovered
is the binarization tree of a.
Proof. ?: it is obvious that if the algorithm suc-
ceeds then a is binarizable using the binarization
pattern recovered.
?: by a complete induction on n, the length of a.
Base case: n = 1, trivial.
Assume it holds for all n? < n.
If a is binarizable, then let a = (b; c) be its right-
most binarizable split. By the induction hypothesis,
the algorithm succeeds on the partial input b, reduc-
ing it to the single element s[0] on the stack and re-
covering its binarization tree bi(b).
Let c = (c1; c2). If c1 is binarizable and trig-gers our binarizer to make a straight combination
of (b; c1), based on the property of permutations, itmust be true that (c1; c2) is a valid straight concate-nation. We claim that c2 must be binarizable in thissituation. So, (b, c1; c2) is a binarizable split to theright of the rightmost binarizable split (b; c), which
is a contradiction. A similar contradiction will arise
if b and c1 can make an inverted concatenation.
Therefore, the algorithm will scan through the
whole c as if from the empty stack. By the in-
duction hypothesis again, it will reduce c into s[1]
on the stack and recover its binarization tree bi(c).
Since b and c are combinable, the algorithm re-
duces s[0] and s[1] in the last step, forming the bi-
narization tree for a, which is either [bi(b), bi(c)] or
?bi(b), bi(c)?.
The running time of Algorithm 1 is linear in n, the
length of the input sequence. This is because there
are exactly n shifts and at most n?1 reductions, and
each shift or reduction takes O(1) time.
3.2 Binarizing tree-to-string transducers
Without loss of generality, we have discussed how
to binarize synchronous productions involving only
nonterminals through binarizing the corresponding
skeleton permutations. We still need to tackle a few
technical problems in the actual system.
First, we are dealing with tree-to-string trans-
ducer rules. We view each left-hand side subtree
as a monolithic nonterminal symbol and factor each
transducer rule into two SCFG rules: one from
the root nonterminal to the subtree, and the other
from the subtree to the leaves. In this way we can
uniquely reconstruct the tree-to-string derivation us-
ing the two-step SCFG derivation. For example,
260
Algorithm 1 The Linear-time Binarization Algorithm
1: function BINARIZABLE(a)
2: top? 0 . stack top pointer
3: PUSH(a1, a1) . initial shift4: for i? 2 to |a| do . for each remaining element
5: PUSH(ai, ai) . shift6: while top > 1 and CONSECUTIVE(s[top], s[top? 1]) do . keep reducing if possible
7: (p, q)? COMBINE(s[top], s[top? 1])
8: top? top? 2
9: PUSH(p, q)
10: return (top = 1) . if reduced to a single element then the input is binarizable, otherwise not
11: function CONSECUTIVE((a, b), (c, d))
12: return (b = c? 1) or (d = a? 1) . either straight or inverted
13: function COMBINE((a, b), (c, d))
14: return (min(a, c), max(b, d))
consider the following tree-to-string rule:
ADJP
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x2:NN
x1:PP
? x0 fuze x1 de x2
We create a specific nonterminal, say, T859, whichis a unique identifier for the left-hand side subtree
and generate the following two SCFG rules:
ADJP ? T859 (1), T859 (1)
T859 ? RB
(1) resp. for the NN(2) PP(3),
RB(1) fuze PP(3) de NN(2)
Second, besides synchronous nonterminals, ter-
minals in the two languages can also be present, as
in the above example. It turns out we can attach the
terminals to the skeleton parse for the synchronous
nonterminal strings quite freely as long as we can
uniquely reconstruct the original rule from its binary
parse tree. In order to do so we need to keep track of
sub-alignments including both aligned nonterminals
and neighboring terminals.
When binarizing the second rule above, we first
run the skeleton algorithm to binarize the under-
lying permutation (1, 3, 2) to its binarization tree
[1, ?3, 2?]. Then we do a post-order traversal to the
skeleton tree, combining Chinese terminals (one at
a time) at the leaf nodes and merging English termi-
nals greedily at internal nodes:
[1, ?3, 2?]
1 ?3, 2?
3 2
?
T859 [1,?3,2?]
V[RB, fuze]1
RB fuze
V?V[PP, de], resp. for the NN??3,2?
V[PP, de]3
PP de
NN2
A pre-order traversal of the decorated binarization
tree gives us the following binary SCFG rules:
T859 ? V1(1) V2(2), V1(1) V2(2)
V1 ? RB(1), RB(1) fuze
V2 ? resp. for the NN(1) V(2)3 , V(2)3 NN(1)V3 ? PP(1), PP(1) de
where the virtual nonterminals are:
V1: V[RB, fuze]V2: V?V[PP, de], resp. for the NN?V3: V[PP, de]
Analogous to the ?dotted rules? in Earley pars-
ing for monolingual CFGs, the names we create
for the virtual nonterminals reflect the underlying
sub-alignments, ensuring intermediate states can be
shared across different tree-to-string rules without
causing ambiguity.
The whole binarization algorithm still runs in time
linear in the number of symbols in the rule (includ-
ing both terminals and nonterminals).
4 Experiments
In this section, we answer two empirical questions.
261
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
 0  5  10  15  20  25  30  35  40
 0
 20
 40
 60
 80
 100
# 
of
 ru
le
s
pe
rc
en
ta
ge
 (%
)
length
Figure 6: The solid-line curve represents the distribution of all rules against permutation lengths. The
dashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set while the dotted-line
denotes that percentage among all permutations.
4.1 How many rules are binarizable?
It has been shown by Shapiro and Stephens (1991)
and Wu (1997, Sec. 4) that the percentage of binariz-
able cases over all permutations of length n quickly
approaches 0 as n grows (see Figure 6). However,
for machine translation, it is more meaningful to
compute the ratio of binarizable rules extracted from
real text. Our rule set is obtained by first doing word
alignment using GIZA++ on a Chinese-English par-
allel corpus containing 50 million words in English,
then parsing the English sentences using a variant
of Collins parser, and finally extracting rules using
the graph-theoretic algorithm of Galley et al (2004).
We did a ?spectrum analysis? on the resulting rule
set with 50,879,242 rules. Figure 6 shows how the
rules are distributed against their lengths (number
of nonterminals). We can see that the percentage
of non-binarizable rules in each bucket of the same
length does not exceed 25%. Overall, 99.7% of
the rules are binarizable. Even for the 0.3% non-
binarizable rules, human evaluations show that the
majority of them are due to alignment errors. It is
also interesting to know that 86.8% of the rules have
monotonic permutations, i.e. either taking identical
or totally inverted order.
4.2 Does synchronous binarizer help decoding?
We did experiments on our CKY-based decoder with
two binarization methods. It is the responsibility of
the binarizer to instruct the decoder how to compute
the language model scores from children nontermi-
nals in each rule. The baseline method is mono-
lingual left-to-right binarization. As shown in Sec-
tion 1, decoding complexity with this method is ex-
ponential in the size of the longest rule and since we
postpone all the language model scorings, pruning
in this case is also biased.
system bleu
monolingual binarization 36.25
synchronous binarization 38.44
alignment-template system 37.00
Table 1: Syntax-based systems vs. ATS
To move on to synchronous binarization, we first
did an experiment using the above baseline system
without the 0.3% non-binarizable rules and did not
observe any difference in BLEU scores. So we
safely move a step further, focusing on the binariz-
able rules only.
The decoder now works on the binary translation
rules supplied by an external synchronous binarizer.
As shown in Section 1, this results in a simplified de-
coder with a polynomial time complexity, allowing
less aggressive and more effective pruning based on
both translation model and language model scores.
We compare the two binarization schemes in
terms of translation quality with various pruning
thresholds. The rule set is that of the previous sec-
tion. The test set has 116 Chinese sentences of no
longer than 15 words. Both systems use trigram as
the integrated language model. Figure 7 demon-
strates that decoding accuracy is significantly im-
proved after synchronous binarization. The number
of edges proposed during decoding is used as a mea-
sure of the size of search space, or time efficiency.
Our system is consistently faster and more accurate
than the baseline system.
We also compare the top result of our syn-
chronous binarization system with the state-of-the-
art alignment-template approach (ATS) (Och and
Ney, 2004). The results are shown in Table 1. Our
system has a promising improvement over the ATS
262
 33.5
 34.5
 35.5
 36.5
 37.5
 38.5
 3e+09  4e+09  5e+09  6e+09  7e+09
bl
eu
 s
co
re
s
# of edges proposed during decoding
synchronous binarization
monolingual binarization
Figure 7: Comparing the two binarization methods
in terms of translation quality against search effort.
system which is trained on a larger data-set but tuned
independently.
5 Conclusion
Modeling reorderings between languages has been a
major challenge for machine translation. This work
shows that the majority of syntactic reorderings, at
least between languages like English and Chinese,
can be efficiently decomposed into hierarchical bi-
nary reorderings. From a modeling perspective, on
the other hand, it is beneficial to start with a richer
representation that has more transformational power
than ITG or binary SCFG. Our work shows how to
convert it back to a computationally friendly form
without harming much of its expressiveness. As a
result, decoding with n-gram models can be fast and
accurate, making it possible for our syntax-based
system to overtake a comparable phrase-based sys-
tem in BLEU score. We believe that extensions of
our technique to more powerful models such as syn-
chronous tree-adjoining grammar (Shieber and Sch-
abes, 1990) is an interesting area for further work.
Acknowledgments Much of this work was done
when H. Zhang and L. Huang were visiting
USC/ISI. The authors wish to thank Wei Wang,
Jonathan Graehl and Steven DeNeefe for help with
the experiments. We are also grateful to Daniel
Marcu, Giorgio Satta, and Aravind Joshi for discus-
sions. This work was partially supported by NSF
ITR IIS-09325646 and NSF ITR IIS-0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, Michigan.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL-
03, companion volume, Sapporo, Japan.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
Proceedings of IWPT-05, Vancouver, BC.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing). LNCS.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of NAACL-03, Ed-
monton.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4).
Giorgio Satta and Enoch Peserico. 2005. Some computa-
tional complexity results for synchronous context-free
grammars. In Proceedings of HLT/EMNLP-05, pages
803?810, Vancouver, Canada, October.
L. Shapiro and A. B. Stephens. 1991. Bootstrap percola-
tion, the Schro?der numbers, and the n-kings problem.
SIAM Journal on Discrete Mathematics, 4(2):275?
280.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In COLING-90, volume III,
pages 253?258.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
263
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 351?358,
New York, June 2006. c?2006 Association for Computational Linguistics
A Better -Best List: Practical Determinization of Weighted Finite Tree
Automata
Jonathan May
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
jonmay@isi.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Ranked lists of output trees from syn-
tactic statistical NLP applications fre-
quently contain multiple repeated entries.
This redundancy leads to misrepresenta-
tion of tree weight and reduced informa-
tion for debugging and tuning purposes.
It is chiefly due to nondeterminism in the
weighted automata that produce the re-
sults. We introduce an algorithm that de-
terminizes such automata while preserv-
ing proper weights, returning the sum of
the weight of all multiply derived trees.
We also demonstrate our algorithm?s ef-
fectiveness on two large-scale tasks.
1 Introduction
A useful tool in natural language processing tasks
such as translation, speech recognition, parsing, etc.,
is the ranked list of results. Modern systems typ-
ically produce competing partial results internally
and return only the top-scoring complete result to
the user. They are, however, also capable of pro-
ducing lists of runners-up, and such lists have many
practical uses:
The lists may be inspected to determine
the quality of runners-up and motivate
model changes.
The lists may be re-ranked with extra
knowledge sources that are difficult to ap-
ply during the main search.
The lists may be used with annotation and
a tuning process, such as in (Collins and
Roark, 2004), to iteratively alter feature
weights and improve results.
Figure 1 shows the best 10 English translation
parse trees obtained from a syntax-based translation
system based on (Galley, et. al., 2004). Notice
that the same tree occurs multiple times in this list.
This repetition is quite characteristic of the output of
ranked lists. It occurs because many systems, such
as the ones proposed by (Bod, 1992), (Galley, et. al.,
2004), and (Langkilde and Knight, 1998) represent
their result space in terms of weighted partial results
of various sizes that may be assembled in multiple
ways. There is in general more than one way to as-
semble the partial results to derive the same com-
plete result. Thus, the -best list of results is really
an -best list of derivations.
When list-based tasks, such as the ones mentioned
above, take as input the top results for some con-
stant , the effect of repetition on these tasks is dele-
terious. A list with many repetitions suffers from a
lack of useful information, hampering diagnostics.
Repeated results prevent alternatives that would be
highly ranked in a secondary reranking system from
even being considered. And a list of fewer unique
trees than expected can cause overfitting when this
list is used to tune. Furthermore, the actual weight of
obtaining any particular tree is split among its repeti-
tions, distorting the actual relative weights between
trees.
(Mohri, 1997) encountered this problem in speech
recognition, and presented a solution to the prob-
lem of repetition in -best lists of strings that are
derived from finite-state automata. That work de-
scribed a way to use a powerset construction along
351
34.73: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.74: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.83: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.83: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.84: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.87: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(arouse) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
34.92: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
Figure 1: Ranked list of machine translation results with repeated trees. Scores shown are negative logs of
calculated weights, thus a lower score indicates a higher weight. The bulleted sentences indicate identical
trees.
with an innovative bookkeeping system to deter-
minize the automaton, resulting in an automaton that
preserves the language but provides a single, prop-
erly weighted derivation for each string in it. Put an-
other way, if the input automaton has the ability to
generate the same string with different weights, the
output automaton generates that string with weight
equal to the sum of all of the generations of that
string in the input automaton. In (Mohri and Riley,
2002) this technique was combined with a procedure
for efficiently obtaining -best ranked lists, yielding
a list of string results with no repetition.
In this paper we extend that work to deal with
grammars that produce trees. Regular tree gram-
mars (Brainerd, 1969), which subsume the tree sub-
stitution grammars developed in the NLP commu-
nity (Schabes, 1990), are of particular interest to
those wishing to work with additional levels of
structure that string grammars cannot provide. The
application to parsing is natural, and in machine
translation tree grammars can be used to model
syntactic transfer, control of function words, re-
ordering, and target-language well-formedness. In
the world of automata these grammars have as a nat-
ural dual the finite tree recognizer (Doner, 1970).
Like tree grammars and packed forests, they are
compact ways of representing very large sets of
trees. We will present an algorithm for determiniz-
ing weighted finite tree recognizers, and use a vari-
ant of the procedure found in (Huang and Chiang,
2005) to obtain -best lists of trees that are weighted
correctly and contain no repetition.
Section 2 describes related work. In Section 3, we
introduce the formalisms of tree automata, specifi-
cally the tree-to-weight transducer. In Section 4, we
present the algorithm. Finally, in Section 5 we show
the results of applying weighted determinization to
recognizers obtained from the packed forest output
of two natural language tasks.
2 Previous Work
The formalisms of tree automata are summarized
well in (Gecseg and Steinby, 1984). Bottom-up
tree recognizers are due to (Thatcher and Wright,
1968), (Doner, 1970), and (Magidor and Moran,
1969). Top-down tree recognizers are due to (Rabin,
1969) and (Magidor and Moran, 1969). (Comon, et.
al., 1997) show the determinization of unweighted
finite-state tree automata, and prove its correctness.
(Borchardt and Vogler, 2003) present determiniza-
tion of weighted finite-state tree automata with a dif-
ferent method than the one we present here. While
our method is applicable to finite tree sets, the previ-
ous method claims the ability to determinize some
classes of infinite tree sets. However, for the fi-
nite case the previous method produces an automa-
ton with size on the order of the number of deriva-
tions, so the technique is limited when applied to
real world data.
3 Grammars, Recognizers, and
Transducers
As described in (Gecseg and Steinby, 1984), tree au-
tomata may be broken into two classes, recognizers
and transducers. Recognizers read tree input and de-
cide whether the input is in the language represented
by the recognizer. Formally, a bottom-up tree recog-
nizer is defined by :1
is a finite set of states,
1Readers familiar with (Gecseg and Steinby, 1984) will no-
tice that we have introduced a start state, modified the notion of
initial assignment, and changed the arity of nullary symbols to
unary symbols. This is to make tree automata more palatable to
those accustomed to string automata and to allow for a useful
graphical interpretation.
352
Figure 2: Visualization of a bottom-up tree recog-
nizer
is a ranked alphabet,
is the initial state,
is a set of final states, and
is a finite set
of transitions from a vector of states to
one state that reads a -ary symbol.
Consider the following tree recognizer:
2
As with string automata, it is helpful to have a vi-
sualization to understand what the recognizer is rec-
ognizing. Figure 2 provides a visualization of the
recognizer above. Notice that some members of
are drawn as arcs with multiple (and ordered) tails.
This is the key difference in visualization between
string and tree automata ? to capture the arity of the
symbol being read we must visualize the automata
as an ordered hypergraph.
The function of the members of in the hyper-
graph visualization leads us to refer to the vector of
states as an input vector of states, and the single
state as an output state. We will refer to as the
transition set of the recognizer.
In string automata, a path through a recognizer
consists of a sequence of edges that can be followed
from a start to an end state. The concatenation of la-
bels of the edges of a path, typically in a left-to-right
order, forms a string in the recognizer?s language.
In tree automata, however, a hyperpath through a
recognizer consists of a sequence of hyperedges that
can be followed, sometimes in parallel, from a start
2The number denotes the arity of the symbol.
Figure 3: Bottom-up tree-to-weight transducer
to an end state. We arrange the labels of the hy-
peredges to form a tree in the recognizer?s language
but must now consider proper order in two dimen-
sions. The proper vertical order is specified by the
order of application of transitions, i.e., the labels of
transitions followed earlier are placed lower in the
tree than the labels of transitions followed later. The
proper horizontal order within one level of the tree is
specified by the order of states in a transition?s input
vector. In the example recognizer, the trees
and are valid. Notice that may be
recognized in two different hyperpaths.
Like tree recognizers, tree transducers read tree
input and decide whether the input is in the lan-
guage, but they simultaneously produce some out-
put as well. Since we wish to associate a weight
with every acceptable tree in a language, we will
consider transducers that produce weights as their
output. Note that in transitioning from recognizers
to transducers we are following the convention es-
tablished in (Mohri, 1997) where a transducer with
weight outputs is used to represent a weighted rec-
ognizer. One may consider the determinization of
tree-to-weight transducers as equivalent to the de-
terminization of weighted tree recognizers.
Formally, a bottom-up tree-to-weight transducer
is defined by where ,
, , and are defined as for recognizers, and:
is a
finite set of transitions from a vector of
states to one state, reading a -ary symbol
and outputting some weight
is the initial weight function mapping
to
is the final weight function mapping
353
to .
We must also specify a convention for propagat-
ing the weight calculated in every transition. This
can be explicitly defined for each transition but we
will simplify matters by defining the propagation of
the weight to a destination state as the multiplication
of the weight at each source state with the weight of
the production.
We modify the previous example by adding
weights as follows: As an example, consider the fol-
lowing tree-to-weight transducer ( , , , and are
as before):
Figure 3 shows the addition of weights onto the
automata, forming the above transducer. Notice the
tree yields the weight 0.036 (
), and yields the weight 0.012 (
) or 0.054 ( ), depending on
the hyperpath followed.
This transducer is an example of a nonsubsequen-
tial transducer. A tree transducer is subsequential if
for each vector v of states and each there
is at most one transition in with input vector v and
label . These restrictions ensure a subsequential
transducer yields a single output for each possible
input, that is, it is deterministic in its output.
Because we will reason about the destination state
of a transducer transition and the weight of a trans-
ducer transition separately, we make the following
definition. For a given v where
v is a vector of states, , , and
, let v and v . Equiva-
lent shorthand forms are and .
4 Determinization
The determinization algorithm is presented as Algo-
rithm 1. It takes as input a bottom-up tree-to-weight
transducer and returns as output a subsequential
bottom-up tree-to-weight transducer such that the
tree language recognized by is equivalent to that
of and the output weight given input tree on is
equal to the sum of all possible output weights given
on . Like the algorithm of (Mohri, 1997), this
Figure 4: a) Portion of a transducer before deter-
minization; b) The same portion after determiniza-
tion
algorithm will terminate for automata that recognize
finite tree languages. It may terminate on some au-
tomata that recognize infinite tree languages, but we
do not consider any of these cases in this work.
Determinizing a tree-to-weight transducer can be
thought of as a two-stage process. First, the structure
of the automata must be determined such that a sin-
gle hyperpath exists for each recognized input tree.
This is achieved by a classic powerset construction,
i.e., a state must be constructed in the output trans-
ducer that represents all the possible reachable desti-
nation states given an input and a label. Because we
are working with tree automata, our input is a vector
of states, not a single state. A comparable power-
set construction on unweighted tree automata and a
proof of correctness can be found in (Comon, et. al.,
1997).
The second consideration to weighted deter-
minization is proper propagation of weights. For this
we will use (Mohri, 1997)?s concept of the residual
weight. We represent in the construction of states
in the output transducer not only a subset of states
of the input transducer, but also a number associated
with each of these states, called the residual. Since
we want ?s hyperpath of a particular input tree to
have as its associated weight the sum of the weights
of the all of ?s hyperpaths of the input tree, we re-
place a set of hyperedges in that have the same
input state vector and label with a single hyperedge
in bearing the label and the sum of ?s hyper-
edge weights. The destination state of the hyper-
edge represents the states reachable by ?s applica-
ble hyperedges and for each state, the proportion of
the weight from the relevant transition.
Figure 4 shows the determinization of a portion
of the example transducer. Note that the hyperedge
354
Figure 5: Determinized bottom-up tree-to-weight
transducer
leading to state in the input transducer contributes
of the weight on the output transducer hyperedge
and the hyperedge leading to state in the input
transducer contributes the remaining . This is re-
flected in the state construction in the output trans-
ducer. The complete determinization of the example
transducer is shown in Figure 5.
To encapsulate the representation of states from
the input transducer and associated residual weights,
we define a state in the output transducer as a set of
tuples, where and . Since
the algorithm builds new states progressively, we
will need to represent a vector of states from the
output transducer, typically depicted as v. We may
construct the vector pair q w from v, where q is
a vector of states of the input transducer and w is
a vector of residual weights, by choosing a (state,
weight) pair from each output state in v. For ex-
ample, let . Then two possible out-
put transducer states could be and
. If we choose v then a
valid vector pair q w is q , w .
The sets v , v , and v are defined
as follows:
v q w from v
q .
v q w from v
q .
v q w from v
q .
.
v is the set of vector pairs q w con-
structed from v where each q is an input vector in
a transition with label . v is the set of
unique transitions paired with the appropriate pair
for each q w in v . v is the set of states
reachable from the transitions in v .
The consideration of vectors of states on the in-
cident edge of transitions effects two noticeable
changes on the algorithm as it is presented in
(Mohri, 1997). The first, relatively trivial, change
is the inclusion of the residual of multiple states in
the calculation of weights and residuals on lines 16
and 17. The second change is the production of
vectors for consideration. Whereas the string-based
algorithm considered newly-created states in turn,
we must consider newly-available vectors. For each
newly created state, newly available vectors can be
formed by using that state with the other states of
the output transducer. This operation is performed
on lines 7 and 22 of the algorithm.
5 Empirical Studies
We now turn to some empirical studies. We examine
the practical impact of the presented work by show-
ing:
That the multiple derivation problem is
pervasive in practice and determinization
is effective at removing duplicate trees.
That duplication causes misleading
weighting of individual trees and the
summing achieved from weighted deter-
minization corrects this error, leading to
re-ordering of the -best list.
That weighted determinization positively
affects end-to-end system performance.
We also compare our results to a commonly used
technique for estimation of -best lists, i.e., sum-
ming over the top derivations to get weight
estimates of the top unique elements.
5.1 Machine translation
We obtain packed-forest English outputs from 116
short Chinese sentences computed by a string-to-
tree machine translation system based on (Galley,
et. al., 2004). The system is trained on all Chinese-
English parallel data available from the Linguistic
Data Consortium. The decoder for this system is a
CKY algorithm that negotiates the space described
in (DeNeefe, et. al., 2005). No language model was
used in this experiment.
The forests contain a median of En-
glish parse trees each. We remove cycles from each
355
Algorithm 1: Weighted Determinization of Tree Automata
Input: BOTTOM-UP TREE-TO-WEIGHT TRANSDUCER .
Output: SUBSEQUENTIAL BOTTOM-UP TREE-TO-WEIGHT TRANSDUCER .
begin1
2
3
PRIORITY QUEUE4
5
6
ENQUEUE7
while do8
v head9
v10
for each v such that do11
if such that then12
s.t.13
14
for each such that v do15
v
v16
v
v
v
v s.t.17
v v v18
/* RANK returns the largest hyperedge size that can leave state .
COMBINATIONS returns all possible vectors of length
containing members of and at least one member of . */
if v is a new state then19
for each u COMBINATIONS v
v
RANK do
20
if u is a new vector then21
ENQUEUE u22
v23
DEQUEUE24
end25
forest,3 apply our determinization algorithm, and ex-
tract the -best trees using a variant of (Huang and
Chiang, 2005). The effects of weighted determiniza-
tion on an -best list are obvious to casual inspec-
tion. Figure 7 shows the improvement in quality of
the top 10 trees from our example translation after
the application of the determinization algorithm.
The improvement observed circumstantially
holds up to quantitative analysis as well. The
forests obtained by the determinized grammars have
between 1.39% and 50% of the number of trees of
their undeterminized counterparts. On average, the
determinized forests contain 13.7% of the original
3As in (Mohri, 1997), determinization may be applicable to
some automata that recognize infinite languages. In practice,
cycles in tree automata of MT results are almost never desired,
since these represent recursive insertion of words.
number of trees. Since a determinized forest con-
tains no repeated trees but contains exactly the same
unique trees as its undeterminized counterpart, this
indicates that an average of 86.3% of the trees in an
undeterminized MT output forest are duplicates.
Weighted determinization also causes a surpris-
ingly large amount of -best reordering. In 77.6%
of the translations, the tree regarded as ?best? is
different after determinization. This means that in
a large majority of cases, the tree with the high-
est weight is not recognized as such in the undeter-
minized list because its weight is divided among its
multiple derivations. Determinization allows these
instances and their associated weights to combine
and puts the highest weighted tree, not the highest
weighted derivation, at the top of the list.
356
method Bleu
undeterminized 21.87
top-500 ?crunching? 23.33
determinized 24.17
Figure 6: Bleu results from string-to-tree machine
translation of 116 short Chinese sentences with no
language model. The use of best derivation (unde-
terminized), estimate of best tree (top-500), and true
best tree (determinized) for selection of translation
is shown.
We can compare our method with the more com-
monly used methods of ?crunching? -best lists,
where . The duplicate sentences in the
trees are combined, hopefully resulting in at least
unique members with an estimation of the true
tree weight for each unique tree. Our results indi-
cate this is a rather crude estimation. When the top
500 derivations of the translations of our test cor-
pus are summed, only 50.6% of them yield an esti-
mated highest-weighted tree that is the same as the
true highest-weighted tree.
As a measure of the effect weighted determiniza-
tion and its consequential re-ordering has on an ac-
tual end-to-end evaluation, we obtain Bleu scores
for our 1-best translations from determinization, and
compare them with the 1-best translations from the
undeterminized forest and the 1-best translations
from the top-500 ?crunching? method. The results
are tabulated in Figure 6. Note that in 26.7% of
cases determinization did not terminate in a reason-
able amount of time. For these sentences we used
the best parse from top-500 estimation instead. It is
not surprising that determinization may occasionally
take a long time; even for a language of monadic
trees (i.e. strings) the determinization algorithm is
NP-complete, as implied by (Casacuberta and de la
Higuera, 2000) and, e.g. (Dijkstra, 1959).
5.2 Data-Oriented Parsing
Weighted determinization of tree automata is also
useful for parsing. Data-Oriented Parsing (DOP)?s
methodology is to calculate weighted derivations,
but as noted in (Bod, 2003), it is the highest ranking
parse, not derivation, that is desired. Since (Sima?an,
1996) showed that finding the highest ranking parse
is an NP-complete problem, it has been common to
estimate the highest ranking parse by the previously
method Recall Precision F-measure
undeterminized 80.23 80.18 80.20
top-500 ?crunching? 80.48 80.29 80.39
determinized 81.09 79.72 80.40
Figure 8: Recall, precision, and F-measure results
on DOP-style parsing of section 23 of the Penn Tree-
bank. The use of best derivation (undeterminized),
estimate of best tree (top-500), and true best tree (de-
terminized) for selection of parse output is shown.
described ?crunching? method.
We create a DOP-like parsing model4 by extract-
ing and weighting a subset of subtrees from sec-
tions 2-21 of the Penn Treebank and use a DOP-
style parser to generate packed forest representa-
tions of parses of the 2416 sentences of section 23.
The forests contain a median of parse
trees. We then remove cycles and apply weighted
determinization to the forests. The number of trees
in each determinized parse forest is reduced by a
factor of between 2.1 and . On aver-
age, the number of trees is reduced by a factor of
900,000, demonstrating a much larger number of du-
plicate parses prior to determinization than in the
machine translation experiment. The top-scoring
parse after determinization is different from the top-
scoring parse before determinization for 49.1% of
the forests, and when the determinization method
is ?approximated? by crunching the top-500 parses
from the undeterminized list only 55.9% of the top-
scoring parses are the same, indicating the crunch-
ing method is not a very good approximation of
determinization. We use the standard F-measure
combination of recall and precision to score the
top-scoring parse in each method against reference
parses. The results are tabulated in Figure 8. Note
that in 16.9% of cases determinization did not ter-
minate. For those sentences we used the best parse
from top-500 estimation instead.
6 Conclusion
We have shown that weighted determinization is
useful for recovering -best unique trees from a
weighted forest. As summarized in Figure 9, the
4This parser acquires a small subset of subtrees, in contrast
with DOP, and the beam search for this problem has not been
optimized.
357
31.87: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
32.11: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
32.15: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(arouse) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
32.55: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(cause) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
32.60: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(attracted) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
33.16: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(provoke) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
33.27: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBG(causing) NP-C(NPB(DT(the) JJ(american) NNS(protests)))) .(.))
33.29: S(NP-C(NPB(DT(this) NN(case))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
33.31: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) NN(protest)) PP(IN(of) NP-C(NPB(DT(the)
NNS(united states))))))) .(.))
33.33: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(incurred) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.))
Figure 7: Ranked list of machine translation results with no repeated trees.
experiment undeterminized determinized
machine translation
parsing
Figure 9: Median trees per sentence forest in ma-
chine translation and parsing experiments before and
after determinization is applied to the forests, re-
moving duplicate trees.
number of repeated trees prior to determinization
was typically very large, and thus determinization is
critical to recovering true tree weight. We have im-
proved evaluation scores by incorporating the pre-
sented algorithm into our MT work and we believe
that other NLP researchers working with trees can
similarly benefit from this algorithm.
Further advances in determinization will provide
additional benefit to the community. The transla-
tion system detailed here is a string-to-tree system,
and the determinization algorithm returns the -best
unique trees from a packed forest. Users of MT sys-
tems are generally interested in the string yield of
those trees, and not the trees per se. Thus, an algo-
rithm that can return the -best unique strings from
a packed forest would be a useful extension.
We plan for our weighted determinization algo-
rithm to be one component in a generally available
tree automata package for intersection, composition,
training, recognition, and generation of weighted
and unweighted tree automata for research tasks
such as the ones described above.
Acknowledgments
We thank Liang Huang for fruitful discussions
which aided in this work and David Chiang, Daniel
Marcu, and Steve DeNeefe for reading an early draft
and providing useful comments. This work was sup-
ported by NSF grant IIS-0428020.
References
Rens Bod. 1992. A Computational model of language perfor-
mance: data oriented parsing. In Proc. COLING
Rens Bod. 2003. An efficient implementation of a new DOP
model. In Proc. EACL,
Bjo?rn Borchardt and Heiko Vogler. 2003. Determinization of
finite state weighted tree automata. Journal of Automata,
Languages and Combinatorics, 8(3).
W. S. Brainerd. 1969. Tree generating regular systems. Infor-
mation and Control, 14.
F. Casacuberta and C. de la Higuera. 2000. Computa-
tional complexity of problems on probabilistic grammars
and transducers. In Proc. ICGI.
Michael Collins and Brian Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proc. ACL.
H. Comon and M. Dauchet and R. Gilleron and F. Jacquemard
and D. Lugiez and S. Tison and M. Tommasi. 1997 Tree
Automata Techniques and Applications.
S. DeNeefe and K. Knight and H. Chan. 2005. Interactively
exploring a machine translation model. Poster in Proc. ACL.
Edsger W. Dijkstra 1959. A note on two problems in connexion
with graphs Numerische Mathematik, 1.
J. E. Doner 1970. Tree acceptors and some of their applications
J. Comput. System Sci., 4.
M. Galley and M. Hopkins and K. Knight and D. Marcu. 2004.
What?s in a translation rule? In Proc. HLT-NAACL.
Ferenc Ge?cseg and Magnus Steinby 1984. Tree Automata.
Akade?miai Kiado?, Budapest.
Liang Huang and David Chiang 2005. Better k-best parsing In
Proc. IWPT.
Irene Langkilde and Kevin Knight 1998 The Practical Value of
N-Grams in Generation In Proc. INLG.
M. Magidor and G. Moran. 1969. Finite automata over finite
trees Technical Report 30. Hebrew University, Jerusalem.
Mehryar Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(2).
Mehryar Mohri and Michael Riley. 2002. An efficient algo-
rithm for the -best strings problem. In Proc. ICSLP.
M. O. Rabin. 1969. Decidability of second-order theories and
automata on infinite trees. Trans. Amer. Math. Soc., 141.
Yves Schabes. 1990. Mathematical and computational aspects
of lexicalized grammars. Ph.D. thesis. University of Penn-
sylvania, Philadelphia, PA.
Khalil Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree-grammars. In Proc.
COLING.
J. W. Thatcher and J. B. Wright. 1968. Generalized finite au-
tomata theory with an application to a decision problem of
second order logic. Mathematical Systems Theory, 2.
358
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Phoneme Mappings for Transliteration without Parallel Data
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We present a method for performing machine
transliteration without any parallel resources.
We frame the transliteration task as a deci-
pherment problem and show that it is possi-
ble to learn cross-language phoneme mapping
tables using only monolingual resources. We
compare various methods and evaluate their
accuracies on a standard name transliteration
task.
1 Introduction
Transliteration refers to the transport of names and
terms between languages with different writing sys-
tems and phoneme inventories. Recently there has
been a large amount of interesting work in this
area, and the literature has outgrown being citable
in its entirety. Much of this work focuses on back-
transliteration, which tries to restore a name or
term that has been transported into a foreign lan-
guage. Here, there is often only one correct target
spelling?for example, given jyon.kairu (the
name of a U.S. Senator transported to Japanese), we
must output ?Jon Kyl?, not ?John Kyre? or any other
variation.
There are many techniques for transliteration and
back-transliteration, and they vary along a number
of dimensions:
? phoneme substitution vs. character substitution
? heuristic vs. generative vs. discriminative mod-
els
? manual vs. automatic knowledge acquisition
We explore the third dimension, where we see
several techniques in use:
? Manually-constructed transliteration models,
e.g., (Hermjakob et al, 2008).
? Models constructed from bilingual dictionaries
of terms and names, e.g., (Knight and Graehl,
1998; Huang et al, 2004; Haizhou et al, 2004;
Zelenko and Aone, 2006; Yoon et al, 2007;
Li et al, 2007; Karimi et al, 2007; Sherif
and Kondrak, 2007b; Goldwasser and Roth,
2008b).
? Extraction of parallel examples from bilin-
gual corpora, using bootstrap dictionaries e.g.,
(Sherif and Kondrak, 2007a; Goldwasser and
Roth, 2008a).
? Extraction of parallel examples from compara-
ble corpora, using bootstrap dictionaries, and
temporal and word co-occurrence, e.g., (Sproat
et al, 2006; Klementiev and Roth, 2008).
? Extraction of parallel examples from web
queries, using bootstrap dictionaries, e.g., (Na-
gata et al, 2001; Oh and Isahara, 2006; Kuo et
al., 2006; Wu and Chang, 2007).
? Comparing terms from different languages in
phonetic space, e.g., (Tao et al, 2006; Goldberg
and Elhadad, 2008).
In this paper, we investigate methods to acquire
transliteration mappings from non-parallel sources.
We are inspired by previous work in unsupervised
learning for natural language, e.g. (Yarowsky, 1995;
37
WFSA - A WFST - B
English word 
sequence
English sound 
sequence
( SPENCER ABRAHAM ) ( S P EH N S ER  EY B R AH HH AE M )
WFST - C WFST - D
Japanese sound 
sequence
( ? ? ? ? ? ? ? ? ? ? ? ? )
Japanese katakana 
sequence
( S U P E N S A A  
E E B U R A H A M U )
Figure 1: Model used for back-transliteration of Japanese katakana names and terms into English. The model employs
a four-stage cascade of weighted finite-state transducers (Knight and Graehl, 1998).
Goldwater and Griffiths, 2007), and we are also in-
spired by cryptanalysis?we view a corpus of for-
eign terms as a code for English, and we attempt to
break the code.
2 Background
We follow (Knight and Graehl, 1998) in tackling
back-transliteration of Japanese katakana expres-
sions into English. Knight and Graehl (1998) devel-
oped a four-stage cascade of finite-state transducers,
shown in Figure 1.
? WFSA A - produces an English word sequence
w with probability P(w) (based on a unigram
word model).
? WFST B - generates an English phoneme se-
quence e corresponding to w with probability
P(e|w).
? WFST C - transforms the English phoneme se-
quence into a Japanese phoneme sequence j ac-
cording to a model P(j|e).
? WFST D - writes out the Japanese phoneme
sequence into Japanese katakana characters ac-
cording to a model P(k|j).
Using the cascade in the reverse (noisy-channel)
direction, they are able to translate new katakana
names and terms into English. They report 36% er-
ror in translating 100 U.S. Senators? names, and they
report exceeding human transliteration performance
in the presence of optical scanning noise.
The only transducer that requires parallel training
data is WFST C. Knight and Graehl (1998) take sev-
eral thousand phoneme string pairs, automatically
align them with the EM algorithm (Dempster et
al., 1977), and construct WFST C from the aligned
phoneme pieces.
We re-implement their basic method by instanti-
ating a densely-connected version of WFST C with
all 1-to-1 and 1-to-2 phoneme connections between
English and Japanese. Phoneme bigrams that occur
fewer than 10 times in a Japanese corpus are omit-
ted, and we omit 1-to-3 connections. This initial
WFST C model has 15320 uniformly weighted pa-
rameters. We then train the model on 3343 phoneme
string pairs from a bilingual dictionary, using the
EM algorithm. EM immediately reduces the con-
nections in the model to those actually observed in
the parallel data, and after 14 iterations, there are
only 188 connections left with P(j|e) ? 0.01. Fig-
ure 2 shows the phonemic substitution table learnt
from parallel training.
We use this trained WFST C model and apply it
to the U.S. Senator name transliteration task (which
we update to the 2008 roster). We obtain 40% er-
ror, roughly matching the performance observed in
(Knight and Graehl, 1998).
3 Task and Data
The task of this paper is to learn the mappings in
Figure 2, but without parallel data, and to test those
mappings in end-to-end transliteration. We imagine
our problem as one faced by monolingual English
speaker wandering around Japan, reading a multi-
tude of katakana signs, listening to people speak
Japanese, and eventually deciphering those signs
into English. To mis-quote Warren Weaver:
?When I look at a corpus of Japanese
katakana, I say to myself, this is really
written in English, but it has been coded
in some strange symbols. I will now pro-
ceed to decode.?
Our larger motivation is to move toward
easily-built transliteration systems for all language
pairs, regardless of parallel resources. While
Japanese/English transliteration has its own partic-
ular features, we believe it is a reasonable starting
point.
38
e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e)
AA o 0.49 AY a i 0.84 EH e 0.94 HH h 0.95 L r 0.62 OY o i 0.89 SH sh y 0.33 V b 0.75a 0.46 i 0.09 a 0.03 w 0.02 r u 0.37 o e 0.04 sh 0.31 b u 0.17o o 0.02 a 0.03 h a 0.02 o 0.04 y u 0.17 w 0.03a a 0.02 i y 0.01 i 0.04 ssh y 0.12 a 0.02a y 0.01 sh i 0.04ssh 0.02e 0.01AE a 0.93 B b 0.82 ER a a 0.8 IH i 0.89 M m 0.68 P p 0.63 T t 0.43 W w 0.73a ssh 0.02 b u 0.15 a 0.08 e 0.05 m u 0.22 p u 0.16 t o 0.25 u 0.17a n 0.02 a r 0.03 i n 0.01 n 0.08 pp u 0.13 tt o 0.17 o 0.04r u 0.02 a 0.01 pp 0.06 ts 0.04 i 0.02o r 0.02 tt 0.03e r 0.02 u 0.02ts u 0.02ch 0.02AH a 0.6 CH tch i 0.27 EY e e 0.58 IY i i 0.58 N n 0.96 PAUSE pause 1.0 TH s u 0.48 Y y 0.7o 0.13 ch 0.24 e 0.15 i 0.3 nn 0.02 s 0.22 i 0.26e 0.11 ch i 0.23 e i 0.12 e 0.07 sh 0.16 e 0.02i 0.07 ch y 0.2 a 0.1 e e 0.03 t o 0.04 a 0.02u 0.06 tch y 0.02 a i 0.03 ch 0.04tch 0.02 t e 0.02ssh y 0.01 t 0.02k 0.01 a 0.02AO o 0.6 D d 0.54 F h 0.58 JH j y 0.35 NG n 0.62 R r 0.61 UH u 0.79 Z z 0.27o o 0.27 d o 0.27 h u 0.35 j 0.24 g u 0.22 a 0.27 u u 0.09 z u 0.25a 0.05 dd o 0.06 hh 0.04 j i 0.21 n g 0.09 o 0.07 u a 0.04 u 0.16o n 0.03 z 0.02 hh u 0.02 jj i 0.14 i 0.04 r u 0.03 dd 0.03 s u 0.07a u 0.03 j 0.02 z 0.04 u 0.01 a a 0.01 u ssh 0.02 j 0.06u 0.01 u 0.01 o 0.01 o 0.02 a 0.06a 0.01 n 0.03i 0.03s 0.02o 0.02AW a u 0.69 DH z 0.87 G g 0.66 K k 0.53 OW o 0.57 S s u 0.43 UW u u 0.67 ZH j y 0.43a w 0.15 z u 0.08 g u 0.19 k u 0.2 o o 0.39 s 0.37 u 0.29 j i 0.29a o 0.06 a z 0.04 gg u 0.1 kk u 0.16 o u 0.02 sh 0.08 y u 0.02 j 0.29a 0.04 g y 0.03 kk 0.05 u 0.05u u 0.02 gg 0.01 k y 0.02 ss 0.02o o 0.02 g a 0.01 k i 0.01 ssh 0.01o 0.02
Figure 2: Phonemic substitution table learnt from 3343 parallel English/Japanese phoneme string pairs. English
phonemes are in uppercase, Japanese in lowercase. Mappings with P(j|e) > 0.01 are shown.
A A CH I D O CH E N J I N E B A D A W A K O B I A
A A K U P U R A Z A CH E S : W A N K A PP U
A A N D O : O P U T I K U S U W A N T E N P O
A A T I S U T O D E K O R A T I B U : W A S E R I N
A A T O S E R I N A P I S U T O N D E T O M O R U T O P I I T A A Y U N I O N
A I A N B I R U E P I G U R A M U P I KK A A Y U N I TT O SH I S U T E M U
A I D I I D O E R A N D O P I N G U U Y U U
A I K E N B E R I I : P I P E R A J I N A M I D O :
A J I A K A PP U J Y A I A N TS U P I S A :
A J I T O J Y A Z U P I U R A Z E N E R A R U E A K O N
A K A SH I A K O O S U : P O I N T O Z E R O
A K U A M Y U U Z E U M U : Z O N B I I Z U
: : : :
: : : :
Figure 3: Some Japanese phoneme sequences generated from the monolingual katakana corpus using WFST D.
Our monolingual resources are:
? 43717 unique Japanese katakana sequences
collected from web newspaper data. We split
multi-word katakana phrases on the center-dot
(???) character, and select a final corpus of
9350 unique sequences. We add monolingual
Japanese versions of the 2008 U.S. Senate ros-
ter.1
? The CMU pronunciation dictionary of English,
1We use ?open? EM testing, in which unlabeled test data
is allowed to be part of unsupervised training. However, no
parallel data is allowed.
with 112,151 entries.
? The English gigaword corpus. Knight and
Graehl (1998) already use frequently-occurring
capitalized words to build the WFSA A compo-
nent of their four-stage cascade.
We seek to use our English knowledge (derived
from 2 and 3) to decipher the Japanese katakana cor-
pus (1) into English. Figure 3 shows a portion of the
Japanese corpus, which we transform into Japanese
phoneme sequences using the monolingual resource
of WFST D. We note that the Japanese phoneme in-
ventory contains 39 unique (?ciphertext?) symbols,
39
compared to the 40 English (?plaintext?) phonemes.
Our goal is to compare and evaluate the WFST C
model learnt under two different scenarios?(a) us-
ing parallel data, and (b) using monolingual data.
For each experiment, we train only the WFST C
model and then apply it to the name translitera-
tion task?decoding 100 U.S. Senator names from
Japanese to English using the automata shown in
Figure 1. For all experiments, we keep the rest of
the models in the cascade (WFSA A, WFST B, and
WFST D) unchanged. We evaluate on whole-name
error-rate (maximum of 100/100) as well as normal-
ized word edit distance, which gives partial credit
for getting the first or last name correct.
4 Acquiring Phoneme Mappings from
Non-Parallel Data
Our main data consists of 9350 unique Japanese
phoneme sequences, which we can consider as a sin-
gle long sequence j. As suggested by Knight et
al (2006), we explain the existence of j as the re-
sult of someone initially producing a long English
phoneme sequence e, according to P(e), then trans-
forming it into j, according to P(j|e). The probabil-
ity of our observed data P(j) can be written as:
P (j) =?
e
P (e) ? P (j|e)
We take P(e) to be some fixed model of mono-
lingual English phoneme production, represented
as a weighted finite-state acceptor (WFSA). P(j|e)
is implemented as the initial, uniformly-weighted
WFST C described in Section 2, with 15320 phone-
mic connections.
We next maximize P(j) by manipulating the sub-
stitution table P(j|e), aiming to produce a result
such as shown in Figure 2. We accomplish this by
composing the English phoneme model P(e) WFSA
with the P(j|e) transducer. We then use the EM al-
gorithm to train just the P(j|e) parameters (inside
the composition that predicts j), and guess the val-
ues for the individual phonemic substitutions that
maximize the likelihood of the observed data P(j).2
2In our experiments, we use the Carmel finite-state trans-
ducer package (Graehl, 1997), a toolkit with an algorithm for
EM training of weighted finite-state transducers.
We allow EM to run until the P(j) likelihood ra-
tio between subsequent training iterations reaches
0.9999, and we terminate early if 200 iterations are
reached.
Finally, we decode our test set of U.S. Senator
names. Following Knight et al(2006), we stretch
out the P(j|e) model probabilities after decipher-
ment training and prior to decoding our test set, by
cubing their values.
Decipherment under the conditions of translit-
eration is substantially more difficult than solv-
ing letter-substitution ciphers (Knight et al, 2006;
Ravi and Knight, 2008; Ravi and Knight, 2009) or
phoneme-substitution ciphers (Knight and Yamada,
1999). This is because the target table contains sig-
nificant non-determinism, and because each symbol
has multiple possible fertilities, which introduces
uncertainty about the length of the target string.
4.1 Baseline P(e) Model
Clearly, we can design P(e) in a number of ways. We
might expect that the more the system knows about
English, the better it will be able to decipher the
Japanese. Our baseline P(e) is a 2-gram phoneme
model trained on phoneme sequences from the CMU
dictionary. The second row (2a) in Figure 4 shows
results when we decipher with this fixed P(e). This
approach performs poorly and gets all the Senator
names wrong.
4.2 Consonant Parity
When training under non-parallel conditions, we
find that we would like to keep our WFST C model
small, rather than instantiating a fully-connected
model. In the supervised case, parallel training al-
lows the trained model to retain only those con-
nections which were observed from the data, and
this helps eliminate many bad connections from the
model. In the unsupervised case, there is no parallel
data available to help us make the right choices.
We therefore use prior knowledge and place a
consonant-parity constraint on the WFST C model.
Prior to EM training, we throw out any mapping
from the P(j|e) substitution model that does not
have the same number of English and Japanese con-
sonant phonemes. This is a pattern that we observe
across a range of transliteration tasks. Here are ex-
40
Phonemic Substitution Model Name Transliteration Error
whole-name error norm. edit distance
1 e ? j = { 1-to-1, 1-to-2 } 40 25.9
+ EM aligned with parallel data
2a e ? j = { 1-to-1, 1-to-2 } 100 100.0
+ decipherment training with 2-gram English P(e)
2b e ? j = { 1-to-1, 1-to-2 } 98 89.8
+ decipherment training with 2-gram English P(e)
+ consonant-parity
2c e ? j = { 1-to-1, 1-to-2 } 94 73.6
+ decipherment training with 3-gram English P(e)
+ consonant-parity
2d e ? j = { 1-to-1, 1-to-2 } 77 57.2
+ decipherment training with a word-based English model
+ consonant-parity
2e e ? j = { 1-to-1, 1-to-2 } 73 54.2
+ decipherment training with a word-based English model
+ consonant-parity
+ initialize mappings having consonant matches with higher proba-
bility weights
Figure 4: Results on name transliteration obtained when using the phonemic substitution model trained under different
scenarios?(1) parallel training data, (2a-e) using only monolingual resources.
amples of mappings where consonant parity is vio-
lated:
K => a N => e e
EH => s a EY => n
Modifying the WFST C in this way leads to bet-
ter decipherment tables and slightly better results
for the U.S. Senator task. Normalized edit distance
drops from 100 to just under 90 (row 2b in Figure 4).
4.3 Better English Models
Row 2c in Figure 4 shows decipherment results
when we move to a 3-gram English phoneme model
for P(e). We notice considerable improvements in
accuracy. On the U.S. Senator task, normalized edit
distance drops from 89.8 to 73.6, and whole-name
error decreases from 98 to 94.
When we analyze the results from deciphering
with a 3-gram P(e) model, we find that many of the
Japanese phoneme test sequences are decoded into
English phoneme sequences (such as ?IH K R IH
N? and ?AE G M AH N?) that are not valid words.
This happens because the models we used for de-
cipherment so far have no knowledge of what con-
stitutes a globally valid English sequence. To help
the phonemic substitution model learn this infor-
mation automatically, we build a word-based P(e)
from English phoneme sequences in the CMU dic-
tionary and use this model for decipherment train-
ing. The word-based model produces complete En-
glish phoneme sequences corresponding to 76,152
actual English words from the CMU dictionary.
The English phoneme sequences are represented as
paths through a WFSA, and all paths are weighted
equally. We represent the word-based model in com-
pact form, using determinization and minimization
techniques applicable to weighted finite-state au-
tomata. This allows us to perform efficient EM train-
ing on the cascade of P(e) and P(j|e) models. Under
this scheme, English phoneme sequences resulting
from decipherment are always analyzable into actual
words.
Row 2d in Figure 4 shows the results we ob-
tain when training our WFST C with a word-based
English phoneme model. Using the word-based
model produces the best result so far on the phone-
mic substitution task with non-parallel data. On the
U.S. Senator task, word-based decipherment outper-
forms the other methods by a large margin. It gets
23 out of 100 Senator names exactly right, with a
much lower normalized edit distance (57.2). We
have managed to achieve this performance using
only monolingual data. This also puts us within
reach of the parallel-trained system?s performance
(40% whole-name errors, and 25.9 word edit dis-
tance error) without using a single English/Japanese
pair for training.
To summarize, the quality of the English phoneme
41
e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e)
AA a 0.37 AY a i 0.36 EH e 0.37 HH h 0.45 L r 0.3 OY a 0.27 SH sh y 0.22 V b 0.34o 0.25 o o 0.13 a 0.24 s 0.12 n 0.19 i 0.16 m 0.11 k 0.14i 0.15 e 0.12 o 0.12 k 0.09 r u 0.15 y u 0.1 r 0.1 m 0.13u 0.08 i 0.11 i 0.12 b 0.08 r i 0.04 o i 0.1 s 0.06 s 0.07e 0.07 a 0.11 u 0.06 m 0.07 t 0.03 y a 0.09 p 0.06 d 0.07o o 0.03 u u 0.05 o o 0.04 w 0.03 m u 0.02 y o 0.08 s a 0.05 r 0.04y a 0.01 y u 0.02 y u 0.01 p 0.03 m 0.02 e 0.08 h 0.05 t 0.03a a 0.01 u 0.02 a i 0.01 g 0.03 w a 0.01 o 0.06 b 0.05 h 0.02o 0.02 k y 0.02 t a 0.01 o o 0.02 t 0.04 sh 0.01e e 0.02 d 0.02 r a 0.01 e i 0.02 k 0.04 n 0.01AE a 0.52 B b 0.41 ER a a 0.47 IH i 0.36 M m 0.3 P p 0.18 T t 0.2 W w 0.23i 0.19 p 0.12 a 0.17 e 0.25 n 0.08 p u 0.08 t o 0.16 r 0.2e 0.11 k 0.09 u 0.08 a 0.15 k 0.08 n 0.05 t a 0.05 m 0.13o 0.08 m 0.07 o 0.07 u 0.09 r 0.07 k 0.05 n 0.04 s 0.08u 0.03 s 0.04 e 0.04 o 0.09 s 0.06 sh i 0.04 k u 0.03 k 0.07u u 0.02 g 0.04 o o 0.03 o o 0.01 h 0.05 k u 0.04 k 0.03 h 0.06o o 0.02 t 0.03 i i 0.03 t 0.04 s u 0.03 t e 0.02 b 0.06z 0.02 y u 0.02 g 0.04 p a 0.03 s 0.02 t 0.04d 0.02 u u 0.02 b 0.04 t 0.02 r 0.02 p 0.04ch y 0.02 i 0.02 m u 0.03 m a 0.02 g u 0.02 d 0.02AH a 0.31 CH g 0.12 EY e e 0.3 IY i 0.25 N n 0.56 PAUSE pause 1.0 TH k 0.21 Y s 0.25o 0.23 k 0.11 a 0.22 i i 0.21 r u 0.09 p u 0.11 k 0.18i 0.17 b 0.09 i 0.11 a 0.15 s u 0.04 k u 0.1 m 0.07e 0.12 sh 0.07 u 0.09 a a 0.12 m u 0.02 d 0.08 g 0.06u 0.1 s 0.07 o 0.06 u 0.07 kk u 0.02 h u 0.07 p 0.05e e 0.02 r 0.07 e 0.06 o 0.05 k u 0.02 s u 0.05 b 0.05o o 0.01 ch y 0.07 o o 0.05 o o 0.02 h u 0.02 b u 0.04 r 0.04a a 0.01 p 0.06 e i 0.04 i a 0.02 t o 0.01 k o 0.03 d 0.04m 0.06 i i 0.02 e e 0.02 pp u 0.01 g a 0.03 u r 0.03ch 0.06 u u 0.01 e 0.02 b i 0.01 s a 0.02 n y 0.03AO o 0.29 D d 0.16 F h 0.18 JH b 0.13 NG tt o 0.21 R r 0.53 UH a 0.24 Z t o 0.14a 0.26 d o 0.15 h u 0.14 k 0.1 r u 0.17 n 0.07 o 0.14 z u 0.11e 0.14 n 0.05 b 0.09 j y 0.1 n 0.14 u r 0.05 e 0.11 r u 0.11o o 0.12 t o 0.03 sh i 0.07 s 0.08 kk u 0.1 r i 0.03 y u 0.1 s u 0.1i 0.08 sh i 0.03 p 0.07 m 0.08 s u 0.07 r u 0.02 a i 0.09 g u 0.09u 0.05 k u 0.03 m 0.06 t 0.07 m u 0.06 d 0.02 i 0.08 m u 0.07y u 0.03 k 0.03 r 0.04 j 0.07 dd o 0.04 t 0.01 u u 0.07 n 0.06e e 0.01 g u 0.03 s 0.03 h 0.07 tch i 0.03 s 0.01 o o 0.07 d o 0.06b 0.03 h a 0.03 sh 0.06 pp u 0.03 m 0.01 a a 0.03 j i 0.02s 0.02 b u 0.02 d 0.05 jj i 0.03 k 0.01 u 0.02 ch i 0.02AW o o 0.2 DH h 0.13 G g u 0.13 K k 0.17 OW a 0.3 S s u 0.4 UW u 0.39 ZH m 0.17a u 0.19 r 0.12 g 0.11 n 0.1 o 0.25 n 0.11 a 0.15 p 0.16a 0.18 b 0.09 k u 0.08 k u 0.1 o o 0.12 r u 0.05 o 0.13 t 0.15a i 0.11 w 0.08 b u 0.06 kk u 0.05 u 0.09 t o 0.03 u u 0.12 h 0.13a a 0.11 t 0.07 k 0.04 t o 0.03 i 0.07 k u 0.03 i 0.04 d 0.1e 0.05 p 0.07 b 0.04 s u 0.03 y a 0.04 sh i 0.02 y u 0.03 s 0.08o 0.04 g 0.06 t o 0.03 sh i 0.02 e 0.04 r i 0.02 i i 0.03 b 0.07i 0.04 j y 0.05 t 0.03 r 0.02 u u 0.02 m u 0.02 e 0.03 r 0.05i y 0.02 d 0.05 h a 0.03 k o 0.02 a i 0.02 h u 0.02 o o 0.02 j y 0.03e a 0.01 k 0.03 d 0.03 k a 0.02 i i 0.01 ch i 0.02 e e 0.02 k 0.02
Figure 5: Phonemic substitution table learnt from non-parallel corpora. For each English phoneme, only the top ten
mappings with P(j|e) > 0.01 are shown.
model used in decipherment training has a large ef-
fect on the learnt P(j|e) phonemic substitution ta-
ble (i.e., probabilities for the various phoneme map-
pings within the WFST C model), which in turn af-
fects the quality of the back-transliterated English
output produced when decoding Japanese.
Figure 5 shows the phonemic substitution table
learnt using word-based decipherment. The map-
pings are reasonable, given the lack of parallel data.
They are not entirely correct?for example, the map-
ping ?S? s u? is there, but ?S? s? is missing.
Sample end-to-end transliterations are illustrated
in Figure 6. The figure shows how the transliteration
results from non-parallel training improve steadily
as we use stronger decipherment techniques. We
note that in one case (LAUTENBERG), the deci-
pherment mapping table leads to a correct answer
where the mapping table derived from parallel data
does not. Because parallel data is limited, it may not
contain all of the necessary mappings.
4.4 Size of Japanese Training Data
Monolingual corpora are more easily available than
parallel corpora, so we can use increasing amounts
of monolingual Japanese training data during de-
cipherment training. The table below shows that
using more Japanese training data produces bet-
ter transliteration results when deciphering with the
word-based English model.
Japanese training data Error on name transliteration task
(# of phoneme sequences) whole-name error normalized word
edit distance
4,674 87 69.7
9,350 77 57.2
42
? ? ?????????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
????
????
?????
???????
?????
???????
????
?????
??????? ?????
???????
????
???
???????? ???? ???? ??????? ????????? ??????????
????
????
??????????? ?????????? ? ?????? ???? ??? ???????????
????
????
? ??? ?? ????? ?? ?????? ?? ???? ?? ????
????? ????????? ???????? ???????? ?????????? ??????????
????
????
???????? ??????? ???????? ?????????? ????????
????
????
?????????? ?????????? ????????? ??? ??????? ??? ???????
????
?????
?????
??????
????
??? ??
??????? ??????
?? ?????
??????
?? ?????
?????????????????????????????????????????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
??????? ??????? ? ???????????? ????????? ??????? ???????????
??????? ???????? ???? ???? ??????? ????????? ??????????
???? ??????????? ?????????? ? ?????? ???? ??? ???????????
???????? ? ??? ?? ????? ?? ?????? ?? ???? ?? ????
????? ????????? ???????? ???????? ?????????? ??????????
?????? ??? ???? ??????? ???????? ?????????? ??? ????
???????? ?????????? ?????????? ????????? ??? ??????? ??? ???????
??????? ?? ???????? ??????? ?? ??????? ? ?????? ????? ? ?????? ?????
??????????????????????????????
?????? ????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
????
?????
?? ????
?????
???????
????
?????
??????? ?????
???????
??
???
???????? ? ?? ???? ??????? ????????? ??????????
?
????
? ????????? ?? ?????? ? ?????? ???? ??? ???????????
????
?
? ??? ?? ????? ?? ?????? ?? ???? ?? ????
?? ?? ??? ? ??? ?? ? ?????? ? ?????????? ??????????
???
????
??? ??? ??? ?? ? ?? ????? ?????????? ????????
???
?? ? ? ?? ?? ????? ?? ?????? ??? ??????? ??? ???????
??
?????
? ??
??????
? ?
??? ??
??? ? ? ??????
?? ?????
??????
?? ?????
?????????????????????????????????????????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
?????? ??????? ? ??????????? ????????? ??????? ???????????
????? ???????? ???? ? ?? ??????? ????????? ??????????
?? ??????????? ?????????? ? ???? ? ???? ??? ???????????
?????? ? ??? ?? ????? ?? ?????? ?? ???? ?? ????
????? ????????? ??????? ? ???? ? ?????????? ??????????
??????? ??? ???? ?????? ???????? ?????????? ??? ????
??????? ?????????? ????????? ? ??? ?? ??? ??????? ??? ???????
??????? ?? ???????? ??????? ?? ??????? ? ?????? ????? ? ?????? ?????
??????????????????????????????
Figure 6: Results for end-to-end name transliteration. This figure shows the correct answer, the answer obtained
by training mappings on parallel data (Knight and Graehl, 1998), and various answers obtained by deciphering non-
parallel data. Method 1 uses a 2-gram P(e), Method 2 uses a 3-gram P(e), and Method 3 uses a word-based P(e).
4.5 P(j|e) Initialization
So far, the P(j|e) connections within the WFST C
model were initialized with uniform weights prior
to EM training. It is a known fact that the EM al-
gorithm does not necessarily find a global minimum
for the given objective function. If the search space
is bumpy and non-convex as is the case in our prob-
lem, EM can get stuck in any of the local minima
depending on what weights were used to initialize
the search. Different sets of initialization weights
can lead to different convergence points during EM
training, or in other words, depending on how the
P(j|e) probabilities are initialized, the final P(j|e)
substitution table learnt by EM can vary.
We can use some prior knowledge to initialize the
probability weights in our WFST C model, so as to
give EM a good starting point to work with. In-
stead of using uniform weights, in the P(j|e) model
we set higher weights for the mappings where En-
glish and Japanese sounds share common consonant
phonemes.
For example, mappings such as:
N => n N => a n
D => d D => d o
are weighted X (a constant) times higher than
other mappings such as:
N => b N => r
D => B EY => a a
in the P(j|e) model. In our experiments, we set
the value X to 100.
Initializing the WFST C in this way results in EM
learning better substitution tables and yields slightly
better results for the Senator task. Normalized edit
distance drops from 57.2 to 54.2, and the whole-
name error is also reduced from 77% to 73% (row
2e in Figure 4).
4.6 Size of English Training Data
We saw earlier (in Section 4.4) that using more
monolingual Japanese training data yields improve-
ments in decipherment results. Similarly, we hy-
pothesize that using more monolingual English data
can drive the decipherment towards better translit-
eration results. On the English side, we build dif-
ferent word-based P(e) models, each trained on dif-
ferent amounts of data (English phoneme sequences
from the CMU dictionary). The table below shows
that deciphering with a word-based English model
43
built from more data produces better transliteration
results.
English training data Error on name transliteration task
(# of phoneme sequences) whole-name error normalized word
edit distance
76,152 73 54.2
97,912 66 49.3
This yields the best transliteration results on the
Senator task with non-parallel data, getting 34 out
of 100 Senator names exactly right.
4.7 Re-ranking Results Using the Web
It is possible to improve our results on the U.S. Sen-
ator task further using external monolingual re-
sources. Web counts are frequently used to auto-
matically re-rank candidate lists for various NLP
tasks (Al-Onaizan and Knight, 2002). We extract
the top 10 English candidates produced by our word-
based decipherment method for each Japanese test
name. Using a search engine, we query the entire
English name (first and last name) corresponding to
each candidate, and collect search result counts. We
then re-rank the candidates using the collected Web
counts and pick the most frequent candidate as our
choice.
For example, France Murkowski gets only 1 hit
on Google, whereas Frank Murkowski gets 135,000
hits. Re-ranking the results in this manner lowers
the whole-name error on the Senator task from 66%
to 61%, and also lowers the normalized edit dis-
tance from 49.3 to 48.8. However, we do note that
re-ranking using Web counts produces similar im-
provements in the case of parallel training as well
and lowers the whole-name error from 40% to 24%.
So, the re-ranking idea, which is simple and re-
quires only monolingual resources, seems like a nice
strategy to apply at the end of transliteration exper-
iments (during decoding), and can result in further
gains on the final transliteration performance.
5 Comparable versus Non-Parallel
Corpora
We also present decipherment results when using
comparable corpora for training the WFST C model.
We use English and Japanese phoneme sequences
derived from a parallel corpus containing 2,683
phoneme sequence pairs to construct comparable
corpora (such that for each Japanese phoneme se-
quence, the correct back-transliterated phoneme se-
quence is present somewhere in the English data)
and apply the same decipherment strategy using a
word-based English model. The table below com-
pares the transliteration results for the U.S. Sena-
tor task, when using comparable versus non-parallel
data for decipherment training. While training on
comparable corpora does have benefits and reduces
the whole-name error to 59% on the Senator task, it
is encouraging to see that our best decipherment re-
sults using only non-parallel data comes close (66%
error).
English/Japanese Corpora Error on name transliteration task
(# of phoneme sequences) whole-name error normalized word
edit distance
Comparable Corpora 59 41.8
(English = 2,608
Japanese = 2,455)
Non-Parallel Corpora 66 49.3
(English = 98,000
Japanese = 9,350)
6 Conclusion
We have presented a method for attacking machine
transliteration problems without parallel data. We
developed phonemic substitution tables trained us-
ing only monolingual resources and demonstrated
their performance in an end-to-end name translitera-
tion task. We showed that consistent improvements
in transliteration performance are possible with the
use of strong decipherment techniques, and our best
system achieves significant improvements over the
baseline system. In future work, we would like to
develop more powerful decipherment models and
techniques, and we would like to harness the infor-
mation available from a wide variety of monolingual
resources, and use it to further narrow the gap be-
tween parallel-trained and non-parallel-trained ap-
proaches.
7 Acknowledgements
This research was supported by the Defense Ad-
vanced Research Projects Agency under SRI Inter-
national?s prime Contract Number NBCHD040058.
44
References
Y. Al-Onaizan and K. Knight. 2002. Translating named
entities using monolingual and bilingual resources. In
Proc. of ACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society Se-
ries, 39(4):1?38.
Y. Goldberg and M. Elhadad. 2008. Identification of
transliterated foreign words in Hebrew script. In Proc.
of CICLing.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In Proc. of
ACL/HLT Short Papers.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In Proc. of EMNLP.
S. Goldwater and L. Griffiths, T. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. Graehl. 1997. Carmel finite-state toolkit.
http://www.isi.edu/licensed-sw/carmel.
L. Haizhou, Z. Min, and S. Jian. 2004. A joint source-
channel model for machine transliteration. In Proc. of
ACL.
U. Hermjakob, K. Knight, and H. Daume. 2008. Name
translation in statistical machine translation?learning
when to transliterate. In Proc. of ACL/HLT.
F. Huang, S. Vogel, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In Proc. of HLT/NAACL.
S. Karimi, F. Scholer, and A. Turpin. 2007. Col-
lapsed consonant and vowel models: New ap-
proaches for English-Persian transliteration and back-
transliteration. In Proc. of ACL.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Learning Machine Translation. MIT press.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4):599?612.
K. Knight and K. Yamada. 1999. A computational ap-
proach to deciphering unknown scripts. In Proc. of the
ACL Workshop on Unsupervised Learning in Natural
Language Processing.
K. Knight, A. Nair, N. Rathod, and K. Yamada. 2006.
Unsupervised analysis for decipherment problems. In
Proc. of COLING/ACL.
J. Kuo, H. Li, and Y. Yang. 2006. Learning translitera-
tion lexicons from the web. In Proc. of ACL/COLING.
H. Li, C. Sim, K., J. Kuo, and M. Dong. 2007. Semantic
transliteration of personal names. In Proc. of ACL.
M. Nagata, T. Saito, and K. Suzuki. 2001. Using the web
as a bilingual dictionary. In Proc. of the ACL Work-
shop on Data-driven Methods in Machine Translation.
J. Oh and H. Isahara. 2006. Mining the web for translit-
eration lexicons: Joint-validation approach. In Proc.
of the IEEE/WIC/ACM International Conference on
Web Intelligence.
S. Ravi and K. Knight. 2008. Attacking decipherment
problems optimally with low-order n-gram models. In
Proc. of EMNLP.
S. Ravi and K. Knight. 2009. Probabilistic methods for a
Japanese syllable cipher. In Proc. of the International
Conference on the Computer Processing of Oriental
Languages (ICCPOL).
T. Sherif and G. Kondrak. 2007a. Bootstrapping a
stochastic transducer for arabic-english transliteration
extraction. In Proc. of ACL.
T. Sherif and G. Kondrak. 2007b. Substring-based
transliteration. In Proc. of ACL.
R. Sproat, T. Tao, and C. Zhai. 2006. Named entity
transliteration with comparable corpora. In Proc. of
ACL.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entity transliteration using tem-
poral and phonetic correlation. In Proc. of EMNLP.
J. Wu and S. Chang, J. 2007. Learning to find English
to Chinese transliterations on the web. In Proc. of
EMNLP/CoNLL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL.
S. Yoon, K. Kim, and R. Sproat. 2007. Multilingual
transliteration using feature based phonetic method. In
Proc. of ACL.
D. Zelenko and C. Aone. 2006. Discriminative methods
for transliteration. In Proc. of EMNLP.
45
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218?226,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
11,001 New Features for Statistical Machine Translation?
David Chiang and Kevin Knight
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
Wei Wang
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292 USA
Abstract
We use the Margin Infused Relaxed Algo-
rithm of Crammer et al to add a large num-
ber of new features to two machine transla-
tion systems: the Hiero hierarchical phrase-
based translation system and our syntax-based
translation system. On a large-scale Chinese-
English translation task, we obtain statistically
significant improvements of +1.5 B??? and
+1.1 B???, respectively. We analyze the im-
pact of the new features and the performance
of the learning algorithm.
1 Introduction
What linguistic features can improve statistical ma-
chine translation (MT)? This is a fundamental ques-
tion for the discipline, particularly as it pertains to
improving the best systems we have. Further:
? Do syntax-based translation systems have
unique and effective levers to pull when design-
ing new features?
? Can large numbers of feature weights be
learned efficiently and stably on modest
amounts of data?
In this paper, we address these questions by exper-
imenting with a large number of new features. We
add more than 250 features to improve a syntax-
based MT system?already the highest-scoring sin-
gle system in the NIST 2008 Chinese-English
common-data track?by +1.1 B???. We also add
more than 10,000 features to Hiero (Chiang, 2005)
and obtain a +1.5 B??? improvement.
?This research was supported in part by DARPA contract
HR0011-06-C-0022 under subcontract to BBN Technologies.
Many of the new features use syntactic informa-
tion, and in particular depend on information that
is available only inside a syntax-based translation
model. Thus they widen the advantage that syntax-
based models have over other types of models.
The models are trained using the Margin Infused
Relaxed Algorithm or MIRA (Crammer et al, 2006)
instead of the standard minimum-error-rate training
or MERT algorithm (Och, 2003). Our results add
to a growing body of evidence (Watanabe et al,
2007; Chiang et al, 2008) that MIRA is preferable to
MERT across languages and systems, even for very
large-scale tasks.
2 Related Work
The work of Och et al(2004) is perhaps the best-
known study of new features and their impact on
translation quality. However, it had a few shortcom-
ings. First, it used the features for reranking n-best
lists of translations, rather than for decoding or for-
est reranking (Huang, 2008). Second, it attempted to
incorporate syntax by applying off-the-shelf part-of-
speech taggers and parsers to MT output, a task these
tools were never designed for. By contrast, we incor-
porate features directly into hierarchical and syntax-
based decoders.
A third difficulty with Och et al?s study was that
it used MERT, which is not an ideal vehicle for fea-
ture exploration because it is observed not to per-
form well with large feature sets. Others have in-
troduced alternative discriminative training meth-
ods (Tillmann and Zhang, 2006; Liang et al, 2006;
Turian et al, 2007; Blunsom et al, 2008; Macherey
et al, 2008), in which a recurring challenge is scal-
ability: to train many features, we need many train-
218
ing examples, and to train discriminatively, we need
to search through all possible translations of each
training example. Another line of research (Watan-
abe et al, 2007; Chiang et al, 2008) tries to squeeze
as many features as possible from a relatively small
dataset. We follow this approach here.
3 Systems Used
3.1 Hiero
Hiero (Chiang, 2005) is a hierarchical, string-to-
string translation system. Its rules, which are ex-
tracted from unparsed, word-aligned parallel text,
are synchronous CFG productions, for example:
X? X1 de X2,X2 of X1
As the number of nonterminals is limited to two, the
grammar is equivalent to an inversion transduction
grammar (Wu, 1997).
The baseline model includes 12 features whose
weights are optimized using MERT. Two of the fea-
tures are n-gram language models, which require
intersecting the synchronous CFG with finite-state
automata representing the language models. This
grammar can be parsed efficiently using cube prun-
ing (Chiang, 2007).
3.2 Syntax-based system
Our syntax-based system transforms source Chinese
strings into target English syntax trees. Following
previous work in statistical MT (Brown et al, 1993),
we envision a noisy-channel model in which a lan-
guage model generates English, and then a transla-
tion model transforms English trees into Chinese.
We represent the translation model as a tree trans-
ducer (Knight and Graehl, 2005). It is obtained from
bilingual text that has been word-aligned and whose
English side has been syntactically parsed. From this
data, we use the the GHKM minimal-rule extraction
algorithm of (Galley et al, 2004) to yield rules like:
NP-C(x0:NPB PP(IN(of x1:NPB))? x1 de x0
Though this rule can be used in either direction,
here we use it right-to-left (Chinese to English). We
follow Galley et al (2006) in allowing unaligned
Chinese words to participate in multiple translation
rules, and in collecting larger rules composed of
minimal rules. These larger rules have been shown
to substantially improve translation accuracy (Gal-
ley et al, 2006; DeNeefe et al, 2007).
We apply Good-Turing discounting to the trans-
ducer rule counts and obtain probability estimates:
P(rule) = count(rule)count(LHS-root(rule))
When we apply these probabilities to derive an En-
glish sentence e and a corresponding Chinese sen-
tence c, we wind up with the joint probability P(e, c).
The baseline model includes log P(e, c), the two
n-gram language models log P(e), and other features
for a total of 25. For example, there is a pair of
features to punish rules that drop Chinese content
words or introduce spurious English content words.
All features are linearly combined and their weights
are optimized using MERT.
For efficient decoding with integrated n-gram lan-
guage models, all transducer rules must be binarized
into rules that contain at most two variables and
can be incrementally scored by the language model
(Zhang et al, 2006). Then we use a CKY-style parser
(Yamada and Knight, 2002; Galley et al, 2006) with
cube pruning to decode new sentences.
We include two other techniques in our baseline.
To get more general translation rules, we restruc-
ture our English training trees using expectation-
maximization (Wang et al, 2007), and to get more
specific translation rules, we relabel the trees with up
to 4 specialized versions of each nonterminal sym-
bol, again using expectation-maximization and the
split/merge technique of Petrov et al (2006).
3.3 MIRA training
We incorporate all our new features into a linear
model (Och and Ney, 2002) and train them using
MIRA (Crammer et al, 2006), following previous
work (Watanabe et al, 2007; Chiang et al, 2008).
Let e stand for output strings or their derivations,
and let h(e) stand for the feature vector for e. Initial-
ize the feature weights w. Then, repeatedly:
? Select a batch of input sentences f1, . . . , fm and
decode each fi to obtain a forest of translations.
? For each i, select from the forest a set of hy-
pothesis translations ei1, . . . , ein, which are the
219
10-best translations according to each of:
h(e) ? w
B???(e) + h(e) ? w
?B???(e) + h(e) ? w
(1)
? For each i, select an oracle translation:
e? = arg max
e
(B???(e) + h(e) ? w) (2)
Let ?hi j = h(e?i ) ? h(ei j).
? For each ei j, compute the loss
`i j = B???(e?i ) ? B???(ei j) (3)
? Update w to the value of w? that minimizes:
1
2?w
? ? w?2 + C
m?
i=1
max1? j?n(`i j ? ?hi j ? w
?) (4)
where C = 0.01. This minimization is per-
formed by a variant of sequential minimal opti-
mization (Platt, 1998).
Following Chiang et al (2008), we calculate the sen-
tence B??? scores in (1), (2), and (3) in the context
of some previous 1-best translations. We run 20 of
these learners in parallel, and when training is fin-
ished, the weight vectors from all iterations of all
learners are averaged together.
Since the interface between the trainer and the de-
coder is fairly simple?for each sentence, the de-
coder sends the trainer a forest, and the trainer re-
turns a weight update?it is easy to use this algo-
rithm with a variety of CKY-based decoders: here,
we are using it in conjunction with both the Hiero
decoder and our syntax-based decoder.
4 Features
In this section, we describe the new features intro-
duced on top of our baseline systems.
Discount features Both of our systems calculate
several features based on observed counts of rules in
the training data. Though the syntax-based system
uses Good-Turing discounting when computing the
P(e, c) feature, we find, as noted above, that it uses
quite a few one-count rules, suggesting that their
probabilities have been overestimated. We can di-
rectly attack this problem by adding features counti
that reward or punish rules seen i times, or features
count[i, j] for rules seen between i and j times.
4.1 Target-side features
String-to-tree MT offers some unique levers to pull,
in terms of target-side features. Because the system
outputs English trees, we can analyze output trees on
the tuning set and design new features to encourage
the decoder to produce more grammatical trees.
Rule overlap features While individual rules ob-
served in decoder output are often quite reasonable,
two adjacent rules can create problems. For exam-
ple, a rule that has a variable of type IN (preposi-
tion) needs another rule rooted with IN to fill the po-
sition. If the second rule supplies the wrong prepo-
sition, a bad translation results. The IN node here
is an overlap point between rules. Considering that
certain nonterminal symbols may be more reliable
overlap points than others, we create a binary fea-
ture for each nonterminal. A rule like:
IN(at)? zai
will have feature rule-root-IN set to 1 and all
other rule-root features set to 0. Our rule root fea-
tures range over the original (non-split) nontermi-
nal set; we have 105 in total. Even though the
rule root features are locally attached to individual
rules?and therefore cause no additional problems
for the decoder search?they are aimed at problem-
atic rule/rule interactions.
Bad single-level rewrites Sometimes the decoder
uses questionable rules, for example:
PP(x0:VBN x1:NP-C)? x0 x1
This rule is learned from 62 cases in our training
data, where the VBN is almost always the word
given. However, the decoder misuses this rule with
other VBNs. So we can add a feature that penalizes
any rule in which a PP dominates a VBN and NP-C.
The feature class bad-rewrite comprises penalties
for the following configurations based on our analy-
sis of the tuning set:
PP? VBN NP-C
PP-BAR? NP-C IN
VP? NP-C PP
CONJP? RB IN
220
Node count features It is possible that the de-
coder creates English trees with too many or too few
nodes of a particular syntactic category. For exam-
ple, there may be an tendency to generate too many
determiners or past-tense verbs. We therefore add a
count feature for each of the 109 (non-split) English
nonterminal symbols. For a rule like
NPB(NNP(us) NNP(president) x0:NNP)
? meiguo zongtong x0
the feature node-count-NPB gets value 1, node-
count-NNP gets value 2, and all others get 0.
Insertion features Among the rules we extract
from bilingual corpora are target-language insertion
rules, which have a word on the English side, but no
words on the source Chinese side. Sample syntax-
based insertion rules are:
NPB(DT(the) x0:NN)? x0
S(x0:NP-C VP(VBZ(is) x1:VP-C))? x0 x1
We notice that our decoder, however, frequently fails
to insert words like is and are, which often have no
equivalent in the Chinese source. We also notice that
the-insertion rules sometimes have a good effect, as
in the translation ?in the bloom of youth,? but other
times have a bad effect, as in ?people seek areas of
the conspiracy.?
Each time the decoder uses (or fails to use) an in-
sertion rule, it incurs some risk. There is no guaran-
tee that the interaction of the rule probabilities and
the language model provides the best way to manage
this risk. We therefore provide MIRA with a feature
for each of the most common English words appear-
ing in insertion rules, e.g., insert-the and insert-is.
There are 35 such features.
4.2 Source-side features
We now turn to features that make use of source-side
context. Although these features capture dependen-
cies that cross boundaries between rules, they are
still local in the sense that no new states need to
be added to the decoder. This is because the entire
source sentence, being fixed, is always available to
every feature.
Soft syntactic constraints Neither of our systems
uses source-side syntactic information; hence, both
could potentially benefit from soft syntactic con-
straints as described by Marton and Resnik (2008).
In brief, these features use the output of an in-
dependent syntactic parser on the source sentence,
rewarding decoder constituents that match syntac-
tic constituents and punishing decoder constituents
that cross syntactic constituents. We use separately-
tunable features for each syntactic category.
Structural distortion features Both of our sys-
tems have rules with variables that generalize over
possible fillers, but neither system?s basic model
conditions a rule application on the size of a filler,
making it difficult to distinguish long-distance re-
orderings from short-distance reorderings. To rem-
edy this problem, Chiang et al (2008) introduce a
structural distortion model, which we include in our
experiment. Our syntax-based baseline includes the
generative version of this model already.
Word context During rule extraction, we retain
word alignments from the training data in the ex-
tracted rules. (If a rule is observed with more than
one set of word alignments, we keep only the
most frequent one.) We then define, for each triple
( f , e, f+1), a feature that counts the number of times
that f is aligned to e and f+1 occurs to the right of
f ; and similarly for triples ( f , e, f?1) with f?1 occur-
ring to the left of f . In order to limit the size of the
model, we restrict words to be among the 100 most
frequently occurring words from the training data;
all other words are replaced with a token <unk>.
These features are somewhat similar to features
used by Watanabe et al (2007), but more in the spirit
of features used in the word sense disambiguation
model introduced by Lee and Ng (2002) and incor-
porated as a submodel of a translation system by
Chan et al (2007); here, we are incorporating some
of its features directly into the translation model.
5 Experiments
For our experiments, we used a 260 million word
Chinese/English bitext. We ran GIZA++ on the en-
tire bitext to produce IBM Model 4 word align-
ments, and then the link deletion algorithm (Fossum
et al, 2008) to yield better-quality alignments. For
221
System Training Features # Tune Test
Hiero MERT baseline 11 35.4 36.1
MIRA syntax, distortion 56 35.9 36.9?
syntax, distortion, discount 61 36.6 37.3??
all source-side, discount 10990 38.4 37.6??
Syntax MERT baseline 25 38.6 39.5
MIRA baseline 25 38.5 39.8?
overlap 132 38.7 39.9?
node count 136 38.7 40.0??
all target-side, discount 283 39.6 40.6??
Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM
B??? scores. ? or ?? = significantly better than MERT baseline (p < 0.05 or 0.01, respectively).
the syntax-based system, we ran a reimplementation
of the Collins parser (Collins, 1997) on the English
half of the bitext to produce parse trees, then restruc-
tured and relabeled them as described in Section 3.2.
Syntax-based rule extraction was performed on a 65
million word subset of the training data. For Hiero,
rules with up to two nonterminals were extracted
from a 38 million word subset and phrasal rules were
extracted from the remainder of the training data.
We trained three 5-gram language models: one on
the English half of the bitext, used by both systems,
one on one billion words of English, used by the
syntax-based system, and one on two billion words
of English, used by Hiero. Modified Kneser-Ney
smoothing (Chen and Goodman, 1998) was applied
to all language models. The language models are
represented using randomized data structures simi-
lar to those of Talbot et al (2007).
Our tuning set (2010 sentences) and test set (1994
sentences) were drawn from newswire data from the
NIST 2004 and 2005 evaluations and the GALE pro-
gram (with no overlap at either the segment or doc-
ument level). For the source-side syntax features,
we used the Berkeley parser (Petrov et al, 2006) to
parse the Chinese side of both sets.
We implemented the source-side context features
for Hiero and the target-side syntax features for the
syntax-based system, and the discount features for
both. We then ran MIRA on the tuning set with 20
parallel learners for Hiero and 73 parallel learners
for the syntax-based system. We chose a stopping it-
eration based on the B??? score on the tuning set,
and used the averaged feature weights from all iter-
Syntax-based Hiero
count weight count weight
1 +1.28 1 +2.23
2 +0.35 2 +0.77
3?5 ?0.73 3 +0.54
6?10 ?0.64 4 +0.29
5+ ?0.02
Table 2: Weights learned for discount features. Nega-
tive weights indicate bonuses; positive weights indicate
penalties.
ations of all learners to decode the test set.
The results (Table 1) show significant improve-
ments in both systems (p < 0.01) over already very
strong MERT baselines. Adding the source-side and
discount features to Hiero yields a +1.5 B??? im-
provement, and adding the target-side syntax and
discount features to the syntax-based system yields a
+1.1 B??? improvement. The results also show that
for Hiero, the various classes of features contributed
roughly equally; for the syntax-based system, we see
that two of the feature classes make small contribu-
tions but time constraints unfortunately did not per-
mit isolated testing of all feature classes.
6 Analysis
How did the various new features improve the trans-
lation quality of our two systems? We begin by ex-
amining the discount features. For these features,
we used slightly different schemes for the two sys-
tems, shown in Table 2 with their learned feature
weights. We see in both cases that one-count rules
are strongly penalized, as expected.
222
Reward
?0.42 a
?0.13 are
?0.09 at
?0.09 on
?0.05 was
?0.05 from
?0.04 ?s
?0.04 by
?0.04 is
?0.03 it
?0.03 its
...
Penalty
+0.67 of
+0.56 the
+0.47 comma
+0.13 period
+0.11 in
+0.08 for
+0.06 to
+0.05 will
+0.04 and
+0.02 as
+0.02 have
...
Table 3: Weights learned for inserting target English
words with rules that lack Chinese words.
6.1 Syntax features
Table 3 shows word-insertion feature weights. The
system rewards insertion of forms of be; examples
1?3 in Figure 1 show typical improved translations
that result. Among determiners, inserting a is re-
warded, while inserting the is punished. This seems
to be because the is often part of a fixed phrase, such
as the White House, and therefore comes naturally
as part of larger phrasal rules. Inserting the outside
these fixed phrases is a risk that the generative model
is too inclined to take. We also note that the system
learns to punish unmotivated insertions of commas
and periods, which get into our grammar via quirks
in the MT training data.
Table 4 shows weights for rule-overlap features.
MIRA punishes the case where rules overlap with
an IN (preposition) node. This makes sense: if a
rule has a variable that can be filled by any English
preposition, there is a risk that an incorrect preposi-
tion will fill it. On the other hand, splitting at a pe-
riod is a safe bet, and frees the model to use rules that
dig deeper into NP and VP trees when constructing
a top-level S. Table 5 shows weights for generated
English nonterminals: SBAR-C nodes are rewarded
and commas are punished.
The combined effect of all weights is subtle.
To interpret them further, it helps to look at gross
changes in the system?s behavior. For example, a
major error in the baseline system is to move ?X
said? or ?X asked? from the beginning of the Chi-
nese input to the middle or end of the English trans-
Bonus
?0.50 period
?0.39 VP-C
?0.36 VB
?0.31 SG-C
?0.30 MD
?0.26 VBG
?0.25 ADJP
?0.22 -LRB-
?0.21 VP-BAR
?0.20 NPB-BAR
?0.16 FRAG
?0.16 PRN
?0.15 NPB
?0.13 RB
?0.12 SBAR-C
?0.12 VP-C-BAR
?0.11 -RRB-
...
Penalty
+0.93 IN
+0.57 NNP
+0.44 NN
+0.41 DT
+0.34 JJ
+0.24 right double quote
+0.20 VBZ
+0.19 NP
+0.16 TO
+0.15 ADJP-BAR
+0.14 PRN-BAR
+0.14 NML
+0.13 comma
+0.12 VBD
+0.12 NNPS
+0.12 PRP
+0.11 SG
...
Table 4: Weights learned for employing rules whose En-
glish sides are rooted at particular syntactic categories.
Bonus
?0.73 SBAR-C
?0.54 VBZ
?0.54 IN
?0.52 NN
?0.51 PP-C
?0.47 right double quote
?0.39 ADJP
?0.34 POS
?0.31 ADVP
?0.30 RP
?0.29 PRT
?0.27 SG-C
?0.22 S-C
?0.21 NNPS
?0.21 VP-BAR
?0.20 PRP
?0.20 NPB-BAR
...
Penalty
+1.30 comma
+0.80 DT
+0.58 PP
+0.44 TO
+0.33 NNP
+0.30 NNS
+0.30 NML
+0.22 CD
+0.18 PRN
+0.16 SYM
+0.15 ADJP-BAR
+0.15 NP
+0.15 MD
+0.15 HYPH
+0.14 PRN-BAR
+0.14 NP-C
+0.11 ADJP-C
...
Table 5: Weights learned for generating syntactic nodes
of various types anywhere in the English translation.
223
lation. The error occurs with many speaking verbs,
and each time, we trace it to a different rule. The
problematic rules can even be non-lexical, e.g.:
S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.)
? x3 x4 x2 x0 x1 x5
It is therefore difficult to come up with a straightfor-
ward feature to address the problem. However, when
we apply MIRA with the features already listed,
these translation errors all disappear, as demon-
strated by examples 4?5 in Figure 1. Why does this
happen? It turns out that in translation hypotheses
that move ?X said? or ?X asked? away from the be-
ginning of the sentence, more commas appear, and
fewer S-C and SBAR-C nodes appear. Therefore, the
new features work to discourage these hypotheses.
Example 6 shows additionally that commas next to
speaking verbs are now correctly deleted.
Examples 7?8 in Figure 1 show other kinds of
unanticipated improvements. We do not have space
for a fuller analysis, but we note that the specific ef-
fects we describe above account for only part of the
overall B??? improvement.
6.2 Word context features
In Table 6 are shown feature weights learned for the
word-context features. A surprising number of the
highest-weighted features have to do with transla-
tions of dates and bylines. Many of the penalties
seem to discourage spurious insertion or deletion
of frequent words (for, ?s, said, parentheses, and
quotes). Finally, we note that several of the features
(the third- and eighth-ranked reward and twelfth-
ranked penalty) shape the translation of shuo ?said?,
preferring translations with an overt complementizer
that and without a comma. Thus these features work
together to attack a frequent problem that our target-
syntax features also addressed.
Figure 2 shows the performance of Hiero with all
of its features on the tuning and test sets over time.
The scores on the tuning set rise rapidly, and the
scores on the test set alo rise, but much more slowly,
and there appears to be slight degradation after the
18th pass through the tuning data. This seems in line
with the finding of Watanabe et al (2007) that with
on the order of 10,000 features, overfitting is possi-
ble, but we can still improve accuracy on new data.
 35
 35.5
 36
 36.5
 37
 37.5
 38
 38.5
 0  5  10  15  20  25
B
L
E
U
Epoch
Tune
Test
Figure 2: Using over 10,000 word-context features leads
to overfitting, but its detrimental effects are modest.
Scores on the tuning set were obtained from the 1-best
output of the online learning algorithm, whereas scores
on the test set were obtained using averaged weights.
Early stopping would have given +0.2 B??? over the
results reported in Table 1.1
7 Conclusion
We have described a variety of features for statisti-
cal machine translation and applied them to syntax-
based and hierarchical systems. We saw that these
features, discriminatively trained using MIRA, led
to significant improvements, and took a closer look
at the results to see how the new features qualita-
tively improved translation quality. We draw three
conclusions from this study.
First, we have shown that these new features can
improve the performance even of top-scoring MT
systems. Second, these results add to a growing body
of evidence that MIRA is preferable to MERT for
discriminative training. When training over 10,000
features on a modest amount of data, we, like Watan-
abe et al (2007), did observe overfitting, yet saw
improvements on new data. Third, we have shown
that syntax-based machine translation offers possi-
bilities for features not available in other models,
making syntax-based MT and MIRA an especially
strong combination for future work.
1It was this iteration, in fact, which was used to derive the
combined feature count used in the title of this paper.
224
1 MERT: the united states pending israeli clarification on golan settlement plan
MIRA: the united states is waiting for israeli clarification on golan settlement plan
2 MERT: . . . the average life expectancy of only 18 months , canada ?s minority goverment will . . .
MIRA: . . . the average life expectancy of canada?s previous minority government is only 18 months . . .
3 MERT: . . . since un inspectors expelled by north korea . . .
MIRA: . . . since un inspectors were expelled by north korea . . .
4 MERT: another thing is . . . , " he said , " obviously , the first thing we need to do . . . .
MIRA: he said : " obviously , the first thing we need to do . . . , and another thing is . . . . "
5 MERT: the actual timing . . . reopened in january , yoon said .
MIRA: yoon said the issue of the timing . . .
6 MERT: . . . us - led coalition forces , said today that the crash . . .
MIRA: . . . us - led coalition forces said today that a us military . . .
7 MERT: . . . and others will feel the danger .
MIRA: . . . and others will not feel the danger .
8 MERT: in residential or public activities within 200 meters of the region , . . .
MIRA: within 200 m of residential or public activities area , . . .
Figure 1: Improved syntax-based translations due to MIRA-trained weights.
Bonus
f e context
?1.19 <unk> <unk> f?1 = ri ?day?
?1.01 <unk> <unk> f?1 = (
?0.84 , that f?1 = shuo ?say?
?0.82 yue ?month? <unk> f+1 = <unk>
?0.78 " " f?1 = <unk>
?0.76 " " f+1 = <unk>
?0.66 <unk> <unk> f+1 = nian ?year?
?0.65 , that f+1 = <unk>
...
Penalty
f e context
+1.12 <unk> ) f+1 = <unk>
+0.83 jiang ?shall? be f+1 = <unk>
+0.83 zhengfu ?government? the f?1 = <unk>
+0.73 <unk> ) f?1 = <unk>
+0.73 <unk> ( f+1 = <unk>
+0.72 <unk> ) f?1 = ri ?day?
+0.70 <unk> ( f?1 = ri ?day?
+0.69 <unk> ( f?1 = <unk>
+0.66 <unk> for f?1 = <unk>
+0.66 <unk> ?s f?1 = ,
+0.65 <unk> said f?1 = <unk>
+0.60 , , f?1 = shuo ?say?
...
Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese
word f , with Chinese word f?1 to the left or f+1 to the right. Glosses for Chinese words are not part of features.
225
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical ma-
chine translation. In Proc. ACL-08: HLT.
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?312.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. ACL 2007.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proc. ACL 1997.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proc. EMNLP-CoNLL-2007.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In Proc. Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Proc. HLT-NAACL 2004, Boston, Massachusetts.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic models. In Proc. ACL 2006.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL 2008.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proceedings of the Sixth International
Conference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing).
Yoong Keok Lee and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation. In
Proc. EMNLP 2002, pages 41?48.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. COLING-ACL
2006.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uskoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. EMNLP 2008.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. ACL-08: HLT.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ACL 2002.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proc. HLT-NAACL 2004, pages 161?168.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL 2006.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in Kernel Methods: Support Vector Learn-
ing, pages 195?208. MIT Press.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proc. ACL 2007, pages 512?519.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. COLING-ACL 2006.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learn-
ing for natural language parsing and translation. In
Proc. NIPS 2006.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proc. EMNLP-CoNLL 2007.
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. EMNLP 2007.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proc. ACL 2002.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. HLT-NAACL 2006.
226
Proceedings of NAACL HLT 2009: Short Papers, pages 141?144,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Faster MT Decoding through Pervasive Laziness
Michael Pust and Kevin Knight
Information Sciences Institute
University of Southern California
lastname@isi.edu
Abstract
Syntax-based MT systems have proven
effective?the models are compelling and
show good room for improvement. However,
decoding involves a slow search. We present
a new lazy-search method that obtains signifi-
cant speedups over a strong baseline, with no
loss in Bleu.
1 Introduction
Syntax-based string-to-tree MT systems have
proven effective?the models are compelling and
show good room for improvement. However, slow
decoding hinders research, as most experiments
involve heavy parameter tuning, which involves
heavy decoding. In this paper, we present a new
method to improve decoding performance, obtain-
ing a significant speedup over a strong baseline with
no loss in Bleu. In scenarios where fast decoding is
more important than optimal Bleu, we obtain better
Bleu for the same time investment. Our baseline
is a full-scale syntax-based MT system with 245m
tree-transducer rules of the kind described in (Gal-
ley et al, 2004), 192 English non-terminal symbols,
an integrated 5-gram language model (LM), and
a decoder that uses state-of-the-art cube pruning
(Chiang, 2007). A sample translation rule is:
S(x0:NP x1:VP) ? x1:VP x0:NP
In CKY string-to-tree decoding, we attack spans
of the input string from shortest to longest. We pop-
ulate each span with a set of edges. An edge contains
a English non-terminal (NT) symbol (NP, VP, etc),
border words for LM combination, pointers to child
edges, and a score. The score is a sum of (1) the
left-child edge score, (2) the right-child edge score,
(3) the score of the translation rule that combined
them, and (4) the target-string LM score. In this pa-
per, we are only concerned with what happens when
constructing edges for a single span [i,j]. The naive
algorithm works like this:
for each split point k
for each edge A in span [i,k]
for each edge B in span [k,j]
for each rule R with RHS = A B
create new edge for span [i,j]
delete all but 1000-best edges
The last step provides a necessary beam. Without
it, edges proliferate beyond available memory and
time. But even with the beam, the naive algorithm
fails, because enumerating all <A,B,R> triples at
each span is too time consuming.
2 Cube Pruning
Cube pruning (Chiang, 2007) solves this problem by
lazily enumerating triples. To work, cube pruning
requires that certain orderings be continually main-
tained at all spans. First, rules are grouped by RHS
into rule sets (eg, all the NP-VP rules are in a set),
and the members of a given set are sorted by rule
score. Second, edges in a span are grouped by NT
into edge sets (eg, all the NP edges are in an edge
set), ordered by edge score.
Consider the sub-problem of building new [i,j]
edges by combining (just) the NP edges over [i,k]
with (just) the VP edges over [k,j], using the avail-
able NP-VP rules. Rather than enumerate all triples,
cube pruning sets up a 3-dimensional cube struc-
ture whose individually-sorted axes are the NP left
edges, the VP right edges, and the NP-VP rules. Be-
cause the corner of the cube (best NP left-edge, best
VP right-edge, best NP-VP rule) is likely the best
edge in the cube, at beam size 1, we would sim-
ply return this edge and terminate, without checking
other triples. We say ?likely? because the corner po-
sition does not take into account the LM portion of
the score.1
After we take the corner and post a new edge from
it, we identify its 3 neighbors in the cube. We com-
1We also employ LM rule and edge forward-heuristics as in
(Chiang, 2007), which improve the sorting.
141
pute their full scores (including LM portion) and
push them onto a priority queue (PQ). We then pop
an item from the PQ, post another new edge, and
push the item?s neighbors onto the PQ. Note that this
PQ grows in size over time. In this way, we explore
the best portion of the cube without enumerating all
its contents. Here is the algorithm:
push(corner, make-edge(corner)) onto PQ
for i = 1 to 1000
pop(position, edge) from top of PQ
post edge to chart
for each n in neighbors(position)
push(n, make-edge(n)) onto PQ
if PQ is empty, break from for-loop
The function make-edge completely scores an edge
(including LM score) before inserting it into the PQ.
Note that in practice, we execute the loop up to 10k
times, to get 1000 edges that are distinct in their NTs
and border words.
In reality, we have to construct many cubes, one
for each combinable left and right edge set for a
given split point, plus all the cubes for all the other
split points. So we maintain a PQ-of-PQs whose el-
ements are cubes.
create each cube, pushing its fully-scored corner
onto the cube?s PQ
push cubes themselves onto a PQ-of-PQs
for i = 1 to 1000:
pop a cube C from the PQ-of-PQs
pop an item from C
post edge to chart
retrieve neighbors, score & push them onto C
push C back onto the PQ-of-PQs
3 Lazy Lists
When we meter the cube pruning algorithm, we find
that over 80% of the time goes to building the initial
queue of cubes, including deriving a corner edge for
each cube?only a small fraction is spent deriving
additional edges via exploring the cubes. For spans
of length 10 or greater, we find that we have to create
more than 1000 cubes, i.e., more than the number of
edges we wish to explore.
Our idea, then, is to create the cubes themselves
lazily. To describe our algorithm, we exploit an ab-
stract data structure called a lazy list (aka generator,
stream, pipe, or iterator), which supports three oper-
ations:
next(list): pops the front item from a list
peek(list): returns the score of the front item
empty(list): returns true if the list is empty
A cube is a lazy list (of edges). For our purposes, a
lazy list can be implemented with a PQ or something
else?we no longer care how the list is populated or
maintained, or even whether there are a finite num-
ber of elements.
Instead of explicitly enumerating all cubes for a
span, we aim to produce a lazy list of cubes. As-
sume for the moment that such a lazy list exists?we
show how to create it in the next section?and call it
L. Let us also say that cubes come off L in order of
their top edges? scores. To get our first edge, we let
C = next(L), and then we call next(C). Now a ques-
tion arises: do we pop the next-best edge off C, or
do we investigate the next cube in L? We can decide
by calling peek(peek(L)). If we choose to pop the
next cube (and then its top edge), then we face an-
other (this time three-way) decision. Bookkeeping
is therefore required if we are to continue to emit
edges in a good order.
We manage the complexity through the abstrac-
tion of a lazy list of lazy lists, to which we routinely
apply a single, key operation called merge-lists. This
operation converts a lazy list of lazy lists of X?s into
a simple lazy list of X?s. X can be anything: edges,
integers, lists, lazy lists, etc.
Figure 1 gives the generic merge-lists algorithm.
The yield function suspends computation and re-
turns to the caller. peek() lets the caller see what is
yielded, next() returns what is yielded and resumes
the loop, and empty() tells if the loop is still active.
We are now free to construct any nested ?list of
lists of lists ... of lists of X? (all lazy) and reduce
it stepwise and automatically to a single lazy list.
Standard cube pruning (Section 2) provides a sim-
ple example: if L is a list of cubes, and each cube is
a lazy list of edges, then merge-lists(L) returns us a
lazy list of edges (M), which is exactly what the de-
coder wants. The decoder can populate a new span
by simply making 1000 calls to next(M).
4 Pervasive Laziness
Now we describe how to generate cubes lazily. As
with standard cube pruning, we need to maintain a
142
merge-lists(L):
(L is a lazy list of lazy lists)
1. set up an empty PQ of lists,
prioritized by peek(list)
2. push next(L) onto PQ
3. pop list L2 off PQ
4. yield pop(L2)
5. if !empty(L2) and peek(L2) is worse than
peek(peek(L)), then push next(L) onto PQ
6. if !empty(L2), then push L2 onto PQ
7. go to step 3
Figure 1: Generic merge-lists algorithm.
small amount of ordering information among edges
in a span, which we exploit in constructing higher-
level spans. Previously, we required that all NP
edges be ordered by score, the same for VP edges,
etc. Now we additionally order whole edge sets
(groups of edges sharing an NT) with respect to each
other, eg, NP > VP > RB > etc. These are ordered
by the top-scoring edges in each set.
Ideally, we would pop cubes off our lazy list in
order of their top edges. Recall that the PQ-of-PQs
in standard cube pruning works this way. We cannot
guarantee this anymore, so we approximate it.
Consider first a single edge set from [i,k], eg, all
the NP edges. We build a lazy list of cubes that all
have a left-NP. Because edge sets from [k,j] are or-
dered with respect to each other, we may find that
it is the VP edge set that contains the best edge in
[k,j]. Pulling in all NP-VP rules, we can now postu-
late a ?best cube,? which generates edges out of left-
NPs and right-VPs. We can either continue making
edge from this cube, or we can ask for a ?second-
best cube? by moving to the next edge set of [k,j],
which might contain all the right-PP edges. Thus,
we have a lazy list of left-NP cubes. Its ordering
is approximate?cubes come off in such a way that
their top edges go from best to worst, but only con-
sidering the left and right child scores, not the rule
scores. This is the same idea followed by standard
cube pruning when it ignores internal LM scores.
We next create similar lazy lists for all the other
[i,k] edge sets (not just NP). We combine these lists
into a higher-level lazy list, whose elements pop off
according to the ordering of edge sets in [i,k]. This
structure contains all edges that can be produced
	












	

		







Figure 2: Organizing lazy lists for the decoder.
from split point k. We call merge-lists recursively
on the structure, leaving us with a single lazy list M
of edges. The decoder can now make 1000 calls to
next(M) to populate the new span.
Edges from other split points, however, must
compete on an equal basis for those 1000 slots. We
therefore produce a separate lazy list for each of the
j ? i ? 1 split points and combine these into an
even higher-level list. Lacking an ordering criterion
among split points, we presently make the top list a
non-lazy one via the PQ-of-PQs structure. Figure 2
shows how our lists are organized.
The quality of our 1000-best edges can be im-
proved. When we organize the higher-level lists by
left edge-sets, we give prominence to the best left
edge-set (eg, NP) over others (eg, VP). If the left
span is relatively short, the contribution of the left
NP to the total score of the new edge is small, so
this prominence is misplaced. Therefore, we repeat
the above process with the higher-level lists orga-
nized by right span instead of left. We merge the
right-oriented and left-oriented structures, making
sure that duplicates are avoided.
Related Work. Huang and Chiang (2007) de-
143
5x108 1x109 1.5x109 2x109 2.5x109 3x109
edges created
42000
43000
44000
45000
m
od
el
 c
os
t
lazy cube generation
exhaustive cube generation
Figure 3: Number of edges produced by the decoder, ver-
sus model cost of 1-best decodings.
scribe a variation of cube pruning called cube grow-
ing, and they apply it to a source-tree to target-
string translator. It is a two pass approach, where
a context-free parser is used to build a source for-
est, and a top down lazy forest expansion is used to
integrate a language model. The expansion recur-
sively calls cubes top-down, in depth first order. The
context-free forest controls which cubes are built,
and acts as a heuristic to minimize the number of
items returned from each cube necessary to generate
k-best derivations at the top.
It is not clear that a decoder such as ours, without
the source-tree constraint, would benefit from this
method, as building a context-free forest consistent
with future language model integration via cubes is
expensive on its own. However, we see potential
integration of both methods in two places: First,
the merge-lists algorithm can be used to lazily pro-
cess any nested for-loops?including vanilla CKY?
provided the iterands of the loops can be priori-
tized. This could speed up the creation of a first-pass
context-free forest. Second, the cubes themselves
could be prioritized in a manner similar to what we
describe, using the context-free forest to prioritize
cube generation rather than antecedent edges in the
chart (since those do not exist yet).
5 Results
We compare our method with standard cube prun-
ing (Chiang, 2007) on a full-scale Arabic/English
syntax-based MT system with an integrated 5-gram
20000 40000 60000 80000
decode time (seconds)
51.2
51.4
51.6
51.8
52
52.2
52.4
52.6
52.8
53
bl
eu
lazy cube generation
exhaustive cube generation
Figure 4: Decoding time versus Bleu.
LM. We report on 500 test sentences of lengths 15-
35. There are three variables of interest: runtime,
model cost (summed across all sentences), and IBM
Bleu. By varying the beam sizes (up to 1350),
we obtain curves that plot edges-produced versus
model-cost, shown in Figure 3. Figure 4 plots Bleu
score against time. We see that we have improved
the way our decoder searches, by teaching it to ex-
plore fewer edges, without sacrificing its ability to
find low-cost edges. This leads to faster decoding
without loss in translation accuracy.
Taken together with cube pruning (Chiang, 2007),
k-best tree extraction (Huang and Chiang, 2005),
and cube growing (Huang and Chiang, 2007), these
results provide evidence that lazy techniques may
penetrate deeper yet into MT decoding and other
NLP search problems.
We would like to thank J. Graehl and D. Chiang
for thoughts and discussions. This work was par-
tially supported under DARPA GALE, Contract No.
HR0011-06-C-0022.
References
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule. In Proc. NAACL-HLT.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. IWPT.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc.
ACL.
144
Proceedings of NAACL HLT 2009: Short Papers, pages 253?256,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Combining Constituent Parsers
Victoria Fossum
Dept. of Computer Science
University of Michigan
Ann Arbor, MI 48104
vfossum@umich.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Combining the 1-best output of multiple
parsers via parse selection or parse hybridiza-
tion improves f-score over the best indi-
vidual parser (Henderson and Brill, 1999;
Sagae and Lavie, 2006). We propose three
ways to improve upon existing methods for
parser combination. First, we propose a
method of parse hybridization that recom-
bines context-free productions instead of con-
stituents, thereby preserving the structure of
the output of the individual parsers to a greater
extent. Second, we propose an efficient linear-
time algorithm for computing expected f-score
using Minimum Bayes Risk parse selection.
Third, we extend these parser combination
methods from multiple 1-best outputs to mul-
tiple n-best outputs. We present results on
WSJ section 23 and also on the English side
of a Chinese-English parallel corpus.
1 Introduction
Parse quality impacts the quality of downstream ap-
plications such as syntax-based machine translation
(Quirk and Corston-Oliver, 2006). Combining the
output of multiple parsers can boost the accuracy
of such applications. Parses can be combined in
two ways: parse selection (selecting the best parse
from the output of the individual parsers) or parse
hybridization (constructing the best parse by recom-
bining sub-sentential components from the output of
the individual parsers).
1.1 Related Work
(Henderson and Brill, 1999) perform parse selec-
tion by maximizing the expected precision of the
selected parse with respect to the set of parses be-
ing combined. (Henderson and Brill, 1999) and
(Sagae and Lavie, 2006) propose methods for parse
hybridization by recombining constituents.
1.2 Our Work
In this work, we propose three ways to improve upon
existing methods for parser combination.
First, while constituent recombination (Hender-
son and Brill, 1999; Sagae and Lavie, 2006) gives a
significant improvement in f-score, it tends to flatten
the structure of the individual parses. To illustrate,
Figures 1 and 2 contrast the output of the Charniak
parser with the output of constituent recombination
on a sentence from WSJ section 24. We recombine
context-free productions instead of constituents, pro-
ducing trees containing only context-free produc-
tions that have been seen in the individual parsers?
output (Figure 3).
Second, the parse selection method of (Hender-
son and Brill, 1999) selects the parse with maxi-
mum expected precision; here, we present an effi-
cient, linear-time algorithm for selecting the parse
with maximum expected f-score within the Mini-
mum Bayes Risk (MBR) framework.
Third, we extend these parser combination meth-
ods from 1-best outputs to n-best outputs. We
present results on WSJ section 23 and also on the
English side of a Chinese-English parallel corpus.
253
SBAR
IN
although
FRAG
RB
not
ADJP
RB
as
RB
sharply
PP
IN
as
NP
NP
DT
the
NN
gain
VP
VBN
reported
NP
NNP
Friday
Figure 1: Output of Charniak Parser
SBAR
IN
although
RB
not
RB
as
RB
sharply
IN
as
NP
DT
the
NN
gain
VP
VBN
reported
NP
NNP
Friday
Figure 2: Output of Constituent Recombination
2 Parse Selection
In the MBR framework, although the true reference
parse is unknown, we assume that the individual
parsers? output forms a reasonable distribution over
possible reference parses. We compute the expected
f-score of each parse tree pi using this distribution:
expected f(pi) =
?
pj
f(pi, pj) ? pr(pj)
where f(pi, pj) is the f-score of parse pi with
respect to parse pj and pr(pj) is the prior prob-
ability of parse pj . We estimate pr(pj) as fol-
lows: pr(pj) = pr(parserk) ? pr(pj |parserk),
where parserk is the parser generating pj . We
set pr(parserk) according to the proportion of sen-
tences in the development set for which the 1-best
output of parserk achieves the highest f-score of
any individual parser, breaking ties randomly.
When n = 1, pr(pj |parserk) = 1 for all pj ;
when n > 1 we must estimate pr(pj |parserk), the
distribution over parses in the n-best list output by
any given parser. We estimate this distribution us-
ing the model score, or log probability, given by
parserk to each entry pj in its n-best list:
pr(pj |parserk) = e
??scorej,k
?n
j?=1 e??scorej?,k
SBAR
IN
although
S
ADVP
ADVP
RB
not
RB
as
ADVP
RB
sharply
PP
IN
as
NP
DT
the
NN
gain
VP
VBN
reported
NP
NNP
Friday
Figure 3: Output of Context-Free Production Recombi-
nation
Parser wsj cedev test dev test
Berkeley 88.6 89.3 82.9 83.5(Petrov and Klein, 2007)
Bikel?Collins Model 2 87.0 88.2 81.2 80.6(Bikel, 2002)
Charniak 90.6 91.4 84.7 84.1(Charniak and Johnson, 2005)
Soricut?Collins Model 2 87.3 88.4 82.3 82.1(Soricut, 2004)
Stanford 85.4 86.4 81.3 80.1(Klein and Manning, 2003)
Table 1: F-Scores of 1-best Output of Individual Parsers
We tune ? on a development set to maximize f-
score,1 and select the parse pi with highest expected
f-score.
Computing exact expected f-score requires
O(m2) operations per sentence, where m is the
number of parses being combined. We can compute
an approximate expected f-score in O(m) time. To
do so, we compute expected precision for all parses
in O(m) time by associating with each unique
constituent ci a list of parses in which it occurs,
plus the total probability qi of those parses. For
each parse p associated with ci, we increment the
expected precision of that parse by qi/size(p). This
computation yields the same result as the O(m2)
algorithm. We carry out a similar operation for
expected recall. We then compute the harmonic
mean of expected precision and expected recall,
which closely approximates the true expected
f-score.
1A low value of ? creates a uniform distribution, while a
high value concentrates probability mass on the 1-best entry in
the n-best list. In practice, tuning ? produces a higher f-score
than setting ? to the value that exactly reproduces the individual
parser?s probability distribution.
254
Parse Selection: Minimum Bayes Risk
System wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R F
best individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parser
n=1 91.7 90.5 91.1 92.5 91.8 92.0 87.1 84.6 85.8 86.7 83.7 85.2
n=10 92.1 90.8 91.5 92.4 91.7 92.0 87.9 85.3 86.6 87.7 84.4 86.0
n=25 92.1 90.9 91.5 92.4 91.7 92.0 88.0 85.4 86.7 87.4 84.2 85.7
n=50 92.1 91.0 91.5 92.4 91.7 92.1 88.0 85.3 86.6 87.6 84.3 85.9
Table 2: Precision, Recall, and F-score Results from Parse Selection
3 Constituent Recombination
(Henderson and Brill, 1999) convert each parse into
constituents with syntactic labels and spans, and
weight each constituent by summing pr(parserk)
over all parsers k in whose output the constituent
appears. They include all constituents with weight
above a threshold t = m+12 , where m is the number
of input parses, in the combined parse.
(Sagae and Lavie, 2006) extend this method by
tuning t on a development set to maximize f-
score.2 They populate a chart with constituents
whose weight meets the threshold, and use a CKY-
style parsing algorithm to find the heaviest tree,
where the weight of a tree is the sum of its con-
stituents? weights. Parsing is not constrained by a
grammar; any context-free production is permitted.
Thus, the combined parses may contain context-free
productions not seen in the individual parsers? out-
puts. While this failure to preserve the structure of
individual parses does not affect f-score, it may hin-
der downstream applications.
To extend this method from 1-best to n-best
lists, we weight each constituent by summing
pr(parserk)?pr(pj |parserk) over all parses pj gen-
erated by parserk in which the constituent appears.
4 Context-Free Production Recombination
To ensure that all context-free productions in the
combined parses have been seen in the individual
parsers? outputs, we recombine context-free produc-
tions rather than constituents. We convert each parse
into context-free productions, labelling each con-
stituent in the production with its span and syntac-
tic category and weighting each production by sum-
2A high threshold results in high precision, while a low
threshold results in high recall.
ming pr(parserk) ? pr(pj |parserk) over all parses
pj generated by parserk in which the production ap-
pears. We re-parse the sentence with these produc-
tions, returning the heaviest tree (where the weight
of a tree is the sum of its context-free productions?
weights). We optimize f-score by varying the trade-
off between precision and recall using a derivation
length penalty, which we tune on a development
set.3
5 Experiments
Table 1 illustrates the 5 parsers used in our combi-
nation experiments and the f-scores of their 1-best
output on our data sets. We use the n-best output
of the Berkeley, Charniak, and Soricut parsers, and
the 1-best output of the Bikel and Stanford parsers.
All parsers were trained on the standard WSJ train-
ing sections. We use two corpora: the WSJ (sec-
tions 24 and 23 are the development and test sets, re-
spectively) and English text from the LDC2007T02
Chinese-English parallel corpus (the development
and test sets contain 400 sentences each).
6 Discussion & Conclusion
Results are shown in Tables 2, 3, and 4. On both
test sets, constituent recombination achieves the best
f-score (1.0 points on WSJ test and 2.3 points on
Chinese-English test), followed by context-free pro-
duction combination, then parse selection, though
the differences in f-score among the combination
methods are not statistically significant. Increasing
the n-best list size from 1 to 10 improves parse se-
lection and context-free production recombination,
3By subtracting higher(lower) values of this length penalty
from the weight of each production, we can encourage the com-
bination method to favor trees with shorter(longer) derivations
and therefore higher precision(recall) at the constituent level.
255
Parse Hybridization: Constituent Recombination
System wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R F
best individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parser
n=1 92.5 90.3 91.4 93.0 91.6 92.3 89.2 84.6 86.8 89.1 83.6 86.2
n=10 92.6 90.5 91.5 93.1 91.7 92.4 89.9 84.4 87.1 89.9 83.2 86.4
n=25 92.6 90.5 91.5 93.2 91.7 92.4 89.9 84.4 87.0 89.7 83.4 86.4
n=50 92.6 90.5 91.5 93.1 91.7 92.4 89.9 84.4 87.1 89.7 83.2 86.3
Table 3: Precision, Recall, and F-score Results from Constituent Recombination
Parse Hybridization: Context-Free Production Recombination
System wsj-dev wsj-test ce-dev ce-testP R F P R F P R F P R F
best individual 91.3 89.9 90.6 91.8 91.0 91.4 86.1 83.4 84.7 85.6 82.6 84.1parser
n=1 91.7 91.0 91.4 92.1 91.9 92.0 86.9 85.4 86.2 86.2 84.3 85.2
n=10 92.1 90.9 91.5 92.5 91.8 92.2 87.8 85.1 86.4 86.2 84.3 86.1
n=25 92.2 91.0 91.6 92.5 91.8 92.2 87.8 85.1 86.4 87.6 84.6 86.1
n=50 92.1 90.8 91.4 92.4 91.7 92.1 87.6 84.9 86.2 87.7 84.6 86.1
Table 4: Precision, Recall, and F-score Results from Context-Free Production Recombination
though further increasing n does not, in general,
help.4 Chinese-English test set f-score gets a bigger
boost from combination than WSJ test set f-score,
perhaps because the best individual parser?s baseline
f-score is lower on the out-of-domain data.
We have presented an algorithm for parse hy-
bridization by recombining context-free produc-
tions. While constituent recombination results in
the highest f-score of the methods explored, context-
free production recombination produces trees which
better preserve the syntactic structure of the indi-
vidual parses. We have also presented an efficient
linear-time algorithm for selecting the parse with
maximum expected f-score.
Acknowledgments
We thank Steven Abney, John Henderson, and
Kenji Sagae for helpful discussions. This research
was supported by DARPA (contract HR0011-06-C-
0022) and by NSF ITR (grant IIS-0428020).
4These diminishing gains in f-score as n increases reflect
the diminishing gains in f-score of the oracle parse produced by
each individual parser as n increases.
References
Daniel M. Bikel. 2004. Design of a Multi-lingual,
Parallel-processing Statistical Parsing Engine. In Pro-
ceedings of HLT.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25-70.
John C. Henderson and Eric Brill. 2000. Exploiting Di-
versity in Natural Language Processing: Combining
Parsers. In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL.
Chris Quirk and Simon Corston-Oliver. 2006. The Im-
pact of Parse Quality on Syntactically-Informed Statis-
tical Machine Translation. In Proceedings of EMNLP.
Kenji Sagae and Alon Lavie. 2006. Parser Combination
by Reparsing. In Proceedings of HLT-NAACL.
Radu Soricut. 2004. A Reimplementation of Collins?
Parsing Models. Technical report, Information Sci-
ences Institute, Department of Computer Science, Uni-
versity of Southern California.
256
Proceedings of NAACL HLT 2009: Tutorials, pages 15?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Writing Systems, Transliteration and Decipherment
Kevin Knight (USC/ISI)
Richard Sproat (CSLU/OHSU)
Description
Nearly all of the core data that computational linguists deal with is in the
form of text, which is to say that it consists of language data written (usually) in
the standard writing system for the language in question. Yet surprisingly little
is generally understood about how writing systems work. This tutorial will be
divided into three parts. In the first part we discuss the history of writing and
introduce a wide variety of writing systems, explaining their structure and how
they encode language. We end this section with a brief review of how some of
the properties of writing systems are handled in modern encoding systems, such
as Unicode, and some of the continued pitfalls that can occur despite the best
intentions of standardization. The second section of the tutorial will focus on the
problem of transcription between scripts (often termed ?transliteration?), and
how this problem?which is important both for machine translation and named
entity recognition?has been addressed. The third section is more theoretical
and, at the same time we hope, more fun. We will discuss the problem of
decipherment and how computational methods might be brought to bear on
the problem of unlocking the mysteries of as yet undeciphered ancient scripts.
We start with a brief review of three famous cases of decipherment. We then
discuss how techniques that have been used in speech recognition and machine
translation might be applied to the problem of decipherment. We end with a
survey of the as-yet undeciphered ancient scripts and give some sense of the
prospects of deciphering them given currently available data.
Outline
First hour:
? History of writing
? Survey of writing systems and how they work
? Modern encodings
Second hour:
? Problems of transcription (transliteration)
? Generative models of transcription
Break
15
? More on generative models of transcription
? Discriminative models
Third Hour
? Famous cases of decipherment
? Prospects for ?autodecipherment?
? What?s left to decipher?
Target Audience
This tutorial will be of interest to anyone who wishes to have a better under-
standing of how writing (the form of language that most computational linguists
deal with) works, and how such problems as transcription (transliteration) and
decipherment are approached computationally.
Bios
Kevin Knight is a Research Associate Professor in Computer Science at
the University of Southern California, a Senior Research Scientist and Fellow
at the USC/Information Sciences Institute, and Chief Scientist at Language
Weaver. Dr. Knight received a Ph.D. from Carnegie Mellon University in 1992,
and a bachelor?s degree from Harvard University. His current interests include
better statistical machine translation through linguistics, and he is also working
on exploiting cryptographic techniques to solve hard translation problems.
Richard Sproat received his Ph.D. in Linguistics from the Massachusetts
Institute of Technology in 1985. Since then he has worked at AT&T Bell Labs,
at Lucent?s Bell Labs and at AT&T Labs ? Research, before joining the faculty
of the University of Illinois, and subsequently the Oregon Health & Science Uni-
versity. Sproat has worked in numerous areas relating to language and compu-
tational linguistics, including syntax, morphology, computational morphology,
articulatory and acoustic phonetics, text processing, text-to-speech synthesis,
writing systems, and text-to-scene conversion.
16
Fast Decoding and Optimal Decoding for Machine Translation
Ulrich Germann   , Michael Jahr  , Kevin Knight   , Daniel Marcu   , and Kenji Yamada  
 
Information Sciences Institute  Department of Computer Science
University of Southern California Stanford University
4676 Admiralty Way, Suite 1001 Stanford, CA 94305
Marina del Rey, CA 90292 jahr@cs.stanford.edu

germann,knight,marcu,kyamada  @isi.edu
Abstract
A good decoding algorithm is critical
to the success of any statistical machine
translation system. The decoder?s job is
to find the translation that is most likely
according to set of previously learned
parameters (and a formula for combin-
ing them). Since the space of possi-
ble translations is extremely large, typ-
ical decoding algorithms are only able
to examine a portion of it, thus risk-
ing to miss good solutions. In this pa-
per, we compare the speed and out-
put quality of a traditional stack-based
decoding algorithm with two new de-
coders: a fast greedy decoder and a
slow but optimal decoder that treats de-
coding as an integer-programming opti-
mization problem.
1 Introduction
A statistical MT system that translates (say)
French sentences into English, is divided into
three parts: (1) a language model (LM) that as-
signs a probability P(e) to any English string, (2) a
translation model (TM) that assigns a probability
P(f  e) to any pair of English and French strings,
and (3) a decoder. The decoder takes a previ-
ously unseen sentence  and tries to find the 
that maximizes P(e  f), or equivalently maximizes
P(e)  P(f  e).
Brown et al (1993) introduced a series of
TMs based on word-for-word substitution and re-
ordering, but did not include a decoding algo-
rithm. If the source and target languages are con-
strained to have the same word order (by choice
or through suitable pre-processing), then the lin-
ear Viterbi algorithm can be applied (Tillmann et
al., 1997). If re-ordering is limited to rotations
around nodes in a binary tree, then optimal decod-
ing can be carried out by a high-polynomial algo-
rithm (Wu, 1996). For arbitrary word-reordering,
the decoding problem is NP-complete (Knight,
1999).
A sensible strategy (Brown et al, 1995; Wang
and Waibel, 1997) is to examine a large subset of
likely decodings and choose just from that. Of
course, it is possible to miss a good translation
this way. If the decoder returns e  but there exists
some e for which P(e  f) 	 P(e   f), this is called
a search error. As Wang and Waibel (1997) re-
mark, it is hard to know whether a search error
has occurred?the only way to show that a decod-
ing is sub-optimal is to actually produce a higher-
scoring one.
Thus, while decoding is a clear-cut optimiza-
tion task in which every problem instance has a
right answer, it is hard to come up with good
answers quickly. This paper reports on mea-
surements of speed, search errors, and translation
quality in the context of a traditional stack de-
coder (Jelinek, 1969; Brown et al, 1995) and two
new decoders. The first is a fast greedy decoder,
and the second is a slow optimal decoder based on
generic mathematical programming techniques.
2 IBM Model 4
In this paper, we work with IBM Model 4, which
revolves around the notion of a word alignment
over a pair of sentences (see Figure 1). A word
alignment assigns a single home (English string
position) to each French word. If two French
words align to the same English word, then that
it is not clear .
| \ | \ \
| \ + \ \
| \/ \ \ \
| /\ \ \ \
CE NE EST PAS CLAIR .
Figure 1: Sample word alignment.
English word is said to have a fertility of two.
Likewise, if an English word remains unaligned-
to, then it has fertility zero. The word align-
ment in Figure 1 is shorthand for a hypothetical
stochastic process by which an English string gets
converted into a French string. There are several
sets of decisions to be made.
First, every English word is assigned a fertil-
ity. These assignments are made stochastically
according to a table n( 
 e  ). We delete from
the string any word with fertility zero, we dupli-
cate any word with fertility two, etc. If a word has
fertility greater than zero, we call it fertile. If its
fertility is greater than one, we call it very fertile.
After each English word in the new string, we
may increment the fertility of an invisible En-
glish NULL element with probability p  (typi-
cally about 0.02). The NULL element ultimately
produces ?spurious? French words.
Next, we perform a word-for-word replace-
ment of English words (including NULL) by
French words, according to the table t(f e  ).
Finally, we permute the French words. In per-
muting, Model 4 distinguishes between French
words that are heads (the leftmost French word
generated from a particular English word), non-
heads (non-leftmost, generated only by very fer-
tile English words), and NULL-generated.
Heads. The head of one English word is as-
signed a French string position based on the po-
sition assigned to the previous English word. If
an English word e  translates into something
at French position j, then the French head word
of e  is stochastically placed in French position
k with distortion probability d  (k?j  class(e ),
class(f  )), where ?class? refers to automatically
determined word classes for French and English
vocabulary items. This relative offset k?j encour-
ages adjacent English words to translate into ad-
jacent French words. If e  is infertile, then j is
taken from e  , etc. If e  is very fertile, then j
is the average of the positions of its French trans-
lations.
Non-heads. If the head of English word e 
is placed in French position j, then its first non-
head is placed in French position k ( 	 j) accord-
ing to another table d  (k?j  class(f  )). The next
non-head is placed at position q with probability
d  (q?k  class(f  )), and so forth.
NULL-generated. After heads and non-heads
are placed, NULL-generated words are permuted
into the remaining vacant slots randomly. If there
are 
 NULL-generated words, then any place-
ment scheme is chosen with probability 1/ 
 ffA Syntax-based Statistical Translation Model
Kenji Yamada and Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
 
kyamada,knight  @isi.edu
Abstract
We present a syntax-based statistical
translation model. Our model trans-
forms a source-language parse tree
into a target-language string by apply-
ing stochastic operations at each node.
These operations capture linguistic dif-
ferences such as word order and case
marking. Model parameters are esti-
mated in polynomial time using an EM
algorithm. The model produces word
alignments that are better than those
produced by IBM Model 5.
1 Introduction
A statistical translation model (TM) is a mathe-
matical model in which the process of human-
language translation is statistically modeled.
Model parameters are automatically estimated us-
ing a corpus of translation pairs. TMs have been
used for statistical machine translation (Berger et
al., 1996), word alignment of a translation cor-
pus (Melamed, 2000), multilingual document re-
trieval (Franz et al, 1999), automatic dictionary
construction (Resnik and Melamed, 1997), and
data preparation for word sense disambiguation
programs (Brown et al, 1991). Developing a bet-
ter TM is a fundamental issue for those applica-
tions.
Researchers at IBM first described such a sta-
tistical TM in (Brown et al, 1988). Their mod-
els are based on a string-to-string noisy channel
model. The channel converts a sequence of words
in one language (such as English) into another
(such as French). The channel operations are
movements, duplications, and translations, ap-
plied to each word independently. The movement
is conditioned only on word classes and positions
in the string, and the duplication and translation
are conditioned only on the word identity. Math-
ematical details are fully described in (Brown et
al., 1993).
One criticism of the IBM-style TM is that it
does not model structural or syntactic aspects of
the language. The TM was only demonstrated for
a structurally similar language pair (English and
French). It has been suspected that a language
pair with very different word order such as En-
glish and Japanese would not be modeled well by
these TMs.
To incorporate structural aspects of the lan-
guage, our channel model accepts a parse tree as
an input, i.e., the input sentence is preprocessed
by a syntactic parser. The channel performs oper-
ations on each node of the parse tree. The oper-
ations are reordering child nodes, inserting extra
words at each node, and translating leaf words.
Figure 1 shows the overview of the operations of
our model. Note that the output of our model is a
string, not a parse tree. Therefore, parsing is only
needed on the channel input side.
The reorder operation is intended to model
translation between languages with different word
orders, such as SVO-languages (English or Chi-
nese) and SOV-languages (Japanese or Turkish).
The word-insertion operation is intended to cap-
ture linguistic differences in specifying syntactic
cases. E.g., English and French use structural po-
sition to specify case, while Japanese and Korean
use case-marker particles.
Wang (1998) enhanced the IBM models by in-
troducing phrases, and Och et al (1999) used
templates to capture phrasal sequences in a sen-
tence. Both also tried to incorporate structural as-
pects of the language, however, neither handles
1. Channel Input
3.  Inserted
     
2. Reordered
  	   
 	  kare ha ongaku wo kiku no ga daisuki desu
5. Channel Output
      
  	          	   
4. Translated
       
A Decoder for Syntax-based Statistical MT
Kenji Yamada and Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
 
kyamada,knight  @isi.edu
Abstract
This paper describes a decoding algorithm
for a syntax-based translation model (Ya-
mada and Knight, 2001). The model
has been extended to incorporate phrasal
translations as presented here. In con-
trast to a conventional word-to-word sta-
tistical model, a decoder for the syntax-
based model builds up an English parse
tree given a sentence in a foreign lan-
guage. As the model size becomes huge in
a practical setting, and the decoder consid-
ers multiple syntactic structures for each
word alignment, several pruning tech-
niques are necessary. We tested our de-
coder in a Chinese-to-English translation
system, and obtained better results than
IBM Model 4. We also discuss issues con-
cerning the relation between this decoder
and a language model.
1 Introduction
A statistical machine translation system based on the
noisy channel model consists of three components:
a language model (LM), a translation model (TM),
and a decoder. For a system which translates from
a foreign language  to English  , the LM gives
a prior probability P  and the TM gives a chan-
nel translation probability P 	 
 . These models
are automatically trained using monolingual (for the
LM) and bilingual (for the TM) corpora. A decoder
then finds the best English sentence given a foreign
sentence that maximizes P  
 , which also maxi-
mizes P 	 
 according to Bayes? rule.
A different decoder is needed for different choices
of LM and TM. Since P  and P  
 are not sim-
ple probability tables but are parameterized models,
a decoder must conduct a search over the space de-
fined by the models. For the IBM models defined
by a pioneering paper (Brown et al, 1993), a de-
coding algorithm based on a left-to-right search was
described in (Berger et al, 1996). Recently (Ya-
mada and Knight, 2001) introduced a syntax-based
TM which utilized syntactic structure in the chan-
nel input, and showed that it could outperform the
IBM model in alignment quality. In contrast to the
IBM models, which are word-to-word models, the
syntax-based model works on a syntactic parse tree,
so the decoder builds up an English parse tree 
given a sentence  in a foreign language. This pa-
per describes an algorithm for such a decoder, and
reports experimental results.
Other statistical machine translation systems such
as (Wu, 1997) and (Alshawi et al, 2000) also pro-
duce a tree  given a sentence  . Their models are
based on mechanisms that generate two languages
at the same time, so an English tree  is obtained
as a subproduct of parsing  . However, their use of
the LM is not mathematically motivated, since their
models do not decompose into P 	 
 and 
unlike the noisy channel model.
Section 2 briefly reviews the syntax-based TM,
and Section 3 describes phrasal translation as an ex-
tension. Section 4 presents the basic idea for de-
coding. As in other statistical machine translation
systems, the decoder has to cope with a huge search
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 303-310.
                         Proceedings of the 40th Annual Meeting of the Association for
space. Section 5 describes how to prune the search
space for practical decoding. Section 6 shows exper-
imental results. Section 7 discusses LM issues, and
is followed by conclusions.
2 Syntax-based TM
The syntax-based TM defined by (Yamada and
Knight, 2001) assumes an English parse tree  as
a channel input. The channel applies three kinds of
stochastic operations on each node  : reordering
children nodes (  ), inserting an optional extra word
to the left or right of the node (  ), and translating
leaf words (  ).1 These operations are independent
of each other and are conditioned on the features
(  , ,  ) of the node. Figure 1 shows an example.
The child node sequence of the top node VB is re-
ordered from PRP-VB1-VB2 into PRP-VB2-VB1
as seen in the second tree (Reordered). An extra
word ha is inserted at the leftmost node PRP as seen
in the third tree (Inserted). The English word He un-
der the same node is translated into a foreign word
kare as seen in the fourth tree (Translated). After
these operations, the channel emits a foreign word
sentence  by taking the leaves of the modified tree.
Formally, the channel probability P Translating Named Entities Using Monolingual and Bilingual Resources
Yaser Al-Onaizan and Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
 
yaser,knight  @isi.edu
Abstract
Named entity phrases are some of the
most difficult phrases to translate because
new phrases can appear from nowhere,
and because many are domain specific, not
to be found in bilingual dictionaries. We
present a novel algorithm for translating
named entity phrases using easily obtain-
able monolingual and bilingual resources.
We report on the application and evalua-
tion of this algorithm in translating Arabic
named entities to English. We also com-
pare our results with the results obtained
from human translations and a commer-
cial system for the same task.
1 Introduction
Named entity phrases are being introduced in news
stories on a daily basis in the form of personal
names, organizations, locations, temporal phrases,
and monetary expressions. While the identifica-
tion of named entities in text has received sig-
nificant attention (e.g., Mikheev et al (1999) and
Bikel et al (1999)), translation of named entities
has not. This translation problem is especially
challenging because new phrases can appear from
nowhere, and because many named-entities are do-
main specific, not to be found in bilingual dictionar-
ies.
A system that specializes in translating named en-
tities such as the one we describe here would be an
important tool for many NLP applications. Statisti-
cal machine translation systems can use such a sys-
tem as a component to handle phrase translation in
order to improve overall translation quality. Cross-
Lingual Information Retrieval (CLIR) systems could
identify relevant documents based on translations
of named entity phrases provided by such a sys-
tem. Question Answering (QA) systems could ben-
efit substantially from such a tool since the answer
to many factoid questions involve named entities
(e.g., answers to who questions usually involve Per-
sons/Organizations, where questions involve Loca-
tions, and when questions involve Temporal Ex-
pressions).
In this paper, we describe a system for Arabic-
English named entity translation, though the tech-
nique is applicable to any language pair and does
not require especially difficult-to-obtain resources.
The rest of this paper is organized as follows. In
Section 2, we give an overview of our approach. In
Section 3, we describe how translation candidates
are generated. In Section 4, we show how mono-
lingual clues are used to help re-rank the translation
candidates list. In Section 5, we describe how the
candidates list can be extended using contextual in-
formation. We conclude this paper with the evalua-
tion results of our translation algorithm on a test set.
We also compare our system with human translators
and a commercial system.
2 Our Approach
The frequency of named-entity phrases in news text
reflects the significance of the events they are associ-
ated with. When translating named entities in news
stories of international importance, the same event
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 400-408.
                         Proceedings of the 40th Annual Meeting of the Association for
will most likely be reported in many languages in-
cluding the target language. Instead of having to
come up with translations for the named entities of-
ten with many unknown words in one document,
sometimes it is easier for a human to find a docu-
ment in the target language that is similar to, but not
necessarily a translation of, the original document
and then extract the translations. Let?s illustrate this
idea with the following example:
2.1 Example
We would like to translate the named entities that
appear in the following Arabic excerpt:
   	

 Feature-Rich Statistical Translation of Noun Phrases
Philipp Koehn and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
koehn@isi.edu, knight@isi.edu
Abstract
We define noun phrase translation as a
subtask of machine translation. This en-
ables us to build a dedicated noun phrase
translation subsystem that improves over
the currently best general statistical ma-
chine translation methods by incorporat-
ing special modeling and special features.
We achieved 65.5% translation accuracy
in a German-English translation task vs.
53.2% with IBM Model 4.
1 Introduction
Recent research in machine translation challenges
us with the exciting problem of combining statisti-
cal methods with prior linguistic knowledge. The
power of statistical methods lies in the quick acquisi-
tion of knowledge from vast amounts of data, while
linguistic analysis both provides a fitting framework
for these methods and contributes additional knowl-
edge sources useful for finding correct translations.
We present work that successfully defines a sub-
task of machine translation: the translation of noun
phrases. We demonstrate through analysis and ex-
periments that it is feasible and beneficial to treat
noun phrase translation as a subtask. This opens the
path to dedicated modeling of other types of syn-
tactic constructs, e.g., verb clauses, where issues of
subcategorization of the verb play a big role.
Focusing on a narrower problem allows not only
more dedicated modeling, but also the use of com-
putationally more expensive methods.
We go on to tackle the task of noun phrase trans-
lation in a maximum entropy reranking framework.
Treating translation as a reranking problem instead
of as a search problem enables us to use features
over the full translation pair. We integrate both em-
pirical and symbolic knowledge sources as features
into our system which outperforms the best known
methods in statistical machine translation.
Previous work on defining subtasks within sta-
tistical machine translation has been performed on,
e.g., noun-noun pair (Cao and Li, 2002) and named
entity translation (Al-Onaizan and Knight, 2002).
2 Noun Phrase Translation as a Subtask
In this work, we consider both noun phrases and
prepositional phrases, which we will refer to as
NP/PPs. We include prepositional phrases for a
number of reasons. Both are attached at the clause
level. Also, the translation of the preposition of-
ten depends heavily on the noun phrase (in the
morning). Moreover, the distinction between noun
phrases and prepositional phrases is not always clear
(note the Japanese bunsetsu) or hard to separate
(German joining of preposition and determiner into
one lexical unit, e.g., ins   in das  in the).
2.1 Definition
We define the NP/PPs in a sentence as follows:
Given a sentence  and its syntactic parse
tree  , the NP/PPs of the sentence  are the
subtrees  that contain at least one noun
and no verb, and are not part of a larger
subtree that contains no verb.
NP/PP
NP/PP
S
NP VP
DT NNP NN
the Bush administration
VBZ VP
has VBN VP
decided TO VP
to VB NP
renounce NP PP
DT NN
any involvement
IN DT-A NN
in a treaty
Figure 1: The noun phrases and preposition phrases (NP/PPs) addressed in this work
The NP/PPs are the maximal noun phrases of the
sentence, not just the base NPs. This definition ex-
cludes NP/PPs that consist of only a pronoun. It also
excludes noun phrases that contain relative clauses.
NP/PPs may have connectives such as and.
For an illustration, see Figure 1.
2.2 Translation of NP/PPs
To understand the behavior of noun phrases in the
translation process, we carried out a study to exam-
ine how they are translated in a typical parallel cor-
pus. Clearly, we cannot simply expect that certain
syntactic types in one language translate to equiv-
alent types in another language. Equivalent types
might not even exist.
This study answers the questions:
  Do human translators translate noun phrases in
foreign texts into noun phrases in English?
  If all noun phrases in a foreign text are trans-
lated into noun phrases in English, is an accept-
able sentence translation possible?
  What are the properties of noun phrases which
cannot be translated as noun phrases without
rendering the overall sentence translation unac-
ceptable?
Using the Europarl corpus1 , we consider a trans-
lation task from German to English. We marked the
NP/PPs in the German side of a small 100 sentence
parallel corpus manually. This yielded 168 NP/PPs
according to our definition.
We examined if these units are realized as noun
phrases in the English side of the parallel corpus.
This is the case for 75% of the NP/PPs.
Second, we tried to construct translations of these
NP/PPs that take the form of NP/PPs in English in
an overall acceptable translation of the sentence. We
could do this for 98% of the NP/PPs.
The four exceptions are:
  in Anspruch genommen; Gloss: take in demand
  Abschied nehmen; take good-bye
  meine Zustimmung geben; give my agreement
  in der Hauptsache; in the main-thing
The first three cases are noun phrases or preposi-
tional phrases that merge with the verb. This is simi-
lar to the English construction make an observation,
which translates best into some languages as a verb
equivalent to observe. The fourth example, literally
translated as in the main thing, is best translated as
mainly.
1Available at http://www.isi.edu/  koehn/
Why is there such a considerable discrepancy be-
tween the number of noun phrases that can be trans-
lated as noun phrases into English and noun phrases
that are translated as noun phrases?
The main reason is that translators generally try
to translate the meaning of a sentence, and do not
feel bound to preserve the same syntactic structure.
This leads them to sometimes arbitrarily restructure
the sentence. Also, occasionally the translations are
sloppy.
The conclusion of this study is: Most NP/PPs in
German are translated to English as NP/PPs. Nearly
all of them, 98%, can be translated as NP/PPs into
English. The exceptions to this rule should be
treated as special cases and handled separately.
We carried out studies for Chinese-English and
Portuguese-English NP/PPs with similar results.
2.3 The Role of External Context
One interesting question is if external context is nec-
essary for the translation of noun phrases. While the
sentence and document context may be available to
the NP/PP subsystem, the English output is only as-
sembled later and therefore harder to integrate.
To address this issue, we carried out a manual ex-
periment to check if humans can translate NP/PPs
without any external context. Using the same corpus
of 168 NP/PPs as in the previous section, a human
translator translated 89% of the noun phrases cor-
rectly, 9% had the wrong leading preposition, and
only 2% were mistranslated with the wrong content
word meaning.
Picking the right phrase start (e.g., preposition or
determiner) can sometimes only be resolved when
the English verb is chosen and its subcategoriza-
tion is known. Otherwise, sentence context does
not play a big role: Word choice can almost always
be resolved within the internal context of the noun
phrase.
2.4 Integration into an MT System
The findings of the previous section indicate that
NP/PP translation can be conceived as a separate
subsystem of a complete machine translation system
? with due attention to special cases. We will now
estimate the importance of such a system.
As a general observation, we note that NP/PPs
cover roughly half of the words in news or similar
System Correct BLEU
Basic MT system 7% 0.16
NP/PPs translated in isolation 8% 0.17
Perfect NP/PP translation 24% 0.35
Table 1: Integration of an NP/PP subsystem: Correct
sentence translations and BLEU score
texts. All nouns are covered by NP/PPs. Nouns are
the biggest group of open class words, in terms of
the number of distinct words. Constantly, new nouns
are added to the vocabulary of a language, be it by
borrowing foreign words such as Fahrvergnu?gen or
Zeitgeist, or by creating new words from acronyms
such as AIDS, or by other means. In addition to
new words, new phrases with distinct meanings are
constantly formed: web server, home page, instant
messaging, etc. Learning new concepts from text
sources when they become available is an elegant
solution for this knowledge acquisition problem.
In a preliminary study, we assess the impact of an
NP/PP subsystem on the quality of an overall ma-
chine translation system. We try to answer the fol-
lowing questions:
  What is the impact on a machine translation
system if noun phrases are translated in isola-
tion?
  What is the performance gain for a machine
translation system if an NP/PP subsystem pro-
vides perfect translations of the noun phrases?
We built a subsystem for NP/PP translation that
uses the same modeling as the overall system (IBM
Model 4), but is trained on only NP/PPs. With this
system, we translate the NP/PPs in isolation, with-
out the assistance of sentence context. These trans-
lations are fixed and provided to the general machine
translation system, which does not change the fixed
NP/PP translation.
In a different experiment, we also provided cor-
rect translations (motivated by the reference transla-
tion) for the NP/PPs to the general machine trans-
lation system. We carried out these experiments on
the same 100 sentence corpus as in the previous sec-
tions. The 164 translatable NP/PPs are marked and
translated in isolation.
The results are summarized in Table 1. Treating
NP/PPs as isolated units, and translating them in iso-
features
Reranker
translation
features
Model
n-best list features features
Figure 2: Design of the noun phrase translation sub-
system: The base model generates an n-best list that
is rescored using additional features
lation with the same methods as the overall system
has little impact on overall translation quality. In
fact, we achieved a slight improvement in results
due to the fact that NP/PPs are consistently trans-
lated as NP/PPs. A perfect NP/PP subsystem would
triple the number of correctly translated sentences.
Performance is also measured by the BLEU score
(Papineni et al, 2002), which measures similarity to
the reference translation taken from the English side
of the parallel corpus.
These findings indicate that solving the NP/PP
translation problem would be a significant step to-
ward improving overall translation quality, even if
the overall system is not changed in any way. The
findings also indicate that isolating the NP/PP trans-
lation task as a subtask does not harm performance.
3 Framework
When translating a foreign input sentence, we detect
its NP/PPs and translate them with an NP/PP trans-
lation subsystem. The best translation (or multiple
best translations) is then passed on to the full sen-
tence translation system which in turn translates the
remaining parts of the sentence and integrates the
chosen NP/PP translations.
Our NP/PP translation subsystem is designed as
follows: We train a translation system on a NP/PP
parallel corpus. We use this system to generate an
n-best list of possible translations. We then rescore
this n-best list with the help of additional features.
This design is illustrated by Figure 2.
3.1 Evaluation
To evaluate our methods, we automatically detected
all of the 1362 NP/PPs in 534 sentences from parts
of the Europarl corpus which are not already used
as training data. Our evaluation metric is human as-
sessment: Can the translation provided by the sys-
tem be part of an acceptable translation of the whole
sentence? In other words, the noun phrase has to be
translated correctly given the sentence context.
The NP/PPs are extracted in the same way that
NP/PPs are initially detected for the acquisition of
the NP/PP training corpus. This means that there
are some problems with parse errors, leading to sen-
tence fragments extracted as NP/PPs that cannot be
translated correctly. Also, the test corpus contains
all detected NP/PPs, even untranslatable ones, as
discussed in Section 2.2.
3.2 Acquisition of an NP/PP Training Corpus
To train a statistical machine translation model, we
need a training corpus of NP/PPs paired with their
translation. We create this corpus by extracting
NP/PPs from a parallel corpus.
First, we word-align the corpus with Giza++ (Och
and Ney, 2000). Then, we parse both sides with syn-
tactic parsers (Collins, 1997; Schmidt and Schulte
im Walde, 2000)2. Our definition easily translates
into an algorithm to detect NP/PPs in a sentence.
Recall that in such a corpus, only part of the
NP/PPs are translated as such into the foreign lan-
guage. In addition, the word-alignment and syntac-
tic parses may be faulty. As a consequence, initially
only 43.4% of all NP/PPs could be aligned. We raise
this number to 67.2% with a number of automatic
data cleaning steps:
  NP/PPs that partially align are broken up
  Systematic parse errors are fixed
  Certain word types that are inconsistently
tagged as nouns in the two languages are har-
monized (e.g., the German wo and the English
today).
  Because adverb + NP/PP constructions (e.g.,
specifically this issue are inconsistently parsed,
2English parser available at http://www.ai.mit.
edu/people/mcollins/code.html, German parser
available at http://www.ims.uni-stuttgart.de/
projekte/gramotron/SOFTWARE/LoPar-en.html
we always strip the adverb from these construc-
tions.
  German verbal adjective constructions are bro-
ken up if they involve arguments or adjuncts
(e.g., der von mir gegessene Kuchen = the by
me eaten cake), because this poses problems
more related to verbal clauses.
  Alignment points involving punctuation are
stripped from the word alignment. Punctuation
is also stripped from the edges of NP/PPs.
A total of 737,388 NP/PP pairs are collected
from the German-English Europarl corpus as train-
ing data.
Certain German NP/PPs consistently do not align
to NP/PPs in English (see the example in Sec-
tion 2.2). These are detected at this point. The
obtained data of unaligned NP/PPs can be used for
dealing with these special cases.
3.3 Base Model
Given the NP/PP corpus, we can use any general sta-
tistical machine translation method to train a transla-
tion system for noun phrases. As a baseline, we use
an IBM Model 4 (Brown et al, 1993) system3 with
a greedy decoder4 (Germann et al, 2001).
We found that phrase based models achieve better
translation quality than IBM Model 4. Such mod-
els segment the input sequence into a number of
(non-linguistic) phrases, translate each phrase using
a phrase translation table, and allow for reordering
of phrases in the output. No phrases may be dropped
or added.
We use a phrase translation model that extracts its
phrase translation table from word alignments gen-
erated by the Giza++ toolkit. Details of this model
are described by Koehn et al (2003).
To obtain an n-best list of candidate translations,
we developed a beam search decoder. This decoder
employs hypothesis recombination and stores the
search states in a search graph ? similar to work by
Ueffing et al (2002) ? which can be mined with stan-
dard finite state machine methods5 for n-best lists.
3Available at http://www-i6.informatik.rwth-
aachen.de/  och/software/GIZA++.html
4Available at http://www.isi.edu/licensed-sw
/rewrite-decoder/
5We use the Carmel toolkit available at http://www.
isi.edu/licensed-sw/carmel/
 

1 2 4 8 16 32 64
60%
70%
80%
90%
100%
size of n-best list
correct
Figure 3: Acceptable NP/PP translations in n-best
list for different sizes 
3.4 Acceptable Translations in the n-Best List
One key question for our approach is how often an
acceptable translation can be found in an n-best list.
The answer to this is illustrated in Figure 3: While
an acceptable translation comes out on top for only
about 60% of the NP/PPs in our test corpus, one can
be found in the 100-best list for over 90% of the
NP/PPs6. This means that rescoring has the potential
to raise performance by 30%.
What are the problems with the remaining 10%
for which no translation can be found? To investi-
gate this, we carried out an error analysis of these
NP/PPs. Results are given in Table 2. The main
sources of error are unknown words (34%) or words
for which the correct translation does not occur in
the training data (14%), and errors during tagging
and parsing that lead to incorrectly detected NP/PPs
(28%).
There are also problems with NP/PPs that require
complex syntactic restructuring (7%), and NP/PPs
that are too long, so an acceptable translation could
not be found in the 100-best list, but only further
down the list (6%). There are also NP/PPs that can-
not be translated as NP/PPs into English (2%), as
discussed in Section 2.2.
3.5 Maximum Entropy Reranking
Given an n-best list of candidates and additional fea-
tures, we transform the translation task from a search
problem into a reranking problem, which we address
using a maximum entropy approach.
As training data for finding feature values, we col-
lected a development corpus of 683 NP/PPs. Each
6Note that these numbers are obtained after compound split-
ting, described in Section 4.1
Error Frequency
Unknown Word 34%
Tagging or parsing error 28%
Unknown translation 14%
Complex syntactic restructuring 7%
Too long 6%
Untranslatable 2%
Other 9%
Table 2: Error analysis for NP/PPs without accept-
able translation in 100-best list
NP/PP comes with an n-best list of candidate trans-
lations that are generated from our base model and
are annotated with accuracy judgments. The initial
features are the logarithm of the probability scores
that the model assigns to each candidate transla-
tion: the language model score, the phrase transla-
tion score and the reordering (distortion) score.
The task for the learning method is to find a prob-
ability distribution   	 that indicates if the can-
didate translation  is an accurate translation of the
input  . The decision rule to pick the best translation
is  best 
 argmax   	 .
The development corpus provides the empirical
probability distribution by distributing the proba-
bility mass over the acceptable translations  :

  	


Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,
Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,
David Traum
University of Southern California
Los Angeles, CA 90089
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
Robert Belvin
HRL Laboratories, LLC
3011 Malibu Canyon Rd.
Malibu, CA 90265
rsbelvin@hrl.com
Abstract
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions.  The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance.  The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
1 Introduction
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories.  The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers.  The system is targeted at
a domain which may be roughly characterized as
"urgent care" medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient.  In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
2 General Design of the system
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary "director" of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
89
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al 2004).
1
 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora.  The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy & Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data.  Language modeling data was also obtained
from multiple sources.  The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other  smaller  amounts of  training  data  were ob
tained from various sources, as detailed in  (Nara-
yanan et al 2003, 2004).  Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine,  both
                                                           
1
 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries.  They are used extensively in medical
education so that doctors in training don't have to "practice"
on real patients.
developed by USC-ISI team members.  The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup.  If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed.  The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages.  As with ASR, the performance of the
component is highly dependent on very large
amounts of training data.  Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team's
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al, op.cit.
3 Enabling Effective Communication
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway.  We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication.  To this end, our design
and development included the following:
MT
English to Farsi
Farsi to English
ASR
English
Prompts or TTS
Farsi
Prompts or TTS
English
ASR
Farsi
GUI:
prompts,
 confirmations,
 ASR switch
Dialog
Manager
SMT
English to Farsi
Farsi to English
Figure 1: Architecture of the Transonics system.  The Dialogue Manager acts as the hub through which the
individual components interact.
90
i. an "educated guess" capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism?this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes  "suggestions" (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
4 Testing and Evaluation
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program.  There were two parts to the
MITRE evaluations, a "live" part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a "canned" part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician?s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed "focused history and physical
exam" style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system.  Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate.  This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.
2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses.  Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury.  The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics.  A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient.  The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated.  A "significant fact" con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient's age or marital status.
3
  We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system.  That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient's answer is translated correctly for the doc-
tor.  In a true medical exam, the doctor may have
                                                           
2
 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing.  This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
3
 Unfortunately, there was no baseline evaluation this could be
compared to,  such as assessing whether any of the critical
facts could be determined without the use of the system at all.
91
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely.  Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance.  The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output.  These scores are shown below.
Table 1:  Farsi BLEU Scores
IBM BLEU
ASR
IBM BLEU
TEXT
English to Farsi
0.2664 0.3059
Farsi  to English 0.2402 0.2935
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
Table 2:  HRL/USC WER for Farsi and English
English Farsi
WER 11.5% 13.4%
5 Conclusion
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation.  Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled.  And this is
very encouraging indeed.
6 Acknowledgements
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
References
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi.  2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang.  2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
92
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 97?100, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Interactively Exploring a Machine Translation Model
Steve DeNeefe, Kevin Knight, and Hayward H. Chan
Information Sciences Institute and Department of Computer Science
The Viterbi School of Engineering, University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{sdeneefe,knight}@isi.edu, hhchan@umich.edu
Abstract
This paper describes a method of in-
teractively visualizing and directing the
process of translating a sentence. The
method allows a user to explore a model
of syntax-based statistical machine trans-
lation (MT), to understand the model?s
strengths and weaknesses, and to compare
it to other MT systems. Using this visual-
ization method, we can find and address
conceptual and practical problems in an
MT system. In our demonstration at ACL,
new users of our tool will drive a syntax-
based decoder for themselves.
1 Introduction
There are many new approaches to statistical ma-
chine translation, and more ideas are being sug-
gested all the time. However, it is difficult to deter-
mine how well a model will actually perform. Ex-
perienced researchers have been surprised by the ca-
pability of unintuitive word-for-word models; at the
same time, seemingly capable models often have se-
rious hidden problems ? intuition is no substitute
for experimentation. With translation ideas growing
more complex, capturing aspects of linguistic struc-
ture in different ways, it becomes difficult to try out
a new idea without a large-scale software develop-
ment effort.
Anyone who builds a full-scale, trainable trans-
lation system using syntactic information faces this
problem. We know that syntactic models often do
not fit the data. For example, the syntactic sys-
tem described in Yamada and Knight (2001) can-
not translate n-to-m-word phrases and does not al-
low for multi-level syntactic transformations; both
phenomena are frequently observed in real data. In
building a new syntax-based MT system which ad-
dresses these flaws, we wanted to find problems in
our framework as early as possible. So we decided
to create a tool that could help us answer questions
like:
1. Does our framework allow good translations
for real data, and if not, where does it get stuck?
2. How does our framework compare to exist-
ing state-of-the-art phrase-based statistical MT
systems such as Och and Ney (2004)?
The result is DerivTool, an interactive translation
visualization tool. It allows a user to build up a
translation from one language to another, step by
step, presenting the user with the myriad of choices
available to the decoder at each point in the pro-
cess. DerivTool simplifies the user?s experience of
exploring these choices by presenting only the de-
cisions relevant to the context in which the user is
working, and allowing the user to search for choices
that fit a particular set of conditions. Some previ-
ous tools have allowed the user to visualize word
alignment information (Callison-Burch et al, 2004;
Smith and Jahr, 2000), but there has been no cor-
responding deep effort into visualizing the decoding
experience itself. Other tools use visualization to aid
the user in manually developing a grammar (Copes-
take and Flickinger, 2000), while our tool visualizes
97
Starting with: ? ?0 ?
and applying the rule: NPB(DT(the) NNS(police)) ? ?0
we get: ? NPB(DT(the) NNS(police)) ?
If we then apply the rule: VBN(killed) ? ?
we get: ? NPB(DT(the) NNS(police)) VBN(killed)
Applying the next rule: NP-C(x0:NPB) ? x0
results in: ? NP-C(NPB(DT(the) NNS(police))) VBN(killed)
Finally, applying the rule: VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ? ? x1 x0
results in the final phrase: VP(VBD(was) VP-C(VBN(killed) PP(IN(by) NP-C(NPB(DT(the) NNS(police))))))
Table 1: By applying applying four rules, a Chinese verb phrase is translated to English.
the translation process itself, using rules from very
large, automatically learned rule sets. DerivTool can
be adapted to visualize other syntax-based MT mod-
els, other tree-to-tree or tree-to-string MT models, or
models for paraphrasing.
2 Translation Framework
It is useful at this point to give a brief descrip-
tion of the syntax-based framework that we work
with, which is based on translating Chinese sen-
tences into English syntax trees. Galley et al (2004)
describe how to learn hundreds of millions of tree-
transformation rules from a parsed, aligned Chi-
nese/English corpus, and Galley et al (submitted)
describe probability estimators for those rules. We
decode a new Chinese sentence with a method simi-
lar to parsing, where we apply learned rules to build
up a complete English tree hypothesis from the Chi-
nese string.
The rule extractor learns rules for many situations.
Some are simple phrase-to-phrase rules such as:
NPB(DT(the) NNS(police)) ? ?0
This rule should be read as follows: replace the Chi-
nese word ?0 with the noun phrase ?the police?.
Others rules can take existing tree fragments and
build upon them. For example, the rule
S(x0:NP-C x1:VP x2:.) ? x0 x1 x2
takes three parts of a sentence, a noun phrase (x0),
a verb phrase (x1), and a period (x2) and ties them
together to build a complete sentence. Rules also
can involve phrase re-ordering, as in
NPB(x0:JJ x1:NN) ? x1 x0
This rule builds an English noun phrase out of an
adjective (x0) and a noun (x1), but in the Chinese,
the order is reversed. Multilevel rules can tie several
of these concepts together; the rule
VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C)))
? ? x1 x0
takes a Chinese word ? and two English con-
stituents ? x1, a noun phrase, and x0, a past-
participle verb ? and translates them into a phrase
of the form ?was [verb] by [noun-phrase]?. Notice
that the order of the constituents has been reversed in
the resulting English phrase, and that English func-
tion words have been generated.
The decoder builds up a translation from the
Chinese sentence into an English tree by apply-
ing these rules. It follows the decoding-as-parsing
idea exemplified by Wu (1996) and Yamada and
Knight (2002). For example, the Chinese verb
phrase ? ?0 ? (literally, ?[passive] police
kill?) can be translated to English via four rules (see
Table 1).
3 DerivTool
In order to test whether good translations can be gen-
erated with rules learned by Galley et al (2004),
we created DerivTool as an environment for interac-
tively using these rules as a decoder would. A user
starts with a Chinese sentence and applies rules one
after another, building up a translation from Chinese
to English. After finishing the translation, the user
can save the trace of rule-applications (the deriva-
tion tree) for later analysis.
We now outline the typical procedure for a user
to translate a sentence with DerivTool. To start, the
user loads a set of sentences to translate and chooses
a particular one to work with. The tool then presents
the user with a window split halfway up. The top
98
Figure 1: DerivTool with a completed derivation.
half is the workspace where the user builds a transla-
tion. It initially displays only the Chinese sentence,
with each word as a separate node. The bottom half
presents a set of tabbed panels which allow the user
to select rules to build up the translation. See Fig-
ure 1 for a picture of the interface showing a com-
pleted derivation tree.
The most immediately useful panel is called Se-
lecting Template, which shows a grid of possible En-
glish phrasal translations for Chinese phrases from
the sentence. This phrase grid contains both phrases
learned in our extracted rules (e.g., ?the police?
from earlier) and phrases learned by the phrase-
based translation system (Och and Ney, 2004)1. The
user presses a grid button to choose a phrase to in-
clude in the translation. At this point, a frequency-
1The phrase-based system serves as a sparring partner. We
display its best decoding in the center of the screen. Note that
in Figure 1 its output lacks an auxiliary verb and an article.
ordered list of rules will appear; these rules trans-
late the Chinese phrase into the button-selected En-
glish phrase, and the user specifies which one to use.
Often there will be more than one rule (e.g., ?
may translate via the rule VBD(killed) ? ? or
VBN(killed) ? ?), and sometimes there are no
rules available. When there are no rules, the buttons
are marked in red, telling us that the phrase-based
system has access to this phrasal translation but our
learned syntactic rules did not capture it. Other but-
tons are marked green to represent translations from
the specialized number/name/date system, and oth-
ers are blue, indicating the phrases in the phrase-
based decoder?s best output. A purple button indi-
cates both red and blue, i.e., the phrase was cho-
sen by the phrase-based decoder but is unavailable
in our syntactic framework. This is a bad combina-
tion, showing us where rule learning is weak. The
99
remaining buttons are gray.
Once the user has chosen the phrasal rules re-
quired for translating the sentence, the next step is
to stitch these phrases together into a complete En-
glish syntax tree using more general rules. These are
found in another panel called Searching. This panel
allows a user to select a set of adjacent, top-level
nodes in the tree and find a rule that will connect
them together. It is commonly used for building up
larger constituents from smaller ones. For example,
if one has a noun-phrase, a verb-phrase, and a pe-
riod, the user can search for the rule that connects
them and builds an ?S? on top, completing the sen-
tence. The results of a search are presented in a list,
again ordered by frequency.
A few more features to note are: 1) loading and
saving your work at any point, 2) adding free-form
notes to the document (e.g. ?I couldn?t find a rule
that...?), and 3) manually typing rules if one cannot
be found by the above methods. This allows us to
see deficiencies in the framework.
4 How DerivTool Helps
First, DerivTool has given us confidence that our
syntax-based framework can work, and that the rules
we are learning are good. We have been able to
manually build a good translation for each sentence
we tried, both for short and long sentences. In fact,
there are multiple good ways to translate sentences
using these rules, because different DerivTool users
translate sentences differently. Ordering rules by
frequency and/or probability helps us determine if
the rules we want are also frequent and favored by
our model.
DerivTool has also helped us to find problems
with the framework and to see clearly how to fix
them. For example, in one of our first sentences
we realized that there was no rule for translat-
ing a date ? likewise for numbers, names, cur-
rency values, and times of day. Our phrase-based
system solves these problems with a specialized
date/name/number translator. Through the process
of manually typing syntactic transformation rules
for dates and numbers in DerivTool, it became clear
that our current date/name/number translator did not
provide enough information to create such syntac-
tic rules automatically. This sparked a new area of
research before we had a fully-functional decoder.
We also found that multi-word noun phrases, such
as ?Israeli Prime Minister Sharon? and ?the French
Ambassador?s visit? were often parsed in a way that
did not allow us to learn good translation rules.
The flat structure of the constituents in the syntax
tree makes it difficult to learn rules that are general
enough to be useful. Phrases with possessives also
gave particular difficulty due to the awkward mul-
tilevel structure of the parser?s output. We are re-
searching solutions to these problems involving re-
structuring the syntax trees before training.
Finally, our tool has helped us find bugs in our
system. We found many cases where rules we
wanted to use were unexpectedly absent. We eventu-
ally traced these bugs to our rule extraction system.
Our decoder would have simply worked around this
problem, producing less desirable translations, but
DerivTool allowed us to quickly spot the missing
rules.
5 Conclusion
We created DerivTool to test our MT framework
against real-world data before building a fully-
functional decoder. By allowing us to play the role
of a decoder and translate sentences manually, it has
given us insight into how well our framework fits
the data, what some of its weaknesses are, and how
it compares to other systems. We continue to use
it as we try out new rule-extraction techniques and
finish the decoding system.
References
Chris Callison-Burch, Colin Bannard and Josh Schroeder.
2004. Improved statistical translation through editing.
EAMT-2004 Workshop.
Ann Copestake and Dan Flickinger. 2000. An open source
grammar development environment and broad-coverage En-
glish grammar using HPSG. Proc. of LREC 2000.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? Proc. of NAACL-
HLT 2004.
Franz Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. Computational
Linguistics, 30(4).
Noah A. Smith and Michael E. Jahr. 2000. Cairo: An Align-
ment Visualization Tool. Proc. of LREC 2000.
Dekai Wu. 1996. A polynomial-time algorithm for statistical
machine translation. Proc. of ACL.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. Proc. of ACL.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-
based statistical MT. Proc. of ACL.
100
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 499?506,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Analysis for Decipherment Problems
Kevin Knight, Anish Nair, Nishit Rathod
Information Sciences Institute
and Computer Science Department
University of Southern California
knight@isi.edu, {anair,nrathod}@usc.edu
Kenji Yamada
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
kyamada@languageweaver.com
Abstract
We study a number of natural language deci-
pherment problems using unsupervised learn-
ing. These include letter substitution ciphers,
character code conversion, phonetic decipher-
ment, and word-based ciphers with relevance
to machine translation. Straightforward unsu-
pervised learning techniques most often fail on
the first try, so we describe techniques for un-
derstanding errors and significantly increasing
performance.
1 Introduction
Unsupervised learning holds great promise for break-
throughs in natural language processing. In cases like
(Yarowsky, 1995), unsupervised methods offer accu-
racy results than rival supervised methods (Yarowsky,
1994) while requiring only a fraction of the data prepa-
ration effort. Such methods have also been a key
driver of progress in statistical machine translation,
which depends heavily on unsupervised word align-
ments (Brown et al, 1993).
There are also interesting problems for which super-
vised learning is not an option. These include deci-
phering unknown writing systems, such as the Easter
Island rongorongo script and the 20,000-word Voynich
manuscript. Deciphering animal language is another
case. Machine translation of human languages is an-
other, when we consider language pairs where little or
no parallel text is available. Ultimately, unsupervised
learning also holds promise for scientific discovery in
linguistics. At some point, our programs will begin
finding novel, publishable regularities in vast amounts
of linguistic data.
2 Decipherment
In this paper, we look at a particular type of unsuper-
vised analysis problem in which we face a ciphertext
stream and try to uncover the plaintext that lies behind
it. We will investigate several applications that can be
profitably analyzed this way. We will also apply the
same technical solution these different problems.
The method follows the well-known noisy-channel
framework. At the top level, we want to find the plain-
text that maximizes the probability P(plaintext   cipher-
text). We first build a probabilistic model P(p) of the
plaintext source. We then build probabilistic channel
model P(c   p) that explains how plaintext sequences
(like p) become ciphertext sequences (like c). Some of
the parameters in these models can be estimated with
supervised training, but most cannot.
When we face a new ciphertext sequence c, we first
use expectation-maximization (EM) (Dempster, Laird,
and Rubin, 1977) to set al free parameters to maximize
P(c), which is the same (by Bayes Rule) as maximiz-
ing the sum over all p of P(p)  P(c   p). We then use
the Viterbi algorithm to choose the p maximizing P(p)
 P(c   p), which is the same (by Bayes Rule) as our
original goal of maximizing P(p   c), or plaintext given
ciphertext.
Figures 1 and 2 show standard EM algorithms
(Knight, 1999) for the case in which we have a bi-
gram P(p) model (driven by a two-dimensional b ta-
ble of bigram probabilities) and a one-for-one P(c   p)
model (driven by a two-dimensional s table of substi-
tution probabilities). This case covers Section 3, while
more complex models are employed in later sections.
3 English Letter Substitution
An informal substitution cipher (Smith, 1943) dis-
guises a text by substituting code letters for normal
letters. This system is usually exclusive, meaning that
each plaintext letter maps to only one ciphertext letter,
and vice versa. There is surprisingly little published
on this problem, e.g., (Peleg and Rosenfeld, 1979), be-
cause fast computers led to public-key cryptography
before much computer analysis was done on such old-
style ciphers. We study this problem first because it re-
sembles many of the other problems we are interested
in, and we can generate arbitrary amounts of test data.
We estimate unsmoothed parameter values for an
English letter-bigram P(p) from news data. This is a
27x27 table that includes the space character. We then
set up a uniform P(c | p), which also happens to be a
499
(a) ingcmpnqsnwf cv fpn owoktvcv hu ihgzsnwfv rqcffnw cw owgcnwf kowazoanv...
(b) wecitherkent is the analysis of wocoments pritten in ancient buncquges...
(c) decipherment is the analysis of documents written in ancient languages...
Figure 3: Letter substitution decipherment. (a) is the ciphertext, (b) is an automatic decipherment, and (c) is an
improved decipherment.
Given a ciphertext c of length  , a plaintext vocabulary
of  tokens, and a plaintext bigram model b:
1. set a s(   ) substitution table initially to be uniform
2. for several iterations do:
a. set up a count table count(  ,  ) with zero entries
b. P(c) = 0
c. for all possible plaintexts 	



(each  drawn from plaintext vocabulary)
compute P(p) = b(	 boundary) 
 b(boundary  )




b(Proceedings of ACL-08: HLT, pages 389?397,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Name Translation in Statistical Machine Translation
Learning When to Transliterate
Ulf Hermjakob and Kevin Knight
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
fulf,knightg@isi.edu
Hal Daume? III
University of Utah
School of Computing
50 S Central Campus Drive
Salt Lake City, UT 84112, USA
me@hal3.name
Abstract
We present a method to transliterate names
in the framework of end-to-end statistical
machine translation. The system is trained
to learn when to transliterate. For Arabic
to English MT, we developed and trained a
transliterator on a bitext of 7 million sen-
tences and Google?s English terabyte ngrams
and achieved better name translation accuracy
than 3 out of 4 professional translators. The
paper also includes a discussion of challenges
in name translation evaluation.
1 Introduction
State-of-the-art statistical machine translation
(SMT) is bad at translating names that are not very
common, particularly across languages with differ-
ent character sets and sound systems. For example,
consider the following automatic translation:1
Arabic input 	? AK
.
?

?? P@
	
P??? pAK
.
?

J?
	
?J


J



?J


???
?J


	
?@P?
	
??
	
JJ


	
KA?kP?
	
?A??

??
	
?
	
???

JJ


K
.
?
	
?J


J


	
????QK
.
?
SMT output musicians such as Bach
Correct translation composers such as Bach,
Mozart, Chopin, Beethoven, Schumann,
Rachmaninoff, Ravel and Prokofiev
The SMT system drops most names in this ex-
ample. ?Name dropping? and mis-translation hap-
pens when the system encounters an unknown word,
mistakes a name for a common noun, or trains on
noisy parallel data. The state-of-the-art is poor for
1taken from NIST02-05 corpora
two reasons. First, although names are important to
human readers, automatic MT scoring metrics (such
as BLEU) do not encourage researchers to improve
name translation in the context of MT. Names are
vastly outnumbered by prepositions, articles, adjec-
tives, common nouns, etc. Second, name translation
is a hard problem ? even professional human trans-
lators have trouble with names. Here are four refer-
ence translations taken from the same corpus, with
mistakes underlined:
Ref1 composers such as Bach, missing name
Chopin, Beethoven, Shumann, Rakmaninov,
Ravel and Prokoviev
Ref2 musicians such as Bach, Mozart, Chopin,
Bethoven, Shuman, Rachmaninoff, Rafael and
Brokoviev
Ref3 composers including Bach, Mozart, Schopen,
Beethoven, missing name Raphael, Rahmaniev
and Brokofien
Ref4 composers such as Bach, Mozart, missing
name Beethoven, Schumann, Rachmaninov,
Raphael and Prokofiev
The task of transliterating names (independent of
end-to-end MT) has received a significant amount
of research, e.g., (Knight and Graehl, 1997; Chen et
al., 1998; Al-Onaizan, 2002). One approach is to
?sound out? words and create new, plausible target-
language spellings that preserve the sounds of the
source-language name as much as possible. Another
approach is to phonetically match source-language
names against a large list of target-language words
389
and phrases. Most of this work has been discon-
nected from end-to-end MT, a problem which we
address head-on in this paper.
The simplest way to integrate name handling into
SMT is: (1) run a named-entity identification system
on the source sentence, (2) transliterate identified
entities with a special-purpose transliteration com-
ponent, and (3) run the SMT system on the source
sentence, as usual, but when looking up phrasal
translations for the words identified in step 1, instead
use the transliterations from step 2.
Many researchers have attempted this, and it does
not work. Typically, translation quality is degraded
rather than improved, for the following reasons:
 Automatic named-entity identification makes
errors. Some words and phrases that should
not be transliterated are nonetheless sent to the
transliteration component, which returns a bad
translation.
 Not all named entities should be transliterated.
Many named entities require a mix of translit-
eration and translation. For example, in the pair
A J


	
KP?
	
? J


?A ? H
.
?
	
J k
.
/jnub kalyfurnya/Southern
California, the first Arabic word is translated,
and the second word is transliterated.
 Transliteration components make errors. The
base SMT system may translate a commonly-
occurring name just fine, due to the bitext it was
trained on, while the transliteration component
can easily supply a worse answer.
 Integration hobbles SMT?s use of longer
phrases. Even if the named-entity identifi-
cation and transliteration components operate
perfectly, adopting their translations means that
the SMT system may no longer have access to
longer phrases that include the name. For ex-
ample, our base SMT system translates ?J



KP
	
?
	
J K
.
? ? Z@P
	
P? ?@ (as a whole phrase) to ?Pre-
mier Li Peng?, based on its bitext knowledge.
However, if we force 	? 	J K
.
? ? to translate as
a separate phrase to ?Li Peng?, then the term
Z @P
	
P??@ ?J



KP becomes ambiguous (with trans-
lations including ?Prime Minister?, ?Premier?,
etc.), and we observe incorrect choices being
subsequently made.
To spur better work in name handling, an ACE
entity-translation pilot evaluation was recently de-
veloped (Day, 2007). This evaluation involves
a mixture of entity identification and translation
concerns?for example, the scoring system asks for
coreference determination, which may or may not be
of interest for improving machine translation output.
In this paper, we adopt a simpler metric. We ask:
what percentage of source-language named entities
are translated correctly? This is a precision metric.
We can readily apply it to any base SMT system, and
to human translations as well. Our goal in augment-
ing a base SMT system is to increase this percentage.
A secondary goal is to make sure that our overall
translation quality (as measured by BLEU) does not
degrade as a result of the name-handling techniques
we introduce. We make all our measurements on an
Arabic/English newswire translation task.
Our overall technical approach is summarized
here, along with references to sections of this paper:
 We build a component for transliterating be-
tween Arabic and English (Section 3).
 We automatically learn to tag those words and
phrases in Arabic text, which we believe the
transliteration component will translate cor-
rectly (Section 4).
 We integrate suggested transliterations into the
base SMT search space, with their use con-
trolled by a feature function (Section 5).
 We evaluate both the base SMT system and the
augmented system in terms of entity translation
accuracy and BLEU (Sections 2 and 6).
2 Evaluation
In this section we present the evaluation method that
we use to measure our system and also discuss chal-
lenges in name transliteration evaluation.
2.1 NEWA Evaluation Metric
General MT metrics such as BLEU, TER, METEOR
are not suitable for evaluating named entity transla-
tion and transliteration, because they are not focused
on named entities (NEs). Dropping a comma or a the
is penalized as much as dropping a name. We there-
fore use another metric, jointly developed with BBN
and LanguageWeaver.
390
The general idea of the Named Entity Weak Ac-
curacy (NEWA) metric is to
 Count number of NEs in source text: N
 Count number of correctly translated NEs: C
 Divide C/N to get an accuracy figure
In NEWA, an NE is counted as correctly translated
if the target reference NE is found in the MT out-
put. The metric has the advantage that it is easy to
compute, has no special requirements on an MT sys-
tem (such as depending on source-target word align-
ment) and is tokenization independent.
In the result section of this paper, we will use the
NEWA metric to measure and compare the accuracy
of NE translations in our end-to-end SMT transla-
tions and four human reference translations.
2.2 Annotated Corpus
BBN kindly provided us with an annotated Arabic
text corpus, in which named entities were marked
up with their type (e.g. GPE for Geopolitical Entity)
and one or more English translations. Example:
?
	
?<GPE alt=?Termoli?>????QJ



K</GPE>
<PER alt=?Abdullah II j Abdallah II?> ? ? ? @ Y J
.
?
?
	
KA

J? @</PER>
The BBN annotations exhibit a number of issues.
For the English translations of the NEs, BBN anno-
tators looked at human reference translations, which
may introduce a bias towards those human transla-
tions. Specifically, the BBN annotations are some-
times wrong, because the reference translations were
wrong. Consider for example the Arabic phrase
? ?? ?Q J



K ?
	
?
	
? @Q

KP? K
.
?
	
J ? ? (mSn? burtran
fY tyrmulY), which means Powertrain plant in Ter-
moli. The mapping from tyrmulY to Termoli is not
obvious, and even less the one from burtran to Pow-
ertrain. The human reference translations for this
phrase are
1. Portran site in Tremolo
2. Termoli plant (one name dropped)
3. Portran in Tirnoli
4. Portran assembly plant, in Tirmoli
The BBN annotators adopted the correct transla-
tion Termoli, but also the incorrect Portran. In
other cases the BBN annotators adopted both a cor-
rect (Khatami) and an incorrect translation (Kha-
timi) when referring to the former Iranian president,
which would reward a translation with such an in-
correct spelling.
 <PER alt=?KhatamijKhatimi?>??KA 	k</PER>
 <GPE alt=?the American?> ?J


?QJ


?A? @</GPE>
In other cases, all translations are correct, but ad-
ditional correct translations are missing, as for ?the
American? above, for which ?the US? is an equally
valid alternative in the specific sentence it was anno-
tated in.
All this raises the question of what is a correct
answer. For most Western names, there is normally
only one correct spelling. We follow the same con-
ventions as standard media, paying attention to how
an organization or individual spells its own name,
e.g. Senator Jon Kyl, not Senator John Kyle. For
Arabic names, variation is generally acceptable if
there is no one clearly dominant spelling in English,
e.g. GaddafijGadhafijQaddafijQadhafi, as long as a
given variant is not radically rarer than the most con-
ventional or popular form.
2.3 Re-Annotation
Based on the issues we found with the BBN annota-
tions, we re-annotated a sub-corpus of 637 sentences
of the BBN gold standard.
We based this re-annotation on detailed annota-
tion guidelines and sample annotations that had pre-
viously been developed in cooperation with Lan-
guageWeaver, building on three iterations of test an-
notations with three annotators.
We checked each NE in every sentence, using
human reference translations, automatic translitera-
tor output, performing substantial Web research for
many rare names, and checked Google ngrams and
counts for the general Web and news archives to de-
termine whether a variant form met our threshold of
occurring at least 20% as often as the most dominant
form.
3 Transliterator
This section describes how we transliterate Arabic
words or phrases. Given a word such as 	??	JJ


	
K A?kP
or a phrase such as ?J


	
?@P ?K


P??, we want to find
the English transliteration for it. This is not just a
391
romanization like rHmanynuf and murys rafyl for
the examples above, but a properly spelled English
name such as Rachmaninoff and Maurice Ravel. The
transliteration result can contain several alternatives,
e.g. RachmaninoffjRachmaninov. Unlike various
generative approaches (Knight and Graehl, 1997;
Stalls and Knight, 1998; Li et al, 2004; Matthews,
2007; Sherif and Kondrak, 2007; Kashani et al,
2007), we do not synthesize an English spelling
from scratch, but rather find a translation in very
large lists of English words (3.4 million) and phrases
(47 million).
We develop a similarity metric for Arabic and En-
glish words. Since matching against millions of can-
didates is computationally prohibitive, we store the
English words and phrases in an index, such that
given an Arabic word or phrase, we quickly retrieve
a much smaller set of likely candidates and apply
our similarity metric to that smaller list.
We divide the task of transliteration into two
steps: given an Arabic word or phrase to translit-
erate, we (1) identify a list of English translitera-
tion candidates from indexed lists of English words
and phrases with counts (section 3.1) and (2) com-
pute for each English name candidate the cost for
the Arabic/English name pair (transliteration scor-
ing model, section 3.2).
We then combine the count information with the
transliteration cost according to the formula:
score(e) = log(count(e))/20 - translit cost(e,f)
3.1 Indexing with consonant skeletons
We identify a list of English transliteration candi-
dates through what we call a consonant skeleton in-
dex. Arabic consonants are divided into 11 classes,
represented by letters b,f,g,j,k,l,m,n,r,s,t. In a one-
time pre-processing step, all 3,420,339 (unique) En-
glish words from our English unigram language
model (based on Google?s Web terabyte ngram col-
lection) that might be names or part of names
(mostly based on capitalization) are mapped to one
or more skeletons, e.g.
Rachmaninoff ! rkmnnf, rmnnf, rsmnnf, rtsmnnf
This yields 10,381,377 skeletons (average of 3.0 per
word) for which a reverse index is created (with
counts). At run time, an Arabic word to be translit-
erated is mapped to its skeleton, e.g.
	
??
	
JJ


	
K A?kP ! rmnnf
This skeleton serves as a key for the previously built
reverse index, which then yields the list of English
candidates with counts:
rmnnf ! Rachmaninov (186,216), Rachmaninoff
(179,666), Armenonville (3,445), Rachmaninow
(1,636), plus 8 others.
Shorter words tend to produce more candidates, re-
sulting in slower transliteration, but since there are
relatively few unique short words, this can be ad-
dressed by caching transliteration results.
The same consonant skeleton indexing process is
applied to name bigrams (47,700,548 unique with
167,398,054 skeletons) and trigrams (46,543,712
unique with 165,536,451 skeletons).
3.2 Transliteration scoring model
The cost of an Arabic/English name pair is com-
puted based on 732 rules that assign a cost to a pair
of Arabic and English substrings, allowing for one
or more context restrictions.
1. ?::q == ::0
2. 	??::ough == ::0
3. h::ch == :[aou],::0.1
4. ?::k == ,$:,$::0.1 ; ::0.2
5. Z:: == :,EC::0.1
The first example rule above assigns to the
straightforward pair ?/q a cost of 0. The second rule
includes 2 letters on the Arabic and 4 on the English
side. The third rule restricts application to substring
pairs where the English side is preceded by the let-
ters a, o, or u. The fourth rule specifies a cost of 0.1
if the substrings occur at the end of (both) names,
0.2 otherwise. According to the fifth rule, the Ara-
bic letter Z may match an empty string on the En-
glish side, if there is an English consonant (EC) in
the right context of the English side.
The total cost is computed by always applying the
longest applicable rule, without branching, result-
ing in a linear complexity with respect to word-pair
length. Rules may include left and/or right context
for both Arabic and English. The match fails if no
rule applies or the accumulated cost exceeds a preset
limit.
Names may have n words on the English and m on
the Arabic side. For example, New York is one word
in Arabic and Abdullah is two words in Arabic. The
392
rules handle spaces (as well as digits, apostrophes
and other non-alphabetic material) just like regular
alphabetic characters, so that our system can handle
cases like where words in English and Arabic names
do not match one to one.
The French name Beaujolais ( ?J


??k
.
?K
.
/bujulyh)
deviates from standard English spelling conventions
in several places. The accumulative cost from the
rules handling these deviations could become pro-
hibitive, with each cost element penalizing the same
underlying offense ? being French. We solve this
problem by allowing for additional context in the
form of style flags. The rule for matching eau/?
specifies, in addition to a cost, an (output) style flag
+fr (as in French), which in turn serves as an ad-
ditional context for the rule that matches ais/ ?K


at
a much reduced cost. Style flags are also used for
some Arabic dialects. Extended characters such as
e?, o?, and s? and spelling idiosyncrasies in names on
the English side of the bitext that come from various
third languages account for a significant portion of
the rule set.
Casting the transliteration model as a scoring
problem thus allows for very powerful rules with
strong contexts. The current set of rules has been
built by hand based on a bitext development corpus;
future work might include deriving such rules auto-
matically from a training set of transliterated names.
This transliteration scoring model described in
this section is used in two ways: (1) to transliter-
ate names at SMT decoding time, and (2) to identify
transliteration pairs in a bitext.
4 Learning what to transliterate
As already mentioned in the introduction, named
entity (NE) identification followed by MT is a bad
idea. We don?t want to identify NEs per se anyway
? we want to identify things that our transliterator
will be good at handling, i.e., things that should be
transliterated. This might even include loanwords
like bnk (bank) and brlman (parliament), but would
exclude names such as National Basketball Associ-
ation that are often translated rather transliterated.
Our method follows these steps:
1. Take a bitext.
2. Mark the Arabic words and phrases that have a
recognizable transliteration on the English side.
3. Remove the English side of the bitext.
4. Divide the annotated Arabic corpus into a train-
ing and test corpus.
5. Train a monolingual Arabic tagger to identify
which words and phrases (in running Arabic)
are good candidates for transliteration (section
4.2)
6. Apply the tagger to test data and evaluate its
accuracy.
4.1 Mark-up of bitext
Given a tokenized (but unaligned and mixed-case)
bitext, we mark up that bitext with links between
Arabic and English words that appear to be translit-
erations. In the following example, linked words are
underlined, with numbers indicating what is linked.
English The meeting was attended by Omani (1)
Secretary of State for Foreign Affairs Yusif (2)
bin (3) Alawi (6) bin (8) Abdallah (10) and
Special Advisor to Sultan (12) Qabus (13)
for Foreign Affairs Umar (14) bin (17)
Abdul Munim (19) al-Zawawi (21).
Arabic (translit.) uHDr allqa? uzyr aldule
al?manY (1) llsh?uun alkharjye yusf (2) bn (3)
?luY (6) bn (8) ?bd allh (10) ualmstshar alkhaS
llslTan (12) qabus (13) ll?laqat alkharjye ?mr (14)
bn (17) ?bd almn?m (19) alzuauY (21) .
For each Arabic word, the linking algorithm tries
to find a matching word on the English side, using
the transliteration scoring model described in sec-
tion 3. If the matcher reaches the end of an Arabic
or English word before reaching the end of the other,
it continues to ?consume? additional words until a
word-boundary observing match is found or the cost
threshold exceeded.
When there are several viable linking alternatives,
the algorithm considers the cost provided by the
transliteration scoring model, as well as context to
eliminate inferior alternatives, so that for example
the different occurrences of the name particle bin
in the example above are linked to the proper Ara-
bic words, based on the names next to them. The
number of links depends, of course, on the specific
corpus, but we typically identify about 3.0 links per
sentence.
The algorithm is enhanced by a number of heuris-
tics:
393
 English match candidates are restricted to cap-
italized words (with a few exceptions).
 We use a list of about 200 Arabic and English
stopwords and stopword pairs.
 We use lists of countries and their adjective
forms to bridge cross-POS translations such
as Italy?s president on the English and ?J



KP
??A?K


A? @ (?Italian president?) on the Arabic side.
 Arabic prefixes such as ?/l- (?to?) are treated
in a special way, because they are translated,
not transliterated like the rest of the word. Link
(12) above is an example.
In this bitext mark-up process, we achieve 99.5%
precision and 95% recall based on a manual
visualization-tool based evaluation. Of the 5% re-
call error, 3% are due to noisy data in the bitext such
as typos, incorrect translations, or names missing on
one side of the bitext.
4.2 Training of Arabic name tagger
The task of the Arabic name tagger (or more
precisely, ?transliterate-me? tagger) is to predict
whether or not a word in an Arabic text should be
transliterated, and if so, whether it includes a prefix.
Prefixes such as ?/u- (?and?) have to be translated
rather than transliterated, so it is important to split
off any prefix from a name before transliterating that
name. This monolingual tagging task is not trivial,
as many Arabic words can be both a name and a non-
name. For example, ?QK


	
Qj
.
? @ (aljzyre) can mean both
Al-Jazeera and the island (or peninsula).
Features include the word itself plus two words
to the left and right, along with various prefixes,
suffixes and other characteristics of all of them, to-
talling about 250 features.
Some of our features depend on large corpus
statistics. For this, we divide the tagged Arabic
side of our training corpus into a stat section and
a core training section. From the stat section we col-
lect statistics as to how often every word, bigram or
trigram occurs, and what distribution of name/non-
name patterns these ngrams have. The name distri-
bution bigram

?K


P???@

?QK


	
Qj
.
? @ 3327 00:133 01:3193 11:1
(aljzyre alkurye/?peninsula Korean?) for example
tells us that in 3193 out of 3327 occurrences in the
stat corpus bitext, the first word is a marked up as
a non-name (?0?) and the second as a name (?1?),
which strongly suggests that in such a bigram con-
text, aljzyre better be translated as island or penin-
sula, and not be transliterated as Al-Jazeera.
We train our system on a corpus of 6 million stat
sentences, and 500; 000 core training sentences. We
employ a sequential tagger trained using the SEARN
algorithm (Daume? III et al, 2006) with aggressive
updates ( = 1). Our base learning algorithm
is an averaged perceptron, as implemented in the
MEGAM package2.
Reference Precision Recall F-meas.
Raw test corpus 87.4% 95.7% 91.4%
Adjusted for GS 92.1% 95.9% 94.0%
deficiencies
Table 1: Accuracy of ?transliterate-me? tagger
Testing on 10,000 sentences, we achieve preci-
sion of 87.4% and a recall of 95.7% with respect to
the automatically marked-up Gold Standard as de-
scribed in section 4.1. A manual error analysis of
500 sentences shows that a large portion are not er-
rors after all, but have been marked as errors because
of noise in the bitext and errors in the bitext mark-
up. After adjusting for these deficiencies in the gold
standard, we achieve precision of 92.1% and recall
of 95.9% in the name tagging task.
5 Integration with SMT
We use the following method to integrate our
transliterator into the overall SMT system:
1. We tag the Arabic source text using the tagger
described in the previous section.
2. We apply the transliterator described in section
3 to the tagged items. We limit this transliter-
ation to words that occur up to 50 times in the
training corpus for single token names (or up
to 100 and 150 times for two and three-word
names). We do this because the general SMT
mechanism tends to do well on more common
names, but does poorly on rare names (and will
2Freely available at http://hal3.name/megam
394
always drop names it has never seen in the
training bitext).
3. On the fly, we add transliterations to SMT
phrase table. Instead of a phrasal probability,
the transliterations have a special binary feature
set to 1. In a tuning step, the Minimim Error
Rate Training component of our SMT system
iteratively adjusts the set of rule weights, in-
cluding the weight associated with the translit-
eration feature, such that the English transla-
tions are optimized with respect to a set of
known reference translations according to the
BLEU translation metric.
4. At run-time, the transliterations then compete
with the translations generated by the gen-
eral SMT system. This means that the MT
system will not always use the transliterator
suggestions, depending on the combination of
language model, translation model, and other
component scores.
5.1 Multi-token names
We try to transliterate names as much as possible in
context. Consider for example the Arabic name:

?J


	
?? ?K
.
@
	
???K


(?yusf abu Sfye?)
If transliterated as single words without context,
the top results would be JosephjJosefjYusufjYosefj
Youssef, AbujAbojIvojApojIbo, and SephiajSofiaj
SophiajSafiehjSafia respectively. However, when
transliterating the three words together against our
list of 47 million English trigrams (section 3), the
transliterator will select the (correct) translation
Yousef Abu Safieh. Note that Yousef was not among
the top 5 choices, and that Safieh was only choice 4.
Similarly, when transliterating 	? A K
.
?

?? P@
	
P? ??
/umuzar ushuban (?and Mozart and Chopin?) with-
out context, the top results would be MoserjMauserj
MozerjMozartjMouser and ShuppanjShoppingj
SchwabenjSchuppanjShobana (with Chopin way
down on place 22). Checking our large English lists
for a matching name, name pattern, the transliterator
identifies the correct translation ?, Mozart, Chopin?.
Note that the transliteration module provides the
overall SMT system with up to 5 alternatives,
augmented with a choice of English translations
for the Arabic prefixes like the comma and the
conjunction and in the last example.
6 End-to-End results
We applied the NEWA metric (section 2) to both
our SMT translations as well as the four human ref-
erence translations, using both the original named-
entity translation annotation and the re-annotation:
Gold Standard BBN GS Re-annotated GS
Human 1 87.0% 85.0%
Human 2 85.3% 86.9%
Human 3 90.4% 91.8%
Human 4 86.5% 88.3%
SMT System 80.4% 89.7%
Table 2: Name translation accuracy with respect to BBN
and re-annotated Gold Standard on 1730 named entities
in 637 sentences.
Almost all scores went up with re-annotations, be-
cause the re-annotations more properly reward cor-
rect answers.
Based on the original annotations, all human
name translations were much better than our SMT
system. However, based on our re-annotation, the
results are quite different: our system has a higher
NEWA score and better name translations than 3 out
of 4 human annotators.
The evaluation results confirm that the original
annotation method produced a relative bias towards
the human translation its annotations were largely
based on, compared to other translations.
Table 3 provides more detailed NEWA results.
The addition of the transliteration module improves
our overall NEWA score from 87.8% to 89.7%, a
relative gain of 16% over base SMT system. For
names of persons (PER) and facilities (FAC), our
system outperforms all human translators. Hu-
mans performed much better on Person Nominals
(PER.Nom) such as Swede, Dutchmen, Americans.
Note that name translation quality varies greatly
between human translators, with error rates ranging
from 8.2-15.0% (absolute).
To make sure our name transliterator does not de-
grade the overall translation quality, we evaluated
our base SMT system with BLEU, as well as our
transliteration-augmented SMT system. Our stan-
dard newswire training set consists of 10.5 million
words of bitext (English side) and 1491 test sen-
395
NE Type Count Baseline SMT with Human 1 Human 2 Human 3 Human 4
SMT Transliteration
PER 342 266 (77.8%) 280 (81.9%) 210 (61.4%) 265 (77.5%) 278 (81.3%) 275 (80.4%)
GPE 910 863 (94.8%) 877 (96.4%) 867 (95.3%) 849 (93.3%) 885 (97.3%) 852 (93.6%)
ORG 332 280 (84.3%) 282 (84.9%) 263 (79.2%) 265 (79.8%) 293 (88.3%) 281 (84.6%)
FAC 27 18 (66.7%) 24 (88.9%) 21 (77.8%) 20 (74.1%) 22 (81.5%) 20 (74.1%)
PER.Nom 61 49 (80.3%) 48 (78.7%) 61 (100.0%) 56 (91.8%) 60 (98.4%) 57 (93.4%)
LOC 58 43 (74.1%) 41 (70.7%) 48 (82.8%) 48 (82.8%) 51 (87.9%) 43 (74.1%)
All types 1730 1519 (87.8%) 1552 (89.7%) 1470 (85.0%) 1503 (86.9%) 1589 (91.8%) 1528 (88.3%)
Table 3: Name translation accuracy in end-to-end statistical machine translation (SMT) system for different named
entity (NE) types: Person (PER), Geopolitical Entity, which includes countries, provinces and towns (GPE), Organi-
zation (ORG), Facility (FAC), Nominal Person, e.g. Swede (PER.Nom), other location (LOC).
tences. The BLEU scores for the two systems were
50.70 and 50.96 respectively.
Finally, here are end-to-end machine translation
results for three sentences, with and without the
transliteration module, along with a human refer-
ence translation.
Old: Al-Basha leads a broad list of musicians such
as Bach.
New: Al-Basha leads a broad list of musical acts
such as Bach, Mozart, Beethoven, Chopin, Schu-
mann, Rachmaninoff, Ravel and Prokofiev.
Ref: Al-Bacha performs a long list of works by
composers such as Bach, Chopin, Beethoven,
Shumann, Rakmaninov, Ravel and Prokoviev.
Old: Earlier Israeli military correspondent turn
introduction programme ?Entertainment Bui?
New: Earlier Israeli military correspondent turn to
introduction of the programme ?Play Boy?
Ref: Former Israeli military correspondent turns
host for ?Playboy? program
Old: The Nikkei president company De Beers said
that ...
New: The company De Beers chairman Nicky Op-
penheimer said that ...
Ref: Nicky Oppenheimer, chairman of the De Beers
company, stated that ...
7 Discussion
We have shown that a state-of-the-art statistical ma-
chine translation system can benefit from a dedi-
cated transliteration module to improve the transla-
tion of rare names. Improved named entity transla-
tion accuracy as measured by the NEWA metric in
general, and a reduction in dropped names in par-
ticular is clearly valuable to the human reader of
machine translated documents as well as for sys-
tems using machine translation for further informa-
tion processing. At the same time, there has been no
negative impact on overall quality as measured by
BLEU.
We believe that all components can be further im-
proved, e.g.
 Automatically retune the weights in the
transliteration scoring model.
 Improve robustness with respect to typos, in-
correct or missing translations, and badly
aligned sentences when marking up bitexts.
 Add more features for learning whether or not
a word should be transliterated, possibly using
source language morphology to better identify
non-name words never or rarely seen during
training.
Additionally, our transliteration method could be ap-
plied to other language pairs.
We find it encouraging that we already outper-
form some professional translators in name transla-
tion accuracy. The potential to exceed human trans-
lator performance arises from the patience required
to translate names right.
Acknowledgment
This research was supported under DARPA Contract
No. HR0011-06-C-0022.
396
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
Transliteration of Names in Arabic Text. In Proceed-
ings of the Association for Computational Linguistics
Workshop on Computational Approaches to Semitic
Languages.
Thorsten Brants, Alex Franz. 2006. Web 1T 5-gram
Version 1. Released by Google through the Linguis-
tic Data Consortium, Philadelphia, as LDC2006T13.
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, and
Shih-Chung Tsai. 1998. Proper Name Translation in
Cross-Language Information Retrieval. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics.
Hal Daume? III, John Langford, and Daniel Marcu.
2006. Search-based Structured Prediction.
Submitted to the Machine Learning Journal.
http://pub.hal3.name/#daume06searn
David Day. 2007. Entity Translation 2007 Pilot Evalua-
tion (ET07). In proceedings of the Workshop on Auto-
matic Content Extraction (ACE). College Park, Mary-
land.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
Transliteration and Back-transliteration by Decision
Tree Learning. In Conference on Language Resources
and Evaluation.
Mehdi M. Kashani, Fred Popowich, and Fatiha Sadat.
2007. Automatic Transliteration of Proper Nouns
from Arabic to English. The Challenge of Arabic For
NLP/MT, 76-84.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilingual
comparable corpora. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics.
Li Haizhou, Zhang Min, and Su Jian. 2004. A Joint
Source-Channel Model for Machine Transliteration.
In Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward Ma-
chine Transliteration by Learning Phonetic Similar-
ity. Sixth Conference on Natural Language Learning,
Taipei, Taiwan, 2002.
David Matthews. 2007. Machine Transliteration of
Proper Names. Master?s Thesis. School of Informat-
ics. University of Edinburgh.
Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001.
Using the Web as a Bilingual Dictionary. In Proceed-
ings of the Workshop on Data-driven Methods in Ma-
chine Translation.
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, Irina
Temnikova, Anna Widiger, Wajdi Zaghouani, and Jan
Zizka. 2006. Multilingual Person Name Recognition
and Transliteration. CORELA - COgnition, REpre-
sentation, LAnguage, Poitiers, France. Volume 3/3,
number 2, pp. 115-123.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
Based Transliteration. In Proceedings of the 45th An-
nual Meeting on Association for Computational Lin-
guistics.
Richard Sproat, ChengXiang Zhai, and Tao Tao. 2006.
Named Entity Transliteration with Comparable Cor-
pora. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting on Association for Computational Lin-
guistics.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating Names and Technical Terms in Arabic Text. In
Proceedings of the COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages.
Stephen Wan and Cornelia Verspoor. 1998. Automatic
English-Chinese Name Transliteration for Develop-
ment of Multilingual Resources. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics. Montreal, Canada.
397
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 504?512,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Minimized Models for Unsupervised Part-of-Speech Tagging
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We describe a novel method for the task
of unsupervised POS tagging with a dic-
tionary, one that uses integer programming
to explicitly search for the smallest model
that explains the data, and then uses EM
to set parameter values. We evaluate our
method on a standard test corpus using
different standard tagsets (a 45-tagset as
well as a smaller 17-tagset), and show that
our approach performs better than existing
state-of-the-art systems in both settings.
1 Introduction
In recent years, we have seen increased interest in
using unsupervised methods for attacking differ-
ent NLP tasks like part-of-speech (POS) tagging.
The classic Expectation Maximization (EM) algo-
rithm has been shown to perform poorly on POS
tagging, when compared to other techniques, such
as Bayesian methods.
In this paper, we develop new methods for un-
supervised part-of-speech tagging. We adopt the
problem formulation of Merialdo (1994), in which
we are given a raw word sequence and a dictio-
nary of legal tags for each word type. The goal is
to tag each word token so as to maximize accuracy
against a gold tag sequence. Whether this is a real-
istic problem set-up is arguable, but an interesting
collection of methods and results has accumulated
around it, and these can be clearly compared with
one another.
We use the standard test set for this task, a
24,115-word subset of the Penn Treebank, for
which a gold tag sequence is available. There
are 5,878 word types in this test set. We use
the standard tag dictionary, consisting of 57,388
word/tag pairs derived from the entire Penn Tree-
bank.1 8,910 dictionary entries are relevant to the
5,878 word types in the test set. Per-token ambigu-
ity is about 1.5 tags/token, yielding approximately
106425 possible ways to tag the data. There are 45
distinct grammatical tags. In this set-up, there are
no unknown words.
Figure 1 shows prior results for this prob-
lem. While the methods are quite different,
they all make use of two common model ele-
ments. One is a probabilistic n-gram tag model
P(ti|ti?n+1...ti?1), which we call the grammar.
The other is a probabilistic word-given-tag model
P(wi|ti), which we call the dictionary.
The classic approach (Merialdo, 1994) is
expectation-maximization (EM), where we esti-
mate grammar and dictionary probabilities in or-
der to maximize the probability of the observed
word sequence:
P (w1...wn) =
?
t1...tn
P (t1...tn) ? P (w1...wn|t1...tn)
?
?
t1...tn
n?
i=1
P (ti|ti?2 ti?1) ? P (wi|ti)
Goldwater and Griffiths (2007) report 74.5%
accuracy for EM with a 3-gram tag model, which
we confirm by replication. They improve this to
83.9% by employing a fully Bayesian approach
which integrates over all possible parameter val-
ues, rather than estimating a single distribution.
They further improve this to 86.8% by using pri-
ors that favor sparse distributions. Smith and Eis-
ner (2005) employ a contrastive estimation tech-
1As (Banko and Moore, 2004) point out, unsupervised
tagging accuracy varies wildly depending on the dictionary
employed. We follow others in using a fat dictionary (with
49,206 distinct word types), rather than a thin one derived
only from the test set.
504
System Tagging accuracy (%)
on 24,115-word corpus
1. Random baseline (for each word, pick a random tag from the alternatives given by
the word/tag dictionary)
64.6
2. EM with 2-gram tag model 81.7
3. EM with 3-gram tag model 74.5
4a. Bayesian method (Goldwater and Griffiths, 2007) 83.9
4b. Bayesian method with sparse priors (Goldwater and Griffiths, 2007) 86.8
5. CRF model trained using contrastive estimation (Smith and Eisner, 2005) 88.6
6. EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008) 91.4*
(*uses linguistic constraints and manual adjustments to the dictionary)
Figure 1: Previous results on unsupervised POS tagging using a dictionary (Merialdo, 1994) on the full
45-tag set. All other results reported in this paper (unless specified otherwise) are on the 45-tag set as
well.
nique, in which they automatically generate nega-
tive examples and use CRF training.
In more recent work, Toutanova and John-
son (2008) propose a Bayesian LDA-based gener-
ative model that in addition to using sparse priors,
explicitly groups words into ambiguity classes.
They show considerable improvements in tagging
accuracy when using a coarser-grained version
(with 17-tags) of the tag set from the Penn Tree-
bank.
Goldberg et al (2008) depart from the Bayesian
framework and show how EM can be used to learn
good POS taggers for Hebrew and English, when
provided with good initial conditions. They use
language specific information (like word contexts,
syntax and morphology) for learning initial P(t|w)
distributions and also use linguistic knowledge to
apply constraints on the tag sequences allowed by
their models (e.g., the tag sequence ?V V? is dis-
allowed). Also, they make other manual adjust-
ments to reduce noise from the word/tag dictio-
nary (e.g., reducing the number of tags for ?the?
from six to just one). In contrast, we keep all the
original dictionary entries derived from the Penn
Treebank data for our experiments.
The literature omits one other baseline, which
is EM with a 2-gram tag model. Here we obtain
81.7% accuracy, which is better than the 3-gram
model. It seems that EM with a 3-gram tag model
runs amok with its freedom. For the rest of this pa-
per, we will limit ourselves to a 2-gram tag model.
2 What goes wrong with EM?
We analyze the tag sequence output produced by
EM and try to see where EM goes wrong. The
overall POS tag distribution learnt by EM is rel-
atively uniform, as noted by Johnson (2007), and
it tends to assign equal number of tokens to each
tag label whereas the real tag distribution is highly
skewed. The Bayesian methods overcome this ef-
fect by using priors which favor sparser distribu-
tions. But it is not easy to model such priors into
EM learning. As a result, EM exploits a lot of rare
tags (like FW = foreign word, or SYM = symbol)
and assigns them to common word types (in, of,
etc.).
We can compare the tag assignments from the
gold tagging and the EM tagging (Viterbi tag se-
quence). The table below shows tag assignments
(and their counts in parentheses) for a few word
types which occur frequently in the test corpus.
word/tag dictionary Gold tagging EM tagging
in? {IN, RP, RB, NN, FW, RBR} IN (355) IN (0)
RP (3) RP (0)
FW (0) FW (358)
of? {IN, RP, RB} IN (567) IN (0)
RP (0) RP (567)
on? {IN,RP, RB} RP (5) RP (127)
IN (129) IN (0)
RB (0) RB (7)
a? {DT, JJ, IN, LS, FW, SYM, NNP} DT (517) DT (0)
SYM (0) SYM (517)
We see how the rare tag labels (like FW, SYM,
etc.) are abused by EM. As a result, many word to-
kens which occur very frequently in the corpus are
incorrectly tagged with rare tags in the EM tagging
output.
We also look at things more globally. We inves-
tigate the Viterbi tag sequence generated by EM
training and count how many distinct tag bigrams
there are in that sequence. We call this the ob-
served grammar size, and it is 915. That is, in
tagging the 24,115 test tokens, EM uses 915 of the
available 45 ? 45 = 2025 tag bigrams.2 The ad-
vantage of the observed grammar size is that we
2We contrast observed size with the model size for the
grammar, which we define as the number of P(t2|t1) entries
in EM?s trained tag model that exceed 0.0001 probability.
505
L8
L0
they    can          fish       .       I        fish
L1
L2 L3
L4
L6
L5 L7
L9
L10
L11
START
PRO
AUX
V
N
PUNC
d1 PRO-they
d2 AUX-can
d3 V-can
d4 N-fish
d5 V-fish
d6 PUNC-.
d7 PRO-I
g1 PRO-AUX
g2 PRO-V
g3 AUX-N
g4  AUX-V
g5 V-N
g6 V-V
g7 N-PUNC
g8 V-PUNC
g9 PUNC-PRO
g10 PRO-N
dictionary 
variables
grammar 
variables
Integer Program
Minimize: ?i=1?10 gi
Constraints:
1. Single left-to-right path (at each node, flow in = flow out)
e.g.,  L0 = 1
L1 = L3 + L4
2. Path consistency constraints (chosen path respects chosen 
dictionary & grammar)
e.g., L0 ? d1
L1 ? g1
IP formulation
training text
link 
variables
Figure 2: Integer Programming formulation for finding the smallest grammar that explains a given word
sequence. Here, we show a sample word sequence and the corresponding IP network generated for that
sequence.
can compare it with the gold tagging?s observed
grammar size, which is 760. So we can safely say
that EM is learning a grammar that is too big, still
abusing its freedom.
3 Small Models
Bayesian sparse priors aim to create small mod-
els. We take a different tack in the paper and
directly ask: What is the smallest model that ex-
plains the text? Our approach is related to mini-
mum description length (MDL). We formulate our
question precisely by asking which tag sequence
(of the 106425 available) has the smallest observed
grammar size. The answer is 459. That is, there
exists a tag sequence that contains 459 distinct tag
bigrams, and no other tag sequence contains fewer.
We obtain this answer by formulating the prob-
lem in an integer programming (IP) framework.
Figure 2 illustrates this with a small sample word
sequence. We create a network of possible tag-
gings, and we assign a binary variable to each link
in the network. We create constraints to ensure
that those link variables receiving a value of 1
form a left-to-right path through the tagging net-
work, and that all other link variables receive a
value of 0. We accomplish this by requiring the
sum of the links entering each node to equal to
the sum of the links leaving each node. We also
create variables for every possible tag bigram and
word/tag dictionary entry. We constrain link vari-
able assignments to respect those grammar and
dictionary variables. For example, we do not allow
a link variable to ?activate? unless the correspond-
ing grammar variable is also ?activated?. Finally,
we add an objective function that minimizes the
number of grammar variables that are assigned a
value of 1.
Figure 3 shows the IP solution for the example
word sequence from Figure 2. Of course, a small
grammar size does not necessarily correlate with
higher tagging accuracy. For the small toy exam-
ple shown in Figure 3, the correct tagging is ?PRO
AUX V . PRO V? (with 5 tag pairs), whereas the
IP tries to minimize the grammar size and picks
another solution instead.
For solving the integer program, we use CPLEX
software (a commercial IP solver package). Alter-
natively, there are other programs such as lp solve,
which are free and publicly available for use. Once
we create an integer program for the full test cor-
pus, and pass it to CPLEX, the solver returns an
506
word sequence: they can fish . I fish
Tagging Grammar Size
PRO AUX N . PRO N 5
PRO AUX V . PRO N 5
PRO AUX N . PRO V 5
PRO AUX V . PRO V 5
PRO V N . PRO N 5
PRO V V . PRO N 5
PRO V N . PRO V 4
PRO V V . PRO V 4
Figure 3: Possible tagging solutions and corre-
sponding grammar sizes for the sample word se-
quence from Figure 2 using the given dictionary
and grammar. The IP solver finds the smallest
grammar set that can explain the given word se-
quence. In this example, there exist two solutions
that each contain only 4 tag pair entries, and IP
returns one of them.
objective function value of 459.3
CPLEX also returns a tag sequence via assign-
ments to the link variables. However, there are
actually 104378 tag sequences compatible with the
459-sized grammar, and our IP solver just selects
one at random. We find that of all those tag se-
quences, the worst gives an accuracy of 50.8%,
and the best gives an accuracy of 90.3%. We
also note that CPLEX takes 320 seconds to return
the optimal solution for the integer program corre-
sponding to this particular test data (24,115 tokens
with the 45-tag set). It might be interesting to see
how the performance of the IP method (in terms
of time complexity) is affected when scaling up to
larger data and bigger tagsets. We leave this as
part of future work. But we do note that it is pos-
sible to obtain less than optimal solutions faster by
interrupting the CPLEX solver.
4 Fitting the Model
Our IP formulation can find us a small model, but
it does not attempt to fit the model to the data. For-
tunately, we can use EM for that. We still give
EM the full word/tag dictionary, but now we con-
strain its initial grammar model to the 459 tag bi-
grams identified by IP. Starting with uniform prob-
abilities, EM finds a tagging that is 84.5% accu-
rate, substantially better than the 81.7% originally
obtained with the fully-connected grammar. So
we see a benefit to our explicit small-model ap-
proach. While EM does not find the most accurate
3Note that the grammar identified by IP is not uniquely
minimal. For the same word sequence, there exist other min-
imal grammars having the same size (459 entries). In our ex-
periments, we choose the first solution returned by CPLEX.
in on
IN IN
RP RP
word/tag dictionary RB RB
NN
FW
RBR
observed EM dictionary FW (358) RP (127)
RB (7)
observed IP+EM dictionary IN (349) IN (126)
RB (9) RB (8)
observed gold dictionary IN (355) IN (129)
RB (3) RP (5)
Figure 4: Examples of tagging obtained from dif-
ferent systems for prepositions in and on.
sequence consistent with the IP grammar (90.3%),
it finds a relatively good one.
The IP+EM tagging (with 84.5% accuracy) has
some interesting properties. First, the dictionary
we observe from the tagging is of higher qual-
ity (with fewer spurious tagging assignments) than
the one we observe from the original EM tagging.
Figure 4 shows some examples.
We also measure the quality of the two observed
grammars/dictionaries by computing their preci-
sion and recall against the grammar/dictionary we
observe in the gold tagging.4 We find that preci-
sion of the observed grammar increases from 0.73
(EM) to 0.94 (IP+EM). In addition to removing
many bad tag bigrams from the grammar, IP min-
imization also removes some of the good ones,
leading to lower recall (EM = 0.87, IP+EM =
0.57). In the case of the observed dictionary, using
a smaller grammar model does not affect the pre-
cision (EM = 0.91, IP+EM = 0.89) or recall (EM
= 0.89, IP+EM = 0.89).
During EM training, the smaller grammar with
fewer bad tag bigrams helps to restrict the dictio-
nary model from making too many bad choices
that EM made earlier. Here are a few examples
of bad dictionary entries that get removed when
we use the minimized grammar for EM training:
in ? FW
a ? SYM
of ? RP
In ? RBR
During EM training, the minimized grammar
4For any observed grammar or dictionary X,
Precision (X) =
|{X}?{observedgold}|
|{X}|
Recall (X) =
|{X}?{observedgold}|
|{observedgold}|
507
Model Tagging accuracy Observed size Model size
on 24,115-word
corpus
grammar(G), dictionary(D) grammar(G), dictionary(D)
1. EM baseline with full grammar + full dictio-
nary
81.7 G=915, D=6295 G=935, D=6430
2. EM constrained with minimized IP-grammar
+ full dictionary
84.5 G=459, D=6318 G=459, D=6414
3. EM constrained with full grammar + dictio-
nary from (2)
91.3 G=606, D=6245 G=612, D=6298
4. EM constrained with grammar from (3) + full
dictionary
91.5 G=593, D=6285 G=600, D=6373
5. EM constrained with full grammar + dictio-
nary from (4)
91.6 G=603, D=6280 G=618, D=6337
Figure 5: Percentage of word tokens tagged correctly by different models. The observed sizes and model
sizes of grammar (G) and dictionary (D) produced by these models are shown in the last two columns.
helps to eliminate many incorrect entries (i.e.,
zero out model parameters) from the dictionary,
thereby yielding an improved dictionary model.
So using the minimized grammar (which has
higher precision) helps to improve the quality of
the chosen dictionary (examples shown in Fig-
ure 4). This in turn helps improve the tagging ac-
curacy from 81.7% to 84.5%. It is clear that the
IP-constrained grammar is a better choice to run
EM on than the full grammar.
Note that we used a very small IP-grammar
(containing only 459 tag bigrams) during EM
training. In the process of minimizing the gram-
mar size, IP ends up removing many good tag bi-
grams from our grammar set (as seen from the low
measured recall of 0.57 for the observed gram-
mar). Next, we proceed to recover some good tag
bigrams and expand the grammar in a restricted
fashion by making use of the higher-quality dic-
tionary produced by the IP+EM method. We now
run EM again on the full grammar (all possible
tag bigrams) in combination with this good dictio-
nary (containing fewer entries than the full dictio-
nary). Unlike the original training with full gram-
mar, where EM could choose any tag bigram, now
the choice of grammar entries is constrained by
the good dictionary model that we provide EM
with. This allows EM to recover some of the
good tag pairs, and results in a good grammar-
dictionary combination that yields better tagging
performance.
With these improvements in mind, we embark
on an alternating scheme to find better models and
taggings. We run EM for multiple passes, and in
each pass we alternately constrain either the gram-
mar model or the dictionary model. The procedure
is simple and proceeds as follows:
1. Run EM constrained to the last trained dictio-
nary, but provided with a full grammar.5
2. Run EM constrained to the last trained gram-
mar, but provided with a full dictionary.
3. Repeat steps 1 and 2.
We notice significant gains in tagging perfor-
mance when applying this technique. The tagging
accuracy increases at each step and finally settles
at a high of 91.6%, which outperforms the exist-
ing state-of-the-art systems for the 45-tag set. The
system achieves a better accuracy than the 88.6%
from Smith and Eisner (2005), and even surpasses
the 91.4% achieved by Goldberg et al (2008)
without using any additional linguistic constraints
or manual cleaning of the dictionary. Figure 5
shows the tagging performance achieved at each
step. We found that it is the elimination of incor-
rect entries from the dictionary (and grammar) and
not necessarily the initialization weights from pre-
vious EM training, that results in the tagging im-
provements. Initializing the last trained dictionary
or grammar at each step with uniform weights also
yields the same tagging improvements as shown in
Figure 5.
We find that the observed grammar also im-
proves, growing from 459 entries to 603 entries,
with precision increasing from 0.94 to 0.96, and
recall increasing from 0.57 to 0.76. The figure
also shows the model?s internal grammar and dic-
tionary sizes.
Figure 6 and 7 show how the precision/recall
of the observed grammar and dictionary varies for
different models from Figure 5. In the case of the
observed grammar (Figure 6), precision increases
5For all experiments, EM training is allowed to run for
40 iterations or until the likelihood ratios between two subse-
quent iterations reaches a value of 0.99999, whichever occurs
earlier.
508
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Pre
cis
ion
 / R
eca
ll o
f o
bse
rve
d g
ram
ma
r
Tagging Model
Model 1 Model 2 Model 3 Model 4 Model 5
PrecisionRecall
Figure 6: Comparison of observed grammars from
the model tagging vs. gold tagging in terms of pre-
cision and recall measures.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Pre
cis
ion
 / R
eca
ll o
f o
bse
rve
d d
icti
on
ary
Tagging Model
Model 1 Model 2 Model 3 Model 4 Model 5
PrecisionRecall
Figure 7: Comparison of observed dictionaries from
the model tagging vs. gold tagging in terms of pre-
cision and recall measures.
Model Tagging accuracy on
24,115-word corpus
no-restarts with 100 restarts
1. Model 1 (EM baseline) 81.7 83.8
2. Model 2 84.5 84.5
3. Model 3 91.3 91.8
4. Model 4 91.5 91.8
5. Model 5 91.6 91.8
Figure 8: Effect of random restarts (during EM
training) on tagging accuracy.
at each step, whereas recall drops initially (owing
to the grammar minimization) but then picks up
again. The precision/recall of the observed dictio-
nary on the other hand, is not affected by much.
5 Restarts and More Data
Multiple random restarts for EM, while not often
emphasized in the literature, are key in this do-
main. Recall that our original EM tagging with a
fully-connected 2-gram tag model was 81.7% ac-
curate. When we execute 100 random restarts and
select the model with the highest data likelihood,
we get 83.8% accuracy. Likewise, when we ex-
tend our alternating EM scheme to 100 random
restarts at each step, we improve our tagging ac-
curacy from 91.6% to 91.8% (Figure 8).
As noted by Toutanova and Johnson (2008),
there is no reason to limit the amount of unlabeled
data used for training the models. Their models
are trained on the entire Penn Treebank data (in-
stead of using only the 24,115-token test data),
and so are the tagging models used by Goldberg
et al (2008). But previous results from Smith and
Eisner (2005) and Goldwater and Griffiths (2007)
show that their models do not benefit from using
more unlabeled training data. Because EM is ef-
ficient, we can extend our word-sequence train-
ing data from the 24,115-token set to the entire
Penn Treebank (973k tokens). We run EM training
again for Model 5 (the best model from Figure 5)
but this time using 973k word tokens, and further
increase our accuracy to 92.3%. This is our final
result on the 45-tagset, and we note that it is higher
than previously reported results.
6 Smaller Tagset and Incomplete
Dictionaries
Previously, researchers working on this task have
also reported results for unsupervised tagging with
a smaller tagset (Smith and Eisner, 2005; Gold-
water and Griffiths, 2007; Toutanova and John-
son, 2008; Goldberg et al, 2008). Their systems
were shown to obtain considerable improvements
in accuracy when using a 17-tagset (a coarser-
grained version of the tag labels from the Penn
Treebank) instead of the 45-tagset. When tag-
ging the same standard test corpus with the smaller
17-tagset, our method is able to achieve a sub-
stantially high accuracy of 96.8%, which is the
best result reported so far on this task. The ta-
ble in Figure 9 shows a comparison of different
systems for which tagging accuracies have been
reported previously for the 17-tagset case (Gold-
berg et al, 2008). The first row in the table
compares tagging results when using a full dictio-
nary (i.e., a lexicon containing entries for 49,206
word types). The InitEM-HMM system from
Goldberg et al (2008) reports an accuracy of
93.8%, followed by the LDA+AC model (Latent
Dirichlet Allocation model with a strong Ambigu-
ity Class component) from Toutanova and John-
son (2008). In comparison, the Bayesian HMM
(BHMM) model from Goldwater et al (2007) and
509
Dict IP+EM (24k) InitEM-HMM LDA+AC CE+spl BHMM
Full (49206 words) 96.8 (96.8) 93.8 93.4 88.7 87.3
? 2 (2141 words) 90.6 (90.0) 89.4 91.2 79.5 79.6
? 3 (1249 words) 88.0 (86.1) 87.4 89.7 78.4 71
Figure 9: Comparison of different systems for English unsupervised POS tagging with 17 tags.
the CE+spl model (Contrastive Estimation with a
spelling model) from Smith and Eisner (2005) re-
port lower accuracies (87.3% and 88.7%, respec-
tively). Our system (IP+EM) which uses inte-
ger programming and EM, gets the highest accu-
racy (96.8%). The accuracy numbers reported for
Init-HMM and LDA+AC are for models that are
trained on all the available unlabeled data from
the Penn Treebank. The IP+EM models used in
the 17-tagset experiments reported here were not
trained on the entire Penn Treebank, but instead
used a smaller section containing 77,963 tokens
for estimating model parameters. We also include
the accuracies for our IP+EM model when using
only the 24,115 token test corpus for EM estima-
tion (shown within parenthesis in second column
of the table in Figure 9). We find that our perfor-
mance does not degrade when the parameter esti-
mation is done using less data, and our model still
achieves a high accuracy of 96.8%.
6.1 Incomplete Dictionaries and Unknown
Words
The literature also includes results reported in a
different setting for the tagging problem. In some
scenarios, a complete dictionary with entries for
all word types may not be readily available to us
and instead, we might be provided with an incom-
plete dictionary that contains entries for only fre-
quent word types. In such cases, any word not
appearing in the dictionary will be treated as an
unknown word, and can be labeled with any of
the tags from given tagset (i.e., for every unknown
word, there are 17 tag possibilities). Some pre-
vious approaches (Toutanova and Johnson, 2008;
Goldberg et al, 2008) handle unknown words ex-
plicitly using ambiguity class components condi-
tioned on various morphological features, and this
has shown to produce good tagging results, espe-
cially when dealing with incomplete dictionaries.
We follow a simple approach using just one
of the features used in (Toutanova and Johnson,
2008) for assigning tag possibilities to every un-
known word. We first identify the top-100 suffixes
(up to 3 characters) for words in the dictionary.
Using the word/tag pairs from the dictionary, we
train a simple probabilistic model that predicts the
tag given a particular suffix (e.g., P(VBG | ing) =
0.97, P(N | ing) = 0.0001, ...). Next, for every un-
known word ?w?, the trained P(tag | suffix) model
is used to predict the top 3 tag possibilities for
?w? (using only its suffix information), and subse-
quently this word along with its 3 tags are added as
a new entry to the lexicon. We do this for every un-
known word, and eventually we have a dictionary
containing entries for all the words. Once the com-
pleted lexicon (containing both correct entries for
words in the lexicon and the predicted entries for
unknown words) is available, we follow the same
methodology from Sections 3 and 4 using integer
programming to minimize the size of the grammar
and then applying EM to estimate parameter val-
ues.
Figure 9 shows comparative results for the 17-
tagset case when the dictionary is incomplete. The
second and third rows in the table shows tagging
accuracies for different systems when a cutoff of
2 (i.e., all word types that occur with frequency
counts < 2 in the test corpus are removed) and
a cutoff of 3 (i.e., all word types occurring with
frequency counts < 3 in the test corpus are re-
moved) is applied to the dictionary. This yields
lexicons containing 2,141 and 1,249 words respec-
tively, which are much smaller compared to the
original 49,206 word dictionary. As the results
in Figure 9 illustrate, the IP+EM method clearly
does better than all the other systems except for
the LDA+AC model. The LDA+AC model from
Toutanova and Johnson (2008) has a strong ambi-
guity class component and uses more features to
handle the unknown words better, and this con-
tributes to the slightly higher performance in the
incomplete dictionary cases, when compared to
the IP+EM model.
7 Discussion
The method proposed in this paper is simple?
once an integer program is produced, there are
solvers available which directly give us the so-
lution. In addition, we do not require any com-
plex parameter estimation techniques; we train our
models using simple EM, which proves to be effi-
cient for this task. While some previous methods
510
word type Gold tag Automatic tag # of tokens tagged incorrectly
?s POS VBZ 173
be VB VBP 67
that IN WDT 54
New NNP NNPS 33
U.S. NNP JJ 31
up RP RB 28
more RBR JJR 27
and CC IN 23
have VB VBP 20
first JJ JJS 20
to TO IN 19
out RP RB 17
there EX RB 15
stock NN JJ 15
what WP WDT 14
one CD NN 14
? POS : 14
as RB IN 14
all DT RB 14
that IN RB 13
Figure 10: Most frequent mistakes observed in the model tagging (using the best model, which gives
92.3% accuracy) when compared to the gold tagging.
introduced for the same task have achieved big
tagging improvements using additional linguistic
knowledge or manual supervision, our models are
not provided with any additional information.
Figure 10 illustrates for the 45-tag set some of
the common mistakes that our best tagging model
(92.3%) makes. In some cases, the model actually
gets a reasonable tagging but is penalized perhaps
unfairly. For example, ?to? is tagged as IN by our
model sometimes when it occurs in the context of
a preposition, whereas in the gold tagging it is al-
ways tagged as TO. The model also gets penalized
for tagging the word ?U.S.? as an adjective (JJ),
which might be considered valid in some cases
such as ?the U.S. State Department?. In other
cases, the model clearly produces incorrect tags
(e.g., ?New? gets tagged incorrectly as NNPS).
Our method resembles the classic Minimum
Description Length (MDL) approach for model
selection (Barron et al, 1998). In MDL, there
is a single objective function to (1) maximize the
likelihood of observing the data, and at the same
time (2) minimize the length of the model descrip-
tion (which depends on the model size). How-
ever, the search procedure for MDL is usually
non-trivial, and for our task of unsupervised tag-
ging, we have not found a direct objective function
which we can optimize and produce good tagging
results. In the past, only a few approaches uti-
lizing MDL have been shown to work for natural
language applications. These approaches employ
heuristic search methods with MDL for the task
of unsupervised learning of morphology of natu-
ral languages (Goldsmith, 2001; Creutz and La-
gus, 2002; Creutz and Lagus, 2005). The method
proposed in this paper is the first application of
the MDL idea to POS tagging, and the first to
use an integer programming formulation rather
than heuristic search techniques. We also note
that it might be possible to replicate our models
in a Bayesian framework similar to that proposed
in (Goldwater and Griffiths, 2007).
8 Conclusion
We presented a novel method for attacking
dictionary-based unsupervised part-of-speech tag-
ging. Our method achieves a very high accuracy
(92.3%) on the 45-tagset and a higher (96.8%) ac-
curacy on a smaller 17-tagset. The method works
by explicitly minimizing the grammar size using
integer programming, and then using EM to esti-
mate parameter values. The entire process is fully
automated and yields better performance than any
existing state-of-the-art system, even though our
models were not provided with any additional lin-
guistic knowledge (for example, explicit syntactic
constraints to avoid certain tag combinations such
as ?V V?, etc.). However, it is easy to model some
of these linguistic constraints (both at the local and
global levels) directly using integer programming,
and this may result in further improvements and
lead to new possibilities for future research. For
direct comparison to previous works, we also pre-
sented results for the case when the dictionaries
are incomplete and find the performance of our
system to be comparable with current best results
reported for the same task.
9 Acknowledgements
This research was supported by the Defense
Advanced Research Projects Agency under
SRI International?s prime Contract Number
NBCHD040058.
511
References
M. Banko and R. C. Moore. 2004. Part of speech
tagging in context. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
A. Barron, J. Rissanen, and B. Yu. 1998. The min-
imum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACL Work-
shop on Morphological and Phonological Learning
of.
M. Creutz and K. Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Publications
in Computer and Information Science, Report A81,
Helsinki University of Technology, March.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL.
M. Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In
Proceedings of the ACL.
K. Toutanova and M. Johnson. 2008. A Bayesian
LDA-based model for semi-supervised part-of-
speech tagging. In Proceedings of the Advances in
Neural Information Processing Systems (NIPS).
512
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567?575,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Fast Consensus Decoding over Translation Forests
John DeNero David Chiang and Kevin Knight
Computer Science Division Information Sciences Institute
University of California, Berkeley University of Southern California
denero@cs.berkeley.edu {chiang, knight}@isi.edu
Abstract
The minimum Bayes risk (MBR) decoding ob-
jective improves BLEU scores for machine trans-
lation output relative to the standard Viterbi ob-
jective of maximizing model score. However,
MBR targeting BLEU is prohibitively slow to op-
timize over k-best lists for large k. In this pa-
per, we introduce and analyze an alternative to
MBR that is equally effective at improving per-
formance, yet is asymptotically faster ? running
80 times faster than MBR in experiments with
1000-best lists. Furthermore, our fast decoding
procedure can select output sentences based on
distributions over entire forests of translations, in
addition to k-best lists. We evaluate our proce-
dure on translation forests from two large-scale,
state-of-the-art hierarchical machine translation
systems. Our forest-based decoding objective
consistently outperforms k-best list MBR, giving
improvements of up to 1.0 BLEU.
1 Introduction
In statistical machine translation, output transla-
tions are evaluated by their similarity to human
reference translations, where similarity is most of-
ten measured by BLEU (Papineni et al, 2002).
A decoding objective specifies how to derive final
translations from a system?s underlying statistical
model. The Bayes optimal decoding objective is
to minimize risk based on the similarity measure
used for evaluation. The corresponding minimum
Bayes risk (MBR) procedure maximizes the ex-
pected similarity score of a system?s translations
relative to the model?s distribution over possible
translations (Kumar and Byrne, 2004). Unfortu-
nately, with a non-linear similarity measure like
BLEU, we must resort to approximating the ex-
pected loss using a k-best list, which accounts for
only a tiny fraction of a model?s full posterior dis-
tribution. In this paper, we introduce a variant
of the MBR decoding procedure that applies effi-
ciently to translation forests. Instead of maximiz-
ing expected similarity, we express similarity in
terms of features of sentences, and choose transla-
tions that are similar to expected feature values.
Our exposition begins with algorithms over k-
best lists. A na??ve algorithm for finding MBR
translations computes the similarity between every
pair of k sentences, entailing O(k2) comparisons.
We show that if the similarity measure is linear in
features of a sentence, then computing expected
similarity for all k sentences requires only k sim-
ilarity evaluations. Specific instances of this gen-
eral algorithm have recently been proposed for two
linear similarity measures (Tromble et al, 2008;
Zhang and Gildea, 2008).
However, the sentence similarity measures we
want to optimize in MT are not linear functions,
and so this fast algorithm for MBR does not ap-
ply. For this reason, we propose a new objective
that retains the benefits of MBR, but can be op-
timized efficiently, even for non-linear similarity
measures. In experiments using BLEU over 1000-
best lists, we found that our objective provided
benefits very similar to MBR, only much faster.
This same decoding objective can also be com-
puted efficiently from forest-based expectations.
Translation forests compactly encode distributions
over much larger sets of derivations and arise nat-
urally in chart-based decoding for a wide variety
of hierarchical translation systems (Chiang, 2007;
Galley et al, 2006; Mi et al, 2008; Venugopal
et al, 2007). The resulting forest-based decoding
procedure compares favorably in both complexity
and performance to the recently proposed lattice-
based MBR (Tromble et al, 2008).
The contributions of this paper include a linear-
time algorithm for MBR using linear similarities,
a linear-time alternative to MBR using non-linear
similarity measures, and a forest-based extension
to this procedure for similarities based on n-gram
counts. In experiments, we show that our fast pro-
cedure is on average 80 times faster than MBR
using 1000-best lists. We also show that using
forests outperforms using k-best lists consistently
across language pairs. Finally, in the first pub-
lished multi-system experiments on consensus de-
567
coding for translation, we demonstrate that bene-
fits can differ substantially across systems. In all,
we show improvements of up to 1.0 BLEU from
consensus approaches for state-of-the-art large-
scale hierarchical translation systems.
2 Consensus Decoding Algorithms
Let e be a candidate translation for a sentence f ,
where e may stand for a sentence or its derivation
as appropriate. Modern statistical machine trans-
lation systems take as input some f and score each
derivation e according to a linear model of fea-
tures:
?
i ?i ??i(f, e). The standard Viterbi decod-
ing objective is to find e? = arg maxe ? ? ?(f, e).
For MBR decoding, we instead leverage a sim-
ilarity measure S(e; e?) to choose a translation us-
ing the model?s probability distribution P(e|f),
which has support over a set of possible transla-
tions E. The Viterbi derivation e? is the mode of
this distribution. MBR is meant to choose a trans-
lation that will be similar, on expectation, to any
possible reference translation. To this end, MBR
chooses e? that maximizes expected similarity to
the sentences in E under P(e|f):1
e? = arg maxe EP(e?|f)
[
S(e; e?)
]
= arg maxe
?
e??E
P(e?|f) ? S(e; e?)
MBR can also be interpreted as a consensus de-
coding procedure: it chooses a translation similar
to other high-posterior translations. Minimizing
risk has been shown to improve performance for
MT (Kumar and Byrne, 2004), as well as other
language processing tasks (Goodman, 1996; Goel
and Byrne, 2000; Kumar and Byrne, 2002; Titov
and Henderson, 2006; Smith and Smith, 2007).
The distribution P(e|f) can be induced from a
translation system?s features and weights by expo-
nentiating with base b to form a log-linear model:
P (e|f) =
b???(f,e)
?
e??E b
???(f,e?)
We follow Ehling et al (2007) in choosing b using
a held-out tuning set. For algorithms in this sec-
tion, we assume that E is a k-best list and b has
been chosen already, so P(e|f) is fully specified.
1Typically, MBR is defined as arg mine?EE[L(e; e
?)] for
some loss function L, for example 1 ? BLEU(e; e?). These
definitions are equivalent.
2.1 Minimum Bayes Risk over Sentence Pairs
Given any similarity measure S and a k-best
list E, the minimum Bayes risk translation can
be found by computing the similarity between all
pairs of sentences in E, as in Algorithm 1.
Algorithm 1 MBR over Sentence Pairs
1: A? ??
2: for e ? E do
3: Ae ? 0
4: for e? ? E do
5: Ae ? Ae + P (e?|f) ? S(e; e?)
6: if Ae > A then A, e?? Ae, e
7: return e?
We can sometimes exit the inner for loop early,
whenever Ae can never become larger than A
(Ehling et al, 2007). Even with this shortcut, the
running time of Algorithm 1 is O(k2 ? n), where
n is the maximum sentence length, assuming that
S(e; e?) can be computed in O(n) time.
2.2 Minimum Bayes Risk over Features
We now consider the case when S(e; e?) is a lin-
ear function of sentence features. Let S(e; e?) be
a function of the form
?
j ?j(e) ? ?j(e
?), where
?j(e?) are real-valued features of e?, and ?j(e) are
sentence-specific weights on those features. Then,
the MBR objective can be re-written as
arg maxe?E EP(e?|f)
[
S(e; e?)
]
= arg maxe
?
e??E
P (e?|f) ?
?
j
?j(e) ? ?j(e
?)
= arg maxe
?
j
?j(e)
[
?
e??E
P (e?|f) ? ?j(e
?)
]
= arg maxe
?
j
?j(e) ? EP(e?|f)
[
?j(e
?)
]
. (1)
Equation 1 implies that we can find MBR trans-
lations by first computing all feature expectations,
then applying S only once for each e. Algorithm 2
proceduralizes this idea: lines 1-4 compute feature
expectations, and lines 5-11 find the translation
with highest S relative to those expectations. The
time complexity is O(k ?n), assuming the number
of non-zero features ?(e?) and weights ?(e) grow
linearly in sentence length n and all features and
weights can be computed in constant time.
568
Algorithm 2 MBR over Features
1: ??? [0 for j ? J ]
2: for e? ? E do
3: for j ? J such that ?j(e?) 6= 0 do
4: ??j ? ??j + P (e?|f) ? ?j(e?)
5: A? ??
6: for e ? E do
7: Ae ? 0
8: for j ? J such that ?j(e) 6= 0 do
9: Ae ? Ae + ?j(e) ? ??j
10: if Ae > A then A, e?? Ae, e
11: return e?
An example of a linear similarity measure is
bag-of-words precision, which can be written as:
U(e; e?) =
?
t?T1
?(e, t)
|e|
? ?(e?, t)
where T1 is the set of unigrams in the language,
and ?(e, t) is an indicator function that equals 1
if t appears in e and 0 otherwise. Figure 1 com-
pares Algorithms 1 and 2 using U(e; e?). Other
linear functions have been explored for MBR, in-
cluding Taylor approximations to the logarithm of
BLEU (Tromble et al, 2008) and counts of match-
ing constituents (Zhang and Gildea, 2008), which
are discussed further in Section 3.3.
2.3 Fast Consensus Decoding using
Non-Linear Similarity Measures
Most similarity measures of interest for machine
translation are not linear, and so Algorithm 2 does
not apply. Computing MBR even with simple
non-linear measures such as BLEU, NIST or bag-
of-words F1 seems to require O(k2) computation
time. However, these measures are all functions
of features of e?. That is, they can be expressed as
S(e;?(e?)) for a feature mapping ? : E ? Rn.
For example, we can express BLEU(e; e?) =
exp
"?
1 ?
|e?|
|e|
?
?
+
1
4
4X
n=1
ln
P
t?Tn
min(c(e, t), c(e?, t))
P
t?Tn
c(e, t)
#
In this expression, BLEU(e; e?) references e? only
via its n-gram count features c(e?, t).2
2The length penalty
?
1 ? |e
?|
|e|
?
?
is also a function of n-
gram counts: |e?| =
P
t?T1
c(e?, t). The negative part oper-
ator (?)? is equivalent to min(?, 0).
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [r(man with)] = 0.4 + 0.6 ? 1.0
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
22.5
45.0
67.5
90.0
Hiero SBMT
70.2
84.6
56.6
61.4
51.1
50.5
Viterbi n-gram precision
Forest n-gram precision at Viterbi recall
Forest n-gram precision for Er(t) ? 1
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+ 13)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) = 0.6+0.7+0.73
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
Figure 1: For the linear similarity measure U(e; e?), which
computes unigram precision, the MBR translation can be
found by iterating either over s ntence pairs (Algorithm 1) or
over features (Algorithm 2). These two algorithms take the
same input (step 1), but diverge in their consensus computa-
tions (steps 2 & 3). However, they produce identical results
for U and any other linear similarity measure.
Following the structure of Equation 1, we can
choose a translation e based on the feature expec-
tations of e?. In particular, we can choose
e? = arg maxe?ES(e;EP(e?|f)
[
?(e?)
]
). (2)
This objective differs from MBR, but has a simi-
lar consensus-building structure. We have simply
moved the expectation inside the similarity func-
tion, just as we did in Equation 1. This new ob-
jective can be optimized by Algorithm 3, a pro-
cedure that runs in O(k ? n) time if the count of
non-zero features in e? and the computation time
of S(e;?(e?)) are both linear in sentence length n.
This fast consensus decoding procedure shares
the same structure as linear MBR: first we com-
pute feature expectations, then we choose the sen-
tence that is most similar to those expectations. In
fact, Algorithm 2 is a special case of Algorithm 3.
Lines 7-9 of the former and line 7 of the latter are
equivalent for linear S(e; e?). Thus, for any linear
similarity measure, Algorithm 3 is an algorithm
for minimum Bayes risk decoding.
569
Algorithm 3 Fast Consensus Decoding
1: ??? [0 for j ? J ]
2: for e? ? E do
3: for j ? J such that ?j(e?) 6= 0 do
4: ??j ? ??j + P (e?|f) ? ?j(e?)
5: A? ??
6: for e ? E do
7: Ae ? S(e; ??)
8: if Ae > A then A, e?? Ae, e
9: return e?
As described, Algorithm 3 can use any sim-
ilarity measure that is defined in terms of real-
valued features of e?. There are some nuances
of this procedure, however. First, the precise
form of S(e;?(e?)) will affect the output, but
S(e;E[?(e?)]) is often an input point for which a
sentence similarity measure S was not originally
defined. For example, our definition of BLEU
above will have integer valued ?(e?) for any real
sentence e?, butE[?(e?)]will not be integer valued.
As a result, we are extending the domain of BLEU
beyond its original intent. One could imagine dif-
ferent feature-based expressions that also produce
BLEU scores for real sentences, but produce dif-
ferent values for fractional features. Some care
must be taken to define S(e;?(e?)) to extend nat-
urally from integer-valued to real-valued features.
Second, while any similarity measure can in
principle be expressed as S(e;?(e?)) for a suffi-
ciently rich feature space, fast consensus decoding
will not apply effectively to all functions. For in-
stance, we cannot naturally use functions that in-
clude alignments or matchings between e and e?,
such as METEOR (Agarwal and Lavie, 2007) and
TER (Snover et al, 2006). Though these functions
can in principle be expressed in terms of features
of e? (for instance with indicator features for whole
sentences), fast consensus decoding will only be
effective if different sentences share many fea-
tures, so that the feature expectations effectively
capture trends in the underlying distribution.
3 Computing Feature Expectations
We now turn our focus to efficiently comput-
ing feature expectations, in service of our fast
consensus decoding procedure. Computing fea-
ture expectations from k-best lists is trivial, but
k-best lists capture very little of the underlying
model?s posterior distribution. In place of k-best
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
20
40
60
80
Hiero SBMT
56.6
61.4
51.1
50.5
N-grams from baseline translation
N-grams with high expected count
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+ 13)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) = 0.6+0.7+0.73
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [c(e, ?man with?)] =
?
h
P (h|f) ? c(h, ?man with?)
= 0.4 ? 1 + (0.6 ? 1.0) ? 1
Figure 2: This translation forest for a Spanish sentence en-
codes two English parse trees. Hyper-edges (boxes) are an-
notated with normalized transition probabilities, as well as
the bigrams produced by each rule application. The expected
count of the bigram ?man with? is the sum of posterior prob-
abilities of the two hyper-edges that produce it. In this exam-
ple, we normalized inside scores at all nodes to 1 for clarity.
lists, compact encodings of translation distribu-
tions have proven effective for MBR (Zhang and
Gildea, 2008; Tromble et al, 2008). In this sec-
tion, we consider BLEU in particular, for which
the relevant features ?(e) are n-gram counts up to
length n = 4. We show how to compute expec-
tations of these counts efficiently from translation
forests.
3.1 Translation Forests
Translation forests compactly encode an exponen-
tial number of output translations for an input
sentence, along with their model scores. Forests
arise naturally in chart-based decoding procedures
for many hierarchical translation systems (Chiang,
2007). Exploiting forests has proven a fruitful av-
enue of research in both parsing (Huang, 2008)
and machine translation (Mi et al, 2008).
Formally, translation forests are weighted
acyclic hyper-graphs. The nodes are states in the
decoding process that include the span (i, j) of the
sentence to be translated, the grammar symbol s
over that span, and the left and right context words
of the translation relevant for computing n-gram
language model scores.3 Each hyper-edge h rep-
resents the application of a synchronous rule r that
combines nodes corresponding to non-terminals in
3Decoder states can include additional information as
well, such as local configurations for dependency language
model scoring.
570
r into a node spanning the union of the child spans
and perhaps some additional portion of the input
sentence covered directly by r?s lexical items. The
weight of h is the incremental score contributed
to all translations containing the rule application,
including translation model features on r and lan-
guage model features that depend on both r and
the English contexts of the child nodes. Figure 2
depicts a forest.
Each n-gram that appears in a translation e is as-
sociated with some h in its derivation: the h corre-
sponding to the rule that produces the n-gram. Un-
igrams are produced by lexical rules, while higher-
order n-grams can be produced either directly by
lexical rules, or by combining constituents. The
n-gram language model score of e similarly de-
composes over the h in e that produce n-grams.
3.2 Computing Expected N-Gram Counts
We can compute expected n-gram counts effi-
ciently from a translation forest by appealing to
the linearity of expectations. Let ?(e) be a vector
of n-gram counts for a sentence e. Then, ?(e) is
the sum of hyper-edge-specific n-gram count vec-
tors ?(h) for all h in e. Therefore, E[?(e)] =
?
h?e E[?(h)].
To compute n-gram expectations for a hyper-
edge, we first compute the posterior probability of
each h, conditioned on the input sentence f :
P(h|f) =
(
?
e:h?e
b???(f,e)
)(
?
e
b???(f,e)
)?1
,
where e iterates over translations in the forest. We
compute the numerator using the inside-outside al-
gorithm, while the denominator is the inside score
of the root node. Note that many possible deriva-
tions of f are pruned from the forest during decod-
ing, and so this posterior is approximate.
The expected n-gram count vector for a hyper-
edge is E[?(h)] = P(h|f) ? ?(h). Hence, after
computing P (h|f) for every h, we need only sum
P(h|f) ? ?(h) for all h to compute E[?(e)]. This
entire procedure is a linear-time computation in
the number of hyper-edges in the forest.
To complete forest-based fast consensus de-
coding, we then extract a k-best list of unique
translations from the forest (Huang et al, 2006)
and continue Algorithm 3 from line 5, which
chooses the e? from the k-best list that maximizes
BLEU(e;E[?(e?)]).
3.3 Comparison to Related Work
Zhang and Gildea (2008) embed a consensus de-
coding procedure into a larger multi-pass decoding
framework. They focus on inversion transduction
grammars, but their ideas apply to richer models as
well. They propose an MBR decoding objective
of maximizing the expected number of matching
constituent counts relative to the model?s distri-
bution. The corresponding constituent-matching
similarity measure can be expressed as a linear
function of features of e?, which are indicators of
constituents. Expectations of constituent indicator
features are the same as posterior constituent prob-
abilities, which can be computed from a transla-
tion forest using the inside-outside algorithm. This
forest-based MBR approach improved translation
output relative to Viterbi translations.
Tromble et al (2008) describe a similar ap-
proach using MBR with a linear similarity mea-
sure. They derive a first-order Taylor approxima-
tion to the logarithm of a slightly modified defini-
tion of corpus BLEU4, which is linear in n-gram
indicator features ?(e?, t) of e?. These features are
weighted by n-gram counts c(e, t) and constants
? that are estimated from held-out data. The lin-
ear similarity measure takes the following form,
where Tn is the set of n-grams:
G(e; e?) = ?0|e|+
4?
n=1
?
t?Tn
?t ? c(e, t) ? ?(e
?, t).
Using G, Tromble et al (2008) extend MBR to
word lattices, which improves performance over
k-best list MBR.
Our approach differs from Tromble et al (2008)
primarily in that we propose decoding with an al-
ternative to MBR using BLEU, while they propose
decoding with MBR using a linear alternative to
BLEU. The specifics of our approaches also differ
in important ways.
First, word lattices are a subclass of forests that
have only one source node for each edge (i.e., a
graph, rather than a hyper-graph). While forests
are more general, the techniques for computing
posterior edge probabilities in lattices and forests
are similar. One practical difference is that the
forests needed for fast consensus decoding are
4The log-BLEU function must be modified slightly to
yield a linear Taylor approximation: Tromble et al (2008)
replace the clipped n-gram count with the product of an n-
gram count and an n-gram indicator function.
571
generated already by the decoder of a syntactic
translation system.
Second, rather than use BLEU as a sentence-
level similarity measure directly, Tromble et al
(2008) approximate corpus BLEU with G above.
The parameters ? of the approximation must be es-
timated on a held-out data set, while our approach
requires no such estimation step.
Third, our approach is also simpler computa-
tionally. The features required to compute G are
indicators ?(e?, t); the features relevant to us are
counts c(e?, t). Tromble et al (2008) compute ex-
pected feature values by intersecting the transla-
tion lattice with a lattices for each n-gram t. By
contrast, expectations of c(e?, t) can all be com-
puted with a single pass over the forest. This con-
trast implies a complexity difference. LetH be the
number of hyper-edges in the forest or lattice, and
T the number of n-grams that can potentially ap-
pear in a translation. Computing indicator expec-
tations seems to require O(H ? T ) time because of
automata intersections. Computing count expec-
tations requires O(H) time, because only a con-
stant number of n-grams can be produced by each
hyper-edge.
Our approaches also differ in the space of trans-
lations from which e? is chosen. A linear similar-
ity measure like G allows for efficient search over
the lattice or forest, whereas fast consensus decod-
ing restricts this search to a k-best list. However,
Tromble et al (2008) showed that most of the im-
provement from lattice-based consensus decoding
comes from lattice-based expectations, not search:
searching over lattices instead of k-best lists did
not change results for two language pairs, and im-
proved a third language pair by 0.3 BLEU. Thus,
we do not consider our use of k-best lists to be a
substantial liability of our approach.
Fast consensus decoding is also similar in char-
acter to the concurrently developed variational de-
coding approach of Li et al (2009). Using BLEU,
both approaches choose outputs that match ex-
pected n-gram counts from forests, though differ
in the details. It is possible to define a similar-
ity measure under which the two approaches are
equivalent.5
5For example, decoding under a variational approxima-
tion to the model?s posterior that decomposes over bigram
probabilities is equivalent to fast consensus decoding with
the similarity measure B(e; e?) =
Q
t?T2
h
c(e?,t)
c(e?,h(t))
ic(e,t)
,
where h(t) is the unigram prefix of bigram t.
4 Experimental Results
We evaluate these consensus decoding techniques
on two different full-scale state-of-the-art hierar-
chical machine translation systems. Both systems
were trained for 2008 GALE evaluations, in which
they outperformed a phrase-based system trained
on identical data.
4.1 Hiero: a Hierarchical MT Pipeline
Hiero is a hierarchical system that expresses its
translation model as a synchronous context-free
grammar (Chiang, 2007). No explicit syntactic in-
formation appears in the core model. A phrase
discovery procedure over word-aligned sentence
pairs provides rule frequency counts, which are
normalized to estimate features on rules.
The grammar rules of Hiero all share a single
non-terminal symbol X , and have at most two
non-terminals and six total items (non-terminals
and lexical items), for example:
my X2 ?s X1 ? X1 de mi X2
We extracted the grammar from training data using
standard parameters. Rules were allowed to span
at most 15 words in the training data.
The log-linear model weights were trained us-
ing MIRA, a margin-based optimization proce-
dure that accommodates many features (Crammer
and Singer, 2003; Chiang et al, 2008). In addition
to standard rule frequency features, we included
the distortion and syntactic features described in
Chiang et al (2008).
4.2 SBMT: a Syntax-Based MT Pipeline
SBMT is a string-to-tree translation system with
rich target-side syntactic information encoded in
the translation model. The synchronous grammar
rules are extracted from word aligned sentence
pairs where the target sentence is annotated with
a syntactic parse (Galley et al, 2004). Rules map
source-side strings to target-side parse tree frag-
ments, and non-terminal symbols correspond to
target-side grammatical categories:
(NP (NP (PRP$ my) NN2 (POS ?s)) NNS1)?
NNS1 de mi NN2
We extracted the grammar via an array of criteria
(Galley et al, 2006; DeNeefe et al, 2007; Marcu
et al, 2006). The model was trained using min-
imum error rate training for Arabic (Och, 2003)
and MIRA for Chinese (Chiang et al, 2008).
572
Arabic-English
Objective Hiero SBMT
Min. Bayes Risk (Alg 1) 2h 47m 12h 42m
Fast Consensus (Alg 3) 5m 49s 5m 22s
Speed Ratio 29 142
Chinese-English
Objective Hiero SBMT
Min. Bayes Risk (Alg 1) 10h 24m 3h 52m
Fast Consensus (Alg 3) 4m 52s 6m 32s
Speed Ratio 128 36
Table 1: Fast consensus decoding is orders of magnitude
faster than MBR when using BLEU as a similarity measure.
Times only include reranking, not k-best list extraction.
4.3 Data Conditions
We evaluated on both Chinese-English and
Arabic-English translation tasks. Both Arabic-
English systems were trained on 220 million
words of word-aligned parallel text. For the
Chinese-English experiments, we used 260 mil-
lion words of word-aligned parallel text; the hi-
erarchical system used all of this data, and the
syntax-based system used a 65-million word sub-
set. All four systems used two language models:
one trained from the combined English sides of
both parallel texts, and another, larger, language
model trained on 2 billion words of English text
(1 billion for Chinese-English SBMT).
All systems were tuned on held-out data (1994
sentences for Arabic-English, 2010 sentences for
Chinese-English) and tested on another dataset
(2118 sentences for Arabic-English, 1994 sen-
tences for Chinese-English). These datasets were
drawn from the NIST 2004 and 2005 evaluation
data, plus some additional data from the GALE
program. There was no overlap at the segment or
document level between the tuning and test sets.
We tuned b, the base of the log-linear model,
to optimize consensus decoding performance. In-
terestingly, we found that tuning b on the same
dataset used for tuning ?was as effective as tuning
b on an additional held-out dataset.
4.4 Results over K-Best Lists
Taking expectations over 1000-best lists6 and us-
ing BLEU7 as a similarity measure, both MBR
6We ensured that k-best lists contained no duplicates.
7To prevent zero similarity scores, we also used a standard
smoothed version of BLEU that added 1 to the numerator and
denominator of all n-gram precisions. Performance results
Arabic-English
Expectations Similarity Hiero SBMT
Baseline - 52.0 53.9
104-best BLEU 52.2 53.9
Forest BLEU 53.0 54.0
Forest Linear G 52.3 54.0
Chinese-English
Expectations Similarity Hiero SBMT
Baseline - 37.8 40.6
104-best BLEU 38.0 40.7
Forest BLEU 38.2 40.8
Forest Linear G 38.1 40.8
Table 2: Translation performance improves when computing
expected sentences from translation forests rather than 104-
best lists, which in turn improve over Viterbi translations. We
also contrasted forest-based consensus decoding with BLEU
and its linear approximation, G. Both similarity measures are
effective, but BLEU outperforms G.
and our variant provided consistent small gains of
0.0?0.2 BLEU. Algorithms 1 and 3 gave the same
small BLEU improvements in each data condition
up to three significant figures.
The two algorithms differed greatly in speed,
as shown in Table 1. For Algorithm 1, we ter-
minated the computation of E[BLEU(e; e?)] for
each e whenever e could not become the maxi-
mal hypothesis. MBR speed depended on how
often this shortcut applied, which varied by lan-
guage and system. Despite this optimization, our
new Algorithm 3 was an average of 80 times faster
across systems and language pairs.
4.5 Results for Forest-Based Decoding
Table 2 contrasts Algorithm 3 over 104-best lists
and forests. Computing E[?(e?)] from a transla-
tion forest rather than a 104-best list improved Hi-
ero by an additional 0.8 BLEU (1.0 over the base-
line). Forest-based expectations always outper-
formed k-best lists, but curiously the magnitude
of benefit was not consistent across systems. We
believe the difference is in part due to more ag-
gressive forest pruning within the SBMT decoder.
For forest-based decoding, we compared two
similarity measures: BLEU and its linear Taylor
approximationG from section 3.3.8 Table 2 shows
were identical to standard BLEU.
8We did not estimate the ? parameters of G ourselves;
instead we used the parameters listed in Tromble et al
(2008), which were also estimated for GALE data. We
also approximated E[?(e?, t)] with a clipped expected count
573
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
20
40
60
80
Hiero SBMT
56.6
61.4
51.1
50.5
N-grams from baseline translations
N-grams with high expected count
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+
1
3)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) =
0.6+0.7+0.7
3
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [c(e, ?man with?)] =
?
h
P (h|f) ? c(h, ?man with?)
= 0.4 ? 1 + (0.6 ? 1.0) ? 1
N
-
g
r
a
m
 
P
r
e
c
i
s
i
o
n
Figure 3: N -grams with high expected count are more likely
to appear in the reference translation that n-grams in the
translation model?s Viterbi translation, e?. Above, we com-
pare the precision, relative to reference translations, of sets of
n-grams chosen in two ways. The left bar is the precision of
the n-grams in e?. The right bar is the precision of n-grams
with E[c(e, t)] > ?. To justify this comparison, we chose ?
so that both methods of choosing n-grams gave the same n-
gram recall: the fraction of n-grams in reference translations
that also appeared in e? or had E[c(e, t)] > ?.
that both similarities were effective, but BLEU
outperformed its linear approximation.
4.6 Analysis
Forest-based consensus decoding leverages infor-
mation about the correct translation from the en-
tire forest. In particular, consensus decoding
with BLEU chooses translations using n-gram
count expectations E[c(e, t)]. Improvements in
translation quality should therefore be directly at-
tributable to information in these expected counts.
We endeavored to test the hypothesis that ex-
pected n-gram counts under the forest distribution
carry more predictive information than the base-
line Viterbi derivation e?, which is the mode of the
distribution. To this end, we first tested the pre-
dictive accuracy of the n-grams proposed by e?:
the fraction of the n-grams in e? that appear in a
reference translation. We compared this n-gram
precision to a similar measure of predictive accu-
racy for expected n-gram counts: the fraction of
the n-grams t with E[c(e, t)] ? ? that appear in
a reference. To make these two precisions com-
parable, we chose ? such that the recall of ref-
erence n-grams was equal. Figure 3 shows that
computing n-gram expectations?which sum over
translations?improves the model?s ability to pre-
dict which n-grams will appear in the reference.
min(1,E[c(e?, t)]). Assuming an n-gram appears at most
once per sentence, these expressions are equivalent, and this
assumption holds for most n-grams.
Reference translation:
Mubarak said that he received a telephone call from
Sharon in which he said he was ?ready (to resume ne-
gotiations) but the Palestinians are hesitant.?
Baseline translation:
Mubarak said he had received a telephone call from
Sharon told him he was ready to resume talks with the
Palestinians.
Fast forest-based consensus translation:
Mubarak said that he had received a telephone call from
Sharon told him that he ?was ready to resume the nego-
tiations) , but the Palestinians are hesitant.?
Figure 4: Three translations of an example Arabic sentence:
its human-generated reference, the translation with the high-
est model score under Hiero (Viterbi), and the translation
chosen by forest-based consensus decoding. The consensus
translation reconstructs content lost in the Viterbi translation.
We attribute gains from fast consensus decoding
to this increased predictive accuracy.
Examining the translations chosen by fast con-
sensus decoding, we found that gains in BLEU of-
ten arose from improved lexical choice. However,
in our hierarchical systems, consensus decoding
did occasionally trigger large reordering. We also
found examples where the translation quality im-
proved by recovering content that was missing
from the baseline translation, as in Figure 4.
5 Conclusion
We have demonstrated substantial speed increases
in k-best consensus decoding through a new pro-
cedure inspired by MBR under linear similarity
measures. To further improve this approach, we
computed expected n-gram counts from transla-
tion forests instead of k-best lists. Fast consensus
decoding using forest-based n-gram expectations
and BLEU as a similarity measure yielded con-
sistent improvements over MBR with k-best lists,
yet required only simple computations that scale
linearly with the size of the translation forest.
The space of similarity measures is large and
relatively unexplored, and the feature expectations
that can be computed from forests extend beyond
n-gram counts. Therefore, future work may show
additional benefits from fast consensus decoding.
Acknowledgements
This work was supported under DARPA GALE,
Contract No. HR0011-06-C-0022.
574
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Workshop on Statistical Machine
Translation for the Association of Computational
Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and CoNLL.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum Bayes risk decoding for BLEU. In Pro-
ceedings of the Association for Computational Lin-
guistics: Short Paper Track.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT: the North American Chapter
of the Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Vaibhava Goel and William Byrne. 2000. Minimum
Bayes-risk automatic speech recognition. In Com-
puter, Speech and Language.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the Association for Compu-
tational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the Associa-
tion for Machine Translation in the Americas.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2002. Minimum
Bayes-risk word alignments of bilingual texts. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Association for Compu-
tational Linguistics and IJCNLP.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics.
David Smith and Noah Smith. 2007. Probabilistic
models of nonprojective dependency trees. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and CoNLL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Roy Tromble, Shankar Kumar, Franz Josef Och, and
Wolfgang Macherey. 2008. Lattice minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan
Vogel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of HLT: the North American Association
for Computational Linguistics Conference.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the Association for Compu-
tational Linguistics.
575
Tutorial Abstracts of ACL-IJCNLP 2009, page 2,
Suntec, Singapore, 2 August 2009.
c?2009 ACL and AFNLP
Topics in Statistical Machine Translation
Kevin Knight
Information Sciences Institute
University of Southern California
knight@isi.edu
Philipp Koehn
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk
1 Introduction
In the past, we presented tutorials called ?Intro-
duction to Statistical Machine Translation?, aimed
at people who know little or nothing about the field
and want to get acquainted with the basic con-
cepts. This tutorial, by contrast, goes more deeply
into selected topics of intense current interest. We
aim at two types of participants:
1. People who understand the basic idea of sta-
tistical machine translation and want to get a
survey of hot-topic current research, in terms
that they can understand.
2. People associated with statistical machine
translation work, who have not had time to
study the most current topics in depth.
We fill the gap between the introductory tutorials
that have gone before and the detailed scientific
papers presented at ACL sessions.
2 Tutorial Outline
Below is our tutorial structure. We showcase the
intuitions behind the algorithms and give exam-
ples of how they work on sample data. Our se-
lection of topics focuses on techniques that deliver
proven gains in translation accuracy, and we sup-
ply empirical results from the literature.
1. QUICK REVIEW (15 minutes)
? Phrase-based and syntax-based MT.
2. ALGORITHMS (45 minutes)
? Efficient decoding for phrase-based and
syntax-based MT (cube pruning, for-
ward/outside costs).
? Minimum-Bayes risk.
? System combination.
3. SCALING TO LARGE DATA (30 minutes)
? Phrase table pruning, storage, suffix ar-
rays.
? Large language models (distributed
LMs, noisy LMs).
4. NEW MODELS (1 hour and 10 minutes)
? New methods for word alignment (be-
yond GIZA++).
? Factored models.
? Maximum entropy models for rule se-
lection and re-ordering.
? Acquisition of syntactic translation
rules.
? Syntax-based language models and
target-language dependencies.
? Lattices for encoding source-language
uncertainties.
5. LEARNING TECHNIQUES (20 minutes)
? Discriminative training (perceptron,
MIRA).
2
 
			ff  	

ffLearning a Translation Lexicon from Monolingual Corpora
Philipp Koehn and Kevin Knight
Information Sciences Institute, University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
koehn@isi.edu, knight@isi.edu
Abstract
This paper presents work on the task
of constructing a word-level translation
lexicon purely from unrelated mono-
lingual corpora. We combine vari-
ous clues such as cognates, similar
context, preservation of word similar-
ity, and word frequency. Experimen-
tal results for the construction of a
German-English noun lexicon are re-
ported. Noun translation accuracy of
39% scored against a parallel test cor-
pus could be achieved.
1 Introduction
Recently, there has been a surge in research
in machine translation that is based on em-
pirical methods. The seminal work by Brown
et al [1990] at IBM on the Candide system laid
the foundation for much of the current work
in Statistical Machine Translation (SMT).
Some of this work has been re-implemented
and is freely available for research purposes [Al-
Onaizan et al, 1999].
Roughly speaking, SMT divides the task of
translation into two steps: a word-level trans-
lation model and a model for word reordering
during the translation process.
The statistical models are trained on parallel
corpora: large amounts of text in one language
along with their translation in another. Var-
ious parallel texts have recently become avail-
able, mostly from government sources such as
parliament proceedings (the Canadian Hansard,
the minutes of the European parliament1) or law
texts (from Hong Kong).
Still, for most language pairs, parallel texts
are hard to come by. This is clearly the case for
low-density languages such as Tamil, Swahili, or
Tetun. Furthermore, texts derived from parlia-
ment speeches may not be appropriate for a par-
ticular targeted domain. Specific parallel texts
can be constructed by hand for the purpose of
training an SMT system, but this is a very costly
endeavor.
On the other hand, the digital revolution and
the wide-spread use of the World Wide Web
have proliferated vast amounts of monolingual
corpora. Publishing text in one language is a
much more natural human activity than produc-
ing parallel texts. To illustrate this point: The
world wide web alone contains currently over
two billion pages, a number that is still grow-
ing exponentially. According to Google,2 the
word directory occurs 61 million times, empathy
383,000 times, and reflex 787,000 times. In the
Hansard, each of these words occurs only once.
The objective of this research to build a trans-
lation lexicon solely from monolingual corpora.
Specifically, we want to automatically generate
a one-to-one mapping of German and English
nouns. We are testing our mappings against
a bilingual lexicon of 9,206 German and 10,645
English nouns.
The two monolingual corpora should be in a
fairly comparable domain. For our experiments
we use the 1990-1992 Wall Street Journal corpus
1Available for download at http://www.isi.edu
/?koehn/publications/europarl/
2http://www.google.com/
                      July 2002, pp. 9-16.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
on the English side and the 1995-1996 German
news wire (DPA) corpus on the German side.
Both corpora are news sources in the general
sense. However, they span different time peri-
ods and have a different orientation: the World
Street Journal covers mostly business news, the
German news wire mostly German politics.
For experiments on training probabilistic
translation lexicons from parallel corpora and
similar tasks on the same test corpus, refer to
our earlier work [Koehn and Knight, 2000, 2001].
2 Clues
This section will describe clues that enable us to
find translations of words of the two monolingual
corpora. We will examine each clue separately.
The following clues are considered:
? Identical words ? Two languages contain
a certain number of identical words, such as
computer or email.
? Similar Spelling ? Some words may have
very similarly written translations due to
common language roots (e.g. Freund and
friend) or adopted words (e.g. Webseite and
website).
? Context ? Words that occur in a certain
context window in one language have trans-
lations that are likely to occur in a similar
context window in the other language (e.g.
Wirtschaft co-occurs frequently with Wach-
stum, as economy does with growth).
? Similarity ? Words that are used simi-
larly in one language should have transla-
tions that are also similar (e.g. Wednesday
is similar to Thursday as Mittwoch is similar
to Donnerstag).
? Frequency ? For comparable corpora, fre-
quent words in one corpus should have
translations that are frequent in the other
corpus (e.g. for news corpora, government is
more frequent than flower, as its translation
Regierung is more frequent than Blume.
We will now look in detail how these clues
may contribute to building a German-English
translation lexicon.
2.1 Identical words
Due to cultural exchange, a large number of
words that originate in one language are adopted
by others. Recently, this phenomenon can be
seen with words such as Internet, or Aids.
These terms may be adopted verbatim, or
changed by well-established rules. For instance,
immigration (German and English) has the Por-
tuguese translation immigrac?a?o, as many words
ending in -tion have translations with the same
spelling except for the ending changed to -c?a?o.
We examined the German words in our lex-
icon and tried to find English words that have
the exact same spelling. Surprisingly, we could
count a total of 976 such words. When check-
ing them against a benchmark lexicon, we found
these mappings to be 88% correct.
The correctness of word mappings acquired
in this fashion depends highly on word length.
This is illustrated in Table 1: While identical 3-
letter words are only translations of each other
60% of the time, this is true for 98% of 10-letter
words. Clearly, for shorter words, the acciden-
tal existence of an identically spelled word in
the other language word is much higher. This
includes words such as fee, ton, art, and tag.
Length Number of words Accuracy
correct wrong
3 33 22 60%
4 127 48 69%
5 129 22 85%
6 162 13 93%
7 131 4 97%
8 86 4 96%
9 80 4 95%
10 57 1 98%
11+ 50 3 94%
Table 1: Testing the assumption that identically
spelled words are in fact translations of each
other: The accuracy of this assumption depends
highly on the length of the words (see Section
2.1)
Knowing this allows us to restrict the word
length to be able to increase the accuracy of the
collected word pairs. For instance, by relying
only on words at least of length 6, we could col-
lect 622 word pairs with 96% accuracy. In our
experiments, however, we included all the words
pairs.
As already mentioned, there are some well-
established transformation rules for the adop-
tion of words from a foreign language. For Ger-
man to English, this includes replacing the let-
ters k and z by c and changing the ending -ta?t
by -ty. Both these rules can be observed in the
word pair Elektrizita?t and electricity.
By using these two rules, we can gather 363
additional word pairs of which 330, or 91%, are
in fact translations of each other. The combined
total of 1339 (976+363) word pairs are separated
and form the seed for some of the following steps.
2.2 Similar Spelling
When words are adopted into another language,
their spelling might change slightly in a manner
that can not be simply generalized in a rule. Ob-
serve, for instance website and Webseite. This is
even more the case for words that can be traced
back to common language roots, such as friend
and Freund, or president and Pra?sident.
Still, these words ? often called cognates ?
maintain a very similar spelling. This can be
defined as differing in very few letters. This
measurement can be formalized as the number
of letters common in sequence between the two
words, divided by the length of the longer word.
The example word pair friend and freund
shares 5 letters (fr-e-nd), and both words have
length 6, hence there spelling similarity is 5/6, or
0.83. This measurement is called longest com-
mon subsequence ratio [Melamed, 1995]. In
related work, string edit distance (or, Lev-
enshtein distance) has been used [Mann and
Yarowski, 2001].
With this computational means at hand, we
can now measure the spelling similarity between
every German and English word, and sort pos-
sible word pairs accordingly. By going through
this list starting at the top we can collect new
word pairs. We do this is in a greedy fashion ?
once a word is assigned to a word pair, we do
not look for another match. Table 2 gives the
top 24 generated word pairs by this algorithm.
German English Score
Organisation organization 0.92 correct
Pra?sident president 0.90 correct
Industrie industries 0.90 correct
Parlament parliament 0.90 correct
Interesse interests 0.89 correct
Institut institute 0.89 correct
Satellit satellite 0.89 correct
Dividende dividend 0.89 correct
Maschine machine 0.88 correct
Magazin magazine 0.88 correct
Februar february 0.88 correct
Programm program 0.88 correct
Gremium premium 0.86 wrong
Branche branch 0.86 wrong
Volumen volume 0.86 correct
Januar january 0.86 correct
Warnung warning 0.86 correct
Partie parties 0.86 correct
Debatte debate 0.86 correct
Experte expert 0.86 correct
Investition investigation 0.85 wrong
Mutter matter 0.83 wrong
Bruder border 0.83 wrong
Nummer number 0.83 correct
Table 2: First 24 word pairs collected by find-
ing words with most similar spelling in a greedy
fashion.
The applied measurement of spelling similar-
ity does not take into account that certain letter
changes (such as z to s, or dropping of the fi-
nal e) are less harmful than others. Tiedemann
[1999] explores the automatic construction of a
string similarity measure that learns which let-
ter changes occur more likely between cognates
of two languages. This measure is trained, how-
ever, on parallel sentence-aligned text, which is
not available here.
Obviously, the vast majority of word pairs can
not be collected this way, since their spelling
shows no resemblance at all. For instance,
Spiegel and mirror share only one vowel, which
is rather accidental.
2.3 Similar Context
If our monolingual corpora are comparable, we
can assume a word that occurs in a certain con-
text should have a translation that occurs in a
similar context.
Context, as we understand it here, is defined
by the frequencies of context words in surround-
ing positions. This local context has to be trans-
lated into the other language, and we can search
the word with the most similar context.
This idea has already been investigated in ear-
lier work. Rapp [1995, 1999] proposes to collect
counts over words occurring in a four word win-
dow around the target word. For each occur-
rence of a target word, counts are collected over
how often certain context words occur in the two
positions directly ahead of the target word and
the two following positions. The counts are col-
lected separately for each position and then en-
tered into in a context vector with an dimension
for each context word in each position. Finally,
the raw counts are normalized, so that for each
of the four word positions the vector values add
up to one. Vector comparison is done by adding
all absolute differences of all components.
Fung and Yee [1998] propose a similar ap-
proach: They count how often another word oc-
curs in the same sentence as the target word.
The counts are then normalized by a using the
tf/idf method which is often used in information
retrieval [Jones, 1979].
The need for translating the context poses a
chicken-and-egg problem: If we already have a
translation lexicon we can translate the context
vectors. But we can only construct a translation
lexicon with this approach if we are already able
to translate the context vectors.
Theoretically, it is possible to use these meth-
ods to build a translation lexicon from scratch
[Rapp, 1995]. The number of possible mappings
has complexity O(n!), and the computing cost of
each mapping has quadratic complexity O(n2).
For a large number of words n ? at least more
than 10,000, maybe more than 100,000 ? the
combined complexity becomes prohibitively ex-
pensive.
Because of this, both Rapp and Fung focus on
expanding an existing large lexicon to add a few
novel terms.
Clearly, a seed lexicon to bootstrap these
methods is needed. Fortunately, we have out-
lined in Section 2.1 how such a seed lexicon can
be obtained: by finding words spelled identically
in both languages.
We can then construct context vectors that
contain information about how a new unmapped
word co-occurs with the seed words. This vec-
tor can be translated into the other language,
since we already know the translations of the
seed words.
Finally, we can look for the best matching
context vector in the target language, and de-
cide upon the corresponding word to construct
a word mapping.
Again, as in Section 2.2, we have to com-
pute all possible word ? or context vector ?
matches. We collect then the best word matches
in a greedy fashion. Table 3 displays the top 15
generated word pairs by this algorithm. The
context vectors are constructed in the way pro-
posed by Rapp [1999], with the difference that
we collect counts over a four noun window, not a
four word window, by dropping all intermediate
words.
German English Score
Jahr mr 5.03024 wrong
Regierung government 5.54937 correct
Prozent percent 5.57756 correct
Angabe us 5.73654 wrong
Mittwoch company 5.83199 wrong
Donnerstag time 5.90623 wrong
Pra?sident president 5.93884 correct
Dienstag year 5.94611 wrong
Staat state 5.96725 correct
Zeit people 6.05552 wrong
Freitag officials 6.11668 wrong
Montag week 6.13604 wrong
Krieg war 6.13604 correct
Woche yesterday 6.15378 wrong
Krankheit disease 6.20817 correct
Kirche church 6.21477 correct
Unternehmen companies 6.22896 correct
Ende money 6.28154 wrong
Streik strike 6.28690 correct
Energie energy 6.29883 correct
O?l oil 6.30794 correct
Markt market 6.31116 correct
Wirtschaft economy 6.34883 correct
Sonntag group 6.34917 wrong
Table 3: First 24 word pairs collected by find-
ing words with most similar context vectors in
a greedy fashion.
2.4 Preserving Word Similarity
Intuitively it is obvious that pairs of words that
are similar in one language should have trans-
lations that are similar in the other language.
For instance, Wednesday is similar to Thursday
as Mittwoch is similar to Donnerstag. Or: dog is
similar to cat in English, as Hund is similar to
Katze in German.
The challenge is now to come up with a quan-
tifiable measurement of word similarity. One
strategy is to define two words as similar if they
occur in a similar context. Clearly, this is the
case for Wednesday and Thursday, as well as for
dog and cat.
Exactly this similarity measurement is used
in the work by Diab and Finch [2000]. Their
approach to constructing and comparing con-
text vectors differs significantly from methods
discussed in the previous section.
For each word in the lexicon, the context vec-
tor consists of co-occurrence counts in respect to
150 so-called peripheral tokens, basically the
most frequent words. These counts are collected
for each position in a 4-word window around the
word in focus. This results in a 600-dimensional
vector.
Instead of comparing these co-occurrence
counts directly, the Spearman rank order
correlation is applied: For each position the
tokens are compared in frequency and the fre-
quency count is replaced by the frequency rank
? the most frequent token count is replaced by
1, the least frequent by n = 150. The similarity
of two context vectors a = (ai) and b = (bi) is
then defined by:3
R(a, b) = 1?
6
?
(ai?bi)2
4n(n2?1)
The result of all this is a matrix with similar-
ity scores between all German words, and second
one with similarity scores between all English
words. Such matrices could also be constructed
using the definitions of context we reviewed in
the previous section. The important point here
is that we have generated a similarity matrix,
which we will use now to find new translation
word pairs.
Again, as in the previous Section 2.3, we as-
3In the given formula we fixed two mistakes of the
original presentation [Diab and Finch, 2000]: The square
of the differences is used, and the denominator contains
the additional factor 4, since essentially 4 150-word vec-
tors are compared.
sume that we will already have a seed lexicon.
For a new word we can look up its similarity
scores to the seed words, thus creating a simi-
larity vector. Such a vector can be translated
into the other language ? recall that dimensions
of the vector are the similarity scores to seed
words, for which we already have translations.
The translated vector can be compared to other
vectors in the second language.
As before, we search greedily for the best
matching similarity vectors and add the corre-
sponding words to the lexicon.
2.5 Word Frequency
Finally, another simple clue is the observation
that in comparable corpora, the same concepts
should be used with similar frequencies. Even if
the most frequent word in the German corpus is
not necessarily the translation of the most fre-
quent English word, it should also be very fre-
quent.
Table 4 illustrates the situation with our cor-
pora. It contains the top 10 German and En-
glish words, together with the frequency ranks
of their best translations. For both languages, 4
of the 10 words have translations that also rank
in the top 10.
Clearly, simply aligning the nth frequent Ger-
man word with the nth frequent English word is
not a viable strategy. In our case, this is addi-
tionally hampered by the different orientation of
the news sources. The frequent financial terms
in the English WSJ corpus (stock, bank, sales,
etc.) are rather rare in the German corpus.
For most words, especially for more compara-
ble corpora, there is a considerable correlation
between the frequency of a word and its transla-
tion. Our frequency measurement is defined as
ratio of the word frequencies, normalized by the
corpus sizes.
3 Experiments
This section provides more detail on the exper-
iments we have carried out to test the methods
just outlined.
Rank German English Rank
1 Jahr year 3
2 Land country 112
3 Regierung government 18
4 Prozent percent 1
5 Pra?sident president 8
6 Staat state 24
7 Million million 22
8 Angabe statement 335
9 Mittwoch wednesday 298
10 USA us 5
4 Prozent percent 1
308 Herr mr 2
1 Jahr year 3
72 Unternehmen company 4
10 USA us 5
58 Markt market 6
150 Aktie stock 7
5 Pra?sident president 8
52 Bank bank 9
119 Umsatz sales 10
Table 4: The frequency ranks of the most fre-
quent German and English words and their
translations.
3.1 Evaluation measurements
We are trying to build a one-to-one German-
English translation lexicon for the use in a ma-
chine translation system.
To evaluate this performance we use two dif-
ferent measurements: Firstly, we record how
many correct word-pairs we have constructed.
This is done by checking the generated word-
pairs against an existing bilingual lexicon.4 In
essence, we try to recreate this lexicon, which
contains 9,206 distinct German and 10,645 dis-
tinct English nouns and 19,782 lexicon entries.
For a machine translation system, it is of-
ten more important to get more frequently used
words right than obscure ones. Thus, our second
evaluation measurement tests the word transla-
tions proposed by the acquired lexicon against
the actual word-level translations in a 5,000 sen-
tence aligned parallel corpus.5
The starting point to extending the lexicon
is the seed lexicon of identically spelled words,
as described in Section 2.1. It consists of 1339
entries, of which are (88.9%) correct according
4extracted from LEO, http://dict.leo.org/
5extracted from the German radio news corpus de-
news, http://www.mathematik.uni-ulm.de/de-news/
to the existing bilingual lexicon. Due to com-
putational constraints,6 we focus on the addi-
tional mapping of only 1,000 German and En-
glish words.
These 1,000 words are chosen from the 1,000
most frequent lexicon entries in the dictionary,
without duplications of words. This frequency
is defined by the sum of two word frequencies of
the words in the entry, as found in the mono-
lingual corpora. We did not collect statistics of
the actual use of lexical entries in, say, a parallel
corpus.
In a different experimental set-up we also sim-
ply tried to match the 1,000 most frequent Ger-
man words with the 1,000 most frequent English
words. The results do not differ significantly.
3.2 Greedy extension
Each of the four clues described in the Sections
2.2 to 2.5 provide a matching score between a
German and an English word. The likelihood
of these two words being actual translations of
each other should correlate to these scores.
There are many ways one could search for the
best set of lexicon entries based on these scores.
We could perform an exhaustive search: con-
struct all possible mappings and find the high-
est combined score of all entries. Since there are
O(n!) possible mappings, a brute force approach
to this is practically impossible.
We therefore employed a greedy search: First
we search for the highest score for any word pair.
We add this word pair to the lexicon, and drop
word pairs that include either the German and
English word from further search. Again, we
search for the highest score and add the corre-
sponding word pair, drop these words from fur-
ther search, and so on. This is done iteratively,
until all words are used up.
Tables 2 and 3 illustrate this process for the
spelling and context similarity clues, when ap-
plied separately.
6For matching 1,000 words, the described algorithms
run up to 3 days. Since the complexity of these algo-
rithms is O(n2) in regard to the number of words, a full
run on 10,000 would take almost a year. Of course, this
may be alleviated by more efficient implementation and
parallelization.
3.3 Results
The results are summarized in Table 5. Recall
that for each word that we are trying to map to
the other language, a thousand possible target
words exist, but only one is correct. The base-
line for this task, choosing words at random, re-
sults on average in only 1 correct mapping in
the entire lexicon. A perfect lexicon, of course,
contains 1000 correct entries.
The starting point for the corpus score is
the 15.8% that are already achieved with the
seed lexicon from Section 2.1. In an experiment
where we identified the best lexical entries using
a very large parallel corpus, we could achieve
89% accuracy on this test corpus.
Clues Entries Corpus
Identical Words (1339 Seed) - 15.8%
Spelling 140 25.4%
Context 107 31.9%
Preserving Similarity 2 15.8%
Frequency 2 17.0%
Spelling+Context 185 38.6%
Spelling+Frequency 151 27.4%
Spelling+Context+Similarity 186 39.0%
All clues 186 39.0%
Table 5: Overview of results. We evaluate how
many correct lexicon entries where added (En-
tries), and how well the resulting translation
lexicon performs compared to the actual word-
level translations in a parallel corpus (Corpus).
For all experiments the starting point was the
seed lexicon of 1339 identical spelled words de-
scribed in Section 2.1. which achieve 15.8% Cor-
pus score.
Taken alone, both the context and spelling
clues learn over a hundred lexicon entries cor-
rectly. The similarity and frequency clues, how-
ever, seem to be too imprecise to pinpoint the
search to the correct translations.
A closer look of the spelling and context scores
reveals that while the spelling clue allows to
learn more correct lexicon entries (140 opposed
to 107), the context clue does better with the
more frequently used lexicon entries, as found
in the test corpus (accuracy of 31.9% opposed
to 25.4%).
3.4 Combining Clues
Combining different clues is quite simple: We
can simply add up the matching scores. The
scores can be weighted. Initially we simply
weighted all clues equally. We then changed the
weights to see, if we can obtain better results.
We found that there is generally a broad range
of weights that result in similar performance.
When using the spelling clue in combination
with others, we found it useful to define a cutoff.
If two words agree in 30% of their letters this
is generally as bad as if they do not agree in
any ? the agreements are purely coincidental.
Therefore we counted all spelling scores below
0.3 as 0.3.
Combining the context and the spelling clues
yields a significantly better result than using
each clue by itself. A total of 185 correct lexical
entries are learned with a corpus score of 38.6%.
Adding in the other scores, however, does not
seem to be beneficial: only adding the frequency
clue to the spelling clue provides some improve-
ment. In all other cases, these scores are not
helpful.
Besides this linear combination of scores from
the different clues, more sophisticated methods
may be possible [Koehn, 2002].
4 Conclusions
We have attempted to learn a one-to-one trans-
lation lexicon purely from unrelated monolin-
gual corpora. Using identically spelled words
proved to be a good starting point. Beyond this,
we examined four different clues. Two of them,
matching similar spelled words and words with
the same context, helped us to learn a significant
number of additional correct lexical entries.
Our experiments have been restricted to
nouns. Verbs, adjectives, adverbs and other part
of speech may be tackled in a similar way. They
might also provide useful context information
that is beneficial to building a noun lexicon.
These methods may be also useful given a
different starting point: For efforts in building
machine translation systems, some small paral-
lel text should be available. From these, some
high-quality lexical entries can be learned, but
there will always be many words that are miss-
ing. These may be learned using the described
methods.
References
Al-Onaizan, Y., Curin, J., Jahr, M., Knight,
K., Lafferty, J., Melamed, D., Och,
F.-J., Purdy, D., Smith, N. A., and
Yarowsky, D. (1999). Statistical ma-
chine translation. Technical report, John
Hopkins University Summer Workshop
http://www.clsp.jhu.edu/ws99/projects
/mt/.
Brown, P., Cocke, J., Pietra, S. A. D., Pietra,
V. J. D., Jelinek, F., Lafferty, J. D., Mercer,
R. L., and Rossin, P. (1990). A statistical ap-
proach to machine translation. Computational
Linguistics, 16(2):76?85.
Diab, M. and Finch, S. (2000). A statistical
word-level translation model for comparable
corpora. In Proceedings of the Conference on
Content-based multimedia information access
(RIAO).
Fung, P. and Yee, L. Y. (1998). An IR approach
for translating new words from nonparallel,
comparable texts. In Proceedings of ACL 36,
pages 414?420.
Jones, K. S. (1979). Experiments in relevance
weighting of search terms. In Information
Processing and Management, pages 133?144.
Koehn, P. (2002). Combining multiclass maxi-
mum entropy classifiers with neural network
voting. In Proceedings of PorTAL.
Koehn, P. and Knight, K. (2000). Estimating
word translation probabilities from unrelated
monolingual corpora using the EM algorithm.
In Proceedings of AAAI.
Koehn, P. and Knight, K. (2001). Knowl-
edge sources for word-level translation mod-
els. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Process-
ing, EMNLP.
Mann, G. S. and Yarowski, D. (2001). Multi-
path translation lexicon induction via bridge
languages. In Proceedings of NAACL, pages
151?158.
Melamed, D. (1995). Automatic evaluation and
uniform filter cascades for inducing n-best
translation lexicons. In Third Workshop on
Very Large Corpora.
Rapp, R. (1995). Identifying word translations
in non-parallel texts. In Proceedings of ACL
33, pages 320?322.
Rapp, R. (1999). Automatic identification of
word translations from unrelated English and
German corpora. In Proceedings of ACL 37,
pages 519?526.
Tiedemann, J. (1999). Automatic construc-
tion of weighted string similarity measures.
In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing,
EMNLP.
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1?8,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
A Syntax-Directed Translator with Extended Domain of Locality
Liang Huang
Dept. of Comp. & Info. Sci.
Univ. of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Kevin Knight
Info. Sci. Inst.
Univ. of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Aravind Joshi
Dept. of Comp. & Info. Sci.
Univ. of Pennsylvania
Philadelphia, PA 19104
joshi@linc.cis.upenn.edu
Abstract
A syntax-directed translator first parses
the source-language input into a parse-
tree, and then recursively converts the tree
into a string in the target-language. We
model this conversion by an extended tree-
to-string transducer that have multi-level
trees on the source-side, which gives our
system more expressive power and flexi-
bility. We also define a direct probabil-
ity model and use a linear-time dynamic
programming algorithm to search for the
best derivation. The model is then ex-
tended to the general log-linear frame-
work in order to rescore with other fea-
tures like n-gram language models. We
devise a simple-yet-effective algorithm to
generate non-duplicate k-best translations
for n-gram rescoring. Initial experimen-
tal results on English-to-Chinese transla-
tion are presented.
1 Introduction
The concept of syntax-directed (SD) translation
was originally proposed in compiling (Irons, 1961;
Lewis and Stearns, 1968), where the source program
is parsed into a tree representation that guides the
generation of the object code. Following Aho and
Ullman (1972), a translation, as a set of string pairs,
can be specified by a syntax-directed translation
schema (SDTS), which is essentially a synchronous
context-free grammar (SCFG) that generates two
languages simultaneously. An SDTS also induces a
translator, a device that performs the transformation
induces implements
SD translator
(source parser + recursive converter)
specifies translation
(string relation)
SD translation schema
(synchronous grammar)
Figure 1: The relationship among SD concepts,
adapted from (Aho and Ullman, 1972).
?
?
?
?
?
?
S
NP(1)? VP
VB(2)? NP
(3)
?
,
S
VB(2)? NP
(1)
? NP
(3)
?
?
?
?
?
?
?
Figure 2: An example of complex reordering repre-
sented as an STSG rule, which is beyond any SCFG.
from input string to output string. In this context, an
SD translator consists of two components, a source-
language parser and a recursive converter which is
usually modeled as a top-down tree-to-string trans-
ducer (Ge?cseg and Steinby, 1984). The relationship
among these concepts is illustrated in Fig. 1.
This paper adapts the idea of syntax-directed
translator to statistical machine translation (MT).
We apply stochastic operations at each node of the
source-language parse-tree and search for the best
derivation (a sequence of translation steps) that con-
verts the whole tree into some target-language string
with the highest probability. However, the structural
divergence across languages often results in non-
isomorphic parse-trees that is beyond the power of
SCFGs. For example, the S(VO) structure in English
is translated into a VSO word-order in Arabic, an in-
stance of complex reordering not captured by any
1
SCFG (Fig. 2).
To alleviate the non-isomorphism problem, (syn-
chronous) grammars with richer expressive power
have been proposed whose rules apply to larger frag-
ments of the tree. For example, Shieber and Sch-
abes (1990) introduce synchronous tree-adjoining
grammar (STAG) and Eisner (2003) uses a syn-
chronous tree-substitution grammar (STSG), which
is a restricted version of STAG with no adjunctions.
STSGs and STAGs generate more tree relations than
SCFGs, e.g. the non-isomorphic tree pair in Fig. 2.
This extra expressive power lies in the extended do-
main of locality (EDL) (Joshi and Schabes, 1997),
i.e., elementary structures beyond the scope of one-
level context-free productions. Besides being lin-
guistically motivated, the need for EDL is also sup-
ported by empirical findings in MT that one-level
rules are often inadequate (Fox, 2002; Galley et al,
2004). Similarly, in the tree-transducer terminology,
Graehl and Knight (2004) define extended tree trans-
ducers that have multi-level trees on the source-side.
Since an SD translator separates the source-
language analysis from the recursive transformation,
the domains of locality in these two modules are or-
thogonal to each other: in this work, we use a CFG-
based Treebank parser but focuses on the extended
domain in the recursive converter. Following Gal-
ley et al (2004), we use a special class of extended
tree-to-string transducer (xRs for short) with multi-
level left-hand-side (LHS) trees.1 Since the right-
hand-side (RHS) string can be viewed as a flat one-
level tree with the same nonterminal root from LHS
(Fig. 2), this framework is closely related to STSGs:
they both have extended domain of locality on the
source-side, while our framework remains as a CFG
on the target-side. For instance, an equivalent xRs
rule for the complex reordering in Fig. 2 would be
S(x1:NP, VP(x2:VB, x3:NP))? x2 x1 x3
While Section 3 will define the model formally,
we first proceed with an example translation from
English to Chinese (note in particular that the in-
verted phrases between source and target):
1Throughout this paper, we will use LHS and source-side
interchangeably (so are RHS and target-side). In accordance
with our experiments, we also use English and Chinese as the
source and target languages, opposite to the Foreign-to-English
convention of Brown et al (1993).
(a) the gunman was [killed]1 by [the police]2 .
parser ?
(b)
S
NP-C
DT
the
NN
gunman
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
PUNC
.
r1, r2 ?
(c) qiangshou
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
?
r3 ?
(d) qiangshou bei
NP-C
DT
the
NN
police
VBN
killed
?
r5 ? r4 ?
(e) qiangshou bei [jingfang]2 [jibi]1 ?
Figure 3: A synatx-directed translation process for
Example (1).
(1) the gunman was killed by the police .
qiangshou
[gunman]
bei
[passive]
jingfang
[police]
jibi
[killed]
?
.
Figure 3 shows how the translator works. The En-
glish sentence (a) is first parsed into the tree in (b),
which is then recursively converted into the Chinese
string in (e) through five steps. First, at the root
node, we apply the rule r1 which preserves the top-
level word-order and translates the English period
into its Chinese counterpart:
(r1) S (x1:NP-C x2:VP PUNC (.) ) ? x1 x2 ?
2
Then, the rule r2 grabs the whole sub-tree for ?the
gunman? and translates it as a phrase:
(r2) NP-C ( DT (the) NN (gunman) )? qiangshou
Now we get a ?partial Chinese, partial English? sen-
tence ?qiangshou VP ?? as shown in Fig. 3 (c). Our
recursion goes on to translate the VP sub-tree. Here
we use the rule r3 for the passive construction:
(r3)
VP
VBD
was
VP-C
x1:VBN PP
IN
by
x2:NP-C
? bei x2 x1
which captures the fact that the agent (NP-C, ?the
police?) and the verb (VBN, ?killed?) are always
inverted between English and Chinese in a passive
voice. Finally, we apply rules r4 and r5 which per-
form phrasal translations for the two remaining sub-
trees in (d), respectively, and get the completed Chi-
nese string in (e).
2 Previous Work
It is helpful to compare this approach with recent ef-
forts in statistical MT. Phrase-based models (Koehn
et al, 2003; Och and Ney, 2004) are good at learn-
ing local translations that are pairs of (consecutive)
sub-strings, but often insufficient in modeling the re-
orderings of phrases themselves, especially between
language pairs with very different word-order. This
is because the generative capacity of these models
lies within the realm of finite-state machinery (Ku-
mar and Byrne, 2003), which is unable to process
nested structures and long-distance dependencies in
natural languages.
Syntax-based models aim to alleviate this prob-
lem by exploiting the power of synchronous rewrit-
ing systems. Both Yamada and Knight (2001) and
Chiang (2005) use SCFGs as the underlying model,
so their translation schemata are syntax-directed as
in Fig. 1, but their translators are not: both systems
do parsing and transformation in a joint search, es-
sentially over a packed forest of parse-trees. To this
end, their translators are not directed by a syntac-
tic tree. Although their method potentially consid-
ers more than one single parse-tree as in our case,
the packed representation of the forest restricts the
scope of each transfer step to a one-level context-
free rule, while our approach decouples the source-
language analyzer and the recursive converter, so
that the latter can have an extended domain of local-
ity. In addition, our translator also enjoys a speed-
up by this decoupling, with each of the two stages
having a smaller search space. In fact, the recursive
transfer step can be done by a a linear-time algo-
rithm (see Section 5), and the parsing step is also
fast with the modern Treebank parsers, for instance
(Collins, 1999; Charniak, 2000). In contrast, their
decodings are reported to be computationally expen-
sive and Chiang (2005) uses aggressive pruning to
make it tractable. There also exists a compromise
between these two approaches, which uses a k-best
list of parse trees (for a relatively small k) to approx-
imate the full forest (see future work).
Besides, our model, as being linguistically mo-
tivated, is also more expressive than the formally
syntax-based models of Chiang (2005) and Wu
(1997). Consider, again, the passive example in rule
r3. In Chiang?s SCFG, there is only one nonterminal
X, so a corresponding rule would be
? was X(1) by X(2), bei X(2) X(1) ?
which can also pattern-match the English sentence:
I was [asleep]1 by [sunset]2 .
and translate it into Chinese as a passive voice. This
produces very odd Chinese translation, because here
?was A by B? in the English sentence is not a pas-
sive construction. By contrast, our model applies
rule r3 only if A is a past participle (VBN) and B
is a noun phrase (NP-C). This example also shows
that, one-level SCFG rule, even if informed by the
Treebank as in (Yamada and Knight, 2001), is not
enough to capture a common construction like this
which is five levels deep (from VP to ?by?).
There are also some variations of syntax-directed
translators where dependency structures are used
in place of constituent trees (Lin, 2004; Ding and
Palmer, 2005; Quirk et al, 2005). Although they
share with this work the basic motivations and simi-
lar speed-up, it is difficult to specify re-ordering in-
formation within dependency elementary structures,
so they either resort to heuristics (Lin) or a sepa-
rate ordering model for linearization (the other two
3
works).2 Our approach, in contrast, explicitly mod-
els the re-ordering of sub-trees within individual
transfer rules.
3 Extended Tree-to-String Tranducers
In this section, we define the formal machinery of
our recursive transformation model as a special case
of xRs transducers (Graehl and Knight, 2004) that
has only one state, and each rule is linear (L) and
non-deleting (N) with regarding to variables in the
source and target sides (henth the name 1-xRLNs).
Definition 1. A 1-xRLNs transducer is a tuple
(N,?,?,R) where N is the set of nonterminals, ?
is the input alphabet, ? is the output alphabet, and
R is a set of rules. A rule in R is a tuple (t, s, ?)
where:
1. t is the LHS tree, whose internal nodes are la-
beled by nonterminal symbols, and whose fron-
tier nodes are labeled terminals from ? or vari-
ables from a set X = {x1, x2, . . .};
2. s ? (X ??)? is the RHS string;
3. ? is a mapping from X to nonterminals N .
We require each variable xi ? X occurs exactly once
in t and exactly once in s (linear and non-deleting).
We denote ?(t) to be the root symbol of tree t.
When writing these rules, we avoid notational over-
head by introducing a short-hand form from Galley
et al (2004) that integrates the mapping into the tree,
which is used throughout Section 1. Following TSG
terminology (see Figure 2), we call these ?variable
nodes? such as x2:NP-C substitution nodes, since
when applying a rule to a tree, these nodes will be
matched with a sub-tree with the same root symbol.
We also define |X | to be the rank of the rule, i.e.,
the number of variables in it. For example, rules r1
and r3 in Section 1 are both of rank 2. If a rule has
no variable, i.e., it is of rank zero, then it is called a
purely lexical rule, which performs a phrasal trans-
lation as in phrase-based models. Rule r2, for in-
stance, can be thought of as a phrase pair ?the gun-
man, qiangshou?.
Informally speaking, a derivation in a transducer
is a sequence of steps converting a source-language
2Although hybrid approaches, such as dependency gram-
mars augmented with phrase-structure information (Alshawi et
al., 2000), can do re-ordering easily.
r1
r2 r3
r4 r5
r1
r2 r6
r4 r7
r5
(a) (b)
Figure 4: (a) the derivation in Figure 3; (b) another
derviation producing the same output by replacing
r3 with r6 and r7, which provides another way of
translating the passive construction:
(r6) VP ( VBD (was) VP-C (x1:VBN x2:PP ) )? x2 x1
(r7) PP ( IN (by) x1:NP-C )? bei x1
tree into a target-language string, with each step ap-
plying one tranduction rule. However, it can also
be formalized as a tree, following the notion of
derivation-tree in TAG (Joshi and Schabes, 1997):
Definition 2. A derivation d, its source and target
projections, noted E(d) and C(d) respectively, are
recursively defined as follows:
1. If r = (t, s, ?) is a purely lexical rule (? = ?),
then d = r is a derivation, where E(d) = t and
C(d) = s;
2. If r = (t, s, ?) is a rule, and di is a (sub-)
derivation with the root symbol of its source
projection matches the corresponding substitu-
tion node in r, i.e., ?(E(di)) = ?(xi), then
d = r(d1, . . . , dm) is also a derivation, where
E(d) = [xi 7? E(di)]t and C(d) = [xi 7?
C(di)]s.
Note that we use a short-hand notation [xi 7? yi]t
to denote the result of substituting each xi with yi
in t, where xi ranges over all variables in t.
For example, Figure 4 shows two derivations for
the sentence pair in Example (1). In both cases, the
source projection is the English tree in Figure 3 (b),
and the target projection is the Chinese translation.
Galley et al (2004) presents a linear-time algo-
rithm for automatic extraction of these xRs rules
from a parallel corpora with word-alignment and
parse-trees on the source-side, which will be used
in our experiments in Section 6.
4
4 Probability Models
4.1 Direct Model
Departing from the conventional noisy-channel ap-
proach of Brown et al (1993), our basic model is a
direct one:
c? = argmax
c
Pr(c | e) (2)
where e is the English input string and c? is the
best Chinese translation according to the translation
model Pr(c | e). We now marginalize over all En-
glish parse trees T (e) that yield the sentence e:
Pr(c | e) =
?
??T (e)
Pr(?, c | e)
=
?
??T (e)
Pr(? | e) Pr(c | ?) (3)
Rather than taking the sum, we pick the best tree ??
and factors the search into two separate steps: pars-
ing (4) (a well-studied problem) and tree-to-string
translation (5) (Section 5):
?? = argmax
??T (e)
Pr(? | e) (4)
c? = argmax
c
Pr(c | ??) (5)
In this sense, our approach can be considered as
a Viterbi approximation of the computationally ex-
pensive joint search using (3) directly. Similarly, we
now marginalize over all derivations
D(??) = {d | E(d) = ??}
that translates English tree ? into some Chinese
string and apply the Viterbi approximation again to
search for the best derivation d?:
c? = C(d?) = C(argmax
d?D(??)
Pr(d)) (6)
Assuming different rules in a derivation are ap-
plied independently, we approximate Pr(d) as
Pr(d) =
?
r?d
Pr(r) (7)
where the probability Pr(r) of the rule r is estimated
by conditioning on the root symbol ?(t(r)):
Pr(r) = Pr(t(r), s(r) | ?(t(r)))
= c(r)?
r?:?(t(r?))=?(t(r)) c(r?)
(8)
where c(r) is the count (or frequency) of rule r in
the training data.
4.2 Log-Linear Model
Following Och and Ney (2002), we extend the direct
model into a general log-linear framework in order
to incorporate other features:
c? = argmax
c
Pr(c | e)? ? Pr(c)? ? e??|c| (9)
where Pr(c) is the language model and e??|c| is the
length penalty term based on |c|, the length of the
translation. Parameters ?, ?, and ? are the weights
of relevant features. Note that positive ? prefers
longer translations. We use a standard trigram model
for Pr(c).
5 Search Algorithms
We first present a linear-time algorithm for searching
the best derivation under the direct model, and then
extend it to the log-linear case by a new variant of
k-best parsing.
5.1 Direct Model: Memoized Recursion
Since our probability model is not based on the noisy
channel, we do not call our search module a ?de-
coder? as in most statistical MT work. Instead, read-
ers who speak English but not Chinese can view it as
an ?encoder? (or encryptor), which corresponds ex-
actly to our direct model.
Given a fixed parse-tree ??, we are to search
for the best derivation with the highest probability.
This can be done by a simple top-down traversal
(or depth-first search) from the root of ??: at each
node ? in ??, try each possible rule r whose English-
side pattern t(r) matches the subtree ??? rooted at ?,
and recursively visit each descendant node ?i in ???
that corresponds to a variable in t(r). We then col-
lect the resulting target-language strings and plug
them into the Chinese-side s(r) of rule r, getting
a translation for the subtree ??? . We finally take the
best of all translations.
With the extended LHS of our transducer, there
may be many different rules applicable at one tree
node. For example, consider the VP subtree in
Fig. 3 (c), where both r3 and r6 can apply. As a re-
sult, the number of derivations is exponential in the
size of the tree, since there are exponentially many
5
decompositions of the tree for a given set of rules.
This problem can be solved by memoization (Cor-
men et al, 2001): we cache each subtree that has
been visited before, so that every tree node is visited
at most once. This results in a dynamic program-
ming algorithm that is guaranteed to run in O(npq)
time where n is the size of the parse tree, p is the
maximum number of rules applicable to one tree
node, and q is the maximum size of an applicable
rule. For a given rule-set, this algorithm runs in time
linear to the length of the input sentence, since p
and q are considered grammar constants, and n is
proportional to the input length. The full pseudo-
code is worked out in Algorithm 1. A restricted
version of this algorithm first appears in compiling
for optimal code generation from expression-trees
(Aho and Johnson, 1976). In computational linguis-
tics, the bottom-up version of this algorithm resem-
bles the tree parsing algorithm for TSG by Eisner
(2003). Similar algorithms have also been proposed
for dependency-based translation (Lin, 2004; Ding
and Palmer, 2005).
5.2 Log-linear Model: k-best Search
Under the log-linear model, one still prefers to
search for the globally best derivation d?:
d? = argmax
d?D(??)
Pr(d)? Pr(C(d))?e??|C(d)| (10)
However, integrating the n-gram model with the
translation model in the search is computationally
very expensive. As a standard alternative, rather
than aiming at the exact best derivation, we search
for top-k derivations under the direct model using
Algorithm 1, and then rerank the k-best list with the
language model and length penalty.
Like other instances of dynamic programming,
Algorithm 1 can be viewed as a hypergraph search
problem. To this end, we use an efficient algo-
rithm by Huang and Chiang (2005, Algorithm 3)
that solves the general k-best derivations problem
in monotonic hypergraphs. It consists of a normal
forward phase for the 1-best derivation and a recur-
sive backward phase for the 2nd, 3rd, . . . , kth deriva-
tions.
Unfortunately, different derivations may have the
same yield (a problem called spurious ambiguity),
due to multi-level LHS of our rules. In practice, this
results in a very small ratio of unique strings among
top-k derivations. To alleviate this problem, deter-
minization techniques have been proposed by Mohri
and Riley (2002) for finite-state automata and ex-
tended to tree automata by May and Knight (2006).
These methods eliminate spurious ambiguity by ef-
fectively transforming the grammar into an equiva-
lent deterministic form. However, this transforma-
tion often leads to a blow-up in forest size, which is
exponential to the original size in the worst-case.
So instead of determinization, here we present a
simple-yet-effective extension to the Algorithm 3 of
Huang and Chiang (2005) that guarantees to output
unique translated strings:
? keep a hash-table of unique strings at each vertex
in the hypergraph
? when asking for the next-best derivation of a ver-
tex, keep asking until we get a new string, and
then add it into the hash-table
This method should work in general for any
equivalence relation (say, same derived tree) that can
be defined on derivations.
6 Experiments
Our experiments are on English-to-Chinese trans-
lation, the opposite direction to most of the recent
work in SMT. We are not doing the reverse direction
at this time partly due to the lack of a sufficiently
good parser for Chinese.
6.1 Data Preparation
Our training set is a Chinese-English parallel corpus
with 1.95M aligned sentences (28.3M words on the
English side). We first word-align them by GIZA++,
then parse the English side by a variant of Collins
(1999) parser, and finally apply the rule-extraction
algorithm of Galley et al (2004). The resulting rule
set has 24.7M xRs rules. We also use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
Chinese trigram model with Knesser-Ney smooth-
ing on the Chinese side of the parallel corpus.
Our evaluation data consists of 140 short sen-
tences (< 25 Chinese words) of the Xinhua portion
of the NIST 2003 Chinese-to-English evaluation set.
Since we are translating in the other direction, we
use the first English reference as the source input
and the Chinese as the single reference.
6
Algorithm 1 Top-down Memoized Recursion
1: function TRANSLATE(?)
2: if cache[?] defined then . this sub-tree visited before?
3: return cache[?]
4: best? 0
5: for r ? R do . try each rule r
6: matched, sublist? PATTERNMATCH(t(r), ?) . tree pattern matching
7: if matched then . if matched, sublist contains a list of matched subtrees
8: prob? Pr(r) . the probability of rule r
9: for ?i ? sublist do
10: pi, si ? TRANSLATE(?i) . recursively solve each sub-problem
11: prob? prob ? pi
12: if prob > best then
13: best? prob
14: str ? [xi 7? si]s(r) . plug in the results
15: cache[?]? best, str . caching the best solution for future use
16: return cache[?] . returns the best string with its prob.
6.2 Initial Results
We implemented our system as follows: for each in-
put sentence, we first run Algorithm 1, which returns
the 1-best translation and also builds the derivation
forest of all translations for this sentence. Then we
extract the top 5000 non-duplicate translated strings
from this forest and rescore them with the trigram
model and the length penalty.
We compared our system with a state-of-the-art
phrase-based system Pharaoh (Koehn, 2004) on the
evaluation data. Since the target language is Chi-
nese, we report character-based BLEU score instead
of word-based to ensure our results are indepen-
dent of Chinese tokenizations (although our lan-
guage models are word-based). The BLEU scores
are based on single reference and up to 4-gram pre-
cisions (r1n4). Feature weights of both systems are
tuned on the same data set.3 For Pharaoh, we use the
standard minimum error-rate training (Och, 2003);
and for our system, since there are only two in-
dependent features (as we always fix ? = 1), we
use a simple grid-based line-optimization along the
language-model weight axis. For a given language-
model weight ?, we use binary search to find the best
length penalty ? that leads to a length-ratio closest
3In this sense, we are only reporting performances on the
development set at this point. We will report results tuned and
tested on separate data sets in the final version of this paper.
Table 1: BLEU (r1n4) score results
system BLEU
Pharaoh 25.5
direct model (1-best) 20.3
log-linear model (rescored 5000-best) 23.8
to 1 against the reference. The results are summa-
rized in Table 1. The rescored translations are better
than the 1-best results from the direct model, but still
slightly worse than Pharaoh.
7 Conclusion and On-going Work
This paper presents an adaptation of the clas-
sic syntax-directed translation with linguistically-
motivated formalisms for statistical MT. Currently
we are doing larger-scale experiments. We are also
investigating more principled algorithms for inte-
grating n-gram language models during the search,
rather than k-best rescoring. Besides, we will extend
this work to translating the top k parse trees, instead
of committing to the 1-best tree, as parsing errors
certainly affect translation quality.
7
References
A. V. Aho and S. C. Johnson. 1976. Optimal code gen-
eration for expression trees. J. ACM, 23(3):488?501.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume I:
Parsing. Prentice Hall, Englewood Cliffs, New Jersey.
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL, pages 132?139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the 43rd
ACL.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. MIT Press, second edition.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd ACL.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
(companion volume), pages 205?208.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proc. of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
F. Ge?cseg and M. Steinby. 1984. Tree Automata.
Akade?miai Kiado?, Budapest.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In HLT-NAACL, pages 105?112.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Nineth International
Workshop on Parsing Technologies (IWPT-2005), 9-10
October 2005, Vancouver, Canada.
E. T. Irons. 1961. A syntax-directed compiler for AL-
GOL 60. Comm. ACM, 4(1):51?55.
Aravind Joshi and Yves Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
? 124. Springer, Berlin.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. of AMTA, pages 115?124.
Shankar Kumar and William Byrne. 2003. A weighted
finite state transducer implementation of the alignment
template model for statistical machine translation. In
Proc. of HLT-NAACL, pages 142?149.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed
transduction. Journal of the ACM, 15(3):465?488.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th COLING.
Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. Submitted to HLT-NAACL 2006.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing 2002 (ICSLP ?02), Denver, Col-
orado, September.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30:417?449.
Franz Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proc. of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. of COLING, pages
253?258.
Andrea Stolcke. 2002. Srilm: an extensible language
modeling toolkit. In Proc. of ICSLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL.
8
Proceedings of the Third Workshop on Statistical Machine Translation, pages 44?52,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Syntax to Improve Word Alignment Precision for Syntax-Based
Machine Translation
Victoria Fossum
Dept. of Computer Science
University of Michigan
Ann Arbor, MI 48104
vfossum@umich.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Steven Abney
Dept. of Linguistics
University of Michigan
Ann Arbor, MI 48104
abney@umich.edu
Abstract
Word alignments that violate syntactic cor-
respondences interfere with the extraction
of string-to-tree transducer rules for syntax-
based machine translation. We present an
algorithm for identifying and deleting incor-
rect word alignment links, using features of
the extracted rules. We obtain gains in both
alignment quality and translation quality in
Chinese-English and Arabic-English transla-
tion experiments relative to a GIZA++ union
baseline.
1 Introduction
1.1 Motivation
Word alignment typically constitutes the first stage
of the statistical machine translation pipeline.
GIZA++ (Och and Ney, 2003), an implementation
of the IBM (Brown et al, 1993) and HMM (?)
alignment models, is the most widely-used align-
ment system. GIZA++ union alignments have been
used in the state-of-the-art syntax-based statistical
MT system described in (Galley et al, 2006) and in
the hierarchical phrase-based system Hiero (Chiang,
2007). GIZA++ refined alignments have been used
in state-of-the-art phrase-based statistical MT sys-
tems such as (Och, 2004); variations on the refined
heuristic have been used by (Koehn et al, 2003)
(diag and diag-and) and by the phrase-based system
Moses (grow-diag-final) (Koehn et al, 2007).
GIZA++ union alignments have high recall but
low precision, while intersection or refined align-
ments have high precision but low recall.1 There are
two natural approaches to improving upon GIZA++
alignments, then: deleting links from union align-
ments, or adding links to intersection or refined
alignments. In this work, we delete links from
GIZA++ union alignments to improve precision.
The low precision of GIZA++ union alignments
poses a particular problem for syntax-based rule ex-
traction algorithms such as (Quirk et al, 2005; Gal-
ley et al, 2006; Huang et al, 2006; Liu et al,
2006): if the incorrect links violate syntactic corre-
spondences, they force the rule extraction algorithm
to extract rules that are large in size, few in number,
and poor in generalization ability.
Figure 1 illustrates this problem: the dotted line
represents an incorrect link in the GIZA++ union
alignment. Using the rule extraction algorithm de-
scribed in (Galley et al, 2004), we extract the rules
shown in the leftmost column (R1?R4). Rule R1 is
large and unlikely to generalize well. If we delete
the incorrect link in Figure 1, we can extract the
rules shown in the rightmost column (R2?R9): Rule
R1, the largest rule from the initial set, disappears,
and several smaller, more modular rules (R5?R9) re-
place it.
In this work, we present a supervised algorithm
that uses these two features of the extracted rules
(size of largest rule and total number of rules), as
well as a handful of structural and lexical features,
to automatically identify and delete incorrect links
from GIZA++ union alignments. We show that link
1For a complete discussion of alignment symmetrization
heuristics, including union, intersection, and refined, refer to
(Och and Ney, 2003).
44
VP
VBZ
starts
PRT
RP
out
PP
IN
from
NP
NP
DT
the
NNS
needs
PP
IN
of
NP
PRP
its
JJ
own
NN
country
,
? ) ?  ? 
FROM OWN-COUNTRY NEEDS STARTS-OUT
Rules Extracted Using GIZA++ Union Alignments Rules Extracted After Deleting Dotted Link
R1: VP
VBZ
starts
PRT
RP
out
PP
x0:IN NP
NP
DT
the
NNS
needs
x1:PP
? x0 x1 ?  ?  R2: IN
from
? ,
R2: IN
from
? , R3: PP
IN
of
x0:NP
? x0
R3: PP
IN
of
x0:NP
? x0 R4: NP
PRP
its
JJ
own
NN
country
? ? )
R4: NP
PRP
its
JJ
own
NN
country
? ? ) R5: PP
x0:IN x1:NP
?x0 x1
R6: NP
x0:NP x1:PP
? x1 x0
R7: NP
DT
the
x0:NNS
? x0
R8: NNS
needs
? ? 
R9: VP
VBZ
starts
PRT
RP
out
x0:PP
? x0 ? 
Figure 1: The impact of incorrect alignment links upon rule extraction. Using the original alignment (including all
links shown) leads to the extraction of the tree-to-string transducer rules whose left hand sides are rooted at the solid
boxed nodes in the parse tree (R1, R2, R3, and R4). Deleting the dotted alignment link leads to the omission of rule
R1, the extraction of R9 in its place, the extraction of R2, R3, and R4 as before, and the extraction of additional rules
whose left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8).
45
deletion improves alignment quality and translation
quality in Chinese-English and Arabic-English MT,
relative to a strong baseline. Our link deletion al-
gorithm is easy to implement, runs quickly, and has
been used by a top-scoring MT system in the Chi-
nese newswire track of the 2008 NIST evaluation.
1.2 Related Work
Recently, discriminative methods for alignment
have rivaled the quality of IBM Model 4 alignments
(Liu et al, 2005; Ittycheriah and Roukos, 2005;
Taskar et al, 2005; Moore et al, 2006; Fraser and
Marcu, 2007b). However, except for (Fraser and
Marcu, 2007b), none of these advances in align-
ment quality has improved translation quality of a
state-of-the-art system. We use a discriminatively
trained model to identify and delete incorrect links,
and demonstrate that these gains in alignment qual-
ity lead to gains in translation quality in a state-
of-the-art syntax-based MT system. In contrast to
the semi-supervised LEAF alignment algorithm of
(Fraser and Marcu, 2007b), which requires 1,500-
2,000 CPU days per iteration to align 8.4M Chinese-
English sentences (anonymous, p.c.), link deletion
requires only 450 CPU hours to re-align such a cor-
pus (after initial alignment by GIZA++, which re-
quires 20-24 CPU days).
Several recent works incorporate syntactic fea-
tures into alignment. (May and Knight, 2007) use
syntactic constraints to re-align a parallel corpus that
has been aligned by GIZA++ as follows: they extract
string-to-tree transducer rules from the corpus, the
target parse trees, and the alignment; discard the ini-
tial alignment; use the extracted rules to construct a
forest of possible string-to-tree derivations for each
string/tree pair in the corpus; use EM to select the
Viterbi derivation tree for each pair; and finally, in-
duce a new alignment from the Viterbi derivations,
using the re-aligned corpus to train a syntax-based
MT system. (May and Knight, 2007) differs from
our approach in two ways: first, the set of possible
re-alignments they consider for each sentence pair is
limited by the initial GIZA++ alignments seen over
the training corpus, while we consider all alignments
that can be reached by deleting links from the ini-
tial GIZA++ alignment for that sentence pair. Sec-
ond, (May and Knight, 2007) use a time-intensive
training algorithm to select the best re-alignment
for each sentence pair, while we use a fast greedy
search to determine which links to delete; in con-
trast to (May and Knight, 2007), who require 400
CPU hours to re-align 330k Chinese-English sen-
tence pairs (anonymous, p.c), link deletion requires
only 18 CPU hours to re-align such a corpus.
(Lopez and Resnik, 2005) and (Denero and Klein,
2007) modify the distortion model of the HMM
alignment model (Vogel et al, 1996) to reflect tree
distance rather than string distance; (Cherry and
Lin, 2006) modify an ITG aligner by introducing
a penalty for induced parses that violate syntac-
tic bracketing constraints. Similarly to these ap-
proaches, we use syntactic bracketing to constrain
alignment, but our work extends beyond improving
alignment quality to improve translation quality as
well.
2 Link Deletion
We propose an algorithm to re-align a parallel bitext
that has been aligned by GIZA++ (IBM Model 4),
then symmetrized using the union heuristic. We then
train a syntax-based translation system on the re-
aligned bitext, and evaluate whether the re-aligned
bitext yields a better translation model than a base-
line system trained on the GIZA++ union aligned
bitext.
2.1 Link Deletion Algorithm
Our algorithm for re-alignment proceeds as follows.
We make a single pass over the corpus. For each sen-
tence pair, we initialize the alignment A = Ainitial
(the GIZA++ union alignment for that sentence
pair). We represent the score of A as a weighted
linear combination of features hi of the alignment
A, the target parse tree parse(e) (a phrase-structure
syntactic representation of e), and the source string
f :
score(A) =
n
?
i=0
?i ? hi(A, parse(e), f)
We define a branch of links to be a contiguous 1-
to-many alignment.2 We define two alignments, A
2In Figure 1, the 1-to-many alignment formed by {? )-
its, ? )- own,? )-country} constitutes a branch, but the
1-to-many alignment formed by {? -starts,? -out,? -
needs} does not.
46
and A?, to be neighbors if they differ only by the
deletion of a link or branch of links. We consider all
alignments A? in the neighborhood of A, greedily
deleting the link l or branch of links b maximizing
the score of the resulting alignment A? = A \ l or
A? = A \ b. We delete links until no further increase
in the score of A is possible.3
In section 2.2 we describe the features hi, and in
section 2.4 we describe how to set the weights ?i.
2.2 Features
2.2.1 Syntactic Features
We use two features of the string-to-tree trans-
ducer rules extracted from A, parse(e), and f ac-
cording to the rule extraction algorithm described in
(Galley et al, 2004):
ruleCount: Total number of rules extracted from
A, parse(e), and f . As Figure 1 illustrates, in-
correct links violating syntactic brackets tend to de-
crease ruleCount; ruleCount increases from 4 to 8
after deleting the incorrect link.
sizeOfLargestRule: The size, measured in terms
of internal nodes in the target parse tree, of the single
largest rule extracted from A, parse(e), and f . In
Figure 1, the largest rules in the leftmost and right-
most columns are R1 (with 9 internal nodes) and R9
(with 4 internal nodes), respectively.
2.2.2 Structural Features
wordsUnaligned: Total number of unaligned
words.
1-to-many Links: Total number of links for which
one word is aligned to multiple words, in either di-
rection. In Figure 1, the links {? -starts,? -
out,? -needs} represent a 1-to-many alignment.
1-to-many links appear more frequently in GIZA++
union alignments than in gold alignments, and are
therefore good candidates for deletion. The cate-
gory of 1-to-many links is further subdivided, de-
pending on the degree of contiguity that the link ex-
hibits with its neighbors.4 Each link in a 1-to-many
3While using a dynamic programming algorithm would
likely improve search efficiency and allow link deletion to find
an optimal solution, in practice, the greedy search runs quickly
and improves alignment quality.
4(Deng and Byrne, 2005) observe that, in a manually aligned
Chinese-English corpus, 82% of the Chinese words that are
alignment can have 0, 1, or 2 neighbors, according
to how many links are adjacent to it in the 1-to-many
alignment:
zeroNeighbors: In Figure 1, the link ? -needs
has 0 neighbors.
oneNeighbor: In Figure 1, the links ? -starts
and ? -out each have 1 neighbor?namely, each
other.
twoNeighbors: In Figure 1, in the 1-to-many
alignment formed by {? )-its,? )-own,? )-
country}, the link ? )-own has 2 neighbors,
namely ? )-it and ? )-country.
2.2.3 Lexical Features
highestLexProbRank: A link ei-fj is ?max-
probable from ei to fj? if p(fj |ei) > p(fj? |ei) for
all alternative words fj? with which ei is aligned
in Ainitial. In Figure 1, p(? |needs) > p(?
|needs), so ? -needs is max-probable for
?needs?. The definition of ?max-probable from fj to
ei? is analogous, and a link is max-probable (nondi-
rectionally) if it is max-probable in either direction.
The value of highestLexProbRank is the total num-
ber of max-probable links. The conditional lexical
probabilities p(ei|fj) and p(fj |ei) are estimated us-
ing frequencies of aligned word pairs in the high-
precision GIZA++ intersection alignments for the
training corpus.
2.2.4 History Features
In addition to the above syntactic, structural,
and lexical features of A, we also incorporate
two features of the link deletion history itself into
Score(A):
linksDeleted: Total number of links deleted
Ainitial thus far. At each iteration, either a link or
a branch of links is deleted.
aligned to multiple English words are aligned to a contiguous
block of English words; similarly, 88% of the English words
that are aligned to multiple Chinese words are aligned to a con-
tiguous block of Chinese words. Thus, if a Chinese word is cor-
rectly aligned to multiple English words, those English words
are likely to be ?neighbors? of each other, and if an English
word is correctly aligned to multiple Chinese words, those Chi-
nese words are likely to be ?neighbors? of each other.
47
stepsTaken: Total number of iterations thus far in
the search; at each iteration, either a link or a branch
is deleted. This feature serves as a constant cost
function per step taken during link deletion.
2.3 Constraints
Protecting Refined Links from Deletion: Since
GIZA++ refined links have higher precision than
union links5, we do not consider any GIZA++ re-
fined links for deletion.6
Stoplist: In our Chinese-English corpora, the 10
most common English words (excluding punc-
tuation marks) include {a,in,to,of,and,the}, while
the 10 most common Chinese words include
{?,4,?,Z,{}. Of these, {a,the} and {?,{}
have no explicit translational equivalent in the other
language. These words are aligned with each other
frequently (and erroneously) by GIZA++ union, but
rarely in the gold standard. We delete all links in
the set {a, an, the} ? {{, ?} from Ainitial as a
preprocessing step.7
2.4 Perceptron Training
We set the feature weights ? using a modified ver-
sion of averaged perceptron learning with structured
outputs (Collins, 2002). Following (Moore, 2005),
we initialize the value of our expected most infor-
mative feature (ruleCount) to 1.0, and initialize all
other feature weights to 0. During each pass over the
discriminative training set, we ?decode? each sen-
tence pair by greedily deleting links from Ainitial in
order to maximize the score of the resulting align-
ment using the current settings of ? (for details, refer
to section 2.1).
5On a 400-sentence-pair Chinese-English data set, GIZA++
union alignments have a precision of 77.32 while GIZA++ re-
fined alignments have a precision of 85.26.
6To see how GIZA++ refined alignments compare to
GIZA++ union alignments for syntax-based translation, we
compare systems trained on each set of alignments for Chinese-
English translation task A. Union alignments result in a test set
BLEU score of 41.17, as compared to only 36.99 for refined.
7The impact upon alignment f-measure of deleting these
stoplist links is small; on Chinese-English Data Set A, the f-
measure of the baseline GIZA++ union alignments on the test
set increases from 63.44 to 63.81 after deleting stoplist links,
while the remaining increase in f-measure from 63.81 to 75.14
(shown in Table 3) is due to the link deletion algorithm itself.
We construct a set of candidate alignments
Acandidates for use in reranking as follows. Starting
with A = Ainitial, we iteratively explore all align-
ments A? in the neighborhood of A, adding each
neighbor to Acandidates, then selecting the neigh-
bor that maximizes Score(A?). When it is no
longer possible to increase Score(A) by deleting
any links, link deletion concludes and returns the
highest-scoring alignment, A1-best.
In general, Agold /? Acandidates; following
(Collins, 2000) and (Charniak and Johnson, 2005)
for parse reranking and (Liang et al, 2006) for trans-
lation reranking, we define Aoracle as alignment in
Acandidates that is most similar to Agold.8 We up-
date each feature weight ?i as follows: ?i = ?i +
hAoraclei ? h
A1-best
i .
9
Following (Moore, 2005), after each training
pass, we average all the feature weight vectors seen
during the pass, and decode the discriminative train-
ing set using the vector of averaged feature weights.
When alignment quality stops increasing on the dis-
criminative training set, perceptron training ends.10
The weight vector returned by perceptron training is
the average over the training set of all weight vectors
seen during all iterations; averaging reduces overfit-
ting on the training set (Collins, 2002).
3 Experimental Setup
3.1 Data Sets
We evaluate the effect of link deletion upon align-
ment quality and translation quality for two Chinese-
English data sets, and one Arabic-English data set.
Each data set consists of newswire, and contains a
small subset of manually aligned sentence pairs. We
divide the manually aligned subset into a training set
(used to discriminatively set the feature weights for
link deletion) and a test set (used to evaluate the im-
pact of link deletion upon alignment quality). Table
1 lists the source and the size of the manually aligned
training and test sets used for each alignment task.
8We discuss alignment similarity metrics in detail in Section
3.2.
9(Liang et al, 2006) report that, for translation reranking,
such local updates (towards the oracle) outperform bold updates
(towards the gold standard).
10We discuss alignment quality metrics in detail in Section
3.2.
48
Using the feature weights learned on the manually
aligned training set, we then apply link deletion to
the remainder (non-manually aligned) of each bilin-
gual data set, and train a full syntax-based statistical
MT system on these sentence pairs. After maximum
BLEU tuning (Och, 2003a) on a held-out tuning set,
we evaluate translation quality on a held-out test set.
Table 2 lists the source and the size of the training,
tuning, and test sets used for each translation task.
3.2 Evaluation Metrics
AER (Alignment Error Rate) (Och and Ney, 2003)
is the most widely used metric of alignment qual-
ity, but requires gold-standard alignments labelled
with ?sure/possible? annotations to compute; lack-
ing such annotations, we can compute alignment f-
measure instead.
However, (Fraser and Marcu, 2007a) show that,
in phrase-based translation, improvements in AER
or f-measure do not necessarily correlate with im-
provements in BLEU score. They propose two mod-
ifications to f-measure: varying the precision/recall
tradeoff, and fully-connecting the alignment links
before computing f-measure.11
Weighted Fully-Connected F-Measure Given a
hypothesized set of alignment links H and a gold-
standard set of alignment links G, we define H+ =
fullyConnect(H) and G+ = fullyConnect(G),
and then compute:
f -measure(H+) = 1?
precision(H+) +
1??
recall(H+)
For phrase-based Chinese-English and Arabic-
English translation tasks, (Fraser and Marcu, 2007a)
obtain the closest correlation between weighted
fully-connected alignment f-measure and BLEU
score using ?=0.5 and ?=0.1, respectively. We
use weighted fully-connected alignment f-measure
as the training criterion for link deletion, and to eval-
uate alignment quality on training and test sets.
Rule F-Measure To evaluate the impact of link
deletion upon rule quality, we compare the rule pre-
cision, recall, and f-measure of the rule set extracted
11In Figure 1, the fully-connected version of the alignments
shown would include the links ? -starts and ? - out.
Language Train Test
Chinese-English A 400 400
Chinese-English B 1500 1500
Arabic-English 1500 1500
Table 1: Size (sentence pairs) of data sets used in align-
ment link deletion tasks
from our hypothesized alignments and a Collins-
style parser against the rule set extracted from gold
alignments and gold parses.
BLEU For all translation tasks, we report case-
insensitive NIST BLEU scores (Papineni et al,
2002) using 4 references per sentence.
3.3 Experiments
Starting with GIZA++ union (IBM Model 4) align-
ments, we use perceptron training to set the weights
of each feature used in link deletion in order to opti-
mize weighted fully-connected alignment f-measure
(?=0.5 for Chinese-English and ?=0.1 for Arabic-
English) on a manually aligned discriminative train-
ing set. We report the (fully-connected) precision,
recall, and weighted alignment f-measure on a held-
out test set after running perceptron training, relative
to the baseline GIZA++ union alignments. Using
the learned feature weights, we then perform link
deletion over the GIZA++ union alignments for the
entire training corpus for each translation task. Us-
ing these alignments, which we refer to as ?GIZA++
union + link deletion?, we train a syntax-based trans-
lation system similar to that described in (Galley et
al., 2006). After extracting string-to-tree translation
rules from the aligned, parsed training corpus, the
system assigns weights to each rule via frequency
estimation with smoothing. The rule probabilities,
as well as trigram language model probabilities and
a handful of additional features of each rule, are used
as features during decoding. The feature weights are
tuned using minimum error rate training (Och and
Ney, 2003) to optimize BLEU score on a held-out
development set. We then compare the BLEU score
of this system against a baseline system trained us-
ing GIZA++ union alignments.
To determine which value of ? is most effective
as a training criterion for link deletion, we set ?=0.4
(favoring recall), 0.5, and 0.6 (favoring precision),
49
Language Train Tune Test1 Test2
Chinese-English A 9.8M/newswire 25.9k/NIST02 29.0k/NIST03 ?
Chinese-English B 12.3M/newswire 42.9k/newswire 42.1k/newswire ?
Arabic-English 174.8M/newswire 35.8k/NIST04-05 40.3k/NIST04-05 53.0k/newswire
Table 2: Size (English words) and source of data sets used in translation tasks
and compare the effect on translation quality for
Chinese-English data set A.
4 Results
For each translation task, link deletion improves
translation quality relative to a GIZA++ union base-
line. For each alignment task, link deletion tends to
improve fully-connected alignment precision more
than it decreases fully-connected alignment recall,
increasing weighted fully-connected alignment f-
measure overall.
4.1 Chinese-English
On Chinese-English translation task A, link deletion
increases BLEU score by 1.26 points on tuning and
0.76 points on test (Table 3); on Chinese-English
translation task B, link deletion increases BLEU
score by 1.38 points on tuning and 0.49 points on
test (Table 3).
4.2 Arabic-English
On the Arabic-English translation task, link dele-
tion improves BLEU score by 0.84 points on tuning,
0.18 points on test1, and 0.56 points on test2 (Ta-
ble 3). Note that the training criterion for Arabic-
English link deletion uses ?=0.1; because this pe-
nalizes a loss in recall more heavily than it re-
wards an increase in precision, it is more difficult
to increase weighted fully-connected alignment f-
measure using link deletion for Arabic-English than
for Chinese-English. This difference is reflected in
the average number of links deleted per sentence:
4.19 for Chinese-English B (Table 3), but only 1.35
for Arabic-English (Table 3). Despite this differ-
ence, link deletion improves translation results for
Arabic-English as well.
4.3 Varying ?
On Chinese-English data set A, we explore the ef-
fect of varying ? in the weighted fully-connected
93 187 375 750 1500
46
48
50
52
54
56
58
60
62
64
Training Sentence Pairs
Te
st
 S
et
 W
ei
gh
te
d 
Fu
lly
?C
on
ne
ct
ed
 A
lig
nm
en
t F
?M
ea
su
re
 
 
GIZA++ union
GIZA++ union + link deletion
Figure 2: Effect of discriminative training set size on link
deletion accuracy for Chinese-English B, ?=0.5
alignment f-measure used as the training criterion
for link deletion. Using ?=0.5 leads to a higher gain
in BLEU score on the test set relative to the base-
line (+0.76 points) than either ?=0.4 (+0.70 points)
or ?=0.6 (+0.67 points).
4.4 Size of Discriminative Training Set
To examine how many manually aligned sentence
pairs are required to set the feature weights reli-
ably, we vary the size of the discriminative training
set from 2-1500 sentence pairs while holding test
set size constant at 1500 sentence pairs; run per-
ceptron training; and record the resulting weighted
fully-connected alignment f-measure on the test set.
Figure 2 illustrates that using 100-200 manually
aligned sentence pairs of training data is sufficient
for Chinese-English; a similarly-sized training set is
also sufficient for Arabic-English.
4.5 Effect of Link Deletion on Extracted Rules
Link deletion increases the size of the extracted
grammar. To determine how the quality of the ex-
tracted grammar changes, we compute the rule pre-
50
Language Alignment Prec Rec ? F-measure Links Del/ Grammar BLEUSent Size Tune Test1 Test2
Chi-Eng A GIZA++ union 54.76 75.38 0.5 63.44 ? 23.4M 41.80 41.17 ?
Chi-Eng A GIZA++ union + 79.59 71.16 0.5 75.14 4.77 59.7M 43.06 41.93 ?link deletion
Chi-Eng B GIZA++ union 36.61 66.28 0.5 47.16 ? 28.9M 39.59 41.39 ?
Chi-Eng B GIZA++ union + 65.52 59.28 0.5 62.24 4.19 73.0M 40.97 41.88 ?link deletion
Ara-Eng GIZA++ union 35.34 84.05 0.1 73.87 ? 52.4M 54.73 50.9 38.16
Ara-Eng GIZA++ union + 52.68 79.75 0.1 75.85 1.35 64.9M 55.57 51.08 38.72link deletion
Table 3: Results of link deletion. Weighted fully-connected alignment f-measure is computed on alignment test sets
(Table 1); BLEU score is computed on translation test sets (Table 2).
Alignment Parse RulePrecision Recall F-measure Total Non-Unique
gold gold 100.00 100.00 100.00 12,809
giza++ union collins 50.49 44.23 47.15 11,021
giza++ union+link deletion, ?=0.5 collins 47.51 53.20 50.20 13,987
giza++ refined collins 44.20 54.06 48.64 15,182
Table 4: Rule precision, recall, and f-measure of rules extracted from 400 sentence pairs of Chinese-English data
cision, recall, and f-measure of the GIZA++ union
alignments and various link deletion alignments on
a held-out Chinese-English test set of 400 sentence
pairs. Table 4 indicates the total (non-unique) num-
ber of rules extracted for each alignment/parse pair-
ing, as well as the rule precision, recall, and f-
measure of each pair. As more links are deleted,
more rules are extracted?but of those, some are of
good quality and others are of bad quality. Link-
deleted alignments produce rule sets with higher rule
f-measure than either GIZA++ union or GIZA++ re-
fined.
5 Conclusion
We have presented a link deletion algorithm that im-
proves the precision of GIZA++ union alignments
without notably decreasing recall. In addition to lex-
ical and structural features, we use features of the ex-
tracted syntax-based translation rules. Our method
improves alignment quality and translation quality
on Chinese-English and Arabic-English translation
tasks, relative to a GIZA++ union baseline. The
algorithm runs quickly, and is easily applicable to
other language pairs with limited amounts (100-200
sentence pairs) of manually aligned data available.
Acknowledgments
We thank Steven DeNeefe and Wei Wang for assis-
tance with experiments, and Alexander Fraser and
Liang Huang for helpful discussions. This research
was supported by DARPA (contract HR0011-06-C-
0022) and by a fellowship from AT&T Labs.
51
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The Mathematics of Sta-
tistical Machine Translation: Parameter Estimation.
Computational Linguistics, Vol. 19, No. 2, 1993.
Eugene Charniak and Mark Johnson. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
Proceedings of ACL, 2005.
Colin Cherry and Dekang Lin. Soft Syntactic Constraints
for Word Alignment through Discriminative Training.
Proceedings of ACL (Poster), 2006.
David Chiang. A Hierarchical Phrase-Based Model for
Statistical Machine Translation. Proceedings of ACL,
2005.
David Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 2007.
Michael Collins. Discriminative Reranking for Natural
Language Parsing. Proceedings of ICML, 2000.
Michael Collins. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. Proceedings of EMNLP,
2002.
John DeNero and Dan Klein. Tailoring Word Align-
ments to Syntactic Machine Translation. Proceedings
of ACL, 2007.
Yonggang Deng and William Byrne. HMM word and
phrase alignment for statistical machine translation.
Proceedings of HLT/EMNLP, 2005.
Alexander Fraser and Daniel Marcu. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, Vol. 33, No. 3, 2007.
Alexander Fraser and Daniel Marcu. Getting the Struc-
ture Right for Word Alignment: LEAF. Proceedings of
EMNLP, 2007.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. What?s in a Translation Rule? Proceedings of
HLT/NAACL-04, 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. Proceedings of
ACL, 2006.
Liang Huang, Kevin Knight, and Aravind Joshi. Statis-
tical Syntax-Directed Translation with Extended Do-
main of Locality. Proceedings of AMTA, 2006.
Abraham Ittycheriah and Salim Roukos. A Maximum En-
tropy Word Aligner for Arabic-English Machine Trans-
lation. Proceedings of HLT/EMNLP, 2005.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
Statistical Phrase-Based Translation. Proceedings of
HLT/NAACL, 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. Moses: Open Source Toolkit for
Statistical Machine Translation. Proceedings of ACL
(demo), 2007.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and
Ben Taskar. An end-to-end discriminative approach to
machine translation. Proceedings of COLING/ACL,
2006.
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear Models
for Word Alignment. Proceedings of ACL, 2005.
Yang Liu, Qun Liu, and Shouxun Lin. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. Proceedings of ACL, 2006.
Adam Lopez and Philip Resnik. Improved HMM Align-
ment Models for Languages with Scarce Resources.
Proceedings of the ACL Workshop on Parallel Text,
2005.
Jonathan May and Kevin Knight. Syntactic Re-Alignment
Models for Machine Translation. Proceedings of
EMNLP-CoNLL, 2007.
Robert C. Moore. A Discriminative Framework for Bilin-
gual Word Alignment. Proceedings of HLT/EMNLP,
2005.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. Im-
proved discriminative bilingual word alignment. Pro-
ceedings of ACL, 2006.
Franz Josef Och. Minimum Error Rate Training in Sta-
tistical Machine Translation. Proceedings of ACL,
2003.
Franz Josef Och and Hermann Ney. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, Vol. 29, No. 1, 2003.
Franz Josef Och and Hermann Ney. The alignment
template approach to statistical machine translation.
Computational Linguistics, 2004.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU:
a Method for Automatic Evaluation of Machine Trans-
lation. Proceedings of ACL, 2002.
Chris Quirk, Arul Menezes, and Colin Cherry. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. Proceedings of ACL, 2005.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. A
Discriminative Matching Approach to Word Align-
ment. Proceedings of HTL/EMNLP, 2005.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
HMM-Based Word Alignment in Statistical Transla-
tion Proceedings of COLING, 1996.
52
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28?35,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A New Objective Function for Word Alignment
Tugba Bodrumlu Kevin Knight Sujith Ravi
Information Sciences Institute & Computer Science Department
University of Southern California
bodrumlu@usc.edu, knight@isi.edu, sravi@isi.edu
Abstract
We develop a new objective function for word
alignment that measures the size of the bilin-
gual dictionary induced by an alignment. A
word alignment that results in a small dictio-
nary is preferred over one that results in a large
dictionary. In order to search for the align-
ment that minimizes this objective, we cast the
problem as an integer linear program. We then
extend our objective function to align corpora
at the sub-word level, which we demonstrate
on a small Turkish-English corpus.
1 Introduction
Word alignment is the problem of annotating a bilin-
gual text with links connecting words that have the
same meanings. Figure 1 shows sample input for
a word aligner (Knight, 1997). After analyzing the
text, we may conclude, for example, that sprok cor-
responds to dat in the first sentence pair.
Word alignment has several downstream con-
sumers. One is machine translation, where pro-
grams extract translation rules from word-aligned
corpora (Och and Ney, 2004; Galley et al, 2004;
Chiang, 2007; Quirk et al, 2005). Other down-
stream processes exploit dictionaries derived by
alignment, in order to translate queries in cross-
lingual IR (Scho?nhofen et al, 2008) or re-score can-
didate translation outputs (Och et al, 2004).
Many methods of automatic alignment have been
proposed. Probabilistic generative models like IBM
1-5 (Brown et al, 1993), HMM (Vogel et al, 1996),
ITG (Wu, 1997), and LEAF (Fraser and Marcu,
2007) define formulas for P(f | e) or P(e, f), with
ok-voon ororok sprok
at-voon bichat dat
erok sprok izok hihok ghirok
totat dat arrat vat hilat
ok-drubel ok-voon anok plok sprok
at-drubel at-voon pippat rrat dat
ok-voon anok drok brok jok 
at-voon krat pippat sat lat 
wiwok farok izok stok
totat jjat quat cat 
lalok sprok izok jok stok
wat dat krat quat cat 
lalok farok ororok lalok sprok izok enemok
wat jjat bichat wat dat vat eneat 
lalok brok anok plok nok 
iat lat pippat rrat nnat
wiwok nok izok kantok ok-yurp
totat nnat quat oloat at-yurp
lalok mok nok yorok ghirok clok
wat nnat gat mat bat hilat
lalok nok crrrok hihok yorok zanzanok
wat nnat arrat mat zanzanat 
lalok rarok nok izok hihok mok
wat nnat forat arrat vat gat 
Figure 1: Word alignment exercise (Knight, 1997).
28
hidden alignment variables. EM algorithms estimate
dictionary and other probabilities in order to maxi-
mize those quantities. One can then ask for Viterbi
alignments that maximize P(alignment | e, f). Dis-
criminative models, e.g. (Taskar et al, 2005), in-
stead set parameters to maximize alignment accu-
racy against a hand-aligned development set. EMD
training (Fraser and Marcu, 2006) combines genera-
tive and discriminative elements.
Low accuracy is a weakness for all systems. Most
practitioners still use 1990s algorithms to align their
data. It stands to reason that we have not yet seen
the last word in alignment models.
In this paper, we develop a new objective function
for alignment, inspired by watching people manu-
ally solve the alignment exercise of Figure 1. When
people attack this problem, we find that once they
create a bilingual dictionary entry, they like to re-
use that entry as much as possible. Previous ma-
chine aligners emulate this to some degree, but they
are not explicitly programmed to do so.
We also address another weakness of current
aligners: they only align full words. With few ex-
ceptions, e.g. (Zhang et al, 2003; Snyder and Barzi-
lay, 2008), aligners do not operate at the sub-word
level, making them much less useful for agglutina-
tive languages such as Turkish.
Our present contributions are as follows:
? We offer a simple new objective function that
scores a corpus alignment based on how many
distinct bilingual word pairs it contains.
? We use an integer programming solver to
carry out optimization and corpus alignment.
? We extend the system to perform sub-
word alignment, which we demonstrate on a
Turkish-English corpus.
The results in this paper constitute a proof of con-
cept of these ideas, executed on small corpora. We
conclude by listing future directions.
2 New Objective Function for Alignment
We search for the legal alignment that minimizes the
size of the induced bilingual dictionary. By dictio-
nary size, we mean the number of distinct word-
pairs linked in the corpus alignment. We can im-
mediately investigate how different alignments stack
up, according to this objective function. Figure 2
garcia and associates 
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates 
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
its groups are in europe
sus grupos estan en europa
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
Figure 2: Gold alignment. The induced bilingual dic-
tionary has 28 distinct entries, including garcia/garcia,
are/son, are/estan, not/no, has/tiene, etc.
29
garcia and associates 
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates 
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
its groups are in europe
sus grupos estan en europa
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
Figure 3: IP alignment. The induced bilingual dictionary
has 28 distinct entries.
shows the gold alignment for the corpus in Figure 1
(displayed here as English-Spanish), which results
in 28 distinct bilingual dictionary entries. By con-
trast, a monotone alignment induces 39 distinct en-
tries, due to less re-use.
Next we look at how to automatically rifle through
all legal alignments to find the one with the best
score. What is a legal alignment? For now, we con-
sider it to be one where:
? Every foreign word is aligned exactly once
(Brown et al, 1993).
? Every English word has either 0 or 1 align-
ments (Melamed, 1997).
We formulate our integer program (IP) as follows.
We set up two types of binary variables:
? Alignment link variables. If link-i-j-k = 1, that
means in sentence pair i, the foreign word at
position j aligns to the English words at posi-
tion k.
? Bilingual dictionary variables. If dict-f-e = 1,
that means word pair (f, e) is ?in? the dictio-
nary.
We constrain the values of link variables to sat-
isfy the two alignment conditions listed earlier. We
also require that if link-i-j-k = 1 (i.e., we?ve decided
on an alignment link), then dict-fij -eik should also
equal 1 (the linked words are recorded as a dictio-
nary entry).1 We do not require the converse?just
because a word pair is available in the dictionary, the
aligner does not have to link every instance of that
word pair. For example, if an English sentence has
two the tokens, and its Spanish translation has two
la tokens, we should not require that all four links
be active?in fact, this would conflict with the 1-1
link constraints and render the integer program un-
solvable. The IP reads as follows:
minimize:?
f,e dict-f-e
subject to:
?i,j
?
k link-i-j-k = 1
?i,k
?
j link-i-j-k ? 1
?i,j,k link-i-j-k ? dict-fij -eik
On our Spanish-English corpus, the cplex2 solver
obtains a minimal objective function value of 28. To
1fij is the jth foreign word in the ith sentence pair.
2www.ilog.com/products/cplex
30
get the second-best alignment, we add a constraint
to our IP requiring the sum of the n variables active
in the previous solution to be less than n, and we
re-run cplex. This forces cplex to choose different
variable settings on the second go-round. We repeat
this procedure to get an ordered list of alignments.3
We find that there are 8 distinct solutions that
yield the same objective function value of 28. Fig-
ure 3 shows one of these. This alignment is not bad,
considering that word-order information is not en-
coded in the IP. We can now compare several align-
ments in terms of both dictionary size and alignment
accuracy. For accuracy, we represent each alignment
as a set of tuples < i, j, k >, where i is the sentence
pair, j is a foreign index, and k is an English index.
We use these tuples to calculate a balanced f-score
against the gold alignment tuples.4
Method Dict size f-score
Gold 28 100.0
Monotone 39 68.9
IBM-1 (Brown et al, 1993) 30 80.3
IBM-4 (Brown et al, 1993) 29 86.9
IP 28 95.9
The last line shows an average f-score over the 8 tied
IP solutions.
Figure 4 further investigates the connection be-
tween our objective function and alignment accu-
racy. We sample up to 10 alignments at each of
several objective function values v, by first adding
a constraint that dict variables add to exactly v, then
iterating the n-best list procedure above. We stop
when we have 10 solutions, or when cplex fails to
find another solution with value v. In this figure, we
see a clear relationship between the objective func-
tion and alignment accuracy?minimizing the for-
mer is a good way to maximize the latter.
3This method not only suppresses the IP solutions generated
so far, but it suppresses additional solutions as well. In partic-
ular, it suppresses solutions in which all link and dict variables
have the same values as in some previous solution, but some
additional dict variables are flipped to 1. We consider this a fea-
ture rather than a bug, as it ensures that all alignments in the
n-best list are unique. For what we report in this paper, we only
create n-best lists whose elements possess the same objective
function value, so the issue does not arise.
4P = proportion of proposed links that are in gold,
R = proportion of gold links that are proposed, and f-
score = 2PR/(P+R).
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
A
ve
ra
ge
 a
lig
nm
en
t a
cc
ur
ac
y 
 (f
-s
co
re
)
Number of bilingual dictionary entries in solution 
 (objective function value)
28 30 32 34 36 38 40 42 44 46
Figure 4: Relationship between IP objective (x-axis =
size of induced bilingual dictionary) and alignment ac-
curacy (y-axis = f-score).
Turkish English
yururum i walk
yururler they walk
Figure 5: Two Turkish-English sentence pairs.
3 Sub-Word Alignment
We now turn to alignment at the sub-word level.
Agglutinative languages like Turkish present chal-
lenges for many standard NLP techniques. An ag-
glutinative language can express in a single word
(e.g., yurumuyoruz) what might require many words
in another language (e.g., we are not walking).
Naively breaking on whitespace results in a very
large vocabulary for Turkish, and it ignores the
multi-morpheme structure inside Turkish words.
Consider the tiny Turkish-English corpus in Fig-
ure 5. Even a non-Turkish speaker might plausi-
bly align yurur to walk, um to I, and ler to they.
However, none of the popular machine aligners
is able to do this, since they align at the whole-
word level. Designers of translation systems some-
times employ language-specific word breakers be-
fore alignment, though these are hard to build and
maintain, and they are usually not only language-
specific, but also language-pair-specific. Good un-
31
supervised monolingual morpheme segmenters are
also available (Goldsmith, 2001; Creutz and Lagus,
2005), though again, these do not do joint inference
of alignment and word segmentation.
We extend our objective function straightfor-
wardly to sub-word alignment. To test our exten-
sion, we construct a Turkish-English corpus of 1616
sentence pairs. We first manually construct a regu-
lar tree grammar (RTG) (Gecseg and Steinby, 1984)
for a fragment of English. This grammar produces
English trees; it has 86 rules, 26 states, and 53 ter-
minals (English words). We then construct a tree-to-
string transducer (Rounds, 1970) that converts En-
glish trees into Turkish character strings, including
space. Because it does not explicitly enumerate the
Turkish vocabulary, this transducer can output a very
large number of distinct Turkish words (i.e., charac-
ter sequences preceded and followed by space). This
transducer has 177 rules, 18 states, and 23 termi-
nals (Turkish characters). RTG generation produces
English trees that the transducer converts to Turk-
ish, both via the tree automata toolkit Tiburon (May
and Knight, 2006). From this, we obtain a parallel
Turkish-English corpus. A fragment of the corpus is
shown in Figure 6. Because we will concentrate on
finding Turkish sub-words, we manually break off
the English sub-word -ing, by rule, as seen in the
last line of the figure.
This is a small corpus, but good for demonstrat-
ing our concept. By automatically tracing the inter-
nal operation of the tree transducer, we also produce
a gold alignment for the corpus. We use the gold
alignment to tabulate the number of morphemes per
Turkish word:
n % Turkish types % Turkish tokens
with n morphemes with n morphemes
1 23.1% 35.5%
2 63.5% 61.6%
3 13.4% 2.9%
Naturally, these statistics imply that standard whole-
word aligners will fail. By inspecting the corpus, we
find that 26.8 is the maximium f-score available to
whole-word alignment methods.
Now we adjust our IP formulation. We broaden
the definition of legal alignment to include breaking
any foreign word (token) into one or more sub-word
(tokens). Each resulting sub-word token is aligned
to exactly one English word token, and every En-
glish word aligns to 0 or 1 foreign sub-words. Our
dict-f-e variables now relate Turkish sub-words to
English words. The first sentence pair in Figure 5
would have previously contributed two dict vari-
ables; now it contributes 44, including things like
dict-uru-walk. We consider an alignment to be a set
of tuples < i, j1, j2, k >, where j1 and j2 are start
and end indices into the foreign character string. We
create align-i-j1-j2-k variables that connect Turkish
character spans with English word indices. Align-
ment variables constrain dictionary variables as be-
fore, i.e., an alignment link can only ?turn on? when
licensed by the dictionary.
We previously constrained every Turkish word to
align to something. However, we do not want ev-
ery Turkish character span to align?only the spans
explicitly chosen in our word segmentation. So we
introduce span-i-j1-j2 variables to indicate segmen-
tation decisions. Only when span-i-j1-j2 = 1 do we
require
?
k align-i-j1-j2-k = 1.
For a coherent segmentation, the set of active span
variables must cover all Turkish letter tokens in the
corpus, and no pair of spans may overlap each other.
To implement these constraints, we create a lattice
where each node represents a Turkish index, and
each transition corresponds to a span variable. In a
coherent segmentation, the sum of all span variables
entering an lattice-internal node equals the sum of
all span variables leaving that node. If the sum of
all variables leaving the start node equals 1, then we
are guaranteed a left-to-right path through the lat-
tice, i.e., a coherent choice of 0 and 1 values for span
variables.
The IP reads as follows:
minimize:?
f,e dict-f-e
subject to:
?i,j1,j2
?
k align-i-j1-j2-k = span-i-j1-j2
?i,k
?
j1,j2 align-i-j1-j2-k ? 1
?i,j1,j2,k align-i-j1-j2-k ? dict-fi,j1,j2-ei,k
?i,j
?
j3 span-i-j3-j =
?
j3 span-i-j-j3
?i,w
?
j>w span-i-w-j = 1
(w ranges over Turkish word start indices)
With our simple objective function, we obtain an
f-score of 61.4 against the gold standard. Sample
gold and IP alignments are shown in Figure 7.
32
Turkish English
onlari gordum i saw them
gidecekler they will go
onu stadyumda gordum i saw him in the stadium
ogretmenlerim tiyatroya yurudu my teachers walked to the theatre
cocuklar yurudu the girls walked
babam restorana gidiyor my father is walk ing to the restaurant
. . . . . .
Figure 6: A Turkish-English corpus produced by an English grammar pipelined with an English-to-Turkish tree-to-
string transducer.
you   go   to   his   office
onun ofisi- -ne   gider- -sin
Gold alignment IP sub-word alignment
you   go   to   his   office
onun ofisi- -ne   gider- -sin
my  teachers  ran  to  their  house
ogretmenler- -im onlarin evi- -ne  kostu
my  teachers  ran  to  their  house
ogretmenler- -im onlarin evi- -ne  kostu
i  saw  him
onu gordu- -m
i  saw  him
onu gordu- -m
we  go  to  the  theatre
tiyatro- -ya gider- -iz
we  go  to  the  theatre
tiyatro- -ya gider- -iz
they  walked  to  the  store
magaza- -ya yurudu- -ler
they  walked  to  the  store
magaza- -ya yurudu- -ler
my aunt goes to their house
hala- -m  onlarin evi- -ne  gider
my aunt goes to their house
hal- -am  onlarin evi- -ne  gider
1.
2.
3.
5.
6.
4.
Figure 7: Sample gold and (initial) IP sub-word alignments on our Turkish-English corpus. Dashes indicate where the
IP search has decided to break Turkish words in the process of aligning. For examples, the word magazaya has been
broken into magaza- and -ya.
33
The last two incorrect alignments in the figure
are instructive. The system has decided to align
English the to the Turkish noun morphemes tiyatro
and magaza, and to leave English nouns theatre and
store unaligned. This is a tie-break decision. It is
equally good for the objective function to leave the
unaligned instead?either way, there are two rele-
vant dictionary entries.
We fix this problem by introducing a special
NULL Turkish token, and by modifying the IP to re-
quire every English token to align (either to NULL
or something else). This introduces a cost for fail-
ing to align an English token x to Turkish, because
a new x/NULL dictionary entry will have to be cre-
ated. (The NULL token itself is unconstrained in
how many tokens it may align to.)
Under this scheme, the last two incorrect align-
ments in Figure 7 induce four relevant dictio-
nary entries (the/tiyatro, the/magaza, theatre/NULL,
store/NULL) while the gold alignment induces only
three (the/NULL, theatre/tiyatro, store/magaza), be-
cause the/NULL is re-used. The gold alignment is
therefore now preferred by the IP optimizer. There
is a rippling effect, causing the system to correct
many other decisions as well. This revision raises
the alignment f-score from 61.4 to 83.4.
The following table summarizes our alignment re-
sults. In the table, ?Dict? refers to the size of the
induced dictionary, and ?Sub-words? refers to the
number of induced Turkish sub-word tokens.
Method Dict Sub-words f-score
Gold (sub-word) 67 8102 100.0
Monotone (word) 512 4851 5.5
IBM-1 (word) 220 4851 21.6
IBM-4 (word) 230 4851 20.3
IP (word) 107 4851 20.1
IP (sub-word, 60 7418 61.4
initial)
IP (sub-word, 65 8105 83.4
revised)
Our search for an optimal IP solution is not fast.
It takes 1-5 hours to perform sub-word alignment on
the Turkish-English corpus. Of course, if we wanted
to obtain optimal alignments under IBM Model 4,
that would also be expensive, in fact NP-complete
(Raghavendra and Maji, 2006). Practical Model 4
systems therefore make substantial search approxi-
mations (Brown et al, 1993).
4 Related Work
(Zhang et al, 2003) and (Wu, 1997) tackle the prob-
lem of segmenting Chinese while aligning it to En-
glish. (Snyder and Barzilay, 2008) use multilingual
data to compute segmentations of Arabic, Hebrew,
Aramaic, and English. Their method uses IBM mod-
els to bootstrap alignments, and they measure the re-
sulting segmentation accuracy.
(Taskar et al, 2005) cast their alignment model as
a minimum cost quadratic flow problem, for which
optimal alignments can be computed with off-the-
shelf optimizers. Alignment in the modified model
of (Lacoste-Julien et al, 2006) can be mapped to a
quadratic assignment problem and solved with linear
programming tools. In that work, linear program-
ming is not only used for alignment, but also for
training weights for the discriminative model. These
weights are trained on a manually-aligned subset of
the parallel data. One important ?mega? feature for
the discriminative model is the score assigned by an
IBM model, which must be separately trained on the
full parallel data. Our work differs in two ways: (1)
our training is unsupervised, requiring no manually
aligned data, and (2) we do not bootstrap off IBM
models. (DeNero and Klein, 2008) gives an integer
linear programming formulation of another align-
ment model based on phrases. There, integer pro-
gramming is used only for alignment, not for learn-
ing parameter values.
5 Conclusions and Future Work
We have presented a novel objective function for
alignment, and we have applied it to whole-word and
sub-word alignment problems. Preliminary results
look good, especially given that new objective func-
tion is simpler than those previously proposed. The
integer programming framework makes the model
easy to implement, and its optimal behavior frees us
from worrying about search errors.
We believe there are good future possibilities for
this work:
? Extend legal alignments to cover n-to-m
and discontinuous cases. While morpheme-
to-morpheme alignment is more frequently a
34
1-to-1 affair than word-to-word alignment is,
the 1-to-1 assumption is not justified in either
case.
? Develop new components for the IP objec-
tive. Our current objective function makes no
reference to word order, so if the same word
appears twice in a sentence, a tie-break en-
sues.
? Establish complexity bounds for optimiz-
ing dictionary size. We conjecture that opti-
mal alignment according to our model is NP-
complete in the size of the corpus.
? Develop a fast, approximate alignment al-
gorithm for our model.
? Test on large-scale bilingual corpora.
Acknowledgments
This work was partially supported under DARPA
GALE, Contract No. HR0011-06-C-0022.
References
P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational linguis-
tics, 19(2).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Proc. AKRR.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. ACL.
A. Fraser and D. Marcu. 2006. Semi-supervised training
for statistical word alignment. In Proc. ACL-COLING.
A. Fraser and D. Marcu. 2007. Getting the structure right
for word alignment: LEAF. In Proc. EMNLP-CoNLL.
M. Galley, M. Hopkins, K. Knight, and D Marcu. 2004.
What?s in a translation rule. In Proc. NAACL-HLT.
F. Gecseg and M. Steinby. 1984. Tree automata.
Akademiai Kiado.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
K. Knight. 1997. Automating knowledge acquisition for
machine translation. AI Magazine, 18(4).
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment. In
Proc. HLT-NAACL.
J. May and K. Knight. 2006. Tiburon: A weighted tree
automata toolkit. In Proc. CIAA.
I. D. Melamed. 1997. A word-to-word model of transla-
tional equivalence. In Proc. ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4).
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proc.
HLT-NAACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. ACL.
U. Raghavendra and H. K. Maji. 2006. Computational
complexity of statistical machine translation. In Proc.
EACL.
W. Rounds. 1970. Mappings and grammars on trees.
Theory of Computing Systems, 4(3).
P. Scho?nhofen, A. Benczu?r, I. B??ro?, and K. Csaloga?ny,
2008. Cross-language retrieval with wikipedia.
Springer.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proc. ACL.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. EMNLP.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. ACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3).
Y. Zhang, S. Vogel, and A. Waibel. 2003. Integrated
phrase segmentation and alignment algorithm for sta-
tistical machine translation. In Proc. Intl. Conf. on
NLP and KE.
35
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, page 27,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Automata for Transliteration and Machine Translation
Kevin Knight
Information Sciences Institute
University of Southern California
knight@isi.edu
Abstract
Automata theory, transliteration, and machine
translation (MT) have an interesting and inter-
twined history.
Finite-state string automata theory became a
powerful tool for speech and language after
the introduction of the AT&T?s FSM software.
For example, string transducers can convert be-
tween word sequences and phoneme sequences,
or between phoneme sequences and acoustic se-
quences; furthermore, these machines can be
pipelined to attack complex problems like speech
recognition. Likewise, n-gram models can be cap-
tured by finite-state acceptors, which can be re-
used across applications.
It is possible to mix, match, and compose trans-
ducers to flexibly solve all kinds of problems. One
such problem is transliteration, which can be mod-
eled as a pipeline of string transformations. MT
has also been modeled with transducers, and de-
scendants of the FSM toolkit are now used to im-
plement phrase-based machine translation. Even
speech recognizers and MT systems can them-
selves be composed to deliver speech-to-speech
MT.
The main rub with finite-state string MT is word
re-ordering. Tree transducers offer a natural mech-
anism to solve this problem, and they have re-
cently been employed with some success.
In this talk, we will survey these ideas (and their
origins), and we will finish with a discussion of
how transliteration and MT can work together.
27
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 746?754, Prague, June 2007. c?2007 Association for Computational Linguistics
Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy
Wei Wang and Kevin Knight and Daniel Marcu
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA, 90292
{wwang,kknight,dmarcu}@languageweaver.com
Abstract
We show that phrase structures in Penn Tree-
bank style parses are not optimal for syntax-
based machine translation. We exploit a se-
ries of binarization methods to restructure
the Penn Treebank style trees such that syn-
tactified phrases smaller than Penn Treebank
constituents can be acquired and exploited in
translation. We find that by employing the
EM algorithm for determining the binariza-
tion of a parse tree among a set of alternative
binarizations gives us the best translation re-
sult.
1 Introduction
Syntax-based translation models (Eisner, 2003; Gal-
ley et al, 2006; Marcu et al, 2006) are usually built
directly from Penn Treebank (PTB) (Marcus et al,
1993) style parse trees by composing treebank gram-
mar rules. As a result, often no substructures corre-
sponding to partial PTB constituents are extracted to
form translation rules.
Syntax translation models acquired by composing
treebank grammar rules assume that long rewrites
are not decomposable into smaller steps. This ef-
fectively restricts the generalization power of the in-
duced model. For example, suppose we have an
xRs (Knight and Graehl, 2004) rule R1 in Figure 1
that translates the Chinese phrase RUSSIA MINISTER
VIKTOR-CHERNOMYRDIN into an English NPB tree
fragment yielding an English phrase. Also suppose
that we want to translate a Chinese phrase
VIKTOR-CHERNOMYRDIN AND HIS COLLEAGUE
into English. What we desire is that if we have
another rule R2 as shown in Figure 1, we could
somehow compose it with R1 to obtain the desir-
able translation. We unfortunately cannot do this
because R1 and R2 are not further decomposable
and their substructures cannot be re-used. The re-
quirement that all translation rules have exactly one
root node does not enable us to use the translation of
VIKTOR-CHERNOMYRDIN in any other contexts than
those seen in the training corpus.
A solution to overcome this problem is to right-
binarize the left-hand side (LHS) (or the English-
side) tree of R1 such that we can decompose
R1 into R3 and R4 by factoring NNP(viktor)
NNP(chernomyrdin) out as R4 according to the
word alignments; and left-binarize the LHS of R2 by
introducing a new tree node that collapses the two
NNP?s, so as to generalize this rule, getting rule R5
and rule R6. We also need to consistently syntact-
ify the root labels of R4 and the new frontier label
of R6 such that these two rules can be composed.
Since labeling is not a concern of this paper, we sim-
ply label new nodes with X-bar where X here is the
parent label. With all these in place, we now can
translate the foreign sentence by composing R6 and
R4 in Figure 1.
Binarizing the syntax trees for syntax-based ma-
chine translation is similar in spirit to generalizing
parsing models via markovization (Collins, 1997;
Charniak, 2000). But in translation modeling, it is
unclear how to effectively markovize the translation
rules, especially when the rules are complex like
those proposed by Galley et al (2006).
In this paper, we explore the generalization abil-
ity of simple binarization methods like left-, right-,
and head-binarization, and also their combinations.
Simple binarization methods binarize syntax trees
in a consistent fashion (left-, right-, or head-) and
746
NPB
JJ         NNP      NNP     NNP
russia    minister    viktor   chernomyrdin
RUSSIA   MINISTER      V?C
NPB
?
NNP         NNP      AND  HIS   COLLEAGUE
V?C AND  HIS COLLEAGUE
NPB
JJ
russia
RUSSIA
minister
MINISTER NNP   NNP
viktor  chernomyrdin
V?C
NPB
colleague
and    his     colleague
COLLEAGUE
PRB$
his
HIS
CC
and
ANDNNP      NNP
x0:NNP  x1:NNP V?C AND  HIS COLLEAGUE
NNS
x0:NNP   x1:NNP   CC   PRB$   NNS
R1 R2
NPB
NPB
R3
NPB
NPBR4 R5
R6
R6
R4
NPB
Figure 1: Generalizing translation rules by binarizing trees.
thus cannot guarantee that all the substructures can
be factored out. For example, right binarization on
the LHS of R1 makes available R4, but misses R6
on R2. We then introduce a parallel restructuring
method, that is, one can binarize both to the left and
right at the same time, resulting in a binarization for-
est. We employ the EM (Dempster et al, 1977) algo-
rithm to learn the binarization bias for each tree node
from the parallel alternatives. The EM-binarization
yields best translation performance.
The rest of the paper is organized as follows.
Section 2 describes related research. Section 3 de-
fines the concepts necessary for describing the bina-
rizations methods. Section 4 describes the tree bina-
rization methods in details. Section 5 describes the
forest-based rule extraction algorithm, and section 6
explains how we restructure the trees using the EM
algorithm. The last two sections are for experiments
and conclusions.
2 Related Research
Several researchers (Melamed et al, 2004; Zhang
et al, 2006) have already proposed methods for bi-
narizing synchronous grammars in the context of
machine translation. Grammar binarization usually
maintains an equivalence to the original grammar
such that binarized grammars generate the same lan-
guage and assign the same probability to each string
as the original grammar does. Grammar binarization
is often employed to make the grammar fit in a CKY
parser. In our work, we are focused on binarization
of parse trees. Tree binarization generalizes the re-
sulting grammar and changes its probability distri-
bution. In tree binarization, synchronous grammars
built from restructured (binarized) training trees still
contain non-binary, multi-level rules and thus still
require the binarization transformation so as to be
employed by a CKY parser.
The translation model we are using in this paper
belongs to the xRs formalism (Knight and Graehl,
2004), which has been proved successful for ma-
chine translation in (Galley et al, 2004; Galley et
al., 2006; Marcu et al, 2006).
3 Concepts
We focus on tree-to-string (in noisy-channel model
sense) translation models. Translation models of
this type are typically trained on tuples of a source-
language sentence f, a target language (e.g., English)
parse tree pi that yields e and translates from f, and
the word alignments a between e and f. Such a tuple
is called an alignment graph in (Galley et al, 2004).
The graph (1) in Figure 2 is such an alignment graph.
747
(1) unbinarized tree
  
NPB
viktor chernomyrdin
VIKTOR?CHERNOMYRDIN
NNP1    NNP2   NNP3   NNP4*
(2) left-binarization (3) right-/head-binarization
NPB
NPB
NNP1 NNP2 NNP3
viktor
NNP?4
chernomyrdin
NPB
NNP1 NPB?
NNP2 NNP3
viktor
NNP4?
chernomyrdin
(4) left-binarization (5) right-binarization (6) left-binarization (7) right-/head-binarization
NPB
NPB
NPB
NNP1 NNP2
NNP3
viktor
NNP4?
chernomyrdin
- -
NPB
NNP1 NPB?
NNP2 NPB?
NNP3
viktor
NNP4?
chernomyrdin
Figure 2: Left, right, and head binarizations. Heads are marked with ??s. New nonterminals introduced by binarization are
denoted by X-bars.
A tree node in pi is admissible if the f string cov-
ered by the node is contiguous but not empty, and
if the f string does not align to any e string that is
not covered by pi. An xRs rule can be extracted only
from an admissible tree node, so that we do not have
to deal with dis-contiguous f spans in decoding (or
synchronous parsing). For example, in tree (2) in
Figure 2, node NPB is not admissible because the
f string that the node covers also aligns to NNP4,
which is not covered by the NPB. Node NPB in tree
(3), on the other hand, is admissible.
A set of sibling tree nodes is called factorizable
if we can form an admissible new node dominating
them. For example, in tree (1) of Figure 2, sibling
nodes NNP2 NNP3 and NNP4 are factorizable be-
cause we can factorize them out and form a new
node NPB, resulting in tree (3). Sibling tree nodes
NNP1 NNP2 and NNP3 are not factorizable. In syn-
chronous parse trees, not all sibling nodes are fac-
torizable, thus not all sub-phrases can be acquired
and syntactified. The main purpose of our paper is
to restructure parse trees by factorization such that
syntactified sub-phrases can be employed in transla-
tion.
4 Binarizing Syntax Trees
We are going to binarize a tree node n that domi-
nates r children n1, ..., nr. Restructuring will be
performed by introducing new tree nodes to domi-
nate a subset of the children nodes. To avoid over-
generalization, we allow ourselves to form only one
new node at a time. For example, in Figure 2, we
can binarize tree (1) into tree (2), but we are not
allowed to form two new nodes, one dominating
NNP1 NNP2 and the other dominating NNP3 NNP4.
Since labeling is not the concern of this paper, we re-
label the newly formed nodes as n.
4.1 Simple binarization methods
The left binarization of node n (i.e., the NPB in
tree (1) of Figure 2) factorizes the leftmost r ? 1
children by forming a new node n (i.e., NPB in
tree (2)) to dominate them, leaving the last child
nr untouched; and then makes the new node n the
left child of n. The method then recursively left-
binarizes the newly formed node n until two leaves
are reached. In Figure 2, we left-binarize tree (1)
into (2) and then into (4).
The right binarization of node n factorizes the
rightmost r ? 1 children by forming a new node n
(i.e., NPB in tree (3)) to dominate them, leaving the
748
first child n1 untouched; and then makes the new
node n the right child of n. The method then recur-
sively right-binarizes the newly formed node n. In
Figure 2, we right-binarize tree (1) into (3) and then
into (7).
The head binarization of node n left-binarizes
n if the head is the first child; otherwise, right-
binarizes n. We prefer right-binarization to left-
binarization when both are applicable under the head
restriction because our initial motivation was to gen-
eralize the NPB-rooted translation rules. As we will
show in the experiments, binarization of other types
of phrases contribute to the translation accuracy im-
provement as well.
Any of these simple binarization methods is easy
to implement, but is incapable of giving us all the
factorizable sub-phrases. Binarizing all the way to
the left, for example, from tree (1) to tree (2) and to
tree (4) in Figure 2, does not enable us to acquire a
substructure that yields NNP3 NNP4 and their trans-
lational equivalences. To obtain more factorizable
sub-phrases, we need to parallel-binarize in both di-
rections.
4.2 Parallel binarization
Simple binarizations transform a parse tree into an-
other single parse tree. Parallel binarization will
transform a parse tree into a binarization forest,
desirably packed to enable dynamic programming
when extracting translation rules from it.
Borrowing terms from parsing semirings (Good-
man, 1999), a packed forest is composed of addi-
tive forest nodes (?-nodes) and multiplicative forest
nodes (?-nodes). In the binarization forest, a ?-
node corresponds to a tree node in the unbinarized
tree; and this ?-node composes several ?-nodes,
forming a one-level substructure that is observed in
the unbinarized tree. A ?-node corresponds to al-
ternative ways of binarizing the same tree node in
the unbinarized tree and it contains one or more ?-
nodes. The same ?-node can appear in more than
one place in the packed forest, enabling sharing.
Figure 3 shows a packed forest obtained by pack-
ing trees (4) and (7) in Figure 2 via the following
parallel binarization algorithm.
To parallel-binarize a tree node n that has children
n1, ..., nr , we employ the following steps:
?1(NPB)
?2(NPB)
?3(NPB)
?4(NPB)
?5(NPB)
?6(NPB)
?7(NNP1) ?8(NNP2)
?9(NNP3)
?10(NNP4)
?11(NPB)
?7(NNP1) ?12(NPB)
?13(NPB)
?8(NNP2) ?14(NPB)
?15(NPB)
?9(NNP3) ?10(NNP4)
Figure 3: Packed forest obtained by packing trees (4) and (7)
in Figure 2
? We recursively parallel-binarize children nodes
n1, ..., nr, producing binarization ?-nodes
?(n1), ..., ?(nr), respectively.
? We right-binarize n, if any contiguous1 subset
of children n2, ..., nr is factorizable, by intro-
ducing an intermediate tree node labeled as n.
We recursively parallel-binarize n to generate
a binarization forest node ?(n). We form a
multiplicative forest node ?R as the parent of
?(n1) and ?(n).
? We left-binarize n if any contiguous subset
of n1, ..., nr?1 is factorizable and if this sub-
set contains n1. Similar to the above right-
binarization, we introduce an intermediate tree
node labeled as n, recursively parallel-binarize
n to generate a binarization forest node ?(n),
form a multiplicative forest node ?L as the par-
ent of ?(n) and ?(n1).
? We form an additive node ?(n) as the parent
of the two already formed multiplicative nodes
?L and ?R.
The (left and right) binarization conditions con-
sider any subset to enable the factorization of small
constituents. For example, in tree (1) of Figure 2,
although NNP1 NNP2 NNP3 of NPB are not factor-
izable, the subset NNP1 NNP2 is factorizable. The
binarization from tree (1) to tree (2) serves as a re-
laying step for us to factorize NNP1 NNP2 in tree
(4). The left-binarization condition is stricter than
1We factorize only subsets that cover contiguous spans to
avoid introducing dis-contiguous constituents for practical pur-
pose. In principle, the algorithm works fine without this bina-
rization condition.
749
the right-binarization condition to avoid spurious bi-
narization; i.e., to avoid the same subconstituent be-
ing reached via both binarizations. We could trans-
form tree (1) directly into tree (4) without bother-
ing to generate tree (3). However, skipping tree (3)
will create us difficulty in applying the EM algo-
rithm to choose a better binarization for each tree
node, since tree (4) can neither be classified as left
binarization nor as right binarization of the original
tree (1) ? it is the result of the composition of two
left-binarizations.
In parallel binarization, nodes are not always bi-
narizable in both directions. For example, we do not
need to right-binarize tree (2) because NNP2 NNP3
are not factorizable, and thus cannot be used to form
sub-phrases. It is still possible to right-binarize tree
(2) without affecting the correctness of the parallel
binarization algorithm, but that will spuriously in-
crease the branching factor of the search for the rule
extraction, because we will have to expand more tree
nodes.
A restricted version of parallel binarization is the
headed parallel binarization, where both the left and
the right binarization must respect the head propaga-
tion property at the same time.
A nice property of parallel binarization is that
for any factorizable substructure in the unbinarized
tree, we can always find a corresponding admissi-
ble ?-node in the parallel-binarized packed forest.
A leftmost substructure like the lowest NPB-subtree
in tree (4) of Figure 2 can be made factorizable
by several successive left binarizations, resulting in
?5(NPB)-node in the packed forest in Figure 3. A
substructure in the middle can be factorized by the
composition of several left- and right-binarizations.
Therefore, after a tree is parallel-binarized, to make
the sub-phrases available to the MT system, all we
need to do is to extract rules from the admissible
nodes in the packed forest. Rules that can be ex-
tracted from the original unrestructured tree can be
extracted from the packed forest as well.
Parallel binarization results in parse forests. Thus
translation rules need to be extracted from training
data consisting of (e-forest, f, a)-tuples.
5 Extracting translation rules from
(e-forest, f, a)-tuples
The algorithm to extract rules from (e-forest, f, a)-
tuples is a natural generalization of the (e-parse, f,
a)-based rule extraction algorithm in (Galley et al,
2006). The input to the forest-based algorithm is a
(e-forest, f, a)-triple. The output of the algorithm is
a derivation forest (Galley et al, 2006) composed of
xRs rules. The algorithm recursively traverses the e-
forest top-down and extracts rules only at admissible
forest nodes.
The following procedure transforms the packed e-
forest in Figure 3 into a packed synchronous deriva-
tion in Figure 4.
Condition 1: Suppose we reach an additive
e-forest node, e.g. ?1(NPB) in Figure 3. For
each of ?1(NPB)?s children, e-forest nodes
?2(NPB) and ?11(NPB), we go to condi-
tion 2 to recursively extract rules on these
two e-forest nodes, generating multiplicative
derivation forest nodes, i.e., ?(NPB(NPB :
x0 NNP3(viktor) NNP4(chernomyrdin)4) ?
x0 V-C) and ?(NPB(NNP1 NPB(NNP2 : x0 NPB :
x1)) ? x0 x1 x2) in Figure 4. We make these
new ? nodes children of ?(NPB) in the derivation
forest.
Condition 2: Suppose we reach a multiplicative
parse forest node, i.e., ?11(NPB) in Figure 3. We
extract rules rooted at it using the procedure in
(Galley et al, 2006), forming multiplicative deriva-
tion forest nodes, i.e., ?(NPB(NNP1 NPB(NNP2 :
x0 NPB : x1)) ? x0 x1 x2) We then go
to condition 1 to form the derivation forest on
the additive frontier e-forest nodes of the newly
extracted rules, generating additive derivation for-
est nodes, i.e., ?(NNP1), ?(NNP2) and ?(NPB).
We make these ? nodes the children of node
?(NPB(NNP1 NPB(NNP2 : x0 NPB : x1)) ?
x0 x1 x2) in the derivation forest.
This algorithm is a natural extension of the extrac-
tion algorithm in (Galley et al, 2006) in the sense
that we have an extra condition (1) to relay rule ex-
traction on additive e-forest nodes.
It is worthwhile to eliminate the spuriously am-
biguous rules that are introduced by the parallel bi-
750
?(NPB)
?
(
NPB(NPB : x0 NNP(viktor) NNP(chernomyrdin)) ? x0 V-C
)
?(NPB)
?
(
NPB(NNP : x0 NNP : x1 ? x0 x1)
)
?
(
NPB(NNP : x0 NPB(NNP : x1 NPB : x2)) ? x0 x1 x2
)
?(NNP) ?(NNP) ?(NPB)
?
(
NPB(NNP(viktor) NNP(chernomyrdin)) ? V-C)
Figure 4: Derivation forest.
narization. For example, we may extract the follow-
ing two rules:
- A(A(B:x0 C:x1)D:x2) ? x1 x0 x2
- A(B:x0 A(C:x1 D:x2)) ? x1 x0 x2
These two rules, however, are not really distinct.
They both converge to the following rules if we
delete the auxiliary nodes A.
- A(B:x0 C:x1 D:x2) ? x1 x0 x2
The forest-base rule extraction algorithm pro-
duces much larger grammars than the tree-based
one, making it difficult to scale to very large training
data. From a 50M-word Chinese-to-English parallel
corpus, we can extract more than 300 million trans-
lation rules, while the tree-based rule extraction al-
gorithm gives approximately 100 million. However,
the restructured trees from the simple binarization
methods are not guaranteed to give the best trees for
syntax-based machine translation. What we desire is
a binarization method that still produces single parse
trees, but is able to mix left binarization and right
binarization in the same tree. In the following, we
shall use the EM algorithm to learn the desirable bi-
narization on the forest of binarization alternatives
proposed by the parallel binarization algorithm.
6 Learning how to binarize via the EM
algorithm
The basic idea of applying the EM algorithm to
choose a restructuring is as follows. We perform a
set {?} of binarization operations on a parse tree ? .
Each binarization ? is the sequence of binarizations
on the necessary (i.e., factorizable) nodes in ? in pre-
order. Each binarization ? results in a restructured
tree ?? . We extract rules from (?? , f, a), generating a
translation model consisting of parameters (i.e., rule
e?parse
  (Galley et al, 2006)
composed rule extraction
1
2
parallel binarization e?forest
forest?based rule extraction
         of minimal rules
f,a
synchronous derivation forests
EM
3
4
viterbi derivationsproject e?parse
model
syntax translation 
Figure 5: Using the EM algorithm to choose restructuring.
probabilities) ?. Our aim is to obtain the binarization
?? that gives the best likelihood of the restructured
training data consisting of (?? , f , a)-tuples. That is
?? = arg max
?
p(??, f ,a|??) (1)
In practice, we cannot enumerate all the exponen-
tial number of binarized trees for a given e-parse.
We therefore use the packed forest to store all the
binarizations that operate on an e-parse in a com-
pact way, and then use the inside-outside algorithm
(Lari and Young, 1990; Knight and Graehl, 2004)
for model estimation.
The probability p(??, f ,a) of a (?? , f, a)-tuple
is what the basic syntax-based translation model is
concerned with. It can be further computed by ag-
gregating the rule probabilities p(r) in each deriva-
tion ? in the set of all derivations ? (Galley et al,
2004; Marcu et al, 2006). That is
p(??, f ,a) =
?
???
?
r??
p(r) (2)
Since it has been well-known that applying EM
with tree fragments of different sizes causes over-
fitting (Johnson, 1998), and since it is also known
that syntax MT models with larger composed rules
in the mix significantly outperform rules that min-
imally explain the training data (minimal rules) in
751
translation accuracy (Galley et al, 2006), we decom-
pose p(?b, f ,a) using minimal rules during running
of the EM algorithm, but, after the EM restructuring
is finished, we build the final translation model using
composed rules for evaluation.
Figure 5 is the actual pipeline that we use for
EM binarization. We first generate a packed e-forest
via parallel binarization. We then extract minimal
translation rules from the (e-forest, f, a)-tuples, pro-
ducing synchronous derivation forests. We run the
inside-outside algorithm on the derivation forests
until convergence. We obtain the Viterbi derivations
and project the English parses from the derivations.
Finally, we extract composed rules using Galley et
al. (2006)?s (e-tree, f, a)-based rule extraction algo-
rithm. This procedure corresponds to the path 13?42
in the pipeline.
7 Experiments
We carried out a series of experiments to compare
the performance of different binarization methods
in terms of BLEU on Chinese-to-English translation
tasks.
7.1 Experimental setup
Our bitext consists of 16M words, all in the
mainland-news domain. Our development set is a
925-line subset of the 993-line NIST02 evaluation
set. We removed long sentences from the NIST02
evaluation set to speed up discriminative training.
The test set is the full 919-line NIST03 evaluation
set.
We used a bottom-up, CKY-style decoder that
works with binary xRs rules obtained via a syn-
chronous binarization procedure (Zhang et al,
2006). The decoder prunes hypotheses using strate-
gies described in (Chiang, 2007).
The parse trees on the English side of the bitexts
were generated using a parser (Soricut, 2004) imple-
menting the Collins parsing models (Collins, 1997).
We used the EM procedure described in (Knight
and Graehl, 2004) to perform the inside-outside al-
gorithm on synchronous derivation forests and to
generate the Viterbi derivation forest.
We used the rule extractor described in (Galley et
al., 2006) to extract rules from (e-parse, f, a)-tuples,
but we made an important modification: new nodes
introduced by binarization will not be counted when
computing the rule size limit unless they appear as
the rule roots. The motivation is that binarization
deepens the parses and increases the number of tree
nodes. In (Galley et al, 2006), a composed rule
is extracted only if the number of internal nodes it
contains does not exceed a limit (i.e., 4), similar
to the phrase length limit in phrase-based systems.
This means that rules extracted from the restructured
trees will be smaller than those from the unrestruc-
tured trees, if the X nodes are deleted. As shown in
(Galley et al, 2006), smaller rules lose context, and
thus give lower translation performance. Ignoring X
nodes when computing the rule sizes preserves the
unstructured rules in the resulting translation model
and adds substructures as bonuses.
7.2 Experiment results
Table 1 shows the BLEU scores of mixed-cased and
detokenized translations of different systems. We
see that all the binarization methods improve the
baseline system that does not apply any binarization
algorithm. The EM-binarization performs the best
among all the restructuring methods, leading to 1.0
BLEU point improvement. We also computed the
bootstrap p-values (Riezler and Maxwell, 2005) for
the pairwise BLEU comparison between the base-
line system and any of the system trained from bina-
rized trees. The significance test shows that the EM
binarization result is statistically significant better
than the baseline system (p > 0.005), even though
the baseline is already quite strong. To our best
knowledge, 37.94 is the highest BLEU score on this
test set to date.
Also as shown in Table 1, the grammars trained
from the binarized training trees are almost two
times of the grammar size with no binarization. The
extra rules are substructures factored out by these bi-
narization methods.
How many more substructures (or translation
rules) can be acquired is partially determined by
how many more admissible nodes each binariza-
tion method can factorize, since rules are extractable
only from admissible tree nodes. According to
Table 1, binarization methods significantly increase
the number of admissible nodes in the training trees.
The EM binarization makes available the largest
752
EXPERIMENT NIST03-BLEU # RULES # ADMISSIBLE NODES IN TRAINING
no-bin 36.94 63.4M 7,995,569
left binarization 37.47 (p = 0.047) 114.0M 10,463,148
right binarization 37.49 (p = 0.044) 113.0M 10,413,194
head binarization 37.54 (p = 0.086) 113.8M 10,534,339
EM binarization 37.94 (p = 0.0047) 115.6M 10,658,859
Table 1: Translation performance, grammar size and # admissible nodes versus binarization algorithms. BLEU scores are for
mixed-cased and detokenized translations, as we usually do for NIST MT evaluations.
nonterminal left-binarization right-binarization
NP 96.97% 3.03%
NP-C 97.49% 2.51%
NPB 0.25% 99.75%
VP 93.90% 6.10%
PP 83.75% 16.25%
ADJP 87.83% 12.17%
ADVP 82.74% 17.26%
S 85.91% 14.09%
S-C 18.88% 81.12%
SBAR 96.69% 3.31%
QP 86.40% 13.60%
PRN 85.18% 14.82%
WHNP 97.93% 2.07%
NX 100% 0
SINV 87.78% 12.22%
PRT 100% 0
SQ 93.53% 6.47%
CONJP 18.08% 81.92%
Table 2: Binarization bias learned by EM.
number of admissible nodes, and thus results in the
most rules.
The EM binarization factorizes more admissible
nodes because it mixes both left and right binariza-
tions in the same tree. We computed the binarization
biases learned by the EM algorithm for each nonter-
minal from the binarization forest of headed-parallel
binarizations of the training trees, getting the statis-
tics in Table 2. Of course, the binarization bias
chosen by left-/right-binarization methods would be
100% deterministic. One noticeable message from
Table 2 is that most of the categories are actually bi-
ased toward left-binarization, although our motivat-
ing example in our introduction section is for NPB,
which needed right binarization. The main reason
might be that the head sub-constituents of most cat-
egories tend to be on the left, but according to the
performance comparison between head binarization
and EM binarization, head binarization does not suf-
fice because we still need to choose the binarization
between left and right if they both are head binariza-
tions.
8 Conclusions
In this paper, we not only studied the impact of
simple tree binarization algorithms on the perfor-
mance of end-to-end syntax-based MT, but also pro-
posed binarization methods that mix more than one
simple binarization in the binarization of the same
parse tree. Binarizing a tree node whether to the left
or to the right was learned by employing the EM
algorithm on a set of alternative binarizations and
by choosing the Viterbi one. The EM binarization
method is informed by word alignments such that
unnecessary new tree nodes will not be ?blindly? in-
troduced.
To our best knowledge, our research is the first
work that aims to generalize a syntax-based trans-
lation model by restructuring and achieves signifi-
cant improvement on a strong baseline. Our work
differs from traditional work on binarization of syn-
chronous grammars in that we are not concerned
with the equivalence of the binarized grammar to the
original grammar, but intend to generalize the orig-
inal grammar via restructuring of the training parse
trees to improve translation performance.
Acknowledgments
The authors would like to thank David Chiang,
Bryant Huang, and the anonymous reviewers for
their valuable feedbacks.
References
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Seattle, May.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
753
35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 16?23, Madrid, Spain,
July.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 205?208, Sapporo,
July.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a Translation Rule? In Proceedings of
the Human Language Technology Conference and the
North American Association for Computational Lin-
guistics (HLT-NAACL), Boston, Massachusetts.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference and
Training of Context-Rich Syntactic Models. In Pro-
ceedings of the 44th Annual Meeting of the Association
for Computational Linguistics (ACL).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
M. Johnson. 1998. The DOP estimation method is
biased and inconsistent. Computational Linguistics,
28(1):71?76.
K. Knight and J. Graehl. 2004. Training Tree Transduc-
ers. In Proceedings of NAACL-HLT.
K. Lari and S. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech and Language, pages 35?56.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phraases.
In Proceedings of EMNLP-2006, pp. 44-52, Sydney,
Australia.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proc. ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summa-
rization.
Radu Soricut. 2004. A reimplementation of Collins?s
parsing models. Technical report, Information Sci-
ences Institute, Department of Computer Science Uni-
versity of Southern California.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the HLT-NAACL.
754
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 1?8,
New York, June 2006. c?2006 Association for Computational Linguistics
Capitalizing Machine Translation
Wei Wang and Kevin Knight and Daniel Marcu
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA, 90292
{wwang, kknight, dmarcu}@languageweaver.com
Abstract
We present a probabilistic bilingual capi-
talization model for capitalizing machine
translation outputs using conditional ran-
dom fields. Experiments carried out on
three language pairs and a variety of ex-
periment conditions show that our model
significantly outperforms a strong mono-
lingual capitalization model baseline, es-
pecially when working with small datasets
and/or European language pairs.
1 Introduction
Capitalization is the process of recovering case in-
formation for texts in lowercase. It is also called
truecasing (Lita et al, 2003). Usually, capitalization
itself tries to improve the legibility of texts. It, how-
ever, can affect the word choice or order when inter-
acting with other models. In natural language pro-
cessing, a good capitalization model has been shown
useful for tasks like name entity recognition, auto-
matic content extraction, speech recognition, mod-
ern word processors, and machine translation (MT).
Capitalization can be viewed as a sequence la-
beling process. The input to this process is a sen-
tence in lowercase. For each lowercased word in
the input sentence, we have several available cap-
italization tags: initial capital (IU), all uppercase
(AU), all lowercase (AL), mixed case (MX), and
all having no case (AN). The output of capital-
ization is a capitalization tag sequence. Associ-
ating a tag in the output with the corresponding
lowercased word in the input results in a surface
form of the word. For example, we can tag the
input sentence ?click ok to save your changes to
/home/doc.? into ?click IU ok AU to AL save AL
your AL changes AL to AL /home/doc MX . AN?,
getting the surface form ?Click OK to save your
changes to /home/DOC .?.
A capitalizer is a tagger that recovers the capi-
talization tag for each input lowercased word, out-
putting a well-capitalized sentence. Since each low-
ercased word can have more than one tag, and as-
sociating a tag with a lowercased word can result
in more than one surface form (e.g., /home/doc MX
can be either /home/DOC or /home/Doc), we need a
capitalization model to solve the capitalization am-
biguities. For example, Lita et al (2003) use a tri-
gram language model estimated from a corpus with
case information; Chelba and Acero (2004) use a
maximum entropy Markov model (MEMM) com-
bining features involving words and their cases.
Capitalization models presented in most previ-
ous approaches are monolingual because the models
are estimated only from monolingual texts. How-
ever, for capitalizing machine translation outputs,
using only monolingual capitalization models is not
enough. For example, if the sentence ?click ok to
save your changes to /home/doc .? in the above
example is the translation of the French sentence
?CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFI-
CATIONS DANS /HOME/DOC .?, the correct capitaliza-
tion result should probably be ?CLICK OK TO SAVE
YOUR CHANGES TO /HOME/DOC .?, where all words
are in all upper-case. Without looking into the case
1
of the MT input, we can hardly get the correct capi-
talization result.
Although monolingual capitalization models in
previous work can apply to MT output, a bilingual
model is more desirable. This is because MT out-
puts usually strongly preserve case from the input,
and because monolingual capitalization models do
not always perform as well on badly translated text
as on well-formed syntactic texts.
In this paper, we present a bilingual capitalization
model for capitalizing machine translation outputs
using conditional random fields (CRFs) (Lafferty et
al., 2001). This model exploits case information
from both the input sentence (source) and the out-
put sentence (target) of the MT system. We define a
series of feature functions to incorporate capitaliza-
tion knowledge into the model.
Experimental results are shown in terms of BLEU
scores of a phrase-based SMT system with the cap-
italization model incorporated, and in terms of cap-
italization precision. Experiments are performed
on both French and English targeted MT systems
with large-scale training data. Our experimental re-
sults show that the CRF-based bilingual capitaliza-
tion model performs better than a strong baseline
capitalizer that uses a trigram language model.
2 Related Work
A simple capitalizer is the 1-gram tagger: the case of
a word is always the most frequent one observed in
training data, with the exception that the sentence-
initial word is always capitalized. A 1-gram capital-
izer is usually used as a baseline for capitalization
experiments (Lita et al, 2003; Kim and Woodland,
2004; Chelba and Acero, 2004).
Lita et al (2003) view capitalization as a lexi-
cal ambiguity resolution problem, where the lexi-
cal choices for each lowercased word happen to be
its different surface forms. For a lowercased sen-
tence e, a trigram language model is used to find the
best capitalization tag sequence T that maximizes
p(T, e) = p(E), resulting in a case-sensitive sen-
tence E. Besides local trigrams, sentence-level
contexts like sentence-initial position are employed
as well.
Chelba and Acero (2004) frame capitalization as
a sequence labeling problem, where, for each low-
MT Decoder
Train Monolingual
Capitalization Model
Monolingual Cap Model Capitalization
Lower Case
Lower Case
f
Lower Case
e
Finput
Eoutput
Train
Translation Model
Train
Language Model
Translation
Model
Languagel
Model
{F}
{E}
{f}
{e}
Figure 1: The monolingual capitalization scheme employed
by most statistical MT systems.
ercased sentence e, they find the label sequence T
that maximizes p(T |e). They use a maximum en-
tropy Markov model (MEMM) to combine features
of words, cases and context (i.e., tag transitions).
Gale et al (1994) report good results on capital-
izing 100 words. Mikheev (1999) performs capital-
ization using simple positional heuristics.
3 Monolingual Capitalization Scheme
Translation and capitalization are usually performed
in two successive steps because removing case infor-
mation from the training of translation models sub-
stantially reduces both the source and target vocabu-
lary sizes. Smaller vocabularies lead to a smaller
translation model with fewer parameters to learn.
For example, if we do not remove the case informa-
tion, we will have to deal with at least nine prob-
abilities for the English-French word pair (click,
cliquez). This is because either ?click? or ?cliquez?
can have at least three tags (IU, AL, AU), and thus
three surface forms. A smaller translation model re-
quires less training data, and can be estimated more
accurately than otherwise from the same amount
of training data. A smaller translation model also
means less memory usage.
Most statistical MT systems employ the monolin-
gual capitalization scheme as shown in Figure 1. In
this scheme, the translation model and the target lan-
guage model are trained from the lowercased cor-
pora. The capitalization model is trained from the
case-sensitive target corpus. In decoding, we first
turn input into lowercase, then use the decoder to
generate the lowercased translation, and finally ap-
2
HYDRAULIC HEADER TILT CYLINDER KIT
Kit de ve?rin d?inclinaison hydraulique de la plate-forme
haut-parleur avant droit +
HAUT-PARLEUR AVANT DROIT +
Seat Controls, Standard
COMMANDES DU SIGE, STANDARD
loading a saved legend
Chargement d?une le?gende sauvegarde
Table 1: Errors made by monolingual capitalization model.
Each row contains a pair of MT input and MT output.
MT Decoder
CapitalizationBilingual
Cap Model
Train Bilingual
    Cap Model
alignment
 Word/Phrase Aligner
f
Lower Case
e
Finput
Eoutput
{F}
{E}
Figure 2: A bilingual capitalization scheme.
ply the capitalization model to recover the case of
the decoding output.
The monolingual capitalization scheme makes
many errors as shown in Table 1. Each cell in
the table contains the MT-input and the MT-output.
These errors are due to the capitalizer does not have
access to the source sentence.
Regardless, estimating mixed-cased translation
models, however, is a very interesting topic and
worth future study.
4 Bilingual Capitalization Model
4.1 The Model
Our probabilistic bilingual capitalization model ex-
ploits case information from both the input sentence
to the MT system and the output sentence from the
system (see Figure 2). An MT system translates a
capitalized sentence F into a lowercased sentence e.
A statistical MT system can also provide the align-
ment A between the input F and the output e; for
example, a statistical phrase-based MT system could
provide the phrase boundaries in F and e, and also
the alignment between the phrases.1
1We shall explain our capitalization model within the
phrase-based SMT framework, the model, however, could be
OK
Click OK
Cliquez
E
F
E?i
F?j
Figure 3: Alignment graph. Brackets mean phrase bound-
aries.
The bilingual capitalization algorithm recovers
the capitalized sentence E from e, according to the
input sentence F , and the alignment A. Formally,
we look for the best capitalized sentence E? such
that
E? = arg maxE?GEN(e)p(E|F,A) (1)
where GEN(e) is a function returning the set of
possible capitalized sentences consistent with e. No-
tice that e does not appear in p(E|F,A) because we
can uniquely obtain e from E. p(E|F,A) is the cap-
italization model of concern in this paper.2
To further decompose the capitalization model
p(E|F,A), we make some assumptions. As shown
in Figure 3, input sentence F , capitalized output E,
and their alignment can be viewed as a graph. Ver-
tices of the graph correspond to words in F and
E. An edge connecting a word in F and a word
in E corresponds to a word alignment. An edge
between two words in E represents the dependency
between them captured by monolingual n-gram lan-
guage models. We also assume that both E and
F have phrase boundaries available (denoted by the
square brackets), and that A is the phrase alignment.
In Figure 3, F?j is the j-th phrase of F , E?i is the i-th
phrase of E, and they align to each other. We do not
require a word alignment; instead we find it reason-
able to think that a word in E?i can be aligned to any
adapted to syntax-based machine translation, too. To this end,
the translational correspondence is described within a transla-
tion rule, i.e., (Galley et al, 2004) (or a synchronous produc-
tion), rather than a translational phrase pair; and the training
data will be derivation forests, instead of the phrase-aligned
bilingual corpus.
2The capitalization model p(E|F, A) itself does not require
the existence of e. This means that in principle this model can
also be viewed as a capitalized translation model that performs
translation and capitalization in an integrated step. In our paper,
however, we consider the case where the machine translation
output e is given, which is reflected by the the fact that GEN(e)
takes e as input in Formula 1.
3
word in F?j . A probabilistic model defined on this
graph is a Conditional Random Field. Therefore,
it is natural to formulate the bilingual capitalization
model using CRFs:3
p?(E|F, A) =
1
Z(F, A, ?)
exp
 I
X
i=1
?ifi(E, F, A)
!
(2)
where
Z(F, A, ?) =
X
E?GEN(e)
exp
 I
X
i=1
?ifi(E,F, A)
!
(3)
fi(E,F,A), i = 1...I are the I features, and
? = (?1, ..., ?I) is the feature weight vector. Based
on this capitalization model, the decoder in the cap-
italizer looks for the best E? such that
E? = arg maxE?GEN(e,F )
I
?
i=1
?ifi(E,F,A) (4)
4.2 Parameter Estimation
Following Roark et al (2004), Lafferty et al (2001)
and Chen and Rosenfeld (1999), we are looking for
the set of feature weights ? maximizing the regu-
larized log-likelihood LLR(?) of the training data
{E(n), F (n), A(n), n = 1, ..., N}.
LLR(?) =
N
X
n=1
log p
?
E(n)|F (n), A(n)
?
? ||?||
2
2?2 (5)
The second term at the right-hand side of For-
mula 5 is a zero-mean Gaussian prior on the pa-
rameters. ? is the variance of the Gaussian prior
dictating the cost of feature weights moving away
from the mean ? a smaller value of ? keeps feature
weights closer to the mean. ? can be determined
by linear search on development data.4 The use of
the Gaussian prior term in the objective function has
been found effective in avoiding overfitting, leading
to consistently better results. The choice of LLR as
an objective function can be justified as maximum
a-posteriori (MAP) training within a Bayesian ap-
proach (Roark et al, 2004).
3We chose CRFs over other sequence labeling models (i.e.
MEMM) because CRFs have no label bias and we do not need
to compute the partition function during decoding.
4In our experiment, we use an empirical value ? = 0.5 as in
(Roark et al, 2004).
4.3 Feature Functions
We define features based on the alignment graph
in Figure 3. Each feature function is defined on a
word.
Monolingual language model feature. The
monolingual LM feature of word Ei is the loga-
rithm of the probability of the n-gram ending at
Ei:
fLM(Ei, F,A) = log p(Ei|Ei?1, ..., Ei?n+1) (6)
p should be appropriately smoothed such that it
never returns zero.
Capitalized translation model feature. Sup-
pose E phrase ?Click OK? is aligned to F
phrase ?Cliquez OK?. The capitalized transla-
tion model feature of ?Click? is computed as
log p(Click|Cliquez)+log p(Click|OK). ?Click? is
assumed to be aligned to any word in the F phrase.
The larger the probability that ?Click? is translated
from an F word, i.e., ?Cliquez?, the more chances
that ?Click? preserves the case of ?Cliquez?. For-
mally, for word Ei, and an aligned phrase pair E?l
and F?m, where Ei ? E?l, the capitalized translation
model feature of Ei is
fcap?t1(Ei, F,A) = log
|F?m|
?
k=1
p(Ei|F?m,k) (7)
p(Ei|F?m,k) is the capitalized translation table. It
needs smoothing to avoid returning zero, and is esti-
mated from a word-aligned bilingual corpus.
Capitalization tag translation feature. The fea-
ture value of E word ?Click? aligning to F phrase
?Cliquez OK? is log p(IU|IU)p(click|cliquez) +
log p(IU|AU)p(click|ok). We see that this feature
is less specific than the capitalized translation model
feature. It is computed in terms of the tag transla-
tion probability and the lowercased word translation
probability. The lowercased word translation proba-
bility, i.e., p(click|ok), is used to decide how much
of the tag translation probability, i.e., p(IU|AU),
will contribute to the final decision. The smaller the
word translation probability, i.e., p(click|ok), is, the
smaller the chance that the surface form of ?click?
4
preserves case from that of ?ok?. Formally, this fea-
ture is defined as
fcap?tag?t1(Ei, F,A) =
log
|f?m|
?
k=1
p(ei|f?m,k) ? p(?(Ei)|?(F?m,k)) (8)
p(ei|f?m,k) is the t-table over lowercased word pairs,
which is the usual ?t-table? in a SMT system.
p(?(Ei)|?(F?m,k)) is the probability of a target cap-
italization tag given a source capitalization tag and
can be easily estimated from a word-aligned bilin-
gual corpus. This feature attempts to help when
fcap?t1 fails (i.e., the capitalized word pair is un-
seen). Smoothing is also applied to both p(ei|f?m,k)
and p(?(Ei)|?(F?m,k)) to handle unseen words (or
word pairs).
Upper-case translation feature. Word Ei is in
all upper case if all words in the corresponding F
phrase F?m are in upper case. Although this fea-
ture can also be captured by the capitalization tag
translation feature in the case where an AU tag in
the input sentence is most probably preserved in the
output sentence, we still define it to emphasize its
effect. This feature aims, for example, to translate
?ABC XYZ? into ?UUU VVV? even if all words are
unseen.
Initial capitalization feature. An E word is ini-
tially capitalized if it is the first word that contains
letters in the E sentence. For example, for sentence
?? Please click the button? that starts with a bul-
let, the initial capitalization feature value of word
?please? is 1 because ??? does not contain a letter.
Punctuation feature template. An E word is ini-
tially capitalized if it follows a punctuation mark.
Non-sentence-ending punctuation marks like com-
mas will usually get negative weights.
As one can see, our features are ?coarse-grained?
(e.g., the language model feature). In contrast, Kim
and Woodland (2004) and Roark et al (2004) use
?fine-grained? features. They treat each n-gram as
a feature for, respectively, monolingual capitaliza-
tion and language modeling. Feature weights tuned
at a fine granularity may lead to better accuracy,
but they require much more training data, and re-
sult in much slower training speed, especially for
large-scale learning problems. Coarse-grained fea-
tures enable us to efficiently get the feature values
from a very large training corpus, and quickly tune
the weights on small development sets. For exam-
ple, we can train a bilingual capitalization model on
a 70 million-word corpus in several hours with the
coarse-grained features presented above, but in sev-
eral days with fine-grained n-gram count features.
4.4 The GEN Function
Function GEN generates the set of case-sensitive
candidates from a lowercased token. For exam-
ple GEN(mt) = {mt, mT, Mt, MT}. The follow-
ing heuristics can be used to reduce the range of
GEN. The returned set of GEN on a lower-cased to-
ken w is the union of: (i) {w,AU(w), IU(w)}, (ii)
{v|v is seen in training data and AL(v) = w},
and (iii) {F?m,k|AL(F?m,k) = AL(w)}. The heuris-
tic (iii) is designed to provide more candidates for
w when it is translated from a very strange input
word F?m,k in the F phrase F?m that is aligned to the
phrase that w is in. This heuristic creates good capi-
talization candidates for the translation of URLs, file
names, and file paths.
5 Generating Phrase-Aligned Training
Data
Training the bilingual capitalization model requires
a bilingual corpus with phrase alignments, which are
usually produced from a phrase aligner. In practice,
the task of phrase alignment can be quite computa-
tionally expensive as it requires to translate the en-
tire training corpus; also a phrase aligner is not al-
ways available. We therefore generate the training
data using a na??ve phrase aligner (NPA) instead of
resorting to a real one.
The input to the NPA is a word-aligned bilingual
corpus. The NPA stochastically chooses for each
sentence pair one segmentation and phrase align-
ment that is consistent with the word alignment. An
aligned phrase pair is consistent with the word align-
ment if neither phrase contains any word aligning
to a word outside the other phrase (Och and Ney,
2004). The NPA chunks the source sentence into
phrases according to a probabilistic distribution over
source phrase lengths. This distribution can be ob-
tained from the trace output of a phrase-based MT
5
Entire Corpus (#W) Test-BLEU
Languages Training Dev Test-Prec. (#sents)
E?F (IT) 62M 13K 15K 763
F?E (news) 144M 11K 22K 241
C?E (news) 50M 8K 17K 919
Table 2: Corpora used in experiments.
decoder on a small development set. The NPA has
to retry if the current source phrase cannot find any
consistent target phrase. Unaligned target words are
attached to the left phrase. Heuristics are employed
to prevent the NPA from not coming to a solution.
Obviously, the NPA is a special case of the phrase
extractor in (Och and Ney, 2004) in that it considers
only one phrase alignment rather than all possible
ones.
Unlike a real phrase aligner, the NPA need not
wait for the training of the translation model to fin-
ish, making it possible for parallelization of transla-
tion model training and capitalization model train-
ing. However, we believe that a real phrase aligner
may make phrase alignment quality higher.
6 Experiments
6.1 Settings
We conducted capitalization experiments on three
language pairs: English-to-French (E?F) with a
bilingual corpus from the Information Technology
(IT) domain; French-to-English (F?E) with a bilin-
gual corpus from the general news domain; and
Chinese-to-English (C?E) with a bilingual corpus
from the general news domain as well. Each lan-
guage pair comes with a training corpus, a develop-
ment corpus and two test sets (see Table 2). Test-
Precision is used to test the capitalization precision
of the capitalizer on well-formed sentences drawn
from genres similar to those used for training. Test-
BLEU is used to assess the impact of our capitalizer
on end-to-end translation performance; in this case,
the capitalizer may operate on ungrammatical sen-
tences. We chose to work with these three language
pairs because we wanted to test our capitalization
model on both English and French target MT sys-
tems and in cases where the source language has no
case information (such as in Chinese).
We estimated the feature functions, such as the
log probabilities in the language model, from the
training set. Kneser-Ney smoothing (Kneser and
Ney, 1995) was applied to features fLM, fcap?t1,
and fcap?tag?t1. We trained the feature weights of
the CRF-based bilingual capitalization model using
the development set. Since estimation of the feature
weights requires the phrase alignment information,
we efficiently applied the NPA on the development
set.
We employed two LM-based capitalizers as base-
lines for performance comparison: a unigram-based
capitalizer and a strong trigram-based one. The
unigram-based capitalizer is the usual baseline for
capitalization experiments in previous work. The
trigram-based baseline is similar to the one in
(Lita et al, 2003) except that we used Kneser-Ney
smoothing instead of a mixture.
A phrase-based SMT system (Marcu and Wong,
2002) was trained on the bitext. The capitalizer
was incorporated into the MT system as a post-
processing module ? it capitalizes the lowercased
MT output. The phrase boundaries and alignments
needed by the capitalizer were automatically in-
ferred as part of the decoding process.
6.2 BLEU and Precision
We measured the impact of our capitalization model
in the context of an end-to-end MT system using
BLEU (Papineni et al, 2001). In this context, the
capitalizer operates on potentially ill-formed, MT-
produced outputs.
To this end, we first integrated our bilingual capi-
talizer into the phrase-based SMT system as a post-
processing module. The decoder of the MT sys-
tem was modified to provide the capitalizer with
the case-preserved source sentence, the lowercased
translation, and the phrase boundaries and their
alignments. Based on this information, our bilin-
gual capitalizer recovers the case information of the
lowercased translation, outputting a capitalized tar-
get sentence. The case-restored machine transla-
tions were evaluated against the target test-BLEU
set. For comparison, BLEU scores were also com-
puted for an MT system that used the two LM-based
baselines.
We also assessed the performance of our capital-
izer on the task of recovering case information for
well-formed grammatical texts. To this end, we used
the precision metric that counted the number of cor-
6
rectly capitalized words produced by our capitalizer
on well-formed, lowercased input
precision = #correctly capitalized words#total words (9)
To obtain the capitalization precision, we im-
plemented the capitalizer as a standalone program.
The inputs to the capitalizer were triples of a case-
preserved source sentence, a lowercased target sen-
tence, and phrase alignments between them. The
output was the case-restored version of the target
sentence. In this evaluation scenario, the capitalizer
output and the reference differ only in case infor-
mation ? word choices and word orders between
them are the same. Testing was conducted on Test-
Precision. We applied the NPA to the Test-Precision
set to obtain the phrases and their alignments be-
cause they were needed to trigger the features in
testing. We used a Test-Precision set that was dif-
ferent from the Test-BLEU set because word align-
ments were by-products only of training of transla-
tion models on the MT training data and we could
not put the Test-BLEU set into the MT training
data. Rather than implementing a standalone word
aligner, we randomly divided the MT training data
into three non-overlapping sets: Test-Precision set,
CRF capitalizer training set and dev set.
6.3 Results
The performance comparisons between our CRF-
based capitalizer and the two LM-based baselines
are shown in Table 3 and Table 4. Table 3 shows
the BLEU scores, and Table 4 shows the precision.
The BLEU upper bounds indicate the ceilings that a
perfect capitalizer can reach, and are computed by
ignoring the case information in both the capitalizer
outputs and the reference. Obviously, the precision
upper bounds for all language pairs are 100%.
The precision and end-to-end BLEU based com-
parisons show that, for European language pairs, the
CRF-based bilingual capitalization model outper-
forms significantly the strong LM-based baseline.
We got more than one BLEU point improvement on
the MT translation between English and French, a
34% relative reduction in capitalization error rate for
the French-to-English language pair, and a 42% rel-
ative error rate reduction for the English-to-French
language pair. These results show that source lan-
guage information provides significant help for cap-
italizing machine translation outputs. The results
also show that when the source language does not
have case, as in Chinese, the bilingual model equals
a monolingual one.
The BLEU difference between the CRF-based
capitalizer and the trigram one were larger than
the precision difference. This indicates that the
CRF-based capitalizer performs much better on non-
grammatical texts that are generated from an MT
system due to the bilingual feature of the CRF capi-
talizer.
6.4 Effect of Training Corpus Size
The experiments above were carried out on large
data sets. We also conducted experiments to exam-
ine the effect of the training corpus size on capital-
ization precision. Figure 4 shows the effects. The
experiment was performed on the E?F corpus. The
bilingual capitalizer performed significantly better
when the training corpus size was small (e.g., un-
der 8 million words). This is common in many do-
mains: when the training corpus size increases, the
difference between the two capitalizers decreases.
7 Conclusions
In this paper, we have studied how to exploit bilin-
gual information to improve capitalization perfor-
mance on machine translation output, and evaluated
the improvement over traditional methods that use
only monolingual language models.
We first presented a probabilistic bilingual cap-
italization model for capitalizing machine transla-
tion outputs using conditional random fields. This
model exploits bilingual capitalization knowledge as
well as monolingual information. We defined a se-
ries of feature functions to incorporate capitalization
knowledge into the model.
We then evaluated our CRF-based bilingual capi-
talization model both on well-formed texts in terms
of capitalization precision, and on possibly ungram-
matical end-to-end machine translation outputs in
terms of BLEU scores. Experiments were per-
formed on both French and English target MT sys-
tems with large-scale training data. Our experimen-
tal results showed that the CRF-based bilingual cap-
7
BLEU Scores
Translation UnigramCapitalizer
Trigram
Capitalizer
CRF-based
Capitalizer
Upper
Bound
F?E 24.96 26.73 27.92 28.85
E?F 32.63 34.66 36.10 36.17
C?E 23.81 25.92 25.89 -
Table 3: Impact of CRF-based capitalizer on end-to-end translation performance compared with two LM-based baselines.
Capitalization Precision (%)
Translation Unigram
capitalizer
Trigram
capitalizer
CRF-based
capitalizer
F?E 94.03 98.79 99.20
E?F 91.52 98.47 99.11
C?E 90.77 96.40 96.76
Table 4: Impact of CRF-based capitalizer on capitalization precision compared with two LM-based baselines.
100
99
98
97
96
95
94
93
92
64.032.016.08.04.02.01.00.50.20.1
Pr
ec
isi
on
 (x
%)
Training Corpus Size (MWs)
CRF-based capitalizer
LM-based capitalizer
Figure 4: Capitalization precision with respect to size of train-
ing corpus. LM-based capitalizer refers to the trigram-based
one. Results were on E?F corpus.
italization model performs significantly better than a
strong baseline, monolingual capitalizer that uses a
trigram language model.
In all experiments carried out at Language Weaver
with customer (or domain specific) data, MT sys-
tems trained on lowercased data coupled with the
CRF bilingual capitalizer described in this paper
consistently outperformed both MT systems trained
on lowercased data coupled with a strong monolin-
gual capitalizer and MT systems trained on mixed-
cased data.
References
Ciprian Chelba and Alex Acero. 2004. Adaptation of maxi-
mum entroy capitalizer: Little data can help a lot. In Pro-
ceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), Barcelona, Spain.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing Maximum Entropy models. Technical Report
CMUCS-99-108, Carnegie Mellon University.
William A. Gale, Kenneth W. Church, and David Yarowsky.
1994. Discrimination decisions for 100,000-dimensional
spaces. In Current issues in computational linguistics.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a Translation Rule? In Proceedings of the Human
Language Technology Conference and the North American
Association for Computational Linguistics (HLT-NAACL),
Boston, Massachusetts.
Ji-Hwan Kim and Philip C. Woodland. 2004. Automatic capi-
talization generation for speech input. Computer Speech and
Language, 18(1):67?90, January.
Reinhard Kneser and Hermann Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Signal
Processing (ICASSP) 1995, pages 181?184, Detroit, Michi-
gan. IEEE.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for segmen-
tation and labeling sequence data.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and Nanda
Kambhatla. 2003. tRuEcasIng. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), Sapporo, Japan, July.
Daniel Marcu and William Wong. 2002. A phrase-based, joint
probability model for statistical machine translation. In Pro-
ceedings of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP), Philadelphia, PA.
A. Mikheev. 1999. A knowledge-free method fro capitalized
word disambiguation. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics
(ACL), College Park, Maryland, June.
Franz Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. Computational
Linguistics, 30(4).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. BLEU: A method for automatic evaluation
of Machine Translation. Technical Report RC22176, IBM,
September.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004. Discriminative language modeling with condi-
tional random field and the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), Barcelona, Spain.
8
Language Weaver Arabic->English MT 
Daniel MARCU, Alex FRASER, William WONG, Kevin KNIGHT 
Language Weaver, Inc.  
4640 Admiralty Way, Suite 1210 
Marina del Rey, CA, USA, 90292 
{marcu,afraser,wong,knight}@languageweaver.com 
 
Abstract 
This presentation is primarily a demonstration 
of a working statistical machine translation 
system which translates Modern Standard 
Arabic into English.  
1 Overview 
Language Weaver has produced a high-
performance statistical Arabic-to-English machine 
translation system, based on research work 
conducted at the University of Southern California, 
Information Sciences Institute (USC/ISI).  Getting 
resource-unlimited laboratory systems to run in 
real time, on a typical desktop Windows machine, 
is among Language Weaver?s contributions.  The 
system is designed to provide broad general 
coverage of Arabic news, and is currently used at 
various sites within the U.S. Government.  
 
 
 
 
 
The Arabic->English translation system to be 
demonstrated has been prepared in versions that 
require 1 or 2 GB of RAM, and run on a 1.5GHz or 
faster processor and translates at a minimum rate 
of 500 words per minute.  The system includes an 
option to trade off speed for quality in the 
translation process allowing users to select the 
fastest possible gisting-quality output, or the best 
possible translation quality for each sentence. 
2 Demonstration 
The translation system will be demonstrated on 
current news, and possibly other postings from 
Internet, or other files: 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 44?52,
Sydney, July 2006. c?2006 Association for Computational Linguistics
SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight
Language Weaver Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
{dmarcu,wwang,aechihabi,kknight}@languageweaver.com
Abstract
We introduce SPMT, a new class of sta-
tistical Translation Models that use Syn-
tactified target language Phrases. The
SPMT models outperform a state of the art
phrase-based baseline model by 2.64 Bleu
points on the NIST 2003 Chinese-English
test corpus and 0.28 points on a human-
based quality metric that ranks translations
on a scale from 1 to 5.
1 Introduction
During the last four years, various implemen-
tations and extentions to phrase-based statistical
models (Marcu and Wong, 2002; Koehn et al,
2003; Och and Ney, 2004) have led to signif-
icant increases in machine translation accuracy.
Although phrase-based models yield high-quality
translations for language pairs that exhibit simi-
lar word order, they fail to produce grammatical
outputs for language pairs that are syntactically
divergent. Recent models that exploit syntactic
information of the source language (Quirk et al,
2005) have been shown to produce better outputs
than phrase-based systems when evaluated on rel-
atively small scale, domain specific corpora. And
syntax-inspired formal models (Chiang, 2005), in
spite of being trained on significantly less data,
have shown promising results when compared on
the same test sets with mature phrase-based sys-
tems. To our knowledge though, no previous re-
search has demonstrated that a syntax-based sta-
tistical translation system could produce better re-
sults than a phrase-based system on a large-scale,
well-established, open domain translation task. In
this paper we present such a system.
Our translation models rely upon and naturally
exploit submodels (feature functions) that have
been initially developed in phrase-based systems
for choosing target translations of source language
phrases, and use new, syntax-based translation and
target language submodels for assembling target
phrases into well-formed, grammatical outputs.
After we introduce our models intuitively, we
discuss their formal underpinning and parameter
training in Section 2. In Section 3, we present our
decoder and, in Section 4, we evaluate our models
empirically. In Section 5, we conclude with a brief
discussion.
2 SPMT: statistical Machine Translation
with Syntactified Phrases
2.1 An intuitive introduction to SPMT
After being exposed to 100M+ words of parallel
Chinese-English texts, current phrase-based statis-
tical machine translation learners induce reason-
ably reliable phrase-based probabilistic dictionar-
ies. For example, our baseline statistical phrase-
based system learns that, with high probabilities,
the Chinese phrases ?ASTRO- -NAUTS?, ?FRANCE
AND RUSSIA? and ?COMINGFROM? can be trans-
lated into English as ?astronauts?/?cosmonauts?,
?france and russia?/?france and russian? and
?coming from?/?from?, respectively. 1 Unfortu-
nately, when given as input Chinese sentence 1,
our phrase-based system produces the output
shown in 2 and not the translation in 3, which
correctly orders the phrasal translations into a
grammatical sequence. We believe this hap-
pens because the distortion/reordering models that
are used by state-of-the-art phrase-based systems,
which exploit phrase movement and ngram target
1To increase readability, in this paper, we represent Chi-
nese words using fully capitalized English glosses and En-
glish words using lowercased letters.
44
language models (Och and Ney, 2004; Tillman,
2004), are too weak to help a phrase-based de-
coder reorder the target phrases into grammatical
outputs.
THESE 7PEOPLE INCLUDE COMINGFROM
FRANCE AND RUSSIA p-DE ASTRO- -NAUTS .
(1)
the 7 people including those from france
and the russian cosmonauts .
(2)
these 7 people include astronauts coming
from france and russia .
(3)
One method for increasing the ability of a de-
coder to reorder target language phrases is that
of decorating them with syntactic constituent in-
formation. For example, we may make ex-
plicit that the Chinese phrase ?ASTRO- -NAUTS?
may be translated into English as a noun phrase,
NP(NNS(astronauts)); that the phrase FRANCE AND
RUSSIA may be translated into a complex noun-
phrase, NP(NP(NNP(france)) CC(and) NP(NNP(russia)));
that the phrase COMINGFROM may be translated
into a partially realized verb phrase that is look-
ing for a noun phrase to its right in order to be
fully realized, VP(VBG(coming) PP(IN(from) NP:x0));
and that the Chinese particle p-DE, when occurring
between a Chinese string that was translated into
a verb phrase to its left and another Chinese string
that was translated into a noun phrase to its right,
VP:x1 p-DE NP:x0, should be translated to noth-
ing, while forcing the reordering of the two con-
stituents, NP(NP:x0, VP:x1). If all these translation
rules (labeled r1 to r4 in Figure 1) were available
to a decoder that derives English parse trees start-
ing from Chinese input strings, this decoder could
produce derivations such as that shown in Fig-
ure 2. Because our approach uses translation rules
with Syntactified target language Phrases (see Fig-
ure 1), we call it SPMT.
2.2 A formal introduction to SPMT
2.2.1 Theoretical foundations
We are interested to model a generative process
that explains how English parse trees pi and their
associated English string yields E, foreign sen-
tences, F , and word-level alignments, A, are pro-
duced. We assume that observed (pi, F,A) triplets
are generated by a stochastic process similar to
r1 :NP(NNS(astronauts)) ? ASTRO- -NAUTS
r2 :NP(NP(NNP(france)) CC(and) NP(NNP(russia)))?
FRANCE AND RUSSIA
r3 :VP(VBG(coming) PP(IN(from) NP:x0)) ?
COMINGFROM x0
r4 :NP(NP:x0, VP:x1) ? x1 p-DE x0
r5 :NNP(france) ? FRANCE
r6 :NP(NP(NNP(france)) CC(and) NP:x0) ? FRANCE AND x0
r7 :NNS(astronauts) ? ASTRO- -NAUTS
r8 :NNP(russia) ? RUSSIA
r9 :NP(NNS:x0)? x0
r10 :PP(IN:x0 NP:x1) ? x0 x1
r11 :NP(NP:x0 CC:x1 NP:x2) ? x0 x1 x2
r12 :NP(NNP:x0)? x0
r13 :CC(and) ? AND
r14 :NP(NP:x0 CC(and) NP:x1) ? x0 AND x1
r15 :NP(NP:x0 VP(VBG(coming) PP(IN(from) NP:x1))) ?
x1 COMINGFROM x0
Figure 1: Examples of xRS rules.
that used in Data Oriented Parsing models (Bon-
nema, 2002). For example, if we assume that the
generative process has already produced the top
NP node in Figure 2, then the corresponding par-
tial English parse tree, foreign/source string, and
word-level alignment could be generated by the
rule derivation r4(r1, r3(r2)), where each rule is
assumed to have some probability.
The extended tree to string transducers intro-
duced by Knight and Graehl (2005) provide a nat-
ural framework for expressing the tree to string
transformations specific to our SPMT models.
The transformation rules we plan to exploit are
equivalent to one-state xRS top-down transduc-
ers with look ahead, which map subtree patterns
to strings. For example, rule r3 in Figure 1 can
be applied only when one is in a state that has a
VP as its syntactic constituent and the tree pat-
tern VP(VBG(coming) PP(IN(from) NP)) immediately
underneath. The rule application outputs the string
?COMINGFROM? as the transducer moves to the
state co-indexed by x0; the outputs produced from
the new state will be concatenated to the right of
the string ?COMINGFROM?.
Since there are multiple derivations that could
lead to the same outcome, the probability of a
tuple (pi, F,A) is obtained by summing over all
derivations ?i ? ? that are consistent with the tu-
45
Figure 2: English parse tree derivation of the Chi-
nese string COMINGFROM FRANCE AND RUSSIA p-
DE ASTRO- -NAUTS.
ple, c(?) = (pi, F,A). The probability of each
derivation ?i is given by the product of the proba-
bilities of all the rules p(rj) in the derivation (see
equation 4).
Pr(pi, F,A) =
?
?i??,c(?)=(pi,F,A)
?
rj??i
p(rj) (4)
In order to acquire the rules specific to our
model and to induce their probabilities, we parse
the English side of our corpus with an in-house
implementation (Soricut, 2005) of Collins pars-
ing models (Collins, 2003) and we word-align the
parallel corpus with the Giza++2 implementation
of the IBM models (Brown et al, 1993). We
use the automatically derived ?English-parse-tree,
English-sentence, Foreign-sentence, Word-level-
alignment? tuples in order to induce xRS rules for
several models.
2.2.2 SPMT Model 1
In our simplest model, we assume that each
tuple (pi, F,A) in our automatically annotated
corpus could be produced by applying a com-
bination of minimally syntactified, lexicalized,
phrase-based compatible xRS rules, and mini-
mal/necessary, non-lexicalized xRS rules. We call
a rule non-lexicalized whenever it does not have
any directly aligned source-to-target words. Rules
r9?r12 in Figure 1 are examples of non-lexicalized
rules.
Minimally syntactified, lexicalized, phrase-
based-compatible xRS rules are extracted via a
2http://www.fjoch.com/GIZA++.html
simple algorithm that finds for each foreign phrase
F ji , the smallest xRS rule that is consistent with
the foreign phrase F ji , the English syntactic tree
pi, and the alignment A. The algorithm finds for
each foreign/source phrase span its projected span
on the English side and then traverses the En-
glish parse tree bottom up until it finds a node
that subsumes the projected span. If this node has
children that fall outside the projected span, then
those children give rise to rules that have variables.
For example, if the tuple shown in Figure 2 is in
our training corpus, for the foreign/source phrases
FRANCE, FRANCE AND, FRANCE AND RUSSIA, and
ASTRO- -NAUTS, we extract the minimally syntac-
tified, lexicalized phrase-based-compatible xRS
rules r5, r6, r2, and r7 in Figure 1, respectively.
Because, as in phrase-based MT, all our rules have
continuous phrases on both the source and target
language sides, we call these phrase-based com-
patible xRS rules.
Since these lexicalized rules are not sufficient to
explain an entire (pi, F,A) tuple, we also extract
the required minimal/necessary, non-lexicalized
xRS rules. The minimal non-lexicalized rules that
are licensed by the tuple in Figure 2 are labeled
r4, r9, r10, r11 and r12 in Figure 1. To obtain the
non-lexicalized xRS rules, we compute the set of
all minimal rules (lexicalized and non-lexicalized)
by applying the algorithm proposed by Galley et
al. (2006) and then remove the lexicalized rules.
We remove the Galley et al?s lexicalized rules
because they are either already accounted for by
the minimally syntactified, lexicalized, phrase-
based-compatible xRS rules or they subsume non-
continuous source-target phrase pairs.
It is worth mentioning that, in our framework,
a rule is defined to be ?minimal? with respect to a
foreign/source language phrase, i.e., it is the min-
imal xRS rule that yields that source phrase. In
contrast, in the work of Galley et al (2004; 2006),
a rule is defined to be minimal when it is necessary
in order to explain a (pi, F,A) tuple.
Under SPMT model 1, the tree in Figure 2 can
be produced, for example, by the following deriva-
tion: r4(r9(r7), r3(r6(r12(r8)))).
2.2.3 SPMT Model 1 Composed
We hypothesize that composed rules, i.e., rules
that can be decomposed via the application of a
sequence of Model 1 rules may improve the per-
formance of an SPMT system. For example, al-
though the minimal Model 1 rules r11 and r13 are
46
Figure 3: Problematic syntactifications of phrasal
translations.
sufficient for building an English NP on top of two
NPs separated by the Chinese conjunction AND,
the composed rule r14 in Figure 1 accomplishes
the same result in only one step. We hope that the
composed rules could play in SPMT the same role
that phrases play in string-based translation mod-
els.
To test our hypothesis, we modify our rule ex-
traction algorithm so that for every foreign phrase
F ji , we extract not only a minimally syntactified,
lexicalized xRS rule, but also one composed rule.
The composed rule is obtained by extracting the
rule licensed by the foreign/source phrase, align-
ment, English parse tree, and the first multi-child
ancestor node of the root of the minimal rule. Our
intuition is that composed rules that involve the ap-
plication of more than two minimal rules are not
reliable. For example, for the tuple in Figure 2,
the composed rule that we extract given the for-
eign phrases AND and COMINGFROM are respec-
tively labeled as rules r14 and r15 in Figure 1.
Under the SPMT composed model 1,
the tree in Figure 2 can be produced,
for example, by the following derivation:
r15(r9(r7), r14(r12(r5), r12(r8))).
2.2.4 SPMT Model 2
In many instances, the tuples (pi, F,A) in our
training corpus exhibit alignment patterns that can
be easily handled within a phrase-based SMT
framework, but that become problematic in the
SPMT models discussed until now.
Consider, for example, the (pi, F,A) tuple frag-
ment in Figure 3. When using a phrase-based
translation model, one can easily extract the
phrase pair (THE MUTUAL; the mutual) and use it
during the phrase-based model estimation phrase
and in decoding. However, within the xRS trans-
ducer framework that we use, it is impossible to
extract an equivalent syntactified phrase transla-
tion rule that subsumes the same phrase pair be-
cause valid xRS translation rules cannot be multi-
headed. When faced with this constraint, one has
several options:
? One can label such phrase pairs as non-
syntactifiable and ignore them. Unfortu-
nately, this is a lossy choice. On our par-
allel English-Chinese corpus, we have found
that approximately 28% of the foreign/source
phrases are non-syntactifiable by this defini-
tion.
? One can also traverse the parse tree upwards
until one reaches a node that is xRS valid, i.e.,
a node that subsumes the entire English span
induced by a foreign/source phrase and the
corresponding word-level alignment. This
choice is also inappropriate because phrase
pairs that are usually available to phrase-
based translation systems are then expanded
and made available in the SPTM models only
in larger applicability contexts.
? A third option is to create xRS compati-
ble translation rules that overcome this con-
straint.
Our SPMT Model 2 adopts the third option by
rewriting on the fly the English parse tree for each
foreign/source phrase and alignment that lead to
non-syntactifiable phrase pairs. The rewriting pro-
cess adds new rules to those that can be created
under the SPMT model 1 constraints. The process
creates one xRS rule that is headed by a pseudo,
non-syntactic nonterminal symbol that subsumes
the target phrase and corresponding multi-headed
syntactic structure; and one sibling xRS rule that
explains how the non-syntactic nonterminal sym-
bol can be combined with other genuine nonter-
minals in order to obtain genuine parse trees. In
this view, the foreign/source phrase THE MUTUAL
and corresponding alignment in Figure 3 licenses
the rules ?NPB? NN(DT(the) JJ(mutual)) ? THE MU-
TUAL and NPB(?NPB? NN:x0 NN:x1) ? x0 x1 even
though the foreign word UNDERSTANDING is
aligned to an English word outside the NPB con-
situent. The name of the non-syntactic nontermi-
nal reflects the intuition that the English phrase ?the
mutual? corresponds to a partially realized NPB that
needs an NN to its right in order to be fully real-
ized.
47
Our hope is that the rules headed by pseudo
nonterminals could make available to an SPMT
system all the rules that are typically available to
a phrase-based system; and that the sibling rules
could provide a sufficiently robust generalization
layer for integrating pseudo, partially realized con-
stituents into the overall decoding process.
2.2.5 SPMT Model 2 Composed
The SPMT composed model 2 uses all rule
types described in the previous models.
2.3 Estimating rule probabilities
For each model, we extract all rule instances that
are licensed by a symmetrized Giza-aligned paral-
lel corpus and the constraints we put on the model.
We condition on the root node of each rule and use
the rule counts f(r) and a basic maximum likeli-
hood estimator to assign to each rule type a condi-
tional probability (see equation 5).
p(r|root(r)) = f(r)?
r?:root(r?)=root(r) f(r?)
(5)
It is unlikely that this joint probability model
can be discriminative enough to distinguish be-
tween good and bad translations. We are not too
concerned though because, in practice, we decode
using a larger set of submodels (feature functions).
Given the way all our lexicalized xRS rules have
been created, one can safely strip out the syntac-
tic information and end up with phrase-to-phrase
translation rules. For example, in string-to-string
world, rule r5 in Figure 1 can be rewritten as ?france
? FRANCE?; and rule r6 can be rewritten as ?france
and ? FRANCE AND?. When one analyzes the lex-
icalized xRS rules in this manner, it is easy to as-
sociate with them any of the submodel probability
distributions that have been proven useful in statis-
tical phrase-based MT. The non-lexicalized rules
are assigned probability distributions under these
submodels as well by simply assuming a NULL
phrase for any missing lexicalized source or target
phrase.
In the experiments described in this paper, we
use the following submodels (feature functions):
Syntax-based-like submodels:
? proot(ri) is the root normalized conditional
probability of all the rules in a model.
? pcfg(ri) is the CFG-like probability of the
non-lexicalized rules in the model. The lexi-
calized rules have by definition pcfg = 1.
? is lexicalized(ri) is an indicator feature func-
tion that has value 1 for lexicalized rules, and
value 0 otherwise.
? is composed(ri) is an indicator feature func-
tion that has value 1 for composed rules.
? is lowcount(ri) is an indicator feature func-
tion that has value 1 for the rules that occur
less than 3 times in the training corpus.
Phrase-based-like submodels:
? lex pef(ri) is the direct phrase-based con-
ditional probability computed over the for-
eign/source and target phrases subsumed by
a rule.
? lex pfe(ri) is the inverse phrase-based condi-
tional probability computed over the source
and target phrases subsumed by a rule.
? m1(ri) is the IBM model 1 probability com-
puted over the bags of words that occur on
the source and target sides of a rule.
? m1inv(ri) is the IBM model 1 inverse prob-
ability computed over the bags of words that
occur on the source and target sides of a rule.
? lm(e) is the language model probability of
the target translation under an ngram lan-
guage model.
? wp(e) is a word penalty model designed to
favor longer translations.
All these models are combined log-linearly dur-
ing decoding. The weights of the models are
computed automatically using a variant of the
Maximum Bleu training procedure proposed by
Och (2003).
The phrase-based-like submodels have been
proved useful in phrase-based approaches to
SMT (Och and Ney, 2004). The first two syntax-
based submodels implement a ?fused? translation
and lexical grounded distortion model (proot) and
a syntax-based distortion model (pcfg). The indi-
cator submodels are used to determine the extent
to which our system prefers lexicalized vs. non-
lexicalized rules; simple vs. composed rules; and
high vs. low count rules.
48
3 Decoding
3.1 Decoding with one SPMT model
We decode with each of our SPMT models using
a straightforward, bottom-up, CKY-style decoder
that builds English syntactic constituents on the
top of Chinese sentences. The decoder uses a bina-
rized representation of the rules, which is obtained
via a syncronous binarization procedure (Zhang et
al., 2006). The CKY-style decoder computes the
probability of English syntactic constituents in a
bottom up fashion, by log-linearly interpolating all
the submodel scores described in Section 2.3.
The decoder is capable of producing nbest
derivations and nbest lists (Knight and Graehl,
2005), which are used for Maximum Bleu train-
ing (Och, 2003). When decoding the test cor-
pus, the decoder returns the translation that has the
most probable derivation; in other words, the sum
operator in equation 4 is replaced with an argmax.
3.2 Decoding with multiple SPMT models
Combining multiple MT outputs to increase per-
formance is, in general, a difficult task (Matusov
et al, 2006) when significantly different engines
compete for producing the best outputs. In our
case, combining multiple MT outputs is much
simpler because the submodel probabilities across
the four models described here are mostly iden-
tifical, with the exception of the root normalized
and CFG-like submodels which are scaled differ-
ently ? since Model 2 composed has, for example,
more rules than Model 1, the root normalized and
CFG-like submodels have smaller probabilities for
identical rules in Model 2 composed than in Model
1. We compare these two probabilities across the
submodels and we scale all model probabilities to
be compatible with those of Model 2 composed.
With this scaling procedure into place, we pro-
duce 6,000 non-unique nbest lists for all sentences
in our development corpus, using all SPMT sub-
models. We concatenate the lists and we learn a
new combination of weights that maximizes the
Bleu score of the combined nbest list using the
same development corpus we used for tuning the
individual systems (Och, 2003). We use the new
weights in order to rerank the nbest outputs on the
test corpus.
4 Experiments
4.1 Automatic evaluation of the models
We evaluate our models on a Chinese to English
machine translation task. We use the same training
corpus, 138.7M words of parallel Chinese-English
data released by LDC, in order to train several
statistical-based MT systems:
? PBMT, a strong state of the art phrase-based
system that implements the alignment tem-
plate model (Och and Ney, 2004); this is the
system ISI has used in the 2004 and 2005
NIST evaluations.
? four SPMT systems (M1, M1C, M2, M2C)
that implement each of the models discussed
in this paper;
? a SPMT system, Comb, that combines the
outputs of all SPMT models using the pro-
cedure described in Section 3.2.
In all systems, we use a rule extraction algo-
rithm that limits the size of the foreign/source
phrases to four words. For all systems, we use
a Kneser-Ney (1995) smoothed trigram language
model trained on 2.3 billion words of English. As
development data for the SPMT systems, we used
the sentences in the 2002 NIST development cor-
pus that are shorter than 20 words; we made this
choice in order to finish all experiments in time for
this submission. The PBMT system used all sen-
tences in the 2002 NIST corpus for development.
As test data, we used the 2003 NIST test set.
Table 1 shows the number of string-to-string or
tree-to-string rules extracted by each system and
the performance on both the subset of sentences in
the test corpus that were shorter than 20 words and
the entire test corpus. The performance is mea-
sured using the Bleu metric (Papineni et al, 2002)
on lowercased, tokenized outputs/references.
The results show that the SPMT models clearly
outperform the phrase-based systems ? the 95%
confidence intervals computed via bootstrap re-
sampling in all cases are around 1 Bleu point. The
results also show that the simple system combina-
tion procedure that we have employed is effective
in our setting. The improvement on the develop-
ment corpus transfers to the test setting as well.
A visual inspection of the outputs shows signif-
icant differences between the outputs of the four
models. The models that use composed rules pre-
fer to produce outputs by using mostly lexicalized
49
System # of rules Bleu score Bleu score Bleu score
(in millions) on Dev on Test on Test
(4 refs) (4 refs) (4 refs)
< 20 words < 20 words
PBMT 125.8 34.56 34.83 31.46
SPMT-M1 34.2 37.60 38.18 33.15
SPMT-M1C 75.7 37.30 38.10 32.39
SPMT-M2 70.4 37.77 38.74 33.39
SPMT-M2C 111.1 37.48 38.59 33.16
SPMT-Comb 111.1 39.44 39.56 34.10
Table 1: Automatic evaluation results.
rules; in contrast, the simple M1 and M2 mod-
els produce outputs in which content is translated
primarily using lexicalized rules and reorderings
and word insertions are explained primarily by the
non-lexical rules. It appears that the two strategies
are complementary, succeeding and failing in dif-
ferent instances. We believe that this complemen-
tarity and the overcoming of some of the search
errors in our decoder during the model rescoring
phase explain the success of the system combina-
tion experiments.
We suspect that our decoder still makes many
search errors. In spite of this, the SPTM outputs
are still significantly better than the PBMT out-
puts.
4.2 Human-based evaluation of the models
We also tested whether the Bleu score improve-
ments translate into improvements that can be per-
ceived by humans. To this end, we randomly se-
lected 138 sentences of less than 20 words from
our development corpus; we expected the transla-
tion quality of sentences of this size to be easier to
assess than that of sentences that are very long.
We prepared a web-based evaluation interface
that showed for each input sentence:
? the Chinese input;
? three English reference translations;
? the output of seven ?MT systems?.
The evaluated ?MT systems? were the six systems
shown in Table 1 and one of the reference trans-
lations. The reference translation presented as
automatically produced output was selected from
the set of four reference translations provided by
NIST so as to be representative of human transla-
tion quality. More precisely, we chose the second
best reference translation in the NIST corpus ac-
cording to its Bleu score against the other three
reference translations. The seven outputs were
randomly shuffled and presented to three English
speakers for assessment.
The judges who participated in our experiment
were instructed to carefully read the three refer-
ence translations and seven machine translation
outputs, and assign a score between 1 and 5 to
each translation output on the basis of its quality.
Human judges were told that the translation qual-
ity assessment should take into consideration both
the grammatical fluency of the outputs and their
translation adequacy. Table 2 shows the average
scores obtained by each system according to each
judge. For convenience, the table also shows the
Bleu scores of all systems (including the human
translations) on three reference translations.
The results in Table 2 show that the human
judges are remarkably consistent in preferring the
syntax-based outputs over the phrase-based out-
puts. On a 1 to 5 quality scale, the difference be-
tween the phrase-based and syntax-based systems
was, on average, between 0.2 and 0.3 points. All
differences between the phrase-based baseline and
the syntax-based outputs were statistically signif-
icant. For example, when comparing the phrase-
based baseline against the combined system, the
improvement in human scores was significant at
P = 4.04e?6(t = 4.67, df = 413).
The results also show that the LDC reference
translations are far from being perfect. Although
we selected from the four references the second
best according to the Bleu metric, this human ref-
erence was judged to be at a quality level of only
4.67 on a scale from 1 to 5. Most of the translation
errors were fluency errors. Although the human
outputs had most of the time the right meaning,
the syntax was sometimes incorrect.
In order to give readers a flavor of the types
of re-orderings enabled by the SPMT models, we
present in Table 3, several translation outputs pro-
duced by the phrase-based baseline and the com-
50
System Bleu score Judge 1 Judge 2 Judge 3 Judge
on Dev avg
(3 refs)
< 20 words
PBMT 31.00 3.00 3.34 2.95 3.10
SPMT-M1 33.79 3.28 3.49 3.04 3.27
SPMT-M1C 33.66 3.23 3.43 3.26 3.31
SPMT-M2 34.05 3.24 3.45 3.10 3.26
SPMT-M2C 33.42 3.24 3.48 3.13 3.28
SPMT-Combined 35.33 3.31 3.59 3.25 3.38
Human Ref 40.84 4.64 4.62 4.75 4.67
Table 2: Human-based evaluation results.
bined SPMT system. The outputs were selected to
reflect both positive and negative effects of large-
scale re-orderings.
5 Discussion
The SPMT models are similar to the models pro-
posed by Chiang (2005) and Galley et al (2006).
If we analyze these three models in terms of ex-
pressive power, the Galley et al (2006) model is
more expressive than the SPMT models, which
in turn, are more expressive than Chiang?s model.
The xRS formalism utilized by Galley et al (2006)
allows for the use of translation rules that have
multi-level target tree annotations and discontin-
uous source language phrases. The SPMT mod-
els are less general: they use translation rules that
have multi-level target tree annotations but require
that the source language phrases are continuous.
The Syncronous Grammar formalism utilized by
Chiang is stricter than SPMT since it allows only
for single-level target tree annotations.
The parameters of the SPMT models presented
in this paper are easier to estimate than those of
Galley et als (2006) and can easily exploit and
expand on previous research in phrase-based ma-
chine translation. Also, the SPMT models yield
significantly fewer rules that the model of Galley
et al In contrast with the model proposed by Chi-
ang, the SPMT models introduced in this paper are
fully grounded in syntax; this makes them good
candidates for exploring the impact that syntax-
based language models could have on translation
performance.
From a machine translation perspective, the
SPMT translation model family we have proposed
in this paper is promising. To our knowledge,
we are the first to report results that show that a
syntax-based system can produce results that are
better than those produced by a strong phrase-
based system in experimental conditions similar
to those used in large-scale, well-established in-
dependent evaluations, such as those carried out
annually by NIST.
Although the number of syntax-based rules
used by our models is smaller than the number
of phrase-based rules used in our state-of-the-art
baseline system, the SPMT models produce out-
puts of higher quality. This feature is encouraging
because it shows that the syntactified translation
rules learned in the SPMT models can generalize
better than the phrase-based rules.
We were also pleased to see that the Bleu
score improvements going from the phrase- to the
syntax-based models, as well as the Bleu improve-
ments going from the simple syntax-based models
to the combined models system are fully consis-
tent with the human qualitative judgments in our
subjective evaluations. This correlation suggests
that we can continue to use the Bleu metric to fur-
ther improve our models and systems.
Acknowledgements. This research was par-
tially supported by the National Institute of Stan-
dards and Technology?s Advanced Technology
Program Award 70NANB4H3050 to Language
Weaver Inc.
References
R. Bonnema. 2002. Probability models for DOP. In
Data-Oriented Parsing. CSLI publications.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June.
51
System Output
PBMT fujian is china ?s coastal areas most rapid development of foreign trade of the region .
SPMT-Combined china ?s coastal areas of fujian is one of the areas of the most rapid development of
foreign trade and economic cooperation .
PBMT investment in macao has become the largest foreign investors .
SPMT-Combined the chinese - funded enterprises have become the largest foreign investor in macao.
PBMT they are now two people were unaccounted for .
SPMT-Combined currently , both of them remain unaccounted for .
PBMT there was no further statement .
SPMT-Combined the statement did not explain further .
Table 3: Sample translations.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637, December.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In HLT-NAACL?2004: Main Proceedings,
pages 273?280, Boston, Massachusetts, USA, May
2 - May 7.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training
of context-rich syntax translation models. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL?2006), Sydney,
Australia, July.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP?95),
volume 1, pages 181?184.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natu-
ral language processing. In Proc. of the Sixth In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing?2005),
pages 1?25. Springer Verlag.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT-NAACL?2003), Edmon-
ton, Canada, May 27?June 1.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?2002), pages 133?139, Philadelphia, PA,
July 6-7.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypothesis alignment. In Proceedings of the An-
nual Meeting of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL?2006),
Trento, Italy.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4), Decem-
ber.
Franz Joseph Och. 2003. Minimum error training
in statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL?2003), pages 160?167,
Sapooro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, John
Henderson, and Florence Reeder. 2002. Corpus-
based comprehensive and diagnostic MT evaluation:
Initial Arabic, Chinese, French, and Spanish results.
In Proceedings of the Human Language Technology
Conference (ACL?2002), pages 124?127, San Diego,
CA, March 24-27.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?2005), pages 271?279, Ann
Arbor, Michigan, June.
Radu Soricut. 2005. A reimplementation of Collins?s
parsing models.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Syncronous binarization for ma-
chine translation. In Proceding of the Human Lan-
guage Technology and North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?2006), New York, June.
52
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 961?968,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Scalable Inference and Training of
Context-Rich Syntactic Translation Models
Michel Galley*, Jonathan Graehl?, Kevin Knight??, Daniel Marcu??,
Steve DeNeefe?, Wei Wang? and Ignacio Thayer?
*Columbia University
Dept. of Computer Science
New York, NY 10027
galley@cs.columbia.edu, {graehl,knight,marcu,sdeneefe}@isi.edu,
wwang@languageweaver.com, thayer@google.com
?University of Southern California
Information Sciences Institute
Marina del Rey, CA 90292
?Language Weaver, Inc.
4640 Admiralty Way
Marina del Rey, CA 90292
Abstract
Statistical MT has made great progress in the last
few years, but current translation models are weak
on re-ordering and target language fluency. Syn-
tactic approaches seek to remedy these problems.
In this paper, we take the framework for acquir-
ing multi-level syntactic translation rules of (Gal-
ley et al, 2004) from aligned tree-string pairs, and
present two main extensions of their approach: first,
instead of merely computing a single derivation that
minimally explains a sentence pair, we construct
a large number of derivations that include contex-
tually richer rules, and account for multiple inter-
pretations of unaligned words. Second, we pro-
pose probability estimates and a training procedure
for weighting these rules. We contrast different
approaches on real examples, show that our esti-
mates based on multiple derivations favor phrasal
re-orderings that are linguistically better motivated,
and establish that our larger rules provide a 3.63
BLEU point increase over minimal rules.
1 Introduction
While syntactic approaches seek to remedy word-
ordering problems common to statistical machine
translation (SMT) systems, many of the earlier
models?particularly child re-ordering models?
fail to account for human translation behavior.
Galley et al (2004) alleviate this modeling prob-
lem and present a method for acquiring millions
of syntactic transfer rules from bilingual corpora,
which we review below. Here, we make the fol-
lowing new contributions: (1) we show how to
acquire larger rules that crucially condition on
more syntactic context, and show how to com-
pute multiple derivations for each training exam-
ple, capturing both large and small rules, as well
as multiple interpretations for unaligned words;
(2) we develop probability models for these multi-
level transfer rules, and give estimation methods
for assigning probabilities to very large rule sets.
We contrast our work with (Galley et al, 2004),
highlight some severe limitations of probability
estimates computed from single derivations, and
demonstrate that it is critical to account for many
derivations for each sentence pair. We also use
real examples to show that our probability mod-
els estimated from a large number of derivations
favor phrasal re-orderings that are linguistically
well motivated. An empirical evaluation against
a state-of-the-art SMT system similar to (Och and
Ney, 2004) indicates positive prospects. Finally,
we show that our contextually richer rules provide
a 3.63 BLEU point increase over those of (Galley
et al, 2004).
2 Inferring syntactic transformations
We assume we are given a source-language (e.g.,
French) sentence f , a target-language (e.g., En-
glish) parse tree pi, whose yield e is a translation
of f , and a word alignment a between f and e.
Our aim is to gain insight into the process of trans-
forming pi into f and to discover grammatically-
grounded translation rules. For this, we need
a formalism that is expressive enough to deal
with cases of syntactic divergence between source
and target languages (Fox, 2002): for any given
(pi, f ,a) triple, it is useful to produce a derivation
that minimally explains the transformation be-
tween pi and f , while remaining consistent with a.
Galley et al (2004) present one such formalism
(henceforth ?GHKM?).
2.1 Tree-to-string alignments
It is appealing to model the transformation of pi
into f using tree-to-string (xRs) transducers, since
their theory has been worked out in an exten-
sive literature and is well understood (see, e.g.,
(Graehl and Knight, 2004)). Formally, transfor-
mational rules ri presented in (Galley et al, 2004)
are equivalent to 1-state xRs transducers mapping
a given pattern (subtree to match in pi) to a right
hand side string. We will refer to them as lhs(ri)
and rhs(ri), respectively. For example, some xRs
961
rules may describe the transformation of does not
into ne ... pas in French. A particular instance may
look like this:
VP(AUX(does), RB(not), x0:VB) ? ne, x0, pas
lhs(ri) can be any arbitrary syntax tree fragment.
Its leaves are either lexicalized (e.g. does) or vari-
ables (x0, x1, etc). rhs(ri) is represented as a se-
quence of target-language words and variables.
Now we give a brief overview of how such
transformational rules are acquired automatically
in GHKM.1 In Figure 1, the (pi, f ,a) triple is rep-
resented as a directed graph G (edges going down-
ward), with no distinction between edges of pi and
alignments. Each node of the graph is labeled with
its span and complement span (the latter in italic
in the figure). The span of a node n is defined by
the indices of the first and last word in f that are
reachable from n. The complement span of n is
the union of the spans of all nodes n? in G that
are neither descendants nor ancestors of n. Nodes
of G whose spans and complement spans are non-
overlapping form the frontier set F ? G.
What is particularly interesting about the fron-
tier set? For any frontier of graph G containing
a given node n ? F , spans on that frontier de-
fine an ordering between n and each other frontier
node n?. For example, the span of VP[4-5] either
precedes or follows, but never overlaps the span of
any node n? on any graph frontier. This property
does not hold for nodes outside of F . For instance,
PP[4-5] and VBG[4] are two nodes of the same
graph frontier, but they cannot be ordered because
of their overlapping spans.
The purpose of xRs rules in this framework is
to order constituents along sensible frontiers in G,
and all frontiers containing undefined orderings,
as between PP[4-5] and VBG[4], must be disre-
garded during rule extraction. To ensure that xRs
rules are prevented from attempting to re-order
any such pair of constituents, these rules are de-
signed in such a way that variables in their lhs can
only match nodes of the frontier set. Rules that
satisfy this property are said to be induced by G.2
For example, rule (d) in Table 1 is valid accord-
ing to GHKM, since the spans corresponding to
1Note that we use a slightly different terminology.
2Specifically, an xRs rule ri is extracted fromG by taking
a subtree ? ? pi as lhs(ri), appending a variable to each
leaf node of ? that is internal to pi, adding those variables to
rhs(ri), ordering them in accordance to a, and if necessary
inserting any word of f to ensure that rhs(ri) is a sequence of
contiguous spans (e.g., [4-5][6][7-8] for rule (f) in Table 1).
DT
CD
VBP
NNS
IN
NNP
NP
NNS
VBG
3
2
2
1
7-8
4
4
5
9
1
2
3
4
5
6
7
8
9
3 1-2,4
-9
2 1-9
2 1-9
1 2-9
7-8 1-5,9
4 1-9
4 1-9
5 1-4,7
-9
9 1-8
1-2 3-9
NP 7-8 1-5,9
NP 5 1-4, 7
-9
PP 4-5 1-4,7
-9
VP 4-5 1-3,7
-9
NP 4-8 1-3,9
VP 3-8 1-2,9
S 1-9 ?
7!
"#
$
%&
'(
)
*+
,
.
Thes
e
peop
le
inclu
de
astro
naut
s
com
ing
from
Fran
ce
..
7
-
Figure 1: Spans and complement-spans determine what
rules are extracted. Constituents in gray are members of the
frontier set; a minimal rule is extracted from each of them.
(a) S(x0:NP, x1:VP, x2:.) ? x0, x1, x2
(b) NP(x0:DT, CD(7), NNS(people)) ? x0, 7?
(c) DT(these) ??
(d) VP(x0:VBP, x1:NP) ? x0, x1
(e) VBP(include) ?-?
(f) NP(x0:NP, x1:VP) ? x1,?, x0
(g) NP(x0:NNS) ? x0
(h) NNS(astronauts) ??*,X
(i) VP(VBG(coming), PP(IN(from), x0:NP)) ?e?, x0
(j) NP(x0:NNP) ? x0
(k) NNP(France) ???
(l) .(.) ? .
Table 1: A minimal derivation corresponding to Figure 1.
its rhs constituents (VBP[3] and NP[4-8]) do not
overlap. Conversely, NP(x0:DT, x1:CD:, x2:NNS)
is not the lhs of any rule extractible from G, since
its frontier constituents CD[2] and NNS[2] have
overlapping spans.3 Finally, the GHKM proce-
dure produces a single derivation from G, which
is shown in Table 1.
The concern in GHKM was to extract minimal
rules, whereas ours is to extract rules of any arbi-
trary size. Minimal rules defined over G are those
that cannot be decomposed into simpler rules in-
duced by the same graph G, e.g., all rules in Ta-
ble 1. We call minimal a derivation that only con-
tains minimal rules. Conversely, a composed rule
results from the composition of two or more min-
imal rules, e.g., rule (b) and (c) compose into:
NP(DT(these), CD(7), NNS(people)) ??, 7?
3It is generally reasonable to also require that the root n
of lhs(ri) be part of F , because no rule induced by G can
compose with ri at n, due to the restrictions imposed on the
extraction procedure, and ri wouldn?t be part of any valid
derivation.
962
OR
NP
(x0
:NP
, x1
:V
P) 
!
x1
,!
, x0
VP
(x0
:VB
P, 
x1:
NP
) 
!
x0
 , x
1
S(x
0:N
P, 
x1:
VP
, x
2:.
) 
!
x0
 , x
1, x
2
NP
(x0
:DT
 CD
(7)
, N
NS
(pe
opl
e))
 
!
x0
, 7"
.(.)
 
!
.
DT
(th
ese
) 
!
#
VB
P(i
ncl
ude
) 
!
$%
&
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
  
!
'(
, x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
NP
(x0
:NN
S) 
!
x0
NP
(x0
:NN
S) 
!
!,
 x0
NP
(x0
:NN
P) 
!
x0
, !
NN
P(F
ran
ce)
 
!
)*
NN
S(a
stro
nau
ts) 
!
+,
, -
OR
OR N
NS
(as
tro
nau
ts) 
!!
,+
,,
 -
OR
NP
(x0
:NN
P) 
!
x0
NP
(x0
:NN
P) 
!
x0
NN
P(F
ran
ce)
 
!
)*
, !
NP
(x0
:NN
S) 
!
x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
co
min
g
fro
m
NN
S
IN
NN
P
NP
VP
NP
VB
G
PP
NP
7-8
5
7-8
5
7-8
4
4
5
4
5
6
7
8
4
4
4-5
4-5
4-8
NN
P(F
ran
ce)
 
!)
*,
 !
NP
(x0
:NN
P) 
!
x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m)
, 
x0:
NP
))  
!
'(
, x0
, !
NN
S(a
stro
nau
ts) 
!
! ,
 +
,,
 -
NP
(x0
:NN
S) 
!
! ,
 x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, !
, x0
(a)
(b)
-
'(
)*
!
+,
as
tro
na
uts
Fra
nce
Figure 2: (a) Multiple ways of aligning? to constituents in the tree. (b) Derivation corresponding to the parse tree in Figure 1,
which takes into account all alignments of? pictured in (a).
Note that these properties are dependent on G, and
the above rule would be considered a minimal rule
in a graph G? similar to G, but additionally con-
taining a word alignment between 7 and ?. We
will see in Sections 3 and 5 why extracting only
minimal rules can be highly problematic.
2.2 Unaligned words
While the general theory presented in GHKM ac-
counts for any kind of derivation consistent with
G, it does not particularly discuss the case where
some words of the source-language string f are
not aligned to any word of e, thus disconnected
from the rest of the graph. This case is highly fre-
quent: 24.1% of Chinese words in our 179 mil-
lion word English-Chinese bilingual corpus are
unaligned, and 84.8% of Chinese sentences con-
tain at least one unaligned word. The question is
what to do with such lexical items, e.g., ? in
Figure 2(a). The approach of building one mini-
mal derivation for G as in the algorithm described
in GHKM assumes that we commit ourselves to
a particular heuristic to attach the unaligned item
to a certain constituent of pi, e.g., highest attach-
ment (in the example, ? is attached to NP[4-8]
and the heuristic generates rule (f)). A more rea-
sonable approach is to invoke the principle of in-
sufficient reason and make no a priori assump-
tion about what is a ?correct? way of assigning
the item to a constituent, and return all derivations
that are consistent with G. In Section 4, we will
see how to use corpus evidence to give preference
to unaligned-word attachments that are the most
consistent across the data. Figure 2(a) shows the
six possible ways of attaching ? to constituents
of pi: besides the highest attachment (rule (f)),?
can move along the ancestors of France, since it is
to the right of the translation of that word, and be
considered to be part of an NNP, NP, or VP rule.
We make the same reasoning to the left: ? can
either start the NNS of astronauts, or start an NP.
Our account of all possible ways of consistently
attaching ? to constituents means we must ex-
tract more than one derivation to explain transfor-
mations in G, even if we still restrict ourselves to
minimal derivations (a minimal derivation for G
is unique if and only if no source-language word
in G is unaligned). While we could enumerate
all derivations separately, it is much more effi-
cient both in time and space to represent them as a
derivation forest, as in Figure 2(b). Here, the for-
est covers all minimal derivations that correspond
to G. It is necessary to ensure that for each deriva-
tion, each unaligned item (here ?) appears only
once in the rules of that derivation, as shown in
Figure 2 (which satisfies the property). That re-
quirement will prove to be critical when we ad-
dress the problem of estimating probabilities for
our rules: if we allowed in our example to spuri-
ously generate?s in multiple successive steps of
the same derivation, we would not only represent
the transformation incorrectly, but also ?-rules
would be disproportionately represented, leading
to strongly biased estimates. We will now see how
to ensure this constraint is satisfied in our rule ex-
traction and derivation building algorithm.
963
2.3 Algorithm
The linear-time algorithm presented in GHKM is
only a particular case of the more general one we
describe here, which is used to extract all rules,
minimal and composed, induced by G. Similarly
to the GHKM algorithm, ours performs a top-
down traversal of G, but differs in the operations
it performs at each node n ? F : we must explore
all subtrees rooted at n, find all consistent ways
of attaching unaligned words of f, and build valid
derivations in accordance to these attachments.
We use a table or-dforest[x, y, c] to store OR-
nodes, in which each OR-node can be uniquely
defined by a syntactic category c and a span [x, y]
(which may cover unaligned words of f). This ta-
ble is used to prevent the same partial derivation
to be followed multiple times (the in-degrees of
OR-nodes generally become large with composed
rules). Furthermore, to avoid over-generating un-
aligned words, the root and variables in each rule
are represented with their spans. For example, in
Figure 2(b), the second and third child of the top-
most OR-node respectively span across [4-5][6-8]
and [4-6][7-8] (after constituent reordering). In
the former case, ? will eventually be realized in
an NP, and in the latter case, in a VP.
The preprocessing step consists of assigning
spans and complement spans to nodes of G, in
the first case by a bottom-up exploration of the
graph, and in the latter by a top-down traversal.
To assign complement spans, we assign the com-
plement span of any node n to each of its children,
and for each of them, add the span of the child
to the complement span of all other children. In
another traversal of G, we determine the minimal
rule extractible from each node in F .
We explore all tree fragments rooted at n by
maintaining an open and a closed queue of rules
extracted from n (qo and qc). At each step, we
pick the smallest rule in qo, and for each of its
variable nodes, try to discover new rules (?succes-
sor rules?) by means of composition with minimal
rules, until a given threshold on rule size or maxi-
mum number of rules in qc is reached. There may
be more that one successor per rule, since we must
account for all possible spans than can be assigned
to non-lexical leaves of a rule. Once a threshold is
reached, or if the open queue is empty, we connect
a new OR-node to all rules that have just been ex-
tracted from n, and add it to or-dforest. Finally,
we proceed recursively, and extract new rules from
each node at the frontier of the minimal rule rooted
at n. Once all nodes of F have been processed, the
or-dforest table contains a representation encod-
ing only valid derivations.
3 Probability models
The overall goal of our translation system is to
transform a given source-language sentence f
into an appropriate translation e in the set E
of all possible target-language sentences. In a
noisy-channel approach to SMT, we uses Bayes?
theorem and choose the English sentence e? ? E
that maximizes:4
e? = argmax
e?E
{
Pr(e) ? Pr(f |e)
}
(1)
Pr(e) is our language model, and Pr(f |e) our
translation model. In a grammatical approach to
MT, we hypothesize that syntactic information
can help produce good translation, and thus
introduce dependencies on target-language syntax
trees. The function to optimize becomes:
e? = argmax
e?E
{
Pr(e) ?
?
pi??(e)
Pr(f |pi) ?Pr(pi|e)
}
(2)
?(e) is the set of all English trees that yield the
given sentence e. Estimating Pr(pi|e) is a prob-
lem equivalent to syntactic parsing and thus is not
discussed here. Estimating Pr(f |pi) is the task of
syntax-based translation models (SBTM).
Given a rule set R, our SBTM makes the
common assumption that left-most compositions
of xRs rules ?i = r1 ? ... ? rn are independent
from one another in a given derivation ?i ? ?,
where ? is the set of all derivations constructible
from G = (pi, f ,a) using rules of R. Assuming
that ? is the set of all subtree decompositions of pi
corresponding to derivations in ?, we define the
estimate:
Pr(f |pi) =
1
|?|
?
?i??
?
rj??i
p(rhs(rj)|lhs(rj)) (3)
under the assumption:
?
rj?R:lhs(rj)=lhs(ri)
p(rhs(rj)|lhs(rj)) = 1 (4)
It is important to notice that the probability
distribution defined in Equation 3 requires a
normalization factor (|?|) in order to be tight, i.e.,
sum to 1 over all strings fi ? F that can be derived
4We denote general probability distributions with Pr(?)
and use p(?) for probabilities assigned by our models.
964
Xa
Y b
a?
b?
c?c
(!,f 1
,a 1):
X
a
Y b
b?
a?
c?c
(!,f 2
,a 2):
Figure 3: Example corpus.
from pi. A simple example suffices to demonstrate
it is not tight without normalization. Figure 3
contains a sample corpus from which four rules
can be extracted:
r1: X(a, Y(b, c)) ? a?, b?, c?
r2: X(a, Y(b, c)) ? b?, a?, c?
r3: X(a, x0:Y) ? a?, x0
r4: Y(b, c) ? b?, c?
From Equation 4, the probabilities of r3 and r4
must be 1, and those of r1 and r2 must sum to
1. Thus, the total probability mass, which is dis-
tributed across two possible output strings a?b?c?
and b?a?c?, is: p(a?b?c?|pi) + p(b?a?c?|pi) = p1 +
p3 ? p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri)).
It is relatively easy to prove that the probabil-
ities of all derivations that correspond to a given
decomposition ?i ? ? sum to 1 (the proof is omit-
ted due to constraints on space). From this prop-
erty we can immediately conclude that the model
described by Equation 3 is tight.5
We examine two estimates p(rhs(r)|lhs(r)).
The first one is the relative frequency estimator
conditioning on left hand sides:
p(rhs(r)|lhs(r)) =
f(r)
?
r?:lhs(r?)=lhs(r) f(r
?)
(5)
f(r) represents the number of times rule r oc-
curred in the derivations of the training corpus.
One of the major negative consequences of
extracting only minimal rules from a corpus is
that an estimator such as Equation 5 can become
extremely biased. This again can be observed
from Figure 3. In the minimal-rule extraction of
GHKM, only three rules are extracted from the ex-
ample corpus, i.e. rules r2, r3, and r4. Let?s as-
sume now that the triple (pi, f1,a1) is represented
99 times, and (pi, f2,a2) only once. Given a tree
pi, the model trained on that corpus can generate
the two strings a?b?c? and b?a?c? only through two
derivations, r3 ? r4 and r2, respectively. Since
all rules in that example have probability 1, and
5If each tree fragment in pi is the lhs of some rule in R,
then we have |?| = 2n, where n is the number of nodes of
the frontier set F ? G (each node is a binary choice point).
given that the normalization factor |?| is 2, both
probabilities p(a?b?c?|pi) and p(b?a?c?|pi) are 0.5.
On the other hand, if all rules are extracted and
incorporated into our relative-frequency probabil-
ity model, r1 seriously counterbalances r2 and the
probability of a?b?c? becomes: 12 ?(
99
100+1) = .995
(since it differs from .99, the estimator remains bi-
ased, but to a much lesser extent).
An alternative to the conditional model of
Equation 3 is to use a joint model conditioning on
the root node instead of the entire left hand side:
p(r|root(r)) =
f(r)
?
r?:root(r?)=root(r) f(r
?)
(6)
This can be particularly useful if no parser or
syntax-based language model is available, and we
need to rely on the translation model to penalize
ill-formed parse trees. Section 6 will describe an
empirical evaluation based on this estimate.
4 EM training
In our previous discussion of parameter estima-
tion, we did not explore the possibility that one
derivation in a forest may be much more plau-
sible than the others. If we knew which deriva-
tion in each forest was the ?true? derivation, then
we could straightforwardly collect rule counts off
those derivations. On the other hand, if we had
good rule probabilities, we could compute the
most likely (Viterbi) derivations for each training
example. This is a situation in which we can em-
ploy EM training, starting with uniform rule prob-
abilities. For each training example, we would like
to: (1) score each derivation ?i as a product of the
probabilities of the rules it contains, (2) compute
a conditional probability pi for each derivation ?i
(conditioned on the observed training pair) by nor-
malizing those scores to add to 1, and (3) collect
weighted counts for each rule in each ?i, where
the weight is pi. We can then normalize the counts
to get refined probabilities, and iterate; the corpus
likelihood is guaranteed to improve with each it-
eration. While it is infeasible to enumerate the
millions of derivations in each forest, Graehl and
Knight (2004) demonstrate an efficient algorithm.
They also analyze how to train arbitrary tree trans-
ducers into two steps. The first step is to build a
derivation forest for each training example, where
the forest contains those derivations licensed by
the (already supplied) transducer?s rules. The sec-
ond step employs EM on those derivation forests,
running in time proportional to the size of the
965
Best minimal-rule derivation (Cm) p(r)
(a) S(x0:NP-C x1:VP x2:.) ? x0 x1 x2 .845
(b) NP-C(x0:NPB) ? x0 .82
(c) NPB(DT(the) x0:NNS) ? x0 .507
(d) NNS(gunmen) ??K .559
(e) VP(VBD(were) x0:VP-C) ? x0 .434
(f) VP-C(x0:VBN x1:PP) ? x1 x0 .374
(g) PP(x0:IN x1:NP-C) ? x0 x1 .64
(h) IN(by) ?? .0067
(i) NP-C(x0:NPB) ? x0 .82
(j) NPB(DT(the) x0:NN) ? x0 .586
(k) NN(police) ?f? .0429
(l) VBN(killed) ??? .0072
(m) .(.) ? . .981
.
 
The
gunm
enw
ere
killed
by
the
polic
e.
DT
VBD
VBN
DT
NN
NP
PP
VP-C
VPS
NNS
IN
NP
.
!"
#$
%
&'
Best composed-rule derivation (C4) p(r)
(o) S(NP-C(NPB(DT(the) NNS(gunmen))) x0:VP .(.)) ??K x0 . 1
(p) VP(VBD(were) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ?? x1 x0 0.00724
(q) NP-C(NPB(DT(the) NN(police))) ?f? 0.173
(r) VBN(killed) ??? 0.00719
Figure 4: Two most probable derivations for the graph on the right: the top table restricted to minimal rules; the bottom one,
much more probable, using a large set of composed rules. Note: the derivations are constrained on the (pi, f ,a) triple, and thus
include some non-literal translations with relatively low probabilities (e.g. killed, which is more commonly translated as{?).
rule nb. of nb. of deriv- EM-
set rules nodes time time
Cm 4M 192M 2 h. 4 h.
C3 142M 1255M 52 h. 34 h.
C4 254M 2274M 134 h. 60 h.
Table 2: Rules and derivation nodes for a 54M-word, 1.95M
sentence pair English-Chinese corpus, and time to build
derivations (on 10 cluster nodes) and run 50 EM iterations.
forests. We only need to borrow the second step
for our present purposes, as we construct our own
derivation forests when we acquire our rule set.
A major challenge is to scale up this EM train-
ing to large data sets. We have been able to run
EM for 50 iterations on our Chinese-English 54-
million word corpus. The derivation forests for
this corpus contain 2.2 billion nodes; the largest
forest contains 1.1 million nodes. The outcome
is to assign probabilities to over 254 million rules.
Our EM runs with either lhs normalization or lhs-
root normalization. In the former case, each lhs
has an average of three corresponding rhs?s that
compete with each other for probability mass.
5 Model coverage
We now present some examples illustrating the
benefit of composed rules. We trained three
p(rhs(ri)|lhs(ri)) models on a 54 million-word
English-Chinese parallel corpus (Table 2): the first
one (Cm) with only minimal rules, and the two
others (C3 and C4) additionally considering com-
posed rules with no more than three, respectively
four, internal nodes in lhs(ri). We evaluated these
models on a section of the NIST 2002 evaluation
corpus, for which we built derivation forests and
lhs: S(x0:NP-C VP(x1:VBD x2:NP-C) x3:.)
corpus rhsi p(rhsi|lhs)
Chinese x1 x0 x2 x3 .3681
(minimal) x0 x1 , x3 x2 .0357
x2 , x0 x1 x3 .0287
x0 x1 , x3 x2 . .0267
Chinese x0 x1 x2 x3 .9047
(composed) x0 x1 , x2 x3 .016
x0 , x1 x2 x3 .0083
x0 x1 ? x2 x3 .0072
Arabic x1 x0 x2 x3 .5874
(composed) x0 x1 x2 x3 .4027
x1 x2 x0 x3 .0077
x1 x0 x2 " x3 .0001
Table 3: Our model transforms English subject-verb-object
(SVO) structures into Chinese SVO and into Arabic VSO.
With only minimal rules, Chinese VSO is wrongly preferred.
extracted the most probable one (Viterbi) for each
sentence pair (based on an automatic alignment
produced by GIZA). We noticed in general that
Viterbi derivations according to C4 make exten-
sive usage of composed rules, as it is the case in
the example in Figure 4. It shows the best deriva-
tion according to Cm and C4 on the unseen (pi,f,a)
triple displayed on the right. The second deriva-
tion (log p = ?11.6) is much more probable than
the minimal one (log p = ?17.7). In the case
of Cm, we can see that many small rules must be
applied to explain the transformation, and at each
step, the decision regarding the re-ordering of con-
stituents is made with little syntactic context. For
example, from the perspective of a decoder, the
word by is immediately transformed into a prepo-
sition (IN), but it is in general useful to know
which particular function word is present in the
sentence to motivate good re-orderings in the up-
966
lhs1: NP-C(x0:NPB PP(IN(of) x1:NP-C)) (NP-of-NP)
lhs2: PP(IN(of) NP-C(x0:NPB PP(IN(of) NP-C(x1:NPB x2:VP)))) (of-NP-of-NP-VP)
lhs3: VP(VBD(said) SBAR-C(IN(that) x0:S-C)) (said-that-S)
lhs4: SBAR(WHADVP(WRB(when)) S-C(x0:NP-C VP(VBP(are) x1:VP-C))) (when-NP-are-VP)
rhs1i p(rhs1i|lhs1) rhs2i p(rhs2i|lhs2) rhs3i p(rhs3i|lhs3) rhs4i p(rhs4i|lhs4)
x1 x0 .54 x2 ? x1 ? x0 .6754 ? , x0 .6062 ( x1 x0 ? .6618
x0 x1 .2351 ( x2 ? x1 ? x0 .035 ? x0 .1073 S x1 x0 ? .0724
x1 ? x0 .0334 x2 ? x1 ? x0 , .0263 h: , x0 .0591 ( x1 x0 ? , .0579
x1 x0 ? .026 x2 ? x1 ? x0 	 .0116 ? ? , x0 .0234 , ( x1 x0 ? .0289
Table 4: Translation probabilities promote linguistically motivated constituent re-orderings (for lhs1 and lhs2), and enable
non-constituent (lhs3) and non-contiguous (lhs4) phrasal translations.
per levels of the tree. A rule like (e) is particu-
larly unfortunate, since it allows the word were to
be added without any other evidence that the VP
should be in passive voice. On the other hand, the
composed-rule derivation of C4 incorporates more
linguistic evidence in its rules, and re-orderings
are motivated by more syntactic context. Rule
(p) is particularly appropriate to create a passive
VP construct, since it expects a Chinese passive
marker (?), an NP-C, and a verb in its rhs, and
creates the were ... by construction at once in the
left hand side.
5.1 Syntactic translation tables
We evaluate the promise of our SBTM by analyz-
ing instances of translation tables (t-table). Table 3
shows how a particular form of SVO construc-
tion is transformed into Chinese, which is also an
SVO language. While the t-table for Chinese com-
posed rules clearly gives good estimates for the
?correct? x0 x1 ordering (p = .9), i.e. subject be-
fore verb, the t-table for minimal rules unreason-
ably gives preference to verb-subject ordering (x1
x0, p = .37), because the most probable transfor-
mation (x0 x1) does not correspond to a minimal
rule. We obtain different results with Arabic, an
VSO language, and our model effectively learns
to move the subject after the verb (p = .59).
lhs1 in Table 4 shows that our model is able
to learn large-scale constituent re-orderings, such
as re-ordering NPs in a NP-of-NP construction,
and put the modifier first as it is more commonly
the case in Chinese (p = .54). If more syntac-
tic context is available as in lhs2, our model
provides much sharper estimates, and appropri-
ately reverses the order of three constituents with
high probability (p = .68), inserting modifiers first
(possessive markers? are needed here for better
syntactic disambiguation).
A limitation of earlier syntax-based systems is
their poor handling of non-constituent phrases.
Table 4 shows that our model can learn rules for
such phrases, e.g., said that (lhs3). While the that
has no direct translation, our model effectively
learns to separate? (said) from the relative clause
with a comma, which is common in Chinese.
Another promising prospect of our model seems
to lie in its ability to handle non-contiguous
phrases, a feature that state of the art systems
such as (Och and Ney, 2004) do not incorpo-
rate. The when-NP-are-VP construction of lhs4
presents such a case. Our model identifies that are
needs to be deleted, that when translates into the
phrase( ...?, and that the NP needs to be moved
after the VP in Chinese (p = .66).
6 Empirical evaluation
The task of our decoder is to find the most likely
English tree pi that maximizes all models involved
in Equation 2. Since xRs rules can be converted to
context-free productions by increasing the number
of non-terminals, we implemented our decoder as
a standard CKY parser with beam search. Its rule
binarization is described in (Zhang et al, 2006).
We compare our syntax-based system against
an implementation of the alignment template
(AlTemp) approach to MT (Och and Ney, 2004),
which is widely considered to represent the state
of the art in the field. We registered both systems
in the NIST 2005 evaluation; results are presented
in Table 5. With a difference of 6.4 BLEU points
for both language pairs, we consider the results
of our syntax-based system particularly promis-
ing, since these are the highest scores to date that
we know of using linguistic syntactic transforma-
tions. Also, on the one hand, our AlTemp sys-
tem represents quite mature technology, and in-
corporates highly tuned model parameters. On
the other hand, our syntax decoder is still work in
progress: only one model was used during search,
i.e., the EM-trained root-normalized SBTM, and
as yet no language model is incorporated in the
search (whereas the search in the AlTemp sys-
tem uses two phrase-based translation models and
967
Syntactic AlTemp
Arabic-to-English 40.2 46.6
Chinese-to-English 24.3 30.7
Table 5: BLEU-4 scores for the 2005 NIST test set.
Cm C3 C4
Chinese-to-English 24.47 27.42 28.1
Table 6: BLEU-4 scores for the 2002 NIST test set, with rules
of increasing sizes.
12 other feature functions). Furthermore, our de-
coder doesn?t incorporate any syntax-based lan-
guage model, and admittedly our ability to penal-
ize ill-formed parse trees is still limited.
Finally, we evaluated our system on the NIST-
02 test set with the three different rule sets (see
Table 6). The performance with our largest rule
set represents a 3.63 BLEU point increase (14.8%
relative) compared to using only minimal rules,
which indicates positive prospects for using even
larger rules. While our rule inference algorithm
scales to higher thresholds, one important area of
future work will be the improvement of our de-
coder, conjointly with analyses of the impact in
terms of BLEU of contextually richer rules.
7 Related work
Similarly to (Poutsma, 2000; Wu, 1997; Yamada
and Knight, 2001; Chiang, 2005), the rules dis-
cussed in this paper are equivalent to productions
of synchronous tree substitution grammars. We
believe that our tree-to-string model has several
advantages over tree-to-tree transformations such
as the ones acquired by Poutsma (2000). While
tree-to-tree grammars are richer formalisms that
provide the potential benefit of rules that are lin-
guistically better motivated, modeling the syntax
of both languages comes as an extra cost, and it
is admittedly more helpful to focus our syntac-
tic modeling effort on the target language (e.g.,
English) in cases where it has syntactic resources
(parsers and treebanks) that are considerably more
available than for the source language. Further-
more, we think there is, overall, less benefit in
modeling the syntax of the source language, since
the input sentence is fixed during decoding and is
generally already grammatical.
With the notable exception of Poutsma, most
related works rely on models that are restricted
to synchronous context-free grammars (SCFG).
While the state-of-the-art hierarchical SMT sys-
tem (Chiang, 2005) performs well despite strin-
gent constraints imposed on its context-free gram-
mar, we believe its main advantage lies in its
ability to extract hierarchical rules across phrasal
boundaries. Context-free grammars (such as Penn
Treebank and Chiang?s grammars) make indepen-
dence assumptions that are arguably often unrea-
sonable, but as our work suggests, relaxations
of these assumptions by using contextually richer
rules results in translations of increasing quality.
We believe it will be beneficial to account for this
finding in future work in syntax-based SMT and in
efforts to improve upon (Chiang, 2005).
8 Conclusions
In this paper, we developed probability models for
the multi-level transfer rules presented in (Galley
et al, 2004), showed how to acquire larger rules
that crucially condition on more syntactic context,
and how to pack multiple derivations, including
interpretations of unaligned words, into derivation
forests. We presented some theoretical arguments
for not limiting extraction to minimal rules, val-
idated them on concrete examples, and presented
experiments showing that contextually richer rules
provide a 3.63 BLEU point increase over the min-
imal rules of (Galley et al, 2004).
Acknowledgments
We would like to thank anonymous review-
ers for their helpful comments and suggestions.
This work was partially supported under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
H. Fox. 2002. Phrasal cohesion and statistical machine trans-
lation. In Proc. of EMNLP, pages 304?311.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In Proc. of HLT/NAACL-04.
J. Graehl and K. Knight. 2004. Training tree transducers. In
Proc. of HLT/NAACL-04, pages 105?112.
F. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
A. Poutsma. 2000. Data-oriented translation. In Proc. of
COLING, pages 635?641.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proc. of ACL, pages 523?530.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Syn-
chronous binarization for machine translation. In Proc. of
HLT/NAACL.
968
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 940?948,
Beijing, August 2010
Fast, Greedy Model Minimization for Unsupervised Tagging
Sujith Ravi and Ashish Vaswani and Kevin Knight and David Chiang
University of Southern California
Information Sciences Institute
{sravi,avaswani,knight,chiang}@isi.edu
Abstract
Model minimization has been shown to
work well for the task of unsupervised
part-of-speech tagging with a dictionary.
In (Ravi and Knight, 2009), the authors in-
voke an integer programming (IP) solver
to do model minimization. However,
solving this problem exactly using an
integer programming formulation is in-
tractable for practical purposes. We pro-
pose a novel two-stage greedy approxima-
tion scheme to replace the IP. Our method
runs fast, while yielding highly accurate
tagging results. We also compare our
method against standard EM training, and
show that we consistently obtain better
tagging accuracies on test data of varying
sizes for English and Italian.
1 Introduction
The task of unsupervised part-of-speech (POS)
tagging with a dictionary as formulated by Meri-
aldo (1994) is: given a raw word sequence and a
dictionary of legal POS tags for each word type,
tag each word token in the text. A common ap-
proach to modeling such sequence labeling prob-
lems is to build a bigram Hidden Markov Model
(HMM) parameterized by tag-bigram transition
probabilities P (ti|ti?1) and word-tag emission
probabilities P (wi|ti). Given a word sequence w
and a tag sequence t, of length N , the joint prob-
ability P (w, t) is given by:
P (w, t) =
N?
i=1
P (wi|ti) ? P (ti|ti?1) (1)
We can train this model using the Expectation
Maximization (EM) algorithm (Dempster and Ru-
bin, 1977) which learns P (wi|ti) and P (ti|ti?1)
that maximize the likelihood of the observed data.
Once the parameters are learnt, we can find the
best tagging using the Viterbi algorithm.
t? = arg max
t
P (w, t) (2)
Ravi and Knight (2009) attack the Merialdo
task in two stages. In the first stage, they search
for a minimized transition model (i.e., the small-
est set of tag bigrams) that can explain the data
using an integer programming (IP) formulation.
In the second stage, they build a smaller HMM
by restricting the transition parameters to only
those tag bigrams selected in the minimization
step. They employ the EM algorithm to train this
model, which prunes away some of the emission
parameters. Next, they use the pruned emission
model along with the original transition model
(which uses the full set of tag bigrams) and re-
train using EM. This alternating EM training pro-
cedure is repeated until the number of tag bigrams
in the Viterbi tagging output does not change be-
tween subsequent iterations. The final Viterbi tag-
ging output from their method achieves state-of-
the-art accuracy for this task. However, their mini-
mization step involves solving an integer program,
which can be very slow, especially when scal-
ing to large-scale data and more complex tagging
problems which use bigger tagsets. In this pa-
per, we present a novel method that optimizes the
same objective function using a fast greedy model
selection strategy. Our contributions are summa-
rized below:
940
? We present an efficient two-phase greedy-
selection method for solving the minimiza-
tion objective from Ravi and Knight (2009),
which runs much faster than their IP.
? Our method easily scales to large data
sizes (and big tagsets), unlike the previ-
ous minimization-based approaches and we
show runtime comparisons for different data
sizes.
? We achieve very high tagging accuracies
comparable to state-of-the-art results for un-
supervised POS tagging for English.
? Unlike previous approaches, we also show
results obtained when testing on the entire
Penn Treebank data (973k word tokens) in
addition to the standard 24k test data used for
this task. We also show the effectiveness of
this approach for Italian POS tagging.
2 Previous work
There has been much work on the unsupervised
part-of-speech tagging problem. Goldwater and
Griffiths (2007) also learn small models employ-
ing a fully Bayesian approach with sparse pri-
ors. They report 86.8% tagging accuracy with
manual hyperparameter selection. Smith and Eis-
ner (2005) design a contrastive estimation tech-
nique which yields a higher accuracy of 88.6%.
Goldberg et al (2008) use linguistic knowledge to
initialize the the parameters of the HMM model
prior to EM training. They achieve 91.4% ac-
curacy. Ravi and Knight (2009) use a Minimum
Description Length (MDL) method and achieve
the best results on this task thus far (91.6% word
token accuracy, 91.8% with random restarts for
EM). Our work follows a similar approach using a
model minimization component and alternate EM
training.
Recently, the integer programming framework
has been widely adopted by researchers to solve
other NLP tasks besides POS tagging such as se-
mantic role labeling (Punyakanok et al, 2004),
sentence compression (Clarke and Lapata, 2008),
decipherment (Ravi and Knight, 2008) and depen-
dency parsing (Martins et al, 2009).
3 Model minimization formulated as a
Path Problem
The complexity of the model minimization step
in (Ravi and Knight, 2009) and its proposed ap-
proximate solution can be best understood if we
formulate it as a path problem in a graph.
Let w = w0, w1, . . . , wN , wN+1 be a word se-
quence where w1, . . . , wN are the input word to-
kens and {w0, wN+1} are the start/end tokens.
Let T = {T1, . . . , TK}?{T0, TK+1} be the fixed
set of all possible tags. T0 and TK+1 are special
tags that we add for convenience. These would be
the start and end tags that one typically adds to
the HMM lattice. The tag dictionary D contains
entries of the form (wi, Tj) for all the possible
tags Tj that word token wi can have. We add en-
tries (w0, T0) and (wK+1, TK+1) to D. Given this
input, we now create a directed graph G(V,E).
Let C0, C1 . . . , CK+1 be columns of nodes in G,
where column Ci corresponds to word token wi.
For all i = 0, . . . , N+1 and j = 0, . . . ,K+1, we
add node Ci,j in column Ci if (wi, Tj) ? D. Now,
?i = 0, . . . , N , we create directed edges from ev-
ery node in Ci to every node in Ci+1. Each of
these edges e = (Ci,j , Ci+1,k) is given the label
(Tj , Tk) which corresponds to a tag bigram. This
creates our directed graph. Let l(e) be the tag bi-
gram label of edges e ? E. For every path P from
C0,0 to CN+1,K+1, we say that P uses an edge la-
bel or tag bigram (Tj , Tk) if there exists an edge
e in P such that l(e) = (Tj , Tk). We can now
formulate the the optimization problem as: Find
the smallest set S of tag bigrams such that there
exists at least one path from C0,0 to CN+1,K+1 us-
ing only the tag bigrams in S. Let us call this the
Minimal Tag Bigram Path (MinTagPath) problem.
Figure 1 shows an example graph where the
input word sequence is w1, . . . , w4 and T =
{T1, . . . , T3} is the input tagset. We add the
start/end word tokens {w0, w5} and correspond-
ing tags {T0, T4}. The edges in the graph are in-
stantiated according to the word/tag dictionary D
provided as input. The node and edge labels are
also illustrated in the graph. Our goal is to find a
path from C0,0 to C5,4 using the smallest set of tag
bigrams.
941
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
0
,T
3
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
2
T
3
,T
2
T
3
,T
4
T
2
,T
4
T
2
,T
3
T
2
,T
2
T
1
,T
3
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
Initial graph: G (V, E)
Figure 1: Graph instantiation for the MinTagPath problem.
4 Problem complexity
Having defined the problem, we now show that
it can be solved in polynomial time even though
the number of paths from C0,0 to CN+1,K+1 is
exponential in N , the input size. This relies on the
assumption that the tagset T is fixed in advance,
which is the case for most tagging tasks.1 Let B
be the set of all the tag bigram labels in the graph,
B = {l(e), ?e ? E}. Now, the size of B would
be at most K2 + 2K where every word could be
tagged with every possible tag. For m = 1 . . . |B|,
let Bm be the set of subsets of B each of which
have size m. Algorithm 1 optimally solves the
MinTagPath problem.
Algorithm 1 basically enumerates all the possi-
ble subsets of B, from the smallest to the largest,
and checks if there is a path. It exits the first time a
path is found and therefore finds the smallest pos-
sible set si of size m such that a path exists that
uses only the tag bigrams in si. This implies the
correctness of the algorithm. To check for path ex-
istence, we could either throw away all the edges
from E not having a label in si, and then execute
a Breadth-First-Search (BFS) or we could traverse
1If K, the size of the tagset, is a variable as well, then we
suspect the problem is NP-hard.
Algorithm 1 Brute Force solution to MinTagPath
for m = 1 to |B| do
for si ? Bm do
Use Breadth First Search (BFS) to check
if ? path P from C0,0 to CN+1,K+1 using
only the tag bigrams in si.
if P exists then
return si,m
end if
end for
end for
only the edges with labels in si during BFS. The
running time of Algorithm 1 is easy to calculate.
Since, in the worst case we go over all the sub-
sets of size m = 1, . . . , |B| of B, the number of
iterations we can perform is at most 2|B|, the size
of the powerset P of B. In each iteration, we do
a BFS through the lattice, which has O(N) time
complexity2 since the lattice size is linear in N
and BFS is linear in the lattice size. Hence the run-
ning time is? 2|B| ?O(N) = O(N). Even though
this shows that MinTagPath can be solved in poly-
nomial time, the time complexity is prohibitively
large. For the Penn Treebank, K = 45 and the
2Including throwing away edges or not.
942
worst case running time would be ? 1013.55 ? N .
Clearly, for all practical purposes, this approach is
intractable.
5 Greedy Model Minimization
We do not know of an efficient, exact algorithm
to solve the MinTagPath problem. Therefore, we
present a simple and fast two-stage greedy ap-
proximation scheme. Notice that an optimal path
P (or any path) covers all the input words i.e., ev-
ery word token wi has one of its possible taggings
in P . Exploiting this property, in the first phase,
we set our goal to cover all the word tokens using
the least possible number of tag bigrams. This can
be cast as a set cover problem (Garey and John-
son, 1979) and we use the set cover greedy ap-
proximation algorithm in this stage. The output
tag bigrams from this phase might still not allow
any path from C0,0 to CN+1,K+1. So we carry out
a second phase, where we greedily add a few tag
bigrams until a path is created.
5.1 Phase 1: Greedy Set Cover
In this phase, our goal is to cover all the word to-
kens using the least number of tag bigrams. The
covering problem is exactly that of set cover. Let
U = {w0, . . . , wN +1} be the set of elements that
needs to be covered (in this case, the word tokens).
For each tag bigram (Ti, Tj) ? B, we define its
corresponding covering set STi,Tj as follows:
STi,Tj = {wn : ((wn, Ti) ? D
? (Cn,i, Cn+1,j) ? E
? l(Cn,i, Cn+1,j) = (Ti, Tj))?
((wn, Tj) ? D
? (Cn?1,i, Cn,j) ? E
? l(Cn?1,i, Cn,j) = (Ti, Tj))}
Let the set of covering sets be X . We assign
a cost of 1 to each covering set in X . The goal
is to select a set CHOSEN ? X such that?
STi,Tj?CHOSEN
= U , minimizing the total cost
of CHOSEN . This corresponds to covering all
the words with the least possible number of tag
bigrams. We now use the greedy approximation
algorithm for set cover to solve this problem. The
pseudo code is shown in Algorithm 2.
Algorithm 2 Set Cover : Phase 1
Definitions
Define CAND : Set of candidate covering sets
in the current iteration
Define Urem : Number of elements in U re-
maining to be covered
Define ESTi,Tj : Current effective cost of a setDefine Itr : Iteration number
Initializations
LET CAND = X
LET CHOSEN = ?
LET Urem = U
LET Itr = 0
LET ESTi,Tj = 1|STi,Tj | , ? STi,Tj ? CAND
while Urem 6= ? do
Itr ? Itr + 1
Define S?Itr = argmin
STi,Tj?CAND
ESTi,Tj
CHOSEN = CHOSEN
?
S?Itr
Remove S?Itr from CAND
Remove all the current elements in S?Itr from
Urem
Remove all the current elements in S?Itr from
every STi,Tj ? CAND
Update effective costs, ? STi,Tj ? CAND,
ESTi,Tj =
1
|STi,Tj |end while
return CHOSEN
For the graph shown in Figure 1, here are a few
possible covering sets STi,Tj and their initial ef-
fective costs ESTi,Tj .
? ST0,T1 = {w0, w1}, EST0,T1 = 1/2
? ST1,T2 = {w1, w2, w3, w4}, EST1,T2 = 1/4
? ST2,T2 = {w2, w3, w4}, EST2,T2 = 1/3
In every iteration Itr of Algorithm 2, we pick a
set S?Itr that is most cost effective. The elements
that S?Itr covers are then removed from all the re-
maining candidate sets and Urem and the effec-
tiveness of the candidate sets is recalculated for
the next iteration. The algorithm stops when all
elements of U i.e., all the word tokens are cov-
ered. Let, BCHOSEN = {(Ti, Tj) : STi,Tj ?
943
CHOSEN}, be the set of tag bigrams that have
been chosen by set cover. Now, we check, using
BFS, if there exists a path from C0,0 to CN+1,K+1
using only the tag bigrams in BCHOSEN . If not,
then we have to add tag bigrams to BCHOSEN to
enable a path. To accomplish this, we carry out the
second phase of this scheme with another greedy
strategy (described in the next section).
For the example graph in Figure 1,
one possible solution BCHOSEN =
{(T0, T1), (T1, T2), (T2, T4)}.
5.2 Phase 2: Greedy Path Completion
We define a graph GCHOSEN (V ?, E?) ?
G(V,E) that contains the edges e ? E such
l(e) ? BCHOSEN .
Let BCAND = B \ BCHOSEN , be the current
set of candidate tag bigrams that can be added to
the final solution which would create a path. We
would like to know how many holes a particular
tag bigram (Ti, Tj) can fill. We define a hole as an
edge e such that e ? G \ GCHOSEN and there
exists e?, e?? ? GCHOSEN such that tail(e?) =
head(e) ? tail(e) = head(e??).
Figure 2 illustrates the graph GCHOSEN using
tag bigrams from the example solution to Phase 1
(Section 5.1). The dotted edge (C2,2, C3,1) rep-
resents a hole, which has to be filled in the cur-
rent phase in order to complete a path from C0,0
to C5,4.
In Algorithm 3, we define the effectiveness of a
candidate tag bigram H(Ti, Tj) to be the number
of holes it covers. In every iteration, we pick the
most effective tag bigram, fill the holes and recal-
culate the effectiveness of the remaining candidate
tag bigrams.
Algorithm 3 returns BFINAL, the final set of
chosen tag bigrams. It terminates when a path has
been found.
5.3 Fitting the Model
Once the greedy algorithm terminates and returns
a minimized grammar of tag bigrams, we follow
the approach of Ravi and Knight (2009) and fit
the minimized model to the data using the alter-
nating EM strategy. The alternating EM iterations
are terminated when the change in the size of the
observed grammar (i.e., the number of unique tag
Algorithm 3 Greedy Path Complete : Phase 2
Define BFINAL : Final set of tag bigrams se-
lected by the two-phase greedy approach
LET BFINAL = BCHOSEN
LET H(Ti, Tj) = |{e}| such that l(e) =
(Ti, Tj) and e is a hole, ? (Ti, Tj) ? BCAND
while @ path P from C0,0 to CN+1,K+1 using
only (Ti, Tj) ? BCHOSEN do
Define (T?i, T?j) = argmax
(Ti,Tj)?BCAND
H(Ti, Tj)
BFINAL = BFINAL
?
(T?i, T?j)
Remove (T?i, T?j) from BCAND
GCHOSEN = GCHOSEN
?{e} such that
l(e) = (Ti, Tj)
? (Ti, Tj) ? BCAND, Recalculate H(Ti, Tj)
end while
return BFINAL
bigrams in the tagging output) is ? 5%. We refer
to our entire approach using greedy minimization
followed by EM training as MIN-GREEDY.
6 Experiments and Results
6.1 English POS Tagging
Data: We use a standard test set (consisting of
24,115 word tokens from the Penn Treebank) for
the POS tagging task (described in Section 1). The
tagset consists of 45 distinct tag labels and the
dictionary contains 57,388 word/tag pairs derived
from the entire Penn Treebank. Per-token ambi-
guity for the test data is about 1.5 tags/token. In
addition to the standard 24k dataset, we also train
and test on larger data sets of 48k, 96k, 193k, and
the entire Penn Treebank (973k).
Methods: We perform comparative evaluations
for POS tagging using three different methods:
1. EM: Training a bigram HMM model using
EM algorithm.
2. IP: Minimizing grammar size using inte-
ger programming, followed by EM training
(Ravi and Knight, 2009).
3. MIN-GREEDY: Minimizing grammar size
using the Greedy method described in Sec-
944
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
4
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
 T
0
,T
1
 T
1
,T
2
 T
2
,T
4
Tag bigrams chosen after Phase 1 
(B
CHOSEN
)
Hole in graph: Edge e = (C
2,2
, C
3,1
)
Graph after Phase 1: G
CHOSEN 
(V?, E?)
Figure 2: Graph constructed with tag bigrams chosen in Phase 1 of the MIN-GREEDY method.
tion 5, followed by EM training.
Results: Figure 3 shows the tagging perfor-
mance (word token accuracy %) achieved by the
three methods on the standard test (24k tokens) as
well as Penn Treebank test (PTB = 973k tokens).
On the 24k test data, the MIN-GREEDY method
achieves a high tagging accuracy comparable to
the previous best from the IP method. However,
the IP method does not scale well which makes
it infeasible to run this method in a much larger
data setting (the entire Penn Treebank). MIN-
GREEDY on the other hand, faces no such prob-
lem and in fact it achieves high tagging accuracies
on all four datasets, consistently beating EM by
significant margins. When tagging all the 973k
word tokens in the Penn Treebank data, it pro-
duces an accuracy of 87.1% which is much better
than EM (82.3%) run on the same data.
Ravi and Knight (2009) mention that it is pos-
sible to interrupt the IP solver and obtain a sub-
optimal solution faster. However, the IP solver did
not return any solution when provided the same
amount of time as taken by MIN-GREEDY for
any of the data settings. Also, our algorithms
were implemented in Python while the IP method
employs the best available commercial software
package (CPLEX) for solving integer programs.
Figure 4 compares the running time efficiency
for the IP method versus MIN-GREEDY method
Test set Efficiency
(average running time in secs.)
IP MIN-GREEDY
24k test 93.0 34.3
48k test 111.7 64.3
96k test 397.8 93.3
193k test 2347.0 331.0
PTB (973k) test ? 1485.0
Figure 4: Comparison of MIN-GREEDY versus
MIN-GREEDY approach in terms of efficiency
(average running time in seconds) for different
data sizes. All the experiments were run on a sin-
gle machine with a 64-bit, 2.4 GHz AMD Opteron
850 processor.
as we scale to larger datasets. Since the IP solver
shows variations in running times for different
datasets of the same size, we show the average
running times for both methods (for each row in
the figure, we run a particular method on three
different datasets with similar sizes and average
the running times). The figure shows that the
greedy approach can scale comfortably to large
data sizes, and a complete run on the entire Penn
Treebank data finishes in just 1485 seconds. In
contrast, the IP method does not scale well?on
average, it takes 93 seconds to finish on the 24k
test (versus 34 seconds for MIN-GREEDY) and
on the larger PTB test data, the IP solver runs for
945
Method Tagging accuracy (%)
when training & testing on:
24k 48k 96k 193k PTB (973k)
EM 81.7 81.4 82.8 82.0 82.3
IP 91.6 89.3 89.5 91.6 ?
MIN-GREEDY 91.6 88.9 89.4 89.1 87.1
Figure 3: Comparison of tagging accuracies on test data of varying sizes for the task of unsupervised
English POS tagging with a dictionary using a 45-tagset. (? IP method does not scale to large data).
 400
 600
 800
 1000
 1200
 1400
 1600
Ob
se
rv
ed
 g
ra
mm
ar
 s
iz
e 
(#
 o
f 
ta
g 
bi
gr
am
s)
 
 i
n 
fi
na
l 
ta
gg
in
g 
ou
tp
ut
Size of test data (# of word tokens)
24k 48k 96k 193k PTB (973k)
EM
IP
Greedy
Figure 5: Comparison of observed grammar size
(# of tag bigram types) in the final tagging output
from EM, IP and MIN-GREEDY.
more than 3 hours without returning a solution.
It is interesting to see that for the 24k dataset,
the greedy strategy finds a grammar set (contain-
ing only 478 tag bigrams). We observe that MIN-
GREEDY produces 452 tag bigrams in the first
minimization step (phase 1), and phase 2 adds an-
other 26 entries, yielding a total of 478 tag bi-
grams in the final minimized grammar set. That
is almost as good as the optimal solution (459
tag bigrams from IP) for the same problem. But
MIN-GREEDY clearly has an advantage since it
runs much faster than IP (as shown in Figure 4).
Figure 5 shows a plot with the size of the ob-
served grammar (i.e., number of tag bigram types
in the final tagging output) versus the size of the
test data for EM, IP and MIN-GREEDY methods.
The figure shows that unlike EM, the other two
approaches reduce the grammar size considerably
and we observe the same trend even when scaling
Test set Average Speedup Optimality Ratio
24k test 2.7 0.96
48k test 1.7 0.98
96k test 4.3 0.98
193k test 7.1 0.93
Figure 6: Average speedup versus Optimality ra-
tio computed for the model minimization step
(when using MIN-GREEDY over IP) on different
datasets.
to larger data. Minimizing the grammar size helps
remove many spurious tag combinations from the
grammar set, thereby yielding huge improvements
in tagging accuracy over the EM method (Fig-
ure 3). We observe that for the 193k dataset, the
final observed grammar size is greater for IP than
MIN-GREEDY. This is because the alternating
EM steps following the model minimization step
add more tag bigrams to the grammar.
We compute the optimality ratio of the MIN-
GREEDY approach with respect to the grammar
size as follows:
Optimality ratio = Size of IP grammarSize of MIN-GREEDY grammar
A value of 1 for this ratio implies that the solu-
tion found by MIN-GREEDY algorithm is exact.
Figure 6 compares the optimality ratio versus av-
erage speedup (in terms of running time) achieved
in the minimization step for the two approaches.
The figure illustrates that our solution is nearly op-
timal for all data settings with significant speedup.
The MIN-GREEDY algorithm presented here
can also be applied to scenarios where the dictio-
nary is incomplete (i.e., entries for all word types
are not present in the dictionary) and rare words
946
Method Tagging accuracy (%) Number of unique tag bigrams in final tagging output
EM 83.4 1195
IP 88.0 875
MIN-GREEDY 88.0 880
Figure 7: Results for unsupervised Italian POS tagging with a dictionary using a set of 90 tags.
take on all tag labels. In such cases, we can fol-
low a similar approach as Ravi and Knight (2009)
to assign tag possibilities to every unknown word
using information from the known word/tag pairs
present in the dictionary. Once the completed dic-
tionary is available, we can use the procedure de-
scribed in Section 5 to minimize the size of the
grammar, followed by EM training.
6.2 Italian POS Tagging
We also compare the three approaches for Italian
POS tagging and show results.
Data: We use the Italian CCG-TUT corpus (Bos
et al, 2009), which contains 1837 sentences. It
has three sections: newspaper texts, civil code
texts and European law texts from the JRC-Acquis
Multilingual Parallel Corpus. For our experi-
ments, we use the POS-tagged data from the
CCG-TUT corpus, which uses a set of 90 tags.
We created a tag dictionary consisting of 8,733
word/tag pairs derived from the entire corpus
(42,100 word tokens). We then created a test set
consisting of 926 sentences (21,878 word tokens)
from the original corpus. The per-token ambiguity
for the test data is about 1.6 tags/token.
Results: Figure 7 shows the results on Italian
POS tagging. We observe that MIN-GREEDY
achieves significant improvements in tagging ac-
curacy over the EM method and comparable to IP
method. This also shows that the idea of model
minimization is a general-purpose technique for
such applications and provides good tagging ac-
curacies on other languages as well.
7 Conclusion
We present a fast and efficient two-stage greedy
minimization approach that can replace the inte-
ger programming step in (Ravi and Knight, 2009).
The greedy approach finds close-to-optimal solu-
tions for the minimization problem. Our algo-
rithm runs much faster and achieves accuracies
close to state-of-the-art. We also evaluate our
method on test sets of varying sizes and show that
our approach outperforms standard EM by a sig-
nificant margin. For future work, we would like
to incorporate some linguistic constraints within
the greedy method. For example, we can assign
higher costs to unlikely tag combinations (such as
?SYM SYM?, etc.).
Our greedy method can also be used for solving
other unsupervised tasks where model minimiza-
tion using integer programming has proven suc-
cessful, such as word alignment (Bodrumlu et al,
2009).
Acknowledgments
The authors would like to thank Shang-Hua Teng
and Anup Rao for their helpful comments and
also the anonymous reviewers. This work was
jointly supported by NSF grant IIS-0904684,
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
References
Bodrumlu, T., K. Knight, and S. Ravi. 2009. A new
objective function for word alignment. In Proceed-
ings of the NAACL/HLT Workshop on Integer Pro-
gramming for Natural Language Processing.
Bos, J., C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8).
Clarke, J. and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence
Research (JAIR), 31(4):399?429.
Dempster, A.P., N.M. Laird and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
947
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
Garey, M. R. and D. S. Johnson. 1979. Computers
and Intractability: A Guide to the Theory of NP-
Completeness. John Wiley & Sons.
Goldberg, Y., M. Adler, and M. Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL/HLT).
Goldwater, Sharon and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Martins, A., N. A. Smith, and E. P. Xing. 2009. Con-
cise integer linear programming formulations for
dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics (ACL) and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP.
Merialdo, B. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
Punyakanok, V., D. Roth, W. Yih, and D. Zimak.
2004. Semantic role labeling via integer linear pro-
gramming inference. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
Ravi, S. and K. Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram
models. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Ravi, S. and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics (ACL) and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Smith, N. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL).
948
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 524?533,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Analysis of Rhythmic Poetry
with Applications to Generation and Translation
Erica Greene
Haverford College
370 Lancaster Ave.
Haverford, PA 19041
ericagreene@gmail.com
Tugba Bodrumlu
Dept. of Computer Science
Univ. of Southern California
Los Angeles, CA 90089
bodrumlu@cs.usc.edu
Kevin Knight
Information Sciences Institute
Univ. of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292
knight@isi.edu
Abstract
We employ statistical methods to analyze,
generate, and translate rhythmic poetry. We
first apply unsupervised learning to reveal
word-stress patterns in a corpus of raw poetry.
We then use these word-stress patterns, in ad-
dition to rhyme and discourse models, to gen-
erate English love poetry. Finally, we trans-
late Italian poetry into English, choosing tar-
get realizations that conform to desired rhyth-
mic patterns.
1 Introduction
When it comes to generating creative language (po-
ems, stories, jokes, etc), people have massive advan-
tages over machines:
? people can construct grammatical, sensible ut-
terances,
? people have a wide range of topics to talk
about, and
? people experience joy and heart-break.
On the other hand, machines have some minor ad-
vantages:
? a machine can easily come up with a five-
syllable word that starts with p and rhymes
with early, and
? a machine can analyze very large online text
repositories of human works and maintain
these in memory.
In this paper we concentrate on statistical methods
applied to the analysis, generation, and translation
of poetry. By analysis, we mean extracting patterns
from existing online poetry corpora. We use these
patterns to generate new poems and translate exist-
ing poems. When translating, we render target text
in a rhythmic scheme determined by the user.
Poetry generation has received research attention
in the past (Manurung et al, 2000; Gervas, 2001;
Diaz-Agudo et al, 2002; Manurung, 2003; Wong
and Chun, 2008; Tosa et al, 2008; Jiang and Zhou,
2008; Netzer et al, 2009), including the use of
statistical methods, although there is a long way
to go. One difficulty has been the evaluation of
machine-generated poetry?this continues to be a
difficulty in the present paper. Less research effort
has been spent on poetry analysis and poetry trans-
lation, which we tackle here.
2 Terms
Meter refers to the rhythmic beat of poetic text when
read aloud. Iambic is a common meter that sounds
like da-DUM da-DUM da-DUM, etc. Each da-
DUM is called a foot. Anapest meter sounds like
da-da-DUM da-da-DUM da-da-DUM, etc.
Trimeter refers to a line with three feet, pentame-
ter to a line with five feet, etc. Examples include:
? a VE-ry NAS-ty CUT (iambic trimeter)
? shall I com-PARE thee TO a SUM-mer?s
DAY? (iambic pentameter)
? twas the NIGHT before CHRIST-mas and
ALL through the HOUSE (anapest tetrame-
ter)
Classical English sonnets are poems most often
composed of 14 lines of iambic pentameter.
524
3 Analysis
We focus on English rhythmic poetry. We define
the following analysis task: given poetic lines in
a known meter (such as sonnets written in iambic
pentameter), assign a syllable-stress pattern to each
word in each line. Making such decisions is part of
the larger task of reading poetry aloud. Later in the
paper, we will employ the concrete statistical tables
from analysis to the problems of poetry generation
and translation.
We create a test set consisting of 70 lines from
Shakespeare?s sonnets, which are written in iambic
pentameter. Here is an input line annotated with
gold output.
shall i compare thee to a summers day
| | /\ | | | /\ |
S S* S S* S S* S S* S S*
S refers to an unstressed syllable, and S* refers to
a stressed syllable. One of the authors created gold-
standard output by listening to Internet recordings
of the 70 lines and marking words according to the
speaker?s stress. The task evaluation consists of per-
word accuracy (how many words are assigned the
correct stress pattern) and per-line accuracy (how
many lines have all words analyzed perfectly).
This would seem simple enough, if we are armed
with something like the CMU pronunciation dictio-
nary: we look up syllable-stress patterns for each
word token and lay these down on top of the se-
quence S S* S S* S S* S S* S S*. However, there
are difficulties:
? The test data contains many words that are un-
known to the CMU dictionary.
? Even when all words are known, many lines
do not seem to contain 10 syllables. Some
lines contain eleven words.
? Spoken recordings include stress reversals,
such as poin-TING instead of POIN-ting.
? Archaic pronunciations abound, such as
PROV-ed (two syllables) instead of PROVED
(one syllable).
? In usage, syllables are often subtracted (PRIS-
ner instead of PRIS-o-ner), added (SOV-e-
reign instead of SOV-reign), or merged.
? Some one-syllable words are mostly stressed,
and others mostly unstressed, but the dictio-
e ? P(m|e) ? m
Figure 1: Finite-state transducer (FST) for mapping se-
quences of English words (e) onto sequences of S* and S
symbols (m), representing stressed and unstressed sylla-
bles.
nary provides no guidance. When we gener-
ate rhythmic text, it is important to use one-
syllable words properly. For example, we
would be happy for an iambic generator to
output big thoughts are not quite here, but not
quite big thoughts are not here.
Therefore, we take a different tack and apply un-
supervised learning to acquire word-stress patterns
directly from raw poetry, without relying on a dic-
tionary. This method easily ports to other languages,
where dictionaries may not exist and where mor-
phology is a severe complication. It may also be
used for dead languages.
For raw data, we start with all Shakespeare son-
nets (17,134 word tokens). Because our learning is
unsupervised, we do not mind including our 70-line
test set in this data (open testing).
Figures 1 and 2 show a finite-state transducer
(FST) that converts sequences of English words to
sequences of S* and S symbols. The FST?s transi-
tions initially map each English word onto all out-
put sub-sequences of lengths 1 to 4 (i.e., S, S*, S-S,
S-S*, S*-S, S*-S*, S-S-S, . . . ) plus the sequences
S-S*-S-S*-S and S*-S-S*-S-S*. Initial probabilities
are set to 1/32. The FST?s main loop allows it to
process a sequence of word tokens. If the same word
appears twice in a sequence, then it may receive two
different pronunciations, since the mapping is prob-
abilistic. However, a token?s syllable/stress pattern
is chosen independently of other tokens in the se-
quence; we look at relaxing this assumption later.
We next use finite-state EM training1 to train the
machine on input/output sequences such as these:
from fairest creatures we desire increase
S S* S S* S S* S S* S S*
but thou contracted to thine own bright eyes
S S* S S* S S* S S* S S*
1All operations in this paper are carried out with the generic
finite-state toolkit Carmel (Graehl, 1997). For example, the
train-cascade command uses EM to learn probabilities in an ar-
bitrary FST cascade from end-to-end input/output string pairs.
525
Figure 2: An efficient FST implementing P(m|e). This machine maps sequences of English words onto sequences of
S* and S symbols, representing stressed and unstressed syllables. Initially every vocabulary word has 32 transitions,
each with probability 1/32. After EM training, far fewer transitions remain.
526
Figure 3: An FST that accepts any of four input meters
and deterministically normalizes its input to strict iambic
pentameter. We call this FST norm.
e ? P(m|e) ? m ? norm ? m
Figure 4: FST cascade that encodes a loose interpretation
of iambic pentameter. The norm FST accepts any of four
near-iambic-pentameter sequences and normalizes them
into strict iambic pentameter.
Note that the output sequences are all the same,
representing our belief that each line should be read
as iambic pentameter.2 After we train the FST,
we can use Viterbi decoding to recover the highest-
probability alignments, e.g.:
from fairest creatures we desire increase
| | /| \ | /\ /\
S S* S S* S S* S S* S S*
but thou contracted to thine own bright eyes
| | /| \ | | | | |
S S* S S* S S* S S* S S*
Note that the first example contains an error?the
words fairest and creatures should each be read with
two syllables. There are many such errors. We next
improve the system in two ways: more data and bet-
ter modeling.
First, we augment the Shakespeare sonnets with
data from the website sonnets.org, increasing the
number of word tokens from 17,134 to 235,463. The
sonnets.org data is noisier, because it contains some
non-iambic-pentameter poetry, but overall we find
that alignments improve, e.g.:
from fairest creatures we desire increase
| /\ /\ | /\ /\
S S* S S* S S* S S* S S*
Second, we loosen our model. When we listen to
recordings, we discover that not all lines are read S
S* S S* S S* S S* S S*. Indeed, some lines in our
data contain eleven words?these are unexplainable
by the EM training system. We also observe that
2We can augment the data with lines of poetry written in
meters other than iambic pentameter, so long as we supply the
desired output pattern for each input line.
Training Training Test token Test line
data tokens accuracy accuracy
Shakespeare 17,134 82.3% 55.7%
sonnets.org 235,463 94.2% 81.4%
Figure 5: Analysis task accuracy.
poets often use the word mother (S* S) at the begin-
nings and ends of lines, where it theoretically should
not appear.
Two well-known variations explain these facts.
One is optional inversion of the first foot (S S*
? S* S). Second is the optional addition of an
eleventh unstressed syllable (the feminine ending).
These variations yield four possible syllable-stress
sequences:
S S* S S* S S* S S* S S*
S* S S S* S S* S S* S S*
S S* S S* S S* S S* S S* S
S* S S S* S S* S S* S S* S
We want to offer EM the freedom to analyze lines
into any of these four variations. We therefore con-
struct a second FST (Figure 3), norm, which maps
all four sequences onto the canonical pattern S S*
S S* S S* S S* S S*. We then arrange both FSTs
in a cascade (Figure 4), and we train the whole
cascade on the same input/output sequences as be-
fore. Because norm has no trainable parameters, we
wind up training only the lexical mapping parame-
ters. Viterbi decoding through the two-step cascade
now reveals EM?s proposed internal meter analysis
as well as token mappings, e.g.:
to be or not to be that is the question
| | | | | | | | | /\
S S* S S* S S* S S* S S* S
| | | | | | | | | |
S S* S S* S S* S S* S S*
Figure 5 shows accuracy results on the 70-line test
corpus mentioned at the beginning of this section.
Over 94% of word tokens are assigned a syllable-
stress pattern that matches the pattern transcribed
from audio. Over 81% of whole lines are also
scanned correctly. The upper limit for whole-line
scanning under our constraints is 88.6%, because
11.4% of gold outputs do not match any of the four
patterns we allow.
We further obtain a probabilistic table of word
mappings that we can use for generation and trans-
527
P(S* S S* | altitude) = 1.00
P(S* S | creatures) = 1.00
P(S* S | pointed) = 0.95
P(S S* | pointed) = 0.05
P(S* S | prisoner) = 0.74
P(S* S S* | prisoner) = 0.26
P(S* S | mother) = 0.95
P(S* | mother) = 0.03
P(S S* | mother) = 0.02
Figure 6: Sample learned mappings between words and
syllable-stress patterns.
word P(S* | word) P(S | word)
a 0.04 0.96
the 0.06 0.94
their 0.09 0.91
mens 0.10 0.90
thy 0.10 0.90
be 0.48 0.52
me 0.49 0.51
quick 0.50 0.50
split 0.50 0.50
just 0.51 0.49
food 0.90 0.10
near 0.90 0.10
raised 0.91 0.09
dog 0.93 0.07
thought 0.95 0.05
Figure 7: Sample mappings for one-syllable words.
lation tasks. Figure 6 shows a portion of this table.
Note that P(S S* | mother) has a very small proba-
bility of 0.02. We would incorrectly learn a much
higher value if we did not loosen the iambic pen-
tameter model, as many mother tokens occur line-
initial and line-final.
Figure 7 shows which one-syllable words are
more often stressed (or unstressed) in iambic pen-
tameter poetry. Function words and possessives tend
to be unstressed, while content words tend to be
stressed, though many words are used both ways.
This useful information is not available in typical
pronunciation dictionaries.
Alignment errors still occur, especially in noisy
P(m) ? m ? P(e|m) ? e ? P(e) ? e
Figure 8: Finite-state cascade for poetry generation.
portions of the data that are not actually written in
iambic pentameter, but also in clean portions, e.g.:
the perfect ceremony of loves rite
| /\ /|\ | | /\
S S* S S* S S* S S* S S*
The word ceremony only occurs this once in the
data, so it is willing to accept any stress pattern.
While rite is correctly analyzed elsewhere as a one-
syllable word, loves prefers S*, and this overwhelms
the one-syllable preference for rite. We can blame
our tokenizer for this, as it conflates loves and love?s,
despite the fact that these words have different stress
probabilities.
4 Generation
Figure 8 shows our concept of generation as a cas-
cade of weighted FSTs.
P(m) is a user-supplied model of desired
meters?normally it deterministically generates a
single string of S* and S symbols. (The user also
supplies a rhyme scheme?see below).
P(e|m) is the reverse of Section 3?s P(m|e), be-
ing a model of word selection. Its generative story
is: (1) probabilistically select n tokens (n = 1 to 5)
from the input, (2) probabilistically select a word w
that realizes that n-token sequence, and (3) recurse
until the input is consumed. Instead of asking how
a given word is likely to be pronounced (e.g., S or
S*), we now ask how a given stress-pattern (e.g., S
or S*) is likely to be realized. This model is trained
with the same method described in Section 3 and is
augmented with the CMU pronunciation dictionary.
Finally, P(e) is a word-trigram model built from a
10,000-line corpus of 105 English love poems.
We select the first line of our poem from the FST
cascade?s 100,000-best list, or by hand. To gener-
ate each subsequent line, we modify the cascade and
run it again. The first modification is to incorporate
a discourse model. From our poetry corpus, we esti-
mate a word?s unigram probability given the words
on the previous line, via IBM Model 1 (Brown et
al., 1993). We modify P(e) by interpolating in these
probabilities. Second, we check if any previous line
528
The women of the night
Again and all the way
Like a mouse in the white
Not the heart of the day.
- - -
Of the bed to trust me
Around her twists the string
But i will not tell thee
Fire changes everything.
- - -
A son of the right hand confines
His uncle could have broken in
Towards the high bank and the pines
Upon the eyes and i have been
- - -
Into one of her hundred year old
Or the house in a house in a cold
The first time she met him
Like a mouse in the dim
For me to the moon and when i told
- - -
Into one of them some years before
His own man or the house in a more
The moon and when the day
Into one of the way
With the breath from the first time
she swore
Figure 9: Sample poems generated with a weighted FST
cascade.
w1, w2, ...wn needs to be rhymed with, according to
the user-supplied scheme. If so, we build an addi-
tional FST that accepts only strings whose final word
rhymes with wn. This is a reasonable approach,
though it will not, for example, rhyme ...tar me with
...army. We say two non-identical words rhyme if
their phoneme strings share a common suffix that in-
cludes the last stressed vowel.
Figure 9 shows several poems that we automati-
cally generate with this scheme.
5 Translation
Automatically generated poetry can sound good
when read aloud, but it often has a ?nonsense? feel to
it. According to (Gervas, 2010), creative-language
researchers interested in realization and surface lan-
guage statistics (?how to say?) have tended to grav-
itate to poetry generation, while researchers inter-
ested in characters, goals, and story-line (?what to
say?) have tended to gravitate to prose story genera-
tion.
Translation provides one way to tie things to-
i ? P(e|i) ? e ? P(m|e) ? m ? P(m) ? m
Figure 10: Finite-state cascade for poetry translation.
gether. The source language provides the input
(?what to say?), and the target language can be
shaped to desired specifications (?how to say?). For
example, we may want to translate Italian sonnets
into fluent English iambic pentameter. This is cer-
tainly a difficult task for people, and one which is
generally assumed to be impossible for computers.
Here we investigate translating Dante?s Divine
Comedy (DC) from Italian into English by machine.
The poem begins:
nel mezzo del cammin di nostra vita
mi ritrovai per una selva oscura
che la via diritta era smarrita.
DC is a long sequence of such three-line stan-
zas (tercets). The meter in Italian is hendecasyl-
labic, which has ten syllables and ensures three
beats. Dante?s Italian rhyme scheme is: ABA, BCB,
CDC, etc, meaning that lines 2, 4, and 6 rhyme with
each other; lines 5, 7, and 9 rhyme with each other,
and so forth. There is also internal rhyme (e.g.,
diritta/smarrita).
Because DC has been translated many times
into English, we have examples of good outputs.
Some translations target iambic pentameter, but even
the most respected translations give up on rhyme,
since English is much harder to rhyme than Italian.
Longfellow?s translation begins:
midway upon the journey of our life
i found myself within a forest dark
for the straightforward pathway had
been lost.
We arrange the translation problem as a cascade
of WFSTs, as shown in Figure 10. We call our Ital-
ian input i. In lieu of the first WFST, we use the
statistical phrase-based machine translation (PBMT)
system Moses (Koehn et al, 2007), which generates
a target-language lattice with paths scored by P(e|i).
We send this lattice through the same P(m|e) device
we trained in Section 3. Finally, we filter the result-
ing syllable sequences with a strict, single-path, de-
terministic iambic pentameter acceptor, P(m).3 Our
3It is also possible to use a looser iambic P(m) model, as
described in Section 3.
529
Parallel Italian/English Data
Collection Word count (English)
DC-train 400,670
Il Fiore 25,995
Detto Damare 2,483
Egloghe 3,120
Misc. 557
Europarl 32,780,960
English Language Model Data
Collection Word count (English)
DC-train 400,670
poemhunter.com 686,714
poetry.eserver.org
poetrymountain.com
poetryarchive.org 58,739
everypoet.com 574,322
sonnets.org 166,465
Europarl 32,780,960
Tune and Blind Test Data (4 reference)
Collection Word count (Italian)
DC-tune 7,674
DC-test 2,861
Figure 11: Data for Italian/English statistical translation.
finite-state toolkit?s top-k paths represent the trans-
lations with the highest product of scores P(e|i) ?
P(m|e) ? P(m).
In general, the P(e|i) and P(m|e) models fight
each other in ranking candidate outputs. In exper-
iments, we find that the P(e|i) preference is some-
times so strong that the P(m|e) model is pushed
into using a low-probability word-to-stress mapping.
This creates output lines that do not scan easily. We
solve this problem by assigning a higher weight to
the P(m|e) model.4
Figure 11 shows the data we used to train the
PBMT system. The vast majority of parallel Ital-
ian/English poetry is DC itself, for which we have
four English translations. We break DC up into DC-
train, DC-tune, and DC-test. We augment our target
language model with English poetry collected from
many sources. We also add Europarl data, which
4We set this weight manually to 3.0, i.e., we raise all prob-
abilities in the P(m|e) model to the power of 3.0. Setting the
weight too high results in lines that scan very well, but whose
translation quality is low.
Original:
nel mezzo del cammin di nostra vita
mi ritrovai per una selva oscura
che la via diritta era smarrita.
Phrase-based translation (PBMT):
midway in the journey of our life
i found myself within a forest dark
for the straight way was lost.
PBMT + meter model:
midway upon the journey of our life
i found myself within a forest dark
for the straightforward pathway had been lost.
Figure 12: Automatic translation of lines from Dante?s
Divine Comedy. In this test-on-train scenario, the ma-
chine reproduces lines from human translations it has
seen.
is out of domain, but which reduces the unknown
word-token rate in DC-test from 9% to 6%, and the
unknown word-type rate from 22% to 13%.
We first experiment in a test-on-train scenario,
where we translate parts of DC that are in our train-
ing set. This is a normal scenario in human poetry
translation, where people have access to previous
translations.
Figure 12 shows how we translate the first lines
of DC, first using only PBMT, then using the full
system. When we use the full system, we not only
get an output string, but also the system?s intended
scan, e.g.:
midway upon the journey of our life
/\ /\ | /\ | | |
S S* S S* S S* S S* S S*
The machine?s translation here is the same as
Longfellow?s, which is in the training data. In other
cases, we observe the machine combining existing
translations, e.g.:
i: bedi la bestia per cu io mi volsi
I5: behold the beast that made me turn aside
H1: BEHOLD THE BEAST for which i have turned back
H2: you see the beast THAT MADE ME TURN ASIDE
H3: see the beast that forced me to turn back
H4: look at the beast that drove me to turn back
I5 refs to the machine?s iambic pentameter transla-
530
tion, while H1-4 refer to human translations. The
machine also creates new translations:
i: diro? de laltre cose chi vho scorte
I5: i shall explain the other things i saw
H1: speak will i of the other things i saw there
H2: ill also tell THE OTHER THINGS I SAW
H3: i will recount the other things i saw
H4: i here will tell the other things i saw
We can further change the target meter to any-
thing we desire. To obtain iambic tetrameter (4-beat)
translations, we delete the last two transitions of the
P(m) model. We then get:
I4: in our life the journey way
i found myself deep on dark wood
that lost straightforward pathway had.
ah how to say the what is hard
this forest savage rough and stern
the very thought renews the fear.
Translations and scans are uneven, but we have
significant flexibility. We can even request transla-
tions that avoid the English letter A, by adding a fil-
ter to the end of the FST cascade, obtaining:
I5: in midst upon the journey of our life
i found myself within the wood obscure
<fail>
To steer clear of the adjective dark in the second
line, the system switches from forest to wood, so
obtain a proper scan. The third line fails because
all paths through the translation lattice contain an A
somewhere.
Translating blind-test data proves to be more dif-
ficult. We hold out Canto XXXI of DC?s Paradiso
section for testing. Figure 13 shows a portion of
the translation results. The MT system handles un-
known Italian words by passing them through to the
output. The P(m|e) meter model cannot process
those words, accounting for the I5 failure rate.
Here, we get a first look at statistical MT trans-
lating poetry into rhythmic structures?as with all
MT, there are successes and problems, and certainly
more to do.
6 Future Work
We plan to release all our of data in useful, processed
form. Below we list directions for future research.
In general, we see many interesting paths to pursue.
Analysis. Proper use of one-syllable words re-
mains tricky. Lines coming out of generation
Original:
in forma dunque di candida rosa
mi si mostrava la milizia santa
che nel suo sangue cristo fece sposa
ma laltra che volando vede e canta
la gloria di colui che la nnamora
e la bonta? che la fece cotanta
Human translation:
in fashion then as of a snow white rose
displayed itself to me the saintly host
whom christ in his own blood had made his bride
but the other host that flying sees and sings
the glory of him who doth enamour it
and the goodness that created it so noble
Phrase-based translation (PBMT):
in the form so rose candida
i now was shown the militia holy
that in his blood christ did bride
but the other that flying sees and sings
the glory of him that the nnamora
and the goodness that the made cotanta
PBMT + meter model:
<fail>
i now was shown the holy soldiery
that in his blood he married jesus christ
but flying sees and sings the other which
<fail>
<fail>
Figure 13: Automatic translation of blind-test data from
Dante?s Divine Comedy.
531
and translation do not always scan naturally when
read aloud by a person. We trace such errors to
the fact that our lexical probabilities are context-
independent. For example, we have:
P(S | off) = 0.39
P(S* | off) = 0.61
When we look at Viterbi alignments from the
analysis task, we see that when off is preceded by
the word far, the probabilities reverse dramatically:
P(S | off, after far) = 0.95
P(S* | off, after far) = 0.05
Similarly, the probability of stressing at is 40%
in general, but this increases to 91% when the next
word is the. Developing a model with context-
dependent probabilities may be useful not only for
improving generation and translation, but also for
improving poetry analysis itself, as measured by an-
laysis task accuracy.
Other potential improvements include the use of
prior knowledge, for example, taking word length
and spelling into account, and exploiting incomplete
pronunciation dictionary information.
Generation. Evaluation is a big open problem for
automatic poetry generation?even evaluating hu-
man poetry is difficult. Previous suggestions for au-
tomatic generation include acceptance for publica-
tion in some established venue, or passing the Tur-
ing test, i.e., confounding judges attempts to distin-
guish machine poetry from human poetry. The Tur-
ing test is currently difficult to pass with medium-
sized Western poetry.
Translation. The advantage of translation over
generation is that the source text provides a coherent
sequence of propositions and images, allowing the
machine to focus on ?how to say? instead of ?what
to say.? However, translation output lattices offer
limited material to work with, and as we dig deeper
into those lattices, we encounter increasingly disflu-
ent ways to string together renderings of the source
substrings.
An appealing future direction is to combine trans-
lation and generation. Rather than translating
the source text, a program may instead use the
source text for inspiration. Such a hybrid trans-
lation/generation program would not be bound to
translate every word, but rather it could more freely
combine lexical material from its translation tables
with other grammatical and lexical resources. In-
terestingly, human translators sometimes work this
way when they translate poetry?many excellent
works have been produced by people with very little
knowledge of the source language.
Paraphrasing. Recently, e?f translation tables
have been composed with f?e tables, to make
e?e tables that can paraphrase English into English
(Bannard and Callison-Burch, 2005). This makes it
possible to consider statistical translation of English
prose into English poetry.
Acknowledgments
This work was partially supported by NSF grant IIS-
0904684.
References
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. ACL.
P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational linguis-
tics, 19(2).
B. Diaz-Agudo, P. Gervas, and P. A. Gonzalez-Calero.
2002. Poetry generation in COLIBRI. In Proc. EC-
CBR.
P. Gervas. 2001. An expert system for the composition of
formal Spanish poetry. Journal of Knowledge-Based
Systems, 14:200?1.
P. Gervas. 2010. Engineering linguistic creativity: Bird
flight and jet planes. Invited talk, CALC-10.
J. Graehl. 1997. Carmel finite-state toolkit.
http://www.isi.edu/licensed-sw/carmel.
L. Jiang and M. Zhou. 2008. Generating Chinese cou-
plets using a statistical MT approach. In Proc. COL-
ING.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proc. ACL.
H. Manurung, G. Ritchie, and H. Thompson. 2000. To-
wards a computational model of poetry generation. In
Proc. AISB?00 Symposium on Creative and Cultural
Aspects and Applications of AI and Cognitive Science.
H. Manurung. 2003. An evolutionary algorithm ap-
proach to poetry generation. Ph.D. thesis, University
of Edinburgh.
Y. Netzer, D. Gabay, Y. Goldberg, and M. Elhadad. 2009.
Gaiku : Generating Haiku with word associations
532
norms. In Proc. NAACL Workshop on Computational
Approaches to Linguistic Creativity.
N. Tosa, H. Obara, and M. Minoh. 2008. Hitch Haiku:
An interactive supporting system for composing Haiku
poem. In Proc. International Conference on Enter-
tainment Computing.
M. T. Wong and A. H. W. Chun. 2008. Automatic Haiku
generation using VSM. In Proc. ACACOS.
533
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 266?275, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Large Scale Decipherment for Out-of-Domain Machine Translation
Qing Dou and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,knight}@isi.edu
Abstract
We apply slice sampling to Bayesian de-
cipherment and use our new decipherment
framework to improve out-of-domain machine
translation. Compared with the state of the
art algorithm, our approach is highly scalable
and produces better results, which allows us
to decipher ciphertext with billions of tokens
and hundreds of thousands of word types with
high accuracy. We decipher a large amount
of monolingual data to improve out-of-domain
translation and achieve significant gains of up
to 3.8 BLEU points.
1 Introduction
Nowadays, state of the art statistical machine trans-
lation (SMT) systems are built using large amounts
of bilingual parallel corpora. Those corpora are
used to estimate probabilities of word-to-word trans-
lation, word sequences rearrangement, and even
syntactic transformation. Unfortunately, as paral-
lel corpora are expensive and not available for ev-
ery domain, performance of SMT systems drops
significantly when translating out-of-domain texts
(Callison-Burch et al2008).
In general, it is easier to obtain in-domain mono-
lingual corpora. Is it possible to use domain specific
monolingual data to improve an MT system trained
on parallel texts from a different domain? Some re-
searchers have attempted to do this by adding a do-
main specific dictionary (Wu et al2008), or mining
unseen words (Daume? and Jagarlamudi, 2011) us-
ing one of several translation lexicon induction tech-
niques (Haghighi et al2008; Koehn and Knight,
2002; Rapp, 1995). However, a dictionary is not al-
ways available, and it is difficult to assign probabil-
ities to a translation lexicon.
(Ravi and Knight, 2011b) have shown that one
can use decipherment to learn a full translation
model from non-parallel data. Their approach is able
to find translations, and assign probabilities to them.
But their work also has certain limitations. First of
all, the corpus they use to build the translation sys-
tem has a very small vocabulary. Secondly, although
their algorithm is able to handle word substitution
ciphers with limited vocabulary, its deciphering ac-
curacy is low.
The contributions of this work are:
? We improve previous decipherment work by in-
troducing a more efficient sampling algorithm.
In experiments, our new method improves de-
ciphering accuracy from 82.5% to 88.1% on
(Ravi and Knight, 2011b)?s domain specific
data set. Furthermore, we also solve a very
large word substitution cipher built from the
English Gigaword corpus and achieve 92.2%
deciphering accuracy on news text.
? With the ability to handle a much larger vocab-
ulary, we learn a domain specific translation ta-
ble from a large amount of monolingual data
and use the translation table to improve out-of-
domain machine translation. In experiments,
we observe significant gains of up to 3.8 BLEU
points. Unlike previous works, the translation
table we build from monolingual data do not
only contain unseen words but also words seen
in parallel data.
266
2 Word Substitution Ciphers
Before we present our new decipherment frame-
work, we quickly review word substitution decipher-
ment.
Recently, there has been an increasing interest in
decipherment work (Ravi and Knight, 2011a; Ravi
and Knight, 2008). While letter substitution ciphers
can be solved easily, nobody has been able to solve
a word substitution cipher with high accuracy.
As shown in Figure 1, a word substitution cipher
is generated by replacing each word in a natural lan-
guage (plaintext) sequence with a cipher token ac-
cording to a substitution table. The mapping in the
table is deterministic ? each plaintext word type is
only encoded with one unique cipher token. Solv-
ing a word substitution cipher means recovering the
original plaintext from the ciphertext without know-
ing the substitution table. The only thing we rely on
is knowledge about the underlying language.
Figure 1: Encoding and Decipherment of a Word Substi-
tution Cipher
How can we solve a word substitution cipher?
The approach is similar to those taken by cryptana-
lysts who try to recover keys that convert encrypted
texts to readable texts. Suppose we observe a large
cipher string f and want to decipher it into English e.
We can follow the work in (Ravi and Knight, 2011b)
and assume that the cipher string f is generated in
the following way:
? Generate English plaintext sequence e =
e1, e2...en with probability P(e).
? Replace each English plaintext token ei with a
cipher token fi with probability P (fi|ei).
Based on the above generative story, we write the
probability of the cipher string f as:
P (f)? =
?
e
P (e) ?
n
?
i
P?(fi|ei) (1)
We use this equation as an objective function for
maximum likelihood training. In the equation, P (e)
is given by an ngram language model, which is
trained using a large amount of monolingual texts.
The rest of the task is to manipulate channel prob-
abilities P?(fi|ei) so that the probability of the ob-
served texts P (f)? is maximized.
Theoretically, we can directly apply EM, as pro-
posed in (Knight et al2006), or Bayesian decipher-
ment (Ravi and Knight, 2011a) to solve the prob-
lem. However, unlike letter substitution ciphers,
word substitution ciphers pose much greater chal-
lenges to algorithm scalability. To solve a word sub-
stitution cipher, the EM algorithm has a computa-
tional complexity of O(N ? V 2 ? R) and the com-
plexity of Bayesian method is O(N ? V ? R), where
V is the size of plaintext vocabulary, N is the length
of ciphertext, and R is the number of iterations. In
the world of word substitution ciphers, both V and
N are very large, making these approaches impracti-
cal. (Ravi and Knight, 2011b) propose several mod-
ifications to the existing algorithms. However, the
modified algorithms are only an approximation of
the original algorithms and produce poor decipher-
ing accuracy, and they are still unable to handle very
large scale ciphers.
To address the above problems, we propose the
following two new improvements to previous deci-
pherment methods.
? We apply slice sampling (Neal, 2000) to scale
up to ciphers with a very large vocabulary.
? Instead of deciphering using the original ci-
phertext, we break the ciphertext into bigrams,
collect their counts, and use the bigrams with
their counts for decipherment.
267
The new improvements allow us to solve a word
substitution cipher with billions of tokens and hun-
dreds of thousands of word types. Through better
approximation, we achieve a significant increase in
deciphering accuracy. In the following section, we
present details of our new approach.
3 Slice Sampling for Bayesian
Decipherment
In this section, we first give an introduction to
Bayesian decipherment and then describe how to use
slice sampling for it.
3.1 Bayesian Decipherment
Bayesian inference has been widely used in natural
language processing (Goldwater and Griffiths, 2007;
Blunsom et al2009; Ravi and Knight, 2011b). It is
very attractive for problems like word substitution
ciphers for the following reasons. First, there are
no memory bottlenecks as compared to EM, which
has an O(N ? V 2) space complexity. Second, priors
encourage the model to learn a sparse distribution.
The inference is usually performed using Gibbs
sampling. For decipherment, a Gibbs sampler keeps
drawing samples from plaintext sequences accord-
ing to derivation probability P (d):
P (d) = P (e) ?
n
?
i
P (ci|ei) (2)
In Bayesian inference, P (e) is still given by an
ngram language model, while the channel probabil-
ity is modeled by the Chinese Restaurant Process
(CRP):
P (ci|ei) =
? ? prior + count(ci, ei)
?+ count(ei)
(3)
Where prior is the base distribution (we set prior
to 1C in all our experiments, where C is the number
of word types in ciphertext), and count, also called
?cache?, records events that occurred in the history.
Each sampling operation involves changing a plain-
text token ei, which has V possible choices, where
V is the plaintext vocabulary size, and the final sam-
ple is chosen with probability P (d)?V
n=1 P (d)
.
3.2 Slice Sampling
With Gibbs sampling, one has to evaluate all possi-
ble plaintext word types (10k?1M) for each sam-
ple decision. This become intractable when the vo-
cabulary is large and the ciphertext is long. Slice
sampling (Neal, 2000) can solve this problem by au-
tomatically adjusting the number of samples to be
considered for each sampling operation.
Suppose the derivation probability for current
sample is P (current s). Then slice sampling draws
a sample in two steps:
? Select a threshold T uniformly from the range
{0, P (current s)}.
? Draw a new sample new s uniformly from a
pool of candidates: {new s|P (new s) > T}.
From the above two steps, we can see that given a
threshold T , we only need to consider those samples
whose probability is higher than the threshold. This
will lead to a significant reduction on the number
of samples to be considered, if probabilities of the
most samples are below T . In practice, the first step
is easy to implement, while it is difficult to make the
second step efficient. An obvious way to collect can-
didate samples is to go over all possible samples and
record those with probabilities higher than T . How-
ever, doing this will not save any time. Fortunately,
for Bayesian decipherment, we are able to complete
the second step efficiently.
According to Equation 1, the probability of the
current sample is given by a language model P (e)
and a channel model P (c|e). The language model
is usually an ngram language model. Suppose our
current sample current s contains English tokens
X , Y , and Z at position i ? 1, i, and i + 1 respec-
tively. Let ci be the cipher token at position i. To
obtain a new sample, we just need to change token
Y to Y ?. Since the rest of the sample stays the same,
we only need to calculate the probability of any tri-
gram 1: P (XY ?Z) and the channel model probabil-
ity: P (ci|Y ?), and multiply them together as shown
in Equation 4.
P (XY ?Z) ? P (ci|Y ?) (4)
1The probability is given by a bigram language model.
268
In slice sampling, each sampling operation has
two steps. For the first step, we choose a thresh-
old T uniformly between 0 and P (XY Z) ?P (ci|Y ).
For the second step, there are two cases.
First, we notice that two types of Y ? are more
likely to pass the threshold T : (1) Those that have
a very high trigram probability , and (2) those that
have high channel model probability. To find can-
didates that have high trigram probability, we build
sorted lists ranked by P (XY ?Z), which can be pre-
computed off-line. We only keep the top K En-
glish words for each of the sorted list. When the
last item YK in the list satisfies P (XYkZ) ? prior <
T , We draw a sample in the following way: set
A = {Y ?|P (XY ?Z) ? prior > T} and set B =
{Y ?|count(ci, Y ?) > 0}, then we only need to sam-
ple Y ? uniformly from A ? B until Equation 4 is
greater than T . 2
Second, what happens when the last item YK in
the list does not satisfy P (XYkZ) ? prior < T ?
Then we always choose a word Y ? randomly and ac-
cept it as a new sample if Equation 4 is greater than
T .
Our algorithm alternates between the two cases.
The actual number of choices the algorithm looks at
depends on the K and the total number of possible
choices. In different experiments, we find that when
K = 500, the algorithm only looks at 0.06% of all
possible choices. When K = 2000, this further re-
duces to 0.007%.
3.3 Deciphering with Bigrams
Since we can decipher with a bigram language
model, we posit that a frequency list of ciphertext
bigrams may contain enough information for deci-
pherment. In our letter substitution experiments, we
find that breaking ciphertext into bigrams doesn?t
hurt decipherment accuracy. Table 1 shows how full
English sentences in the original data are broken into
bigrams and their counts.
Instead of doing sampling on full sentences, we
treat each bigram as a full ?sentence?. There are
2It is easy to prove that all other candidates that are not in
the sorted list and with count(ci, Y ?) = 0 have a upper bound
probability: P (XYkZ) ? prior. Therefore, they are ignored
when P (XYkZ) ? prior < T .
man they took our land .
they took our arable land .
took our 2
they took 2
land . 2
man they 1
arable land 1
Table 1: Converting full sentences to bigrams
two advantages to use bigrams and their counts for
decipherment.
First of all, the bigrams and counts are a much
more compact representation of the original cipher-
text with full sentences. For instance, after breaking
a billion tokens from the English Gigaword corpus,
we find only 29m bigrams and 58m tokens, which
is only 1/17 of the original text. In practice, we fur-
ther discard all bigrams with low frequency, which
makes the ciphertext even shorter.
Secondly, using bigrams significantly reduces the
number of sorted lists (from |V |2 to 2|V |) mentioned
in the previous section. The number of lists reduces
from |V |2 to 2|V | because words in a bigram only
have one neighbor. Therefore, for any word W in a
bigram, we need only 2|V | lists (?words to the right
of W? and ?words to the left of W?) instead of |V |2
lists (?pairs of words that surround W?).
3.4 Iterative Sampling
Although we can directly apply slice sampling on
a large number of bigrams, we find that gradually
including less frequent bigrams into a sampling pro-
cess saves deciphering time ? we call this iterative
sampling:
? Break the ciphertext into bigrams and collect
their counts
? Keep bigrams whose counts are greater than a
threshold ?. Then initialize the first sample
randomly and use slice sampling to perform
maximum likelihood training. In the end, ex-
tract a translation table T according to the final
sample.
? Lower the threshold ? to include more bi-
grams into the sampling process. Initialize the
first sample using the translation table obtained
from the previous sampling run (for each ci-
269
pher token f, choose a plaintext token e whose
P (e|f) is the largest). Perform sampling again.
? Repeat until ? = 1.
3.5 Parallel Sampling
Inspired by (Newman et al2009), our parallel sam-
pling procedure is described below:
? Collect bigrams and their counts from cipher-
text and split the bigrams into N parts.
? Run slice sampling on each part for 5 iterations
independently.
? Combine counts from each part to form a new
count table and run sampling again on each part
using the new table.3
4 Decipherment Experiments
In this section, we evaluate our new sampling algo-
rithm in two different experiments. In the first ex-
periment, we compare our method with (Ravi and
Knight, 2011b) on their data set to prove correct-
ness of our approach. In the second experiment, we
scale up to the whole English Gigaword corpus and
achieve a much higher deciphering accuracy.
4.1 Deciphering Transtac Corpus
4.1.1 Data
We split the Transtac corpus the same way it was
split in (Ravi and Knight, 2011b). The data used to
create ciphertext consists of 1 million tokens, and
3397 word types. The data for language model
training contains 2.7 million tokens and 25761 word
types.4 The ciphertext is created by replacing each
English word with a cipher word.
We use a bigram language model for decipher-
ment training. When the training terminates, a trans-
lation table with probability P (c|e) is built based on
the counts collected from the final sample. For de-
coding, we employ a trigram language model using
full sentences. We use Moses (Koehn et al2007)
3Except for combining the counts to form a new count table,
other parameters remain the same. For instance, each part i has
its own prior set to 1Ci , where Ci is the number of word types
in that part of ciphertext.
4In practice, we replaced singletons with a ?UNK? symbol,
leaving around 16904 word types.
Method Deciphering Accuracy
Ravi and Knight 80.0 (with bigram LM)
82.5 (with trigram LM)
Slice Sampling 88.1 (with bigram LM)
Table 2: Decipherment Accuracy on Transtac Corpus
from (Ravi and Knight, 2011b)
Gold Decoded
man i?ve come to file
a complaint against
some people .
man i?ve come to hand
a telephone lines some
people .
man they took our land
.
man they took our
farm .
they took our arable
land .
they took our slide
door .
okay man . okay man .
eighty donums . miflih donums .
Table 3: Sample Decoding Results on Transtac Corpus
from (Ravi and Knight, 2011b)
to perform the decoding. We set the distortion limit
to 0 and cube the translation probabilities. Essen-
tially, Moses tries to find an English sequence e that
maximizes P (e) ? P (c|e)3
4.1.2 Results
We evaluate the performance of our algorithm
by decipherment accuracy, which measures the per-
centage of correctly deciphered cipher tokens. Table
2 compares the deciphering accuracy with the state
of the art algorithm.
Results show that our algorithm improves the de-
ciphering accuracy to 88.1%, which amounts to 33%
reduction in error rate. This justifies our claim: do-
ing better approximation using slice sampling im-
proves decipherment accuracy.
Table 3 shows the first 5 decoding results and
compares them with the gold plaintext. From the ta-
ble we can see that the algorithm recovered the ma-
jority of the plaintext correctly.
4.2 Deciphering Gigaword Corpus
To prove the scalability of our new approach, we ap-
ply it to solve a much larger word substitution cipher
built from English Gigaword corpus. The corpus
contains news articles from different news agencies
270
and has a much larger vocabulary compared with the
Transtac corpus.
4.2.1 Data
We split the corpus into two parts chronologically.
Each part contains approximately 1.2 billion tokens.
We uses the first part to build a word substitution
cipher, which is 10k times longer than the one in the
previous experiment, and the second part to build a
bigram language model. 5
4.2.2 Results
We first use a single machine and apply iterative
sampling to solve a 68 million token cipher. Then
we use the result from the first step to initialize our
parallel sampling process, which uses as many as
100 machines. For evaluation, we calculate deci-
phering accuracy over the first 1000 sentences (33k
tokens).
After 2000 iterations of the parallel sampling pro-
cess, the deciphering accuracy reaches 92.2%. Fig-
ure 2 shows the learning curve of the algorithm. It
can be seen from the graph that both token and type
accuracy increase as more and more data becomes
available.
Figure 2: Learning curve for a very large word substitu-
tion cipher: Both token and type accuracy rise as more
and more ciphertext becomes available.
5Before building the language model, we replace low fre-
quency word types with an ?UNK? symbol, leaving 129k
unique word types.
5 Improving Out-of-Domain Machine
Translation
Domain specific machine translation (MT) is a chal-
lenge for statistical machine translation (SMT) sys-
tems trained on parallel corpora. It is common to see
a significant drop in translation quality when trans-
lating out-of-domain texts. Although it is hard to
find parallel corpora for any specific domain, it is
relatively easy to find domain specific monolingual
corpora. In this section, we show how to use our new
decipherment framework to learn a domain specific
translation table and use it to improve out-of-domain
translations.
5.1 Baseline SMT System
We build a state of the art phrase-based SMT system
using Moses (Koehn et al2007). The baseline sys-
tem has 3 models: a translation model, a reordering
model, and a language model. The language model
can be trained on monolingual data, and the rest are
trained on parallel data. By default, Moses uses the
following 8 features to score a candidate translation:
? direct and inverse translation probabilities
? direct and inverse lexical weighting
? phrase penalty
? a language model
? a re-ordering model
? word penalty
Each of the 8 features has its own weight, which
can be tuned on a held-out set using minimum error
rate training. (Och, 2003). In the following sections,
we describe how to use decipherment to learn do-
main specific translation probabilities, and use the
new features to improve the baseline.
5.2 Learning a New Translation Table with
Decipherment
From a decipherment perspective, machine transla-
tion is a much more complex task than solving a
word substitution cipher and poses three major chal-
lenges:
? Mappings between languages are nondetermin-
istic, as words can have multiple translations
271
? Re-ordering of words
? Insertion and deletion of words
Fortunately, our decipherment model does not as-
sume deterministic mapping and is able to discover
multiple translations. For the reordering problem,
we treat Spanish as a simple word substitution for
French. Despite the simplification in the assump-
tion, we still expect to learn a useful word-to-word
lexicon via decipherment and use the lexicon to im-
prove our baseline.
Problem formulation: By ignoring word re-
orderings, we can formulate MT decipherment prob-
lem as word substitution decipherment. We view
source language f as ciphertext and target language
e as plaintext. Our goal is to decipher f into e and
estimate translation probabilities based on the deci-
pherment.
Probabilistic decipherment: Similar to solving
a word substitution cipher, all we have to do here is
to estimate the translation model parameters P?(f |e)
using a large amount of monolingual data in f and
e respectively. According to Equation 5, our objec-
tive is to estimate the model parameters so that the
probability of source text P(f) is maximized.
argmax
?
?
e
P (e) ?
n
?
i
P?(fi|ei) (5)
Building a translation table: Once the sampling
process completes, we estimate translation probabil-
ity P (f |e) from the final sample using maximum
likelihood estimation. We also decipher from the re-
verse direction to estimate P (e|f). Finally, we build
a phrase table by taking translation pairs seen in both
decipherments.
5.3 Combining Phrase Tables
We now have two phrase tables: one learnt from par-
allel corpus and one learnt from non-parallel mono-
lingual corpus through decipherment. The phrase ta-
ble learnt through decipherment only contains word
to word translations, and each translation option
only has two scores. Moses has a function to decode
with multiple phrase tables, so we just need to add
the newly learnt phrase table and specify two more
weights for the scores in it. During decoding, if a
source word only appears in the decipherment table,
Train French: 28.5 million tokensSpanish: 26.6 million tokens
Tune French: 28k tokensSpanish: 26k tokens
Test French: 30k tokensSpanish: 28k tokens
Table 4: Europarl Training, Tuning, and Testing Data
that table?s translation will be used. If a source word
exists in both tables, Moses will create two separate
decoding paths and choose the best one after taking
other features into account. If a word is not seen in
either of the tables, it is copied literally to the output.
6 MT Experiments and Results
6.1 Data
In our MT experiments, we translate French into
Spanish and use the following corpora to learn our
translation systems:
? Europarl Corpus (Koehn, 2005): The Europarl
parallel corpus is extracted from the proceed-
ings of the European Parliament and includes
versions in 11 European languages. The cor-
pus contains articles from the political domain
and is used to train our baseline system. We
use the 6th version of the corpus. After clean-
ing, there are 1.3 million lines left for training.
We use the last 2k lines for tuning and testing
(1k for each), and the rest for training. Details
of training, tuning, and testing data are listed in
Table 4.
? EMEA Corpus (Tiedemann, 2009): EMEA is
a parallel corpus made out of PDF documents
from the European Medicines Agency. It con-
tains articles from the medical domain, which
is a good test bed for out-of-domain tasks. We
use the first 2k pairs of sentences for tuning
and testing (1k for each), and use the rest (1.1
million lines) for decipherment training. We
split the training corpus in ways that no parallel
sentences are included in the training set. The
splitting methods are listed in Table 5.
For decipherment training, we use lexical transla-
tion tables learned from the Europarl corpus to ini-
272
Comparable EMEA :
French: Every odd line, 8.7 million tokens
Spanish: Every even line, 8.1 million tokens
Non-parallel EMEA:
French: First 550k sentences, 9.1 million tokens
Spanish: Last 550k sentences, 7.7 million to-
kens
Table 5: EMEA Decipherment Training Data
tialize our sampling process.
6.2 Results
BLEU (Papineni et al2002) is used as a standard
evaluation metric. We compare the following 3 sys-
tems in our experiments, and present the results in
Table 6.
? Baseline: Trained on Europarl
? Decipher-CP: Trained on Europarl + Compa-
rable EMEA
? Decipher-NP: Trained on Europarl + Non-
Parallel EMEA
Our baseline system achieves 38.2 BLEU score
on Europarl test set. In the second row of Table
6, the test set changes to EMEA, and the baseline
BLEU score drops to 24.9. In the third row, the base-
line score rises to 30.5 with a language model built
from EMEA corpus. Although it is much higher
than the previous baseline, we further improve it
by including a new phrase table learnt from domain
specific monolingual data. In a real out-of-domain
task, we are unlikely to have any parallel data to
tune weights for the new phrase table. Therefore,
we can only set it manually. In experiments, each
score in the new phrase table has a weight of 5, and
the BLEU score rises up to 33.2. In the fourth row
of the table, we assume that there is a small amount
of domain specific parallel data for tuning. With
better weights, our baseline BLEU score increases
to 37.3, and our combined systems increase to 41.1
and 39.7 respectively. In the last row of the table, we
compare the combined systems with an even better
baseline. This time, the baseline is given half of the
EMEA tuning set for training and uses the other half
French Spanish P (fr|es) P (es|fr)
< < 0.32 1.00
he?patique hepa?tico 0.88 0.08
hepa?tica 0.76 0.85
injectable inyectable 0.91 0.92
dl dl 1.00 0.70
> > 0.32 1.00
ribavirine ribavirina 0.40 1.00
olanzapine olanzapina 0.57 1.00
clairance aclaramiento 0.99 0.64
pellicule?ss recubiertos 1.00 1.00
pharmaco-
cine?tique
farmaco-
cine?tico 1.00 1.00
Table 7: 10 most frequent OOV words in the table learnt
from non-parallel EMEA corpus
for weight tuning. Results show that our combined
systems still outperform the baseline.
The phrase table learnt from monolingual data
consists of both observed and unknown words. Ta-
ble 7 shows the top 10 most frequent OOV words
in the table learnt from non-parallel EMEA corpus.
Among the 10 words, 9 have correct translations. It
is interesting to see that our algorithm finds mul-
tiple correct translations for the word ?he?patique?.
The only mistake in the table is sensible as French
word ?pellicule?s? is translated into ?recubiertos con
pel??cula? in Spanish.
7 Conclusion and Future Work
We apply slice sampling to Bayesian Decipherment
and show significant improvement in deciphering
accuracy compared with the state of the art algo-
rithm. Our method is not only accurate but also
highly scalable. In experiments, we decipher at the
scale of the English Gigaword corpus, which con-
tains over billions of tokens and hundreds of thou-
sands word types. We further show the value of
our new decipherment algorithm by using it to im-
prove out-of-domain translation. In the future, we
will work with more language pairs, especially those
with significant word re-orderings. Moreover, the
monolingual corpora used in the experiments are far
smaller than what our algorithm can handle. We will
continue to work in scenarios where large amount of
monolingual data is readily available.
273
Train Data Tune Data Tune LM Test Data Test LM Baseline Decipher-CP
Decipher-
NP
Europarl Europarl Europarl Europarl Europarl 38.2
Europarl Europarl Europarl EMEA Europarl 24.9
Europarl Europarl Europarl EMEA EMEA 30.5 33.2(+2.7)
32.4
(+1.9)
Europarl EMEA EMEA EMEA EMEA 37.3 41.1(+3.8)
39.7
(+2.4)
Europarl +
EMEA EMEA EMEA EMEA EMEA 67.4
68.7
(+1.3)
68.7
(+1.3)
Table 6: MT experiment results: The table shows how much the combined systems outperform the baseline system in
different experiments. Each row has a different set of training, tuning, and testing data. Baseline is trained on parallel
data only. Tune LM and Test LM specify language models used for tuning and testing respectively. Decipher-CP and
Decipher-NP use a phrase table learnt from comparable and non-parallel EMEA corpus respectively.
8 Acknowledgments
This work was supported by NSF Grant 0904684.
The authors would like to thank Philip Koehen,
David Chiang, Jason Riesa, Ashish Vaswani, and
Hui Zhang for their comments and suggestions.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. Associa-
tion for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies. Association for Com-
putational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics. Association
for Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In In Proceedings of the
Tenth Machine Translation Summit, Phuket, Thailand.
Asia-Pacific Association for Machine Translation.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
David Newman, Arthur Asuncion, Padhrai Smyth, and
Max Welling. 2009. Distributed algorithms for topic
models. Journal of Machine Learning Research, 10.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
274
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics.
Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing. Associ-
ation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011a. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2011b. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Association for
Computational Linguistics.
Jo?rg Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing V, volume 309 of Current Issues in Linguistic
Theory. John Benjamins.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics. Association for Computa-
tional Linguistics.
275
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1668?1676,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Dependency-Based Decipherment for Resource-Limited Machine
Translation
Qing Dou and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,knight}@isi.edu
Abstract
We introduce dependency relations into deci-
phering foreign languages and show that de-
pendency relations help improve the state-of-
the-art deciphering accuracy by over 500%.
We learn a translation lexicon from large
amounts of genuinely non parallel data with
decipherment to improve a phrase-based ma-
chine translation system trained with limited
parallel data. In experiments, we observe
BLEU gains of 1.2 to 1.8 across three different
test sets.
1 Introduction
State-of-the-art machine translation (MT) systems
apply statistical techniques to learn translation rules
from large amounts of parallel data. However, par-
allel data is limited for many language pairs and do-
mains.
In general, it is easier to obtain non parallel data.
The ability to build a machine translation system
using monolingual data could alleviate problems
caused by insufficient parallel data. Towards build-
ing a machine translation system without a paral-
lel corpus, Klementiev et al (2012) use non paral-
lel data to estimate parameters for a large scale MT
system. Other work tries to learn full MT systems
using only non parallel data through decipherment
(Ravi and Knight, 2011; Ravi, 2013). However, the
performance of such systems is poor compared with
those trained with parallel data.
Given that we often have some parallel data,
it is more practical to improve a translation sys-
tem trained on parallel corpora with non parallel
Figure 1: Improving machine translation with deci-
pherment (Grey boxes represent new data and process).
Mono: monolingual; LM: language model; LEX: trans-
lation lexicon; TM: translation model.
data. Dou and Knight (2012) successfully apply
decipherment to learn a domain specific translation
lexicon from monolingual data to improve out-of-
domain machine translation. Although their ap-
proach works well for Spanish/French, they do not
show whether their approach works for other lan-
guage pairs. Moreover, the non parallel data used in
their experiments is created from a parallel corpus.
Such highly comparable data is difficult to obtain in
reality.
In this work, we improve previous work by Dou
and Knight (2012) using genuinely non parallel data,
1668
and propose a framework to improve a machine
translation system trained with a small amount of
parallel data. As shown in Figure 1, we use a lexi-
con learned from decipherment to improve transla-
tions of both observed and out-of-vocabulary (OOV)
words. The main contributions of this work are:
? We extract bigrams based on dependency re-
lations for decipherment, which improves the
state-of-the-art deciphering accuracy by over
500%.
? We demonstrate how to improve translations
of words observed in parallel data by us-
ing a translation lexicon obtained from large
amounts of non parallel data.
? We show that decipherment is able to find cor-
rect translations for OOV words.
? We use a translation lexicon learned by de-
ciphering large amounts of non parallel data
to improve a phrase-based MT system trained
with limited amounts of parallel data. In ex-
periments, we observe 1.2 to 1.8 BLEU gains
across three different test sets.
2 Previous Work
Motivated by the idea that a translation lexicon in-
duced from non parallel data can be applied to
MT, a variety of prior research has tried to build a
translation lexicon from non parallel or compara-
ble data (Rapp, 1995; Fung and Yee, 1998; Koehn
and Knight, 2002; Haghighi et al, 2008; Garera et
al., 2009; Bergsma and Van Durme, 2011; Daume?
and Jagarlamudi, 2011; Irvine and Callison-Burch,
2013b; Irvine and Callison-Burch, 2013a). Al-
though previous work is able to build a translation
lexicon without parallel data, little has used the lex-
icon to improve machine translation.
There has been increasing interest in learning
translation lexicons from non parallel data with de-
cipherment techniques (Ravi and Knight, 2011; Dou
and Knight, 2012; Nuhn et al, 2012). Decipher-
ment views one language as a cipher for another and
learns a translation lexicon that produces a good de-
cipherment.
In an effort to build a MT system without a paral-
lel corpus, Ravi and Knight (2011) view Spanish as a
cipher for English and apply Bayesian learning to di-
rectly decipher Spanish into English. Unfortunately,
their approach can only work on small data with lim-
ited vocabulary. Dou and Knight (2012) propose two
techniques to make Bayesian decipherment scalable.
First, unlike Ravi and Knight (2011), who deci-
pher whole sentences, Dou and Knight (2012) deci-
pher bigrams. Reducing a ciphertext to a set of bi-
grams with counts significantly reduces the amount
of cipher data. According to Dou and Knight (2012),
a ciphertext bigram F is generated through the fol-
lowing generative story:
? Generate a sequence of two plaintext tokens
e1e2 with probability P (e1e2) given by a lan-
guage model built from large numbers of plain-
text bigrams.
? Substitute e1 with f1 and e2 with f2 with prob-
ability P (f1|e1) ? P (f2|e2).
The probability of any cipher bigram F is:
P (F ) =
?
e1e2
P (e1e2)
2?
i=1
P (fi|ei)
Given a corpus of N cipher bigrams F1...FN , the
probability of the corpus is:
P (corpus) =
N?
j=1
P (Fj)
Given a plaintext bigram language model,
the goal is to manipulate P (f |e) to maximize
P (corpus). Theoretically, one can directly apply
EM to solve the problem (Knight et al, 2006). How-
ever, EM has time complexity O(N ? V 2e ) and space
complexity O(Vf ? Ve), where Vf , Ve are the sizes
of ciphertext and plaintext vocabularies respectively,
and N is the number of cipher bigrams.
Ravi and Knight (2011) apply Bayesian learning
to reduce the space complexity. Instead of esti-
mating probabilities P (f |e), Bayesian learning tries
to draw samples from plaintext sequences given ci-
phertext bigrams. During sampling, the probability
of any possible plaintext sample e1e2 is given as:
Psample(e1e2) = P (e1e2)
2?
i=1
Pbayes(fi|ei)
1669
misio?n de naciones unidas en oriente medio
misio?n de misio?n naciones
de naciones naciones unidas
naciones unidas misio?n en
unidas en en oriente
en oriente oriente medio
oriente medio
Table 1: Comparison of adjacent bigrams (left) and de-
pendency bigrams (right) extracted from the same Span-
ish text
with Pbayes(fi|ei) defined as:
Pbayes(fi|ei) =
?P0(fi|ei) + count(fi, ei)
?+ count(ei)
where P0 is a base distribution, and ? is a parameter
that controls how much we trust P0. count(fi, ei)
and count(ei) record the number of times fi, ei and
ei appear in previously generated samples respec-
tively.
At the end of sampling, P (fi|ei) is estimated by:
P (fi|ei) =
count(fi, ei)
count(ei)
However, Bayesian decipherment is still very
slow with Gibbs sampling (Geman and Geman,
1987), as each sampling step requires considering
Ve possibilities. Dou and Knight (2012) solve the
problem by introducing slice sampling (Neal, 2000)
to Bayesian decipherment.
3 From Adjacent Bigrams to Dependency
Bigrams
A major limitation of work by Dou and Knight
(2012) is their monotonic generative story for deci-
phering adjacent bigrams. While the generation pro-
cess works well for deciphering similar languages
(e.g. Spanish and French) without considering re-
ordering, it does not work well for languages that
are more different in grammar and word order (e.g.
Spanish and English). In this section, we first look
at why adjacent bigrams are bad for decipherment.
Then we describe how to use syntax to solve the
problem.
The left column in Table 1 contains adjacent bi-
grams extracted from the Spanish phrase ?misio?n
de naciones unidas en oriente medio?. The cor-
rect decipherment for the bigram ?naciones unidas?
should be ?united nations?. Since the deciphering
model described by Dou and Knight (2012) does
not consider word reordering, it needs to decipher
the bigram into ?nations united? in order to get
the right word translations ?naciones???nations?
and ?unidas???united?. However, the English lan-
guage model used for decipherment is built from En-
glish adjacent bigrams, so it strongly disprefers ?na-
tions united? and is not likely to produce a sensi-
ble decipherment for ?naciones unidas?. The Span-
ish bigram ?oriente medio? poses the same prob-
lem. Thus, without considering word reordering, the
model described by Dou and Knight (2012) is not a
good fit for deciphering Spanish into English.
However, if we extract bigrams based on depen-
dency relations for both languages, the model fits
better. To extract such bigrams, we first use de-
pendency parsers to parse both languages, and ex-
tract bigrams by putting head word first, followed
by the modifier.1 We call these dependency bi-
grams. The right column in Table 1 lists exam-
ples of Spanish dependency bigrams extracted from
the same Spanish phrase. With a language model
built with English dependency bigrams, the same
model used for deciphering adjacent bigrams is
able to decipher Spanish dependency bigram ?na-
ciones(head) unidas(modifier)? into ?nations(head)
united(modifier)?.
We might instead propose to consider word re-
ordering when deciphering adjacent bigrams (e.g.
add an operation to swap tokens in a bigram). How-
ever, using dependency bigrams has the following
advantages:
? First, using dependency bigrams avoids com-
plicating the model, keeping deciphering effi-
cient and scalable.
? Second, it addresses the problem of long dis-
tance reordering, which can not be modeled by
swapping tokens in bigrams.
Furthermore, using dependency bigrams al-
lows us to use dependency types to further
1As use of ?del? and ?de? in Spanish is much more frequent
than the use of ?of? in English, we skip those words by using
their head words as new heads if any of them serves as a head.
1670
improve decipherment. Suppose we have a
Spanish dependency bigram ?accepto?(verb) solici-
tud(object)?. Then all of the following English de-
pendency bigrams are possible decipherments: ?ac-
cepted(verb) UN(subject)?, ?accepted(verb) govern-
ment(subject)?, ?accepted(verb) request(object)?.
However, if we know the type of the Spanish depen-
dency bigram and use a language model built with
the same type in English, the only possible decipher-
ment is ?accepted(verb) request(object)?. If we limit
the search space, a system is more likely to find a
better decipherment.
4 Deciphering Spanish Gigaword
In this section, we compare dependency bigrams
with adjacent bigrams for deciphering Spanish into
English.
4.1 Data
We use the Gigaword corpus for our decipherment
experiments. The corpus contains news articles from
different news agencies and is available in Spanish
and English. We use only the AFP (Agence France-
Presse) section of the corpus in decipherment ex-
periments. We tokenize the corpus using tools that
come with the Europarl corpus (Koehn, 2005). To
shorten the time required for running different sys-
tems on large amounts of data, we keep only the top
5000 most frequent word types in both languages
and replace all other word types with UNK. We also
throw away lines with more than 40 tokens, as the
Spanish parser (Bohnet, 2010) we use is slow when
processing long sentences. After preprocessing, the
corpus contains approximately 440 million tokens in
Spanish and 350 million tokens in English. To ob-
tain dependency bigrams, we use the Bohnet parsers
(Bohnet, 2010) to parse both the Spanish and En-
glish version of the corpus.
4.2 Systems
Three systems are evaluated in the experiments. We
implement a baseline system, Adjacent, based on
Dou and Knight (2012). The baseline system col-
lects adjacent bigrams and their counts from Spanish
and English texts. It then builds an English bigram
language model using the English adjacent bigrams
and uses it to decipher the Spanish adjacent bigrams.
Dependency Types
Group 1 Verb/Subject
Group 2 Preposition/Preposition-Object,
Noun/Noun-Modifier
Group 3 Verb/Noun-Object
Table 2: Dependency relations divided into three groups
We build the second system, Dependency, using
dependency bigrams for decipherment. As the two
parsers do not output the same set of dependency re-
lations, we cannot extract all types of dependency
bigrams. Instead, we select a subset of dependency
bigrams whose dependency relations are shared by
the two parser outputs. The selected dependency re-
lations are: Verb/Subject, Verb/Noun-Object, Prepo-
sition/Object, Noun/Modifier. Decipherment runs
the same way as in the baseline system.
The third system, DepType, is built using both
dependent bigrams and their dependency types. We
first extract dependency bigrams for both languages,
then group them based on their dependency types.
As both parsers treat noun phrases dependent on
?del?, ?de?, and ?of? as prepositional phrases, we
choose to divide the dependency bigrams into 3
groups and list them in Table 2. A separate language
model is built for each group of English dependency
bigrams and used to decipher the group of Spanish
dependency bigrams with same dependency type.
For all the systems, language models are built us-
ing the SRILM toolkit (Stolcke, 2002). For the Ad-
jacent system, we use Good-Turing smoothing. For
the other systems, we use a mix of Witten-Bell and
Good-Turing smoothing.
4.3 Sampling Procedure
In experiments, we find that the iterative sam-
pling method described by Dou and Knight (2012)
helps improve deciphering accuracy. We also find
that combining results from different decipherments
helps find more correct translations at each iteration.
Thus, instead of using a single sampling process, we
use 10 different sampling processes at each iteration.
The details of the new sampling procedure are pro-
vided here:
? Extract dependency bigrams from parsing out-
puts and collect their counts.
1671
? Keep bigrams whose counts are greater than a
threshold ?. Then start 10 different randomly
seeded and initialized sampling processes. Per-
form sampling.
? At the end of sampling, extract word transla-
tion pairs (f, e) from the final sample. Esti-
mate translation probabilities P (e|f) for each
pair. Then construct a translation table by keep-
ing translation pairs (f, e) seen in more than
one decipherment and use the average P (e|f)
as the new translation probability.
? Lower the threshold ? to include more bigrams
into the sampling process. Start 10 differ-
ent sampling processes again and initialize the
first sample using the translation pairs obtained
from the previous step (for each Spanish token
f, choose an English token e whose P (e|f) is
the highest). Perform sampling again.
? Repeat until ? = 1.
4.4 Deciphering Accuracy
We choose the first 1000 lines of the monolingual
Spanish texts as our test data. The data contains
37,505 tokens and 6556 word types. We use type ac-
curacy as our evaluation metric: Given a word type
f in Spanish, we find a translation pair (f, e) with
the highest average P (e|f) from the translation ta-
ble learned through decipherment. If the translation
pair (f, e) can also be found in a gold translation
lexicon Tgold, we treat the word type f as correctly
deciphered. Let |C| be the number of word types
correctly deciphered, and |V | be the total number of
word types evaluated. We define type accuracy as
|C|
|V | .
To create Tgold, we use GIZA (Och and Ney,
2003) to align a small amount of Spanish-English
parallel text (1 million tokens for each language),
and use the lexicon derived from the alignment as
our gold translation lexicon. Tgold contains a subset
of 4408 types seen in the test data, among which,
2878 are also top 5000 frequent word types.
4.5 Results
During decipherment, we gradually increase the size
of Spanish texts and compare the learning curves of
three deciphering systems in Figure 2.
Figure 2: Learning curves for three decipherment sys-
tems. Compared with Adjacent (previous state of the art),
systems that use dependency bigrams improve decipher-
ing accuracy by over 500%.
With 100k tokens of Spanish text, the perfor-
mance of the three systems are similar. However, the
learning curve of Adjacent plateaus quickly, while
those of the dependency based systems soar up as
more data becomes available and still rise sharply
when the size of Spanish texts increases to 10 mil-
lion tokens, where the DepType system improves
deciphering accuracy of the Adjacent system from
4.2% to 24.6%. In the end, with 100 million tokens,
the accuracy of the DepType system rises to 27.0%.
The accuracy is even higher (41%), when evaluated
against the top 5000 frequent word types only.
5 Improving Machine Translation with
Decipherment
In this section, we demonstrate how to use a trans-
lation lexicon learned by deciphering large amounts
of in-domain (news) monolingual data to improve
a phrase-based machine translation system trained
with limited out-of-domain (politics) parallel data.
5.1 Data
We use approximately one million tokens of the Eu-
roparl corpus (Koehn, 2005) as our small out-of-
domain parallel training data and Gigaword as our
large in-domain monolingual training data to build
language models and a new translation lexicon to
improve a phrase-based MT baseline system. For
tuning and testing, we use the development data
1672
Parallel
Spanish English
Europarl 1.1 million 1.0 million
Tune-2008 52.6k 49.8k
Test-2009 68.1k 65.6k
Test-2010 65.5k 61.9k
Test-2011 79.4k 74.7k
Non Parallel
Spanish English
Gigaword 894 million 940 million
Table 3: Size of training, tuning, and testing data in num-
ber of tokens
from the NAACL 2012 workshop on statistical ma-
chine translation. The data contains test data in the
news domain from the 2008, 2009, 2010, and 2011
workshops. We use the 2008 test data for tuning and
the rest for testing. The sizes of the training, tuning,
and testing sets are listed in Table 3.
5.2 Systems
5.2.1 Baseline Machine Translation System
We build a state-of-the-art phrase-based MT sys-
tem, PBMT, using Moses (Koehn et al, 2007).
PBMT has 3 models: a translation model, a distor-
tion model, and a language model. We build a 5-
gram language model using the AFP section of the
English Gigaword. We train the other models using
the Europarl corpus. By default, Moses uses the fol-
lowing 8 features to score a candidate translation:
? direct and inverse translation probabilities
? direct and inverse lexical weighting
? a language model score
? a distortion score
? phrase penalty
? word penalty
The 8 features have weights adjusted on the tun-
ing data using minimum error rate training (MERT)
(Och, 2003). PBMT has a phrase table Tphrase.
During decoding, Moses copies out-of-vocabulary
(OOV) words, which can not be found in Tphrase,
directly to output. In the following sections, we de-
scribe how to use a translation lexicon learned from
large amounts of non parallel data to improve trans-
lation of OOV words, as well as words observed in
Tphrase.
5.2.2 Decipherment for Machine Translation
To achieve better decipherment, we:
? Increase the size of Spanish ciphertext from
100 million tokens to 894 million tokens.
? Keep top 50k instead of top 5k most frequent
word types of the ciphertext.
? Instead of seeding the sampling process ran-
domly, we use a translation lexicon learned
from a limited amount of parallel data as seed:
For each Spanish dependency bigram f1, f2,
where both f1 and f2 are found in the seed lex-
icon, we find the English sequence e1, e2 that
maximizes P (e1, e2)P (e1|f1)P (e2|f2). Other-
wise, for any Spanish token f that can be found
in the seed lexicon, we choose English word e,
where P (e|f) is the highest as the initial sam-
ple; for any f that are not seen in the seed lexi-
con, we do random initialization.
We perform 20 random restarts with 10k iter-
ations on each and build a word-to-word transla-
tion lexicon Tdecipher by collecting translation pairs
seen in at least 3 final decipherments with either
P (f |e) ? 0.2 or P (e|f) ? 0.2.
5.2.3 Improving Translation of Observed
Words with Decipherment
To improve translation of words observed in our
parallel corpus, we simply use Tdecipher as an addi-
tional parallel corpus. First, we filter Tdecipher by
keeping only translation pairs (f, e), where f is ob-
served in the Spanish part and e is observed in the
English part of the parallel corpus. Then we ap-
pend all the Spanish and English words in the fil-
tered Tdecipher to the end of Spanish part and En-
glish part of the parallel corpus respectively. The
training and tuning process is the same as the base-
line machine translation system PBMT. We denote
this system as Decipher-OBSV.
1673
5.2.4 Improving OOV translation with
Decipherment
As Tdecipher is learned from large amounts of in-
domain monolingual data, we expect that Tdecipher
contains a number of useful translations for words
not seen in the limited amount of parallel data (OOV
words). Instead of copying OOV words directly to
output, which is what Moses does by default, we try
to find translations from Tdecipher to improve trans-
lation.
During decoding, if a source word f is in Tphrase,
its translation options are collected from Tphrase ex-
clusively. If f is not in Tphrase but in Tdecipher,
the decoder will find translations from Tdecipher. If
f is not in either translation table, the decoder just
copies it directly to the output. We call this system
Decipher-OOV.
However, when an OOV?s correct translation is
same as its surface form and all its possible transla-
tions in Tdecipher are wrong, it is better to just copy
OOV words directly to output. This scenario hap-
pens frequently, as Spanish and English share many
common words. To avoid over trusting Tdecipher,
we add a new translation pair (f, f) for each source
word f in Tdecipher if the translation pair (f, f) is
not originally in Tdecipher. For each newly added
translation pair, both of its log translation probabil-
ities are set to 0. To distinguish the added transla-
tion pairs from the others learned through decipher-
ment, we add a binary feature ? to each translation
pair in Tdecipher. The final version of Tdecipher has
three feature scores: P (e|f), P (f |e), and ?. Finally,
we tune weights of the features in Tdecipher using
MERT (Och, 2003) on the tuning set.
5.2.5 A Combined Approach
In the end, we build a system Decipher-COMB,
which uses Tdecipher to improve translation of both
observed and OOV words with methods described in
sections 5.2.3 and 5.2.4.
5.3 Results
We tune each system three times with MERT and
choose the best weights based on BLEU scores on
tuning set.
Table 4 shows that the translation lexicon learned
from decipherment helps achieve higher BLEU
scores across tuning and testing sets. Decipher-
OBSV improves BLEU scores by as much as 1.2
points. We analyze the results and find the gain
mainly comes from two parts. First, adding Tdecipher
to small amounts of parallel corpus improves word
level translation probabilities, which lead to better
lexical weighting; second, Tdecipher contains new al-
ternative translations for words observed in the par-
allel corpus.
Moreover, Decipher-OOV also achieves better
BLEU scores compared with PBMT across all tun-
ing and test sets. We also observe that systems us-
ing Tdecipher learned by deciphering dependency bi-
grams leads to larger gains in BLEU scores. When
decipherment is used to improve translation of both
observed and OOV words, we see improvement in
BLEU score as high as 1.8 points on the 2010 news
test set.
The consistent improvement on the tuning and
different testing data suggests that decipherment is
capable of learning good translations for a number
of OOV words. To further demonstrate that our
decipherment approach finds useful translations for
OOV words, we list the top 10 most frequent OOV
words from both the tuning set and testing set as well
as their translations (up to three most likely transla-
tions) in Table 5. P (e|f) and P (f |e) are average
scores over different decipherment runs.
From the table, we can see that decipherment
finds correct translations (bolded) for 7 out of the
10 most frequent OOV words. Moreover, many
OOVs and their correct translations are homographs
, which makes copying OOVs directly to the output
a strong baseline to beat. Nonetheless, decipherment
still finds enough correct translations to improve the
baseline.
6 Conclusion
We introduce syntax for deciphering Spanish into
English. Experiment results show that using de-
pendency bigrams improves decipherment accuracy
by over 500% compared with the state-of-the-art
approach. Moreover, we learn a domain specific
translation lexicon by deciphering large amounts of
monolingual data and show that the lexicon can im-
prove a baseline machine translation system trained
with limited parallel data.
1674
Decipherment System Tune2008 Test2009 Test2010 Test2011
None PBMT (Baseline) 19.1 19.6 21.3 22.1
Adjacent
Decipher-OBSV 19.5 20.1 22.2 22.6
Decipher-OOV 19.4 19.9 21.7 22.5
Decipher-COMB 19.5 20.2 22.3 22.5
Dependency
Decipher-OBSV 19.7 20.5 22.5 23.0
Decipher-OOV 19.9 20.4 22.4 22.9
Decipher-COMB 20.0 20.8 23.1 23.4
Table 4: Systems that use translation lexicons learned from decipherment show consistent improvement over the
baseline system across tuning and testing sets. The best system, Decipher-COMB, achieves as much as 1.8 BLEU
point gain on the 2010 news test set.
Spanish English P (e|f) P (f |e)
obama his 0.33 0.01
bush 0.27 0.07
clinton 0.23 0.11
bush bush 0.47 0.45
yeltsin 0.28 0.81
he 0.24 0.05
festival event 0.68 0.35
festival 0.61 0.72
wikileaks zeta 0.03 0.33
venus venus 0.61 0.74
serena 0.47 0.62
colchones mattresses 0.55 0.73
cars 0.31 0.01
helado frigid 0.52 0.44
chill 0.37 0.14
sandwich 0.42 0.27
google microsoft 0.67 0.18
google 0.59 0.69
cantante singer 0.44 0.92
jackson 0.14 0.33
artists 0.14 0.77
mccain mccain 0.66 0.92
it 0.22 0.00
he 0.21 0.00
Table 5: Decipherment finds correct translations for 7 out
of 10 most frequent OOV word types.
7 Acknowledgments
This work was supported by NSF Grant 0904684
and ARO grant W911NF-10-1-0533. The authors
would like to thank David Chiang, Malte Nuhn,
Victoria Fossum, Ashish Vaswani, Ulf Hermjakob,
Yang Gao, and Hui Zhang (in no particular order)
for their comments and suggestions.
References
Shane Bergsma and Benjamin Van Durme. 2011. Learn-
ing bilingual lexicons using the visual similarity of
labeled web images. In Proceedings of the Twenty-
Second international joint conference on Artificial In-
telligence - Volume Volume Three. AAAI Press.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics. Coling.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies. Association for Com-
putational Linguistics.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Associa-
tion for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
1675
Linguistics - Volume 1. Association for Computational
Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In Proceed-
ings of the Thirteenth Conference on Computational
Natural Language Learning. Association for Compu-
tational Linguistics.
Stuart Geman and Donald Geman. 1987. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. In Readings in computer vision: is-
sues, problems, principles, and paradigms. Morgan
Kaufmann Publishers Inc.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013a. Combin-
ing bilingual and comparable corpora for low resource
machine translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation. Associ-
ation for Computational Linguistics, August.
Ann Irvine and Chris Callison-Burch. 2013b. Supervised
bilingual lexicon induction with multiple monolingual
signals. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: a parallel corpus for sta-
tistical machine translation. In In Proceedings of the
Tenth Machine Translation Summit, Phuket, Thailand.
Asia-Pacific Association for Machine Translation.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics: Long Papers - Volume 1. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics.
Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Association for
Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of the
51th Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing.
1676
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 233?243,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Aligning context-based statistical models of language
with brain activity during reading
Leila Wehbe
1,2
, Ashish Vaswani
3
, Kevin Knight
3
and Tom Mitchell
1,2
1
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA
2
Center for the Neural Basis of Computation, Carnegie Mellon University, Pittsburgh, PA
3
Information Sciences Institute, University of Southern California, Los Angeles, CA
lwehbe@cs.cmu.edu, vaswani@usc.edu, knight@isi.edu, tom.mitchell@cs.cmu.edu
Abstract
Many statistical models for natural language pro-
cessing exist, including context-based neural net-
works that (1) model the previously seen context
as a latent feature vector, (2) integrate successive
words into the context using some learned represen-
tation (embedding), and (3) compute output proba-
bilities for incoming words given the context. On
the other hand, brain imaging studies have sug-
gested that during reading, the brain (a) continu-
ously builds a context from the successive words
and every time it encounters a word it (b) fetches its
properties from memory and (c) integrates it with
the previous context with a degree of effort that is
inversely proportional to how probable the word is.
This hints to a parallelism between the neural net-
works and the brain in modeling context (1 and a),
representing the incoming words (2 and b) and in-
tegrating it (3 and c). We explore this parallelism to
better understand the brain processes and the neu-
ral networks representations. We study the align-
ment between the latent vectors used by neural net-
works and brain activity observed via Magnetoen-
cephalography (MEG) when subjects read a story.
For that purpose we apply the neural network to the
same text the subjects are reading, and explore the
ability of these three vector representations to pre-
dict the observed word-by-word brain activity.
Our novel results show that: before a new word i
is read, brain activity is well predicted by the neural
network latent representation of context and the pre-
dictability decreases as the brain integrates the word
and changes its own representation of context. Sec-
ondly, the neural network embedding of word i can
predict the MEG activity when word i is presented
to the subject, revealing that it is correlated with the
brain?s own representation of word i. Moreover, we
obtain that the activity is predicted in different re-
gions of the brain with varying delay. The delay is
consistent with the placement of each region on the
processing pathway that starts in the visual cortex
and moves to higher level regions. Finally, we show
that the output probability computed by the neural
networks agrees with the brain?s own assessment of
the probability of word i, as it can be used to predict
the brain activity after the word i?s properties have
been fetched from memory and the brain is in the
process of integrating it into the context.
1 Introduction
Natural language processing has recently seen a
surge in increasingly complex models that achieve
impressive goals. Models like deep neural net-
works and vector space models have become pop-
ular to solve diverse tasks like sentiment analy-
sis and machine translation. Because of the com-
plexity of these models, it is not always clear how
to assess and compare their performances as they
might be useful for one task and not the other.
It is also not easy to interpret their very high-
dimensional and mostly unsupervised representa-
tions. The brain is another computational system
that processes language. Since we can record brain
activity using neuroimaging, we propose a new di-
rection that promises to improve our understand-
ing of both how the brain is processing language
and of what the neural networks are modeling by
aligning the brain data with the neural networks
representations.
In this paper we study the representations of two
kinds of neural networks that are built to predict
the incoming word: recurrent and finite context
models. The first model is the Recurrent Neural
Network Language Model (Mikolov et al., 2011)
which uses the entire history of words to model
context. The second is the Neural Probabilistic
Language Model (NPLM) which uses limited con-
text constrained to the recent words (3 grams or 5
grams). We trained these models on a large Harry
Potter fan fiction corpus and we then used them to
predict the words of chapter 9 of Harry Potter and
the Sorcerer?s Stone (Rowling, 2012). In paral-
lel, we ran an MEG experiment in which 3 subject
read the words of chapter 9 one by one while their
brain activity was recorded. We then looked for
the alignment between the word-by-word vectors
produced by the neural networks and the word-by-
word neural activity recorded by MEG.
Our neural networks have 3 key constituents:
a hidden layer that summarizes the history of the
previous words ; an embeddings vector that sum-
marizes the (constant) properties of a given word
and finally the output probability of a word given
233
Reading comprehension is reflected in the subsequent acti-
vation of the left superior temporal cortex at 200?600 ms
(Halgren et al., 2002; Helenius et al., 1998; Pylkka?nen
et al., 2002, 2006; Pylkka?nen and Marantz, 2003; Simos
et al., 1997). This sustained activation differentiates
between words and nonwords (Salmelin et al., 1996; Wil-
son et al., 2005; Wydell et al., 2003). Apart from lexical-se-
mantic aspects it also seems to be sensitive to phonological
manipulation (Wydell et al., 2003).
As discussed above, in speech perception activation is
concentrated to a rather small area in the brain and we
have to rely on time information to dissociate between dif-
ferent processes. Here, the different processes are separable
both in timing and location. Because of that, one might
think that it is easier to characterize language-related pro-
cesses in the visual than auditory modality. However, here
the difficulties appear at another level. In reading, activa-
tion is detected bilaterally in the occipital cortex, along
the temporal lobes, in the parietal cortex and, in vocalized
reading, also in the frontal lobes, at various times with
respect to stimulus onset. Interindividual variability further
complicates the picture, resulting in practically excessive
amounts of temporal and spatial information. The areas
and time windows depicted in Fig. 5, with specific roles
in reading, form a limited subset of all active areas
observed during reading. In order to perform proper func-
tional localization one needs to vary the stimuli and tasks
systematically, in a parametric fashion. Let us now consid-
er how one may extract activation reflecting pre-lexical let-
ter-string analysis and lexical-semantic processing.
3.2. Pre-lexical analysis
In order to tease apart early pre-lexical processes in
reading, Tarkiainen and colleagues (Tarkiainen et al.,
1999) used words, syllables, and single letters, imbedded
in a noisy background, at four different noise levels
(Fig. 6). For control, the sequences also contained symbol
strings. One sequence was composed of plain noise stimuli.
The stimuli were thus varied along two major dimensions:
the amount of features to process increased with noise and
with the number of items, letters or symbols. On the other
hand, word-likeness was highest for clearly visible complete
words and lowest for symbols and noise.
At the level of the brain, as illustrated in Fig. 7, the data
showed a clear dissociation between two processes within
the first 200 ms: visual feature analysis occurred at about
100 ms after stimulus presentation, with the active areas
around the occipital midline, along the ventral stream. In
these areas, the signal increased with increasing noise and
with the number of items in the string, similarly for letters
and symbols. Only 50 ms later, at about 150 ms, the left
inferior occipitotemporal cortex showed letter-string spe-
cific activation. This signal increased with the visibility of
the letter strings. It was strongest for words, weaker for syl-
lables, and still weaker for single letters. Crucially, the acti-
vation was significantly stronger for letter than symbol
strings of equal length.
Bilateral occipitotemporal activation at about 200 ms
post-stimulus is consistently reported in MEG studies of
reading (Cornelissen et al., 2003b; Pammer et al., 2004; Sal-
melin et al., 1996, 2000b) but, interestingly, functional
specificity for letter-strings is found most systematically
in the left hemisphere. The MEG data on letter-string spe-
cific activation are in good agreement with intracranial
recordings, both with respect to timing and location and
the pre-lexical nature of the activation (Nobre et al., 1994).
3.3. Lexical-semantic analysis
To identify cortical dynamics of reading comprehension,
Helenius and colleagues (Helenius et al., 1998) employed a
Visual feature
analysis
Non-specific Words =Nonwords Nonwords
Letter-string
analysis
Time (ms)
0 400 800 0 400 800 0 400 800
Lexical-semantic
analysis
Fig. 5. Cortical dynamics of silent reading. Dots represent centres of active cortical patches collected from individual subjects. The curves display the
mean time course of activation in the depicted source areas. Visual feature analysis in the occipital cortex (!100 ms) is stimulus non-specific. The stimulus
content starts to matter by !150 ms when activation reflecting letter-string analysis is observed in the left occipitotemporal cortex. Subsequent activation
of the left superior temporal cortex at !200?600 ms reflects lexical-semantic analysis and, probably, also phonological analysis. Modified from Salmelin
et al. (2000a).
!L
ipp
inc
ott
W
illi
am
s&
W
ilk
ins
20
00
R. Salmelin / Clinical Neurophysiology xxx (2006) xxx?xxx 5
ARTICLE IN PRESS
Please cite this article as: Riitta Salmelin, Clinical neurophysiology of language: The MEG approach, Clinical Neurophysiology
(2006), doi:10.1016/j.clinph.2006.07.316
Figure 1: Cortical dynamics of silent reading. This figure
is adapted from (Salmelin, 2007). Dots represent projected
sources of activity in the visual cortex (left brain sketch) and
the temporal cortex (right brain sketch). The curves display
the m an time course of activation in the depicted ource ar-
eas for different conditions. The initial visual feature anal-
ysis in the visual cortex at ?100 ms is non-specific to lan-
guage. Comparing responses to letter strings and other vi-
sual stimuli rev als that letter string nalysis ccurs around
150 ms. Finally comparing the responses to words and non-
words (made-up words) reveals lexical-se antic analysis in
the temporal cortex at ?200-500ms.
the context. We set out to find the brain analogs
of these model constituents using an MEG decod-
ing task. We compare the different models and
h ir represen ations n term of how well they
can be used to decode the word being read from
MEG data. We obtain correspondences between
the models and the brain data that are consistent
with model of language processing in which
brain activity encodes story context, and where
each new word generates additional brain activity,
flowing generally from visual processing areas to
more high level areas, culminating in an updated
story cont xt, and reflecting a overall m gnitude
of neural effort influenced by the probability of
that new word given the previous context.
1.1 Neural processes involved in reading
Humans read with an average speed of 3 words
per second. Reading requires us to perceive in-
coming words and gradually integrate them into
a representation of the meaning. As words are
read, it takes 100ms for the visual input to reach
the visual cortex. 50ms later, the visual input is
processed as letter strings in a specialized region
of the left visual cortex (Salmelin, 2007). Be-
tween 200-500ms, the word?s semantic properties
are processed (see Fig. 1). Less is understood
about the cortical dynamics of word integration, as
multiple theories exist (Friederici, 2002; Hagoort,
2003).
Magnetoencephalography (MEG) is a brain-
imaging tool that is well suited for studying lan-
guage. MEG records the change in the magnetic
field on the surface of the head that is caused by
a large set of aligned neurons that are changing
their firing patterns in synchrony in response to
a stimulus. Because of the nature of the signal,
MEG recordings are directly related to neural ac-
tivity and have no latency. They are sampled at
a high frequency (typically 1kHz) that is ideal for
tracking the fast dynamics of language processing.
In this work, we are interested in the mecha-
nism of human text understanding as the meaning
of ncoming words is fetched from memory and
integrated with the context. Interestingly, this is
analogous to neural network models of language
that are used to predict the incoming word. The
mental representation of the previous context is
analogous to the latent layer of the neural network
which summarizes the relevant context before see-
ing the word. The representation of the meaning
of a word is analogous to the embedding that the
neural network learns in training and then uses.
Finally, one common hypotheses is that the brain
integrates the word with inversely proportional ef-
fort to how predictable the word is (Frank et al.,
2013). There is a well studied response known as
the N400 that is an increase of the activity in the
mporal cortex that has been recently shown to be
graded by the amount of surprisal of the incoming
word given the context (Frank et al., 2013). This is
analogous to the output probability of the incom-
ing word from the neural network.
Fig. 2 shows a hypothetical activity in an MEG
sensor as a subject reads a story in our experi-
ment, in which words are presented one at a time
for 500ms each. We conjecture that the activity in
time window a, i.e. before word i is understood, is
mostly related to the previous context before see-
ing word i. We also conjecture that the activity in
time window b is related to understanding word i
and integrating it into the context, leading to a new
representation of context in window c.
Using three types of features from neural net-
works (hidden layer context representation, output
probabilities and word embeddings) from three
different models of language (one recurrent model
and two finite context models), we therefore set to
predict the activity in the brain in different time
windows. We want to align the brain data with the
various model constituents to understand where
and when different types of processes are com-
puted in the brain, and simultaneously, we want to
234
Harry'
Harry'
had'
had'
never'
never'
embedding(iC1)' embedding(i)' embedding(i+1)'
context(iC1)'context(iC2)' context(i)'
out.'prob.(iC1)' out.'prob.(i)' out.'prob.(i+1)'
Studying'the'construc<on'of'meaning'
3'
word i+1 word i-1 word i 
0.5's'0.5's' 0.5's'
a b
c
Leila'Wehbe'
'?''Harry'''''''had''''''''never'?''
Figure 2: [Top] Sketch of the updates of a neural network
reading chapter 9 after it has been trained. Every word cor-
responds to a fixed embedding vector (magenta). A context
vector (blue) is computed before the word is seen given the
previous words. Given the context vector, the probability of
every word can be computed (symbolized by the histogram
in green). We only use the output probability of the actual
word (red circle). [Bottom] Hypothetical activity in an MEG
sensor when the subject reads the corresponding words. The
time periods approximated as a, b and c can be tested for in-
formation content relating to: the context of the story before
seeing word i (modeled by the context vector at i), the repre-
sentation of the properties of word i (the embedding of word
i) and the integration of word i into the context (the output
probability of word i). The periods drawn here are only a
conjecture on the timings of such cognitive events.
use the brain data to shed light on what the neural
network vectors are representing.
Related work
Decoding cognitive states from brain data is a
recent field that has been growing in popularity.
Most decoding studies that study language use
functional Magnetic Resonance Imaging (fMRI),
while some studies use MEG. MEG?s high tempo-
ral resolution makes it invaluable for looking at the
dynamics of language understanding. (Sudre et
al., 2012) decode from MEG the word a subject is
reading. The authors estimate from the MEG data
the semantic features of the word and use these as
an intermediate step to decode what the word is.
This is in principle similar to the classification ap-
proach we follow, as we will also use the feature
vectors as an intermediate step for word classifica-
tion. However the experimental paradigm in (Su-
dre et al., 2012) is to present to the subjects sin-
gle isolated words and to find how the brain rep-
resents their semantic features; whereas we have a
much more complex and ?naturalistic? experiment
in which the subjects read a non-artificial passage
of text, and we look at processes that exceed in-
dividual word processing: the construction of the
meanings of the successive words and the predic-
tion/integration of incoming words.
In (Frank et al., 2013), the amount of surprisal
that a word has given its context is used to pre-
dict the intensity of the N400 response described
previously. This is the closest study we could find
to our approach. This study was concerned with
analyzing the brain processes related only to sur-
prisal while we propose a more integral account
of the processes in the brain. The study also didn?t
address the major contribution we propose here,
which is to shed light on the inner constituents of
language models using brain imaging.
1.2 Recurrent and finite context neural
networks
Similar to standard language models, neural lan-
guage models also learn probability distributions
over words given their previous context. However,
unlike standard language models, words are rep-
resented as real-valued vectors in a high dimen-
sional space. These word vectors, referred to as
word embeddings, can be different for input and
output words, and are learned from training data.
Thus, although at training and test time, the in-
put and output to the neural language models are
one-hot representation of words, it is their em-
beddings that are used to compute word proba-
bility distributions. After training the embedding
vectors are fixed and it is these vectors that we
will use later on to predict MEG data. To predict
MEG data, we will also use the latent vector rep-
resentations of context that these neural networks
produce, as well as the probability of the current
word given the context. In this section, we will
describe how recurrent neural network language
models and feedforward neural probabilistic lan-
guage models compute word probabilities. In the
interest of space, we keep this description brief,
and for details, the reader is requested to refer to
the original papers describing these models.
235
w(t) s(t? 1)
y(t) c(t)
hidden
s(t)
output
P (w
t+1
| s(t))
D W
D
?
X
Figure 3: Recurrent neural network language model.
Recurrent Neural Network Language Model
Unlike standard feedforward neural language
models that only look at a fixed number of past
words, recurrent neural network language models
use all the previous history from position 1 to t?1
to predict the next word. This is typically achieved
by feedback connections, where the hidden layer
activations used for predicting the word in posi-
tion t ? 1 are fed back into the network to com-
pute the hidden layer activations for predicting the
next word. The hidden layer thus stores the history
of all previous words. We use the RNNLM archi-
tecture as described in Mikolov (2012), shown in
Figure 3. The input to the RNNLM at position t
are the one-hot representation of the current word,
w(t), and the activations from the hidden layer at
position t ? 1, s(t ? 1). The output of the hidden
layer at position t? 1 is
s(t) = ? (Dw(t) + Ws(t? 1)) ,
where D is the matrix of input word embeddings,
W is a matrix that transforms the activations from
the hidden layer in position t ? 1, and ? is a
sigmoid function, defined as ?(x) =
1
1+exp(?x)
,
that is applied elementwise. We need to compute
the probability of the next word w(t + 1) given
the hidden state s(t). For fast estimation of out-
put word probabilities, Mikolov (2012) divides the
computation into two stages: First, the probability
distribution over word classes is computed, after
which the probability distribution over the subset
of words belonging to the class are computed. The
class probability of a particular class with indexm
at position t is computed as:
P (c
m
(t) | s(t)) =
exp (s(t)Xv
m
)
?
C
c=1
(exp (s(t)Xv
c
))
,
where X is a matrix of class embeddings and v
m
is a one-hot vector representing the class with in-
dex m. The normalization constant is computed
u
1
u
2
input
words
input
embeddings
hidden
h
1
hidden
h
2
output
P (w | u)
D
?
M
C
1
C
2
D
Figure 4: Neural probabilistic language model
over all classes C. Each class specifies a subset
V
?
of words, potentially smaller than the entire vo-
cabulary V . The probability of an output word l at
position t + 1 given that its class is m is defined
as:
P (y
l
(t+ 1) | c
m
(t), s(t)) =
exp (s(t)D
?
v
l
)
?
V
?
k=1
(exp (s(t)D
?
v
k
))
,
where D
?
is a matrix of output word embeddings
and v
l
is a one hot vector representing the word
with index l. The probability of the word w(t+1)
given its class c
i
can now be computed as:
P (w(t+ 1) | s(t)) =P (w(t+ 1) | c
i
, s(t))
P (c
i
| s(t)).
Neural Probabilistic Language Model
We use the feedforward neural probabilistic lan-
guage model architecture of Vaswani et al. (2013),
as shown in Figure 4. Each context u comprises
a sequence of words u
j
(1 ? j ? n ? 1) repre-
sented as one-hot vectors, which are fed as input
to the neural network. At the output layer, the neu-
ral network computes the probability P (w | u) for
each word w, as follows.
The output of the first hidden layer h
1
is
h
1
= ?
?
?
n?1
?
j=1
C
j
Du
j
+ b
1
?
?
,
where D is a matrix of input word embeddings
which is shared across all positions, the C
j
are the
context matrices for each word in u, b
1
is a vec-
tor of biases with the same dimension as h
1
, and ?
is applied elementwise. Vaswani et al. (2013) use
rectified linear units (Nair and Hinton, 2010) for
236
the hidden layers h
1
and h
2
, which use the activa-
tion function ?(x) = max(0, x).
The output of the second layer h
2
is
h
2
= ? (Mh
1
+ b
2
) ,
where M is a weight matrix between h
1
and h
2
and b
2
is a vector of biases for h
2
. The probabil-
ity of the output word is computed at the output
softmax layer as:
P (w | u) =
exp
(
v
w
D
?
h
2
+ b
T
v
w
)
?
V
w
?
=1
exp (v
w
?
D
?
h
2
+ b
T
v
w
?
)
,
where D
?
is the matrix of output word embed-
dings, b is a vector of biases for every output word
and v
w
its the one hot representation of the word
w in the vocabulary.
2 Methods
We describe in this section our approach. In sum-
mary, we trained the neural network models on
a Harry Potter fan fiction database. We then ran
these models on chapter 9 of Harry Potter and the
Sorcerer?s Stone (Rowling, 2012) and computed
the context and embedding vectors and the output
probability for each word. In parallel, 3 subjects
read the same chapter in an MEG scanner. We
build models that predict the MEG data for each
word as a function of the different neural network
constituents. We then test these models with a
classification task that we explain below. We de-
tect correspondences between the neural network
components and the brain processes that under-
lie reading in the following fashion. If using a
neural network vector (e.g. the RNNLM embed-
ding vector) allows us to classify significantly bet-
ter than chance in a given region of the brain at
a given time (e.g. the visual cortex at time 100-
200ms), then we can hypothesize a relationship
between that neural network constituent and the
time/location of the analogous brain process.
2.1 Training the Neural Networks
We used the freely available training tools pro-
vided by Mikolov (2012)
1
and Vaswani et al.
(2013)
2
to train our RNNLM and NPLM models
used in our brain data classification experiments.
Our training data comprised around 67.5 million
1
http://rnnlm.org/
2
http://nlg.isi.edu/software/nplm
words for training and 100 thousand words for val-
idation from the Harry Potter fan fiction database
(http://harrypotterfanfiction.com). We restricted
the vocabulary to the top 100 thousand words
which covered all but 4 words from Chapter 9 of
Harry Potter and the Sorcerer?s Stone.
For the RNNLM, we trained models with differ-
ent hidden layers and learning rates and found the
RNNLM with 250 hidden units to perform best on
the validation set. We extracted our word embed-
dings from the input matrix D (Figure 3). We used
the default settings for all other hyper parameters.
We trained 3-gram and 5-gram NPLMs with
150 dimensional word embeddings and experi-
mented with different number of units for the first
hidden layer (h
1
in Figure 4), and different learn-
ing rates. For both the 3-gram and 5-gram mod-
els, we found 750 hidden units to perform the best
on the validation set and chose those models for
our final experiments. We used the output word
embeddings D
?
in our experiments. We visually
inspected the nearest neighbors in the 150 dimen-
sional word embedding space for some words and
didn?t find the neighbors from D
?
or D to be dis-
tinctly better than each other. We leave the com-
parison of input and output embeddings on brain
activity prediction for future work.
2.2 MEG paradigm
We recorded MEG data for three subjects (2 fe-
males and one male) while they read chapter 9
of Harry Potter and the Sorcerer?s Stone (Rowl-
ing, 2012). The participants were native English
speakers and right handed. They were chosen to
be familiar with the material: we made sure they
had read the Harry Potter books or seen the movies
series and were familiar with the characters and
the story. All the participants signed the consent
form, which was approved by the University of
Pittsburgh Institutional Review Board, and were
compensated for their participation.
The words of the story were presented in rapid
serial visual format (Buchweitz et al., 2009):
words were presented one by one at the center
of the screen for 0.5 seconds each. The text was
shown in 4 experimental blocks of ?11 minutes.
In total, 5176 words were presented. Chapter 9
was presented in its entirety without modifications
and each subject read the chapter only once.
One can think of an MEG machine as a large
helmet, with sensors located on the helmet that
237
record the magnetic activity. Our MEG recordings
were acquired on an Elekta Neuromag device at
the University of Pittsburgh Medical Center Pres-
byterian Hospital. This machine has 306 sensors
distributed into 102 locations on the surface of the
subject?s head. Each location groups 3 sensors or
two types: one magnometer that records the in-
tensity of the magnetic field and two planar gra-
diometers that record the change in the magnetic
field along two orthogonal planes
3
.
Our sampling frequency was 1kHz. For prepro-
cessing, we used Signal Space Separation method
(SSS, (Taulu et al., 2004)), followed by its tempo-
ral extension (tSSS, (Taulu and Simola, 2006)).
For each subject, the experiment data consists
therefore of a 306 dimensional time series of
length?45 minutes. We averaged the signal in ev-
ery sensor into 100ms non-overlapping time bins.
Since words were presented for 500ms each, we
therefore obtain for every word p = 306 ? 5 val-
ues corresponding to 306 vectors of 5 points.
2.3 Decoding experiment
To find which parts of brain activity are related to
the neural network constituents (e.g. the RNNLM
context vector), we run a prediction and classifica-
tion experiment in a 10-fold cross validated fash-
ion. At every fold, we train a linear model to pre-
dict MEG data as a function of one of the feature
sets, using 90% of the data. On the remaining 10%
of the data, we run a classification experiment.
MEG data is very noisy. Therefore, classify-
ing single word waveforms yields a low accuracy,
peaking at 60%, which might lead to false nega-
tives when looking for correspondences between
neural network features and brain data. To reveal
informative features, one can boost signal by ei-
ther having several repetitions of the stimuli in the
experiment and then averaging (Sudre et al., 2012)
or by combining the words into larger chunks (We-
hbe et al., 2014). We chose the latter because the
former sacrifices word and feature diversity.
At testing, we therefore repeat the following
300 times. Two sets of words are chosen ran-
domly from the test fold. To form the first set, 20
words are sampled without replacement from the
test sample (unseen by the classifier). To form the
second set, the k
th
word is chosen randomly from
all words in the test fold having the same length as
3
In this paper, we treat these three different sensors as
three different dimensions without further exploiting their
physical properties.
the k
th
word of the first set. Since every fold of
the data was used 9 times in the training phase and
once in the testing phase, and since we use a high
number of randomized comparisons, this averages
out biases in the accuracy estimation. Classifying
sets of 20 words improves the classification accu-
racy greatly while lowering its variance and makes
it dissociable from chance performance. We com-
pare only between words of equal length, to mini-
mize the effect of the low level visual features on
the classification accuracy.
After averaging out the results of multiple folds,
we end up with average accuracies that reveal how
related one of the models? constituents (e.g. the
RNNLM context vector) is to brain data.
2.3.1 Annotation of the stimulus text
We have 9 sets of annotations for the words of the
experiment. Each set j can be described as a ma-
trix F
j
in which each row i corresponds to the vec-
tor of annotations of word i. Our annotations cor-
respond to the 3 model constituents for each of the
3 models: the hidden layer representation before
word i, the output probability of word i and the
learned embeddings for word i.
2.3.2 Classification
In order to align the brain processes and the differ-
ent constituents of the different models, we use a
classification task. The task is to classify the word
a subject is reading out of two possible choices
from its MEG recording. The classifier uses one
type of feature in an intermediate classification
step. For example, the classifier learns to predict
the MEG activity for any setting of the RNNLM
hidden layer. Given an unseen MEG recording for
an unknown word i and two possible story words
i
?
and i
??
(one of which being the true word i), the
classifier predicts the MEG activity when reading
i
?
and i
??
from their hidden layer vectors. It then
assigns the label i
?
or i
??
to the word recording i
depending on which prediction is the closest to the
recording. The following are the detailed steps of
this complex classification task. However, for the
rest of the paper the most useful point to keep in
mind is that the main purpose of the classification
is to find a correspondence between the brain data
and a given feature set j.
1. Normalize the columns of M (zero mean,
standard deviation = 1). Pick feature set F
j
and normalize its columns to a minimum of 0
and a maximum of 1.
238
2. Divide the data into 10 folds, for each fold b:
(a) Isolate M
b
and F
b
j
as test data. The re-
mainder M
?b
and F
?b
j
will be used for
training
4
.
(b) Subtract the mean of the columns of
M
?b
from M
b
and M
?b
and the mean
of the columns of F
?b
j
from F
b
j
and F
?b
j
(c) Use ridge regression to solve
M
?b
= F
?b
j
? ?
t
j
by tuning the ? parameter to every one
of the p output dimensions indepen-
dently. ? is chosen via generalized cross
validation (Golub et al., 1979).
(d) Perform a binary classification. Sample
from the set of words in b a set c of 20
words. Then sample from b another set
of 20 words such that the k
th
word in c
and d have the same number of letters.
For every sample (c,d):
i. predict the MEG data for c and d as:
P
c
= F
c
j
? ?
b
j
and P
d
= F
d
j
? ?
b
j
ii. assign to M
c
the label c or d depend-
ing on which of P
c
or P
d
is closest
(Euclidean distance).
iii. assign to M
d
the label c or d de-
pending on which of P
c
or P
d
is
closest (Euclidean distance).
3. Compute the average accuracy.
2.3.3 Restricting the analysis spatially: a
searchlight equivalent
We adapt the searchlight method (Kriegeskorte et
al., 2006) to MEG. The searchlight is a discovery
procedure used in fMRI in which a cube is slid
over the brain and an analysis is performed in each
location separately. It allows to find regions in the
brain where a specific phenomenon is occurring.
In the MEG sensor space, for every one of the 102
sensor locations `, we assign a group of sensors g
`
.
For every location `, we identify the locations that
immediately surround it in any direction (Anterior,
Right Anterior, Right etc...) when looking at the
2D flat representation of the location of the sensors
in the MEG helmet (see Fig. 9 for an illustration of
the 2D helmet). g
`
therefore contains the 3 sensors
at location ` and at the neighboring locations. The
maximum number of sensors in a group is 3 ? 9.
4
The rows from M
?b
and F
?b
j
that correspond to the five
words before or after the test set are ignored in order to make
the test set independent.
The locations at the edge of the helmet have fewer
sensors because of the missing neighbor locations.
2.3.4 Restricting the analysis temporally
Instead of using the entire time course of the word,
we can use only one of the corresponding 100ms
time windows. Obtaining a high classification ac-
curacy using one of the time windows and feature
set j means that the analogous type of information
is encoded at that time.
2.3.5 Classification accuracy by time and
region
The above steps compute whole brain accuracy us-
ing all the time series. In order to perform a more
precise spatio-temporal analysis, one can use only
one time windowm and one location ` for the clas-
sification. This can answer the question of when
and where different information is represented by
brain activity. For every location, we will use only
the columns corresponding to the time pointm for
the sensors belonging to the group g
`
. Step (d) of
the classification procedure is changed as such:
(d) Perform a binary classification. Sample from
the set of words in b a set c of 20 words. Then
sample from b another set of 20 words such
that the k
th
word in c and d have the same
number of letters. For every sample (c,d), and
for every setting of {m, `}:
i. predict the MEG data for c and d as:
P
c
{m,`}
= F
c
j
? ?
b
j,{m,`}
and
P
d
{m,`}
= F
d
j
? ?
b
j,{m,`}
ii. assign to M
c
{m,`}
the label c or d depend-
ing on which of P
c
{m,`}
or P
d
{m,`}
is clos-
est (Euclidean distance).
iii. assign to M
d
{m,`}
the label c or d depend-
ing on which of P
c
{m,`}
or P
d
{m,`}
is clos-
est (Euclidean distance).
2.3.6 Statistical significance testing
We determine the distribution for chance perfor-
mance empirically. Because the successive word
samples in our MEG and feature matrices are not
independent and identically distributed, we break
the relationship between the MEG and feature ma-
trices by shifting the feature matrices by large de-
lays (e.g. 2000 to 2500 words) and we repeat
the classification using the delayed matrices. This
simulates chance performance more fairly than a
permutation test because it keeps the time struc-
ture of the matrices. It was used in (Wehbe et al.,
239
2014) and inspired by (Chwialkowski and Gret-
ton, 2014). For every {m, `} setting we can there-
fore compute a standardized z-value by subtract-
ing the mean of the shifted classifications and di-
viding by the standard deviation. We then com-
pute the p-value for the true classification accu-
racy being due to chance. Since the three p-values
for the three subjects for a given {m, `} are inde-
pendent, we combine them using Fisher?s method
for independent test statistics (Fisher, 1925). The
statistics we obtain for every {m, `} are depen-
dent because they comprise nearby time and space
windows. We control the false discovery rate us-
ing (Benjamini and Yekutieli, 2001) to adjust for
the testing at multiple locations and time windows.
This method doesn?t assume any kind of indepen-
dence or positive dependence.
3 Results
We present in Fig. 5 the accuracy using all the time
windows and sensors. In Fig. 6 we present the
classification accuracy when running the classifi-
cation at every time window exclusively. In Fig. 9
we present the accuracy when running the classifi-
cation using different time windows and groups of
sensors centered at every one of the 102 locations.
It is important to lay down some conventions
to understand the complex results in these plots.
To recap, we are trying to find parallels between
model constituents and brain processes. We use:
? a subset of the data (for example the time
window 0-100ms and all the sensors)
? one type of feature (for example the hidden
context layer from the NPLM 3g model)
and we obtain a classification accuracy A. If A
is low, there is probably no relationship between
the feature set and the subset of data. If A is high,
it hints to an association between the subset of data
and the mental process that is analogous to the fea-
ture set. For example, when using all the sensors
and time window 0-100ms, along with the NPLM
3g hidden layer, we obtain an accuracy of 0.70
(higher than chance with p < 10
?14
, see Fig. 6).
Since the NPLM 3g hidden layer summarizes the
context of the story before seeing word i, this sug-
gests that the brain is still processing the context
of the story before word i between 0-100ms.
Fig. 6 shows the accuracy for different types
of features when using all of the time points and
all the sensors to classify a word. We can see
NPLM 3g NPLM 5g RNNLM0.6
0.8
1
class
ificati
on ac
curac
y
 
 hidden layer output probability embeddings
hidden layer output probability embeddings0.6
0.8
1
class
ificati
on ac
curac
y
 
 NPLM 3g NPLM 5g RNNLM
Figure 5: Average accuracy using all time windows and
sensors, grouped by model (top) and type of feature (bot-
tom). All accuracies are significantly higher than chance
(p < 10
?8
).
0 200 4000.5
0.6
0.7
NPLM 3g
0 200 4000.5
0.6
0.7
NPLM 5g
0 200 4000.5
0.6
0.7
RNNLM
 
 hidden layeroutput probabilityembeddings
Figure 6: Average accuracy in different time windows
when using different types of features as input to the clas-
sifier, for different models. Accuracy is plotted in the center
of the respective time window. Points marked with a circle
are significantly higher than chance accuracy for the given
feature set and time window after correction.
similar classification accuracies for the three types
of models, with RNNLM ahead for the hidden
layer and embeddings and behind for the output
probability features. The hidden layer features
are the most powerful for classification. Between
the three types of features, the hidden layer fea-
tures are the best at capturing the information con-
tained in the brain data, suggesting that most of
the brain activity is encoding the previous context.
The embedding features are the second best. Fi-
nally the output probability have the smallest ac-
curacies. This makes sense considering that they
capture much less information than the other two
high dimensional descriptive vectors, as they do
not represent the complex properties of the words,
only a numerical assessment of their likelihood.
Fig. 6 shows the accuracy when using different
windows of time exclusively, for the 100ms time
240
windows starting at 0, 100 . . . 400ms after word
presentation. We can see that using the embed-
ding vector becomes increasingly more useful for
classification until 300-400ms, and then its perfor-
mance starts decreasing. This results aligns with
the following hypothesis: the word is being per-
ceived and understood by the brain gradually after
its presentation, and therefore the brain represen-
tation of the word becomes gradually similar to the
neural network representation of the word (i.e. the
embedding vector). The output probability feature
accuracy peaks at a later time than the embeddings
accuracy. Obtaining a higher than chance accu-
racy at time window m using the output probabil-
ity as input to the classifier suggests strongly that
the brain is integrating the word at time window
m, because it is responding differently for pre-
dictable and unpredictable words
5
. The integra-
tion step happens after the perception step, which
is probably why the output probability curves peak
later than the embeddings curves.
?500 0 500 10000.5
0.6
0.7
0.8
Hidden Layer
 
 NPLM 3GNPLM 5GRNNLM
Figure 7: Average accuracy in time for the different hidden
layers. The analysis is extended to the time windows before
and after the word is presented, the input feature is restricted
to be the hidden layer before the central word is seen. The
first vertical bar indicates the onset of the word, the second
one indicates the end of its presentation.
?1000 0 10000.6
0.8 Subject 1
?1000 0 10000.6
0.8 Subject 2
?1000 0 10000.6
0.8 Subject 3
 
 
hidden layer output probability embeddings
Figure 8: Accuracy in time when using the RNNLM fea-
tures for each of the three subjects.
To understand the time dynamics of the hidden
layer accuracy we need to see a larger time scale
than the word itself. The hidden layer captures the
5
the fact that we can classify accurately during windows
300-400ms indicates that the classifier is taking advantage of
the N400 response discussed in the introduction
context before word i is seen. Therefore it seems
reasonable that the hidden layer is not only related
to the activity when the word is on the screen, but
also related to the activity before the word is pre-
sented, which is the time when the brain is inte-
grating the previous words to build that context.
On the other hand, as the word i and subsequent
words are integrated, the context starts diverging
from the context of word i (computed before see-
ing word i). We therefore ran the same analysis
as before, but this time we also included the time
windows before and after word i in the analysis,
while maintaining the hidden layer vector to be the
context before word i is seen. We see the behav-
ior we predicted in the results: the context before
seeing word i becomes gradually more useful for
classification until word i is seen, and then it grad-
ually decreases until it is no longer useful since
the context has changed. We observe the RNNLM
hidden layer has a higher classification accuracy
than the finite context NPLMs. This might be due
to the fact that the RNNLM has a more complete
representation of context that captures more of the
properties of the previous words.
To show the consistency of the results, we plot
as illustration the three curves we obtain for each
subject for the RNNLM (Fig. 8). The patterns
seem very consistent indicating the phenomena we
described can be detected at the subject level.
We now move on to the spatial decomposition
of the analysis. When the visual input enters the
brain, it first reaches the visual cortex at the back
of the head, and then moves anteriorly towards the
left and right temporal cortices and eventually the
frontal cortex. As it flows through these areas, it
is processed to higher levels of interpretations. In
Fig. 9, we plot the accuracy for different regions
of the brain and different time windows for the
RNNLM features. To make the plots simpler we
multiplied by zero the accuracies which were not
significantly higher than chance. We expand a few
characteristic plots. We see that in the back of the
head the embedding features have an accuracy that
seems to peak very early on. As we move forward
in the brain towards the left and right temporal cor-
tices, we see the embeddings accuracy peaking at
a later time, reflecting the delay it takes for the in-
formation to reach this part of the brain. The out-
put probability start being useful for classification
after the embeddings, and specifically in the left
temporal cortex which is the cite where the N400
241
Back%
 
 
hidden layer
output probability
embeddings 0 5000.50.6
0.7
time (s)ac
cur
acy
Le(% Right%
 
 
hidden layer
output probability
embeddings 0 5000.5
0.60.7
time (s)a
ccu
rac
y
 
 
hidden layer
output probability
embeddings 0 5000.5
0.60.7
time (s)a
ccu
rac
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
 
 
hidden layer
output probability
embeddings
0 500
0.5
0.6
0.7
time (s)
a
c
c
u
ra
c
y
Figure 9: Average accuracy in time and space on the MEG helmet when using the RNNLM features. For each of the 102
locations the average accuracy for the group of sensors centered at that location is plotted versus time. The axes are defined
in the rightmost, empty plot. Three plots have been magnified to show the increasing delay in high accuracy when using the
embeddings feature, reflecting the delay in processing the incoming word as information travels through the brain. A sensor
map is provided in the lower right corner: visual cortex = cyan, temporal = red, frontal = dark green.
is reported in the literature. Finally, as we reach
the frontal cortex, we see that the embeddings fea-
tures have an even later accuracy peak.
4 Conclusion and contributions
Novel brain data exploration We present here
a novel and revealing approach to shed light on
the brain processes involved in reading. This is a
departure from the classical approach of control-
ling for a few variables in the text (e.g. showing
a sentence with an expected target word versus an
unexpected one). While we cannot make clear cut
causal claims because we did not control for our
variables, we are able to explore the data much
more and offer a much richer interpretation than
is possible with artificially constrained stimuli.
Comparing two models of language Adding
brain data into the equation allowed us to com-
pare the performance of the models and to identify
a slight advantage for the RNNLM in capturing
the text contents. Numerical comparison is how-
ever a secondary contribution of our approach. We
showed that it might be possible to use brain data
to understand, interpret and illustrate what exactly
is being encoded by the obscure vectors that neural
networks compute, by drawing parallels between
the models constituents and brain processes.
Anecdotally, in the process of running the ex-
periments, we noticed that the accuracy for the
hidden layer of the RNNLM was peaking in the
time window corresponding to word i?2, and that
it was decreasing during word i ? 1. Since this
was against our expectations, we went back and
looked at the code and found that it was indeed
returning a delayed value and corrected the fea-
tures. We therefore used the brain data in order to
correct a mis-specification in our neural network
model. This hints if not proves the potential of our
approach for assessing language models.
Future Work The work described here is our
first attempt along the promising endeavor of
matching complex computational models of lan-
guage with brain processes using brain recordings.
We plan to extend our efforts by (1) collecting data
from more subjects and using various types of text
and (2) make the brain data help us with training
better statistical language models by using it to de-
termine whether the models are expressive enough
or have reached a sufficient degree of convergence.
Acknowledgements
This research was supported in part by NICHD
grant 5R01HD07328-02. We thank Nicole Rafidi
for help with data acquisition.
242
References
Yoav Benjamini and Daniel Yekutieli. 2001. The con-
trol of the false discovery rate in multiple testing un-
der dependency. Annals of statistics, pages 1165?
1188.
Augusto Buchweitz, Robert A Mason, L?eda Tomitch,
and Marcel Adam Just. 2009. Brain activation
for reading and listening comprehension: An fMRI
study of modality effects and individual differences
in language comprehension. Psychology & neuro-
science, 2(2):111?123.
Kacper Chwialkowski and Arthur Gretton. 2014.
A kernel independence test for random processes.
arXiv preprint arXiv:1402.4501.
Ronald Aylmer Fisher. 1925. Statistical methods for
research workers. Genesis Publishing Pvt Ltd.
Stefan L Frank, Leun J Otten, Giulia Galli, and
Gabriella Vigliocco. 2013. Word surprisal predicts
N400 amplitude during reading. In Proceedings of
the 51st annual meeting of the Association for Com-
putational Linguistics, pages 878?883.
Angela D Friederici. 2002. Towards a neural basis
of auditory sentence processing. Trends in cognitive
sciences, 6(2):78?84.
Gene H Golub, Michael Heath, and Grace Wahba.
1979. Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics,
21(2):215?223.
Peter Hagoort. 2003. How the brain solves the binding
problem for language: a neurocomputational model
of syntactic processing. Neuroimage, 20:S18?S29.
Nikolaus Kriegeskorte, Rainer Goebel, and Peter Ban-
dettini. 2006. Information-based functional brain
mapping. Proceedings of the National Academy
of Sciences of the United States of America,
103(10):3863?3868.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and J Cernocky. 2011. RNNLM-
recurrent neural network language modeling toolkit.
In Proc. of the 2011 ASRU Workshop, pages 196?
201.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of ICML, pages 807?814.
Joanne K. Rowling. 2012. Harry Potter and the Sor-
cerer?s Stone. Harry Potter US. Pottermore Limited.
Riitta Salmelin. 2007. Clinical neurophysiology of
language: the MEG approach. Clinical Neurophysi-
ology, 118(2):237?254.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking neural coding of percep-
tual and semantic features of concrete nouns. Neu-
roImage, 62(1):451?463.
Samu Taulu and Juha Simola. 2006. Spatiotem-
poral signal space separation method for rejecting
nearby interference in MEG measurements. Physics
in medicine and biology, 51(7):1759.
Samu Taulu, Matti Kajola, and Juha Simola. 2004.
Suppression of interference and artifacts by the sig-
nal space separation method. Brain topography,
16(4):269?275.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation.
Leila Wehbe, Brian Murphy, Partha Talukdar, Alona
Fyshe, Aaditya Ramdas, and Tom Mitchell. 2014.
Simultaneously uncovering the patterns of brain re-
gions involved in different story reading subpro-
cesses. in press.
243
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 425?429,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Aligning English Strings with Abstract Meaning Representation Graphs
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{damghani,yanggao,ulf,knight}@isi.edu
Abstract
We align pairs of English sentences and
corresponding Abstract Meaning Repre-
sentations (AMR), at the token level. Such
alignments will be useful for downstream
extraction of semantic interpretation and
generation rules. Our method involves
linearizing AMR structures and perform-
ing symmetrized EM training. We obtain
86.5% and 83.1% alignment F score on de-
velopment and test sets.
1 Introduction
Banarescu et al. (2013) describe a semantics bank
of English sentences paired with their logical
meanings, written in Abstract Meaning Represen-
tation (AMR). The designers of AMR leave open
the question of how meanings are derived from
English sentences (and vice-versa), so there are
no manually-annotated alignment links between
English words and AMR concepts. This paper
studies how to build such links automatically, us-
ing co-occurrence and other information. Auto-
matic alignments may be useful for downstream
extraction of semantic interpretation and genera-
tion rules.
AMRs are directed, acyclic graphs with labeled
edges, e.g., the sentence The boy wants to go is
represented as:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
We have hand-aligned a subset of the 13,050
available AMR/English pairs. We evaluate our
automatic alignments against this gold standard.
A sample hand-aligned AMR is here (??n? speci-
fies a link to the nth English word):
the boy wants to go
(w / want-01?3
:arg0 (b / boy?2)
:arg1 (g / go-01?5
:arg0 b))
This alignment problem resembles that of statisti-
cal machine translation (SMT). It is easier in some
ways, because AMR and English are highly cog-
nate. It is harder in other ways, as AMR is graph-
structured, and children of an AMR node are un-
ordered. There are also fewer available training
pairs than in SMT.
One approach is to define a generative model
from AMR graphs to strings. We can then use
EM to uncover hidden derivations, which align-
ments weakly reflect. This approach is used in
string/string SMT (Brown et al., 1993). How-
ever, we do not yet have such a generative graph-
to-string model, and even if we did, there might
not be an efficient EM solution. For exam-
ple, in syntax-based SMT systems (Galley et al.,
2004), the generative tree/string transduction story
is clear, but in the absence of alignment con-
straints, there are too many derivations and rules
for EM to efficiently consider.
We therefore follow syntax-based SMT custom
and use string/string alignment models in align-
ing our graph/string pairs. However, while it is
straightforward to convert syntax trees into strings
data (by taking yields), it is not obvious how to do
this for unordered AMR graph elements. The ex-
ample above also shows that gold alignment links
reach into the internal nodes of AMR.
Prior SMT work (Jones et al., 2012) describes
alignment of semantic graphs and strings, though
their experiments are limited to the GeoQuery do-
main, and their methods are not described in de-
tail. Flanigan et al (2014) describe a heuristic
AMR/English aligner. While heuristic aligners
can achieve good accuracy, they will not automat-
ically improve as more AMR/English data comes
425
online.
The contributions of this paper are:
? A set of gold, manually-aligned
AMR/English pairs.
? An algorithm for automatically aligning
AMR/English pairs.
? An empirical study establishing alignment
accuracy of 86.5% and 83.1% F score for de-
velopment and test sets respectively.
2 Method
We divide the description of our method into three
parts: preprocessing, training, and postprocessing.
In the preprocessing phase, we linearize the AMR
graphs to change them into strings, clean both the
AMR and English sides by removing stop words
and simple stemming, and add a set of correspond-
ing AMR/English token pairs to the corpus to help
the training phase. The training phase is based
on IBM models, but we modify the learning algo-
rithm to learn the parameters symmetrically. Fi-
nally, in the postprocessing stage we rebuild the
aligned AMR graph. These components are de-
scribed in more detail below.
2.1 Preprocessing
The first step of the preprocessing component is to
linearize the AMR structure into a string. In this
step we record the original structure of nodes in
the graph for later reconstruction of AMR. AMR
has a rooted graph structure. To linearize this
graph we run a depth first search from the root and
print each node as soon as it it visited. We print
but not expand the nodes that are seen previously.
For example the AMR:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
is linearized into this order: w / want-01 :arg0 b /
boy :arg1 g / go-01 :arg0 b.
Note that semantically related nodes often stay
close together after linearization.
After linearizing the AMR graph into a string,
we perform a series of preprocessing steps includ-
ing lowercasing the letters, removing stop words,
and stemming.
The AMR and English stop word lists are gen-
erated based on our knowledge of AMR design.
We know that tokens like an, the or to be verbs
will very rarely align to any AMR token; similarly,
AMR role tokens like :arg0, :quant, :opt1 etc. as
well as the instance-of token /, and tokens like
temporal-quantity or date-entity rarely align to any
English token. We remove these tokens from the
parallel corpus, but remember their position to be
able to convert the resulting string/string align-
ment back into a full AMR graph/English string
alignment. Although some stopwords participate
in gold alignments, by removing them we will buy
a large precision gain for some recall cost.
We remove the word sense indicator and quo-
tation marks for AMR concepts. For instance we
will change want-01 to want and ?ohio? to ohio.
Then we stem AMR and English tokens into their
first four letters, except for role tokens in AMR.
The purpose of stemming is to normalize English
morphological variants so that they are easier to
match to AMR tokens. For example English to-
kens wants, wanting, wanted, and want as well as
the AMR token want-01 will all convert to want
after removing the AMR word sense indicator and
stemming.
In the last step of preprocessing, we benefit
from the fact that AMR concepts and their cor-
responding English ones are frequently cognates.
Hence, after stemming, an AMR token often can
be translated to a token spelled similarly in En-
glish. This is the case for English token want and
AMR token want in the previous paragraph. To
help the training model learn from this fact, we
extend our sentence pair corpus with the set of
AMR/English token pairs that are spelled identi-
cally after preprocessing. Also, for English tokens
that can be translated into multiple AMR tokens,
like higher and high :degree more we add the cor-
responding string/string pairs to the corpus. This
set is extracted from existing lexical resources, in-
cluding lists of comparative/superlative adjectives,
negative words, etc.
After preprocessing, the AMR at the start of
this section will change into: want boy go and
the sentence The boy wants to go changes into boy
want to go, and we will also add the identity pairs
want/want, boy/boy, and go/go to the corpus.
2.2 Training
Our training method is based on IBM word align-
ment models (Brown et al., 1993). We modify
the objective functions of the IBM models to en-
426
courage agreement between learning parameters
in English-to-AMR and AMR-to-English direc-
tions of EM. The solution of this objective func-
tion can be approximated in an extremely simple
way that requires almost no extra coding effort.
Assume that we have a set of sentence pairs
{(E,A)}, where each E is an English sentence
and each A is a linearized AMR. According to
IBM models, A is generated from E through a
generative story based on some parameters.
For example, in IBM Model 2, given E we
first decide the length of A based on some prob-
ability l = p(len(A)|len(E)), then we decide
the distortions based on a distortion table: d =
p(i|j, len(A), len(E)). Finally, we translate En-
glish tokens into AMR ones based on a translation
table t = p(a|e) where a and e are AMR and En-
glish tokens respectively.
IBM models estimate these parameters to max-
imize the conditional likelihood of the data:
?
A|E
= argmaxL
?
A|E
(A|E) or ?
E|A
=
argmaxL
?
E|A
(E|A) where ? denotes the set of
parameters. The conditional likelihood is intrinsic
to the generative story of IBM models. However,
word alignment is a symmetric problem. Hence it
is more reasonable to estimate the parameters in a
more symmetric manner.
Our objective function in the training phase is:
?
A|E
, ?
E|A
= argmaxL
?
A|E
(A|E)+L
?
E|A
(E|A)
subject to ?
A|E
?
E
= ?
E|A
?
A
= ?
A,E
We approximate the solution of this objective
function with almost no change to the existing
implementation of the IBM models. We relax
the constraint to ?
A|E
= ?
E|A
, then apply the
following iterative process:
1. Optimize the first part of the objective func-
tion: ?
A|E
= argmaxL
?
A|E
(A|E) using EM
2. Satisfy the constraint: set ?
E|A
? ?
A|E
3. Optimize the second part of the objective
function: ?
E|A
= argmaxL
?
E|A
(E|A)
using EM
4. Satisfy the constraint: set ?
A|E
? ?
E|A
5. Iterate
Note that steps 1 and 3 are nothing more than
running the IBM models, and steps 2 and 4 are
just initialization of the EM parameters, using ta-
bles from the previous iteration. The initialization
steps only make sense for the parameters that in-
volve both sides of the alignment (i.e., the transla-
tion table and the distortion table). For the trans-
lation table we set t
E|A
(e|a) = t
A|E
(a|e) for En-
glish and AMR tokens e and a and then normalize
the t table. The distortion table can also be initial-
ized in a similar manner. We initialize the fertility
table with its value in the previous iteration.
Previously Liang et al. (2006) also presented a
symmetric method for training alignment parame-
ters. Similar to our work, their objective function
involves summation of conditional likelihoods in
both directions; however, their constraint is on
agreement between predicted alignments while we
directly focus on agreement between the parame-
ters themselves. Moreover their method involves a
modification of the E step of EM algorithm which
is very hard to implement for IBM Model 3 and
above.
After learning the parameters, alignments are
computed using the Viterbi algorithm in both di-
rections of the IBM models. We tried merging
the alignments of the two directions using meth-
ods like grow-diag-final heuristic or taking inter-
section of the alignments and adding some high
probability links in their union. But these methods
did not help the alignment accuracy.
2.3 Postprocessing
The main goal of the postprocessing component is
to rebuild the aligned AMR graph. We first insert
words removed as stop words into their positions,
then rebuild the graph using the recorded original
structure of the nodes in the AMR graph.
We also apply a last modification to the align-
ments in the postprocessing. Observing that pairs
like worker and person :arg0-of work-01 appear
frequently, and in all such cases, all the AMR to-
kens align to the English one, whenever we see
any of AMR tokens person, product, thing or com-
pany is followed by arg0-of, arg1-of or arg2-of
followed by an AMR concept, we align the two
former tokens to what the concept is aligned to.
3 Experiments
3.1 Data Description
Our data consists of 13,050 publicly available
AMR/English sentence pairs
1
. We have hand
1
LDC AMR release 1.0, Release date: June 16, 2014
https://catalog.ldc.upenn.edu/LDC2014T12
427
aligned 200 of these pairs to be used as develop-
ment and test sets
2
. We train the parameters on
the whole data. Table 1 presents a description of
the data. We do not count parenthesis, slash and
AMR variables as AMR tokens. Role tokens are
those AMR tokens that start with a colon. They
do not represent any concept, but provide a link
between concepts. For example in:
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
the first :arg0 states that the first argument of the
concept wanting is the boy and the second argu-
ment is going.
train dev test
Sent. pairs 13050 100 100
AMR tokens 465 K 3.8 K (52%) 2.3 K (%55)
AMR role tokens 226 K 1.9 K (23%) 1.1 K (%22)
ENG tokens 248 K 2.3 K (76%) 1.7 K (%74)
Table 1: AMR/English corpus. The number in
parentheses is the percent of the tokens aligned in
gold annotation. Almost half of AMR tokens are
role tokens, and less than a quarter of role tokens
are aligned.
3.2 Experiment Results
We use MGIZA++ (Gao and Vogel, 2008) as
the implementation of the IBM models. We run
Model 1 and HMM for 5 iterations each, then run
our training algorithm on Model 4 for 4 iterations,
at which point the alignments become stable. As
alignments are usually many to one from AMR to
English, we compute the alignments from AMR to
English in the final step.
Table 2 shows the alignment accuracy for
Model 1, HMM, Model 4, and Model 4 plus the
modification described in section 2.2 (Model 4+).
The alignment accuracy on the test set is lower
than the development set mainly because it is in-
trinsically a harder set, as we only made small
modifications to the system based on the develop-
ment set. Recall error due to stop words is one
difference.
2
The development and test AMR/English pairs can be
found in /data/split/dev/amr-release-1.0-dev-consensus.txt
and /data/split/test/amr-release-1.0-test-consensus.txt, re-
spectively. The gold alignments are not included in these
files but are available separately.
model precision recall F score
Dev
Model 1 70.9 71.1 71.0
HMM 87.6 80.1 83.7
Model 4 89.7 80.4 84.8
Model 4+ 94.1 80.0 86.5
Test
Model 1 74.8 71.8 73.2
HMM 83.8 73.8 78.5
Model 4 85.8 74.9 80.0
Model 4+ 92.4 75.6 83.1
Table 2: Results on different models. Our training
method (Model 4+) increases the F score by 1.7
and 3.1 points on dev and test sets respectively.
Table 3 breaks down precision, recall, and
F score for role and non-role AMR tokens, and
also shows in parentheses the amount of recall er-
ror that was caused by removing either side of the
alignment as a stop word.
token type precision recall F score
Dev
role 77.1 48.7 59.7
non-role 97.2 88.2 92.5
all 94.1 80.0 (34%) 86.5
Test
role 71.0 37.8 49.3
non-role 95.5 84.7 89.8
all 92.4 75.6 (36%) 83.1
Table 3: Results breakdown into role and non-
role AMR tokens. The numbers in the parentheses
show the percent of recall errors caused by remov-
ing aligned tokens as stop words.
While the alignment method works very well on
non-role tokens, it works poorly on the role tokens.
Role tokens are sometimes matched with a word
or part of a word in the English sentence. For ex-
ample :polarity is matched with the un part of the
word unpopular, :manner is matched with most
adverbs, or even in the pair:
thanks
(t / thank-01
:arg0 (i / i)
:arg1 (y / you))
all AMR tokens including :arg0 and :arg1 are
matched to the only English word thanks. Incon-
sistency in aligning role tokens has made this a
hard problem even for human experts.
428
4 Conclusions and Future Work
In this paper we present the first set of manually
aligned English/AMR pairs, as well as the first
published system for learning the alignments be-
tween English sentences and AMR graphs that
provides a strong baseline for future research in
this area. As the proposed system learns the
alignments automatically using very little domain
knowledge, it can be applied in any domain and
for any language with minor adaptations.
Computing the alignments between English
sentences and AMR graphs is a first step for ex-
traction of semantic interpretation and generation
rules. Hence, a natural extension to this work
will be automatically parsing English sentences
into AMR and generating English sentences from
AMR.
Acknowledgments
This work was supported by DARPA con-
tracts HR0011-12-C-0014 and FA-8750-13-2-
0045. The authors would like to thank David Chi-
ang, Tomer Levinboim, and Ashish Vaswani (in
no particular order) for their comments and sug-
gestions.
References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Linguistic Annotation Workshop
(LAW VII-ID), ACL.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing Workshop, ACL.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
429
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 557?565,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Beyond Parallel Data: Joint Word Alignment and Decipherment
Improves Machine Translation
Qing Dou , Ashish Vaswani, and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{qdou,avaswani,knight}@isi.edu
Abstract
Inspired by previous work, where decipher-
ment is used to improve machine translation,
we propose a new idea to combine word align-
ment and decipherment into a single learning
process. We use EM to estimate the model pa-
rameters, not only to maximize the probabil-
ity of parallel corpus, but also the monolingual
corpus. We apply our approach to improve
Malagasy-English machine translation, where
only a small amount of parallel data is avail-
able. In our experiments, we observe gains of
0.9 to 2.1 Bleu over a strong baseline.
1 Introduction
State-of-the-art machine translation (MT) systems ap-
ply statistical techniques to learn translation rules au-
tomatically from parallel data. However, this reliance
on parallel data seriously limits the scope of MT ap-
plication in the real world, as for many languages and
domains, there is not enough parallel data to train a de-
cent quality MT system.
However, compared with parallel data, there are
much larger amounts of non parallel data. The abil-
ity to learn a translation lexicon or even build a ma-
chine translation system using monolingual data helps
address the problems of insufficient parallel data. Ravi
and Knight (2011) are among the first to learn a full
MT system using only non parallel data through deci-
pherment. However, the performance of such systems
is much lower compared with those trained with par-
allel data. In another work, Klementiev et al. (2012)
show that, given a phrase table, it is possible to esti-
mate parameters for a phrase-based MT system from
non parallel data.
Given that we often have some parallel data, it is
more practical to improve a translation system trained
on parallel data by using additional non parallel data.
Rapp (1995) shows that with a seed lexicon, it is possi-
ble to induce new word level translations from non par-
allel data. Motivated by the idea that a translation lexi-
con induced from non parallel data can be used to trans-
late out of vocabulary words (OOV), a variety of prior
research has tried to build a translation lexicon from
non parallel or comparable data (Fung and Yee, 1998;
Koehn and Knight, 2002; Haghighi et al., 2008; Garera
Figure 1: Combine word alignment and decipherment
into a single learning process.
et al., 2009; Bergsma and Van Durme, 2011; Daum?e
and Jagarlamudi, 2011; Irvine and Callison-Burch,
2013b; Irvine and Callison-Burch, 2013a; Irvine et al.,
2013).
Lately, there has been increasing interest in learn-
ing translation lexicons from non parallel data with de-
cipherment techniques (Ravi and Knight, 2011; Dou
and Knight, 2012; Nuhn et al., 2012; Dou and Knight,
2013). Decipherment views one language as a cipher
for another and learns a translation lexicon that pro-
duces fluent text in the target (plaintext) language. Pre-
vious work has shown that decipherment not only helps
find translations for OOVs (Dou and Knight, 2012), but
also improves translations of observed words (Dou and
Knight, 2013).
We find that previous work using monolingual or
comparable data to improve quality of machine transla-
tion separates two learning tasks: first, translation rules
are learned from parallel data, and then the information
learned from parallel data is used to bootstrap learning
with non parallel data. Inspired by approaches where
joint inference reduces the problems of error propaga-
tion and improves system performance, we combine
the two separate learning processes into a single one,
as shown in Figure 1. The contributions of this work
are:
557
? We propose a new objective function for word
alignment that combines the process of word
alignment and decipherment into a single learning
task.
? In experiments, we find that the joint process out-
performs the previous pipeline approach, and ob-
serve Bleu gains of 0.9 and 2.1 on two different
test sets.
? We release 15.3 million tokens of monolingual
Malagasy data from the web, as well as a small
Malagasy dependency tree bank containing 20k
tokens.
2 Joint Word Alignment and
Decipherment
2.1 A New Objective Function
In previous work that uses monolingual data to im-
prove machine translation, a seed translation lexicon
learned from parallel data is used to find new transla-
tions through either word vector based approaches or
decipherment. In return, selection of a seed lexicon
needs to be careful as using a poor quality seed lexi-
con could hurt the downstream process. Evidence from
a number of previous work shows that a joint inference
process leads to better performance in both tasks (Jiang
et al., 2008; Zhang and Clark, 2008).
In the presence of parallel and monolingual data, we
would like the alignment and decipherment models to
benefit from each other. Since the decipherment and
word alignment models contain word-to-word transla-
tion probabilities t( f | e), having them share these pa-
rameters during learning will allow us to pool infor-
mation from both data types. This leads us to de-
velop a new objective function that takes both learn-
ing processes into account. Given our parallel data,
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), and monolingual
data F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, we seek to maximize
the likelihood of both. Our new objective function is
defined as:
F
joint
=
M
?
m=1
log P(F
m
| E
m
) + ?
N
?
n=1
log P(F
n
mono
) (1)
The goal of training is to learn the parameters that
maximize this objective, that is
?
?
= arg max
?
F
joint
(2)
In the next two sections, we describe the word align-
ment and decipherment models, and present how they
are combined to perform joint optimization.
2.2 Word Alignment
Given a source sentence F = f
1
, . . . , f
j
, . . . , f
J
and a
target sentence E = e
1
, . . . , e
i
, . . . , e
I
, word alignment
models describe the generative process employed to
produce the French sentence from the English sentence
through alignments a = a
1
, . . . , a
j
, . . . , a
J
.
The IBM models 1-2 (Brown et al., 1993) and the
HMM word alignment model (Vogel et al., 1996) use
two sets of parameters, distortion probabilities and
translation probabilities, to define the joint probabil-
ity of a target sentence and alignment given a source
sentence.
P(F, a | E) =
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
). (3)
These alignment models share the same translation
probabilities t( f
j
| e
a
j
), but differ in their treatment of
the distortion probabilities d(a
j
| a
j?1
, j). Brown et
al. (1993) introduce more advanced models for word
alignment, such as Model 3 and Model 4, which use
more parameters to describe the generative process. We
do not go into details of those models here and the
reader is referred to the paper describing them.
Under the Model 1-2 and HMM alignment models,
the probability of target sentence given source sentence
is:
P(F | E) =
?
a
J
?
j=1
d(a
j
| a
j?1
, j)t( f
j
| e
a
j
).
Let ? denote all the parameters of the word align-
ment model. Given a corpus of sentence pairs
(E
1
,F
1
), . . . , (E
m
,F
m
), . . . , (E
M
,F
M
), the standard ap-
proach for training is to learn the maximum likelihood
estimate of the parameters, that is,
?
?
= arg max
?
M
?
m=1
log P(F
m
| E
m
)
= arg max
?
log
?
?
?
?
?
?
?
?
a
P(F
m
, a | E
m
)
?
?
?
?
?
?
?
.
We typically use the EM algorithm (Dempster et al.,
1977), to carry out this optimization.
2.3 Decipherment
Given a corpus of N foreign text sequences (cipher-
text), F
1
mono
, . . . ,F
n
mono
, . . . ,F
N
mono
, decipherment finds
word-to-word translations that best describe the cipher-
text.
Knight et al. (2006) are the first to study several natu-
ral language decipherment problems with unsupervised
learning. Since then, there has been increasing interest
in improving decipherment techniques and its applica-
tion to machine translation (Ravi and Knight, 2011;
558
Dou and Knight, 2012; Nuhn et al., 2012; Dou and
Knight, 2013; Nuhn et al., 2013).
In order to speed up decipherment, Dou and Knight
(2012) suggest that a frequency list of bigrams might
contain enough information for decipherment. Accord-
ing to them, a monolingual ciphertext bigram F
mono
is
generated through the following generative story:
? Generate a sequence of two plaintext tokens e
1
e
2
with probability P(e
1
e
2
) given by a language
model built from large numbers of plaintext bi-
grams.
? Substitute e
1
with f
1
and e
2
with f
2
with probabil-
ity t( f
1
|e
1
) ? t( f
2
|e
2
).
The probability of any cipher bigram F is:
P(F
mono
) =
?
e
1
e
2
P(e
1
e
2
) ? t( f
1
|e
1
) ? t( f
2
|e
2
) (4)
And the probability of the corpus is:
P(corpus) =
N
?
n=1
P(F
n
mono
) (5)
Given a plaintext bigram language model, the goal is
to manipulate t( f |e) to maximize P(corpus). Theoret-
ically, one can directly apply EM to solve the problem
(Knight et al., 2006). However, EM has time complex-
ity O(N ?V
2
e
) and space complexity O(V
f
?V
e
), where V
f
,
V
e
are the sizes of ciphertext and plaintext vocabularies
respectively, and N is the number of cipher bigrams.
There have been previous attempts to make decipher-
ment faster. Ravi and Knight (2011) apply Bayesian
learning to reduce the space complexity. However,
Bayesian decipherment is still very slow with Gibbs
sampling (Geman and Geman, 1987). Dou and Knight
(2012) make sampling faster by introducing slice sam-
pling (Neal, 2000) to Bayesian decipherment. Besides
Bayesian decipherment, Nuhn et al. (2013) show that
beam search can be used to solve a very large 1:1 word
substitution cipher. In subsection 2.4.1, we describe
our approach that uses slice sampling to compute ex-
pected counts for decipherment in the EM algorithm.
2.4 Joint Optimization
We now describe our EM approach to learn the param-
eters that maximize F
joint
(equation 2), where the dis-
tortion probabilities, d(a
j
| a
j?1
, j) in the word align-
ment model are only learned from parallel data, and
the translation probabilities, t( f | e) are learned using
both parallel and non parallel data. The E step and M
step are illustrated in Figure 2.
Our algorithm starts with EM learning only on par-
allel data for a few iterations. When the joint inference
starts, we first compute expected counts from parallel
data and non parallel data using parameter values from
the last M step separately. Then, we add the expected
counts from both parallel data and non parallel data to-
gether with different weights for the two. Finally we
Figure 2: Joint Word Alignment and Decipherment
with EM
renormalize the translation table and distortion table to
update parameters in the new M step.
The E step for parallel part can be computed effi-
ciently using the forward-backward algorithm (Vogel et
al., 1996). However, as we pointed out in Section 2.3,
the E step for the non parallel part has a time com-
plexity of O(V
2
) with the forward-backward algorithm,
where V is the size of English vocabulary, and is usu-
ally very large. Previous work has tried to make de-
cipherment scalable (Ravi and Knight, 2011; Dou and
Knight, 2012; Nuhn et al., 2013; Ravi, 2013). How-
ever, all of them are designed for decipherment with ei-
ther Bayesian inference or beam search. In contrast, we
need an algorithm to make EM decipherment scalable.
To overcome this problem, we modify the slice sam-
pling (Neal, 2000) approach used by Dou and Knight
(2012) to compute expected counts from non parallel
data needed for the EM algorithm.
2.4.1 Draw Samples with Slice Sampling
To start the sampling process, we initialize the first
sample by performing approximate Viterbi decoding
using results from the last EM iteration. For each for-
eign dependency bigram f
1
, f
2
, we find the top 50 can-
didates for f
1
and f
2
ranked by t(e| f ), and find the En-
glish sequence e
1
, e
2
that maximizes t(e
1
| f
1
) ? t(e
2
| f
2
) ?
P(e
1
, e
2
).
Suppose the derivation probability for current sam-
ple e current is P(e current), we use slice sampling to
draw a new sample in two steps:
? Select a threshold T uniformly between 0 and
P(e current).
? Draw a new sample e new uniformly from a pool
559
of candidates: {e new|P(e new) > T }.
The first step is straightforward to implement. How-
ever, it is not trivial to implement the second step. We
adapt the idea from Dou and Knight (2012) for EM
learning.
Suppose our current sample e current contains En-
glish tokens e
i?1
, e
i
, and e
i+1
at position i ? 1, i, and
i+1 respectively, and f
i
be the foreign token at position
i. Using point-wise sampling, we draw a new sample
by changing token e
i
to a new token e
?
. Since the rest
of the sample remains the same, only the probability of
the trigram P(e
i?1
e
?
e
i+1
) (The probability is given by a
bigram language model.), and the channel model prob-
ability t( f
i
|e
?
) change. Therefore, the probability of a
sample is simplified as shown Equation 6.
P(e
i?1
e
?
e
i+1
) ? t( f
i
|e
?
) (6)
Remember that in slice sampling, a new sample is
drawn in two steps. For the first step, we choose a
threshold T uniformly between 0 and P(e
i?1
e
i
e
i+1
) ?
t( f
i
|e
i
). We divide the second step into two cases based
on the observation that two types of samples are more
likely to have a probability higher than T (Dou and
Knight, 2012): (1) those whose trigram probability is
high, and (2) those whose channel model probability is
high. To find candidates that have high trigram proba-
bility, Dou and Knight (2012) build a top k sorted lists
ranked by P(e
i?1
e
?
e
i+1
), which can be pre-computed
off-line. Then, they test if the last item e
k
in the list
satisfies the following inequality:
P(e
i?1
e
k
e
i+1
) ? c < T (7)
where c is a small constant and is set to prior in their
work. In contrast, we choose c empirically as we do
not have a prior in our model. When the inequality in
Equation 7 is satisfied, a sample is drawn in the fol-
lowing way: Let set A = {e
?
|e
i?1
e
?
e
i+1
? c > T } and
set B = {e
?
|t( f
i
|e
?
) > c}. Then we only need to sample
e
?
uniformly from A ? B until P(e
i?1
e
?
e
i+1
) ? t( f
i
|e
?
) is
greater than T . It is easy to prove that all other candi-
dates that are not in the sorted list and with t( f
i
|e
?
) ? c
have a upper bound probability: P(e
i?1
e
k
e
i+1
)?c. There-
fore, they do not need to be considered.
Second, when the last item e
k
in the list does not
meet the condition in Equation 7, we keep drawing
samples e
?
randomly until its probability is greater than
the threshold T .
As we mentioned before, the choice of the small con-
stant c is empirical. A large c reduces the number of
items in set B, but makes the condition P(e
i?1
e
k
e
i+1
) ?
c < T less likely to satisfy, which slows down the sam-
pling. On the contrary, a small c increases the number
of items in set B significantly as EM does not encour-
age a sparse distribution, which also slows down the
sampling. In our experiments, we set c to 0.001 based
on the speed of decipherment. Furthermore, to reduce
the size of set B, we rank all the candidate translations
Spanish English
Parallel 10.3k 9.9k
Non Parallel 80 million 400 million
Table 1: Size of parallel and non parallel data for word
alignment experiments (Measured in number of tokens)
of f
i
by t(e
?
| f
i
), then we add maximum the first 1000
candidates whose t( f
i
|e
?
) >= c into set B. For the rest
of the candidates, we set t( f
i
|e
?
) to a value smaller than
c (0.00001 in experiments).
2.4.2 Compute Expected Counts from Samples
With the ability to draw samples efficiently for deci-
pherment using EM, we now describe how to compute
expected counts from those samples. Let f
1
, f
2
be a
specific ciphertext bigram, N be the number of sam-
ples we want to use to compute expected counts, and
e
1
, e
2
be one of the N samples. The expected counts
for pairs ( f
1
, e
1
) and ( f
2
, e
2
) are computed as:
? ?
count( f
1
, f
2
)
N
where count( f
1
, f
2
) is count of the bigram, and ? is the
weight for non parallel data as shown in Equation 1.
Expected counts collected for f
1
, f
2
are accumulated
from each of its N samples. Finally, we collect ex-
pected counts using the same approach from each for-
eign bigram.
3 Word Alignment Experiments
In this section, we show that joint word alignment and
decipherment improves the quality of word alignment.
We choose to evaluate word alignment performance
for Spanish and English as manual gold alignments
are available. In experiments, our approach improves
alignment F score by as much as 8 points.
3.1 Experiment Setup
As shown in Table 1, we work with a small amount of
parallel, manually aligned Spanish-English data (Lam-
bert et al., 2005), and a much larger amount of mono-
lingual data.
The parallel data is extracted from Europarl, which
consists of articles from European parliament plenary
sessions. The monolingual data comes from English
and Spanish versions of Gigaword corpra containing
news articles from different news agencies.
We view Spanish as a cipher of English, and follow
the approach proposed by Dou and Knight (2013) to
extract dependency bigrams from parsed Spanish and
English monolingual data for decipherment. We only
keep bigrams where both tokens appear in the paral-
lel data. Then, we perform Spanish to English (En-
glish generating Spanish) word alignment and Span-
ish to English decipherment simultaneously with the
method discussed in section 2.
560
3.1.1 Results
We align all 500 sentences in the parallel corpus, and
tune the decipherment weight (?) for Model 1 and
HMM using the last 100 sentences. The best weights
are 0.1 for Model 1, and 0.005 for HMM. We start with
Model 1 with only parallel data for 5 iterations, and
switch to the joint process for another 5 iterations with
Model 1 and 5 more iterations of HMM. In the end, we
use the first 100 sentence pairs of the corpus for evalu-
ation.
Figure 3 compares the learning curve of alignment
F-score between EM without decipherment (baseline)
and our joint word alignment and decipherment. From
the learning curve, we find that at the 6th iteration, 2
iterations after we start the joint process, alignment F-
score is improved from 34 to 43, and this improvement
is held through the rest of the Model 1 iterations. The
alignment model switches to HMM from the 11th iter-
ation, and at the 12th iteration, we see a sudden jump
in F-score for both the baseline and the joint approach.
We see consistent improvement of F-score till the end
of HMM iterations.
4 Improving Low Density Languages
Machine Translation with Joint Word
Alignment and Decipherment
In the previous section, we show that the joint word
alignment and decipherment process improves quality
of word alignment significantly for Spanish and En-
glish. In this section, we test our approach in a more
challenging setting: improving the quality of machine
translation in a real low density language setting.
In this task, our goal is to build a system to trans-
late Malagasy news into English. We have a small
amount of parallel data, and larger amounts of mono-
lingual data collected from online websites. We build a
dependency parser for Malagasy to parse the monolin-
gual data to perform dependency based decipherment
(Dou and Knight, 2013). In the end, we perform joint
word alignment and decipherment, and show that the
joint learning process improves Bleu scores by up to
2.1 points over a phrase-based MT baseline.
4.1 The Malagasy Language
Malagasy is the official language of Madagascar. It has
around 18 million native speakers. Although Mada-
gascar is an African country, Malagasy belongs to the
Malayo-Polynesian branch of the Austronesian lan-
guage family. Malagasy and English have very dif-
ferent word orders. First of all, in contrast to En-
glish, which has a subject-verb-object (SVO) word or-
der, Malagasy has a verb-object-subject (VOS) word
order. Besides that, Malagasy is a typical head ini-
tial language: Determiners precede nouns, while other
modifiers and relative clauses follow nouns (e.g. ny
?the? boky ?book? mena ?red?). The significant dif-
ferences in word order pose great challenges for both
Source Malagasy English
Parallel
Global Voices 2.0 million 1.8 million
Web News 2.2k 2.1k
Non Parallel
Gigaword N/A 2.4 billion
allAfrica N/A 396 million
Local News 15.3 million N/A
Table 2: Size of Malagasy and English data used in our
experiments (Measured in number of tokens)
machine translation and decipherment.
4.2 Data
Table 2 shows the data available to us in our experi-
ments. The majority of parallel text comes from Global
Voices
1
(GV). The website contains international news
translated into different foreign languages. Besides
that, we also have a very small amount of parallel text
containing local web news, with English translations
provided by native speakers at the University of Texas,
Austin. The Malagasy side of this small parallel corpus
also has syntactical annotation, which is used to train a
very basic Malagasy part of speech tagger and depen-
dency parser.
We also have much larger amounts of non paral-
lel data for both languages. For Malagasy, we spent
two months manually collecting 15.3 million tokens of
news text from local news websites in Madagascar.
2
We have released this data for future research use. For
English, we have 2.4 billion tokens from the Gigaword
corpus. Since the Malagasy monolingual data is col-
lected from local websites, it is reasonable to argue that
those data contain significant amount of information re-
lated to Africa. Therefore, we also collect 396 million
tokens of African news in English from allAfrica.com.
4.3 Building A Dependency Parser for Malagasy
Since Malagasy and English have very different word
orders, we decide to apply dependency based decipher-
ment for the two languages as suggested by Dou and
Knight (2013). To extract dependency relations, we
need to parse monolingual data in Malagasy and En-
glish. For English, there are already many good parsers
available. In our experiments, we use Turbo parser
(Martins et al., 2013) trained on the English Penn Tree-
bank (Marcus et al., 1993) to parse all our English
monolingual data. However, there is no existing good
parser for Malagasy.
The quality of a dependency parser depends on the
amount of training data available. State-of-the-art En-
glish parsers are built from Penn Treebank, which con-
tains over 1 million tokens of annotated syntactical
1
globalvoicesonline.org
2
aoraha.com, gazetiko.com, inovaovao.com,
expressmada.com, lakroa.com
561
Figure 3: Learning curve showing our joint word alignment and decipherment approach improves word alignment
quality over the traditional EM without decipherment (Model 1: Iteration 1 to 10, HMM: Iteration 11 to 15)
trees. In contrast, the available data for training a Mala-
gasy parser is rather limited, with only 168 sentences,
and 2.8k tokens, as shown in Table 2. At the very be-
ginning, we use the last 120 sentences as training data
to train a part of speech (POS) tagger using a toolkit
provided by Garrette et al. (2013) and a dependency
parser with the Turbo parser. We test the performance
of the parser on the first 48 sentences and obtain 72.4%
accuracy.
One obvious way to improve tagging and parsing ac-
curacy is to get more annotated data. We find more data
with only part of speech tags containing 465 sentences
and 10k tokens released by (Garrette et al., 2013), and
add this data as extra training data for POS tagger.
Also, we download an online dictionary that contains
POS tags for over 60k Malagasy word types from mala-
gasyword.org. The dictionary is very helpful for tag-
ging words never seen in the training data.
It is natural to think that creation of annotated data
for training a POS tagger and a parser requires large
amounts of efforts from annotators who understand the
language well. However, we find that through the help
of parallel data and dictionaries, we are able to create
more annotated data by ourselves to improve tagging
and parsing accuracy. This idea is inspired by previ-
ous work that tries to learn a semi-supervised parser
by projecting dependency relations from one language
(with good dependency parsers) to another (Yarowsky
and Ngai, 2001; Ganchev et al., 2009). However, we
find those automatic approaches do not work well for
Malagasy.
To further expand our Malagasy training data, we
first use a POS tagger and parser with poor perfor-
mance to parse 788 sentences (20k tokens) on the
Malagasy side of the parallel corpus from Global
Voices. Then, we correct both the dependency links
and POS tags based on information from dictionaries
3
and the English translation of the parsed sentence. We
spent 3 months to manually project English dependen-
cies to Malagasy and eventually improve test set pars-
ing accuracy from 72.4% to 80.0%. We also make this
data available for future research use.
4.4 Machine Translation Experiments
In this section, we present the data used for our MT
experiments, and compare three different systems to
justify our joint word alignment and decipherment ap-
proach.
4.4.1 Baseline Machine Translation System
We build a state-of-the-art phrase-based MT system,
PBMT, using Moses (Koehn et al., 2007). PBMT has 3
models: a translation model, a distortion model, and
a language model. We train the other models using
half of the Global Voices parallel data (the rest is re-
served for development and testing), and build a 5-
gram language model using 834 million tokens from
AFP section of English Gigaword, 396 million tokens
from allAfrica, and the English part of the parallel cor-
pus for training. For alignment, we run 10 iterations
of Model 1, and 5 iterations of HMM. We did not run
Model 3 and Model 4 as we see no improvements in
Bleu scores from running those models. We do word
3
an online dictionary from malagasyword.org, as well as
a lexicon learned from the parallel data
562
alignment in two directions and use grow-diag-final-
and heuristic to obtain final alignment. During decod-
ing, we use 8 standard features in Moses to score a can-
didate translation: direct and inverse translation proba-
bilities, direct and inverse lexical weighting, a language
model score, a distortion score, phrase penalty, and
word penalty. The weights for the features are learned
on the tuning data using minimum error rate training
(MERT) (Och, 2003).
To compare with previous decipherment approach to
improve machine translation, we build a second base-
line system. We follow the work by Dou and Knight
(2013) to decipher Malagasy into English, and build a
translation lexicon T
decipher
from decipherment. To im-
prove machine translation, we simply use T
decipher
as
an additional parallel corpus. First, we filter T
decipher
by keeping only translation pairs ( f , e), where f is ob-
served in the Spanish part and e is observed in the En-
glish part of the parallel corpus. Then we append all
the Spanish and English words in the filtered T
decipher
to the end of Spanish part and English part of the paral-
lel corpus respectively. The training and tuning process
is the same as the baseline machine translation system
PBMT. We call this system Decipher-Pipeline.
4.4.2 Joint Word Alignment and Decipherment
for Machine Translation
When deciphering Malagasy to English, we extract
Malagasy dependency bigrams using all available
Malagasy monolingual data plus the Malagasy part of
the Global Voices parallel data, and extract English
dependency bigrams using 834 million tokens from
English Gigaword, and 396 million tokens from al-
lAfrica news to build an English dependency language
model. In the other direction, we extract English de-
pendency bigrams from English part of the entire paral-
lel corpus plus 9.7 million tokens from allAfrica news
4
, and use 17.3 million tokens Malagasy monolingual
data (15.3 million from the web and 2.0 million from
Global Voices) to build a Malagasy dependency lan-
guage model. We require that all dependency bigrams
only contain words observed in the parallel data used
to train the baseline MT system.
During learning, we run Model 1 without decipher-
ment for 5 iterations. Then we perform joint word
alignment and decipherment for another 5 iterations
with Model 1 and 5 iterations with HMM. We tune
decipherment weights (?) for Model 1 and HMM us-
ing grid search against Bleu score on a development
set. In the end, we only extract rules from one di-
rection P(English|Malagasy), where the decipherment
weights for Model 1 and HMM are 0.5 and 0.005 re-
spectively. We chose this because we did not find any
benefits to tune the weights on each direction, and then
use grow-diag-final-end heuristic to form final align-
ments. We call this system Decipher-Joint.
4
We do not find further Bleu gains by using more English
monolingual data.
Parallel
Malagasy English
Train (GV) 0.9 million 0.8 million
Tune (GV) 22.2k 20.2k
Test (GV) 23k 21k
Test (Web) 2.2k 2.1k
Non Parallel
Malagasy English
Gigaword N/A 834 million
Web 15.3 million 396 million
Table 3: Size of training, tuning, and testing data in
number of tokens (GV: Global Voices)
4.5 Results
We tune each system three times with MERT and
choose the best weights based on Bleu scores on tuning
set.
Table 4 shows that while using a translation lexicon
learnt from decipherment does not improve the quality
of machine translation significantly, the joint approach
improves Bleu score by 0.9 and 2.1 on Global Voices
test set and web news test set respectively. The results
show that the parsing quality correlates with gains in
Bleu scores. Scores in the brackets in the last row of
the table are achieved using a dependency parser with
72.4% attachment accuracy, while scores outside the
brackets are obtained using a dependency parser with
80.0% attachment accuracy.
We analyze the results and find the gain mainly
comes from two parts. First, adding expected counts
from non parallel data makes the distribution of trans-
lation probabilities sparser in word alignment models.
The probabilities of translation pairs favored by both
parallel data and decipherment becomes higher. This
gain is consistent with previous observation where a
sparse prior is applied to EM to help improve word
alignment and machine translation (Vaswani et al.,
2012). Second, expected counts from decipherment
also help discover new translation pairs in the paral-
lel data for low frequency words, where those words
are either aligned to NULL or wrong translations in the
baseline.
5 Conclusion and Future Work
We propose a new objective function for word align-
ment to combine the process of word alignment and
decipherment into a single task. In, experiments, we
find that the joint process performs better than previous
pipeline approach, and observe Bleu gains of 0.9 and
2.1 point on Global Voices and local web news test sets,
respectively. Finally, our research leads to the release
of 15.3 million tokens of monolingual Malagasy data
from the web as well as a small Malagasy dependency
tree bank containing 20k tokens.
Given the positive results we obtain by using the
joint approach to improve word alignment, we are in-
563
Decipherment System Tune (GV) Test (GV) Test (Web)
None PBMT (Baseline) 18.5 17.1 7.7
Separate Decipher-Pipeline 18.5 17.4 7.7
Joint Decipher-Joint 18.9 (18.7) 18.0 (17.7) 9.8 (8.5)
Table 4: Decipher-Pipeline does not show significant improvement over the baseline system. In contrast, Decipher-
Joint using joint word alignment and decipherment approach achieves a Bleu gain of 0.9 and 2.1 on the Global
Voices test set and the web news test set, respectively. The results in brackets are obtained using a parser trained
with only 120 sentences. (GV: Global Voices)
spired to apply this approach to help find translations
for out of vocabulary words, and to explore other pos-
sible ways to improve machine translation with deci-
pherment.
Acknowledgments
This work was supported by NSF Grant 0904684 and
ARO grant W911NF-10-1-0533. The authors would
like to thank David Chiang, Malte Nuhn, Victoria Fos-
sum, Ashish Vaswani, Ulf Hermjakob, Yang Gao, and
Hui Zhang (in no particular order) for their comments
and suggestions.
References
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual similar-
ity of labeled web images. In Proceedings of the
Twenty-Second International Joint Conference on
Artificial Intelligence - Volume Three. AAAI Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19:263?311.
Hal Daum?e, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1?
38.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics.
Qing Dou and Kevin Knight. 2013. Dependency-
based decipherment for resource-limited machine
translation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1. Association for Com-
putational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1. Association for Computational
Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1987. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. In Readings in computer vi-
sion: issues, problems, principles, and paradigms.
Morgan Kaufmann Publishers Inc.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguis-
tics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion. Association for Computational Linguistics, Au-
gust.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
564
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Ann Irvine, Chris Quirk, and Hal Daume III. 2013.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?u.
2008. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT. Association for Com-
putational Linguistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions.
Association for Computational Linguistics.
Patrik Lambert, Adri?a De Gispert, Rafael Banchs, and
Jos?e B. Mari?no. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39(4):267?285.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the Turbo: Fast third-order non-
projective Turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers - Volume
1. Association for Computational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the 33rd an-
nual meeting of Association for Computational Lin-
guistics. Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for machine
translation via hash sampling. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller alignment models for better trans-
lations: Unsupervised word alignment with the l0-
norm. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1. Association for Computational
Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2. Associa-
tion for Computational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language Technologies. Association for Compu-
tational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, Columbus,
Ohio. Association for Computational Linguistics.
565
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1769?1773,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Cipher Type Detection
Malte Nuhn
Human Language Technology
and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
nuhn@cs.rwth-aachen.de
Kevin Knight
Information Sciences Institute
University of Southern California
knight@isi.edu
Abstract
Manual analysis and decryption of enci-
phered documents is a tedious and error
prone work. Often?even after spend-
ing large amounts of time on a par-
ticular cipher?no decipherment can be
found. Automating the decryption of var-
ious types of ciphers makes it possible
to sift through the large number of en-
crypted messages found in libraries and
archives, and to focus human effort only
on a small but potentially interesting sub-
set of them. In this work, we train a clas-
sifier that is able to predict which enci-
pherment method has been used to gener-
ate a given ciphertext. We are able to dis-
tinguish 50 different cipher types (speci-
fied by the American Cryptogram Associ-
ation) with an accuracy of 58.5%. This is a
11.2% absolute improvement over the best
previously published classifier.
1 Introduction
Libraries and archives contain a large number of
encrypted messages created throughout the cen-
turies using various encryption methods. For the
great majority of the ciphers an analysis has not
yet been conducted, simply because it takes too
much time to analyze each cipher individually, or
because it is too hard to decipher them. Automatic
methods for analyzing and classifying given ci-
phers makes it possible to sift interesting messages
and by that focus the limited amount of human re-
sources to a promising subset of ciphers.
For specific types of ciphers, there exist au-
tomated tools to decipher encrypted messages.
However, the publicly available tools often de-
pend on a more or less educated guess which
type of encipherment has been used. Furthermore,
they often still need human interaction and are
only restricted to analyzing very few types of ci-
phers. In practice however, there are many differ-
ent types of ciphers which we would like to an-
alyze in a fully automatic fashion: Bauer (2010)
gives a good overview over historical methods that
have been used to encipher messages in the past.
Similarly, the American Cryptogram Association
(ACA) specifies a set of 56 different methods for
enciphering a given plaintext:
Each encipherment method M
i
can be seen as
a function that transforms a given plaintext into a
ciphertext using a given key, or short:
cipher = M
i
(plain, key)
When analyzing an unknown ciphertext, we are
interested in the original plaintext that was used to
generate the ciphertext, i.e. the opposite direction:
plain = M
?1
i
(cipher, key)
Obtaining the plaintext from an enciphered mes-
sage is a difficult problem. We assume that the
decipherment of a message can be separated into
solving three different subproblems:
1. Find the encipherment method M
i
that was
used to create the cipher
cipher ? M
i
.
2. Find the key that was used together with the
methodM
i
to encipher the plaintext to obtain
cipher = M
i
(plain, key).
3. Decode the message using M
i
and key
cipher ? M
?1
i
(cipher, key)
Thus, an intermediate step to deciphering an un-
known ciphertext is to find out which encryption
method was used. In this paper, we present a clas-
sifier that is able to predict just that: Given an un-
known ciphertext, it can predict what kind of en-
cryption method was most likely used to generate
1769
? Type: CMBIFID
? Plaintext:
WOMEN NSFOO TBALL ISGAI
NINGI NPOPU LARIT YANDT
HETOU RNAME
? Key:
LEFTKEY=?IACERATIONS?
RIGHTKEY=?KNORKOPPING?
PERIOD=3, LROUTE=1
RROUTE=1, USE6X6=0
? Ciphertext:
WTQNG GEEBQ BPNQP VANEN
KDAOD GAHQS PKNVI PTAAP
DGMGR PCSGN
Figure 1: Example ?CMBIFID? cipher: Text is
grouped in five character chunks for readability.
it. The results of our classifier are a valuable input
to human decipherers to make a first categoriza-
tion of an unknown ciphertext.
2 Related Work
Central to this work is the list of encryption meth-
ods provided by the American Cipher Associa-
tion
1
. This list contains detailed descriptions and
examples of each of the cipher types, allowing us
to implement them. Figure 3 lists these methods.
We compare our work to the only previously
published cipher type classifier for classical ci-
phers
2
. This classifier is trained on 16, 800 cipher-
texts and is implemented in javascript to run in the
web browser: The user can provide the ciphertext
as input to a web page that returns the classifier?s
predictions. The source code of the classifier is
available online. Our work includes a reimple-
mentation of the features used in that classifier.
As examples for work that deals with the auto-
mated decipherment of cipher texts, we point to
(Ravi and Knight, 2011), and (Nuhn et al., 2013).
These publications develop specialized algorithms
for solving simple and homophonic substitution
ciphers, which are just two out of the 56 cipher
types defined by the ACA. We also want to men-
tion (de Souza et al., 2013), which presents a ci-
pher type classifier for the finalist algorithms of
the Advanced Encryption Standard (AES) contest.
1
http://cryptogram.org/cipher_types.html
2
See http://bionsgadgets.appspot.com/gadget_forms/
refscore_extended.html and https://sites.google.com/site/
bionspot/cipher-id-tests
plaintext keyencipherciphertextclassifier training
type
Figure 2: Overview over the data generation and
training of the classifier presented in this work.
3 General Approach
Given a ciphertext, the task is to find the right en-
cryption method. Our test set covers 50 out of 56
cipher types specified by ACA, as listed in Fig-
ure 3. We are going to take a machine learning ap-
proach which is based on the observation that we
can generate an infinite amount of training data.
3.1 Data Flow
The training procedure is depicted in Figure 2:
Based upon a large English corpus, we first choose
possible plaintext messages. Then, for each enci-
pherment method, we choose a random key and
encipher each of the plaintext messages using the
encipherment method and key. By doing this, we
can obtain (a theoretically infinite) amount of la-
beled data of the form (type, ciphertext). We can
then train a classifier on this data and evaluate it
on some held out data.
Figure 1 shows that in general the key can con-
sist of more than just a codeword: In this case,
the method uses two codewords, a period length,
two different permutation parameters, and a gen-
eral decision whether to use a special ?6?6? vari-
ant of the cipher or not. If not defined otherwise,
we choose random settings for these parameters.
If the parameters are integers, we choose random
values from a uniform distribution (in a sensible
range). In case of codewords, we choose the 450k
most frequent words from an English dictionary.
We train on cipher texts of random length.
3.2 Classifiers
The previous state-of-the-art classifier by BION
uses a random forest classifier (Breiman, 2001).
The version that is available online, uses 50 ran-
1770
? 6x6bifid
? 6x6playfair
? amsco
? bazeries
? beaufort
? bifid6
? bifid7
? (cadenus)
? cmbifid
? columnar
? digrafid
? dbl chckrbrd
? four square
? fracmorse
? grandpre
? (grille)
? gromark
? gronsfeld
? homophonic
? mnmedinome
? morbit
? myszkowski
? nicodemus
? nihilistsub
? (nihilisttransp)
? patristocrat
? period 7 vig.
? periodic gro-
mark
? phillips
? plaintext
? playfair
? pollux
? porta
? portax
? progkey beau-
fort
? progressivekey
? quagmire2
? quagmire3
? quagmire4
? ragbaby
? randomdigit
? randomtext
? redefence
? (route transp)
? runningkey
? seriatedpfair
? swagman
? tridigital
? trifid
? trisquare
? trisquare hr
? two square
? two sq. spiral
? vigautokey
? (vigenere)
? (vigslidefair)
Figure 3: Cipher types specified by ACA. Our classifier is able to recognize 50 out of these 56 ciphers.
The braced cipher types are not covered in this work.
dom decision trees. The features used by this clas-
sifier are described in Section 4.
Further, we train a support vector machine using
the libSVM toolkit (Chang and Lin, 2011). This
is feasible for up to 100k training examples. Be-
yond this point, training times become too large.
We perform multi class classification using ?-SVC
and a polynomial kernel. Multi class classification
is performed using one-against-one binary classifi-
cation. We select the SVM?s free parameters using
a small development set of 1k training examples.
We also use Vowpal Wabbit (Langford et al.,
2007) to train a linear classifier using stochastic
gradient descent. Compared to training SVMs,
Vowpal Wabbit is extremely fast and allows using
a lot of training examples. We use a squared loss
function, adaptive learning rates and don?t employ
any regularization. We train our classifier with up
to 1M training examples. The best performing set-
tings use one-against-all classification, 20 passes
over the training data and the default learning rate.
Quadratic features resulted in much slower train-
ing, while not providing any gains in accuracy.
4 Features
We reimplemented all of the features used in the
BION classifier, and add three newly developed
sets of features, resulting in a total of 58 features.
In order to further structure these features, we
group these features as follows: We call the set
of features that relate to the length of the cipher
LEN. This set contains binary features firing when
the cipher length is a multiple of 2, 3, 5, 25, any
of 4-15, and any of 4-30. We call the set of fea-
tures that are based on the fact that the cipher-
text contains a specific symbol HAS. This set con-
tains binary features firing when the cipher con-
tains a digit, a letter (A-Z), the ?#? symbol, the
letter ?j?, the digit ?0?. We also introduce an-
other set of features called DGT that contains two
features, firing when the cipher is starting or end-
ing with a digit. The set VIG contains 5 features:
The feature score is based on the best possible bi-
gram LM perplexity of a decipherment compatible
with the decipherment process of the cipher types
Autokey, Beaufort, Porta, Slidefair and Vigenere.
Further, we also include the features IC, MIC,
MKA, DIC, EDI, LR, ROD and LDI, DBL, NOMOR,
RDI, PTX, NIC, PHIC, BDI, CDD, SSTD, MPIC,
SERP, which were introduced in the BION classi-
fier
3
. Thus, the first 22 data points in Figure 4 are
based on previously known features by BION. We
further present the following additional features.
4.1 Repetition Feature (REP)
This set of features is based on how often the ci-
phertext contains symbols that are repeated ex-
actly n times in a row: For example the cipher-
text shown in Figure 1 contains two positions with
repetitions of length n = 2, because the cipher-
text contains EE, as well as AA. Beyond length
2, there are no repeats. These numbers are then
normalized by dividing them by the total number
of repeats of length 2 ? n ? 5.
4.2 Amsco Feature (AMSC)
The idea of the AMSCO cipher is to fill consec-
utive chunks of one and two plaintext characters
into n columns of a grid (see Table 1). Then a
permutation of the columns is performed, and the
resulting permuted plaintext is read of line by line
and forms the final ciphertext. This feature reads
the ciphertext into a similar grid of up to 5 columns
3
See http://home.comcast.net/
?
acabion/acarefstats.html
1771
Plaintext w om e ns f
oo t ba l li
Permutation 3 5 1 4 2
Table 1: Example grid used for AMSCO ciphers.
and then tries all possible permutations to retain
the original plaintext. The result of this opera-
tion is then scored with a bigram language model.
Depending on whether the difference in perplexity
between ciphertext and deciphered text exceeds a
given threshold, this binary feature fires.
4.3 Variant Feature (VAR)
In the variant cipher, the plaintext is written into
a block under a key word. All letters in the first
column are enciphered by shifting them using the
first key letter of the key word, the second column
uses the second key letter, etc. For different pe-
riods (i.e. lengths of key words), the ciphertext
is structured into n columns and unigram statis-
tics for each column are calculated. The frequency
profile of each column is compared to the unigram
frequency profile using a perplexity measure. This
binary feature fires when the resulting perplexities
are lower than a specific threshold.
5 Results
Figure 4 shows the classification accuracy for the
BION baseline, as well as our SVM and VW based
classifiers for a test set of 305 ciphers that have
been published in the ACA. The classifiers shown
in this figure are trained on cipher texts of ran-
dom length. We show the contribution of all the
features we used in the classifier on the x-axis.
Furthermore we also vary the amount of training
data we use to train the classifiers from 10k to 1M
training examples. It can be seen that when using
the same features as BION, our prediction accu-
racy is compatible with the BION classifier. The
main improvement of our classifier stems from the
REP, AMSC and VAR features. Our best classi-
fier is more than 11% more accurate than previous
state-of-the-art BION classifier.
We identified the best classifier on a held-out
set of 1000 ciphers, i.e. 20 ciphers for each ci-
pher type. Here the three new features improve the
VW-1M classifier from 50.9% accuracy to 56.0%
accuracy, and the VW-100k classifier from 48.9%
to 54.6%. Note that this held-out set is based on
the exact same generator that we used to create the
training data with. However, we also report the
results of our method on the completely indepen-
dently created ACA test set in Figure 4.
6 Conclusion
We presented a state-of-the art classifier for cipher
type detection. The approach we present is easily
extensible to cover more cipher types and allows
incorporating new features.
Acknowledgements
We thank Taylor Berg-Kirkpatrick, Shu Cai, Bill
Mason, Be?ata Megyesi, Julian Schamper, and
Megha Srivastava for their support and ideas. This
work was supported by ARL/ARO (W911NF-10-
1-0533) and DARPA (HR0011-12-C-0014).
10
20
30
40
50
60
H
A
S
L
E
N
V
I
G
I
C
M
I
C
M
K
A
D
I
C
E
D
I
L
R
R
O
D
L
D
I
D
B
L
N
M
O
R
R
D
I
P
T
X
N
I
C
P
H
I
C
B
D
I
C
D
D
S
S
T
D
M
P
I
C
S
E
R
P
R
E
P
A
M
S
C
V
A
R
Features
A
c
c
u
r
a
c
y
(
%
)
BION
SVM 10k
SVM100k
VW 100k
VW 1M
Figure 4: Classifier accuracy vs. training data and set of features used. From left to right more and
more features are used, the x-axis shows which features are added. The feature names are described in
Section 4. The features right of the vertical line are presented in this paper. The horizontal line shows
the previous state-of-the art accuracy (BION) of 47.3%, we achieve 58.49%.
1772
References
F.L. Bauer. 2010. Decrypted Secrets: Methods and
Maxims of Cryptology. Springer.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32, October.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
William AR de Souza, Allan Tomlinson, and Luiz MS
de Figueiredo. 2013. Cipher identification with a
neural network.
John Langford, Lihong Li, and Alex Strehl. 2007.
Vowpal Wabbit. https://github.com/
JohnLangford/vowpal_wabbit/wiki.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In ACL (1), pages 1568?1576.
Sujith Ravi and Kevin Knight. 2011. Bayesian Infer-
ence for Zodiac and Other Homophonic Ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239?247, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
1773
Binarization of Synchronous
Context-Free Grammars
Liang Huang?
USC/Information Science Institute
Hao Zhang??
Google Inc.
Daniel Gildea?
University of Rochester
Kevin Knight?
USC/Information Science Institute
Systems based on synchronous grammars and tree transducers promise to improve the quality
of statistical machine translation output, but are often very computationally intensive. The
complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings
between the two languages. We develop a theory of binarization for synchronous context-free
grammars and present a linear-time algorithm for binarizing synchronous rules when possible.
In our large-scale experiments, we found that almost all rules are binarizable and the resulting
binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-
based machine translation system. We also discuss the more general, and computationally more
difficult, problem of finding good parsing strategies for non-binarizable rules, and present an
approximate polynomial-time algorithm for this problem.
1. Introduction
Several recent syntax-based models for machine translation (Chiang 2005; Galley et al
2004) can be seen as instances of the general framework of synchronous grammars
and tree transducers. In this framework, both alignment (synchronous parsing) and
decoding can be thought of as parsing problems, whose complexity is in general ex-
ponential in the number of nonterminals on the right-hand side of a grammar rule.
To alleviate this problem, we investigate bilingual binarization as a technique to fac-
tor each synchronous grammar rule into a series of binary rules. Although mono-
lingual context-free grammars (CFGs) can always be binarized, this is not the case
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: lhuang@isi.edu,
liang.huang.sh@gmail.com.
?? 1600 Amphitheatre Parkway, Mountain View, CA 94303. E-mail: haozhang@google.com.
? Computer Science Dept., University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu.
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu.
Submission received: 14 March 2007; revised submission received: 8 January 2009; accepted for publication:
25 March 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
for all synchronous rules; we investigate algorithms for non-binarizable rules as well.
In particular:
r We develop a technique called synchronous binarization and devise a
linear-time binarization algorithm such that the resulting rule set alows
efficient algorithms for both synchronous parsing and decoding with
integrated n-gram language models.
r We examine the effect of this binarization method on end-to-end
translation quality on a large-scale Chinese-to-English syntax-based
system, compared to a more typical baseline method, and a state-of-the-art
phrase-based system.
r We examine the ratio of binarizability in large, empirically derived rule
sets, and show that the vast majority is binarizable. However, we also
provide, for the first time, real examples of non-binarizable cases verified
by native speakers.
r In the final, theoretical, sections of this article, we investigate the general
problem of finding the most efficient synchronous parsing or decoding
strategy for arbitrary synchronous context-free grammar (SCFG) rules,
including non-binarizable cases. Although this problem is believed to be
NP-complete, we prove two results that substantially reduce the search
space over strategies. We also present an optimal algorithm that runs
tractably in practice and a polynomial-time algorithm that is a good
approximation of the former.
Melamed (2003) discusses binarization of multi-text grammars on a theoretical
level, showing the importance and difficulty of binarization for efficient synchronous
parsing. One way around this difficulty is to stipulate that all rules must be binary
from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary
SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases.
In contrast, the rule extraction method of Galley et al (2004) aims to incorporate more
syntactic information by providing parse trees for the target language and extracting
tree transducer rules that apply to the parses. This approach results in rules with many
nonterminals, making good binarization techniques critical.
We explain how synchronous rule binarization interacts with n-gram language
models and affects decoding for machine translation in Section 2. We define binarization
formally in Section 3, and present an efficient algorithm for the problem in Section 4.
Experiments described in Section 51 show that binarization improves machine trans-
lation speed and quality. Some rules cannot be binarized, and we present a decoding
strategy for these rules in Section 6. Section 7 gives a solution to the general theo-
retical problem of finding optimal decoding and synchronous parsing strategies for
arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our
Chinese?English data. These final two sections are of primarily theoretical interest, as
nonbinarizable rules have not been shown to benefit real-world machine translation sys-
tems. However, the algorithms presented may become relevant as machine translation
systems improve.
1 A preliminary version of Section 1?5 appeared in Zhang et al (2006).
560
Huang et al Binarization of Synchronous Context-Free Grammars
2. Motivation
Consider the following Chinese sentence and its English translation:
(1) ?
Ba`owe?ier
Powell

yu?
with
??
Sha?lo?ng
Sharon
>L
ju?x??ng
hold
?
le
[past.]
Re-structuring, Re-labeling, and Re-aligning
for Syntax-Based Machine Translation
Wei Wang?
Language Weaver, Inc.
Jonathan May??
USC/Information Sciences Institute
Kevin Knight?
USC/Information Sciences Institute
Daniel Marcu?
Language Weaver, Inc.
This article shows that the structure of bilingual material from standard parsing and alignment
tools is not optimal for training syntax-based statistical machine translation (SMT) systems.
We present three modifications to the MT training data to improve the accuracy of a state-of-the-
art syntax MT system: re-structuring changes the syntactic structure of training parse trees to
enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context;
and re-aligning unifies word alignment across sentences to remove bad word alignments and
refine good ones. Better structures, labels, and word alignments are learned by the EM algorithm.
We show that each individual technique leads to improvement as measured by BLEU, and we
also show that the greatest improvement is achieved by combining them. We report an overall
1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English
translation.
1. Background
Syntactic methods have recently proven useful in statistical machine translation (SMT).
In this article, we explore different ways of exploiting the structure of bilingual material
for syntax-based SMT. In particular, we ask what kinds of tree structures, tree labels,
and word alignments are best suited for improving end-to-end translation accuracy.
We begin with structures from standard parsing and alignment tools, then use the EM
algorithm to revise these structures in light of the translation task. We report an overall
+1.48 BLEU improvement on a standard Chinese-to-English test.
? 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: wwang@languageweaver.com.
?? 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: jonmay@isi.edu.
? 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: knight@isi.edu.
? 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: dmarcu@languageweaver.com.
Submission received: 6 November 2008; revised submission received: 10 September 2009; accepted for
publication: 1 January 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
We carry out our experiments in the context of a string-to-tree translation system.
This system accepts a Chinese string as input, and it searches through a multiplicity of
English tree outputs, seeking the one with the highest score. The string-to-tree frame-
work is motivated by a desire to improve target-language grammaticality. For example,
it is common for string-basedMT systems to output sentences with no verb. By contrast,
the string-to-tree framework forces the output to respect syntactic requirements?for
example, if the output is a syntactic tree whose root is S (sentence), then the S will gen-
erally have a child of type VP (verb phrase), which will in turn contain a verb. Another
motivation is better treatment of function words. Often, these words are not literally
translated (either by themselves or as part of a phrase), but rather they control what
happens in the translation, as with case-marking particles or passive-voice particles.
Finally, much of the re-ordering we find in translation is syntactically motivated, and
this can be captured explicitly with syntax-based translation rules. Tree-to-tree systems
are also promising, but in this work we concentrate only on target-language syntax. The
target-language generation problem presents a difficult challenge, whereas the source
sentence is fixed and usually already grammatical.
To prepare training data for such a system, we begin with a bilingual text that has
been automatically processed into segment pairs. We require that the segments be single
sentences on the English side, whereas the corresponding Chinese segments may be
sentences, sentence fragments, or multiple sentences. We then parse the English side of
the bilingual text using a re-implementation of the Collins (1997) parsing model, which
we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
Finally, we word-align the segment pairs according to IBMModel 4 (Brown et al 1993).
Figure 1 shows a sample (tree, string, alignment) triple.
We build two generative statistical models from this data. First, we construct a
smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the
English side of the bilingual data. This model assigns a probability P(e) to any candidate
translation, rewarding translations whose subsequences have been observed frequently
in the training data.
Second, we build a syntax-based translation model that we can use to produce
candidate English trees fromChinese strings. Following previouswork in noisy-channel
Figure 1
A sample learning case for the syntax-based machine translation system described in this article.
248
Wang et al Re-structuring, Re-labeling, and Re-aligning
SMT (Brown et al 1993), our model operates in the English-to-Chinese direction?
we envision a generative top?down process by which an English tree is gradually
transformed (by probabilistic rules) into an observed Chinese string. We represent a
collection of such rules as a tree transducer (Knight and Graehl 2005). In order to
construct this transducer from parsed and word-aligned data, we use the GHKM rule
extraction algorithm of Galley et al (2004). This algorithm computes the unique set of
minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the
minimal rules extracted from the example (tree, string, alignment) triple in Figure 1.
Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and
deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example
form a derivation tree.
We collect all rules over the entire bilingual corpus, and we normalize rule counts
in this way: P(rule) = count(rule)count(LHS-root(rule)) . When we apply these probabilities to derive an
English sentence e and a corresponding Chinese sentence c, we wind up computing the
joint probability P(e, c). We smooth the rule counts with Good?Turing smoothing (Good
1953).
This extraction method assigns each unaligned Chinese word to a default rule in
the derivation tree. We next follow Galley et al (2006) in allowing unaligned Chinese
words to participate in multiple translation rules. In this case, we obtain a derivation
forest of minimal rules. Galley et al show how to use EM to count rules over deriva-
tion forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley
et al in collecting composed rules, namely, compositions of minimal rules. These larger
rules have been shown to substantially improve translation accuracy (Galley et al 2006;
DeNeefe et al 2007). Figure 3 shows some of the additional rules.
With these models, we can decode a new Chinese sentence by enumerating and
scoring all of the English trees that can be derived from it by rule. The score is a
weighted product of P(e) and P(e, c). To search efficiently, we employ the CKY dynamic-
programming parsing algorithm (Yamada and Knight 2002; Galley et al 2006). This
algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix,
we store the non-terminal symbol at the root of the English tree being built up. We also
Figure 2
Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure.
249
Computational Linguistics Volume 36, Number 2
Figure 3
Additional rules extracted from the learning case in Figure 1.
store English words that appear at the left and right corners of the tree, as these are
needed for computing the P(e) score when cells are combined. For CKY to work, all
transducer rules must be broken down, or binarized, into rules that contain at most two
variables?more efficient search can be gained if this binarization produces rules that
can be incrementally scored by the language model (Melamed, Satta, and Wellington
2004; Zhang et al 2006). Finally, we employ cube pruning (Chiang 2007) for further
efficiency in the search.
When scoring translation candidates, we add several smaller models. One model
rewards longer translation candidates, off-setting the language model?s desire for short
output. Other models punish rules that drop Chinese content words or introduce
spurious English content words. We also include lexical smoothing models (Gale and
Sampson 1996; Good 1953) to help distinguish good low-count rules from bad low-
count rules. The final score of a translation candidate is a weighted linear combination
of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain
weights through minimum error-rate training (Och 2003).
The system thus constructed performs fairly well at Chinese-to-English translation,
as reflected in the NIST06 common evaluation of machine translation quality.1
However, it would be surprising if the parse structures and word alignments in our
bilingual data were somehow perfectly suited to syntax-based SMT?we have so far
used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and
Knight (2006) already investigated whether different syntactic labels would be more
appropriate for SMT, though their study was carried out on a weak baseline translation
system. In this article, we take a broad view and investigate how changes to syntactic
structures, syntactic labels, and word alignments can lead to substantial improvements
in translation quality on top of a strong baseline. We design our methods around
problems that arise in MT data whose parses and alignments use some Penn Treebank-
style annotations. We believe that some of the techniques will apply to other annotation
schemes, but conclusions here are limited to Penn Treebank-style trees.
The rest of this article is structured as follows. Section 2 describes the corpora and
model configurations used in our experiments. In each of the next three sections we
present a technique for modifying the training data to improve syntax MT accuracy:
tree re-structuring in Section 3, tree re-labeling in Section 4, and re-aligning in Section 5.
In each of these three sections, we also present experiment results to show the impact
of each individual technique on end-to-end MT accuracy. Section 6 shows the improve-
ment made by combining all three techniques. We conclude in Section 7.
1 http://nist.gov/speech/tests/mt/2006/doc/mt06eval official results.html.
250
Wang et al Re-structuring, Re-labeling, and Re-aligning
2. Corpora for Experiments
For our experiments, we use a 245 million word Chinese/English bitext, available from
LDC. A re-implementation of the Collins (1997) parser runs on the English half of the
bitext to produce parse trees, and GIZA runs on the entire bitext to produce M4 word
alignments. We extract a subset of 36 million words from the entire bitext, by selecting
only sentences in the mainland news domain. We extract translation rules from these
selected 36 million words. Experiments show that our Chinese/English syntax MT
systems built from this selected bitext give as high BLEU scores as from the entire bitext.
Our development set consists of 1,453 lines and is extracted from the NIST02?
NIST05 evaluation sets, for tuning of feature weights. The development set is from the
newswire domain, andwe chose it to represent awide period of time rather than a single
year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set
is a mix of newswire text andWeb text, we also report the BLEU scores on the newswire
portion.
We use two 5-gram languagemodels. One is trained on the English half of the bitext.
The other is trained on one billion words of monolingual data. Kneser?Ney smoothing
(Kneser and Ney 1995) is applied to both language models. Language models are
represented using randomized data structures similar to those of Talbot and Osborne
(2007) in decoding for efficient RAM usage.
To test the significance of improvements over the baseline, we compute paired boot-
strap p-values (Koehn 2004) for BLEU between the baseline system and each improved
system.
3. Re-structuring Trees for Training
Our translation system is trained on Chinese/English data, where the English side
has been automatically parsed into Penn Treebank-style trees. One striking fact about
these trees is that they contain many flat structures. For example, base noun phrases
frequently have five or more direct children. It is well known in monolingual parsing
research that these flat structures cause problems. Although thousands of rewrite rules
can be learned from the Penn Treebank, these rules still do not cover the new rewrites
observed in held-out test data. For this reason, and to extract more general knowl-
edge, many monolingual parsing models aremarkovized so that they can produce flat
structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing
systems binarize the training trees in a pre-processing step, then learn to model the
binarized corpus (Petrov et al 2006); after parsing, their results are flattened back
in a post-processing step. In addition, Johnson (1998b) shows that different types of
tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II
representation) can have a large effect on the parsing performance of a PCFG estimated
from these trees.
We find that flat structures are also problematic for syntax-based machine transla-
tion. The rules we learn from tree/string/alignment triples often lack sufficient gener-
alization power. For example, consider the training samples in Figure 4. We should be
able to learn enough from these two samples to translate the new phrase
?.W-#L?? Z ?{ 3?
VIKTOR CHERNOMYRDIN AND HIS COLLEAGUE
into its English equivalent victor chernomyrdin and his colleagues.
251
Computational Linguistics Volume 36, Number 2
Figure 4
Learning translation rules from flat English structures.
However, the learned rules R12 and R13 do not fit together nicely. R12 can translate
?.W-#L?? into an English base noun phrase (NPB) that includes viktor
chernomyrdin, but only if it is preceded by words that translate into an English JJ and
an English NNP. Likewise, R13 can translate
Z ?{ 3?
into an NPB that includes
and his colleagues, but only if preceded by two NNPs. Both rules want to create an NPB,
and neither can supply the other with what it needs.
If we re-structure the training trees as shown in Figure 5, we get much better
behavior. Now rule R14 translates
?.W-#L?? into a free-standing NPB. This
gives rule R15 the ability to translate
Z ?{ 3?
, because it finds the necessary
NPB to its left.
Here, we are re-structuring the trees in our MT training data by binarizing them.
This allows us to extract better translation rules, though of course an extracted rule may
have more than two variables. Whether the rules themselves should be binarized is a
separate question, addressed in Melamed, Satta, andWellington (2004) and Zhang et al
(2006). One can decide to re-structure training data trees, binarize translation rules, or
do both, or do neither. Here we focus on English tree re-structuring.
In this section, we explore the generalization ability of simple re-structuring meth-
ods like left-, right-, and head-binarization, and also their combinations. Simple bina-
rization methods binarize syntax trees in a consistent fashion (left-, right-, or head-)
and thus cannot guarantee that all the substructures can be factored out. For example,
consistent right binarization of the training examples in Figure 4 makes available R14,
but misses R15. We therefore also introduce a parallel re-structuring method in which
we binarize both to the left and right at the same time, resulting in a binarization
forest. We employ the EM algorithm (Dempster, Laird, and Rubin 1977) to learn the
binarization bias for each tree node in the corpus from the parallel alternatives.
252
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 5
Learning translation rules from binarized English structures.
3.1 Some Concepts
We now explain some concepts to facilitate the descriptions of the re-structuring meth-
ods. We train our translation model on alignment graphs (Galley et al 2004). An
alignment graph is a tuple of a source-language sentence f, a target-language parse tree
that yields e and translates from f, and the word alignments a between e and f. The
graphs in Figures 1, 4, and 5 are examples of alignment graphs.
In the alignment graph, a node in the parse tree is called admissible if rules can be
extracted from it.We can extract rules from a node if and only if the yield of the tree node
is consistent with the word alignments?the f string covered by the node is contiguous
but not empty, and the f string does not align to any e string that is not covered by
the node. An admissible tree node is one where rules overlap. Figure 6 shows different
binarizations of the left tree in Figure 4. In this figure, the NPB node in tree (1) is not
admissible because the f string, V-C, that the node covers also aligns to NNP3, which is
not covered by the NPB. Node NPB in tree (2), on the other hand, is admissible.
A set of sibling tree nodes is called factorizable if we can form an admissible
new node dominating them. In Figure 6, sibling nodes NNP1, NNP2, and NNP3 are
factorizable because we can factorize them out and form a new node NPB, resulting
in tree (2). Sibling tree nodes JJ, NNP1, and NNP2 are not factorizable. Not all sibling
nodes are factorizable, so not all sub-phrases can be acquired and syntactified. Ourmain
purpose is to re-structure parse trees by factorization such that syntactified sub-phrases
can be employed in translation.
With these concepts defined, we now present the re-structuring methods.
253
Computational Linguistics Volume 36, Number 2
Figure 6
Left, right, and head binarizations on the left tree in Figure 4. Tree leaves of nodes JJ and NNP1
are omitted for convenience. Heads are marked with ?. New nonterminals introduced by
binarization are denoted by X-bars.
3.2 Binarizing Syntax Trees
We re-structure parse trees by binarizing the trees. We are going to binarize a tree node
n that dominates r children n1, ..., nr. Binarization is performed by introducing new tree
nodes to dominate a subset of the children nodes. We allow ourselves to form only one
new node at a time to avoid over-generalization. Because labeling is not the concern of
this section, we re-label the newly formed nodes as n.
3.2.1 Simple Binarization Methods. The left binarization of node n (e.g., the NPB in tree
(1) of Figure 6) factorizes the leftmost r ? 1 children by forming a new node n (i.e., the
NPB in tree (1)) to dominate them, leaving the last child nr untouched; and then makes
the new node n the left child of n. The method then recursively left-binarizes the newly
formed node n until two leaves are reached. We left-binarize the left tree in Figure 4 into
Figure 6 (1).
The right binarization of node n factorizes the rightmost r ? 1 children by forming
a new node n (i.e., the NPB in tree (2)) to dominate them, leaving the first child n1
untouched; and then makes the new node n the right child of n. The method then
recursively right-binarizes the newly formed node n. For instance, we right-binarize
the left tree in Figure 4 into Figure 6 (2) and then into Figure 6 (6).
The head binarization of node n left-binarizes n if the head is the first child;
otherwise, it right-binarizes n. We prefer right-binarization to left-binarization when
both are applicable under the head restriction because our initial motivation was to
generalize the NPB-rooted translation rules. As we show in experiments, binarization
of other types of phrases contributes to translation accuracy as well.
Any of these simple binarization methods is easy to implement, but each in itself
is incapable of giving us all the factorizable sub-phrases. Binarizing all the way to the
left, for example, from unbinarized tree to tree (1) and to tree (3) in Figure 6, does not
enable us to acquire a substructure that yields NNP2, NNP3, and their translational
equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in
both directions.
254
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 7
Packed forest obtained by packing trees (3) and (6) in Figure 6.
3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single
parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed
to enable dynamic programming when we extract translation rules from it.
Borrowing terms from parsing semirings (Goodman 1999), a packed forest is com-
posed of additive forest nodes (?-nodes) and multiplicative forest nodes (?-nodes). In
the binarization forest, a ?-node corresponds to a tree node in the unbinarized tree or a
new tree node introduced during tree binarization; and this ?-node composes several
?-nodes, forming a one-level substructure that is observed in the unbinarized tree or
in one of its binarized tree. A ?-node corresponds to alternative ways of binarizing the
same tree node and it contains one or more ?-nodes. The same ?-node can appear in
more than one place in the packed forest, enabling sharing. Figure 7 shows a packed
forest obtained by packing trees (3) and (6) in Figure 6 via the following tree parallel
binarization algorithm.
We use a memoization procedure to recursively parallel-binarize a parse tree.
To parallel-binarize a tree node n that has children n1, ...,nr, we employ the following
steps:
 If r ? 2, parallel-binarize tree nodes n1, ..., nr, producing binarization
?-nodes ?(n1), ..., ?(nr), respectively. Construct node ?(n) as the parent
of ?(n1), ...,?(nr). Construct an additive node ?(n) as the parent of ?(n).
Otherwise, execute the following steps.
 Right-binarize n, if any contiguous2 subset of children n2, ...,nr is
factorizable, by introducing an intermediate tree node labeled as n. We
recursively parallel-binarize n to generate a binarization forest node ?(n).
We also recursively parallel-binarize n1, forming a binarization forest
node ?(n1). We form a multiplicative forest node ?R as the parent of
?(n1) and ?(n).
 Left-binarize n if any contiguous subset of n1, ...,nr?1 is factorizable and
if this subset contains n1. Similar to the previous right-binarization,
we introduce an intermediate tree node labeled as n, recursively
parallel-binarize n to generate a binarization forest node ?(n), recursively
2 For practical purposes we factorize only subsets that cover contiguous spans to avoid introducing
discontiguous constituents. In principle, the algorithm works fine without this condition.
255
Computational Linguistics Volume 36, Number 2
parallel-binarize nr to generate a binarization forest node ?(nr), and then
form a multiplicative forest node ?L as the parent of ?(n) and ?(nr).
 Form an additive node ?(n) as the parent of the two already formed
multiplicative nodes ?L and ?R.
The (left and right) binarization conditions consider any subset to enable the fac-
torization of small constituents. For example, in the left tree of Figure 4, although the
JJ, NNP1, and NNP2 children of the NPB are not factorizable, the subset JJ NNP1 is
factorizable. The binarization from this tree to the tree in Figure 6 (1) serves as a relaying
step for us to factorize JJ and NNP1 in the tree in Figure 6 (3). The left-binarization
condition is stricter than the right-binarization condition to avoid spurious binarization,
that is, to avoid the same subconstituent being reached via both binarizations.
In parallel binarization, nodes are not always binarizable in both directions. For
example, we do not need to right-binarize tree (2) because NNP2 and NNP3 are not
factorizable, and thus cannot be used to form sub-phrases. It is still possible to right-
binarize tree (2) without affecting the correctness of the parallel binarization algorithm,
but that will spuriously increase the branching factor of the search for the rule extrac-
tion, because we will have to expand more tree nodes.
A special version of parallel binarization is the parallel head binarization, where
both the left and the right binarization must respect the head propagation property
at the same time. Parallel head binarization guarantees that new nodes introduced by
binarization always contain the head constituent, which will become convenient when
head-driven syntax-based language models are integrated into a bottom?up decoding
search by intersecting with the trees inferred from the translation model.
Our re-structuring of MT training trees is realized by tree binarization, but this does
not mean that our re-structuring method can factor out phrases covered only by two
(binary) constituents. In fact, a nice property of parallel binarization is that for any
factorizable substructure in the unbinarized tree, we can always find a corresponding
admissible ?-node in the parallel-binarized packed forest, and thus we can always
extract that phrase. A leftmost substructure like the lowest NPB-subtree in tree (3) of
Figure 6 can be made factorizable by several successive left binarizations, resulting in
the ?5(NPB)-node in the packed forest in Figure 7. A substructure in the middle can be
factorized by the composition of several left- and right-binarizations. Therefore, after a
tree is parallel-binarized, to make the sub-phrases available to the MT system, all we
need to do is to extract rules from the admissible nodes in the packed forest. Rules that
can be extracted from the original unrestructured tree can be extracted from the packed
forest as well.
Parallel binarization results in parse forests. Thus translation rules need to be ex-
tracted from training data consisting of (e-forest, f, a)-tuples.
3.3 Extracting Translation Rules from (e-forest, f, a)-tuples
Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization
of the (e-parse, f, a)-based rule extraction algorithm in Galley et al (2006). A similar
problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based
rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation
forest (Galley et al 2006), which consists of overlapping translation rules. The algorithm
recursively traverses the e-forest top?down, extracts rules only at admissible e-forest
256
Wang et al Re-structuring, Re-labeling, and Re-aligning
nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the
following two procedures, depending on which condition is met.
 Condition 1: If we reach an additive e-forest node, for each of its children,
which are multiplicative e-forest nodes, we go to condition 2 to recursively
extract rules from it to obtain a set of multiplicative derivation-forest
nodes, respectively. We form an additive derivation-forest node, and take
these newly produced multiplicative derivation-forest nodes (by going to
condition 2) as children. After this, we return the additive
derivation-forest node.
For instance, at node ?1(NPB) in Figure 7, for each of its children, e-forest
nodes ?2(NPB) and ?11(NPB), we go to condition 2 to extract rules on it,
to form multiplicative derivation forest nodes, ?(R16) and ?(R17) in
Figure 8.
 Condition 2: If we reach a multiplicative e-forest node, we extract a set of
rules rooted at it using the procedure in Galley et al (2006); and for each
rule, we form a multiplicative derivation-forest node, and go to condition 1
to form the additive derivation-forest nodes for the additive frontier
e-forest nodes of the newly extracted rule, and then make these additive
derivation-forest nodes the children of the multiplicative derivation-forest
node. After this, we return a set of multiplicative derivation-forest nodes,
each corresponding to one rule extracted from the multiplicative e-forest
node we just reached.
Figure 8
A synchronous derivation forest built from a (e-forest, f, a) triple. The e-forest is shown in
Figure 7.
257
Computational Linguistics Volume 36, Number 2
For example, at node ?11(NPB) in Figure 7, we extract a rule from it and
form derivation-forest node ?(R17) in Figure 8. We then go to condition 1
to obtain, for each of the additive frontier e-forest nodes (in Figure 7) of
this rule, a derivation-forest node, namely, ?(NNP), ?(NNP), and ?(NPB)
in Figure 8. We make these derivation-forest ?-nodes the children of
derivation-forest node ?(R17).
This procedure transforms the packed e-forest in Figure 7 into a packed synchro-
nous derivation in Figure 8. This algorithm is an extension of the extraction algorithm
in Galley et al (2006), in the sense that we have an extra condition (1) to relay rule
extraction on additive e-forest nodes.
The forest-based rule extraction algorithm produces much larger grammars than
the tree-based one, making it difficult to scale to very large training data. From a
50M-word Chinese-to-English parallel corpus, we can extract more than 300 million
translation rules, while the tree-based rule extraction algorithm gives approximately
100 million. However, the restructured trees from the simple binarization methods are
not guaranteed to give the best trees for syntax-based machine translation. What we
desire is a binarization method that still produces single parse trees, but is able to mix
left binarization and right binarization in the same tree. In the following, we use the EM
algorithm to learn the desirable binarization on the forest of binarization alternatives
proposed by the parallel binarization algorithm.
3.4 Learning How to Binarize Via the EM Algorithm
The basic idea of applying the EM algorithm to choose a re-structuring is as follows. We
perform a set {?} of binarization operations on a parse tree ?. Each binarization ? is
the sequence of binarizations on the necessary (i.e., factorizable) nodes in ? in pre-order.
Each binarization ? results in a restructured tree ??. We extract rules from (??, f, a),
generating a translation model consisting of parameters (i.e., rule probabilities) ?. Our
aim is to first obtain the rule probabilities that are the maximum likelihood estimate
of the training tuples, and then produce the Viterbi binarization tree for each training
tuple.
The probability P(??, f, a) of a (??, f, a)-tuple is what the basic syntax-based trans-
lation model is concerned with. It can be further computed by aggregating the rule
probabilities P(r) in each derivation? in the set of all derivations ? (Galley et al 2004).
That is,
P(??, f, a) =
?
???
?
r??
P(r) (1)
The rule probabilities are estimated by the inside?outside algorithm (Lari and
Young 1990; Knight, Graehl, and May 2008), which needs to run on derivation forests.
Our previous sections have already presented algorithms to transform a parse tree into
a binarization forest, and then transform the (e-forest, f, a)-tuples into derivation forests
(e.g., Figure 8), on which the inside?outside algorithm can then be applied.
In the derivation forests, an additive node labeled as A dominates several mul-
tiplicative nodes, each corresponding to a translation rule resulting from either left
binarization or right binarization of the original structure. We use rule r to either refer
to a rule or to a multiplicative node in the derivation forest. We use root(r) to represent
the root label of the rule, and parent(r) to refer to the additive node that is the parent
258
Wang et al Re-structuring, Re-labeling, and Re-aligning
of the node corresponding to the r. Each rule node (or multiplicative node) dom-
inates several other additive children nodes, and we present the ith child node as
childi(r), among the total number of n children. For example, in Figure 8, for the rule r
corresponding to the left child of the forest root (labeled as NPB), parent(r) is NPB, and
child1(NPB) = r. Based on these notations, we can compute the inside probability ?(A)
of an additive node labeled as A and the outside probability ?(B) of an additive forest
node labeled as B as follows.
?(A) =
?
r?{child(A)}
P(r)?
?
i=1...n
?(childi(r)) (2)
?(B) =
?
r:B?{child(r)}
P(r)? ?(parent(r))?
?
C?{child(r)}?{B}
?(C) (3)
In the expectation step, the contribution of each occurrence of a rule in a derivation-
forest to the total expected count of that rule is computed as
?(parent(r))? P(r)?
?
i=1...n
?(childi(r)) (4)
In the maximization step, we use the expected counts of rules, #r, to update the proba-
bilities of the rules.
P(r) = #r?
rule q:root(q)=root(r) #q
(5)
Because it is well known that applying EM with tree fragments of different sizes
causes overfitting (Johnson 1998a), and because it is also known that syntax MT models
with larger composed rules in the mix significantly outperform rules that minimally
explain the training data (minimal rules) in translation accuracy (Galley et al 2006), we
use minimal rules to construct the derivation forests from (e-binarization-forest, f, a)-
tuples during running of the EM algorithm, but, after the EM re-structuring is finished,
we build the final translation model using composed rules for MT evaluation.
Figure 9 is the actual pipeline that we use for EM binarization. We first generate a
packed e-forest via parallel binarization. We then extract minimal translation rules from
Figure 9
Using the EM algorithm to choose re-structuring.
259
Computational Linguistics Volume 36, Number 2
the (e-forest, f, a)-tuples, producing synchronous derivation forests. We run the inside?
outside algorithm on the derivation forests until convergence. We obtain the Viterbi
derivations and project the English parses from the derivations. Finally, we extract
composed rules using Galley et al (2006)?s (e-tree, f, a)-based rule extraction algorithm.
When extracting composed rules from (e-parse, f, a)-tuples, we use an ?ignoring-X-
node? trick to the rule extraction method in Galley et al (2006) to avoid breaking the
local dependencies captured in complex rules. The trick is that new nodes introduced
by binarization are not counted when computing the rule size limit unless they appear
as the rule roots. The motivation is that newly introduced nodes break the local depen-
dencies, deepening the parses. In Galley et al, a composed rule is extracted only if the
number of internal nodes it contains does not exceed a limit, similar to the phrase length
limit in phrase-based systems. This means that rules extracted from the restructured
trees will be smaller than those from the unrestructured trees, if the X nodes are deleted
from the rules. As shown in Galley et al, smaller rules lose context, and thus give
lower translation accuracy. Ignoring X nodes when computing the rule sizes preserves
the unrestructured rules in the resulting translation model and adds substructures as
bonuses.
3.5 Experimental Results
We carried out experiments to evaluate different tree binarization methods in terms
of translation accuracy for Chinese-to-English translation. The baseline syntax MT sys-
tem was trained on the original, non-restructured trees. We also built one MT system
by training on left-binarizations of training trees, and another by training on EM-
binarizations of training trees.
Table 1 shows the results on end-to-endMT. The bootstrap p-values were computed
for the pairwise BLEU comparison between the EM binarization and the baseline. The
results show that tree binarization improves MT system accuracy, and that EM binariza-
tion outperforms left binarization. The results also show that the EM re-structuring sig-
nificantly outperforms (p <0.05) the no re-structuring baseline on the NIST08 eval set.
The MT improvement by tree re-structuring is also validated by our previous work
(Wang, Knight, and Marcu 2007), in which we reported a 1 BLEU point gain from EM
binarization under other training/testing conditions; other simple binarizationmethods
were examined in that work aswell, showing that simple binarizations also improveMT
accuracy, and that EM binarization consistently outperforms the simple binarization
methods.
Table 1
Translation accuracy versus binarization algorithms. In this and all other tables reporting BLEU
performance, statistically significant improvements over the baseline are highlighted. p = the
paired bootstrap p-value computed between each system and the baseline, showing the level at
which the two systems are significantly different.
EXPERIMENT NIST08 NIST08-NW
BLEU p BLEU p
no binarization (baseline) 29.12 ? 35.33 ?
left binarization 29.35 0.184 35.46 0.360
EM binarization 29.74 0.010 36.12 0.016
260
Wang et al Re-structuring, Re-labeling, and Re-aligning
Table 2
# admissible nodes, # rules versus re-structuring methods.
RE-STRUCTURING METHOD # ADMISSIBLE NODES (M) # RULES (M)
no binarization 13 76.0
left binarization 17.2 153.4
EM binarization 17.4 154.8
We think that these improvements are explained by the fact that tree re-structuring
introduces more admissible trees nodes in the training trees and enables the forming
of additional rules. As a result, re-structuring produces more rules. Table 2 shows the
number of admissible nodes made available by each re-structuring method, as well as
by the baseline. Table 2 also shows the sizes of the resulting grammars.
The EM binarization is able to introduce more admissible nodes because it mixes
both left and right binarizations in the same tree. We computed the binarization biases
learned by the EM algorithm for each nonterminal from the binarization forest of
parallel head binarizations of the training trees (Table 3). Of course, the binarization
bias chosen by left-/right-binarization methods would be 100% deterministic. One
noticeable message from Table 3 is that most of the categories are actually biased toward
left-binarization. The reason might be that the head sub-constituents of most English
categories tend to be on the left.
Johnson (1998b) argues that the more nodes there are in a treebank, the stronger
the independence assumptions implicit in the PCFG model are, and the less accurate
the estimated PCFG will usually be?more nodes break more local dependencies. Our
experiments, on the other hand, show MT accuracy improvement by introducing more
admissible nodes. This initial contradiction actually makes sense. The key is that we use
composed rules to build our final MT system and that we introduce the ?ignoring-X-
node? trick to preserve the local dependencies. More nodes in training trees weaken the
accuracy of a translation model of minimal rules, but boost the accuracy of a translation
model of composed rules.
Table 3
Binarization bias learned by the EM re-structuring method on the model 4 word alignments.
nonterminal left-binarization (%) right-binarization (%)
NP 98 2
NPB 1 99
VP 95 5
PP 86 14
ADJP 67 33
ADVP 76 24
S 94 6
S-C 17 83
SBAR 93 7
QP 89 11
WHNP 98 2
SINV 94 6
CONJP 69 31
261
Computational Linguistics Volume 36, Number 2
4. Re-Labeling Trees for Training
The syntax translation model explains (e-parse, f, a)-tuples by a series of applications
of translation rules. At each derivation step, which rule to apply next depends only
on the nonterminal label of the frontier node being expanded. In the Penn Treebank
annotation, the nonterminal labels are too coarse to encode enough context information
to accurately predict the next translation rule to apply. As a result, using the Penn
Treebank annotation can license ill-formed subtrees (Figure 10). This subtree contains
an error that induces a VP as an SG-C when the head of the VP is the finite verb dis-
cussed. The translation error leads to the ungrammatical ?... confirmed discussed ... ?. This
translation error occurs due to the fact that there is no distinction between finite VPs
and non-finite VPs in Penn Treebank annotation. Monolingual parsing suffers similarly,
but to a lesser degree.
Re-structuring of training trees enables the reuse of sub-constituent structures, but
further introduces new nonterminals and actually reduces the context for rules, thus
making this ?coarse nonterminal? problem more severe. In Figure 11, R23 may be
extracted from a construct like S(S CC S) via tree binarization, and R24 may be extracted
from a construct like S(NP NP-C VP) via tree binarization. Composing R23 and R24
forms the structure in Figure 11(b), which, however, is ill-formed. This wrong structure
in Figure 11(b) yields ungrammatical translations like he likes reading she does not like
reading. Tree binarization enables the reuse of substructures, but causes over-generation
of trees at the same time.
We solve the coarse-nonterminal problem by refining/re-labeling the training tree
labels. Re-labeling is done by enriching the nonterminal label of each tree node based
on its context information.
Re-labeling has already been used in monolingual parsing research to improve
parsing accuracy of PCFGs. We are interested in two types of re-labeling methods:
Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches
the labels of parser training trees using parent labels, head word tag labels, and/or
sibling labels. Automatic category splitting (Petrov et al 2006) refines a nonterminal
Figure 10
MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule
overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22.
262
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 11
Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired
from binarized training trees, aiming for reuse of substructures. Composing R23 and R24,
however, results in an ill-formed tree. The new nonterminal S introduced in tree binarization
needs to be refined into different sub-categories to prevent R23 and R24 from being composed.
Automatic category splitting can be employed for refining the S.
by classifying the nonterminal into a fine-grained sub-category, and this sub-classing
is learned via the EM algorithm. Category splitting is realized by several splitting-and-
merging cycles. In each cycle, the nonterminals in the PCFG rules are split by splitting
each nonterminal into two. The EM algorithm is employed to estimate the split PCFG on
the Penn Treebank training trees. After that, 50% of the new nonterminals are merged
based on some loss function, to avoid overfitting.
4.1 Linguistic Re-labeling
In the linguistically motivated approach, we employ the following set of rules to re-label
tree nodes. In our MT training data:
 SPLIT-VP: annotates each VP nodes with its head tag, and then merges all
finite VP forms to a single VPF.
 SPLIT-IN: annotates each IN node with the combination of IN and its
parent node label. IN is frequently overloaded in the Penn Treebank. For
instance, its parent can be PP or SBAR.
These two operations re-label the tree in Figure 12(a1) to Figure 12(b1). Example
rules extracted from these two trees are shown in Figure 12(a2) and Figure 12(b2),
respectively. We apply this re-labeling on the MT training tree nodes, and then acquire
rules from these re-labeled trees. We chose to split only these two categories because
our syntax MT system tends to frequently make parse errors in these two categories,
and because, as shown by Klein and Manning (2003), further refining the VP and IN
categories is very effective in improving monolingual parsing accuracy.
This type of re-labeling fixes the parse error in Figure 10. SPLIT-VP transforms the
R18 root to VPF, and the R17 frontier node to VP.VBN. Thus R17 and R18 can never be
composed, preventing the wrong tree being formed by the translation model.
4.2 Statistical Re-labeling
Our second re-labeling approach is to learn the split categories for the node labels of
the training trees via the EM algorithm, as in Petrov et al (2006). Rather than using
263
Computational Linguistics Volume 36, Number 2
Figure 12
Re-labeling of parse trees.
264
Wang et al Re-structuring, Re-labeling, and Re-aligning
their parser to directly produce category-split parse trees for the MT training data, we
separate the parsing step and the re-labeling step. The re-labeling method is as follows.
1. Run a parser to produce the MT training trees.
2. Binarize the MT training trees via the EM binarization algorithm.
3. Learn an n-way split PCFG from the binarized trees via the algorithm
described in Petrov et al (2006).
4. Produce the Viterbi split annotations on the binarized training trees with
the learned category-split PCFG.
As we mentioned earlier, tree binarization sometimes makes the decoder over-
generalize the trees in the MT outputs, but we still binarize the training trees before
performing category splitting, for two reasons. The first reason is that the improve-
ment on MT accuracy we achieved by tree re-structuring indicates that the benefit
we obtained from structure reuse triumphs the problem of tree over-generalization.
The second is that carrying out category splitting on unbinarized training trees blows
up the grammar?splitting a CFG rule of rank 10 results in 211 split rules. This re-
labeling procedure tries to achieve further improvement by trying to fix the tree over-
generalization problem of re-structuring while preserving the gain we have already
obtained from tree re-structuring.
Figure 12(c1) shows a category-split tree, and Figure 12(c2) shows the minimal xRs
rules extracted from the split tree. In Figure 12(c2), the two VPs (VP-0 and VP-2) now
belong to two different categories and cannot be used in the same context.
In this re-labeling procedure, we separate the re-labeling step from the parsing step,
rather than using a parser like the one in Petrov et al (2006) to directly produce category-
split parse trees on the English corpus. We think that we benefit from this separation in
the following ways: First, this gives us the freedom to choose the parser to produce the
initial trees. Second, this enables us to train the re-labeler on the domains where the MT
system is trained, instead of on the Penn Treebank. Third, this enables us to choose our
own tree binarization methods.
Tree re-labeling fragments the translation rules. Each refined rule now fits in fewer
contexts than its corresponding coarse rule. Re-labeling, however, does not explode
the grammar size, nor does re-labeling deteriorate the reuse of substructures. This
is because the re-labeling (whether linguistic or automatic) results in very consistent
annotations. Table 4 shows the sizes of the translation grammars from different re-
labelings of the training trees, as well as that from the unrelabeled ones.
Table 4
Grammar size vs. re-labeling methods. Re-labeling does not explode the grammar size.
RE-LABELING METHOD # RULES (M) NONTERMINAL SET SIZE
No re-labeling 154.80 144
Linguistically motivated re-labeling 154.97 210
4-way splitting (90% merging) 158.89 178
8-way splitting (90% merging) 160.62 195
4-way splitting (50% merging) 164.15 326
265
Computational Linguistics Volume 36, Number 2
It would be very interesting to perform automatic category splitting with synchro-
nous translation rules and run the EM algorithm on the synchronous derivation forests.
Synchronous category splitting is computationally much more expensive, so we do not
study it here.
4.3 Experimental Results
We ran end-to-end MT experiments by re-labeling the MT training trees. Our two base-
line systems were a syntax MT system with neither re-structuring nor re-labeling, and
a syntax MT system with re-structuring but no re-labeling. The linguistically motivated
re-labeling method was applied directly on the original (unrestructured) training trees,
so that it could be compared to the first baseline. The automatic category splitting re-
labeling method was applied to binarized trees so as to avoid the explosion of the split
grammar, so it is compared to the second baseline. The experiment results are shown in
Table 5.
Both re-labeling methods help MT accuracy. Putting both re-structuring and re-
labeling together results in 0.93 BLEU points improvement on NIST08 set, and 1 BLEU
point improvement on the newswire subset. All p-values are computed between the
re-labeling systems and Baseline1. The improvement made by the linguistically moti-
vated re-labeling method is significant at the 0.05 level. Because the automatic category
splitting is carried out on the top of EM re-structuring and because, as we have already
shown, EM re-structuring significantly improves Baseline1, putting them together re-
sults in better translations with more confidence.
If we compare these results to those in Table 1, we notice that re-structuring tends
to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring
overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and
Table 1 show substructure reuse mitigates structure over-generalization in our tree re-
structuring method.
5. Re-aligning (Tree, String) Pairs for Training
So far, we have improved the English structures in our parsed, aligned training corpus.
We now turn to improving the word alignments.
Some MT systems use the same model for alignment and translation?examples
include Brown et al (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada
and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al
for alignment, then collect counts for a completely different model, such as Och andNey
Table 5
Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was
carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values
are computed against Baseline1.
EXPERIMENT NIST08 NIST08-NW
BLEU p BLEU p
Baseline1 (no re-structuring and no re-labeling) 29.12 ? 35.33 ?
Linguistically motivated re-labeling 29.57 0.029 35.85 0.050
Baseline2 (EM re-structuring but no re-labeling) 29.74 ? 36.12 ?
4-way splitting (w/ 90% merging) 30.05 0.001 36.42 0.003
266
Wang et al Re-structuring, Re-labeling, and Re-aligning
(2004) or Chiang (2007). Our basic syntax-based system falls into this second category,
as we learn our syntactic translation model from a corpus aligned with word-based
techniques. We would like to inject more syntactic reasoning into the alignment process.
We start by contrasting two generative translation models.
5.1 The Traditional IBM Alignment Model
IBMModel 4 (Brown et al 1993) learns a set of four probability tables to compute P( f |e)
given a foreign sentence f and its target translation e via the following (simplified)
generative story:
1. A fertility y for each word ei in e is chosen with probability Pfert(y|ei).
2. A null word is inserted next to each fertility-expanded word with
probability Pnull.
3. Each token ei in the fertility-expanded word and null string is translated
into some foreign word fi in f with probability Ptrans( fi|ei).
4. The position of each foreign word fi that was translated from ei is changed
by? (which may be positive, negative, or zero) with probability
Pdistortion(?|A(ei),B( fi)), where A and B are functions over the source and
target vocabularies, respectively.
Brown et al (1993) describe an EM algorithm for estimating values for the four
tables in the generative story. With those values in hand, we can calculate the highest-
probability (Viterbi) alignment for any given string pair.
Two scale problems arise in this algorithm. The first is the time complexity of enu-
merating alignments for fractional count collection. This is solved by considering only
a subset of alignments, and by bootstrapping the Ptrans table with a simpler model that
admits fast count collection via dynamic programming, such as IBM Model 1 (Brown
et al 1993) or Aachen HMM (Vogel, Ney, and Tillmann 1996). The second problem is
one of space. In theory, the initial Ptrans table contains a cell for every English word
paired with every Chinese word?this would be infeasible. Fortunately, in practice, the
table can be initialized with only those word pairs observed co-occurring in the parallel
training text.
5.2 A Syntax Re-alignment Model
Our syntax translation model learns a single probability table to compute P(etree, f )
given a foreign sentence f and a parsed target translation etree. In the following gen-
erative story we assume a starting variable with syntactic type v.
1. Choose a rule r to replace v, with probability Prule(r|v).
2. For each variable with syntactic type vi in the partially completed (tree,
string) pair, continue to choose rules ri with probability Prule(ri|vi) to
replace these variables until there are no variables remaining.
We can use this model to explain unaligned (tree, string) pairs from our training
data. With a large enough rule set, any given (tree, string) pair will admit many deriva-
tions. Consider again the example from Figure 1. The particular alignment associated
267
Computational Linguistics Volume 36, Number 2
with that (tree, string) pair yields the minimal rules of Figure 2. A different alignment
yields different rules. Figure 13 shows two other alignments and their corresponding
minimal rules. As noted before, a set of minimal rules in proper sequence forms
a derivation tree of rules that explains the (tree, string) pair. Because rules explain
variable-size fragments (e.g., R35 vs. R6), the possible derivation trees of rules that
explain a sentence pair have varying sizes. The smallest such derivation tree has a single
large rule (which does not appear in Figure 13). When our model chooses a particular
derivation tree of minimal rules to explain a given (tree, string) pair it implicitly chooses
the alignment that produced these rules as well.3 Our model can choose a derivation by
using any of the rules in Figures 13 and 2. We would prefer it select the derivation that
yields the good alignment in Figure 1.
We can also develop an EM learning approach for this model. As in the IBM
approach, we have both time and space issues. Time complexity, as we will see sub-
sequently, is O(mn3), where m is the number of nodes in the English training tree and
n is the length of the corresponding Chinese string. Space is more of a problem. We
would like to initialize EM with all the rules that might conceivably be used to explain
the training data. However, this set is too large to practically enumerate.
To reduce the model space we first create a bootstrap alignment using a simpler
word-based model. Then we acquire a set of minimal translation rules from the (tree,
string, alignment) triples. Armed with these rules, we can discard the word-based
alignments and re-alignwith the syntax translation model.
We summarize the approach described in this section as:
1. Obtain bootstrap alignments for a training corpus using word-based
alignment.
2. Extract minimal rules from the corpus and alignments using GHKM,
noting the partial alignment that is used to extract each rule.
3. Construct derivation forests for each (tree, string) pair, ignoring the
alignments, and run EM to obtain Viterbi derivation trees, then use the
annotated partial alignments to obtain Viterbi alignments.
4. Use the new alignments to re-train the full MT system, this time collecting
composed rules as well as minimal rules.
5.3 EM Training for the Syntax Translation Model
Consider the example of Figure 13 again. The top alignment was the bootstrap align-
ment, and thus prior to any experiment we obtained the corresponding indicated
minimal rule set and derivation. This derivation is reasonable but there are some poorly
motivated rules, from a linguistic standpoint. The Chinese word
??
roughly means
the two shores in this context, but the rule R35 learned from the alignment incorrectly
includes between. However, other sentences in the training corpus have the correct
3 Strictly speaking there is actually a one-to-many mapping between a derivation tree of minimal rules and
the alignment that yields these rules, due to the handling of unaligned words. However, the choice of one
partial alignment over another does not affect results and in practice we impose a one-to-one mapping
between minimal rules and the partial alignments that imply them by selecting the most frequently
observed partial alignment for a given minimal rule.
268
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 13
The minimal rules extracted from two different alignments of the sentence in Figure 1.
alignment, which yields rules in Figure 2, such as R8. Figure 2 also contains rules R4
and R6, learned from yet other sentences in the training corpus, which handle the
?
...
?
structure (which roughly translates to in between), thus allowing a derivation which
contains the minimal rule set of Figure 2 and implies the alignment in Figure 1.
EM distributes rule probabilities in such a way as to maximize the probability of the
training corpus. It thus prefers to use one rule many times instead of several different
269
Computational Linguistics Volume 36, Number 2
rules for the same situation over several sentences, if possible. R35 is a possible rule
in 46 of the 329,031 sentence pairs in the training corpus, and R8 is a possible rule in
100 sentence pairs. Well-formed rules are more usable than ill-formed rules and the
partial alignments behind these rules, generally also well-formed, become favored as
well. The top row of Figure 14 contains an example of an alignment learned by the
bootstrap alignment model that includes an incorrect link. Rule R25, which is extracted
from this alignment, is a poor rule. A set of commonly seen rules learned from other
training sentences provide a more likely explanation of the data, and the consequent
alignment omits the spurious link.
Figure 14
The impact of a bad alignment on rule extraction. Including the alignment link indicated by the
dotted line in the example leads to the rule set in the second row. The re-alignment procedure
described in Section 5.2 learns to prefer the rule set at bottom, which omits the bad link.
270
Wang et al Re-structuring, Re-labeling, and Re-aligning
Table 6
Translation performance, grammar size versus the re-alignment algorithm proposed in
Section 5.2, and re-alignment as modified in Section 5.4.
EXPERIMENT NIST08 NIST08-NW # RULES (M)
BLEU p BLEU p
no re-alignment (baseline) 29.12 ? 35.33 ? 76.0
EM re-alignment 29.18 0.411 35.52 0.296 75.1
EM re-alignment with size prior 29.37 0.165 35.96 0.050 110.4
Now we need an EM algorithm for learning the parameters of the rule set that
maximize
?
corpus
P(tree, string). Knight, Graehl, andMay (2008) present a generic such algo-
rithm for tree-to-string transducers that runs in O(mn3) time, as mentioned earlier. The
algorithm consists of two components: DERIV, which is a procedure for constructing
a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus
given that corpus and a rule set, and TRAIN, which is an iterative parameter-setting
procedure.
We initially attempted to use the top-down DERIV algorithm of Knight, Graehl, and
May (2008), but as the constraints of the derivation forests are largely lexical, too much
time was spent on exploring dead-ends. Instead we build derivation forests using the
following sequence of operations:
1. Binarize rules using the synchronous binarization algorithm for
tree-to-string transducers described in Zhang et al (2006).
2. Construct a parse chart with a CKY parser simultaneously constrained on
the foreign string and English tree, similar to the bilingual parsing of Wu
(1997).4
3. Recover all reachable edges by traversing the chart, starting from the
topmost entry.
Because the chart is constructed bottom-up, leaf lexical constraints are encountered
immediately, resulting in a narrower search space and faster running time than the
top-down DERIV algorithm for this application. The Viterbi derivation tree tells us
which English words produce which Chinese words, so we can extract a word-to-word
alignment from it.
Although in principle the re-alignment model and translation model learn parame-
ter weights over the same rule space, in practice we limit the rules used for re-alignment
to the set of minimal rules.
5.4 Adding a Rule Size Prior
An initial re-alignment experiment shows a small rise in BLEU scores from the baseline
(Table 6), but closer inspection of the rules favored by EM implies we can do even better.
4 In the cases where a rule is not synchronous-binarizable, standard left?right binarization is performed
and proper permutation of the disjoint English tree spans must be verified when building the part of the
chart that uses this rule.
271
Computational Linguistics Volume 36, Number 2
EM has a tendency to favor a few large rules over many small rules, even when the
small rules are more useful. Referring to the rules in Figures 2 and 13, note that possible
derivations for (taiwan?s,
?l
)5 are R33, R2?R3, and R38?R40. Clearly the third deriva-
tion is not desirable, and we do not discuss it further. Between the first two derivations,
R2?R3 is preferred over R33, as the conditioning for possessive insertion is not related to
the specific Chinese word being inserted. Of the 1,902 sentences in the training corpus
where this pair is seen, the bootstrap alignments yield the R33 derivation 1,649 times
and the R2?R3 derivation 0 times. Re-alignment does not change the result much; the
new alignments yield the R33 derivation 1,613 times and again never choose R2?R3. The
rules in the second derivation themselves are not rarely seen?R2 is in 13,311 forests
other than those where R33 is seen, and R3 is in 2,500 additional forests. EM gives R2 a
probability of e?7.72?better than 98.7% of rules, and R3 a probability of e?2.96. But R33
receives a probability of e?6.32 and is preferred over the R2?R3 derivation, which has a
combined probability of e?10.68.
The preference for shorter derivations containing large rules over longer derivations
containing small rules is due to a general tendency for EM to prefer derivations with
few atoms. Marcu and Wong (2002) note this preference but consider the phenomenon
a feature, rather than a bug. Zollmann and Sima?an (2005) combat the overfitting aspect
for parsing by using a held-out corpus and a straight maximum likelihood estimate,
rather than EM. DeNero, Bouchard-Co?te?, and Klein (2008) encourage small rules with a
modeling approach; they put a Dirichlet process prior of rule size over their model and
learn the parameters of the geometric distribution of that prior with Gibbs sampling.We
use a simpler modeling approach to accomplish the same goals as DeNero, Bouchard-
Co?te?, and Klein which, although less elegant, is more scalable and does not require a
separate Bayesian inference procedure.
As the probability of a derivation is determined by the product of its atom probabil-
ities, longer derivations with more probabilities to multiply have an inherent disadvan-
tage against shorter derivations, all else being equal. EM is an iterative procedure and
thus such a bias can lead the procedure to converge with artificially raised probabilities
for short derivations and the large rules that constitute them. The relatively rare ap-
plicability of large rules (and thus lower observed partial counts) does not overcome the
inherent advantage of large coverage. To combat this, we introduce size terms into our
generative story, ensuring that all competing derivations for the same sentence contain
the same number of atoms:
1. Choose a rule size s with cost csize(s)
s?1.
2. Choose a rule r (of size s) to replace the start symbol with probability
Prule(r|s, v).
3. For each variable in the partially completed (tree, string) pair, continue to
choose sizes followed by rules, recursively to replace these variables until
there are no variables remaining.
This generative story changes the derivation comparison from R33 vs. R2?R3 to S2?
R33 vs. R2?R3, where S2 is the atom that represents the choice of size 2 (the size of a rule
5 The Chinese gloss is simply ?taiwan?.
272
Wang et al Re-structuring, Re-labeling, and Re-aligning
in this context is the number of non-leaf and non-root nodes in its tree fragment). Note
that the variable number of inclusions implied by the exponent in the generative story
above ensures that all derivations have the same size. For example, a derivation with
one size-3 rule, a derivation with one size-2 and one size-1 rule, and a derivation with
three size-1 rules would each have three atoms. With this revised model that allows for
fair comparison of derivations, the R2?R3 derivation is chosen 1,636 times, and S2?R33
is not chosen. R33 does, however, appear in the translation model, as the expanded rule
extraction described in Section 1 creates R33 by joining R2 and R3.
The probability of size atoms, like that of rule atoms, is decided by EM. The revised
generative story tends to encourage smaller sizes by virtue of the exponent. This does
not, however, simply ensure the largest number of rules per derivation is used in all
cases. Ill-fitting and poorly motivated rules such as R42, R43, and R44 in Figure 13 are
not preferred over R8, even though they are smaller. However, R6 and R8 are preferred
over R35, as the former are useful rules. Although the modified model does not sum to
1, it can nevertheless lead to an improvement in BLEU score.
5.5 Experimental Results
The end-to-end MT experiment used as baseline and described in Sections 3.5 and 4.3
was trained on IBMModel 4 word alignments, obtained by running GIZA, as described
in Section 2. We compared this baseline to an MT system that used alignments obtained
by re-aligning the GIZA alignments using the method of Section 5.2 with the 36 million
word subset of the training corpus used for re-alignment learning. We next compared
the baseline to an MT system that used re-alignments obtained by also incorporating
the size prior described in Section 5.4. As can be seen by the results in Table 6, the size
prior method is needed to obtain reasonable improvement in BLEU. These results are
consistent with those reported in May and Knight (2007), where gains in Chinese and
ArabicMT systemswere observed, though over aweaker baseline andwith less training
data than is used in this work.
6. Combining Techniques
We have thus far seen gains in BLEU score by independent improvements in training
data tree structure, syntax labeling, and alignment. This naturally raises the question
of whether the techniques can be combined, that is, if improvement in one aspect of
training data aids in improvement of another. As reported in Section 4.3 and Table 5,
we were able to improve re-labeling efforts and take advantage of the split-and-merge
technique of Petrov et al (2006) by first re-structuring via the method described in Sec-
tion 3.4. It is unlikely that such re-structuring or re-labeling would aid in a subsequent
re-alignment procedure like that of Section 5.2, for re-structuring changes trees based
on a given alignment, and re-alignment can only change links when multiple instances
of a (subtree, substring) tuple are found in the data with different partial alignments.
Re-structuring beforehand changes the trees over different alignments differently. It is
unlikely that many (subtree, substring) tuples with more than one partial alignment
would remain after a re-structuring.
However, re-structuring may benefit from a prior re-alignment. We do not want re-
structuring decisions to be made over bad alignments, so unifying alignments based
on common syntax should lead EM to make a more confident binarization decision.
273
Computational Linguistics Volume 36, Number 2
Table 7
Summary of experiments in this article, including a combined experiment with re-alignment,
re-structuring, and re-labeling.
EXPERIMENT NIST08 NIST08-NW # RULES (M)
BLEU p BLEU p
Baseline (no binarization, no re-labeling, 29.12 ? 35.33 ? 76.0
Model 4 alignments)
left binarization 29.35 0.184 35.46 0.360 153.4
EM binarization 29.74 0.010 36.12 0.016 154.8
Linguistic re-labeling 29.57 0.029 35.85 0.050 154.97
EM binarization + EM re-labeling 30.05 0.001 36.42 0.003 158.89
(4-way splitting w/ 90% merging)
EM re-alignment 29.18 0.411 35.52 0.296 75.1
Size prior EM re-alignment 29.37 0.165 35.96 0.050 110.4
Size prior EM re-alignment + 30.6 0.001 36.73 0.002 222.0
EM binarization + EM re-labeling
Better re-structuring should in turn lead to better re-labeling, and this should increase
the performance of the overall MT pipeline.
To test this hypothesis we pre-processed alignments using the modified re-
alignment procedure described in Section 5.4. We next used those alignments to obtain
new binarizations of trees following the EM binarization method described in Sec-
tion 3.4. Finally, re-labeling was done on these binarized trees using 4-way splitting with
90% merging, as described in Section 4. The final trees, along with the alignments used
to get these trees and of course the parallel Chinese sentences, were then used as the
training data of our MT pipeline. The results of this combined experiment are shown
in Table 7 along with the other experiments from this article, for ease of comparison.
As can be seen from this table, the progressive improvement of training data leads
to an overall improvement in MT system performance. As noted previously, there
tends to be a correspondence between the number of unique rules extracted and MT
performance. The final combined experiment has the greatest number of unique rules.
The improvements made to syntax and alignment described in this article unify these
two independently determined annotations over the bitext, and this thus leads to more
admissible nodes and a greater ability to extract rules. Such a unification can lead to
over-generalization, as rules lacking sufficient context may be extracted and used to the
system?s detriment. This is why a re-labeling technique is also needed, to ensure that
sufficient rule specificity is maintained.
7. Conclusion
This article considered three modifications to MT training data that encourage im-
proved performance in a state-of-the-art syntactic MT system. The improvements
changed syntactic structure, altered bracket labels, and unified alignment across sen-
tences, and when combined led to an improvement of 1.48 BLEU points over a strong
baseline in Chinese?English translation. The techniques herein described require only
274
Wang et al Re-structuring, Re-labeling, and Re-aligning
the training data used in the original MT task and are thus applicable to a string-to-tree
MT system for any language pair.
Acknowledgments
Some of the results in this article appear
in Wang, Knight, and Marcu (2007) and
May and Knight (2007). The linguistically
motivated re-labeling method is due to
Steve DeNeefe, Kevin Knight, and David
Chiang. The authors also wish to thank
Slav Petrov for his help with the Berkeley
parser, and the anonymous reviewers
for their helpful comments. This research
was supported under DARPA Contract
No. HR0011-06-C-0022, BBN subcontract
9500008412.
References
Alshawi, Hiyan, Srinivas Bangalore,
and Shona Douglas. 1998. Automatic
acquisition of hierarchical transduction
models for machine translation. In
Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
(ACL) and 17th International Conference on
Computational Linguistics (COLING) 1998,
pages 41?47, Montre?al.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?312.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st North American Chapter of the
Association for Computational Linguistics
Conference (NAACL), pages 132?139,
Seattle, WA.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Cohn, Trevor and Phil Blunsom. 2009.
A Bayesian model of syntax-directed
tree to string grammar induction. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 352?361, Singapore.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 16?23, Madrid.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
EM algorithm. Journal of the Royal
Statistical Society, 39(1):1?38.
DeNeefe, Steve, Kevin Knight, Wei Wang,
and Daniel Marcu. 2007. What can
syntax-based MT learn from phrase-based
MT? In Proceedings of EMNLP?CoNLL-2007,
pages 755?763, Prague.
DeNero, John, Alexandre Bouchard-Co?te?,
and Dan Klein. 2008. Sampling alignment
structure under a Bayesian translation
model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 314?323,
Honolulu, HI.
Gale, William A. and Geoffrey Sampson.
1996. Good-Turing frequency estimation
without tears. Journal of Quantitative
Linguistics, 2(3):217?237.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic translation models.
In Proceedings of the 21st International
Conference on Computational Linguistics
(COLING) and 44th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 961?968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
Human Language Technology Conference and
the North American Association for
Computational Linguistics (HLT-NAACL),
pages 273?280, Boston, MA.
Good, Irving J. 1953. The population
frequencies of species and the estimation
of population parameters. Biometrika,
40(3):237?264.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573?605.
Huang, Bryant and Kevin Knight. 2006.
Relabeling syntax trees to improve
syntax-based machine translation
accuracy. In Proceedings of the main
conference on Human Language Technology
Conference of the North American Chapter
of the Association of Computational
Linguistics (NAACL-HLT), pages 240?247,
New York, NY.
Johnson, Mark. 1998a. The DOP estimation
method is biased and inconsistent.
Computational Linguistics, 28(1):71?76.
Johnson, Mark. 1998b. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Klein, Dan and Chris Manning. 2003.
Accurate unlexicalized parsing. In
275
Computational Linguistics Volume 36, Number 2
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 423?430, Sapporo.
Kneser, Reinhard and Hermann Ney.
1995. Improved backing-off for
m-gram language modeling. In
Proceedings of the International Conference
on Acoustics, Speech, and Signal
Processing (ICASSP) 1995, pages 181?184,
Detroit, MI.
Knight, Kevin and Jonathan Graehl. 2005.
An overview of probabilistic tree
transducers for natural language
processing. In Proceedings of the Sixth
International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing), pages 1?25,
Mexico City.
Knight, Kevin, Jonathan Graehl, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391?427.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 388?395,
Barcelona.
Lari, Karim and Steve Young. 1990. The
estimation of stochastic context-free
grammars using the inside-outside
algorithm. Computer Speech and Language,
4:35?56.
Marcu, Daniel and William Wong. 2002.
A phrase-based, joint probability model
for statistical machine translation.
In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 133?139,
Philadelphia, PA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
May, Jonathan and Kevin Knight. 2007.
Syntactic re-alignment models for
machine translation. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP?CoNLL), pages 360?368,
Prague.
Melamed, I. Dan, Giorgio Satta, and
Benjamin Wellington. 2004. Generalized
multitext grammars. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 662?669, Barcelona.
Mi, Haitao and Liang Huang. 2008.
Forest-based translation rule extraction.
In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 206?214,
Honolulu, HI.
Och, Franz and Hermann Ney. 2004. The
alignment template approach to statistical
machine translation. Computational
Linguistics, 30(4):417?449.
Och, Franz Josef. 2003. Minimum error rate
training for machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo.
Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning
accurate, compact, and interpretable
tree annotation. In Proceedings of the
21st International Conference on
Computational Linguistics (COLING) and
44th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 433?440, Sydney.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit.
In Proceedings of the 7th International
Conference on Spoken Language
Processing (ICSLP) 2002, pages 901?904,
Denver, CO.
Talbot, David and Miles Osborne. 2007.
Randomised language modelling for
statistical machine translation. In
Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics
(ACL), pages 512?519, Prague.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation. In
Proceedings of the International Conference on
Computational Linguistics (COLING) 1996,
pages 836?841, Copenhagen.
Wang, Wei, Kevin Knight, and Daniel
Marcu. 2007. Binarizing syntax trees
to improve syntax-based machine
translation accuracy. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP?CoNLL), pages 746?754,
Prague.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
Yamada, Kenji and Kevin Knight. 2001.
A syntax-based statistical translation
model. In Proceedings of the 39th
Annual Meeting of the Association for
276
Wang et al Re-structuring, Re-labeling, and Re-aligning
Computational Linguistics (ACL),
pages 523?530, Toulouse.
Yamada, Kenji and Kevin Knight. 2002.
A decoder for syntax-based statistical
MT. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 303?310,
Philadelphia, PA.
Zhang, Hao, Liang Huang, Daniel Gildea,
and Kevin Knight. 2006. Synchronous
binarization for machine translation.
In Proceedings of the main conference
on Human Language Technology
Conference of the North American Chapter
of the Association of Computational
Linguistics (HLT-NAACL), pages 256?263,
New York, NY.
Zollmann, Andreas and Khalil Sima?an.
2005. A consistent and efficient estimator
for data-oriented parsing. Journal of
Automata, Languages and Combinatorics,
10(2/3):367?388.
277

Squibs
Does GIZA++ Make Search Errors?
Sujith Ravi?
University of Southern California
Kevin Knight??
University of Southern California
Word alignment is a critical procedure within statistical machine translation (SMT). Brown
et al (1993) have provided the most popular word alignment algorithm to date, one that has
been implemented in the GIZA (Al-Onaizan et al 1999) and GIZA++ (Och and Ney 2003)
software and adopted by nearly every SMT project. In this article, we investigate whether this
algorithm makes search errors when it computes Viterbi alignments, that is, whether it returns
alignments that are sub-optimal according to a trained model.
1. Background
Word alignment is the problem of annotating a bilingual text with links connecting
words that have the same meanings. Brown et al (1993) align an English/French sen-
tence pair by positing a probabilistic model by which an English sentence is translated
into French.1 The model provides a set of non-deterministic choices. When a particular
sequence of choices is applied to an English input sentence e1...el, the result is a partic-
ular French output sentence f1...fm. In the Brown et al models, a decision sequence also
implies a specific word alignment vector a1...am. We say aj = i when French word fj was
produced by English word ei during the translation. Here is a sample sentence pair (e, f)
and word alignment a:
e: NULL0 Mary1 did2 not3 slap4 the5 green6 witch7
f : Mary1 no2 dio?3 una4 bofetada5 a6 la7 bruja8 verde9
a: [ 1 3 4 5 5 0 5 7 6 ]
Notice that the English sentence contains a special NULL word (e0) that generates
?spurious? target words (in this case, a6). The Brown et al (1993) models are many-
to-one, meaning that each English word can produce several French children, but each
? Information Sciences Institute, University of Southern California, 4676 Admiralty Way, Marina del Rey,
CA 90292. E-mail: sravi@isi.edu.
?? Information Sciences Institute, University of Southern California, 4676 Admiralty Way, Marina del Rey,
CA 90292. E-mail: knight@isi.edu.
1 We follow standard convention in using ?English? and ?French? simply as shorthand for ?source? and
?target?. In fact, we use English/Spanish examples in this article.
Submission received: 16 December 2009; accepted for publication: 7 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
French word has only one English parent. This is why we can represent an alignment as
a vector a1...am. There are (l + 1)m ways to align (e, f). For Brown et al the goal of word
alignment is to find the alignment a that is most likely, given a sentence pair (e, f):
argmaxa P(a|e, f) = argmaxaP(a, f|e) (1)
2. IBM Model 3
Supplied with a formula for P(a|e, f), we can search through the (l + 1)m alignments
for the highest-scoring one. Brown et al (1993) come up with such a formula by first
positing this generative story (IBM Model 3):
Given an English sentence e1...el:
1. Choose a fertility ?i for each English word ei, according to the distribution
n(?i|ei).
2. Let m? =
?
i=1...l ?i.
3. Choose a number ?0 of ?spurious? French words, by doing the following
m? times: with probability p1, increment ?0 by one.
4. Let m = m? + ?0.
5. Choose a French translation ?ik for each English word (including e0) and
fertility value, according to the distribution t(?ik|ei).
6. Choose a French position j for each ?ik (i > 0), according to d(j|i, l,m). If the
same j is chosen twice, fail the procedure.
7. Place each of the ?0 spuriously generated words, one by one, into vacant
positions in the English string, according to uniform probability.
8. Output the French string f1...fm, where fj is the French word ?ik that was
placed into position j in Step 6 or 7.
9. Output alignment a1...am, where aj is the English position i from that
same ?ik.
Different decision sequences can result in the same outputs f and a. With this in
mind, Brown et al provide the following formula:
P(a, f|e) =
m
?
j=1
t( fj|eaj ) ?
l
?
i=1
n(?i|ei) ?
m
?
aj =0,j=1
d( j|aj, l,m)
?
l
?
i=0
?i! ? 1?0!
?
(
m ? ?0
?0
)
? p?01 ? p
m?2?0
0 (2)
Note that terms ?0...?l are only shorthand, as their values are completely deter-
mined by the alignment a1...am.
296
Ravi and Knight Does GIZA++ Make Search Errors?
3. Finding the Viterbi Alignment
We assume that probability tables n, t, d, and p have already been learned from data,
using the EM method described by Brown et al (1993). We are concerned solely with the
problem of then finding the best alignment for a given sentence pair (e, f) as described
in Equation 1. Brown et al were unable to discover a polynomial time algorithm for
this problem, which was in fact subsequently shown to be NP-complete (Udupa and
Maji 2006). Brown et al therefore devise a hill-climbing algorithm. This algorithm starts
with a reasonably good alignment (Viterbi IBM Model 2, computable in quadratic time),
after which it greedily executes small changes to the alignment structure, gradually
increasing P(a, f|e). The small changes consist of moves, in which the value of some
aj is changed, and swaps, in which a pair aj and ak exchange values. At each step in
the greedy search, all possible moves and swaps are considered, and the one which
increases P(a, f|e) the most is executed. When P(a, f|e) can no longer be improved, the
search halts.2
Our question is whether this hill-climbing algorithm, as implemented in GIZA++,
makes search errors when it tries to locate IBM Model 3 Viterbi alignments. To answer
this, we built a slow but optimal IBM Model 3 aligner, by casting the problem in the
integer linear programming (ILP) framework. Given a sentence pair (e1...el, f1...fm), we
set up the following variables:
 Binary link variables linkij. We have one such variable for each pair of
English and French tokens. If linkij = 1, then there is an alignment link
between ei and fj. The English NULL word is represented by i = 0.
 Binary fertility variables fertik. We have one variable for each English token
paired with an integer fertility value 0..9. If fertik = 1, then token ei has
fertility k.
To ensure that values we assign to variables are legal and self-consistent, we in-
troduce several constraints. First, we require that for each j, all linkxj values sum to
one. This means each French token has exactly one alignment link, as required by IBM
Model 3. Second, we require that for each i, all fertix values sum to one, so that each
English token has a unique fertility. Third, we require that link and fertility variable
assignments be consistent with one another: for each i, the sum of linkix variables equals
1 ? ferti1 + 2 ? ferti2 + . . . + 9 ? ferti9.3
We then define an objective function whose minimization corresponds to finding
the Viterbi IBM Model 3 alignment. Figure 1 presents the components of the objective
function, alongside counterparts from the P(a, f|e) formula previously given. Note that
the coefficients for the objective function are derived from the already-learned probabil-
ity tables n, d, t, and p.
For each sentence pair in our test set, we create and solve an ILP problem. Figure 2
demonstrates this for a simple example. The reader can extract the optimal alignment
from the alignment variables chosen in the ILP solution.
2 Brown et al (1993) describe a variation called pegging, which carries out multiple additional greedy
hill-climbs, each with a different IBM Model 2 Viterbi link fixed (pegged) for the duration of the
hill-climb. In practice, pegging is slow, and the vast majority of GIZA++ users do not employ it.
3 The default configuration of GIZA++ includes this same fertility cap of 9, though the Brown et al (1993)
description does not.
297
Computational Linguistics Volume 36, Number 3
Figure 1
Objective function and constraints (left) for the ILP formulation that encodes the problem of
selecting the Viterbi alignment for a sentence pair under IBM Model 3. Components of the ILP
objective function are paired with counterparts (right) from the Model 3?s P(a, f|e) formula.
4. Experiments
For Chinese/English experiments, we run GIZA++ training on 101,880 sentence pairs.
We evaluate Viterbi alignments on a smaller test set of 1,880 sentence pairs with manual
alignments. For Arabic/English, we train on 300,000 sentence pairs and test on 2,000.
In tests, we compare GIZA++ Viterbi alignments (based on greedy hill-climbing) with
optimal ILP alignments. We use CPLEX to solve our ILP problems.
298
Ravi and Knight Does GIZA++ Make Search Errors?
Figure 2
Illustration of ILP-based optimal alignment of a single sentence pair. Already-trained log
probabilities are shown at the top of the figure. In the middle is a schematic of variables
introduced for the English/Spanish sentence pair seatbelts / los cinturones de seguridad.
At the bottom is the ILP formulation and its solution.
Figure 3 compares the results from GIZA++ alignment with optimal ILP alignment
for different language pairs and alignment directions, and for unions of uni-directional
alignments. We measure the rate at which GIZA++ makes search errors, and we com-
pute alignment F-scores for the various testing conditions. We conclude that although
299
Computational Linguistics Volume 36, Number 3
Figure 3
Comparison of GIZA++ Viterbi alignments (based on greedy hill-climbing) with optimal ILP
alignments for different language pairs and alignment directions in terms of alignment quality
(F-score). The figure also shows the alignment F-scores for UNION alignments that combine
alignment links from both directions. The fourth column shows the percentage of sentences on
which GIZA++ makes search errors in comparison to optimal ILP alignments.
GIZA++ makes search errors on 5?15% of sentence pairs, these errors do not contribute
to an overall loss in alignment task accuracy, as measured by F-score.
Focusing on sentence pairs where GIZA++ makes a search error, we plot the av-
erage difference in log model scores between GIZA++ and ILP Viterbi alignments in
Figure 4. We notice a positive correlation between sentence length and the search error
Figure 4
Average difference in log model scores between GIZA++ and ILP alignments at different English
sentence lengths for English/Chinese alignment. Points in the plot that appear to be on the
x-axis actually lie just above it.
300
Ravi and Knight Does GIZA++ Make Search Errors?
Figure 5
Average time (msec) taken by the ILP aligner at different English sentence lengths for
English/Chinese alignment. The experiments were run on a single machine with a 64-bit,
2.4 GHz AMD Opteron 850 processor.
gap between GIZA++ and ILP scores. As we move to longer sentences, the alignment
procedure becomes harder and GIZA++ makes more errors. Finally, Figure 5 plots the
time taken for ILP alignment at different sentence lengths showing a positive correlation
as well.
5. Discussion
We have determined that GIZA++ makes few search errors, despite the heuristic nature
of the algorithm. These search errors do not materially affect overall alignment accuracy.
In practice, this means that researchers should not spend time optimizing this particular
aspect of SMT systems.
Search errors can occur in many areas of SMT. The area that has received the most
attention is runtime decoding/translation. For example, Germann et al (2001) devise an
optimal ILP decoder to identify types of search errors made by other decoders. A second
area (this article) is finding Viterbi alignments, given a set of alignment parameter
values. A third area is actually learning those parameter values. Brown et al?s (1993)
EM learning algorithm aims to optimize the probability of the French side of the parallel
corpus given the English side. For Model 3 and above, Brown et al collect parameter
counts over subsets of alignments, instead of over all alignments. These subsets, like
Viterbi alignments, are generated heuristically, and it may be that true n-best lists of
301
Computational Linguistics Volume 36, Number 3
alignments would yield better counts and better overall parameter values. Of course,
even if we were able to collect accurate counts, EM is not guaranteed to find a global
optimum, which provides further opportunity for search errors. We leave the problem
of search errors in alignment training to future study.
Acknowledgments
This work was supported by NSF grant
0904684 and DARPA GALE Contract
Number HR0011-06-C-0022.
References
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Franz
Josef Och Dan Melamed, David Purdy,
Noah Smith, and David Yarowsky. 1999.
Statistical machine translation. Technical
report, Johns Hopkins University.
Brown, Peter, Vincent Della Pietra,
Stephen Della Pietra, and Robert Mercer.
1993. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 39th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 228?235, Toulouse.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Udupa, Raghavendra and Hemanta K. Maji.
2006. Computational complexity of
statistical machine translation. In
Proceedings of the Conference of the
European Chapter of the Association of
Computational Linguistics (EACL), pages
25?32, Trento.
302
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118?126,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Syntactic Alignment with Inversion Transduction Grammars
Adam Pauls Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
David Chiang Kevin Knight
Information Sciences Institute
University of Southern California
{chiang,knight}@isi.edu
Abstract
Syntactic machine translation systems cur-
rently use word alignments to infer syntactic
correspondences between the source and tar-
get languages. Instead, we propose an un-
supervised ITG alignment model that directly
aligns syntactic structures. Our model aligns
spans in a source sentence to nodes in a target
parse tree. We show that our model produces
syntactically consistent analyses where possi-
ble, while being robust in the face of syntactic
divergence. Alignment quality and end-to-end
translation experiments demonstrate that this
consistency yields higher quality alignments
than our baseline.
1 Introduction
Syntactic machine translation has advanced signif-
icantly in recent years, and multiple variants cur-
rently achieve state-of-the-art translation quality.
Many of these systems exploit linguistically-derived
syntactic information either on the target side (Gal-
ley et al, 2006), the source side (Huang et al, 2006),
or both (Liu et al, 2009). Still others induce their
syntax from the data (Chiang, 2005). Despite differ-
ences in detail, the vast majority of syntactic meth-
ods share a critical dependence on word alignments.
In particular, they infer syntactic correspondences
between the source and target languages through
word alignment patterns, sometimes in combination
with constraints from parser outputs.
However, word alignments are not perfect indi-
cators of syntactic alignment, and syntactic systems
are very sensitive to word alignment behavior. Even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, while
unaligned words can result in an exponentially large
set of extractable rules to choose from. Researchers
have worked to incorporate syntactic information
into word alignments, resulting in improvements to
both alignment quality (Cherry and Lin, 2006; DeN-
ero and Klein, 2007), and translation quality (May
and Knight, 2007; Fossum et al, 2008).
In this paper, we remove the dependence on word
alignments and instead directly model the syntactic
correspondences in the data, in a manner broadly
similar to Yamada and Knight (2001). In particu-
lar, we propose an unsupervised model that aligns
nodes of a parse tree (or forest) in one language to
spans of a sentence in another. Our model is an in-
stance of the inversion transduction grammar (ITG)
formalism (Wu, 1997), constrained in such a way
that one side of the synchronous derivation respects
a syntactic parse. Our model is best suited to sys-
tems which use source- or target-side trees only.
The design of our model is such that, for divergent
structures, a structurally integrated backoff to flatter
word-level (or null) analyses is available. There-
fore, our model is empirically robust to the case
where syntactic divergence between languages pre-
vents syntactically accurate ITG derivations.
We show that, with appropriate pruning, our
model can be efficiently trained on large parallel cor-
pora. When compared to standard word-alignment-
backed baselines, our model produces more con-
sistent analyses of parallel sentences, leading to
high-count, high-quality transfer rules. End-to-
end translation experiments demonstrate that these
higher quality rules improve translation quality by
1.0 BLEU over a word-alignment-backed baseline.
2 Syntactic Rule Extraction
Our model is intended for use in syntactic transla-
tion models which make use of syntactic parses on
either the target (Galley et al, 2006) or source side
(Huang et al, 2006; Liu et al, 2006). Our model?s
118
SNP
DT* NN NN
VP
VBZ
ADVP
RB VBN
the trade surplus has drastically fallen
??
??
???
 ??
 ?
trade
surplus
drastically
fall
(past)
Figure 1: A single incorrect alignment removes an ex-
tractable node, and hence several desirable rules. We
represent correct extractable nodes in bold, spurious ex-
tractable nodes with a *, and incorrectly blocked ex-
tractable nodes in bold strikethrough.
chief purpose is to align nodes in the syntactic parse
in one language to spans in the other ? an alignment
we will refer to as a ?syntactic? alignment. These
alignments are employed by standard syntactic rule
extraction algorithms, for example, the GHKM al-
gorithm of Galley et al (2004). Following that work,
we will assume parses are present in the target lan-
guage, though our model applies in either direction.
Currently, although syntactic systems make use of
syntactic alignments, these alignments must be in-
duced indirectly from word-level alignments. Pre-
vious work has discussed at length the poor interac-
tion of word-alignments with syntactic rule extrac-
tion (DeNero and Klein, 2007; Fossum et al, 2008).
For completeness, we provide a brief example of this
interaction, but for a more detailed discussion we re-
fer the reader to these presentations.
2.1 Interaction with Word Alignments
Syntactic systems begin rule extraction by first iden-
tifying, for each node in the target parse tree, a
span of the foreign sentence which (1) contains ev-
ery source word that aligns to a target word in the
yield of the node and (2) contains no source words
that align outside that yield. Only nodes for which
a non-empty span satisfying (1) and (2) exists may
form the root or leaf of a translation rule; for that
reason, we will refer to these nodes as extractable
nodes.
Since extractable nodes are inferred based on
word alignments, spurious word alignments can rule
out otherwise desirable extraction points. For exam-
ple, consider the alignment in Figure 1. This align-
ment, produced by GIZA++ (Och and Ney, 2003),
contains 4 correct alignments (the filled circles),
but incorrectly aligns the to the Chinese past tense
marker ? (the hollow circle). This mistaken align-
ment produces the incorrect rule (DT ? the ; ?),
and also blocks the extraction of (VBN ? fallen ;
???).
More high-level syntactic transfer rules are also
ruled out, for example, the ?the insertion rule? (NP
? the NN1 NN2 ; NN1 NN2) and the high-level (S
? NP1 VP2 ; NP1 VP2).
3 A Syntactic Alignment Model
The most common approach to avoiding these prob-
lems is to inject knowledge about syntactic con-
straints into a word alignment model (Cherry and
Lin, 2006; DeNero and Klein, 2007; Fossum et al,
2008).1 While syntactically aware, these models re-
main limited by the word alignment models that un-
derly them.
Here, we describe a model which directly infers
alignments of nodes in the target-language parse tree
to spans of the source sentence. Formally, our model
is an instance of a Synchronous Context-Free Gram-
mar (see Chiang (2004) for a review), or SCFG,
which generates an English (target) parse tree T and
foreign (source) sentence f given a target sentence e.
The generative process underlying this model pro-
duces a derivation d of SCFG rules, from which T
and f can be read off; because we condition on e,
the derivations produce e with probability 1. This
model places a distribution over T and f given by
p(T, f | e) =
?
d
p(d | e) =
?
d
?
r?d
p(r | e)
where the sum is over derivations d which yield T
and f . The SCFG rules r come from one of 4 types,
pictured in Table 1. In general, because our model
can generate English trees, it permits inference over
forests. Although we will restrict ourselves to a sin-
gle parse tree for our experiments, in this section, we
discuss the more general case.
1One notable exception is May and Knight (2007), who pro-
duces syntactic alignments using syntactic rules derived from
word-aligned data.
119
Rule Type Root English Foreign Example Instantiation
TERMINAL E e ft FOUR ? four ;?
UNARY A B fl B fr CD ? FOUR ;  FOUR ?
BINARYMONO A B C fl B fm C fr NP ? NN NN ;  NN ? NN 
BINARYINV A B C fl C fm B fr PP ? IN NP ;? NP  IN 
Table 1: Types of rules present in the SCFG describing our model, along with some sample instantiations of each type.
Empty word sequences f have been explicitly marked with an .
The first rule type is the TERMINAL production,
which rewrites a terminal symbol2 E as its En-
glish word e and a (possibly empty) sequence of
foreign words ft. Generally speaking, the majority
of foreign words are generated using this rule. It
is only when a straightforward word-to-word corre-
spondence cannot be found that our model resorts to
generating foreign words elsewhere.
We can also rewrite a non-terminal symbol A us-
ing a UNARY production, which on the English side
produces a single symbol B, and on the foreign side
produces the symbol B, with sequences of words fl
to its left and fr to its right.
Finally, there are two binary productions: BINA-
RYMONO rewrites A with two non-terminals B and
C on the English side, and the same non-terminals
B and C in monotonic order on the foreign side,
with sequences of words fl, fr, and fm to the left,
right, and the middle. BINARYINV inverts the or-
der in which the non-terminals B and C are written
on the source side, allowing our model to capture a
large subset of possible reorderings (Wu, 1997).
Derivations from this model have two key prop-
erties: first, the English side of a derivation is con-
strained to form a valid constituency parse, as is re-
quired in a syntax system with target-side syntax;
and second, for each parse node in the English pro-
jection, there is exactly one (possibly empty) con-
tiguous span of the foreign side which was gener-
ated from that non-terminal or one of its descen-
dants. Identifying extractable nodes from a deriva-
tion is thus trivial: any node aligned to a non-empty
foreign span is extractable.
In Figure 2, we show a sample sentence pair frag-
2For notational convenience, we imagine that for each par-
ticular English word e, there is a special preterminal symbol E
which produces it. These symbols E act like any other non-
terminal in the grammar with respect to the parameterization in
Section 3.1. To denote standard non-terminals, we will use A,
B, and C.
PP[0,4]
IN[3,4]
NP[1,3]
DT[1,1]
NNS[1,3]
the[1,1]
elections[1,3]
? ??
?
??
at parliament election
before
before[3,4]
PP
NP IN
NNSDT
0 1 2 3 4
?
PP ? IN NP ; ? NP IN
NP ? DT NNS ; DT NNS
IN ? before ; before
before ? before ; ??
DT ? the ; the
the ? the ; !
NNS ? elections ; elections
elections ? elections ; ?? ??
Figure 2: Top: A synchronous derivation of a small sen-
tence pair fragment under our model. The English pro-
jection of the derivation represents a valid constituency
parse, while the foreign projection is less constrained.
We connect each foreign terminal with a dashed line to
the node in the English side of the synchronous deriva-
tion at which it is generated. The foreign span assigned
to each English node is indicated with indices. All nodes
with non-empty spans, shown in boldface, are extractable
nodes. Bottom: The SCFG rules used in the derivation.
ment as generated by our model. Our model cor-
rectly identifies that the English the aligns to nothing
on the foreign side. Our model also effectively cap-
tures the one-to-many alignment3 of elections to ?
3While our model does not explicitly produce many-to-one
alignments, many-to-one rules can be discovered via rule com-
position (Galley et al, 2006).
120
? ??. Finally, our model correctly analyzes the
Chinese circumposition ? . . .?? (before . . . ). In
this construction, ?? carries the meaning of ?be-
fore?, and thus correctly aligns to before, while ?
functions as a generic preposition, which our model
handles by attaching it to the PP. This analysis per-
mits the extraction of the general rule (PP ? IN1
NP2 ;? NP2 IN1), and the more lexicalized (PP?
before NP ;? NP??).
3.1 Parameterization
In principle, our model could have one parameter for
each instantiation r of a rule type. This model would
have an unmanageable number of parameters, pro-
ducing both computational and modeling issues ? it
is well known that unsupervised models with large
numbers of parameters are prone to degenerate anal-
yses of the data (DeNero et al, 2006). One solution
might be to apply an informed prior with a compu-
tationally tractable inference procedure (e.g. Cohn
and Blunsom (2009) or Liu and Gildea (2009)). We
opt here for the simpler, statistically more robust so-
lution of making independence assumptions to keep
the number of parameters at a reasonable level.
Concretely, we define the probability of the BI-
NARYMONO rule,4
p(r = A? B C; fl B fm C fr|A, eA)
which conditions on the root of the rule A and the
English yield eA, as
pg(A? B C | A, eA) ? pinv(I | B,C)?
pleft(fl | A, eA)?pmid(fm | A, eA)?pright(fr | A, eA)
In words, we assume that the rule probability de-
composes into a monolingual PCFG grammar prob-
ability pg, an inversion probability pinv, and a proba-
bility of left, middle, and right word sequences pleft,
pmid, and pright.5 Because we condition on e, the
monolingual grammar probability pg must form a
distribution which produces e with probability 1.6
4In the text, we only describe the factorization for the BI-
NARYMONO rule. For a parameterization of all rules, we refer
the reader to Table 2.
5All parameters in our model are multinomial distributions.
6A simple case of such a distribution is one which places all
of its mass on a single tree. More complex distributions can be
obtained by conditioning an arbitrary PCFG on e (Goodman,
1998).
We further assume that the probability of produc-
ing a foreign word sequence fl decomposes as:
pleft(fl | A, eA) = pl(|fl| = m | A)
m?
j=1
p(fj | A, eA)
where m is the length of the sequence fl. The pa-
rameter pl is a left length distribution. The prob-
abilities pmid, pright, decompose in the same way,
except substituting a separate length distribution pm
and pr for pl. For the TERMINAL rule, we emit ft
with a similarly decomposed distribution pterm us-
ing length distribution pw.
We define the probability of generating a foreign
word fj as
p(fj | A, eA) =
?
i?eA
1
| eA |
pt(fj | ei)
with i ? eA denoting an index ranging over the in-
dices of the English words contained in eA. The
reader may recognize the above expressions as the
probability assigned by IBM Model 1 (Brown et al,
1993) of generating the words fl given the words eA,
with one important difference ? the length m of the
foreign sentence is often not modeled, so the term
pl(|fl| = m | A) is set to a constant and ignored.
Parameterizing this length allows our model to ef-
fectively control the number of words produced at
different levels of the derivation.
It is worth noting how each parameter affects the
model?s behavior. The pt distribution is a standard
?translation? table, familiar from the IBM Models.
The pinv distribution is a ?distortion? parameter, and
models the likelihood of inverting non-terminals B
and C. This parameter can capture, for example,
the high likelihood that prepositions IN and noun
phrases NP often invert in Chinese due to its use
of postpositions. The non-terminal length distribu-
tions pl, pm, and pr model the probability of ?back-
ing off? and emitting foreign words at non-terminals
when a more refined analysis cannot be found. If
these parameters place high mass on 0 length word
sequences, this heavily penalizes this backoff be-
haviour. For the TERMINAL rule, the length distri-
bution pw parameterizes the number of words pro-
duced for a particular English word e, functioning
similarly to the ?fertilities? employed by IBM Mod-
els 3 and 4 (Brown et al, 1993). This allows us
121
to model, for example, the tendency of English de-
terminers the and a translate to nothing in the Chi-
nese, and of English names to align to multiple Chi-
nese words. In general, we expect an English word
to usually align to one Chinese word, and so we
place a weak Dirichlet prior on on the pe distribution
which puts extra mass on 1-length word sequences.
This is helpful for avoiding the ?garbage collection?
(Moore, 2004) problem for rare words.
3.2 Exploiting Non-Terminal Labels
There are often foreign words that do not correspond
well to any English word, which our model must
also handle. We elected for a simple augmentation
to our model to account for these words. When gen-
erating foreign word sequences f at a non-terminal
(i.e. via the UNARY or BINARY productions), we
also allow for the production of foreign words from
the non-terminal symbol A. We modify p(fj | eA)
from the previous section to allow production of fj
directly from the non-terminal7 A:
p(fj | eA) = pnt ? p(fj | A)
+ (1? pnt) ?
?
i?eA
1
|eA|
pt(fj | ei)
where pnt is a global binomial parameter which con-
trols how often such alignments are made.
This necessitates the inclusion of parameters like
pt(? | NP) into our translation table. Generally,
these parameters do not contain much information,
but rather function like a traditional NULL rooted
at some position in the tree. However, in some
cases, the particular annotation used by the Penn
Treebank (Marcus et al, 1993) (and hence most
parsers) allows for some interesting parameters to
be learned. For example, we found that our aligner
often matched the Chinese word ?, which marks
the past tense (among other things), to the preter-
minals VBD and VBN, which denote the English
simple past and perfect tense. Additionally, Chinese
measure words like ? and ? often align to the CD
(numeral) preterminal. These generalizations can be
quite useful ? where a particular number might pre-
dict a measure word quite poorly, the generalization
that measure words co-occur with the CD tag is very
robust.
7For terminal symbols E, this production is not possible.
3.3 Membership in ITG
The generative process which describes our model
contains a class of grammars larger than the com-
putationally efficient class of ITG grammars. For-
tunately, the parameterization described above not
only reduces the number of parameters to a man-
ageable level, but also introduces independence as-
sumptions which permit synchronous binarization
(Zhang et al, 2006) of our grammar. Any SCFG that
can be synchronously binarized is an ITG, meaning
that our parameterization permits efficient inference
algorithms which we will make use of in the next
section. Although several binarizations are possi-
ble, we give one such binarization and its associated
probabilities in Table 2.
3.4 Robustness to Syntactic Divergence
Generally speaking, ITG grammars have proven
more useful without the monolingual syntactic con-
straints imposed by a target parse tree. When deriva-
tions are restricted to respect a target-side parse tree,
many desirable alignments are ruled out when the
syntax of the two languages diverges, and align-
ment quality drops precipitously (Zhang and Gildea,
2004), though attempts have been made to address
this issue (Gildea, 2003).
Our model is designed to degrade gracefully in
the case of syntactic divergence. Because it can pro-
duce foreign words at any level of the derivation,
our model can effectively back off to a variant of
Model 1 in the case where an ITG derivation that
both respects the target parse tree and the desired
word-level alignments cannot be found.
For example, consider the sentence pair fragment
in Figure 3. It is not possible to produce an ITG
derivation of this fragment that both respects the
English tree and also aligns all foreign words to
their obvious English counterparts. Our model han-
dles this case by attaching the troublesome ?? at
the uppermost VP. This analysis captures 3 of the
4 word-level correspondences, and also permits ex-
traction of abstract rules like (S? NP VP ; NP VP)
and (NP? the NN ; NN).
Unfortunately, this analysis leaves the English
word tomorrow with an empty foreign span, permit-
ting extraction of the incorrect translation (VP ?
announced tomorrow ; ??), among others. Our
122
Rule Type Root English side Foreign side Probability
TERMINAL E e wt pterm(wt | E)
UNARY A Bu wl Bu pg(A ? B | A)pleft(wl | A, eA)
Bu B B wr pright(wr | A, eA)
BINARY A A1 wl A1 pleft(wl | A, eA)
A1 B C1 B C1 pg(A ? B C | A)pinv(I=false | B,C)
A1 B C1 C1 B pg(A ? B C | A)pinv(I=true | B,C)
C1 C2 fm C2 pmid(fm | A, eA)
C2 C C fr pright(fr | A, eA)
Table 2: A synchronous binarization of the SCFG describing our model.
S[0,4]
NP[3,4]
DT[3,3] NN[3,4]
VP[0,3]
VB[2,2]
VP[2,3]
VBN[2,3]
NN[3,3]
VP[2,3]
MD[1,2]
?? ? ?? ??
listannouncewilltomorrow0 1 2 3 4
the[3,3] list[3,4]
be[2,2]
announced[2,3] tomorrow[3,3]
will[1,2]
(a)
Figure 3: The graceful degradation of our model in the
face of syntactic divergence. It is not possible to align
all foreign words with their obvious English counterparts
with an ITG derivation. Instead, our model analyzes as
much as possible, but must resort to emitting ?? high
in the tree.
point here is not that our model?s analysis is ?cor-
rect?, but ?good enough? without resorting to more
computationally complicated models. In general,
our model follows an ?extract as much as possi-
ble? approach. We hypothesize that this approach
will capture important syntactic generalizations, but
it also risks including low-quality rules. It is an em-
pirical question whether this approach is effective,
and we investigate this issue further in Section 5.3.
There are possibilities for improving our model?s
treatment of syntactic divergence. One option is
to allow the model to select trees which are more
consistent with the alignment (Burkett et al, 2010),
which our model can do since it permits efficient in-
ference over forests. The second is to modify the
generative process slightly, perhaps by including the
?clone? operator of Gildea (2003).
4 Learning and Inference
4.1 Parameter Estimation
The parameters of our model can be efficiently
estimated in an unsupervised fashion using the
Expectation-Maximization (EM) algorithm. The E-
step requires the computation of expected counts un-
der our model for each multinomial parameter. We
omit the details of obtaining expected counts for
each distribution, since they can be obtained using
simple arithmetic from a single quantity, namely, the
expected count of a particular instantiation of a syn-
chronous rule r. This expectation is a standard quan-
tity that can be computed in O(n6) time using the
bitext Inside-Outside dynamic program (Wu, 1997).
4.2 Dynamic Program Pruning
While our model permits O(n6) inference over a
forest of English trees, inference over a full forest
would be very slow, and so we fix a single n-ary En-
glish tree obtained from a monolingual parser. How-
ever, it is worth noting that the English side of the
ITG derivation is not completely fixed. Where our
English trees are more than binary branching, we
permit any binarization in our dynamic program.
For efficiency, we also ruled out span alignments
that are extremely lopsided, for example, a 1-word
English span aligned to a 20-word foreign span.
Specifically, we pruned any span alignment in which
one side is more than 5 times larger than the other.
Finally, we employ pruning based on high-
precision alignments from simpler models (Cherry
and Lin, 2007; Haghighi et al, 2009). We com-
pute word-to-word alignments by finding all word
pairs which have a posterior of at least 0.7 according
to both forward and reverse IBM Model 1 parame-
ters, and prune any span pairs which invalidate more
than 3 of these alignments. In total, this pruning re-
123
Span P R F1
Syntactic Alignment 50.9 83.0 63.1
GIZA++ 56.1 67.3 61.2
Rule P R F1
Syntactic Alignment 39.6 40.3 39.9
GIZA++ 46.2 34.7 39.6
Table 3: Alignment quality results for our syntactic
aligner and our GIZA++ baseline.
duced computation from approximately 1.5 seconds
per sentence to about 0.3 seconds per sentence, a
speed-up of a factor of 5.
4.3 Decoding
Given a trained model, we extract a tree-to-string
alignment as follows: we compute, for each node
in the English tree, the posterior probability of a
particular foreign span assignment using the same
dynamic program needed for EM. We then com-
pute the set of span assignments which maximizes
the sum of these posteriors, constrained such that
the foreign span assignments nest in the obvious
way. This algorithm is a natural synchronous gener-
alization of the monolingual Maximum Constituents
Parse algorithm of Goodman (1996).
5 Experiments
5.1 Alignment Quality
We first evaluated our alignments against gold stan-
dard annotations. Our training data consisted of the
2261 manually aligned and translated sentences of
the Chinese Treebank (Bies et al, 2007) and approx-
imately half a million unlabeled sentences of parallel
Chinese-English newswire. The unlabeled data was
subsampled (Li et al, 2009) from a larger corpus by
selecting sentences which have good tune and test
set coverage, and limited to sentences of length at
most 40. We parsed the English side of the train-
ing data with the Berkeley parser.8 For our baseline
alignments, we used GIZA++, trained in the stan-
dard way.9 We used the grow-diag-final alignment
heuristic, as we found it outperformed union in early
experiments.
We trained our unsupervised syntactic aligner on
the concatenation of the labelled and unlabelled
8http://code.google.com/p/berkeleyparser/
95 iterations of model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4.
data. As is standard in unsupervised alignment mod-
els, we initialized the translation parameters pt by
first training 5 iterations of IBM Model 1 using the
joint training algorithm of Liang et al (2006), and
then trained our model for 5 EM iterations. We
extracted syntactic rules using a re-implementation
of the Galley et al (2006) algorithm from both our
syntactic alignments and the GIZA++ alignments.
We handle null-aligned words by extracting every
consistent derivation, and extracted composed rules
consisting of at most 3 minimal rules.
We evaluate our alignments against the gold stan-
dard in two ways. We calculated Span F-score,
which compares the set of extractable nodes paired
with a foreign span, and Rule F-score (Fossum et al,
2008) over minimal rules. The results are shown in
Table 3. By both measures, our syntactic aligner ef-
fectively trades recall for precision when compared
to our baseline, slightly increasing overall F-score.
5.2 Translation Quality
For our translation system, we used a re-
implementation of the syntactic system of Galley et
al. (2006). For the translation rules extracted from
our data, we computed standard features based on
relative frequency counts, and tuned their weights
using MERT (Och, 2003). We also included a
language model feature, using a 5-gram language
model trained on 220 million words of English text
using the SRILM Toolkit (Stolcke, 2002).
For tuning and test data, we used a subset of the
NIST MT04 and MT05 with sentences of length at
most 40. We used the first 1000 sentences of this set
for tuning and the remaining 642 sentences as test
data. We used the decoder described in DeNero et
al. (2009) during both tuning and testing.
We provide final tune and test set results in Ta-
ble 4. Our alignments produce a 1.0 BLEU improve-
ment over the baseline. Our reported syntactic re-
sults were obtained when rules were thresholded by
count; we discuss this in the next section.
5.3 Analysis
As discussed in Section 3.4, our aligner is designed
to extract many rules, which risks inadvertently ex-
tracting low-quality rules. To quantify this, we
first examined the number of rules extracted by our
aligner as compared with GIZA++. After relativiz-
124
Tune Test
Syntactic Alignment 29.78 29.83
GIZA++ 28.76 28.84
GIZA++ high count 25.51 25.38
Table 4: Final tune and test set results for our grammars
extracted using the baseline GIZA++ alignments and our
syntactic aligner. When we filter the GIZA++ grammars
with the same count thresholds used for our aligner (?high
count?), BLEU score drops substantially.
ing to the tune and test set, we extracted approx-
imately 32 million unique rules using our aligner,
but only 3 million with GIZA++. To check that
we were not just extracting extra low-count, low-
quality rules, we plotted the number of rules with
a particular count in Figure 4. We found that while
our aligner certainly extracts many more low-count
rules, it also extracts many more high-count rules.
Of course, high-count rules are not guaranteed
to be high quality. To verify that frequent rules
were better for translation, we experimented with
various methods of thresholding to remove rules
with low count extracted from using aligner. We
found in early development found that removing
low-count rules improved translation performance
substantially. In particular, we settled on the follow-
ing scheme: we kept all rules with a single foreign
terminal on the right-hand side. For entirely lexical
(gapless) rules, we kept all rules occurring at least
3 times. For unlexicalized rules, we kept all rules
occurring at least 20 times per gap. For rules which
mixed gaps and lexical items, we kept all rules oc-
curring at least 10 times per gap. This left us with
a grammar about 600 000 rules, the same grammar
which gave us our final results reported in Table 4.
In contrast to our syntactic aligner, rules extracted
using GIZA++ could not be so aggressively pruned.
When pruned using the same count thresholds, ac-
curacy dropped by more than 3.0 BLEU on the tune
set, and similarly on the test set (see Table 4). To
obtain the accuracy shown in our final results (our
best results with GIZA++), we had to adjust the
count threshold to include all lexicalized rules, all
unlexicalized rules, and mixed rules occurring at
least twice per gap. With these count thresholds, the
GIZA++ grammar contained about 580 000 rules,
roughly the same number as our syntactic grammar.
We also manually searched the grammars for
rules that had high count in the syntactically-
0 200 400 600 800 1000
1e+00
1e+02
1e+04
1e+06
Count
Numbe
r of rul
es with
 count SyntacticGIZA++
Figure 4: Number of extracted translation rules with a
particular count. Grammars extracted from our syntactic
aligner produce not only more low-count rules, but also
more high-count rules than GIZA++.
extracted grammar and low (or 0) count in the
GIZA++ grammar. Of course, we can always
cherry-pick such examples, but a few rules were il-
luminating. For example, for the ? . . .?? con-
struction discussed earlier, our aligner permits ex-
traction of the general rule (PP? IN1 NP2 ;? NP2
IN1) 3087 times, and the lexicalized rule (PP? be-
fore NP ; ? NP ??) 118 times. In constrast, the
GIZA++ grammar extracts the latter only 23 times
and the former not at all. The more complex rule
(NP? NP2 , who S1 , ; S1 ? NP2), which captures
a common appositive construction, was absent from
the GIZA++ grammar but occurred 63 in ours.
6 Conclusion
We have described a syntactic alignment model
which explicitly aligns nodes of a syntactic parse in
one language to spans in another, making it suitable
for use in many syntactic translation systems. Our
model is unsupervised and can be efficiently trained
with a straightforward application of EM. We have
demonstrated that our model can accurately capture
many syntactic correspondences, and is robust in the
face of syntactic divergence between language pairs.
Our aligner permits the extraction of more reliable,
high-count rules when compared to a standard word-
alignment baseline. These high-count rules also pro-
duce improvements in BLEU score.
Acknowledgements
This project is funded in part by the NSF under grant 0643742;
by BBN under DARPA contract HR0011-06-C-0022; and an
NSERC Postgraduate Fellowship. The authors would like to
thank Michael Auli for his input.
125
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007.
English chinese translation treebank v 1.0. web download.
In LDC2007T02.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint pars-
ing and alignment with weakly synchronized grammar. In
Proceedings of the North American Association for Compu-
tational Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In Pro-
ceedings of the Association of Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion transduction
grammar for joint phrasal translation modeling. In Workshop
on Syntax and Structure in Statistical Translation.
David Chiang. 2004. Evaluating grammar formalisms for ap-
plications to natural language processing and biological se-
quence analysis. Ph.D. thesis, University of Pennsylvania.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In The Annual Conference of
the Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of
syntax-directed tree to string grammar induction. In Pro-
ceedings of the Conference on Emprical Methods for Natural
Language Processing.
John DeNero and Dan Klein. 2007. Tailoring word alignments
to syntactic machine translation. In The Annual Conference
of the Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Workshop on Statistical Machine Translation at
NAACL.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of NAACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Us-
ing syntax to improve word alignment precision for syntax-
based machine translation. In Proceedings of the Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proceed-
ings of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proceedings of the Association for Compu-
tational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Association for Computational Linguis-
tics.
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Aria Haghighi, John Blitzer, John Denero, and Dan Klein.
2009. Better word alignments with supervised itg models.
In Proceedings of the Association for Computational Lin-
guistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of CHSLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch,
Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton,
Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an
open source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by
agreement. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2009. Bayesian learning of
phrasal tree-to-string templates. In Proceedings of EMNLP.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-
tree translation with packed forests. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn Treebank.
In Computational Linguistics.
Jonathan May and Kevin Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of the Con-
ference on Emprical Methods for Natural Language Pro-
cessing.
Robert C. Moore. 2004. Improving ibm word alignment model
1. In The Annual Conference of the Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the Association
for Computational Linguistics.
Andreas Stolcke. 2002. SRILM: An extensible language mod-
eling toolkit. In ICSLP 2002.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23:377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the Association of
Computational Linguistics.
Hao Zhang and Daniel Gildea. 2004. Syntax-based alignment:
supervised or unsupervised? In Proceedings of the Confer-
ence on Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proceedings of the North American Chapter of the Associa-
tion for Computational Linguistics.
126
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447?455,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Bayesian Inference for Finite-State Transducers?
David Chiang1 Jonathan Graehl1 Kevin Knight1 Adam Pauls2 Sujith Ravi1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
2Computer Science Division
University of California at Berkeley
Soda Hall
Berkeley, CA 94720
Abstract
We describe a Bayesian inference algorithm
that can be used to train any cascade of
weighted finite-state transducers on end-to-
end data. We also investigate the problem
of automatically selecting from among mul-
tiple training runs. Our experiments on four
different tasks demonstrate the genericity of
this framework, and, where applicable, large
improvements in performance over EM. We
also show, for unsupervised part-of-speech
tagging, that automatic run selection gives a
large improvement over previous Bayesian ap-
proaches.
1 Introduction
In this paper, we investigate Bayesian infer-
ence for weighted finite-state transducers (WFSTs).
Many natural language models can be captured
by weighted finite-state transducers (Pereira et al,
1994; Sproat et al, 1996; Knight and Al-Onaizan,
1998; Clark, 2002; Kolak et al, 2003; Mathias and
Byrne, 2006), which offer several benefits:
? WFSTs provide a uniform knowledge represen-
tation.
? Complex problems can be broken down into a
cascade of simple WFSTs.
? Input- and output-epsilon transitions allow
compact designs.
? Generic algorithms exist for doing inferences
with WFSTs. These include best-path de-
coding, k-best path extraction, composition,
?The authors are listed in alphabetical order. Please direct
correspondence to Sujith Ravi (sravi@isi.edu). This work
was supported by NSF grant IIS-0904684 and DARPA contract
HR0011-06-C0022.
intersection, minimization, determinization,
forward-backward training, forward-backward
pruning, stochastic generation, and projection.
? Software toolkits implement these generic al-
gorithms, allowing designers to concentrate on
novel models rather than problem-specific in-
ference code. This leads to faster scientific ex-
perimentation with fewer bugs.
Weighted tree transducers play the same role for
problems that involve the creation and transforma-
tion of tree structures (Knight and Graehl, 2005). Of
course, many problems do not fit either the finite-
state string or tree transducer framework, but in this
paper, we concentrate on those that do.
Bayesian inference schemes have become popu-
lar recently in natural language processing for their
ability to manage uncertainty about model param-
eters and to allow designers to incorporate prior
knowledge flexibly. Task-accuracy results have gen-
erally been favorable. However, it can be time-
consuming to apply Bayesian inference methods to
each new problem. Designers typically build cus-
tom, problem-specific sampling operators for ex-
ploring the derivation space. They may factor their
programs to get some code re-use from one problem
to the next, but highly generic tools for string and
tree processing are not available.
In this paper, we marry the world of finite-state
machines with the world of Bayesian inference, and
we test our methods across a range of natural lan-
guage problems. Our contributions are:
? We describe a Bayesian inference algorithm
that can be used to train any cascade of WFSTs
on end-to-end data.
? We propose a method for automatic run selec-
447
tion, i.e., how to automatically select among
multiple training runs in order to achieve the
best possible task accuracy.
The natural language applications we consider
in this paper are: (1) unsupervised part-of-speech
(POS) tagging (Merialdo, 1994; Goldwater and
Griffiths, 2007), (2) letter substitution decipher-
ment (Peleg and Rosenfeld, 1979; Knight et al,
2006; Ravi and Knight, 2008), (3) segmentation of
space-free English (Goldwater et al, 2009), and (4)
Japanese/English phoneme alignment (Knight and
Graehl, 1998; Ravi and Knight, 2009a). Figure 1
shows how each of these problems can be repre-
sented as a cascade of finite-state acceptors (FSAs)
and finite-state transducers (FSTs).
2 Generic EM Training
We first describe forward-backward EM training for
a single FST M. Given a string pair (v,w) from our
training data, we transform v into an FST Mv that
just maps v to itself, and likewise transform w into
an FST Mw. Then we compose Mv with M, and com-
pose the result with Mw. This composition follows
Pereira and Riley (1996), treating epsilon input and
output transitions correctly, especially with regards
to their weighted interleaving. This yields a deriva-
tion lattice D, each of whose paths transforms v into
w.1 Each transition in D corresponds to some tran-
sition in the FST M. We run the forward-backward
algorithm over D to collect fractional counts for the
transitions in M. After we sum fractional counts for
all examples, we normalize with respect to com-
peting transitions in M, assign new probabilities to
M, and iterate. Transitions in M compete with each
other if they leave the same state with the same input
symbol, which may be empty ().
In order to train an FSA on observed string data,
we convert the FSA into an FST by adding an input-
epsilon to every transition. We then convert each
training string v into the string pair (, v). After run-
ning the above FST training algorithm, we can re-
move all input- from the trained machine.
It is straightforward to modify generic training to
support the following controls:
1Throughout this paper, we do not assume that lattices are
acyclic; the algorithms described work on general graphs.
B:E
a:A b:B A:D
A:C
=
a: 
 :D
 :E b: 
a:  :C
Figure 2: Composition of two FSTs maintaining separate
transitions.
Maximum iterations and early stopping. We spec-
ify a maximum number of iterations, and we halt
early if the ratio of log P(data) from one iteration
to the next exceeds a threshold (such as 0.99999).
Initial point. Any probabilities supplied on the pre-
trained FST are interpreted as a starting point for
EM?s search. If no probabilities are supplied, EM
begins with uniform probabilities.
Random restarts. We can request n random restarts,
each from a different, randomly-selected initial
point.
Locking and tying. Transitions on the pre-trained
FST can be marked as locked, in which case EM
will not modify their supplied probabilities. Groups
of transitions can be tied together so that their frac-
tional counts are pooled, and when normalization
occurs, they all receive the same probability.
Derivation lattice caching. If memory is available,
training can cache the derivation lattices computed
in the first EM iteration for all training pairs. Subse-
quent iterations then run much faster. In our experi-
ments, we observe an average 10-fold speedup with
caching.
Next we turn to training a cascade of FSTs on
end-to-end data. The algorithm takes as input: (1) a
sequence of FSTs, and (2) pairs of training strings
(v,w), such that v is accepted by the first FST in
the cascade, and w is produced by the last FST. The
algorithm outputs the same sequence of FSTs, but
with trained probabilities.
To accomplish this, we first compose the supplied
FSTs, taking care to keep the transitions from differ-
ent machines separate. Figure 2 illustrates this with a
small example. It may thus happen that a single tran-
sition in an input FST is represented multiple times
in the composed device, in which case their prob-
448
ABCD:a 
REY:r 
?:c 
1.  Unsupervised part-of-speech tagging with constrained dictionary 
POS Tag 
sequence 
Observed 
word 
sequence 
2.  Decipherment of letter-substitution cipher 
English 
letter 
sequence 
Observed 
enciphered 
text 
3.  Re-Spacing of English text written without spaces 
Word 
sequence 
Observed 
letter 
sequence 
w/o spaces 
4.  Alignment of Japanese/English phoneme sequences 
English 
phoneme 
sequence 
Japanese 
katakana 
phoneme 
sequence 
26 x 26 table 
letter bigram model, 
learned separately 
constrained tag?word 
substitution model tag bigram model 
unigram model over 
words and non-words deterministic spell-out 
mapping from each English  
phoneme to each Japanese  
phoneme sequence of length 1 to 3 
NN 
JJ 
JJ 
JJ 
NN 
VB ? 
? 
? 
NN:fish 
IN:at 
VB:fish 
SYM:a DT:a 
a 
b 
b 
b 
a 
c ? 
? 
? 
a:A 
a:B 
a:C 
b:A b:B b:C 
A AR 
ARE AREY 
AREYO 
?:? 
AREY:a 
?:b 
?:d 
?:r ?:e 
?:y 
AE:? 
?:S 
?:S 
?:U 
Figure 1: Finite-state cascades for five natural language problems.
449
abilities are tied together. Next, we run FST train-
ing on the end-to-end data. This involves creating
derivation lattices and running forward-backward on
them. After FST training, we de-compose the trained
device back into a cascade of trained machines.
When the cascade?s first machine is an FSA,
rather than an FST, then the entire cascade is viewed
as a generator of strings rather than a transformer of
strings. Such a cascade is trained on observed strings
rather than string pairs. By again treating the first
FSA as an FST with empty input, we can train using
the FST-cascade training algorithm described in the
previous paragraph.
Once we have our trained cascade, we can apply it
to new data, obtaining (for example) the k-best out-
put strings for an input string.
3 Generic Bayesian Training
Bayesian learning is a wide-ranging field. We focus
on training using Gibbs sampling (Geman and Ge-
man, 1984), because it has been popularly applied
in the natural language literature, e.g., (Finkel et al,
2005; DeNero et al, 2008; Blunsom et al, 2009).
Our overall plan is to give a generic algorithm
for Bayesian training that is a ?drop-in replacement?
for EM training. That is, we input an FST cas-
cade and data and output the same FST cascade
with trained weights. This is an approximation to a
purely Bayesian setup (where one would always in-
tegrate over all possible weightings), but one which
makes it easy to deploy FSTs to efficiently decode
new data. Likewise, we do not yet support non-
parametric approaches?to create a drop-in replace-
ment for EM, we require that all parameters be spec-
ified in the initial FST cascade. We return to this is-
sue in Section 5.
3.1 Particular Case
We start with a well-known application of Bayesian
inference, unsupervised POS tagging (Goldwater
and Griffiths, 2007). Raw training text is provided,
and each potential corpus tagging corresponds to a
hidden derivation of that data. Derivations are cre-
ated and probabilistically scored as follows:
1. i? 1
2. Choose tag t1 according to P0(t1)
3. Choose word w1 according to P0(w1 | t1)
4. i? i + 1
5. Choose tag ti according to
?P0(ti | ti?1) + ci?11 (ti?1, ti)
? + ci?11 (ti?1)
(1)
6. Choose word wi according to
?P0(wi | ti) + ci?11 (ti,wi)
? + ci?11 (ti)
(2)
7. With probability Pquit, quit; else go to 4.
This defines the probability of any given derivation.
The base distribution P0 represents prior knowl-
edge about the distribution of tags and words, given
the relevant conditioning context. The ci?11 are the
counts of events occurring before word i in the
derivation (the ?cache?).
When ? and ? are large, tags and words are essen-
tially generated according to P0. When ? and ? are
small, tags and words are generated with reference
to previous decisions inside the cache.
We use Gibbs sampling to estimate the distribu-
tion of tags given words. The key to efficient sam-
pling is to define a sampling operator that makes
some small change to the overall corpus derivation.
With such an operator, we derive an incremental
formula for re-scoring the probability of an entire
new derivation based on the probability of the old
derivation. Exchangeability makes this efficient?
we pretend like the area around the small change oc-
curs at the end of the corpus, so that both old and
new derivations share the same cache. Goldwater
and Griffiths (2007) choose the re-sampling operator
?change the tag of a single word,? and they derive
the corresponding incremental scoring formula for
unsupervised tagging. For other problems, design-
ers develop different sampling operators and derive
different incremental scoring formulas.
3.2 Generic Case
In order to develop a generic algorithm, we need
to abstract away from these problem-specific de-
sign choices. In general, hidden derivations corre-
spond to paths through derivation lattices, so we first
450
Figure 3: Changing a decision in the derivation lattice.
All paths generate the observed data. The bold path rep-
resents the current sample, and the dotted path represents
a sidetrack in which one decision is changed.
compute derivation lattices for our observed training
data through our cascade of FSTs. A random path
through these lattices constitutes the initial sample,
and we calculate its derivation probability directly.
One way to think about a generic small change
operator is to consider a single transition in the cur-
rent sample. This transition will generally compete
with other transitions. One possible small change is
to ?sidetrack? the derivation to a competing deriva-
tion. Figure 3 shows how this works. If the sidetrack
path quickly re-joins the old derivation path, then an
incremental score can be computed. However, side-
tracking raises knotty questions. First, what is the
proper path continuation after the sidetracking tran-
sition is selected? Should the path attempt to re-join
the old derivation as soon as possible, and if so, how
is this efficiently done? Then, how can we compute
new derivation scores for all possible sidetracks, so
that we can choose a new sample by an appropriate
weighted coin flip? Finally, would such a sampler be
reversible? In order to satisfy theoretical conditions
for Gibbs sampling, if we move from sample A to
sample B, we must be able to immediately get back
to sample A.
We take a different tack here, moving from point-
wise sampling to blocked sampling. Gao and John-
son (2008) employed blocked sampling for POS tag-
ging, and the approach works nicely for arbitrary
derivation lattices. We again start with a random
derivation for each example in the corpus. We then
choose a training example and exchange its entire
derivation lattice to the end of the corpus. We cre-
ate a weighted version of this lattice, called the pro-
posal lattice, such that we can approximately sample
whole paths by stochastic generation. The probabil-
ities are based on the event counts from the rest of
the sample (the cache), and on the base distribution,
and are computed in this way:
P(r | q) =
?P0(r | q) + c(q, r)
? + c(q)
(3)
where q and r are states of the derivation lattice, and
the c(?) are counts collected from the corpus minus
the entire training example being resampled. This is
an approximation because we are ignoring the fact
that P(r | q) in general depends on choices made
earlier in the lattice. The approximation can be cor-
rected using the Metropolis-Hastings algorithm, in
which the sample drawn from the proposal lattice is
accepted only with a certain probability ?; but Gao
and Johnson (2008) report that ? > 0.99, so we skip
this step.
3.3 Choosing the best derivations
After the sampling run has finished, we can choose
the best derivations using two different methods.
First, if we want to find the MAP derivations of the
training strings, then following Goldwater and Grif-
fiths (2007), we can use annealing: raise the proba-
bilities in the sampling distribution to the 1T power,
where T is a temperature parameter, decrease T to-
wards zero, and take a single sample.
But in practice one often wants to predict the
MAP derivation for a new string w? not contained
in the training data. To approximate the distribution
of derivations of w? given the training data, we aver-
age the transition counts from all the samples (after
burn-in) and plug the averaged counts into (3) to ob-
tain a single proposal lattice.2 The predicted deriva-
tion is the Viterbi path through this lattice. Call this
method averaging. An advantage of this approach is
that the trainer, taking a cascade of FSAs as input,
outputs a weighted version of the same cascade, and
this trained cascade can be used on unseen examples
without having to rerun training.
3.4 Implementation
That concludes the generic Bayesian training algo-
rithm, to which we add the following controls:
2A better approximation might have been to build a proposal
lattice for each sample (after burn-in), and then construct a sin-
gle FSA that computes the average of the probability distribu-
tions computed by all the proposal lattices. But this FSA would
be rather large.
451
Number of Gibbs sampling iterations. We execute
the full number specified.
Base distribution. Any probabilities supplied on the
pre-trained FST are interpreted as base distribution
probabilities. If no probabilities are supplied, then
the base distribution is taken to be uniform.
Hyperparameters. We supply a distinct ? for each
machine in the FST cascade. We do not yet support
different ? values for different states within a single
FST.
Random restarts. We can request multiple runs
from different, randomly-selected initial samples.
EM-based initial point. If random initial samples
are undesirable, we can request that the Gibbs sam-
pler be initialized with the Viterbi path using param-
eter values obtained by n iterations of EM.
Annealing schedule. If annealing is used, it follows
a linear annealing schedule with starting and stop-
ping temperature specified by the user.
EM and Bayesian training for arbitrary FST
cascades are both implemented in the finite-state
toolkit Carmel, which is distributed with source
code.3 All controls are implemented as command-
line switches. We use Carmel to carry out the exper-
iments in the next section.
4 Run Selection
For both EM and Bayesian methods, different train-
ing runs yield different results. EM?s objective func-
tion (probability of observed data) is very bumpy for
the unsupervised problems we work on?different
initial points yield different trained WFST cascades,
with different task accuracies. Averaging task accu-
racies across runs is undesirable, because we want to
deploy a particular trained cascade in the real world,
and we want an estimate of its performance. Select-
ing the run with the best task accuracy is illegal in an
unsupervised setting. With EM, we have a good al-
ternative: select the run that maximizes the objective
function, i.e., the likelihood of the observed training
data. We find a decent correlation between this value
and task accuracy, and we are generally able to im-
prove accuracy using this run selection method. Fig-
ure 4 shows a scatterplot of 1000 runs for POS tag-
ging. A single run with a uniform start yields 81.8%
3http://www.isi.edu/licensed-sw/carmel
 0.75
 0.8
 0.85
 0.9
 211200
 211300
 211400
 211500
 211600
 211700
 211800
 211900
 212000
 212100
 212200
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(data)
EM (random start)EM (uniform start)
Figure 4: Multiple EM restarts for POS tagging. Each
point represents one random restart; the y-axis is tag-
ging accuracy and the x-axis is EM?s objective function,
? log P(data).
accuracy, while automatic selection from 1000 runs
yields 82.4% accuracy.
Gibbs sampling runs also yield WFST cascades
with varying task accuracies, due to random initial
samples and sampling decisions. In fact, the varia-
tion is even larger than what we find with EM. It is
natural to ask whether we can do automatic run se-
lection for Gibbs sampling. If we are using anneal-
ing, it makes sense to use the probability of the fi-
nal sample, which is supposed to approximate the
MAP derivation. When using averaging, however,
choosing the final sample would be quite arbitrary.
Instead, we propose choosing the run that has the
highest average log-probability (that is, the lowest
entropy) after burn-in. The rationale is that the runs
that have found their way to high-probability peaks
are probably more representative of the true distri-
bution, or at least capture a part of the distribution
that is of greater interest to us.
We find that this method works quite well in prac-
tice. Figure 5 illustrates 1000 POS tagging runs
for annealing with automatic run selection, yield-
ing 84.7% accuracy. When using averaging, how-
ever, automatic selection from 1000 runs (Figure 6)
produces a much higher accuracy of 90.7%. This
is better than accuracies reported previously using
452
 0.75
 0.8
 0.85
 0.9
 235100
 235150
 235200
 235250
 235300
 235350
 235400
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) for final sample
Bayesian run (with annealing)
Figure 5: Multiple Bayesian learning runs (using anneal-
ing with temperature decreasing from 2 to 0.08) for POS
tagging. Each point represents one run; the y-axis is tag-
ging accuracy and the x-axis is the ? log P(derivation) of
the final sample.
 0.75
 0.8
 0.85
 0.9
 236800
 236900
 237000
 237100
 237200
 237300
 237400
 237500
 237600
 237700
 237800
 237900
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) averaged over all post-burnin samples
Bayesian run (using averaging)
Figure 6: Multiple Bayesian learning runs (using averag-
ing) for POS tagging. Each point represents one run; the
y-axis is tagging accuracy and the x-axis is the average
? log P(derivation) over all samples after burn-in.
Bayesian methods (85.2% from Goldwater and Grif-
fiths (2007), who use a trigram model) and close to
the best accuracy reported on this task (91.8% from
Ravi and Knight (2009b), who use an integer linear
program to minimize the model directly).
5 Experiments and Results
We run experiments for various natural language ap-
plications and compare the task accuracies achieved
by the EM and Bayesian learning methods. The
tasks we consider are:
Unsupervised POS tagging. We adopt the com-
mon problem formulation for this task described
by Merialdo (1994), in which we are given a raw
24,115-word sequence and a dictionary of legal tags
for each word type. The tagset consists of 45 dis-
tinct grammatical tags. We use the same modeling
approach as as Goldwater and Griffiths (2007), us-
ing a probabilistic tag bigram model in conjunction
with a tag-to-word model.
Letter substitution decipherment. Here, the task
is to decipher a 414-letter substitution cipher and un-
cover the original English letter sequence. The task
accuracy is defined as the percent of ciphertext to-
kens that are deciphered correctly. We work on the
same standard cipher described in previous litera-
ture (Ravi and Knight, 2008). The model consists
of an English letter bigram model, whose probabil-
ities are fixed and an English-to-ciphertext channel
model, which is learnt during training.
Segmentation of space-free English. Given
a space-free English text corpus (e.g.,
iwalkedtothe...), the task is to segment the
text into words (e.g., i walked to the ...).
Our input text corpus consists of 11,378 words,
with spaces removed. As illustrated in Figure 1,
our method uses a unigram FSA that models every
letter sequence seen in the data, which includes
both words and non-words (at most 10 letters long)
composed with a deterministic spell-out model.
In order to evaluate the quality of our segmented
output, we compare it against the gold segmentation
and compute the word token f-measure.
Japanese/English phoneme alignment. We
use the problem formulation of Knight and
Graehl (1998). Given an input English/Japanese
katakana phoneme sequence pair, the task is to
produce an alignment that connects each English
453
MLE Bayesian
EM prior VB-EM Gibbs
POS tagging 82.4 ? = 10?2, ? = 10?1 84.1 90.7
Letter decipherment 83.6 ? = 106, ? = 10?2 83.6 88.9
Re-spacing English 0.9 ? = 10?8, ? = 104 0.8 42.8
Aligning phoneme strings? 100 ? = 10?2 99.9 99.1
Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. ?The output of
EM alignment was used as the gold standard.
phoneme to its corresponding Japanese sounds (a
sequence of one or more Japanese phonemes). For
example, given a phoneme sequence pair ((AH B
AW T) ? (a b a u t o)), we have to produce
the alignments ((AH ? a), (B ? b), (AW ?
a u), (T ? t o)). The input data consists of
2,684 English/Japanese phoneme sequence pairs.
We use a model that consists of mappings from each
English phoneme to Japanese phoneme sequences
(of length up to 3), and the mapping probabilities
are learnt during training. We manually analyzed
the alignments produced by the EM method for
this task and found them to be nearly perfect.
Hence, for the purpose of this task we treat the EM
alignments as our gold standard, since there are no
gold alignments available for this data.
In all the experiments reported here, we run EM
for 200 iterations and Bayesian for 5000 iterations
(the first 2000 for burn-in). We apply automatic run
selection using the objective function value for EM
and the averaging method for Bayesian.
Table 1 shows accuracy results for our four tasks,
using run selection for both EM and Bayesian learn-
ing. For the Bayesian runs, we compared two infer-
ence methods: Gibbs sampling, as described above,
and Variational Bayesian EM (Beal and Ghahra-
mani, 2003), both of which are implemented in
Carmel. We used the hyperparameters (?, ?) as
shown in the table. Setting a high value yields a fi-
nal distribution that is close to the original one (P0).
For example, in letter decipherment we want to keep
the language model probabilities fixed during train-
ing, and hence we set the prior on that model to
be very strong (? = 106). Table 1 shows that the
Bayesian methods consistently outperform EM for
all the tasks (except phoneme alignment, where EM
was taken as the gold standard). Each iteration of
Gibbs sampling was 2.3 times slower than EM for
POS tagging, and in general about twice as slow.
6 Discussion
We have described general training algorithms for
FST cascades and their implementation, and exam-
ined the problem of run selection for both EM and
Bayesian training. This work raises several interest-
ing points for future study.
First, is there an efficient method for perform-
ing pointwise sampling on general FSTs, and would
pointwise sampling deliver better empirical results
than blocked sampling across a range of tasks?
Second, can generic methods similar to the ones
described here be developed for cascades of tree
transducers? It is straightforward to adapt our meth-
ods to train a single tree transducer (Graehl et al,
2008), but as most types of tree transducers are
not closed under composition (Ge?cseg and Steinby,
1984), the compose/de-compose method cannot be
directly applied to train cascades.
Third, what is the best way to extend the FST for-
malism to represent non-parametric Bayesian mod-
els? Consider the English re-spacing application. We
currently take observed (un-spaced) data and build
a giant unigram FSA that models every letter se-
quence seen in the data of up to 10 letters, both
words and non-words. This FSA has 207,253 tran-
sitions. We also define P0 for each individual transi-
tion, which allows a preference for short words. This
set-up works fine, but in a nonparametric approach,
P0 is defined more compactly and without a word-
length limit. An extension of FSTs along the lines
of recursive transition networks may be appropriate,
but we leave details for future work.
454
References
Matthew J. Beal and Zoubin Ghahramani. 2003. The
Variational Bayesian EM algorithm for incomplete
data: with application to scoring graphical model
structures. Bayesian Statistics, 7:453?464.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of ACL-
IJCNLP 2009.
Alexander Clark. 2002. Memory-based learning of mor-
phology with stochastic transducers. In Proceedings
of ACL 2002.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of ACL 2005.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised Hidden Markov
Model POS taggers. In Proceedings of EMNLP 2008.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21 ? 54.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Kevin Knight and Yaser Al-Onaizan. 1998. Transla-
tion with finite-state devices. In Proceedings of AMTA
1998.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Knight Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proceedings of CICLing-2005.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of COLING-ACL 2006.
Okan Kolak, Willian Byrne, and Philip Resnik. 2003. A
generative probabilistic OCR model for NLP applica-
tions. In Proceedings of HLT-NAACL 2003.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP 2006.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Communications of the ACM, 22(11):598?605.
Fernando C. N. Pereira and Michael D. Riley. 1996.
Speech recognition by composition of weighted finite
automata. Finite-State Language Processing, pages
431?453.
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their appli-
cations to human language processing. In ARPA Hu-
man Language Technology Workshop.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of EMNLP 2008.
Sujith Ravi and Kevin Knight. 2009a. Learning
phoneme mappings for transliteration without parallel
data. In Proceedings of NAACL HLT 2009.
Sujith Ravi and Kevin Knight. 2009b. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009.
Richard Sproat, Chilin Shih, William Gale, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377?404.
455
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495?503,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Minimized models and grammar-informed initialization
for supertagging with highly ambiguous lexicons
Sujith Ravi1 Jason Baldridge2 Kevin Knight1
1University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
2Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
jbaldrid@mail.utexas.edu
Abstract
We combine two complementary ideas
for learning supertaggers from highly am-
biguous lexicons: grammar-informed tag
transitions and models minimized via in-
teger programming. Each strategy on its
own greatly improves performance over
basic expectation-maximization training
with a bitag Hidden Markov Model, which
we show on the CCGbank and CCG-TUT
corpora. The strategies provide further er-
ror reductions when combined. We de-
scribe a new two-stage integer program-
ming strategy that efficiently deals with
the high degree of ambiguity on these
datasets while obtaining the full effect of
model minimization.
1 Introduction
Creating accurate part-of-speech (POS) taggers
using a tag dictionary and unlabeled data is an
interesting task with practical applications. It
has been explored at length in the literature since
Merialdo (1994), though the task setting as usu-
ally defined in such experiments is somewhat arti-
ficial since the tag dictionaries are derived from
tagged corpora. Nonetheless, the methods pro-
posed apply to realistic scenarios in which one
has an electronic part-of-speech tag dictionary or
a hand-crafted grammar with limited coverage.
Most work has focused on POS-tagging for
English using the Penn Treebank (Marcus et al,
1993), such as (Banko and Moore, 2004; Gold-
water and Griffiths, 2007; Toutanova and John-
son, 2008; Goldberg et al, 2008; Ravi and Knight,
2009). This generally involves working with the
standard set of 45 POS-tags employed in the Penn
Treebank. The most ambiguous word has 7 dif-
ferent POS tags associated with it. Most methods
have employed some variant of Expectation Max-
imization (EM) to learn parameters for a bigram
or trigram Hidden Markov Model (HMM). Ravi
and Knight (2009) achieved the best results thus
far (92.3% word token accuracy) via a Minimum
Description Length approach using an integer pro-
gram (IP) that finds a minimal bigram grammar
that obeys the tag dictionary constraints and cov-
ers the observed data.
A more challenging task is learning supertag-
gers for lexicalized grammar formalisms such as
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000). For example, CCGbank (Hocken-
maier and Steedman, 2007) contains 1241 dis-
tinct supertags (lexical categories) and the most
ambiguous word has 126 supertags. This pro-
vides a much more challenging starting point
for the semi-supervised methods typically ap-
plied to the task. Yet, this is an important task
since creating grammars and resources for CCG
parsers for new domains and languages is highly
labor- and knowledge-intensive. Baldridge (2008)
uses grammar-informed initialization for HMM
tag transitions based on the universal combinatory
rules of the CCG formalism to obtain 56.1% accu-
racy on ambiguous word tokens, a large improve-
ment over the 33.0% accuracy obtained with uni-
form initialization for tag transitions.
The strategies employed in Ravi and Knight
(2009) and Baldridge (2008) are complementary.
The former reduces the model size globally given
a data set, while the latter biases bitag transitions
toward those which are more likely based on a uni-
versal grammar without reference to any data. In
this paper, we show how these strategies may be
combined straightforwardly to produce improve-
ments on the task of learning supertaggers from
lexicons that have not been filtered in any way.1
We demonstrate their cross-lingual effectiveness
on CCGbank (English) and the Italian CCG-TUT
1See Banko and Moore (2004) for a description of how
many early POS-tagging papers in fact used a number of
heuristic cutoffs that greatly simplify the problem.
495
corpus (Bos et al, 2009). We find a consistent im-
proved performance by using each of the methods
compared to basic EM, and further improvements
by using them in combination.
Applying the approach of Ravi and Knight
(2009) naively to CCG supertagging is intractable
due to the high level of ambiguity. We deal with
this by defining a new two-stage integer program-
ming formulation that identifies minimal gram-
mars efficiently and effectively.
2 Data
CCGbank. CCGbank was created by semi-
automatically converting the Penn Treebank to
CCG derivations (Hockenmaier and Steedman,
2007). We use the standard splits of the data
used in semi-supervised tagging experiments (e.g.
Banko and Moore (2004)): sections 0-18 for train-
ing, 19-21 for development, and 22-24 for test.
CCG-TUT. CCG-TUT was created by semi-
automatically converting dependencies in the Ital-
ian Turin University Treebank to CCG deriva-
tions (Bos et al, 2009). It is much smaller than
CCGbank, with only 1837 sentences. It is split
into three sections: newspaper texts (NPAPER),
civil code texts (CIVIL), and European law texts
from the JRC-Acquis Multilingual Parallel Corpus
(JRC). For test sets, we use the first 400 sentences
of NPAPER, the first 400 of CIVIL, and all of JRC.
This leaves 409 and 498 sentences from NPAPER
and CIVIL, respectively, for training (to acquire a
lexicon and run EM). For evaluation, we use two
different settings of train/test splits:
TEST 1 Evaluate on the NPAPER section of test
using a lexicon extracted only from NPAPER
section of train.
TEST 2 Evaluate on the entire test using lexi-
cons extracted from (a) NPAPER + CIVIL,
(b) NPAPER, and (c) CIVIL.
Table 1 shows statistics for supertag ambiguity
in CCGbank and CCG-TUT. As a comparison, the
POS word token ambiguity in CCGbank is 2.2: the
corresponding value of 18.71 for supertags is in-
dicative of the (challenging) fact that supertag am-
biguity is greatest for the most frequent words.
3 Grammar informed initialization for
supertagging
Part-of-speech tags are atomic labels that in and of
themselves encode no internal structure. In con-
Data Distinct Max Type ambig Tok ambig
CCGbank 1241 126 1.69 18.71
CCG-TUT
NPAPER+CIVIL 849 64 1.48 11.76
NPAPER 644 48 1.42 12.17
CIVIL 486 39 1.52 11.33
Table 1: Statistics for the training data used to ex-
tract lexicons for CCGbank and CCG-TUT. Dis-
tinct: # of distinct lexical categories; Max: # of
categories for the most ambiguous word; Type
ambig: per word type category ambiguity; Tok
ambig: per word token category ambiguity.
trast, supertags are detailed, structured labels; a
universal set of grammatical rules defines how cat-
egories may combine with one another to project
syntactic structure.2 Because of this, properties of
the CCG formalism itself can be used to constrain
learning?prior to considering any particular lan-
guage, grammar or data set. Baldridge (2008) uses
this observation to create grammar-informed tag
transitions for a bitag HMM supertagger based on
two main properties. First, categories differ in
their complexity and less complex categories tend
to be used more frequently. For example, two cat-
egories for buy in CCGbank are (S[dcl]\NP)/NP
and ((((S[b]\NP)/PP)/PP)/(S[adj]\NP))/NP; the
former occurs 33 times, the latter once. Second,
categories indicate the form of categories found
adjacent to them; for example, the category for
sentential complement verbs ((S\NP)/S) expects
an NP to its left and an S to its right.
Categories combine via rules such as applica-
tion and composition (see Steedman (2000) for de-
tails). Given a lexicon containing the categories
for each word, these allow derivations like:
Ed might see a cat
NP (S\NP)/(S\NP) (S\NP)/NP NP/N N
>B >
(S\NP)/NP NP
>
S\NP
>
S
Other derivations are possible. In fact, every pair
of adjacent words above may be combined di-
rectly. For example, see and a may combine
through forward composition to produce the cate-
gory (S\NP)/N, and Ed?s category may type-raise
to S/(S\NP) and compose with might?s category.
Baldridge uses these properties to define tag
2Note that supertags can be lexical categories of CCG
(Steedman, 2000), elementary trees of Tree-adjoining Gram-
mar (Joshi, 1988), or types in a feature hierarchy as in Head-
driven Phrase Structure Grammar (Pollard and Sag, 1994).
496
transition distributions that have higher likeli-
hood for simpler categories that are able to
combine. For example, for the distribution
p(ti|ti?1=NP ), (S\NP)\NP is more likely than
((S\NP)/(N/N))\NP because both categories may
combine with a preceding NP but the former is
simpler. In turn, the latter is more likely than NP: it
is more complex but can combine with the preced-
ing NP. Finally, NP is more likely than (S/NP)/NP
since neither can combine, but NP is simpler.
By starting EM with these tag transition dis-
tributions and an unfiltered lexicon (word-to-
supertag dictionary), Baldridge obtains a tagging
accuracy of 56.1% on ambiguous words?a large
improvement over the accuracy of 33.0% obtained
by starting with uniform transition distributions.
We refer to a model learned from basic EM (uni-
formly initialized) as EM, and to a model with
grammar-informed initialization as EMGI .
4 Minimized models for supertagging
The idea of searching for minimized models is
related to classic Minimum Description Length
(MDL) (Barron et al, 1998), which seeks to se-
lect a small model that captures the most regularity
in the observed data. This modeling strategy has
been shown to produce good results for many nat-
ural language tasks (Goldsmith, 2001; Creutz and
Lagus, 2002; Ravi and Knight, 2009). For tagging,
the idea has been implemented using Bayesian
models with priors that indirectly induce sparsity
in the learned models (Goldwater and Griffiths,
2007); however, Ravi and Knight (2009) show a
better approach is to directly minimize the model
using an integer programming (IP) formulation.
Here, we build on this idea for supertagging.
There are many challenges involved in using IP
minimization for supertagging. The 1241 distinct
supertags in the tagset result in 1.5 million tag bi-
gram entries in the model and the dictionary con-
tains almost 3.5 million word/tag pairs that are rel-
evant to the test data. The set of 45 POS tags for
the same data yields 2025 tag bigrams and 8910
dictionary entries. We also wish to scale our meth-
ods to larger data settings than the 24k word tokens
in the test data used in the POS tagging task.
Our objective is to find the smallest supertag
grammar (of tag bigram types) that explains the
entire text while obeying the lexicon?s constraints.
However, the original IP method of Ravi and
Knight (2009) is intractable for supertagging, so
we propose a new two-stage method that scales to
the larger tagsets and data involved.
4.1 IP method for supertagging
Our goal for supertagging is to build a minimized
model with the following objective:
IPoriginal: Find the smallest supertag gram-
mar (i.e., tag bigrams) that can explain the en-
tire text (the test word token sequence).
Using the full grammar and lexicon to perform
model minimization results in a very large, diffi-
cult to solve integer program involving billions of
variables and constraints. This renders the mini-
mization objective IPoriginal intractable. One way
of combating this is to use a reduced grammar
and lexicon as input to the integer program. We
do this without further supervision by using the
HMM model trained using basic EM: entries are
pruned based on the tag sequence it predicts on
the test data. This produces an observed grammar
of distinct tag bigrams (Gobs) and lexicon of ob-
served lexical assignments (Lobs). For CCGbank,
Gobs and Lobs have 12,363 and 18,869 entries,
respectively?far less than the millions of entries
in the full grammar and lexicon.
Even though EM minimizes the model some-
what, many bad entries remain in the grammar.
We prune further by supplying Gobs and Lobs as
input (G,L) to the IP-minimization procedure.
However, even with the EM-reduced grammar and
lexicon, the IP-minimization is still very hard to
solve. We thus split it into two stages. The first
stage (Minimization 1) finds the smallest grammar
Gmin1 ? G that explains the set of word bigram
types observed in the data rather than the word
sequence itself, and the second (Minimization 2)
finds the smallest augmentation of Gmin1 that ex-
plains the full word sequence.
Minimization 1 (MIN1). We begin with a sim-
pler minimization problem than the original one
(IPoriginal), with the following objective:
IPmin 1: Find the smallest set of tag bigrams
Gmin1 ? G, such that there is at least one
tagging assignment possible for every word bi-
gram type observed in the data.
We formulate this as an integer program, creat-
ing binary variables gvari for every tag bigram
gi = tjtk in G. Binary link variables connect tag
bigrams with word bigrams; these are restricted
497
          :
          :
        t
i
 t
j
          :
          :
Input Grammar (G) word bigrams: 
w
1
 w
2
w
2
 w
3
:
:
w
i
 w
j
:
:
MIN 1
          :
          :
        t
i
 t
j
          :
          :
Input Grammar (G) word bigrams: 
w
1
 w
2
w
2
 w
3
:
:
w
i
 w
j
:
:
word sequence: 
w
1
       w
2          
w
3          
w
4        
w
5
t
1
t
2
t
3   
:
:
     
t
k
supertags
tag bigrams chosen in first minimization step (G
min1
)
(does not explain the word sequence)
word sequence: 
w
1
       w
2          
w
3          
w
4        
w
5
t
1
t
2
t
3   
:
:
     
t
k
supertags
tag bigrams chosen in second minimization step (G
min2
)
MIN 2
IP Minimization 1
IP Minimization 2
Figure 1: Two-stage IP method for selecting minimized models for supertagging.
to the set of links that respect the lexicon L pro-
vided as input, i.e., there exists a link variable
linkjklm connecting tag bigram tjtk with word bi-
gram wlwm only if the word/tag pairs (wl, tj) and
(wm, tk) are present in L. The entire integer pro-
gramming formulation is shown Figure 2.
The IP solver3 solves the above integer program
and we extract the set of tag bigrams Gmin1 based
on the activated grammar variables. For the CCG-
bank test data, MIN1 yields 2530 tag bigrams.
However, a second stage is needed since there is
no guarantee that Gmin1 can explain the test data:
it contains tags for all word bigram types, but it
cannot necessarily tag the full word sequence. Fig-
ure 1 illustrates this. Using only tag bigrams from
MIN1 (shown in blue), there is no fully-linked tag
path through the network. There are missing links
between words w2 and w3 and between words w3
and w4 in the word sequence. The next stage fills
in these missing links.
Minimization 2 (MIN2). This stage uses the
original minimization formulation for the su-
pertagging problem IPoriginal, again using an in-
teger programming method similar to that pro-
posed by Ravi and Knight (2009). If applied to
the observed grammar Gobs, the resulting integer
program is hard to solve.4 However, by using the
partial solution Gmin1 obtained in MIN1 the IP
optimization speeds up considerably. We imple-
ment this by fixing the values of all binary gram-
mar variables present in Gmin1 to 1 before opti-
mization. This reduces the search space signifi-
3We use the commercial CPLEX solver.
4The solver runs for days without returning a solution.
Minimize:
?
?gi?G
gvari
Subject to constraints:
1. For every word bigram wlwm, there exists at least
one tagging that respects the lexicon L.
?
? tj?L(wl), tk?L(wm)
linkjklm ? 1
where L(wl) and L(wm) represent the set of tags seen
in the lexicon for words wl and wm respectively.
2. The link variable assignments are constrained to re-
spect the grammar variables chosen by the integer pro-
gram.
linkjklm ? gvari
where gvari is the binary variable corresponding to tag
bigram tjtk in the grammar G.
Figure 2: IP formulation for Minimization 1.
cantly, and CPLEX finishes in just a few hours.
The details of this method are described below.
We instantiate binary variables gvari and lvari
for every tag bigram (in G) and lexicon entry (in
L). We then create a network of possible taggings
for the word token sequence w1w2....wn in the
corpus and assign a binary variable to each link
in the network. We name these variables linkcjk,
where c indicates the column of the link?s source
in the network, and j and k represent the link?s
source and destination (i.e., linkcjk corresponds to
tag bigram tjtk in column c). Next, we formulate
the integer program given in Figure 3.
Figure 1 illustrates how MIN2 augments the
grammar Gmin1 (links shown in blue) with addi-
498
Minimize:
?
?gi?G
gvari
Subject to constraints:
1. Chosen link variables form a left-to-right path
through the tagging network.
?c=1..n?2?k
?
j linkcjk =
?
j link(c+1)kj
2. Link variable assignments should respect the chosen
grammar variables.
for every link: linkcjk ? gvari
where gvari corresponds to tag bigram tjtk
3. Link variable assignments should respect the chosen
lexicon variables.
for every link: linkcjk ? lvarwctj
for every link: linkcjk ? lvarwc+1tk
where wc is the cth word in the word sequence w1...wn,
and lvarwctj is the binary variable corresponding to the
word/tag pair wc/tj in the lexicon L.
4. The final solution should produce at least one com-
plete tagging path through the network.
?
?j,k
link1jk ? 1
5. Provide minimized grammar from MIN1as partial
solution to the integer program.
?gi?Gmin1 gvari = 1
Figure 3: IP formulation for Minimization 2.
tional tag bigrams (shown in red) to form a com-
plete tag path through the network. The minimized
grammar set in the final solution Gmin2 contains
only 2810 entries, significantly fewer than the
original grammar Gobs?s 12,363 tag bigrams.
We note that the two-stage minimization pro-
cedure proposed here is not guaranteed to yield
the optimal solution to our original objective
IPoriginal. On the simpler task of unsupervised
POS tagging with a dictionary, we compared
our method versus directly solving IPoriginal and
found that the minimization (in terms of grammar
size) achieved by our method is close to the opti-
mal solution for the original objective and yields
the same tagging accuracy far more efficiently.
Fitting the minimized model. The IP-
minimization procedure gives us a minimal
grammar, but does not fit the model to the data.
In order to estimate probabilities for the HMM
model for supertagging, we use the EM algorithm
but with certain restrictions. We build the transi-
tion model using only entries from the minimized
grammar set Gmin2, and instantiate an emission
model using the word/tag pairs seen in L (pro-
vided as input to the minimization procedure). All
the parameters in the HMM model are initialized
with uniform probabilities, and we run EM for 40
iterations. The trained model is used to find the
Viterbi tag sequence for the corpus. We refer to
this model (where the EM output (Gobs, Lobs) was
provided to the IP-minimization as initial input)
as EM+IP.
Bootstrapped minimization. The quality of the
observed grammar and lexicon improves consid-
erably at the end of a single EM+IP run. Ravi
and Knight (2009) exploited this to iteratively im-
prove their POS tag model: since the first mini-
mization procedure is seeded with a noisy gram-
mar and tag dictionary, iterating the IP procedure
with progressively better grammars further im-
proves the model. We do likewise, bootstrapping a
new EM+IP run using as input, the observed gram-
mar Gobs and lexicon Lobs from the last tagging
output of the previous iteration. We run this until
the chosen grammar set Gmin2 does not change.5
4.2 Minimization with grammar-informed
initialization
There are two complementary ways to use
grammar-informed initialization with the IP-
minimization approach: (1) using EMGI output
as the starting grammar/lexicon and (2) using the
tag transitions directly in the IP objective function.
The first takes advantage of the earlier observation
that the quality of the grammar and lexicon pro-
vided as initial input to the minimization proce-
dure can affect the quality of the final supertagging
output. For the second, we modify the objective
function used in the two IP-minimization steps to
be:
Minimize:
?
?gi?G
wi ? gvari (1)
where, G is the set of tag bigrams provided as in-
put to IP, gvari is a binary variable in the integer
program corresponding to tag bigram (ti?1, ti) ?
G, and wi is negative logarithm of pgii(ti|ti?1)
as given by Baldridge (2008).6 All other parts of
5In our experiments, we run three bootstrap iterations.
6Other numeric weights associated with the tag bi-
grams could be considered, such as 0/1 for uncombin-
499
the integer program including the constraints re-
main unchanged, and, we acquire a final tagger in
the same manner as described in the previous sec-
tion. In this way, we combine the minimization
and GI strategies into a single objective function
that finds a minimal grammar set while keeping
the more likely tag bigrams in the chosen solution.
EMGI+IPGI is used to refer to the method that
uses GI information in both ways: EMGI output
as the starting grammar/lexicon and GI weights in
the IP-minimization objective.
5 Experiments
We compare the four strategies described in Sec-
tions 3 and 4, summarized below:
EM HMM uniformly initialized, EM training.
EM+IP IP minimization using initial grammar
provided by EM.
EMGI HMM with grammar-informed initializa-
tion, EM training.
EMGI+IPGI IP minimization using initial gram-
mar/lexicon provided by EMGI and addi-
tional grammar-informed IP objective.
For EM+IP and EMGI+IPGI , the minimization
and EM training processes are iterated until the
resulting grammar and lexicon remain unchanged.
Forty EM iterations are used for all cases.
We also include a baseline which randomly
chooses a tag from those associated with each
word in the lexicon, averaged over three runs.
Accuracy on ambiguous word tokens. We
evaluate the performance in terms of tagging accu-
racy with respect to gold tags for ambiguous words
in held-out test sets for English and Italian. We
consider results with and without punctuation.7
Recall that unlike much previous work, we do
not collect the lexicon (tag dictionary) from the
test set: this means the model must handle un-
known words and the possibility of having missing
lexical entries for covering the test set.
Precision and recall of grammar and lexicon.
In addition to accuracy, we measure precision and
able/combinable bigrams.
7The reason for this is that the ?categories? for punctua-
tion in CCGbank are for the most part not actual categories;
for example, the period ?.? has the categories ?.? and ?S?.
As such, these supertags are outside of the categorial system:
their use in derivations requires phrase structure rules that are
not derivable from the CCG combinatory rules.
Model ambig ambig all all
-punc -punc
Random 17.9 16.2 27.4 21.9
EM 38.7 35.6 45.6 39.8
EM+IP 52.1 51.0 57.3 53.9
EMGI 56.3 59.4 61.0 61.7
EMGI+IPGI 59.6 62.3 63.8 64.3
Table 2: Supertagging accuracy for CCGbank sec-
tions 22-24. Accuracies are reported for four
settings?(1) ambiguous word tokens in the test
corpus, (2) ambiguous word tokens, ignoring
punctuation, (3) all word tokens, and (4) all word
tokens except punctuation.
recall for each model on the observed bitag gram-
mar and observed lexicon on the test set. We cal-
culate them as follows, for an observed grammar
or lexicon X:
Precision =
|{X} ? {Observedgold}|
|{X}|
Recall =
|{X} ? {Observedgold}|
|{Observedgold}|
This provides a measure of model performance on
bitag types for the grammar and lexical entry types
for the lexicon, rather than tokens.
5.1 English CCGbank results
Accuracy on ambiguous tokens. Table 2 gives
performance on the CCGbank test sections. All
models are well above the random baseline, and
both of the strategies individually boost perfor-
mance over basic EM by a large margin. For the
models using GI, accuracy ignoring punctuation is
higher than for all almost entirely due to the fact
that ?.? has the supertags ?.? and S, and the GI
gives a preference to S since it can in fact combine
with other categories, unlike ?.??the effect is that
nearly every sentence-final period (?5.5k tokens) is
tagged S rather than ?.?.
EMGI is more effective than EM+IP; however,
it should be kept in mind that IP-minimization
is a general technique that can be applied to
any sequence prediction task, whereas grammar-
informed initialization may be used only with
tasks in which the interactions of adjacent labels
may be derived from the labels themselves. In-
terestingly, the gap between the two approaches
is greater when punctuation is ignored (51.0 vs.
59.4)?this is unsurprising because, as noted al-
ready, punctuation supertags are not actual cate-
500
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 7.5 32.9 52.6 68.1
Recall 26.9 13.2 34.0 19.8
Lexicon
Precision 58.4 63.0 78.0 80.6
Recall 50.9 56.0 71.5 67.6
Table 3: Comparison of grammar/lexicon ob-
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCGbank data.
gories, so EMGI is unable to model their distribu-
tion. Most importantly, the complementary effects
of the two approaches can be seen in the improved
results for EMGI+IPGI , which obtains about 3%
better accuracy than EMGI .
Accuracy on all tokens. Table 2 also gives per-
formance when taking all tokens into account. The
HMM when using full supervision obtains 87.6%
accuracy (Baldridge, 2008),8 so the accuracy of
63.8% achieved by EMGI+IPGI nearly halves the
gap between the supervised model and the 45.6%
obtained by basic EM semi-supervised model.
Effect of GI information in EM and/or IP-
minimization stages. We can also consider the
effect of GI information in either EM training or
IP-minimization to see whether it can be effec-
tively exploited in both. The latter, EM+IPGI ,
obtains 53.2/51.1 for all/no-punc?a small gain
compared to EM+IP?s 52.1/51.0. The former,
EMGI+IP, obtains 58.9/61.6?a much larger gain.
Thus, the better starting point provided by EMGI
has more impact than the integer program that in-
cludes GI in its objective function. However, we
note that it should be possible to exploit the GI
information more effectively in the integer pro-
gram than we have here. Also, our best model,
EMGI+IPGI , uses GI information in both stages
to obtain our best accuracy of 59.6/62.3.
P/R for grammars and lexicons. We can ob-
tain a more-fine grained understanding of how the
models differ by considering the precision and re-
call values for the grammars and lexicons of the
different models, given in Table 3. The basic EM
model has very low precision for the grammar, in-
dicating it proposes many unnecessary bitags; it
8A state-of-the-art, fully-supervised maximum entropy
tagger (Clark and Curran, 2007) (which also uses part-of-
speech labels) obtains 91.4% on the same train/test split.
achieves better recall because of the sheer num-
ber of bitags it proposes (12,363). EM+IP prunes
that set of bitags considerably, leading to better
precision at the cost of recall. EMGI ?s higher re-
call and precision indicate the tag transition dis-
tributions do capture general patterns of linkage
between adjacent CCG categories, while EM en-
sures that the data filters out combinable, but un-
necessary, bitags. With EMGI+IPGI , we again
see that IP-minimization prunes even more entries,
improving precision at the loss of some recall.
Similar trends are seen for precision and recall
on the lexicon. IP-minimization?s pruning of inap-
propriate taggings means more common words are
not assigned highly infrequent supertags (boosting
precision) while unknown words are generally as-
signed more sensible supertags (boosting recall).
EMGI again focuses taggings on combinable con-
texts, boosting precision and recall similarly to
EM+IP, but in greater measure. EMGI+IPGI then
prunes some of the spurious entries, boosting pre-
cision at some loss of recall.
Tag frequencies predicted on the test set. Ta-
ble 4 compares gold tags to tags generated by
all four methods for the frequent and highly am-
biguous words the and in. Basic EM wanders
far away from the gold assignments; it has little
guidance in the very large search space available
to it. IP-minimization identifies a smaller set of
tags that better matches the gold tags; this emerges
because other determiners and prepositions evoke
similar, but not identical, supertags, and the gram-
mar minimization pushes (but does not force)
them to rely on the same supertags wherever pos-
sible. However, the proportions are incorrect;
for example, the tag assigned most frequently to
in is ((S\NP)\(S\NP))/NP though (NP\NP)/NP
is more frequent in the test set. EMGI ?s tags
correct that balance and find better proportions,
but also some less common categories, such as
(((N/N)\(N/N))\((N/N)\(N/N)))/N, sneak in be-
cause they combine with frequent categories like
N/N and N. Bringing the two strategies together
with EMGI+IPGI filters out the unwanted cate-
gories while getting better overall proportions.
5.2 Italian CCG-TUT results
To demonstrate that both methods and their com-
bination are language independent, we apply them
to the Italian CCG-TUT corpus. We wanted
to evaluate performance out-of-the-box because
501
Lexicon Gold EM EM+IP EMGI EMGI+IPGI
the? (41 distinct tags in Ltrain) (14 tags) (18 tags) (9 tags) (25 tags) (12 tags)
NP[nb]/N 5742 0 4544 4176 4666
((S\NP)\(S\NP))/N 14 5 642 122 107
(((N/N)\(N/N))\((N/N)\(N/N)))/N 0 0 0 698 0
((S/S)/S[dcl])/(S[adj]\NP) 0 733 0 0 0
PP/N 0 1755 0 3 1
: : : : : :
in? (76 distinct tags in Ltrain) (35 tags) (20 tags) (17 tags) (37 tags) (14 tags)
(NP\NP)/NP 883 0 649 708 904
((S\NP)\(S\NP))/NP 793 0 911 320 424
PP/NP 177 1 33 12 82
((S[adj]\NP)/(S[adj]\NP))/NP 0 215 0 0 0
: : : : : :
Table 4: Comparison of tag assignments from the gold tags versus model tags obtained on the test set.
The table shows tag assignments (and their counts for each method) for the and in in the CCGbank test
sections. The number of distinct tags assigned by each method is given in parentheses. Ltrain is the
lexicon obtained from sections 0-18 of CCGbank that is used as the basis for EM training.
Model TEST 1 TEST 2 (using lexicon from:)
NPAPER+CIVIL NPAPER CIVIL
Random 9.6 9.7 8.4 9.6
EM 26.4 26.8 27.2 29.3
EM+IP 34.8 32.4 34.8 34.6
EMGI 43.1 43.9 44.0 40.3
EMGI+IPGI 45.8 43.6 47.5 40.9
Table 5: Comparison of supertagging results for
CCG-TUT. Accuracies are for ambiguous word
tokens in the test corpus, ignoring punctuation.
bootstrapping a supertagger for a new language is
one of the main use scenarios we envision: in such
a scenario, there is no development data for chang-
ing settings and parameters. Thus, we determined
a train/test split beforehand and ran the methods
exactly as we had for CCGbank.
The results, given in Table 5, demonstrate the
same trends as for English: basic EM is far more
accurate than random, EM+IP adds another 8-10%
absolute accuracy, and EMGI adds an additional 8-
10% again. The combination of the methods gen-
erally improves over EMGI , except when the lex-
icon is extracted from NPAPER+CIVIL. Table 6
gives precision and recall for the grammars and
lexicons for CCG-TUT?the values are lower than
for CCGbank (in line with the lower baseline), but
exhibit the same trends.
6 Conclusion
We have shown how two complementary
strategies?grammar-informed tag transitions and
IP-minimization?for learning of supertaggers
from highly ambiguous lexicons can be straight-
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 23.1 26.4 44.9 46.7
Recall 18.4 15.9 24.9 22.7
Lexicon
Precision 51.2 52.0 54.8 55.1
Recall 43.6 42.8 46.0 44.9
Table 6: Comparison of grammar/lexicon ob-
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCG-TUT.
forwardly integrated. We verify the benefits of
both cross-lingually, on English and Italian data.
We also provide a new two-stage integer program-
ming setup that allows model minimization to be
tractable for supertagging without sacrificing the
quality of the search for minimal bitag grammars.
The experiments in this paper use large lexi-
cons, but the methodology will be particularly use-
ful in the context of bootstrapping from smaller
ones. This brings further challenges; in particular,
it will be necessary to identify novel entries con-
sisting of seen word and seen category and to pre-
dict unseen, but valid, categories which are needed
to explain the data. For this, it will be necessary
to forgo the assumption that the provided lexicon
is always obeyed. The methods we introduce here
should help maintain good accuracy while open-
ing up these degrees of freedom. Because the lexi-
con is the grammar in CCG, learning new word-
category associations is grammar generalization
and is of interest for grammar acquisition.
502
Finally, such lexicon refinement and generaliza-
tion is directly relevant for using CCG in syntax-
based machine translation models (Hassan et al,
2009). Such models are currently limited to lan-
guages for which corpora annotated with CCG
derivations are available. Clark and Curran (2006)
show that CCG parsers can be learned from sen-
tences labeled with just supertags?without full
derivations?with little loss in accuracy. The im-
provements we show here for learning supertag-
gers from lexicons without labeled data may be
able to help create annotated resources more ef-
ficiently, or enable CCG parsers to be learned with
less human-coded knowledge.
Acknowledgements
The authors would like to thank Johan Bos, Joey
Frazee, Taesun Moon, the members of the UT-
NLL reading group, and the anonymous review-
ers. Ravi and Knight acknowledge the support
of the NSF (grant IIS-0904684) for this work.
Baldridge acknowledges the support of a grant
from the Morris Memorial Trust Fund of the New
York Community Trust.
References
J. Baldridge. 2008. Weakly supervised supertagging
with grammar-informed initialization. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 57?64,
Manchester, UK, August.
M. Banko and R. C. Moore. 2004. Part of speech
tagging in context. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), page 556, Morristown, NJ, USA.
A. R. Barron, J. Rissanen, and B. Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 27?38, Milan, Italy.
S. Clark and J. Curran. 2006. Partial training for
a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 144?151, New
York City, USA, June.
S. Clark and J. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4).
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACLWork-
shop on Morphological and Phonological Learning,
pages 21?30, Morristown, NJ, USA.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL, pages 746?
754, Columbus, Ohio, June.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL, pages 744?751,
Prague, Czech Republic, June.
H. Hassan, K. Sima?an, and A. Way. 2009. A syntac-
tified direct translation model with linear-time de-
coding. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1182?1191, Singapore, August.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
A. Joshi. 1988. Tree Adjoining Grammars. In David
Dowty, Lauri Karttunen, and Arnold Zwicky, ed-
itors, Natural Language Parsing, pages 206?250.
Cambridge University Press, Cambridge.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2).
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
C. Pollard and I. Sag. 1994. Head Driven Phrase
Structure Grammar. CSLI/Chicago University
Press, Chicago.
S. Ravi and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 504?512, Suntec, Singapore, August.
M. Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of the Ad-
vances in Neural Information Processing Systems
(NIPS), pages 1521?1528, Cambridge, MA. MIT
Press.
503
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1048?1057,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Statistical Model for Lost Language Decipherment
Benjamin Snyder and Regina Barzilay
CSAIL
Massachusetts Institute of Technology
{bsnyder,regina}@csail.mit.edu
Kevin Knight
ISI
University of Southern California
knight@isi.edu
Abstract
In this paper we propose a method for the
automatic decipherment of lost languages.
Given a non-parallel corpus in a known re-
lated language, our model produces both
alphabetic mappings and translations of
words into their corresponding cognates.
We employ a non-parametric Bayesian
framework to simultaneously capture both
low-level character mappings and high-
level morphemic correspondences. This
formulation enables us to encode some of
the linguistic intuitions that have guided
human decipherers. When applied to
the ancient Semitic language Ugaritic, the
model correctly maps 29 of 30 letters to
their Hebrew counterparts, and deduces
the correct Hebrew cognate for 60% of
the Ugaritic words which have cognates in
Hebrew.
1 Introduction
Dozens of lost languages have been deciphered
by humans in the last two centuries. In each
case, the decipherment has been considered a ma-
jor intellectual breakthrough, often the culmina-
tion of decades of scholarly efforts. Computers
have played no role in the decipherment any of
these languages. In fact, skeptics argue that com-
puters do not possess the ?logic and intuition? re-
quired to unravel the mysteries of ancient scripts.1
In this paper, we demonstrate that at least some of
this logic and intuition can be successfully mod-
eled, allowing computational tools to be used in
the decipherment process.
1?Successful archaeological decipherment has turned out
to require a synthesis of logic and intuition . . . that comput-
ers do not (and presumably cannot) possess.? A. Robinson,
?Lost Languages: The Enigma of the World?s Undeciphered
Scripts? (2002)
Our definition of the computational decipher-
ment task closely follows the setup typically faced
by human decipherers (Robinson, 2002). Our in-
put consists of texts in a lost language and a corpus
of non-parallel data in a known related language.
The decipherment itself involves two related sub-
tasks: (i) finding the mapping between alphabets
of the known and lost languages, and (ii) translat-
ing words in the lost language into corresponding
cognates of the known language.
While there is no single formula that human de-
cipherers have employed, manual efforts have fo-
cused on several guiding principles. A common
starting point is to compare letter and word fre-
quencies between the lost and known languages.
In the presence of cognates the correct mapping
between the languages will reveal similarities in
frequency, both at the character and lexical level.
In addition, morphological analysis plays a cru-
cial role here, as highly frequent morpheme cor-
respondences can be particularly revealing. In
fact, these three strands of analysis (character fre-
quency, morphology, and lexical frequency) are
intertwined throughout the human decipherment
process. Partial knowledge of each drives discov-
ery in the others.
We capture these intuitions in a generative
Bayesian model. This model assumes that each
word in the lost language is composed of mor-
phemes which were generated with latent coun-
terparts in the known language. We model bilin-
gual morpheme pairs as arising through a series
of Dirichlet processes. This allows us to assign
probabilities based both on character-level corre-
spondences (using a character-edit base distribu-
tion) as well as higher-level morpheme correspon-
dences. In addition, our model carries out an im-
plicit morphological analysis of the lost language,
utilizing the known morphological structure of the
related language. This model structure allows us
to capture the interplay between the character-
1048
and morpheme-level correspondences that humans
have used in the manual decipherment process.
In addition, we introduce a novel technique
for imposing structural sparsity constraints on
character-level mappings. We assume that an ac-
curate alphabetic mapping between related lan-
guages will be sparse in the following way: each
letter will map to a very limited subset of letters
in the other language. We capture this intuition
by adapting the so-called ?spike and slab? prior to
the Dirichlet-multinomial setting. For each pair
of characters in the two languages, we posit an
indicator variable which controls the prior likeli-
hood of character substitutions. We define a joint
prior over these indicator variables which encour-
ages sparse settings.
We applied our model to a corpus of Ugaritic,
an ancient Semitic language discovered in 1928.
Ugaritic was manually deciphered in 1932, us-
ing knowledge of Hebrew, a related language.
We compare our method against the only existing
decipherment baseline, an HMM-based character
substitution cipher (Knight and Yamada, 1999;
Knight et al, 2006). The baseline correctly maps
the majority of letters ? 22 out of 30 ? to their
correct Hebrew counterparts, but only correctly
translates 29% of all cognates. In comparison, our
method yields correct mappings for 29 of 30 let-
ters, and correctly translates 60.4% of all cognates.
2 Related Work
Our work on decipherment has connections to
three lines of work in statistical NLP. First, our
work relates to research on cognate identifica-
tion (Lowe and Mazaudon, 1994; Guy, 1994;
Kondrak, 2001; Bouchard et al, 2007; Kondrak,
2009). These methods typically rely on informa-
tion that is unknown in a typical deciphering sce-
nario (while being readily available for living lan-
guages). For instance, some methods employ a
hand-coded similarity function (Kondrak, 2001),
while others assume knowledge of the phonetic
mapping or require parallel cognate pairs to learn
a similarity function (Bouchard et al, 2007).
A second related line of work is lexicon in-
duction from non-parallel corpora. While this
research has similar goals, it typically builds on
information or resources unavailable for ancient
texts, such as comparable corpora, a seed lexi-
con, and cognate information (Fung and McKe-
own, 1997; Rapp, 1999; Koehn and Knight, 2002;
Haghighi et al, 2008). Moreover, distributional
methods that rely on co-occurrence analysis oper-
ate over large corpora, which are typically unavail-
able for a lost language.
Finally, Knight and Yamada (1999) and Knight
et al (2006) describe a computational HMM-
based method for deciphering an unknown script
that represents a known spoken language. This
method ?makes the text speak? by gleaning
character-to-sound mappings from non-parallel
character and sound sequences. It does not relate
words in different languages, thus it cannot encode
deciphering constraints similar to the ones consid-
ered in this paper. More importantly, this method
had not been applied to archaeological data. While
lost languages are gaining increasing interest in
the NLP community (Knight and Sproat, 2009),
there have been no successful attempts of their au-
tomatic decipherment.
3 Background on Ugaritic
Manual Decipherment of Ugaritic Ugaritic
tablets were first found in Syria in 1929 (Smith,
1955; Watson and Wyatt, 1999). At the time, the
cuneiform writing on the tablets was of an un-
known type. Charles Virolleaud, who lead the ini-
tial decipherment effort, recognized that the script
was likely alphabetic, since the inscribed words
consisted of only thirty distinct symbols. The lo-
cation of the tablets discovery further suggested
that Ugaritic was likely to have been a Semitic
language from the Western branch, with proper-
ties similar to Hebrew and Aramaic. This real-
ization was crucial for deciphering the Ugaritic
script. In fact, German cryptographer and Semitic
scholar Hans Bauer decoded the first two Ugaritic
letters?mem and lambda?by mapping them to
Hebrew letters with similar occurrence patterns
in prefixes and suffixes. Bootstrapping from this
finding, Bauer found words in the tablets that were
likely to serve as cognates to Hebrew words?
e.g., the Ugaritic word for king matches its He-
brew equivalent. Through this process a few
more letters were decoded, but the Ugaritic texts
were still unreadable. What made the final deci-
pherment possible was a sheer stroke of luck?
Bauer guessed that a word inscribed on an ax dis-
covered in the Ras Shamra excavations was the
Ugaritic word for ax. Bauer?s guess was cor-
rect, though he selected the wrong phonetic se-
quence. Edouard Dhorme, another cryptographer
1049
and Semitic scholar, later corrected the reading,
expanding a set of translated words. Discoveries
of additional tablets allowed Bauer, Dhorme and
Virolleaud to revise their hypothesis, successfully
completing the decipherment.
Linguistic Features of Ugaritic Ugaritic
shares many features with other ancient Semitic
languages, following the same word order, gender,
number, and case structure (Hetzron, 1997). It is a
morphologically rich language, with triliteral roots
and many prefixes and suffixes.
At the same time, it exhibits a number of fea-
tures that distinguish it from Hebrew. Ugaritic has
a bigger phonemic inventory than Hebrew, yield-
ing a bigger alphabet ? 30 letters vs. 22 in He-
brew. Another distinguishing feature of Ugaritic
is that vowels are only written with glottal stops
while in Hebrew many long vowels are written us-
ing homorganic consonants. Ugaritic also does not
have articles, while Hebrew nouns and adjectives
take definite articles which are realized as prefixes.
These differences result in significant divergence
between Hebrew and Ugaritic cognates, thereby
complicating the decipherment process.
4 Problem Formulation
We are given a corpus in a lost language and a non-
parallel corpus in a related language from the same
language family. Our primary goal is to translate
words in the unknown language by mapping them
to cognates in the known language. As part of this
process, we induce a lower-level mapping between
the letters of the two alphabets, capturing the reg-
ular phonetic correspondences found in cognates.
We make several assumptions about the writ-
ing system of the lost language. First, we assume
that the writing system is alphabetic in nature. In
general, this assumption can be easily validated by
counting the number of symbols found in the writ-
ten record. Next, we assume that the corpus has
been transcribed into electronic format, where the
graphemes present in the physical text have been
unambiguously identified. Finally, we assume that
words are explicitly separated in the text, either by
white space or a special symbol.
We also make a mild assumption about the mor-
phology of the lost language. We posit that each
word consists of a stem, prefix, and suffix, where
the latter two may be omitted. This assumption
captures a wide range of human languages and a
variety of morphological systems. While the cor-
rect morphological analysis of words in the lost
language must be learned, we assume that the in-
ventory and frequencies of prefixes and suffixes in
the known language are given.
In summary, the observed input to the model
consists of two elements: (i) a list of unanalyzed
word types derived from a corpus in the lost lan-
guage, and (ii) a morphologically analyzed lexicon
in a known related language derived from a sepa-
rate corpus, in our case non-parallel.
5 Model
5.1 Intuitions
Our goal is to incorporate the logic and intuition
used by human decipherers in an unsupervised sta-
tistical model. To make these intuitions concrete,
consider the following toy example, consisting of
a lost language much like English, but written us-
ing numerals:
? 15234 (asked)
? 1525 (asks)
? 4352 (desk)
Analyzing the undeciphered corpus, we might first
notice a pair of endings, -34, and -5, which both
occur after the initial sequence 152- (and may like-
wise occur at the end of a variety of words in
the corpus). If we know this lost language to be
closely related to English, we can surmise that
these two endings correspond to the English ver-
bal suffixes -ed and -s. Using this knowledge,
we can hypothesize the following character corre-
spondences: (3 = e), (4 = d), (5 = s). We now know
that (4252 = des2) and we can use our knowl-
edge of the English lexicon to hypothesize that this
word is desk, thereby learning the correspondence
(2 = k). Finally, we can use similar reasoning to
reveal that the initial character sequence 152- cor-
responds to the English verb ask.
As this example illustrates, human deci-
pherment efforts proceed by discovering both
character-level and morpheme-level correspon-
dences. This interplay implicitly relies on a
morphological analysis of words in the lost lan-
guage, while utilizing knowledge of the known
language?s lexicon and morphology.
One final intuition our model should capture is
the sparsity of the alphabetic correspondence be-
tween related languages. We know from compar-
ative linguistics that the correct mapping will pre-
1050
serve regular phonetic relationships between the
two languages (as exemplified by cognates). As a
result, each character in one language will map to
a small number of characters in the other language
(typically one, but sometimes two or three). By
incorporating this structural sparsity intuition, we
can allow the model to focus on on a smaller set of
linguistically valid hypotheses.
Below we give an overview of our model, which
is designed to capture these linguistic intuitions.
5.2 Model Structure
Our model posits that every observed word in the
lost language is composed of a sequence of mor-
phemes (prefix, stem, suffix). Furthermore we
posit that each morpheme was probabilistically
generated jointly with a latent counterpart in the
known language.
Our goal is to find those counterparts that lead to
high frequency correspondences both at the char-
acter and morpheme level. The technical chal-
lenge is that each level of correspondence (char-
acter and morpheme) can completely describe the
observed data. A probabilistic mechanism based
simply on one leaves no room for the other to play
a role. We resolve this tension by employing a
non-parametric Bayesian model: the distributions
over bilingual morpheme pairs assign probabil-
ity based on recurrent patterns at the morpheme
level. These distributions are themselves drawn
from a prior probabilistic process which favors
distributions with consistent character-level corre-
spondences.
We now give a formal description of the model
(see Figure 1 for a graphical overview). There are
four basic layers in the generative process:
1. Structural sparsity: draw a set of indicator
variables ?? corresponding to character-edit
operations.
2. Character-edit distribution: draw a base
distribution G0 parameterized by weights on
character-edit operations.
3. Morpheme-pair distributions: draw a set
of distributions on bilingual morpheme pairs
Gstm, Gpre|stm, Gsuf |stm.
4. Word generation: draw pairs of cognates
in the lost and known language, as well as
words in the lost language with no cognate
counterpart.
G0
word
G stm
ustm
hstm
u pr e
hpre
u suf
h su f
stm stm
Gsuf |stmGpre|stm
!v!?
Figure 1: Plate diagram of the decipherment
model. The structural sparsity indicator variables
?? determine the values of the base distribution hy-
perparameters v?. The base distribution G0 de-
fines probabilities over string-pairs based solely on
character-level edits. The morpheme-pair distri-
butions Gstm, Gpre|stm, Gsuf |stm directly assign
probabilities to highly frequent morpheme pairs.
We now go through each step in more detail.
Structural Sparsity The first step of the genera-
tive process provides a control on the sparsity of
edit-operation probabilities, encoding the linguis-
tic intuition that the correct character-level map-
pings should be sparse. The set of edit opera-
tions includes character substitutions, insertions,
and deletions, as well as a special end sym-
bol: {(u, h), (?, h), (u, ?), END} (where u and h
range over characters in the lost and known lan-
guages, respectively). For each edit operation ewe
posit a corresponding indicator variable ?e. The
set of character substitutions with indicators set to
one, {(u, h) : ?(u,h) = 1}) conveys the set of
phonetically valid correspondences. We define a
joint prior over these variables to encourage sparse
character mappings. This prior can be viewed as a
distribution over binary matrices and is defined to
encourage rows and columns to sum to low integer
values (typically 1). More precisely, for each char-
acter u in the lost language, we count the number
of mappings c(u) =
?
h ?(u,h). We then define
a set of features which count how many of these
characters map to i other characters beyond some
budget bi: fi = max (0, |{u : c(u) = i}| ? bi).
Likewise, we define corresponding features f ?i and
budgets b?i for the characters h in the known lan-
1051
guage. The prior over ?? is then defined as
P (??) =
exp
(
f? ? w? + f? ? ? w?
)
Z
(1)
where the feature weight vector w? is set to encour-
age sparse mappings, and Z is a corresponding
normalizing constant, which we never need com-
pute. We set w? so that each character must map to
at least one other character, and so that mappings
to more than one other character are discouraged 2
Character-edit Distribution The next step in
the generative process is drawing a base distri-
bution G0 over character edit sequences (each of
which yields a bilingual pair of morphemes). This
distribution is parameterized by a set of weights ??
on edit operations, where the weights over substi-
tutions, insertions, and deletions each individually
sum to one. In addition, G0 provides a fixed dis-
tribution q over the number of insertions and dele-
tions occurring in any single edit sequence. Prob-
abilities over edit sequences (and consequently on
bilingual morpheme pairs) are then defined ac-
cording to G0 as:
P (e?) =
?
i
?ei ? q (#ins(e?),#del(e?))
We observe that the average Ugaritic word is over
two letters longer than the average Hebrew word.
Thus, occurrences of Hebrew character insertions
are a priori likely, and Ugaritic character deletions
are very unlikely. In our experiments, we set q
to disallow Ugaritic deletions, and to allow one
Hebrew insertion per morpheme (with probability
0.4).
The prior on the base distribution G0 is a
Dirichlet distribution with hyperparameters v?, i.e.,
?? ? Dirichlet(v?). Each value ve thus corre-
sponds to a character edit operation e. Crucially,
the value of each ve depends deterministically on
its corresponding indicator variable:
ve =
{
1 if ?e = 0,
K if ?e = 1.
where K is some constant value > 1.3 The overall
effect is that when ?e = 0, the marginal prior den-
sity of the corresponding edit weight ?e spikes at
2We set w0 = ??, w1 = 0, w2 = ?50, w>2 = ??,
with budgets b?2 = 7, b?3 = 1 (otherwise zero), reflecting the
knowledge that there are eight more Ugaritic than Hebrew
letters.
3Set to 50 in our experiments.
0. When ?e = 1, the corresponding marginal prior
density remains relatively flat and unconstrained.
See (Ishwaran and Rao, 2005) for a similar appli-
cation of ?spike-and-slab? priors in the regression
scenario.
Morpheme-pair Distributions Next we draw a
series of distributions which directly assign prob-
ability to morpheme pairs. The previously drawn
base distribution G0 along with a fixed concentra-
tion parameter ? define a Dirichlet process (An-
toniak, 1974): DP (G0, ?), which provides prob-
abilities over morpheme-pair distributions. The
resulting distributions are likely to be skewed in
favor of a few frequently occurring morpheme-
pairs, while remaining sensitive to the character-
level probabilities of the base distribution.
Our model distinguishes between three types of
morphemes: prefixes, stems, and suffixes. As a
result, we model each morpheme type as arising
from distinct Dirichlet processes, that share a sin-
gle base distribution:
Gstm ? DP (G0, ?stm)
Gpre|stm ? DP (G0, ?pre)
Gsuf |stm ? DP (G0, ?suf )
We model prefix and suffix distributions as con-
ditionally dependent on the part-of-speech of the
stem morpheme-pair. This choice capture the lin-
guistic fact that different parts-of-speech bear dis-
tinct affix frequencies. Thus, while we draw a sin-
gle distribution Gstm, we maintain separate distri-
butions Gpre|stm and Gsuf |stm for each possible
stem part-of-speech.
Word Generation Once the morpheme-pair
distributions have been drawn, actual word pairs
may now be generated. First the model draws a
boolean variable ci to determine whether word i in
the lost language has a cognate in the known lan-
guage, according to some prior P (ci). If ci = 1,
then a cognate word pair (u, h) is produced:
(ustm, hstm) ? Gstm
(upre, hpre) ? Gpre|stm
(usuf , hsuf ) ? Gsuf |stm
u = upreustmusuf
h = hprehstmhsuf
Otherwise, a lone word u is generated, according
a uniform character-level language model.
1052
In summary, this model structure captures both
character and lexical level correspondences, while
utilizing morphological knowledge of the known
language. An additional feature of this multi-
layered model structure is that each distribution
over morpheme pairs is derived from the single
character-level base distribution G0. As a re-
sult, any character-level mappings learned from
one type of morphological correspondence will be
propagated to all other morpheme distributions.
Finally, the character-level mappings discovered
by the model are encouraged to obey linguistically
motivated structural sparsity constraints.
6 Inference
For each word ui in our undeciphered lan-
guage we predict a morphological segmentation
(upreustmusuf )i and corresponding cognate in the
known language (hprehstmhsuf )i. Ideally we
would like to predict the analysis with highest
marginal probability under our model given the
observed undeciphered corpus and related lan-
guage lexicon. In order to do so, we need to
integrate out all the other latent variables in our
model. As these integrals are intractable to com-
pute exactly, we resort to the standardMonte Carlo
approximation. We collect samples of the vari-
ables over which we wish to marginalize but for
which we cannot compute closed-form integrals.
We then approximate the marginal probabilities
for undeciphered word ui by summing over all the
samples, and predicting the analysis with highest
probability.
In our sampling algorithm, we avoid sam-
pling the base distribution G0 and the derived
morpheme-pair distributions (Gstm etc.), instead
using analytical closed forms. We explicitly sam-
ple the sparsity indicator variables ??, the cognate
indicator variables ci, and latent word analyses
(segmentations and Hebrew counterparts). To do
so tractably, we use Gibbs sampling to draw each
latent variable conditioned on our current sample
of the others. Although the samples are no longer
independent, they form a Markov chain whose sta-
tionary distribution is the true joint distribution de-
fined by the model (Geman and Geman, 1984).
6.1 Sampling Word Analyses
For each undeciphered word, we need to sample
a morphological segmentation (upre, ustm, usuf )i
along with latent morphemes in the known lan-
guage (hpre, hstm, hsuf )i. More precisely, we
need to sample three character-edit sequences
e?pre, e?stm, e?suf which together yield the observed
word ui.
We break this into two sampling steps. First
we sample the morphological segmentation of ui,
along with the part-of-speech pos of the latent
stem cognate. To do so, we enumerate each pos-
sible segmentation and part-of-speech and calcu-
late its joint conditional probability (for notational
clarity, we leave implicit the conditioning on the
other samples in the corpus):
P (upre, ustm, usuf , pos) =
?
e?stm
P (e?stm)
?
e?pre
P (e?pre|pos)
?
e?suf
P (e?suf |pos)
(2)
where the summations over character-edit se-
quences are restricted to those which yield the seg-
mentation (upre, ustm, usuf ) and a latent cognate
with part-of-speech pos.
For a particular stem edit-sequence e?stm, we
compute its conditional probability in closed form
according to a Chinese Restaurant Process (An-
toniak, 1974). To do so, we use counts from
the other sampled word analyses: countstm(e?stm)
gives the number of times that the entire edit-
sequence e?stm has been observed:
P (e?stm) ?
countstm(e?stm) + ?
?
i p(ei)
n + ?
where n is the number of other word analyses sam-
pled, and ? is a fixed concentration parameter. The
product
?
i p(ei) gives the probability of e?stm ac-
cording to the base distribution G0. Since the
parameters of G0 are left unsampled, we use the
marginalized form:
p(e) = ve + count(e)?
e? ve? + k
(3)
where count(e) is the number of times that
character-edit e appears in distinct edit-sequences
(across prefixes, stems, and suffixes), and k is the
sum of these counts across all character-edits. Re-
call that ve is a hyperparameter for the Dirichlet
prior on G0 and depends on the value of the corre-
sponding indicator variable ?e.
Once the segmentation (upre, ustm, usuf ) and
part-of-speech pos have been sampled, we pro-
ceed to sample the actual edit-sequences (and thus
1053
latent morphemes counterparts). Now, instead of
summing over the values in Equation 2, we instead
sample from them.
6.2 Sampling Sparsity Indicators
Recall that each sparsity indicator ?e determines
the value of the corresponding hyperparameter ve
of the Dirichlet prior for the character-edit base
distributionG0. In addition, we have an unnormal-
ized joint prior P (??) = g(??)Z which encourages a
sparse setting of these variables. To sample a par-
ticular ?e, we consider the set ?? in which ?e = 0
and ??? in which ?e = 1. We then compute:
P (??) ? g(??) ? v
[count(e)]
e
?
e? v
[k]
e?
where k is the sum of counts for all edit opera-
tions, and the notation a[b] indicates the ascending
factorial. Likewise, we can compute a probability
for ??? with corresponding values v?e.
6.3 Sampling Cognate Indicators
Finally, for each word ui, we sample a correspond-
ing indicator variable ci. To do so, we calcu-
late Equation 2 for all possible segmentations and
parts-of-speech and sum the resulting values to ob-
tain the conditional likelihood P (ui|ci = 1). We
also calculate P (ui|ci = 0) using a uniform uni-
gram character-level language model (and thus de-
pends only on the number of characters in ui). We
then sample from among the two values:
P (ui|ci = 1) ? P (ci = 1)
P (ui|ci = 0) ? P (ci = 0)
6.4 High-level Resampling
Besides the individual sampling steps detailed
above, we also consider several larger sampling
moves in order to speed convergence. For exam-
ple, for each type of edit-sequence e? which has
been sampled (and may now occur many times
throughout the data), we consider a single joint
move to another edit-sequence e?? (both of which
yield the same lost language morpheme u). The
details are much the same as above, and as before
the set of possible edit-sequences is limited by the
string u and the known language lexicon.
We also resample groups of the sparsity indica-
tor variables ?? in tandem, to allow a more rapid ex-
ploration of the probability space. For each char-
acter u, we block sample the entire set {?(u,h)}h,
and likewise for each character h.
6.5 Implementation Details
Many of the steps detailed above involve the con-
sideration of all possible edit-sequences consis-
tent with (i) a particular undeciphered word ui and
(ii) the entire lexicon of words in the known lan-
guage (or some subset of words with a particu-
lar part-of-speech). In particular, we need to both
sample from and sum over this space of possibil-
ities repeatedly. Doing so by simple enumeration
would needlessly repeat many sub-computations.
Instead we use finite-state acceptors to compactly
represent both the entire Hebrew lexicon as well
as potential Hebrew word forms for each Ugaritic
word. By intersecting two such FSAs and mini-
mizing the result we can efficiently represent all
potential Hebrew words for a particular Ugaritic
word. We weight the edges in the FSA according
to the base distribution probabilities (in Equation 3
above). Although these intersected acceptors have
to be constantly reweighted to reflect changing
probabilities, their topologies need only be com-
puted once. One weighted correctly, marginals
and samples can be computed using dynamic pro-
gramming.
Even with a large number of sampling rounds, it
is difficult to fully explore the latent variable space
for complex unsupervised models. Thus a clever
initialization is usually required to start the sam-
pler in a high probability region. We initialize our
model with the results of the HMM-based baseline
(see section 8), and rule out character substitutions
with probability < 0.05 according to the baseline.
7 Experiments
7.1 Corpus and Annotations
We apply our model to the ancient Ugaritic lan-
guage (see Section 3 for background). Our un-
deciphered corpus consists of an electronic tran-
scription of the Ugaritic tablets (Cunchillos et al,
2002). This corpus contains 7,386 unique word
types. As our known language corpus, we use the
Hebrew Bible, which is both geographically and
temporally close to Ugaritic. To extract a Hebrew
morphological lexicon we assume the existence
of manual morphological and part-of-speech an-
notations (Groves and Lowery, 2006). We divide
Hebrew stems into four main part-of-speech cat-
egories each with a distinct affix profile: Noun,
Verb, Pronoun, and Particle. For each part-of-
speech category, we determine the set of allowable
affixes using the annotated Bible corpus.
1054
Words Morphemes
type token type token
Baseline 28.82% 46.00% N/A N/A
Our Model 60.42% 66.71% 75.07% 81.25%
No Sparsity 46.08% 54.01% 69.48% 76.10%
Table 1: Accuracy of cognate translations, mea-
sured with respect to complete word-forms and
morphemes, for the HMM-based substitution ci-
pher baseline, our complete model, and our model
without the structural sparsity priors. Note that the
baseline does not provide per-morpheme results,
as it does not predict morpheme boundaries.
To evaluate the output of our model, we anno-
tated the words in the Ugaritic lexicon with the
corresponding Hebrew cognates found in the stan-
dard reference dictionary (del Olo Lete and San-
mart??n, 2004). In addition, manual morphological
segmentation was carried out with the guidance of
a standard Ugaritic grammar (Schniedewind and
Hunt, 2007). Although Ugaritic is an inflectional
rather than agglutinative language, in its written
form (which lacks vowels) words can easily be
segmented (e.g. wyplt.n becomes wy-plt.-n).
Overall, we identified Hebrew cognates for
2,155 word forms, covering almost 1/3 of the
Ugaritic vocabulary.4
8 Evaluation Tasks and Results
We evaluate our model on four separate decipher-
ment tasks: (i) Learning alphabetic mappings,
(ii) translating cognates, (iii) identifying cognates,
and (iv) morphological segmentation.
As a baseline for the first three of these tasks
(learning alphabetic mappings and translating and
identifying cognates), we adapt the HMM-based
method of Knight et al (2006) for learning let-
ter substitution ciphers. In its original setting, this
model was used to map written texts to spoken lan-
guage, under the assumption that each character
was emitted from a hidden phonemic state. In our
adaptation, we assume instead that each Ugaritic
character was generated by a hidden Hebrew let-
ter. Hebrew character trigram transition probabili-
ties are estimated using the Hebrew Bible, and He-
brew to Ugaritic character emission probabilities
are learned using EM. Finally, the highest prob-
4We are confident that a large majority of Ugaritic words
with known Hebrew cognates were thus identified. The
remaining Ugaritic words include many personal and geo-
graphic names, words with cognates in other Semitic lan-
guages, and words whose etymology is uncertain.
ability sequence of latent Hebrew letters is pre-
dicted for each Ugaritic word-form, using Viterbi
decoding.
Alphabetic Mapping The first essential step to-
wards successful decipherment is recovering the
mapping between the symbols of the lost language
and the alphabet of a known language. As a gold
standard for this comparison, we use the well-
established relationship between the Ugaritic and
Hebrew alphabets (Hetzron, 1997). This mapping
is not one-to-one but is generally quite sparse. Of
the 30 Ugaritic symbols, 28 map predominantly
to a single Hebrew letter, and the remaining two
map to two different letters. As the Hebrew alpha-
bet contains only 22 letters, six map to two dis-
tinct Ugaritic letters and two map to three distinct
Ugaritic letters.
We recover our model?s predicted alphabetic
mappings by simply examining the sampled val-
ues of the binary indicator variables ?u,h for each
Ugaritic-Hebrew letter pair (u, h). Due to our
structural sparsity prior P (??), the predicted map-
pings are sparse: each Ugaritic letter maps to only
a single Hebrew letter, and most Hebrew letters
map to only a single Ugaritic letter. To recover
alphabetic mappings from the HMM substitution
cipher baseline, we predict the Hebrew letter h
which maximizes the model?s probability P (h|u),
for each Ugaritic letter u.
To evaluate these mappings, we simply count
the number of Ugaritic letters that are correctly
mapped to one of their Hebrew reflexes. By this
measure, the baseline recovers correct mappings
for 22 out of 30 Ugaritic characters (73.3%). Our
model recovers correct mappings for all but one
(very low frequency) Ugaritic characters, yielding
96.67% accuracy.
Cognate Decipherment We compare the deci-
pherment accuracy for Ugaritic words that have
corresponding Hebrew cognates. We evaluate
our model?s predictions on each distinct Ugaritic
word-form at both the type and token level. As
Table 1 shows, our method correctly translates
over 60% of all distinct Ugaritic word-forms with
Hebrew cognates and over 71% of the individ-
ual morphemes that compose them, outperform-
ing the baseline by significant margins. Accu-
racy improves when the frequency of the word-
forms is taken into account (token-level evalua-
tion), indicating that the model is able to deci-
pher frequent words more accurately than infre-
1055
0 0.2 0.4 0.6 0.8 1False positive rate0
0.2
0.4
0.6
0.8
1
True
 posi
tive r
ate
Our ModelBaselineRandom
Figure 2: ROC curve for cognate identification.
quent words. We also measure the average Leven-
shtein distance between predicted and actual cog-
nate word-forms. On average, our model?s pre-
dictions lie 0.52 edit operations from the true cog-
nate, whereas the baseline?s predictions average a
distance of 1.26 edit operations.
Finally, we evaluated the performance of our
model when the structural sparsity constraints are
not used. As Table 1 shows, performance degrades
significantly in the absence of these priors, indi-
cating the importance of modeling the sparsity of
character mappings.
Cognate identification We evaluate our
model?s ability to identify cognates using the
sampled indicator variables ci. As before, we
compare our performance against the HMM
substitution cipher baseline. To produce baseline
cognate identification predictions, we calculate
the probability of each latent Hebrew letter se-
quence predicted by the HMM, and compare it to
a uniform character-level Ugaritic language model
(as done by our model, to avoid automatically
assigning higher cognate probability to shorter
Ugaritic words). For both our model and the
baseline, we can vary the threshold for cognate
identification by raising or lowering the cognate
prior P (ci). As the prior is set higher, we detect
more true cognates, but the false positive rate
increases as well.
Figure 2 shows the ROC curve obtained by
varying this prior both for our model and the base-
line. At all operating points, our model outper-
forms the baseline, and both models always pre-
dict better than chance. In practice for our model,
we use a high cognate prior, thus only ruling out
precision recall f-measure
Morfessor 88.87% 67.48% 76.71%
Our Model 86.62% 90.53% 88.53%
Table 2: Morphological segmentation accuracy for
a standard unsupervised baseline and our model.
those Ugaritic word-forms which are very unlikely
to have Hebrew cognates.
Morphological segmentation Finally, we eval-
uate the accuracy of our model?s morphological
segmentation for Ugaritic words. As a baseline
for this comparison, we use Morfessor Categories-
MAP (Creutz and Lagus, 2007). As Table 2
shows, our model provides a significant boost in
performance, especially for recall. This result is
consistent with previous work showing that mor-
phological annotations can be projected to new
languages lacking annotation (Yarowsky et al,
2000; Snyder and Barzilay, 2008), but generalizes
those results to the case where parallel data is un-
available.
9 Conclusion and Future Work
In this paper we proposed a method for the au-
tomatic decipherment of lost languages. The key
strength of our model lies in its ability to incorpo-
rate a range of linguistic intuitions in a statistical
framework.
We hope to address several issues in future
work. Our model fails to take into account
the known frequency of Hebrew words and mor-
phemes. In fact, the most common error is incor-
rectly translating the masculine plural suffix (-m)
as the third person plural possessive suffix (-m)
rather than the correct and much more common
plural suffix (-ym). Also, even with the correct al-
phabetic mapping, many words can only be deci-
phered by examining their literary context. Our
model currently operates purely on the vocabulary
level and thus fails to take this contextual infor-
mation into account. Finally, we intend to explore
our model?s predictive power when the family of
the lost language is unknown.5
5The authors acknowledge the support of the NSF (CA-
REER grant IIS-0448168, grant IIS-0835445, and grant IIS-
0835652) and the Microsoft Research New Faculty Fellow-
ship. Thanks to Michael Collins, Tommi Jaakkola, and
the MIT NLP group for their suggestions and comments.
Any opinions, findings, conclusions, or recommendations ex-
pressed in this paper are those of the authors, and do not nec-
essarily reflect the views of the funding organizations.
1056
References
C. E. Antoniak. 1974. Mixtures of Dirichlet pro-
cesses with applications to bayesian nonparametric
problems. The Annals of Statistics, 2:1152?1174,
November.
Alexandre Bouchard, Percy Liang, Thomas Griffiths,
and Dan Klein. 2007. A probabilistic approach to
diachronic phonology. In Proceedings of EMNLP,
pages 887?896.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1).
Jesus-Luis Cunchillos, Juan-Pablo Vita, and Jose-
A?ngel Zamora. 2002. Ugaritic data bank. CD-
ROM.
Gregoria del Olo Lete and Joaqu??n Sanmart??n. 2004.
A Dictionary of the Ugaritic Language in the Alpha-
betic Tradition. Number 67 in Handbook of Oriental
Studies. Section 1 The Near and Middle East. Brill.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing terminology translations from non-parallel cor-
pora. In Proceedings of the Annual Workshop on
Very Large Corpora, pages 192?202.
S. Geman and D. Geman. 1984. Stochastic relaxation,
gibbs distributions and the bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12:609?628.
Alan Groves and Kirk Lowery, editors. 2006. The
Westminster Hebrew Bible Morphology Database.
Westminster Hebrew Institute, Philadelphia, PA,
USA.
Jacques B. M. Guy. 1994. An algorithm for identifying
cognates in bilingual wordlists and its applicability
to machine translation. Journal of Quantitative Lin-
guistics, 1(1):35?42.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the
ACL/HLT, pages 771?779.
Robert Hetzron, editor. 1997. The Semitic Languages.
Routledge.
H. Ishwaran and J.S. Rao. 2005. Spike and slab vari-
able selection: frequentist and Bayesian strategies.
The Annals of Statistics, 33(2):730?773.
Kevin Knight and Richard Sproat. 2009. Writing sys-
tems, transliteration and decipherment. NAACL Tu-
torial.
K. Knight and K. Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
ACL Workshop on Unsupervised Learning in Natu-
ral Language Processing.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for deci-
pherment problems. In Proceedings of the COL-
ING/ACL, pages 499?506.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 workshop on Unsuper-
vised lexical acquisition, pages 9?16.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In Proceeding of
NAACL, pages 1?8.
Grzegorz Kondrak. 2009. Identification of cognates
and recurrent sound correspondences in word lists.
Traitement Automatique des Langues, 50(2):201?
235.
John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguis-
tics, 20(3):381?417.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the ACL, pages 519?526.
Andrew Robinson. 2002. Lost Languages: The
Enigma of the World?s Undeciphered Scripts.
McGraw-Hill.
William M. Schniedewind and Joel H. Hunt. 2007. A
Primer on Ugaritic: Language, Culture and Litera-
ture. Cambridge University Press.
Mark S. Smith, editor. 1955. Untold Stories: The Bible
and Ugaritic Studies in the Twentieth Century. Hen-
drickson Publishers.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
Proceedings of the AAAI, pages 848?854.
Wilfred Watson and Nicolas Wyatt, editors. 1999.
Handbook of Ugaritic Studies. Brill.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2000. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of HLT, pages 161?168.
1057
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1058?1066,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Inference Through Cascades of Weighted Tree Transducers
Jonathan May and Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
{jonmay,knight}@isi.edu
Heiko Vogler
Technische Universita?t Dresden
Institut fu?r Theoretische Informatik
01062 Dresden, Germany
heiko.vogler@tu-dresden.de
Abstract
Weighted tree transducers have been pro-
posed as useful formal models for rep-
resenting syntactic natural language pro-
cessing applications, but there has been
little description of inference algorithms
for these automata beyond formal founda-
tions. We give a detailed description of
algorithms for application of cascades of
weighted tree transducers to weighted tree
acceptors, connecting formal theory with
actual practice. Additionally, we present
novel on-the-fly variants of these algo-
rithms, and compare their performance
on a syntax machine translation cascade
based on (Yamada and Knight, 2001).
1 Motivation
Weighted finite-state transducers have found re-
cent favor as models of natural language (Mohri,
1997). In order to make actual use of systems built
with these formalisms we must first calculate the
set of possible weighted outputs allowed by the
transducer given some input, which we call for-
ward application, or the set of possible weighted
inputs given some output, which we call backward
application. After application we can do some in-
ference on this result, such as determining its k
highest weighted elements.
We may also want to divide up our problems
into manageable chunks, each represented by a
transducer. As noted by Woods (1980), it is eas-
ier for designers to write several small transduc-
ers where each performs a simple transformation,
rather than painstakingly construct a single com-
plicated device. We would like to know, then,
the result of transformation of input or output by
a cascade of transducers, one operating after the
other. As we will see, there are various strate-
gies for approaching this problem. We will con-
sider offline composition, bucket brigade applica-
tion, and on-the-fly application.
Application of cascades of weighted string
transducers (WSTs) has been well-studied (Mohri,
1997). Less well-studied but of more recent in-
terest is application of cascades of weighted tree
transducers (WTTs). We tackle application of WTT
cascades in this work, presenting:
? explicit algorithms for application of WTT cas-
cades
? novel algorithms for on-the-fly application of
WTT cascades, and
? experiments comparing the performance of
these algorithms.
2 Strategies for the string case
Before we discuss application of WTTs, it is help-
ful to recall the solution to this problem in the WST
domain. We recall previous formal presentations
of WSTs (Mohri, 1997) and note informally that
they may be represented as directed graphs with
designated start and end states and edges labeled
with input symbols, output symbols, and weights.1
Fortunately, the solution for WSTs is practically
trivial?we achieve application through a series
of embedding, composition, and projection oper-
ations. Embedding is simply the act of represent-
ing a string or regular string language as an iden-
tity WST. Composition of WSTs, that is, generat-
ing a single WST that captures the transformations
of two input WSTs used in sequence, is not at all
trivial, but has been well covered in, e.g., (Mohri,
2009), where directly implementable algorithms
can be found. Finally, projection is another triv-
ial operation?the domain or range language can
be obtained from a WST by ignoring the output or
input symbols, respectively, on its arcs, and sum-
ming weights on otherwise identical arcs. By em-
bedding an input, composing the result with the
given WST, and projecting the result, forward ap-
plication is accomplished.2 We are then left with
a weighted string acceptor (WSA), essentially a
weighted, labeled graph, which can be traversed
1We assume throughout this paper that weights are in
R+ ? {+?}, that the weight of a path is calculated as the
product of the weights of its edges, and that the weight of a
(not necessarily finite) set T of paths is calculated as the sum
of the weights of the paths of T .
2For backward applications, the roles of input and output
are simply exchanged.
1058
A B
a : a / 1 a : a / 1
C
(a) Input string ?a a? embedded in an
identity WST
E
a : b / . 1 a : a / . 9
b : a / . 5 D
a : b / . 4
a : a / . 6
b : a / . 5
b : b / . 5
b : b / . 5
(b) first WST in cascade
a : c / . 6
b
: c / . 7
F
a : d / . 4
b
: d / . 3
(c) second WST in cascade
E F
a : c / . 0 7
a : c / . 5 4
b
: c / . 6 5
b
: d / . 3 5
D F
a : c / . 2 8
a : c / . 3 6
b
: c / . 6 5
b
: d / . 3 5
a : d / . 3 6
a : d / . 0 3 a : d / . 2 4
a : d / . 1 2
(d) Offline composition approach:
Compose the transducers
A D
B D C
D
a : b / . 1
B E
a : a / . 9
C
E
(e) Bucket brigade approach:
Apply WST (b) to WST (a)
A D F
B D F C
D F
d / . 0 3
c
/ . 0 7
B E F
c / . 5 4
C
E F
c
/ . 5 4
c / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 0 3
d / . 3 6
d / . 1 2
d / . 2 4
(f) Result of offline or bucket application
after projection
A D F
B D F C
D F
d / . 0 3
B E F
c
/ . 5 4
C
E F
c / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 1 2
d / . 2 4
(g) Initial on-the-fly
stand-in for (f)
A D F
B D F C
D F
d / . 0 3
B E F
c / . 5 4
C E F
c / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 1 2
d / . 2 4
(h) On-the-fly stand-in after exploring
outgoing edges of state ADF
A D F
B D F C
D F
d / . 0 3
B E F
c / . 5 4
C
E Fc / . 3 6
c
/ . 2 8
c
/ . 0 7
d / . 3 6
d / . 1 2
d / . 2 4
(i) On-the-fly stand-in after best path has been found
Figure 1: Three different approaches to application through cascades of WSTs.
by well-known algorithms to efficiently find the k-
best paths.
Because WSTs can be freely composed, extend-
ing application to operate on a cascade of WSTs
is fairly trivial. The only question is one of com-
position order: whether to initially compose the
cascade into a single transducer (an approach we
call offline composition) or to compose the initial
embedding with the first transducer, trim useless
states, compose the result with the second, and so
on (an approach we call bucket brigade). The ap-
propriate strategy generally depends on the struc-
ture of the individual transducers.
A third approach builds the result incrementally,
as dictated by some algorithm that requests in-
formation about it. Such an approach, which we
call on-the-fly, was described in (Pereira and Ri-
ley, 1997; Mohri, 2009; Mohri et al, 2000). If
we can efficiently calculate the outgoing edges of
a state of the result WSA on demand, without cal-
culating all edges in the entire machine, we can
maintain a stand-in for the result structure, a ma-
chine consisting at first of only the start state of
the true result. As a calling algorithm (e.g., an im-
plementation of Dijkstra?s algorithm) requests in-
formation about the result graph, such as the set of
outgoing edges from a state, we replace the current
stand-in with a richer version by adding the result
of the request. The on-the-fly approach has a dis-
tinct advantage over the other two methods in that
the entire result graph need not be built. A graphi-
cal representation of all three methods is presented
in Figure 1.
3 Application of tree transducers
Now let us revisit these strategies in the setting
of trees and tree transducers. Imagine we have a
tree or set of trees as input that can be represented
as a weighted regular tree grammar3 (WRTG) and
a WTT that can transform that input with some
weight. We would like to know the k-best trees the
WTT can produce as output for that input, along
with their weights. We already know of several
methods for acquiring k-best trees from a WRTG
(Huang and Chiang, 2005; Pauls and Klein, 2009),
so we then must ask if, analogously to the string
case, WTTs preserve recognizability4 and we can
form an application WRTG. Before we begin, how-
ever, we must define WTTs and WRTGs.
3.1 Preliminaries5
A ranked alphabet is a finite set ? such that ev-
ery member ? ? ? has a rank rk(?) ? N. We
call ?(k) ? ?, k ? N the set of those ? ? ?
such that rk(?) = k. The set of variables is de-
notedX = {x1, x2, . . .} and is assumed to be dis-
joint from any ranked alphabet used in this paper.
We use ? to denote a symbol of rank 0 that is not
in any ranked alphabet used in this paper. A tree
t ? T? is denoted ?(t1, . . . , tk) where k ? 0,
? ? ?(k), and t1, . . . , tk ? T?. For ? ? ?(0) we
3This generates the same class of weighted tree languages
as weighted tree automata, the direct analogue of WSAs, and
is more useful for our purposes.
4A weighted tree language is recognizable iff it can be
represented by a wrtg.
5The following formal definitions and notations are
needed for understanding and reimplementation of the pre-
sented algorithms, but can be safely skipped on first reading
and consulted when encountering an unfamiliar term.
1059
write ? ? T? as shorthand for ?(). For every set
S disjoint from ?, let T?(S) = T??S , where, for
all s ? S, rk(s) = 0.
We define the positions of a tree
t = ?(t1, . . . , tk), for k ? 0, ? ? ?(k),
t1, . . . , tk ? T?, as a set pos(t) ? N? such that
pos(t) = {?} ? {iv | 1 ? i ? k, v ? pos(ti)}.
The set of leaf positions lv(t) ? pos(t) are those
positions v ? pos(t) such that for no i ? N,
vi ? pos(t). We presume standard lexicographic
orderings < and ? on pos.
Let t, s ? T? and v ? pos(t). The label of t
at position v, denoted by t(v), the subtree of t at
v, denoted by t|v, and the replacement at v by s,
denoted by t[s]v, are defined as follows:
1. For every ? ? ?(0), ?(?) = ?, ?|? = ?, and
?[s]? = s.
2. For every t = ?(t1, . . . , tk) such that
k = rk(?) and k ? 1, t(?) = ?, t|? = t,
and t[s]? = s. For every 1 ? i ? k and
v ? pos(ti), t(iv) = ti(v), t|iv = ti|v, and
t[s]iv = ?(t1, . . . , ti?1, ti[s]v, ti+1, . . . , tk).
The size of a tree t, size (t) is |pos(t)|, the car-
dinality of its position set. The yield set of a tree
is the set of labels of its leaves: for a tree t, yd (t)
= {t(v) | v ? lv(t)}.
Let A and B be sets. Let ? : A ? T?(B)
be a mapping. We extend ? to the mapping ? :
T?(A)? T?(B) such that for a ?A, ?(a) = ?(a)
and for k ? 0, ? ? ?(k), and t1, . . . , tk ? T?(A),
?(?(t1, . . . , tk)) = ?(?(t1), . . . , ?(tk)). We indi-
cate such extensions by describing ? as a substi-
tution mapping and then using ? without further
comment.
We use R+ to denote the set {w ? R | w ? 0}
and R?+ to denote R+ ? {+?}.
Definition 3.1 (cf. (Alexandrakis and Bozapa-
lidis, 1987)) A weighted regular tree grammar
(WRTG) is a 4-tuple G = (N,?, P, n0) where:
1. N is a finite set of nonterminals, with n0 ? N
the start nonterminal.
2. ? is a ranked alphabet of input symbols, where
? ?N = ?.
3. P is a tuple (P ?, pi), where P ? is a finite set
of productions, each production p of the form
n ?? u, n ? N , u ? T?(N), and pi : P ? ? R+
is a weight function of the productions. We will
refer to P as a finite set of weighted produc-
tions, each production p of the form n
pi(p)
??? u.
A production p is a chain production if it is
of the form ni
w
?? nj , where ni, nj ? N .6
6In (Alexandrakis and Bozapalidis, 1987), chain produc-
tions are forbidden in order to avoid infinite summations. We
explicitly allow such summations.
A WRTG G is in normal form if each produc-
tion is either a chain production or is of the
form n
w
?? ?(n1, . . . , nk) where ? ? ?(k) and
n1, . . . , nk ? N .
For WRTG G = (N,?, P, n0), s, t, u ? T?(N),
n ? N , and p ? P of the form n
w
?? u, we
obtain a derivation step from s to t by replacing
some leaf nonterminal in s labeled n with u. For-
mally, s ?pG t if there exists some v ? lv(s)
such that s(v) = n and s[u]v = t. We say this
derivation step is leftmost if, for all v? ? lv(s)
where v? < v, s(v?) ? ?. We henceforth as-
sume all derivation steps are leftmost. If, for
some m ? N, pi ? P , and ti ? T?(N) for all
1 ? i ? m, n0 ?p1 t1 . . . ?pm tm, we say
the sequence d = (p1, . . . , pm) is a derivation
of tm in G and that n0 ?? tm; the weight of d
is wt(d) = pi(p1) ? . . . ? pi(pm). The weighted
tree language recognized by G is the mapping
LG : T? ? R?+ such that for every t ? T?, LG(t)
is the sum of the weights of all (possibly infinitely
many) derivations of t in G. A weighted tree lan-
guage f : T? ? R?+ is recognizable if there is a
WRTG G such that f = LG.
We define a partial ordering  on WRTGs
such that for WRTGs G1 = (N1,?, P1, n0) and
G2 = (N2,?, P2, n0), we say G1  G2 iff
N1 ? N2 and P1 ? P2, where the weights are
preserved.
Definition 3.2 (cf. Def. 1 of (Maletti, 2008))
A weighted extended top-down tree transducer
(WXTT) is a 5-tupleM = (Q,?,?, R, q0) where:
1. Q is a finite set of states.
2. ? and ? are the ranked alphabets of in-
put and output symbols, respectively, where
(? ??) ?Q = ?.
3. R is a tuple (R?, pi), where R? is a finite set
of rules, each rule r of the form q.y ?? u for
q ? Q, y ? T?(X), and u ? T?(Q ? X).
We further require that no variable x ? X ap-
pears more than once in y, and that each vari-
able appearing in u is also in y. Moreover,
pi : R? ? R?+ is a weight function of the
rules. As for WRTGs, we refer to R as a finite
set of weighted rules, each rule r of the form
q.y
pi(r)
??? u.
A WXTT is linear (respectively, nondeleting)
if, for each rule r of the form q.y
w
?? u, each
x ? yd (y) ? X appears at most once (respec-
tively, at least once) in u. We denote the class
of all WXTTs as wxT and add the letters L and N
to signify the subclasses of linear and nondeleting
WTT, respectively. Additionally, if y is of the form
?(x1, . . . , xk), we remove the letter ?x? to signify
1060
the transducer is not extended (i.e., it is a ?tradi-
tional? WTT (Fu?lo?p and Vogler, 2009)).
For WXTT M = (Q,?,?, R, q0), s, t ? T?(Q
? T?), and r ? R of the form q.y
w
?? u, we obtain
a derivation step from s to t by replacing some
leaf of s labeled with q and a tree matching y by a
transformation of u, where each instance of a vari-
able has been replaced by a corresponding subtree
of the y-matching tree. Formally, s?rM t if there
is a position v ? pos(s), a substitution mapping
? : X ? T?, and a rule q.y
w
?? u ? R such that
s(v) = (q, ?(y)) and t = s[??(u)]v, where ?? is
a substitution mapping Q ? X ? T?(Q ? T?)
defined such that ??(q?, x) = (q?, ?(x)) for all
q? ? Q and x ? X . We say this derivation step
is leftmost if, for all v? ? lv(s) where v? < v,
s(v?) ? ?. We henceforth assume all derivation
steps are leftmost. If, for some s ? T?, m ? N,
ri ? R, and ti ? T?(Q ? T?) for all 1 ? i ? m,
(q0, s) ?r1 t1 . . . ?rm tm, we say the sequence
d = (r1, . . . , rm) is a derivation of (s, tm) in M ;
the weight of d is wt(d) = pi(r1) ? . . . ? pi(rm).
The weighted tree transformation recognized by
M is the mapping ?M : T? ? T? ? R?+ , such
that for every s ? T? and t ? T?, ?M (s, t) is the
sum of the weights of all (possibly infinitely many)
derivations of (s, t) inM . The composition of two
weighted tree transformations ? : T??T? ? R?+
and ? : T??T? ? R?+ is the weighted tree trans-
formation (? ;?) : T? ? T? ?R?+ where for every
s ? T? and u ? T?, (? ;?)(s, u) =
?
t?T?
?(s, t)
? ?(t, u).
3.2 Applicable classes
We now consider transducer classes where recog-
nizability is preserved under application. Table 1
presents known results for the top-down tree trans-
ducer classes described in Section 3.1. Unlike
the string case, preservation of recognizability is
not universal or symmetric. This is important for
us, because we can only construct an application
WRTG, i.e., a WRTG representing the result of ap-
plication, if we can ensure that the language gen-
erated by application is in fact recognizable. Of
the types under consideration, only wxLNT and
wLNT preserve forward recognizability. The two
classes marked as open questions and the other
classes, which are superclasses of wNT, do not or
are presumed not to. All subclasses of wxLT pre-
serve backward recognizability.7 We do not con-
sider cases where recognizability is not preserved
in the remainder of this paper. If a transducer M
of a class that preserves forward recognizability is
applied to a WRTG G, we can call the forward ap-
7Note that the introduction of weights limits recognizabil-
ity preservation considerably. For example, (unweighted) xT
preserves backward recognizability.
plication WRTG M(G). and ifM preserves back-
ward recognizability, we can call the backward ap-
plication WRTG M(G)/.
Now that we have explained the application
problem in the context of weighted tree transduc-
ers and determined the classes for which applica-
tion is possible, let us consider how to build for-
ward and backward application WRTGs. Our ba-
sic approach mimics that taken for WSTs by us-
ing an embed-compose-project strategy. As in
string world, if we can embed the input in a trans-
ducer, compose with the given transducer, and
project the result, we can obtain the application
WRTG. Embedding a WRTG in a wLNT is a triv-
ial operation?if the WRTG is in normal form and
chain production-free,8 for every production of the
form n
w
?? ?(n1, . . . , nk), create a rule of the form
n.?(x1, . . . , xk)
w
?? ?(n1.x1, . . . , nk.xk). Range
projection of a wxLNT is also trivial?for every
q ? Q and u ? T?(Q ? X) create a production
of the form q
w
?? u? where u? is formed from u
by replacing all leaves of the form q.x with the
leaf q, i.e., removing references to variables, and
w is the sum of the weights of all rules of the form
q.y ?? u in R.9 Domain projection for wxLT is
best explained by way of example. The left side of
a rule is preserved, with variables leaves replaced
by their associated states from the right side. So,
the rule q1.?(?(x1), x2)
w
?? ?(q2.x2, ?(?, q3.x1))
would yield the production q1
w
?? ?(?(q3), q2) in
the domain projection. However, a deleting rule
such as q1.?(x1, x2)
w
?? ?(q2.x2) necessitates the
introduction of a new nonterminal ? that can gen-
erate all of T? with weight 1.
The only missing piece in our embed-compose-
project strategy is composition. Algorithm 1,
which is based on the declarative construction of
Maletti (2006), generates the syntactic composi-
tion of a wxLT and a wLNT, a generalization
of the basic composition construction of Baker
(1979). It calls Algorithm 2, which determines
the sequences of rules in the second transducer
that match the right side of a single rule in the
first transducer. Since the embedded WRTG is of
type wLNT, it may be either the first or second
argument provided to Algorithm 1, depending on
whether the application is forward or backward.
We can thus use the embed-compose-project strat-
egy for forward application of wLNT and back-
ward application of wxLT and wxLNT. Note that
we cannot use this strategy for forward applica-
8Without loss of generality we assume this is so, since
standard algorithms exist to remove chain productions
(Kuich, 1998; E?sik and Kuich, 2003; Mohri, 2009) and con-
vert into normal form (Alexandrakis and Bozapalidis, 1987).
9Finitely many such productions may be formed.
1061
tion of wxLNT, even though that class preserves
recognizability.
Algorithm 1 COMPOSE
1: inputs
2: wxLTM1 = (Q1,?,?, R1, q10)
3: wLNTM2 = (Q2,?,?, R2, q20)
4: outputs
5: wxLTM3 = ((Q1?Q2),?,?, R3, (q10 , q20)) such
thatM3 = (?M1 ; ?M2).
6: complexity
7: O(|R1|max(|R2|size(u?), |Q2|)), where u? is the
largest right side tree in any rule in R1
8: Let R3 be of the form (R?3, pi)
9: R3 ? (?, ?)
10: ?? {(q10 , q20)} {seen states}
11: ?? {(q10 , q20)} {pending states}
12: while ? 6= ? do
13: (q1, q2)?any element of ?
14: ?? ? \ {(q1, q2)}
15: for all (q1.y
w1??? u) ? R1 do
16: for all (z, w2) ? COVER(u,M2, q2) do
17: for all (q, x) ? yd (z)? ((Q1?Q2)?X) do
18: if q 6? ? then
19: ?? ? ? {q}
20: ?? ? ? {q}
21: r ? ((q1, q2).y ?? z)
22: R?3 ? R
?
3 ? {r}
23: pi(r)? pi(r) + (w1 ? w2)
24: return M3
4 Application of tree transducer cascades
What about the case of an input WRTG and a cas-
cade of tree transducers? We will revisit the three
strategies for accomplishing application discussed
above for the string case.
In order for offline composition to be a viable
strategy, the transducers in the cascade must be
closed under composition. Unfortunately, of the
classes that preserve recognizability, only wLNT
is closed under composition (Ge?cseg and Steinby,
1984; Baker, 1979; Maletti et al, 2009; Fu?lo?p and
Vogler, 2009).
However, the general lack of composability of
tree transducers does not preclude us from con-
ducting forward application of a cascade. We re-
visit the bucket brigade approach, which in Sec-
tion 2 appeared to be little more than a choice of
composition order. As discussed previously, ap-
plication of a single transducer involves an embed-
ding, a composition, and a projection. The embed-
ded WRTG is in the class wLNT, and the projection
forms another WRTG. As long as every transducer
in the cascade can be composed with a wLNT
to its left or right, depending on the application
type, application of a cascade is possible. Note
that this embed-compose-project process is some-
what more burdensome than in the string case. For
strings, application is obtained by a single embed-
ding, a series of compositions, and a single projec-
Algorithm 2 COVER
1: inputs
2: u ? T?(Q1 ?X)
3: wTM2 = (Q2,?,?, R2, q20)
4: state q2 ? Q2
5: outputs
6: set of pairs (z, w) with z ? T?((Q1 ? Q2) ? X)
formed by one or more successful runs on u by rules
in R2, starting from q2, and w ? R?+ the sum of the
weights of all such runs.
7: complexity
8: O(|R2|size(u))
9: if u(?) is of the form (q1, x) ? Q1 ?X then
10: zinit ? ((q1, q2), x)
11: else
12: zinit ? ?
13: ?last ? {(zinit, {((?, ?), q2)}, 1)}
14: for all v ? pos(u) such that u(v) ? ?(k) for some
k ? 0 in prefix order do
15: ?v ? ?
16: for all (z, ?, w) ? ?last do
17: for all v? ? lv(z) such that z(v?) = ? do
18: for all (?(v, v?).u(v)(x1, . . . , xk)
w?
??h)?R2
do
19: ?? ? ?
20: Form substitution mapping ? : (Q2 ? X)
? T?((Q1 ? Q2 ?X) ? {?}).
21: for i = 1 to k do
22: for all v?? ? pos(h) such that
h(v??) = (q?2, xi) for some q
?
2 ? Q2 do
23: ??(vi, v?v??)? q?2
24: if u(vi) is of the form
(q1, x) ? Q1 ?X then
25: ?(q?2, xi)? ((q1, q
?
2), x)
26: else
27: ?(q?2, xi)? ?
28: ?v ? ?v ? {(z[?(h)]v? , ?
?, w ? w?)}
29: ?last ? ?v
30: Z ? {z | (z, ?, w) ? ?last}
31: return {(z,
X
(z,?,w)??last
w) | z ? Z}
tion, whereas application for trees is obtained by a
series of (embed, compose, project) operations.
4.1 On-the-fly algorithms
We next consider on-the-fly algorithms for ap-
plication. Similar to the string case, an on-the-
fly approach is driven by a calling algorithm that
periodically needs to know the productions in a
WRTG with a common left side nonterminal. The
embed-compose-project approach produces an en-
tire application WRTG before any inference al-
gorithm is run. In order to admit an on-the-fly
approach we describe algorithms that only gen-
erate those productions in a WRTG that have a
given left nonterminal. In this section we ex-
tend Definition 3.1 as follows: a WRTG is a 6-
tuple G = (N,?, P, n0,M,G) where N,?, P,
and n0 are defined as in Definition 3.1, and either
M = G = ?,10 orM is a wxLNT and G is a nor-
mal form, chain production-free WRTG such that
10In which case the definition is functionally unchanged
from before.
1062
type preserved? source
w[x]T No See w[x]NT
w[x]LT OQ (Maletti, 2009)
w[x]NT No (Ge?cseg and Steinby, 1984)
wxLNT Yes (Fu?lo?p et al, 2010)
wLNT Yes (Kuich, 1999)
(a) Preservation of forward recognizability
type preserved? source
w[x]T No See w[x]NT
w[x]LT Yes (Fu?lo?p et al, 2010)
w[x]NT No (Maletti, 2009)
w[x]LNT Yes See w[x]LT
(b) Preservation of backward recognizability
Table 1: Preservation of forward and backward recognizability for various classes of top-down tree
transducers. Here and elsewhere, the following abbreviations apply: w = weighted, x = extended LHS, L
= linear, N = nondeleting, OQ = open question. Square brackets include a superposition of classes. For
example, w[x]T signifies both wxT and wT.
Algorithm 3 PRODUCE
1: inputs
2: WRTG Gin = (Nin,?, Pin, n0,M,G) such
that M = (Q,?,?, R, q0) is a wxLNT and
G = (N,?, P, n?0,M
?, G?) is a WRTG in normal
form with no chain productions
3: nin ? Nin
4: outputs
5: WRTG Gout = (Nout, ?, Pout, n0,M,G), such that
Gin  Gout and
(nin
w
?? u) ? Pout? (nin
w
?? u) ?M(G).
6: complexity
7: O(|R||P |size(y?)), where y? is the largest left side tree
in any rule in R
8: if Pin contains productions of the form nin
w
?? u then
9: return Gin
10: Nout ? Nin
11: Pout ? Pin
12: Let nin be of the form (n, q), where n ? N and q ? Q.
13: for all (q.y
w1??? u) ? R do
14: for all (?, w2) ? REPLACE(y,G, n) do
15: Form substitution mapping ? : Q ? X ?
T?(N ?Q) such that, for all v ? yd (y) and q? ?
Q, if there exist n? ?N and x ?X such that ?(v)
= n? and y(v) = x, then ?(q?, x) = (n?, q?).
16: p? ? ((n, q)
w1?w2????? ?(u))
17: for all p ? NORM(p?, Nout) do
18: Let p be of the form n0
w
?? ?(n1, . . . , nk) for
? ? ?(k).
19: Nout ? Nout ? {n0, . . . , nk}
20: Pout ? Pout ? {p}
21: return CHAIN-REM(Gout)
G M(G).. In the latter case, G is a stand-in for
M(G)., analogous to the stand-ins for WSAs and
WSTs described in Section 2.
Algorithm 3, PRODUCE, takes as input a
WRTG Gin = (Nin,?, Pin, n0,M,G) and a de-
sired nonterminal nin and returns another WRTG,
Gout that is different from Gin in that it has more
productions, specifically those beginning with nin
that are in M(G).. Algorithms using stand-ins
should call PRODUCE to ensure the stand-in they
are using has the desired productions beginning
with the specific nonterminal. Note, then, that
PRODUCE obtains the effect of forward applica-
Algorithm 4 REPLACE
1: inputs
2: y ? T?(X)
3: WRTG G = (N,?, P, n0,M,G) in normal form,
with no chain productions
4: n ? N
5: outputs
6: set ? of pairs (?, w) where ? is a mapping
pos(y) ? N and w ? R?+ , each pair indicating
a successful run on y by productions in G, starting
from n, and w is the weight of the run.
7: complexity
8: O(|P |size(y))
9: ?last ? {({(?, n)}, 1)}
10: for all v ? pos(y) such that y(v) 6? X in prefix order
do
11: ?v ? ?
12: for all (?, w) ? ?last do
13: ifM 6= ? and G 6= ? then
14: G? PRODUCE(G, ?(v))
15: for all (?(v) w
?
?? y(v)(n1, . . . , nk)) ? P do
16: ?v ? ?v?{(??{(vi, ni), 1 ? i ? k}, w?w?)}
17: ?last ? ?v
18: return ?last
Algorithm 5 MAKE-EXPLICIT
1: inputs
2: WRTG G = (N,?, P, n0,M,G) in normal form
3: outputs
4: WRTG G? = (N ?,?, P ?, n0,M,G), in normal form,
such that ifM 6= ? andG 6= ?, LG? = LM(G). , and
otherwise G? = G.
5: complexity
6: O(|P ?|)
7: G? ? G
8: ?? {n0} {seen nonterminals}
9: ?? {n0} {pending nonterminals}
10: while ? 6= ? do
11: n?any element of ?
12: ?? ? \ {n}
13: ifM 6= ? and G 6= ? then
14: G? ? PRODUCE(G?, n)
15: for all (n w?? ?(n1, . . . , nk)) ? P ? do
16: for i = 1 to k do
17: if ni 6? ? then
18: ?? ? ? {ni}
19: ?? ? ? {ni}
20: return G?
1063
g0
g0
w1??? ?(g0, g1)
g0
w2??? ? g1
w3??? ?
(a) Input WRTG G
a0
a0.?(x1, x2)
w4??? ?(a0.x1, a1.x2)
a0.?(x1, x2)
w5??? ?(a2.x1, a1.x2)
a0.?
w6??? ? a1.?
w7??? ? a2.?
w8??? ?
(b) First transducerMA in the cascade
b0
b0.?(x1, x2)
w9??? ?(b0.x1, b0.x2)
b0.?
w10??? ?
(c) Second transducerMB in the cascade
g0a0
w1?w4????? ?(g0a0, g1a1)
g0a0
w1?w5????? ?(g0a2, g1a1)
g0a0
w2?w6????? ? g1a1
w3?w7????? ?
(d) Productions ofMA(G). built as a consequence
of building the completeMB(MA(G).).
g0a0b0
g0a0b0
w1?w4?w9??????? ?(g0a0b0, g1a1b0)
g0a0b0
w2?w6?w10???????? ? g1a1b0
w3?w7?w10???????? ?
(e) CompleteMB(MA(G).).
Figure 2: Forward application through a cascade
of tree transducers using an on-the-fly method.
tion in an on-the-fly manner.11 It makes calls to
REPLACE, which is presented in Algorithm 4, as
well as to a NORM algorithm that ensures normal
form by replacing a single production not in nor-
mal form with several normal-form productions
that can be combined together (Alexandrakis and
Bozapalidis, 1987) and a CHAIN-REM algorithm
that replaces a WRTG containing chain productions
with an equivalent WRTG that does not (Mohri,
2009).
As an example of stand-in construction, con-
sider the invocation PRODUCE(G1, g0a0), where
G1 = ({g0a0}, {?, ?, ?, ?}, ?, g0a0, MA, G), G
is in Figure 2a,12 and MA is in 2b. The stand-in
WRTG that is output contains the first three of the
four productions in Figure 2d.
To demonstrate the use of on-the-fly application
in a cascade, we next show the effect of PRO-
DUCE when used with the cascadeG?MA ?MB ,
where MB is in Figure 2c. Our driving al-
gorithm in this case is Algorithm 5, MAKE-
11Note further that it allows forward application of class
wxLNT, something the embed-compose-project approach did
not allow.
12By convention the initial nonterminal and state are listed
first in graphical depictions of WRTGs and WXTTs.
rJJ.JJ(x1, x2, x3) ?? JJ(rDT.x1, rJJ.x2, rVB.x3)
rVB.VB(x1, x2, x3) ?? VB(rNNPS.x1, rNN.x3, rVB.x2)
t.?gentle? ?? ?gentle?
(a) Rotation rules
iVB.NN(x1, x2) ?? NN(INS iNN.x1, iNN.x2)
iVB.NN(x1, x2) ?? NN(iNN.x1, iNN.x2)
iVB.NN(x1, x2) ?? NN(iNN.x1, iNN.x2, INS)
(b) Insertion rules
t.VB(x1, x2, x3) ?? X(t.x1, t.x2, t.x3)
t.?gentleman? ?? j1
t.?gentleman? ?? EPS
t.INS ?? j1
t.INS ?? j2
(c) Translation rules
Figure 3: Example rules from transducers used
in decoding experiment. j1 and j2 are Japanese
words.
EXPLICIT, which simply generates the full ap-
plication WRTG using calls to PRODUCE. The
input to MAKE-EXPLICIT is G2 = ({g0a0b0},
{?, ?}, ?, g0a0b0,MB ,G1).13 MAKE-EXPLICIT
calls PRODUCE(G2, g0a0b0). PRODUCE then
seeks to cover b0.?(x1, x2)
w9?? ?(b0.x1, b0.x2)
with productions from G1, which is a stand-in for
MA(G).. At line 14 of REPLACE, G1 is im-
proved so that it has the appropriate productions.
The productions of MA(G). that must be built
to form the complete MB(MA(G).). are shown
in Figure 2d. The complete MB(MA(G).). is
shown in Figure 2e. Note that because we used
this on-the-fly approach, we were able to avoid
building all the productions in MA(G).; in par-
ticular we did not build g0a2
w2?w8????? ?, while a
bucket brigade approach would have built this pro-
duction. We have also designed an analogous on-
the-fly PRODUCE algorithm for backward appli-
cation on linear WTT.
We have now defined several on-the-fly and
bucket brigade algorithms, and also discussed the
possibility of embed-compose-project and offline
composition strategies to application of cascades
of tree transducers. Tables 2a and 2b summa-
rize the available methods of forward and back-
ward application of cascades for recognizability-
preserving tree transducer classes.
5 Decoding Experiments
The main purpose of this paper has been to
present novel algorithms for performing applica-
tion. However, it is important to demonstrate these
algorithms on real data. We thus demonstrate
bucket-brigade and on-the-fly backward applica-
tion on a typical NLP task cast as a cascade of
wLNT. We adapt the Japanese-to-English transla-
13Note that G2 is the initial stand-in for MB(MA(G).).,
since G1 is the initial stand-in forMA(G)..
1064
method WST wxLNT wLNT
oc
?
?
?
bb
?
?
?
otf
? ? ?
(a) Forward application
method WST wxLT wLT wxLNT wLNT
oc
?
? ? ?
?
bb
? ? ? ? ?
otf
? ? ? ? ?
(b) Backward application
Table 2: Transducer types and available methods of forward and backward application of a cascade.
oc = offline composition, bb = bucket brigade, otf = on the fly.
tion model of Yamada and Knight (2001) by trans-
forming it from an English-tree-to-Japanese-string
model to an English-tree-to-Japanese-tree model.
The Japanese trees are unlabeled, meaning they
have syntactic structure but all nodes are labeled
?X?. We then cast this modified model as a cas-
cade of LNT tree transducers. Space does not per-
mit a detailed description, but some example rules
are in Figure 3. The rotation transducer R, a sam-
ple of which is in Figure 3a, has 6,453 rules, the
insertion transducer I, Figure 3b, has 8,122 rules,
and the translation transducer, T , Figure 3c, has
37,311 rules.
We add an English syntax language model L to
the cascade of transducers just described to bet-
ter simulate an actual machine translation decod-
ing task. The language model is cast as an iden-
tity WTT and thus fits naturally into the experimen-
tal framework. In our experiments we try several
different language models to demonstrate varying
performance of the application algorithms. The
most realistic language model is a PCFG. Each
rule captures the probability of a particular se-
quence of child labels given a parent label. This
model has 7,765 rules.
To demonstrate more extreme cases of the use-
fulness of the on-the-fly approach, we build a lan-
guage model that recognizes exactly the 2,087
trees in the training corpus, each with equal
weight. It has 39,455 rules. Finally, to be ultra-
specific, we include a form of the ?specific? lan-
guage model just described, but only allow the
English counterpart of the particular Japanese sen-
tence being decoded in the language.
The goal in our experiments is to apply a single
tree t backward through the cascadeL?R?I?T ?t
and find the 1-best path in the application WRTG.
We evaluate the speed of each approach: bucket
brigade and on-the-fly. The algorithm we use to
obtain the 1-best path is a modification of the k-
best algorithm of Pauls and Klein (2009). Our al-
gorithm finds the 1-best path in a WRTG and ad-
mits an on-the-fly approach.
The results of the experiments are shown in
Table 3. As can be seen, on-the-fly application
is generally faster than the bucket brigade, about
double the speed per sentence in the traditional
LM type method time/sentence
pcfg bucket 28s
pcfg otf 17s
exact bucket >1m
exact otf 24s
1-sent bucket 2.5s
1-sent otf .06s
Table 3: Timing results to obtain 1-best from ap-
plication through a weighted tree transducer cas-
cade, using on-the-fly vs. bucket brigade back-
ward application techniques. pcfg = model rec-
ognizes any tree licensed by a pcfg built from
observed data, exact = model recognizes each of
2,000+ trees with equal weight, 1-sent = model
recognizes exactly one tree.
experiment that uses an English PCFG language
model. The results for the other two language
models demonstrate more keenly the potential ad-
vantage that an on-the-fly approach provides?the
simultaneous incorporation of information from
all models allows application to be done more ef-
fectively than if each information source is consid-
ered in sequence. In the ?exact? case, where a very
large language model that simply recognizes each
of the 2,087 trees in the training corpus is used,
the final application is so large that it overwhelms
the resources of a 4gb MacBook Pro, while the
on-the-fly approach does not suffer from this prob-
lem. The ?1-sent? case is presented to demonstrate
the ripple effect caused by using on-the fly. In the
other two cases, a very large language model gen-
erally overwhelms the timing statistics, regardless
of the method being used. But a language model
that represents exactly one sentence is very small,
and thus the effects of simultaneous inference are
readily apparent?the time to retrieve the 1-best
sentence is reduced by two orders of magnitude in
this experiment.
6 Conclusion
We have presented algorithms for forward and
backward application of weighted tree trans-
ducer cascades, including on-the-fly variants, and
demonstrated the benefit of an on-the-fly approach
to application. We note that a more formal ap-
proach to application of WTTs is being developed,
1065
independent from these efforts, by Fu?lo?p et al
(2010).
Acknowledgments
We are grateful for extensive discussions with
Andreas Maletti. We also appreciate the in-
sights and advice of David Chiang, Steve De-
Neefe, and others at ISI in the preparation of
this work. Jonathan May and Kevin Knight were
supported by NSF grants IIS-0428020 and IIS-
0904684. Heiko Vogler was supported by DFG
VO 1011/5-1.
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Information Processing Letters, 24(1):1?4.
Brenda S. Baker. 1979. Composition of top-down and
bottom-up tree transductions. Information and Con-
trol, 41(2):186?213.
Zolta?n E?sik and Werner Kuich. 2003. Formal tree se-
ries. Journal of Automata, Languages and Combi-
natorics, 8(2):219?285.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9, pages 313?404.
Springer-Verlag.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2010. Backward and forward application of
weighted extended tree transducers. Unpublished
manuscript.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Harry Bunt, Robert Malouf, and Alon
Lavie, editors, Proceedings of the Ninth Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 53?64, Vancouver, October. Association for
Computational Linguistics.
Werner Kuich. 1998. Formal power series over trees.
In Symeon Bozapalidis, editor, Proceedings of the
3rd International Conference on Developments in
Language Theory (DLT), pages 61?101, Thessa-
loniki, Greece. Aristotle University of Thessaloniki.
Werner Kuich. 1999. Tree transducers and formal tree
series. Acta Cybernetica, 14:135?149.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-
down tree transducers. SIAM Journal on Comput-
ing, 39(2):410?430.
Andreas Maletti. 2006. Compositions of tree se-
ries transformations. Theoretical Computer Science,
366:248?271.
Andreas Maletti. 2008. Compositions of extended top-
down tree transducers. Information and Computa-
tion, 206(9?10):1187?1196.
Andreas Maletti. 2009. Personal Communication.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The design principles of a weighted
finite-state transducer library. Theoretical Computer
Science, 231:17?32.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269?312.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, chapter 6, pages 213?254. Springer-Verlag.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Keh-Yih Su, Jian Su, Janyce Wiebe, and Haizhou
Li, editors, Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 958?966, Suntec,
Singapore, August. Association for Computational
Linguistics.
Fernando Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In Emmanuel Roche and Yves Schabes, ed-
itors, Finite-State Language Processing, chapter 15,
pages 431?453. MIT Press, Cambridge, MA.
William A. Woods. 1980. Cascaded ATN gram-
mars. American Journal of Computational Linguis-
tics, 6(1):1?12.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France, July. Association for Computational Lin-
guistics.
1066
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 12?21,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Deciphering Foreign Language
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
In this work, we tackle the task of ma-
chine translation (MT) without parallel train-
ing data. We frame the MT problem as a de-
cipherment task, treating the foreign text as
a cipher for English and present novel meth-
ods for training translation models from non-
parallel text.
1 Introduction
Bilingual corpora are a staple of statistical machine
translation (SMT) research. From these corpora,
we estimate translation model parameters: word-
to-word translation tables, fertilities, distortion pa-
rameters, phrase tables, syntactic transformations,
etc. Starting with the classic IBM work (Brown et
al., 1993), training has been viewed as a maximiza-
tion problem involving hidden word alignments (a)
that are assumed to underlie observed sentence pairs
(e, f ):
argmax
?
?
e,f
P?(f |e) (1)
= argmax
?
?
e,f
?
a
P?(f, a|e) (2)
Brown et al (1993) give various formulas that boil
P?(f, a|e) down to the specific parameters to be es-
timated.
Of course, for many language pairs and domains,
parallel data is not available. In this paper, we
address the problem of learning a full transla-
tion model from non-parallel data, and we use the
learned model to translate new foreign strings. As
successful work develops along this line, we expect
more domains and language pairs to be conquered
by SMT.
How can we learn a translation model from non-
parallel data? Intuitively, we try to construct trans-
lation model tables which, when applied to ob-
served foreign text, consistently yield sensible En-
glish. This is essentially the same approach taken by
cryptanalysts and epigraphers when they deal with
source texts.
In our case, we observe a large number of foreign
strings f , and we apply maximum likelihood train-
ing:
argmax
?
?
f
P?(f) (3)
Following Weaver (1955), we imagine that this cor-
pus of foreign strings ?is really written in English,
but has been coded in some strange symbols,? thus:
argmax
?
?
f
?
e
P (e) ? P?(f |e) (4)
The variable e ranges over all possible English
strings, and P (e) is a language model built from
large amounts of English text that is unrelated to the
foreign strings. Re-writing for hidden alignments,
we get:
argmax
?
?
f
?
e
P (e) ?
?
a
P?(f, a|e) (5)
Note that this formula has the same free
P?(f, a|e) parameters as expression (2). We seek
to manipulate these parameters in order to learn the
12
same full translation model. We note that for each
f , not only is the alignment a still hidden, but now
the English translation e is hidden as well.
A language model P (e) is typically used in SMT
decoding (Koehn, 2009), but here P (e) actually
plays a central role in training translation model pa-
rameters. To distinguish the two, we refer to (5) as
decipherment, rather than decoding.
We can now draw on previous decipherment
work for solving simpler substitution/transposition
ciphers (Bauer, 2006; Knight et al, 2006). We must
keep in mind, however, that foreign language is a
much more demanding code, involving highly non-
deterministic mappings and very large substitution
tables.
The contributions of this paper are therefore:
? We give first results for training a full transla-
tion model from non-parallel text, and we apply
the model to translate previously-unseen text.
This work is thus distinguished from prior work
on extracting or augmenting partial lexicons
using non-parallel corpora (Rapp, 1995; Fung
and McKeown, 1997; Koehn and Knight, 2000;
Haghighi et al, 2008). It also contrasts with
self-training (McClosky et al, 2006), which re-
quires a parallel seed and often does not engage
in iterative maximization.
? We develop novel methods to deal with large-
scale vocabularies inherent in MT problems.
2 Word Substitution Decipherment
Before we tackle machine translation without par-
allel data, we first solve a simpler problem?word
substitution decipherment. Here, we do not have to
worry about hidden alignments since there is only
one alignment. In a word substitution cipher, every
word in the natural language (plaintext) sequence is
substituted by a cipher token, according to a substi-
tution key. The key is deterministic?there exists a
1-to-1 mapping between cipher units and the plain-
text words they encode.
For example, the following English plaintext se-
quences:
I SAW THE BOY .
THE BOY RAN .
may be enciphered as:
xyzz fxyy crqq tmnz lxwz
crqq tmnz gdxx lxwz
according to the key:
THE ? crqq, SAW ? fxyy, RAN ? gdxx,
. ? lxwz, BOY ? tmnz, I ? xyzz
The goal of word substitution decipherment is to
guess the original plaintext from given cipher data
without any knowledge of the substitution key.
Word substitution decipherment is a good test-bed
for unsupervised statistical NLP techniques for two
reasons?(1) we face large vocabularies and corpora
sizes typically seen in large-scale MT problems, so
our methods need to scale well, (2) similar deci-
pherment techniques can be applied for solving NLP
problems such as unsupervised part-of-speech tag-
ging.
Probabilistic decipherment: Our decipherment
method follows a noisy-channel approach. We first
model the process by which the ciphertext sequence
c = c1...cn is generated. The generative story for
decipherment is described here:
1. Generate an English plaintext sequence e =
e1...en, with probability P (e).
2. Substitute each plaintext word ei with a cipher-
text token ci, with probability P?(ci|ei) in order
to generate the ciphertext sequence c = c1...cn.
We model P (e) using a statistical word n-gram
English language model (LM). During decipher-
ment, our goal is to estimate the channel model pa-
rameters ?. Re-writing Equations 3 and 4 for word
substitution decipherment, we get:
argmax
?
?
c
P?(c) (6)
= argmax
?
?
c
?
e
P (e) ?
n?
i=1
P?(ci|ei) (7)
Challenges: Unlike letter substitution ciphers
(having only 26 plaintext letters), here we have to
deal with large-scale vocabularies (10k-1M word
types) and corpora sizes (100k cipher tokens). This
poses some serious scalability challenges for word
substitution decipherment.
13
We propose novel methods that can deal with
these challenges effectively and solve word substi-
tution ciphers:
1. EM solution: We would like to use the Expecta-
tion Maximization (EM) algorithm (Dempster
et al, 1977) to estimate ? from Equation 7, but
EM training is not feasible in our case. First,
EM cannot scale to such large vocabulary sizes
(running the forward-backward algorithm for
each iteration requires O(V 2) time). Secondly,
we need to instantiate the entire channel and re-
sulting derivation lattice before we can run EM,
and this is too big to be stored in memory. So,
we introduce a new training method (Iterative
EM) that fixes these problems.
2. Bayesian decipherment: We also propose a
novel decipherment approach using Bayesian
inference. Typically, Bayesian inference is very
slow when applied to such large-scale prob-
lems. Our method overcomes these challenges
and does fast, efficient inference using (a) a
novel strategy for selecting sampling choices,
and (b) a parallelized sampling scheme.
In the next two sections, we describe these meth-
ods in detail.
2.1 Iterative EM
We devise a method which overcomes memory and
running time efficiency issues faced by EM. Instead
of instantiating the entire channel model (with all its
parameters), we iteratively train the model in small
steps. The training procedure is described here:
1. Identify the top K frequent word types in both
the plaintext and ciphertext data. Replace all
other word tokens with Unknown. Now, instan-
tiate a small channel with just (K + 1)2 pa-
rameters and use the EM algorithm to train this
model to maximize likelihood of cipher data.
2. Extend the plaintext and ciphertext vocabular-
ies from the previous step by adding the next
K most frequent word types (so the new vo-
cabulary size becomes 2K + 1). Regenerate
the plaintext and ciphertext data.
3. Instantiate a new (2K+1)? (2K+1) channel
model. From the previous EM-trained channel,
identify all the e ? c mappings that were as-
signed a probability P (c|e) > 0.5. Fix these
mappings in the new channel, i.e. set P (c|e) =
1.0. From the new channel, eliminate all other
parameters e ? cj associated with the plain-
text word type e (where cj 6= c). This yields a
much smaller channel with size < (2K + 1)2.
Retrain the new channel using EM algorithm.
4. Goto Step 2 and repeat the procedure, extend-
ing the channel size iteratively in each stage.
Finally, we decode the given ciphertext c by using
the Viterbi algorithm to choose the plaintext decod-
ing e that maximizes P (e) ? P?trained(c|e)
3, stretch-
ing the channel probabilities (Knight et al, 2006).
2.2 Bayesian Decipherment
Bayesian inference methods have become popular
in natural language processing (Goldwater and Grif-
fiths, 2007; Finkel et al, 2005; Blunsom et al, 2009;
Chiang et al, 2010; Snyder et al, 2010). These
methods are attractive for their ability to manage un-
certainty about model parameters and allow one to
incorporate prior knowledge during inference.
Here, we propose a novel decipherment approach
using Bayesian learning. Our method holds sev-
eral other advantages over the EM approach?(1)
inference using smart sampling strategies permits
efficient training, allowing us to scale to large
data/vocabulary sizes, (2) incremental scoring of
derivations during sampling allows efficient infer-
ence even when we use higher-order n-gram LMs,
(3) there are no memory bottlenecks since the full
channel model and derivation lattice are never in-
stantiated during training, and (4) prior specification
allows us to learn skewed distributions that are useful
here?word substitution ciphers exhibit 1-to-1 cor-
respondence between plaintext and cipher types.
We use the same generative story as before for
decipherment, except that we use Chinese Restau-
rant Process (CRP) formulations for the source and
channel probabilities. We use an English word bi-
gram LM as the base distribution (P0) for the source
model and specify a uniform P0 distribution for the
14
channel.1 We perform inference using point-wise
Gibbs sampling (Geman and Geman, 1984). We de-
fine a sampling operator that samples plaintext word
choices for every cipher token, one at a time. Using
the exchangeability property, we efficiently score
the probability of each derivation in an incremental
fashion. In addition, we make further improvements
to the sampling procedure which makes it faster.
Smart sample-choice selection: In the original
sampling step, for each cipher token we have to sam-
ple from a list of all possible plaintext choices (10k-
1M English words). There are 100k cipher tokens
in our data which means we have to perform ? 109
sampling operations to make one entire pass through
the data. We have to then repeat this process for
2000 iterations. Instead, we now reduce our choices
in each sampling step.
Say that our current plaintext hypothesis contains
English words X, Y and Z at positions i ? 1, i and
i+1 respectively. In order to sample at position i, we
choose the topK English words Y ranked by P (X Y
Z), which can be computed offline from a statistical
word bigram LM. If this probability is 0 (i.e., X and
Z never co-occurred), we randomly pick K words
from the plaintext vocabulary. We set K = 100 in
our experiments. This significantly reduces the sam-
pling possibilities (10k-1M reduces to 100) at each
step and allows us to scale to large plaintext vocab-
ulary sizes without enumerating all possible choices
at every cipher position.2
Parallelized Gibbs sampling: Secondly, we paral-
lelize our sampling step using a Map-Reduce frame-
work. In the past, others have proposed parallelized
sampling schemes for topic modeling applications
(Newman et al, 2009). In our method, we split the
entire corpus into separate chunks and we run the
sampling procedure on each chunk in parallel. At
1For word substitution decipherment, we want to keep the
language model probabilities fixed during training, and hence
we set the prior on that model to be high (? = 104). We use a
sparse Dirichlet prior for the channel (? = 0.01). We use the
output from Iterative EM decoding (using 101 x 101 channel)
as initial sample and run the sampler for 2000 iterations. Dur-
ing sampling, we use a linear annealing schedule decreasing the
temperature from 1? 0.08.
2Since we now sample from an approximate distribution, we
have to correct this with the Metropolis-Hastings algorithm. But
in practice we observe that samples from our proposal distribu-
tion are accepted with probability > 0.99, so we skip this step.
the end of each sampling iteration, we combine the
samples corresponding to each chunk and collect the
counts of all events?this forms our cache for the
next sampling iteration. In practice, we observe that
the parallelized sampling run converges quickly and
runs much faster than the conventional point-wise
sampling?for example, 3.1 hours (using 10 nodes)
versus 11 hours for one of the word substitution ex-
periments. We also notice a higher speedup when
scaling to larger vocabularies.3
Decoding the ciphertext: After the sampling run
has finished, we choose the final sample and ex-
tract a trained version of the channel model P?(c|e)
from this sample following the technique of Chi-
ang et al (2010). We then use the Viterbi algo-
rithm to choose the English plaintext e that maxi-
mizes P (e) ? P?trained(c|e)
3.
2.3 Experiments and Results
Data: For the word substitution experiments, we use
two corpora:
? Temporal expression corpus containing short
English temporal expressions such as ?THE
NEXT MONTH?, ?THE LAST THREE
YEARS?, etc. The cipher data contains 5000
expressions (9619 tokens, 153 word types).
We also have access to a separate English
corpus (which is not parallel to the ciphertext)
containing 125k temporal expressions (242k
word tokens, 201 word types) for LM training.
? Transtac corpus containing full English sen-
tences. The data consists of 10k cipher sen-
tences (102k tokens, 3397 word types); and
a plaintext corpus of 402k English sentences
(2.7M word tokens, 25761 word types) for LM
training. We use all the cipher data for deci-
pherment training but evaluate on the first 1000
cipher sentences.
The cipher data was originally generated from En-
glish text by substituting each English word with a
unique cipher word. We use the plaintext corpus to
3Type sampling could be applied on top of our methods to
further optimize performance. But more complex problems like
MT do not follow the same principles (1-to-1 key mappings)
as seen in word substitution ciphers, which makes it difficult to
identify type dependencies.
15
Method Decipherment Accuracy (%)
Temporal expr. Transtac
9k 100k
0. EM with 2-gram LM 87.8 Intractable
1. Iterative EM
with 2-gram LM 87.8 70.5 71.8
2. Bayesian
with 2-gram LM 88.6 60.1 80.0
with 3-gram LM 82.5
Figure 1: Comparison of word substitution decipherment
results using (1) Iterative EM, and (2) Bayesian method.
For the Transtac corpus, decipherment performance is
also shown for different training data sizes (9k versus
100k cipher tokens).
build an English word n-gram LM, which is used in
the decipherment process.
Evaluation: We compute the accuracy of a particu-
lar decipherment as the percentage of cipher tokens
that were correctly deciphered from the whole cor-
pus. We run the two methods (Iterative EM4 and
Bayesian) and then compare them in terms of word
substitution decipherment accuracies.
Results: Figure 1 compares the word substitution
results from Iterative EM and Bayesian decipher-
ment. Both methods achieve high accuracies, de-
coding 70-90% of the two word substitution ciphers.
Overall, Bayesian decipherment (with sparse priors)
performs better than Iterative EM and achieves the
best results on this task. We also observe that both
methods benefit from better LMs and more (cipher)
training data. Figure 2 shows sample outputs from
Bayesian decipherment.
3 Machine Translation as a Decipherment
Task
We now turn to the problem of MT without par-
allel data. From a decipherment perspective, ma-
chine translation is a much more complex task than
word substitution decipherment and poses several
technical challenges: (1) scalability due to large
corpora sizes and huge translation tables, (2) non-
determinism in translation mappings (a word can
have multiple translations), (3) re-ordering of words
4For Iterative EM, we start with a channel of size 101x101
(K=100) and in every pass we iteratively increase the vocabu-
lary sizes by 50, repeating the training procedure until the chan-
nel size becomes 351x351.
C: 3894 9411 4357 8446 5433
O: a diploma that?s good .
D: a fence that?s good .
C: 8593 7932 3627 9166 3671
O: three families living here ?
D: three brothers living here ?
C: 6283 8827 7592 6959 5120 6137 9723 3671
O: okay and what did they tell you ?
D: okay and what did they tell you ?
C: 9723 3601 5834 5838 3805 4887 7961 9723 3174 4518
9067 4488 9551 7538 7239 9166 3671
O: you mean if we come to see you in the afternoon after
five you?ll be here ?
D: i mean if we come to see you in the afternoon after thirty
you?ll be here ?
...
Figure 2: Comparison of the original (O) English plain-
text with output from Bayesian word substitution deci-
pherment (D) for a few samples cipher (C) sentences
from the Transtac corpus.
or phrases, (4) a single word can translate into a
phrase, and (5) insertion/deletion of words.
Problem Formulation: We formulate the MT de-
cipherment problem as?given a foreign text f (i.e.,
foreign word sequences f1...fm) and a monolingual
English corpus, our goal is to decipher the foreign
text and produce an English translation.
Probabilistic decipherment: Unlike parallel train-
ing, here we have to estimate the translation model
P?(f |e) parameters using only monolingual data.
During decipherment training, our objective is to es-
timate the model parameters ? in order to maximize
the probability of the foreign corpus f . From Equa-
tion 4 we have:
argmax
?
?
f
?
e
P (e) ? P?(f |e)
For P (e), we use a word n-gram LM trained on
monolingual English data. We then estimate param-
eters of the translation model P?(f |e) during train-
ing. Next, we present two novel decipherment ap-
proaches for MT training without parallel data.
1. EM Decipherment: We propose a new transla-
tion model for MT decipherment which can be
efficiently trained using the EM algorithm.
2. Bayesian Decipherment: We introduce a novel
method for estimating IBM Model 3 parame-
ters without parallel data, using Bayesian learn-
ing. Unlike EM, this method does not face any
16
memory issues and we use sampling to perform
efficient inference during training.
3.1 EM Decipherment
For the translation model P?(f |e), we would like
to use a well-known statistical model such as IBM
Model 3 and subsequently train it using the EM
algorithm. But without parallel training data, EM
training for IBM Model 3 becomes intractable due
to (1) scalability and efficiency issues because of
large-sized fertility and distortion parameter tables,
and (2) the resulting derivation lattices become too
big to be stored in memory.
Instead, we propose a simpler generative story for
MT without parallel data. Our model accounts for
(word) substitutions, insertions, deletions and local
re-ordering during the translation process but does
not incorporate fertilities or global re-ordering. We
describe the generative process here:
1. Generate an English string e = e1...el, with
probability P (e).
2. Insert a NULL word at any position in the En-
glish string, with uniform probability.
3. For each English word token ei (including
NULLs), choose a foreign word translation fi,
with probability P?(fi|ei). The foreign word
may be NULL.
4. Swap any pair of adjacent foreign words
fi?1, fi, with probability P?(swap). We set
this value to 0.1.
5. Output the foreign string f = f1...fm, skipping
over NULLs.
We use the EM algorithm to estimate all the pa-
rameters ? in order to maximize likelihood of the
foreign corpus. Finally, we use the Viterbi algo-
rithm to decode the foreign sentence f and pro-
duce an English translation e that maximizes P (e) ?
P?trained(f |e).
Linguistic knowledge for decipherment: To help
limit translation model size and deal with data spar-
sity problem, we use prior linguistic knowledge. We
use identity mappings for numeric values (for ex-
ample, ?8? maps to ?8?), and we split nouns into
morpheme units prior to decipherment training (for
example, ?YEARS?? ?YEAR? ?+S?).
Whole-segment Language Models: When using
word n-gram models of English for decipherment,
we find that some of the foreign sentences are
decoded into sequences (such as ?THANK YOU
TALKING ABOUT ??) that are not good English.
This stems from the fact that n-gram LMs have no
global information about what constitutes a valid
English segment. To learn this information auto-
matically, we build a P (e) model that only recog-
nizes English whole-segments (entire sentences or
expressions) observed in the monolingual training
data. We then use this model (in place of word n-
gram LMs) for decipherment training and decoding.
3.2 Bayesian Method
Brown et al (1993) provide an efficient algorithm
for training IBM Model 3 translation model when
parallel sentence pairs are available. But we wish
to perform IBM Model 3 training under non-parallel
conditions, which is intractable using EM training.
Instead, we take a Bayesian approach.
Following Equation 5, we represent the transla-
tion model as P?(f, a|e) in terms of hidden align-
ments a. Recall the generative story for IBM Model
3 translation which has the following formula:
P?(f, a|e) =
l?
i=0
t?(faj |ei) ?
l?
i=1
n?(?i|ei)
?
m?
aj 6=0,j=1
d?(aj |i, l,m) ?
l?
i=0
?i!
?
1
?0!
?
(
m? ?0
?0
)
?p?01? ? p
m?2?0
0?
(8)
The alignment a is represented as a vector; aj = i
implies that the foreign word fj is produced by the
English word ei during translation.
Bayesian Formulation: Our goal is to learn the
probability tables t (translation parameters) n (fer-
tility parameters), d (distortion parameters), and p
(English NULL word probabilities) without parallel
data. In order to apply Bayesian inference for de-
cipherment, we model each of these tables using a
17
Chinese Restaurant Process (CRP) formulation. For
example, to model the translation probabilities, we
use the formula:
t?(fj |ei) =
? ? P0(fj |ei) + Chistory(ei, fj)
?+ Chistory(ei)
(9)
where, P0 represents the base distribution (which
is set to uniform) and Chistory represents the count
of events occurring in the history (cache). Similarly,
we use CRP formulations for the other probabilities
(n, d and p). We use sparse Dirichlet priors for all
these models (i.e., low values for ?) and plug these
probabilities into Equation 8 to get P?(f, a|e).
Sampling IBM Model 3: We use point-wise Gibbs
sampling to estimate the IBM Model 3 parameters.
The sampler is seeded with an initial English sample
translation and a corresponding alignment for every
foreign sentence. We define several sampling oper-
ators, which are applied in sequence one after the
other to generate English samples for the entire for-
eign corpus. Some of the sampling operators are de-
scribed below:
? TranslateWord(j): Sample a new English word
translation for foreign word fj , from all possi-
bilities (including NULL).
? SwapSegment(i1, i2): Swap the alignment
links for English words ei1 and ei2 .
? JoinWords(i1, i2): Eliminate the English word
ei1 and transfer its links to the word ei2 .
During sampling, we apply each of these opera-
tors to generate a new derivation e, a for the foreign
text f and compute its score as P (e) ? P?(f, a|e).
These small-change operators are similar to the
heuristic techniques used for greedy decoding by
German et al (2001). But unlike the greedy method,
which can easily get stuck, our Bayesian approach
guarantees that once the sampler converges we will
be sampling from the true posterior distribution.
As with Bayesian decipherment for word sub-
stitution, we compute the probability of each new
derivation incrementally, which makes sampling ef-
ficient. We also apply blocked sampling on top
of point-wise sampling?we treat all occurrences
of a particular foreign sentence as a single block
and sample a single derivation for the entire block.
We also parallelize the sampling procedure (as de-
scribed in Section 2.2).5
Choosing the best translation: Once the sampling
run finishes, we select the final sample and extract
the corresponding English translations for every for-
eign sentence. This yields the final decipherment
output.
3.3 MT Experiments and Results
Data: We work with the Spanish/English language
pair and use the following corpora in our MT exper-
iments:
? Time corpus: We mined English newswire
text on the Web and collected 295k tempo-
ral expressions such as ?LAST YEAR?, ?THE
FOURTH QUARTER?, ?IN JAN 1968?, etc.
We first process the data and normalize num-
bers and names of months/weekdays?for ex-
ample, ?1968? is replaced with ?NNNN?,
?JANUARY? with ?[MONTH]?, and so on. We
then translate the English temporal phrases into
Spanish using an automatic translation soft-
ware (Google Translate) followed by manual
annotation to correct mistakes made by the
software. We create the following splits out of
the resulting parallel corpus:
TRAIN (English): 195k temporal expressions
(7588 unique), 382k word tokens, 163 types.
TEST (Spanish): 100k temporal expressions
(2343 unique), 204k word tokens, 269 types.
? OPUS movie subtitle corpus: This is a large
open source collection of parallel corpora avail-
able for multiple language pairs (Tiedemann,
2009). We downloaded the parallel Span-
ish/English subtitle corpus which consists of
aligned Spanish/English sentences from a col-
lection of movie subtitles. For our MT ex-
periments, we select only Spanish/English sen-
tences with frequency > 10 and create the fol-
lowing train/test splits:
5For Bayesian MT decipherment, we set a high prior value
on the language model (104) and use sparse priors for the IBM 3
model parameters t, n, d, p (0.01, 0.01, 0.01, 0.01). We use the
output from EM decipherment as the initial sample and run the
sampler for 2000 iterations, during which we apply annealing
with a linear schedule (2? 0.08).
18
Method Decipherment Accuracy
Time expressions OPUS subtitles
1a. Parallel training (MOSES)
with 2-gram LM 5.6 (85.6) 26.8 (63.6)
with 5-gram LM 4.7 (88.0)
1b. Parallel training (IBM 3 without distortion)
with 2-gram LM 10.1 (78.9) 29.9 (59.6)
with whole-segment LM 9.0 (79.2)
2a. Decipherment (EM)
with 2-gram LM 37.6 (44.6) 67.2 (15.3)
with whole-segment LM 28.7 (48.7) 65.1 (19.3)
2b. Decipherment (Bayesian IBM 3)
with 2-gram LM 34.0 (30.2) 66.6 (15.1)
Figure 3: Comparison of Spanish/English MT performance on the Time and OPUS test corpora achieved by various
MT systems trained under (1) parallel?(a) MOSES, (b) IBM 3 without distortion, and (2) decipherment settings?
(a) EM, (b) Bayesian. The scores reported here are normalized edit distance values with BLEU scores shown in
parentheses.
TRAIN (English): 19770 sentences (1128
unique), 62k word tokens, 411 word types.
TEST (Spanish): 13181 sentences (1127
unique), 39k word tokens, 562 word types.
Both Spanish/English sides of TRAIN are used for
parallel MT training, whereas decipherment uses
only monolingual English data for training LMs.
MT Systems: We build and compare different MT
systems under two training scenarios:
1. Parallel training using: (a) MOSES, a phrase
translation system (Koehn et al, 2007) widely
used in MT literature, and (b) a simpler version
of IBM Model 3 (without distortion param-
eters) which can be trained tractably using the
strategy of Knight and Al-Onaizan (1998).
2. Decipherment without parallel data using:
(a) EM method (from Section 3.1), and (b)
Bayesian method (from Section 3.2).
Evaluation: All the MT systems are run on the
Spanish test data and the quality of the result-
ing English translations are evaluated using two
different measures?(1) Normalized edit distance
score (Navarro, 2001),6 and (2) BLEU (Papineni et
6When computing edit distance, we account for substitu-
tions, insertions, deletions as well as local-swap edit operations
required to convert a given English string into the (gold) refer-
ence translation.
al., 2002), a standard MT evaluation measure.
Results: Figure 3 compares the results of vari-
ous MT systems (using parallel versus decipherment
training) on the two test corpora in terms of edit dis-
tance scores (a lower score indicates closer match to
the gold translation). The figure also shows the cor-
responding BLEU scores in parentheses for compar-
ison (higher scores indicate better MT output).
We observe that even without parallel training
data, our decipherment strategies achieve MT accu-
racies comparable to parallel-trained systems. On
the Time corpus, the best decipherment (Method
2a in the figure) achieves an edit distance score of
28.7 (versus 4.7 for MOSES). Better LMs yield bet-
ter MT results for both parallel and decipherment
training?for example, using a segment-based En-
glish LM instead of a 2-gram LM yields a 24% re-
duction in edit distance and a 9% improvement in
BLEU score for EM decipherment.
We also investigate how the performance of dif-
ferent MT systems vary with the size of the training
data. Figure 4 plots the BLEU scores versus training
sizes for different MT systems on the Time corpus.
Clearly, using more training data yields better per-
formance for all systems. However, higher improve-
ments are observed when using parallel data in com-
parison to decipherment training which only uses
monolingual data. We also notice that the scores do
not improve much when going beyond 10,000 train-
19
Figure 4: Comparison of training data size versus MT ac-
curacy in terms of BLEU score under different training
conditions: (1) Parallel training?(a) MOSES, (b) IBM
Model 3 without distortion, and (2) Decipherment with-
out parallel data using EM method (from Section 3.1).
ing instances for this domain.
It is interesting to quantify the value of parallel
versus non-parallel data for any given MT task. In
other words, ?how much non-parallel data is worth
how much parallel data in order to achieve the same
MT accuracy?? Figure 4 provides a reasonable an-
swer to this question for the Spanish/English MT
task described here. We see that deciphering with
10k monolingual Spanish sentences yields the same
performance as training with around 200-500 paral-
lel English/Spanish sentence pairs. This is the first
attempt at such a quantitative comparison for MT
and our results are encouraging. We envision that
further developments in unsupervised methods will
help reduce this gap further.
4 Conclusion
Our work is the first attempt at doing MT with-
out parallel data. We discussed several novel deci-
pherment approaches for achieving this goal. Along
the way, we developed efficient training methods
that can deal with large-scale vocabularies and data
sizes. For future work, it will be interesting to see if
we can exploit both parallel and non-parallel data to
improve on both.
Acknowledgments
This material is based in part upon work supported
by the National Science Foundation (NSF) under
Grant No. IIS-0904684 and the Defense Advanced
Research Projects Agency (DARPA) through the
Department of Interior/National Business Center un-
der Contract No. NBCHD040058. Any opinion,
findings and conclusions or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency (DARPA), or
the Department of the Interior/National Business
Center.
References
Friedrich L. Bauer. 2006. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer-Verlag.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (ACL-IJCNLP),
pages 782?790.
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: Parameter estimation. Com-
putational linguistics, 19(2):263?311.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference for
finite-state transducers. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL/HLT), pages 447?455.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
Jenny Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 363?
370.
Pascal Fung and Kathleen McKeown. 1997. Finding ter-
minology translations from non-parallel corpora. In
Proceedings of the Fifth Annual Workshop on Very
Large Corpora, pages 192?202.
20
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting on Association for
Computational Linguistics, pages 228?235.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744?
751.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics - Human Language Technologies
(ACL/HLT), pages 771?779.
Kevin Knight and Yaser Al-Onaizan. 1998. Transla-
tion with finite-state devices. In David Farwell, Laurie
Gerber, and Eduard Hovy, editors, Machine Transla-
tion and the Information Soup, volume 1529 of Lecture
Notes in Computer Science, pages 421?437. Springer
Berlin / Heidelberg.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the Joint Conference of
the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics, pages 499?506.
Philipp Koehn and Kevin Knight. 2000. Estimating word
translation probabilities from unrelated monolingual
corpora using the EM algorithm. In Proceedings of
the Seventeenth National Conference on Artificial In-
telligence and Twelfth Conference on Innovative Ap-
plications of Artificial Intelligence, pages 711?715.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions.
Philip Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the main conference on Human Language Tech-
nology Conference of the North American Chapter of
the Association of Computational Linguistics, pages
152?159.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys, 33:31?88,
March.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics, pages
320?322.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1048?1057.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Warren Weaver. 1955. Translation (1949). Reproduced
in W.N. Locke, A.D. Booth (eds.). In Machine Trans-
lation of Languages, pages 15?23. MIT Press.
21
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 239?247,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Bayesian Inference for Zodiac and Other Homophonic Ciphers
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We introduce a novel Bayesian approach for
deciphering complex substitution ciphers. Our
method uses a decipherment model which
combines information from letter n-gram lan-
guage models as well as word dictionaries.
Bayesian inference is performed on our model
using an efficient sampling technique. We
evaluate the quality of the Bayesian deci-
pherment output on simple and homophonic
letter substitution ciphers and show that un-
like a previous approach, our method consis-
tently produces almost 100% accurate deci-
pherments. The new method can be applied
on more complex substitution ciphers and we
demonstrate its utility by cracking the famous
Zodiac-408 cipher in a fully automated fash-
ion, which has never been done before.
1 Introduction
Substitution ciphers have been used widely in the
past to encrypt secrets behind messages. These
ciphers replace (English) plaintext letters with ci-
pher symbols in order to generate the ciphertext se-
quence.
There exist many published works on automatic
decipherment methods for solving simple letter-
substitution ciphers. Many existing methods use
dictionary-based attacks employing huge word dic-
tionaries to find plaintext patterns within the ci-
phertext (Peleg and Rosenfeld, 1979; Ganesan and
Sherman, 1993; Jakobsen, 1995; Olson, 2007).
Most of these methods are heuristic in nature and
search for the best deterministic key during deci-
pherment. Others follow a probabilistic decipher-
ment approach. Knight et al (2006) use the Expec-
tation Maximization (EM) algorithm (Dempster et
al., 1977) to search for the best probabilistic key us-
ing letter n-gram models. Ravi and Knight (2008)
formulate decipherment as an integer programming
problem and provide an exact method to solve sim-
ple substitution ciphers by using letter n-gram mod-
els along with deterministic key constraints. Corlett
and Penn (2010) work with large ciphertexts con-
taining thousands of characters and provide another
exact decipherment method using an A* search al-
gorithm. Diaconis (2008) presents an analysis of
Markov Chain Monte Carlo (MCMC) sampling al-
gorithms and shows an example application for solv-
ing simple substitution ciphers.
Most work in this area has focused on solving
simple substitution ciphers. But there are variants
of substitution ciphers, such as homophonic ciphers,
which display increasing levels of difficulty and
present significant challenges for decipherment. The
famous Zodiac serial killer used one such cipher sys-
tem for communication. In 1969, the killer sent a
three-part cipher message to newspapers claiming
credit for recent shootings and crimes committed
near the San Francisco area. The 408-character mes-
sage (Zodiac-408) was manually decoded by hand in
the 1960?s. Oranchak (2008) presents a method for
solving the Zodiac-408 cipher automatically with a
dictionary-based attack using a genetic algorithm.
However, his method relies on using plaintext words
from the known solution to solve the cipher, which
departs from a strict decipherment scenario.
In this paper, we introduce a novel method for
239
solving substitution ciphers using Bayesian learn-
ing. Our novel contributions are as follows:
? We present a new probabilistic decipherment
approach using Bayesian inference with sparse
priors, which can be used to solve different
types of substitution ciphers.
? Our new method combines information from
word dictionaries along with letter n-gram
models, providing a robust decipherment
model which offsets the disadvantages faced by
previous approaches.
? We evaluate the Bayesian decipherment output
on three different types of substitution ciphers
and show that unlike a previous approach, our
new method solves all the ciphers completely.
? Using the Bayesian decipherment, we show for
the first time a truly automated system that suc-
cessfully solves the Zodiac-408 cipher.
2 Letter Substitution Ciphers
We use natural language processing techniques to
attack letter substitution ciphers. In a letter substi-
tution cipher, every letter p in the natural language
(plaintext) sequence is replaced by a cipher token c,
according to some substitution key.
For example, an English plaintext
?H E L L O W O R L D ...?
may be enciphered as:
?N O E E I T I M E L ...?
according to the key:
p: ABCDEFGHIJKLMNOPQRSTUVWXYZ
c: XYZLOHANBCDEFGIJKMPQRSTUVW
where, ? ? represents the space character (word
boundary) in the English and ciphertext messages.
If the recipients of the ciphertext message have
the substitution key, they can use it (in reverse) to
recover the original plaintext. The sender can en-
crypt the message using one of many different ci-
pher systems. The particular type of cipher system
chosen determines the properties of the key. For ex-
ample, the substitution key can be deterministic in
both the encipherment and decipherment directions
as shown in the above example?i.e., there is a 1-to-
1 correspondence between the plaintext letters and
ciphertext symbols. Other types of keys exhibit non-
determinism either in the encipherment (or decipher-
ment) or both directions.
2.1 Simple Substitution Ciphers
The key used in a simple substitution cipher is deter-
ministic in both the encipherment and decipherment
directions, i.e., there is a 1-to-1 mapping between
plaintext letters and ciphertext symbols. The exam-
ple shown earlier depicts how a simple substitution
cipher works.
Data: In our experiments, we work with a 414-
letter simple substitution cipher. We encrypt an
original English plaintext message using a randomly
generated simple substitution key to create the ci-
phertext. During the encipherment process, we pre-
serve spaces between words and use this information
for decipherment?i.e., plaintext character ? ? maps
to ciphertext character ? ?. Figure 1 (top) shows
a portion of the ciphertext along with the original
plaintext used to create the cipher.
2.2 Homophonic Ciphers
A homophonic cipher uses a substitution key that
maps a plaintext letter to more than one cipher sym-
bol.
For example, the English plaintext:
?H E L L O W O R L D ...?
may be enciphered as:
?65 82 51 84 05 60 54 42 51 45 ...?
according to the key:
A: 09 12 33 47 53 67 78 92
B: 48 81
...
E: 14 16 24 44 46 55 57 64 74 82 87
...
L: 51 84
...
Z: 02
Here, ? ? represents the space character in both
English and ciphertext. Notice the non-determinism
involved in the enciphering direction?the English
240
letter ?L? is substituted using different symbols (51,
84) at different positions in the ciphertext.
These ciphers are more complex than simple sub-
stitution ciphers. Homophonic ciphers are generated
via a non-deterministic encipherment process?the
key is 1-to-many in the enciphering direction. The
number of potential cipher symbol substitutes for a
particular plaintext letter is often proportional to the
frequency of that letter in the plaintext language?
for example, the English letter ?E? is assigned more
cipher symbols than ?Z?. The objective of this is
to flatten out the frequency distribution of cipher-
text symbols, making a frequency-based cryptanaly-
sis attack difficult.
The substitution key is, however, deterministic in
the decipherment direction?each ciphertext symbol
maps to a single plaintext letter. Since the ciphertext
can contain more than 26 types, we need a larger
alphabet system?we use a numeric substitution al-
phabet in our experiments.
Data: For our decipherment experiments
on homophonic ciphers, we use the same
414-letter English plaintext used in Sec-
tion 2.1. We encrypt this message using a
homophonic substitution key (available from
http://www.simonsingh.net/The Black Chamber/ho
mophoniccipher.htm). As before, we preserve
spaces between words in the ciphertext. Figure 1
(middle) displays a section of the homophonic
cipher (with spaces) and the original plaintext
message used in our experiments.
2.3 Homophonic Ciphers without spaces
(Zodiac-408 cipher)
In the previous two cipher systems, the word-
boundary information was preserved in the cipher.
We now consider a more difficult homophonic ci-
pher by removing space characters from the original
plaintext.
The English plaintext from the previous example
now looks like this:
?HELLOWORLD ...?
and the corresponding ciphertext is:
?65 82 51 84 05 60 54 42 51 45 ...?
Without the word boundary information, typical
dictionary-based decipherment attacks fail on such
ciphers.
Zodiac-408 cipher: Homophonic ciphers with-
out spaces have been used extensively in the past to
encrypt secret messages. One of the most famous
homophonic ciphers in history was used by the in-
famous Zodiac serial killer in the 1960?s. The killer
sent a series of encrypted messages to newspapers
and claimed that solving the ciphers would reveal
clues to his identity. The identity of the Zodiac killer
remains unknown to date. However, the mystery
surrounding this has sparked much interest among
cryptanalysis experts and amateur enthusiasts.
The Zodiac messages include two interesting ci-
phers: (1) a 408-symbol homophonic cipher without
spaces (which was solved manually by hand), and
(2) a similar looking 340-symbol cipher that has yet
to be solved.
Here is a sample of the Zodiac-408 cipher mes-
sage:
...
and the corresponding section from the original
English plaintext message:
I L I K E K I L L I N G P E O P L
E B E C A U S E I T I S S O M U C
H F U N I T I S M O R E F U N T H
A N K I L L I N G W I L D G A M E
I N T H E F O R R E S T B E C A U
S E M A N I S T H E M O S T D A N
G E R O U E A N A M A L O F A L L
T O K I L L S O M E T H I N G G I
...
Besides the difficulty with missing word bound-
aries and non-determinism associated with the key,
the Zodiac-408 cipher poses several additional chal-
lenges which makes it harder to solve than any
standard homophonic cipher. There are spelling
mistakes in the original message (for example,
the English word ?PARADISE? is misspelt as
241
?PARADICE?) which can divert a dictionary-based
attack. Also, the last 18 characters of the plaintext
message does not seem to make any sense (?EBE-
ORIETEMETHHPITI?).
Data: Figure 1 (bottom) displays the Zodiac-408
cipher (consisting of 408 tokens, 54 symbol types)
along with the original plaintext message. We run
the new decipherment method (described in Sec-
tion 3.1) and show that our approach can success-
fully solve the Zodiac-408 cipher.
3 Decipherment
Given a ciphertext message c1...cn, the goal of de-
cipherment is to uncover the hidden plaintext mes-
sage p1...pn. The size of the keyspace (i.e., num-
ber of possible key mappings) that we have to navi-
gate during decipherment is huge?a simple substi-
tution cipher has a keyspace size of 26!, whereas a
homophonic cipher such as the Zodiac-408 cipher
has 2654 possible key mappings.
Next, we describe a new Bayesian decipherment
approach for tackling substitution ciphers.
3.1 Bayesian Decipherment
Bayesian inference methods have become popular
in natural language processing (Goldwater and Grif-
fiths, 2007; Finkel et al, 2005; Blunsom et al, 2009;
Chiang et al, 2010). Snyder et al (2010) proposed
a Bayesian approach in an archaeological decipher-
ment scenario. These methods are attractive for their
ability to manage uncertainty about model parame-
ters and allow one to incorporate prior knowledge
during inference. A common phenomenon observed
while modeling natural language problems is spar-
sity. For simple letter substitution ciphers, the origi-
nal substitution key exhibits a 1-to-1 correspondence
between the plaintext letters and cipher types. It is
not easy to model such information using conven-
tional methods like EM. But we can easily spec-
ify priors that favor sparse distributions within the
Bayesian framework.
Here, we propose a novel approach for decipher-
ing substitution ciphers using Bayesian inference.
Rather than enumerating all possible keys (26! for
a simple substitution cipher), our Bayesian frame-
work requires us to sample only a small number of
keys during the decipherment process.
Probabilistic Decipherment: Our decipherment
method follows a noisy-channel approach. We are
faced with a ciphertext sequence c = c1...cn and
we want to find the (English) letter sequence p =
p1...pn that maximizes the probability P (p|c).
We first formulate a generative story to model the
process by which the ciphertext sequence is gener-
ated.
1. Generate an English plaintext sequence p =
p1...pn, with probability P (p).
2. Substitute each plaintext letter pi with a cipher-
text token ci, with probability P (ci|pi) in order
to generate the ciphertext sequence c = c1...cn.
We build a statistical English language model
(LM) for the plaintext source model P (p), which
assigns a probability to any English letter sequence.
Our goal is to estimate the channel model param-
eters ? in order to maximize the probability of the
observed ciphertext c:
argmax
?
P (c) = argmax
?
?
p
P?(p, c) (1)
= argmax
?
?
p
P (p) ? P?(c|p) (2)
= argmax
?
?
p
P (p) ?
n?
i=1
P?(ci|pi) (3)
We estimate the parameters ? using Bayesian
learning. In our decipherment framework, a Chinese
Restaurant Process formulation is used to model
both the source and channel. The detailed genera-
tive story using CRPs is shown below:
1. i? 1
2. Generate the English plaintext letter p1, with
probability P0(p1)
3. Substitute p1 with cipher token c1, with proba-
bility P0(c1|p1)
4. i? i+ 1
5. Generate English plaintext letter pi, with prob-
ability
? ? P0(pi|pi?1) + C
i?1
1 (pi?1, pi)
?+ Ci?11 (pi?1)
242
Plaintext: D E C I P H E R M E N T I S T H E A N A L Y S I S O F D O C U M E N T S
W R I T T E N I N A N C I E N T L A N G U A G E S W H E R E T H E ...
Ciphertext: i n g c m p n q s n w f c v f p n o w o k t v c v h u i h g z s n w f v
r q c f f n w c w o w g c n w f k o w a z o a n v r p n q n f p n ...
Bayesian solution: D E C I P H E R M E N T I S T H E A N A L Y S I S O F D O C U M E N T S
W R I T T E N I N A N C I E N T L A N G U A G E S W H E R E T H E ...
Plaintext: D E C I P H E R M E N T I S T H E A N A L Y S I S
O F D O C U M E N T S W R I T T E N I N ...
Ciphertext: 79 57 62 93 95 68 44 77 22 74 59 97 32 86 85 56 82 67 59 67 84 52 86 73 11
99 10 45 90 13 61 27 98 71 49 19 60 80 88 85 20 55 59 32 91 ...
Bayesian solution: D E C I P H E R M E N T I S T H E A N A L Y S I S
O F D O C U M E N T S W R I T T E N I N ...
Ciphertext:
Plaintext:
Bayesian solution (final decoding): I L I K E K I L L I N G P E O P L E B E C A U S E
I T I S S O M U C H F U N I T I A M O R E F U N T
H A N K I L L I N G W I L D G A M E I N T H E F O
R R E S T B E C A U S E M A N I S T H E M O A T D
A N G E R T U E A N A M A L O F A L L ...
(with spaces shown): I L I K E K I L L I N G P E O P L E B E C A U S E
I T I S S O M U C H F U N I T I A M O R E
F U N T H A N K I L L I N G W I L D G A M E I N
T H E F O R R E S T B E C A U S E M A N I S T H E
M O A T D A N G E R T U E A N A M A L O F A L L ...
Figure 1: Samples from the ciphertext sequence, corresponding English plaintext message and output from Bayesian
decipherment (using word+3-gram LM) for three different ciphers: (a) Simple Substitution Cipher (top), (b) Homo-
phonic Substitution Cipher with spaces (middle), and (c) Zodiac-408 Cipher (bottom).
243
6. Substitute pi with cipher token ci, with proba-
bility
? ? P0(ci|pi) + C
i?1
1 (pi, ci)
? + Ci?11 (pi)
7. With probability Pquit, quit; else go to Step 4.
This defines the probability of any given deriva-
tion, i.e., any plaintext hypothesis corresponding to
the given ciphertext sequence. The base distribu-
tion P0 represents prior knowledge about the model
parameter distributions. For the plaintext source
model, we use probabilities from an English lan-
guage model and for the channel model, we spec-
ify a uniform distribution (i.e., a plaintext letter can
be substituted with any given cipher type with equal
probability). Ci?11 represents the count of events
occurring before plaintext letter pi in the derivation
(we call this the ?cache?). ? and ? represent Dirich-
let prior hyperparameters over the source and chan-
nel models respectively. A large prior value implies
that characters are generated from the base distribu-
tion P0, whereas a smaller value biases characters
to be generated with reference to previous decisions
inside the cache (favoring sparser distributions).
Efficient inference via type sampling: We use a
Gibbs sampling (Geman and Geman, 1984) method
for performing inference on our model. We could
follow a point-wise sampling strategy, where we
sample plaintext letter choices for every cipher to-
ken, one at a time. But we already know that the
substitution ciphers described here exhibit determin-
ism in the deciphering direction,1 i.e., although we
have no idea about the key mappings themselves,
we do know that there exists only a single plaintext
letter mapping for every cipher symbol type in the
true key. So sampling plaintext choices for every
cipher token separately is not an efficient strategy?
our sampler may spend too much time exploring in-
valid keys (which map the same cipher symbol to
different plaintext letters).
Instead, we use a type sampling technique similar
to the one proposed by Liang et al (2010). Under
1This assumption does not strictly apply to the Zodiac-408
cipher where a few cipher symbols exhibit non-determinism in
the decipherment direction as well.
this scheme, we sample plaintext letter choices for
each cipher symbol type. In every step, we sample
a new plaintext letter for a cipher type and update
the entire plaintext hypothesis (i.e., plaintext letters
at all corresponding positions) to reflect this change.
For example, if we sample a new choice pnew for
a cipher symbol which occurs at positions 4, 10, 18,
then we update plaintext letters p4, p10 and p18 with
the new choice pnew.
Using the property of exchangeability, we derive
an incremental formula for re-scoring the probabil-
ity of a new derivation based on the probability of
the old derivation?when sampling at position i, we
pretend that the area affected (within a context win-
dow around i) in the current plaintext hypothesis oc-
curs at the end of the corpus, so that both the old
and new derivations share the same cache.2 While
we may make corpus-wide changes to a derivation
in every sampling step, exchangeability allows us to
perform scoring in an efficient manner.
Combining letter n-gram language models with
word dictionaries: Many existing probabilistic ap-
proaches use statistical letter n-gram language mod-
els of English to assign P (p) probabilities to plain-
text hypotheses during decipherment. Other de-
cryption techniques rely on word dictionaries (using
words from an English dictionary) for attacking sub-
stitution ciphers.
Unlike previous approaches, our decipherment
method combines information from both sources?
letter n-grams and word dictionaries. We build an
interpolated word+n-gram LM and use it to assign
P (p) probabilities to any plaintext letter sequence
p1...pn.3 The advantage is that it helps direct the
sampler towards plaintext hypotheses that resemble
natural language?high probability letter sequences
which form valid words such as ?H E L L O? in-
stead of sequences like ??T X H R T?. But in ad-
dition to this, using letter n-gram information makes
2The relevant context window that is affected when sam-
pling at position i is determined by the word boundaries to the
left and right of i.
3We set the interpolation weights for the word and n-gram
LM as (0.9, 0.1). The word-based LM is constructed from a
dictionary consisting of 9,881 frequently occurring words col-
lected from Wikipedia articles. We train the letter n-gram LM
on 50 million words of English text available from the Linguis-
tic Data Consortium.
244
our model robust against variations in the origi-
nal plaintext (for example, unseen words or mis-
spellings as in the case of Zodiac-408 cipher) which
can easily throw off dictionary-based attacks. Also,
it is hard for a point-wise (or type) sampler to ?find
words? starting from a random initial sample, but
easier to ?find n-grams?.
Sampling for ciphers without spaces: For ciphers
without spaces, dictionaries are hard to use because
we do not know where words start and end. We in-
troduce a new sampling operator which counters this
problem and allows us to perform inference using
the same decipherment model described earlier. In
a first sampling pass, we sample from 26 plaintext
letter choices (e.g., ?A?, ?B?, ?C?, ...) for every ci-
pher symbol type as before. We then run a second
pass using a new sampling operator that iterates over
adjacent plaintext letter pairs pi?1, pi in the current
hypothesis and samples from two choices?(1) add
a word boundary (space character ? ?) between pi?1
and pi, or (2) remove an existing space character be-
tween pi?1 and pi.
For example, given the English plaintext hypoth-
esis ?... A B O Y ...?, there are two sam-
pling choices for the letter pair A,B in the second
step. If we decide to add a word boundary, our new
plaintext hypothesis becomes ?... A B O Y
...?.
We compute the derivation probability of the new
sample using the same efficient scoring procedure
described earlier. The new strategy allows us to ap-
ply Bayesian decipherment even to ciphers without
spaces. As a result, we now have a new decipher-
ment method that consistently works for a range of
different types of substitution ciphers.
Decoding the ciphertext: After the sampling run
has finished,4 we choose the final sample as our En-
glish plaintext decipherment output.
4For letter substitution decipherment we want to keep the
language model probabilities fixed during training, and hence
we set the prior on that model to be high (? = 104). We use
a sparse prior for the channel (? = 0.01). We instantiate a key
which matches frequently occurring plaintext letters to frequent
cipher symbols and use this to generate an initial sample for the
given ciphertext and run the sampler for 5000 iterations. We
use a linear annealing schedule during sampling decreasing the
temperature from 10? 1.
4 Experiments and Results
We run decipherment experiments on different types
of letter substitution ciphers (described in Sec-
tion 2). In particular, we work with the following
three ciphers:
(a) 414-letter Simple Substitution Cipher
(b) 414-letter Homophonic Cipher (with spaces)
(c) Zodiac-408 Cipher
Methods: For each cipher, we run and compare the
output from two different decipherment approaches:
1. EM Method using letter n-gram LMs follow-
ing the approach of Knight et al (2006). They
use the EM algorithm to estimate the chan-
nel parameters ? during decipherment training.
The given ciphertext c is then decoded by us-
ing the Viterbi algorithm to choose the plain-
text decoding p that maximizes P (p)?P?(c|p)3,
stretching the channel probabilities.
2. Bayesian Decipherment method using
word+n-gram LMs (novel approach described
in Section 3.1).
Evaluation: We evaluate the quality of a particular
decipherment as the percentage of cipher tokens that
are decoded correctly.
Results: Figure 2 compares the decipherment per-
formance for the EM method with Bayesian deci-
pherment (using type sampling and sparse priors)
on three different types of substitution ciphers. Re-
sults show that our new approach (Bayesian) out-
performs the EM method on all three ciphers, solv-
ing them completely. Even with a 3-gram letter LM,
our method yields a +63% improvement in decipher-
ment accuracy over EM on the homophonic cipher
with spaces. We observe that the word+3-gram LM
proves highly effective when tackling more complex
ciphers and cracks the Zodiac-408 cipher. Figure 1
shows samples from the Bayesian decipherment out-
put for all three ciphers. For ciphers without spaces,
our method automatically guesses the word bound-
aries for the plaintext hypothesis.
245
Method LM Accuracy (%) on 414-letter
Simple Substitution Cipher
Accuracy (%) on 414-letter
Homophonic Substitution
Cipher (with spaces)
Accuracy (%) on Zodiac-
408 Cipher
1. EM 2-gram 83.6 30.9
3-gram 99.3 32.6 0.3?
(?28.8 with 100 restarts)
2. Bayesian 3-gram 100.0 95.2 23.0
word+2-gram 100.0 100.0
word+3-gram 100.0 100.0 97.8
Figure 2: Comparison of decipherment accuracies for EM versus Bayesian method when using different language
models of English on the three substitution ciphers: (a) 414-letter Simple Substitution Cipher, (b) 414-letter Homo-
phonic Substitution Cipher (with spaces), and (c) the famous Zodiac-408 Cipher.
For the Zodiac-408 cipher, we compare the per-
formance achieved by Bayesian decipherment under
different settings:
? Letter n-gram versus Word+n-gram LMs?
Figure 2 shows that using a word+3-gram LM
instead of a 3-gram LM results in +75% im-
provement in decipherment accuracy.
? Sparse versus Non-sparse priors?We find that
using a sparse prior for the channel model (? =
0.01 versus 1.0) helps for such problems and
produces better decipherment results (97.8%
versus 24.0% accuracy).
? Type versus Point-wise sampling?Unlike
point-wise sampling, type sampling quickly
converges to better decipherment solutions.
After 5000 sampling passes over the entire
data, decipherment output from type sampling
scores 97.8% accuracy compared to 14.5% for
the point-wise sampling run.5
We also perform experiments on shorter substitu-
tion ciphers. On a 98-letter simple substitution ci-
pher, EM using 3-gram LM achieves 41% accuracy,
whereas the method from Ravi and Knight (2009)
scores 84% accuracy. Our Bayesian method per-
forms the best in this case, achieving 100% with
word+3-gram LM.
5 Conclusion
In this work, we presented a novel Bayesian deci-
pherment approach that can effectively solve a va-
5Both sampling runs were seeded with the same random ini-
tial sample.
riety of substitution ciphers. Unlike previous ap-
proaches, our method combines information from
letter n-gram language models and word dictionar-
ies and provides a robust decipherment model. We
empirically evaluated the method on different substi-
tution ciphers and achieve perfect decipherments on
all of them. Using Bayesian decipherment, we can
successfully solve the Zodiac-408 cipher?the first
time this is achieved by a fully automatic method in
a strict decipherment scenario.
For future work, there are other interesting deci-
pherment tasks where our method can be applied.
One challenge is to crack the unsolved Zodiac-340
cipher, which presents a much harder problem than
the solved version.
Acknowledgements
The authors would like to thank the reviewers for
their comments. This research was supported by
NSF grant IIS-0904684.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (ACL-IJCNLP),
pages 782?790.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference for
finite-state transducers. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL/HLT), pages 447?455.
246
Eric Corlett and Gerald Penn. 2010. An exact A* method
for deciphering letter-substitution ciphers. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1040?1047.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
Persi Diaconis. 2008. The Markov Chain Monte Carlo
revolution. Bulletin of the American Mathematical So-
ciety, 46(2):179?205.
Jenny Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 363?
370.
Ravi Ganesan and Alan T. Sherman. 1993. Statistical
techniques for language recognition: An introduction
and guide for cryptanalysts. Cryptologia, 17(4):321?
366.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744?
751.
Thomas Jakobsen. 1995. A fast method for cryptanalysis
of substitution ciphers. Cryptologia, 19(3):265?274.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the Joint Conference of
the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics, pages 499?506.
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-based MCMC. In Proceedings of the Conference
on Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 573?
581.
Edwin Olson. 2007. Robust dictionary attack of short
simple substitution ciphers. Cryptologia, 31(4):332?
342.
David Oranchak. 2008. Evolutionary algorithm for de-
cryption of monoalphabetic homophonic substitution
ciphers encoded as constraint satisfaction problems. In
Proceedings of the 10th Annual Conference on Genetic
and Evolutionary Computation, pages 1717?1718.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Comm. ACM, 22(11):598?605.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the Empirical Methods in
Natural Language Processing (EMNLP), pages 812?
819.
Sujith Ravi and Kevin Knight. 2009. Probabilistic meth-
ods for a Japanese syllable cipher. In Proceedings
of the International Conference on the Computer Pro-
cessing of Oriental Languages (ICCPOL), pages 270?
281.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1048?1057.
247
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 77?82,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Rhyme Schemes
Sravana Reddy
Department of Computer Science
The University of Chicago
Chicago, IL 60637
sravana@cs.uchicago.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
This paper describes an unsupervised,
language-independent model for finding
rhyme schemes in poetry, using no prior
knowledge about rhyme or pronunciation.
1 Introduction
Rhyming stanzas of poetry are characterized by
rhyme schemes, patterns that specify how the lines
in the stanza rhyme with one another. The question
we raise in this paper is: can we infer the rhyme
scheme of a stanza given no information about pro-
nunciations or rhyming relations among words?
Background A rhyme scheme is represented as a
string corresponding to the sequence of lines that
comprise the stanza, in which rhyming lines are de-
noted by the same letter. For example, the limerick?s
rhyme scheme is aabba, indicating that the 1st, 2nd,
and 5th lines rhyme, as do the the 3rd and 4th.
Motivation Automatic rhyme scheme annotation
would benefit several research areas, including:
? Machine Translation of Poetry There has been
a growing interest in translation under con-
straints of rhyme and meter, which requires
training on a large amount of annotated poetry
data in various languages.
? ?Culturomics? The field of digital humanities
is growing, with a focus on statistics to track
cultural and literary trends (partially spurred
by projects like the Google Books Ngrams1).
1http://ngrams.googlelabs.com/
Rhyming corpora could be extremely useful for
large-scale statistical analyses of poetic texts.
? Historical Linguistics/Study of Dialects
Rhymes of a word in poetry of a given time
period or dialect region provide clues about its
pronunciation in that time or dialect, a fact that
is often taken advantage of by linguists (Wyld,
1923). One could automate this task given
enough annotated data.
An obvious approach to finding rhyme schemes
is to use word pronunciations and a definition of
rhyme, in which case the problem is fairly easy.
However, we favor an unsupervised solution that uti-
lizes no external knowledge for several reasons.
? Pronunciation dictionaries are simply not avail-
able for many languages. When dictionaries
are available, they do not include all possible
words, or account for different dialects.
? The definition of rhyme varies across poetic
traditions and languages, and may include
slant rhymes like gate/mat, ?sight rhymes? like
word/sword, assonance/consonance like shore/
alone, leaves/lance, etc.
? Pronunciations and spelling conventions
change over time. Words that rhymed histori-
cally may not anymore, like prove and love ?
or proued and beloued.
2 Related Work
There have been a number of recent papers on the
automated annotation, analysis, or translation of po-
77
etry. Greene et al (2010) use a finite state trans-
ducer to infer the syllable-stress assignments in lines
of poetry under metrical constraints. Genzel et al
(2010) incorporate constraints on meter and rhyme
(where the stress and rhyming information is derived
from a pronunciation dictionary) into a machine
translation system. Jiang and Zhou (2008) develop a
system to generate the second line of a Chinese cou-
plet given the first. A few researchers have also ex-
plored the problem of poetry generation under some
constraints (Manurung et al, 2000; Netzer et al,
2009; Ramakrishnan et al, 2009). There has also
been some work on computational approaches to
characterizing rhymes (Byrd and Chodorow, 1985)
and global properties of the rhyme network (Son-
deregger, 2011) in English. To the best of our knowl-
edge, there has been no language-independent com-
putational work on finding rhyme schemes.
3 Finding Stanza Rhyme Schemes
A collection of rhyming poetry inevitably contains
repetition of rhyming pairs. For example, the word
trees will often rhyme with breeze across different
stanzas, even those with different rhyme schemes
and written by different authors. This is partly due
to sparsity of rhymes ? many words that have no
rhymes at all, and many others have only a handful,
forcing poets to reuse rhyming pairs.
In this section, we describe an unsupervised al-
gorithm to infer rhyme schemes that harnesses this
repetition, based on a model of stanza generation.
3.1 Generative Model of a Stanza
1. Pick a rhyme scheme r of length n with proba-
bility P (r).
2. For each i ? [1, n], pick a word sequence,
choosing the last2 word xi as follows:
(a) If, according to r, the ith line does not
rhyme with any previous line in the stanza, pick
a word xi from a vocabulary of line-end words
with probability P (xi).
(b) If the ith line rhymes with some previous
line(s) j according to r, choose a word xi that
2A rhyme may span more than one word in a line ? for ex-
ample, laureate... / Tory at... / are ye at (Byron, 1824), but this
is uncommon. An extension of our model could include a latent
variable that selects the entire rhyming portion of a line.
rhymes with the last words of all such lines
with probability
?
j<i:ri=rj
P (xi|xj).
The probability of a stanza x of length n is given
by Eq. 1. Ii,r is the indicator variable for whether
line i rhymes with at least one previous line under r.
P (x) =
?
r?R
P (r)P (x|r) =
?
r?R
P (r)
n?
i=1
(1? Ii,r)P (xi) + Ii,r
?
j<i:ri=rj
P (xi|xj) (1)
3.2 Learning
We denote our data by X , a set of stanzas. Each
stanza x is represented as a sequence of its line-end
words, xi, . . . xlen(x). We are also given a large set
R of all possible rhyme schemes.3
If each stanza in the data is generated indepen-
dently (an assumption we relax in ?4), the log-
likelihood of the data is
?
x?X logP (x). We would
like to maximize this over all possible rhyme scheme
assignments, under the latent variables ?, which rep-
resents pairwise rhyme strength, and ?, the distribu-
tion of rhyme schemes. ?v,w is defined for all words
v and w as a non-negative real value indicating how
strongly the words v and w rhyme, and ?r is P (r).
The expectation maximization (EM) learning al-
gorithm for this formulation is described below. The
intuition behind the algorithm is this: after one iter-
ation, ?v,w = 0 for all v and w that never occur to-
gether in a stanza. If v and w co-occur in more than
one stanza, ?v,w has a high pseudo-count, reflecting
the fact that they are likely to be rhymes.
Initialize: ? and ? uniformly (giving ? the same
positive value for all word pairs).
Expectation Step: Compute P (r|x) =
P (x|r)?r/
?
q?R P (x|q)?q, where
P (x|r) =
n?
i=1
(1? Ii,r)P (xi) +
Ii,r
?
j<i:ri=rj
?xi,xj/
?
w
?w,xi (2)
3While the number of rhyme schemes of length n is tech-
nically the number of partitions of an n- element set (the Bell
number), only a subset of these are typically used.
78
P (xi) is simply the relative frequency of the
word xi in the data.
Maximization Step: Update ? and ?:
?v,w =
?
r,x:v rhymes with w
P (r|x) (3)
?r =
?
x?X
P (r|x)/
?
q?R,x?X
P (q|x) (4)
After Convergence: Label each stanza x with the
best rhyme scheme, argmaxr?R P (r|x).
3.3 Data
We test the algorithm on rhyming poetry in En-
glish and French. The English data is an edited ver-
sion of the public-domain portion of the corpus used
by Sonderegger (2011), and consists of just under
12000 stanzas spanning a range of poets and dates
from the 15th to 20th centuries. The French data
is from the ARTFL project (Morrissey, 2011), and
contains about 3000 stanzas. All poems in the data
are manually annotated with rhyme schemes.
The set R is taken to be all the rhyme schemes
from the gold standard annotations of both corpora,
numbering 462 schemes in total, with an average of
6.5 schemes per stanza length. There are 27.12 can-
didate rhyme schemes on an average for each En-
glish stanza, and 33.81 for each French stanza.
3.4 Results
We measure the accuracy of the discovered rhyme
schemes relative to the gold standard. We also eval-
uate for each word token xi, the set of words in
{xi+1, xi+2, . . .} that are found to rhyme with xi by
measuring precision and recall. This is to account
for partial correctness ? if abcb is found instead of
abab, for example, we would like to credit the algo-
rithm for knowing that the 2nd and 4th lines rhyme.
Table 1 shows the results of the algorithm for the
entire corpus in each language, as well as for a few
sub-corpora from different time periods.
3.5 Orthographic Similarity Bias
So far, we have relied on the repetition of rhymes,
and have made no assumptions about word pronun-
ciations. Therefore, the algorithm?s performance
is strongly correlated4 with the predictability of
rhyming words. For writing systems where the
written form of a word approximates its pronunci-
ation, we have some additional information about
rhyming: for example, English words ending with
similar characters are most probably rhymes. We
do not want to assume too much in the interest of
language-independence ? following from our earlier
point in ?1 about the nebulous definition of rhyme
? but it is safe to say that rhyming words involve
some orthographic similarity (though this does not
hold for writing systems like Chinese). We therefore
initialize ? at the start of EM with a simple similarity
measure: (Eq. 5). The addition of  = 0.001 ensures
that words with no letters in common, like new and
you, are not eliminated as rhymes.
?v,w =
# letters common to v & w
min(len(v), len(w))
+  (5)
This simple modification produces results that
outperform the na??ve baselines for most of the data
by a considerable margin, as detailed in Table 2.
3.6 Using Pronunciation, Rhyming Definition
How does our algorithm compare to a standard sys-
tem where rhyme schemes are determined by pre-
defined rules of rhyming and dictionary pronunci-
ations? We use the accepted definition of rhyme
in English: two words rhyme if their final stressed
vowels and all following phonemes are identical.
For every pair of English words v, w, we let ?v,w =
1 +  if the CELEX (Baayen et al, 1995) pronun-
ciations of v and w rhyme, and ?v,w = 0 +  if not
(with  = 0.001). If either v or w is not present
in CELEX, we set ?v,w to a random value in [0, 1].
We then find the best rhyme scheme for each stanza,
using Eq. 2 with uniformly initialized ?.
Figure 1 shows that the accuracy of this system
is generally much lower than that of our model for
the sub-corpora from before 1750. Performance is
comparable for the 1750-1850 data, after which we
get better accuracies using the rhyming definition
than with our model. This is clearly a reflection of
language change; older poetry differs more signifi-
cantly in pronunciation and lexical usage from con-
4For the five English sub-corpora,R2 = 0.946 for the nega-
tive correlation of accuracy with entropy of rhyming word pairs.
79
Table 1: Rhyme scheme accuracy and F-Score (computed from average precision and recall over all lines) using our algorithm
for independent stanzas, with uniform initialization of ?. Rows labeled ?All? refer to training and evaluation on all the data in the
language. Other rows refer to training and evaluating on a particular sub-corpus only. Bold indicates that we outperform the na??ve
baseline, where most common scheme of the appropriate length from the gold standard of the entire corpus is assigned to every
stanza, and italics that we outperform the ?less na??ve? baseline, where we assign the most common scheme of the appropriate length
from the gold standard of the given sub-corpus.
Sub-corpus Sub-corpus overview Accuracy (%) F-Score
(time- # of Total # # of line- EM Na??ve Less na??ve EM Na??ve Less
period) stanzas of lines end words induction baseline baseline induction baseline na??ve
En
All 11613 93030 13807 62.15 56.76 60.24 0.79 0.74 0.77
1450-1550 197 1250 782 17.77 53.30 97.46 0.41 0.73 0.98
1550-1650 3786 35485 7826 67.17 62.28 74.72 0.82 0.78 0.85
1650-1750 2198 20110 4447 87.58 58.42 82.98 0.94 0.68 0.91
1750-1850 2555 20598 5188 31.00 69.16 74.52 0.65 0.83 0.87
1850-1950 2877 15587 4382 50.92 37.43 49.70 0.81 0.55 0.68
Fr
All 2814 26543 10781 40.29 39.66 64.46 0.58 0.57 0.80
1450-1550 1478 14126 7122 28.21 58.66 77.67 0.59 0.83 0.89
1550-1650 1336 12417 5724 52.84 18.64 61.23 0.70 0.28 0.75
temporary dictionaries, and therefore, benefits more
from a model that assumes no pronunciation knowl-
edge. (While we may get better results on older
data using dictionaries that are historically accurate,
these are not easily available, and require a great
deal of effort and linguistic knowledge to create.)
Initializing ? as specified above and then running
EM produces some improvement compared to or-
thographic similarity (Table 2).
4 Accounting for Stanza Dependencies
So far, we have treated stanzas as being indepen-
dent of each other. In reality, stanzas in a poem are
usually generated using the same or similar rhyme
schemes. Furthermore, some rhyme schemes span
multiple stanzas ? for example, the Italian form terza
rima has the scheme aba bcb cdc... (the 1st and 3rd
lines rhyme with the 2nd line of the previous stanza).
4.1 Generative Model
We model stanza generation within a poem as a
Markov process, where each stanza is conditioned
on the previous one. To generate a poem y consist-
ing of m stanzas, for each k ? [1,m], generate a
stanza xk of length nk as described below:
1. If k = 1, pick a rhyme scheme rk of length nk
with probability P (rk), and generate the stanza
as in the previous section.
Figure 1: Comparison of EM with a definition-based system
0?
0.2?
0.4?
0.6?
0.8?
1?
1.2?
1.4?
1.6?
1450-1550 1550-1650 1650-1750 1750-1850 1850-1950 
Ra
tio
 of
 rh
ym
ing
 ru
les
 to
 
EM
 pe
rfo
rm
an
ce
 
Accuracy 
F-Score 
(a) Accuracy and F-Score ratios of the rhyming-definition-
based system over that of our model with orthographic sim-
ilarity. The former is more accurate than EM for post-1850
data (ratio > 1), but is outperformed by our model for older
poetry (ratio< 1), largely due to pronunciation changes like
the Great Vowel Shift that alter rhyming relations.
Found by EM Found by definitions
1450-1550 left/craft, shone/done edify/lie, adieu/hue
1550-1650 appeareth/weareth, obtain/vain, amend/
speaking/breaking, depend, breed/heed,
proue/moue, doe/two prefers/hers
1650-1750 most/cost, presage/ see/family, blade/
rage, join?d/mind shade, noted/quoted
1750-1850 desponds/wounds, gore/shore, ice/vice,
o?er/shore, it/basket head/tread, too/blew
1850-1950 of/love, lover/ old/enfold, within/
half-over, again/rain win, be/immortality
(b) Some examples of rhymes in English found by EM but not
the definition-based system (due to divergence from the contem-
porary dictionary or rhyming definition), and vice-versa (due to
inadequate repetition).
80
Table 2: Performance of EM with ? initialized by orthographic similarity (?3.5), pronunciation-based rhyming definitions (?3.6),
and the HMM for stanza dependencies (?4). Bold and italics indicate that we outperform the na??ve baselines shown in Table 1.
Sub-corpus Accuracy (%) F-Score
(time- HMM Rhyming Orthographic Uniform HMM Rhyming Ortho. Uniform
period) stanzas definition init. initialization initialization stanzas defn. init. init. init.
En
All 72.48 64.18 63.08 62.15 0.88 0.84 0.83 0.79
1450-1550 74.31 75.63 69.04 17.77 0.86 0.86 0.82 0.41
1550-1650 79.17 69.76 71.98 67.17 0.90 0.86 0.88 0.82
1650-1750 91.23 91.95 89.54 87.58 0.97 0.97 0.96 0.94
1750-1850 49.11 42.74 33.62 31.00 0.82 0.77 0.70 0.65
1850-1950 58.95 57.18 54.05 50.92 0.90 0.89 0.84 0.81
Fr
All 56.47 - 48.90 40.29 0.81 - 0.75 0.58
1450-1550 61.28 - 35.25 28.21 0.86 - 0.71 0.59
1550-1650 67.96 - 63.40 52.84 0.79 - 0.77 0.70
2. If k > 1, pick a scheme rk of length nk with
probability P (rk|rk?1). If no rhymes in rk
are shared with the previous stanza?s rhyme
scheme, rk?1, generate the stanza as before.
If rk shares rhymes with rk?1, generate the
stanza as a continuation of xk?1. For exam-
ple, if xk?1 = [dreams, lay, streams], and rk?1
and rk = aba and bcb, the stanza xk should be
generated so that xk1 and x
k
3 rhyme with lay.
4.2 Learning
This model for a poem can be formalized as an au-
toregressive HMM, an hidden Markov model where
each observation is conditioned on the previous ob-
servation as well as the latent state. An observation
at a time step k is the stanza xk, and the latent state at
that time step is the rhyme scheme rk. This model is
parametrized by ? and ?, where ?r,q = P (r|q) for all
schemes r and q. ? is initialized with orthographic
similarity. The learning algorithm follows from EM
for HMMs and our earlier algorithm.
Expectation Step: Estimate P (r|x) for each
stanza in the poem using the forward-backward
algorithm. The ?emission probability? P (x|r)
for the first stanza is same as in ?3, and for
subsequent stanzas xk, k > 1 is given by:
P (xk|xk?1, rk) =
nk?
i=1
(1? Ii,rk)P (x
k
i ) +
Ii,rk
?
j<i:rki =r
k
j
P (xki |x
k
j )
?
j:rki =r
k?1
j
P (xki |x
k?1
j ) (6)
Maximization Step: Update ? and ? analogously
to HMM transition and emission probabilities.
4.3 Results
As Table 2 shows, there is considerable improve-
ment over models that assume independent stanzas.
The most gains are found in French, which contains
many instances of ?linked? stanzas like the terza
rima, as well as English data containing long poems
made of several stanzas with the same scheme.
5 Future Work
Some possible extensions of our work include au-
tomatically generating the set of possible rhyme
schemes R, and incorporating partial supervision
into our algorithm as well as better ways of using
and adapting pronunciation information when avail-
able. We would also like to test our method on a
range of languages and texts.
To return to the motivations, one could use
the discovered annotations for machine translation
of poetry, or to computationally reconstruct pro-
nunciations, which is useful for historical linguis-
tics as well as other applications involving out-of-
vocabulary words.
Acknowledgments
We would like to thank Morgan Sonderegger for
providing most of the annotated English data in the
rhyming corpus and for helpful discussion, and the
anonymous reviewers for their suggestions.
81
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (CD-ROM). Linguistic
Data Consortium.
Roy J. Byrd and Martin S. Chodorow. 1985. Using an
online dictionary to find rhyming words and pronunci-
ations for unknown words. In Proceedings of ACL.
Lord Byron. 1824. Don Juan.
Dmitriy Genzel, Jakob Uszkoreit, and Franz Och. 2010.
?Poetic? statistical machine translation: Rhyme and
meter. In Proceedings of EMNLP.
Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010.
Automatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceedings of
EMNLP.
Long Jiang and Ming Zhou. 2008. Generating Chinese
couplets using a statistical MT approach. In Proceed-
ings of COLING.
Hisar Maruli Manurung, Graeme Ritchie, and Henry
Thompson. 2000. Towards a computational model of
poetry generation. In Proceedings of AISB Symposium
on Creative and Cultural Aspects and Applications of
AI and Cognitive Science.
Robert Morrissey. 2011. ARTFL : American research
on the treasury of the French language. http://artfl-
project.uchicago.edu/content/artfl-frantext.
Yael Netzer, David Gabay, Yoav Goldberg, and Michael
Elhadad. 2009. Gaiku : Generating Haiku with word
associations norms. In Proceedings of the NAACL
workshop on Computational Approaches to Linguistic
Creativity.
Ananth Ramakrishnan, Sankar Kuppan, and
Sobha Lalitha Devi. 2009. Automatic genera-
tion of Tamil lyrics for melodies. In Proceedings of
the NAACL workshop on Computational Approaches
to Linguistic Creativity.
Morgan Sonderegger. 2011. Applications of graph the-
ory to an English rhyming corpus. Computer Speech
and Language, 25:655?678.
Henry Wyld. 1923. Studies in English rhymes from Sur-
rey to Pope. J Murray, London.
82
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 80?84,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Decoding Running Key Ciphers
Sravana Reddy?
Department of Computer Science
The University of Chicago
1100 E. 58th Street
Chicago, IL 60637, USA
sravana@cs.uchicago.edu
Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292, USA
knight@isi.edu
Abstract
There has been recent interest in the problem
of decoding letter substitution ciphers using
techniques inspired by natural language pro-
cessing. We consider a different type of classi-
cal encoding scheme known as the running key
cipher, and propose a search solution using
Gibbs sampling with a word language model.
We evaluate our method on synthetic cipher-
texts of different lengths, and find that it out-
performs previous work that employs Viterbi
decoding with character-based models.
1 Introduction
The running key cipher is an encoding scheme that
uses a secret keyR that is typically a string of words,
usually taken from a book or other text that is agreed
upon by the sender and receiver. When sending a
plaintext message P , the sender truncates R to the
length of the plaintext. The scheme also relies on
a substitution function f , which is usually publicly
known, that maps a plaintext letter p and key letter
r to a unique ciphertext letter c. The most common
choice for f is the tabula recta, where c = (p + r)
mod 26 for letters in the English alphabet, with A
= 0, B = 1, and so on.
To encode a plaintext with a running key, the
spaces in the plaintext and the key are removed, and
for every 0 ? i < |P |, the ciphertext letter at posi-
tion i is computed to be Ci ? f(Pi, Ri). Figure 1
shows an example encoding using the tabula recta.
For a given ciphertext and known f , the plaintext
uniquely determines the running key and vice versa.
?Research conducted while the author was visiting ISI.
Since we know that the plaintext and running key
are both drawn from natural language, our objective
function for the solution plaintext under some lan-
guage model is:
P? = argmax
P
log Pr(P ) Pr(RP,C) (1)
where the running key RP,C is the key that corre-
sponds to plaintext P and ciphertext C.
Note that if RP,C is a perfectly random sequence
of letters, this scheme is effectively a ?one-time pad?,
which is provably unbreakable (Shannon, 1949).
The knowledge that both the plaintext and the key
are natural language strings is important in breaking
a running key cipher.
The letter-frequency distribution of running key
ciphertexts is notably flatter than than the plaintext
distribution, unlike substitution ciphers where the
frequency profile remains unchanged, modulo letter
substitutions. However, the ciphertext letter distri-
bution is not uniform; there are peaks corresponding
to letters (like I) that are formed by high-frequency
plaintext/key pairs (like E and E).
2 Related Work
2.1 Running Key Ciphers
Bauer and Tate (2002) use letter n-grams (without
smoothing) up to order 6 to find the most probable
plaintext/key character pair at each position in the ci-
phertext. They test their method on 1000-character
ciphertexts produced from plaintexts and keys ex-
tracted from Project Gutenberg. Their accuracies
range from 28.9% to 33.5%, where accuracy is mea-
sured as the percentage of correctly decoded char-
80
Figure 1: Example of a running key cipher. Note that key is truncated to the length of the plaintext.
Plaintext ? linguistics is fun, Running Key ? colorless green ideas, tabula recta substitution where Ci ? (Pi +Ri) mod 26
Plaintext: L I N G U I S T I C S I S F U N
Running Key: C O L O R L E S S G R E E N I D
Ciphertext: N W Y U L T W L A I J M W S C Q
acters. Such figures are too low to produce read-
able plaintexts, especially if the decoded regions are
not contiguous. Griffing (2006) uses Viterbi decod-
ing and letter 6-grams to improve on the above re-
sult, achieving a median 87% accuracy over several
1000-character ciphertexts. A key shortcoming of
this work is that it requires searching through about
265 states at each position in the ciphertext.
2.2 Letter Substitution Ciphers
Previous work in decipherment of classical ciphers
has mainly focused on letter substitution. These ci-
phers use a substitution table as the secret key. The
ciphertext is generated by substituting each letter of
the plaintext according to the substitution table. The
table may be homophonic; that is, a single plaintext
letter could map to more than one possible cipher-
text letter. Just as in running key ciphers, spaces in
the plaintext are usually removed before encoding.
Proposed decipherment solutions for letter substi-
tution ciphers include techniques that use expecta-
tion maximization (Ravi and Knight, 2008), genetic
algorithms (Oranchak, 2008), integer programming
(Ravi and Knight, 2009), A* decoding (Corlett and
Penn, 2010), and Bayesian learning with Dirichlet
processes (Ravi and Knight, 2011).
2.3 Vigene`re Ciphers
A scheme similar to the running key cipher is the Vi-
gene`re cipher, also known as the periodic key cipher.
Instead of a single long string spanning the length of
the plaintext, the key is a short string ? usually but
not always a single word or phrase ? repeated to the
length of the plaintext. Figure 2 shows an example
Vigene`re cipher encoding. This cipher is less secure
than the running key, since the short length of the
key vastly reduces the size of the search space, and
the periodic repetition of the key leaks information.
Recent work on decoding periodic key ciphers
perform Viterbi search on the key using letter n-
gram models (Olsen et al, 2011), with the assump-
tion that the length of the key is known. If unknown,
the key length can be inferred using the Kasiski Test
(Kasiski, 1863) which takes advantage of repeated
plaintext/key character pairs.
3 Solution with Gibbs Sampling
In this paper, we describe a search algorithm that
uses Gibbs Sampling to break a running key cipher.
3.1 Choice of Language Model
The main advantage of a sampling-based approach
over Viterbi decoding is that it allows us to seam-
lessly use word-based language models. Lower or-
der letter n-grams may fail to decipher most cipher-
texts even with perfect search, since an incorrect
plaintext and key could have higher likelihood un-
der a weak language model than the actual message.
3.2 Blocked Sampling
One possible approach is to sample a plaintext letter
at each position in the ciphertext. The limitation of
such a sampler for the running key problem is that
is extremely slow to mix, especially for longer ci-
phertexts: we found that in practice, it does not usu-
ally converge to the optimal solution in a reasonable
number of iterations even with simulated annealing.
We therefore propose a blocked sampling algorithm
that samples words rather than letters in the plain-
text, as follows:
1. Initialize randomly P := p1p2 . . . p|C|, fix R as
the key that corresponds to P,C
2. Repeat for some number of iterations
(a) Sample spaces (word boundaries) in P ac-
cording to Pr(P )
(b) Sample spaces in R according to Pr(R)
(c) Sample each word in P according to
Pr(P ) Pr(R), updating R along with P
(d) Sample each word in R according to
Pr(P ) Pr(R), updating P along with R
81
Figure 2: Example of a Vigene`re cipher cipher, with a 5-letter periodic key, repeated to the length of the plaintext.
Plaintext ? linguistics is fun, Periodic Key ? green, tabula recta substitution.
Plaintext: L I N G U I S T I C S I S F U N
Running Key: G R E E N G R E E N G R E E N G
Ciphertext: R Z R K H O J X M P Y Z W J H T
3. Remove spaces and return P,R
Note that every time a word in P is sampled, it
induces a change in R that may not be a word or a
sequence of words, and vice versa. Sampling word
boundaries will also produce hypotheses contain-
ing non-words. For this reason, we use a word tri-
gram model linearly interpolated with letter trigrams
(including the space character).1 The interpolation
mainly serves to smooth the search space, with the
added benefit of accounting for out-of-vocabulary,
misspelled, or truncated words in the actual plaintext
or key. Table 1 shows an example of one sampling
iteration on the ciphertext shown in Figure 1.
Table 1: First sampling iteration on the ciphertext
NWYULTWLAIJMWSCQ
Generate P,R P : WERGATERYBVIEDOW
with letter trigrams R: RSHOLASUCHOESPOU
Sample spaces in P P : WERGAT ER YB VIEDOW
Sample spaces in R R: RS HOLASUCHOES POU
Sample words in P P : ADJUST AN MY WILLOW
R: NT PATAWYOKNEL HOU
Sample words in R P : NEWNXI ST HE SYLACT
R: AS CHOLESTEROL SAX
4 Experiments
4.1 Data
We randomly select passages from the Project
Gutenberg and Wall Street Journal Corpus extracts
that are included in the NLTK toolkit (Bird et al,
2009). The passages are used as plaintext and key
pairs, and combined to generate synthetic ciphertext
data. Unlike previous works which used constant-
length ciphertexts, we study the effect of message
length on decipherment by varying the ciphertext
length (10, 100, and 1000 characters).
Our language model is an interpolation of word
trigrams and letter trigrams trained on the Brown
1Pr(P ) = ?Pr(P |word LM) + (1 ? ?) Pr(P |letter LM),
and similarly for Pr(R).
Corpus (Nelson and Kucera, 1979), with Kneser-
Ney smoothing. We fixed the word language model
interpolation weight to ? = 0.7.
4.2 Baseline and Evaluation
For comparison with the previous work, we re-
implement Viterbi decoding over letter 6-grams
(Griffing, 2006) trained on the Brown Corpus. In
addition to decipherment accuracy, we compare the
running time in seconds of the two algorithms.
Both decipherment programs were implemented in
Python and run on the same machines. The Gibbs
Sampler was run for 10000 iterations.
As in the Griffing (2006) paper, since the plaintext
and running key are interchangeable, we measure
the accuracy of a hypothesized solution against the
reference as the max of the accuracy between the hy-
pothesized plaintext and the reference plaintext, and
the hypothesized plaintext and the reference key.
4.3 Results
Table 2 shows the average decipherment accuracy of
our algorithm and the baseline on each dataset. Also
shown is the number of times that the Gibbs Sam-
pling search failed ? that is, when the algorithm did
not hypothesize a solution that had a probability at
least as high as the reference within 10000 iterations.
It is clear that the Gibbs Sampler is orders of mag-
nitude faster than Viterbi decoding. Performance
on the short (length 10) ciphertexts is poor under
both algorithms. This is expected, since the degree
of message uncertainty, or message equivocation as
defined by Shannon, is high for short ciphertexts:
there are several possible plaintexts and keys be-
sides the original that are likely under an English
language model. Consider the ciphertext WAEEXF-
PROV which was generated by the plaintext seg-
ment ON A REFEREN and key INENTAL AKI.
The algorithm hypothesizes that the plaintext is THE
STRAND S and key DTAME OPELD, which both
receive high language model probability.
82
Table 2: Decipherment accuracy (proportion of correctly deciphered characters). Plaintext and key sources for the
ciphertext test data were extracted by starting at random points in the corpora, and selecting the following n characters.
Length of Plaintext and key # Cipher- Average Accuracy Avg. running time (sec) # Failed Gibbs
ciphertext source texts Viterbi Gibbs Viterbi Gibbs searches
10
Project Gutenberg 100 14% 17% 1005 47 5
Wall Street Journal 100 10% 26% 986 38 2
100
Project Gutenberg 100 27% 42% 10212 236 19
Wall Street Journal 100 22% 58% 10433 217 12
1000
Project Gutenberg 100 63% 88% 112489 964 32
Wall Street Journal 100 60% 93% 117303 1025 25
Table 3: Substitution function parameterized by the keyword, CIPHER. f(p, r) is the entry in the row corresponding to p and the
column corresponding to r.
A B C D E F G H I J K L M N O P Q R S T U V W X Y Z
A C I P H E R A B D F G J K L M N O Q S T U V W X Y Z
B I P H E R A B D F G J K L M N O Q S T U V W X Y Z C
C P H E R A B D F G J K L M N O Q S T U V W X Y Z C I
. . .
However, on the long ciphertexts, our algorithm
gets close to perfect decipherment, surpassing the
Viterbi algorithm by a large margin.2 Accuracies on
the Wall Street Journal ciphertexts are higher than on
the Gutenberg ciphertexts for our algorithm, which
may be because the latter is more divergent from the
Brown Corpus language model.
5 Future Work
5.1 Unknown substitution functions
Some running key ciphers also use a secret substi-
tution function f rather than the tabula recta or an-
other known function. In typical cases, these func-
tions are not arbitrary, but are parameterized by a se-
cret keyword that mutates the tabula recta table. For
example, the function with the keyword CIPHER
would be the substitution table shown in Table 3.
Decoding a running key ciphertext under a latent
substitution function is an open line of research. One
possibility is to extend our approach by sampling the
keyword or function in addition to the plaintext.
5.2 Exact search
Since some the errors in Gibbs Sampling decipher-
ment are due to search failures, a natural exten-
sion of this work would be to adapt Viterbi search
2The accuracies that we found for Viterbi decoding are
lower than those reported by Griffing (2006), which might be
because they use an in-domain language model.
or other exact decoding algorithms like A* to use
word-level language models. A naive implementa-
tion of Viterbi word-based decoding results in com-
putationally inefficient search spaces for large vo-
cabularies, so more sophisticated methods or heuris-
tics will be required.
5.3 Analysis of Running Key Decipherment
While there has been theoretical and empirical anal-
ysis of the security of letter substitution ciphers
of various lengths under different language models
(Shannon, 1949; Ravi and Knight, 2008), there has
been no similar exposition of running key ciphers,
which we reserve for future work.
6 Conclusion
We propose a decipherment algorithm for running
key ciphers that uses Blocked Gibbs Sampling and
word-based language models, which shows signifi-
cant speed and accuracy improvements over previ-
ous research into this problem.
Acknowledgments
We would like to thank Sujith Ravi for initial exper-
iments using Gibbs sampling, and the anonymous
reviewers. This research was supported in part by
NSF grant 0904684.
83
References
Craig Bauer and Christian Tate. 2002. A statistical attack
on the running key cipher. Cryptologia, 26(4).
Steven Bird, Edward Loper, and Ewan Klein. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia Inc.
Eric Corlett and Gerald Penn. 2010. An exact A* method
of deciphering letter-substitution ciphers. In Proceed-
ings of ACL.
Alexander Griffing. 2006. Solving the running key ci-
pher with the Viterbi algorithm. Cryptologia, 30(4).
Friedrich Kasiski. 1863. Die Geheimschriften und die
Dechiffrir-Kunst. E. S. Mittler und Sohn.
Francis Nelson and Henry Kucera. 1979. The Brown
Corpus: A Standard Corpus of Present-Day Edited
American English. Brown University.
Peder Olsen, John Hershey, Steven Rennie, and Vaib-
hava Goel. 2011. A speech recognition solution to
an ancient cryptography problem. Technical Report
RC25109 (W1102-005), IBM Research.
David Oranchak. 2008. Evolutionary algorithm for de-
cryption of monoalphabetic homophonic substitution
ciphers encoded as constraint satisfaction problems. In
Proceedings of the Conference on Genetic and Evolu-
tionary Computation.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of EMNLP.
Sujith Ravi and Kevin Knight. 2009. Attacking letter
substitution ciphers with integer programming. Cryp-
tologia, 33(4).
Sujith Ravi and Kevin Knight. 2011. Bayesian inference
for Zodiac and other homophonic ciphers. In Proceed-
ings of ACL.
Claude Shannon. 1949. Communication theory of se-
crecy systems. Bell System Technical Journal, 28(4).
84
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924?932,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing Graphs with Hyperedge Replacement Grammars
David Chiang
Information Sciences Institute
University of Southern California
Jacob Andreas
Columbia University
University of Cambridge
Daniel Bauer
Department of Computer Science
Columbia University
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Bevan Jones
University of Edinburgh
Macquarie University
Kevin Knight
Information Sciences Institute
University of Southern California
Abstract
Hyperedge replacement grammar (HRG)
is a formalism for generating and trans-
forming graphs that has potential appli-
cations in natural language understand-
ing and generation. A recognition al-
gorithm due to Lautemann is known to
be polynomial-time for graphs that are
connected and of bounded degree. We
present a more precise characterization of
the algorithm?s complexity, an optimiza-
tion analogous to binarization of context-
free grammars, and some important im-
plementation details, resulting in an algo-
rithm that is practical for natural-language
applications. The algorithm is part of Boli-
nas, a new software toolkit for HRG pro-
cessing.
1 Introduction
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al, 1997), and its synchronous
counterpart can be used for transforming graphs
to/from other graphs or trees. As such, it has great
potential for applications in natural language un-
derstanding and generation, and semantics-based
machine translation (Jones et al, 2012). Fig-
ure 1 shows some examples of graphs for natural-
language semantics.
A polynomial-time recognition algorithm for
HRGs was described by Lautemann (1990), build-
ing on the work of Rozenberg and Welzl (1986)
on boundary node label controlled grammars, and
others have presented polynomial-time algorithms
as well (Mazanek and Minas, 2008; Moot, 2008).
Although Lautemann?s algorithm is correct and
tractable, its presentation is prefaced with the re-
mark: ?As we are only interested in distinguish-
ing polynomial time from non-polynomial time,
the analysis will be rather crude, and implemen-
tation details will be explicated as little as possi-
ble.? Indeed, the key step of the algorithm, which
matches a rule against the input graph, is described
at a very high level, so that it is not obvious (for a
non-expert in graph algorithms) how to implement
it. More importantly, this step as described leads
to a time complexity that is polynomial, but poten-
tially of very high degree.
In this paper, we describe in detail a more effi-
cient version of this algorithm and its implementa-
tion. We give a more precise complexity analysis
in terms of the grammar and the size and maxi-
mum degree of the input graph, and we show how
to optimize it by a process analogous to binariza-
tion of CFGs, following Gildea (2011). The re-
sulting algorithm is practical and is implemented
as part of the open-source Bolinas toolkit for hy-
peredge replacement grammars.
2 Hyperedge replacement grammars
We give a short example of how HRG works, fol-
lowed by formal definitions.
2.1 Example
Consider a weighted graph language involving just
two types of semantic frames (want and believe),
two types of entities (boy and girl), and two roles
(arg0 and arg1). Figure 1 shows a few graphs from
this language.
Figure 2 shows how to derive one of these
graphs using an HRG. The derivation starts with
a single edge labeled with the nonterminal sym-
bol S . The first rewriting step replaces this edge
with a subgraph, which we might read as ?The
924
boy?girl?
want? arg0
arg1
boy?
believe? arg1
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 1: Sample members of a graph language,
representing the meanings of (clockwise from up-
per left): ?The girl wants the boy,? ?The boy is
believed,? and ?The boy wants the girl to believe
that he wants her.?
boy wants something (X) involving himself.? The
second rewriting step replaces the X edge with an-
other subgraph, which we might read as ?The boy
wants the girl to believe something (Y) involving
both of them.? The derivation continues with a
third rewriting step, after which there are no more
nonterminal-labeled edges.
2.2 Definitions
The graphs we use in this paper have edge labels,
but no node labels; while node labels are intu-
itive for many graphs in NLP, using both node and
edge labels complicates the definition of hyper-
edge grammar and algorithms. All of our graphs
are directed (ordered), as the purpose of most
graph structures in NLP is to model dependencies
between entities.
Definition 1. An edge-labeled, ordered hyper-
graph is a tuple H = ?V, E, ??, where
? V is a finite set of nodes
? E ? V+ is a finite set of hyperedges, each of
which connects one or more distinct nodes
? ? : E ? C assigns a label (drawn from the
finite set C) to each edge.
For brevity we use the terms graph and hyper-
graph interchangeably, and similarly for edge and
hyperedge. In the definition of HRGs, we will use
the notion of hypergraph fragments, which are the
elementary structures that the grammar assembles
into hypergraphs.
Definition 2. A hypergraph fragment is a tuple
?V, E, ?, X?, where ?V, E, ?? is a hypergraph and
X ? V+ is a list of distinct nodes called the ex-
ternal nodes.
The function of graph fragments in HRG is
analogous to the right-hand sides of CFG rules
and to elementary trees in tree adjoining gram-
mars (Joshi and Schabes, 1997). The external
nodes indicate how to integrate a graph into an-
other graph during a derivation, and are analogous
to foot nodes. In diagrams, we draw them with a
black circle ( ).
Definition 3. A hyperedge replacement grammar
(HRG) is a tuple G = ?N,T, P, S ? where
? N and T are finite disjoint sets of nonterminal
and terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
A ? R, where A ? N and R is a graph frag-
ment over N ? T .
We now describe the HRG rewriting mecha-
nism.
Definition 4. Given a HRG G, we define the re-
lation H ?G H? (or, H? is derived from H in one
step) as follows. Let e = (v1 ? ? ? vk) be an edge in
H with label A. Let (A? R) be a production ofG,
where R has external nodes XR = (u1 ? ? ? uk). Then
we write H ?G H? if H? is the graph formed by
removing e from H, making an isomorphic copy
of R, and identifying vi with (the copy of) ui for
i = 1, . . . , k.
Let H ??G H? (or, H? is derived from H) be thereflexive, transitive closure of?G. The graph lan-
guage of a grammar G is the (possibly infinite) set
of graphs H that have no edges with nonterminal
labels such that
S ??G H.
When a HRG rule (A ? R) is applied to an
edge e, the mapping of external nodes in R to the
925
1
X ?
believe? arg1
girl?
arg0
1
Y
1 2
Y
?
12
want?
arg0
arg1
S
1
boy?
X
want? arg1
arg0
2 believe? arg1
want? arg1
girl?
arg0
boy?
arg0
Y
3
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of ?The
boy wants the girl to believe that he wants her.?
nodes of e is implied by the ordering of nodes
in e and XR. When writing grammar rules, we
make this ordering explicit by writing the left hand
side of a rule as an edge and indexing the external
nodes of R on both sides, as shown in Figure 2.
HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label of the replaced edge only.
This allows us to represent a derivation as a deriva-
tion tree, and sets of derivations of a graph as a
derivation forest (which can in turn represented as
hypergraphs). Thus we can apply many of the
methods developed for other context free gram-
mars. For example, it is easy to define weighted
and synchronous versions of HRGs.
Definition 5. If K is a semiring, a K-weighted
HRG is a tuple G = ?N,T, P, S , ??, where
?N, T, P, S ? is a HRG and ? : P ? K assigns a
weight in K to each production. The weight of a
derivation ofG is the product of the weights of the
productions used in the derivation.
We defer a definition of synchronous HRGs un-
til Section 4, where they are discussed in detail.
3 Parsing
Lautemann?s recognition algorithm for HRGs is a
generalization of the CKY algorithm for CFGs.
Its key step is the matching of a rule against the
input graph, analogous to the concatenation of
two spans in CKY. The original description leaves
open how this matching is done, and because it
tries to match the whole rule at once, it has asymp-
totic complexity exponential in the number of non-
terminal edges. In this section, we present a re-
finement that makes the rule-matching procedure
explicit, and because it matches rules little by lit-
tle, similarly to binarization of CFG rules, it does
so more efficiently than the original.
Let H be the input graph. Let n be the number of
nodes in H, and d be the maximum degree of any
node. Let G be a HRG. For simplicity, we assume
that the right-hand sides of rules are connected.
This restriction entails that each graph generated
by G is connected; therefore, we assume that H is
connected as well. Finally, let m be an arbitrary
node of H called the marker node, whose usage
will become clear below.1
3.1 Representing subgraphs
Just as CKY deals with substrings (i, j] of the in-
put, the HRG parsing algorithm deals with edge-
induced subgraphs I of the input. An edge-
induced subgraph of H = ?V, E, ?? is, for some
1To handle the more general case where H is not con-
nected, we would need a marker for each component.
926
subset E? ? E, the smallest subgraph containing
all edges in E?. From now on, we will assume that
all subgraphs are edge-induced subgraphs.
In CKY, the two endpoints i and j com-
pletely specify the recognized part of the input,
wi+1 ? ? ?w j. Likewise, we do not need to store all
of I explicitly.
Definition 6. Let I be a subgraph of H. A bound-
ary node of I is a node in I which is either a node
with an edge in H\I or an external node. A bound-
ary edge of I is an edge in I which has a boundary
node as an endpoint. The boundary representation
of I is the tuple ?bn(I), be(I, v),m ? I?, where
? bn(I) is the set of boundary nodes of I
? be(I, v) be the set of boundary edges of v in I
? (m ? I) is a flag indicating whether the
marker node is in I.
The boundary representation of I suffices to
specify I compactly.
Proposition 1. If I and I? are two subgraphs of H
with the same boundary representation, then I =
I?.
Proof. Case 1: bn(I) is empty. If m ? I and m ? I?,
then all edges of H must belong to both I and I?,
that is, I = I? = H. Otherwise, if m < I and m < I?,
then no edges can belong to either I or I?, that is,
I = I? = ?.
Case 2: bn(I) is nonempty. Suppose I , I?;
without loss of generality, suppose that there is an
edge e that is in I \ I?. Let ? be the shortest path
(ignoring edge direction) that begins with e and
ends with a boundary node. All the edges along ?
must be in I \ I?, or else there would be a boundary
node in the middle of ?, and ? would not be the
shortest path from e to a boundary node. Then, in
particular, the last edge of ?must be in I \ I?. Since
it has a boundary node as an endpoint, it must be a
boundary edge of I, but cannot be a boundary edge
of I?, which is a contradiction. 
If two subgraphs are disjoint, we can use their
boundary representations to compute the boundary
representation of their union.
Proposition 2. Let I and J be two subgraphs
whose edges are disjoint. A node v is a boundary
node of I ? J iff one of the following holds:
(i) v is a boundary node of one subgraph but not
the other
(ii) v is a boundary node of both subgraphs, and
has an edge which is not a boundary edge of
either.
An edge is a boundary edge of I ? J iff it has a
boundary node of I ? J as an endpoint and is a
boundary edge of I or J.
Proof. (?) v has an edge in either I or J and an
edge e outside both I and J. Therefore it must be a
boundary node of either I or J. Moreover, e is not
a boundary edge of either, satisfying condition (ii).
(?) Case (i): without loss of generality, assume
v is a boundary node of I. It has an edge e in I, and
therefore in I ? J, and an edge e? outside I, which
must also be outside J. For e < J (because I and
J are disjoint), and if e? ? J, then v would be a
boundary node of J. Therefore, e? < I ? J, so v is
a boundary node of I ? J. Case (ii): v has an edge
in I and therefore I ? J, and an edge not in either
I or J. 
This result leads to Algorithm 1, which runs in
time linear in the number of boundary nodes.
Algorithm 1 Compute the union of two disjoint
subgraphs I and J.
for all v ? bn(I) do
E ? be(I, v) ? be(J, v)
if v < bn(J) or v has an edge not in E then
add v to bn(I ? J)
be(I ? J, v)? E
for all v ? bn(J) do
if v < bn(I) then
add v to bn(I ? J)
be(I ? J, v)? be(I, v) ? be(J, v)
(m ? I ? J)? (m ? I) ? (m ? J)
In practice, for small subgraphs, it may be more
efficient simply to use an explicit set of edges in-
stead of the boundary representation. For the Geo-
Query corpus (Tang and Mooney, 2001), whose
graphs are only 7.4 nodes on average, we gener-
ally find this to be the case.
3.2 Treewidth
Lautemann?s algorithm tries to match a rule
against the input graph all at once. But we can op-
timize the algorithm by matching a rule incremen-
tally. This is analogous to the rank-minimization
problem for linear context-free rewriting systems.
Gildea has shown that this problem is related to
927
the notion of treewidth (Gildea, 2011), which we
review briefly here.
Definition 7. A tree decomposition of a graph
H = ?V, E? is a tree T , each of whose nodes ?
is associated with sets V? ? V and E? ? E, with
the following properties:
1. Vertex cover: For each v ? V , there is a node
? ? T such that v ? V?.
2. Edge cover: For each e = (v1 ? ? ? vk) ? E,
there is exactly one node ? ? T such that e ?
E?. We say that ? introduces e. Moreover,
v1, . . . , vk ? V?.
3. Running intersection: For each v ? V , the set
{? ? T | v ? V?} is connected.
The width of T is max |V?| ? 1. The treewidth of H
is the minimal width of any tree decomposition
of H.
A tree decomposition of a graph fragment
?V, E, X? is a tree decomposition of ?V, E? that has
the additional property that all the external nodes
belong to V? for some ?. (Without loss of general-
ity, we assume that ? is the root.)
For example, Figure 3b shows a graph, and Fig-
ure 3c shows a tree decomposition. This decom-
position has width three, because its largest node
has 4 elements. In general, a tree has width one,
and it can be shown that a graph has treewidth at
most two iff it does not have the following graph
as a minor (Bodlaender, 1997):
K4 =
Finding a tree decomposition with minimal
width is in general NP-hard (Arnborg et al, 1987).
However, we find that for the graphs we are inter-
ested in in NLP applications, even a na??ve algo-
rithm gives tree decompositions of low width in
practice: simply perform a depth-first traversal of
the edges of the graph, forming a tree T . Then,
augment the V? as necessary to satisfy the running
intersection property.
As a test, we extracted rules from the Geo-
Query corpus (Tang and Mooney, 2001) using the
SynSem algorithm (Jones et al, 2012), and com-
puted tree decompositions exactly using a branch-
and-bound method (Gogate and Dechter, 2004)
and this approximate method. Table 1 shows that,
in practice, treewidths are not very high even when
computed only approximately.
method mean max
exact 1.491 2
approximate 1.494 3
Table 1: Mean and maximum treewidths of rules
extracted from the GeoQuery corpus, using exact
and approximate methods.
(a) 0
a
believe? arg1
b
girl?
arg0
1
Y
(b) 0
1
0
b 1
0
a
b 1
arg1
a
b 1
Y
?
0
b
arg0
b
girl?
?
0believe?
?
Figure 3: (a) A rule right-hand side, and (b) a nice
tree decomposition.
Any tree decomposition can be converted into
one which is nice in the following sense (simpli-
fied from Cygan et al (2011)). Each tree node ?
must be one of:
? A leaf node, such that V? = ?.
? A unary node, which introduces exactly one
edge e.
? A binary node, which introduces no edges.
The example decomposition in Figure 3c is nice.
This canonical form simplifies the operation of the
parser described in the following section.
Let G be a HRG. For each production (A ?
R) ? G, find a nice tree decomposition of R and
call it TR. The treewidth of G is the maximum
928
treewidth of any right-hand side in G.
The basic idea of the recognition algorithm is
to recognize the right-hand side of each rule incre-
mentally by working bottom-up on its tree decom-
position. The properties of tree decomposition al-
low us to limit the number of boundary nodes of
the partially-recognized rule.
More formally, let RD? be the subgraph of R in-
duced by the union of E?? for all ?? equal to or
dominated by ?. Then we can show the following.
Proposition 3. Let R be a graph fragment, and as-
sume a tree decomposition of R. All the boundary
nodes of RD? belong to V? ? Vparent(?).
Proof. Let v be a boundary node of RD?. Node v
must have an edge in RD? and therefore in R?? for
some ?? dominated by or equal to ?.
Case 1: v is an external node. Since the root
node contains all the external nodes, by the run-
ning intersection property, both V? and Vparent(?)
must contain v as well.
Case 2: v has an edge not in RD?. Therefore
there must be a tree node not dominated by or
equal to ? that contains this edge, and therefore
v. So by the running intersection property, ? and
its parent must contain v as well. 
This result, in turn, will allow us to bound the
complexity of the parsing algorithm in terms of the
treewidth of G.
3.3 Inference rules
We present the parsing algorithm as a deductive
system (Shieber et al, 1995). The items have
one of two forms. A passive item has the form
[A, I, X], where X ? V+ is an explicit ordering
of the boundary nodes of I. This means that we
have recognized that A ??G I. Thus, the goalitem is [S ,H, ?]. An active item has the form
[A? R, ?, I, ?], where
? (A? R) is a production of G
? ? is a node of TR
? I is a subgraph of H
? ? is a bijection between the boundary nodes
of RD? and those of I.
The parser must ensure that ? is a bijection when
it creates a new item. Below, we use the notation
{e 7? e?} or {e 7? X} for the mapping that sends
each node of e to the corresponding node of e?
or X.
Passive items are generated by the following
rule:
? Root [B? Q, ?, J, ?]
[B, J, X]
where ? is the root of TQ, and X j = ?(XQ, j).
If we assume that the TR are nice, then the in-
ference rules that generate active items follow the
different types of nodes in a nice tree decomposi-
tion:
? Leaf
[A? R, ?, ?, ?]
where ? is a leaf node of TR.
? (Unary) Nonterminal
[A? R, ?1, I, ?] [B, J, X]
[A? R, ?, I ? J, ? ? {e 7? X}]
where ?1 is the only child of ?, and e is intro-
duced by ? and is labeled with nonterminal B.
? (Unary) Terminal
[A? R, ?1, I, ?]
[A? R, ?, I ? {e?}, ? ? {e 7? e?}]
where ?1 is the only child of ?, e is introduced
by ?, and e and e? are both labeled with ter-
minal a.
? Binary
[A? R, ?1, I, ?1] [A? R, ?2, J, ?2]
[A? R, ?, I ? J, ?1 ? ?2]
where ?1 and ?2 are the two children of ?.
In the Nonterminal, Terminal, and Binary rules,
we form unions of subgraphs and unions of map-
pings. When forming the union of two subgraphs,
we require that the subgraphs be disjoint (however,
see Section 3.4 below for a relaxation of this con-
dition). When forming the union of two mappings,
we require that the result be a bijection. If either
of these conditions is not met, the inference rule
cannot apply.
For efficiency, it is important to index the items
for fast access. For the Nonterminal inference
rule, passive items [B, J, X] should be indexed by
key ?B, |bn(J)|?, so that when the next item on the
agenda is an active item [A ? R, ?1, I, ?], we
know that all possible matching passive items are
929
S ?
X
X
X
X ?
a
a a
a
a
(a) (b)
a
a a
a aa
(c)
Figure 4: Illustration of unsoundness in the recog-
nition algorithm without the disjointness check.
Using grammar (a), the recognition algorithm
would incorrectly accept the graph (b) by assem-
bling together the three overlapping fragments (c).
under key ??(e), |e|?. Similarly, active items should
be indexed by key ??(e), |e|? so that they can be
found when the next item on the agenda is a pas-
sive item. For the Binary inference rule, active
items should be indexed by their tree node (?1
or ?2).
This procedure can easily be extended to pro-
duce a packed forest of all possible derivations
of the input graph, representable as a hypergraph
just as for other context-free rewriting formalisms.
The Viterbi algorithm can then be applied to
this representation to find the highest-probability
derivation, or the Inside/Outside algorithm to set
weights by Expectation-Maximization.
3.4 The disjointness check
A successful proof using the inference rules above
builds an HRG derivation (comprising all the
rewrites used by the Nonterminal rule) which de-
rives a graph H?, as well as a graph isomorphism
? : H? ? H (the union of the mappings from all
the items).
During inference, whenever we form the union
of two subgraphs, we require that the subgraphs
be disjoint. This is a rather expensive operation:
it can be done using only their boundary represen-
tations, but the best algorithm we are aware of is
still quadratic in the number of boundary nodes.
Is it possible to drop the disjointness check? If
we did so, it would become possible for the algo-
rithm to recognize the same part of H twice. For
example, Figure 4 shows an example of a grammar
and an input that would be incorrectly recognized.
However, we can replace the disjointness check
with a weaker and faster check such that any
derivation that merges two non-disjoint subgraphs
will ultimately fail, and therefore the derived
graph H? is isomorphic to the input graph H? as
desired. This weaker check is to require, when
merging two subgraphs I and J, that:
1. I and J have no boundary edges in common,
and
2. If m belongs to both I and J, it must be a
boundary node of both.
Condition (1) is enough to guarantee that ? is lo-
cally one-to-one in the sense that for all v ? H?, ?
restricted to v and its neighbors is one-to-one. This
is easy to show by induction: if ?I : I? ? H and
?J : J? ? H are locally one-to-one, then ?I ? ?J
must also be, provided condition (1) is met. Intu-
itively, the consequence of this is that we can de-
tect any place where ? changes (say) from being
one-to-one to two-to-one. So if ? is two-to-one,
then it must be two-to-one everywhere (as in the
example of Figure 4).
But condition (2) guarantees that ? maps only
one node to the marker m. We can show this again
by induction: if ?I and ?J each map only one node
to m, then ?I??J must map only one node to m, by
a combination of condition (2) and the fact that the
inference rules guarantee that ?I , ?J , and ?I ? ?J
are one-to-one on boundary nodes.
Then we can show that, since m is recognized
exactly once, the whole graph is also recognized
exactly once.
Proposition 4. If H and H? are connected graphs,
? : H? ? H is locally one-to-one, and ??1 is de-
fined for some node of H, then ? is a bijection.
Proof. Suppose that ? is not a bijection. Then
there must be two nodes v?1, v?2 ? H? such that
?(v?1) = ?(v?2) = v ? H. We also know that thereis a node, namely, m, such that m? = ??1(m) is de-
fined.2 Choose a path ? (ignoring edge direction)
from v to m. Because ? is a local isomorphism,
we can construct a path from v?1 to m? that mapsto ?. Similarly, we can construct a path from v?2to m? that maps to ?. Let u? be the first node that
these two paths have in common. But u? must have
two edges that map to the same edge, which is a
contradiction. 
2If H were not connected, we would choose the marker in
the same connected component as v.
930
3.5 Complexity
The key to the efficiency of the algorithm is that
the treewidth of G leads to a bound on the number
of boundary nodes we must keep track of at any
time.
Let k be the treewidth of G. The time complex-
ity of the algorithm is the number of ways of in-
stantiating the inference rules. Each inference rule
mentions only boundary nodes of RD? or RD?i , all
of which belong to V? (by Proposition 3), so there
are at most |V?| ? k + 1 of them. In the Nonter-
minal and Binary inference rules, each boundary
edge could belong to I or J or neither. Therefore,
the number of possible instantiations of any infer-
ence rule is in O((3dn)k+1).
The space complexity of the algorithm is the
number of possible items. For each active item
[A? R, ?, I, ?], every boundary node of RD? must
belong to V??Vparent(?) (by Proposition 3). There-
fore the number of boundary nodes is at most k+1
(but typically less), and the number of possible
items is in O((2dn)k+1).
4 Synchronous Parsing
As mentioned in Section 2.2, because HRGs have
context-free derivation trees, it is easy to define
synchronous HRGs, which define mappings be-
tween languages of graphs.
Definition 8. A synchronous hyperedge re-
placement grammar (SHRG) is a tuple G =
?N, T, T ?, P, S ?, where
? N is a finite set of nonterminal symbols
? T and T ? are finite sets of terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
(A? ?R,R?,??), where R is a graph fragment
over N ? T and R? is a graph fragment over
N ? T ?. The relation ? is a bijection linking
nonterminal mentions in R and R?, such that
if e ? e?, then they have the same label. We
call R the source side and R? the target side.
Some NLP applications (for example, word
alignment) require synchronous parsing: given a
pair of graphs, finding the derivation or forest of
derivations that simultaneously generate both the
source and target. The algorithm to do this is a
straightforward generalization of the HRG parsing
algorithm. For each rule (A? ?R,R?,??), we con-
struct a nice tree decomposition of R?R? such that:
? All the external nodes of both R and R? be-
long to V? for some ?. (Without loss of gen-
erality, assume that ? is the root.)
? If e ? e?, then e and e? are introduced by the
same tree node.
In the synchronous parsing algorithm, passive
items have the form [A, I, X, I?, X?] and active
items have the form [A? R : R?, ?, I, ?, I?, ??].
For brevity we omit a re-presentation of all the in-
ference rules, as they are very similar to their non-
synchronous counterparts. The main difference is
that in the Nonterminal rule, two linked edges are
rewritten simultaneously:
[A? R : R?, ?1, I, ?, I?, ??] [B, J, X, J?, X?]
[A? R : R?, ?, I ? J, ? ? {e j 7? X j},
I? ? J?, ?? ? {e?j 7? X?j}]
where ?1 is the only child of ?, e and e? are both
introduced by ? and e ? e?, and both are labeled
with nonterminal B.
The complexity of the parsing algorithm is
again in O((3dn)k+1), where k is now the max-
imum treewidth of the dependency graph as de-
fined in this section. In general, this treewidth will
be greater than the treewidth of either the source or
target side on its own, so that synchronous parsing
is generally slower than standard parsing.
5 Conclusion
Although Lautemann?s polynomial-time extension
of CKY to HRGs has been known for some time,
the desire to use graph grammars for large-scale
NLP applications introduces some practical con-
siderations not accounted for in Lautemann?s orig-
inal presentation. We have provided a detailed de-
scription of our refinement of his algorithm and its
implementation. It runs in O((3dn)k+1) time and
requires O((2dn)k+1) space, where n is the num-
ber of nodes in the input graph, d is its maximum
degree, and k is the maximum treewidth of the
rule right-hand sides in the grammar. We have
also described how to extend this algorithm to
synchronous parsing. The parsing algorithms de-
scribed in this paper are implemented in the Boli-
nas toolkit.3
3The Bolinas toolkit can be downloaded from
?http://www.isi.edu/licensed-sw/bolinas/?.
931
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful comments. This research was sup-
ported in part by ARO grant W911NF-10-1-0533.
References
Stefan Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal on Algebraic and
Discrete Methods, 8(2).
Hans L. Bodlaender. 1997. Treewidth: Algorithmic
techniques and results. In Proc. 22nd International
Symposium on Mathematical Foundations of Com-
puter Science (MFCS ?97), pages 29?36, Berlin.
Springer-Verlag.
Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,
Micha? Pilipczuk, Johan M. M. van Rooij, and
Jakub Onufry Wojtaszczyk. 2011. Solving connec-
tivity problems parameterized by treewidth in single
exponential time. Computing Research Repository,
abs/1103.0534.
Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95?162. World Scientific.
Daniel Gildea. 2011. Grammar factorization by
tree decomposition. Computational Linguistics,
37(1):231?248.
Vibhav Gogate and Rina Dechter. 2004. A complete
anytime algorithm for treewidth. In Proceedings of
the Conference on Uncertainty in Artificial Intelli-
gence.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. COLING.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69?124.
Springer.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399?421.
Steffen Mazanek and Mark Minas. 2008. Parsing of
hyperedge replacement grammars with graph parser
combinators. In Proc. 7th International Work-
shop on Graph Transformation and Visual Modeling
Techniques.
Richard Moot. 2008. Lambek grammars, tree ad-
joining grammars and hyperedge replacement gram-
mars. In Proc. TAG+9, pages 65?72.
Grzegorz Rozenberg and Emo Welzl. 1986. Bound-
ary NLC graph grammars?basic definitions, nor-
mal forms, and complexity. Information and Con-
trol, 69:136?167.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Proc. European
Conference on Machine Learning.
932
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748?752,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Smatch: an Evaluation Metric for Semantic Feature Structures
Shu Cai
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
shucai@isi.edu
Kevin Knight
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
knight@isi.edu
Abstract
The evaluation of whole-sentence seman-
tic structures plays an important role in
semantic parsing and large-scale seman-
tic structure annotation. However, there is
no widely-used metric to evaluate whole-
sentence semantic structures. In this pa-
per, we present smatch, a metric that cal-
culates the degree of overlap between two
semantic feature structures. We give an
efficient algorithm to compute the metric
and show the results of an inter-annotator
agreement study.
1 Introduction
The goal of semantic parsing is to generate all se-
mantic relationships in a text. Its output is of-
ten represented by whole-sentence semantic struc-
tures. Evaluating such structures is necessary for
semantic parsing tasks, as well as semantic anno-
tation tasks which create linguistic resources for
semantic parsing.
However, there is no widely-used evalua-
tion method for whole-sentence semantic struc-
tures. Current whole-sentence semantic parsing
is mainly evaluated in two ways: 1. task cor-
rectness (Tang and Mooney, 2001), which eval-
uates on an NLP task that uses the parsing re-
sults; 2. whole-sentence accuracy (Zettlemoyer
and Collins, 2005), which counts the number of
sentences parsed completely correctly.
Nevertheless, it is worthwhile to explore evalua-
tion methods that use scores which range from 0 to
1 (?partial credit?) to measure whole-sentence se-
mantic structures. By using such methods, we are
able to differentiate between two similar whole-
sentence semantic structures regardless of specific
tasks or domains. In this work, we provide an eval-
uation metric that uses the degree of overlap be-
tween two whole-sentence semantic structures as
the partial credit.
In this paper, we observe that the difficulty
of computing the degree of overlap between two
whole-sentence semantic feature structures comes
from determining an optimal variable alignment
between them, and further prove that finding such
alignment is NP-complete. We investigate how to
compute this metric and provide several practical
and replicable computing methods by using Inte-
ger Linear Programming (ILP) and hill-climbing
method. We show that our metric can be used
for measuring the annotator agreement in large-
scale linguistic annotation, and evaluating seman-
tic parsing.
2 Semantic Overlap
We work on a semantic feature structure represen-
tation in a standard neo-Davidsonian (Davidson,
1969; Parsons, 1990) framework. For example,
semantics of the sentence ?the boy wants to go? is
represented by the following directed graph:
In this graph, there are three concepts: want-
01, boy, and go-01. Both want-01 and go-01 are
frames from PropBank framesets (Kingsbury and
Palmer, 2002). The frame want-01 has two argu-
ments connected with ARG0 and ARG1, and go-
01 has an argument (which is also the same boy
instance) connected with ARG0.
748
Following (Langkilde and Knight, 1998) and
(Langkilde-Geary, 2002), we refer to this semantic
representation as AMR (Abstract Meaning Repre-
sentation).
Semantic relationships encoded in the AMR
graph can also be viewed as a conjunction of logi-
cal propositions, or triples:
instance(a, want-01) ?
instance(b, boy) ?
instance(c, go-01) ?
ARG0(a, b) ?
ARG1(a, c) ?
ARG0(c, b)
Each AMR triple takes one of these forms:
relation(variable, concept) (the first three triples
above), or relation(variable1, variable2) (the last
three triples above).
Suppose we take a second AMR (for ?the boy
wants the football?) and its associated proposi-
tional triples:
instance(x, want-01) ?
instance(y, boy) ?
instance(z, football) ?
ARG0(x, y) ?
ARG1(x, z)
Our evaluation metric measures precision, re-
call, and f-score of the triples in the second AMR
against the triples in the first AMR, i.e., the
amount of propositional overlap.
The difficulty is that variable names are not
shared between the two AMRs, so there are mul-
tiple ways to compute the propositional overlap
based on different variable mappings. We there-
fore define the smatch score (for semantic match)
as the maximum f-score obtainable via a one-to-
one matching of variables between the two AMRs.
In the example above, there are six ways to
match up variables between the two AMRs:
M P R F
x=a, y=b, z=c: 4 4/5 4/6 0.73
x=a, y=c, z=b: 1 1/5 1/6 0.18
x=b, y=a, z=c: 0 0/5 0/6 0.00
x=b, y=c, z=a: 0 0/5 0/6 0.00
x=c, y=a, z=b: 0 0/5 0/6 0.00
x=c, y=b, z=a: 2 2/5 2/6 0.36
----------------------------------
smatch score: 0.73
Here, M is the number of propositional triples that
agree given a variable mapping, P is the precision
of the second AMR against the first, R is its re-
call, and F is its f-score. The smatch score is the
maximum of the f-scores.
However, for AMRs that contain large number
of variables, it is not efficient to get the f-score by
simply using the method above. Exhaustively enu-
merating all variable mappings requires comput-
ing the f-score for n!/(n?m)! variable mappings
(assuming one AMR has n variables and the other
has m variables, and m ? n). This algorithm is
too slow for all but the shortest AMR pairs.
3 Computing the Metric
This section describes how to compute the smatch
score. As input, we are given AMR1 (with m vari-
ables) and AMR2 (with n variables). Without loss
of generality, m ? n.
Baseline. Our baseline first matches variables
that share concepts. For example, it would match
a in the first AMR example with x in the second
AMR example of Section 2, because both are in-
stances of want-01. If there are two or more vari-
ables to choose from, we pick the first available
one. The rest of the variables are mapped ran-
domly.
ILP method. We can get an optimal solution
using integer linear programming (ILP). We create
two types of variables:
? (Variable mapping) vij = 1 iff the ith vari-
able in AMR1 is mapped to the jth variable
in AMR2 (otherwise vij = 0)
? (Triple match) tkl = 1 iff AMR1 triple
k matches AMR2 triple l, otherwise tkl
= 0. A triple relation1(xy) matches
relation2(wz) iff relation1 = relation2, vxw
= 1, and vyz = 1 or y and z are the same con-
cept.
Our constraints ensure a one-to-one mapping of
variables, and they ensure that the chosen t values
are consistent with the chosen v values:
For all i,
?
j
vij ? 1
For all j,
?
i
vij ? 1
For all triple pairs r(xy)r(wz) (r for relation),
tr(xy)r(wz) ? vxw
749
tr(xy)r(wz) ? vyz
when y and z are variables.
Finally, we ask the ILP solver to maximize:
?
kl
tkl
which denotes the maximum number of matching
triples which lead to the smatch score.
Hill-climbing method. Finally, we develop a
portable heuristic algorithm that does not require
an ILP solver1. This method works in a greedy
style. We begin with m random one-to-one map-
pings between the m variables of AMR1 and the
n variables of AMR2. Each variable mapping is
a pair (i,map(i)) with 1 ? i ? m and 1 ?
map(i) ? n. We refer to the m mappings as a
variable mapping state.
We first generate a random initial variable map-
ping state, compute its triple match number, then
hill-climb via two types of small changes:
1. Move one of the m mappings to a currently-
unmapped variable from the n.
2. Swap two of the m mappings.
Any variable mapping state has m(n ? m) +
m(m ? 1) = m(n ? 1) neighbors during the
hill-climbing search. We greedily choose the best
neighbor, repeating until no neighbor improves the
number of triple matches.
We experiment with two modifications to the
greedy search: (1) executing multiple random
restarts to avoid local optima, and (2) using our
Baseline concept matching (?smart initialization?)
instead of random initialization.
NP-completeness. There is unlikely to be
an exact polynomial-time algorithm for comput-
ing smatch. We can reduce the 0-1 Maximum
Quadratic Assignment Problem (0-1-Max-QAP)
(Nagarajan and Sviridenko, 2009) and the sub-
graph isomorphism problem directly to the full
smatch problem on graphs.2
We note that other widely-used metrics, such as
TER (Snover et al, 2006), are also NP-complete.
Fortunately, the next section shows that the smatch
methods above are efficient and effective.
1The tool can be downloaded at
http://amr.isi.edu/evaluation.html.
2Thanks to David Chiang for observing the subgraph iso-
morphism reduction.
4 Using Smatch
We report an AMR inter-annotator agreement
study using smatch.
1. Our study has 4 annotators (A, B, C, D), who
then converge on a consensus annotation E.
We thus have 10 pairs of annotations: A-B,
A-C, . . . , D-E.
2. The study is carried out 5 times. Each
time annotators build AMRs for 4 sentences
from the Wall Street Journal corpus. Sen-
tence lengths range from 12 to 54 words, and
AMRs range from 6 to 29 variables.
3. We use 7 smatch calculation methods in our
experiments:
? Base: Baseline matching method
? ILP: Integer Linear Programming
? R: Hill-climbing with random initializa-
tion
? 10R: Hill-climbing with random initial-
ization plus 9 random restarts
? S: Hill-climbing with smart initializa-
tion
? S+4R: Hill-climbing with smart initial-
ization plus 4 random restarts
? S+9R: Hill-climbing with smart initial-
ization plus 9 random restarts
Table 1 shows smatch scores provided by the
methods. Columns labeled 1-5 indicate sen-
tence groups. Each individual smatch score is
a document-level score of 4 AMR pairs.3 ILP
scores are optimal, so lower scores (in bold) in-
dicate search errors.
Table 2 summarizes search accuracy as a per-
centage of smatch scores that equal that of ILP.
Results show that the restarts are essential for hill-
climbing, and that 9 restarts are sufficient to obtain
good quality. The table also shows total runtimes
over 200 AMR pairs (10 annotator pairs, 5 sen-
tence groups, 4 AMR pairs per group). Heuris-
tic search with smart initialization and 4 restarts
(S+4R) gives the best trade-off between accuracy
and speed, so this is the setting we use in practice.
Figure 1 shows smatch scores of each annotator
(A-D) against the consensus annotation (E). The
3For documents containing multiple AMRs, we use the
sum of matched triples over all AMR pairs to compute pre-
cision, recall, and f-score, much like corpus-level Bleu (Pap-
ineni et al, 2002).
750
B C D E1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5Base 0.68 0.74 0.84 0.71 0.83 0.69 0.70 0.80 0.69 0.78 0.77 0.72 0.75 0.68 0.63 0.79 0.86 0.92 0.85 0.89ILP 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92R 0.74 0.79 0.84 0.75 0.86 0.74 0.75 0.80 0.77 0.88 0.83 0.76 0.75 0.72 0.75 0.85 0.92 0.92 0.89 0.89A 10R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S 0.74 0.80 0.84 0.75 0.88 0.75 0.78 0.80 0.76 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S+4R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S+9R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92
Base - - - - - 0.72 0.68 0.74 0.69 0.79 0.71 0.72 0.76 0.65 0.57 0.68 0.71 0.83 0.79 0.86ILP - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89R - - - - - 0.74 0.83 0.72 0.72 0.83 0.78 0.83 0.76 0.68 0.68 0.74 0.81 0.83 0.83 0.89B 10R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S - - - - - 0.73 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S+4R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S+9R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89
Base - - - - - - - - - - 0.68 0.68 0.74 0.69 0.65 0.64 0.64 0.87 0.79 0.83ILP - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89R - - - - - - - - - - 0.74 0.79 0.74 0.75 0.78 0.71 0.76 0.87 0.85 0.89C 10R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89S - - - - - - - - - - 0.74 0.79 0.74 0.77 0.81 0.74 0.76 0.87 0.85 0.89S+4R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89S+9R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89
Base - - - - - - - - - - - - - - - 0.68 0.69 0.81 0.74 0.64ILP - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79R - - - - - - - - - - - - - - - 0.77 0.73 0.81 0.78 0.79D 10R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S+4R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S+9R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79
Table 1: Inter-annotator smatch agreement for 5 groups of sentences, as computed with seven different
methods (Base, ILP, R, 10R, S, S+4R, S+9R). The number 1-5 indicate the sentence group number. Bold
scores are search errors.
Base ILP R 10R S S+4R S+9RAccuracy 20% 100% 66.5% 100% 92% 100% 100%Time (sec) 0.86 49.67 5.85 64.78 2.31 28.36 59.69
Table 2: Accuracy and running time (seconds) of
various computing methods of smatch over 200
AMR pairs.
plot demonstrates that, as time goes by, annotators
reach better agreement with the consensus.
We also note that smatch is used to measure
the accuracy of machine-generated AMRs. (Jones
et al, 2012) use it to evaluate automatic seman-
tic parsing in a narrow domain, while Ulf Her-
mjakob4 has developed a heuristic algorithm that
exploits and supplements Ontonotes annotations
(Pradhan et al, 2007) in order to automatically
create AMRs for Ontonotes sentences, with a
smatch score of 0.74 against human consensus
AMRs.
5 Related Work
Related work on directly measuring the seman-
tic representation includes the method in (Dri-
dan and Oepen, 2011), which evaluates semantic
parser output directly by comparing semantic sub-
structures, though they require an alignment be-
tween sentence spans and semantic sub-structures.
In contrast, our metric does not require the align-
4personal communication
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1  2  3  4  5Sm
atc
h s
co
re
s o
f e
ac
h a
nn
ota
tor
 ag
ain
st 
co
ns
en
su
s
Time
Annotator A
Annotator B
Annotator C
Annotator D
Figure 1: Smatch scores of annotators (A-D)
against the consensus annotation (E) over time.
ment between an input sentence and its semantic
analysis. (Allen et al, 2008) propose a metric
which computes the maximum score by any align-
ment between LF graphs, but they do not address
how to determine the alignments.
6 Conclusion and Future Work
We present an evaluation metric for whole-
sentence semantic analysis, and show that it can
be computed efficiently. We use the metric to
measure semantic annotation agreement rates and
parsing accuracy. In the future, we plan to investi-
gate how to adapt smatch to other semantic repre-
sentations.
751
7 Acknowledgements
We would like to thank David Chiang, Hui Zhang,
other ISI colleagues and our anonymous review-
ers for their thoughtful comments. This work was
supported by NSF grant IIS-0908532.
References
J.F. Allen, M. Swift, and W. Beaumont. 2008. Deep
Semantic Analysis of Text. In Proceedings of the
2008 Conference on Semantics in Text Processing.
D. Davidson. 1969. The Individuation of Events. In
Nicholas Rescher (ed.) Essays in Honor of Carl G.
HempeL Dordrecht: D. Reidel.
R. Dridan and S. Oepen. 2011. Parser Evaluation us-
ing Elementary Dependency Matching. In Proceed-
ings of the 12th International Conference on Parsing
Technologies.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-Based Machine Trans-
lation with Hyperedge Replacement Grammars. In
Proceedings of COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank to
Propbank. In Proceedings of LREC.
I. Langkilde and K. Knight. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of COLING-ACL.
I. Langkilde-Geary. 2002. An Empirical Verifica-
tion of Coverage and Correctness for a General-
Purpose Sentence Generator. In Proceedings of In-
ternational Natural Language Generation Confer-
ence (INLG?02).
V. Nagarajan and M. Sviridenko. 2009. On the Maxi-
mum Quadratic Assignment Problem. Mathematics
of Operations Research, 34.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics.
T. Parsons. 1990. Events in the Semantics of English.
The MIT Press.
S. S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A Unified Relational Semantic Representation. In
Proceedings of the International Conference on Se-
mantic Computing (ICSC ?07).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of the 7th Conference of the Association for Machine
Translation in the Americas (AMTA-2006).
L. R. Tang and R. J. Mooney. 2001. Using Multiple
Clause Constructors in Inductive Logic Program-
ming for Semantic Parsing. In Proceedings of the
12th European Conference on Machine Learning.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
Map Sentences to Logical Form: Structured Classi-
fication with Probabilistic Categorial Grammars. In
Proceedings of the 21st Conference in Uncertainty
in Artificial Intelligence.
752
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 3?4,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Decipherment
Kevin Knight
USC/ISI
4676 Admiralty Way
Marina del Rey CA 90292
knight@isi.edu
Abstract
The first natural language processing sys-
tems had a straightforward goal: deci-
pher coded messages sent by the en-
emy. This tutorial explores connections
between early decipherment research and
today?s NLP work. We cover classic mili-
tary and diplomatic ciphers, automatic de-
cipherment algorithms, unsolved ciphers,
language translation as decipherment, and
analyzing ancient writing as decipher-
ment.
1 Tutorial Overview
The first natural language processing systems had
a straightforward goal: decipher coded messages
sent by the enemy. Sixty years later, we have many
more applications, including web search, ques-
tion answering, summarization, speech recogni-
tion, and language translation. This tutorial ex-
plores connections between early decipherment
research and today?s NLP work. We find that
many ideas from the earlier era have become core
to the field, while others still remain to be picked
up and developed.
We first cover classic military and diplomatic
cipher types, including complex substitution ci-
phers implemented in the first electro-mechanical
encryption machines. We look at mathematical
tools (language recognition, frequency counting,
smoothing) developed to decrypt such ciphers on
proto-computers. We show algorithms and exten-
sive empirical results for solving different types of
ciphers, and we show the role of algorithms in re-
cent decipherments of historical documents.
We then look at how foreign language can be
viewed as a code for English, a concept devel-
oped by Alan Turing and Warren Weaver. We de-
scribe recently published work on building auto-
matic translation systems from non-parallel data.
We also demonstrate how some of the same algo-
rithmic tools can be applied to natural language
tasks like part-of-speech tagging and word align-
ment.
Turning back to historical ciphers, we explore a
number of unsolved ciphers, giving results of ini-
tial computer experiments on several of them. Fi-
nally, we look briefly at writing as a way to enci-
pher phoneme sequences, covering ancient scripts
and modern applications.
2 Outline
1. Classical military/diplomatic ciphers (15
minutes)
? 60 cipher types (ACA)
? Ciphers vs. codes
? Enigma cipher: the mother of natural
language processing
? computer analysis of text
? language recognition
? Good-Turing smoothing
2. Foreign language as a code (10 minutes)
? Alan Turing?s ?Thinking Machines?
? Warren Weaver?s Memorandum
3. Automatic decipherment (55 minutes)
? Cipher type detection
? Substitution ciphers (simple, homo-
phonic, polyalphabetic, etc)
? plaintext language recognition
? how much plaintext knowledge is
needed3
? index of coincidence, unicity dis-
tance, and other measures
? navigating a difficult search space
? frequencies of letters and words
? pattern words and cribs
? EM, ILP, Bayesian models, sam-
pling
? recent decipherments
? Jefferson cipher, Copiale cipher,
civil war ciphers, naval Enigma
? Application to part-of-speech tagging,
word alignment
? Application to machine translation with-
out parallel text
? Parallel development of cryptography
and translation
? Recently released NSA internal
newsletter (1974-1997)
4. *** Break *** (30 minutes)
5. Unsolved ciphers (40 minutes)
? Zodiac 340 (1969), including computa-
tional work
? Voynich Manuscript (early 1400s), in-
cluding computational work
? Beale (1885)
? Dorabella (1897)
? Taman Shud (1948)
? Kryptos (1990), including computa-
tional work
? McCormick (1999)
? Shoeboxes in attics: DuPonceau jour-
nal, Finnerana, SYP, Mopse, diptych
6. Writing as a code (20 minutes)
? Does writing encode ideas, or does it en-
code phonemes?
? Ancient script decipherment
? Egyptian hieroglyphs
? Linear B
? Mayan glyphs
? Ugaritic, including computational
work
? Chinese Nu?shu, including computa-
tional work
? Automatic phonetic decipherment
? Application to transliteration
7. Undeciphered writing systems (15 minutes)
? Indus Valley Script (3300BC)
? Linear A (1900BC)
? Phaistos disc (1700BC?)
? Rongorongo (1800s?)
8. Conclusion and further questions (15 min-
utes)
3 About the Presenter
Kevin Knight is a Senior Research Scientist and
Fellow at the Information Sciences Institute of the
University of Southern California (USC), and a
Research Professor in USC?s Computer Science
Department. He received a PhD in computer sci-
ence from Carnegie Mellon University and a bach-
elor?s degree from Harvard University. Profes-
sor Knight?s research interests include natural lan-
guage processing, machine translation, automata
theory, and decipherment. In 2001, he co-founded
Language Weaver, Inc., and in 2011, he served
as President of the Association for Computational
Linguistics. Dr. Knight has taught computer sci-
ence courses at USC for more than fifteen years
and co-authored the widely adopted textbook Ar-
tificial Intelligence.
4
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 278?282,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
How to Speak a Language without Knowing It
Xing Shi and Kevin Knight
Information Sciences Institute
Computer Science Department
University of Southern California
{xingshi, knight}@isi.edu
Heng Ji
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180, USA
jih@rpi.edu
Abstract
We develop a system that lets people over-
come language barriers by letting them
speak a language they do not know. Our
system accepts text entered by a user,
translates the text, then converts the trans-
lation into a phonetic spelling in the user?s
own orthography. We trained the sys-
tem on phonetic spellings in travel phrase-
books.
1 Introduction
Can people speak a language they don?t know?
Actually, it happens frequently. Travel phrase-
books contain phrases in the speaker?s language
(e.g., ?thank you?) paired with foreign-language
translations (e.g., ? ????????). Since the speaker
may not be able to pronounce the foreign-language
orthography, phrasebooks additionally provide
phonetic spellings that approximate the sounds of
the foreign phrase. These spellings employ the fa-
miliar writing system and sounds of the speaker?s
language. Here is a sample entry from a French
phrasebook for English speakers:
English: Leave me alone.
French: Laissez-moi tranquille.
Franglish: Less-ay mwah trahn-KEEL.
The user ignores the French and goes straight
to the Franglish. If the Franglish is well designed,
an English speaker can pronounce it and be under-
stood by a French listener.
Figure 1 shows a sample entry from another
book?an English phrasebook for Chinese speak-
ers. If a Chinese speaker wants to say ???
????????, she need only read off the
Chinglish ?????????????, which
approximates the sounds of ?Thank you for this
wonderful meal? using Chinese characters.
Phrasebooks permit a form of accurate, per-
sonal, oral communication that speech-to-speech
Figure 1: Snippet from phrasebook
translation devices lack. However, the user is lim-
ited to a small set of fixed phrases. In this paper,
we lift this restriction by designing and evaluating
a software program with the following:
? Input: Text entered by the speaker, in her own
language.
? Output: Phonetic rendering of a foreign-
language translation of that text, which, when
pronounced by the speaker, can be under-
stood by the listener.
The main challenge is that different languages
have different orthographies, different phoneme
inventories, and different phonotactic constraints,
so mismatches are inevitable. Despite this, the
system?s output should be both unambiguously
pronounceable by the speaker and readily under-
stood by the listener.
Our goal is to build an application that covers
many language pairs and directions. The current
paper describes a single system that lets a Chinese
person speak English.
We take a statistical modeling approach to this
problem, as is done in two lines of research that are
most related. The first is machine transliteration
(Knight and Graehl, 1998), in which names and
technical terms are translated across languages
with different sound systems. The other is re-
spelling generation (Hauer and Kondrak, 2013),
where an English speaker is given a phonetic hint
about how to pronounce a rare or foreign word
to another English speaker. By contrast, we aim
278
Chinese ?????
English It?s eight o?clock now
Chinglish ????????? (yi si ai te e ke lao ke nao)
Chinese ??????????
English this shirt is very stylish and not very expensive
Chinglish ????????????????????????
Chinese ??????????15??
English our minimum charge for delivery is fifteen dollars
Chinglish ????????????????????
Table 1: Examples of <Chinese, English, Chinglish> tuples from a phrasebook.
to help people issue full utterances that cross lan-
guage barriers.
2 Evaluation
Our system?s input is Chinese. The output is
a string of Chinese characters that approximate
English sounds, which we call Chinglish. We
build several candidate Chinese-to-Chinglish sys-
tems and evaluate them as follows:
? We compute the normalized edit distance
between the system?s output and a human-
generated Chinglish reference.
? A Chinese speaker pronounces the system?s
output out loud, and an English listener takes
dictation. We measure the normalized edit
distance against an English reference.
? We automate the previous evaluation by re-
place the two humans with: (1) a Chinese
speech synthesizer, and (2) a English speech
recognizer.
3 Data
We seek to imitate phonetic transformations found
in phrasebooks, so phrasebooks themselves are a
good source of training data. We obtained a col-
lection of 1312 <Chinese, English, Chinglish>
phrasebook tuples
1
(see Table 1).
We use 1182 utterances for training, 65 for de-
velopment, and 65 for test. We know of no other
computational work on this type of corpus.
Our Chinglish has interesting gross empirical
properties. First, because Chinglish and Chinese
are written with the same characters, they render
the same inventory of 416 distinct syllables. How-
ever, the distribution of Chinglish syllables differs
1
Dataset can be found at http://www.isi.edu/
natural-language/mt/chinglish-data.txt
a great deal from Chinese (Table 2). Syllables ?si?
and ?te? are very popular, because while conso-
nant clusters like English ?st? are impossible to re-
produce exactly, the particular vowels in ?si? and
?te? are fortunately very weak.
Frequency Rank Chinese Chinglish
1 de si
2 shi te
3 yi de
4 ji yi
5 zhi fu
Table 2: Top 5 frequent syllables in Chinese
(McEnery and Xiao, 2004) and Chinglish
We find that multiple occurrences of an English
word type are generally associated with the same
Chinglish sequence. Also, Chinglish characters do
not generally span multiple English words. It is
reasonable for ?can I? to be rendered as ?kan nai?,
with ?nai? spanning both English words, but this
is rare.
4 Model
We model Chinese-to-Chinglish translation with
a cascade of weighted finite-state transducers
(wFST), shown in Figure 2. We use an online
MT system to convert Chinese to an English word
sequence (Eword), which is then passed through
FST A to generate an English sound sequence
(Epron). FST A is constructed from the CMU Pro-
nouncing Dictionary (Weide, 2007).
Next, wFST B translates English sounds into
Chinese sounds (Pinyin-split). Pinyin is an official
syllable-based romanization of Mandarin Chinese
characters, and Pinyin-split is a standard separa-
tion of Pinyin syllables into initial and final parts.
Our wFST allows one English sound token to map
279
Figure 2: Finite-state cascade for modeling the re-
lation between Chinese and Chinglish.
to one or two Pinyin-split tokens, and it also allows
two English sounds to map to one Pinyin-split to-
ken.
Finally, FST C converts Pinyin-split into Pinyin,
and FST D chooses Chinglish characters. We also
experiment with an additional wFST E that trans-
lates English words directly into Chinglish.
5 Training
FSTs A, C, and D are unweighted, and remain so
throughout this paper.
5.1 Phoneme-based model
We must now estimate the values of FST B pa-
rameters, such as P(si|S). To do this, we first
take our phrasebook triples and construct sample
string pairs <Epron, Pinyin-split> by pronounc-
ing the phrasebook English with FST A, and by
pronouncing the phrasebook Chinglish with FSTs
D and C. Then we run the EM algorithm to learn
FST B parameters (Table 3) and Viterbi align-
ments, such as:
g r ae n d
g e r uan d e
5.2 Phoneme-phrase-based model
Mappings between phonemes are context-
sensitive. For example, when we decode English
?grandmother?, we get:
labeled Epron Pinyin-split P (p|e)
d d 0.46
d e 0.40
d i 0.06
s 0.01
ao r u 0.26
o 0.13
ao 0.06
ou 0.01
Table 3: Learned translation tables for the
phoneme based model
g r ae n d m ah dh er
g e r an d e m u e d e
where as the reference Pinyin-split sequence is:
g e r uan d e m a d e
Here, ?ae n? should be decoded as ?uan? when
preceded by ?r?. Following phrase-based meth-
ods in statistical machine translation (Koehn et
al., 2003) and machine transliteration (Finch and
Sumita, 2008), we model substitution of longer se-
quences. First, we obtain Viterbi alignments using
the phoneme-based model, e.g.:
g r ae n d m ah dh er
g e r uan d e m a d e
Second, we extract phoneme phrase pairs con-
sistent with these alignments. We use no phrase-
size limit, but we do not cross word boundaries.
From the example above, we pull out phrase pairs
like:
g? g e
g r? g e r
...
r? r
r ae n? r uan
...
We add these phrase pairs to FST B, and call
this the phoneme-phrase-based model.
5.3 Word-based model
We now turn to WFST E, which short-cuts di-
rectly from English words to Pinyin. We create
<English, Pinyin> training pairs from our phrase-
book simply by pronouncing the Chinglish with
FST D. We initially allow each English word type
to map to any sequence of Pinyin, up to length 7,
with uniform probability. EM learns values for pa-
rameters like P (nai te|night), plus Viterbi align-
ments such as:
280
Model
Top-1 Overall Top-1 Valid
Coverage
Average Edit Distance Average Edit Distance
Word based 0.664 0.042 29/65
Word-based hybrid training 0.659 0.029 29/65
Phoneme based 0.611 0.583 63/65
Phoneme-phrase based 0.194 0.136 63/65
Hybrid training and decoding 0.175 0.115 63/65
Table 4: English-to-Pinyin decoding accuracy on a test set of 65 utterances. Numbers are average edit
distances between system output and Pinyin references. Valid average edit distance is calculated based
only on valid outputs (e.g. 29 outputs for word based model).
accept tips
a ke sha pu te ti pu si
Notice that this model makes alignment errors
due to sparser data (e.g., the word ?tips? and ?ti pu
si? only appear once each in the training data).
5.4 Hybrid training
To improve the accuracy of word-based EM align-
ment, we use the phoneme based model to de-
code each English word in the training data to
Pinyin. From the 100-best list of decodings, we
collect combinations of start/end Pinyin syllables
for the word. We then modify the initial, uniform
English-to-Pinyin mapping probabilities by giving
higher initial weight to mappings that respect ob-
served start/end pairs. When we run EM, we find
that alignment errors for ?tips? in section 5.3 are
fixed:
accept tips
a ke sha pu te ti pu si
5.5 Hybrid decoding
The word-based model can only decode 29 of the
65 test utterances, because wFST E fails if an ut-
terance contains a new English word type, pre-
viously unseen in training. The phoneme-based
models are more robust, able to decode 63 of the
65 utterances, failing only when some English
word type falls outside the CMU pronouncing dic-
tionary (FST A).
Our final model combines these two, using the
word-based model for known English words, and
the phoneme-based models for unknown English
words.
6 Experiments
Our first evaluation (Table 4) is intrinsic, measur-
ing our Chinglish output against references from
the test portion of our phrasebook, using edit dis-
tance. Here, we start with reference English and
measure the accuracy of Pinyin syllable produc-
tion, since the choice of Chinglish character does
not affect the Chinglish pronunciation. We see that
the Word-based method has very high accuracy,
but low coverage. Our best system uses the Hy-
brid training/decoding method. As Table 6 shows,
the ratio of unseen English word tokens is small,
thus large portion of tokens are transformed us-
ing word-based method. The average edit dis-
tance of phoneme-phrase model and that of hy-
brid training/decoding model are close, indicating
that long phoneme-phrase pairs can emulate word-
pinyin mappings.
Unseen Total Ratio
Word Type 62 249 0.249
Token 62 436 0.142
Table 6: Unseen English word type and tokens in
test data.
Model
Valid Average
Edit Distance
Reference English 0.477
Phoneme based 0.696
Hybrid training and decoding 0.496
Table 7: Chinglish-to-English accuracy in dicta-
tion task.
Our second evaluation is a dictation task. We
speak our Chinglish character sequence output
aloud and ask an English monolingual person to
transcribe it. (Actually, we use a Chinese synthe-
sizer to remove bias.) Then we measure edit dis-
tance between the human transcription and the ref-
erence English from our phrasebook. Results are
shown in Table 7.
281
Chinese ?????????
Reference English what do you have for the Reunion dinner
Reference Chinglish ??????????????
Hybrid training/decoding Chinglish ??????????????
Dictation English what do you have for the reunion dinner
ASR English what do you high for 43 Union Cena
Chinese ???
Reference English wait for me
Reference Chinglish ???? (wei te fo mi)
Hybrid training/decoding Chinglish ???? (wei te fo mi)
Dictation English wait for me
ASR English wait for me
Table 5: Chinglish generated by hybrid training and decoding method and corresponding recognized
English by dictation and automatic synthesis-recognition method.
Model
Valid Average
Edit Distance
Word based 0.925
Word-based hybrid training 0.925
Phoneme based 0.937
Phoneme-phrase based 0.896
Hybrid training and decoding 0.898
Table 8: Chinglish-to-English accuracy in auto-
matic synthesis-recognition (ASR) task. Numbers
are average edit distance between recognized En-
glish and reference English.
Finally, we repeat the last experiment, but re-
moving the human from the loop, using both
automatic Chinese speech synthesis and English
speech recognition. Results are shown in Table 8.
Speech recognition is more fragile than human
transcription, so edit distances are greater. Table 5
shows a few examples of the Chinglish generated
by the hybrid training and decoding method, as
well as the recognized English from the dictation
and ASR tasks.
7 Conclusions
Our work aims to help people speak foreign lan-
guages they don?t know, by providing native pho-
netic spellings that approximate the sounds of for-
eign phrases. We use a cascade of finite-state
transducers to accomplish the task. We improve
the model by adding phrases, word boundary con-
straints, and improved alignment.
In the future, we plan to cover more language
pairs and directions. Each target language raises
interesting new challenges that come from its nat-
ural constraints on allowed phonemes, syllables,
words, and orthography.
References
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proceedings of the
Workshop on Technologies and Corpora for Asia-
Pacific Speech Translation (TCAST), pages 13?18.
Bradley Hauer and Grzegorz Kondrak. 2013. Auto-
matic generation of English respellings. In Proceed-
ings of NAACL-HLT, pages 634?643.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Anthony McEnery and Zhonghua Xiao. 2004. The
lancaster corpus of Mandarin Chinese: A corpus for
monolingual and contrastive language study. Reli-
gion, 17:3?4.
R Weide. 2007. The CMU pronunciation dictionary,
release 0.7a.
282
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 706?711,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Be Appropriate and Funny: Automatic Entity Morph Encoding
Boliang Zhang
1
, Hongzhao Huang
1
, Xiaoman Pan
1
, Heng Ji
1
, Kevin Knight
2
Zhen Wen
3
, Yizhou Sun
4
, Jiawei Han
5
, Bulent Yener
1
1
Computer Science Department, Rensselaer Polytechnic Institute
2
Information Sciences Institute, University of Southern California
3
IBM T. J. Watson Research Center
4
College of Computer and Information Science, Northeastern University
5
Computer Science Department, Univerisity of Illinois at Urbana-Champaign
1
{zhangb8,huangh9,panx2,jih,yener}@rpi.edu,
2
knight@isi.edu
3
zhenwen@us.ibm.com,
4
yzsun@ccs.neu.edu,
5
hanj@illinois.edu
Abstract
Internet users are keen on creating differ-
ent kinds of morphs to avoid censorship,
express strong sentiment or humor. For
example, in Chinese social media, users
often use the entity morph ???? (In-
stant Noodles)? to refer to ???? (Zhou
Yongkang)? because it shares one char-
acter ?? (Kang)? with the well-known
brand of instant noodles ???? (Master
Kang)?. We developed a wide variety of
novel approaches to automatically encode
proper and interesting morphs, which can
effectively pass decoding tests
1
.
1 Introduction
One of the most innovative linguistic forms in so-
cial media is Information Morph (Huang et al,
2013). Morph is a special case of alias to hide the
original objects (e.g., sensitive entities and events)
for different purposes, including avoiding censor-
ship (Bamman et al, 2012; Chen et al, 2013),
expressing strong sentiment, emotion or sarcasm,
and making descriptions more vivid. Morphs are
widely used in Chinese social media. Here is an
example morphs: ?????????????
????. (Because of Gua Dad?s issue, Instant
Noodles faces down with Antenna.)?, where
? ??? (Gua Dad)? refers to ???? (Bo Xilai)?
because it shares one character ?? (Gua)? with
???? (Bo Guagua)? who is the son of ???
? (Bo Xilai)?;
? ???? (Instant Noodles)? refers to ????
(Zhou Yongkang)? because it shares one char-
acter ?? (kang)? with the well-known instant
noodles brand ???? (Master Kang)?;
1
The morphing data set is available for research purposes:
http://nlp.cs.rpi.edu/data/morphencoding.tar.gz
? ??? (Antenna)? refers to ???? (Wen Ji-
abao)? because it shares one character ??
(baby)? with the famous children?s television
series ????? (Teletubbies)?;
In contrast with covert or subliminal chan-
nels studied extensively in cryptography and se-
curity, Morphing provides confidentiality against
a weaker adversary which has to make a real time
or near real time decision whether or not to block
a morph within a time interval t. It will take longer
than the duration t for a morph decoder to decide
which encoding method is used and exactly how it
is used; otherwise adversary can create a codebook
and decode the morphs with a simple look up.
We note that there are other distinct characteristics
of morphs that make them different from crypto-
graphic constructs: (1) Morphing can be consid-
ered as a way of using natural language to com-
municate confidential information without encryp-
tion. Most morphs are encoded based on seman-
tic meaning and background knowledge instead
of lexical changes, so they are closer to Jargon.
(2) There can be multiple morphs for an entity.
(3) The Shannon?s Maxim ?the enemy knows the
system? does not always hold. There is no com-
mon code-book or secret key between the sender
and the receiver of a morph. (4) Social networks
play an important role in creating morphs. One
main purpose of encoding morphs is to dissemi-
nate them widely so they can become part of the
new Internet language. Therefore morphs should
be interesting, fun, intuitive and easy to remem-
ber. (5) Morphs rapidly evolve over time, as some
morphs are discovered and blocked by censorship
and newly created morphs emerge.
We propose a brand new and challenging re-
search problem - can we automatically encode
morphs for any given entity to help users commu-
nicate in an appropriate and fun way?
706
2 Approaches
2.1 Motivation from Human Approaches
Let?s start from taking a close look at human?s
intentions and general methods to create morphs
from a social cognitive perspective. In Table 1
and Table 2, we summarize 548 randomly selected
morphs into different categories. In this paper we
automate the first seven human approaches, with-
out investigating the most challenging Method 8,
which requires deep mining of rich background
and tracking all events involving the entities.
2.2 M1: Phonetic Substitution
Given an entity name e, we obtain its pho-
netic transcription pinyin(e). Similarly, for each
unique term t extracted from Tsinghua Weibo
dataset (Zhang et al, 2013) with one billion
tweets from 1.8 million users from 8/28/2012 to
9/29/2012, we obtain pinyin(t). According to the
Chinese phonetic transcription articulation man-
ner
2
, the pairs (b, p), (d, t), (g,k), (z,c), (zh,ch),
( j,q), (sh,r), (x,h), (l,n), (c,ch), (s,sh) and (z,zh)
are mutually transformable.
If a part of pinyin(e) and pinyin(t) are identi-
cal or their initials are transformable, we substi-
tute the part of e with t to form a new morph.
For example, we can substitute the characters of
??? ?? (Bill Gates) [Bi Er Gai Ci]? with
??? (Nose and ear) [Bi Er]? and ??? (Lid)
[Gai Zi]? to form new morph ??? ?? (Nose
and ear Lid) [Bi Er Gai Zi]?. We rank the candi-
dates based on the following two criteria: (1) If the
morph includes more negative words (based on a
gazetteer including 11,729 negative words derived
from HowNet (Dong and Dong, 1999), it?s more
humorous (Valitutti et al, 2013). (2) If the morph
includes rarer terms with low frequency, it is more
interesting (Petrovic and Matthews, 2013).
2.3 M2: Spelling Decomposition
Chinese characters are ideograms, hieroglyphs
and mostly picture-based. It allows us to natu-
rally construct a virtually infinite range of combi-
nations from a finite set of basic units - radicals (Li
and Zhou, 2007). Some of these radicals them-
selves are also characters. For a given entity name
e = c
1
...c
n
, if any character c
k
can be decomposed
into two radicals c
1
k
and c
2
k
which are both char-
acters or can be converted into characters based
on their pictograms (e.g., the radical ??? can be
2
http://en.wikipedia.org/wiki/Pinyin#Initials and finals
converted into??? (grass) ), we create a morph by
replacing c
k
with c
1
k
c
2
k
in e. Here we use a charac-
ter to radical mapping table that includes 191 rad-
icals (59 of them are characters) and 1328 com-
mon characters. For example, we create a morph
???? (Person Dumb Luo)? for ??? (Paul)?
by decomposing ?? (Pau-)? into ?? (Person)?
and ?? (Dull)?. A natural alternative is to com-
posing two chracter radicals in an entity name to
form a morph. However, very few Chinese names
include two characters with single radicals.
2.4 M3: Nickname Generation
We propose a simple method to create morphs by
duplicating the last character of an entity?s first
name. For example, we create a morph ???
(Mimi)? to refer to ??? (Yang Mi)?.
2.5 M4: Translation and Transliteration
Given an entity e, we search its English translation
EN(e) based on 94,015 name translation pairs (Ji
et al, 2009). Then, if any name component in
EN(e) is a common English word, we search for
its Chinese translation based on a 94,966 word
translation pairs (Zens and Ney, 2004), and use the
Chinese translation to replace the corresponding
characters in e. For example, we create a morph
??? ?? (Larry bird)? for ??? ?? (Larry
Bird)? by replacing the last name ??? (Bird)?
with its Chinese translation ??? (bird)?.
2.6 M5: Semantic Interpretation
For each character c
k
in the first name of a given
entity name e, we search its semantic interpreta-
tion sentence from the Xinhua Chinese character
dictionary including 20,894 entries
3
. If a word
in the sentence contains c
k
, we append the word
with the last name of e to form a new morph. Sim-
ilarly to M1, we prefer positive, negative or rare
words. For example, we create a morph ????
(Bo Mess)? for ???? (Bo Xi Lai)? because the
semantic interpretation sentence for ?? (Lai)? in-
cludes a negative word ??? (Mess)?.
2.7 M6: Historical Figure Mapping
We collect a set of 38 famous historical figures
including politicians, emperors, poets, generals,
ministers and scholars from a website. For a given
entity name e, we rank these candidates by ap-
plying the resolution approach as described in our
previous work (Huang et al, 2013) to measure the
similarity between an entity and a historic figure
3
http://xh.5156edu.com/
707
Category
Frequency
Distribution
Examples
Entity Morph Comment
(1) Avoid censorship 6.56% ??? (Bo Xi-
lai)
B?? (B Secre-
tary)
?B? is the first letter of ?Bo? and ?Secretary? is
the entity?s title.
(2) Express strong
sentiment, sarcasm,
emotion
15.77% ??? (Wang
Yongping)
? ? ? (Miracle
Brother)
Sarcasm on the entity?s public speech: ?It?s a mir-
acle that the girl survived (from the 2011 train col-
lision)?.
(3) Be humorous or
make descriptions
more vivid
25.91% ?? (Yang Mi) ???? (Tender
Beef Pentagon)
The entity?s face shape looks like the shape of fa-
mous KFC food ?Tender Beef Pentagon?.
Mixture 25.32% ? ? ?
(Gaddafi)
???? (Crazy
Duck Colonel)
Sarcasm on Colonel Gaddafi?s violence.
Others 23.44% ??? (Chi-
ang Kai-shek)
??? (Peanut) Joseph Stilwell, a US general in China during
World War II, called Chiang Kai-shek ????
(Peanut)? in his diary because of his stubbornness.
Table 1: Morph Examples Categorized based on Human Intentions
No. Category
Frequency
Distribution
Example
Entity Morph Comment
M1 Phonetic Sub-
stitution
12.77% ? ? ?
(Sarkozy)
??? (Silly Po-
lite)
The entity?s phonetic transcript ?Sa Ke Qi? is
similar to the morph?s ?Sha Ke Qi?.
M2 Spelling De-
composition
0.73% ??? (Hu
Jintao)
?? (Old Moon) The entity?s last name is decomposed into the
morph ??? (Old Moon)??
M3 Nickname Gen-
eration
12.41% ??? (Jiang
Zemin)
?? (Old Jiang) The morph is a conventional name for old people
with last name ?Jiang?.
M4 Translation &
Transliteration
3.28% ?? (Bush) ?? (shrub) The morph is the Chinese translation of ?bush?.
M5 Semantic Inter-
pretation
20.26% ??? (Kim
Il Sung)
??? (Kim Sun) The character ??? in the entity name means ??
? (Sun)?.
M6 Historical Fig-
ure Mapping
3.83% ??? (Bo
Xilai)
??? (Conquer
West King)
The entity shares characteristics and political ex-
periences similar to the morph.
M7 Characteristics
Modeling
20.62% ??? (Kim
Il Sung)
??? (Kim Fat) ??? (Fat)? describes ???? (Kim Il
Sung)??s appearance.
M8
Reputation and
public perception
26.09%
? ? ?
(Obama)
?? (Staring at
the sea)
Barack Obama received a calligraphy ????
? (Staring at sea and listening to surf)? as a
present when he visited China.
??? (Ma
Jingtao)
???? (Roar
Bishop)
In the films Ma Jingtao starred, he always used
exaggerated roaring to express various emotions.
??? (Ma
Yingjiu)
??? (Ma Se-
cession)
The morph derives from Ma Yingjiu?s political
position on cross-strait relations.
Table 2: Morph Examples Categorized based on Human Generation Methods
based on their semantic contexts. For example,
this approach generates a morph ??? (the First
Emperor)? for ???? (Mao Zedong)? who is the
first chairman of P. R. China and ??? (the Sec-
ond Emperor )? for ???? (Deng Xiaoping )?
who succeeded Mao.
2.8 M7: Characteristics Modeling
Finally, we propose a novel approach to auto-
matically generate an entity?s characteristics using
Google word2vec model (Mikolov et al, 2013).
To make the vocabulary model as general as pos-
sible, we use all of the following large corpora
that we have access to: Tsinghua Weibo dataset,
Chinese Gigaword fifth edition
4
which includes
10 million news documents, TAC-KBP 2009-2013
Source Corpora (McNamee and Dang, 2009; Ji et
4
http://catalog.ldc.upenn.edu/LDC2011T13
al., 2010; Ji et al, 2011; Ji and Grishman, 2011)
which include 3 million news and web documents,
and DARPA BOLT program?s discussion forum
corpora with 300k threads. Given an entity e, we
compute the semantic relationship between e and
each word from these corpora. We then rank the
words by: (1) cosine similarity, (2) the same cri-
teria as in section 2.6. Finally we append the top
ranking word to the entity?s last name to obtain
a new morph. Using this method, we are able
to generate many vivid morphs such as ?? ??
(Yao Wizard)? for ??? (Yao Ming)?.
3 Experiments
3.1 Data
We collected 1,553,347 tweets from Chinese Sina
Weibo from May 1 to June 30, 2013. We extracted
708
187 human created morphs based on M1-M7 for
55 person entities. Our approach generated 382
new morphs in total.
3.2 Human Evaluation
We randomly asked 9 Chinese native speakers
who regularly access Chinese social media and are
not involved in this work to conduct evaluation in-
dependently. We designed the following three cri-
teria based on Table 1:
? Perceivability: Who does this morph refer to?
(i) Pretty sure, (ii) Not sure, and (iii) No clues.
? Funniness: How interesting is the morph? (i)
Funny, (ii) Somewhat funny, and (iii) Not funny.
? Appropriateness: Does the morph describe the
target entity appropriately? (i) Make sense, (ii)
Make a little sense, and (iii) Make no sense.
The three choices of each criteria account for
100% (i), 50% (ii) and 0% (iii) satisfaction rate,
respectively. If the assessor correctly predicts the
target entity with the Perceivability measure, (s)he
is asked to continue to answer the Funniness and
Appropriateness questions; otherwise the Funni-
ness and Appropriateness scores are 0. The hu-
man evaluation results are shown in Table 4. The
Fleiss?s kappa coefficient among all the human as-
sessors is 0.147 indicating slight agreement.
From Table 4 we can see that overall the sys-
tem achieves 66% of the human performance
with comparable stability as human. In partic-
ular, Method 4 based on translation and translit-
eration generates much more perceivable morphs
than human because the system may search in a
larger vocabulary. Interestingly, similar encour-
aging results - system outperforms human - have
been observed by previous back-transliteration
work (Knight and Graehl, 1998).
It?s also interesting to see that human assessors
can only comprehend 76% of the human generated
morphs because of the following reasons: (1) the
morph is newly generated or it does not describe
the characteristics of the target entity well; and (2)
the target entity itself is not well known to human
assessors who do not keep close track of news top-
ics. In fact only 64 human generated morphs and
72 system generated morphs are perceivable by all
human assessors.
For Method 2, the human created morphs are
assessed as much more and funny than the sys-
tem generated ones because human creators use
this approach only if: (1). the radicals still reflect
the meaning of the character (e.g., ?? (worry)?
is decomposed into two radicals ??? (heart au-
tumn)? instead of three ????? (grain fire heart)
because people tend to feel sad when the leaves
fall in the autumn), (2). the morph reflects some
characteristics of the entity (e.g., ???? (Jiang
Zemin)? has a morph ????? (Water Engi-
neer Zemin)? because he gave many instructions
on water conservancy construction); or (3). The
morph becomes very vivid and funny (e.g., the
morph ?????? (Muji Yue Yue Bird)? for
???? is assessed as very funny because ??
?(Muji)? looks like a Japanese name, ???(Yue
Yue)? can also refer to a famous chubby woman,
and ??? (bird man)? is a bad word referring to
bad people); or (4). The morph expresses strong
sentiment or sarcasm; or (5) The morph is the
name of another entity (e.g., the morph ???(Gu
Yue)? for ????(Hu Jintao)? is also the name
of a famous actor who often acts as Mao Zedong).
The automatic approach didn?t explore these intel-
ligent constraints and thus produced more boring
morph. Moreover, sometimes human creators fur-
ther exploit traditional Chinese characters, gener-
alize or modify the decomposition results.
Table 3 presents some good (with average score
above 80%) and bad (with average score below
20%) examples.
Good Examples
Entity Morph Method
??? (Osama bin
Laden)
??? (The silly turn-
ing off light)
M1
??? (Chiang Kai-
shek)
???? (Grass Gen-
eral Jie Shi)
M2
???? (Bill Gates) ???? (Bill Gates) M4
Bad Examples
Entity Morph Method
?? (Kobe) ?? (Arm) M1
? ? ? ? ?
(Medvedev)
??? (Mei Virtue) M5
??? (Jeremy Lin) ?? (Lao Tze) M6
Table 3: System Generated Morph Examples
To understand whether users would adopt sys-
tem generated morphs for their social media com-
munication, we also ask the assessors to recite
the morphs that they remember after the survey.
Among all the morphs that they remember cor-
rectly, 20.4% are system generated morphs, which
is encouraging.
3.3 Automatic Evaluation
Another important goal of morph encoding is to
avoid censorship and freely communicate about
709
Human System Human System Human System Human System Human System Human System Human System Human System# of morphs 17 124 4 21 10 54 9 28 64 87 9 18 74 50 187 382Perceivability 75 76 95 86 94 81 61 71 87 59 66 5 77 34 76 67Funniness 78 49 92 43 44 41 70 47 70 35 74 28 79 44 76 46Appropriateness 71 51 89 59 81 43 75 49 76 36 78 18 82 38 79 43Average 75 59 92 57 73 55 69 56 78 43 73 17 79 39 77 52Standard Deviation 12.29 21.81 7.32 11.89 13.2 9.2 17.13 20.3 18.83 17.54 10.01 21.23 15.18 15.99 15.99 18.14
h s2568 58984214.3 29691742 45712641 1153922692 26766901.8 811317052 1278447812 1E+05255.7 329.12568 58984 214.3 2969 1742 4571 2641 11539 22692 26766 901.8 8113 17052 12784
M6 M7 OverallM1 M2 M3 M4 M5
Table 4: Human Evaluation Satisfaction Rate (%)
certain entities. To evaluate how well the new
morphs can pass censorship, we simulate the cen-
sorship using an automatic morph decoder con-
sisted of a morph candidate identification system
based on Support Vector Machines incorporating
anomaly analysis and our morph resolution sys-
tem (Huang et al, 2013). We use each system gen-
erated morph to replace its corresponding human-
created morphs in Weibo tweets and obtain a new
?morphed? data set. The morph decoder is then
applied to it. We define discovery rate as the per-
centage of morphs identified by the decoder, and
the ranking accuracy Acc@k to evaluate the reso-
lution performance. We conduct this decoding ex-
periment on 247 system generated and 151 human
generated perceivable morphs with perceivability
scores > 70% from human evaluation.
Figure 1 shows that in general the decoder
achieves lower discovery rate on system gener-
ated morphs than human generated ones, because
the identification component in the decoder was
trained based on human morph related features.
This result is promising because it demonstrates
that the system generated morphs contain new and
unique characteristics which are unknown to the
decoder. In contrast, from Figure 2 we can see
that system generated morphs can be more easily
resolved into the right target entities than human
generated ones which are more implicit.
0	 ?
20	 ?
40	 ?
60	 ?
80	 ?
100	 ?
M1	 ? M2	 ? M3	 ? M4	 ? M5	 ? M6	 ? M7	 ? ALL	 ?
Human	 ?created	 ?	 ?morph	 ? System	 ?generated	 ?morph	 ?
Figure 1: Discovery Rate (%)
4 Related Work
Some recent work attempted to map between Chi-
nese formal words and informal words (Xia et al,
2005; Xia and Wong, 2006; Xia et al, 2006; Li
Figure 2: Resolution Acc@K Accuracy (%)
and Yarowsky, 2008; Wang et al, 2013; Wang and
Kan, 2013). We incorporated the pronunciation,
lexical and semantic similarity measurements pro-
posed in these approaches. Some of our basic se-
lection criteria are also similar to the constraints
used in previous work on generating humors (Val-
itutti et al, 2013; Petrovic and Matthews, 2013).
5 Conclusions and Future Work
This paper proposed a new problem of encoding
entity morphs and developed a wide variety of
novel automatic approaches. In the future we will
focus on improving the language-independent ap-
proaches based on historical figure mapping and
culture and reputation modeling. In addition, we
plan to extend our approaches to other types of in-
formation including sensitive events, satires and
metaphors so that we can generate fable stories.
We are also interested in tracking morphs over
time to study the evolution of Internet language.
Acknowledgments
This work was supported by U.S. ARL No.
W911NF-09-2-0053, DARPA No. FA8750-13-
2-0041 and No. W911NF-12-C-0028, ARO
No. W911NF-13-1-0193, NSF IIS-0953149,
CNS-0931975, IIS-1017362, IIS-1320617, IIS-
1354329, IBM, Google, DTRA, DHS and RPI.
The views and conclusions in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
710
References
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2012. Censorship and deletion practices in
Chinese social media. First Monday, 17(3).
Le Chen, Chi Zhang, and Christo Wilson. 2013.
Tweeting under pressure: analyzing trending topics
and evolving word choice on sina weibo. In Pro-
ceedings of the first ACM conference on Online so-
cial networks, pages 89?100.
Zhendong Dong and Qiang Dong. 1999. Hownet. In
http://www.keenage.com.
Hongzhao Huang, Zhen Wen, Dian Yu, Heng Ji,
Yizhou Sun, Jiawei Han, and He Li. 2013. Resolv-
ing entity morphs in censored data. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL2013).
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the Association for Computational
Linguistics (ACL2011).
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens, and Hermann Ney. 2009. Name extraction
and translation for distillation. Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Text Analysis
Conference (TAC) 2010.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac 2011 knowledge base popula-
tion track. In Proc. Text Analysis Conference (TAC)
2011.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Zhifei Li and David Yarowsky. 2008. Mining and
modeling relations between formal and informal chi-
nese phrases from web corpora. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP2008), pages 1031?
1040.
Jianyu Li and Jie Zhou. 2007. Chinese character struc-
ture analysis based on complex networks. Phys-
ica A: Statistical Mechanics and its Applications,
380:629?638.
Paul McNamee and Hoa Trang Dang. 2009.
Overview of the tac 2009 knowledge base popula-
tion track. In Proceedings of Text Analysis Confer-
ence (TAC2009).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111?3119.
Sasa Petrovic and David Matthews. 2013. Unsuper-
vised joke generation from big data. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL2013).
Alessandro Valitutti, Hannu Toivonen, Antoine
Doucet, and Jukka M. Toivanen. 2013. ?let every-
thing turn well in your wife?: Generation of adult
humor using lexical constraints. In Proceedings
of the Association for Computational Linguistics
(ACL2013).
Aobo Wang and Min-Yen Kan. 2013. Mining informal
language from chinese microtext: Joint word recog-
nition and segmentation. In Proceedings of the As-
sociation for Computational Linguistics (ACL2013).
Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi
Onishi, and Kai Ishikawa. 2013. Chinese informal
word normalization: an experimental study. In Pro-
ceedings of International Joint Conference on Natu-
ral Language Processing (IJCNLP2013).
Yunqing Xia and Kam-Fai Wong. 2006. Anomaly de-
tecting within dynamic chinese chat text. In Proc.
Workshop On New Text Wikis And Blogs And Other
Dynamic Text Sources.
Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. Nil
is not nothing: Recognition of chinese network in-
formal language expressions. In 4th SIGHAN Work-
shop on Chinese Language Processing at IJCNLP,
volume 5.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006.
A phonetic-based approach to chinese chat text nor-
malization. In Proceedings of COLING-ACL2006,
pages 993?1000.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proceedings of HLT-NAACL2004.
Jing Zhang, Biao Liu, Jie Tang, Ting Chen, and Juanzi
Li. 2013. Social influence locality for modeling
retweeting behaviors. In Proceedings of the 23rd
International Joint Conference on Artificial Intelli-
gence (IJCAI?13), pages 2761?2767.
711
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 10?18,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Decoder for
Probabilistic Synchronous Tree Insertion Grammars
Steve DeNeefe ? and Kevin Knight ?
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
{sdeneefe,knight}@isi.edu
Heiko Vogler ?
Department of Computer Science
Technische Universita?t Dresden
D-01062 Dresden
Heiko.Vogler@tu-dresden.de
Abstract
Synchronous tree insertion grammars
(STIG) are formal models for syntax-
based machine translation. We formal-
ize a decoder for probabilistic STIG; the
decoder transforms every source-language
string into a target-language tree and cal-
culates the probability of this transforma-
tion.
1 Introduction
Tree adjoining grammars (TAG) were invented in
(Joshi et al 1975) in order to better character-
ize the string sets of natural languages1. One of
TAG?s important features is the ability to introduce
two related syntactic units in a single rule, then
push those two units arbitrarily far apart in sub-
sequent derivation steps. For machine translation
(MT) between two natural languages, each being
generated by a TAG, the derivations of the two
TAG may be synchronized (Abeille et al, 1990;
Shieber and Shabes, 1990) in the spirit of syntax-
directed transductions (Lewis and Stearns, 1968);
this results in synchronous TAG (STAG). Recently,
in (Nesson et al, 2005, 2006) probabilistic syn-
chronous tree insertion grammars (pSTIG) were
discussed as model of MT; a tree insertion gram-
mar is a particular TAG in which the parsing prob-
lem is solvable in cubic-time (Schabes and Wa-
ters, 1994). In (DeNeefe, 2009; DeNeefe and
Knight 2009) a decoder for pSTIG has been pro-
posed which transforms source-language strings
into (modifications of) derivation trees of the
pSTIG. Nowadays, large-scale linguistic STAG
rule bases are available.
In an independent tradition, the automata-
theoretic investigation of the translation of trees
? financially supported by NSF STAGES project, grant
#IIS-0908532.
? financially supported by DFG VO 1011/5-1.
1see (Joshi and Shabes, 1997) for a survey
led to the rich theory of tree transducers (Ge?cseg
and Steinby, 1984, 1997). Roughly speaking, a
tree transducer is a finite term rewriting system. If
each rewrite rule carries a probablity or, in gen-
eral, a weight from some semiring, then they are
weighted tree transducers (Maletti, 2006, 2006a;
Fu?lo?p and Vogler, 2009). Such weighted tree
transducers have also been used for the specifi-
cation of MT of natural languages (Yamada and
Knight, 2001; Knight and Graehl, 2005; Graehl et
al., 2008; Knight and May 2009).
Martin and Vere (1970) and Schreiber (1975)
established the first connections between the two
traditions; also Shieber (2004, 2006) and Maletti
(2008, 2010) investigated their relationship.
The problem addressed in this paper is the
decoding of source-language strings into target-
language trees where the transformation is de-
scribed by a pSTIG. Currently, this decoding re-
quires two steps: first, every source string is
translated into a derivation tree of the underly-
ing pSTIG (DeNeefe, 2009; DeNeefe and Knight
2009), and second, the derivation tree is trans-
formed into the target tree using an embedded tree
transducer (Shieber, 2006). We propose a trans-
ducer model, called a bottom-up tree adjoining
transducer, which performs this decoding in a sin-
gle step and, simultaneously, computes the prob-
abilities of its derivations. As a basis of our ap-
proach, we present a formal definition of pSTIG.
2 Preliminaries
For two sets ? and A, we let U?(A) be the set of
all (unranked) trees over ? in which also elements
of A may label leaves. We abbreviate U?(?) by
U?. We denote the set of positions, leaves, and
non-leaves of ? ? U? by pos(?) ? N?, lv(?), and
nlv(?), resp., where ? denotes the root of ? and
w.i denotes the ith child of position w; nlv(?) =
pos(?) \ lv(?). For a position w ? pos(?), the la-
bel of ? at w (resp., subtree of ? at w) is denoted
10
by ?(w) (resp., ?|w). If additionally ? ? U?(A),
then ?[?]w denotes the tree which is obtained from
? by replacing its subtree at w by ?. For every
? ? ? ?A, the set pos?(?) is the set of all those
positions w ? pos(?) such that ?(w) ? ?. Simi-
larly, we can define lv?(?) and nlv?(?). The yield
of ? is the sequence yield(?) ? (? ? A)? of sym-
bols that label the leaves from left to right.
If we associate with ? ? ? a rank k ? N, then
we require that in every tree ? ? U?(A) every ?-
labeled position has exactly k children.
3 Probabilistic STAG and STIG
First we will define probabilistic STAG, and sec-
ond, as a special case, probabilistic STIG.
Let N and T be two disjoint sets of, resp., non-
terminals and terminals. A substitution rule r is a
tuple (?s, ?t, V,W, P radj) where
? ?s, ?t ? UN (T ) (source and target tree) and
|lvN (?s)| = |lvN (?t)|,
? V ? lvN (?s)?lvN (?t) (substitution sites), V
is a one-to-one relation, and |V | = |lvN (?s)|,
? W ? nlvN (?s)?nlvN (?t) (potential adjoin-
ing sites), and
? P radj : W ? [0, 1] (adjoining probability).
An auxiliary rule r is a tuple (?s, ?t, V,W, ?, P radj)
where ?s, ?t,W , and P radj are defined as above and
? V is defined as above except that |V | =
|lvN (?s)| ? 1 and
? ? = (?s, ?t) ? lvN (?s)? lvN (?t) and neither
?s nor ?t occurs in any element of V ; more-
over, ?s(?) = ?s(?s) and ?t(?) = ?t(?t), and
?s 6= ? 6= ?t; the node ?s (and ?t) is called
the foot-node of ?s (resp., ?t).
An (elementary) rule is either a substitution rule
or an auxiliary rule. The root-category of a rule r
is the tuple (?s(?), ?t(?)), denoted by rc(r).
A probabilistic synchronous tree ad-
joining grammar (pSTAG) is a tuple
G = (N,T, (Ss, St),S,A, P ) such that N
and T are two disjoint sets (resp., of nonterminals
and terminals), (Ss, St) ? N?N (start nontermi-
nal), S and A are finite sets of, resp., substitution
rules and auxiliary rules, and P : S ? A ? [0, 1]
such that for every (A,B) ? N ?N ,
?
r?S
rc(r)=(A,B)
P (r) = 1 and
?
r?A
rc(r)=(A,B)
P (r) = 1
assuming that in each case the number of sum-
mands is not zero. In the following, let G always
denote an arbitrary pSTAG.
Ss
? A? A
?
a
r1??
St
B
?
B? ?a
P (r1) = 1
P r1adj(a) = .9
A
A ?
b,c
?
r2??
B
B
?
B
b
c ?
P (r2) = .4
P r2adj(b) = .2
P r2adj(c) = .6
A
A A
?
d
? e
r3??
B
? B
d,e
?
P (r3) = .6
P r3adj(d) = .3
P r3adj(e) = .8
A
?
r4??
B
?
P (r4) = .1
A
?
r5??
B
?
P (r5) = .9
Figure 1: The running example pSTAG G.
In Fig. 1 we show the rules of our running ex-
ample pSTAG, where the capital Roman letters are
the nonterminals and the small Greek letters are
the terminals. The substitution site (in rule r1) is
indicated by ?, and the potential adjoining sites are
denoted2 by a, b, c, d, and e. For instance, in for-
mal notation the rules r1 and r2 are written as fol-
lows:
r1 = (Ss(?,A,A(?)), St(B(?), B, ?), {?}, {a}, P r1adj)
where ? = (2, 2) and a = (3, 1), and
r2 = (A(A, ?), B(B(?), B), ?, {b, c}, ?, P r2adj)
where b = (?, ?), c = (?, 1), and ? = (1, 2).
In the derivation relation of G we will distin-
guish four types of steps:
1. substitution of a rule at a substitution site
(substitution),
2. deciding to turn a potential adjoining site into
an activated adjoining site (activation),
3. deciding to drop a potential adjoining site,
i.e., not to adjoin, (non-adjoining) and
4. adjoining of a rule at an activated adjoining
site (adjoining).
In the sentential forms (defined below) we will
maintain for every adjoining site w a two-valued
flag g(w) indicating whether w is a potential
(g(w) = p) or an activated site (g(w) = a).
The set of sentential forms ofG is the set SF(G)
of all tuples ? = (?s, ?t, V,W, g) with
2Their placement (as left or right index) does not play a
role yet, but will later when we introduce pSTIG.
11
? ?s, ?t ? UN (T ),
? V ? lvN (?s) ? lvN (?t) is a one-to-one rela-
tion, |V | = |lvN (?s)| = |lvN (?t)|,
? W ? nlvN (?s)? nlvN (?t), and
? g : W ? {p, a}.
The derivation relation (of G) is the binary
relation ? ? SF(G) ? SF(G) such that
for every ?1 = (?1s , ?1t , V1,W1, g1) and ?2 =
(?2s , ?2t , V2,W2, g2) we have ?1 ? ?2 iff one of
the following is true:
1. (substitution) there are w = (ws, wt) ? V1
and r = (?s, ?t, V,W, P radj) ? S such that
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [?s]ws and ?2t = ?1t [?t]wt ,
? V2 = (V1 \ {w}) ? w.V ,3
? W2 = W1 ? w.W , and
? g2 is the union of g1 and the set of pairs
(w.u, p) for every u ?W ;
this step is denoted by ?1
w,r=? ?2;
2. (activation) there is a w ? W1 with g1(w) =
p and (?1s , ?1t , V1,W1) = (?2s , ?2t , V2,W2),
and g2 is the same as g1 except that g2(w) =
a; this step is denoted by ?1
w=? ?2;
3. (non-adjoining) there is w ? W1 with
g1(w) = p and (?1s , ?1t , V1) = (?2s , ?2t , V2),
W2 = W1 \ {w}, and g2 is g1 restricted to
W2; this step is denoted by ?1
?w=? ?2;
4. (adjoining) there are w ? W1 with g1(w) =
a, and r = (?s, ?t, V,W, ?, P radj) ? A such
that, for w = (ws, wt),
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [? ?s]ws where ? ?s = ?s[?1s |ws ]?s ,
?2t = ?1t [? ?t]wt where ? ?t = ?t[?1t |wt ]?t ,
? V2 is the smallest set such that (i) for
every (us, ut) ? V1 we have (u?s, u?t) ? V2
where
u?s =
{
us if ws is not a prefix of us,
ws. ?s .u if us = ws.u for some u;
and u?t is obtained in the same way from ut,
wt, and ?t, and (ii) V2 contains w.V ;
? W2 is the smallest set such that (i) for every
(us, ut) ? W1 \ {w} we have (u?s, u?t) ?
W2 where u?s and u?t are obtained in the
same way as for V2, and g2(u?s, u?t) =
g1(us, ut) and (ii) W2 contains w.W and
g2(w.u) = p for every u ?W ;
this step is denoted by ?1
w,r=? ?2.
3w.V = {(ws.vs, wt.vt) | (vs, vt) ? V }
In Fig. 2 we show a derivation of our running
example pSTAG where activated adjoining sites
are indicated by surrounding circles, the other ad-
joining sites are potential.
Ss? ?? St?
substitution of
r1 at (?, ?)
=? P (r1) = 1
Ss
? A? A
?
a ??
St
B
?
B? ?a
substitution of
r4 at (2, 2)
=? P (r4) = .1
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
activation
at a = (3, 1)
=? P r1adj(a) = .9
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
adjoining of
r2 at a = (3, 1)
=? P (r2) = .4
Ss
? A
?
A
A
?
?
b,c ??
St
B
B
?
B
?
B
?
?b
c
non-adjoining
at c = (3, 1.1)
=? 1? P r2adj(c) = .4
Ss
? A
?
A
A
?
?
b ??
St
B
B
?
B
?
B
?
?b
non-adjoining
at b = (3, 1)
=? 1? P r2adj(b) = .8
Ss
? A
?
A
A
?
?
??
St
B
B
?
B
?
B
?
?
Figure 2: An example derivation with total proba-
bility 1? .1? .9? .4? .4? .8 = .01152.
The only initial sentential form is ?in =
(Ss, St, {(?, ?)}, ?, ?). A sentential form ? is final
if it has the form (?s, ?t, ?, ?, ?). Let ? ? SF(G).
A derivation (of ?) is a sequence d of the form
?0u1?1 . . . un?n with ?0 = ?in and n ? 0,
?i?1
ui? ?i for every 1 ? i ? n (and ?n = ?). We
12
denote ?n also by last(d), and the set of all deriva-
tions of ? (resp., derivations) by D(?) (resp., D).
We call d ? D successful if last(d) is final.
The tree transformation computed by G is
the relation ?G ? UN (T ) ? UN (T ) with
(?s, ?t) ? ?G iff there is a successful derivation
of (?s, ?t, ?, ?, ?).
Our definition of the probability of a deriva-
tion is based on the following observation.4 Let
d ? D(?) for some ? = (?s, ?t, V,W, g). Then,
for every w ? W , the rule which created w and
the corresponding local position in that rule can
be retrieved from d. Let us denote this rule by
r(d, ?, w) and the local position by l(d, ?, w).
Now let d be the derivation ?0u1?1 . . . un?n.
Then the probability of d is defined by
P (d) =
?
1?i?n
Pd(?i?1
ui? ?i)
where
1. (substitution) Pd(?i?1 w,r=? ?i) = P (r)
2. (activation)
Pd(?i?1 w=? ?i) = P r
?
adj(w?) where r? =
r(d, ?i?1, w) and w? = l(d, ?i?1, w)
3. (non-adjoining)
Pd(?i?1 ?w=? ?i) = 1 ? P r
?
adj(w?) where r?
and w? are defined as in the activation case
4. (adjoining)
Pd(?i?1
w,r=? ?i) = P (r).
In order to describe the generative model of
G, we impose a deterministic strategy sel on the
derivation relation in order to obtain, for every
sentential form, a probability distribution among
the follow-up sentential forms. A deterministic
derivation strategy is a mapping sel : SF(G) ?
(N? ? N?) ? {?} such that for every ? =
(?s, ?t, V,W, g) ? SF(G), we have that sel(?) ?
V ?W if V ?W 6= ?, and sel(?) = ? otherwise.
In other words, sel chooses the next site to operate
on. Then we define ?sel in the same way as ? but
in each of the cases we require that w = sel(?1).
Moreover, for every derivation d ? D, we denote
by next(d) the set of all derivations of the form
du? where last(d) u?sel ?.
The generative model of G comprises all the
generative stories of G. A generative story is a
tree t ? UD; the root of t is labeled by ?in. Let
w ? pos(t) and t(w) = d. Then either w is a
leaf, because we have stopped the generative story
4We note that a different definition occurs in (Nesson et
al., 2005, 2006).
at w, or w has |next(d)| children, each one repre-
sents exactly one possible decision about how to
extend d by a single derivation step (where their
order does not matter). Then, for every generative
story t, we have that
?
w?lv(t)
P (t(w)) = 1 .
We note that (D,next, ?) can be considered as
a discrete Markov chain (cf., e.g. (Baier et al,
2009)) where the initial probability distribution
? : D ? [0, 1] maps d = ?in to 1, and all the
other derivations to 0.
A probabilistic synchronous tree insertion
grammar (pSTIG) G is a pSTAG except that
for every rule r = (?s, ?t, V,W, P radj) or r =
(?s, ?t, V,W, ?, P radj) we have that
? if r ? A, then |lv(?s)| ? 2 and |lv(?t)| ? 2,
? for ? = (?s, ?t) we have that ?s is either the
rightmost leaf of ?s or its leftmost one; then
we call r, resp., L-auxiliary in the source and
R-auxiliary in the source; similarly, we re-
strict ?t; the source-spine of r (target-spine
of r) is the set of prefixes of ?s (resp., of ?t)
? W ? nlvN (?s)?{L,R}?nlvN (?t)?{L,R}
where the new components are the direction-
type of the potential adjoining site, and
? for every (ws, ?s, wt, ?t) ? W , if ws lies on
the source-spine of r and r is L-auxiliary (R-
auxiliary) in the source, then ?s = L (resp.,
?s = R), and corresponding restrictions hold
for the target component.
According to the four possibilities for the foot-
node ? we call r LL-, LR-, RL-, or RR-auxiliary.
The restriction for the probability distribution P of
G is modified such that for every (A,B) ? N?N
and x, y ? {L,R}:
?
r?A, rc(r)=(A,B)
r is xy?auxiliary
P (r) = 1 .
In the derivation relation of the pSTIG G we
will have to make sure that the direction-type of
the chosen adjoining site w matches with the type
of auxiliarity of the auxiliary rule. Again we as-
sume that the data structure SF(G) is enriched
such that for every potential adjoining site w of
? ? SF(G) we know its direction-type dir(w).
We define the derivation relation of the pSTIG
G to be the binary relation ?I ? SF(G)?SF(G)
such that we have ?1 ?I ?2 iff (i) ?1 ? ?2 and
13
(ii) if adjoining takes place atw, then the used aux-
iliary rule must be dir(w)-auxiliary. Since ?I is
a subset of ?, the concepts of derivation, success-
ful derivation, and tree transformation are defined
also for a pSTIG.
In fact, our running example pSTAG in Fig. 1 is
a pSTIG, where r2 and r3 are RL-auxiliary and
every potential adjoining site has direction-type
RL; the derivation shown in Fig. 2 is a pSTIG-
derivation.
4 Bottom-up tree adjoining transducer
Here we introduce the concept of a bottom-up tree
adjoining transducer (BUTAT) which will be used
to formalize a decoder for a pSTIG.
A BUTAT is a finite-state machine which trans-
lates strings into trees. The left-hand side of each
rule is a string over terminal symbols and state-
variable combinations. A variable is either a sub-
stitution variable or an adjoining variable; a substi-
tution variable (resp., adjoining variable) can have
an output tree (resp., output tree with foot node) as
value. Intuitively, each variable value is a transla-
tion of the string that has been reduced to the cor-
responding state. The right-hand side of a rule has
the form q(?) where q is a state and ? is an output
tree (with or without foot-node); ? may contain the
variables from the left-hand side of the rule. Each
rule has a probability p ? [0, 1].
In fact, BUTAT can be viewed as the string-
to-tree version of bottom-up tree transducers (En-
gelfriet, 1975; Gecseg and Steinby, 1984,1997) in
which, in addition to substitution, adjoining is al-
lowed.
Formally, we let X = {x1, x2, . . .} and F =
{f1, f2, . . .} be the sets of substitution variables
and adjoining variables, resp. Each substitu-
tion variable (resp., adjoining variable) has rank
0 (resp., 1). Thus when used in a tree, substitu-
tion variables are leaves, while adjoining variables
have a single child.
A bottom-up tree adjoining transducer (BU-
TAT) is a tuple M = (Q,?,?, Qf , R) where
? Q is a finite set (of states),
? ? is an alphabet (of input symbols), assuming
that Q ? ? = ?,
? ? is an alphabet (of output symbols),
? Qf ? Q (set of final states), and
? R is a finite set of rules of the form
?0 q1(z1) ?1 . . . qk(zk) ?k
p? q(?) (?)
where p ? [0, 1] (probability of (?)), k ? 0,
?0, ?1, . . . , ?k ? ??, q, q1, . . . , qk ? Q,
z1, . . . , zk ? X ? F , and ? ? RHS(k)
where RHS(k) is the set of all trees over
? ? {z1, . . . , zk} ? {?} in which the nullary
? occurs at most once.
The set of intermediate results of M is the set
IR(M) = {? | ? ? U?({?}), |pos{?}(?)| ? 1}
and the set of sentential forms of M is the set
SF(M) = (? ? {q(?) | q ? Q, ? ? IR(M)})?.
The derivation relation induced by M is the bi-
nary relation ? ? SF(M) ? SF(M) such that
for every ?1, ?2 ? SF(M) we define ?1 ? ?2 iff
there are ?, ?? ? SF(M), there is a rule of the form
(?) in R, and there are ?1, . . . , ?k ? IR(M) such
that:
? for every 1 ? i ? k: if zi ? X , then ?i does
not contain ?; if zi ? F , then ?i contains ?
exactly once,
? ?1 = ? ?0 q1(?1) ?1 . . . qk(?k) ?k ??, and
? ?2 = ? q(?(?)) ??
where ? is a function that replaces variables
in a right-hand side with their values (subtrees)
from the left-hand side of the rule. Formally,
? : RHS(k) ? IR(M) is defined as follows:
(i) for every ? = ?(?1, . . . , ?n) ? RHS(k), ? ?
?, we have ?(?) = ?(?(?1), . . . , ?(?n)),
(ii) (substitution) for every zi ? X , we have
?(zi) = ?i,
(iii) (adjoining) for every zi ? F and ? ?
RHS(k), we have ?(zi(?)) = ?i[?(?)]v
where v is the uniquely determined position
of ? in ?i, and
(iv) ?(?) = ?.
Clearly, the probablity of a rule carries over to
derivation steps that employ this rule. Since, as
usual, a derivation d is a sequence of derivation
steps, we let the probability of d be the product of
the probabilities of its steps.
The string-to-tree transformation computed by
M is the set ?M of all tuples (?, ?) ? ???U? such
that there is a derivation of the form ? ?? q(?) for
some q ? Qf .
5 Decoder for pSTIG
Now we construct the decoder dec(G) for a pSTIG
G that transforms source strings directly into tar-
get trees and simultaneously computes the proba-
bility of the corresponding derivation of G. This
decoder is formalized as a BUTAT.
Since dec(G) is a string-to-tree transducer, we
14
have to transform the source tree ?s of a rule r
into a left-hand side ? of a dec(G)-rule. This is
done similarly to (DeNeefe and Knight, 2009) by
traversing ?s via recursive descent using a map-
ping ? (see an example after Theorem 1); this
creates appropriate state-variable combinations for
all substitution sites and potential adjoining sites
of r. In particular, the source component of the
direction-type of a potential adjoining site deter-
mines the position of the corresponding combina-
tion in ?. If there are several potential adjoining
sites with the same source component, then we
create a ? for every permutation of these sites. The
right-hand side of a dec(G)-rule is obtained by
traversing the target tree ?t via recursive descent
using a mapping ?? and, whenever a nonterminal
with a potential adjoining site w is met, a new po-
sition labeled by fw is inserted.5 If there is more
than one potential adjoining site, then the set of
all those sites is ordered as in the left-hand side ?
from top to bottom.
Apart from these main rules we will employ
rules which implement the decision of whether or
not to turn a potential adjoining site w into an ac-
tivated adjoining site. Rules for the first purpose
just pass the already computed output tree through
from left to right, whereas rules for the second pur-
pose create for an empty left-hand side the output
tree ?.
We will use the state behavior of dec(G) in or-
der to check that (i) the nonterminals of a substi-
tution or potential adjoining site match the root-
category of the used rule, (ii) the direction-type
of an adjoining site matches the auxiliarity of the
chosen auxiliary rule, and (iii) the decisions of
whether or not to adjoin for each rule r of G are
kept separate.
Whereas each pair (?s, ?t) in the translation of
G is computed in a top-down way, starting at the
initial sentential form and substituting and adjoin-
ing to the present sentential form, dec(G) builds
?t in a bottom-up way. This change of direction is
legitimate, because adjoining is associative (Vijay-
Shanker and Weir, 1994), i.e., it leads to the same
result whether we first adjoin r2 to r1, and then
align r3 to the resulting tree, or first adjoin r3 to
r2, and then adjoin the resulting tree to r1.
In Fig. 3 we show some rules of the decoder
of our running example pSTIG and in Fig. 4 the
5We will allow variables to have structured indices that
are not elements of N. However, by applying a bijective re-
naming, we can always obtain rules of the form (?).
derivation of this decoder which correponds to the
derivation in Fig. 2.
Theorem 1. Let G be a pSTIG over N and T .
Then there is a BUTAT dec(G) such that for ev-
ery (?s, ?t) ? UN (T ) ? UN (T ) and p ? [0, 1] the
following two statements are equivalent:
1. there is a successful derivation of
(?s, ?t, ?, ?, ?) by G with probability p,
2. there is a derivation from yield(?s) to
[Ss, St](?t) by dec(G) with probability p.
PROOF. Let G = (N,T, [Ss, St],S,A, P ) be a
pSTIG. We will construct the BUTAT dec(G) =
(Q,T,N ?T, {[Ss, St]}, R) as follows (where the
mappings ? and ?? will be defined below):
? Q = [N ?N ] ? [N ?{L,R}?N ?{L,R}]
?{[r, w] | r ? A, w is an adjoining site of r},
? R is the smallest set R? of rules such
that for every r ? S ? A of the form
(?s, ?t, V,W, P radj) or (?s, ?t, V,W, ?, P radj):
? for every ? ? ?(?), if r ? S, then the
main rule
? P (r)? [?s(?), ?t(?)]
(
??(?)
)
is in R?, and if r ? A and r is ?s?t-
auxiliary, then the main rule
? P (r)? [?s(?), ?s, ?t(?), ?t]
(
??(?)
)
is in R?, and
? for every w = (ws, ?s, wt, ?t) ? W the
rules
qw
(
fw
) P radj(w)?? [r, w]
(
fw(?)
)
with qw = [?(ws), ?s, ?t(wt), ?t] for ac-
tivation at w, and the rule
?
1?P radj(w)?? [r, w](?)
for non-adjoining at w are in R?.
We define the mapping
? : pos(?s) ? P((T ?Q(X ? F ))?)
with Q(X ? F ) = {q(z) | q ? Q, z ? X ? F}
inductively on its argument as follows. Let w ?
pos(?s) and let w have n children.
(a) Let ?s(w) ? T . Then ?(w) = {?s(w)}.
15
(b) (substitution site) Let ?s(w) ? N and let
w? ? pos(?t) such that (w,w?) ? V . Then
?(w) = {[?s(w), ?t(w?)]
(
x(w,w?)
)
}.
(c) (adjoining site) Let ?s(w) ? N and let there
be an adjoining site in W with w as first
component. Then, we define ?(w) to be the
smallest set such that for every permutation
(u1, . . . , ul) (resp., (v1, . . . , vm)) of all the L-
adjoining (resp., R-adjoining) sites inW with
w as first component, the set6
J ? ?(w.1) ? . . . ? ?(w.n) ?K
is a subset of ?(w), where J = {u?1 . . . u?l}
and K = {v?m . . . v?1} and
u?i = [r, ui]
(
fui
)
and v?j = [r, vj ]
(
fvj
)
for 1 ? i ? l and 1 ? j ? m.
(d) Let ?s(w) ? N , w 6= ?, and let w be neither
the first component of a substitution site in V
nor the first component of an adjoining site in
W . Then
?(w) = ?(w.1) ? . . . ? ?(w.n) .
(e) Let w = ?. Then we define ?(w) = {?}.
For every ? ? ?(?), we define the mapping
?? : pos(?t) ? UN?F?X(T ? {?})
inductively on its argument as follows. Let
w ? pos(?t) and let w have n children.
(a) Let ?t(w) ? T . Then ??(w) = ?t(w).
(b) (substitution site) Let ?t(w) ? N and let
w? ? pos(?s) such that (w?, w) ? V . Then
??(w) = x(w?,w).
(c) (adjoining site) Let ?t(w) ? N and let there
be an adjoining site in W with w as third
component. Then let {u1, . . . , ul} ? W be
the set of all potential adjoining sites with w
as third component, and we define
??(w) = fu1(. . . ful(?) . . .)
where ? = ?t(w)(??(w.1), . . . , ??(w.n))
and the ui?s occur in ??(w) (from the root
towards the leaves) in exactly the same order
as they occur in ? (from left to right).
(d) Let ?t(w) ? N , w 6= ?, and let w be neither
the second component of a substitution site
in V nor the third component of an adjoining
site in W . Then
??(w) = ?t(w)(??(w.1), . . . , ??(w.n)).
6using the usual concatenation ? of formal languages
(e) Let w = ?. Then ??(w) = ?.
With dec(G) constructed as shown, for each
derivation of G there is a corresponding deriva-
tion of dec(G), with the same probability, and vice
versa. The derivations proceed in opposite direc-
tions. Each sentential form in one has an equiv-
alent sentential form in the other, and each step
of the derivations correspond. There is no space
to present the full proof, but let us give a slightly
more precise idea about the formal relationship be-
tween the derivations of G and dec(G).
In the usual way we can associate a deriva-
tion tree dt with every successful derivation d of
G. Assume that last(d) = (?s, ?t, ?, ?, ?), and
let Es and Et be the embedded tree transducers
(Shieber, 2006) associated with, respectively, the
source component and the target component of
G. Then it was shown in (Shieber, 2006) that
?Es(dt) = ?s and ?Et(dt) = ?t where ?E de-
notes the tree-to-tree transduction computed by an
embedded tree transducer E. Roughly speaking,
Es and Et reproduce the derivations of, respec-
tively, the source component and the target com-
ponent of G that are prescribed by dt. Thus, for
? = (??s, ??t, V,W, g), if ?in ??G ? and ? is a prefix
of d, then there is exactly one subtree dt[(w,w?)]
of dt associated with every (w,w?) ? V ? W ,
which prescribes how to continue at (w,w?) with
the reproduction of d. Having this in mind, we ob-
tain the sentential form of the dec(G)-derivation
which corresponds to ? by applying a modifica-
tion of ? to ? where the modification amounts to
replacing x(w,w?) and f(w,w?) by ?Et(dt[(w,w?)]);
note that ?Et(dt[(w,w?)]) might contain ?. 
As illustration of the construction in Theorem 1
let us apply the mappings ? and ?? to rule r2 of
Fig. 1, i.e., to r2 = (?s, ?t, ?, {b, c}, ?, P r2adj)
with ?s = A(A, ?), ?t = B(B(?), B),
b = (?,R, ?,L), c = (?,R, 1,L), and ? = (1, 2).
Let us calculate ?(?) on ?s. Due to (c),
?(?) = J ? ?(1) ? ?(2) ?K.
Since there are no L-adjoinings at ?, we have that
J = {?}. Since there are the R-adjoinings b and c
at ?, we have the two permutations (b, c) and (c, b).
(v1, v2) = (b, c): K = {[r2, c](fc)[r2, b](fb)}
(v1, v2) = (c, b): K = {[r2, b](fb)[r2, c](fc)}
Due to (e) and (a), we have that ?(1) = {?} and
?(2) = {?}, resp. Thus, ?(?) is the set:
{? [r2, c](fc) [r2, b](fb), ? [r2, b](fb) [r2, c](fc)}.
16
r1
(r1, a)
r2
(r2,?b) (r2,?c)
r4
?
[A,B]
x(2,2)
?
[r1, a]
fa
1??
[Ss, St]
St
f
B
?
x ?a (2,2)
[A,R, B,L]
fa
.9??
[r1, a]
f
?
a
?
[r2, b]
f b
[r2, c]
fc
.4??
[A,R, B,L]
f
B
f
B
?
?
b
c
? .8??
[r2, b]
?
? .4??
[r2, c]
?
? .1??
[A,B]
B
?
Figure 3: Some rules of the running example de-
coder.
Now let ? = ? [r2, b](fb) [r2, c](fc). Let us cal-
culate ??(?) on ?t.
??(?)
(c)= fb(B(??(1), ??(2)))
(c)= fb(B(fc(B(??(1.1))), ??(2)))
(a)= fb(B(fc(B(?)), ??(2)))
(e)= fb(B(fc(B(?)), ?))
Hence we obtain the rule
? [r2, b](fb) [r2, c](fc) ?
[A,R, B,L](fb(B(fc(B(?)), ?)))
which is also shown in Fig. 3.
? ? ? ?
? ? ? ?
[r2, b]
?
? ? ? ?
[r2, b]
?
[r2, c]
?
? ? ?
? ? ?
?
[A,B]
B
?
?
[A,R, B,L]
B
B
?
?
=?
=?
=?
=?
=?
[Ss, St]
St
B
B
?
B
?
B
?
?
prob. .8
prob. .4
prob. .4
prob. .9
prob. .1
=? prob. .1
[r1, a]
B
B
?
?
(r2,?b)
(r2,?c)
(r2, bc)
(r1, a)
r4
r1
[r1, a]
B
B
?
?
Figure 4: Derivation of the decoder corresponding
to the derivation in Fig. 2.
17
References
A. Abeille, Y. Schabes, A.K. Joshi. Using lexicalized
TAGs for machine translation. In Proceedings of
the 13th International Conference on Computational
Linguistics, volume 3, pp. 1?6, Helsinki, Finland,
1990.
C. Baier, M. Gro??er, F. Ciesinski. Model checking
linear-time properties of probabilistic systems. In
Handbook of Weighted Automata, Chapter 13, pp.
519?570, Springer, 2009.
S. DeNeefe. Tree adjoining machine translation. Ph.D.
thesis proposal, Univ. of Southern California, 2009.
S. DeNeefe, K. Knight. Synchronous tree adjoining
machine translation. In Proc. of Conf. Empirical
Methods in NLP, pp. 727?736, 2009.
J. Engelfriet. Bottom-up and top-down tree transfor-
mations ? a comparison. Math. Systems Theory,
9(3):198?231, 1975.
J. Engelfriet. Tree transducers and syntax-directed se-
mantics. In CAAP 1982: Lille, France, 1982.
A. Fujiyoshi, T. Kasai. Spinal-formed context-free tree
grammars. Theory of Computing Systems, 33:59?
83, 2000.
Z. Fu?lo?p, H. Vogler. Weighted tree automata and tree
transducers. In Handbook of Weighted Automata,
Chapter 9, pp. 313?403, Spinger, 2009.
F. Ge?cseg, M. Steinby. Tree Automata. Akade?miai
Kiado?, Budapest, 1984.
F. Ge?cseg, M. Steinby. Tree languages. In Handbook
of Formal Languages, volume 3, chapter 1, pages
1?68. Springer-Verlag, 1997.
J. Graehl, K. Knight, J. May. Training tree transducers.
Computational Linguistics, 34(3):391?427, 2008
A.K. Joshi, L.S. Levy, M. Takahashi. Tree adjunct
grammars. Journal of Computer and System Sci-
ences, 10(1):136?163, 1975.
A.K. Joshi, Y. Schabes. Tree-adjoining grammars. In
Handbook of Formal Languages. Chapter 2, pp. 69?
123, Springer-Verlag, 1997.
K. Knight, J. Graehl. An overview of probabilis-
tic tree transducers for natural language processing.
In Computational Linguistics and Intelligent Text
Processing, CICLing 2005, LNCS 3406, pp. 1?24,
Springer, 2005.
K. Knight, J. May. Applications of Weighted Au-
tomata in Natural Language Processing. In Hand-
book of Weighted Automata, Chapter 14, pp. 571?
596, Springer, 2009.
P.M. Lewis, R.E. Stearns. Syntax-directed transduc-
tions. Journal of the ACM, 15:465?488, 1968.
A. Maletti. Compositions of tree series transforma-
tions. Theoret. Comput. Sci., 366:248?271, 2006.
A. Maletti. The Power of Tree Series Transducers.
Ph.D. thesis, TU Dresden, Germany, 2006.
A. Maletti. Compositions of extended top-down
tree transducers. Information and Computation,
206:1187?1196, 2008.
A. Maletti. Why synchronous tree substitution gram-
mars? in Proc. 11th Conf. North American Chap-
ter of the Association of Computational Linguistics.
2010.
D.F. Martin and S.A. Vere. On syntax-directed trans-
ductions and tree transducers. In Ann. ACM Sympo-
sium on Theory of Computing, pp. 129?135, 1970.
R. Nesson, S.M. Shieber, and A. Rush. Induction
of probabilistic synchronous tree-insertion gram-
mars. Technical Report TR-20-05, Computer Sci-
ence Group, Harvard Univeristy, Cambridge, Mas-
sachusetts, 2005.
R. Nesson, S.M. Shieber, and A. Rush. Induction of
probabilistic synchronous tree-inserting grammars
for machine translation. In Proceedings of the 7th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), 2006.
Y. Schabes, R.C. Waters. Tree insertion grammars:
a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees
produced. Computational Linguistics, 21:479?513,
1994.
P.P. Schreiber. Tree-transducers and syntax-connected
transductions. In Automata Theory and Formal
Languages, Lecture Notes in Computer Science 33,
pp. 202?208, Springer, 1975.
S.M. Shieber. Synchronous grammars and tree trans-
ducers. In Proc. 7th Workshop on Tree Adjoin-
ing Grammars and Related Formalisms, pp. 88?95,
2004.
S.M. Shieber. Unifying synchronous tree-adjoining
grammars and tree transducers via bimorphisms. In
Proc. 11th Conf. European Chapter of ACL, EACL
06, pp. 377?384, 2006.
S.M. Shieber, Y. Schabes. Synchronous tree-adjoining
grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguistics,
volume 3, pp. 253?258, Helsinki, Finland, 1990.
K. Vijay-Shanker, D.J. Weir. The equivalence of four
extensions of context-free grammars. Mathematical
Systems Theory, 27:511?546, 1994.
K. Yamada and K. Knight. A syntax-based statistical
translation model. In Proc. of 39th Annual Meeting
of the Assoc. Computational Linguistics, pp. 523?
530, 2001.
18
Putting a Value on Comparable Data 
 
Kevin Knight 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA, 90292 USA 
knight@isi.edu 
 
Invited Talk 
 
 
Abstract 
 
Machine translation began in 1947 with an 
influential memo by Warren Weaver.  In that 
memo, Weaver noted that human code-breakers 
could transform ciphers into natural language 
(e.g., into Turkish)  
? without access to parallel 
ciphertext/plaintext data, and  
? without knowing the plaintext 
language?s syntax and semantics.   
Simple word- and letter-statistics seemed to be 
enough for the task.  Weaver then predicted that 
such statistical methods could also solve a 
tougher problem, namely language translation.   
This raises the question: can sufficient 
translation knowledge be derived from 
comparable (non-parallel) data? 
 In this talk, I will discuss initial work in 
treating foreign language as a code for English, 
where we assume the code to involve both word 
substitutions and word transpositions.  In doing 
so, I will quantitatively estimate the value of 
non-parallel data, versus parallel data, in terms 
of end-to-end accuracy of trained translation 
systems.  Because we still know very little about 
solving word-based codes, I will also describe 
successful techniques and lessons from the 
realm of letter-based ciphers, where the non-
parallel resources are (1) enciphered text, and (2) 
unrelated plaintext.  As an example, I will 
describe how we decoded the Copiale cipher 
with limited ?computer-like? knowledge of the 
plaintext language.   
 The talk will wrap up with challenges in 
exploiting comparable data at all levels: letters, 
words, phrases, syntax, and semantics. 
 
1
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, page 1,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
The Copiale Cipher* 
 
Kevin Knight Be?ta Megyesi and Christiane Schaefer 
 
USC Information Sciences Institute Department of Linguistics and Philology 
4676 Admiralty Way Uppsala University 
Marina del Rey, CA, 90292, USA 
knight@isi.edu 
 
SDL Language Weaver, Inc. 
6060 Center Drive, Suite 150 
Los Angeles, CA 90045 
kknight@sdl.com 
Box 635, S-751 26 Uppsala, Sweden 
beata.megyesi@lingfil.uu.se 
christiane.schaefer@lingfil.uu.se 
 
 
Abstract 
 
The Copiale cipher is a 105-page enciphered 
book dated 1866.  We describe the features of 
the book and the method by which we 
deciphered it. 
 
1. Features of the Text 
 
Figure 1 shows a portion of an enciphered book 
from the East Berlin Academy.  The book has 
the following features: 
 
? It is 105 pages long, containing about 
75,000 handwritten characters. 
? The handwriting is extremely neat. 
? Some characters are Roman letters (such 
as a and b), while others are abstract 
symbols (such as 1 and <).  Roman 
letters appear in both uppercase and 
lowercase forms. 
? Lines of text are both left- and right-
justified. 
? There are only a few author corrections. 
? There is no word spacing.   
 
There are no illustrations or chapter breaks, but 
the text has formatting: 
 
? Paragraphs are indented. 
? Some lines are centered. 
 
? 
*This material was presented as part of an invited 
talk at the 4th Workshop on Building and Using 
Comparable Corpora (BUCC 2011). 
 
 
? Some sections of text contain a double-
quote mark (?) before each line. 
? Some lines end with full stop (.) or colon 
(:).  The colon (:) is also a frequent 
word-internal cipher letter. 
? Paragraphs and section titles always 
begin with Roman letters (in capitalized 
form). 
 
The only non-enciphered inscriptions in the 
book are ?Philipp 1866? and ?Copiales 3?, the 
latter of which we used to name the cipher. 
The book also contains preview 
fragments (?catchwords?) at the bottom of left-
hand pages.  Each catchword is a copy of the 
first few letters from the following (right-hand) 
page.  For example, in Figure 1, the short 
sequence 3A^ floats at the bottom of the left page, 
and the next page begins 3A^om!...  In early 
printing, catchwords were used to help printers 
validate the folding and stacking of pages. 
 
2. Transcription 
 
To get a machine-readable version of the text, 
we devised the transcription scheme in Figure 2.  
According to this scheme, the line  
 
        >Ojv-</E3CA=/^Ub2Gr@J 
 
is typed as:  
 
pi oh j v hd tri arr eh three c. ah  
ni arr lam uh b lip uu r o.. zs 
2
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 2?9,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
  
Figure 1.  Two pages from the Copiale cipher.
The transcription uses easy-to-reach keyboard 
characters, so a transcriber can work without 
taking his/her eyes off the original document.   
There are approximately 90 cipher 
letters, including 26 unaccented Roman letters, 
a-z.  The letters c, h, m, n, p, r, s, and x have 
dotted forms (e.g., C), while the letter i also has 
an un-dotted form.  The letters m, n, r, and u 
have underlined variants (e.g., B), and the 
vowels have circumflexed variants (e.g., A).  The 
plain letter y does not appear unaccented until 
page 30, but it appears quite frequently with an 
umlaut (y).The four Roman letters d, g, n, and z 
appear in both plain (d, g, n, z) and fancy forms 
(L, K, Y, J).  Capitalized Roman letters are used 
to start paragraphs.  We transcribe these with A-
Z, though we down-case them before counting 
frequencies (Section 3).  Down-casing D, G, N, 
and Z is not trivial, due to the presence of both 
plain and fancy lowercase forms.   
The non-Roman characters are an 
eclectic mix of symbols, including some Greek 
letters.  Eight symbols are rendered larger than 
others in the text: 9, @, #, %, 2, *, ?, and ?. 
We transcribed a total of 16 pages 
(10,840 letters).  We carried out our analysis on 
those pages, after stripping catchwords and 
down-casing all Roman letters. 
 
3.  Letter Frequencies and Contexts 
 
Figure 3 shows cipher letter frequencies.  The 
distribution is far from flat, the most frequent 
letter being ^ (occurring 412 times).  Here are 
the most common cipher digraphs (letter pairs) 
and trigraphs, with frequencies: 
? - 99  ? - ^ 47 
C : 66  C : G 23 
- ^ 49  Y ? - 22 
: G 48  y ? - 18 
z ) 44  H C | 17 
3
a a A ah   6 del 
b b     < tri 
c c   C c. 5 gam 
d d     ! iot 
e e E eh   ^ lam 
f f     > pi 
g g     / arr 
h h H h. - hd ? bas 
i i I ih   4 car 
j j     + plus 
k k     T cross 
l l     0 fem 
m m M m. B mu 1 mal 
n n N n. D nu \ ft 
o o O oh & o. W no 
p p P p.   Q sqp 
q q     Z zzz 
r r R r. F ru _ pipe 
s s S s.   ` longs 
t t     ) grr 
u u U uh G uu ] grl 
v v     [ grc 
w w   # tri.. 7 hk 
x x X x. 2 lip ~ sqi 
( y y y.. 9 nee : : 
z z   @ o.. . . 
L ds = ni * star , ? 
K gs ? ki % bigx | bar 
J zs $ smil ? gat 3 three 
Y ns ? smir ? toe 8 inf 
 
Figure 2.  Transcription scheme.  Columns 
alternate between the cipher letters and their 
transcriptions. 
 
The full digraph counts reveal 
interesting patterns among groups of letters.  For 
example, letters with circumflexes (A, E, I, O, U) 
have behaviors in common: all five tend to be 
preceded by z and >, and all five tend to be 
followed by 3 and j.  To get a better handle on 
letter similarities, we automatically clustered the 
cipher letters based on their contexts.  The result 
is shown in Figure 4.  We did the clustering as 
follows.  For each distinct letter x, we created a  
 
 
Figure 3.  Cipher letter frequencies. 
0
5
0
1
0
0
1
5
0
2
0
0
2
5
0
3
0
0
3
5
0
4
0
0
4
5
0
^
|z
G
-
C
Z
j
!
3
Y
)
U
y
+
O
F
H
=
:
I
>
b
g
R
M
E
X
c
?
6
K
N
n
<
/
Q
~
A
D
p
B
P
"
S
l
L
k
m
1
&
e
5
f
v
h
r
J
7
i
T
s
o
]
a
t
d
u
8
9
[
0
w
_
W
4
q
@
x
2
#
,
`
\
*
%
4
 
 
 
Figure 4.  Automatic clustering of cipher letters 
based on similarity of contexts. 
 
 
co-occurrence vector of length 90, to capture the 
distribution of letters than precede x.  For 
example, if x is preceded 12 times by >, 0 times 
by U, 4 times by y, 1 time by 6, etc, then its 
vector looks like this:  [12, 0, 4, 1, ?].  For the 
same letter x, we created another vector that 
captures the distribution of letters than follow x, 
e.g., [0, 0, 7, 2, ?].  Then we concatenated the 
two vectors to create v(x) = [12, 0, 4, 1, ?, 0, 0, 
7, 2, ?].  We deemed two letters a and b to be 
similar if the cosine distance between v(a) and 
v(b) is small, indicating that they appear in 
similar contexts.  We used the Scipy software 
(http://users.soe.ucsc.edu/~eads/cluster.html) to 
perform and plot a clustering that incrementally 
merges similar letters (and groups of letters) in a 
bottom-up fashion. 
The cluster diagram confirms that 
circumflexed letters (A, E, I, O, U) behave 
similarly.  It also shows that the unaccented 
Roman letters form a natural grouping, as do 
underlined letters.  Merges that happen low in 
the cluster map indicate very high similarity, e.g., 
the group (y, !, Y). 
 
4. First Decipherment Approach 
 
Building on the self-similarity of Roman letters, 
our first theory was that the Roman letters carry 
all the information in the cipher, and that all 
other symbols are NULLs (meaningless tokens 
added after encipherment to confuse 
cryptanalysis).  If we remove all other symbols, 
the remaining Roman letters indeed follow a 
typical natural language distribution, with the 
most popular letter occurring 12% of the time, 
and the least popular letters occurring rarely. 
The revealed sequence of Roman letters 
is itself nonsensical, so we posited a simple 
substitution cipher.  We carried out automatic 
computer attacks against the revealed Roman-
letter sequence, first assuming German source, 
then English, then Latin, then forty other 
candidate European and non-European 
languages.  The attack method is given in 
[Knight et al 2006].  That method automatically 
combines plaintext-language identification with 
decipherment.  Unfortunately, this failed, as no 
5
language identified itself as a more likely 
plaintext candidate than the others. 
We then gave up our theory regarding 
NULLs and posited a homophonic cipher, with 
each plaintext letter being encipherable by any 
of several distinct cipher letters.  While a well-
executed homophonic cipher will employ a flat 
letter frequency distribution, to confound 
analysis, we guessed that the Copiale cipher is 
not optimized in this regard.   
We confirmed that our computer attack 
does in fact work on a synthetic homophonic 
cipher, i.e., it correctly identifies the plaintext 
language, and yields a reasonable, if imperfect, 
decipherment.  We then loosed the same attack 
on the Copiale cipher.  Unfortunately, all 
resulting decipherments were nonsense, though 
there was a very slight numerical preference for 
German as a candidate plaintext language. 
 
5. Second Decipherment Approach 
 
We next decided to focus on German as the most 
likely plaintext language, for three reasons: 
 
? the book is located in Germany 
? the computer homophonic attack gave a very 
slight preference to German 
? the book ends with the inscription ?Philipp 
1866?, using the German double-p spelling. 
 
Pursuing the homophonic theory, our thought 
was that all five circumflexed letters (A, E, I, O, 
U), behaving similarly, might represent the same 
German letter.  But which German letter?  Since 
the circumflexed letters are preceded by z and >, 
the circumflexed letters would correspond to the 
German letter that often follows whatever z and > 
stand for.  But what do they, in turn, stand for? 
From German text, we built a digraph 
frequency table, whose the most striking 
characteristic is that C is almost always followed 
by H.  The German CH pair is similar to the 
English QU pair, but C is fairly frequent in 
German.  A similar digraph table for the cipher 
letters shows that ? is almost always followed by 
-.  So we posited our first two substitutions: ?=C 
and -=H.  We then looked for what typically 
precedes and follows CH in German, and what 
typically precedes and follows ?- in the cipher.  
For example, ?-^ is the most frequent cipher 
trigraph, while CHT is a common German 
trigraph.  We thus hypothesized the further 
substitution ^=T, and this led to a cascade of 
others.  We retracted any hypothesis that 
resulted in poor German digraphs and trigraphs, 
and in this way, we could make steady progress 
(Figure 5).   
The cluster map in Figure 4 was of great 
help.  For example, once we established a 
substitution like y=I, we could immediately add 
Y=I and !=I, because the three cipher letters 
behave so similarly.  In this way, we mapped all 
circumflexed letters (A, E, I, O, U) to plaintext E.  
These leaps were frequently correct, and we 
soon had substitutions for over 50 cipher letters.  
Despite progress, some very frequent 
German trigraphs like SCH were still drastically 
under-represented in our decipherment.  Also, 
many cipher letters (including all unaccented 
Roman letters) still lacked substitution values.  
A fragment of the decipherment thus far looked 
like this (where ??? stands for an as-yet-
unmapped cipher letter): 
 
?GEHEIMER?UNTERLIST?VOR?DIE?GESELLE 
?ERDER?TITUL 
?CEREMONIE?DER?AUFNAHME 
 
On the last line, we recognized the two words 
CEREMONIE and DER separated by a cipher 
letter.  It became clear that the unaccented 
Roman letters serve as spaces in the cipher.  
Note that this is the opposite of our first 
decipherment approach (Section 4).  The non-
Roman letters are not NULLs -- they carry 
virtually all the information.  This also explains 
why paragraphs start with capitalized Roman 
letters, which look nice, but are meaningless. 
We next put our hypothesized 
decipherment into an automatic German-to-
English translator (www.freetranslation.com), 
where we observed that many plaintext words 
were still untranslatable.  For example, 
ABSCHNITL was not recognized as a 
translatable German word.  The final cipher 
letter for this word is colon (:), which we had 
mapped previously to L.  By replacing the final 
L in ABSCHNITL with various letters of the 
alphabet (A-Z), we hit on the recognized word
6
 
Figure 5.  Progress of decipherment.  The main grid shows plaintext (German) letters across the top and 
ciphertext letters down the left side.  The ciphertext letters are grouped into clusters.  To the right of the 
main grid are frequent German trigraphs (der, und, ein, ?) and frequent cipher trigraphs (?-^, C:G, HC|, 
?), with the two columns being separated by hypothesized trigraph decipherments. 
 
ABSCHNITT (translated as ?section?).  We then 
realized that the function of colon (:) is to double 
the previous consonant (whether it be T, L, F, or 
some other letter).  Old German writing uses a 
stroke with the same function. 
The cipher letter T was still unknown, 
appearing in partially deciphered words like 
TAFLNER, TNUPFTUCHS, and GESELLTAFLT.  
We tried substituting each of the letters A-Z for 
T, but this did not yield valid German.  However, 
we found GESELLSCHAFLT in a German 
dictionary, so we concluded that T stands for 
SCH.  This opened the door for other multi-
plaintext-letter substitutions. 
Finally, we brought full native-German 
expertise to bear, by taking hypothesized 
decipherments (hyp) and correcting them (corr): 
 
ey/t+Nc-ZKGQOF~PC|nMYC5]-3Cy/OnQZMEX?g6G 
hyp:  is  mache ebenfals wilhuhrlise  bewegunge  
corr:  ich mache ebenfals wilk?hrliche bewegungen  
  
 
7
a></b+!^Kz)jvHgzZ3gs-NB>v 
hyp:  dos  mit der andern hand 
corr:  doch mit der andern hand 
   
rzjY^:Ig|eGyDIjJBY+:^b^&QNc5p+!^f>GKzH=+Gc 
hyp:  dritlens einer n mlt tobach mit de  daume  
corr:  drittens einer ? ??? tobach mit dem daumen  
  
"B>lzGt+!^:OC7Gc~ygXZ3sz)RhC!F?5GL-NDzb 
hyp:  und de mitlelde finger der linche hand 
corr:  und dem  mittelsten finger der linchen hand 
  
QUj]-REs+!^K>ZjLCYD?5Gl-HF>mz)yFK 
hyp:  beruhre mit der linche  hand dein  
corr:  ber?hre mit der linchen hand dein  
 
This allowed us to virtually complete our table 
of substitutions (Figure 6).  Three cipher letters 
remained ambiguous:  
? [ could represent either SS or S 
? 5 could represent either H or K 
? G could represent either EN or EM  
However, these three symbols are ambiguous 
only with respect to deciphering into modern 
German, not into old German, which used 
different spelling conventions.   
The only remaining undeciphered 
symbols were the large ones: 9, @, #, %, 2, *, 
?, and ?.  These appear to be logograms, 
standing for the names of (doubly secret) people 
and organizations, as we see in this section: ?the 
9 asks him whether he desires to be 2?. 
 
6.  Contents 
 
The book describes the initiation of ?DER 
CANDIDAT? into a secret society, some 
functions of which are encoded with logograms.  
Appendix A contains our decipherment of the 
beginning of the manuscript. 
 
7.  Conclusion 
 
We described the Copiale cipher and its 
decipherment.  It remains to transcribe the rest 
of the manuscript and to do a careful translation.  
The document may contain further encoded 
information, and given the amount of work it 
represents, we believe there may be other 
documents using the same or similar schemes. 
Plaintext (German) Ciphertext 
A P N H 0* 
? | 0* 
B Q 
C ? 
D > z 
E A E I O U ) Z 
F ~ 
G 6 X 
H - 5* 
I y Y ! 
J 4 
K 5* 
L C 
M + 
N B F D g 
O < & 
?| W 
P d 
R R 3 j 
S | [* 
T ^ 
U = ? 
?| ] 
V 1 
W M 
X f 
Y 8 
Z S 
SCH T 
SS [* 
ST 7 
CH / 
repeat previous 
consonant 
: 
EN / EM G 
space a b c L e f \ K h  
i k l m n o p q r s  
` t u v w x ( J  
 
Figure 6.  Letter substitutions resulting from 
decipherment.  Asterisks (*) indicate ambiguous 
cipher letters that appear twice in the chart.  This 
table does not include the large characters:  
9, @, #, %, 2, *, ?, and ?. 
 
 
 
8
8.  Acknowledgments 
 
Thanks to Jonathan Graehl for plotting the 
cluster map of cipher letters.  This work was 
supported in part by NSF grant 0904684. 
 
9.  References 
 
K. Knight, A. Nair, N. Rathod, and K. Yamada, 
?Unsupervised Analysis for Decipherment 
Problems?, Proc. ACL-COLING, 2006. 
 
 
Appendix A 
 
 
Ciphertext:        
lit:mz)|bl 
vXZ|I^SkQ"/|wn 
>Ojv-</E3CA=/^Ub2Gr@J 
6)-Z!+Ojnp^-IYCf. 
cUj7E3tPQTgY^:k 
LXU-EY+ZRp"B^I3:y/^l1&jqz!IXA|EC:GL. 
fUR7Ojk^!^=CJ. 
m?)RA+<gyGxzO3mN"~DH-+I_. 
   kMUF:pz!Of|y?-Ej-Z!^n>A3b#Kz=j/lzGp0C^ARgk^- 
]R-]^ZjnQI|<R6)^a=gzw>yEc#r1<+bzYR!XyjIFzGf9J 
>"j/LP=~|U^S"B6m|ZYgA|k-=^-|lXOW~:FI^b!7uMyjzv 
zAjx?PB>!zH^c1&gKzU+p4]DXE3gh^-]j-]^I3tN"|rOy 
BA+hHFzO3DbSY+:ZRkPQ6)-<CU^K=DzkQZ8nz)3f-NF> 
JEyDK=F>K1<3nz)|k>Yj!XyRIg>G9bK^!Tb6U~]-3O^e 
zYO|URc~306^bY-BL: 
   nIR7C!/e&QhORLQZ6U-3Ak2KS=LMEjzGli 
   KSMU8^OD|o>EgGL1ZR<jzg"FXGK>U3k@n|Y/b=D 
^ERMO3~:Ga=Fzl<-F)LMYAzOj|dIF7!65Oy^vz!EKCU-
RSEY^ 
cP=|7A-GbM<C:ZL. 
   hzjY^:Og|ezYAJ*n>A3@pS"r1I3TMAy6Gli=D> 
zPS=nH"~rzP|n1ZjQ!g>Cy?-7Ob|!/kNg-ZyT!6cS=b+N/GL 
XO|!F:U^v|I8n. 
   l>O3f?Hgzy>P^lNB^M<R^)^oe4Nr. 
Plaintext, as deciphered: 
gesetz buchs 
der hocherleuchte 2 e @ 
geheimer theil. 
erster abschnitt 
geheimer unterricht vor die gesellen. 
erster titul. 
ceremonien der aufnahme. 
     wenn die sicherheit der # durch den ?ltern 
th?rheter besorget und die # vom dirigirenden 9 
durch aufsetzung seines huths ge?ffnet ist wird der 
candidat von dem j?ngern th?rh?ter aus einem andern 
zimmer abgeholet und bey der hand ein und vor des 
dirigirenden 9  tisch gef?hret dieser fr?gt ihn: 
     erstlich ob er begehre 2 zu werden  
     zweytens denen verordnungen der @ sich 
unterwerffen und ohne wiederspenstigkeit die lehrzeit 
ausstehen wolle.   
     drittens die * der @ gu verschweigen und dazu 
auf das verbindlichste sich anheischig zu machen  
gesinnet sey.  
     der candidat antwortet ja.  
 
 
 
Initial translation: 
First lawbook  
of the 2 e @ 
Secret part. 
First section 
Secret teachings for apprentices. 
First title. 
Initiation rite. 
     If the safety of the # is guaranteed, and the # is 
opened by the chief 9, by putting on his hat, the 
candidate is fetched from another room by the 
younger doorman and by the hand is led in and to the 
table of the chief 9, who asks him:  
     First, if he desires to become 2.   
     Secondly, if he submits to the rules of the @ and 
without rebelliousness suffer through the time of 
apprenticeship. 
     Thirdly, be silent about the * of the @ and 
furthermore be willing to offer himself to volunteer 
in the most committed way.  
     The candidate answers yes. 
 
9
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 78?86,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
What We Know About The Voynich Manuscript
Sravana Reddy?
Department of Computer Science
The University of Chicago
Chicago, IL 60637
sravana@cs.uchicago.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
The Voynich Manuscript is an undeciphered
document from medieval Europe. We present
current knowledge about the manuscript?s text
through a series of questions about its linguis-
tic properties.
1 Introduction
The Voynich manuscript, also referred to as the
VMS, is an illustrated medieval folio written in an
undeciphered script.
There are several reasons why the study of the
manuscript is of interest to the natural language pro-
cessing community, besides its appeal as a long-
enduring unsolved mystery. Since even the ba-
sic structure of the text is unknown, it provides a
perfect opportunity for the application of unsuper-
vised learning algorithms. Furthermore, while the
manuscript has been examined by various scholars,
it has much to benefit from attention by a commu-
nity with the right tools and knowledge of linguis-
tics, text analysis, and machine learning.
This paper presents a review of what is currently
known about the VMS, as well as some original ob-
servations. Although the manuscript raises several
questions about its origin, authorship, the illustra-
tions, etc., we focus on the text through questions
about its properties. These range from the level of
the letter (for example, are there vowels and conso-
nants?) to the page (do pages have topics?) to the
document as a whole (are the pages in order?).
? This work was completed while the author was visiting
the Information Sciences Institute.
2 Background
2.1 History
From the illustrations ? hairstyles and features of the
human figures ? as well as the shapes of the glyphs,
the manuscript is posited to have been created in Eu-
rope. Carbon-dating at the University of Arizona
has found that the vellum was created in the 15th
century, and the McCrone Research Institute has as-
serted that the ink was added shortly afterwards1.
The exact history of the VMS is not established.
According to Zandbergen (2010), the earliest owner
that it can be traced to is Jacobus de Tepenec in
Prague in the early 1600s. It is speculated that it was
given to him by Emperor Rudolf II, but it is unclear
how and from where the manuscript entered Prague.
The VMS appears to have circulated in Prague for
some time, before being sent to Athanasius Kircher
in Italy in 1665. It remained in Italy until 1912,
when it was sold to Wilfrid Voynich, who brought
it to America. It was then sold to the bookdealer
Kraus, who later donated it to the Yale University
library2, where it is currently housed.
2.2 Overview
The manuscript is divided into quires ? sections
made out of folded parchment, each of which con-
sists of folios, with writing on both sides of each fo-
lio (Reeds, 2002). Including blank pages and pages
with no text, there are 240 pages, although it is be-
lieved that some are missing (Pelling, 2006). 225
1These results are as yet unpublished. A paper about the
carbon-dating experiments is forthcoming in 2011.
2High-resolution scans are available at
http://beinecke.library.yale.edu/digitallibrary/voynich.html
78
pages include text, and most are illustrated. The text
was probably added after the illustrations, and shows
no evidence of scratching or correction.
The text is written left to right in paragraphs that
are left-aligned, justified, and divided by whitespace
into words. Paragraphs do not span multiple pages.
A few glyphs are ambiguous, since they can
be interpreted as a distinct character, or a ligature
of two or more other characters. Different tran-
scriptions of the manuscript have been created,
depending on various interpretations of the glyphs.
We use a machine-readable transcription based on
the alphabet proposed by Currier (1976), edited
by D?Imperio (1980) and others, made avail-
able by the members of the Voynich Manuscript
Mailing List (Gillogly and Reeds, 2005) at
http://www.voynich.net/reeds/gillogly/voynich.now.
The Currier transcription maps the characters to the
ASCII symbols A-Z, 0-9, and *. Under this tran-
scription, the VMS is comprised of 225 pages, 8114
word types, and 37919 word tokens. Figure 1 shows
a sample VMS page and its Currier transcription.
2.3 Manuscript sections
Based on the illustrations, the manuscript has tradi-
tionally been divided into six sections: (1) herbal,
containing drawings of plants; (2) Astronomical,
containing zodiac-like illustrations; (3) Biological,
mainly containing drawings of female human fig-
ures; (4) Cosmological, consisting of circular illus-
trations; (5) Pharmaceutical, containing drawing of
small containers and parts of plants, and (6) Stars
(sometimes referred to as Recipes), containing very
dense text with drawings of stars in the margins.
Currier (1976) observed from letter and substring
frequencies that the text is comprised of two distinct
?languages?, A and B. Interestingly, the Biological
and Stars sections are mainly written in the B lan-
guage, and the rest mainly in A.
Using a two-state bigram HMM over the entire
text, we find that the two word classes induced by
EM more or less correspond to the same division
? words in pages classified as being in the A lan-
guage tend to be tagged as one class, and words in
B language pages as the other, indicating that the
manuscript does indeed contain two different vocab-
ularies (which may be related languages, dialects, or
simply different textual domains). In Figure 2, we
Figure 1: Page f81v (from the Biological section).
(a) Scan of page
BAR ZC9 FCC89 ZCFAE 8AE 8AR OE BSC89 ZCF 8AN
OVAE ZCF9 4OFC89 OFAM FAT OFAE 2AR OE FAN
OEFAN AE OE ROE 8E 2AM 8AM OEFCC89 OFC89 89FAN
ZCF S89 8AEAE OE89 4OFAM OFAN SCCF9 89 OE FAM
8AN 89 8AM SX9 OFAM 8AM OPAN SX9 OFCC89 4OF9
FAR 8AM OFAR 4OFAN OFAM OE SC89 SCOE EF9 E2
AM OFAN 8AE89 OEOR OE ZCXAE 8AM 4OFCC8AE 8AM
SX9 2SC89 4OE 9FOE OR ZC89 ZCC89 4OE FCC89 8AM
8FAN WC89 OE89 9AR OESC9 FAM OFCC9 8AM OEOR
SCX9 8AII89
BOEZ9 OZ9PCC8 4OB OFCC89 OPC89 OFZC89 4OP9
8ATAJ OZC9 4OFCC9 OFCC9 OF9 9FCC9 4OF9 OF9EF9
OES9 F9 8ZOE98 4OE OE S89 ZC89 4OFC89 9PC89
SCPC89 EFC8C9 9PC89 9FCC2C9 8SC8 9PC89 9PC89
8AR 9FC8A IB*9 4OP9 9FC89 OFAE 8ZC89 9FCC89
C2CCF9 8AM OFC89 4OFCC8 4OFC89 ESBS89 4OFAE
SC89 OE ZCC9 2AEZQ89 4OVSC89 R SC89 EPAR9
EOR ZC89 4OCC89 OE S9 RZ89 EZC89 8AR S89
BS89 2ZFS89 SC89 OE ZC89 4OESC89 4OFAN ZX9 8E
RAE 4OFS89 SC9 OE SCF9 OE ZC89 4OFC89 4OFC89
SX9 4OF9 2OEFCC9 OE ZC89 4OFAR ZCX9 8C2C89
4OFAR 4OFAE 8OE S9 4OQC9 SCFAE SO89 4OFC89
EZCP9 4OE89 EPC89 4OPAN EZO 4OFC9 EZC89 EZC89
SC89 4OEF9 ESC8AE 4OE OPAR 4OFAE 4OE OM SCC9
8AE EO*C89 ZC89 2AE SPC89PAR ZOE 4CFS9 9FAM
OEFAN ZC89 4OF9 8SC89 ROE OE Q89 9PC9 OFSC89
4OFAE OFCC9 4OE SCC89 2AE PCOE 8S89 E9 OZC89
4OPC89 ZOE SC89 9ZSC9 OE SC9 4OE SC89 PS8 OF9
OE SCSOE PAR OM OFC89 8AE ZC9 OEFCOE OEFCC89
OFCOE 8ZCOE O3 OEFCC89 PC89 SCF9 ZXC89 SAE
OPON OEFOE
(b) Transcription in the Currier alphabet. Paragraph (but not
line) breaks are indicated.
79
illustrate the division of the manuscript pages into
the six sections, and show the proportion of words
in each page that are classified as the B language.
For coherence, all our experimental results in the
rest of this paper are on the B language (which we
denote by VMS B) ? specifically, the Biological and
Stars sections ? unless otherwise specified. These
sections together contain 43 pages, with 3920 word
types, 17597 word tokens, and 35 characters. We
compare the VMS?s statistical properties with three
natural language texts of similar size: the first 28551
words from the English Wall Street Journal Corpus,
19327 words from the Arabic Quran (in Buckwalter
transcription), and 18791 words from the Chinese
Sinica Treebank.
3 The Letter
3.1 Are vowels and consonants represented?
If a script is alphabetic, i.e., it uses approximately
one character per phoneme, vowel and consonant
characters can be separated in a fully unsupervised
way. Guy (1991) applies the vowel-consonant sep-
aration algorithm of (Sukhotin, 1962) on two pages
of the Biological section, and finds that four charac-
ters (O, A, C, G) separate out as vowels. However,
the separation is not very strong, and several words
do not contain these characters.
Another method is to use a two-state bigram
HMM (Knight et al, 2006; Goldsmith and Xanthos,
2009) over letters, and induce two clusters of letters
with EM. In alphabetic languages like English, the
clusters correspond almost perfectly to vowels and
consonants. We find that a curious phenomenon oc-
curs with the VMS ? the last character of every word
is generated by one of the HMM states, and all other
characters by another; i.e., the word grammar is a?b.
There are a few possible interpretations of this. It
is possible that the vowels from every word are re-
moved and placed at the end of the word, but this
means that even long words have only one vowel,
which is unlikely. Further, the number of vowel
types would be nearly half the alphabet size. If the
script is a syllabary or a logograph, a similar clus-
tering will surface, but given that there are only 35
characters, it is unlikely that each of them represents
a syllable or word. A more likely explanation is that
the script is an abjad, like the scripts of Semitic lan-
guages, where all or most vowels are omitted. In-
deed, we find that a 2-state HMM on Arabic without
diacritics and English without vowels learns a simi-
lar grammar, a?b+.
3.2 Do letters have cases?
Some characters (F, B, P, V) that appear mainly at
paragraphs beginnings are referred to ?gallows? ?
glyphs that are taller and more ornate than others.
Among the glyphs, these least resemble Latin, lead-
ing to the belief that they are null symbols, which
Morningstar (2001) refutes.
Another hypothesis is that gallows are upper-
case versions of other characters. We define
BESTSUB(c) to be the character x that produces the
highest decrease in unigram word entropy when x
is substituted for all instances of c. For English up-
percase characters c, BESTSUB(c) is the lowercase
version. However, BESTSUB of the VMS gallows
is one of the other gallows! This demonstrates that
they are not uppercase versions of other letters, and
also that they are contextually similar to one another.
3.3 Is there punctuation?
We define punctuation as symbols that occur only at
word edges, whose removal from the word results in
an existing word. There are two characters that are
only found at the ends of words (Currier K and L), but
most of the words produced by removing K and L are
not in the vocabulary. Therefore, there is most likely
no punctuation, at least in the traditional sense.
4 The Word
4.1 What are the word frequency and length
distributions?
The word frequency distribution follows Zipf?s law,
which is a necessary (though not sufficient) test of
linguistic plausibility. We also find that the unigram
word entropy is comparable to the baseline texts (Ta-
ble 1).
Table 1: Unigram word entropy in bits.
VMS B English Arabic Chinese
9.666 10.07 9.645 10.31
Several works have noted the narrow binomial
distribution of word lengths, and contrasted it with
80
Figure 2: VMS sections, and percentage of word tokens in each page that are tagged as language B by the HMM.
the wide asymmetric distribution of English, Latin,
and other European languages. This contributed to
speculation that the VMS is not a natural language,
but a code or generated by some other stochastic
process. However, Stolfi (2005) show that Pinyin
Chinese, Tibetan, and Vietnamese word lengths fol-
low a binomial distribution, and we found (Figure 3)
that certain scripts that do not contain vowels, like
Buckwalter Arabic and devoweled English, have a
binomial distribution as well.3 The similarity with
devoweled scripts, especially Arabic, reinforces the
hypothesis that the VMS script may be an abjad.
Figure 3: Word length distributions (word types).
Landini (2001) found that the VMS follows Zipf?s
law of word lengths: there is an inverse relationship
between the frequency and length of a word.
3This is an example of why comparison with a range of
languages is required before making conclusions about the
language-like nature of a text.
4.2 How predictable are letters within a word?
Bennett (1976) notes that the second-order entropy
of VMS letters is lower than most European lan-
guages. Stolfi (2005) computes the entropy of each
character given the left and right contexts and finds
that it is low for most of the VMS text, particularly
the Biological section, compared to texts in other
languages. He also ascertains that spaces between
words have extremely low entropy.
We measure the predictability of letters, and com-
pare it to English, Arabic, and Pinyin Chinese. Pre-
dictability is measured by finding the probabilities
over a training set of word types, guessing the most
likely letter (the one with the highest probability) at
each position in a word in the held-out test set, and
counting the proportion of times a guess is correct.
Table 2 shows the predictability of letters as uni-
grams, and given the preceding letter in a word (bi-
grams). VMS letters are more predictable than other
languages, with the predictability increasing sharply
given the preceding contexts, similarly to Pinyin.
Table 2: Predictability of letters, averaged over 10-fold
cross-validation runs.
VMS B English Arabic Pinyin
Bigram 40.02% 22.62% 24.78% 38.92%
Unigram 14.65% 11.09% 13.29% 11.20%
Zandbergen (2010) computes the entropies of
characters at different positions in words in the Stars
section, and finds that the 1st and 2nd characters of a
word are more predictable than in Latin or Vulgate,
but the 3rd and 4th characters are less predictable.
81
It has also been observed that word-final char-
acters have much lower entropy compared to most
other languages ? some characters appear almost ex-
clusively at the ends of words.
4.3 Is there morphological structure?
The above observations suggest that words are made
up of morpheme-like chunks. Several hypotheses
about VMS word structure have been proposed. Tilt-
man (1967) proposed a template consisting of roots
and suffixes. Stolfi (2005) breaks down the morphol-
ogy into ?prefix-midfix-suffix?, where the letters in
the midfixes are more or less disjoint from the let-
ters in the suffixes and prefixes. Stolfi later modified
this to a ?core-mantel-crust? model, where words are
composed of three nested layers.
To determine whether VMS words have affixal
morphology, we run an unsupervised morphologi-
cal segmentation algorithm, Linguistica (Goldsmith,
2001), on the VMS text. The MDL-based algo-
rithm segments words into prefix+stem+suffix, and
extracts ?signatures?, sets of affixes that attach to the
same set of stems. Table 3 lists a few sample signa-
tures, showing that stems in the same signature tend
to have some structural similarities.
Table 3: Some morphological signatures.
Affixes Stems
OE+, A3 AD AE AE9 AEOR AJ AM AN AR AT
OP+, E O O2 OE OJ OM ON OR
null+ SAJ SAR SCC9 SCCO SCO2 SO
OE+ BSC28 BSC9 CCC8 COC8CR FAEOE
FAK FAU FC8 FC8AM FCC FCC2 FCC9R
FCCAE FCCC2 FCCCAR9 FCO9 FCS9
FCZAR FCZC9 OEAR9 OESC9 OF9 OR8
SC29 SC89O SC8R SCX9 SQ9
+89, 4OFCS 4OFCZ 4OFZ 4OPZ 8AES 8AEZ
+9, 9FS 9PS EFCS FCS PS PZ
+ C89 OEFS OF OFAES OFCS OFS OFZ
5 Syntax
5.1 Is there word order?
One of the most puzzling features of the VMS is its
weak word order. Notably, the text has very few re-
peated word bigrams or trigrams, which is surpris-
ing given that the unigram word entropy is com-
parable to other languages. Furthermore, there are
sequences of two or more repeated words, or rep-
etitions of very similar words. For example, the
first page of the Biological section contains the line
4OFCC89 4OFCC89 4OFC89 4OFC89 4OFCC89 E89.
We compute the predictability of a word given the
previous word (Table 4). Bigram contexts only pro-
vide marginal improvement in predictability for the
VMS, compared to the other texts. For comparison
with a language that has ?weak word order?, we also
compute the same numbers for the first 22766 word
tokens of the Hungarian Bible, and find that the em-
pirical word order is not that weak after all.
Table 4: Predictability of words (over 10-fold cross-
validation) with bigram contexts, compared to unigrams.
Unigram Bigram Improvement
VMS B 2.30% 2.50% 8.85%
English 4.72% 11.9% 151%
Arabic 3.81% 14.2% 252%
Chinese 16.5% 19.8% 19.7%
Hungarian 5.84% 13.0% 123%
5.2 Are there latent word classes?
While there are very few repeated word bigrams,
perhaps there are latent classes of words that gov-
ern word order. We induce ten word classes using a
bigram HMM trained with EM (Figure 4). As with
the stems in the morphological signatures, the words
in each class show some regularities ? although it
is hard to quantify the similarities ? suggesting that
these latent classes are meaningful.
Currier (1976) found that some word-initial char-
acters are affected by the word-final characters of
the immediately preceding word. He concludes that
the ?words? being syllables or digits would explain
this phenomenon, although that is unlikely given the
rarity of repeated sequences.
We redo the predictability experiments of the pre-
vious section, using the last m letters of the previous
word to predict the first n letters of the current word.
When n > 2, improvement in predictability remains
low. However, when n is 1 or 2, there is a noticeable
improvement when using the last few characters of
the previous word as contexts (Table 5).
5.3 Are there long-distance word correlations?
Weak bigram word order can arise if the text is
scrambled or is generated by a unigram process. Al-
ternately, the text might have been created by inter-
82
Figure 4: Some of the induced latent classes.
(a) (b) (c)
Table 5: Relative improvement in predictability of first
n word-characters using last m characters of previous
word, over using no contextual information.
VMS B English Arabic
Whole words 8.85% 151% 252%
m = 1 31.8% 31.1% 26.8%
n = 1 m = 2 30.7% 45.8% 61.5%
m = 3 29.9% 60.3% 92.4%
m = 1 16.0% 42.8% 0.0736%
n = 2 m = 2 12.4% 67.5% 14.1%
m = 3 10.9% 94.6% 33.2%
leaving the words of two or more texts, in which case
there will be long-distance correlations.
Schinner (2007) shows that the probability of sim-
ilar words repeating in the text at a given distance
from each other follows a geometric distribution.
Figure 5 illustrates the ?collocationness? at dis-
tance d, measured as the average pointwise mutual
information over all pairs of words w1, w2 that occur
more than once at distance d apart. VMS words do
not show significant long-distance correlations.
6 The Page
6.1 Do pages have topics?
That is, do certain words ?burst? with a high fre-
quency within a page, or are words randomly dis-
tributed across the manuscript? Figure 6 shows a vi-
sualization of the TF-IDF values of words in a VMS
B page, where the ?documents? are pages, indicating
the relevance of each word to the page. Also shown
is the same page in a version of the document created
by scrambling the words of the original manuscript,
and repaginating to the same page lengths. This sim-
ulates a document where words are generated inde-
pendent of the page, i.e., the pages have no topics.
Figure 5: Long-range collocationness. Arabic shows
stronger levels of long-distance correlation compared to
English and Chinese. VMS B shows almost no correla-
tions for distance d > 1.
To quantify the degree to which a page contains
topics, we measure the entropy of words within the
page, and denote the overall ?topicality? T of a doc-
ument as the average entropy over all the pages. As
a control, we compute the topicality Trand of the
scrambled version of the document. 1 ? T/Trand
indicates the extent to which the pages of the docu-
ment contain topics. Table 6 shows that by this mea-
sure, the VMS?s strength of page topics is less than
the English texts, but more than the Quran4, signify-
ing that the pages probably do have topics, but are
not independent of one another.
6.2 Is the text prose?
Visually, the text looks like prose written in para-
graphs. However, Currier (1976) stated that ?the line
4We demarcate a ?page? to be approximately 25 verses for
the Quran, a chapter for the Genesis, and an article for the WSJ.
83
Figure 6: TF-IDF visualization of page f108v in the Stars section.
(a) Original document, showing bursts (b) Scrambled version ? flatter distribution
Table 6: Strength of page topics in VMS and other texts,
cropped to be of comparable length to the VMS.
VMS English English Arabic
B WSJ Genesis Quran
T 7.5 6.3 6.6 7.7
Trand 7.7 6.5 7.1 7.9
1? T/Trand 0.033 0.037 0.069 0.025
is a functional entity? ? that is, there are patterns to
lines on the page that are uncharacteristic of prose.
In particular, certain characters or sequences appear
almost exclusively at the beginnings or ends of lines.
Figure 7 shows the distribution of characters at
line-edges, relative to their occurrences at word
beginnings or endings,confirming Currier?s obser-
vation. It is particularly interesting that lower-
frequency characters occur more at line-ends, and
higher-frequency ones at the beginnings of lines.
Schinner (2007) found that characters show long-
range correlations at distances over 72 characters,
which is a little over the average line length.
7 The Document
7.1 Are the pages in order?
We measure the similarity between two pages as the
cosine similarity over bags of words, and count the
proportion of pages Pi where the page Pi?1 or Pi+1
is the most similar page to Pi. We denote this mea-
sure by ADJPAGESIM. If ADJPAGESIM is high, it
indicates that (1) the pages are not independent of
each other and (2) the pages are in order.
Table 7 shows ADJPAGESIM for the VMS and
other texts. As expected, ADJPAGESIM is close to
zero for the VMS with pages scrambled, as well as
the WSJ, where each page is an independent article,
and is highest for the VMS, particularly the B pages.
Table 7: ADJPAGESIM for VMS and other texts.
VMS B 38.8%
VMS All 15.6%
VMS B pages scrambled 0%
VMS All pages scrambled 0.444%
WSJ 1.34%
English Genesis 25.0%
Arabic Quran 27.5%
This is a convincing argument for the pages be-
ing mostly in order. However, the non-contiguity
of the herbal and pharmaceutical sections and the
interleaving of the A and B languages indicates
that larger chunks of pages were probably re-
ordered. In addition, details involving illustrations
and ink-transfer across pages point to a few local re-
orderings (Pelling, 2006).
7.2 How many authors were involved?
Currier (1976) observed that the distinction between
the A and B languages corresponds to two different
types of handwriting, implying at least two authors.
He claimed that based on finer handwriting analysis,
there may have been as many as eight scribes.
8 Latin, Cipher, or Hoax?
Claims of decipherment of the VMS script have
been surfacing for several years, none of which are
convincing. Newbold (1928) believed that micro-
scopic irregularities of glyph edges correspond to
anagrammed Latin. Feely in 1943 proposed that the
script is a code for abbreviated Latin (D?Imperio,
1980). Sherwood (2008) believes that the words
are coded anagrams of Italian. Others have hypoth-
84
Figure 7: Proportion of word-edge characters at line-edges for lines that span the width of the page. Characters are in
ascending order of their total frequencies.
(a) Original document, showing biased distribution. (b) Flat distribution when words within lines are scrambled.
esized that the script is an encoding of Ukrainian
(Stojko, 1978), English (Strong, 1945; Brumbaugh,
1976), or a Flemish Creole (Levitov, 1987). The
word length distribution and other properties have
invoked decodings into East Asian languages like
Manchu (Banasik, 2004). These theories tend to rely
on arbitrary anagramming and substitutions, and are
not falsifiable or well-defined.
The mysterious properties of the text and its resis-
tance to decoding have led some to conclude that it
is a hoax ? a nonsensical string made to look vaguely
language-like. Rugg (2004) claims that words might
have been generated using a ?Cardan Grille? ? a
way to deterministically generate words from a ta-
ble of morphemes. However, it seems that the Grille
emulates a restricted finite state grammar of words
over prefixes, midfixes, and suffixes. Such a gram-
mar underlies many affixal languages, including En-
glish. Martin (2008) proposes a method of generat-
ing VMS text from anagrams of number sequences.
Like the previous paper, it only shows that this
method can create VMS-like words ? not that it is
the most plausible way of generating the manuscript.
It is also likely that the proposed scheme can be used
to generate any natural language text.
Schinner (2007) votes for the hoax hypothesis
based on his observations about characters showing
long-range correlations, and the geometric distribu-
tion of the probability of similar words repeating at
a fixed distance. These observations only confirm
that the VMS has some properties unlike natural lan-
guage, but not that it is necessarily a hoax.
9 Conclusion
We have detailed various known properties of the
Voynich manuscript text. Some features ? the lack
of repeated bigrams and the distributions of letters at
line-edges ? are linguistically aberrant, which others
? the word length and frequency distributions, the
apparent presence of morphology, and most notably,
the presence of page-level topics ? conform to natu-
ral language-like text.
It is our hope that this paper will motivate re-
search into understanding the manuscript by schol-
ars in computational linguistics. The questions pre-
sented here are obviously not exhaustive; a deeper
examination of the statistical features of the text in
comparison to a number of scripts and languages is
needed before any definite conclusions can be made.
Such studies may also inspire a quantitative interest
in linguistic and textual typologies, and be applica-
ble to the decipherment of other historical scripts.
Acknowledgments
We would like to thank the anonymous reviewers
and our colleagues at ISI and Chicago for their help-
ful suggestions. This work was supported in part by
NSF Grant 0904684.
85
References
Zbigniew Banasik. 2004.
http://www.ic.unicamp.br/ stolfi/voynich/04-05-
20-manchu-theo/alphabet.html.
William Ralph Bennett. 1976. Scientific and engineering
problem solving with a computer. Prentice-Hall.
Robert Brumbaugh. 1976. The Voynich ?Roger Bacon?
cipher manuscript: deciphered maps of stars. Journal
of the Warburg and Courtauld Institutes.
Prescott Currier. 1976. New research on the
Voynich Manuscript: Proceedings of a semi-
nar. Unpublished communication, available from
http://www.voynich.nu/extra/curr pdfs.html.
Mary D?Imperio. 1980. The Voynich Manuscript: An
Elegant Enigma. Aegean Park Press.
Jim Gillogly and Jim Reeds. 2005. Voynich Manuscript
mailing list. http://voynich.net/.
John Goldsmith and Aris Xanthos. 2009. Learning
phonological categories. Language, 85:4?38.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics.
Jacques Guy. 1991. Statistical properties of two folios of
the Voynich Manuscript. Cryptologia.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of COLING.
Gabriel Landini. 2001. Evidence of linguistic struc-
ture in the Voynich Manuscript using spectral analysis.
Cryptologia.
Leo Levitov. 1987. Solution of the Voynich Manuscript:
A Liturgical Manual for the Endura Rite of the Cathari
Heresy, the Cult of Isis. Aegean Park Press.
Claude Martin. 2008. Voynich, the game is over.
http://www.voynich.info/.
Jason Morningstar. 2001. Gallows variants as null char-
acters in the Voynich Manuscript. Master?s thesis,
University of North Carolina.
William Newbold. 1928. The Cipher of Roger Bacon.
University of Pennsylvania Press.
Nicholas John Pelling. 2006. The Curse of the Voyn-
ich: The Secret History of the World?s Most Mysteri-
ous Manuscript. Compelling Press.
Jim Reeds. 2002. Voynich Manuscript.
http://www.ic.unicamp.br/ stolfi/voynich/mirror/reeds.
Gordon Rugg. 2004. The mystery of the Voynich
Manuscript. Scientific American Magazine.
Andreas Schinner. 2007. The Voynich Manuscript: Evi-
dence of the hoax hypothesis. Cryptologia.
Edith Sherwood. 2008. The
Voynich Manuscript decoded?
http://www.edithsherwood.com/voynich decoded/.
John Stojko. 1978. Letters to God?s Eye: The Voynich
Manuscript for the first time deciphered and translated
into English. Vantage Press.
Jorge Stolfi. 2005. Voynich Manuscript stuff.
http://www.dcc.unicamp.br/ stolfi/voynich/.
Leonell Strong. 1945. Anthony Ashkam, the author of
the Voynich Manuscript. Science.
Boris Sukhotin. 1962. Eksperimental?noe vydelenie
klassov bukv s pomoscju evm. Problemy strukturnoj
lingvistiki.
John Tiltman. 1967. The Voynich Manuscript, the most
mysterious manuscript in the world. NSA Technical
Journal.
Rene? Zandbergen. 2010. Voynich MS.
http://www.voynich.nu/index.html.
86
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 76?85,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Towards Probabilistic Acceptors and Transducers for Feature Structures
Daniel Quernheim
Institute for Natural Language Processing
Universita?t Stuttgart, Germany
Pfaffenwaldring 5b, 70569 Stuttgart
daniel@ims.uni-stuttgart.de
Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
knight@isi.edu
Abstract
Weighted finite-state acceptors and transduc-
ers (Pereira and Riley, 1997) are a critical
technology for NLP and speech systems. They
flexibly capture many kinds of stateful left-to-
right substitution, simple transducers can be
composed into more complex ones, and they
are EM- trainable. They are unable to han-
dle long-range syntactic movement, but tree
acceptors and transducers address this weak-
ness (Knight and Graehl, 2005). Tree au-
tomata have been profitably used in syntax-
based MT systems. Still, strings and trees are
both weak at representing linguistic structure
involving semantics and reference (?who did
what to whom?). Feature structures provide
an attractive, well-studied, standard format
(Shieber, 1986; Rounds and Kasper, 1986),
which we can view computationally as di-
rected acyclic graphs. In this paper, we de-
velop probabilistic acceptors and transducers
for feature structures, demonstrate them on
linguistic problems, and lay down a founda-
tion for semantics-based MT.
1 Introduction
Weighted finite-state acceptors and transducers
(Pereira and Riley, 1997) provide a clean and
practical knowledge representation for string-based
speech and language problems. Complex problems
can be broken down into cascades of simple trans-
ducers, and generic algorithms (best path, composi-
tion, EM, etc) can be re-used across problems.
String automata only have limited memory and
cannot handle complex transformations needed in
machine translation (MT). Weighted tree acceptors
and transducers (Ge?cseg and Steinby, 1984; Knight
and Graehl, 2005) have proven valuable in these sce-
narios. For example, systems that transduce source
strings into target syntactic trees performed well in
recent MT evaluations (NIST, 2009).
To build the next generation of language systems,
we would like to represent and transform deeper lin-
guistic structures, e.g., ones that explicitly capture
semantic ?who does what to whom? relationships,
with syntactic sugar stripped away. Feature struc-
tures are a well-studied formalism for capturing nat-
ural language semantics; Shieber (1986) and Knight
(1989) provide overviews. A feature structure is de-
fined as a collection of unordered features, each of
which has a value. The value may be an atomic sym-
bol, or it may itself be another feature structure. Fur-
thermore, structures may be re-entrant, which means
that two feature paths may point to the same value.
Figure 1 shows a feature structure that captures
the meaning of a sample sentence. This seman-
tic structure provides much more information than
a typical parse, including semantic roles on both
nouns and verbs. Note how ?Pascale? plays four
different semantic roles, even though it appears only
once overtly in the string. The feature structure also
makes clear which roles are unfilled (such as the
agent of the charging), by omitting them. For com-
putational purposes, feature structures are often rep-
resented as rooted, directed acyclic graphs with edge
and leaf labels.
While feature structures are widely used in hand-
built grammars, there has been no compelling pro-
posal for weighted acceptors and transducers for
76
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INSTANCE charge
THEME 1
[
INSTANCE person
NAME ?Pascale?
]
PRED
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INSTANCE and
OP1
?
?
?
?
?
?
INSTANCE resist
AGENT 1
THEME
[
INSTANCE arrest
THEME 1
]
?
?
?
?
?
?
OP2
?
?
?
?
INSTANCE intoxicate
THEME 1
LOCATION
[
INSTANCE public
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
charge
and
resist
arrest
intoxicate
person Pascale
public
instance pred
theme
instance op1op2
instance theme
agent
instancetheme
instance
theme
location
instance name
instance
CHARGE
AND
RESIST INTOXICATE
ARREST
PERSON PUBLIC
PASCALE
CHARGE 7? charge(theme, pred)
AND 7? and(op1, op2)
RESIST 7? resist(agent, theme)
ARREST 7? arrest(theme)
INTOXICATE 7? intoxicate(theme, location)
PUBLIC 7? public()
PERSON 7? person(name)
PASCALE 7? ?Pascale?
Figure 1: A feature structure representing the semantics of ?Pascale was charged with resisting arrest and public intox-
ication,? the corresponding dag, and the simplified dag with argument mapping. Dag edges always point downward.
FSA
fstring?
nl-XTOPs?1
translate ?
RTG
etree?
FSA
rank ?
FSA
estring
fstring? parse + understand ? esem? rank ? esem? generate ? etree? rank ? estring
Figure 2: Pipelines for syntax-based and for semantics-based MT. Devices: FSA = finite string automaton;
ln-XTOPs = linear non-deleting extended top-down tree-to-string transducer; RTG = regular tree grammar.
77
string automata tree automata graph automata
k-best . . . paths through a WFSA
(Viterbi, 1967; Eppstein, 1998)
. . . trees in a weighted forest
(Jime?nez and Marzal, 2000;
Huang and Chiang, 2005)
?
EM training Forward-backward EM (Baum
et al, 1970; Eisner, 2003)
Tree transducer EM training
(Graehl et al, 2008)
?
Determinization . . . of weighted string acceptors
(Mohri, 1997)
. . . of weighted tree acceptors
(Borchardt and Vogler, 2003;
May and Knight, 2006a)
?
Transducer com-
position
WFST composition (Pereira and
Riley, 1997)
Many transducers not closed un-
der composition (Maletti et al,
2009)
?
General tools AT&T FSM (Mohri et al,
2000), Carmel (Graehl, 1997),
OpenFST (Riley et al, 2009)
Tiburon (May and Knight,
2006b)
?
Table 1: General-purpose algorithms for strings, trees and feature structures.
them. Such automata would be of great use. For
example, a weighted graph acceptor could form the
basis of a semantic language model, and a weighted
graph-to-tree transducer could form the basis of a
natural language understanding (NLU) or genera-
tion (NLG) system, depending on which direction
it is employed. Putting NLU and NLG together,
we can also envision semantics-based MT systems
(Figure 2). A similar approach has been taken
by Graham et al (2009) who incorporate LFG f-
structures, which are deep syntax feature structures,
into their (automatically acquired) transfer rules.
Feature structure graph acceptors and transducers
could themselves be learned from semantically-
annotated data, and their weights trained by EM.
However, there is some distance to be traveled.
Table 1 gives a snapshot of some efficient, generic
algorithms for string automata (mainly developed in
the last century), plus algorithms for tree automata
(mainly developed in the last ten years). These algo-
rithms have been packaged in general-purpose soft-
ware toolkits like AT&T FSM (Mohri et al, 2000),
OpenFST (Riley et al, 2009), and Tiburon (May
and Knight, 2006b). A research program for graphs
should hold similar value.
Formal graph manipulation has, fortunately, re-
ceived prior attention. A unification grammar
can specify semantic mappings for strings (Moore,
1989), effectively capturing an infinite set of
string/graph pairs. But unification grammars seem
too powerful to admit the efficient algorithms we
desire in Table 1, and weighted versions are not
popular. Hyperedge replacement grammars (Drewes
et al, 1997; Courcelle and Engelfriet, 1995)
are another natural candidate for graph acceptors,
and a synchronous hyperedge replacement gram-
mar might serve as a graph transducer. Finally,
Kamimura and Slutzki (1981, 1982) propose graph
acceptor and graph-to-tree transducer formalisms
for rooted directed acyclic graphs. Their model has
been extended to multi-rooted dags (Bossut et al,
1988; Bossut and Warin, 1992; Bossut et al, 1995)
and arbitrary hypergraphs (Bozapalidis and Kalam-
pakas, 2006; Bozapalidis and Kalampakas, 2008);
however, these extensions seem too powerful for
NLP. Hence, we use the model of Kamimura and
Slutzki (1981, 1982) as a starting point for our def-
inition, then we give a natural language example,
followed by an initial set of generic algorithms for
graph automata.
2 Preliminaries
In this section, we will define directed acyclic graphs
which are our model for semantic structures.
Let us just define some basic notions: We will
write R for the real numbers. An alphabet is just
a finite set of symbols.
Intuitively, a rooted ordered directed acyclic
graph, or dag for short, can be seen as a tree that
allows sharing of subtrees. However, it is not nec-
essarily a maximally shared tree that has no isomor-
phic subtrees (consider the examples in Figure 3).
78
(3a)
WANT
BELIEVE
BOY GIRL (3b)
WANT
BELIEVE
BOY BOY GIRL
Figure 3: Maximally shared tree (a) and not maximally
shared tree (b; note the two BOY nodes) can be distinct
dags. The dag in (a) means ?The boy wants to believe the
girl,? while the dag in (b) means ?The boy wants some
other boy to believe the girl.?
(4a)
>
BELIEVE
BOY GIRL (4b)
>
BOY GIRL
Figure 4: Subdag of dag (3a) and subdag of dag (3b) in
Figure 3.
More formally, we define a directed graph over an
alphabet ? as a triple G = (V,E, `) of a finite set of
nodes V , a finite set of edgesE ? V ?V connecting
two nodes each and a labeling function ` : V ? ?.
We say that (v, w) is an outgoing edge of v and an
incoming edge of w, and we say that w is a child of
v and v is a parent of w. A directed graph is a dag if
it is
? acyclic: V is totally ordered such that there is
no (v, w) ? E with v > w;
? ordered: for each V , there is a total order both
on the incoming edges and the outgoing edges;
? and rooted: min(V ) is transitively connected
by to all other nodes.
This is a simplified account of the dags presented in
Section 1. Instead of edge-labels, we will assume
that this information is encoded explicitely in the
node-labels for the INSTANCE feature and implic-
itly in the node-labels and the order of the outgoing
edges for the remaining features. Figure 1 shows a
feature structure and its corresponding dag. Nodes
with differently-labeled outgoing edges can thus be
differentiated. Since the number of ingoing edges is
not fixed, a node can have arbitrary many parents.
For instance, the PERSON node in Figure 1 has four
parents. We call the number of incoming edges of a
given node its head rank, and the number of outgo-
ing edges its tail rank.
(left)
WANT
? (right)
WANT
BOY GIRL
Figure 5: (left) Remainder of (3a) after removing (4a).
(right) Dag resulting from replacing (4a) by (4b) in (3a).
We also need incomplete dags in order to com-
pose larger dags from smaller ones. An incomplete
dag is a dag in which some edges does not nec-
essarily have to be connected to two nodes; they
can be ?dangling? from one node. We represent
this by adding special nodes > and ? to the dag.
If an incomplete dag has m edges (>, v) and n
edges (v,?), we call it an (m,n)-dag. An (m,n)-
dag G can be composed with an (n, o)-dag G? by
identifying the n downward-dangling edges of G
with the n upward-dangling edges of G? in the right
order; the result G ? G? is a (m, o)-dag. Fur-
thermore, two dags H and H ? of type (m,n) and
(m?, n?) can be composed horizontally by putting
their upward-dangling edges next to each other and
their downward-dangling edges next to each other,
resulting in a new (m + m?, n + n?) dag H ? H ?.
If G1, . . . , G` can be composed (vertically and hor-
izontally) in such a way that we obtain G, then Gi
are called subdags of G.
An (m,n)-subdag H of a dag G can be replaced
by an (m,n)-subdag H ?, resulting in the dag G?,
written G[H ? H ?] = G?. An example is depicted
in Figure 5, showing how a dag is split into two sub-
dags, of which one is replaced by another incom-
plete dag. Our account of dag replacement is a sim-
plified version of general hypergraph replacement
that has been formulated by Engelfriet and Vereijken
(1997) and axiomatized by Bozapalidis and Kalam-
pakas (2004).
Trees are dags where every node has at most one
incoming edge. Tree substitution is then just a spe-
cial case of dag composition. We will write the set of
dags over an alphabet ? as D? and the set of trees
over ? as T?, and T?(V ) is the set of trees with
leaves labeled with variables from the set V .
3 Dag acceptors and transducers
The purpose of dag acceptors and dag transducers
is to compactly represent (i) a possibly-infinite set
of dags, (ii) a possibly-infinite set of (dag, tree)
79
pairs, and (iii) a possibly-infinite set of (graph, tree,
weight) triples.
Dag acceptors and dag transducers are a gener-
alization of tree acceptors and transducers (Comon
et al, 2007). Our model is a variant of the dag
acceptors defined by Kamimura and Slutzki (1981)
and the dag-to-tree transducers by Kamimura and
Slutzki (1982). The original definition imposed
stricter constraints on the class of dags. Their
devices operated on graphs called derivation dags
(short: d-dags) which are always planar. In particu-
lar, the authors required all the parents and children
of a given node to be adjacent, which was due to the
fact that they were interested in derivation graphs
of unrestricted phrase-structure grammar. (While
the derivation structures of context-free grammar are
trees, the derivation structures of type-0 grammars
are d-dags.) We dropped this constraint since it
would render the class of dags unsuitable for linguis-
tic purposes. Also, we do not require planarity.
Kamimura and Slutzki (1981, 1982) defined three
devices: (i) the bottom-up dag acceptor, (ii) the top-
down dag acceptor (both accepting d-dags) and (iii)
the bottom-up dag-to-tree transducer (transforming
d-dags into trees). We demonstrate the application
of a slightly extended version of (ii) to unrestricted
dags (semantic dags) and describe a top-down dag-
to-tree transducer model, which they did not investi-
gate. Furthermore, we add weights to the models.
A (weighted) finite dag acceptor is a structure
M = (Q, q0,?, R,w) where Q is a finite set of
states and q0 the start state, ? is an alphabet of
node labels, and R is a set of rules of the form
r : ? ? ?, where r is the (unique) rule identifier
and (i) ? ? Qm(?) and ? ? r(Qn) for m,n ? N
and some ? ? ? (an explicit rule of type (m,n)) or
(ii) ? ? Qm and ? ? r(Q) (an implicit rule of type
(m, 1)). The function w : R ? R assigns a weight
to each rule.
Intuitively, explicit rules consume input, while
implicit rules are used for state changes and joining
edges only. The devices introduced by Kamimura
and Slutzki (1981) only had explicit rules.
We define the derivation relation of M by rewrit-
ing of configurations. A configuration of M is a dag
over ? ?R ?Q with the restriction that every state-
labeled node has head and tail rank 1. Let c be a
configuration of M and r : ? ? ? an explicit rule
(q)WANT ? 1(r, q) ?0.3? (1)
(q)BELIEVE ? 2(r, q) ?0.2? (2)
(r)BOY ? 3 ?0.3? (3)
(r)GIRL ? 4 ?0.3? (4)
(r)? ? 5 ?0.1? (5)
(q)? ? 6 ?0.1? (6)
(q)? 7(r) ?0.4? (7)
Figure 7: Ruleset of the dag acceptor in Example 1.
of type (m,n). Then c =?r c? if ? matches a sub-
dag of c, and c? = c[?? ?].
Now let c be a configuration of M and r : ?? ?
an implicit rule of type (m, 1). If a configuration
c? can be obtained by replacing m nodes labeled ?
such that all tails lead to the same node and are in the
right order, by the single state-node ?, then we say
c =?r c?. Example derivation steps are shown in
Figure 6 (see Example 1). We denote the transitive
and reflexive closure of =? by =??.
A dag G is accepted by M if there is a deriva-
tion q0(G) =?? G?, where G? is a dag over ?(R).
Note that the derivation steps of a given derivation
are partially ordered; many derivations can share the
same partial order. In order to avoid spurious deriva-
tions, recall that the nodes of G are ordered, and as-
sume that nodes are rewritten according to this or-
der: the resulting derivation is called a canonical
derivation. The set of all canonical derivations for
a given graph G is D(G). The set of all dags ac-
cepted byM is the dag language L(M). The weight
w(d) of a derivation dag (represented by its canon-
ical derivation) d = G =?r1 G1 =?r2 . . . =?rn
Gn is
?n
i=1w(rn), and the weight of a dag G is?
d?D(G)w(d). The weighted language L(N) is a
function that maps every dag to its weight in N .
Example 1. Let
? = {GIRL, BOY, BELIEVE,WANT, ?}
and consider the top-down dag acceptor M =
({q, r}, q,?, R,w) which has a ruleset containing
the explicit and implicit (1, 1) rules given in Fig-
ure 7. The weights defined by w have been written
directly after the rules in angle brackets. This ac-
80
qWANT
BELIEVE
BOY GIRL
=?1
1
r q
BELIEVE
BOY GIRL
=?2
1
r 2
r q
BOY GIRL
=?8
1
2
8 q
r GIRL
BOY
=?3
1
2
8 q
3 GIRL
=?7
1
2
8 7
3 r
GIRL
=?4
1
2
8 7
3 4
Figure 6: Derivation of a dag using the dag acceptor of Example 1. The weight of the derivation is w(1) ?w(2) ?w(8) ?
w(3) ? w(7) ? w(4) = 0.3 ? 0.2 ? 0.2 ? 0.3 ? 0.4 ? 0.3 = 0.000432.
ceptor can accept dags that involve boys and girls
believing and wanting. One of them is given in Fig-
ure 3b. To obtain dags that are not trees, let us add
the following implicit (2, 1) and (3, 1) rules:
(r, r)? 8(r) ?0.2? (8)
(r, r, r)? 9(r) ?0.1? (9)
A non-treelike dag is given in Figure 3a, while its
derivation is given in Figure 6. Note that the effect
of rule (8) could be simulated by rule (9).
Let us now define dag-to-tree transducers. Con-
trarily to Kamimura and Slutzki (1982), who defined
only the bottom-up case and were skeptical of an el-
egant top-down formulation, we only consider top-
down devices.
A (weighted) top-down dag-to-tree transducer is
a machine T = (Q, q0,?,?, R,w) which is defined
in the same way as a finite dag acceptor, except for
the additional output alphabet ? and the rules? right-
hand side. A dag-to-tree transducer explicit rule
has the form r : ? ? ? where ? ? Qm(?) and
? ? (T?(Q(Xn)))m for m,n ? N. Intuitively, this
means that the left-hand side still consists of a sym-
bol and m ?incoming states?, while the right-hand
side now are m trees over ? with states and n vari-
ables used to process the n child subdags. Implicit
(m, 1) rules are defined in the same way, having m
output trees over one variable. The dag-to-tree trans-
ducer T defines a relation L(T ) ? D? ? T? ? R.
A derivation step of T is defined analogously to
the acceptor case by replacement of ? by ?. How-
ever, copying rules (those that use a variable more
than once in a right-hand side) and deleting rules
(those that do not use a rule at all) are problematic in
the dag case. In the tree world, every tree can be bro-
ken up into a root symbol and independent subtrees.
This is not true in the dag world, where there is shar-
ing between subdags. Therefore, if an edge reach-
ing a given symbol ? is not followed at all (deleting
rule), the transducer is going to choke if not every
edge entering ? is ignored. In the case of copying
rules, the part of the input dag that has not yet been
processed must be copied, and the configuration is
split into two sub-configurations which must both
be derived in parallel. We will therefore restrict our-
selves to linear (non-copying) non-deleting rules in
this paper.
4 NLP example
Recall the example dag acceptor from Example 1.
This acceptor generates an sentences about boys and
girls wanting and believing. Figure 3 shows some
sample graphs from this language.
Next, we build a transducer that relates these
graphs to corresponding English. This is quite chal-
lenging, as BOY may be referred to in many ways
(?the boy?, ?he?, ?him?, ?himself?, ?his?, or zero),
and of course, there are many syntactic devices for
representing semantic role clusters. Because of am-
biguity, the mapping between graphs and English is
many-to-many. Figure 8 is a fragment of our trans-
ducer, and Figure 9 shows a sample derivation.
Passives are useful for realizing graphs with
empty roles (?the girl is wanted? or ?the girl wants
to be believed?). Note that we can remove syntactic
0 (zero) elements with a standard tree-to-tree trans-
ducer, should we desire.
(qs)WANT(x, y)? S(qnomg(x), is wanted, qzero(y))
(qinfg)BELIEVE(x, y)? INF(qzero(y), to be believed, qzerog(y))
(qzero)? ? 0
81
(qs)WANT(x, y)? S(qnomb(x),wants, qinfb(y)) (10)
(qinfb)BELIEVE(x, y)? INF(qaccg(x), to believe, qaccb(y)) (11)
(qaccg)GIRL ? NP(the girl) (12)
(qnomb, qaccb)BOY ? NP(the boy), NP(him) (13)
Figure 8: Transducer rules mapping semantic graphs to syntactic trees.
q
WANT
BELIEVE
BOY GIRL
=?10
S
qnomb wants qinfb
BELIEVE
BOY GIRL
=?11
S
qnomb wants INF
qaccg to believe qaccb
BOY GIRL
=?12,13
S
INF
NP NP NP
the boy wants the girl to believe him
Figure 9: Derivation from graph to tree ?the boy wants the girl to believe him?.
Events can be realized with nouns as well as verbs
(?his desire for her, to believe, him?):
(qnp)WANT(x, y)? NP(qpossb(x), ?s desire, qinfb(y))
We note that transducer rules can be applied in ei-
ther direction, semantics-to-English or English-to-
semantics. Though this microworld is small, it cer-
tainly presents interesting challenges for any graph
transduction framework. For example, given ?the
boy?s desire is to be believed by the girl,? the trans-
ducer?s graph must make BOY the theme of BE-
LIEVE.
5 Generic dag acceptor and transducer
algorithms
In this section we give algorithms for standard tasks.
5.1 Membership checking
Membership checking is the task of determining, for
a given finite dag acceptor M and an input dag G,
whether G ? L(M), or in the weighted case, com-
pute the weight of G. Recall that the set of nodes
of G is ordered. We can therefore walk through G
according to this order and process each node on its
own. A very simple algorithm can be given in the
framework of ?parsing as deduction? (Shieber et al,
1995):
Items: configurations, i.e. dags over ? ?Q ?R
Axiom: G, a dag over ?
Goal: dag over R
Inference rule: if an item has only ancestors from
Q, apply a matching rule from R to obtain a
new item
This algorithm is correct and complete and can be
implemented in time O(2|G|) since there are expo-
nentially many configurations. Moreover, the set of
derivation dags is the result of this parser, and a fi-
nite dag acceptor representing the derivation dags
can be constructed on the fly. It can be easily ex-
tended to check membership of (dag, tree) pairs in a
dag-to-tree transducer and to generate all the trees
that are obtained from a given dag (?forward ap-
plication?). In order to compute weights, the tech-
niques by Goodman (1999) can be used.
5.2 1-best and k-best generation
The k-best algorithm finds the highest-weighted k
derivations (not dags) in a given (weighted) dag ac-
ceptor. If no weights are available, other measures
can be used (e.g. the number of derivation steps or
symbol frequencies). We can implement the k-best
algorithm (of which 1-best is a special case) by gen-
erating graphs and putting incomplete graphs on a
priority queue sorted by weight. If rule weights are
probabilities between 0 and 1, monotonicity ensures
that the k-best graphs are found, as the weights of
incomplete hypotheses never increase.
82
q?
=?1
WANT
r q
?
=?8
WANT
> q
r
r
?
=?2
WANT
BELIEVE
r q
?
=?3
WANT
BELIEVE
q
BOY ?
=?7
WANT
BELIEVE
r
BOY ?
=?4
WANT
BELIEVE
BOY GIRL
Figure 10: Example derivation in ?generation mode?.
Dags are generated by taking the basic incomplete
dags (rule dags) defined by each rule and concate-
nating them using the dangling edges. Every dan-
gling edge of the rule dag can be identified with a
dangling edge of the current hypothesis (if the orien-
tation matches) or be left unconnected for later con-
nection. In that way, all children and parents for a
given node are eventually created. Strictly speaking,
the resulting structures are not dags anymore as they
can contain multiple > and ? symbols. A sample
generation is shown in Figure 10. Note how the or-
der of rules applied is different from the example in
Figure 6.
Using the dag acceptor as a generating device in
this way is unproblematic, but poses two challenges.
First, we have to avoid cyclicity, which is easily con-
firmed by keeping nodes topologically sorted.
Second, to avoid spurious ambiguity (where
derivations describe the same derivation dag, but
only differ by the order of rule application), spe-
cial care is needed. A simple solution is to sort the
edges in each incomplete dag to obtain a canonical
(?leftmost?) derivation. We start with the start state
(which has head rank 0). This is the first incomplete
dag that is pushed on the dag queue. Then we repeat-
edly pop an incomplete dag G from the dag queue.
The first unused edge e of G is then attached to a
new node v by identifying e with one of v?s edges
if the states are compatible. Remaining edges of the
new node (incoming or outgoing) can be identified
with other unused edges of G or left for later attach-
ment. The resulting dags are pushed onto the queue.
Whenever a dag has no unused edges, it is com-
plete and the corresponding derivation can be re-
turned. The generation process stops when k com-
plete derivations have been produced. This k-best
algorithm can also be used to generate tree output
for a dag-to-tree transducer, and by restricting the
shape of the output tree, for ?backward application?
(given a tree, which dags map to it?).
6 Future work
The work presented in this paper is being imple-
mented in a toolkit that will be made publicly avail-
able. Of course, there is a lot of room for improve-
ment, both from the theoretical and the practical
viewpoint. This is a brief list of items for future re-
search:
? Complexity analysis of the algorithms.
? Closure properties of dag acceptors and dag-
to-tree transducers as well as composition with
tree transducers.
? Investigate a reasonable probabilistic model
and training procedures.
? Extended left-hand sides to condition on a
larger semantic context, just like extended top-
down tree transducers (Maletti et al, 2009).
? Handling flat, unordered, sparse sets of rela-
tions that are typical of feature structures. Cur-
rently, rules are very specific to the number of
children and parents. A first step in this direc-
tion is given by implicit rules that can handle a
potentially arbitrary number of parents.
? Hand-annotated resources such as (dag, tree)
pairs, similar to treebanks for syntactic repre-
sentations.
Acknowledgements
This research was supported in part by ARO grant
W911NF-10-1-0533. The first author was supported
by the German Research Foundation (DFG) grant
MA 4959/1?1.
83
References
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970.
A maximization technique occurring in the statistical
analysis of probabilistic functions of Markov chains.
Ann. Math. Statist., 41(1):164171.
Bjo?rn Borchardt and Heiko Vogler. 2003. Determiniza-
tion of finite state weighted tree automata. J. Autom.
Lang. Comb., 8(3):417?463.
Francis Bossut and Bruno Warin. 1992. Automata and
pattern matching in planar directed acyclic graphs.
In Imre Simon, editor, Proc. LATIN, volume 583 of
LNCS, pages 76?86. Springer.
Francis Bossut, Max Dauchet, and Bruno Warin. 1988.
Automata and rational expressions on planar graphs.
In Michal Chytil, Ladislav Janiga, and Va?clav Koubek,
editors, Proc. MFCS, volume 324 of LNCS, pages
190?200. Springer.
Francis Bossut, Max Dauchet, and Bruno Warin. 1995.
A kleene theorem for a class of planar acyclic graphs.
Inf. Comput., 117(2):251?265.
Symeon Bozapalidis and Antonios Kalampakas. 2004.
An axiomatization of graphs. Acta Inf., 41(1):19?61.
Symeon Bozapalidis and Antonios Kalampakas. 2006.
Recognizability of graph and pattern languages. Acta
Inf., 42(8-9):553?581.
Symeon Bozapalidis and Antonios Kalampakas. 2008.
Graph automata. Theor. Comput. Sci., 393(1-3):147?
165.
H. Comon, M. Dauchet, R. Gilleron, C. Lo?ding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree automata techniques and appli-
cations. Available on: http://www.grappa.
univ-lille3.fr/tata. release October, 12th
2007.
Bruno Courcelle and Joost Engelfriet. 1995. A logical
characterization of the sets of hypergraphs defined by
hyperedge replacement grammars. Math. Syst. The-
ory, 28(6):515?552.
Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement, graph grammars.
In Grzegorz Rozenberg, editor, Handbook of Graph
Grammars, pages 95?162. World Scientific.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. ACL, pages
205?208. ACL.
Joost Engelfriet and Jan Joris Vereijken. 1997. Context-
free graph grammars and concatenation of graphs.
Acta Inf., 34(10):773?803.
David Eppstein. 1998. Finding the k shortest paths.
SIAM J. Comput., 28(2):652?673.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest, Hungary.
Joshua Goodman. 1999. Semiring parsing. Comput.
Linguist., 25:573?605.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391?427.
Jonathan Graehl. 1997. Carmel finite-state toolkit.
http://www.isi.edu/licensed-sw/
carmel.
Yvette Graham, Josef van Genabith, and Anton Bryl.
2009. F-structure transfer-based statistical machine
translation. In Proc. LFG.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. IWPT.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proc. SSPR/SPR, vol-
ume 1876 of LNCS, pages 183?192. Springer.
Tsutomu Kamimura and Giora Slutzki. 1981. Paral-
lel and two-way automata on directed ordered acyclic
graphs. Inf. Control, 49(1):10?51.
Tsutomu Kamimura and Giora Slutzki. 1982. Transduc-
tions of dags and trees. Math. Syst. Theory, 15(3):225?
249.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proc. CICLing, volume 3406 of LNCS,
pages 1?24. Springer.
Kevin Knight. 1989. Unification: A multidisciplinary
survey. ACM Comput. Surv., 21(1):93?124.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-down
tree transducers. SIAM J. Comput., 39(2):410?430.
Jonathan May and Kevin Knight. 2006a. A better n-best
list: Practical determinization of weighted finite tree
automata. In Proc. HLT-NAACL. ACL.
Jonathan May and Kevin Knight. 2006b. Tiburon: A
weighted tree automata toolkit. In Oscar H. Ibarra and
Hsu-Chun Yen, editors, Proc. CIAA, volume 4094 of
LNCS, pages 102?113. Springer.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The design principles of a weighted
finite-state transducer library. Theor. Comput. Sci.,
231(1):17?32.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269?311.
Robert C. Moore. 1989. Unification-based semantic in-
terpretation. In Proc. ACL, pages 33?41. ACL.
NIST. 2009. NIST Open Machine Translation 2009
Evaluation (MT09). http://www.itl.nist.
gov/iad/mig/tests/mt/2009/.
84
Fernando Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In Finite-State Language Processing, pages
431?453. MIT Press.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. OpenFST: An open-source, weighted finite-
state transducer library and its applications to speech
and language. In Ciprian Chelba, Paul B. Kantor, and
Brian Roark, editors, Proc. HLT-NAACL (Tutorial Ab-
stracts), pages 9?10. ACL.
William C. Rounds and Robert T. Kasper. 1986. A com-
plete logical calculus for record structures representing
linguistic information. In Proc. LICS, pages 38?43.
IEEE Computer Society.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. J. Log. Program., 24(1&2):3?36.
Stuart M. Shieber. 1986. An Introduction to Unification-
Based Approaches to Grammar, volume 4 of CSLI
Lecture Notes. CSLI Publications, Stanford, CA.
Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE Transactions on Information Theory,
13(2):260?269.
85
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 40?44,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
DAGGER: A Toolkit for Automata on Directed Acyclic Graphs
Daniel Quernheim
Institute for Natural Language Processing
Universita?t Stuttgart, Germany
Pfaffenwaldring 5b, 70569 Stuttgart
daniel@ims.uni-stuttgart.de
Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
knight@isi.edu
Abstract
This paper presents DAGGER, a toolkit for
finite-state automata that operate on directed
acyclic graphs (dags). The work is based on a
model introduced by (Kamimura and Slutzki,
1981; Kamimura and Slutzki, 1982), with a
few changes to make the automata more ap-
plicable to natural language processing. Avail-
able algorithms include membership checking
in bottom-up dag acceptors, transduction of
dags to trees (bottom-up dag-to-tree transduc-
ers), k-best generation and basic operations
such as union and intersection.
1 Introduction
Finite string automata and finite tree automata have
proved to be useful tools in various areas of natural
language processing (Knight and May, 2009). How-
ever, some applications, especially in semantics, re-
quire graph structures, in particular directed acyclic
graphs (dags), to model reentrancies. For instance,
the dags in Fig. 1 represents the semantics of the sen-
tences ?The boy wants to believe the girl? and ?The
boy wants the girl to believe him.? The double role
of ?the boy? is made clear by the two parent edges of
the BOY node, making this structure non-tree-like.
Powerful graph rewriting systems have been used
for NLP (Bohnet and Wanner, 2010), yet we con-
sider a rather simple model: finite dag automata that
have been introduced by (Kamimura and Slutzki,
1981; Kamimura and Slutzki, 1982) as a straight-
forward extension of tree automata. We present the
toolkit DAGGER (written in PYTHON) that can be
used to visualize dags and to build dag acceptors
(a)
WANT
BELIEVE
BOY GIRL (b)
WANT
BELIEVE
BOY GIRL
Figure 1: (a) ?The boy wants to believe the girl.? and
(b) ?The boy wants the girl to believe him.? First edge
represents :agent role, second edge represents :patient
role.
and dag-to-tree transducers similar to their model.
Compared to those devices, in order to use them for
actual NLP tasks, our machines differ in certain as-
pects:
? We do not require our dags to be planar, and we
do not only consider derivation dags.
? We add weights from any commutative semir-
ing, e.g. real numbers.
The toolkit is available under an open source li-
cence.1
2 Dags and dag acceptors
DAGGER comes with a variety of example dags and
automata. Let us briefly illustrate some of them. The
dag of Fig. 1(a) can be defined in a human-readable
format called PENMAN (Bateman, 1990):
(1 / WANT
:agent (2 / BOY)
:patient (3 / BELIEVE
:agent 2
:patient (4 / GIRL)))
1http://www.ims.uni-stuttgart.de/
?daniel/dagger/
40
s
s -> (WANT :agent i :patient s)
s -> (BELIEVE :agent i :patient s)
i -> (0)
s -> (0)
i -> (GIRL)
i -> (BOY)
s -> (GIRL)
s -> (BOY)
i i -> (GIRL)
i i -> (BOY)
i s -> (GIRL)
i s -> (BOY)
s s -> (GIRL)
s s -> (BOY)
Figure 2: Example dag acceptor example.bda.
In this format, every node has a unique identifier,
and edge labels start with a colon. The tail node of
an edge is specified as a whole subdag, or, in the
case of a reentrancy, is referred to with its identifier.
Fig. 2 shows a dag acceptor. The first line con-
tains the final state, and the remaining lines contain
rules. Mind that the rules are written in a top-down
fashion, but are evaluated bottom-up for now. Let us
consider a single rule:
s -> (WANT :agent i :patient s)
The right-hand side is a symbol (WANT :agent
:patient) whose tail edges are labeled with states (i
and s), and after applying the rule, its head edges are
labeled with new states (s). All rules are height one,
but in the future we will allow for larger subgraphs.
In order to deal with symbols of arbitrary head
rank (i.e. symbols that can play multiple roles), we
can use rules using special symbols such as 2=1 and
3=1 that split one edge into more than one:
i s -> (2=1 :arg e)
Using these state-changing rules, the ruleset can
be simplified (see Fig. 3), however the dags look a
bit different now:
(1 / WANT
:agent (2 / 2=1
:arg (3 / BOY))
:patient (4 / BELIEVE
:agent 2
:patient (5 / GIRL)))
Note that we also added weights to the ruleset now.
Weights are separated from the rest of a rule by the @
sign. The weight semantics is the usual one, where
weights are multiplied along derivation steps, while
the weights of alternative derivations are added.
s
s -> (WANT :agent i :patient s) @ 0.6
s -> (BELIEVE :agent i :patient s) @ 0.4
i -> (0) @ 0.2
s -> (0) @ 0.4
i -> (GIRL) @ 0.3
s -> (GIRL) @ 0.3
i -> (BOY) @ 0.2
s -> (BOY) @ 0.2
i i -> (2=1 :arg e) @ 0.3
i s -> (2=1 :arg e) @ 0.3
s s -> (2=1 :arg e) @ 0.3
e -> (GIRL) @ 0.4
e -> (BOY) @ 0.6
Figure 3: Simplified dag acceptor simple.bda.
2.1 Membership checking and derivation
forests
DAGGER is able to perform various operations on
dags. The instructions can be given in a simple ex-
pression language. The general format of an expres-
sion is:
(command f1 .. fm p1 .. pn)
Every command has a number of (optional) features
fi and a fixed number of arguments pi. Most com-
mands have a short and a long name; we will use the
short names here to save space. In order to evaluate
a expression, you can either
? supply it on the command-line:
./dagger.py -e EXPRESSION
? or read from a file:
./dagger.py -f FILE
We will now show a couple of example expres-
sions that are composed of smaller expressions.
Assume that the dag acceptor of Fig. 2 is saved
in the file example.bda, and the file boywants.dag
contains the example dag in PENMAN format.
We can load the dag with the expression (g (f
boywants.dag)), and the acceptor with the expres-
sion (a w (f example.bda)) where w means that the
acceptor is weighted. We could also specify the dag
directly in PENMAN format using p instead of f. We
can use the command r:
(r (a w (f example.bda)) (g (f
boywants.dag)))
to check whether example.bda recognizes
boywants.dag. This will output one list item
41
qWANT
BELIEVE
BOY GIRL
=?
S
qnomb wants qinfb
BELIEVE
BOY GIRL
=?
S
qnomb wants INF
qaccg to believe qaccb
BOY GIRL
=?
S
INF
NP NP NP
the boy wants the girl to believe him
Figure 4: Derivation from graph to tree ?the boy wants the girl to believe him?.
q
q.S(x1 wants x2)) -> (WANT :agent nomb.x1 :patient inf.x2)
inf.INF(x1 to believe x2) -> (BELIEVE :agent accg.x1 :patient accb.x2)
accg.NP(the girl) -> (GIRL)
nomb.NP(the boy) accb.(him) -> (BOY)
Figure 5: Example dag-to-tree-transducer example.bdt.
for each successful derivation (and, if the acceptor
is weighted, their weights), in this case: (?s?,
?0.1?, 0, ?0?), which means that the acceptor can
reach state s with a derivation weighted 0.1. The
rest of the output concerns dag-to-tree transducers
and will be explained later.
Note that in general, there might be multiple
derivations due to ambiguity (non-determinism).
Fortunately, the whole set of derivations can be effi-
ciently represented as another dag acceptor with the
d command. This derivation forest acceptor has the
set of rules as its symbol and the set of configura-
tions (state-labelings of the input dag) as its state set.
(d (a w (f example.bda)) (g f
boywants.dag)))
will write the derivation forest acceptor to the stan-
dard output.
2.2 k-best generation
To obtain the highest-weighted 7 dags generated by
the example dag acceptor, run:
(k 7 (a w (f example.bda)))
(1 / BOY)
(1 / GIRL)
(1 / BELIEVE :agent (2 / GIRL) :patient 2)
(1 / WANT :agent (2 / GIRL) :patient 2)
(1 / 0)
(1 / BELIEVE :agent (2 / BOY) :patient 2)
(1 / WANT :agent (2 / BOY) :patient 2)
If the acceptor is unweighted, the smallest dags
(in terms of derivation steps) are returned.
(1 / 0)
(1 / BOY)
(1 / GIRL)
(1 / BELIEVE :agent (2 / GIRL) :patient 2)
(1 / BELIEVE :agent (2 / BOY) :patient 2)
(1 / BELIEVE :agent (2 / GIRL) :patient
(3 / 0))
(1 / BELIEVE :agent (2 / GIRL) :patient
(3 / GIRL))
2.3 Visualization of dags
Both dags and dag acceptors can be visualized using
GRAPHVIZ2. For this purpose, we use the q (query)
command and the v feature:
(v (g (f boywants.dag)) boywants.pdf)
(v (a (f example.bda)) example.pdf)
Dag acceptors are represented as hypergraphs,
where the nodes are the states and each hyperedge
represents a rule labeled with a symbol.
2.4 Union and intersection
In order to construct complex acceptors from sim-
pler building blocks, it is helpful to make use of
union (u) and intersection (i). The following code
will intersect two acceptors and return the 5 best
dags of the intersection acceptor.
(k 5 (i (a (f example.bda)) (a (f
someother.bda))))
Weighted union, as usual, corresponds to sum,
weighted intersection to product.
2available under the Eclipse Public Licence from http://
www.graphviz.org/
42
string automata tree automata dag automata
compute . . . strings (sentences) . . . (syntax) trees . . . semantic representations
k-best . . . paths through a WFSA (Viterbi,
1967; Eppstein, 1998)
. . . derivations in a weighted forest
(Jime?nez and Marzal, 2000; Huang and
Chiang, 2005)
3
EM training Forward-backward EM (Baum et al,
1970; Eisner, 2003)
Tree transducer EM training (Graehl et
al., 2008)
?
Determinization . . . of weighted string acceptors (Mohri,
1997)
. . . of weighted tree acceptors (Bor-
chardt and Vogler, 2003; May and
Knight, 2006a)
?
Transducer composi-
tion
WFST composition (Pereira and Riley,
1997)
Many transducers not closed under com-
position (Maletti et al, 2009)
?
General tools AT&T FSM (Mohri et al, 2000),
Carmel (Graehl, 1997), OpenFST (Riley
et al, 2009)
Tiburon (May and Knight, 2006b),
ForestFIRE (Cleophas, 2008; Strolen-
berg, 2007)
DAGGER
Table 1: General-purpose algorithms for strings, trees and feature structures.
3 Dag-to-tree transducers
Dag-to-tree transducers are dag acceptors with tree
output. In every rule, the states on the right-hand
sides have tree variables attached that are used to
build one tree for each state on the left-hand side. A
fragment of an example dag-to-tree transducer can
be seen in Fig. 5.
Let us see what happens if we apply this trans-
ducer to our example dag:
(r (a t (f example.bdt)) (g (f
boywants.dag)))
All derivations including output trees will be listed:
(?q?, ?1.0?,
S(NP(the boy) wants INF(NP(the girl)
to believe NP(him))),
?the boy wants the girl to believe
him?)
A graphical representation of this derivation (top-
down instead of bottom-up for illustrative purposes)
can be seen in Fig. 4.
3.1 Backward application and force decoding
Sometimes, we might want to see which dags map
to a certain input tree in a dag-to-tree transducer.
This is called backward application since we use the
transducer in the reverse direction: We are currently
implementing this by ?generation and checking?, i.e.
a process that generates dags and trees at the same
time. Whenever a partial tree does not match the
input tree, it is discarded, until we find a derivation
and a dag for the input tree. If we also restrict the
dag part, we have force decoding.
4 Future work
This work describes the basics of a dag automata
toolkit. To the authors? knowledge, no such im-
plementation already exists. Of course, many algo-
rithms are missing, and there is a lot of room for im-
provement, both from the theoretical and the practi-
cal viewpoint. This is a brief list of items for future
research (Quernheim and Knight, 2012):
? Complexity analysis of the algorithms.
? Closure properties of dag acceptors and dag-
to-tree transducers as well as composition with
tree transducers.
? Extended left-hand sides to condition on a
larger semantic context, just like extended top-
down tree transducers (Maletti et al, 2009).
? Handling flat, unordered, sparse sets of rela-
tions that are typical of feature structures. Cur-
rently, rules are specific to the rank of the
nodes. A first step in this direction could be
gone by getting rid of the explicit n=m symbols.
? Hand-annotated resources such as (dag, tree)
pairs, similar to treebanks for syntactic repre-
sentations as well as a reasonable probabilistic
model and training procedures.
? Useful algorithms for NLP applications that
exist for string and tree automata (cf. Ta-
ble 1). The long-term goal could be to build a
semantics-based machine translation pipeline.
Acknowledgements
This research was supported in part by ARO grant W911NF-10-
1-0533. The first author was supported by the German Research
Foundation (DFG) grant MA 4959/1?1.
43
References
John A. Bateman. 1990. Upper modeling: organizing
knowledge for natural language processing. In Proc.
Natural Language Generation Workshop, pages 54?
60.
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970.
A maximization technique occurring in the statistical
analysis of probabilistic functions of Markov chains.
Ann. Math. Statist., 41(1):164171.
Bernd Bohnet and Leo Wanner. 2010. Open source
graph transducer interpreter and grammar develop-
ment environment. In Proc. LREC.
Bjo?rn Borchardt and Heiko Vogler. 2003. Determiniza-
tion of finite state weighted tree automata. J. Autom.
Lang. Comb., 8(3):417?463.
Loek G. W. A. Cleophas. 2008. Tree Algorithms: Two
Taxonomies and a Toolkit. Ph.D. thesis, Department of
Mathematics and Computer Science, Eindhoven Uni-
versity of Technology.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. ACL, pages
205?208.
David Eppstein. 1998. Finding the k shortest paths.
SIAM J. Comput., 28(2):652?673.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391?427.
Jonathan Graehl. 1997. Carmel finite-state toolkit.
http://www.isi.edu/licensed-sw/
carmel.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. IWPT.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proc. SSPR/SPR, pages
183?192.
Tsutomu Kamimura and Giora Slutzki. 1981. Paral-
lel and two-way automata on directed ordered acyclic
graphs. Inf. Control, 49(1):10?51.
Tsutomu Kamimura and Giora Slutzki. 1982. Transduc-
tions of dags and trees. Math. Syst. Theory, 15(3):225?
249.
Kevin Knight and Jonathan May. 2009. Applications
of weighted automata in natural language processing.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata. Springer.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-down
tree transducers. SIAM J. Comput., 39(2):410?430.
Jonathan May and Kevin Knight. 2006a. A better n-best
list: Practical determinization of weighted finite tree
automata. In Proc. HLT-NAACL.
Jonathan May and Kevin Knight. 2006b. Tiburon: A
weighted tree automata toolkit. In Oscar H. Ibarra and
Hsu-Chun Yen, editors, Proc. CIAA, volume 4094 of
LNCS, pages 102?113. Springer.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The design principles of a weighted
finite-state transducer library. Theor. Comput. Sci.,
231(1):17?32.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269?311.
Fernando Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In Finite-State Language Processing, pages
431?453. MIT Press.
Daniel Quernheim and Kevin Knight. 2012. To-
wards probabilistic acceptors and transducers for fea-
ture structures. In Proc. SSST. (to appear).
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. OpenFST: An open-source, weighted finite-
state transducer library and its applications to speech
and language. In Proc. HLT-NAACL (Tutorial Ab-
stracts), pages 9?10.
Roger Strolenberg. 2007. ForestFIRE and FIREWood.
a toolkit & GUI for tree algorithms. Master?s thesis,
Department of Mathematics and Computer Science,
Eindhoven University of Technology.
Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE Transactions on Information Theory,
13(2):260?269.
44
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 178?186,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Abstract Meaning Representation for Sembanking
Laura Banarescu
SDL
lbanarescu@sdl.com
Claire Bonial
Linguistics Dept.
Univ. Colorado
claire.bonial@colorado.edu
Shu Cai
ISI
USC
shucai@isi.edu
Madalina Georgescu
SDL
mgeorgescu@sdl.com
Kira Griffitt
LDC
kiragrif@ldc.upenn.edu
Ulf Hermjakob
ISI
USC
ulf@isi.edu
Kevin Knight
ISI
USC
knight@isi.edu
Philipp Koehn
School of Informatics
Univ. Edinburgh
pkoehn@inf.ed.ac.uk
Martha Palmer
Linguistics Dept.
Univ. Colorado
martha.palmer@colorado.edu
Nathan Schneider
LTI
CMU
nschneid@cs.cmu.edu
Abstract
We describe Abstract Meaning Represen-
tation (AMR), a semantic representation
language in which we are writing down
the meanings of thousands of English sen-
tences. We hope that a sembank of simple,
whole-sentence semantic structures will
spur new work in statistical natural lan-
guage understanding and generation, like
the Penn Treebank encouraged work on
statistical parsing. This paper gives an
overview of AMR and tools associated
with it.
1 Introduction
Syntactic treebanks have had tremendous impact
on natural language processing. The Penn Tree-
bank is a classic example?a simple, readable file
of natural-language sentences paired with rooted,
labeled syntactic trees. Researchers have ex-
ploited manually-built treebanks to build statisti-
cal parsers that improve in accuracy every year.
This success is due in part to the fact that we have
a single, whole-sentence parsing task, rather than
separate tasks and evaluations for base noun iden-
tification, prepositional phrase attachment, trace
recovery, verb-argument dependencies, etc. Those
smaller tasks are naturally solved as a by-product
of whole-sentence parsing, and in fact, solved bet-
ter than when approached in isolation.
By contrast, semantic annotation today is balka-
nized. We have separate annotations for named en-
tities, co-reference, semantic relations, discourse
connectives, temporal entities, etc. Each annota-
tion has its own associated evaluation, and training
data is split across many resources. We lack a sim-
ple readable sembank of English sentences paired
with their whole-sentence, logical meanings. We
believe a sizable sembank will lead to new work in
statistical natural language understanding (NLU),
resulting in semantic parsers that are as ubiquitous
as syntactic ones, and support natural language
generation (NLG) by providing a logical seman-
tic input.
Of course, when it comes to whole-sentence se-
mantic representations, linguistic and philosophi-
cal work is extensive. We draw on this work to de-
sign an Abstract Meaning Representation (AMR)
appropriate for sembanking. Our basic principles
are:
? AMRs are rooted, labeled graphs that are
easy for people to read, and easy for pro-
grams to traverse.
? AMR aims to abstract away from syntac-
tic idiosyncrasies. We attempt to assign the
same AMR to sentences that have the same
basic meaning. For example, the sentences
?he described her as a genius?, ?his descrip-
tion of her: genius?, and ?she was a ge-
nius, according to his description? are all as-
signed the same AMR.
? AMR makes extensive use of PropBank
framesets (Kingsbury and Palmer, 2002;
Palmer et al, 2005). For example, we rep-
resent a phrase like ?bond investor? using
the frame ?invest-01?, even though no verbs
appear in the phrase.
? AMR is agnostic about how we might want
to derive meanings from strings, or vice-
versa. In translating sentences to AMR, we
do not dictate a particular sequence of rule
applications or provide alignments that re-
flect such rule sequences. This makes sem-
banking very fast, and it allows researchers
to explore their own ideas about how strings
178
are related to meanings.
? AMR is heavily biased towards English. It
is not an Interlingua.
AMR is described in a 50-page annotation guide-
line.1 In this paper, we give a high-level descrip-
tion of AMR, with examples, and we also provide
pointers to software tools for evaluation and sem-
banking.
2 AMR Format
We write down AMRs as rooted, directed, edge-
labeled, leaf-labeled graphs. This is a com-
pletely traditional format, equivalent to the sim-
plest forms of feature structures (Shieber et al,
1986), conjunctions of logical triples, directed
graphs, and PENMAN inputs (Matthiessen and
Bateman, 1991). Figure 1 shows some of these
views for the sentence ?The boy wants to go?. We
use the graph notation for computer processing,
and we adapt the PENMAN notation for human
reading and writing.
3 AMR Content
In neo-Davidsonian fashion (Davidson, 1969), we
introduce variables (or graph nodes) for entities,
events, properties, and states. Leaves are labeled
with concepts, so that ?(b / boy)? refers to an in-
stance (called b) of the concept boy. Relations link
entities, so that ?(d / die-01 :location (p / park))?
means there was a death (d) in the park (p). When
an entity plays multiple roles in a sentence, we
employ re-entrancy in graph notation (nodes with
multiple parents) or variable re-use in PENMAN
notation.
AMR concepts are either English words
(?boy?), PropBank framesets (?want-01?), or spe-
cial keywords. Keywords include special entity
types (?date-entity?, ?world-region?, etc.), quan-
tities (?monetary-quantity?, ?distance-quantity?,
etc.), and logical conjunctions (?and?, etc).
AMR uses approximately 100 relations:
? Frame arguments, following PropBank
conventions. :arg0, :arg1, :arg2, :arg3, :arg4,
:arg5.
? General semantic relations. :accompa-
nier, :age, :beneficiary, :cause, :compared-to,
:concession, :condition, :consist-of, :degree,
:destination, :direction, :domain, :duration,
1AMR guideline: amr.isi.edu/language.html
LOGIC format:
? w, b, g:
instance(w, want-01) ? instance(g, go-01) ?
instance(b, boy) ? arg0(w, b) ?
arg1(w, g) ? arg0(g, b)
AMR format (based on PENMAN):
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
GRAPH format:
Figure 1: Equivalent formats for representating
the meaning of ?The boy wants to go?.
:employed-by, :example, :extent, :frequency,
:instrument, :li, :location, :manner, :medium,
:mod, :mode, :name, :part, :path, :polarity,
:poss, :purpose, :source, :subevent, :subset,
:time, :topic, :value.
? Relations for quantities. :quant, :unit,
:scale.
? Relations for date-entities. :day, :month,
:year, :weekday, :time, :timezone, :quarter,
:dayperiod, :season, :year2, :decade, :cen-
tury, :calendar, :era.
? Relations for lists. :op1, :op2, :op3, :op4,
:op5, :op6, :op7, :op8, :op9, :op10.
AMR also includes the inverses of all these rela-
tions, e.g., :arg0-of, :location-of, and :quant-of. In
addition, every relation has an associated reifica-
tion, which is what we use when we want to mod-
ify the relation itself. For example, the reification
of :location is the concept ?be-located-at-91?.
Our set of concepts and relations is designed to
allow us represent all sentences, taking all words
into account, in a reasonably consistent manner. In
the rest of this section, we give examples of how
AMR represents various kinds of words, phrases,
and sentences. For full documentation, the reader
is referred to the AMR guidelines.
179
Frame arguments. We make heavy use of
PropBank framesets to abstract away from English
syntax. For example, the frameset ?describe-01?
has three pre-defined slots (:arg0 is the describer,
:arg1 is the thing described, and :arg2 is what it is
being described as).
(d / describe-01
:arg0 (m / man)
:arg1 (m2 / mission)
:arg2 (d / disaster))
The man described the mission as a disaster.
The man?s description of the mission:
disaster.
As the man described it, the mission was a
disaster.
Here, we do not annotate words like ?as? or ?it?,
considering them to be syntactic sugar.
General semantic relations. AMR also in-
cludes many non-core relations, such as :benefi-
ciary, :time, and :destination.
(s / hum-02
:arg0 (s2 / soldier)
:beneficiary (g / girl)
:time (w / walk-01
:arg0 g
:destination (t / town)))
The soldier hummed to the girl as she
walked to town.
Co-reference. AMR abstracts away from co-
reference gadgets like pronouns, zero-pronouns,
reflexives, control structures, etc. Instead we re-
use AMR variables, as with ?g? above. AMR
annotates sentences independent of context, so if
a pronoun has no antecedent in the sentence, its
nominative form is used, e.g., ?(h / he)?.
Inverse relations. We obtain rooted structures
by using inverse relations like :arg0-of and :quant-
of.
(s / sing-01
:arg0 (b / boy
:source (c / college)))
The boy from the college sang.
(b / boy
:arg0-of (s / sing-01)
:source (c / college))
the college boy who sang ...
(i / increase-01
:arg1 (n / number
:quant-of (p / panda)))
The number of pandas increased.
The top-level root of an AMR represents the fo-
cus of the sentence or phrase. Once we have se-
lected the root concept for an entire AMR, there
are no more focus considerations?everything else
is driven strictly by semantic relations.
Modals and negation. AMR represents nega-
tion logically with :polarity, and it expresses
modals with concepts.
(g / go-01
:arg0 (b / boy)
:polarity -)
The boy did not go.
(p / possible
:domain (g / go-01
:arg0 (b / boy))
:polarity -))
The boy cannot go.
It?s not possible for the boy to go.
(p / possible
:domain (g / go-01
:arg0 (b / boy)
:polarity -))
It?s possible for the boy not to go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy))
:polarity -)
The boy doesn?t have to go.
The boy isn?t obligated to go.
The boy need not go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy)
:polarity -))
The boy must not go.
It?s obligatory that the boy not go.
(t / think-01
:arg0 (b / boy)
:arg1 (w / win-01
:arg0 (t / team)
:polarity -))
The boy doesn?t think the team will win.
The boy thinks the team won?t win.
Questions. AMR uses the concept ?amr-
unknown?, in place, to indicate wh-questions.
(f / find-01
:arg0 (g / girl)
:arg1 (a / amr-unknown))
What did the girl find?
(f / find-01
:arg0 (g / girl)
:arg1 (b / boy)
:location (a / amr-unknown))
Where did the girl find the boy?
180
(f / find-01
:arg0 (g / girl)
:arg1 (t / toy
:poss (a / amr-unknown)))
Whose toy did the girl find?
Yes-no questions, imperatives, and embedded wh-
clauses are treated separately with the AMR rela-
tion :mode.
Verbs. Nearly every English verb and verb-
particle construction we have encountered has a
corresponding PropBank frameset.
(l / look-05
:arg0 (b / boy)
:arg1 (a / answer))
The boy looked up the answer.
The boy looked the answer up.
AMR abstracts away from light-verb construc-
tions.
(a / adjust-01
:arg0 (g / girl)
:arg1 (m / machine))
The girl adjusted the machine.
The girl made adjustments to the machine.
Nouns.We use PropBank verb framesets to rep-
resent many nouns as well.
(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy?s destruction of the room ...
The boy destroyed the room.
We never say ?destruction-01? in AMR. Some
nominalizations refer to a whole event, while oth-
ers refer to a role player in an event.
(s / see-01
:arg0 (j / judge)
:arg1 (e / explode-01))
The judge saw the explosion.
(r / read-01
:arg0 (j / judge)
:arg1 (t / thing
:arg1-of (p / propose-01))
The judge read the proposal.
(t / thing
:arg1-of (o / opine-01
:arg0 (g / girl)))
the girl?s opinion
the opinion of the girl
what the girl opined
Many ?-er? nouns invoke PropBank framesets.
This enables us to make use of slots defined for
those framesets.
(p / person
:arg0-of (i / invest-01))
investor
(p / person
:arg0-of (i / invest-01
:arg1 (b / bond)))
bond investor
(p / person
:arg0-of (i / invest-01
:manner (s / small)))
small investor
(w / work-01
:arg0 (b / boy)
:manner (h / hard))
the boy is a hard worker
the boy works hard
However, a treasurer is not someone who trea-
sures, and a president is not (just) someone who
presides.
Adjectives. Various adjectives invoke Prop-
Bank framesets.
(s / spy
:arg0-of (a / attract-01))
the attractive spy
(s / spy
:arg0-of (a / attract-01
:arg1 (w / woman)))
the spy who is attractive to women
?-ed? adjectives frequently invoke verb framesets.
For example, ?acquainted with magic? maps to
?acquaint-01?. However, we are not restricted to
framesets that can be reached through morpholog-
ical simplification.
(f / fear-01
:arg0 (s / soldier)
:arg1 (b / battle-01))
The soldier was afraid of battle.
The soldier feared battle.
The soldier had a fear of battle.
For other adjectives, we have defined new frame-
sets.
(r / responsible-41
:arg1 (b / boy)
:arg2 (w / work))
The boy is responsible for the work.
The boy has responsibility for the work.
While ?the boy responsibles the work? is not good
English, it is perfectly good Chinese. Similarly,
we handle tough-constructions logically.
181
(t / tough
:domain (p / please-01
:arg1 (g / girl)))
Girls are tough to please.
It is tough to please girls.
Pleasing girls is tough.
?please-01? and ?girl? are adjacent in the AMR,
even if they are not adjacent in English. ?-able?
adjectives often invoke the AMR concept ?possi-
ble?, but not always (e.g., a ?taxable fund? is actu-
ally a ?taxed fund?).
(s / sandwich
:arg1-of (e / eat-01
:domain-of (p / possible)))
an edible sandwich
(f / fund
:arg1-of (t / tax-01))
a taxable fund
Pertainym adjectives are normalized to root form.
(b / bomb
:mod (a / atom))
atom bomb
atomic bomb
Prepositions. Most prepositions simply sig-
nal semantic frame elements, and are themselves
dropped from AMR.
(d / default-01
:arg1 (n / nation)
:time (d2 / date-entity
:month 6))
The nation defaulted in June.
Time and location prepositions are kept if they
carry additional information.
(d / default-01
:arg1 (n / nation)
:time (a / after
:op1 (w / war-01))
The nation defaulted after the war.
Occasionally, neither PropBank nor AMR has an
appropriate relation, in which case we hold our
nose and use a :prep-X relation.
(s / sue-01
:arg1 (m / man)
:prep-in (c / case))
The man was sued in the case.
Named entities. Any concept in AMR can be
modified with a :name relation. However, AMR
includes standardized forms for approximately 80
named-entity types, including person, country,
sports-facility, etc.
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown"))
Mollie Brown
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown")
:arg0-of (s / slay-01
:arg1 (o / orc)))
the orc-slaying Mollie Brown
Mollie Brown, who slew orcs
AMR does not normalize multiple ways of re-
ferring to the same concept (e.g., ?US? versus
?United States?). It also avoids analyzing seman-
tic relations inside a named entity?e.g., an orga-
nization named ?Stop Malaria Now? does not in-
voke the ?stop-01? frameset. AMR gives a clean,
uniform treatment to titles, appositives, and other
constructions.
(c / city
:name (n / name
:op1 "Zintan"))
Zintan
the city of Zintan
(p / president
:name (n / name
:op1 "Obama"))
President Obama
Obama, the president ...
(g / group
:name (n / name
:op1 "Elsevier"
:op2 "N.V.")
:mod (c / country
:name (n2 / name
:op1 "Netherlands"))
:arg0-of (p / publish-01))
Elsevier N.V., the Dutch publishing group...
Dutch publishing group Elsevier N.V. ...
Copula. Copulas use the :domain relation.
(w / white
:domain (m / marble))
The marble is white.
(l / lawyer
:domain (w / woman))
The woman is a lawyer.
(a / appropriate
:domain (c / comment)
:polarity -))
The comment is not appropriate.
182
The comment is inappropriate.
Reification. Sometimes we want to use an
AMR relation as a first-class concept?to be able
to modify it, for example. Every AMR relation has
a corresponding reification for this purpose.
(m / marble
:location (j / jar))
the marble in the jar ...
(b / be-located-at-91
:arg1 (m / marble)
:arg2 (j / jar)
:polarity -)
:time (y / yesterday))
The marble was not in the jar yesterday.
If we do not use the reification, we run into trou-
ble.
(m / marble
:location (j / jar
:polarity -)
:time (y / yesterday))
yesterday?s marble in the non-jar ...
Some reifications are standard PropBank frame-
sets (e.g., ?cause-01? for :cause, or ?age-01? for
:age).
This ends the summary of AMR content. For
lack of space, we omit descriptions of compara-
tives, superlatives, conjunction, possession, deter-
miners, date entities, numbers, approximate num-
bers, discourse connectives, and other phenomena
covered in the full AMR guidelines.
4 Limitations of AMR
AMR does not represent inflectional morphology
for tense and number, and it omits articles. This
speeds up the annotation process, and we do not
have a nice semantic target representation for these
phenomena. A lightweight syntactic-style repre-
sentation could be layered in, via an automatic
post-process.
AMR has no universal quantifier. Words like
?all? modify their head concepts. AMR does not
distinguish between real events and hypothetical,
future, or imagined ones. For example, in ?the boy
wants to go?, the instances of ?want-01? and ?go-
01? have the same status, even though the ?go-01?
may or may not happen.
We represent ?history teacher? nicely as ?(p /
person :arg0-of (t / teach-01 :arg1 (h / history)))?.
However, ?history professor? becomes ?(p / pro-
fessor :mod (h / history))?, because ?profess-01?
is not an appropriate frame. It would be reason-
able in such cases to use a NomBank (Meyers et
al., 2004) noun frame with appropriate slots.
5 Creating AMRs
We have developed a power editor for AMR, ac-
cessible by web interface.2 The AMR Editor al-
lows rapid, incremental AMR construction via text
commands and graphical buttons. It includes on-
line documentation of relations, quantities, reifi-
cations, etc., with full examples. Users log in,
and the editor records AMR activity. The ed-
itor also provides significant guidance aimed at
increasing annotator consistency. For example,
users are warned about incorrect relations, discon-
nected AMRs, words that have PropBank frames,
etc. Users can also search existing sembanks for
phrases to see how they were handled in the past.
The editor also allows side-by-side comparison of
AMRs from different users, for training purposes.
In order to assess inter-annotator agreement
(IAA), as well as automatic AMR parsing accu-
racy, we developed the smatch metric (Cai and
Knight, 2013) and associated script.3 Smatch re-
ports the semantic overlap between two AMRs by
viewing each AMR as a conjunction of logical
triples (see Figure 1). Smatch computes precision,
recall, and F-score of one AMR?s triples against
the other?s. To match up variables from two in-
put AMRs, smatch needs to execute a brief search,
looking for the variable mapping that yields the
highest F-score.
Smatch makes no reference to English strings
or word indices, as we do not enforce any par-
ticular string-to-meaning derivation. Instead, we
compare semantic representations directly, in the
same way that the MT metric Bleu (Papineni et
al., 2002) compares target strings without making
reference to the source.
For an initial IAA study, and prior to adjust-
ing the AMR Editor to encourage consistency, 4
expert AMR annotators annotated 100 newswire
sentences and 80 web text sentences. They then
created consensus AMRs through discussion. The
average annotator vs. consensus IAA (smatch) was
0.83 for newswire and 0.79 for web text. When
newly trained annotators doubly annotated 382
web text sentences, their annotator vs. annotator
IAA was 0.71.
2AMR Editor: amr.isi.edu/editor.html
3Smatch: amr.isi.edu/evaluation.html
183
6 Current AMR Bank
We currently have a manually-constructed AMR
bank of several thousand sentences, a subset of
which can be freely downloaded,4 the rest being
distributed via the LDC catalog.
In initially developing AMR, the authors built
consensus AMRs for:
? 225 short sentences for tutorial purposes
? 142 sentences of newswire (*)
? 100 sentences of web data (*)
Trained annotators at LDC then produced AMRs
for:
? 1546 sentences from the novel ?The Little
Prince?
? 1328 sentences of web data
? 1110 sentences of web data (*)
? 926 sentences from Xinhua news (*)
? 214 sentences from CCTV broadcast con-
versation (*)
Collections marked with a star (*) are also in
the OntoNotes corpus (Pradhan et al, 2007;
Weischedel et al, 2011).
Using the AMR Editor, annotators are able to
translate a full sentence into AMR in 7-10 minutes
and postedit an AMR in 1-3 minutes.
7 Related Work
Researchers working on whole-sentence semantic
parsing today typically use small, domain-specific
sembanks like GeoQuery (Wong and Mooney,
2006). The need for larger, broad-coverage sem-
banks has sparked several projects, including the
Groningen Meaning Bank (GMB) (Basile et al,
2012a), UCCA (Abend and Rappoport, 2013),
the Semantic Treebank (ST) (Butler and Yoshi-
moto, 2012), the Prague Dependency Treebank
(Bo?hmova? et al, 2003), and UNL (Uchida et al,
1999; Uchida et al, 1996; Martins, 2012).
Concepts. Most systems use English words
as concepts. AMR uses PropBank frames (e.g.,
?describe-01?), and UNL uses English WordNet
synsets (e.g., ?200752493?).
Relations. GMB uses VerbNet roles (Schuler,
2005), and AMR uses frame-specific PropBank
relations. UNL has a dedicated set of over 30 fre-
quently used relations.
Formalism. GMB meanings are written in
DRT (Kamp et al, 2011), exploiting full first-
4amr.isi.edu/download.html
order logic. GMB and ST both include universal
quantification.
Granularity. GMB and UCCA annotate short
texts, so that the same entity can participate in
events described in different sentences; other sys-
tems annotate individual sentences.
Entities. AMR uses 80 entity types, while
GMB uses 7.
Manual versus automatic. AMR, UNL, and
UCCA annotation is fully manual. GMB and ST
produce meaning representations automatically,
and these can be corrected by experts or crowds
(Venhuizen et al, 2013).
Derivations. AMR and UNL remain agnostic
about the relation between strings and their mean-
ings, considering this a topic of open research.
ST and GMB annotate words and phrases directly,
recording derivations as (for example) Montague-
style compositional semantic rules operating on
CCG parses.
Top-down verus bottom-up. AMR annota-
tors find it fast to construct meanings from the
top down, starting with the main idea of the sen-
tence (though the AMR Editor allows bottom-up
construction). GMB and UCCA annotators work
bottom-up.
Editors, guidelines, genres. These projects
have graphical sembanking tools (e.g., Basile et al
(2012b)), annotation guidelines,5 and sembanks
that cover a wide range of genres, from news to
fiction. UNL and AMR have both annotated many
of the same sentences, providing the potential for
direct comparison.
8 Future Work
Sembanking. Our main goal is to continue
sembanking. We would like to employ a large
sembank to create shared tasks for natural lan-
guage understanding and generation. These
tasks may additionally drive interest in theoreti-
cal frameworks for probabilistically mapping be-
tween graphs and strings (Quernheim and Knight,
2012b; Quernheim and Knight, 2012a; Chiang et
al., 2013).
Applications. Just as syntactic parsing has
found many unanticipated applications, we expect
sembanks and statistical semantic processors to be
used for many purposes. To get started, we are
exploring the use of statistical NLU and NLG in
5UNL guidelines: www.undl.org/unlsys/unl/unl2005
184
a semantics-based machine translation (MT) sys-
tem. In this system, we annotate bilingual Chi-
nese/English data with AMR, then train compo-
nents to map Chinese to AMR, and AMR to En-
glish. A prototype is described by Jones et al
(2012).
Disjunctive AMR. AMR aims to canonicalize
multiple ways of saying the same thing. We plan
to test how well we are doing by building AMRs
on top of large, manually-constructed paraphrase
networks from the HyTER project (Dreyer and
Marcu, 2012). Rather than build individual AMRs
for different paths through a network, we will con-
struct highly-packed disjunctive AMRs. With this
application in mind, we have developed a guide-
line6 for disjunctive AMR. Here is an example:
(o / *OR*
:op1 (t / talk-01)
:op2 (m / meet-03)
:OR (o2 / *OR*
:mod (o3 / official)
:arg1-of (s / sanction-01
:arg0 (s2 / state))))
official talks
state-sanctioned talks
meetings sanctioned by the state
AMR extensions. Finally, we would like
to deepen the AMR language to include more
relations (to replace :mod and :prep-X, for
example), entity normalization (perhaps wik-
ification), quantification, and temporal rela-
tions. Ultimately, we would like to also in-
clude a comprehensive set of more abstract
frames like ?Earthquake-01? (:magnitude, :epi-
center, :casualties), ?CriminalLawsuit-01? (:de-
fendant, :crime, :jurisdiction), and ?Pregnancy-
01? (:father, :mother, :due-date). Projects like
FrameNet (Baker et al, 1998) and CYC (Lenat,
1995) have long pursued such a set.
References
O. Abend and A. Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
Proc. IWCS.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berke-
ley FrameNet project. In Proc. COLING.
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012a.
Developing a large semantically annotated corpus.
In Proc. LREC.
6Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012b.
A platform for collaborative semantic annotation. In
Proc. EACL demonstrations.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The Prague dependency treebank. In Tree-
banks. Springer.
A. Butler and K. Yoshimoto. 2012. Banking meaning
representations from treebanks. Linguistic Issues in
Language Technology, 7.
S. Cai and K. Knight. 2013. Smatch: An accu-
racy metric for abstract meaning representations. In
Proc. ACL.
D. Chiang, J. Andreas, D. Bauer, K. M. Hermann,
B. Jones, and K. Knight. 2013. Parsing graphs with
hyperedge replacement grammars. In Proc. ACL.
D. Davidson. 1969. The individuation of events.
In N. Rescher, editor, Essays in Honor of Carl G.
Hempel. D. Reidel, Dordrecht.
M. Dreyer and D. Marcu. 2012. Hyter: Meaning-
equivalent semantics for translation evaluation. In
Proc. NAACL.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-based machine trans-
lation with hyperedge replacement grammars. In
Proc. COLING.
H. Kamp, J. Van Genabith, and U. Reyle. 2011. Dis-
course representation theory. In Handbook of philo-
sophical logic, pages 125?394. Springer.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. LREC.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11).
R. Martins. 2012. Le Petit Prince in UNL. In Proc.
LREC.
C. M. I. M. Matthiessen and J. A. Bateman. 1991.
Text Generation and Systemic-Functional Linguis-
tics. Pinter, London.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 workshop: Frontiers in corpus anno-
tation.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, Philadelphia, PA.
185
S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In-
ternational Journal of Semantic Computing (IJSC),
1(4).
D. Quernheim and K. Knight. 2012a. DAGGER: A
toolkit for automata on directed acyclic graphs. In
Proc. FSMNLP.
D. Quernheim and K. Knight. 2012b. Towards prob-
abilistic acceptors and transducers for feature struc-
tures. In Proc. SSST Workshop.
K. Schuler. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
S. Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay.
1986. Compilation of papers on unification-based
grammar formalisms. Technical Report CSLI-86-
48, Center for the Study of Language and Informa-
tion, Stanford, California.
H. Uchida, M. Zhu, and T. Della Senta. 1996. UNL:
Universal Networking Language?an electronic lan-
guage for communication, understanding and col-
laboration. Technical report, IAS/UNU Tokyo.
H. Uchida, M. Zhu, and T. Della Senta. 1999. A
gift for a millennium. Technical report, IAS/UNU
Tokyo.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. In Proc.
IWCS.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R. Belvin, S. Pradhan, L. Ramshaw, and N. Xue.
2011. OntoNotes: A large training corpus for en-
hanced processing. In J. Olive, C. Christianson, and
J. McCary, editors, Handbook of Natural Language
Processing and Machine Translation. Springer.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation.
In Proc. HLT-NAACL.
186
