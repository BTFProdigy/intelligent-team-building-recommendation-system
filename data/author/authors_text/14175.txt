Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671?677,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Apposition Extraction with Syntactic and Semantic Constraints
Will Radford and James R. Currane-lab, School of Information Technologies
University of Sydney
NSW, 2006, Australia
{wradford,james}@it.usyd.edu.au
Abstract
Appositions are adjacent NPs used to add
information to a discourse. We propose
systems exploiting syntactic and seman-
tic constraints to extract appositions from
OntoNotes. Our joint log-linear model
outperforms the state-of-the-art Favre and
Hakkani-Tu?r (2009) model by ?10% on
Broadcast News, and achieves 54.3% F-
score on multiple genres.
1 Introduction
Appositions are typically adjacent coreferent noun
phrases (NP) that often add information about
named entities (NEs). The apposition in Figure 1
consists of three comma-separated NPs ? the first
NP (HEAD) names an entity and the others (ATTRs)
supply age and profession attributes. Attributes
can be difficult to identify despite characteristic
punctuation cues, as punctuation plays many roles
and attributes may have rich substructure.
While linguists have studied apposition in de-
tail (Quirk et al, 1985; Meyer, 1992), most appo-
sition extraction has been within other tasks, such
as coreference resolution (Luo and Zitouni, 2005;
Culotta et al, 2007) and textual entailment (Roth
and Sammons, 2007). Extraction has rarely been
intrinsically evaluated, with Favre and Hakkani-
Tu?r?s work a notable exception.
We analyze apposition distribution in
OntoNotes 4 (Pradhan et al, 2007) and com-
pare rule-based, classification and parsing
extraction systems. Our best system uses a joint
model to classify pairs of NPs with features
that faithfully encode syntactic and semantic
restrictions on appositions, using parse trees and
WordNet synsets.
{John Ake}h , {48}a , {a former vice-president
in charge of legal compliance at American Capital
Management & Research Inc., in Houston,}a , . . .
Figure 1: Example apposition from OntoNotes 4
Our approach substantially outperforms Favre
and Hakkani-Tu?r on Broadcast News (BN) at
54.9% F-score and has state-of-the-art perfor-
mance 54.3% F-score across multiple genres. Our
results will immediately help the many systems
that already use apposition extraction components,
such as coreference resolution and IE.
2 Background
Apposition is widely studied, but ?grammarians
vary in the freedom with which they apply the
term ?apposition?? (Quirk et al, 1985). They are
usually composed of two or more adjacent NPs,
hierarchically structured, so one is the head NP
(HEAD) and the rest attributes (ATTRs). They are
often flagged using punctuation in text and pauses
in speech. Pragmatically, they allow an author to
introduce new information and build a shared con-
text (Meyer, 1992).
Quirk et al propose three tests for apposition: i)
each phrase can be omitted without affecting sen-
tence acceptability, ii) each fulfils the same syntac-
tic function in the resultant sentences, iii) extralin-
guistic reference is unchanged. Strict interpreta-
tions may exclude other information-bearing cases
like pseudo-titles (e.g. ({President}a {Bush}h)NP),
but include some adverbial phrases (e.g. {(John
Smith)NP}h, {(formerly (the president)NP)AP}a). We
adopt the OntoNotes guidelines? relatively strict
interpretation: ?a noun phrase that modifies an
immediately-adjacent noun phrase (these may be
separated by only a comma, colon, or parenthe-
sis).? (BBN, 2004?2007).
671
Unit TRAINF DEVF TESTF TRAIN DEV TEST
Sents. 9,595 976 1,098 48,762 6,894 6,896
Appos. 590 64 68 3,877 502 490
Table 1: Sentence and apposition distribution
Apposition extraction is a common component
in many NLP tasks: coreference resolution (Luo
and Zitouni, 2005; Culotta et al, 2007; Bengt-
son and Roth, 2008; Poon and Domingos, 2008),
textual entailment (Roth and Sammons, 2007;
Cabrio and Magnini, 2010), sentence simplifica-
tion (Miwa et al, 2010; Candido et al, 2009;
Siddharthan, 2002) and summarization (Nenkova
et al, 2005). Comma ambiguity has been studied
in the RTE (Srikumar et al, 2008) and generation
domains (White and Rajkumar, 2008).
Despite this, few papers to our knowledge ex-
plicitly evaluate apposition extraction. Moreover,
apposition extraction is rarely the main research
goal and descriptions of the methods used are of-
ten accordingly terse or do not match our guide-
lines. Lee et al (2011) use rules to extract appo-
sitions for coreference resolution, selecting only
those that are explicitly flagged using commas or
parentheses. They do not separately mark HEAD
and ATTR and permit relative clauses as an ATTR.
While such differences capture useful information
for coreference resolution, these methods would
be unfairly disadvantaged in a direct evaluation.
Favre and Hakkani-Tu?r (2009, FHT) directly
evaluate three extraction systems on OntoNotes
2.9 news broadcasts. The first retrains the Berke-
ley parser (Petrov and Klein, 2007) on trees la-
belled with appositions by appending the HEAD
and ATTR suffix to NPs ? we refer to this as a La-
belled Berkeley Parser (LBP). The second is a CRF
labelling words using an IOB apposition scheme.
Token, POS, NE and BP-label features are used,
as are presence of speech pauses. The final sys-
tem classifies parse tree phrases using an Adaboost
classifier (Schapire and Singer, 2000) with similar
features.
The LBP, IOB and phrase systems score 41.38%,
32.76% and 40.41%, while their best uses LBP tree
labels as IOB features, scoring 42.31%. Their fo-
cus on BN automated speech recognition (ASR)
output, which precludes punctuation cues, does
not indicate how well the methods perform on tex-
tual genres. Moreover all systems use parsers or
parse-label features and do not completely evalu-
ate non-parser methods for extraction despite in-
cluding baselines.
Form # % Reverse form # % ?%
H t A 2109 55.9 A t H 724 19.2 75.1
A H 482 12.8 H A 205 5.4 93.3
H , A 1843 48.9 A , H 532 14.1 63.0
A H 482 12.9 H A 205 5.4 81.3
H ( A 146 3.9 A ( H 16 0.4 85.6
A : H 94 2.5 H : A 23 0.6 88.7
H -- A 66 1.8 A -- H 35 0.9 91.4
A - H 31 0.8 H - A 21 0.6 92.8
Table 2: Apposition forms in TRAIN with abstract
(top) and actual (bottom) tokens, e.g., H t A in-
dicates an HEAD, one token then an ATTR.
3 Data
We use apposition-annotated documents from the
English section of OntoNotes 4 (Weischedel et al,
2011). We manually adjust appositions that do not
have exactly one HEAD and one or more ATTR1.
Some appositions are nested, and we keep only
?leaf? appositions, removing the higher-level ap-
positions.
We follow the CoNLL-2011 scheme to select
TRAIN, DEV and TEST datasets (Pradhan et al,
2011). OntoNotes 4 is made up of a wide vari-
ety of sources: broadcast conversation and news,
magazine, newswire and web text. Appositions
are most frequent in newswire (one per 192 words)
and least common in broadcast conversation (one
per 645 words) with the others in between (around
one per 315 words).
We also replicate the OntoNotes 2.9 BN data
used by FHT, selecting the same sentences from
OntoNotes 4 (TRAINF/DEVF/TESTF). We do not
?speechify? our data and take a different approach
to nested apposition. Table 1 shows the distri-
bution of sentences and appositions (HEAD-ATTR
pairs).
3.1 Analysis
Most appositions in TRAIN have one ATTR
(97.4%) with few having two (2.5%) or three
(0.1%). HEADs are typically shorter (median 5
tokens, 95% < 7) than ATTRs (median 7 tokens,
95% < 15). Table 2 shows frequent apposition
forms. Comma-separated apposition is the most
common (63%) and 93% are separated by zero or
one token. HEADs are often composed of NEs:
52% PER and 13% ORG, indicating an entity about
which the ATTR adds information.
1Available at http://schwa.org/resources
672
Pattern and Example P R F
{ne:PER}h # {pos:NP (pos:IN ne:LOC|ORG|GPE)?}a #
?{Jian Zhang}h, {the head of Chinese delegation}a,? 73.1 21.9 33.7
{pos:DT gaz:role|relation}a #? {ne:PER}h
?{his new wife}a {Camilla}h? 45.9 9.5 15.8
{ne:ORG|GPE}h # {pos:DT pos:NP}a #
?{Capetronic Inc.}h, {a Taiwan electronics maker}a,? 60.4 6.0 10.9
{pos:NP}a # {ne:PER}h #
?{The vicar}a, {W.D. Jones}h,? 33.7 4.5 7.9
{ne:PER}h # {pos:NP pos:POS pos:NP}a #
?{Laurence Tribe}h, {Gore ?s attorney}a,? 82.0 4.0 7.7
Table 3: The top-five patterns by recall in the TRAIN dataset. ?#? is a pause (e.g., punctuation), ?|? a
disjunction and ??? an optional part. Patterns are used to combine tokens into NPs for pos:NP.
4 Extracting Appositions
We investigate different extraction systems using
a range of syntactic information. Our systems that
use syntactic parses generate candidates (pairs of
NPs: p1 and p2) that are then classified as apposi-
tion or not.
This paper contributes three complementary
techniques for more faithfully modelling apposi-
tion. Any adjacent NPs, disregarding intervening
punctuation, could be considered candidates, how-
ever stronger syntactic constraints that only allow
sibling NP children provide higher precision can-
didate sets. Semantic compatibility features en-
coding that an ATTR provides consistent informa-
tion for its HEAD. A joint classifier models the
complete apposition rather than combining sepa-
rate phrase-wise decisions. Taggers and parsers
are trained on TRAIN and evaluated on DEV or
TEST. We use the C&C tools (Curran and Clark,
2003) for POS and NE tagging and the and the
Berkeley Parser (Petrov and Klein, 2007), trained
with default parameters.
Pattern POS, NE and lexical patterns are used
to extract appositions avoiding parsing?s compu-
tational overhead. Rules are applied indepen-
dently to tokenized and tagged sentences, yield-
ing HEAD-ATTR tuples that are later deduplicated.
The rules were manually derived from TRAIN2 and
Table 3 shows the top five of sixteen rules by re-
call over TRAIN. The ?role? gazetteer is the transi-
tive closure of hyponyms of the WordNet (Miller,
1995) synset person.n.01 and ?relation? man-
ually constructed (e.g., ?father?, ?colleague?). Tu-
ples are post-processed to remove spurious appo-
2There is some overlap between TRAIN and DEVF/TESTF
with appositions from the latter used in rule generation.
sitions such as comma-separated NE lists3.
Adjacent NPs This low precision, high recall
baseline assumes all candidates, depending on
generation strategy, are appositions.
Rule We only consider HEADs whose syntactic
head is a PER, ORG, LOC or GPE NE. We formalise
semantic compatibility by requiring the ATTR head
to match a gazetteer dependent on the HEAD?s NE
type. To create PER, ORG and LOC gazetteers,
we identified common ATTR heads in TRAIN and
looked for matching WordNet synsets, selecting
the most general hypernym that was still seman-
tically compatible with the HEAD?s NE type.
Gazetteer words are pluralized using pattern.en
(De Smedt and Daelemans, 2012) and normalised.
We use partitive and NML-aware rules (Collins,
1999; Vadas and Curran, 2007) to extract syntactic
heads from ATTRs. These must match the type-
appropriate gazetteer, with ORG and LOC/GPE
falling back to PER (e.g., ?the champion, Apple?).
Extracted tuples are post-processed as for Pat-
tern and reranked by the OntoNotes specificity
scale (i.e., NNP > PRO > Def. NP > Indef. NP
> NP), and the more specific unit is assigned
HEAD. Possible ATTRs further to the left or right
are checked, allowing for cases such as Figure 1.
Labelled Berkeley Parser We train a LBP on
TRAIN and recover appositions from parsed sen-
tences. Without syntactic constraints this is equiv-
alent to FHT?s LBP system (LBPF) and indicated by
? in Tables.
Phrase Each NP is independently classified as
HEAD, ATTR or None. We use a log-linear model
with a SGD optimizer from scikit-learn (Pedregosa
3Full description: http://schwa.org/resources
673
Model Full system -syn -sem -both +gold
Pattern 44.8 34.9 39.2 - - - - - - - - - 52.2 39.6 45.1
Adj NPs 11.6 58.0 19.3 3.6 65.1 6.8 - - - - - - 16.0 85.3 27.0
Rule 65.3 46.8 54.5 43.7 50.0 46.7 - - - - - - 79.1 62.0 69.5
LBP 66.3 52.2 58.4 47.8 53.0 ?50.3 - - - - - - - - -
Phrase 73.2 45.6 56.2 77.7 41.0 53.7 73.2 44.6 55.4 77.7 40.8 ?53.5 89.0 58.2 70.4
Joint 66.3 49.0 56.4 68.5 48.6 56.9 70.4 47.0 56.4 68.9 48.0 56.6 87.9 69.5 77.6
Joint LBP 69.6 51.0 58.9 69.6 49.6 57.9 71.5 49.0 58.2 68.3 48.6 56.8 - -
Table 4: Results over DEV: each column shows precision, recall and F-score. -syn/-sem/-both show the
impact of removing constraints/features, +gold shows the impact of parse and tagging errors.
et al, 2011). The binary features are calculated
from a generated candidate phrase (p) and are the
same as FHT?s phrase system (PhraseF), denoted
? in Tables. In addition, we propose the fea-
tures below and to decode classifications, adjacent
apposition-classified NPs are re-ordered by speci-
ficity.
? p precedes/follows punctuation/interjection
? p starts with a DT or PRP$ (e.g., ?{the
director}a? or ?{her husband}a?)
? p?s syntactic head matches a NE-specific se-
mantic gazetteer (e.g., ?{the famous actor}a?
? PER, ?{investment bank}a?? ORG)
? p?s syntactic head has the POS CD (e.g.,
?{John Smith}h, {34}a, . . . ?)
? p?s NE type (e.g., ?{John Smith}h?? PER)
? Specificity rank
Joint The final system classifies pairs of phrases
(p1, p2) as: HEAD-ATTR, ATTR-HEAD or None.
The system uses the phrase model features as
above as well as pairwise features:
? the cross-product of selected features for p1
and p2: gazetteer matches, NE type, speci-
ficity rank. This models the compatibility be-
tween p1 and p2. For example, if the HEAD
has the NE type PER and the ATTR has the
syntactic head in the PER gazetteer, for ex-
ample ?{Tom Cruise}h, {famous actor}a,??
(p1: PER, p2: PER-gaz)
? If semantic features are found in p1 and p2
? p1/p2 specificity (e.g., equal, p1 > p2)
? whether p1 is an acronym of p2 or vice-versa
5 Results
We evaluate by comparing the extracted HEAD-
ATTR pairs against the gold-standard. Correct
pairs match gold-standard bounds and label. We
report precision (P), recall (R) and F1-score (F).
Table 4 shows our systems? performance on the
multi-genre DEV dataset, the impact of remov-
ing syntactic constraints, semantic features and
parse/tag error. Pattern performance is reasonable
at 39.2% F-score given its lack of full syntactic
information. All other results use parses and, al-
though it has a low F-score, the Adjacent NPs?
65.1% recall, without syntactic constraints, is the
upper bound for the parse-based systems. Statis-
tical models improve performance, with the joint
models better than the higher-precision phrase
model as the latter must make two independently
correct classification decisions. Our best system
has an F-score of 58.9% using a joint model over
the de-labelled trees produced by the LBP. This
indicates that although our model does not use
the apposition labels from the tree, the tree is a
more suitable structure for extraction. This sys-
tem substantially improves on our implementation
of FHT?s LBPF (?) and PhraseF (?) systems by 8.6%
and 5.4%4.
Removing syntactic constraints mostly reduces
performance in parse-based systems as the system
must consider lower-quality candidates. The F-
score increase is driven by higher precision at min-
imal cost to recall. Removing semantic features
has less impact and removing both is most detri-
mental to performance. These features have less
impact on joint models; indeed, joint performance
using BP trees increases without the features, per-
haps as joint models already model the syntactic
context.
We evaluate the impact of parser and tagger
error by using gold-standard resources. Gold-
standard tags and trees improve recall in all cases
leading to F-score improvements (+gold). The
pattern system is reasonably robust to automatic
tagging errors, but parse-based models suffer con-
siderably from automatic parses. To compare the
impact of tagging and parsing error, we configure
the joint system to use gold parses and automatic
NE tags and vice versa. Using automatic tags does
not greatly impact performance (-1.3%), whereas
4We do not implement the IOB or use LBP features for
TRAIN as these would require n-fold parser training.
674
Model P R F
LBPF ? 53.1 46.9 49.8
PhraseF ? 71.5 30.2 42.5
Pattern 44.8 34.3 38.8
LBP 63.9 45.1 52.9
Joint LBP 66.9 45.7 54.3
Table 5: Results over TEST: FHT?s (top) and our
(bottom) systems.
Error BP LBP ?
PP Attachment 5,585 5,396 -189
NP Internal Structure 1,483 1,338 -145
Other 3,164 3,064 -100
Clause Attachment 3,960 3,867 -93
Modifier Attachment 1,523 1,700 177
Co-ordination 3,095 3,245 150
NP Attachment 2,615 2,680 65
Total 30,189 29,859 -330
Table 6: Selected BP/LBP parse error distribution.
using automatic parses causes a drop of around
20% to 57.7%, demonstrating that syntactic infor-
mation is crucial for apposition extraction.
We compare our work with Favre and Hakkani-
Tu?r (2009), training with TRAINF and evaluating
over TESTF? exclusively BN data. Our implemen-
tations of their systems, PhraseF and LBPF, per-
form at 43.6% and 44.1%. Our joint LBP system
is substantially better, scoring 54.9%.
Table 5 shows the performance of our best sys-
tems on the TEST dataset and these follow the
same trend as DEV. Joint LBP performs the best
at 54.3%, 4.5% above LBPF.
Finally, we test whether labelling appositions
can help parsing. We parse DEV trees with LBP
and BP, remove apposition labels and analyse
the impact of labelling using the Berkeley Parser
Analyser (Kummerfeld et al, 2012). Table 6
shows the LBP makes fewer errors, particularly
NP internal structuring, PP and clause attachment
classes at the cost of modifier attachment and co-
ordination errors. Rather than increasing parsing
difficulty, apposition labels seem complementary,
improving performance.
6 Conclusion
We present three apposition extraction techniques.
Linguistic tests for apposition motivate strict syn-
tactic constraints on candidates and semantic fea-
tures encode the addition of compatible informa-
tion. Joint models more faithfully capture apposi-
tion structure and our best system achieves state-
of-the-art performance of 54.3%. Our results will
immediately benefit the large number of systems
with apposition extraction components for coref-
erence resolution and IE.
Acknowledgements
The authors would like to thank the anonymous re-
viewers for their suggestions. Thanks must also go
to Benoit Favre for his clear writing and help an-
swering our questions as we replicated his dataset
and system. This work has been supported by
ARC Discovery grant DP1097291 and the Capi-
tal Markets CRC Computable News project.
References
BBN. 2004?2007. Co-reference guidelines for en-
glish ontonotes. Technical Report v6.0, BBN
Technologies.
Eric Bengtson and Dan Roth. 2008. Understand-
ing the value of features for coreference resolu-
tion. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 294?303. Association for Com-
putational Linguistics, Honolulu, Hawaii.
Elena Cabrio and Bernardo Magnini. 2010. To-
ward qualitative evaluation of textual entailment
systems. In Coling 2010: Posters, pages 99?
107. Coling 2010 Organizing Committee, Bei-
jing, China.
Arnaldo Candido, Erick Maziero, Lucia Specia,
Caroline Gasperin, Thiago Pardo, and Sandra
Aluisio. 2009. Supporting the adaptation of
texts for poor literacy readers: a text simplifi-
cation editor for brazilian portuguese. In Pro-
ceedings of the Fourth Workshop on Innovative
Use of NLP for Building Educational Applica-
tions, pages 34?42. Association for Computa-
tional Linguistics, Boulder, Colorado.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Aron Culotta, Michael Wick, and Andrew Mc-
Callum. 2007. First-order probabilistic mod-
els for coreference resolution. In Human Lan-
guage Technologies 2007: The Conference of
the North American Chapter of the Association
for Computational Linguistics; Proceedings of
the Main Conference, pages 81?88. Association
675
for Computational Linguistics, Rochester, New
York.
James Curran and Stephen Clark. 2003. Language
independent ner using a maximum entropy tag-
ger. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference
on Natural Language Learning at HLT-NAACL
2003, pages 164?167.
Tom De Smedt and Walter Daelemans. 2012. Pat-
tern for python. Journal of Machine Learning
Research, 13:2013?2035.
Benoit Favre and Dilek Hakkani-Tu?r. 2009.
Phrase and word level strategies for detecting
appositions in speech. In Proceedings of Inter-
speech 2009, pages 2711?2714. Brighton, UK.
Jonathan K. Kummerfeld, David Hall, James R.
Curran, and Dan Klein. 2012. Parser show-
down at the wall street corral: An empirical in-
vestigation of error types in parser output. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 1048?1059. Jeju Island, South
Korea.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford?s multi-pass sieve
coreference resolution system at the conll-
2011 shared task. In Proceedings of the
CoNLL-2011 Shared Task. URL pubs/
conllst2011-coref.pdf.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic
features. In Proceedings of Human Lan-
guage Technology Conference and Conference
on Empirical Methods in Natural Language
Processing, pages 660?667. Association for
Computational Linguistics, Vancouver, British
Columbia, Canada.
Charles F. Meyer. 1992. Apposition in Contem-
porary English. Cambridge University Press,
Cambridge, UK.
George A. Miller. 1995. Wordnet: A lexical
database for english. Communications of the
ACM, 38:39?41.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2010. Entity-focused sentence
simplification for relation extraction. In Pro-
ceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010),
pages 788?796. Coling 2010 Organizing Com-
mittee, Beijing, China.
Ani Nenkova, Advaith Siddharthan, and Kath-
leen McKeown. 2005. Automatically learn-
ing cognitive status for multi-document sum-
marization of newswire. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 241?248. Associa-
tion for Computational Linguistics, Vancouver,
British Columbia, Canada.
F. Pedregosa, G. Varoquaux, A. Gramfort,
V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Van-
derplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine Learning in Python . Journal
of Machine Learning Research, 12:2825?2830.
Slav Petrov and Dan Klein. 2007. Learning and in-
ference for hierarchically split PCFGs. In Pro-
ceedings of the 22nd AAAI Conference of Artifi-
cial Intelligence, pages 1642?1645. Vancouver,
Canada.
Hoifung Poon and Pedro Domingos. 2008.
Joint unsupervised coreference resolution with
Markov Logic. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 650?659. Associ-
ation for Computational Linguistics, Honolulu,
Hawaii.
Sameer Pradhan, Lance Ramshaw, Mitchell Mar-
cus, Martha Palmer, Ralph Weischedel, and
Nianwen Xue. 2011. CoNLL-2011 shared
task: Modeling unrestricted coreference in
OntoNotes. In Proceedings of the Fifteenth
Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27.
Portland, OR USA.
Sameer S. Pradhan, Eduard Hovy, Mitch Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified rela-
tional semantic representation. In Proceedings
of the International Conference on Semantic
Computing, pages 517?526. Washington, DC
USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey
Leech, and Jan Svartvik. 1985. A Comprehen-
sive Grammar of the English Language. Gen-
eral Grammar Series. Longman, London, UK.
676
Dan Roth and Mark Sammons. 2007. Seman-
tic and logical inference model for textual en-
tailment. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Para-
phrasing, pages 107?112. Association for Com-
putational Linguistics, Prague.
Robert E. Schapire and Yoram Singer. 2000. Boos-
texter: A boosting-based systemfor text catego-
rization. Machine Learning, 39(2-3):135?168.
Advaith Siddharthan. 2002. Resolving attachment
and clause boundary ambiguities for simplify-
ing relative clause constructs. In Proceedings of
the ACL Student Research Workshop (ACLSRW
2002), pages 60?65. Association for Computa-
tional Linguistics, Philadelphia.
Vivek Srikumar, Roi Reichart, Mark Sammons,
Ari Rappoport, and Dan Roth. 2008. Extraction
of entailed semantic relations through syntax-
based comma resolution. In Proceedings of
ACL-08: HLT, pages 1030?1038. Columbus,
OH USA.
David Vadas and James R. Curran. 2007. Pars-
ing internal noun phrase structure with collins?
models. In Proceedings of the Australasian
Language Technology Workshop 2007, pages
109?116. Melbourne, Australia.
Ralph Weischedel, Martha Palmer, Mitchell Mar-
cus, Eduard Hovy, Sameer Pradhan, Lance
Ramshaw, Nianwen Xue, Ann Taylor, Jeff
Kaufman, Michelle Franchini, Mohammed El-
Bachouti, Robert Belvin, and Ann Houston.
2011. OntoNotes Release 4.0. Technical re-
port, Linguistic Data Consortium, Philadelphia,
PA USA.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for
broad-coverage surface realization with CCG.
In Coling 2008: Proceedings of the workshop
on Grammar Engineering Across Frameworks,
pages 17?24. Coling 2008 Organizing Commit-
tee, Manchester, England.
677
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 464?469,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cheap and easy entity evaluation
Ben Hachey Joel Nothman Will Radford
School of Information Technologies
University of Sydney
NSW 2006, Australia
ben.hachey@sydney.edu.au
{joel,wradford}@it.usyd.edu.au
Abstract
The AIDA-YAGO dataset is a popular tar-
get for whole-document entity recogni-
tion and disambiguation, despite lacking a
shared evaluation tool. We review eval-
uation regimens in the literature while
comparing the output of three approaches,
and identify research opportunities. This
utilises our open, accessible evaluation
tool. We exemplify a new paradigm of
distributed, shared evaluation, in which
evaluation software and standardised, ver-
sioned system outputs are provided online.
1 Introduction
Modern entity annotation systems detect mentions
in text and disambiguate them to a knowledge base
(KB). Disambiguation typically returns the corre-
sponding Wikipedia page or NIL if none exists.
Named entity linking (NEL) work is driven by
the TAC shared tasks on query-driven knowledge
base population (Ji and Grishman, 2011). Eval-
uation focuses on disambiguating queried names
and clustering NIL mentions, but most systems
internally perform whole-document named en-
tity recognition, coreference, and disambiguation
(Cucerzan and Sil, 2013; Pink et al, 2013; Cheng
et al, 2013; Fahrni et al, 2013). Wikification
work generally evaluates end-to-end entity anno-
tation including KB-driven mention spotting and
disambiguation (Milne and Witten, 2008b; Kulka-
rni et al, 2009; Ratinov et al, 2011; Ferragina and
Scaiella, 2010). Despite important differences in
mention handling, NEL and wikification work have
followed a similar trajectory. Yet to our knowl-
edge, there are no comparative whole-document
evaluations of NEL and wikification systems.
Public data sets have also driven research
in whole-document entity disambiguation
(Cucerzan, 2007; Milne and Witten, 2008b;
Kulkarni et al, 2009; Bentivogli et al, 2010; Hof-
fart et al, 2011; Meij et al, 2012). However, with
many task variants and evaluation methodologies
proposed, it is very difficult to synthesise a clear
picture of the state of the art.
We present an evaluation suite for named entity
linking, leveraging and advocating for the AIDA
disambiguation annotations (Hoffart et al, 2011)
over the large and widely used CoNLL NER data
(Tjong Kim Sang and Meulder, 2003). This builds
on recent rationalisation and benchmarking work
(Cornolti et al, 2013), adding an isolated evalua-
tion of disambiguation. Contributions include:
? a simple, open-source evaluation suite for
end-to-end, whole-document NEL;
? disambiguation evaluation facilitated by
gold-standard mentions;
? reference outputs from state-of-the-art NEL
and wikification systems published with the
suite for easy comparison;
? implementation of statistical significance and
error sub-type analysis, which are often lack-
ing in entity linking evaluation;
? a venue for publishing benchmark results
continuously, complementing the annual cy-
cle of shared tasks;
? a repository for versioned corrections to
ground truth annotation.
We see this repository, at https://github.
com/wikilinks/conll03_nel_eval, as a
model for the future of informal shared evaluation.
We survey entity annotation tasks and evalua-
tion, proposing a core suite of metrics for end-to-
end linking and tagging, and settings that isolate
mention detection and disambiguation. A compar-
ison of state-of-the-art NEL and wikification sys-
tems illustrates how key differences in mention
handling affect performance. Analysis suggests
that focusing evaluation too tightly on subtasks
like candidate ranking can lead to results that do
not reflect end-to-end performance.
464
2 Tasks and metrics
The literature includes many variants of the en-
tity annotation task and even more evaluation ap-
proaches. Systems can be invoked under two set-
tings: given text with expressions to be linked
(gold mentions); or given plain text only (system
mentions). The former enables a diagnostic evalu-
ation of disambiguation, while the latter simulates
a realistic end-to-end application setting.
Within each setting, metrics may consider
different subsets of the gold (G) and system (S)
annotations. Given sets of (doc, token span, kbid)
tuples, we define precision, recall and F
1
score
with respect to some annotation filter f :
P
f
=
|f(G) ? f(S)|
|f(S)|
, R
f
=
|f(G) ? f(S)|
|f(G)|
We advocate two core metrics, corresponding to
the major whole-document entity annotation tasks.
Link annotation measures performance over every
linked mention. Its filter f
L
matches spans and
link targets, disregarding NILs. This is particularly
apt when entity annotation is a step in an informa-
tion extraction pipeline. Tag annotation measures
performance over document-level entity sets: f
T
disregards span information and NILs. This is ap-
propriate when entity annotation is used, e.g., for
document indexing or social media mining (Mi-
halcea and Csomai, 2007; Meij et al, 2012). We
proceed to ground these metrics and diagnostic
variants in the literature.
2.1 End-to-end evaluation
We follow Cornolti et al (2013) in evaluating end-
to-end entity annotation, including both mention
detection and disambiguation. In this context,
f
L
equates to Cornolti et al?s strong annotation
match; f
T
measures what they call entity match.
2.2 Mention evaluation
Mention detection performance may be evaluated
regardless of linking decisions. A filter f
M
dis-
cards the link target (kbid). Of the present metrics,
only this considers NIL-linked system mentions as
different from non-mentions. For comparability
with wikification, we consider an additional filter
f
M
KB
to NEL output that retains only linked men-
tions. f
M
and f
M
KB
are equivalent to Cucerzan?s
(2007) mention evaluation and Cornolti et al?s
strong mention match respectively. f
M
is com-
parable to the NER evaluation from the CoNLL
2003 shared task (Tjong Kim Sang and Meulder,
2003): span equivalence is handled the same way,
but metrics here ignore mention types.
2.3 Disambiguation evaluation
Most NEL and wikification literature focuses on
disambiguation, evaluating the quality of link tar-
get annotations in isolation from NER error. Pro-
viding systems with ground truth mentions makes
f
L
equivalent to Mihalcea and Csomai?s (2007)
sense disambiguation evaluation and Milne and
Witten?s (2008b) disambiguation evaluation. It
differs from Kulkarni et al?s (2009) metric in be-
ing micro-averaged (equal weight to each men-
tion), rather than macro-averaged across docu-
ments. f
L
recall is comparable to TAC?s KB recall
(Ji and Grishman, 2011). It differs in that all men-
tions are evaluated rather than specific queries.
Related evaluations have also isolated disam-
biguation performance by: considering the links
of only correctly identified mentions (Cucerzan,
2007); or only true mentions where the correct
entity appears among top candidates before dis-
ambiguation (Ratinov et al, 2011; Hoffart et al,
2011; Pilz and Paass, 2012). We do not prefer this
approach as it makes system comparison difficult.
For comparability, we implement a filter f
L
HOF
that retains only Hoffart-linkable mentions having
a YAGO means relation to the correct entity.
Tag annotation (f
T
) with ground truth men-
tions is equivalent to Milne and Witten?s (2008b)
link evaluation, Mihalcea and Csomai?s (2007)
keyword extraction evaluation and Ratinov et
al.?s (2011) bag-of-titles evaluation. It is compara-
ble to Pilz and Paass?s (2012) bag-of-titles evalua-
tion, but does not account for sequential order and
keeps all gold-standard links regardless of whether
they are found by candidate generation.
2.4 Further diagnostics and rank evaluation
Several evaluations in the literature are beyond the
scope of this paper but planned for future versions
of the code. This includes further diagnostic sub-
task evaluation, particularly candidate set recall
(Hachey et al, 2013), NIL accuracy (Ji and Grish-
man, 2011) and weak mention matching (Cornolti
et al, 2013). With a score for each prediction, fur-
ther metrics are possible: rank evaluation of tag
annotation with r-precision, mean reciprocal rank
and mean average precision (Meij et al, 2012);
and rank evaluation of mentions for comparison to
Hoffart et al (2011) and Pilz and Paass (2012).
465
3 Data
The CoNLL-YAGO dataset (Hoffart et al, 2011)
is an excellent target for end-to-end, whole-
document entity annotation. It is public, free and
much larger than most entity annotation data sets.
It is based on the widely used NER data from
the CoNLL 2003 shared task (Tjong Kim Sang
and Meulder, 2003), building disambiguation on
ground truth mentions. It has standard training
and development splits that are representative of
the held-out test data, all being sourced from the
Reuters text categorisation corpus (Lewis et al,
2004), which is provided free for research pur-
poses. Training and development comprise 1,162
stories from 22-31 August 1996 and held-out test
comprises 231 stories from 6-7 December 1996.
The layered annotation provides useful informa-
tion for analysis including categorisation topics
(e.g., general news, markets, sport) and NE type
markup (PER, ORG, LOC, MISC).
The primary drawback is that KB annotations
are currently present only if there is a YAGO
means relation between the mention string and
the correct entity. This means that there are a
number of CoNLL entity mentions referring to
entities that exist in Wikipedia that are nonethe-
less marked NIL in the ground truth (e.g. ?DSE?
for ?Dhaka Stock Exchange?). This may be ad-
dressed by using a shared repository to adopt ver-
sioned improvements to the ground truth. Anno-
tation over CoNLL tokenisation sometimes results
in strange mentions (e.g., ?Washington-based? in-
stead of ?Washington?). However, prescribed to-
kenisation simplifies comparison and analysis.
Another concern is that link annotation goes
stale, since Wikipedia titles are only canonical
with respect to a particular point in time. This is
because pages may be renamed or reorganised:
? to improve editorial structure, such as down-
grading an entity from having a page of its
own, to a mere section in another page;
? to account for newly notable entities, such as
creating a disambiguation page for a title that
formerly had a single known referent; or
? because of changes in fact, such as corporate
mergers and name changes.
All systems compared provide Wikipedia titles as
labels, which are mapped to current titles for com-
parison: for each entity title t linked in the gold
data, we query the Wikipedia API to find t?s canon-
ical form t
c
and retrieve titles of all redirects to t
c
.
4 Reference systems
Even on public data sets, comparison to published
results can be very difficult and extremely costly
(Fokkens et al, 2013). We include reference sys-
tem output in our repository for simple compar-
ison. Other researchers are welcome to add ref-
erence output, providing a continuous benchmark
that complements the annual cycle of large shared
tasks like TAC KBP.
4.1 TagMe
TagMe (Ferragina and Scaiella, 2010) is an end-
to-end wikification system specialising in short
texts. TagMe performs best among publicly avail-
able wikification systems (Cornolti et al, 2013).
Mention detection uses a dictionary of anchor
text from links between Wikipedia pages. Candi-
date ranking is based on entity relatedness (Milne
and Witten, 2008a), followed by mention prun-
ing. We use thresholds on annotation scores sup-
plied by Marco Cornolti (personal communica-
tion) of 0.289 and 0.336 respectively for men-
tion/link and tag evaluation. TagMe annota-
tions may not align with CoNLL token bound-
aries, e.g., <annot title=?Oakland, New Jer-
sey?>OAKLAND, N.J</annot>. Before evalua-
tion, we extend annotations to overlapping tokens.
4.2 AIDA
AIDA (Hoffart et al, 2011) is the system pre-
sented with the CoNLL-YAGO dataset and places
emphasis on state-of-the-art ranking of candi-
date entity sets. Mentions are ground truth from
the CoNLL data to isolate ranking performance,
equivalent to applying the f
L
HOF
filter. Ranking is
informed by a graph model of entity compatibility.
4.3 Schwa
Schwa (Radford et al, 2012) is a heuristic NEL
system based on a TAC 2012 shared task en-
trant. Mention detection uses a NER model trained
on news text followed by rule-based corefer-
ence. Disambiguation uses an unweighted com-
bination of KB statistics, document compatibil-
ity (Cucerzan, 2007), graph similarity and targeted
textual similarity. Candidates that score below
a threshold learned from TAC data are linked to
NIL. The system is very competitive, performing
at 93% and 97% respectively of the best accuracy
numbers we know of on 2011 and 2012 TAC eval-
uation data (Cucerzan and Sil, 2013).
466
System Mentions Filter P R F
1
Cucerzan System f
M
82.2 84.8 83.5
Schwa System f
M
86.9 76.7 81.5
TagMe System f
M
KB
75.2 60.4 67.0
Schwa System f
M
KB
82.5 74.5 78.3
Table 1: Mention detection results. Cucerzan re-
sults as reported (Cucerzan, 2007).
5 Results
We briefly report results over the reference sys-
tems to highlight characteristics of the evaluation
metrics and task settings. Results hinge upon
Schwa since we have obtained only its output in all
settings. Except where noted, all differences are
significant (p < 0.05) according to approximate
randomisation (Noreen, 1989), permuting annota-
tions over whole documents.
5.1 Mention evaluation
Table 1 evaluates mentions with and without NILs.
None of the systems reported use a CoNLL-
trained NER tagger, for which top shared task par-
ticipants approached 90% F
1
in a stricter evalu-
ation than f
M
. We note the impressive numbers
reported by Cucerzan (2007) using a novel ap-
proach to mention detection based on capitalisa-
tion and corpus co-occurrence statistics, and the
similar performance
1
to Schwa, whose NER com-
ponent is trained on another news corpus.
In wikification, NIL-linked mentions may not
be relevant, and it may suffice to identify only
the most canonical forms of names, rather than
all mentions in a coreference chain. With f
M
KB
,
Schwa has much higher recall than TagMe, though
TagMe?s precision is understated because it gener-
ates non-NE annotations that are not present in the
CoNLL-YAGO ground truth (e.g., linking ?striker?
to Forward (association football)).
5.2 Disambiguation evaluation
Table 2 contains results isolating disambiguation
performance. AIDA ranking outperforms Schwa
according to both the link (f
L
HOF
) and tag metrics
(f
T
HOF
). If we remove the Hoffart et al (2011)
linkable constraint, we observe that Schwa disam-
biguation performance loses about 8 points in pre-
cision on the link metric (f
L
) and 2 points on the
tag metric (f
T
). This suggests that disambiguation
1
Significance cannot be tested since we do not have the
Cucerzan (2007) output.
System Mentions Filter P R F
1
Schwa Gold f
L
67.5 78.3 72.5
Schwa Gold f
L
HOF
79.7 78.3 79.0
AIDA Gold f
L
HOF
83.2 83.2 83.2
Schwa Gold f
T
77.8 77.7 77.7
Schwa Gold f
T
HOF
80.1 77.6 78.8
AIDA Gold f
T
HOF
87.7 84.2 85.9
Table 2: Disambiguation results for mention-level
linking and document-level tagging.
System Mentions Filter P R F
1
TagMe System f
L
63.2 50.7 56.3
Schwa System f
L
67.6 61.0 64.2
TagMe System f
T
65.0 65.4 65.2
Schwa System f
T
71.2 62.6 66.6
Table 3: End-to-end results for mention-level link-
ing and document-level tagging.
evaluation without the linkable constraint is im-
portant, especially if the application requires de-
tecting and disambiguating all mentions.
The comparison here highlights a notable eval-
uation intricacy. The Schwa system disambiguates
all gold mentions rather than those with KB links,
and the document compatibility approach means
that evidence from a NIL mention may offer con-
founding evidence when linking linkable men-
tions. Further, although using the same mentions,
systems use search resources with different recall
characteristics, so the Schwa system may not re-
trieve the correct candidate to disambiguate.
5.3 End-to-end evaluation
Finally, Table 3 contains end-to-end entity anno-
tation results. Again, these results highlight key
differences in mention handling between NEL and
wikification. Coreference modelling helps NEL
detect and link ambiguous names (e.g., ?Presi-
dent Bush?) that refer to the same entity as unam-
biguous names in the same text (e.g., ?George W.
Bush?). And restricting the the universe to named
entities is appropriate for the CoNLL-YAGO data.
The advantage is marked in the mention-level link
evaluation (f
L
). However, the systems are statis-
tically indistinguishable in the document-level tag
evaluation (f
T
). Thus the extra NER and corefer-
ence machinery may not be justified if the applica-
tion is document indexing or social media mining
(Meij et al, 2012), wherein a KB-driven mention
detector may be favourable for other reasons.
467
Error
f
L
HOF
f
L
AIDA Schwa TagMe Schwa
wrong link 752 896 429 605
link as nil - 79 - 111
nil as link - - 183 337
missing - - 1,780 1,031
extra - - 1,663 927
Table 4: f
L
HOF
and f
L
error profiles.
0 20 40 60 80 100
LOC
ORG
PER
MISC
Figure 1: Schwa f
L
and f
L
HOF
F
1
for NE types
6 Analysis
We analyse the types of error that a system makes.
We also harness the multi-layered annotation to
quantify the effect of NE type and document topic.
By error type Table 4 shows error counts based
on the disambiguation link evaluation with the
linkable constraint (f
L
HOF
) and the end-to-end
link evaluation (f
L
). Errors are divided as follows:
wrong link: mention linked to wrong KB entry
link as nil: KB-entity mention linked to NIL
nil as link: NIL mention linked to the KB
missing: true mention not detected
extra: mention detected spuriously
AIDA outperforms Schwa under the linkable eval-
uation, making fewer wrong link errors. Schwa
also overgenerates NIL, which may reflect candi-
date recall errors or a conservative disambiguation
threshold. On the end-to-end evaluation, Schwa
makes more linking errors (wrong link, link as nil,
nil as link) than TagMe, but fewer in mention de-
tection, leading to higher overall performance.
By entity type Figure 1 evaluates only men-
tions where the CoNLL 2003 corpus (Tjong Kim
Sang and Meulder, 2003) marks a NE mention of
each type. This is based on the link evaluation
of Schwa. The left and right bars correspond to
end-to-end (f
L
) and disambiguation (f
L
HOF
) F
1
respectively. In accord with TAC results (Ji and
Grishman, 2011), high accuracy can be achieved
on PER when a full name is given, while ORG is
substantially more challenging. MISC entities are
somewhat difficult to disambiguate, with identifi-
cation errors hampering end-to-end performance.
0 20 40 60 80 100
Sports
Domestic Politics
Corporate / Industrial
Internat?l Relations
Markets
War / Civil War
Crime / Law Enforc?t
Figure 2: Schwa f
L
and f
L
HOF
F
1
for top topics
By topical category The underlying Reuters
Corpus documents are labelled with topic, country
and industry codes (Lewis et al, 2004). Figure 2
reports F
1
on test documents from each frequent
topic. It highlights that much ambiguity remains
unresolved in Sports, while very high performance
linking is attainable in categories such as Markets
and Domestic Politics, only when given ground
truth linkable mentions.
7 Conclusion
We surveyed entity annotation tasks and advocated
a core set of metrics for mention, disambiguation
and end-to-end evaluation. This enabled a direct
comparison of state-of-the-art NEL and wikifica-
tion systems, highlighting the effect of key differ-
ences. In particular, NER and coreference mod-
ules make NEL approaches suitable for applica-
tions that require all mentions, including ambigu-
ous names and entities that are not in the KB. For
applications where document-level entity tags are
appropriate, the NEL and wikification approaches
we evaluate have similar performance.
The big picture we wish to convey is a new
approach to community evaluation that makes
benchmarking and qualitative comparison cheap
and easy. In addition to the code being open
source, we use the repository to store reference
system output, and ? we hope ? emendations to
the ground truth. We encourage other researchers
to contribute reference output and hope that this
will provide a continuous benchmark to comple-
ment the current cycle of shared tasks.
Acknowledgements
Many thanks to Johannes Hoffart, Marco Cornolti,
Xiao Ling and Edgar Meij for reference outputs
and guidance. Ben Hachey is the recipient of
an Australian Research Council Discovery Early
Career Researcher Award (DE120102900). The
other authors were supported by the Capital Mar-
kets CRC Computable News project.
468
References
Luisa Bentivogli, Pamela Forner, Claudio Giu-
liano, Alessandro Marchetti, Emanuele Pianta, and
Kateryna Tymoshenko. 2010. Extending English
ACE 2005 corpus annotation with ground-truth links
to Wikipedia. In COLING Workshop on The Peo-
ple?s Web Meets NLP: Collaboratively Constructed
Semantic Resources, pages 19?27.
Xiao Cheng, Bingling Chen, Rajhans Samdani, Kai-
Wei Chang, Zhiye Fei, Mark Sammons, John Wi-
eting, Subhro Roy, Chizheng Wang, and Dan Roth.
2013. Illinois cognitive computation group UI-CCG
TAC 2013 entity linking and slot filler validation
systems. In Text Analysis Conference.
Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmark-
ing entity-annotation systems. In 22nd International
Conference on the World Wide Web, pages 249?260.
Silviu Cucerzan and Avirup Sil. 2013. The MSR sys-
tems for entity linking and temporal slot filling at
TAC 2013. In Text Analysis Conference.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 708?716.
Angela Fahrni, Benjamin Heinzerling, Thierry G?ockel,
and Michael Strube. 2013. HITS? monolingual and
cross-lingual entity linking system at TAC 2013. In
Text Analysis Conference.
Paolo Ferragina and Ugo Scaiella. 2010. TAGME:
On-the-fly annotation of short text fragments (by
Wikipedia entities). In 19th International Confer-
ence on Information and Knowledge Management,
pages 1625?1628.
Antske Fokkens, Marieke van Erp, Marten Postma, Ted
Pedersen, Piek Vossen, and Nuno Freire. 2013. Off-
spring from reproduction problems: What replica-
tion failure teaches us. In 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1691?1701.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evaluat-
ing entity linking with Wikipedia. Artificial Intel-
ligence, 194:130?150.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Conference on Empirical Methods
in Natural Language Processing, pages 782?792.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In 49th Annual Meeting of the Association for Com-
putational Linguistics, pages 1148?1158.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of Wikipedia entities in web text. In 15th Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 457?466.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361?397.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In 5th
International Conference on Web Search and Data
Mining, pages 563?572.
Rada Mihalcea and Andras Csomai. 2007. Wik-
ify! Linking documents to encyclopedic knowledge.
In 16th Conference on Information and Knowledge
Management, pages 233?242.
David Milne and Ian H. Witten. 2008a. An effec-
tive, low-cost measure of semantic relatedness ob-
tained from Wikipedia links. In AAAI Workshop on
Wikipedia and Articial Intelligence, pages 25?30.
David Milne and Ian H. Witten. 2008b. Learning
to link with Wikipedia. In 17th Conference on In-
formation and Knowledge Management, pages 509?
518.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. John Wiley & Sons.
Anja Pilz and Gerhard Paass. 2012. Collective
search for concept disambiguation. In 24th Inter-
national Conference on Computational Linguistics,
pages 2243?2258.
Glen Pink, Will Radford, Will Cannings, Andrew
Naoum, Joel Nothman, Daniel Tse, and James R.
Curran. 2013. SYDNEY CMCRC at TAC 2013. In
Text Analysis Conference.
Will Radford, Will Cannings, Andrew Naoum, Joel
Nothman, Glen Pink, Daniel Tse, and James R.
Curran. 2012. (Almost) Total Recall ? SYD-
NEY CMCRC at TAC 2012. In Text Analysis Con-
ference.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1375?1384.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Conference On Computational Natural Language
Learning, pages 142?147.
469
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 29?30,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Tracking Information Flow between Primary and Secondary News Sources
Will Radford ?? Ben Hachey ? James R. Curran ?? Maria Milosavljevic 
School of Information Technologies? Capital Markets CRC? Centre for Language Technology
University of Sydney 55 Harrington Street Macquarie University
NSW 2006, Australia NSW 2000, Australia NSW 2109, Australia
{wradford,james}@it.usyd.edu.au bhachey@cmcrc.com mariam@ics.mq.edu.au
Abstract
Tracking information flow (IFLOW) is crucial
to understanding the evolution of news sto-
ries. We present analysis and experiments for
IFLOW between company announcements and
newswire. Error analysis shows that many FPs
are annotation errors and many FNs are due
to coarse-grained document-level modelling.
Experiments show that document meta-data
features (e.g., category, length, timing) im-
prove f-scores relative to upper bound by 23%.
1 Introduction
Tracking IFLOW between primary and secondary
news sources provides insight into the contribution
of participants and the role of sources. In finance,
being alert and responsive to the nature of incoming
information (e.g., novelty, price sensitivity) is cen-
tral to successful trading (Zaheer and Zaheer, 1997).
Traders need tools that flag price-sensitive informa-
tion in a high-volume news feed. IFLOW is central to
market surveillance, where unusual market activity
(e.g., abnormal changes in trading price or volume)
is linked to explanations in the information ecosys-
tem (Milosavljevic et al, 2009).
In Australia, the Australian Securities Exchange
(ASX) is the official syndicator of information that
might affect a company?s share price. Subsequently,
a variety of secondary sources (e.g., news media,
blogs, forums) repackage this information. We fo-
cus on the relationship between ASX company an-
nouncements and Reuters newswire, which filters
and aggregates the key details from company an-
nouncements in near-real time.
2 Preliminary Results
We define IFLOW for capital markets as a pair of
documents where one repeats price-sensitive infor-
mation from the other (Radford et al, 2009). Pairs
of ASX announcements and Reuters NewsScope
Archive (RNA) stories covering the same company
and released within a week of one another are man-
ually annotated for presence or absence of IFLOW.
These are used to train MEGAM (Daume? III, 2004)
maximum entropy models for identifying IFLOW.
Textual features include set-theoretic bags of word
unigrams and bigrams over the document text and
titles. Text, title and numeric token similarity scores
(Metzler et al, 2005) provide a more general no-
tion of similarity. The precision of numeric tokens is
also represented. Counts of matched sentences and
longest common sub-sequences capture longer units
of reused text. Temporal features model the news
cycle and news source responsiveness.
In development experiments (ten-fold cross vali-
dation, 30,249 ASX-RNA pairs), the system identi-
fies IFLOW pairs at 89.5% f-score (Radford et al,
2009). In evaluation experiments (held-out test set,
1,621 ASX-RNA pairs), it achieves 76.6% f-score,
significantly better than a text-only baseline (62.5%)
and 10% less than the human upper bound (86.4%).
3 Error analysis
We engaged finance students (fourth-year or higher)
to examine the 20 false positive (FP) errors with the
highest IFLOW probabilities and the 20 false neg-
ative (FN) errors with the lowest IFLOW probabili-
ties. Table 1 shows the resulting reassessment of the
29
Error Correct Incorrect Ambiguous
FP 4 (20%) 15 (75%) 1 (5%)
FN 15 (75%) 4 (20%) 1 (5%)
Table 1: Analysis of original annotation correctness.
original IFLOW annotation. For FPs, 75% were de-
termined to have been incorrectly annotated as ab-
sent of IFLOW. This is not unexpected since IFLOW
can be based on small details (e.g., ?$2.45m profit?)
which are easily missed by annotators. This suggests
that the system?s actual precision may be higher
than 90.9%. Mis-annotation is less common for FNs
(20%). However, the proportion of DIGEST docu-
ments (those that report on multiple events) is much
higher for FNs (75% compared to 30% for FPs). It is
likely that legitimate textual similarity is lost in the
noise of the irrelevant content.
4 Document Metadata Features
We add new features that take advantage of categori-
sation information in the source metadata. These in-
clude ASX tags for price sensitivity, ASX and RNA
type tags and journalist revision comments embed-
ded in RNA stories. These features model differ-
ences in IFLOW between document types (e.g., pe-
riodic reports are more likely to be reported than a
dividend rate announcement). A feature represent-
ing the length of each ASX-RNA document is also in-
cluded. We also add detail to the temporal features,
including the day and month the announcement was
released, as well as whether the announcement and
story were released on the same day.
The metadata features lead to significantly better
f-score in development experiments (Table 2). Sub-
tractive feature analysis suggests that the document
type and length features are effective (p<0.05) but
the detailed temporal features are not. The revision
comments are borderline (p=0.051). In Table 3,
the metadata features improve the f-score by 23%
over Radford et al (2009) with respect to the upper
bound, but the difference is not significant. The dif-
ferent precision-recall balance between experiments
is consistent with Section 3.
5 Discussion and Future Work
We have developed a dataset for IFLOW in the con-
text of financial text mining and demonstrated it is a
Features P (%) R (%) F (%)
Radford et al (2009) 90.9 88.1 89.5
+ Metadata Features 91.1 ??89.3 ?90.2
Table 2: Precision (P), recall (R) and f-score (F) for de-
velopment experiments (?: p<0.05, ??: p<0.01).
Features P (%) R (%) F (%)
Text-only Baseline 80.0 51.3 62.5
Radford et al (2009) 84.5 70.1 76.6
+ Metadata Features 86.3 72.6 78.9
Human Upper Bound 88.9 85.1 86.4
Table 3: P, R and F for evaluation experiments.
feasible task using simple approaches. Future work
will involve more advanced models. First, we will
consider sub-document analysis, as suggested by the
DIGEST FNs in the error analysis. This will also en-
able tools that highlight specific types of contribu-
tion (e.g., adding background context, novel anal-
ysis) within secondary sources. Furthermore, the
wider IFLOW ecosystem includes other sources (e.g.,
bloggers, forum contributors) that should be anal-
ysed for leading and lagging indicators. Finally, a
number of specific applications might serve as ex-
trinsic evaluations of the IFLOW task. These include
de-duplicating and aggregating information feeds
and automatically attributing reported content to a
source story.
References
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. http://hal3.
name/docs/daume04cg-bfgs.pdf.
Donald Metzler, Yaniv Bernstein, W. Bruce Croft, Alis-
tair Moffat, and Justin Zobel. 2005. Similarity mea-
sures for tracking information flow. In Proc. CIKM,
pages 517?524.
Maria Milosavljevic, Jean-Yves Delort, Ben Hachey, Ba-
vani Arunasalam, Will Radford, and James R. Curran.
2009. Automating financial surveillance. In Proceed-
ings of the Workshop on Mining User-Generated Con-
tent for Security.
Will Radford, Ben Hachey, James R. Curran, and Maria
Milosavljevic. 2009. Tracking information flow in fi-
nancial text. In Proc. ALTA, pages 11?19.
Akbar Zaheer and Srilata Zaheer. 1997. Catching the
wave: alertness, responsiveness, and market influence
in global electronic networks. Management Science,
43(11):1493?1509.
30
