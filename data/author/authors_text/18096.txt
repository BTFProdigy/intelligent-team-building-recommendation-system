Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104?113,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Rare Social Phenomena in Conversation:
Empowerment Detection in Support Group Chatrooms
Elijah Mayfield, David Adamson, and Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA 15213
{emayfiel, dadamson, cprose}@cs.cmu.edu
Abstract
Automated annotation of social behavior
in conversation is necessary for large-scale
analysis of real-world conversational data.
Important behavioral categories, though,
are often sparse and often appear only
in specific subsections of a conversation.
This makes supervised machine learning
difficult, through a combination of noisy
features and unbalanced class distribu-
tions. We propose within-instance con-
tent selection, using cue features to selec-
tively suppress sections of text and bias-
ing the remaining representation towards
minority classes. We show the effective-
ness of this technique in automated anno-
tation of empowerment language in online
support group chatrooms. Our technique
is significantly more accurate than multi-
ple baselines, especially when prioritizing
high precision.
1 Introduction
Quantitative social science research has experi-
enced a recent expansion, out of controlled set-
tings and into natural environments. With this
influx of interest comes new methodology, and
the inevitable question arises of how to move
towards testable hypotheses, using these uncon-
trolled sources of data as scientific lenses into the
real world.
The study of conversational transcripts is a key
domain in this new frontier. There are certain
social and behavioral phenomena in conversation
that cannot be easily identified through question-
naire data, self-reported surveys, or easily ex-
tracted user metadata. Examples of these social
phenomena in conversation include overt displays
of power (Prabhakaran et al, 2012) or indicators
of rapport and relationship building (Wang et al,
2012). Manually annotating these social phenom-
ena cannot scale to large data, so researchers turn
to automated annotation of transcripts (Rose? et al,
2008). While machine learning is highly effec-
tive for annotation tasks with relatively balanced
labels, such as sentiment analysis (Pang and Lee,
2004), more complex social functions are often
rarer. This leads to unbalanced class label distri-
butions and a much more difficult machine learn-
ing task. Moreover, features indicative of rare so-
cial annotations tend to be drowned out in favor of
features biased towards the majority class. The net
effect is that classification algorithms tend to bias
towards the majority class, giving low accuracy for
rare class detection.
Automated annotation of social phenomena also
brings opportunities for real-world applications.
For example, real-time annotation of conversation
can power adaptive intervention in collaborative
learning settings (Rummel et al, 2008; Adamson
and Rose?, 2012). However, with the considerable
power of automation comes great responsibility. It
is critical to avoid intervening in the case of er-
roneous annotations, as providing unnecessary or
inappropriate support in such a setting has been
shown to be harmful to group performance and so-
cial cohesion (Dillenbourg, 2002; Stahl, 2012).
We propose adaptations to existing machine
learning algorithms which improve recognition of
rare annotations in conversational text data. Our
primary contribution comes in the form of within-
instance content selection. We develop a novel al-
gorithm based on textual cues, suppressing infor-
mation which is likely to be irrelevant to an in-
stance?s class label. This allows features which
predict minority classes to gain prominence, help-
ing to sidestep the frequency of common features
pointing to a majority class label.
Additionally, we propose modifications to ex-
isting algorithms. First, we identify a new appli-
cation of logistic model trees to text data. Next,
104
we define a modification of confidence-based en-
semble voting which encourages minority class la-
beling. Using these techniques, we demonstrate a
significant improvement in classifier performance
when recognizing the language of empowerment
in support group chatrooms, a critical application
area for researchers studying conversational inter-
actions in healthcare (Uden-Kraan et al, 2009).
The remainder of this paper is structured as fol-
lows. We introduce the domain of empowerment
in support contexts, along with previous studies on
the challenges that these annotations (and similar
others) bring to machine learning. We introduce
our new technique for improving the ability to au-
tomate this annotation, along with other optimiza-
tions to the machine learning workflow which are
tailored to this skewed class balance. We present
experimental results showing that our method is
effective, and provide a detailed analysis of the be-
havior of our model and the features it uses most.
We conclude with a discussion of particularly use-
ful applications of this work.
2 Background
We ground this paper?s discussion of machine
learning with a real problem, turning to the an-
notation of empowerment language in chat1. The
concept of empowerment, while a prolific area
of research, lacks a broad definition across pro-
fessionals, but broadly relates to ?the power to
act efficaciously to bring about desired results?
(Boehm and Staples, 2002) and ?experiencing per-
sonal growth as a result of developing skills and
abilities along with a more positive self-definition?
(Staples, 1990). Participants in online support
groups feel increased empowerment (Uden-Kraan
et al, 2009; Barak et al, 2008). Quantita-
tive studies have shown the effect of empower-
ment through statistical methods such as structural
equation modeling (Vauth et al, 2007), as have
qualitative methods such as deductive transcript
analysis (Owen et al, 2008) and interview studies
(Wahlin et al, 2006).
The transition between these styles of research
has been gradual. Pioneering work has demon-
strated the ability to distinguish empowerment lan-
guage in written texts, including prompted writ-
ing samples (Pennebaker and Seagal, 1999), nar-
1Definitions of empowerment are closely related to the
notion of self-efficacy (Bandura, 1997). For simplicity, we
use the former term exclusively in this paper.
Table 1: Empowerment label distribution in our
corpus.
Annotation Label # %
Self-Empowerment NA 1522 79.3
POS 202 10.5
NEG 196 10.2
Other-Empowerment NA 1560 81.3
POS 217 11.3
NEG 143 7.4
ratives in online forums (Hoybye et al, 2005), and
some preliminary analysis of synchronous discus-
sion (Ogura et al, 2008; Mayfield et al, 2012b).
These transitional works have used limited analy-
sis methodology; in the absence of sophisticated
natural language processing, their conclusions of-
ten rely on coarse measures, such as word counts
and proportions of annotations in a text.
Users, of course, do not express empowerment
in every thread in which they participate, which
leads to a challenge for machine learning. Threads
often focus on a single user?s experiences, in
which most participants in a chat are merely com-
mentators, if they participate at all, matching pre-
vious research on shifts in speaker salience over
time (Hassan et al, 2008). This leads to many
user threads which are annotated as not applicable
(N/A). We move to our proposed approach with
these skewed distributions in mind.
3 Data
Our data consists of a set of chatroom conversa-
tion transcripts from the Cancer Support Commu-
nity2. Each 90-minute conversation took place in
the context of a weekly meeting in a real-time chat,
with up to 6 participants in addition to a profes-
sional therapist facilitating the discussion. In to-
tal, 2,206 conversations were collected from 2007-
2011. This data offers potentially rich insight into
coping and social support; however, annotating
such a dataset by hand would be prohibitively ex-
pensive, even when it is already transcribed.
Twenty-one of these conversations have been
annotated, as originally described and analyzed
in (Mayfield et al, 2012b)3. This data was dis-
entangled into threads based on common themes
or topics, as in prior work (Elsner and Charniak,
2www.cancersupportcommunity.org
3All annotations were found to be adequately reliable be-
tween humans, with thread disentanglement f = 0.75 and
empowerment annotation ? > 0.7.
105
Figure 1: An example mapping from a single thread?s chat lines (left) to the per-user, per-thread instances
used for classification in this paper (right), with example annotations for self-empowerment indicated.
2010; Adams and Martel, 2010). A novel per-
user, per-thread annotation was then employed
for empowerment annotation, following a coding
manual based on definitions like those in Section
2. Each user was assigned a label of positive
or negative empowerment if they exhibited such
emotions, or was left blank if they did not do so
within the context of that thread. This annotation
was performed both for their self-empowerment
as well as their attitude towards others? situations
(other-empowerment). An example of this annota-
tion for self-empowerment is presented in Figure
1 and the distribution of labels is given in Table 1.
Most previous annotation tasks attempt to an-
notate on a per-utterance basis, such as dialogue
act tagging (Popescu-Belis, 2008), or on arbitrary
spans of text, such as in the MPQA subjectivity
corpus (Wiebe et al, 2005). However, for our task,
a per-user, per-thread annotation is more appropri-
ate, because empowerment is often indicated best
through narrative (Hoybye et al, 2005). Human
annotators are instructed to take this context into
account when annotating (Mayfield et al, 2012b).
It would therefore be nonsensical to annotate indi-
vidual lines as ?embodying? empowerment. Simi-
lar arguments have been made for sentiment, espe-
cially as the field moves towards aspect-oriented
sentiment (Breck et al, 2007). Assigning labels
based on thread boundaries allows for context to
be meaningfully taken into account, without cross-
ing topic boundaries.
However, this granularity comes with a price:
the distribution of class values in these instances
is highly skewed. In our data, the vast majority of
users? threads are marked as not applicable to em-
powerment. Perhaps more inconveniently, while
taking context into account is important for reli-
able annotation, it leads to extraneous information
in many cases. Many threads can have multiple
lines of contributions that are topically related to
an expression of empowerment (and thus belong
in the same thread), but which do not indicate any
empowerment themselves. This exacerbates the
likelihood of instances being classified as N/A.
We choose to take advantage of these attributes
of threads. We know from research in discourse
analysis that many sections of conversations are
formulaic and rote, like introductions and greet-
ings (Schegloff, 1968). We additionally know that
polarity often shifts in dialogue through the use
of discourse connectives such as conjunctions and
transitional phrases. These issues have been ad-
dressed in work in the language technologies com-
munity, most notably through the Penn Discourse
Treebank (Prasad et al, 2008); however, their ap-
plications to noisier synchronous conversation has
beenrare in computational linguistics.
With these linguistic insights in mind, we ex-
amine how we can make best use of them for
machine learning performance. While techniques
for predicting rare events (Weiss and Hirsh, 1998)
and compensating for class imbalance (Frank and
106
Bouckaert, 2006), these approaches generally fo-
cus on statistical properties of large class sets with-
out taking the nature of their datasets into account.
In the next section, we propose a new algorithm
which takes advantage specifically of the linguis-
tic phenomena in the conversation-based data that
we study for empowerment detection. As such,
our algorithm is highly suited to this data and task,
with the necessary tradeoff in uncertain generality
to new domains with unrelated data.
4 Cue Discovery for Content Selection
Our algorithm performs content selection by
learning a set of cue features. Each of these fea-
tures indicates some linguistic function within the
discourse which should downplay the importance
of features either before or after that discourse
marker. Our algorithm allows us to evaluate the
impact of rules against a baseline, and to itera-
tively judge each rule atop the changes made by
previous rules.
This algorithm fits into existing language tech-
nologies research which has attempted to partition
documents into sections which are more or less
relevant for classification. Many researchers have
attempted to make use of cue phrases (Hirschberg
and Litman, 1993), especially for segmentation
both in prose (Hearst, 1997) and conversation
(Galley et al, 2003). The approach of content se-
lection, meanwhile, has been explored for senti-
ment analysis (Pang and Lee, 2004), where indi-
vidual sentences may be less subjective and there-
fore less relevant to the sentiment classification
task. It is also similar conceptually to content
selection algorithms that have been used for text
summarization (Teufel and Moens, 2002) and text
generation (Sauper and Barzilay, 2009), both of
which rely on finding highly-relevant passages
within source texts.
Our work is distinct from these approaches.
While we have coarse-grained annotations of em-
powerment, there is no direct annotation of what
makes a good cue for content selection. With
our cues, we hope to take advantage of shallow
discourse structure in conversation, such as con-
trastive markers, making use of implicit structure
in the conversational domain.
4.1 Notation
Before describing extensions to the baseline lo-
gistic regression model, we define notation. Our
data is arranged hierarchically. We assume that
we have a collection of d training documents Tr =
{D1 . . . Dd}, each of which contains many train-
ing instances (in our task, an instance consists of
all lines of chat from one user in one thread). Our
total set of n instances I thus consists of instances
{I1, I2, . . . In}. Each document contains lines of
chat L and each instance Ii is comprised of some
subset of those lines, Li ? L.
Our feature space X = {x1, x2, . . . xm} con-
sists of m unigram features representing the ob-
served vocabulary used in our corpus. Each in-
stance is associated with a feature vector x? con-
taining values for each x ? X, and each feature
x that is present in the i-th instance maintains a
?memory? of the lines in which it appeared in that
instance, Lix, where Lix ? Li. Our potential out-
put labels consist of Y = {NA,NEG,POS},
though this generalizes to any nominal classifica-
tion task. Each instance I is associated with ex-
actly one y ? Y for self-empowerment and one
for other-empowerment; these two labels do not
interact and our tasks are treated as independent
in this paper4. We define classifiers as functions
f(x?? y ? Y); in practice, we use logistic regres-
sion via LibLINEAR (Fan et al, 2008).
We define a content selection rule as a pairing
r = ?c, t? between a cue feature c ? X and a se-
lection function t ? T . We created a list of possi-
ble selection functions, given a cue c, maximizing
for generality while being expressive. These are
illustrated in Figure 2 and described below:
? Ignore Local Future (A): Ignore all features
from the two lines after each occurrence of c.
? Ignore All Future (B): Ignore all features
occurring after the first occurrence of c.
? Ignore Local History (C): Ignore all features
in the two lines preceding each occurrence of
c.
? Ignore All History (D): Ignore all features
occurring only before the last occurrence of
c.
We define an ensemble member E = ?R, fR? -
the ordered list of learned content selection rules
R = [r1, r2, . . . ] and a classifier fC trained on in-
stances transformed by those rules. Our final out-
4Future work may examine the interaction of jointly an-
notating multiple sparse social phenomena.
107
Figure 2: Effects of content selection rules, based
on a cue feature (ovals) observed at lines m and n.
put of a trained model is a set of ensemble mem-
bers {E1, . . . , Ek}.
4.2 Algorithm
Our ensemble learning follows the paradigm
of cross-validated committees (Parmanto et al,
1996), where k ensemble members are trained by
subdividing our training data into k subfolds. For
each ensemble classifier, cue rulesR are generated
on k ? 1 subfolds (Trk) and evaluated on the re-
maining subfold (Tek). In practice, with 21 train-
ing documents, 7-fold cross-validation, and k = 3
ensemble members, each generation set consists
of 12 documents? instances, while each evaluation
set contains instances from 6 documents.
Our full algorithm is presented in Algorithm
1, and is broken into component parts for clar-
ity. Algorithm 2 begins by measuring the base-
line classifier?s ability to recognize minority-class
labels. After training on Trk, we measure the
average probability assigned to the correct label
of instances in Tek, but only for instances whose
correct labels are minority classes (remember, be-
cause both Trk and Tek are drawn from the over-
all Tr, we have access to true class labels). We
choose this subset of only minority instances, as
we are not interested in optimizing to the majority
class.
We next enumerate all rules that we wish to
judge. To keep this problem tractable, we ignore
features which do not occur in at least 5% of train-
ing instances. For the remaining features, we cre-
ate a candidate rule for each possible pairing of
features and selection functions. For each of these
candidates, we test its utility by selecting content
as if it were an actual rule, then building a new
classifier (trained on the generation set) using in-
stances that have been altered in that way. In the
evaluation set, we measure the difference in prob-
ability of minority class labels being assigned cor-
rectly between the baseline and this altered space.
This measure of an individual rule?s impact is de-
scribed in Algorithm 3.
Once we have evaluated every possible rule
once, we select the top-ranked rule and ap-
ply it to the feature set. We then iteratively
progress through our now-ranked list of candi-
dates, each time treating the newly filtered dataset
as our new baseline. We search only top can-
didates for efficiency, following the fixed-width
search methodology for feature selection in very
high-dimensionality feature spaces (Gu?tlein et al,
2009). Each ensemble classifier is finally retrained
on all training data, after applying the correspond-
ing content selection rules to that data.
5 Prediction
Our prediction algorithm begins with a stan-
dard implementation of cross-validated commit-
tees (Parmanto et al, 1996), whose results are
aggregated with a confidence voting method in-
tended to favor rare labels (Erp et al, 2002).
Cross-validated committees are an ensemble tech-
nique used to subsample training data to produce
multiple hypotheses for classification. Each clas-
sifier produced by our cue-based transformation
is trained on a subset of our training data. Each
makes predictions on all test set instances, pro-
ducing a distribution of confidence across possi-
ble labels. These values serve as inputs to a voting
method to produce a final label for each instance.
Compared to other ensemble methods, cross-
validated committees as described above are a
good fit for our task, because of its unique unit of
analysis. As thread-level analysis is the set of in-
dividual participants? turns in a conversation, we
risk overfitting if we sample from the same con-
versations for the training and testing sets. In con-
trast to standard bagging, hard sampling bound-
aries never train and test on instances drawn from
the same conversation.
To aggregate the votes from members of this en-
semble into a final prediction, we employ a variant
on Selfridge?s Pandemonium (Selfridge, 1958).
If a minority label is selected as the highest-
confidence value in any classifier in our ensem-
ble, it is selected. The majority label, by contrast,
is only selected if it is the most likely prediction
by all classifiers in our ensemble. Thus consen-
sus is required to elect the majority class, and the
strongest minority candidate is elected otherwise.
108
In : generation set Trk, evaluation set Tek
Out: ensemble committee {E1 . . . Ek}
for i = 1 to k do
Rfinal ? [ ];
Xfreq ? {x ? X | freq(x) ? Trk >
5%};
R? Xfreq ? T ;
R? ? R;
repeat
Pbase ? EvaluateClassifier(Trk,Tek);
EvaluateRules(Pbase,Trk,Tek, R?);
Trk,Tek ? ApplyRule(R?[0]);
R? R?R?[0];
?? score(R?[0]);
Rfinal ? Rfinal +R?[0];
R? ? R[0 . . . 50];
until ? < threshold;
Trfinal ? Trk ? Tek;
foreach r ? Rfinal do
Trfinal ? ApplyRule(Trfinal, r);
end
Train f(x?? y) on Trfinal;
end
Algorithm 1: LearnSelectionCues()
This approach is designed to bias the prediction
of our machine learning algorithms in favor of mi-
nority classes in a coherent manner. If there is a
plausible model that has been trained which rec-
ognizes the possibility of a rare label, it is used;
the prediction only reverts to the majority class
when no plausible minority label could be chosen.
As validation of this technique, we compare our
?minority pandemonium? approach against both
typical pandemonium and standard sum-rule con-
fidence voting (Erp et al, 2002).
5.1 Logistic Model Stumps
One characteristic of highly skewed data is that,
while minority labels may be expressed in a num-
ber of different surface forms, there are many ob-
vious cases in which they do not apply. These
cases can actually be harmful to classification of
borderline cases. Features that could be given high
weight in marginal cases may be undervalued in
?low-hanging fruit? easy cases. To remove those
obvious instances, a very simple screening heuris-
tic is often enough to eliminate frequent pheno-
types of instances where the rare annotation is
not present. Prior work has sometimes screened
training data through obvious heuristic rules, espe-
In : generation set Trk, evaluation set Tek
Out: minority class probability average Pbase
Train f(x?? y) on Trk;
Temink ? {Instance I ? Tek | yI 6= ?NA?}
;
Pbase ? 0 ;
foreach Instance I ? Temink do
Pbase ? Pbase + P (f(x?I) = yI)
end
Pbase = Pbase/size(Temink )Algorithm 2: EvaluateClassifier()
In : Trk, Tek, rules R, base probability Pbase
Out: R sorted on each rule?s improvement
score
foreach Rule r ? R do
Tr?k,Te?k ? ApplyRule(Trk,Tek, r);
Palter ? EvaluateClassifier(Tr?k,Te?k);
score(r)? Palter ? Pbase;
end
Sort R on score(r) from high to low;
Algorithm 3: EvaluateRules()
cially in speech recognition; for instance, training
speech recognition for words followed by a pause
separately from words followed by another word
(Franco et al, 2010), or training separate models
based on gender (Jiang et al, 1999).
We achieve this instance screening by learn-
ing logistic model tree stumps (Landwehr et al,
2005), which allow us to quickly partition data if
there is a particularly easy heuristic that can be
learned to eliminate a large number of majority-
class labels. One challenge of this approach is
our underlying unigram feature space - tree-based
algorithms are generally poor classifiers for the
high-dimensionality, low-information features in a
lexical feature space (Han et al, 2001). To com-
pensate, we employ a smaller, denser set of binary
features for tree stump screening: instance length
thresholds and LIWC category membership.
First, we define a set of features that split based
on the number of lines an instance contains, from
1 to 10 (only a tiny fraction of instances are more
than 10 lines long). For example, a feature split-
ting on instances with lines ? 2 would be true
for one- and two-line instances, and false for all
others. Second, we define a feature for each cate-
gory in the Linguistic Inquiry and Word Count dic-
tionary (Tausczik and Pennebaker, 2010) - these
broad classes of words allow for more balanced
109
Figure 3: Precision/recall curves for algorithms.
After 50% recall all models converge and there are
no significant differences in performance.
splits than would unigrams alone. Each category?s
feature is true if any word in that category was
used at least once in that instance.
We exhaustively sweep this feature space, and
report the most successful stump rules for each an-
notation task. In our other experiments, we report
results with and without the best rule for this pre-
processing step; we also measure its impact alone.
6 Experimental Results
All experiments were performed using LightSIDE
(Mayfield and Rose?, 2013). We use a binary uni-
gram feature space, and we perform 7-fold cross-
validation. Instances from the same chat transcript
never occur in both train and testing folds. Fur-
thermore, we assume that threads have been dis-
entangled already, and our experiments use gold
standard thread structure. While this is not a triv-
ial assumption, prior work has shown thread dis-
entanglement to be manageable (Mayfield et al,
2012a); we consider it an acceptable simplify-
ing assumption for our experiments. We compare
our methods against baselines including a majority
baseline, a baseline logistic regression classifier
with L2 regularized features, and two common en-
semble methods, AdaBoost (Freund and Schapire,
1996) and bagging (Breiman, 1996) with logistic
regression base classifiers5.
Table 2 presents the best-performing result
from each classification method. For self-
empowerment recognition, all methods that we
introduce are significant improvements in ?, the
5These methods usually use weak, unstable base classi-
fiers; however, in our experiments, those performed poorly.
Table 2: Performance for baselines, common en-
semble algorithms, and proposed methods. Statis-
tically significant improvements over baseline are
marked (p < .01, ?; p < .05, *; p < 0.1, +).
Self Other
Method % ? % ?
Majority 79.3 .000 81.3 .000
LR Baseline 81.0 .367 81.0 .270
LR + Boosting 78.1 .325 78.5 .275
LR + Bagging 81.2 .352 81.9 .265
LR + Committee 81.0 .367 81.0 .270
Learned Stumps 81.8* .385? 81.7 .293+
Content Selection 80.9 .389? 80.7 .282
Stumps+Selection 81.3 .406? 79.4 .254
Table 3: Performance of content-selection
wrapped learners, for minority voting and two
baseline voting methods.
Self Other
Method % ? % ?
Pandemonium 80.3 .283 81.4 .239
Averaged 80.6 .304 81.6 .251
Minority Voting 80.9? .389? 80.7 .282
measurement of agreement over chance, compared
to all baselines. While accuracy remains stable,
this is due to predictions shifting away from the
majority class and towards minority classes. Our
combined model using both logistic model tree
stumps and content selection is significantly better
than either alone (p < .01). To compare the mi-
nority pandemonium voting method against base-
lines of simple pandemonium and summed confi-
dence voting, Table 3 presents the results of con-
tent selection wrappers with each voting method.
Minority voting is more effective compared to
standard confidence voting, improving ? while
modestly reducing accuracy; this is typical of a
shift towards minority class predictions.
7 Discussion
These results show promise for our techniques,
which are able to distinguish features of rare la-
bels, previously awash in a sea of irrelevance. Fig-
ure 3 shows the impact of our rules as we tune
to different levels of recall, with a large boost in
precision when recall is not important; our model
converges with the baseline for high-recall, low-
precision tuning. This suggests that our method is
particularly suitable for tasks where confident la-
110
Table 4: Cue rules commonly selected by the algo-
rithm. Average improvement over the LR baseline
is also shown.
Self-Empowerment
Cue Transformation ?%
and,but Ignore Local Future +5.0
have Ignore All History +4.3
! Ignore All History +4.2
me,my Ignore All History +3.4
Other-Empowerment
Cue Transformation ?%
and,but Ignore Local Future +5.5
you Ignore Local History +5.2
?s Ignore Local History +4.1
that Ignore Local History +3.9
beling of a few instances is more important than
labeling as many instances as possible. This is
common when tasks have a high cost or carry high
risk (for instance, providing real-time conversa-
tional supports with an agent, where inappropriate
intervention could be disruptive). Other low-recall
applications include exploration large corpora for
exemplar instances, where the most confident pre-
dictions for a given label should be presented first
for analyst use. In the rest of this section, we
examine notable within-instance and per-instance
rules selected by our methods. These rules are
summarized in Tables 4 and 5.
For both self- and other-empowerment, we find
pronoun rules that match the task (first-person and
second-person pronouns for self-Empowerment
and other-Empowerment respectively). In both
tasks, we find cue rules that suppress the context
preceding personal pronouns. These, as well as
the possessive suffix ?s, echo the per-instance ef-
fect of the Self and You splits, anticipating that
what follows such a personal reference is likely to
bear an evaluation of empowerment. Exclamation
marks may indicate strong emotion - we find many
instances where what precedes a line with an ex-
clamation is more objective, and what follows in-
cludes an assessment. Conjunctions but and and
are selected as cue rules suppressing the two lines
that follow the occurrence - suggesting, as sus-
pected, that connective discourse markers play a
role in indicating empowerment (Fraser, 1999).
The best-performing stump splits for the Self-
Empowerment annotation are Line Length ? 1
and the LIWC word-categories Article, Swear, and
Table 5: Best decision rules for logistic model
stumps. Significant improvement (p < 0.05) in-
dicated with *.
Self-Empowerment
Split Rule ? ?? % ?%
Split ? 1 * 0.385 +.018 81.8 +0.8
LIWC-Article 0.379 +.012 81.6 +0.6
LIWC-Swear * 0.376 +.009 81.4 +0.4
LIWC-Self * 0.376 +.009 81.5 +0.5
Other-Empowerment
Split Rule ? ?? % ?%
LIWC-You 0.293 +.023 81.7 +0.7
LIWC-Eating * 0.283 +.013 81.6 +0.6
LIWC-Negate * 0.282 +.012 82.3 +1.3
LIWC-Present 0.281 +.011 81.6 +0.6
Self. The split on line length corresponds to the
observation that longer instances provide greater
opportunity for personal narrative self-assessment
to occur (95% of single-line instances are labeled
NA). The Article category may serve as a proxy for
content length - article-less instances in our corpus
include one-line social greetings and exchanges
of contact information. Swear words may be a
cue for awareness of self-empowerment - a recent
study of women coping with illness reported that
swearing in the presence of others, but not alone,
was related to potentially harmful outcomes (Rob-
bins et al, 2011). Among other- oriented split
rules, Eating stands out as non-obvious, although
medical literature has suggested a link between
dietary behavior and empowerment attitudes in a
study of women with cancer (Pinto et al, 2002).
8 Conclusion
We have demonstrated an algorithm for improv-
ing automated classification accuracy on highly
skewed tasks for conversational data. This algo-
rithm, particularly its focus on content selection, is
rooted in the structural format of our data, which
can generalize to many tasks involving conversa-
tional data. Our experiments show that this model
significantly improves machine learning perfor-
mance. Our algorithm is taking advantage of
structural facets of discourse markers, lending ba-
sic sociolinguistic validity to its behavior. Though
we have treated each of these rarely-occurring la-
bels as independent thus far, in practice we know
that this is not the case. Joint prediction of labels
through structured modeling is an obvious next
111
step for improving classification accuracy.
This is an important step towards large-scale
analysis of the impact of support groups on pa-
tients and caregivers. Our method can be used to
confidently highlight occurrences of rare labels in
large data sets. This has real-world implications
for professional intervention in social conversa-
tional domains, especially in scenarios where such
an intervention is likely to be associated with a
high cost or high risk. With the construction of
more accurate classifiers, we open the possibility
of automating annotation on large conversational
datasets, enabling new directions for researchers
with domain expertise.
Acknowledgments
The research reported here was supported by Na-
tional Science Foundation grant IIS-0968485.
References
Paige Adams and Craig Martel. 2010. Conversational
thread extraction and topic detection in text-based
chat. In Semantic Computing.
David Adamson and Carolyn Penstein Rose?. 2012.
Coordinating multi-dimensional support in collabo-
rative conversational agents. In Proceedings of In-
telligent Tutoring Systems.
Albert Bandura. 1997. Self-Efficacy: The Exercise of
Control.
Azy Barak, Meyran Boniel-Nissim, and John Suler.
2008. Fostering empowerment in online support
groups. Computers in Human Behavior.
A Boehm and L H Staples. 2002. The functions of the
social worker in empowering: The voices of con-
sumers and professionals. Social Work.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI.
Leo Breiman. 1996. Bagging predictors. Machine
Learning.
Pierre Dillenbourg. 2002. Over-scripting cscl: The
risks of blending collaborative learning with instruc-
tional design. Three worlds of CSCL. Can we sup-
port CSCL?
Micha Elsner and Eugene Charniak. 2010. Disentan-
gling chat. Computational Linguistics.
Merijn Van Erp, Louis Vuurpijl, and Lambert
Schomaker. 2002. An overview and comparison of
voting methods for pattern recognition. In Frontiers
in Handwriting Recognition. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification.
Horacio Franco, Harry Bratt, Romain Rossier,
Venkata Rao Gadde, Elizabeth Shriberg, Victor
Abrash, and Kristin Precoda. 2010. Eduspeak: A
speech recognition and pronunciation scoring toolkit
for computer-aided language learning applications.
Language Testing.
Eibe Frank and Remco R Bouckaert. 2006. Naive
bayes for text classification with unbalanced classes.
Knowledge Discovery in Databases.
Bruce Fraser. 1999. What are discourse markers?
Journal of pragmatics, 31(7):931?952.
Yoav Freund and Robert E Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of ICML.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of ACL.
Martin Gu?tlein, Eibe Frank, Mark Hall, and Andreas
Karwath. 2009. Large-scale attribute selection us-
ing wrappers. In Proceedings of IEEE CIDM.
Eui-Hong Han, George Karypis, and Vipin Kumar.
2001. Text categorization using weight adjusted
k-nearest neighbor classification. Lecture Notes in
Computer Science: Advances in Knowledge Discov-
ery and Data Mining.
Ahmed Hassan, Anthony Fader, Michael H Crespin,
Kevin M Quinn, Burt L Monroe, Michael Colaresi,
and Dragomir R Radev. 2008. Tracking the dy-
namic evolution of participant salience in a discus-
sion. In Proceedings of Coling.
Marti A Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics.
Mette Terp Hoybye, Christoffer Johansen, and Tine
Tjornhoj-Thomsen. 2005. Online interaction ef-
fects of storytelling in an internet breast cancer sup-
port group. Psycho-oncology.
Hui Jiang, Keikichi Hirose, and Qiang Huo. 1999. Ro-
bust speech recognition based on a bayesian predic-
tion approach. In IEEE Transactions on Speech and
Audio Processing.
Niels Landwehr, Mark Hall, and Eibe Frank. 2005.
Logistic model trees. Machine Learning.
Elijah Mayfield and Carolyn Penstein Rose?. 2013.
Lightside: Open source machine learning for text.
In Handbook of Automated Essay Evaluation: Cur-
rent Applications and New Directions.
112
Elijah Mayfield, David Adamson, and Carolyn Pen-
stein Rose?. 2012a. Hierarchical conversation struc-
ture prediction in multi-party chat. In Proceedings
of SIGDIAL Meeting on Discourse and Dialogue.
Elijah Mayfield, Miaomiao Wen, Mitch Golant, and
Carolyn Penstein Rose?. 2012b. Discovering habits
of effective online support group chatrooms. In
ACM Conference on Supporting Group Work.
Kanayo Ogura, Takashi Kusumi, and Asako Miura.
2008. Analysis of community development using
chat logs: A virtual support group of cancer patients.
In Proceedings of the IEEE Symposium on Universal
Communication.
Jason E. Owen, Erin O?Carroll Bantum, and Mitch
Golant. 2008. Benefits and challenges experienced
by professional facilitators of online support groups
for cancer survivors. In Psycho-Oncology.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the Association for Computational Linguistics.
Bambang Parmanto, Paul Munro, and Howard R
Doyle. 1996. Improving committee diagnosis with
resampling techniques. In Proceedings of NIPS.
James W Pennebaker and J D Seagal. 1999. Forming
a story: The health benefits of narrative. Journal of
Clinical Psychology.
Bernardine M Pinto, Nancy C Maruyama, Matthew M
Clark, Dean G Cruess, Elyse Park, and Mary
Roberts. 2002. Motivation to modify lifestyle risk
behaviors in women treated for breast cancer. In
Mayo Clinic Proceedings.
Andrei Popescu-Belis. 2008. Dimensionality of di-
alogue act tagsets: An empirical analysis of large
corpora. In Language Resources and Evaluation.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting overt display of power in
written dialogs. In Proceedings of NAACL.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Megan L Robbins, Elizabeth S Focella, Shelley Kasle,
Ana Mar??a Lo?pez, Karen L Weihs, and Matthias R
Mehl. 2011. Naturalistically observed swear-
ing, emotional support, and depressive symptoms
in women coping with illness. Health Psychology,
30:789.
Carolyn Penstein Rose?, Yi-Chia Wang, Yue Cui, Jaime
Arguello, Karsten Stegmann, Armin Weinberger,
and Frank Fischer. 2008. Analyzing collabo-
rative learning processes automatically: Exploit-
ing the advances of computational linguistics in
computer-supported collaborative learning. In Inter-
national Journal of Computer Supported Collabora-
tive Learning.
Nikol Rummel, Armin Weinberger, Christof Wecker,
Frank Fischer, Anne Meier, Eleni Voyiatzaki,
George Kahrimanis, Hans Spada, Nikolaos Avouris,
and Erin Walker. 2008. New challenges in cscl:
Towards adaptive script support. In Proceedings of
ICLS.
Christina Sauper and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of ACL.
Emanuel A Schegloff. 1968. Sequencing in conversa-
tional openings. American Anthropologist.
Oliver G Selfridge. 1958. Pandemonium: a
paradigm for learning. In Proceedings of Sympo-
sium on Mechanisation of Thought Processes, Na-
tional Physical Laboratory.
Gerry Stahl. 2012. Interaction analysis of a biology
chat. Productive multivocality.
Lee H Staples. 1990. Powerful ideas about empower-
ment. Administration in Social Work.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientic articles: Experiments with relevance and
rhetorical status. Computational Linguistics.
C F Van Uden-Kraan, C H C Drossaert, E Taal, E R
Seydel, and M A F J Van de Laar. 2009. Partici-
pation in online patient support groups endorses pa-
tients empowerment. Patient Education and Coun-
seling.
R Vauth, B Kleim, M Wirtz, and P W Corrigan. 2007.
Self-efficacy and empowerment as outcomes of self-
stigmatizing and coping in schizophrenia. Psychia-
try Research.
Ingrid Wahlin, Anna-Christina Ek, and Ewa Idvali.
2006. Patient empowerment in intensive carean in-
terview study. Intensive and Critical Care Nursing.
William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan Black, and Justine Cassell. 2012. ?love
ya, jerkface:? using sparse log-linear models to build
positive (and impolite) relationships with teens. In
Proceedings of SIGDIAL.
Gary M Weiss and Haym Hirsh. 1998. Learning to
predict rare events in event sequences. In Proceed-
ings of KDD.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation.
113
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 836?842,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extracting Events with Informal Temporal References in Personal
Histories in Online Communities
Miaomiao Wen, Zeyu Zheng, Hyeju Jang, Guang Xiang, Carolyn Penstein Rose?
Language Technologies Institute, Carnegie Mellon University
{mwen,zeyuz,hyejuj,guangx,cprose}@cs.cmu.edu
Abstract
We present a system for extracting the
dates of illness events (year and month of
the event occurrence) from posting histo-
ries in the context of an online medical
support community. A temporal tagger re-
trieves and normalizes dates mentioned in-
formally in social media to actual month
and year referents. Building on this, an
event date extraction system learns to in-
tegrate the likelihood of candidate dates
extracted from time-rich sentences with
temporal constraints extracted from event-
related sentences. Our integrated model
achieves 89.7% of the maximum perfor-
mance given the performance of the tem-
poral expression retrieval step.
1 Introduction
In this paper we present a challenging new event
date extraction task. Our technical contribution
is a temporal tagger that outperforms previously
published baseline approaches in its ability to
identify informal temporal expressions (TE) and
that normalizes each of them to an actual month
and year (Chang and Manning, 2012; Strotgen
and Gertz, 2010). This temporal tagger then con-
tributes towards high performance at matching
event mentions with the month and year in which
they occurred based on the complete posting his-
tory of users. It does so with high accuracy on
informal event mentions in social media by learn-
ing to integrate the likelihood of multiple candi-
date dates extracted from event mentions in time-
rich sentences with temporal constraints extracted
from event-related sentences.
Despite considerable prior work in temporal in-
formation extraction, to date state-of-the-art re-
sources are designed for extracting temporally
scoped facts about public figures/organizations
from newswire or Wikipedia articles (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
[11/15/2008] I have noticed some pulling recently and I 
won't start rads until March.
[11/20/2008] It is sloowwwly healing, so slowly, in fact, 
that she said she HOPES it will be healed by March, when 
I am supposed to start rads.
[1/13/2009] I still have one last chemo to go on the 19th 
and then start rads in 5 wks.
[1/31/2009] I go for my first meeting with the rad onc on 
 2/10 (my 50th birthday!).
[2/23/2009] I had my first rad today.
[3/31/2009] Tomorrow will be my last full rads
[4/2/2009] I started rads in Feb, just did #29 today.
[4/8/2009] The rad onc wants to see me again next week 
for a skin check as I have had cellulitis twice since August.
[6/21/2010] My friend Lisa had her port put in last week 
and will begin 2 weeks of radiation on Tuesday.
Figure 1: User posts containing keywords for the
start of Radiation. Event keywords are in bold and
temporal expressions are in italics.
al., 2012). When people are instead communi-
cating informally about their lives, they refer to
time more informally and frequently from their
personal frame of reference rather than from an
impersonal third person frame of reference. For
example, they may use their own birthday as a
time reference. The proportion of relative (e.g.,
?last week?, ?two days from now?), or personal
time references in our data is more than one and a
half times as high as in newswire and Wikipedia.
Therefore, it is not surprising that there would be
difficulty in applying a temporal tagger designed
for newswire to social media data (Strotgen and
Gertz, 2012; Kolomiyets et al, 2011). Recent be-
havioral studies (Choudhury et al, 2013; Park and
Choi, 2012; Wen et al, 2012) demonstrate that
user-focused event mentions extracted from social
media data can provide a useful timeline-like tool
for studying how behavior patterns change over
time in response to mentioned events. Our re-
search contributes towards automating this work.
2 Task
Our task is to extract personal illness events men-
tioned in the posting histories of online commu-
nity participants. The input to our system is
836
a candidate event and a posting history. The
output is the event date (month and year) for
the event if it occurred, or ?unknown? if it
did not occur. The process iterates through a
list of 10 cancer events (CEs). This list in-
cludes breast cancer Diagnosis, Metastasis, Re-
currence, Mastectomy, Lumpectomy, Reconstruc-
tion, Chemotherapy-Start, Chemotherapy-End,
Radiation-Start and Radiation-End. For each of
these target CEs, we manually designed an event
keyword set that includes the name of the event,
abbreviations, slang, aliases and related words.
For each of the 10 events, all sentences that
mention a related event keyword are extracted
from the user?s posting history. Figure 1 shows
sevaral sentences that were extracted for one user
for the start date of Radiation. The task is to de-
termine that the beginning of this user?s Radiation
therapy was 2/2009. Note that the user began to
post about Radiation before she started it. She first
reported planning to start Radiation in March, but
then rescheduled for February. Most of the TEs
are non-standard and need to be resolved to calen-
dar dates (year and month).
Once the full set of event mention sentences has
been extracted for a user, all the temporal expres-
sions (TEs) that appear in the same sentence with
an event mention are resolved to a set of candi-
date dates. Besides a standard event-time classi-
fier for within-sentence event-time anchoring, we
leverage a new source of temporal information to
train a constraint-based event-time classifier. Pre-
vious work only retrieves time-rich sentences that
include both the query and some TEs (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
al., 2012). However, sentences that contain only
the event mention but no explicit TE can also be
informative. For example, the post time (usually
referred to as document creation time or DCT) of
the sentence ?metastasis was found in my bone?
might be labeled as being after the ?metastasis?
event date. These DCTs impose constraints on
the possible event dates, which can be integrated
with the event-time classifier, as a variant on re-
lated work(Chambers, 2012).
3 Related Work
Previous work on TE extraction has focused
mainly on newswire text (Strotgen and Gertz,
2010; Chang and Manning, 2012). This paper
presents a rule-based TE extractor that identifies
and resolves a higher percentage of nonstandard
TEs than earlier state-of-art temporal taggers.
Our task is closest to the temporal slot filling
track in the TAC-KBP 2011 shared task (Ji et al,
2011) and timelining task (McClosky and Man-
ning, 2012). Their goal was to extract the tempo-
ral bounds of event relations. Our task has two key
differences. First, they used newswire, Wikipedia
and blogs as data sources from which they extract
temporal bounds of facts found in Wikipedia in-
foboxes. Second, in the KBP task, the set of gold
event relations are provided as input, so that the
task is only to identify a date for an event that is
guaranteed to have been mentioned. In our task,
we provide a set of potential events. However,
most of the candidate events won?t have ever been
reported within a user?s posting history.
Temporal constraints have proven to be use-
ful for producing a globally consistent timeline.
In most temporal relation bound extraction sys-
tems, the constraints are included as input rather
than learned by the system (Talukdar et al, 2012;
Wang et al, 2011). A notable exception is Mc-
Closkyet al (2012) who developed an approach to
learning constraints such as that people cannot at-
tend school if they have not been born yet. A no-
table characteristic of our task is that constraints
are softer. Diseases may occur in very different
ways across patients. Recurring illnesses falsely
appear to have an unpredictable order. Thus, there
can be no universal logical constraints on the order
of cancer events.
Our approach to using temporal constraints is a
variant on previously published approaches. Gar-
rido et al (2012) made use of DCT (document cre-
ation time) as well, however, they have assumed
the DCT is within the time-range of the event
stated in the document, which is often not true
in our data. Chambers (2012) utilized the within-
sentence time-DCT relation to learn constrains for
predicting DCT. We learn the event-DCT relations
to produce constrains for the event date.
4 Corpus Annotation
We have scraped the posts, users, and profiles from
a large online cancer support community. From
this collection we extracted and then annotated
two separate corpora, one for evaluating our TE
retrieval and normalization, the other one for event
date extraction.
For creating the TE extraction corpus, we ran-
837
domly picked one post from each of 1,000 ran-
domly selected users. We used this sampling tech-
nique because each user tends to use a narrow
range of date expression forms. From these posts,
we manually extracted 601 TEs and resolved them
to a specific month and year or just year if the
month was not mentioned. Events not reported
to have occurred were annotated as ?unknown?.
Our corpus for event date extraction consists of
the complete posting history of 300 users that were
randomly drawn from our dataset. Three annota-
tors were provided with guidelines for how to in-
fer the date of the events (Wen et al, 2013). We
achieved .94 Kappa on identification of whether an
event has a reported event date in a user?s history
or not. In evaluation of agreement on extracted
dates, we achieved a .99 Cronbach?s alpha. From
this corpus, 509 events were annotated with occur-
rence dates (year and month). In our evaluation,
we use data from 250 users for training, and 50 for
testing.
5 Method
Now we explain on a more technical level how our
system works on our task. Given an event and a
user?s post history, the system searches for all of
the sentences that contain an event keyword (key-
word sentence) and all the sentences that contain
both a keyword and a TE (date sentence). The TEs
in the date sentences are resolved and then used as
candidate dates for the event. For selecting among
candidate dates, our model integrates two main
components. First, the Date Classifier is trained
from date sentences to predict how likely its can-
didate TE and the gold event date are to overlap.
Then, because constraints over event dates can be
informed by temporal relations between the event
date and the DCT, the Constraint-based Classifier
provides an indication of the plausibility of can-
didate dates. The integrated system combines the
predictions from both classifiers.
5.1 Temporal Tagger
We design a rule-based temporal tagger that is
built using regular expression patterns to recog-
nize informal TEs. Similar to SUTime (Chang and
Manning, 2012), we identify and resolve a wide
range of non-standard TE types such as ?Feb ?07
(2/2007)?. The additional types of TE we han-
dle include: 1)user-specific TEs: A user?s age,
cancer anniversary and survivorship can provide
temporal information about the user?s CEs. We
obtain the birth date of users from their personal
profile to resolve age date expressions such as ?at
the age of 57?. 2)non-whole numbers such as ?a
year and half? and ?1/2 weeks?. 3)abbreviations
of time units : e.g. ?wk? as the abbreviation of
?week?. 4)underspecified month mentions, we
resolve the year information according to the DCT
month, the mentioned month and the verb tense.
5.2 Date Classifier
We train a MaxEnt classifier to predict the tem-
poral relationship between the retrieved TE and
the event date as overlap or no-overlap, similar
to the within-sentence event-time anchoring task
in TempEval-2 (UzZaman and Allen, 2010). Fea-
tures for the classifier include many of those in
(McClosky and Manning, 2012; Yoshikawa et al,
2009): namely, event keyword and its dominant
verb, verb and preposition that dominate TE, de-
pendency path between TE and keyword and its
length, unigram and bigram word and POS fea-
tures. New features include the Event-Subject,
Negative and Modality features. In online sup-
port groups, users not only tell stories about them-
selves, they also share other patients? stories (as
shown in Figure 1). So we add subject fea-
tures to remove this kind of noise, which in-
cludes the governing subject of the event key-
word and its POS tag. Modality features include
the appearance of modals before the event key-
word (e.g., may, might). Negative features include
the presence/absence of negative words (e.g., no,
never). These two features indicate a hypothetical
or counter-factual expression of the event.
To calculate the likelihood of a candidate date
for an event, we need to aggregate the hard de-
cisions from the classifier. Let DSu be the set
of the user?s date sentences, let Du be the set of
dates resolved from each TE. We represent a Max-
Ent classifier by Prelation(R|t, ds) for a candidate
date t in date sentence ds and possible relation
R = {overlap, no-overlap}. We map the distri-
bution over relations to a distribution over dates
by defining PDateSentence(t|DSu):
PDateSentence(t|DSu) = (1)
1
Z(Du)
?
tj?Du
?tj (t)Prelation(overlap|tj , dsj)
?tj (t) =
{
1 if t = tj
0 otherwise
838
We refer to this model as the Date Classifier.
5.3 Constraint-based Classifier
Previous work only retrieves time-rich sentences
(i.e., date sentences) (Ling and Weld, 2010; Ji et
al., 2011; McClosky and Manning, 2012; Garrido
et al, 2012). However, keyword sentences can in-
form temporal constraints for events and therefore
should not be ignored. For example, ?Well, I?m
officially a Radiation grad!? indicates the user has
done radiation by the time of the post (DCT). ?Ra-
diation is not a choice for me.? indicates the user
probably never had radiation. The topic of the
sentence can also indicate the temporal relation.
For example, before chemotherapy, the users tend
to talk about choices of drug combinations. After
chemotherapy, they talk about side-effects.
This section departs from the above Date Clas-
sifier and instead predicts whether each keyword
sentence is posted before or overlap-or-after the
user?s event date. The goal is to automatically
learn time constraints for the event. This task is
similar to the sentence event-DCT ordering task
in TempEval-2 (UzZaman and Allen, 2010). We
create training examples by computing the tempo-
ral relation between the DCT and the user?s gold
event date. If the user has not reported an event
date, the label should be unknown.
We train a MaxEnt classifier on each event
mention paired with its corresponding DCT. All
the features used in the classifier component that
are not related to the TEs are included. Let
KSu be the set of the user?s keyword sentences,
let Du be the set of dates resolved from each
date sentence. We define a MaxEnt classifier by
Prelation(R|ks) for a keyword sentence ks and
possible relation R = {before, overlap-or-after,
unknown}. DCT is the post time of the keyword
sentence ks. The rel(DCT, t) function simply de-
termines if the DCT is before or overlap-or-after
the candidate date t. We map this distribution over
relations to a distribution over dates by defining
PKeywordSentence(t,KSu):
PKeywordSentence(t,KSu) = (2)
1
Z(Du)
?
ksj?KSu
Prelation(rel(dctj , t)|ksj)
rel(dct, t) =
{
before if dct < t
overlap-or-after if dct ? t
5.4 Integrated Model
Given the Date Classifier of Section 5.2 and the
Constraint-based Classifier of Section 5.3, we cre-
ate a Integrated Model combining the two with the
following linear interpolation as follows:
P (t|postsu) = ?PDateSentence(t|DSu)
+ (1? ?)PKeywordSentence(t|KSu)
where t is a candidate event date. The system will
output t that maximizes P (t|postsu) and unknown
if DSu is empty. ? was set to 0.7 by maximizing
accuracy using five-fold cross-validation over the
training set.
6 Evaluation Metric and Results
6.1 Temporal Expression Retrieval
We compare our temporal tagger?s performance
with SUTime (Chang and Manning, 2012) on the
601 manually extracted TEs. We exclude user-
specific TEs such as birthday references since SU-
Time cannot handle those. We first evaluate iden-
tification of the extent of a TE and then production
of the correctly resolved date for each recognized
expression. Table 1 shows that our tagger has sig-
nificantly higher precision and recall for both.
P R F1
Extents SUTime 97.5 75.4 85.0
Our tagger 97.9 91.8 94.8
Normalization SUTime 89.4 71.2 79.3
Our tagger 91.3 85.5 88.3
Table 1: Temporal expression retrieval results
6.2 Event-date Extraction
6.2.1 Evaluation metric
The extracted date is only considered correct if it
completely matches the gold date. For less than
4% of users, we have multiple dates for the same
event (e.g., a user had a mastectomy twice). Sim-
ilar to the evaluation metric in a previous study(Ji
et al, 2011), in these cases, we give the system the
benefit of the doubt and the extracted date is con-
sidered correct if it matches one of the gold dates.
In previous work (McClosky and Manning, 2012;
Ji et al, 2011), the evaluation metric score is de-
fined as 1/((1 + |d|)) where d is the difference
between the values in years. We choose a much
stricter evaluation metric because we need a pre-
cise event date to study user behavior changes.
6.2.2 Baselines and oracle
Based on our temporal tagger, we provide two
baselines to describe heuristic methods of ag-
gregating the hard decisions from the classifier
839
Baseline1 Baseline2 Date Integrated Oracle
CE count P R F1 P R F1 P R F1 P R F1 F1
Diagnosis 112 .64 .70 .67 .60 .66 .63 .68 .75 .71 .68 .75 .71 .80
Metastasis 7 .16 .58 .25 .12 .43 .19 .25 .86 .39 .25 .86 .39 .86
Recurrence 14 .14 .35 .20 .11 .29 .16 .13 .36 .19 .13 .36 .19 .47
Chemo-start 54 .49 .61 .54 .42 .52 .46 .52 .66 .58 .58 .74 .65 .76
Chemo-end 43 .44 .59 .50 .36 .49 .42 .47 .63 .54 .48 .66 .56 .84
Rad-start 38 .35 .47 .40 .30 .40 .34 .36 .47 .41 .40 .53 .46 .64
Rad-end 35 .48 .63 .54 .30 .39 .34 .50 .66 .57 .50 .66 .57 .84
Mastectomy 68 .58 .71 .64 .52 .62 .57 .62 .76 .68 .62 .76 .68 .77
Lumpectomy 33 .49 .71 .58 .43 .76 .46 .46 .79 .58 .46 .79 .62 .91
Reconstruction 43 .38 .57 .46 .29 .44 .35 .41 .63 .50 .43 .65 .52 .86
Table 2: Event-level five-fold cross-validation performance of models and baselines on training data.
learned in Section 5.3. The first baseline, Base-
line1, is to pick the date with the highest clas-
sifier?s prediction confidence. The second base-
line, Baseline2, is along the same lines as the
Combined Classifier used in (McClosky and Man-
ning, 2012). For example, if the candidate
date is ?6/2009? and we have retrieved two TEs
that are resolved to ?6/2009? and ?4/2008?, then
P (?6/2009?) = Prelation(overlap|?6/2009?) ?
Prelation(no-overlap|?4/2008?).
To set an upper bound on performance given our
TE retrieval system, we calculate the oracle score
by considering an extraction as correct if the gold
date is one of the retrieved candidate dates. The
oracle score can differ from a perfect score since
we can only use candidate temporal expressions
if (a)the relation is known and (b)mentions of the
event are retrievable, (c)the TE and event keyword
appear in the same sentence, and (d)our temporal
tagger is able to recognize and resolve it correctly.
6.2.3 Results
We present the performance of our models, base-
lines and the oracle in Table 2. Both the Date Clas-
sifier and Integrated model significantly outper-
form the baselines (p < 0.0001, McNemar?s test,
2-tailed). This shows the value of our approach to
leveraging redundancy of event date mentions. In-
corporating time constraints further improves the
F1 of the Date Classifier by 3%. The Integrated
model achieves 89.7% of the oracle result.
Model P R F1
Baseline1 46.1 63.7 53.5
Baseline2 39.3 54.4 45.6
Date Classifier 49.6 67.7 57.3
Integrated Model 51.0 69.3 58.8
Oracle 77.3 77.3 77.3
Table 3: Performance of systems on the test set.
Table 3 shows the performance of our systems
and baselines on individual event types. The Joint
Model derives most of its improvement from per-
formance related to the Chemotherapy/Radiation-
start date. This is mainly because Chemotherapy
and Radiation last for a period of time and there
are more event-related discussions containing the
event keyword. None of our systems improves on
cancer Metastasis and Recurrence. This is likely
due to the sparsity of these events.
7 Conclusion
We presented a novel event date extraction task
that requires extraction and resolution of non-
standard TEs, namely personal illness event dates,
from the posting histories of online community
participants. We constructed an evaluation corpus
and designed a temporal tagger for non-standard
TEs in social media. Using a much stricter stan-
dard correctness measure than in previous work,
our method achieves promising results that are sig-
nificantly better than two types of baseline. By
creating an analogous keyword set, our event date
extraction method could be easily adapted to other
datasets.
8 Acknowledgments
We want to thank Dong Nguyen and Yi-chia
Wang, who helped provide the data for this
project. The research reported here was sup-
ported by National Science Foundation grant IIS-
0968485.
840
References
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne
Tamang, and Heng Ji. 2011. CUNY BLENDER
TACKBP2011 Temporal Slot Filling System De-
scription. In Proceedings of Text Analysis Confer-
ence (TAC).
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.
Nathanael Chambers. 2012. Labeling documents with
timestamps: Learning from their time expressions.
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normaliz-
ing time expressions. In 8th International Confer-
ence on Language Resources and Evaluation(LREC
2012).
De Choudhury, M., Counts, S., and Horvitz, E. 2013.
Major Life Changes and Behavioral Markers in So-
cial Media: Case of Childbirth. In Proc. CSCW
2013.
Guillermo Garrido, Anselmo Penas, Bernardo Ca-
baleiro, and Alvaro Rodrigo. 2012. Temporally An-
chored Relation Extraction. In Proceedings of the
50th annual meeting of the as-sociation for compu-
tational linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC 2011 Knowledge Base Popu-
lation track. In Proceedings of Text Analysis Con-
ference (TAC).
Hyuckchul Jung, James Allen, Nate Blaylock, Will de
Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records:
initial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011.
Oleksandr Kolomiyets, Steven Bethard and Marie-
Francine Moens. 2011. Model-Portability Experi-
ments for Textual Temporal Analysis. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Pro-
ceedings of the 50th annual meeting of the Associ-
ation for Computational Linguistics.
Xiao Ling and Daniel S Weld. 2010 Temporal infor-
mation extraction. Proceedings of the Twenty Fifth
National Conference on Artificial Intelligence.
David McClosky and Christopher D. Manning. 2012.
Learning Constraints for Consistent Timeline Ex-
traction. Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP2012).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
47th annual meeting of the Association for Compu-
tational Linguistics.
Heekyong Park and Jinwook Choi 2012. V-model: a
new innovative model to chronologically visualize
narrative clinical texts. In Proceedings of the 2012
ACM annual conference on Human Factors in Com-
puting Systems. ACM.
Catherine Plaisant, Brett Milash, Anne Rose, Seth Wid-
off, and Ben Shneiderman. 1996. LifeLines: vi-
sualizing personal histories. In Proceedings of the
SIGCHI conference on Human factors in computing
systems.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. TimeML: Ro-
bust specification of event and temporal expressions
in text. TimeML: Robust specification of event and
temporal expressions in text. In New Directions in
Question Answering?03.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev. 2003. The Timebank corpus. In
Corpus Linguistics.
Preethi Raghavan, Eric Fosler-Lussier, and Albert M.
Lai. 2012. Learning to Temporally Order Medical
Events in Clinical Text. In Proceedings of the 50th
annual meeting of the Association for computational
Linguistics.
Jannik Strotgen and Michael Gertz. 2010. Heidel-
Time:High Quality Rule-Based Extraction and Nor-
malizationof Temporal Expressions. In SemEval
?10.
Jannik Strotgen and Michael Gertz. 2012. Temporal
Tagging on Different Domains: Challenges, Strate-
gies, and Gold Standards. In LREC2012.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of re-
lational facts. In Proceedings of the fifth ACM inter-
national conference on Web search and data mining.
ACM.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the
5th International Workshop on Semantic Evaluation.
Yafang Wang, Bing Yang, Lizhen Qu, Marc Spaniol,
and GerhardWeikum. 2011. Harvesting facts from
textual web sources by constrained label propaga-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement.
841
Miaomiao Wen, Hyeju Jang, and Carolyn Rose?. 2013.
Coding Manual for Illness Event Date Extraction.
Carnegie Mellon University, School of Computer
Science, Language Technology Institute.
K.-Y. Wen, F. McTavish, G. Kreps, M. Wise, and D.
Gustafson. 2012. From diagnosis to death: A
case study of coping with breast cancer as seen
through online discussion group messages. Jour-
nal of Computer-Mediated Communication, 16:331-
361.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with markov logic. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data?a review with emphasis
on medical natural language processing. Journal of
biomedical informatics 40.2 (2007): 183.
842
