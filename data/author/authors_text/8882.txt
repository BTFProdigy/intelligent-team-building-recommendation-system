Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 39?46,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Choosing an Optimal Architecture for Segmentation and POS-Tagging of
Modern Hebrew
Roy Bar-Haim
Dept. of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
barhair@cs.biu.ac.il
Khalil Sima?an
ILLC
Universiteit van Amsterdam
Amsterdam, The Netherlands
simaan@science.uva.nl
Yoad Winter
Dept. of Computer Science
Technion
Haifa 32000, Israel
winter@cs.technion.ac.il
Abstract
A major architectural decision in de-
signing a disambiguation model for seg-
mentation and Part-of-Speech (POS) tag-
ging in Semitic languages concerns the
choice of the input-output terminal sym-
bols over which the probability distribu-
tions are defined. In this paper we de-
velop a segmenter and a tagger for He-
brew based on Hidden Markov Models
(HMMs). We start out from a morpholog-
ical analyzer and a very small morpholog-
ically annotated corpus. We show that a
model whose terminal symbols are word
segments (=morphemes), is advantageous
over a word-level model for the task of
POS tagging. However, for segmentation
alone, the morpheme-level model has no
significant advantage over the word-level
model. Error analysis shows that both
models are not adequate for resolving a
common type of segmentation ambiguity
in Hebrew ? whether or not a word in a
written text is prefixed by a definiteness
marker. Hence, we propose a morpheme-
level model where the definiteness mor-
pheme is treated as a possible feature of
morpheme terminals. This model exhibits
the best overall performance, both in POS
tagging and in segmentation. Despite the
small size of the annotated corpus avail-
able for Hebrew, the results achieved us-
ing our best model are on par with recent
results on Modern Standard Arabic.
1 Introduction
Texts in Semitic languages like Modern Hebrew
(henceforth Hebrew) and Modern Standard Ara-
bic (henceforth Arabic), are based on writing sys-
tems that allow the concatenation of different lexi-
cal units, called morphemes. Morphemes may be-
long to various Part-of-Speech (POS) classes, and
their concatenation forms textual units delimited by
white space, which are commonly referred to as
words. Hence, the task of POS tagging for Semitic
languages consists of a segmentation subtask and
a classification subtask. Crucially, words can be
segmented into different alternative morpheme se-
quences, where in each segmentation morphemes
may be ambiguous in terms of their POS tag. This
results in a high level of overall ambiguity, aggra-
vated by the lack of vocalization in modern Semitic
texts.
One crucial problem concerning POS tagging of
Semitic languages is how to adapt existing methods
in the best way, and which architectural choices have
to be made in light of the limited availability of an-
notated corpora (especially for Hebrew). This paper
outlines some alternative architectures for POS tag-
ging of Hebrew text, and studies them empirically.
This leads to some general conclusions about the op-
timal architecture for disambiguating Hebrew, and
(reasonably) other Semitic languages as well. The
choice of tokenization level has major consequences
for the implementation using HMMs, the sparseness
of the statistics, the balance of the Markov condi-
39
tioning, and the possible loss of information. The
paper reports on extensive experiments for compar-
ing different architectures and studying the effects
of this choice on the overall result. Our best result
is on par with the best reported POS tagging results
for Arabic, despite the much smaller size of our an-
notated corpus.
The paper is structured as follows. Section 2 de-
fines the task of POS tagging in Hebrew, describes
the existing corpora and discusses existing related
work. Section 3 concentrates on defining the dif-
ferent levels of tokenization, specifies the details of
the probabilistic framework that the tagger employs,
and describes the techniques used for smoothing the
probability estimates. Section 4 compares the differ-
ent levels of tokenization empirically, discusses their
limitations, and proposes an improved model, which
outperforms both of the initial models. Finally, sec-
tion 5 discusses the conclusions of our study for seg-
mentation and POS tagging of Hebrew in particular,
and Semitic languages in general.
2 Task definition, corpora and related
work
Words in Hebrew texts, similar to words in Ara-
bic and other Semitic languages, consist of a stem
and optional prefixes and suffixes. Prefixes include
conjunctions, prepositions, complementizers and the
definiteness marker (in a strict well-defined order).
Suffixes include inflectional suffixes (denoting gen-
der, number, person and tense), pronominal comple-
ments with verbs and prepositions, and possessive
pronouns with nouns.
By the term word segmentation we henceforth re-
fer to identifying the prefixes, the stem and suffixes
of the word. By POS tag disambiguation we mean
the assignment of a proper POS tag to each of these
morphemes.
In defining the task of segmentation and POS tag-
ging, we ignore part of the information that is usu-
ally found in Hebrew morphological analyses. The
internal morphological structure of stems is not an-
alyzed, and the POS tag assigned to stems includes
no information about their root, template/pattern, in-
flectional features and suffixes. Only pronominal
complement suffixes on verbs and prepositions are
identified as separate morphemes. The construct
state/absolute,1 and the existence of a possessive
suffix are identified using the POS tag assigned to
the stem, and not as a separate segment or feature.
Some of these conventions are illustrated by the seg-
mentation and POS tagging of the word wfnpgfnw
(?and that we met?, pronounced ve-she-nifgashnu):2
w/CC: conjunction
f /COM: complementizer
npgfnw/VB: verb
Our segmentation and POS tagging conform with
the annotation scheme used in the Hebrew Treebank
(Sima?an et al, 2001), described next.
2.1 Available corpora
The Hebrew Treebank (Sima?an et al, 2001) con-
sists of syntactically annotated sentences taken from
articles from the Ha?aretz daily newspaper. We ex-
tracted from the treebank a mapping from each word
to its analysis as a sequence of POS tagged mor-
phemes. The treebank version used in the current
work contains 57 articles, which amount to 1,892
sentences, 35,848 words, and 48,332 morphemes.
In addition to the manually tagged corpus, we have
access to an untagged corpus containing 337,651
words, also originating from Ha?aretz newspaper.
The tag set, containing 28 categories, was ob-
tained from the full morphological tagging by re-
moving the gender, number, person and tense fea-
tures. This tag set was used for training the POS
tagger. In the evaluation of the results, however, we
perform a further grouping of some POS tags, lead-
ing to a reduced POS tag set of 21 categories. The
tag set and the grouping scheme are shown below:
{NN}, {NN-H}, {NNT}, {NNP}, {PRP,AGR}, {JJ}, {JJT},
{RB,MOD}, {RBR}, {VB,AUX}, {VB-M}, {IN,COM,REL},
{CC}, {QW}, {HAM}, {WDT,DT}, {CD,CDT}, {AT}, {H},
{POS}, {ZVL}.
2.2 Related work on Hebrew and Arabic
Due to the lack of substantial tagged corpora, most
previous corpus-based work on Hebrew focus on the
1The Semitic construct state is a special form of a word
that participates in compounds. For instance, in the Hebrew
compound bdiqt hjenh (?check of the claim?), the word bdiqt
(?check of?/?test of?) is the construct form of the absolute form
bdiqh (?check?/?test?).
2In this paper we use Latin transliteration for Hebrew letters
following (Sima?an et al, 2001).
40
development of techniques for learning probabilities
from large unannotated corpora. The candidate anal-
yses for each word were usually obtained from a
morphological analyzer.
Levinger et al (1995) propose a method for
choosing a most probable analysis for Hebrew
words using an unannotated corpus, where each
analysis consists of the lemma and a set of morpho-
logical features. They estimate the relative frequen-
cies of the possible analyses for a given word w by
defining a set of ?similar words? SW (A) for each
possible analysis A of w. Each word w? in SW (A)
corresponds to an analysis A? which differs from A
in exactly one feature. Since each set is expected to
contain different words, it is possible to approximate
the frequency of the different analyses using the av-
erage frequency of the words in each set, estimated
from the untagged corpus.
Carmel and Maarek (1999) follow Levinger et
al. in estimating context independent probabilities
from an untagged corpus. Their algorithm learns fre-
quencies of morphological patterns (combinations
of morphological features) from the unambiguous
words in the corpus.
Several works aimed at improving the ?similar
words? method by considering the context of the
word. Levinger (1992) adds a short context filter that
enforces grammatical constraints and rules out im-
possible analyses. Segal?s (2000) system includes,
in addition to a somewhat different implementation
of ?similar words?, two additional components: cor-
rection rules a` la Brill (1995), and a rudimentary de-
terministic syntactic parser.
Using HMMs for POS tagging and segmenting
Hebrew was previously discussed in (Adler, 2001).
The HMM in Adler?s work is trained on an untagged
corpus, using the Baum-Welch algorithm (Baum,
1972). Adler suggests various methods for perform-
ing both tagging and segmentation, most notable are
(a) The usage of word-level tags, which uniquely de-
termine the segmentation and the tag of each mor-
pheme, and (b) The usage of a two-dimensional
Markov model with morpheme-level tags. Only the
first method (word-level tags) was tested, resulting
in an accuracy of 82%. In the present paper, both
word-level tagging and morpheme-level tagging are
evaluated.
Moving on to Arabic, Lee et al (2003) describe a
word segmentation system for Arabic that uses an n-
gram language model over morphemes. They start
with a seed segmenter, based on a language model
and a stem vocabulary derived from a manually seg-
mented corpus. The seed segmenter is improved it-
eratively by applying a bootstrapping scheme to a
large unsegmented corpus. Their system achieves
accuracy of 97.1% (per word).
Diab et al (2004) use Support Vector Machines
(SVMs) for the tasks of word segmentation and POS
tagging (and also Base Phrase Chunking). For seg-
mentation, they report precision of 99.09% and re-
call of 99.15%, when measuring morphemes that
were correctly identified. For tagging, Diab et al
report accuracy of 95.49%, with a tag set of 24 POS
tags. Tagging was applied to segmented words, us-
ing the ?gold? segmentation from the annotated cor-
pus (Mona Diab, p.c.).
3 Architectures for POS tagging Semitic
languages
Our segmentation and POS tagging system consists
of a morphological analyzer that assigns a set of
possible candidate analyses to each word, and a dis-
ambiguator that selects from this set a single pre-
ferred analysis per word. Each candidate analysis
consists of a segmentation of the word into mor-
phemes, and a POS tag assignment to these mor-
phemes. In this section we concentrate on the ar-
chitectural decisions in devising an optimal disam-
biguator, given a morphological analyzer for He-
brew (or another Semitic language).
3.1 Defining the input/output
An initial crucial decision in building a disambigua-
tor for a Semitic text concerns the ?tokenization? of
the input sentence: what constitutes a terminal (i.e.,
input) symbol. Unlike English POS tagging, where
the terminals are usually assumed to be words (de-
limited by white spaces), in Semitic texts there are
two reasonable options for fixing the kind of termi-
nal symbols, which directly define the correspond-
ing kind of nonterminal (i.e., output) symbols:
Words (W): The terminals are words as they ap-
pear in the text. In this case a nonterminal a
that is assigned to a word w consists of a se-
quence of POS tags, each assigned to a mor-
41
pheme of w, delimited with a special segmenta-
tion symbol. We henceforth refer to such com-
plex nonterminals as analyses. For instance,
the analysis IN-H-NN for the Hebrew word
bbit uniquely encodes the segmentation b-h-bit.
In Hebrew, this unique encoding of the segmen-
tation by the sequence of POS tags in the anal-
ysis is a general property: given a word w and
a complex nonterminal a = [t1 . . . tp] for w, it
is possible to extend a back to a full analysis
a? = [(m1, t1) . . . (mp, tp)], which includes the
morphemes m1 . . .mp that make out w. This is
done by finding a match for a in Analyses(w),
the set of possible analyses of w. Except for
very rare cases, this match is unique.
Morphemes (M): In this case the nonterminals are
the usual POS tags, and the segmentation is
given by the input morpheme sequence. Note
that information about how morphemes are
joined into words is lost in this case.
Having described the main input-output options for
the disambiguator, we move on to describing the
probabilistic framework that underlies their work-
ings.
3.2 The probabilistic framework
Let wk1 be the input sentence, a sequence of words
w1 . . . wk. If tokenization is per word, then the
disambiguator aims at finding the nonterminal se-
quence ak1 that has the highest joint probability with
the given sentence wk1 :
argmax
ak1
P (wk1 ,a
k
1) (1)
This setting is the standard formulation of proba-
bilistic tagging for languages like English.
If tokenization is per morpheme, the disambigua-
tor aims at finding a combination of a segmentation
mn1 and a tagging tn1 for mn1 , such that their joint
probability with the given sentence, wk1 , is maxi-
mized:
argmax
(mn1 ,t
n
1 )?ANALY SES(w
k
1 )
P (wk1 ,m
n
1 , t
n
1 ), (2)
where ANALY SES(wk1) is the set of possible
analyses for the input sentence wk1 (output by the
morphological analyzer). Note that n can be dif-
ferent from k, and may vary for different segmen-
tations. The original sentence can be uniquely re-
covered from the segmentation and the tagging.
Since all the ?mn1 , tn1 ? pairs that are the input for
the disambiguator were derived from wk1 , we have
P (wk1 |m
n
1 , t
n
1 ) = 1, and thus P (wk1 ,mn1 , tn1 ) =
P (tn1 ,m
n
1 ). Therefore, Formula (2) can be simpli-
fied as:
argmax
(mn1 ,t
n
1 )?ANALY SES(w
k
1 )
P (mn1 , t
n
1 ) (3)
Formulas (1) and (3) can be represented in a unified
formula that applies to both word tokenization and
morpheme tokenization:
argmax
(en1 ,A
n
1 )?ANALY SES(w
k
1 )
P (en1 , A
n
1 ) (4)
In Formula (4) en1 represents either a sequence of
words or a sequence of morphemes, depending on
the level of tokenization, and An1 are the respective
nonterminals ? either POS tags or word-level anal-
yses. Thus, the disambiguator aims at finding the
most probable ?terminal sequence, nonterminal
sequence? for the given sentence, where in the
case of word-tokenization there is only one possible
terminal sequence for the sentence.
3.3 HMM probabilistic model
The actual probabilistic model used in this work for
estimating P (en1 , An1 ) is based on Hidden Markov
Models (HMMs). HMMs underly many successful
POS taggers , e.g. (Church, 1988; Charniak et al,
1993).
For a k-th order Markov model (k = 1 or k = 2),
we rewrite (4) as:
argmax
en1 ,A
n
1
P (en1 , A
n
1 ) ?
argmax
en1 ,A
n
1
n?
i=1
P (Ai | Ai?k, . . . , Ai?1)P (ei | Ai)
(5)
For reasons of data sparseness, actual models we use
work with k = 2 for the morpheme level tokeniza-
tion, and with k = 1 for the word level tokenization.
42
For these models, two kinds of probabilities need
to be estimated: P (ei | Ai) (lexical model) and
P (Ai |Ai?k, . . . , Ai?1) (language model). Because
the only manually POS tagged corpus that was avail-
able to us for training the HMM was relatively small
(less than 4% of the Wall Street Journal (WSJ) por-
tion of the Penn treebank), it is inevitable that major
effort must be dedicated to alleviating the sparseness
problems that arise. For smoothing the nonterminal
language model probabilities we employ the stan-
dard backoff smoothing method of Katz (1987).
Naturally, the relative frequency estimates of
the lexical model suffer from more severe data-
sparseness than the estimates for the language
model. On average, 31.3% of the test words do
not appear in the training corpus. Our smooth-
ing method for the lexical probabilities is described
next.
3.4 Bootstrapping a better lexical model
For the sake of exposition, we assume word-level
tokenization for the rest of this subsection. The
method used for the morpheme-level tagger is very
similar.
The smoothing of the lexical probability of a word
w given an analysis a, i.e., P (w | a) = P (w,a)P (a) ,
is accomplished by smoothing the joint probability
P (w,a) only, i.e., we do not smooth P (a).3 To
smooth P (w,a), we use a linear interpolation of
the relative frequency estimates from the annotated
training corpus (denoted rf tr(w,a)) together with
estimates obtained by unsupervised estimation from
a large unannotated corpus (denoted emauto(w,a)):
P (w,a) = ? rf tr(w,a)+(1??) emauto(w,a)
(6)
where ? is an interpolation factor, experimentally set
to 0.85.
Our unsupervised estimation method can be
viewed as a single iteration of the Baum-Welch
(Forward-Backward) estimation algorithm (Baum,
1972) with minor differences. We apply this method
to the untagged corpus of 340K words. Our method
starts out from a naively smoothed relative fre-
3the smoothed probabilities are normalized so that?
w P (w,a) = P (a)
quency lexical model in our POS tagger:
PLM0(w|a) =
{
(1 ? p0) rf tr(w,a) ftr(w) > 0
p0 otherwise
(7)
Where ftr(w) is the occurrence frequency of w in
the training corpus, and p0 is a constant set experi-
mentally to 10?10. We denote the tagger that em-
ploys a smoothed language model and the lexical
model PLM0 by the probability distribution Pbasic
(over analyses, i.e., morpheme-tag sequences).
In the unsupervised algorithm, the model Pbasic
is used to induce a distribution of alternative analy-
ses (morpheme-tag sequences) for each of the sen-
tences in the untagged corpus; we limit the num-
ber of alternative analyses per sentence to 300. This
way we transform the untagged corpus into a ?cor-
pus? containing weighted analyses (i.e., morpheme-
tag sequences). This corpus is then used to calcu-
late the updated lexical model probabilities using
maximum-likelihood estimation. Adding the test
sentences to the untagged corpus ensures non-zero
probabilities for the test words.
3.5 Implementation4
The set of candidate analyses was obtained from Se-
gal?s morphological analyzer (Segal, 2000). The
analyzer?s dictionary contains 17,544 base forms
that can be inflected. After this dictionary was ex-
tended with the tagged training corpus, it recog-
nizes 96.14% of the words in the test set.5 For each
train/test split of the corpus, we only use the training
data for enhancing the dictionary. We used SRILM
(Stolcke, 2002) for constructing language models,
and for disambiguation.
4 Evaluation
In this section we report on an empirical comparison
between the two levels of tokenization presented in
the previous section. Analysis of the results leads to
an improved morpheme-level model, which outper-
forms both of the initial models.
Each architectural configuration was evaluated in
5-fold cross-validated experiments. In a train/test
4http://www.cs.technion.ac.il/?barhaim/MorphTagger/
5Unrecognized words are assumed to be proper nouns, and
the morphological analyzer proposes possible segmentations for
the word, based on the recognition of possible prefixes.
43
split of the corpus, the training set includes 1,598
sentences on average, which on average amount to
28,738 words and 39,282 morphemes. The test set
includes 250 sentences. We estimate segmentation
accuracy ? the percentage of words correctly seg-
mented into morphemes, as well as tagging accu-
racy ? the percentage of words that were correctly
segmented for which each morpheme was assigned
the correct POS tag.
For each parameter, the average over the five folds
is reported, with the standard deviation in parenthe-
ses. We used two-tailed paired t-test for testing the
significance of the difference between the average
results of different systems. The significance level
(p-value) is reported.
The first two lines in Table 1 detail the results ob-
tained for both word (W) and morpheme (M) lev-
els of tokenization. The tagging accuracy of the
Accuracy per word (%)
Tokenization Tagging Segmentation
W 89.42 (0.9) 96.43 (0.3)
M 90.21 (1.2) 96.25 (0.5)
M+h 90.51 (1.0) 96.74 (0.5)
Table 1: Level of tokenization - experimental results
morpheme tagger is considerably better than what
is achieved by the word tagger (difference of 0.79%
with significance level p = 0.01). This is in spite of
the fact that the segmentation achieved by the word
tagger is a little better (and a segmentation error im-
plies incorrect tagging). Our hypothesis is that:
Morpheme-level taggers outperform
word-level taggers in their tagging ac-
curacy, since they suffer less from data
sparseness. However, they lack some
word-level knowledge that is required for
segmentation.
This hypothesis is supported by the number of
once-occurring terminals in each level: 8,582 in the
word level, versus 5,129 in the morpheme level.
Motivated by this hypothesis, we next consider
what kind of word-level information is required for
the morpheme-level tagger in order to do better in
segmentation. One natural enhancement for the
morpheme-level model involves adding information
about word boundaries to the tag set. In the en-
hanced tag set, nonterminal symbols include addi-
tional features that indicate whether the tagged mor-
pheme starts/ends a word. Unfortunately, we found
that adding word boundary information in this way
did not improve segmentation accuracy.
However, error analysis revealed a very common
type of segmentation errors, which was found to be
considerably more frequent in morpheme tagging
than in word tagging. This kind of errors involves
a missing or an extra covert definiteness marker ?h?.
For example, the word bbit can be segmented either
as b-bit (?in a house?) or as b-h-bit (?in the house?),
pronounced bebayit and babayit, respectively. Un-
like other cases of segmentation ambiguity, which
often just manifest lexical facts about spelling of He-
brew stems, this kind of ambiguity is productive: it
occurs whenever the stem?s POS allows definiteness,
and is preceded by one of the prepositions b/k/l. In
morpheme tagging, this type of error was found on
average in 1.71% of the words (46% of the segmen-
tation errors). In word tagging, it was found only
in 1.36% of the words (38% of the segmentation er-
rors).
Since in Hebrew there should be agreement be-
tween the definiteness status of a noun and its related
adjective, this kind of ambiguity can sometimes be
resolved syntactically. For instance:
?bbit hgdwl? implies b-h-bit (?in the big house?)
?bbit gdwl? implies b-bit (?in a big house?)
By contrast, in many other cases both analyses
are syntactically valid, and the choice between them
requires consideration of a wider context, or some
world knowledge. For example, in the sentence
hlknw lmsibh (?we went to a/the party?), lmsibh
can be analyzed either as l-msibh (indefinite,?to a
party?) or as l-h-mbsibh (definite,?to the party?).
Whether we prefer ?the party? or ?a party? depends
on contextual information that is not available for
the POS tagger.
Lexical statistics can provide valuable informa-
tion in such situations, since some nouns are more
common in their definite form, while other nouns are
more common as indefinite. For example, consider
the word lmmflh (?to a/the government?), which can
be segmented either as l-mmflh or l-h-mmflh. The
44
Tokenization Analysis
W (lmmflh IN-H-NN)
M (IN l) (H h) (NN mmflh)
M+h (IN l) (H-NN hmmflh)
Table 2: Representation of l-h-mmflh in each level
of tokenization
stem mmflh (?government?) was found 25 times in
the corpus, out of which only two occurrences were
indefinite. This strong lexical evidence in favor of
l-h-mmflh is completely missed by the morpheme-
level tagger, in which morphemes are assumed to
be independent. The lexical model of the word-
level tagger better models this difference, since it
does take into account the frequencies of l-mmflh
and l-h-mmlh, in measuring P(lmmflh|IN-NN) and
P(lmmflh|IN-H-NN). However, since the word tag-
ger considers lmmflh, hmmflh (?the government?),
and mmflh (?a government?) as independent words,
it still exploits only part of the potential lexical evi-
dence about definiteness.
In order to better model such situations, we
changed the morpheme-level model as follows. In
definite words the definiteness article h is treated
as a manifestation of a morphological feature of the
stem. Hence the definiteness marker?s POS tag (H)
is prefixed to the POS tag of the stem. We refer by
M+h to the resulting model that uses this assump-
tion, which is rather standard in theoretical linguistic
studies of Hebrew. The M+h model can be viewed as
an intermediate level of tokenization, between mor-
pheme and word tokenization. The different analy-
ses obtained by the three models of tokenization are
demonstrated in Table 2.
As shown in Table 1, the M+h model shows
remarkable improvement in segmentation (0.49%,
p < 0.001) compared with the initial morpheme-
level model (M). As expected, the frequency of seg-
mentation errors that involve covert definiteness (h)
dropped from 1.71% to 1.25%. The adjusted mor-
pheme tagger also outperforms the word level tagger
in segmentation (0.31%, p = 0.069). Tagging was
improved as well (0.3%, p = 0.068). According to
these results, tokenization as in the M+h model is
preferable to both plain-morpheme and plain-word
tokenization.
5 Conclusion
Developing a word segmenter and POS tagger for
Hebrew with less than 30K annotated words for
training is a challenging task, especially given the
morphological complexity and high degree of am-
biguity in Hebrew. For comparison, in English a
baseline model that selects the most frequent POS
tag achieves accuracy of around the 90% (Charniak
et al, 1993). However, in Hebrew we found that a
parallel baseline model achieves only 84% using the
available corpus.
The architecture proposed in this paper addresses
the severe sparseness problems that arise in a num-
ber of ways. First, the M+h model, which was
found to perform best, is based on morpheme-
level tokenization, which suffers of data sparse-
ness less than word tokenization, and makes use of
multi-morpheme nonterminals only in specific cases
where it was found to be valuable. The number of
nonterminal types found in the corpus for this model
is 49 (including 11 types of punctuation marks),
which is much closer to the morpheme-level model
(39 types) than to the word-level model (205 types).
Second, the bootstrapping method we present ex-
ploits additional resources such as a morphological
analyzer and an untagged corpus, to improve lexi-
cal probabilities, which suffer from data sparseness
the most. The improved lexical model contributes
1.5% to the tagging accuracy, and 0.6% to the seg-
mentation accuracy (compared with using the basic
lexical model), making it a crucial component of our
system.
Among the few other tools available for POS tag-
ging and morphological disambiguation in Hebrew,
the only one that is freely available for extensive
training and evaluation as performed in this paper
is Segal?s ((Segal, 2000), see section 2.2). Com-
paring our best architecture to the Segal tagger?s re-
sults under the same experimental setting shows an
improvement of 1.5% in segmentation accuracy and
4.5% in tagging accuracy over Segal?s results.
Moving on to Arabic, in a setting comparable to
(Diab et al, 2004), in which the correct segmenta-
tion is given, our tagger achieves accuracy per mor-
pheme of 94.9%. This result is close to the re-
45
sult reported by Diab et al, although our result was
achieved using a much smaller annotated corpus.
We therefore believe that future work may benefit
from applying our model, or variations thereof, to
Arabic and other Semitic languages.
One of the main sources for tagging errors in our
model is the coverage of the morphological analyzer.
The analyzer misses the correct analysis of 3.78% of
the test words. Hence, the upper bound for the accu-
racy of the disambiguator is 96.22%. Increasing the
coverage while maintaining the quality of the pro-
posed analyses (avoiding over-generation as much
as possible), is crucial for improving the tagging re-
sults.
It should also be mentioned that a new version of
the Hebrew treebank, now containing approximately
5,000 sentences, was released after the current work
was completed. We believe that the additional an-
notated data will allow to refine our model, both in
terms of accuracy and in terms of coverage, by ex-
panding the tag set with additional morpho-syntactic
features like gender and number, which are prevalent
in Hebrew and other Semitic languages.
Acknowledgments
We thank Gilad Ben-Avi, Ido Dagan and Alon Itai
for their insightful remarks on major aspects of this
work. The financial and computational support of
the Knowledge Center for Processing Hebrew is
gratefully acknowledged. The first author would like
to thank the Technion for partially funding his part
of the research. The first and third authors are grate-
ful to the ILLC of the University of Amsterdam for
its hospitality while working on this research. We
also thank Andreas Stolcke for his devoted technical
assistance with SRILM.
References
Meni Adler. 2001. Hidden Markov Model for Hebrew
part-of-speech tagging. Master?s thesis, Ben Gurion
University, Israel. In Hebrew.
Leonard Baum. 1972. An inequality and associated max-
imization technique in statistical estimation for proba-
bilistic functions of a Markov process. In Inequalities
III:Proceedings of the Third Symposium on Inequali-
ties, University of California, Los Angeles, pp.1-8.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistic, 21:784?789.
David Carmel and Yoelle Maarek. 1999. Morphological
disambiguation for Hebrew search systems. In Pro-
ceedings of the 4th international workshop,NGITS-99.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for part-of-
speech tagging. In National Conference on Artificial
Intelligence, pages 784?789.
K. W. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In Proc. of
the Second Conference on Applied Natural Language
Processing, pages 136?143, Austin, TX.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In HLT-NAACL 2004: Short Pa-
pers, pages 149?152.
S.M. Katz. 1987. Estimation of probabilities from sparse
data from the language model component of a speech
recognizer. IEEE Transactions of Acoustics, Speech
and Signal Processing, 35(3):400?401.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In ACL,
pages 399?406.
M. Levinger, U. Ornan, and A. Itai. 1995. Morphological
disambiguation in Hebrew using a priori probabilities.
Computational Linguistics, 21:383?404.
Moshe Levinger. 1992. Morphological disambiguation
in Hebrew. Master?s thesis, Computer Science Depart-
ment, Technion, Haifa, Israel. In Hebrew.
Erel Segal. 2000. Hebrew morphological ana-
lyzer for Hebrew undotted texts. Master?s the-
sis, Computer Science Department, Technion,
Haifa, Israel. http://www.cs.technion.ac.il/-
?erelsgl/bxi/hmntx/teud.html.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of Modern Hebrew text.
Traitment Automatique des Langues, 42:347?380.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP, pages 901?904, Denver,
Colorado, September.
46
Controlled Language for Geographical
Information System Queries
Sela Mador-Haim, Yoad Winter, and Anthony Braun
Technion I.I.T
selam@ cs. technion. ac. il,winter@ cs. technion. ac. il ,
tonyb@ geofocus. co. il
Abstract
Natural language interfaces to spatial databases have not received a lot of attention
in computational linguistics, in spite of the potential value of such systems for
users of Geographical Information Systems (GISs). This paper presents a controlled
language for GIS queries, solves some of the semantic problems for spatial inference
in this language, and introduces a system that implements this controlled language
as a novel interface for GIS.
1 Introduction
Geographical Information Systems (GISs) are information systems for pro-
cessing of data that pertain to spatial or geographic coordinates [14]. Even
though GISs are enjoying a rapidly growing users community, the current
systems are often difficult to use or require a long learning process [13]. In
the GIS literature [15,16,5,8], it has been well-acknowledged that natural lan-
guage interfaces (NLIs) would significantly enhance the exploitation of the
more complex features of GISs, yet despite the potential value of NLIs for
GISs, the work on this subject has so far been rather limited [16]. To the best
of our knowledge, existing NLIs for GISs are limited in scope and expressive
power and lack the ability to express complex relationships over spatial en-
tities. Some works ([9,17,12]) have demonstrated NLIs using databases that
contain geographically related data. Those databases, however, lack any ac-
tual spatial information (e.g. geometric polygons representing buildings), and
therefore do not deal with the problem of inferring spatial relations from such
representations.
In general, the design of NLIs to databases is regarded as a difficult problem
since human interaction is often vague, ambiguous or highly contextualized
[15,1]. The approach we take in this paper is to avoid many of these problems
by designing a system that uses a controlled language for GIS queries. Such
controlled languages [10,11], which are based on fragments of English, can be
designed in a way that minimizes the use of vague, ambiguous and context-
dependent expressions, while maintaining the ability to express very complex
queries in a language that is a subset of English. We benefit from the fact
that GISs are a closed, well-defined domain, which enables us to focus on
data independent parts of the language. We show that the addition of data
dependent portions can be done semi-automatically and requires very low
effort.
Our implementation of an NLI for GISs involves four major tasks: first,
defining the data independent lexicon, which was done using simple applicative
categorial grammar (Ajdukiewicz-Bar-Hillel calculus). Second, we develop a
suitable semantic representation for GIS queries, which we call ?SQL, and a
method to translate natural-language queries via ?SQL into spatially-enabled
SQL. The third task is defining of the semantics of spatial relations (esp.
prepositions) in the lexicon in accordance with the intuitive understanding of
such relations, which involves tackling certain aspects of spatial prepositions
that where never dealt with before. The fourth task is the development of
methods to add the data dependent portion of the lexicon with minimal effort,
including an automatic tool that generates lexical entries from the actual
geographical database in use.
The paper is organized as follows: Section 2 introduces ?SQL and de-
scribes the translation scheme from natural language into SQL queries. Sec-
tion 3 reviews the architecture of the lexicon. Section 4 discusses semantic
issues concerning spatial relations in natural languages. Section 5 presents
our implementation, and section 6 concludes.
2 A compositional approach for building SQL queries
SQL is a recursive language in the sense that it allows using one query as part
of an expression within another query. However, due to its complex syntax,
the construction of an SQL query in a compositional way from a query in
natural language is far from being a straightforward task. One way to tackle
this problem is by using an intermediate representation [4,10]. While such
an intermediate language avoids the complications of composing SQL queries
directly, its downsides are the additional translation phase it requires and the
fact that such intermediate languages are usually not as expressive as the
target language.
We introduce an intermediate representation language, which we call ?SQL.
This language only adds the necessary ?compositional glue? to SQL. As a re-
sult, only a simple translation process is necessary to convert ?SQL queries
into normal SQL syntax. ?SQL expressions are basically expressions in the
simply typed ? Calculus with the addition of syntactic sugar for SQL-like
syntax.
The typical syntax of a select SQL-command for querying a database is:
SELECT < selectlist > FROM < tablelist > WHERE < whereclause >;
The selectlist parameter is usually a list of fields to be displayed, but it also
allows other expressions such as aggregate functions (e.g. field summation).
The tablelist parameter is a list of tables to query and whereclause is a
boolean expression that restricts the rows in the query.
The syntax of ?SQL is very close to that of an SQL whereclause, with
the addition of ? operators. The atoms of ?SQL are real numbers, strings
and typed identifiers. The base types in ?SQL are: t - Boolean, r - real
numbers, str - strings, g - spatial data and e - entries in the database. These
base types correspond to the base types that are found in GIS databases, with
the addition of one additional type, e, for database entries (tuples). Complex
expressions are built from atomic ones using function application exp1(exp2),
infix operators exp1 op exp2, and the operators ?v.exp and ?v.exp. The
infix operators in ?SQL correspond to SQL operators, and include Boolean
AND/OR, arithmetic operators (+,?, ?, /) and comparators (>,<,=, <=
, >=, ! =). One additional important operator in ?SQL is the dot operator,
as in var.fieldname, where var is of type e and fieldname is a function from
entries in the database to entities of a basic type (i.e. it is of type et, er or es).
A dot expression is equivalent to fieldname(var), a function that returns the
value of a field of a given entry.
In general, the only two syntactic elements in ?SQL that do not corre-
spond directly to SQL syntax are the ? and ? operators. Translation from
?SQL expressions to SQL queries is done by recursive traversal over the ex-
pression. During traversal, whenever certain patterns are recognized, these
patterns are replaced by a corresponding SQL select statement. Each ? op-
erator corresponds to a select statement, which can be nested inside another
select. In addition to ? operators, three different synthetic elements may affect
the translation pattern:
P1 A function over a ? expressions, as in f(?v.exp), is treated as an aggregate
function.
P2 In the simplest pattern, the type of the variable x in ?x is e, and it
corresponds to a query that returns a set of entries. When the variable that
the ? operator binds is of any other base type, the pattern: ?x.?y.(x = exp1
AND y.layer =?? layer1?? AND exp2) is expected, which is translated into
SELECT exp1 FROM layer1 WHERE exp2.
P3 Any additional ? operator which is not part of the pattern above is trans-
lated as a table join (where tablelist parameter contains more than one
query). For example, the expression ?xe.?ye.(x.layer = ?layer1?? AND
y.layer = ?layer2?? AND exp) is translated into: SELECT x.* FROM
layer1 AS x, layer2 AS y WHERE exp. Each additional ? adds an addi-
tional table to the list.
The translation process is guaranteed to be successful due to constraints
over the ?SQL expressions in the lexicon that enforce conformity to the above
patterns. As an example for ?SQL, consider the following fragment from our
lexicon:
Word Category Semantics
buildings N ?xe.(x.layeres = ?building?)
with N\N/N ?n1et.?n2et.?xe.(n1(x) AND n2(x))
more than Rs/R ?nr.?xr.(x > n)
two R 2
floors N\Rs ?prt.?xe.p(x.floorser)
highest N/N ?net.?xe.(n(x) AND (x.heighter =
max(rt)r(?rr.?ye.(n(y) AND r = y.heighter))))
Category R in the above table corresponds to type r and Rs corresponds
to the type (rt).
The natural language expression ?buildings with more than two floors?
will be parsed into the ?SQL expression: ?xe.(x.layeres = ?building? AND
x.floorser > 5). Note that while functional applications during parsing elimi-
nated most ? operators, the ? operator that is introduced by the lexical entry
for buildings is not eliminated. This remaining ?xe is used to describe a
query over a variable x. In order to generate an SQL query, however, one
additional piece of information is required: the name of a table to query. This
information is provided via the layer keyword (layers, or feature sets in GIS
terminology, are equivalent to tables in general databases). While usually
the fieldname following the dot operator is a name for an actual field in the
database (such as floors in the above example), layer is a virtual attribute
in ?SQL, used to associate a layer with a variable. Whenever an expression
such as x.layeres = ?building? is found, the parser associates x with the table
?building?, and hence the above expression is translated into the SQL query:
SELECT x.* FROM building AS x WHERE x.floors>5;
A bit more complex example is the query ?highest buildings?, which is
translated into: ?xe.(x.layeres = ?building?? AND x.heighter = max(rt)r(?rr.
?ye.(y.layeres =?? building?? AND r = y.heighter)))). This expression demon-
strates several features of ?SQL. Note that max is a free identifier, which is
expected to be a name of an SQL function. The function max receives a ?
expression, and is therefore interpreted as an aggregate function. Finally, the
expression in the argument of max fits pattern P2 above, and the result is:
SELECT x.* FROM building AS x WHERE x.floors=(SELECT max(y.floors)
FROM building);
3 Lexicon architecture
The data independent part of the lexicon is the core of our controlled lan-
guage. This is the part of the lexicon that involves general logical and spatial
operators that do not depend on the actual GIS. By carefully selecting the
data-independent lexical items, we are able to express very complex queries
while avoiding vagueness and ambiguity problems that often undermine the
usability of NLIs. An important part of our work is the ability to express
spatial relations between GIS objects. However, non-spatial lexical items are
an important part of the lexicon as well. In the first part of this section we de-
scribe the non-spatial items in the lexicon. In the following part we review the
spatially-related lexical items. Finally we present classes of data-dependent
lexical items.
3.1 Non-spatial lexical items
Non-spatial lexical items can be partitioned into the following groups:
? Measure units, such as meters, kilometers, miles, acres. The lexical def-
inition for these items converts any unit into standard units (e.g. metric
units).
? Numerical predicates, such as less than n, at least n, between n and m.
Numerical predicates represent sets of real numbers.
? Superlatives: biggest, smallest, most, least. The words most and least can
be used to refer to the maximal or minimal value of any numerical field in the
database. Other words such as largest and longest are used as abbreviation
for ?most area? and ?most length?.
? Boolean connectives: and, or, not.
? Other lexical entries: that, which, is, are, with, without, have.
3.2 Spatial lexical items
As mentioned before, we aim to design a controlled language that would avoid
the pitfalls of vagueness and context-dependent ambiguity. In order to satisfy
this requirement, we need to avoid vague qualitative relations such as near, far
and almost. Another type of relations that need to be avoided are projective
relations such as in front of, behind, left and right. The meaning of these
prepositions involves context-dependent[6] elements that are hard to handle
within a controlled language.
The following spatial relations are included in the lexicon:
? Intersectional relations, following Egenhofer?s 9-intersection model [3]: in,
outside of, borders, overlaps, crosses, contains and intersects. Note that
only the first two expressions are prepositions, while the others are verbs.
? Distance: the word from is used to specify exact distance, as in ?200m from
a lake?.
? Constructors: intersection of, border of and center of. These words are
used to refer to spatial entities that do not exist in the database, but can
be derived from existing objects. For example, assuming ?42nd Street? and
?Broadway? are objects in the database, ?the intersection of 42nd street
and Broadway? can be constructed by intersecting the geometrical repre-
sentations of the two streets.
? Relative orientation: north of, south east of and the 3-place relation between
are all used to describe the orientation of one object relative to another
object (or objects, as in the case of between).
? Superlatives: closest and furthest are spatially-related superlatives.
3.3 Data-dependent lexical items
Data dependent lexical items are lexical items that refer to specific data inside
the database and may therefore change from one data set to another. GIS
data are divided into separate thematic feature classes or layers, whereby each
layer consists of one type of geometrical entity such as a building, street or
utility pole. For each layer there is usually an associated set of attributes that
represent non-spatial data attached to real world geometric objects. These
may be boolean data, numeric data or strings. Examples for such attributes
are the number of floors in a building or the name of a street. String values
such as street names should be part of the lexicon as well.
Data-dependent items are represented in the lexicon in the form of tem-
plates, which are lexical items with parametrized values for layer name, at-
tribute name and attribute value. An example for such a template is:
?#strval? N/N{l = #layer} ?n.?x.(n(x) AND (x.#attr like #strval))
The ?#strval? template defines lexical items that refer to strings inside the
database. The lexical analyzer searches the database for strings that match
lexical tokens that are not present in the lexicon. For each such string the
above template is instantiated with the relevant layer name, attribute name
and string value. Similar templates are used for layer names and attributes of
various types. In case the lexical entries need to be different than the actual
names in the database, a definition file is used to add those lexical items and
instantiate the relevant templates for those items. No knowledge in ?SQL is
required in order to edit the definition file.
4 Semantics of spatial prepositions
While some progress was made in semantic theories of prepositional phrases
in recent years [18,7], certain aspects of spatial linguistic phenomena have
not been extensively treated in the semantic literature, but are nevertheless
crucial for interfaces to spatial databases. Two such aspects that are treated
in our system and are discussed below.
4.1 Eigenspace vs. Existential semantics
While previous work on prepositional semantics mainly dealt with relation-
ships between two distinct objects, GIS queries often correspond to relation-
ships between sets of objects. Consider the query ?buildings that are up to
200m from a lake?. In case there is more than one lake, we expect the system
to return any building such that there is at least one lake up to 200m from it.
In other words, it appears like the query existentially quantifies over lakes. On
the other hand, if we change the query to ?buildings that are at least 200m
from a lake?, we would expect the system to return buildings that are over
200m away from all the lakes. The query ?buildings that are between 200m
and 500m from a lake? has a yet more complex semantics, and should result
in any building such that there is at least one lake less than 500m from it and
there is no lake less than 200m from it.
The semantics of the above three queries becomes much clearer, however,
when instead of interpreting the indefinite ?a lake? as a quantifier (existential,
universal or other) over the lakes in the database, ?a lake? is interpreted as the
set of all lakes, and distance is measured with respect to the space taken by
the union of all lakes. We refer to this kind of interpretation for indefinites as
eigenspace semantics. In SQL, the eigenspace of a set of objects is evaluated
by using the aggregate function GeomUnion over a set of objects, as in:
SELECT geomunion(x.the geom) FROM lake AS x;
In our framework, eigenspace semantics is treated by enabling a type-
shifting from an indefinite noun-phrase into a special category G used for
representing the eigenspace. The ?SQL expression for G/N type-shifting is:
?n.geomunion(?g.?x.(n(x) AND g = x.the geom)) where the geom is the
attribute for the geometrical data of an object in GIS database. The ?SQL
expression for the preposition from, of category (((N\N)\RS)/G), is then
defined by: ?g.?p.?n.?x. (n(x) AND p(distance(x.the geom, g))).
It is important to note that while eigenspace semantics are used for spatial
prepositions, in the case of other spatial relations that are not expressed using
prepositions, such as the verbs contains and intersects, an indefinite is treated
in the usual way, as an existential quantifier. For example, if we ask about
?towns that contain a building with more than 10 floors?, the eigenspace se-
mantics would mean finding a town than contains all buildings with more than
one floor, whereas we expect to get any town that contains at least one building
Fig 1. Example for between Fig 2. Query result in QGIS
with more than 10 floors. We achieve the correct semantics in this case by pro-
viding a ?SQL expression for verbs such as contains that existentially quanti-
fies over the set of contained objects: ?n1.?n2.?x.?y.(n1(y) AND n2(y) AND
contains(x.the geom, y.the geom)).
4.2 Semantics of between
An additional aspect of spatial relations that has so far been ignored in the se-
mantic literature concerns the relations between non-convex objects. A funda-
mental spatial relation which is quite problematic in the context of non-convex
objects is the 3-place relation between.
Zwarts and Winter [18] suggest the following definition for between: X is
between Y and Z if X ? convexHull(Y ?Z)\Y \Z, for convex objects in X, Y
and Z. The problem is that many objects we deal with in the context of GISs
are not convex. For example, it could be quite handy to talk about objects
between two streets. However, streets are often non-convex shapes. As can
be seen in figure 1, the convex hull for two streets represented by the solid
lines includes areas that do not agree with our understanding of the expression
between the two streets. In order to overcome this problem, we suggest the
following definition:
Definition 4.1 Let X, Y and Z be sets of points. We say that X is between
Y and Z iff either there is a point x on the border of Y such that the shortest
line connecting x to Z crosses X, but does not cross Y, or there is a point y on
the border of Z such that the shortest line connecting y to Y crosses X, but
does not cross Z.
The areas between the streets according to Definition 1 are marked by
stripes. As can be seen from the illustration, the new definition is more
in agreement with our intuitive understanding of between. Note that while
the above is a strictly spatial definition of between, in some contexts people
may use between in sloppier ways (e.g., Buxton is between Manchester and
Sheffield). In our system, however, we wish to avoid the vagueness of such
sloppy usages.
5 Implementation
The NLI presented in this paper was implemented in C++. The parser reads
the lexicon from a text file that includes the syntactic categories, and the se-
mantics is represented using ?SQL expressions for all data-independent lexical
items. Data-dependent items are represented using templates, as explained in
section 3.3. When the user enters a natural-language query, the query is parsed
using a bottom-up right-to-left tabular Combinatorial Categorial Grammar
(CCG) parser that was developed as part of the NLI prototype. The resulting
?SQL expression is then converted into an SQL query as explained in section
2, which is sent to a spatially enabled database engine.
The system presented here uses PostGIS (http://postgis.refractions.net/)
as a back-end. PostGIS is an open-source GIS extension to the PostgreSQL
database engine, which implements the OpenGIS ?Simple features specifica-
tion for SQL? standard [2]. PostGIS basically supplies a set of functions that
operate on vector representations, such as a function that calculates distance
between polygons. The SQL queries are sent to PostGIS, which generates the
result in a form of a table which is loaded into a GIS front-end that supports
PostGIS, such as QGIS (http://www.qgis.org).
For example, the query ?Buildings that are up to 500m from the intersec-
tion of Elm street and Oak street? are converted into the SQL query, which
generates the result in figure 2:
(SELECT x.* FROM building AS x WHERE distance(x.the geom, intersection((
SELECT GeomUnion(x2.the geom) FROM street AS x2 WHERE x2.street nam
LIKE ?elm?),(SELECT GeomUnion(x3.the geom) FROM street AS x3 WHERE
x3.street nam LIKE ?oak?)))<=500)
6 Conclusions and future work
This work has presented an interface to GISs that is based on a controlled
fragment of English. We believe to have demonstrated that it is possible to
build such usable interfaces and express quite complex queries using a simple
fragment of English. Future work on this subject can be done at several dif-
ferent levels: expanding the lexicon further by adding quantifiers, comparison
between attributes of different objects and possibly anaphoric expressions.
More thorough theoretical study is required regarding semantic issues such
as eigenspace and between presented here, and finally, an empirical study is
necessary to evaluate how usable such interfaces are for actual GIS users of
varying skills and needs. We believe, however, that the general architecture
and prototype demo interface that we suggest can be developed into a useful
tool for planners and other professional users of GISs.
References
[1] I. Androutsopoulos and G. Ritchie. Database interfaces. In R. Dale, H. Moisl,
and H. Somers, editors, Handbook of Natural Language Processing, chapter 9,
pages 209?240. Marcel Dekker Inc., 2000.
[2] Open Geospatial Consortium. Simple Features Specification for SQL. http:
//www.opengis.org/docs/99-049.pdf.
[3] M. Egenhofer and J. Herring. Categorizing binary topological relations between
regions, lines and points in geographic databases. Technical report, Department
of Surveying Engineering, University of Maine, Orono, ME, 1991.
[4] P.P. Filipe and N.J. Mamede. Databases and natural language interfaces. In
JISBD 2000, pages 321?332, 2000.
[5] A.U. Frank and D.M. Mark. Language issues for GIS. In D. MacGuire,
M.F. Goodchild, and D. Rhind, editors, Geographical Information Systems:
Principles and Applications, pages 147?163. Wiley, New York, 1991.
[6] A. Hershkovits. Language and Spatial Cognition: an interdisciplinary study of
the prepositions in English. Cambridge University Press, Cambridge, 1986.
[7] M. Kracht. On the semantics of locatives. Linguistics and Philosophy, 25:157?
232, 2002.
[8] D.M Mark, S. Svorou, and D. Zubin. Spatial terms and spatial concepts:
Geographic, cognitive and linguistic perspectives. In International Geographic
Information Systems (IGIS), pages 101?112, Arlington, VA, 1987.
[9] M. Minock. A phrasal approach to natural language interfaces over databases.
In NLDB-2005, Alicante, Spain, June 2005.
[10] R. Nelken and N. Francez. Querying temporal databases using controlled
natural language. In COLING 2000 - Volume 2, pages 1076?1080, 2000.
[11] I. Pratt. Temporal prepositions and their logic. Artificial Intelligence, 166(1?
2):1?36, 2005.
[12] Mukesh Kumar Rohil. Natural language processing to query a geographic
information system(india) knowledgebase. In Map India, India, 2000.
[13] I. Schlaisich and M. Egenhofer. Multimodal spatial querying: What people
sketch and talk about. In C. Stephanidis, editor, 1st International Conference
on Universal Access in Human-Computer Interaction, pages 732?736, New
Orleans, LA, August 2001.
[14] J. Star and J. Estes. Geographic Information System, An Introduction. Prentice
Hall, Englewood Cliffs, NJ, 1990.
[15] Fangju Wang. Handling grammatical errors, ambiguity and impreciseness in
GIS natural language queries. Transactions in GIS, 7(1):103?121, 2003.
[16] H. Wang, A.M MacEachren, and G. Cai. Design of human-GIS dialogue
for communication of vague spatial concepts based on human communication
framework. In GIScience 2004, Adelphi, MD, 2004.
[17] J.M. Zelle and R.J. Mooney. Learning to parse database queries using
inductive logic programming. In Thirteenth National Conference on Aritificial
Intelligence, pages 1050?1055, Portland, OR, August 1996.
[18] J. Zwarts and Y. Winter. Vector space semantics: a modeltheoretic analysis of
locative prepositions. Journal of Logic, Language and Information, 9:171?213,
2000.
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 97?103,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Smoothing a Lexicon-based POS Tagger for Arabic and Hebrew 
 
 
Saib Mansour Khalil Sima'an Yoad Winter 
Computer Science, Technion ILLC Computer Science, Technion 
Haifa, 32000, Israel  Universiteit van Amsterdam Haifa, 32000, Israel  
 Amsterdam, The Netherlands and Netherlands Institute for Ad-
vanced Study 
Wassenaar, The Netherlands 
saib@cs.technion.ac.il simaan@science.uva.nl winter@cs.technion.ac.il 
 
  
Abstract 
We propose an enhanced Part-of-Speech 
(POS) tagger of Semitic languages that 
treats Modern Standard Arabic (hence-
forth Arabic) and Modern Hebrew 
(henceforth Hebrew) using the same 
probabilistic model and architectural set-
ting. We start out by porting an existing 
Hidden Markov Model POS tagger for 
Hebrew to Arabic by exchanging a mor-
phological analyzer for Hebrew with 
Buckwalter's (2002) morphological ana-
lyzer for Arabic. This gives state-of-the-
art accuracy (96.12%), comparable to Ha-
bash and Rambow?s (2005) analyzer-
based POS tagger on the same Arabic 
datasets. However, further improvement 
of such analyzer-based tagging methods is 
hindered by the incomplete coverage of 
standard morphological analyzer (Bar 
Haim et al, 2005). To overcome this cov-
erage problem we supplement the output 
of Buckwalter's analyzer with syntheti-
cally constructed analyses that are pro-
posed by a model which uses character 
information (Diab et al, 2004) in a way 
that is similar to Nakagawa's (2004) sys-
tem for Chinese and Japanese. A version 
of this extended model that (unlike Naka-
gawa) incorporates synthetically con-
structed analyses also for known words 
achieves 96.28% accuracy on the standard 
Arabic test set. 
 
1 Introduction 
Part-of-Speech tagging for Semitic languages has 
been an active topic of research in recent years. 
(Diab et al, 2004; Habash and Rambow, 2005; 
Bar-Haim et al, 2005) are some examples for this 
line of work on Modern Standard Arabic and Mod-
ern Hebrew. POS tagging systems aim at classify-
ing input sequences of lexemes by assigning each 
such sequence a corresponding sequence of most 
probable POS tags. It is often assumed that for 
each input lexeme there is a set of a priori possible 
POS tag categories, or a probability function over 
them, and the tagger has to choose from this lim-
ited set of candidate categories. We henceforth use 
the term lexicon to refer to the set of lexemes in a 
language and the mapping that assigns each of 
them candidate POS tags, possibly with additional 
probabilities.  
Two ways to obtain a lexicon can be distin-
guished in recent works on POS tagging in Semitic 
languages. Data-driven approaches like (Diab et al 
2004) employ the lexicon only implicitly when 
extracting features on possible POS tags from an-
notated corpora that are used for training the POS 
tagger. Lexicon-based approaches (Habash and 
Rambow, 2005; Bar-Haim et al, 2005) use a lexi-
con that is extracted from a manually constructed 
morphological analyzer (Buckwalter 2002 and 
Segal 2001 respectively).  
In this paper we show that although lexicon-
based taggers for Arabic and Hebrew may initially 
outperform data-driven taggers, they do not ex-
haust the advantages of data-driven approaches. 
97
 Consequently, we propose a hybrid model of data-
driven methods and lexicon-based methods, and 
show its advantages over both models, in a way 
that is reminiscent of Nakagawa's (2004) results 
for Chinese and Japanese.  
As a first step, we develop a Part-of-Speech tag-
ger that treats Arabic and Hebrew using the same 
probabilistic model and architectural setting. We 
start out from MorphTagger, a lexicon-based tag-
ger for Hebrew developed by Bar-Haim et al 
(2005), which uses standard Hidden Markov 
Model techniques. We port the existing 
MorphTagger implementation to Arabic by ex-
changing Segal's (2001) morphological analyzer 
with Buckwalter's (2002) morphological analyzer, 
and then training the tagger on the Arabic Tree-
bank (Maamouri et al, 2001). Remarkably, this 
gives state-of-the-art accuracy (96.12%) on the 
same Arabic datasets as Habash and Rambow 
(2005). To the best of our knowledge, this is the 
first time the same POS tagging architecture is 
used both for Arabic and Hebrew texts with com-
parable accuracy.  
Despite the initial advantages of this setting, our 
empirical study shows that in both languages, fur-
ther improvement in accuracy is hindered by the 
incompleteness of the morphological analyzer. By 
"incompleteness" we refer not only to the well-
studied problem of unknown words (out-of-
vocabulary). Our results show that for both Arabic 
and Hebrew, a more serious problem involves 
words for which the analyzer provides a set of 
analyses that does not contain the correct one. We 
find out that this is the case for 3% of the words in 
the development set. This obviously sets an upper 
bound on tagger accuracy using methods that are 
purely based on a manually constructed lexicon. 
We refer to this problem as the "incomplete lexi-
con" problem.  
We focus on devising a solution to the incom-
plete lexicon problem by smoothing. We supple-
ment the output of Buckwalter's analyzer with 
synthetically constructed analyses that are pro-
posed by a model which uses character information 
(Diab et al, 2004) in a way that is similar to Naka-
gawa's (2004) system for Japanese. Unlike Naka-
gawa's method, however, our smoothing method 
incorporates synthetically constructed analyses 
also for known words, though only when all avail-
able taggings of the sentence have low probabili-
ties according to our model. A version of this 
extended model achieves a modest improvement 
(96.28%) in accuracy over the baseline on the 
standard Arabic test set. 
This paper is structured as follows. In section  2 
we start with a brief discussion of previous work. 
Section  3 describes our adaptation of Bar Haim et 
al.?s POS tagging system to Arabic. In section  4 
we show that an architecture like Bar Haim et al?s, 
which relies on a morphological analyzer, is likely 
to suffer from coverage problems under any con-
figuration where it is used as a stand-alone. In sec-
tion  5 we present our new architecture and the 
method of combining the models. Section  6 con-
cludes. 
2 Relation to Previous Works 
Quite a few works have dealt with extending a 
given POS tagger, mainly by smoothing it using 
extra-information about untreated words. For ex-
ample, (Church, 1988) uses the simple heuristic of 
predicting proper nouns from capitalization. This 
method is not applicable to Arabic and Hebrew, 
which lack typographical marking of proper nouns. 
More advanced methods like those described by 
Weischedel et al (1993) incorporate the treatment 
of unknown words within the probability model. 
Weischedel et al use derivational and inflectional 
endings to infer POS tags of unknown words. Na-
kagawa (2004) addresses the problem of unknown 
words for Japanese and Chinese, and uses a hybrid 
method of word-level and character-level informa-
tion. In his model, Nakagawa uses character in-
formation (only) when handling unknown words, 
claiming that in word-level methods information 
about known words helps to achieve higher accu-
racy compared to character-level models. On the 
other hand, when it comes to unknown words, Na-
kagawa uses a character-level method, which is 
hypothesized to be more robust in such cases than 
word-level methods. 
Virtually all works that dealt with coverage 
problems of POS taggers have concentrated on the 
problem of ?unknown? words ? words that have no 
analysis in the initial tagging system. However, in 
the context of analyzer-based tagging systems, we 
also have to deal with the problem of ?known? 
words that miss the correct analysis in the morpho-
logical analyzer. In the Arabic and Hebrew data-
sets we have examined, this problem is more 
severe than the unknown words problem. Unlike 
98
 previous works, we propose to smooth the word-
segment driven model also for ?known? words. To 
avoid overgeneration, this is done only when all 
taggings of the sentence have low probability. 
3 Adapting a Hebrew POS-tagger to 
Arabic 
Bar Haim et al's (2005) POS tagging system, 
MorphTagger, was developed initially for Hebrew. 
Our work is mainly developed for Arabic and 
tested over Arabic data. Due to the similarity in the 
morphological processes in Hebrew and Arabic 
and the generality of Bar Haim et al's architecture, 
the adaptation process was fairly simple. However, 
as far as we know this is the first implementation 
of a unified model for Arabic and Hebrew that 
achieves state-of-the-art accuracy. MorphTagger 
requires two components: a morphological ana-
lyzer to produce a set of analyses for every lexeme, 
and a POS tagged corpus for acquiring an HMM 
disambiguator. The HMM disambiguator assigns a 
probability to every pair xxxxx, where 
n
n www ...11 =  is a sentence and nn ttt ...11 =  a corre-
sponding sequence of POS tags hypothesized by 
the analyzer. This probability is approximated in a 
standard HMM fashion: 
1 1 1 1 1 1 2
1
( , ) ( ) ( | ) ( | , ) ( | )
n
n n n n n
i i i i i
i
P w t P t P w t P t t t P w t? ?
=
= =?  
For an input sentence nw1 , the pair  xxxxx with 
the highest probability is selected. The language 
( ),|( 21 ?? iii tttP ) and lexical ( )|( ii twP ) models' 
parameters are estimated from the tagged corpus 
by Maximum-Likelihood Estimator (MLE) fol-
lowed by Katz backoff smoothing for the language 
model and Add-? smoothing for the lexical model, 
where a small ?=1 count is given to analyzes pro-
vided by the analyzer but not found in the training 
corpus. Furthermore, MorphTagger employs an 
array of other smoothing techniques explained in 
Bar Haim et al (2005).  
Our implementation of MorphTagger for Arabic 
was developed using Buckwalter?s (2002) Mor-
phological Analyzer v1.0 (BMA1.0), and the Ara-
bic Treebank part 1 v2.0 (ATB1), Part 2 v2.0 
(ATB2) and Part 3 v1.0 (ATB3). The ATB was 
chosen not only because of its size and comprehen-
siveness, but also because Buckwalter?s analyzer 
was developed in accordance with the ATB, which 
makes the task of combining information from 
both sources easier. In all our experiments we use a 
tag-set of 24 tags which was mapped from the 
original tag-set (191 tags in ATB1) using the map-
ping script of the ATB distribution. 
To check the ambiguity level and the difficulty 
of the task at hand, we ran BMA1.0 over a testing 
set extracted from ATB1. The average number of 
analyses per word is 1.83, and the average number 
of segmentations per word is 1.2, however, the task 
of disambiguating Arabic is still not easy, as 46% 
of the data is ambiguous. Those results are compa-
rable to the results of Bar Haim et al for Hebrew, 
according to which the average number of analyses 
per word is 2.17 with 1.25 segmentations on aver-
age per word, and 54% of the words are ambigu-
ous. 
The performance of MorphTagger over Arabic 
was measured using the same test settings of Diab 
et al (2004). Habash and Rambow (2005) use a 
different test setting drawn from ATB1. Although 
we could not reproduce the exact setting of Habash 
and Rambow, comparison to their reported accu-
racy is still quite telling due to the similarity of the 
data. The comparison between the accuracy of the 
three systems is summarized in Table 1. The re-
sults in this table were obtained using the correct 
(?gold?) segmentation and applying the standard F-
measure for POS tagging accuracy. The result of 
Diab et al was reproduced on their setting, and the 
result of Habash and Rambow is as reported in 
their paper. 
 
System Tagging accuracy 
MorphTagger 96.12 
Diab et al 95.81 
Habash and Rambow 97.5 
Table 1 - Comparison between systems over ATB1 
 
The result achieved by MorphTagger slightly 
exceeds Diab et al?s result (on the same test set-
ting) and is slightly inferior to Habash and Ram-
bow?s reported result. Overall, it is an encouraging 
result that the MorphTagger system that was de-
veloped for Hebrew could be easily ported to Ara-
bic and yield state-of-the-art results. 
In Table 2, we present the accuracies achieved 
for MorphTagger on a cross validated, 10-fold test, 
including the standard deviation results in paren-
theses. The results are reported both for gold-
segmentation (GS) and without GS.  
nn tw 11 ,
nn tw 11 ,
99
 Test setting Accuracy per word (%) F?=1 per Word-segment (%) 
 Segmentation Tagging Segmentation Tagging 
GS 100 
 
94.89 
(0.62) 
 
100 
 
95.436 
(0.53) 
 
without GS 99.015 (0.24) 
 
94.374 
(0.64) 
98.854 (0.28) 
 
94.727 
(0.56) 
 
Table 2 - MorphTagger performance cross validated 
 
Note that by tagging accuracy per word we 
mean the percentage of words correctly segmented 
and tagged. The tagging F-measure is calculated in 
the standard way, counting the correctly tagged 
word-segments and dividing it by the number of 
"gold" word-segments for recall, and further by the 
number of outputted word-segments for precision. 
Analyzing the POS tagging errors of MorphTag-
ger, we found that about 2.8% of the words in 
ATB1 were not correctly analyzed by the morpho-
logical analyzer. Such ?incomplete lexicon? prob-
lems inevitably lead to tagging errors in 
MorphTagger?s architecture. This problem is more 
serious still on data taken from ATB2 and ATB3, 
where respectively 4.5% and 5.3% of the data led 
to ?incomplete lexicon? problems. We conclude 
that a morphological analyzer can be used to im-
prove upon Diab et al?s results, as done in Habash 
and Rambow and in our straightforward applica-
tion of MorphTagger to Arabic. However, this 
method still suffers from considerable coverage 
problems, which are discussed in the following 
section. 
4 Coverage of Morphological Analysis 
for Arabic 
In order to analyze the coverage problem, we 
tested the coverage of BMA1.0 over parts of the 
ATB which were composed from articles taken on 
different periods of times. The results are summa-
rized in Table 3. The schema of the table includes, 
for each part of the ATB: (i) the number of tokens 
that include at least one Arabic character (hence-
forth ?Arabic words?1); (ii) Out-of-Vocabulary 
(OOV) words, unanalyzed by BMA1.0; (iii) the 
percentage of proper nouns (NNP) out of the OOV 
words; (iv) the number of ?no correct? words ? 
                                                          
1 This definition of Arabic words is taken from Buckwalter's 
analyzer. 
words for which BMA1.0 found at least one solu-
tion but the correct analysis according to the ATB 
was not among them; and (v,vi,vii) the number of 
proper nouns (NNP), nouns (NN) and adjectives 
(JJ) from "no correct". A problem that is unique to 
the ATB is that some words in the corpus were not 
manually annotated and were given the NO_FUNC 
tag. Those words are counted as Arabic words, but 
are ignored in the rest of the statistics of Table 3. 
The noticeable difference in OOV words be-
tween ATB1 and ATB2/ATB3 is expected, be-
cause the lexicon of BMA1.0 was developed using 
information extracted from ATB1. ATB2 and 
ATB3, which were developed after BMA1.0 was 
released (using a more advanced version of Buck-
walter's analyzer), show a different picture. In 
those two parts the OOV problem is not too hard: a 
heuristic that would assign NNP to each OOV 
word would be sufficient in most of the cases. 
However, the ?No Correct? problem is more diffi-
cult: NNPs account for 5% in ATB2 and 18% in 
ATB3 of these words, which are mostly dominated 
by missing adjectives and missing nouns (54% 
jointly in ATB2 and 37% jointly in ATB3).  
Taken together, the OOV problem and the ?No 
Correct? problem mean that more than 5% of the 
words in ATB2 and ATB3 cannot be tagged cor-
rectly using BMA1.0 unless further data are added 
to those provided by the morphological analyzer. A 
similar coverage result was reached for Hebrew by 
Bar Haim et al, using a morphological analyzer for 
Hebrew (Segal, 2001). Bar Haim et al report that 
for about 4% of the Hebrew words in their corpus, 
the correct analysis was missing. From these data 
we conclude that on top of systems like the ones 
proposed by Bar Haim et al and Habash and Ram-
bow,   we   need   to   enhance   the   morphological 
analyzer using additional analyses.
100
 ATB 
part 
Arabic 
words 
OOV NNP of 
OOV 
No Correct NNP of No 
Correct 
NN of No 
Correct 
JJ of No 
Correct 
1 123798 126   
(0.11%) 
21     
(16.67%) 
3369  
 (2.82%) 
0 517 
(15.35%) 
980 
(29.09%) 
2 125729 958   
(0.77%) 
497   
(51.88%) 
5663   
(4.53%) 
282 
(4.98%) 
1254 
(22.14%) 
1818 
(32.1%) 
3 293026 6405 
(2.2%) 
5241 
(81.83%) 
15484 
(5.32%) 
2864 
(18.5%) 
2238 
(14.45%) 
3494 
(22.57%) 
Table 3 - Coverage of Buckwalter's Analyzer 
 
5 Smoothing Using a Data-driven Charac-
ter-based Model 
So far we have shown that POS tagging models 
that use a morphological analyzer achieve high 
accuracy but suffer from coverage problems that 
can not be solved by a simple heuristic. On the 
other hand, models that use character-based infor-
mation are likely to make relatively good predic-
tions for words that are out of the vocabulary of the 
morphological analyzer. We hypothesize that this 
may be especially true for Semitic languages, due 
to their rich and systematic pattern (template) para-
digms. Such patterns add constant characters to 
root characters, and features of substrings of words 
may therefore help in predicting POS tags from 
those patterns. 
Our baseline models for the experiments are 
MorphTagger with a NNP heuristic (MorphTag-
ger+NNP) and ArabicSVM (Diab et al's system). 
As we have already reported in section  3, 
MorphTagger+NNP achieved 96.12% tagging ac-
curacy and ArabicSVM achieved 95.87% over the 
same testing data used by Diab et al One simple 
hybrid model would be adding the analyses pro-
duced by the SVM to the morphological analyzer 
analyses and disambiguate these analyses using 
MorphTagger's HMM. This system has improved 
accuracy ? it achieved accuracy of 96.18%, higher 
than both of the base models. 
The problem with such model is over-generation 
of the SVM: when checked over ATB1 and ATB2, 
40% of the new analyses introduced by the SVM 
are correct analyses, and 60% are wrong. To avoid 
this problem, we suggest conditioning the addition 
of SVM analyses on the sentence's tagging prob-
ability calculated by the HMM model. This is justi-
fied due to the fact that there is correlation between 
the probability of the tagging of a sentence given 
by a language model and the accuracy of the tag-
ging. The relation is shown in Figure 1. 
75
80
85
90
95
100
-800 -700 -600 -500 -400 -300 -200 -100 0
normalized log(P(s))
ac
cu
ra
cy
 
Figure 1 Probability VS Accuracy 
 
Figure 1 shows the relation between the accu-
racy of the tagging and the normalized logarithmic 
probability of the tagging. We normalize the prob-
ability of the tagging by the sentence length as 
longer sentences usually have lower probabilities.  
Following the previous conclusions, we propose 
a hybrid model which adds the analyses of the 
SVM only in cases where the tagging probability 
by the basic MorphTagger system is lower than an 
empirically calculated threshold. If the HMM is 
confident about the tagging it produces, the prob-
ability of the tagging will be high enough to pass 
the threshold, and then the tagging will be output-
ted without adding the SVM analyses which might 
add  noise  to  the  morphological  analyzer  output. A  
general algorithm is shown in Figure 2.
 
101
  
Figure 2 - Enhanced Tagging Algorithm 
 
Note that in the algorithm, a new (word, tag) 
pair introduced by the morphological analyzer or 
by the character model does not appear in the 
tagged corpus, therefore a small count ?=1 is given 
in such cases. This method can be improved fur-
ther, especially for the analyses produced by the 
data-driven character-based method. 
The accuracy we obtained using this system was 
96.28% which shows slight improvement over the 
previous simple hybrid system. Examining the er-
rors in the simple hybrid method and the condi-
tioned method, we see that the improvement is not 
smooth: the conditioned model includes errors 
which did not exist in the simple model. These er-
rors occur when correct analyses of the character-
based model were discarded. In general, however, 
the conditioned method chooses more correct 
analyses. It should be noted that adding the charac-
ter-based model analyses boosted the coverage 
from 97% to 98%, but the accuracy did not im-
prove to the same level. The main cause for this is 
the weak relation between the probability of a sen-
tence and the accuracy. As it is difficult to model 
this relation, we believe that more time should be 
invested to improve the HMM probabilities espe-
cially for the character model analyses, which can 
boost the chances of choosing good analyses. 
6 Conclusions and Future Work 
This paper demonstrates that it is possible to suc-
cessfully port a POS tagger originally built for He-
brew to Arabic using a morphological analyzer and 
a tagged corpus. The POS tagger (called 
MorphTagger) achieves state-of-the-art results 
both on Hebrew and Arabic. Despite this positive 
result we find that further improvement of accu-
racy is hindered by the coverage of the morpho-
logical analyzer. Contrary to earlier work on POS 
tagging, the problem turns out not so much in un-
known (OOV) lexemes as much as in known lex-
emes for which the correct tag is missing. We 
showed empirical evidence that this problem arises 
for the available treebanks and morphological ana-
lyzers for both Arabic and Hebrew. We propose an 
approach that smoothes a given lexical model (ob-
tained from a morphological analyzer and an anno-
tated corpus) by adding synthetically constructed 
analyses, obtained from a POS tagger that com-
bines character-level information. Unlike earlier 
work, we apply this smoothing only when the 
probabilistic model assigns probabilities lower 
than a threshold to all possible POS taggings of the 
input sentence. This way we obtain moderate im-
provement in Arabic POS tagging. 
The problem of missing lexeme-POS pairs in 
POS taggers for Semitic languages is more severe 
than in languages like English. We conjecture that 
this is because of the more complex morphology of 
Semitic languages. 
In future work it might be worthwhile to con-
sider morphological processes that are more com-
plex than the standard affixation 
(suffixing/prefixing) processes in order to general-
ize better over cases in the training data. Such a 
generalization may provide better coverage of lex-
eme-POS pairs and would increase the upper 
bound on accuracy. 
Given a sentence s, perform the following steps: 
1. Produce analyses for each word in s using the morphological analyzer 
combined with the corpus analyses. 
2. Calculate lexical and contextual probabilities using available annotated 
corpora (using Maximum Likelihood Estimation). 
3. Run Viterbi's Algorithm for HMM disambiguation, and calculate a rank 
of the tagging which is composed from the probability given by the 
model and the length of the sentence. 
4. If [rank>threshold] output tagging. 
4'. [Otherwise] run the character based model over the sentence and add the 
new analyses generated.  
5'. Combine the analyses generated by the morphological analyzer and the 
character-based model, update the lexical probabilities and rerun the 
model. 
102
 References  
Roy Bar Haim, Khalil Sima?an and Yoad Winter. 2005. 
Choosing an Optimal Architecture for Segmentation 
and POS-Tagging of Modern Hebrew. ACL Work-
shop on Computational Approaches to Semitic Lan-
guages. A revised and extended version to appear in 
Journal of Natural Language Engineering. 
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: a case 
study in part of speech tagging. In Computational 
Linguistics 21, pages 543-565. 
Tim Buckwalter. 2002. Arabic Morphological Analyzer 
Version 1.0. Linguistic Data Consortium, University 
of Pennsylvania. 
Kenneth W. Church. 1988. A stochastic parts program 
and noun phrase parser for unrestricted text. Pro-
ceedings of the second conference on Applied natural 
language processing, Pages 136-143. 
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004. 
Automatic Tagging of Arabic Text: From Raw Text to 
Base Phras e Chunks. In HLT-NAACL: Short Pa-
pers, pages 149-152. 
Nizar Habash and Owen Rambow. 2005. Arabic To-
kenization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In 
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 573-
580, Ann Arbor. 
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language 
model based Arabic word segmentation. In ACL, 
pages 399-406. 
Mohamed Maamouri and Ann Bies. 2004. Developing 
an Arabic treebank: Methods, guidelines, proce-
dures, and tools. In Proceedings of the Workshop on 
Computational Approaches to Arabic Script-based 
Languages (COLING), Geneva. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language Proc-
essing. The MIT press, Cambridge, Massachusetts. 
Tetsuji Nakagawa. 2004. Chinese and Japanese word 
segmentation using word-level and character-level 
information. In Proceedings of the 20th International 
Conference on Computational Linguistics, pages 
466-472, Geneva. 
Erel Segal. 2001. Hebrew morphological analyzer for 
Hebrew undotted texts. Master's thesis, Computer 
Science Department, Technion, Haifa, Israel. 
Ralph Weischedel, Marie Meteer, Richard Schwartz, 
Lance Ramshaw and Jeff Palmucci. 1993. Coping 
with Ambiguity and Unknown Words through Prob-
abilistic Models. Computational Linguistics (Special 
issue on using large corpora: II) volume 19, pages 
361-382. 
 
103
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 77?80,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Annotating by Proving using SemAnTE
Assaf Toledo
1
Stavroula Alexandropoulou
1
Sophie Chesney
2
Robert Grimm
1
Pepijn Kokke
1
Benno Kruit
3
Kyriaki Neophytou
1
Antony Nguyen
1
1
- Utrecht University
2
- University College London
3
- University of Amsterdam
{a.toledo,s.alexandropoulou,y.winter}@uu.nl
sophie.chesney.10@ucl.ac.uk, {pepijn.kokke,bennokr}@gmail.com
{r.m.grimm,k.neophytou,a.h.nguyen}@students.uu.nl
Yoad Winter
1
Abstract
We present SemAnTE, a platform for
marking and substantiating a semantic an-
notation scheme of textual entailment ac-
cording to a formal model. The plat-
form introduces a novel approach to an-
notation by providing annotators immedi-
ate feedback whether the data they mark
are substantiated: for positive entailment
pairs, the system uses the annotations to
search for a formal logical proof that val-
idates the entailment relation; for negative
pairs, the system verifies that a counter-
model can be constructed. By integrating
a web-based user-interface, a formal lexi-
con, a lambda-calculus engine and an off-
the-shelf theorem prover, this platform fa-
cilitates the creation of annotated corpora
of textual entailment. A corpus of several
hundred annotated entailments is currently
in preparation using the platform and will
be available for the research community.
1 Introduction
The Recognizing Textual Entailment (RTE) chal-
lenges (Dagan et al., 2006) advance the devel-
opment of systems that automatically determine
whether an entailment relation obtains between a
naturally occurring text T and a manually com-
posed hypothesis H. The RTE corpus (Bar Haim
et al., 2006; Giampiccolo et al., 2008), which is
currently the only available resource of textual en-
tailments, marks entailment candidates as posi-
tive/negative.
1
For example:
Example 1
? T: The book contains short stories by the fa-
mous Bulgarian writer, Nikolai Haitov.
? H: Nikolai Haitov is a writer.
2
? Entailment: Positive
This categorization does not indicate the linguistic
phenomena that underlie entailment or their con-
tribution to inferential processes. In default of
a gold standard identifying linguistic phenomena
triggering inferences, entailment systems can be
compared based on their performance, but the in-
ferential processes they employ to recognize en-
tailment are not directly accessible and conse-
quently cannot be either evaluated or improved
straightforwardly.
We address this problem by elucidating some
of the central inferential processes underlying en-
tailments in the RTE corpus, which we model for-
mally within a standard semantic theory. This al-
lows us not only to indicate linguistic phenomena
that are involved in the recognition of entailment
by speakers, but also to provide formal proofs that
substantiate the annotations and explain how the
1
Pairs of sentences in RTE 1-3 are categorized in two
classes: yes- or no-entailment; pairs in RTE 4-5 are cate-
gorized in three classes: entailment, contradiction and un-
known. We label the judgments yes-entailment from RTE 1-3
and entailment from RTE 4-5 as positive, and the other judg-
ments as negative.
2
Pair 622 from the development set of RTE 2.
77
modeled phenomena interact and contribute to the
recognition process. In this sense the we adopt an
Annotating by Proving approach to textual entail-
ment annotation.
The annotation work is done using the Se-
mAnTE (Semantic Annotation of Textual Entail-
ment) platform, which incorporates a web-based
user-interface, a formal lexicon, a lambda-calculus
engine and an off-the-shelf theorem prover. We
are currently using this platform to build a new
corpus of several hundred annotated entailments
comprising both positive and negative pairs. We
decided to focus on the semantic phenomena of
appositive, restrictive and intersective modifica-
tion as these semantic phenomena are prevalent in
the RTE datasets and can be annotated with high
consistency, and as their various syntactic expres-
sions can be captured by a limited set of concepts.
3
In the future, we plan to extend this sematic model
to cover other, more complex phenomena.
2 Semantic Model
To model entailment in natural language, we as-
sume that entailment describes a preorder on sen-
tences. Thus, any sentence trivially entails itself
(reflexivity); and given two entailments T
1
? H
1
and T
2
? H
2
where H
1
and T
2
are identical sen-
tences, we assume T
1
? H
2
(transitivity). We
use a standard model-theoretical extensional se-
mantics, based on the simple partial order on the
domain of truth-values. Each model M assigns
sentences a truth-value in the set {0, 1}. Such a
Tarskian theory of entailment is considered ade-
quate if the intuitive entailment preorder on sen-
tences can be described as the pairs of sentences
T and H whose truth-values [[T]]
M
and [[H]]
M
sat-
isfy [[T]]
M
? [[H]]
M
for all models M .
We use annotations to link between textual
representations in natural language and model-
theoretic representations. This link is established
by marking the words and structural configura-
tions in T and H with lexical items that encode
semantic meanings for the linguistic phenomena
that we model. The lexical items are defined for-
mally in a lexicon, as illustrated in Table 1 for ma-
jor lexical categories over type:s e for entities, t
for truth-values, and their functional compounds.
3
This conclusion is based on an analysis of RTE 1-4, in
which these modification phenomena were found to occur in
80.65% of the entailments and were annotated with cross-
annotator agreement of 68% on average.
Category Type Example Denotation
Proper Name e Dan dan
Indef. Article (et)(et) a A
Def. Article (et)e the ?
Copula (et)(et) is IS
Noun et book book
Intrans. verb et sit sit
Trans. verb eet contain contain
Pred. Conj. (et)((et)(et)) and AND
Res. Adj. (et)(et) short R
m
(short)
Exist. Quant. (et)(et)t some SOME
Table 1: Lexicon Illustration
Denotations that are assumed to be arbitrary are
given in boldface. For example, the intransitive
verb sit is assigned the type et, which describes
functions from entities to a truth-values, and its
denotation sit is an arbitrary function of this type.
By contrast, other lexical items have their denota-
tions restricted by the given model M . As illus-
trated in Figure 1, the coordinator and is assigned
the type (et)((et)(et)) and its denotation is a func-
tion that takes a function A of type et and returns
a function that takes a function B, also of type et,
and returns a function that takes an entity x and
returns 1 if and only if x satisfies both A and B.
A = IS = ?A
et
.A
? = ?A
et
.
{
a A = (?x
e
.x = a)
undefined otherwise
WHO
A
= ?A
et
.?x
e
.?(?y.y = x ?A(x))
R
m
= ?M
(et)(et)
.?A
et
.?x
e
.M(A)(x) ?A(x)
SOME = ?A
et
.?B
et
.?x.A(x) ?B(x)
AND = ?A
et
.?B
et
.?x
e
.A(x) ?B(x)
Figure 1: Functions in the Lexicon
By marking words and syntactic constructions
with lexical items, annotators indicate the under-
lying linguistic phenomena in the data. Further-
more, the formal foundation of this approach al-
lows annotators to verify that the entailment re-
lation (or lack thereof) that obtains between the
textual forms of T and H also obtains between
their respective semantic forms. This verification
guarantees that the annotations are sufficient in the
sense of providing enough information for recog-
nizing the entailment relation based on the seman-
tic abstraction. For example, consider the simple
entailment Dan sat and sang?Dan sang and as-
sume annotations of Dan as a proper name, sat
and sang as intransitive verbs and and as predi-
cate conjunction. The formal model can be used
to verify these annotations by constructing a proof
as follows: for each model M :
78
[[Dan [sat [and sang]] ]]
M
= ((AND(sing))(sit))(dan) analysis
= (((?A
et
.?B
et
.?x
e
.A(x) ?
B(x))(sing))(sit))(dan)
def. of AND
= sit(dan) ? sing(dan) func. app. to sing,
sit and dan
? sing(dan) def. of ?
= [[Dan sang ]]
M
analysis
3 Platform Architecture
The platform?s architecture is based on a client-
server model, as illustrated in Figure 2.
Figure 2: Platform Architecture
The user interface (UI) is implemented as a
web-based client using Google Web Toolkit (Ol-
son, 2007) and allows multiple annotators to ac-
cess the RTE data, to annotate, and to substanti-
ate their annotations. These operations are done
by invoking corresponding remote procedure calls
at the server side. Below we describe the system
components as we go over the work-flow of anno-
tating Example 1.
Data Preparation: We extract T -H pairs from
the RTE datasets XML files and use the Stanford
CoreNLP (Klein and Manning, 2003; Toutanova
et al., 2003; de Marneffe et al., 2006) to parse each
pair and to annotate it with part-of-speech tags.
4
Consequently, we apply a naive heuristic to map
the PoS tags to the lexicon.
5
This process is called
4
Version 1.3.4
5
This heuristic is naive in the sense of not disambiguating
verbs, adjectives and other types of terms according to their
semantic features. It is meant to provide a starting point for
the annotators to correct and fine-tune.
as part of the platform?s installation and when an-
notators need to simplify the original RTE data in
order to avoid syntactic/semantic phenomena that
the semantic engine does not support. For exam-
ple, the bare plural short stories is simplified to
some short stories as otherwise the engine is un-
able to determine the quantification of this noun.
Annotation: The annotation is done by mark-
ing the tree-leaves with entries from the lexicon.
For example, short is annotated as a restrictive
modifier (MR) of the noun stories, and contains
is annotated as a transitive verb (V 2). In addition,
annotators manipulate the tree structure to fix pars-
ing mistakes and to add leaves that mark semantic
relations. For instance, a leaf that indicates the ap-
position between the famous Bulgarian writer and
Nikolai Haitov is added and annotated as WHO
A
.
The server stores a list of all annotation actions.
Figure 3 shows the tree-view, lexicon, prover and
annotation history panels in the UI.
Proving: When annotating all leaves and ma-
nipulating the tree structures of T and H are done,
the annotators use the prover interface to request
a search for a proof that indicates that their anno-
tations are substantiated. Firstly, the system uses
lambda calculus reductions to create logical forms
that represent the meanings of T and H in higher-
order logic. At this stage, type errors may be re-
ported due to erroneous parse-trees or annotations.
In this case an annotator will fix the errors and re-
run the proving step. Secondly, once all type er-
rors are resolved, the higher-order representations
are lowered to first order and Prover9 (McCune,
2010) is executed to search for a proof between
the logical expressions of T and H .
6
The proofs
are recorded in order to be included in the corpus
release. Figure 4 shows the result of translating T
and H to an input to Prover9.
4 Corpus Preparation
We have so far completed annotating 40 positive
entailments based on data from RTE 1-4. The an-
notation is a work in progress, done by four Master
students of Linguistics who are experts in the data
and focus on entailments whose recognition re-
lies on a mixture of appositive, restrictive or inter-
sective modification. As we progress towards the
compilation of a corpus of several hundred pairs,
we extend the semantic model to support more in-
ferences with less phenomena simplification.
6
Version 2009-11A
79
Figure 3: User Interface Panels: Annotation History, Tree-View, Prover Interface and Lexicon Toolbox
formulas(assumptions).
all x0 (((writer(x0) & bulgarian(x0)) &
famous writer bulgarian(x0))? x0=c1).
all x0 (((stories(x0) & short stories(x0)) & exists x1 (by
stories short stories(x1, x0) & (x1=c1 & x1=Nikolai
Haitov)))? x0=c2). all x0 (book(x0)? x0=c3).
contains(c2, c3).
end of list.
formulas(goals).
exists x0 (writer(x0) & x0=Nikolai Haitov).
end of list.
Figure 4: Input for Theorem Prover
5 Conclusions
We introduced a new concept of an annotation
platform which implements an Annotating by
Proving approach. The platform is currently in
use by annotators to indicate linguistic phenomena
in entailment data and to provide logical proofs
that substantiate their annotations. This method
guarantees that the annotations constitute a com-
plete description of the entailment relation and can
serve as a gold-standard for entailment recogniz-
ers. The new corpus will be publicly available.
Acknowledgments
The work of Stavroula Alexandropoulou, Robert
Grimm, Sophie Chesney, Pepijn Kokke, Benno
Kruit, Kyriaki Neophytou, Antony Nguyen, Assaf
Toledo and Yoad Winter was supported by a VICI
grant number 277-80-002 by the Netherlands Or-
ganisation for Scientific Research (NWO).
References
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. Machine Learning Challenges. Evaluat-
ing Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment, pages
177?190.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology. The Stanford Natural
Language Processing Group.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Elena Cabrio. 2008. The
fourth pascal recognising textual entailment chal-
lenge. In TAC 2008 Proceedings.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. ACL.
William McCune. 2010. Prover9 and Mace4. http:
//www.cs.unm.edu/
?
mccune/prover9/.
Steven Douglas Olson. 2007. Ajax on Java. O?Reilly
Media.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. ACL.
80
